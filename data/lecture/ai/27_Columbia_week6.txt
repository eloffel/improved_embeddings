arti   cial intelligence

machine learning

basic concepts

ansaf salleb-aouissi

columbia university - fall 2014 w4701 section 2

terminology

machine learning, data science, data mining, data analysis, sta-
tistical learning, knowledge discovery in databases, pattern dis-
covery.

realized with c(cid:13)tagxedo.com

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

applications of ml

    we all use it on a daily basis. examples:

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

applications of ml

id126s (collaborative    ltering)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

applications of ml

search engines

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

applications of ml

email

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

applications of ml

zipcode handwritten recognition

lecun et al. 1989.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

applications of ml

face recognition

credit: at&t laboratories cambridge.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

applications of ml

detection of breast cancer in mammography images

credit: http://marathon.csee.usf.edu/mammography/database.html

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

applications of ml

other applications:

    credit card fraud detection

    machine translation from a language to another

    natural language processing (e.g., id39)

    obama 2012 campaign

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

applications of ml

    we all contribute with some data on a daily basis.

    supermarket,

    movies,

    doctor visit,

    internet,

    etc.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

10

types of data

    vector of features (variables, attributes, covariates).

e.g.,

(age, occupation, bmi).

    image
    video
    graph
    heterogenous

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

11

interdisciplinary    eld

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

12

ml!statistics!visualization!economics!  databases! signal processing! engineering !biology!ml versus statistics

statistics:

machine learning:

    hypothesis testing
    experimental design
    anova
    id75
    id28
    glm
    pca

    id90
    rule induction
    neural networks
    id166s
    id91 method
    association rules
    feature selection
    visualization
    id114
    genetic algorithm

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

13

ml versus statistics

http://statweb.stanford.edu/~jhf/ftp/dm-stat.pdf

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

14

de   nition

   how do we create computer programs that improve with experi-
ence?   

http://videolectures.net/mlas06_mitchell_itm/

tom mitchell

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

15

de   nition

   how do we create computer programs that improve with experi-
ence?   

http://videolectures.net/mlas06_mitchell_itm/

tom mitchell

   a computer program is said to learn from experience e with
respect to some class of tasks t and performance measure p , if
its performance at tasks in t , as measured by p , improves with
experience e.    

tom mitchell. machine learning 1997.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

16

ml tasks

binary classi   cation (categorization)
input:    examples    (instances, objects, individuals, observations)
with labels (outcome, output, response variable).

(x1, y1), . . . , (xn, yn) / xi     x     rn, yi     y = {   1, +1}

output: h : x        y
example: approve credit yes/no, spam/ham.

question: how about digit recognition?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

17

ml tasks

binary classi   cation

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

18

ml tasks

data segmentation or id91
input:    examples    without labels.

x1, . . . , xn, xi     x     rn

output: f : x        {c1, . . . ck} (set of clusters).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

19

!"#!$#ml tasks

regression
input:    examples    with labels.

(x1, y1), . . . , (xn, yn) / xi     x     rn, yi     r

output: f : x        r
example: amount of credit.

density estimation
input:    examples    with labels.

(x1, y1), . . . , (xn, yn) / xi     x     rn, yi     y = {   1, +1}

output: f : x        [0, 1](p (y = 1|x)
example: id203 of loan default, disease.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

20

ml tasks

other tasks: ranking, frequent pattern mining, etc.
top algorithms:

http://www.cs.umd.edu/~samir/498/10algorithms-08.pdf

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

21

machine learning founders

    tom mitchell
    vladimir vapnik
    leo breiman
    robert schapire
    trevor hastie
    robert tibshirani
    jerome friedman
    ryszard s. michalski
    richard o. duda
    peter e. hart
    david g. stork
    alexander smola
    andrew mccallum
    lise getoor
    zoubin ghahramani
    jaime carbonell
    michael collins
    michael i. jordan
    andrew ng
    bernhard sch  olkopf
    leo breiman
    yann lecun
    etc.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

22

machine learning books

- richard o. duda, peter e. hart, david g. stork. pattern classi   cation.
wiley, 2001.
- kearns and vazirani: an introduction to computational learning theory, mit
press, (1994).
- l. wasserman: all of statistics, springer, (2004).
- mackay: id205, id136, and learning algorithms, (2003).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

23

machine learning resources

    major journals/conferences: icml, nips, uai, ecml/pkdd,

jmlr, mlj, etc.

    machine learning video lectures:

http://videolectures.net/top/computer_science/machine_

learning/

    machine learning (theory):

http://hunch.net/

    linkedin ml groups:    big data    scientist, etc.
    women in machine learning:

https://groups.google.com/forum/#!forum/

women-in-machine-learning

    kdd nuggets http://www.kdnuggets.com/
    nyc machine learning meetups

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

24

unsupervised learning

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

25

!"#$%&"'('!"#$%&"')'unsupervised learning

examples: id116, gaussian mixtures, hierarchical id91,
spectral id91, etc.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

26

!"#$%&"'('!"#$%&"')'supervised learning

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

27

!"#$%&"'('!"#$%&"')'supervised learning

examples: support vector machines, neural networks, decision
trees, k-nearest neighbors, naive bayes, etc.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

28

!"#$%&"'('!"#$%&"')'*"+,-,./'0.%/1#&2'supervised vs. unsupervised

unsupervised learning:
learning a model from unlabeled data.

supervised learning:
learning a model from labeled data.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

29

supervised learning

training data:   examples    x with    labels    y.

(x1, y1), . . . , (xn, yn) / xi     rd

    classi   cation: y is discrete. to simplify, y     {   1, +1}

f : rd        {   1, +1}

f is called a binary classi   er.

    regression: y is a real value, y     r

f : rd        r

f is called a regressor.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

30

supervised learning

classi   cation:

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

31

!"#$%&"'('!"#$%&"')'!"#$%&"'('!"#$%&"')'!"#$%&"'('!"#$%&"')'!"#$%&"'('!"#$%&"')'!"#$%&"'('!"#$%&"')'supervised learning

regression:

example: income in function of age.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

32

!"#$%&'($")"supervised learning

regression:

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

33

!"#$%&'($")"supervised learning

regression:

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

34

!"#$%&'($")"supervised learning

regression:

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

35

!"#$%&'($")"training and testing

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

36

!"#$%$%&'()*'+,'-.&/"$*01'+/2).'345'training and testing

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

37

!"#$%$%&'()*'+,'-.&/"$*01'+/2).'345'6%7/1)8''&)%2)"8''#&)8''4#1$.9'(*#*:(8';$<7/2)'=")2$*'#1/:%*'>'=")2$*'9)(?%/'training and testing

note: not every ml method builds a model!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

38

!"#$%$%&'()*'+,'-.&/"$*01'+/2).'345'6%7/1)8''&)%2)"8''#&)8''4#1$.9'(*#*:(8';$<7/2)'=")2$*'#1/:%*'>'=")2$*'9)(?%/'k-nearest neighbors
    a basic non-parametric instance-based learning method.
    lazy learning (why?).
    assumes all examples (instances) are points in the d dimen-

sional space rd.

    uses the standard euclidian distance to de   ne nearest neigh-
bors. the distance between two instances (examples) xi and
xj is de   ned by:

(cid:118)(cid:117)(cid:117)(cid:117)(cid:116) d(cid:88)

k=1

d(xi, xj) =

(xik     xjk)2

    what do we do when features are at di   erent scales? (e.g.,

weight in pounds and height in feet).

    label can be either discrete-valued or real-valued (qualitative

or quantitative).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

39

k-nearest neighbors

training algorithm:
add each training example (x, y) to the dataset d.
x     rd, y     {+1,    1}.

classi   cation algorithm:

given an example xq to be classi   ed. suppose nk(xq) is the set of
the k-nearest neighbors of xq.

  yq = sign( (cid:88)

yi)

xi   nk(xq)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

40

k-nearest neighbors

training algorithm:
add each training example (x, y) to the dataset d
x     rd, y     {+1,    1}.

classi   cation algorithm:

given an example xq to be classi   ed. suppose nk(xq) is the set of
the k-nearest neighbors of xq.

  yq = sign( (cid:88)

yi)

xi   nk(xq)

question1: what if the sum is zero?

question2: how to extend it to the regression problem?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

41

k-nearest neighbors

3-nn. credit: introduction to statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

42

k-nearest neighbors

credit: introduction to statistical learning.

question: can you draw an approximate decision boundary for k = 3?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

43

k-nearest neighbors

credit: introduction to statistical learning.

question: what are the pros and cons of id92?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

44

k-nearest neighbors

pros:
+ simple to implement.
+ works well in practice.
+ does not require to build a model, make assumptions, tune

parameters.

+ can be extended easily with news examples.

cons:

- requires large space to store the entire training dataset.
- slow! given n examples and d features. the method takes

o(n    d) to run.

- su   ers from curse of dimensionality.

question: improvements?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

45

applications of id92

credit: document spectrum for layout analysis. l. o   gorman. tpami 1993

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

46

applications of id92

document spectrum for document analysis.

credit: document spectrum for layout analysis. l. o   gorman. tpami 1993

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

47

applications of id92

1. information retrieval.
2. handwritten character classi   cation using nearest neighbor in

large databases. pami 1994.

3. oversampling.
4. etc.

further readings:
1. duda and hart book.
2. tom mitchell book.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

48

training and testing

question: how can we be con   dent about f predicting y given x?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

49

!"#$%$%&'()*'+,'-.&/"$*01'+/2).'345'6%7/1)8''&)%2)"8''#&)8''4#1$.9'(*#*:(8';$<7/2)'=")2$*'#1/:%*'>'=")2$*'9)(?%/'training and testing

question: how can we be con   dent about f predicting y given x?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

50

!"#$%$%&'()*'+,'-.&/"$*01'+/2).'345'6%7/1)8''&)%2)"8''#&)8''4#1$.9'(*#*:(8';$<7/2)'=")2$*'#1/:%*'>'=")2$*'9)(?%/'training and testing

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

51

!"#$%&'()&'!"#$%&'()&'!"#$%&'()&'training and testing

high bias (under   tting)

just right!

high variance (over   tting)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

52

!"#$%&'()&'!"#$%&'()&'!"#$%&'()&'training and testing

    we calculate etrain the in-sample error (training error or em-

pirical error/risk).

etrain(f ) =

n(cid:88)

i=1

(cid:96)oss(yi, f (xi))

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

53

training and testing

    we calculate etrain the in-sample error (training error or em-

pirical error/risk).

etrain(f ) =

n(cid:88)

    classi   cation error:

(cid:96)oss(yi, f (xi)) =

(cid:96)oss(yi, f (xi))

i=1

(cid:40) 1 if sign(yi) (cid:54)= sign(f (xi))

0 otherwise

    least square loss:

(cid:96)oss(yi, f (xi)) = (yi     f (xi))2

    there are other id168s used in machine learning e.g.,

hinge loss, logistic loss, etc. we will see some of them.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

54

training and testing

    we calculate etrain the in-sample error (training error or em-

pirical error/risk).

n(cid:88)

etrain(f ) =

i=1

(cid:96)oss(yi, f (xi))

    we aim to have etrain(f ) small, i.e., minimize etrain(f )
    we hope that etest(f ), the out-sample error (test/true error),

will be small too.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

55

over   tting/under   tting

an intuitive example

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

56

structural risk minimization

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

57

!"#$%&   ()*+""("*,(-*******************************************.(/01#2%34*(5*36#*/($#1*************************************7%86*9999:#;3*#""("****9999:"<%)%)8*#""("*7%86*=%<;****************,(-*=%<;**,(-*><"%<)&#*********7%86*><"%<)&#*?)$#"@a)8****************b(($*/($#1;*****cd#"@a)8************structural risk minimization

question: how do we check that the model is under   tting?

question: how do we check that the model is over   tting?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

58

!"#$%&   ()*+""("*,(-*******************************************.(/01#2%34*(5*36#*/($#1*************************************7%86*9999:#;3*#""("****9999:"<%)%)8*#""("*7%86*=%<;****************,(-*=%<;**,(-*><"%<)&#*********7%86*><"%<)&#*?)$#"@a)8****************b(($*/($#1;*****cd#"@a)8************avoid over   tting

introduce a bias! e.g., use simple models.

    reduce the number of features manually or do feature selection

(later).

    do a model selection (later).
    use id173 (keep the features but reduce their impor-

tance by setting small parameter values), (later).
    do a cross-validation to estimate the test error.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

59

occam   s razor

occam   s razor:

prefer the shortest hypothesis that    ts the data.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

60

id173: intuition

we want to minimize:

classi   cation term + c    id173 term

n(cid:88)

i=1

(cid:96)oss(yi, f (xi)) + c    r(f )

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

61

id173: intuition

f (x) =   0 +   1x ... (1)
f (x) =   0 +   1x +   2x2 ... (2)
f (x) =   0 +   1x +   2x2 +   3x3 +   4x4 ... (3)

hint: avoid high-degree polynomials. get rid of the fourth and
   fth terms in (3).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

62

!"#$%&'()&'!"#$%&'()&'!"#$%&'()&'train, validation and test

train-validation-test framework:

1. training set is a set of examples used for learning a model

(e.g., a classi   cation model).

2. validation set is a set of examples that cannot be used for
learning the model but can help tune model parameters (e.g.,
selecting k in id92). validation helps control over   tting.

3. test set is used to assess the performance of the    nal model

and provide an estimation of the test error.
note: never use the test set in any way to further tune
the parameters or revise the model.

example: split the data randomly into 60% for training, 20 %
for validation and 20% for testing.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

63

train validation test k-fold cross validation

a method for estimating test error using training data.

algorithm:
given a learning algorithm a and a dataset d
step 1: randomly partition d into k equal-size subsets d1, . . . , dk
step 2:
for j = 1 to k

train a on all di, i     1, . . . k and i (cid:54)= j, and get fj.
apply fj to dj and compute edj

step 3: average error over all folds.
(edj)

k(cid:88)

j=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

64

review

review the concepts and terminology:

instance, example, feature, label, supervised learning, unsu-
pervised learning, classi   cation, regression, id91, pre-
diction, training set, validation set, test set, k-fold cross
validation, classi   cation error, id168, over   tting, un-
der   tting, occam   s razor, id173.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

65

