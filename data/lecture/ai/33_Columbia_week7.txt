arti   cial intelligence

machine learning

linear models

ansaf salleb-aouissi

columbia university - fall 2014 w4701 section 2

outline

i - machine learning review

ii - linear models

1. id75

(a) id75: history
(b) id75 with least squares
(c) matrix representation and normal equation method
(d) iterative method: id119
(e) pros and cons of both methods
2. linear classi   cation: id88

(a) de   nition and history
(b) example
(c) algorithm
(d) demo

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

review

review the concepts and terminology:

instance, example, feature, label, supervised learning, unsu-
pervised learning, classi   cation, regression, id91, pre-
diction, training set, validation set, test set, k-fold cross
validation, classi   cation error, id168, over   tting, un-
der   tting, occam   s razor, id173.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

de   nition

   how do we create computer programs that improve with experi-
ence?   

http://videolectures.net/mlas06_mitchell_itm/

tom mitchell

   a computer program is said to learn from experience e with
respect to some class of tasks t and performance measure p , if
its performance at tasks in t , as measured by p , improves with
experience e.    

tom mitchell. machine learning 1997.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

supervised learning

training data:   examples    x with    labels    y.

(x1, y1), . . . , (xn, yn) / xi     rd

    regression: y is a real value, y     r

f : rd        r

f is called a regressor.

    classi   cation: y is discrete. to simplify, y     {   1, +1}

f : rd        {   1, +1}

f is called a binary classi   er.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

id75: history

    a very popular technique.
    rooted in statistics.
    method of least squares used as early as

1795 by gauss.

    re-invented in 1805 by legendre.
    used in astronomy.
    still a very useful tool today.

carl friedrich gauss

credit: wikipedia

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

. . .

example x1    
example xi    
example xn    

. . .

x11 x12 . . . x1d
. . .
. . .
. . .
xi1
. . . xid
. . .
. . .
. . .
xn1 xn2 . . . xnd

. . .
xi2
. . .

. . .

y1     label
yi     label
yn     label

. . .

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

. . .

example x1    
example xi    
example xn    

. . .

x11 x12 . . . x1d
. . .
. . .
. . .
xi1
. . . xid
. . .
. . .
. . .
xn1 xn2 . . . xnd

. . .
xi2
. . .

. . .

y1     label
yi     label
yn     label

. . .

task: learn a regression function:

f : rd        r
f (x) = y

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

. . .

example x1    
example xi    
example xn    

. . .

x11 x12 . . . x1d
. . .
. . .
. . .
xi1
. . . xid
. . .
. . .
. . .
xn1 xn2 . . . xnd

. . .
xi2
. . .

. . .

y1     label
yi     label
yn     label

. . .

task: learn a regression function:

f : rd        r
f (x) = y

id75: a regression model
is said to be linear if
it is represented by a linear function (linear hyperplane in rd+1
dimensional space).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

id75

d = 1, line in r2

d = 2, hyperplane is r3

credit: introduction to statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

10

050100150200250300510152025tvsales!"#"x1x2yid75

id75 model:

d(cid:88)

j=1

f (x) =   0 +

  jxj with   j     r,

j     {1, . . . , d}

      s are called parameters or coe   cients or weights.
learning the linear model        learning the   (cid:48)s

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

11

id75

id75 model:

d(cid:88)

j=1

f (x) =   0 +

  jxj with   j     r,

j     {1, . . . , d}

      s are called parameters or coe   cients or weights.
learning the linear model        learning the   (cid:48)s

estimation with least squares:
use least square loss: (cid:96)oss(yi, f (xi)) = (yi     f (xi))2
we want to minimize the loss over all examples, that is minimize
the risk or cost function r:
1
2n

(yi     f (xi))2

n(cid:88)

r =

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

12

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

13

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

we want to minimize:

r =

1
2n

n(cid:88)

i=1

(yi     f (xi))2

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

14

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

we want to minimize:

r =

r(  ) =

1
2n

1
2n

n(cid:88)
n(cid:88)

i=1

i=1

(yi     f (xi))2

(yi       0       1xi)2

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

15

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

we want to minimize:

r =

1
2n

(yi     f (xi))2

r(  ) =

1
2n

(yi       0       1xi)2

i=1
find   0 and   1 that minimize:

r(  ) =

1
2n

(yi       0       1xi)2

n(cid:88)
n(cid:88)

i=1

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

16

id75

credit: introduction to statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

17

  0  1 2.15  2.2  2.3  2.5  3  3  3  3 567890.030.040.050.06rss  1  0id75

find   0 and   1 so that:

argmin  (

n(cid:88)

i=1

1
2n

(yi       0       1xi)2)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

18

id75

find   0 and   1 so that:

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

   r
     1

= 0

n(cid:88)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

19

id75

find   0 and   1 so that:

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

   r
     1

= 0

n(cid:88)

   r
     0

= 2    1
2n

(yi       0       1xi)       
     0

(yi       0       1xi)

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

20

id75

find   0 and   1 so that:

n(cid:88)

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

   r
     1

= 0

   r
     0

= 2    1
2n

   r
     0

=

n(cid:88)
(yi       0       1xi)       
     0
n(cid:88)

i=1

1
n

i=1

(yi       0       1xi)    (   1) = 0

(yi       0       1xi)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

21

id75

find   0 and   1 so that:

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

n(cid:88)

   r
     0

= 2    1
2n

   r
     0

=

(yi       0       1xi)

   r
     1

= 0

n(cid:88)
(yi       0       1xi)       
     0
n(cid:88)

i=1

1
n

i=1

  0 =

1
n

(yi       0       1xi)    (   1) = 0
n(cid:88)

n(cid:88)

yi       1

1
n

xi

i=1

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

22

id75

   r
     1

= 2    1
2n

(yi       0       1xi)       
     1

(yi       0       1xi)

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

23

id75

   r
     1

= 2    1
2n

   r
     1

=

n(cid:88)
(yi       0       1xi)       
     1
n(cid:88)

i=1

1
n

i=1

(yi       0       1xi)    (   xi) = 0

(yi       0       1xi)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

24

id75

   r
     1

= 2    1
2n

   r
     1

=

(yi       0       1xi)

i=1

n(cid:88)
(yi       0       1xi)       
     1
n(cid:88)
n(cid:88)

xiyi     n(cid:88)

n(cid:88)

2 =

i=1

1
n

xi

i=1

i=1

i=1

  1

  0xi

(yi       0       1xi)    (   xi) = 0

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

25

id75

   r
     1

= 2    1
2n

   r
     1

=

(yi       0       1xi)

(yi       0       1xi)    (   xi) = 0

i=1

1
n

i=1

n(cid:88)
(yi       0       1xi)       
     1
n(cid:88)
n(cid:88)

xiyi     n(cid:88)
(cid:80)n
(cid:80)n
(cid:80)n
(cid:80) xi
(cid:80)n
(cid:80)n
i=1 yixi     1
i     1

i=1 yi
i=1 xi

i=1 x2

n(cid:88)

2 =

i=1

i=1

i=1

xi

n

n

  0xi

i=1 xi

  1

plugging   0 in   1:

  1 =

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

26

id75
d(cid:88)

with more than one feature:

f (x) =   0 +

  jxj

j=1

find the   j that minimize:

n(cid:88)

(yi       0     d(cid:88)

i=1

j=1

r =

1
2n

  jxij))2

let   s write it more elegantly with matrices!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

27

matrix representation
let x be an n    (d + 1) matrix where each row starts with a 1
followed by a feature vector.
let y be the label vector of the training set.
let    be the vector of weights (that we want to estimate!).

                        
                        

x :=

                        

y1...
yi...
yn

y :=

...

...

1 x11          x1j
...
...
1 xi1          xij
...
...
1 xn1          xnj

...

...

         x1d
...
...
         xid
...
...
         xnd

                        
                        

                        

  0...
  j...
  d

   :=

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

28

normal equation
we want to    nd (d + 1)      s that minimize r. we write r:

r(  ) =

r(  ) =

   r
     

1
2n
=    1
n

||(y     x  )||2

1
2n
(y     x  )t (y     x  )

xt (y     x  )

we have that:

   2r
     

=    1
n

xt x

is positive de   nite which ensures that    is a minimum. we solve:

the unique solution is:

xt (y     x  ) = 0
   = (xt x)   1xt y

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

29

id119

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

30

!"#$%&'()'*$%&'('+,-./0$'12,3$'4.5'id119

id119 is an optimization method.

repeat until convergence:

update simultaneously all   j for (j = 0 and j = 1)

  0 :=   0       

  1 :=   1       

   

     0

   

     1

r(  0,   1)

r(  0,   1)

   is a learning rate.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

31

id119

in the linear case:

   r
     1
let   s generalize it!

n(cid:88)

i=1

=

1
n

(yi       0       1xi)    (   xi)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

32

id119

in the linear case:

   r
     1

=

1
n

n(cid:88)

i=1

(yi       0       1xi)    (   xi)

repeat until convergence:

update simultaneously all   j for (j = 0 and j = 1)

  0 :=   0       

  1 :=   1       

1
n

1
n

n(cid:88)
n(cid:88)

i=1

i=1

(  0 +   1xi     yi)(xi0)

(  0 +   1xi     yi)(xi1)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

33

pros and cons

analytical approach: normal equation

+ no need to specify a convergence rate or iterate.
- works only if xtx is invertible
- very slow if d is large o(d3) to compute (xt x)   1
- treats all features equally.

iterative approach: id119

+ e   ective and e   cient even in high dimensions.

- iterative (sometimes need many iterations to converge).
- needs to choose the rate   .
- treats all features equally.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

34

practical considerations

1. scaling: bring your features to a similar scale.

xi       i

xi :=

max(xi)     min(xi)

xi :=

xi       i
stdev(xi)

2. learning rate: don   t use a rate that is too small or too large.
3. r should decrease after each iteration.
4. declare convergence if it start decreasing by less  
5. if xt x is not invertible?

(a) too many features as compared to the number of examples

(e.g., 50 examples and 500 features)

(b) features linearly dependent: e.g., weight in pounds and in

kilo.

6. how can we extend to polynomial regression?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

35

classi   cation

now back to classi   cation...

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

36

classi   cation

training data:

given:
discrete (categorical/qualitative), yi     y.
example y = {   1, +1}, y = {0, 1}.

(x1, y1), . . . , (xn, yn)/xi     rd and yi

is

task: learn a classi   cation function:

f : rd        y

linear classi   cation: a classi   cation model is said to be linear
if it is represented by a linear function f (linear hyperplane in rd+1
dimensional space).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

37

classi   cation: examples

1. email spam/ham     which email is junk?
2. tumor benign/malignant     which patient has cancer?
3. credit default/not default     which customers will default on

their credit card debt?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

38

balanceincomedefault300$20,000.00no2000$60,000.00no5000$45,000.00yes.........classi   cation: example

credit: introduction to statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

39

050010001500200025000200004000060000balanceincomeid88

    belongs to neural networks class of algorithms (algorithms

that try to mimic how the brain functions).

    the    rst algorithm used was the id88 (resenblatt

1959).

    worked extremely well to recognize:

1. handwritten characters (lecun et a. 1989),
2. spoken words (lang et al. 1990),
3. faces (cottrel 1990)

    popular in the 90   s but then lost some of its popularity.
    now nn back with deep learning!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

40

perfectly separable data

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

41

!"#$%&"'('!"#$%&"')'id88

    linear classi   cation method.
    simplest classi   cation method.
    simplest neural network.
    for perfectly separated data.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

42

!"#$%&"'('!"#$%&"')'id88

    works perfectly if data is linearly separable (theorem). if not,

it will not converge.

    idea: start with a random hyperplane and adjust it using your

training data.

    iterative method.
    toy example on the board.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

43

id88

id88 algorithm

input: a set of examples, (x1, y1),          , (xn, yn)
output: a id88 de   ned by (w0, w1,          , wd)

begin

2. initialize the weights wj to 0    j     {0,          , d}
3. repeat until convergence

4. for each example xi    i     {1,          , n}
5.
6.

if yif (xi)     0

#an error?

update all wj with wj := wj + yixi #adjust the weights

end

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

44

example

run the algorithm on:

f2
2
1

f1
y
1
1
2
1
   1    1    1
   1 1    1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

45

id88

some observations:
    the weights w1, . . . , wd determine the slope of the decision

boundary.

    w0 determines the o   set of the decision boundary (sometimes

noted b).

    line 6 corresponds to:

mistake on positive: add x to weight vector.
mistake on negative: substract x from weight vector.
some other variants of the algorithm add or subtract 1.

    convergence happen when the weights do not change anymore

(di   erence between the last two weight vectors is 0).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

46

id88 demo

http://www.eee.metu.edu.tr/~alatan/courses/demo/appletid88.html

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

47

choice of the hyperplane

lots of possible solutions!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

48

+ + + + + - + + - - - - - - + id88

    the wi determine the contribution of xi to the label.
       w0 is a quantity that (cid:80)n

i=1 wix1 needs to exceed for the per-

ceptron to output 1.

    can be used to represent many boolean functions: and, or,

nand, nor, not but not all of them (xor and xnor).

    neural networks use the ability of the id88s to represent
elementary functions and combine them in a network of layers
of elementary questions.

    but...wait: a cascade of linear functions is still linear!
    and we want networks that represent highly non-linear func-

tions!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

49

id88 with sigmoid

credit: adapted from machine learning. tom mitchell.

given n examples and d features.
for an example xi (the ith line in the matrix of examples)

1

   (cid:80)d

f (xi) =

1 + e

j=0 wjxij

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

50

choice of the hyperplane

lots of possible solutions!

digression: idea of id166 is to    nd the optimal solution.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

51

+ + + + + - + + - - - - - - + credit
    the elements of statistical learning. data mining, id136,
and prediction. 10th edition 2009. t. hastie, r. tibshirani,
j. friedman.

    machine learning 1997. tom mitchell.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

52

