arti   cial intelligence

machine learning

support vector machines

ansaf salleb-aouissi

columbia university - fall 2014 w4701 section 2

outline

1. history of support vector machines (id166s)

2. basic idea

3. choice of the hyperplane: linearly separable case

4. choice of the hyperplane: non-linearly separable case

5. id166 primal form

6. lagrange duality

7. id166 dual form

8. id166 with a soft-margin

9. a hint of kernels (more in the next lecture)

10. id166s in practice

11. demo

12. non-linearity: example

13. from linear models to non-linear models

14. kernels

15. examples of kernels

16. validity of kernels

17. composition of kernels

18. conclusion

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

history of id166s
    id166s: state-of-the-art classi   cation method.
    boser, guyon and vapnik 1992.
    powerful and widely used in both academia and industry:

1. handles high-dimensional data

2. handles non-linear problems

3. allows overlap in the classes

    a kernel method that depend only on the data through inner

products.

    come with theoretical guaranteed about their performance.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

basic idea
    find the optimal hyperplane for linearly separable examples.
    for non linearly separable data, transform the original data

using a id81.

    to allow for some overlap in the classes, use slack variables.
    the support vectors are the examples that are the closest to

the decision surface.

    support vectors are the most di   cult to classify.
    output a discrete answer     y = {   1, +1}.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

choice of the hyperplane

given: training data: (x1, y1), . . . , (xn, yn)/xi     rd and yi is dis-
crete yi     y = {   1, +1}.

task: learn a classi   cation function:
f : rd        y

f (x) = sign(

wixi)

d(cid:88)

i=0

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

choice of the hyperplane

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

!"!"!"!"!"!"!"!"!"!"!"!"!"!"!"choice of the hyperplane

lots of possible solutions!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

+ + + + + - + + - - - - - - + choice of the hyperplane

lots of possible solutions!

idea of id166:    nd the    best    margin   .

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

+ + + + + - + + - - - - - - + maximum margin: intuition

why is a fat margin the best?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

maximum margin: intuition

why is a fat margin the best?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

choice of the hyperplane
given: training data: (x1, y1), . . . , (xn, yn)/xi     rd and yi is dis-
crete yi     y = {   1, +1}.
task: learn a classi   cation function: f : rd        y

f (x) = sign(

wixi)

f (x) = sign(

wixi + b)

d(cid:88)
d(cid:88)

i=0

i=1

f (x) = sign(w.x + b)

(with    .    is the dot product)

note: b corresponds to the intercept   0 in the methods we have seen before.
we use w and b as these are the most commonly used for id166s. it will help in
case you read id166s literature.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

10

choice of the hyperplane

the separable case:

    the hyperplane satis   es:

w.x + b = 0.

    w is the normal to the hyperplane.

||w|| is its norm.

    |b|/||w||

is the perpendicular dis-
tance from the hyperplane to the
origin.

    d+ is the shortest distance from the
hyperplane to the closest positive
example. d    is the shortest from
the hyperplane to the closest neg-
ative example.

    margin: d+ + d   

the id166 algorithm looks for separating hyperplane with the largest margin.

the examples on h1 and h2 are called the support vectors (svs) and
have w.x + b = 0 .

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

11

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| choice of the hyperplane

the separable case:

    h1 and h2 are parallel.
    h1: w.xi + b = +1 with normal
w and perpendicular distance from
the origin |1     b|/||w||.

    h2: w.xi + b =    1 with normal
w and perpendicular distance from
the origin |     1     b|/||w||.

    d+ = d    = 1/||w||
    w.xi + b     +1 if yi = +1
w.xi + b        1 if yi =    1
    these can be combined:

yi(w.xi + b)     1    i = 1, . . . , n

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

12

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| choice of the hyperplane
why is d+ = d    = 1||w||?

the distance from a point (x0, y0) to a line with equation ax + by + c = 0 is:

(cid:112)

|ax0 + by0 + c|

a2 + b2

distance(ax + by + c = 0, (x0, y0)) =

(cid:113)

if we reason in 2d without loss of gener-
ality, we have vector w = (w1, w2). we
have a = w1, b = w2, c = b.

w2

1 + w2

2 = ||w||

that is norm of vector w

distance d+ from any positive point x+
in h1 to the hyperplane h.
|w.x+ + b|

d+ =

||w||

x+ being on h1 veri   es the equation
w.x+ + b = 1.
hence: d+ = 1||w||

one could do a similar calculation for a point x    on h2 to get d   .

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

13

id166s primal form

the maximum margin classi   er is the function that maximizes the
geometric margin 1/||w||, equivalent to minimizing ||w||2.

solve the constrained optimization problem:

argmin

w,b

||w||2

1
2

subject to:

yi(w.xi + b)     1    i = 1, . . . , n

inequality constraint

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

14

choice of the hyperplane

the non separable case:
we allow errors but not too much!

argmin

w,b

||w||2 + c

1
2

(cid:88)

i

  i

subject to:

yi(w.xi + b)     1       i       i     0    i = 1, . . . , n
a large c corresponds to assigning a higher penalty to errors.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

15

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| + -!i/||w|| lagrange duality

solving constrained optimization problems:

                     

argmin

w

f (w)

s.t.

hi(w) = 0    i = 1, . . . , n

can be solved with lagrange multipliers.

lagrangian:

l(w,   ) = f (w) +

n(cid:88)

i=1

  ihi(w)

the   s are called lagrange multipliers.
   l
     i

   l
   wi

= 0

= 0

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

16

lagrange duality

argmin

w

f (w)

                                 

s.t.
s.t.

gi(w)     0    i = 1, . . . , k
hi(w) = 0    i = 1, . . . , l
l(cid:88)

k(cid:88)

  igi(w) +

i=1

i=1

generalized lagrangian:

l(w,   ,   ) = f (w) +

  ihi(w)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

17

lagrange duality

argmin

w

f (w)

                                 

s.t.
s.t.

generalized lagrangian:

gi(w)     0    i = 1, . . . , k
hi(w) = 0    i = 1, . . . , l
l(cid:88)
i=1
solution w    in the primal,       and       in the dual.
for a solution to exist (and hence the primal and id78 are
equivalent), the karush-kuhn-tucker kkt conditions must be
ful   lled.

l(w,   ,   ) = f (w) +

  igi(w) +

  ihi(w)

k(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

18

lagrange duality

karush-kuhn-tucker kkt conditions.

1.

   
   wi

l(w   ,      ,      ) = 0,    i = 1, . . . , n
l(w   ,      ,      ) = 0,    i = 1, . . . , l
i gi(w   ) = 0, ,    i = 1, . . . , k

   
2.
     i
3.      
4. gi(w   )     0,    i = 1, . . . , k
5.           0,    i = 1, . . . , k

for more details: t. rockarfeller (1970) convex analysis, princeton university press.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

19

id166s dual form

l(w, b,   ) =

||w||2     n(cid:88)

i=1

1
2

  i[yi(w.xi + b)     1]

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

20

id166s dual form

||w||2     n(cid:88)

1
2

i=1

l(w, b,   ) =

= w     n(cid:88)

i=1

   l
   w

  i[yi(w.xi + b)     1]

n(cid:88)

i=1

  iyixi

  iyixi = 0     w =

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

21

id166s dual form

||w||2     n(cid:88)

1
2

i=1

l(w, b,   ) =

   l
   w

= w     n(cid:88)
=     n(cid:88)

i=1

   l
   b

i=1

  i[yi(w.xi + b)     1]

n(cid:88)

i=1

  iyixi

  iyi = 0

  iyixi = 0     w =

  iyi = 0     n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

22

id166s dual form

||w||2     n(cid:88)

1
2

i=1

l(w, b,   ) =

   l
   w

= w     n(cid:88)
=     n(cid:88)

i=1

   l
   b

i=1

  i[yi(w.xi + b)     1]

n(cid:88)

i=1

  iyixi

  iyi = 0

  iyixi = 0     w =

  iyi = 0     n(cid:88)

i=1

l(w, b,   ) =

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

  i  jyiyjxixj

by plugging in these 2 quantities back into l:

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

23

id166s dual form
n(cid:88)

n(cid:88)

n(cid:88)

argmax

  

n  i     1
2

i=1

j=1

  i  jyiyjxixj

i=1

s.t.

  i     0,    i = 1,          , n
n(cid:88)

  iyi = 0

i=1

solve the dual problem to    nd the      s!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

24

id166s dual form

few observations:

    total dependence on the dot product.
    the dual form depends only on the inputs.
    once we    nd the      s, we can    nd the optimal w   s.

n(cid:88)

i=1

w    =

     
i yixi

    we can    nd the optimal b, that is b    but reconsidering the

primal form.
teaser: calculate b    using w   .

    except for support vectors, all      s will be 0 (from kkt).
    how can we make a prediction given an example u (unknown)?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

25

id166s dual form

few observations:

    total dependence on the dot product.
    the dual form depends only on the inputs.
    once we    nd the      s, we can    nd the optimal w   s.

n(cid:88)

i=1

w    =

     
i yixi

    we can    nd the optimal b, that is b    but reconsidering the

primal form.
teaser: calculate b    using w   .

    except for support vectors, all      s will be 0 (from kkt).
    how can we make a prediction given an example u (unknown)?

n(cid:88)

i=1

     
i yixiu + b   

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

26

soft margin

the non separable case: we allow errors but not too much!

argmin

w,b

||w||2 + c

1
2

n(cid:88)

i=1

  i

subject to:

yi(w.xi + b)     1       i

  i     0    i = 1, . . . , n

a large c corresponds to assigning a higher penalty to errors.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

27

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| + -!i/||w|| soft margin: dual form

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

  i  jyiyjxixj

0       i     c,    i = 1,          , n

argmax

  

s.t.

n(cid:88)

i=1

  iyi = 0

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

28

id166 in practice

    id172 is important.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

29

id166 in practice

    id172 is important.
    do model selection by searching the parameters.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

30

id166 in practice

    id172 is important.
    do model selection by searching the parameters.
    rbf (gaussian kernel) is an e   ective and mostly used kernel.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

31

id166 in practice

    id172 is important.
    do model selection by searching the parameters.
    rbf (gaussian kernel) is an e   ective and mostly used kernel.
    id166 with unbalanced datasets:

||w||2 + c    (cid:88)
1
  i + c+
2
yk[w(cid:62)xk + b]     1       k,    k,

yi=   1

(cid:88)

  j

yj=+1

argmin

w,  

s.t.

c+ n+ = c    n   

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

32

id166 in practice

    free implementations: libid166, id166light.
    http://www.kernel-machines.org/
    demo (at the end of the lecture):

http://las.ethz.ch/courses/ml-f13/applets/

jsupportvectorapplet.html
    more on kernels in a bit...

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

33

maximum margin

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

34

id166s primal and dual forms

separable case:

argmin

w,b

||w||2

1
2

subject to: yi(w.xi + b)     1    i = 1, . . . , n

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

argmax

  

  i  jyiyjxixj

s.t.

  i     0,    i = 1,          , n
n(cid:88)

  iyi = 0

i=1

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

35

id166s dual form
    solve the dual problem to    nd the      s!
    calculate w   s as follows:

w    =

     
i yixi

n(cid:88)

i=1

n(cid:88)

i=1

    how can we make a prediction given an example u (unknown)?

f (x) = sign(

     
i yixiu + b   )

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

36

soft margin

the non separable case:

argmin

w,b

||w||2 + c

1
2

n(cid:88)

i=1

  i

subject to: yi(w.xi + b)     1      i
n(cid:88)

n(cid:88)

n(cid:88)

argmax

  

  i     1
2

i=1

i=1

j=1

  i     0    i = 1, . . . , n

  i  jyiyjxixj

s.t.

0       i     c,    i = 1,          , n

n(cid:88)

i=1

  iyi = 0

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

37

non-linear problems

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

38

!"!"!"!"!"#"#"#"#"#"#"!"#!$#"#$#non-linear problems

  (x) = (x2

1, x2)

f (x) = w.  (x) + b

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

39

!"!"!"!"!"#"#"#"#"#"#"!"#!$#"#$#!"!"!"!"!"#"#"#"#"#"!"#$!#$#""$#$"%$&$'$non-linear problems

  (x) = (x2
1,

   

2x1x2, x2
2)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

40

beyond the input space

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

41

!"#$%&'#()*&+*(%$,*&'#()*&!"#$%&'()"*+"&%,*'-,'.(*'/*%.0+*'1"%2*'-plug in    into the dual?

argmin

w,b

||w||2 + c

1
2

n(cid:88)

i=1

  i

subject to:

yi(w.xi + b)     1       i

  i     0    i = 1, . . . , n

argmax

  

s.t.

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

  i  jyiyjxixj

0       i     c,    i = 1,          , n

n(cid:88)

  iyi = 0

i=1

w    =

n(cid:88)

i=1

     
i yixi

replace all xi by   (xi)!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

42

problems with plugging   

exponential number of features in the feature space!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

43

problems with plugging   

exponential number of features in the feature space!

do we have to create and represent all these features?

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

44

problems with plugging   

exponential number of features in the feature space!

do we have to create and represent all these features?

no need to do it explicitly. instead, use kernels!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

45

problems with plugging   

exponential number of features in the feature space!

do we have to create and represent all these features?

no need to do it explicitly. instead, use kernels!

k(x, x(cid:48)) =   (x).  (x(cid:48))

we can do so because the dual form relies on inner products!

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

46

example

  (x) = (1,

   

2x1,

   

2x2, x2

1, x2
2,

   

2x1x2)

k(x, x(cid:48)) =   (x).  (x(cid:48))
k(x, x(cid:48)) = [x.x(cid:48) + 1]2
given two points xt = (x1, x2) and x(cid:48)t = (x(cid:48)
k(x, x(cid:48)) = [x.x(cid:48) + 1]2
k(x, x(cid:48)) = (x1x(cid:48)
1x2x(cid:48)
k(x, x(cid:48)) = x2
1x(cid:48)
which is the inner product   (x).  (x(cid:48)).

2 + 1)2
2 + 2x1x(cid:48)
2

1 + x2x(cid:48)
2x(cid:48)
2 + x2

2 + 2x1x(cid:48)

1

1, x(cid:48)
2)

1 + 2x2x(cid:48)

2

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

47

dual with kernel
n(cid:88)

n(cid:88)

n(cid:88)

argmax

  

  i     1
2

i=1

j=1

  i  jyiyjk(xi, xj)

i=1

s.t.

0       i     c,    i = 1,          , n

  iyi = 0

n(cid:88)
n(cid:88)

i=1

i=1

f (x) = sign(

     
i yik(xi, u) + b   )

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

48

examples of kernels

    linear: k(x, x(cid:48)) = x.x(cid:48)
    polynomial: k(x, x(cid:48)) = [x.x(cid:48) + 1]d
    radial basis function (rbf): exp(     [x     x(cid:48)]2)

kernels can compute inner products e   ciently.

note: in the polynomial kernel, d refers to the degree of
the polynomial, not the number of features.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

49

example of polynomial kernel

k(x, x(cid:48)) = [x.x(cid:48) + 1]2

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

50

example of rbf kernel

exp(     [x     x(cid:48)]2)

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

51

demo

http://las.ethz.ch/courses/ml-f13/applets/jsupportvectorapplet.

html

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

52

validity of kernels

a kernel k(x, x(cid:48)) is a valid kernel i    for all example x1, x2, . . . , xn,
it produces a gram matrix:

1. symmetric: .

2. positive semi-de   nite:

gij = k(xi, xj)

g = gt

  t g       0

     

these are mercer conditions.
form.

it ensures convexity of the dual

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

53

composition of kernels

given two valid kernels k1 and k2,    > 0, 0            1, f a real-
valued function, a mapping   , k a positive semi-de   nite matrix,
then the following functions are valid kernels:
1. k(x, z) =   k1(x, z) + (1       )k2(x, z)
2. k(x, z) =   k1(x, z)

3. k(x, z) = k1(x, z)k2(x, z)

4. k(x, z) = f (x)f (z)

5. k(x, z) = k3(  (x),   (z))

6. k(x, z) = xt kz

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

54

conclusion

id166 with kernels:

    independent of the dimensionality of feature space.
    has one global optima.
    can represent any boolean function and reasonably any arbi-

trary smooth decision boundary.

    need to choose a kernel type and its parameters.
    setting the hyper-parameter is crucial but non-trivial.
    in practice, they are usually set using cross validation.
    rbf kernel is a reasonable    rst choice.
    there are very speci   c kernels depending on the applications

(e.g., tree kernels, graph kernels, etc.).

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

55

credit
    a user   s guide to support vector machines. benhur and we-

ston 2010.

    a tutorial on support vector machines for pattern recogni-
tion. burges, christopher data mining and knowledge dis-
covery 2, no. 2 (june 1998): 121-167.

    statistical learning theory. vapnik, 1998.
    check out also    the practical guide to support vector clas-

si   cation    hsu and al. 2010, available online.

copyright c(cid:13)ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

56

