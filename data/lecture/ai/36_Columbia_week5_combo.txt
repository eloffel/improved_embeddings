arti   cial intelligence

a* optimality
local search

ansaf salleb-aouissi

university - fall 2014 w4701 section 2

note

please note that the numbering in the following slides is o   

as the slides are coming from di   erent resources!

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

id67

    minimize the total estimated solution cost
    combines:

    g(n): cost to reach node n
    h(n): cost to get from n to the goal
    f(n) = g(n) + h(n)

f(n) is the estimated cost of the cheapest

solution through n

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

admissible heuristics

    an admissible heuristic never overestimates the cost to reach

the goal, that is it is optimistic

    a heuristic h is admissible if

where h    is true cost to reach the goal from n.

    node n, h(n)     h   (n)

    hsld (used as a heuristic in the romanian example) is admis-
sible because it is by de   nition the shortest distance between
two points.

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

a* optimality
theorem: if h(n) is admissible, a* using tree search is optimal.
proof:
    suppose g is the optimal goal.
    suppose g2 is some suboptimal goal has been generated.
    suppose n is an unexpanded node in the fringe such that n is
    f(g2) = g(g2) since h(g2) = 0

on the shortest path to g.

f(g2) > g(g) since g2 is suboptimal
f(g) = g(g) since h(g) = 0
then f(g2) > f(g) . . . (1)

    h(n)     h     (n) since h is admissible

g(n) + h(n)     g(n) + h     (n) = g(g) = f(g)
then, f(n)     f(g) . . . (2)
from (1) and (2) f(g2) > f(n) so a* will never select g2 for
expansion.

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

id67 criteria

    complete: yes :)
    time: exponential :(
    space: keeps every node in memory, the biggest problem) :(
    optimal: yes! :)

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

local search

the following are from russell slide repository, available on the
aima book resources.

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

iterative improvement algorithms

in many optimization problems, path is irrelevant;
the goal state itself is the solution

then state space = set of    complete    con   gurations;

   nd optimal con   guration, e.g., tsp
or,    nd con   guration satisfying constraints, e.g., timetable

in such cases, can use iterative improvement algorithms;
keep a single    current    state, try to improve it

constant space, suitable for online as well as o   ine search

chapter 4, sections 3   4

3

hill-climbing (or gradient ascent/descent)

   like climbing everest in thick fog with amnesia   

function hill-climbing( problem) returns a state that is a local maximum

inputs: problem, a problem
local variables: current, a node
neighbor, a node

current    make-node(initial-state[problem])
loop do
neighbor    a highest-valued successor of current
if value[neighbor]     value[current] then return state[current]
current    neighbor

end

chapter 4, sections 3   4

6

hill-climbing contd.

useful to consider state space landscape

objective function

global maximum

shoulder

local maximum

"flat" local maximum

current
state

state space

random-restart hill climbing overcomes local maxima   trivially complete

random sideways moves

escape from shoulders

loop on    at maxima

chapter 4, sections 3   4

7

simulated annealing

idea: escape local maxima by allowing some    bad    moves
but gradually decrease their size and frequency

function simulated-annealing( problem, schedule) returns a solution state

inputs: problem, a problem

schedule, a mapping from time to    temperature   

local variables: current, a node

next, a node
t, a    temperature    controlling prob. of downward steps

current    make-node(initial-state[problem])
for t    1 to     do
t    schedule[t]
if t = 0 then return current
next    a randomly selected successor of current
   e    value[next]     value[current]
if    e > 0 then current    next
else current    next only with id203 e    e/t

chapter 4, sections 3   4

8

properties of simulated annealing

at    xed    temperature    t , state occupation id203 reaches
boltzman distribution

p(x) =   e

e(x)
kt

t decreased slowly enough =    always reach best state x   
because e

e(x)
kt = e

e(x   )
kt /e

e(x   )   e(x)

kt " 1 for small t

is this necessarily an interesting guarantee??

devised by metropolis et al., 1953, for physical process modelling

widely used in vlsi layout, airline scheduling, etc.

chapter 4, sections 3   4

9

id107

= stochastic local id125 + generate successors from pairs of states

24748552

24

31%

32752411

32748552

32748152

32752411

23

29%

24748552

24752411

24752411

24415124

20

26%

32752411

32752124

32252124

32543213

11

14%

24415124

24415411

24415417

fitness

selection

pairs

cross!over

mutation

chapter 4, sections 3   4

11

id107 contd.

gas require states encoded as strings (gps use programs)

crossover helps i    substrings are meaningful components

+

=

gas != evolution: e.g., real genes encode replication machinery!

chapter 4, sections 3   4

12

search: case study

search for association rules.

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

applications

    market basket analysis: cross-selling (ex. amazon), product

placement, a   nity promotion, customer behavior analysis

    collaborative    ltering
    symptoms-diseases associations

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

a two-step process

given a transaction dataset d
1. mining frequent patterns in d

2. generation of strong association rules

example:

{bread, butter} is a frequent pattern (itemset)

bread     butter is a strong rule

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

de   nitions

    item: an object belonging to i = {x1, x2, ..., xm}.
    itemset: any subset of i.
    k-itemset: an itemset of cardinality k.
    we de   ne a total order (i, <) on the items.
    p(i) is a lattice with     =     and # = i.
    transaction: itemset identi   ed by a unique identi   er tid.
    t : the set of all transactions ids. tidset: a subset of t .
    transaction dataset: d = {(tid, xtid) /tid     t , xtid     i}

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

example

item name

a
b
c
d

cafe
milk
butter
bread

d
a b
a c
c d
b c d
a b c d

tid transaction
1
2
3
4
5

i = {a, b, c, d}
t = {1, 2, 3, 4, 5}
d = {(1, ab), (2, ac), (3, cd), (4, bcd), (5, abcd)}

e.g., {b, c} is a 2-itemset, for writing simpli   cation we will give up
the braces and write bc. {3, 4, 5} is a tidset similarly let   s abandon
the braces here too and write 345.

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

example

lattice of itemsets of size . . .

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

example

lattice of itemsets of size 2|i| = 16.

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

example

minsupp=40%

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

bfs and dfs

a : 1

a : 2

b : 1

chapitre 2. extraction des motifs fr  equents et r`egles d   association

d : 1

c : 4

null
4
3
index
   lms
drame
horreur
policier
com  edies
science    ction
et fantastiques
      
musicales
dramatiques

   

a

b

c

d

ab

ac

ad

bc

bd

cd

abc

abd

acd

bcd

abcd

breadth first search

depth first search

fig. 2.4     ordre de rymon

propri  et  e 2.1 (propri  et  e d   antimonotonicit  e). tout sous-ensemble d   un itemset fr  equent
est un itemset fr  equent.

preuve

soit x un itemset fr  equent. supposons qu   il existe un itemset peu fr  equent x !     x.
x!     x        x!!     x tel que x = x!     x!!
supp(x) = |t(x)|
<    (car x! n   est pas
|d|
|d|
fr  equent), ce qui aboutit `a une contradiction, car x est fr  equent par hypoth`ese.

= |t(x!)   t(x!!)|

< |t(x!)|
|d|

= |t(x!   x!!)|

|d|

intuitivement, cela veut dire que si un itemset a un support inf  erieur au support
minimum aucun de ses sur ensembles n   aura un support sup  erieur ou   egal au support
minimum.

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

la d  ecouverte de tous les itemsets fr  equents s   e   ectue par un parcours du treillis des
itemsets. plusieurs sortes de parcours sont possibles, dont les plus communs sont le par-
cours en largeur d   abord3 et le parcours en profondeur d   abord4.

16

dans le parcours en largeur, le treillis des itemsets est parcouru niveau par niveau,

example

minsupp=2/5 (40%)

tid
1
2
3
4
5

d

transaction

a b
a c
c d
b c d
a b c d

itemset support

itemset support

f1

itemset

c1
a
b
c
d

itemset

c2
ab
ac
ad
bc
bd
cd

itemset

c3
abc
bcd

scan            of d

a
b
c
d

scan            of d

scan            of d

ab
ac
ad
bc
bd
cd

abc
bcd

c1

c2

3/5
3/5
4/5
3/5

2/5
2/5
1/5
2/5
2/5
3/5

1/5
2/5

   

   

   

a
b
c
d

ab
ac
bc
bd
cd

3/5
3/5
4/5
3/5

2/5
2/5
2/5
2/5
3/5

itemset support

c3

f3

itemset support

bcd

2/5

itemset support

itemset support

f2

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

example

minconf=60%

ab

ac

con   dence

itemset rule# rule
2/3 = 66.66%
a     b
2/3 = 66.66%
b     a
2/3 = 66.66%
a     c
2/4 = 50.00%
c     a
2/3 = 66.66%
b     c
2/4 = 50.00%
c     b
2/3 = 66.66%
b     d
2/3 = 66.66%
d     b
3/4 = 75.00%
c     d
d     c 3/3 = 100.00%

1
2
3
4
5
6
7
8
9
10

cd

bd

bc

strong?

yes
yes
yes
no
yes
no
yes
yes
yes
yes

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

example

minconf=60%

itemset rule# rule
2/3 = 66.66%
cd     b
bd     c 2/2 = 100.00%
bc     d 2/2 = 100.00%

con   dence

11
12
13

bcd

itemset rule# rule

bcd

14
15
16

con   dence
d     bc 2/3 = 66.66%
c     bd 2/4 = 50.00%
b     cd 2/3 = 66.66%

strong?

yes
yes
yes

strong?

yes
no
yes

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

fp algorithms

according to the strategy to traverse the search space:

    breadth first search (ex: apriori, aprioritid, partition, dic)

    depth first search (ex: eclat, clique, depth project)

    hybrid (ex: apriorihybrid, hybrid, viper, kdci)

    pattern growth, i.e. no candidate generation (ex: fpgrowth,

hmine, co   )

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

quantitative ar

question: mining quantitative ar is not a simple extension of
mining categorical ar. why?
    in   nite search space:

in boolean ar, the ariori property
allows to prune the search space e   ciently, but we do explore
the whole space of hypothesis (lattice of itemsets), which is
impossible for quantitative ar.

    the support-con   dence tradeo   : choosing intervals is

quite sensitive to support and con   dence.

- intervals too small, not enough support;
- intervals too large, not enough con   dence.

    what is the di   erence between supervised and unsupervised

discretization?

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

approaches to mine qars

    discretization-based approaches

    distribution-based approaches

    optimization-based approaches

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

approaches to mine qars

optimization-based approaches
    ruckert et al. 2004 use half-spaces to mine such rules like:

x1 > 20     0.5x3 + 2.3x6     100

cannot handle categorical attributes.

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

approaches to mine qars

function fitness(a     b)
tempfitness = gain(a     b)
if tempfitness     0 then

optimization-based approaches
    ruckert et al. 2004 use half-spaces to mine such rules like:

foreach interval i in a     b do
//favor small intervals
tempfitness * = (1-prop(i))2
if support(a     b) < m insupp then

//penalize low support rules
tempfitness - = nbtuples

a1, . . . , an, i.e., the set of instantiations v1, . . . , vn such that
a1 = v1     . . . an = vn is frequent and generate a rule tem-
plate for each such instantiation. this leads to as many rules
as the number of instantiations. notice that this step is similar
to apriori [agrawal et al., 1993].
    individual representation an individual is a set of items
of the form attributei     [li, ui], where attributei is the ith
numeric attribute in the rule template from the left to the right.
    initial population the initial population of individuals is
in the    rst individual, the intervals
generated as follows:
[li, ui] represent the whole domain of the i th numeric at-
tribute, and the following individuals encode intervals with
decreasing amplitudes until they reach a minimum support in
the dataset. once the amplitudes are    xed for an individual,
the bounds li and ui are chosen at random. this ensures to
start with enough diversity in the initial population that thus
model general and speci   c rules.

x1 > 20     0.5x3 + 2.3x6     100

gain(a     b) = supp(ab)     m inconf     supp(a)

    salleb et al. 2007: quantminer optimize the gain of rules

templates using a genetic algorithm.

cannot handle categorical attributes.

return tempfitness

domain of attributei

lili

uiui

l!i

u!i

crossover

lili

uiui

l!i

lili

u!i

u!i

l!i

ui

lili

uiui

mutation

lili

u!!i
l!!i

uiui

figure 1: crossover and mutation operators used in quant-
miner

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

    genetic operators mutation and crossover (figure 1) are
both used in order to transform a generation of individuals
into another one, improving its quality. the crossover oper-

algorithm 2: quantminer

input: a dataset composed of nbtuples, popsize,

gennb, cr, mr, minsupp, minconf

output: quantitative association rules r
select a set of attributes
let rt a set of rule templates de   ned on these attributes
compute the set of frequent itemsets on categorical
attributes in rt
r =    
foreach r     rt do

generate a random population pop of popsize
instantiated rules following the template r
i=1
while i     gennb do

form the next generation of population by
mutation and crossover w.r.t. mr and cr.
keep popsize rules in pop with the best fitness
values
i++

r = r     argmaxr   p op f itness(r)

22

return r

approaches to mine qars

optimization-based approaches: quantminer cont   d.
example uci iris dataset:

species=
value

   ! pw     [l1, u1] sw     [l2, u2]

pl     [l3, u3] sl     [l4, u4] " supp%

conf%

species=

species=

setosa    ! pw     [1, 6] sw     [31, 39]
versicolor    ! pw     [10, 15] sw     [22, 30]
virginica    ! pw     [18, 25] sw     [27, 33]

pl     [10, 19] sl     [46, 54] " 23%
pl     [35, 47] sl     [55, 66] " 21%
pl     [48, 60] sl     [58, 72] " 20%

species=

70%

64%

60%

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

quantminer

http://quantminer.github.io/quantminer/

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

quantminer

uci iris dataset 

copyright c!ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

questions?

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

credit

    aima book chapters 4.
    russell   s slides.

copyright c!ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

