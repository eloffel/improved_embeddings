arti   cial intelligence

machine learning

unsupervised learning

ansaf salleb-aouissi

columbia university - fall 2014 w4701 section 2

outline

1. unsupervised learning

2. id116

3. em

4. id84

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

unsupervised vs. supervised

supervised learning:
    labeled data
    two main tasks:
1. classi   cation
2. regression

    most explored machine learning paradigm (tons of papers).
    question: enumerate few methods seen in class?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

unsupervised vs. supervised

supervised learning:
    labeled data
    two main tasks:
1. classi   cation
2. regression

    most explored machine learning paradigm (tons of papers).
    question: enumerate few methods seen in class?
unsupervised learning:
    unlabeled data
    two main tasks:

1. data segmentation or id91
2. id84

    much less explored machine learning paradigm although very

important. why?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

unsupervised learning

data segmentation or id91
input:    examples    with unobserved labels.
x1, . . . , xn, xi 2 x     rn

output: f : x  ! {c1, . . . ck} (set of clusters).

!$#

!"#

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

unsupervised learning

data segmentation or id91
input:    examples    with unobserved labels.
x1, . . . , xn, xi 2 x     rn

output: f : x  ! {c1, . . . ck} (set of clusters).

!$#

!"#

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

id91 examples

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

id91 examples

    id91 of the us population in the us.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

id91 examples

    id91 of the us population in the us.
    audio signal separation. example?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

id91 examples

    id91 of the us population in the us.
    audio signal separation. example?
    image segmentation. example?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

id91 methods

    id116: simplest approach!
    em: probabilistic approach (generalization of id116)
    spectral id91

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

10

id116: example

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

11

id116: example

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

12

id116: example

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

13

id91: id116

    goal: assign each example (x1, . . . , xn) to one of the k clusters

{c1, . . .ck}.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

14

id91: id116

    goal: assign each example (x1, . . . , xn) to one of the k clusters

{c1, . . .ck}.

      j is the mean of all examples in the jth cluster.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

15

id91: id116

    goal: assign each example (x1, . . . , xn) to one of the k clusters

{c1, . . .ck}.

      j is the mean of all examples in the jth cluster.
    minimize:

j =

kxj=1 xxi2cj

||xi     j||2

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

16

id91: id116

algorithm id116:

initialize randomly   1,         k.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

17

id91: id116

algorithm id116:

initialize randomly   1,         k.
repeat

assign each point xi to the cluster with the closest   j.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

18

id91: id116

algorithm id116:

initialize randomly   1,         k.
repeat

assign each point xi to the cluster with the closest   j.
calculate the new mean for each cluster as follows:

  j =

1

|cj| xxi2cj

xi

until convergence   .

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

19

id91: id116

algorithm id116:

initialize randomly   1,         k.
repeat

assign each point xi to the cluster with the closest   j.
calculate the new mean for each cluster as follows:

  j =

1

|cj| xxi2cj

xi

until convergence   .

   convergence:
number of iterations reached.

means no change in the clusters or maximum

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

20

id116: applet

http://home.deib.polimi.it/matteucc/id91/tutorial_html/appletkm.html

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

21

id116: pros and cons

+ easy to implement

but...

- converges to a locally optimal solution
- need to know k
- su   er from the curse of dimensionality
- no theoretical foundation

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

22

id116: pros and cons

+ easy to implement

but...

- converges to a locally optimal solution
- need to know k
- su   er from the curse of dimensionality
- no theoretical foundation

let   s generalize id116 with expectation maximization

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

23

id116 & em

algorithm id116:

initialize randomly   1,         k.
repeat

//assign each point xi to the cluster with the closest   j.
expectation:

freeze the   j, minimize j

until convergence.

freeze the points:

note:
to clusters fixed.

means keep the belonging of points

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

24

id116 & em

algorithm id116:

initialize randomly   1,         k.
repeat

//assign each point xi to the cluster with the closest   j.
expectation:
//calculate the new mean for each cluster as follows:
maximization:

freeze the   j, minimize j

freeze the points, minimize j

  j =

1

|cj| xxi2cj

xi

until convergence.

freeze the points:

note:
to clusters fixed.

means keep the belonging of points

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

25

em in general

given:
    x: set of observed data,
    z: set of unobserved latent data,
       : set of unknown parameters,
    l(   ; x, z) = p(x, z|   ), a likelihood function.

in general, em is an iterative procedure for    nding the id113.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

26

em in general

given:
    x: set of observed data,
    z: set of unobserved latent data,
       : set of unknown parameters,
    l(   ; x, z) = p(x, z|   ), a likelihood function.

in general, em is an iterative procedure for    nding the id113.

map the framework to id116.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

27

em in general

both     and z are unknown.

1. initialize the parameters     to some random values.

2. compute the best values of z given these parameter values.

3. use the just-computed values of z to find better estimates

for    .

4. go to 2.

until convergence.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

28

extra hw
search how we can    nd k.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

29

credit and further reading
1. greg hamerly and charles elkan. learning the k in id116.

in neural information processing systems, 2003.

2. david arthur and sergei vassilvitskii. id116 ++ : the ad-
vantages of careful seeding. in proceedings of the eighteenth
annual acm-siam symposium on discrete algorithms, volume
8, pages 10271035, 2007.

3. c.f. wu. on the convergence properties of the em algorithm.

the annals of statistics, 11:95103, 1983.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

30

digression: id113
given a set of potential distributions: {p(x|   ) |     2 rd}
given a dataset {x1,       , xn}
find the distribution that best explains or    t the data.

find the best parameter      .

maximum likelihood approach to    nd the best distribution:
maximum likelihood assumes that data is best explained by the
distribution under which it has the highest id203.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

31

digression: id113
given a set of potential distributions: {p(x|   ) |     2 rd}
given a dataset {x1,       , xn}
find the distribution that best explains or    t the data.

find the best parameter      .

maximum likelihood approach to    nd the best distribution:
maximum likelihood assumes that data is best explained by the
distribution under which it has the highest id203.

maximum likelihood estimator (id113) is:

     m le = argmax    p(x1,       , xn|   )

the parameter which maximizes the joint density of the data.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

32

digression: id113

we assume that the training examples (sequence of variables or
features) are independent, and identically distributed (i.i.d. us-
ing the machine learning jargon):
1. if each feature (random variable) has the same id203 dis-

tribution as the others and

2. if all features are mutually independent

     m le = argmax   

nyi=1

p(xi|   )

id113 equation:

r   (

nyi=1

p(xi|   ) = 0

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

33

digression: logarithm trick

we have:

log(yi

fi) =xi

log(fi)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

34

digression: logarithm trick

we have:

log(yi

fi) =xi

log(fi)

     m le = argmax   

nyi=1

p(xi|   )

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

35

digression: logarithm trick

we have:

log(yi

fi) =xi

log(fi)

     m le = argmax   

     m le = argmax   

p(xi|   )

nyi=1
nxi=1
log p(xi|   )

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

36

digression: logarithm trick

we have:

log(yi

fi) =xi

log(fi)

     m le = argmax   

     m le = argmax   

nxi=1r    log p(xi|   ) =

p(xi|   )

nyi=1
nxi=1
log p(xi|   )
nxi=1
r    p(xi|   )
p(xi|   )

= 0

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

37

gaussian distributions

density of the multivariate gaus-
sian (normal) distribution.

p(x|  ,    ) = n (x;   ,    ) =

1

p(2   )d|   |

exp( 

@ln n (x1,       , xn|  ,    )

   

1

2 x     )t     1(x     ) 

= 0

    m le =

1
n

nxi=1

xi

     m le =

1
n

nxi=1

(xi       m le)(xi       m le)t

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

38

gmm assumption

assumes a linear combination of gaussians:

f(x) =

kxi=1

wi n (x;   i,    i),

where

kxi=1

wi = 1

we denote a mixture of gaussians by g.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

39

em for gmm

given:
    training set {x1,       , xn}.
    number of clusters k.

model the data using a mixture of gaussians:
    weights {w1,       , wk}.
    means and covariances:   1,       ,   k,    1,       ,    k.

f(x) =

kxi=1

wi n (x;   i,    i),

where

kxi=1

wi = 1

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

40

em for gmm

likelihood of the whole training set:

p(xi|g)

nyi=1
p(x1,       , xn|g) =
kxj=1
nyi=1
wjn (xi|  j,    j)

p(x1,       , xn|g) =
write the em for id91.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

41

em for gmm

http://www.cs.cmu.edu/~alad/em/

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

42

arti   cial intelligence

machine learning

naive bayes

ansaf salleb-aouissi

columbia university - fall 2014 w4701 section 2

outline

1. generative models

2. naive bayes classi   er.

3. setting

4. example

5. estimating probabilities

6. text classi   cation

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

bayes rule

p(a|b) =

p(a ^ b)

p(b)

p(a ^ b) = p(a|b)     p(b)

two events are independent, if:

p(a ^ b) = p(a)     p(b)

p(c ^ a ^ b) = p(c|a ^ b)     p(a|b)     p(b)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

bayes rule

writing p(a ^ b) in two di   erent ways:

p(a ^ b) = p(b|a)     p(a)
p(a ^ b) = p(a|b)     p(b)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

bayes rule

writing p(a ^ b) in two di   erent ways:

p(a ^ b) = p(b|a)     p(a)
p(a ^ b) = p(a|b)     p(b)

p(a|b) =

p(b|a)     p(a)

p(b)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

bayes rule

writing p(a ^ b) in two di   erent ways:

p(a ^ b) = p(b|a)     p(a)
p(a ^ b) = p(a|b)     p(b)

p(a|b) =

p(b|a)     p(a)

p(b)

p(a|b) is called posterior (posterior distribution on a given b.)
p(a) is called prior.
p(b) is called evidence.
p(b|a) is called likelihood.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

bayes rule

other forms:

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

bayes rule

other forms:

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

p(a|b ^ x) =

p(b|a ^ x)     p(a ^ x)

p(b ^ x)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008

p(  a) = 0.992

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008
p(b|a) = 0.98

p(  a) = 0.992
p(  b|a) = 0.02

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

10

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008
p(b|a) = 0.98
p(b|  a) = 0.03

p(  a) = 0.992
p(  b|a) = 0.02
p(  b|  a) = 0.97

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

11

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008
p(b|a) = 0.98
p(b|  a) = 0.03

p(  a) = 0.992
p(  b|a) = 0.02
p(  b|  a) = 0.97

p(a|b) =

0.98     0.008

0.98     0.008 + 00.3     0.992

= 0.21

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

12

why probabilities?

why are we bringing here a bayesian framework?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

13

why probabilities?

why are we bringing here a bayesian framework?

recall classi   cation framework:

given: training data: (x1, y1), . . . , (xn, yn)/xi 2 rd and yi 2 y.
task: learn a classi   cation function: f : rd  ! y

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

14

why probabilities?

why are we bringing here a bayesian framework?

recall classi   cation framework:

given: training data: (x1, y1), . . . , (xn, yn)/xi 2 rd and yi 2 y.
task: learn a classi   cation function: f : rd  ! y

learn a mapping from x to y.
we would like to    nd this mapping f(x) = y through p(y|x)!

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

15

discriminative algorithms

    discriminative algorithms:

    idea: model p(y|x;  ), conditional distribution of y given x.
    e.g., we modeled in id28 p(y|x;  ) as g( t x)

where g is the sigmoid function.

    in discriminative algorithms:    nd a decision boundary that

separates positive from negative example.

    to predict a new example, check on which side of the de-

cision boundary it falls.
    model p(y|x) directly!

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

16

generative algorithms

    generative algorithms adopt a di   erent approach:

    idea: build a model for what positive examples look like.
build a di   erent model for what negative example look like.

    to predict a new example, match it with each of the models

and see which match is best.

    model p(x|y) and p(y)!
    use bayes rule to obtain p(y|x) = p(x|y)p(y)
    to make a prediction:

p(x)

.

argmaxyp(y|x) = argmaxy
argmaxyp(y|x)     argmaxyp(x|y)p(y)

p(x)

p(x|y)p(y)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

17

naive bayes classi   er

    probabilistic model.
    highly practical method.
    application domains to natural language text documents.
    naive because of the strong independence assumption it makes

(not realistic).

    simple model.
    strong method can be comparable to id90 and neural

networks in some cases.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

18

naive bayes classi   er

an empirical comparison of supervised learning algorithms

table 3. normalized scores of each learning algorithm by problem (averaged over eight metrics)

model

cal

covt adult ltr.p1 ltr.p2 medis

slac

hs

mg calhous cod

bact

mean

bst-dt
rf
bag-dt
bst-dt
rf
bag-dt
rf
bag-dt
id166
ann
id166
ann
ann
bst-dt
knn
knn
knn
bst-stmp
id166
bst-stmp
bst-stmp
dt
dt
dt
lr
lr
lr
nb
nb
nb

plt
plt
   
iso
   
plt
iso
iso
plt
   
iso
plt
iso
   
plt
   
iso
plt
   
iso
   
iso
   
plt
   
iso
plt
iso
plt
   

.959
.857
.930
.897
.944* .883

.938
.876
.878
.922* .865
.876
.873
.865
.867
.765
.764
.758
.766
.767
.874
.819
.807
.814
.644
.696
.639
.605
.671
.652
.661
.625
.616
.610
.574
.572
.552

.946* .883
.877
.931
.851
.934
.840
.933
.886
.936
.913
.884
.899
.882
.898
.872
.821
.882
.842
.875
.920
.785
.912
.780
.879
.784
.949
.767
.731
.819
.700
.941
.540
.865
.729
.869
.872
.723
.734
.863
.195
.886
.229
.881
.870
.185
.674
.904
.648
.892
.843
.534

.976
.941
.911
.901* .969
.922
.920
.935
.915
.962
.901
.954
.894
.891
.913
.937
.936
.935
.688
.860
.681
.615
.760
.763
.756
.448
.440
.446
.557
.561
.556

.974
.855
.869
.933
.700
.937
.883
.907* .884
.810
.948
.898
.898* .856
.762
.845
.965
.927
.692* .878
.891* .941
.912* .871
.785
.944
.884
.863
.752
.885
.933
.876
.877
.767* .920
.940
.856
.897
.749
.884
.913* .816
.733
.866
.897
.923
.932* .859
.791* .881
.897
.907
.693* .878
.827
.919
.929* .846
.775
.871
.915
.926* .841
.785* .895
.523
.807
.860
.785
.933
.827
.844
.803
.777
.626
.827
.853
.801
.800
.598
.824
.832
.794
.791
.633
.723
.806
.800
.862
.923
.833
.776
.788
.859
.600
.912
.862
.793
.807
.711
.817
.799
.683
.779
.624
.832
.815
.622
.777
.424
.449
.769
.609
.829
.831
.826
.822
.607
.416
.779
.838
.849
.675
.777* .852
.833
.827
.659
.763* .834
.738
.835
.667
.823
.832
.758
.687
.205
.724
.709
.755
.690
.213
.732
.694
.011
.714
-.654
.655
.759

.907*

.878*

.915
.903* .847
.856
.926
.912* .861
.874
.824
.865
.912*
.897* .821
.859
.900* .807
.667
.882
.900* .778
.871
.665
.862
.672
.835
.858
.855
.774
.852
.748
.833
.777
.622
.915*
.763
.864
.902*
.632
.906*
.581
.884
.415
.389
.899*
.890*
.407
.905*
.647
.889*
.636
.633
.895
.770
.633
.756
.632
.636
.688

.896*
.892
.887*
.885*
.884
.882
.880
.877
.862
.854
.852
.846
.842
.828
.815
.810
.809
.791
.781
.780
.710
.709
.708
.706
.700
.692
.685
.654
.650
.481

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

19

than boosting stumps only on seven problems. occa-
sionally boosted stumps perform very well, but some-
times they perform very poorly so their average per-
formance is low. on adult, when boosting trees, the
   rst iteration of boosting hurts the performance of all
tree types, and never recovers in subsequent rounds.
when this happens even single id90 outper-
form their boosted counterparts. bagged trees and

to help evaluate the impact of the choice of problems
and metrics we performed a bootstrap analysis. we
randomly select a bootstrap sample (sampling with
replacement) from the original 11 problems. for this
sample of problems we then randomly select a boot-
strap sample of 8 metrics from the original 8 metrics
(again sampling with replacement). for this bootstrap
sample of problems and metrics we rank the ten algo-

setting

    a training data (xi, yi), xi is a feature vector and yi is a discrete

label.

    d features, and n examples.
    example: consider document classi   cation, each example is a
documents, each feature represents the presence or absence of
a particular word in the document.

    we have a training set.
    a new example with feature values xnew = (a1, a2,       , ad).
    we want to predict the label ynew of the new example.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

20

setting

ynew = y2y p(y|a1, a2,       , ad)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

21

setting

ynew = y2y p(y|a1, a2,       , ad)

use bayes rule to obtain:

ynew = y2y

p(a1, a2,       , ad|y)     p(y)

p(a1, a2,       , ad)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

22

setting

ynew = y2y p(y|a1, a2,       , ad)

use bayes rule to obtain:

ynew = y2y

p(a1, a2,       , ad|y)     p(y)

p(a1, a2,       , ad)

ynew = y2y p(a1, a2,       , ad|y)     p(y)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

23

setting

ynew = y2y p(y|a1, a2,       , ad)

use bayes rule to obtain:

ynew = y2y

p(a1, a2,       , ad|y)     p(y)

p(a1, a2,       , ad)

ynew = y2y p(a1, a2,       , ad|y)     p(y)

can we estimate these two terms from the training data?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

24

setting

ynew = y2y p(y|a1, a2,       , ad)

use bayes rule to obtain:

ynew = y2y

p(a1, a2,       , ad|y)     p(y)

p(a1, a2,       , ad)

ynew = y2y p(a1, a2,       , ad|y)     p(y)

can we estimate these two terms from the training data?
1. p(y) can be easy to estimate: count the frequency with which

each label y occurs in the training data.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

25

setting

ynew = y2y p(y|a1, a2,       , ad)

use bayes rule to obtain:

ynew = y2y

p(a1, a2,       , ad|y)     p(y)

p(a1, a2,       , ad)

ynew = y2y p(a1, a2,       , ad|y)     p(y)

can we estimate these two terms from the training data?
1. p(y) can be easy to estimate: count the frequency with which

each label y.

2. p(a1, a2,       , ad|y) is not easy to estimate unless we have a very
very large sample. (we need to see every example many times
to get reliable estimates) #possible ex. * #possible y

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

26

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,       , ad is the product of the probabilities for the
individual features:

p(a1, a2,       , ad|y) =yj

p(aj|y)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

27

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,       , ad is the product of the probabilities for the
individual features:

naive bayes classi   er:

p(aj|y)

p(a1, a2,       , ad|y) =yj
ynew = y2y p(y)yj

p(aj|y)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

28

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,       , ad is the product of the probabilities for the
individual features:

naive bayes classi   er:

p(aj|y)

p(a1, a2,       , ad|y) =yj
ynew = y2y p(y)yj

p(aj|y)

can we estimate these two terms from the training data?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

29

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,       , ad is the product of the probabilities for the
individual features:

naive bayes classi   er:

p(aj|y)

p(a1, a2,       , ad|y) =yj
ynew = y2y p(y)yj

p(aj|y)

can we estimate these two terms from the training data?

#possible distinct feature values * #possible y

yes!!!

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

30

algorithm

learning: based on the frequency counts in the dataset:
1. estimate all p(y), 8y 2 y.
2. estimate all p(aj|y) 8y 2 y, 8ai.
classi   cation: for a new example, use:

ynew =y2y p(y)yj

p(aj|y)

note: no model per se or hyperplane, just count the frequencies
of various data combinations within the training examples.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

31

example

outlook  temperature humidity windy play
no
sunny
sunny
no
yes
overcast
rainy
yes
yes
rainy
no
rainy
overcast
yes
no
sunny
sunny
yes
yes
rainy
yes
sunny
overcast
yes
yes
overcast
rainy
no

high
high
high
high
normal
normal
normal
high
normal
normal
normal
high
normal
high

false
true
false
false
false
true
true
false
false
false
true
true
false
true

hot
hot
hot
mild
cool
cool
cool
mild
cool
mild
mild
mild
hot
mild

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

32

example

outlook  temperature humidity windy play
no
sunny
sunny
no
yes
overcast
rainy
yes
yes
rainy
no
rainy
overcast
yes
no
sunny
sunny
yes
yes
rainy
yes
sunny
overcast
yes
yes
overcast
rainy
no

high
high
high
high
normal
normal
normal
high
normal
normal
normal
high
normal
high

false
true
false
false
false
true
true
false
false
false
true
true
false
true

hot
hot
hot
mild
cool
cool
cool
mild
cool
mild
mild
mild
hot
mild

new example:

(outlook = sunny, temperature = cool, humidity = high, wind =strong).

can we predict the class of the new example?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

33

example

ynew =y2{yes,no} p(y)     p(outlook = sunny|y)     p(temp = cool|y)   

p(humidity = high|y)     p(windy = strong|y)

p(play = yes) = 9/14 = 0.64

p(play = no) = 5/14 = 0.36

conditional probabilities:

p(wind = strong|play = yes) = 3/9 = 0.33
p(wind = strong|play = no) = 3/5 = 0.6

p(yes)     p(sunny|yes)     p(cool|yes)     p(high|yes)     p(strong|yes) = 0.0053

p(no)     p(sunny|no)     p(cool|no)     p(high|no)     p(strong|no) = 0.0206

ynew = no

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

34

estimating probabilities

m-estimate of the id203:

p(aj|y) =

nc + m     p
ny + m

intuition:
augment the sample size by m virtual examples, distributed ac-
cording to prior p (prior estimate of each value). if a feature has
k values, we can set p = 1
k.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

35

credit
    machine learning. tom mitchell 1997.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

36

arti   cial intelligence

id124

problems

ansaf salleb-aouissi

university - fall 2014 w4701 section 2

outline

i - id124 problems
1. de   nition
2. examples
3. varieties of csps
4. real world csps
5. solving csps

    backtracking search
    constraint propagation
    ordering of variables and values
    local search
    leveraging structures

ii - historical moment today

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

1

recall

    search problems:

    find the sequence of actions that lead to the goal.

    sequence of actions means a path in the search space.

    paths come with di   erent costs and depths.

    we use    rules of thumb    a.k.a heuristics to guide the

search e ciently.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

2

recall

    search problems:

    find the sequence of actions that lead to the goal.

    sequence of actions means a path in the search space.

    paths come with di   erent costs and depths.

    we use    rules of thumb    a.k.a heuristics to guide the

search e ciently.

    id124 problems:

    a search problem too!

    we don   t care about the path but about the goal itself.

    all path are of same depth

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

3

csps de   nition

    search problems:

    a state is a black box, implemented as some data structure.

    a goal test is a function over the states.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

4

csps de   nition

    search problems:

    a state is a black box, implemented as some data structure.

    a goal test is a function over the states.

    csps problems:

    a state: de   ned by variables xi with values from domain

di.

    a goal test is a set of constraints specifying allowable

combinations of values for subsets of variables.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

5

example: map coloring

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

6

example: map coloring

variables: wa, nt, q, nsw, v , sa, t

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

7

example: map coloring

variables: wa, nt, q, nsw, v , sa, t
domains: di = {red, green, blue}

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

8

example: map coloring

variables: wa, nt, q, nsw, v , sa, t
domains: di = {red, green, blue}
constraints: adjacent regions must have di   erent colors;

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

9

example: map coloring

variables: wa, nt, q, nsw, v , sa, t
domains: di = {red, green, blue}
constraints: adjacent regions must have di   erent colors;
e.g., wa 6= nt or (w a, n t ) 2 {(red, green), (red, blue), etc..}
question: find the set of all constraints.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

10

example: map coloring

find the solution: assignment that satisfy all constraints
solutions are assignments satisfying all.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

11

example: map coloring

example:
{wa=red,
sa=blue,t=green}

nt=green,

q=red,

nsw=green,v=red,

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

12

other examples

n-queen:

problem formalization:

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

13

other examples

sudoku:

http://www.websudoku.com/

problem formalization:

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

14

constraint graph

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

15

constraint graph

binary csp: each constraint relates at most two variables con-
straint graph: nodes are variables, arcs show constraints

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

16

constraint graph

binary csp: each constraint relates at most two variables con-
straint graph: nodes are variables, arcs show constraints

csp algorithms: use the graph structure to speed up search.
e.g., tasmania is an independent subproblem!

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

17

varieties of csps

    discrete variables

    finite domains. size = d ) o(dn) complete assignments
    in   nite domains (integers, strings, etc.)

e.g., map coloring, time scheduling

e.g., job scheduling, variables are start and end days for
each job
need a constraint language, e.g., start job1 + 5     start job3

    continuous variables

e.g., start/end times for hubble telescope observation (very
precise timing with power and astronomical constraints, etc.)

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

18

varieties of constraints

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

19

example cryptarithmetic

find a substitution of digits to letters s.t. the sum is correct.

    xi auxiliary variables representing digits that are carried over.
    each letter is a di   erent digit
    hypernodes represent n-ary constraints.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

20

real-world csps

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

21

solving csps

    bfs: develop the complete tree
    dfs: fine but time consuming
    bts: backtracking search is the basic uninformed search

for csps. it   s a dfs s.t.

1. assign one variable at a time: assignments are commuta-
tive. e.g., (wa=red, nt=green) is same as (nt=green,
wa=red)

2. check constraints on the go: consider values that do not

con   ict with previous assignments.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

22

solving csps

    initial state: empty assignment {}
    states: are partial assignments
    successor function: assign a value to an unassigned variable
    goal test: the current assignment is complete and satis   es all

constraints

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

23

backtracking search

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

24

backtracking search

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

25

backtracking search

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

26

backtracking search

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

27

backtracking search

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

28

improving bts

1. which variable should be assigned next?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

29

improving bts

1. which variable should be assigned next?

2. in what order should its values be tried?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

30

improving bts

1. which variable should be assigned next?

2. in what order should its values be tried?

3. can we detect inevitable failure early?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

31

improving bts

1. which variable should be assigned next?

2. in what order should its values be tried?

3. can we detect inevitable failure early?

4. can we take advantage of problem structure?

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

32

minimum remaining values

    mrv: choose the variable with the fewest legal values in

its domain

pick the hardest!

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

33

least constraining value

    lcv: given a variable, choose the least constraining
value: the one that rules out the fewest values in the
remaining variables

pick the ones that are likely to work!

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

34

forward checking

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

35

forward checking

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

36

forward checking

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

37

forward checking

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

38

constraint propagation

    in state-space search: search for a solution (goal)
    in csps, two approaches!

1. search: choose a new variable assignment from several

possibilities

2. id136: do a kind of id136 called constraint prop-

agation to reduce the number of legal values.

the two approaches can be intertwined. constraint satisfac-
tion can solve some problems completely without any search.

key idea consistency.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

39

node consistency

simple! just remove undesired values.

a node (variable) in a graph is node consistent if it satis   es all
unary constraints.

e.g., wa 6= blue.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

40

arc consistency

nt and sa cannot both be blue.

to deal with it, we will be enforcing arc consistency.
visit all the arcs over and over until they are all consistent.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

41

arc consistency

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

42

arc consistency

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

43

arc consistency

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

44

arc consistency

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

45

problem structure

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

46

problem structure

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

47

problem structure

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

48

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

49

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

2. backtracking = depth-   rst search with one variable assigned

per node

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

50

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

2. backtracking = depth-   rst search with one variable assigned

per node

3. variable ordering and value selection heuristics help signicantly

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

51

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

2. backtracking = depth-   rst search with one variable assigned

per node

3. variable ordering and value selection heuristics help signicantly

4. forward checking prevents assignments that guarantee later

failure

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

52

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

2. backtracking = depth-   rst search with one variable assigned

per node

3. variable ordering and value selection heuristics help signicantly

4. forward checking prevents assignments that guarantee later

failure

5. constraint propagation (e.g., arc consistency) does additional

work to constrain values and detect inconsistencies

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

53

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

2. backtracking = depth-   rst search with one variable assigned

per node

3. variable ordering and value selection heuristics help signicantly

4. forward checking prevents assignments that guarantee later

failure

5. constraint propagation (e.g., arc consistency) does additional

work to constrain values and detect inconsistencies

6. the csp representation allows analysis of problem structure

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

54

summary

1. csps are a special kind of search problems:

    states de   ned by values of a    xed set of variables
    goal test de   ned by constraints on variable values

2. backtracking = depth-   rst search with one variable assigned

per node

3. variable ordering and value selection heuristics help signicantly

4. forward checking prevents assignments that guarantee later

failure

5. constraint propagation (e.g., arc consistency) does additional

work to constrain values and detect inconsistencies

6. the csp representation allows analysis of problem structure

7. tree-structured csps can be solved in linear time

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

55

historical moment today

david waltz (1943 - 2012)

his ph.d. dissertation on id161 initiated the    eld of constraint
propagation which allowed a computer program to generate a detailed three-
dimensional view of an object given a two dimensional drawing with shadows.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

56

credit

    aima book chapters 6.
    russell   s slides.

copyright c ansaf salleb-aouissi: fall 2014 computer science w4701 section 2 artificial intelligence

57

