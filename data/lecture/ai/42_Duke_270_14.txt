machine learning

george konidaris	
gdk@cs.duke.edu

spring 2016

machine learning
sub   eld of ai concerned with learning from data.	
!
!
broadly, using:	

    experience 
    to improve performance	
    on some task	

!
(tom mitchell, 1997)	
!

vs    

ml 
vs	

statistics 

vs	

data mining

why?
developing effective learning methods has proved dif   cult.	
why bother?	
!
autonomous discovery	

    we don   t know something, want to    nd out.	

!
hard to program	
!
!

    easier to specify task, collect data.

adaptive behavior	

    our agents should adapt to new data, unforeseen 

circumstances.	

types
depends on feedback available:	
!
labeled data:	

    supervised learning	

!
no feedback, just data:	

    unsupervised learning.	

!
sequential data, weak labels:	
    id23

supervised learning
input: 	
 x = {x1,    , xn}	
  y = {y1,    , yn}	
!
!
learn to predict new labels.	
given x: y?

inputs
labels

training data

inputs

unsupervised learning
input:	
 x = {x1,    , xn}	
!
try to understand the	
structure of the data.	
!
!
!
e.g., how many types of cars?	
how can they vary?

id23
learning counterpart of planning.	
!

   : s     a

max

 

r =

 trt

  t=0

today: supervised learning
formal de   nition:	
!
given training data:	
 x = {x1,    , xn}	
  y = {y1,    , yn}	
!
produce:	
 decision function	
!
 that minimizes error: 

inputs
labels

f : x ! y

err(f (xi), yi)

xi

classi   cation vs. regression
if the set of labels y is discrete:	

    classi   cation	
    minimize number of errors	

!
!
if y is real-valued:	

    regression	
    minimize sum squared error	

!
!
!

today we focus on classi   cation.

key ideas
class of functions f, from which to    nd f.	
    f is known as the hypothesis space.	

!
!
e.g., if-then rules:	

if condition then class1	
else class2	

!
!
!
learning:	

    search over f to    nd f that minimizes error.

test/train split
minimize error measured on what?	

    don   t get to see future data.	
    could use test data     but! may not generalize.	

!
general principle:	
do not measure error on the data you train on!  
!
methodology:	

    split data into training set and test set.	
    fit f using training set.	
    measure error on test set.	

!
always do this.

id90
let   s assume: 	
   
   
   

 discrete inputs.	
 two classes (true and false).	
 input x is a vector of values.	
!

relatively simple classi   er:	
    tree of tests.	
    evaluate test for for each xi, follow branch.	
    leaves are class labels.

id90

true

false

y=2

b?

true

y=1

a?

false

xi = [a, b, c]	
each boolean

false

y=1

true

b?

c?

false

y=1

true

y=2

id90
how to make one?	
!
given 	
  x = {x1,    , xn}	
  y = {y1,    , yn}	
!
repeat:	

   
   
   

  if all the labels are the same, we have a leaf node.	
  pick an attribute and split data on it.	
  recurse on each half.	
!

if we run out of splits, and data not perfectly in one class, then 
take a max.

id90

a?

a b c l
t f t 1
1
t t f
t f
1
f
f t f
2
f t t 2
2
f t f
f t 1
f
1
f
f

f

id90

a?

true

y=1

a b c l
t f t 1
1
t t f
t f
1
f
f t f
2
f t t 2
2
f t f
f t 1
f
1
f
f

f

id90

a?

true

y=1

false

b?

a b c l
t f t 1
1
t t f
t f
1
f
f t f
2
f t t 2
2
f t f
f t 1
f
1
f
f

f

id90

a?

true

y=1

false

b?

true

y=2

a b c l
t f t 1
1
t t f
t f
1
f
f t f
2
f t t 2
2
f t f
f t 1
f
1
f
f

f

id90

a?

true

y=1

false

b?

true

y=2

false

y=1

a b c l
t f t 1
1
t t f
t f
1
f
f t f
2
f t t 2
2
f t f
f t 1
f
1
f
f

f

attribute picking
key question:	

    which attribute to split over?	

i(a) =  f1 log2 f1   f2 log2 f2

!
information contained in a data set:	
!
!
how many    bits    of information do we need to determine the 
label in a dataset?	
!
pick the attribute with the max information gain:	

gain(b) = i(a)  xi

fii(bi)

example

a b c l
t f t 1
1
t t f
1
f
t f
f t f
2
f t t 2
2
f t f
f t 1
f
1
f
f

f

id90
what if the inputs are real-valued?	

    have inequalities rather than equalities.

a > 3.1

true

y=1

false

b < 0.6?

true

y=2

false

y=1

hypothesis class
what is the hypothesis class for a decision tree?	

    discrete inputs?	
    real-valued inputs?

