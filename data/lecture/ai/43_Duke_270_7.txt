uncertainty

george konidaris	
gdk@cs.duke.edu

spring 2016

logic is insuf   cient
the world is not deterministic.	
there is such thing as a fact.	
generalization is hard.	
sensors and actuators are noisy.	
plans fail.	
models are not perfect.	
learned models are especially imperfect.	

8x, f ruit(x) =) t asty(x)

probabilities
powerful tool for reasoning about uncertainty. 	
!
but, they   re tricky:	

intuition often wrong or inconsistent.	

   
    dif   cult to get.	

!
!

what do probabilities really mean?

relative frequencies
de   ned over events.	
!
!
!
!
!
!
!
!
!
p(a): id203 random event falls in a, rather than not a.	
works well for dice and coin    ips!

not a

a

relative frequencies 
but this feels limiting.	
!
what is the id203 that the blue devils will beat virginia on 
saturday?	

    meaningful question to ask.	
    can   t count frequencies (except naively).	
    only really happens once.	

!

in general, all events only happen once. 

probabilities and beliefs
suppose i    ip a coin and hide outcome.	

    what is p(heads)?	

!
this is a statement about a belief, not the world.	
(the world is in exactly one state, with prob. 1)	
!
assigning truth values to probabilities is tricky - must reference 
speaker   s state of knowledge.	
!
frequentists: probabilities come from relative frequencies.	
subjectivists: probabilities are degrees of belief.

for our purposes
no two events are identical, or completely unique.	
!
!
use probabilities as beliefs, but allow data (relative frequencies)  
to in   uence these beliefs.	
!
we use bayes    rule to combine prior beliefs with new data.  
!
!
!
can prove that a person who holds a system of beliefs 
inconsistent with id203 theory can be fooled.

to the math
probabilities talk about random variables:	
!

    x, y, z, with domains d(x), d(y), d(z).	

!

    domains may be discrete or continuous.	

!

    x = x: rv x has taken value x.	

!

    p(x) is short for p(x = x).

examples
x: rv indicating winner of duke vs. virginia game.	
!

d(x) = {duke, virginia, tie}. 	

!
a id203 is associated with each event in the domain:	

    p(x = duke) = 0.8	
    p(x = virginia) = 0.19	
    p(x = tie) = 0.01	

!
note: probabilities over the entire event space must sum to 1.	

expectation
common use of probabilities: each event has numerical value.	
!
example: 6 sided die. 	
what is the average die value?	
!
!
!
!
in general, given rv x and function f(x):

(1 + 2 + 3 + 4 + 5 + 6)

= 3.5

6

e[f (x)] =xx

p (x)f (x)

expectation
for example, in min-max search, we assumed the opposing 
player took the min valued action (for us).	
!
but that assumes perfect play. if we have a id203 distribution 
over the player   s actions, we can calculate their expected value (as 
opposed to min value) for each action.	
!
result: expecti-max algorithm.

kolmogorov   s axioms of id203
    0 <= p(x) <= 1	
    p(true) = 1, p(false) = 0	
    p(a or b) = p(a) + p(b) - p(a and b)	

!

suf   cient to completely specify id203 theory for discrete 
variables. 

multiple events
when several variables are involve, think about atomic events.	

    complete assignment of all variables.	
    all possible events.	
    mutually exclusive.	

!
rvs: raining, cold (both binary):	

joint distribution

!
!
!
!
!

raining cold prob.
true
true
false
false

true
false
true
false

0.3
0.1
0.4
0.2

!
 note: still adds up to 1.

joint id203 distribution
probabilities to all possible atomic events (grows fast)	
!
!
!
!
!
!
!
can de   ne individual probabilities in terms of jpd:	
p(raining) = p(raining, cold) + p(raining, not cold) = 0.4.

raining cold prob.
true
true
false
false

true
false
true
false

0.3
0.1
0.4
0.2

p (a) = xei2e(a)

p (ei)

independence
critical property! but rare.	
!
if a and b are independent:	
    p(a and b) = p(a)p(b)	
    p(a or b) = p(a) + p(b) - p(a)p(b)	

!
can break joint prob. table into separate tables.

independence
are raining and cold independent?	
!
!
!
!
!
!
!
p(raining) = 0.4	
p(cold)    = 0.7

raining cold prob.
true
true
false
false

true
false
true
false

0.3
0.1
0.4
0.2

independence: examples
independence: two events don   t effect each other.	

    duke winning ncaa, dem winning presidency.	
    two successive, fair, coin    ips.	
   
    poker hand and date.	

it raining, and winning the lottery.	

!
often we have an intuition about independence, but always 
verify. dependence does not mean causation!

mutual exclusion
two events are mutually exclusive when:	

    p(a or b) = p(a) + p(b).	
    p(a and b) = 0.	

!

this is different from independence.

independence is critical
to compute p(a and b) we need a joint id203.	

    this grows very fast.	
    need to sum out the other variables.	
    might require lots of data.	
    not a function of p(a) and p(b).	

!
if a and b are independent, then you can use separate, smaller 
tables. 	
!
much of machine learning and statistics is concerned with 
identifying and leveraging independence and mutual exclusivity.

conditional probabilities
what if you have a joint id203, and you acquire new data?	
!
my iphone tells me that its	
cold. what is the id203	
that it is raining?	
!
!
!
write this as:	

raining cold prob.
true
true
false
false

true
false
true
false

0.3
0.1
0.4
0.2

    p(raining | cold)	

!

conditional probabilities
we can write:	
!
!
!
!
this tells us the id203 of a given only knowledge b.	
!
this is a id203 w.r.t a state of knowledge. 

p (a|b) =

p (a and b)

p (b)

    p(disease | symptom)	
    p(raining | cold)	
    p(duke win | injury)

conditional probabilities
p(raining | cold)	
 = p(raining and cold) 	
    / p(cold)	
!
     p(cold) = 0.7	
     p(raining and cold) = 0.3	
!
p(raining | cold) ~= 0.43.	
!
note!	
p(raining | cold) + p(not raining | cold) = 1!

raining cold prob.
true
true
false
false

true
false
true
false

0.3
0.1
0.4
0.2

bayes   s rule
special piece of conditioning magic.	
!
!
!
!
!
!
!
if we have conditional p(b | a) and we receive new data for b, 
we can compute new distribution for a. (don   t need joint.)	
!
as evidence comes in, revise belief.

p (b|a)p (a)

p (a|b) =

p (b)

bayes example
suppose p(cold) = 0.7, p(headache) = 0.6.	
p(headache | cold) = 0.57	
!
what is p(cold | headache)? 	
!
!
!
!
!
!
!
not always symmetric!	
not always intuitive!

p (h)
0.57     0.7

p (h|c)p (c)

p (c|h) =

p (c|h) =

= 0.66

0.6

