unsupervised learning

george konidaris	
gdk@cs.duke.edu

spring 2016

machine learning
sub   eld of ai concerned with learning from data.	
!
!
broadly, using:	

    experience 
    to improve performance	
    on some task	

!
(tom mitchell, 1997)	
!

inputs

unsupervised learning
input:	
 x = {x1,    , xn}	
!
try to understand the	
structure of the data.	
!
!
!
e.g., how many types of cars?	
how can they vary?

id91
one particular type of unsupervised learning:	

    split the data into discrete clusters.	
    assign new data points to each cluster.	
    clusters can be thought of as types.	

!
!
formal de   nition 
	 given:	

    data points x = {x1,    , xn},	

	

find:	

    number of clusters k	
    assignment function f(x) = {1,    , k}	

id91

id116
one approach:	

    pick k	
    place k points (   means   ) in the data	
    assign new point to ith cluster if nearest to ith    mean   .	

!

id116

id116
major question:	
    where to put the    means   ?	
!
very simple algorithm:	

    place k    means                       at random.	
    assign all points in the data to each    mean   	

{  1, ...,   k}

!
f (xj) = i such that d(xj,   i)     d(xj,   l)8l 6= i
!

    move    mean    to mean of assigned data.	

!

  i = xv2ci

xv
|ci|

id116

id116

id116

id116

id116
remaining questions    	
!
how to choose k?	
!
what about bad initializations?	
!
broadly:	

    use a quality metric.	
    search through k.	
    random restart initial position.

density estimation
id91: can answer which cluster, but not does this belong?

density estimation
estimate the distribution the data is drawn from.	
!
this allows us to evaluate the id203 that a new point is 
drawn from the same distribution as the old data.	
!
formal de   nition 
	 given:	

    data points x = {x1,    , xn},	

	

find:	

    pdf p(x)	

gmm
simple approach:	

    model the data as a mixture of gaussians.	

!
each gaussian has its own mean and variance.	
each has its own weight (sum to 1).	
!
weighted sum of gaussians still a pdf.

gmm

gmm
!
algorithm - broadly as before:	
!
!

    place k    means                       at random.	
    set variances to be high.	

{  1, ...,   k}

!

    assign all points to highest id203 distribution.	

!

ci = {xv|n (xv|  i,  2

i ) > n (xv|  j,  2
    set mean, variance to match assigned data.	

j ),8j}

!

!

  i = xv2ci

xv
|ci|

 2

i = variance(ci) wi = |ci|pj |cj|

gmm

gmm

gmm

gmm
major issue:	

    how to decide between two gmms?	
    how to choose k?	

!
general statistical question: model selection.	
several good answers for this.	
!
simple example: bayesian information criterion (bic).	
trades off model complexity (k) with    t (likelihood).	

 2 log l + k log n

# parameters	

in model

likelihood

# data	
points

id84
x = {x1,    , xn}	
!
if n is high, data can be hard to deal with.	
    high-dimensional decision boundary.	
    need more data.	
    but data is often not really high-dimensional.	

!
!
id84: 

    reduce or compress the data	
    try not to lose too much!	
    find intrinsic dimensionality

id84
for example, imagine if x1 and x2 are meaningful features, and 
x3     xn are random noise.	
!
what happens to k-nearest neighbors?	
!
what happens to a decision tree?	
!
what happens to the id88 algorithm?	
!
what happens if you want to do id91?

id84
often can be phrased as a projection:	
!
!
!
where:	
  	
|x0| << |x|
 our goal: retain as much variance as possible.	
!
!

f : x ! x0

   
   

variance captures what varies within the data.

pca
principle components analysis. 
!
project data into a new space:	

    dimensions are linearly uncorrelated.	
    we have a measure of importance for each dimension.	

!
!

pca

pca
    gather data x1,    , xm.	
    adjust data to be zero-mean:	

!

!

xi = xi  xj

xj
m

    compute covariance matrix c.	
    compute unit eigenvectors vi and eigenvalues vi of c.	

!
each vi is a direction, and each vi is its importance - the amount 
of the data   s variance it accounts for.	
!
new data points: 
  xi = [v1, ..., vp]xi

eigenfaces

(courtesy orl database)

isomap
another approach:	

    estimate intrinsic geometric dimensionality of data.	
    recover natural distance metric

isomap
core idea: distance metric locally euclidean	
    small radius r, connect each point to neighbors	
    weight based on euclidean distance

isomap
solve all-points shortest pairs:	

    transforms local distance to global distance.	
    compute embedding.

isomap

from tenenbaum, de silva, and langford, science 290:2319-2323, december 2000.

application: novelty detection
intrusion detection - when is a user behaving unusually?	
!
first proposed by prof. dorothy denning in 1986.	
(1995 acm fellow)

