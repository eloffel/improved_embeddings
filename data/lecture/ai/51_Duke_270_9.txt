id110s ii

george konidaris	
gdk@cs.duke.edu

spring 2016

recall: id110

flu

allergy

sinus

nose

headache

recall: bn

flu

flu
true
false

p
0.6
0.4

sinus

sinus
true
false
true
false
true
false
true
false

flu allergy p
true
0.9
0.1
true
true
0.6
0.4
true
false
0.2
0.8
false
0.4
false
false
0.6

true
true
false
false
false
false
true
true

allergy

allergy p
0.2
0.8

true
false

headache

headache

true
false
true
false

sinus p
0.6
true
true
0.4
0.5
false
false
0.5

nose

nose
true
false
true
false

sinus p
true
0.8
0.2
true
false
0.3
0.7
false

joint: 32 (31) entries

id136
given a compute p(b | a).

flu

allergy

sinus

nose

headache

last time: variable elimination
!
!
!
!
!
     we can eliminate variables one at a time:	
(distributive law)

p (h) = xsan f

p (h|s)p (n|s)p (s|a, f )p (f )p (a)

p (h) =xsn
p (h) =xs

p (h|s)p (n|s)xaf
p (n|s)xaf
p (h|s)xn

p (s|a, f )p (f )p (a)

p (s|a, f )p (f )p (a)

sampling
id110s are generative models:	
    describe a id203 distribution.	
    can draw samples from that distribution.	
    this is like a stochastic simulation.	
    computationally expensive, but easy to code!	
!
!

generative models
widely used methodology in machine learning (later).	
!
describe a generative process for the data.	

    each variable is generated by a distribution	
    can generate more data.	

!
natural way to include domain knowledge.	

generative models

flu

flu
true
false

p
0.6
0.4

sinus

sinus
true
false
true
false
true
false
true
false

flu allergy p
0.9
true
true
0.1
0.6
true
true
0.4
0.2
false
false
0.8
0.4
false
false
0.6

true
true
false
false
false
false
true
true

nose

nose
true
false
true
false

sinus p
true
0.8
0.2
true
false
0.3
0.7
false

allergy

allergy p
0.2
0.8

true
false

headache

headache

true
false
true
false

sinus p
0.6
true
true
0.4
0.5
false
false
0.5

sampling the joint
algorithm for generating samples drawn from the joint 
distribution:	
!
for each node with no parents:	

    draw sample from marginal distribution.	
    condition children on choice (removes edge)	
    repeat.	

  	
results in arti   cial data set.	
id203 values - literally just count.

sampling the conditional
what if we want to know p(a | b)?	
!
we could use the previous procedure, and just divide the data 
up based on b.	
!
what if we want p(a | b)?	

    could do the same, just use data with b=b.	
    but what if b doesn   t happen often?	
    what is b involves many variables?

sampling the conditional
two broad approaches. 
!
rejection sampling:	

    sample, throw away when mismatch occurs. (b != b)	

!
importance sampling:	

    bias the sampling process to get more    hits   .	
    use a reweighing trick to unbias probabilities.

sampling
properties of sampling:	

    slow.	
    always works.	
    always applicable.	
    computers are getting faster.

bayes nets
high-level thoughts.	
!
bayes nets are a type of representation.	
!
!
there are multiple algorithms for id136; you can choose 
whichever you like.	
!
!

ai researchers talk about models more than algorithms.

id203 distributions
if you have a discrete rv, id203 distribution is a table:	
!
!
!
!
!
what if you have a real-valued random variable?	

flu
true
false

p
0.6
0.4

    temperature tomorrow	
    rainfall	
    number of votes in election	
    height

pdfs
continuous probabilities described by id203 density 
function f(x). 
!
pdf is about density, not id203.  

    non-negative.	
   
   

 	
f(x) might be greater than 1. 

zx

f (x) = 1

integrates to 1

f

x

pdfs
can   t ask p(x = 0.0014245)?	
!
the id203 of a single real-valued number is zero.	
!
instead we can ask for a range:	
!
!

p (a     x     b) =z b

a

f (x)dx

distributions
distributions usually speci   ed by a pdf type or family.	
!
each family is a parametrized function describing the pdf. 	
!
get a speci   c distribution by    xing the parameters.

uniform distribution
for example, uniform distribution over [0, 0.5].	
!
parameter: mean.

f

  

0

x

0.5

gaussian (normal)
a mean + an exponential drop-off, characterized by variance.

f

0

 2

  

x

0.5

f (x,   ,  2) =

1

 p2   

e  (x   )2

2 2

pdfs
when dealing with a real-valued variable, two steps:	

    specifying the family of distribution.	
    specifying the values of the parameters.	

!
conditioning on a discrete variable just means picking from a 
discrete number of parameter settings.

  a
0.5
0.1

 2
a
0.02
0.06

b

true
false

pdfs
conditioning on real-valued rv: 	
    parameters function of rv	

!

id75:

f (x) = w    x +    
y     n (w    x,  2)

y

0

x

0.5

parametrized forms
many machine learning algorithms start with 
parametrized, generative models. 
!
!
find pdfs / cpts (i.e., parameters) such that id203 that 
they generated the data is maximized.	
!
there are also non-parametric forms: describe the pdf directly 
from the data itself, not a function.

