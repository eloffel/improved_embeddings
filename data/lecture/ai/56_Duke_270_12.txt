probabilistic planning

george konidaris	
gdk@cs.duke.edu

spring 2016

the planning problem
finding a sequence of actions to achieve some goal.	
!
!
!
!
!
!
!
!
!
!
!

plans
it   s great when a plan works    	
!
!
!
!
!
!
!
!
!
     but the world doesn   t work like that.	
!
to plan effectively we need to take uncertainty seriously.

probabilistic planning
as before:	
    generalize logic to probabilities.	
    probabilistic planning.	
!
!
!
!
this results in a harder planning problem.	
in particular: 

    plans can fail. 
    can no longer compute straight-line plans.

probabilistic planning
recall:	

    problem has a state.	
    state has the markov property.	
p (st|st 1, at 1, st 2, at 2, ..., s0, a0) = p (st|st 1, at 1)

!
!
!
id100 (mdps):	

    the canonical decision making formulation.	
    problem has a set of states.	
    agent has available actions.	
    actions cause stochastic state transitions.	
    transitions have costs/rewards.	

s

a

  

r

t

< s, a,  , r, t >

id100
: set of states	
: set of actions	
: discount factor 	
!
: reward function	
                 is the reward received taking action    from state 	
r(s, a, s   )
 and transitioning to state   .	
!
: transition function	
                is the id203 of transitioning to state    after 
t (s   |s, a)
taking action    in state  . 	
!

   
s

   
s

a

a

s

s

(some states are absorbing - execution stops)

st+1
       depends only on    and 	
at
rt
    depends only on     and 	
at

the markov property
needs to be extended for mdps:	
   
   
!
!
current state is a suf   cient statistic of agent   s history. 
!
this means that:	

st

st

    decision-making depends only on current state	
    the agent does not need to remember its history	

!

example

0.9

0.05

0.05

!
states: set of grid locations	
actions: up, down, left, right	
transition function: move in direction of action with p=0.9	
reward function: -1 for every step, 1000 for    nding the goal

example

a
b

c

0.8
r=-2

0
.
2

r

=
-
5

a
b

a
b

c

c

mdps
our goal is to    nd a policy:	
!
!
!
!
!
that maximizes return: expected sum of rewards. 
(equiv: min sum of costs)	

    : s ! a

e[ iri]

1xi=1

policies and plans
compare a policy:	

    an action for every state.	

!
!
!
!
     with a plan:	

    a sequence of actions.	

!
!
!
!

why the difference?

   

r   (s)

      (s) = max

planning
so our goal is to produce optimal policy.	
!
!
!
planning: we know t and r.	
!
de   ne the value function to estimate this quantity:	
!
!
!
!
v is a useful thing to know. 
how to    nd v?	

v    (s) = e" 1xi=0

 iri(si)#

monte carlo estimation
one approach:	
!
    for each state s	
    repeat many times:	

    start at s	
 t
    run policy forward until absorbing state (or     small) 	
    write down discount sum of rewards received	
    this is a sample of v(s)	
    average these samples	

!

this always works! 	
but very high variance. why?

bellman
bellman   s equation is a condition that must hold for v:

v    (s) = r(s,    (s)) +   max

a xs0

t (s0|s, a)v    (s0)

reward

value of this state

expected value 	
of next state

value iteration
this gives us an algorithm for learning the value function for a 
speci   c given    xed policy:	
!
repeat:	

    make a copy of the vf.	
    for each state in vf, assign value using be.	
    replace old vf.	

!

this is known as value iteration.  	
(in practice, only adjust    reachable    states.)	
!
why do we can so much about vf?

policy iteration
we can adjust the policy given the vf:	
!
!
!
!
adjust policy to be greedy w.r.t vf.	
!
we can alternate value and policy iteration.	
surprising results:	

a "r(s, a) +  xs0

   (s) = max

t (s0|s, a)v (s0)#

    this converges even if alternate every step.	
    converges to optimal policy.	
    converges in polynomial time.

elevator scheduling
crites and barto (1985):	
    system with 4 elevators, 10    oors.	
    realistic simulator.	
    46 dimensional state space.

micromap
   drivers and loads    (trucking), castle lab at princeton	
!
   the model was used by 20 of the largest truckload carriers to 
dispatch over 66,000 drivers   

back to pddl
note that an mdp does not contain the structure of pddl.	
!
!
!
if we wish to combine that structure and probabilistic planning, 
we can use a related language called ppddl - probabilistic 
problem domain de   nition language.	
!

ppddl operators
now operators have probabilistic outcomes:	
!
(:action move_left	
 :parameters (x, y)	
 :precondition (not (wall(x-1, y))	
 :effect (probabilistic	
          0.8 (and (at(x-1)) (not at(x)) (decrease (reward) 1))	
          0.2 (and (at(x+1)) (not(at(x))(decrease (reward) 1))	
         )	
)

ppddl
instead of computing a plan, we again need a policy.	
!
most planners:	

    take as input ppddl.	
    use a simulator.	
    compute policy for states reachable from start.	
    are evaluated jointly with simulator.

robot planning
one more common type of planning left! 

