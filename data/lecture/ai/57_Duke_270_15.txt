machine learning

george konidaris	
gdk@cs.duke.edu

spring 2016

machine learning
sub   eld of ai concerned with learning from data.	
!
!
broadly, using:	

    experience 
    to improve performance	
    on some task	

!
(tom mitchell, 1997)	
!

recall: supervised learning
formal de   nition:	
!
given training data:	
 x = {x1,    , xn}	
  y = {y1,    , yn}	
!
produce:	
 decision function	
!
 that minimizes error: 

inputs
labels - if discrete: classi   cation

f : x ! y

err(f (xi), yi)

xi

id90

true

false

y=2

b?

true

y=1

a?

false

false

y=1

true

b?

c?

false

y=1

true

y=2

the id88
if your input (xi) is real-valued     explicit decision boundary?

+
+

+ +
+

+

-

-

-

-

the id88
for x = [x(1),     , x(n)]:	
    create an n-d line 	
    slope for each x(i)	
    constant offset	

f (x) = sign(w    x + c)

gradient

offset

y = mx + c

the id88
which side of a line are you on?

x

w

w    x = ||w||||x|| cos(   )

the id88
how do you reduce error?	

-

e = (yi   (w    xi + c))2

=  2(yi   (wi    xi + c))xi(j)

@e
@wj

descend this gradient	

to reduce error

the id88 algorithm
assume you have a batch of data:	
x = {x1,    , xn}	
y = {y1,    , yn}	
!
set w, c to 0.	
for each xi:	
   predict zi = sign(w.xi + c)	
   if zi != yi:	
      w = w + a(yi - zi)xi

converges if data	
is linearly separate

learning rate

probabilities
what if you want a probabilistic classi   er?	
!
instead of sign, squash output of 	
linear sum down to [0, 1]:	
!
!
!
resulting algorithm: 	
id28.

 (w    x + c)

id88s
what can   t you do?

+
+

+ +
+

+

-

-

-

-

frank rosenblatt
built the mark i in 1960.	
!

neural networks

 (w    x + c)

id28

nonparametric methods
most ml methods can be characterized by setting a few 
parameters. 	
!
alternative approach:	

    let the data speak for itself.	
    no    nite-sized parameter vector.	
    usually more interesting decision boundaries.

k-nearest neighbors
given training data:	
 x = {x1,    , xn}	
  y = {y1,    , yn}	
  distance metric d(xi, xj)	
!
for a new data point xnew:	
     nd k nearest points in x (measured via d)	
  set ynew is the majority label	

k-nearest neighbors

-

-
++ +

+

-
-

-

+
+

+ +
-
-
+
+

+

-

k-nearest neighbors
properties:	

    no learning phase.	
    must store all the data.	
   

log(n) computation per sample - grows with data.	
!
!

decision boundary: any function, given enough data. 
!
classic trade-off: memory and compute time for    exibility.

applications
training set: 60k digits	
test set: 10k digits	
!
!
!
!
!
!
!
!
!
best performance: ~0.3% error rate.

applications
     fraud detection	
     internet advertising	
     friend or link prediction	
     id31	
     face recognition	
     spam    ltering

   a breakthrough in machine learning would 	

be worth ten microsofts.   	

 - bill gates

