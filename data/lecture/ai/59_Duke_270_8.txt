id110s

george konidaris	
gdk@cs.duke.edu

spring 2016

recall
joint distributions: 

    p(x1,    , xn). 	
    all you (statistically) need to know about x1     xn.	
    from it you can infer p(x1), p(x1 | xs), etc.	

!
!
	

raining cold prob.
true
true
false
false

true
false
true
false

0.3
0.1
0.4
0.2

joint distributions are useful
classi   cation	

    p(x1 | x2     xn)	

things you know

!
!
co-occurrence	

    p(xa, xb)	

thing you want to know

how likely are these two things together?

!
!
rare id37	
    p(x1,    , xn)

modeling joint distributions
gets large fast	

    2n entries for n binary rvs.	

!
!
independence!	

    a bit too strong.	
    rarely holds.	

!
conditional independence.	
    good compromise.

conditional independence
a and b are conditionally independent given c if:	

    p(a | b, c) = p(a | c)	
    p(a, b | c) = p(a | c) p(b | c)	

!
(recall independence: p(a, b) = p(a)p(b))	
!
!
this means that, if we know c, we can treat a and b as if they 
were independent.	
!
a and b might not be independent otherwise!

example
consider 3 rvs:	

    temperature	
    humidity	
    season	

!
temperature and humidity are not independent. 
!
!
but, they might be, given the season: the season explains both, 
and they become independent of each other.

bayes nets
a particular type of graphical model:	

    a directed, acyclic graph.	
    a node for each rv.	

s

!
!
!
!
!
!
!
given parents, each rv independent of non-descendants.

h

t

s

bayes net
!
!
!
!
!
!
!
jpd decomposes:	
!
!
!
so for each node, store id155 table (cpt):

p (x1, ..., xn) =yi

p (xi|parents(xi))

h

t

p (xi|parents(xi))

example
suppose we know:	

    the    u causes sinus in   ammation.	
    allergies cause sinus in   ammation.	
    sinus in   ammation causes a runny nose.	
    sinus in   ammation causes headaches.

example

flu

allergy

sinus

nose

headache

example

flu

flu
true
false

p
0.6
0.4

sinus

sinus
true
false
true
false
true
false
true
false

flu allergy p
true
0.9
0.1
true
true
0.6
0.4
true
false
0.2
0.8
false
0.4
false
false
0.6

true
true
false
false
false
false
true
true

allergy

allergy p
0.2
0.8

true
false

headache

headache

true
false
true
false

sinus p
0.6
true
true
0.4
0.5
false
false
0.5

nose

nose
true
false
true
false

sinus p
true
0.8
0.2
true
false
0.3
0.7
false

joint: 32 (31) entries

naive bayes

p(s)

s

w1

w2

w3

   

wn

p(w1|s) p(w2|s) p(w3|s)

p(wn|s)

(spam    lter!)

uses
things you can do with a bayes net:	

   

id136: given some variables, posterior?	
   

(might be intractable: np-hard)	

    learning (   ll in cpts)	
    structure learning (   ll in edges)	

!
!

generally:	

    often few parents.	
id136 cost often reasonable.	
   
    can include domain knowledge.	

id136
what is:	
p(f | h)?	

flu

allergy

sinus

nose

headache

id136
!
!
!
!
!
we know from de   nition of bayes net:

p (f, h)
p (h)

p (f|h) =

= psan p (f, h, s, a, n )
psan f p (h, s, a, n, f )

p (h) = xsan f
p (h) = xsan f

p (h, s, a, n, f )

p (h|s)p (n|s)p (s|a, f )p (f )p (a)

p (h|s)p (n|s)p (s|a, f )p (f )p (a)

variable elimination
so we have:	
!
!
!
!
     we can eliminate variables one at a time:	
(distributive law)

p (h) = xsan f

p (h) =xsn
p (h) =xs

p (h|s)p (n|s)xaf
p (n|s)xaf
p (h|s)xn

p (s|a, f )p (f )p (a)

p (s|a, f )p (f )p (a)

variable elimination
generically:	

    query about xi and xj.	
    write out p(x1     xn) in terms of p(xi | parents(xi))	
    sum out all variables except xi and xj	
    answer query using joint distribution p(xi, xj)	

!
good news:	

    potentially exponential reduction in computation.	
    polynomial for trees.	

!
bad news:	

    picking variables in optimal order np-hard.	
    for some networks, no elimination.

spam filter (naive bayes)

p(s)

s

w1

w2

w3

   

wn

p(w1|s) p(w2|s) p(w3|s)

p(wn|s)

want p(s | w1     wn)

naive bayes

p (s|w1, ..., wn) =

p (w1, ..., wn|s)p (s)

p (w1, ..., wn)

given

p (w1, ..., wn|s) =yi

p (wi|s)

(from the 	
bayes net)

bayes nets
potentially very compressed but exact.	

    requires careful construction!	

!
                               vs	
!
approximate representation.	

    hope you   re not too wrong!	

!
!

many, many applications in all areas.

