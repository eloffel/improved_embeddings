id23

george konidaris	
gdk@cs.duke.edu

spring 2016

machine learning
sub   eld of ai concerned with learning from data.	
!
!
broadly, using:	

    experience 
    to improve performance	
    on some task	

!
(tom mitchell, 1997)	
!

id23
learning counterpart of planning.	
!

   : s     a

max

 

r =

 trt

  t=0

rl
the problem of learning how to interact with an 
environment to maximize reward.	

rl

agent interacts with an environment	
at each time t:	

st

     receives sensor signal	
     executes action 	
     transition:	
    new sensor signal 	
    reward	

at

rt

st+1

goal:    nd policy    that maximizes expected return (sum of 
discounted future rewards):

  

max

  

e!r =

   trt#

   

"t=0

rl
this formulation is general enough to encompass a wide 
variety of learned control problems.

s

a

  

r

t

< s, a,  , r, t >

id100
: set of states	
: set of actions	
: discount factor 	
!
: reward function	
                 is the reward received taking action    from state 	
r(s, a, s   )
 and transitioning to state   .	
!
: transition function	
                is the id203 of transitioning to state    after 
t (s   |s, a)
taking action    in state  . 	
!
rl: one or both of t, r unknown.

   
s

   
s

a

a

s

s

rl
example:	
!
!
!
!
!
!
!
!
states: set of grid locations	
actions: up, down, left, right	
transition function: move in direction of action with p=0.9	
reward function: -1 for every step, 1000 for    nding the goal

rl
example:	
!
!
!
!
!
!
!
!
states:                        (real-valued vector)	
actions: +1, -1, 0 units of torque added to elbow	
transition function: physics!	
reward function: -1 for every step

(  1,     1,   2,     2)

   : s     a

mdps
our target is a policy:	
!
!
!
a policy maps states to actions.	
!
the optimal policy maximizes: 	
!
!
!
!
this means that we wish to    nd a policy that maximizes the 
return from every state.

   s, e!r(s) =

   trt#####

s0 = s$

"t=0

max

   

  

value functions

given a policy, we can estimate of         for every state.	

r(s)

    this is a value function.	
    it can be used to improve our policy.	

!
!
!
!

v  (s) = e!    
"t=0

  , s0 = s$

   trt#####

this is the value of state   under policy   .	

  

s

value functions

value functions
similarly, we de   ne a state-action value function as follows:	
!
!
!
!
this is the value of executing   in state  , then following   .	
!
note that:	

q  (s, a) = e!    
"t=0

  , s0 = s, a0 = a$

   trt#####

  

a

s

q  (s,   (s)) = v  (s)

v  (s),    s

policy iteration
recall that we seek the policy that maximizes               .	
!
therefore we know that, for the optimal policy     :	
!
!
!
!
!
this means that any change to    that increases     anywhere 
obtains a better policy.

q     (s, a)     q  (s, a),      , s, a

v     (s)     v  (s),      , s

q

  

  

   

policy iteration
this leads to a general policy improvement framework:	

  

1. start with a policy 	
2. learn 	
3.

q  
improve 	
a.

 	
  (s) = max

  

q(s, a),    s

repeat

!

a

this is known as policy iteration. 	
it is guaranteed to converge to the optimal policy.	
!
steps 2 and 3 can be interleaved as rapidly as you like.	
usually, perform 3a every time step.

value function learning
learning proceeds by gathering samples of            .	
q(s, a)

methods differ by:	

    how you get the samples.	
    how you use them to update    .q

   
monte carlo
simplest thing you can do: sample        .	
r(s)
!
!
!
!
!
!
!
!
!
do this repeatedly, average values:

r

r

r

r

r

r

r

r

q(s, a) =

r1(s) + r2(s) + ... + rn(s)

n

temporal difference learning
where can we get more (immediate) samples?	
!
idea:  there is an important relationship between temporally 
successive states.	
!
!

r
r(st) = rt +   r(st+1)

r

r

r

r

r

r

r

td learning
ideally and in expectation:	
!
!
   is correct if this holds in expectation for all states.	
v
!
!
when it does not, it is known as a temporal difference error. 	

rt +   v (st+1)     v (st) = 0

td learning
what does this look like?

st

st+1

at

rt

v (st)     rt +   v (st+1)
q(st, at)     rt +   q(st+1, at+1)

sarsa
sarsa: very simple algorithm	
!
1. initialize q(s, a)	
2. for n episodes	

   

    observe transition 	
    compute td error 	
    update q: 	
    select and execute action based on q

q(s, a) = q(s, a) +     

(s, a, r, s
   = r +   q(s   , a   )     q(s, a)

   )

, a

   

, a

   )

(s, a, r, s

td
in sarsa, we use a sample transition:	
this is a sample backup.	
!
given t, could replace with the full expectation:	
!
!
!
!
!
!
this is known as a full backup - id145.

p p

p

st+1

st+1

st+1

st

   = e  ,t [r +   q(s   , a   )]     q(s, a)

finds an optimal policy in time polynomial in      and      .	
|a|
(there are          possible policies.)

|s|

|a||s|

td vs. mc
td and mc two extremes of obtaining samples of q:	

r +   v

r +   v

r +   v

t=1

t=2

t=3

t=4

t=l

...

!

i

  iri

...

t=1

t=2

t=3

t=4

t=l

generalizing td
we can generalize this to the idea of an n-step rollout:	
!
!
!
!
!
each tells us something about the value function.	

    we can combine all n-step rollouts.	
    this is known as a complex backup.

td(  )

weighted sum:	
!
r(1) = r0 +  v (s1)
!
r(2) = r0 +  r1 +  2v (s2)
   .	
   .	
   .	
!
r(n) =
!
!
!
estimator:	
!

 iri +  nv (sn)

n 1xi=0

1
 

 n

weights

td(  )
this is called the   -return.	
    at   =0 we get td, at   =1 we get mc.	
    intermediate values of    usually best.	
    td(  ) family of algorithms

real-valued states
what if the states are real-valued?	

    cannot use table to represent q.	
    states may never repeat: must generalize.

vs

2.5

2

1.5

1

0.5

0

100

80

60

40

20

0
0

10

20

30

40

50

60

70

80

90

function approximation
how do we represent general function of state variables?	
!
many choices: 	

    most popular is linear value function approximation.	
    use set of basis functions	
    de   ne linear function of them:	

!

m

  v (x) =

!
!
!
learning task is to    nd vector of weights w to best 
approximate v. 

wi  i(x)

!

i=1

function approximation
one choice of basis functions: 	

    just use state variables directly: 	

[1, x, y]

!
another:	

    polynomials in state variables.	
    e.g.,	
    this is like a taylor expansion.	

[1, x, y, xy, x

, xy

, x

, y

2

2

2

2

2

yx

2]

y

!

another:	

    fourier terms on state variables.	
    e.g., 	
[1, cos(  x), cos(  y), cos(  [x + y])]
    this is like a fourier series expansion.

acrobot

acrobot

function approximation

td-gammon: tesauro (circa 1992-1995)	

    at or near best human level	
    learn to play backgammon through self-play	
    1.5 million games	
    neural network function approximator	
    td(  )	

!

changed the way the best human players played.

policy search
so far: improve policy via value function.	
sometimes policies are simpler than value functions:	

    parametrized program 	

  (s, a|  )

!

sometimes we wish to search in space of restricted policies.	
!
in such cases it makes sense to search directly in policy-space 
rather than trying to learn a value function. 	
!

policy search
can apply any generic optimization method for   .  	
!
one particular approach: policy gradient.	

  

    compute and ascend	
    this is the gradient of return w.r.t policy parameters	

   r/     

!

policy gradient theorem:	
!
!
!
!
therefore, one way is to learn q and then ascend gradient.	
q need only be de   ned using basis functions computed from  .
  

aibo gait optimization
from kohl and stone, icra 2004.	
!

postural recovery

id23
machine learning for control.	
!
very active area of current research, applications in:	

    robotics	
    operations research	
    computer games	
    theoretical neuroscience	

!

    the primary function of the brain is control.	

ai	

!
!

