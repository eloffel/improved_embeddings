game playing

chapter 5.1     5.3, 5.5

computers playing chess

game playing and ai

game playing as a problem for ai research

    game playing is non-trivial

    players need (cid:1)human-like(cid:2) intelligence
    games can be very complex (e.g., chess, go)
    requires decision making within limited time

    games usually are:

    well-defined and repeatable
    fully observable and limited environments

    can directly compare humans and computers

types of games

definitions:

    zero-sum: one player   s gain is the other player   s 

loss.  does not mean fair.

    discrete: states and decisions have discrete values
    finite: finite number of states and decisions
    deterministic: no coin flips, die rolls     no chance
    perfect information: each player can see the 

complete game state.  no simultaneous decisions.

game playing and ai

game playing as search

deterministic

stochastic (chance)

fully observable

(perfect info)

checkers, chess,

go, othello

backgammon, 

monopoly

partially observable

(imperfect info)

stratego,
battleship

bridge, poker, 

scrabble

all are also multi-agent, adversarial, static tasks

game tree representation

what's the new aspect
to the search problem?

there   s an opponent
we cannot control!

x
x

x

x

   

x

ox

ox

x
o

   

   

x
x

o
o

how can we handle this?

x x

o

x
x

x o
ox

x

x o

    consider two-player, perfect information, 

deterministic, 0-sum board games:
    e.g., chess, checkers, tic-tac-toe
    board configuration:  a specific arrangement of 

"pieces"

    representing board games as search problem:

    states: board configurations
    actions: legal moves
    initial state: starting board configuration
    goal state: game over/terminal board configuration

greedy search

using an evaluation function

    a utility function is used to map each terminal state 
of the board (i.e., states where the game is over) to a 
score indicating the value of that outcome to the 
computer

    we   ll use:

    positive for winning;  large + means better for computer
    negative for losing;  large      means better for opponent
    0 for a draw
    typical values (loss to win):

    -    to +   
    -1.0 to +1.0

greedy search

using an evaluation function

    expand the search tree to the terminal states
    evaluate the utility of each terminal board 
    make the initial move that results in the board 

on each branch
configuration
configuration with the maximum value

a
a
9

computer's
possible moves

b
b
-5

f
-7

g
-5

h
3

c
c
9

i
9

d
d
2

j
-6

k
0

l
2

m
1

e
e
3

n
3

opponent's
possible moves

o
2

terminal states

board evaluation from computer's perspective

greedy search

using an evaluation function

    assuming a reasonable search space, what's the 

problem?

this ignores what the opponent might do!
computer chooses c
opponent chooses j and defeats computer

computer's
possible moves

a
9

b
-5

f
-7

g
-5

h
3

c
9

i
9

d
2

j
-6

k
0

l
2

m
1

e
3

n
3

opponent's
possible moves

o
2

terminal states

board evaluation from computer's perspective

minimax principle

minimax principle

assume both players play optimally

    assuming there are two moves until the 
terminal states,
    high utility values favor the computer

    computer should choose maximizing moves

    low utility values favor the opponent

    smart opponent chooses minimizing moves

    the computer assumes after it moves

the opponent will choose the minimizing move

    the computer chooses the best move 

considering both its move and the opponent(cid:1)s 
optimal move

computer's
possible moves

a
a
1

b
b
-7

f
-7

g
-5

h
3

c
c
-6

i
9

d
d
0

j
-6

k
0

l
2

m
1

e
e
1

n
3

opponent's
possible moves

o
2

terminal states

board evaluation from computer's perspective

propagating minimax values

up the game tree
    explore the tree to the terminal states
    evaluate the utility of the resulting board 

configurations

    the computer makes a move to put the board
in the best configuration for it assuming the 
opponent makes her best moves on her turn(s):
    start at the leaves
    assign value to the parent node as follows

    use minimum when node is the opponent(cid:1)s move
    use maximum when node is the computer's move

general minimax algorithm

for each move by the computer:
1. perform depth-first search, stopping 
2. evaluate each terminal state
3. propagate upwards the minimax values

at terminal states

if opponent's move, propagate up 
if computer's move, propagate up 

minimum value of its children
maximum value of its children

4. choose move at root with the maximum 
of the minimax values of its children

search algorithm independently invented by claude 
shannon (1950) and alan turing (1951)

deeper game trees

    minimax can be generalized to more than 2 moves
    propagate values up the tree

a
a
3

j
j
9

q
-6

b
b
-5

o
o
-5

f
f
4

n
4

c
c
3

i
8

p
9

g
-5

h
3

opponent

min

w
-3

x
-5

computer max

d
0

e
e
-7

l
2

k
k
5

opponent

min
computer

max

m
m
-7

r
0

s
3

t
5

u
-7

v
-9

terminal states

complexity of minimax algorithm

there are b possible moves at each step

assume all terminal states are at depth d and 
    space complexity

    time complexity

depth-first search, so o(bd)
branching factor b, so o(bd)

    time complexity is a major problem since 

computer typically only has a limited amount 
of time to make a move

complexity of game playing

complexity of minimax algorithm

    assume the opponent   s moves can be
predicted given the computer   s moves

    how complex would search be in this case?

    worst case: o(bd) branching factor, depth
    tic-tac-toe: ~5 legal moves, 9 moves max game

    59 = 1,953,125 states

    chess: ~35 legal moves, ~100 moves per game

    bd ~ 35100 ~10154 states, only ~1040 legal states

    go:  ~250 legal moves, ~150 moves per game

    common games produce enormous search trees

static board evaluation

    a static board evaluation (sbe) function is used 

to estimate how good the current board 
configuration is for the computer
    it reflects the computer   s chances of winning from 

    it must be easy to calculate from a board 

that node 

configuration

    for example, for chess:

sbe =    * materialbalance +    * centercontrol +    *    
where material balance = value of white pieces - value of 
black pieces, pawn = 1, rook = 5, queen = 9, etc.

    minimax algorithm applied to complete game 

trees is impractical in practice
    instead do depth-limited search to ply (depth) m
    but utility function defined only for terminal 

states

    we need to know a value for non-terminal states

    static evaluation functions use heuristics to 

estimate the value of non-terminal states

static board evaluation

    typically, one subtracts how good it is for the 

opponent from how good it is for the 
computer

    if the sbe gives x for a player, then it gives  -x 

for the opponent 

    sbe should agree with the utility function

when calculated at terminal nodes

minimax with evaluation functions

tic-tac-toe 
example

    the same as general minimax, except

    only go to depth m
    estimates value at leaves using the sbe function
    how would this algorithm perform at chess?
    if could look ahead ~4 pairs of moves (i.e., 8 ply), 
would be consistently beaten by average players
    if could look ahead ~8 pairs, is as good as human 

master

evaluation function = (# 3-lengths open for me)     (# 3-lengths open for opponent)

minimax  algorithm

minimax  example

function max-value(s)
inputs:

s: current state in game, max about to play
output: best-score (for max) available from s
if ( s is a terminal state or at depth limit )
then return ( sbe value of s )
else

v =         
foreach s    in successors(s)

v = max( v , min-value(s   ))

return v

function min-value(s)
output: best-score (for min) available from s
if ( s is a terminal state or at depth limit )
then return ( sbe value of s)
else

v =  +   
foreach s    in successors(s)

v = min( v , max-value(s   ))

return v

summary so far

    can't use minimax search to end of the game
    if we could, then choosing optimal move is easy

    sbe isn't perfect at estimating/scoring

    if it was, just choose best move without searching
    since neither is feasible for interesting games, 

combine minimax and sbe concepts:
    use minimax to cutoff search at depth m
    use sbe to estimate/score board configuration

max

min

max

min

max

b

o

g
-5

x
-5

n
4

f

w
-3

a

j

q
-6

h
3

c

i
8

p
9

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

alpha-beta idea

    some of the branches of the game tree won't be 

taken if playing against an intelligent opponent

    (cid:2)if you have an idea that is surely bad, don(cid:1)t take 

the time to see how truly awful it is.(cid:3)
-- pat winston

    pruning can be used to ignore some branches
    while doing dfs of game tree, keep track of:

    at maximizing levels:

    highest sbe value, v, seen so far in subtree below each node
    lower bound on node's final minimax value

    at minimizing levels:

    lowest sbe value, v, seen so far in subtree below each node
    upper bound on node's final minimax value

    also keep track of

   = best already explored option along the path to 
the root for max, including the current node

   = best already explored option along the path to 
the root for min, including the current node

alpha cutoff

    at each min node, keep track of the minimum 

value returned so far from its visited children

    store this value as v
    each time v is updated (at a min node), check 

its value against the    value of all its max 
node ancestors

    if v        for some max node ancestor, don   t 

visit any more of the current min node   s 
children;  i.e., prune (cutoff) all subtrees 
rooted at remaining children of min

alpha-beta idea:  alpha cutoff

max

min

a
100

s

b

   = 100

v = 20

v       

c
200

d
100

e
120

f
20

g

    depth-first traversal order
    after returning from a, can get at least 100 at s
    after returning from f, can get at most 20 at b
    at this point no matter what minimax value is computed at 

g, s will prefer a over b.  so, s loses interest in b

    there is no need to visit g.  the subtree at g is pruned.  

saves time.  called    alpha cutoff    (at min node b)

beta cutoff example

max

min

max

b
20

s

a
20

c
25

d
20

e
-10

f
-20

g
25

v       

   = 20

v = 25
x
x
h

c

    after returning from b, can get at most 20 at min node a
    after returning from g, can get at least 25 at max node 
    no matter what minimax value is found at h, a will never 
choose c over b, so don   t visit node h
    called    beta cutoff    (at max node c)

beta cutoff

    at each max node, keep track of the maximum

value returned so far from its visited children

    store this value as v
    each time v is updated (at a max node), check 
its value against the    value of all its min node 
ancestors

    if v        for some min node ancestor, don   t visit
any more of the current max node   s children;  
i.e., prune (cutoff) all subtrees rooted at 
remaining children of max

implementation of alpha cutoff
max

   = -   
   = +   

s

initialize root   s values

min

a

b

c
200

d
100

e
120

f
20

g

    at each node, keep two bounds (based on all nodes on 

path back to root): 
     : the best (largest) max can do at any ancestor
     : the best (smallest) min can do at any ancestor

   v: the best value returned by current node   s visited children
    if at anytime        v  at a min node, the remaining children 

are pruned (i.e., not visited)

implementation of cutoffs
at each node, keep both    and    values, where    = 
largest (i.e.,  best) value of all max node ancestors in 
search tree, and    = smallest (i.e., best) value of all min 
node ancestors in search tree.  pass these down the 
tree during traversal

    at max node, v = largest value from its children

visited so far;   cutoff if  v       
    v value at max comes from its descendants
       value at max comes from its min node ancestors

    at min node, v = smallest value from its children

visited so far;   cutoff if  v       
       value at min comes from its max node ancestors
    v value at min comes from its descendants

alpha cutoff example

max

min

a

   = -   
   = +   
v =  +     
c
200

   = -   
   = +   

s

b

d
100

e
120

f
20

g

alpha cutoff example

max

min

a
200

   = -   
   = 200
v = 200 
c
200

   = -   
   = +   

s

b

d
100

e
120

f
20

g

alpha cutoff example

max

min

  = -   
  =100
v=100

a
100

  = -   
  = +   

s

b

c
200

d
100

e
120

f
20

g

alpha cutoff example

max

  =100
  =+   
v=100

s
100

min

  =-   
  =100

a
100

b

  =100
  =+   

c
200

d
100

e
120

f
20

g

alpha cutoff example

max

min

  =-   
  =100

a
100

  =100
  =+   

s
100

b
120

  =100
  =120
v=120

c
200

d
100

e
120

f
20

g

v >    so 
continue to 
next child of 
b

alpha cutoff example

max

  =100
  =+   

s
100

min

  =-   
  =100

a
100

b
20

c
200

d
100

e
120

f
20

  =100
  =120
v=20
x

g

v = 20        = 100 
so prune

notes:
    alpha cutoff  means not visiting some of a min node   s children
    v values at min come from descendants
    alpha value at min come from max node ancestors

alpha-beta algorithm

alpha-beta cutoffs

    at a max node, if v        then don   t visit (i.e., 
cutoff) remaining children of this max node
    v is the max value found so far from visiting current max 

node   s children

       is the best value found so far at any min node ancestor 

of the current max node

    at a min node, if v        then don   t visit 

remaining children of this min node
    v is the min value found so far from visiting current min 

node   s children

       is the best value found so far at any max node ancestor 

of the current min node

function max-value (s,   ,   )
inputs:

starting from the root:
max-value(root, -   , +   )

s: current state in game, max about to play
  : best score (highest) for max along path from s to root
  : best score (lowest) for min along path from s to root
if ( s is a terminal state or at depth limit )
then return ( sbe value of s )
v = -   
for each s    in successors(s)

v = max( v, min-value(s   ,   ,   ))
if (v        ) then return v
   = max(  , v)
return v

// return value of best child

// prune remaining children

function min-value(s,   ,   )

if ( s is a terminal state or at depth limit )
then return ( sbe value of s)
v = +   
for each s    in successors(s)

v = min( v, max-value(s   ,   ,   ))
if (v        ) then return v
   = min(  , v)

return v

// return value of best child

// prune remaining children

alpha-beta example

alpha-beta example

max

aa
a

  =-   ,   =+   

h
3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

b

o

g
-5

x
-5

n
4

f

w
-3

alpha-beta example

max

a

  =-   ,   =+   

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

min

b

  =-   ,   =+   

max

ff
f

  =-   ,   =+   

g
-5

h
3

n
4

o

w
-3

x
-5

call
stack

a

call
stack

f
b
a

max

a

  =-   ,   =+   

h
3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

b
a

min

b
b
b

  =-   ,   =+   

f

n
4

o

w
-3

g
-5

x
-5

alpha-beta example

max

a

  =-      =+   

h
3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

brown: terminal state

call
stack

n
f
b
a

b

min

  =-   ,   =+   

max

f

g
  =-   ,   =+   
-5

n
n
4
4

o

w
-3

x
-5

alpha-beta example

alpha-beta example

v(f) = alpha(f) = 4, maximum seen so far

min

b

  =-   ,   =+   

max

g
v=4,   =4,   =+   
-5

f
f
  =

n
4

o

w
-3

x
-5

max

a

  =-   ,   =+   

h
3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

brown: terminal state

call
stack

f
b
a

max

a

  =-   ,   =+   

min

b

  =-   ,   =+   

max

f
  =4

g
-5

h
3

min

n
4

oo
o

  =4,   =+   

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

w
-3

x
-5

brown: terminal state

call
stack

o
f
b
a

alpha-beta example

alpha-beta example

max

a
  =-   

min

b
  =+   

max

f
  =4

g
-5

h
3

min

n
4

o

  =4,   =+   

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

w
w
-3
-3

x
-5

blue: terminal state
brown: terminal state (depth limit)

call
stack

w
o
f
b
a

v(o) = -3, minimum seen so far below o
max

a
  =-   

call
stack

min

b
  =+   

max

f
  =4

g
-5

h
3

min

n
4

o
o
  =

  =4, v=-3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

w
-3

x
-5

brown: terminal state

o
f
b
a

alpha-beta example

alpha-beta example

v(o) = -3     alpha(o) = 4: stop expanding o (cutoff)

min

b
  =+   

max

min

n
4

f
  =4

g
-5

o
  =4
v=-3
x
x
-5
-5

w
-3

max

a
  =-   

h
3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

red:  not visited

call
stack

o
f
b
a

why? smart opponent will choose w or worse, thus o's upper 
bound is    3.  so, at f computer shouldn't choose o:-3 since 
n:4 is better.

max

a
  =-   

h
3

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

min

b
  =+   

max

min

n
4

f
  =4

g
-5

o
  =4, 
v=-3
x
-5

w
-3

alpha-beta example

alpha-beta example

v(f) = alpha(f) = 4 not changed (maximizing)

max

a
  =-   

min

b
  =+   

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

f
b
a

v(b) = beta(b) = 4, minimum seen so far

max

a
  =-   

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

min

b
b
  =
  =4

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

call
stack

o
f
b
a

call
stack

b
a

alpha-beta example

alpha-beta example

max

a
  =-   

min

b
  =4

max

f
  =4

g
g
-5
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

v(a) = alpha(a) = -5, maximum seen so far

max

a
a
  =

  =-5,   =+   

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

call
stack

g
b
a

call
stack

a

v(b) = beta(b) = -5, updated to minimum seen so far

max

a
  =-   

c

i
8

p
9

j

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

min

b
b
  =4
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

alpha-beta example

copy alpha and beta values from a to c

max

  =-5,   =+   

a
a
  =

min

b
  =-5

cc
c

  =-5,   =+   

d
0

max

min

f
  =4

g
-5

h
3

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

j

q
-6

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

b
a

call
stack

c
a

alpha-beta example

alpha-beta example

v(c) = beta(c) = 3, minimum seen so far 

v(c) (=3) >   (c) (= -5), so no cutoff

max

  =-5,   =+   

a
a
  =

min

b
  =-5

c

  =-5,   =+   

d
0

max

f
  =4

g
-5

h
h
3
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

j

q
-6

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
i
8
8

p
9

j

q
-6

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

h
c
a

call
stack

i
c
a

max

a
a
  =

  =-5,   =+   

min

b
  =-5

  =-5,   =3, v=3

c
c
  =

d
0

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

j

q
-6

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

beta(c) not changed (minimizing)

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

j

q
-6

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

c
a

call
stack

c
a

alpha-beta example

alpha-beta example

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

jj
j

  =-5,   =3

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

e

l
2

k

m

q
-6

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

v(j) = 9

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

max

f
  =4

g
-5

h
3

i
  =-5,   =3, v=9
8

j
j
  =

k

e

l
2

m

min

n
4

o
v=-3

p
9

q
-6

r
0

s
3

t
5

u
-7

v
-9

w
-3

x
x
-5
-5

call
stack

j
c
a

call
stack

j
c
a

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

max

f
  =4

g
-5

h
3

i
  =-5,   =3, v=9
8

j

k

e

l
2

m

min

n
4

o
v=-3

p
p
9
9

q
-6

r
0

s
3

t
5

u
-7

v
-9

w
-3

x
x
-5
-5

alpha-beta example

v(j) (=9)     beta(j) (=3) so stop expanding j (beta cutoff)

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

max

f
  =4

g
-5

h
3

i
  =-5,   =3, v=9
8

j

k

e

l
2

m

min

n
4

o
v=-3

p
9

q
q
-6
-6

r
r
0
0

s
3

t
5

u
-7

v
-9

w
-3

x
x
-5
-5

red:  not visited

call
stack

p
j
c
a

call
stack

j
c
a

alpha-beta example

why? computer should choose p or better at j, so j's 

lower bound is 9.  but, smart opponent at c won't take 
j:9 since h:3 is better for opponent.

alpha-beta example

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

j

v=9,   =3

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

e

l
2

k

m

q
-6

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

v(a) = alpha(a) = 3, updated to maximum seen so far

max

a
a
a
  =
  =-5

  =3,   =+   

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

c
  =3

i
8

p
9

j
v=9

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

j
c
a

call
stack

a

v(c) and beta(c) not changed (minimizing)

max

a
a
  =

  =-5,   =+   

min

b
  =-5

c

  =-5,   =3

d
0

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

i
8

p
9

j
v=9

q
-6

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

max

a
a
  =

  =3,   =+   

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

c
  =3

i
8

p
9

j
v=9

q
-6

d
d
0
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

c
a

call
stack

d
a

alpha-beta example

alpha-beta example

alpha(a) and v(a) not updated after returning 
from d (because a is a maximizing node)

max

a
a
  =

  =3,   =+   

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

c
  =3

i
8

p
9

j
v=9

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

alpha-beta example

after visiting k, s, t, l and returning to e,
v(e) (=2)     alpha(e) (=3) so stop expanding e 

and don   t visit m (alpha cutoff) 

max

a
a
  =

  =3,   =+   

c
  =3

i
8

p
9

j
v=9

q
-6

d
0

e

  =3,   =5, v=2

k

  =5,   =+   

l
2

m

r
0

s
3

t
5

u
-7

v
-9

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

call
stack

a

call
stack

a

how does the algorithm finish the search tree?

max

a
a
  =

  =3,   =+   

c
  =3

i
8

p
9

j
v=9

q
-6

d
0

e

l
2

k

m

r
0

s
3

t
5

u
-7

v
-9

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

alpha-beta example

why? smart opponent will choose l or worse, thus e's 
upper bound is 2.  so computer at a shouldn't choose 
e:2 since c:3 is a better move.

max

a
a
  =

  =3,   =+   

c
  =3

i
8

p
9

j
v=9

q
-6

d
0

e

  =3,   =5, v=2

k

  =5,   =2

l
2

m

r
0

s
3

t
5

u
-7

v
-9

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

call
stack

a

call
stack

a

alpha-beta example

final result:  computer chooses move c

min

b
  =-5

max

f
  =4

g
-5

h
3

min

n
4

o
v=-3

w
-3

x
x
-5
-5

max

a
a
  =

  =3,   =+   

c
  =3

i
8

p
9

j
v=9

q
-6

d
0

e

  =3,   =5, v=2

k

  =5,   =2

l
2

m

r
0

s
3

t
5

u
-7

v
-9

call
stack

a

effectiveness of alpha-beta search
    effectiveness (i.e., amount of pruning) depends 
on the order in which successors are examined

    worst case:

    ordered so that no pruning takes place
    no improvement over exhaustive search

    best case:

    each player   s best move is visited first

    in practice, performance is closer to best,

rather than worst, case

another step-by-step example (from ai course at 
uc berkeley) given at

https://www.youtube.com/watch?v=xbxhtz4gbdo

effectiveness of alpha-beta search
    in practice often get o(b(d/2)) rather than o(bd)

    same as having a branching factor of    b
since (   b)d =  b(d/2)

    example:  chess

    deep blue went from b ~ 35 to  b ~ 6, visiting   1 

billionth the number of nodes visited by the 
minimax algorithm

    permits much deeper search for the same time
    makes computer chess competitive with humans

dealing with limited time

dealing with limited time

    in real games, there is usually a time limit, t,

on making a move

    how do we deal with this? 

    cannot stop alpha-beta algorithm midway and 

expect to use results with any confidence

    solution #1:  set a (conservative) depth-limit

that guarantees we will finish in time < t
    but the search may finish very early and

the opportunity is lost to do more searching

solution #2: use iterative deepening search (ids)

    run alpha-beta search with depth-first search and 

an increasing depth-limit

    when time runs out, use the solution found for the 
last completed alpha-beta search (i.e., the deepest 
search that was completed)

       anytime algorithm   

the horizon effect

the horizon effect

    sometimes disaster lurks just beyond the 

search depth
    computer captures queen, but a few moves later

the opponent checkmates (i.e., wins)

    the computer has a limited horizon, it cannot 

see that this significant event could happen
    how do you avoid catastrophic losses due to 

(cid:1)short-sightedness(cid:2)?
    quiescence search
    secondary search

    quiescence search

    when sbe value is frequently changing, look deeper 

than the depth-limit

    look for point when game (cid:1)quiets down(cid:2)
    e.g., always expand any forced sequences

    secondary search

1.
2.
3.

find best move looking to depth d
look k steps beyond to verify that it still looks good
if it doesn't, repeat step 2 for next best move

book moves

more on evaluation functions

    build a database of opening moves, end 

   

games, and studied configurations
if the current state is in the database, 
use database:
    to determine the next move
    to evaluate the board

    otherwise, do alpha-beta search

the board evaluation function estimates
how good the current board configuration
is for the computer

    it is a heuristic function of the board's features

    i.e.,  function(f1, f2, f3,    , fn)

    the features are numeric characteristics
    feature 1, f1, is number of white pieces
    feature 2, f2, is number of black pieces
    feature 3, f3, is f1/f2
    feature 4, f4, is estimate of (cid:1)threat(cid:2) to white king
    etc.

linear evaluation functions

    a linear evaluation function of the 
features is a weighted sum of f1, f2, f3, ...
w1 * f1 +  w2 * f2 +  w3 * f3 +       +  wn * fn
    where f1,  f2,    , fn are the features
    and w1, w2 ,    , wn are the weights

    more important features get more weight

linear evaluation functions

    the quality of play depends directly on the 

quality of the evaluation function

    to build an evaluation function we have to:
1. construct good features using expert domain 

knowledge or machine learning

2. pick or learn good weights

examples of algorithms
that learn to play well

checkers

a. l. samuel, (cid:1)some studies in machine learning 
using the game of checkers,(cid:2) ibm journal of 
research and development, 11(6):601-617, 1959
    learned by playing thousands of times against  a 

copy of itself

    used an ibm 704 with 10,000 words of ram, 

magnetic tape, and a clock speed of 1 khz

    successful enough to compete well at human 

tournaments

examples of algorithms
that learn to play well

backgammon

g. tesauro and t. j. sejnowski, (cid:1)a parallel 
network that learns to play backgammon,(cid:2)
artificial intelligence, 39(3), 357-390, 1989

    also learned by playing against itself
    used a non-linear evaluation function - a neural 

network 

    rated one of the top three players in the world

non-deterministic games
0

8 9 10 11 12

1 2 3 4 5

7

6

25

24 23 22

21

20

19

18 17 16 15 14 13

non-deterministic games
    some games involve chance, for example:

    roll of dice
    spin of game wheel
    deal cards from a shuffled deck

    how can we handle games with random 

elements?
    modify the game search tree to include (cid:1)chance 

nodes:(cid:2)

1. computer moves
2. chance nodes (representing random events)
3. opponent moves

non-deterministic games

non-deterministic games

extended game tree representation:

a

max

50/50

50/50

chance

.5

c
6

b
2

7

2

9

.5

6

.5

5

.5

d
0

e
-4

min

0

8

-4

    weight score by the id203 that move occurs
    use expected value for move:  instead of using 

max or min, compute the average, weighted by the 
probabilities of each child
a

max

50/50
50/50

4

.5

c
6

b
2

7

2

9

.5

6

50/50
50/50
-2
.5

d
0

.5

chance

e
-4

min

5

0

8

-4

non-deterministic games

expectiminimax

choose move with the highest expected value

expectiminimax(n) =

a
a
  =
4

max

50/50

4

.5

c
6

b
2

7

2

9

.5

6

50/50
-2
.5

d
0

.5

chance

e
-4

min

5

0

8

-4

sbe(n)
max
min

  s succ(n) 

  s succ(n) 
s succ n p s

(

)

  s

expectiminimax( )s
expectiminimax( )s

s
( )*expectiminimax( )

for n, a terminal state or 
state at cutoff depth
for n, a max node

for n, a min node

for n, a chance node

non-deterministic games

    non-determinism increases branching factor
    21 possible distinct rolls with 2 dice (since 6-5 is 

same as 5-6)

    value of look-ahead diminishes:  as depth 
increases, id203 of reaching a given 
node decreases

    alpha-beta pruning is less effective

computers play grandmaster chess
(cid:1)deep blue(cid:2) (1997, ibm)

    parallel processor, 32 (cid:1)nodes(cid:2)
    each node had 8 dedicated vlsi (cid:1)chess chips(cid:2)
    searched 200 million configurations/second
    used minimax, alpha-beta, sophisticated heuristics
    average branching factor ~6 instead of ~40
    in 2001 searched to 14 ply (i.e., 7 pairs of moves)
    avoided horizon effect by searching as deep as 40 ply
    used book moves

computers can play grandmaster chess

   game over: kasparov and the machine    (2003)

kasparov vs. deep blue, may 1997
    6 game full-regulation chess match sponsored by 

acm

    kasparov lost the match 2 wins to 3 wins and 1 tie
    historic achievement for computer chess; the first 
time a computer became the best chess player on 
the planet

    deep blue played by (cid:1)brute force(cid:2) (i.e., raw power 

from computer speed and memory); it used 
relatively little that is similar to human intuition and 
cleverness

game playing:  go

google   s alphago beat korean grandmaster 
lee sedol 4 games to 1 in 2016

status of computers playing other games

    checkers

    first computer world champion:  chinook
    beat all humans (beat marion tinsley in 1994)
    used alpha-beta search and book moves

    othello

    computers easily beat world experts

    go

    branching factor b ~ 360, very large!

alphago documentary movie (2017)

game playing summary

    game playing is modeled as a search problem, 

doing a limited look-ahead (depth bound)

    search trees for games represent both computer 

and opponent moves

    a single evaluation functions estimates the 

quality of a given board configuration for both 
players (zero-sum assumption)
    good for opponent
0  neutral
+  good for computer

summary

    minimax algorithm determines the 

(cid:1)optimal(cid:2) moves by assuming that both 
players always chooses their best move

    alpha-beta algorithm can avoid large parts of 

the search tree, thus enabling the search to 
go deeper

    for many well-known games, computer

algorithms using heuristic search can match 
or out-perform human experts

how to improve performance?

    reduce depth of search

    better sbes

    use machine learning to learn good features 
rather than use    manually-defined    features

    reduce breadth of search

    explore a subset of the possible moves instead of 

exploring all
    use randomized exploration of the search 
space

id169 (mcts)

pure id169

    concentrate search on most promising moves
    best-first search based on random sampling of 

search space

    monte carlo methods are a broad class of 
algorithms that rely on repeated random 
sampling to obtain numerical results.  they can 
be used to solve problems having a 
probabilistic interpretation.

    for each possible legal move of current player, 
simulate k random games by selecting moves 
at random for both players until game over 
(called playouts); count how many were wins 
out of each k playouts; move with most wins is 
selected

    stochastic simulation of game
    game must have finite number of possible 

moves, and game length is finite

exploitation vs. exploration

    rather than selecting a child at random, how 
to select best child node during tree descent? 
    exploitation:  keep track of average win rate for 
each child from previous searches; prefer child 
that has previously lead to more wins

    exploration:  allow for exploration of relatively 

unvisited children (moves) too

    combine these factors to compute a    score    
for each child; pick child with highest score at 
each successive node in search 

mcts algorithm

recursively build search tree, where each round 
consists of:
1. starting at root, successively select best child 
nodes using scoring method until leaf node l
reached

2. create and add best (or random) new child 

node, c, of l

3. perform a random playout from c
4. update score at c and all of c   s ancestors in 

search tree based on playout results

scoring child nodes

    choose child node, i, with highest score
    one way of defining the score at a node:

    upper confidence bound for trees (uct):

exploitation 

term

wi
ni

+ c lnt
ni

exploration 

term

where wi = number of wins after ith move,
ni = number of playout simulations after ith move,
t = total number of playout simulations

id169 (mcts)

l

note: only exploitation used here 
to pick best child  

key:  number games won / number playouts

c

state-of-the art go programs

    google   s alphago
    facebook   s darkforest
    mcts implemented using multiple threads and 

gpus, and up to 110k playouts

    also used a deep neural network to compute 

sbe

update
scores

playout

key:  number games won / number playouts

