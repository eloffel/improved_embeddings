id110s 

 

(aka bayes nets, belief nets,  
directed id114) 

chapter 14.1, 14.2, and 14.4 
plus optional paper    bayesian 
networks without tears    

[based on slides by jerry zhu and andrew moore]  

              

full joint id203 distribution 

making a joint distribution of n variables: 

1.    list all combinations of values (if each variable 

has k values, there are kn combinations) 

2.    assign each combination a id203 
3.    they should sum to 1 

introduction 

       probabilistic models allow us to use 
probabilistic id136 (e.g., bayes   s rule) to 
compute the id203 distribution over a set 
of unobserved (   hypothesis   ) given a set of 
observed variables 
       full joint id203 distribution table is great 
for id136 in an uncertain world, but is 
terrible to obtain and store 
       id110s allow us to represent joint 
distributions in manageable chunks using 
      independence, conditional independence 
       id110 can do any id136 

using the full joint distribution 

       once you have the joint distribution, you can do anything, 

e.g. marginalization: 

p(e) =    rows matching e p(row) 

       e.g., p(sunny or hot) = (150+50+40+5)/365 

convince yourself this is the same as p(sunny) + p(hot) - p(sunny and hot) 

weather 
sunny 
sunny 
cloudy 
cloudy 
rainy 
rainy 

temperature 

hot 
cold 
hot 
cold 
hot 
cold 

prob. 
150/365 
50/365 
40/365 
60/365 
5/365 
60/365 

weather 
sunny 
sunny 
cloudy 
cloudy 
rainy 
rainy 

temperature 

hot 
cold 
hot 
cold 
hot 
cold 

prob. 
150/365 
50/365 
40/365 
60/365 
5/365 
60/365 

              

1 

using the joint distribution 

       you can also do id136: 
              rows matching q and e p(row) 

p(q | e) =   

 

          rows matching e p(row) 
temperature 

weather 
sunny 
sunny 
cloudy 
cloudy 
rainy 
rainy 

hot 
cold 
hot 
cold 
hot 
cold 

p(hot | rainy) 
prob. 
150/365 
50/365 
40/365 
60/365 
5/365 
60/365 

              

the bad news 

       full joint distribution requires a lot of storage space 
       for n variables, each taking k values, the joint 
distribution has kn numbers (and kn     1 degrees of 
freedom) 
       it would be nice to use fewer numbers     
       id110s to the rescue! 
representation of the fjpd 

      provides a decomposed / factorized  

      encodes a collection of conditional 

independence relations 

id110s 

example 

       idea:  represent statistical dependencies graphically 
       directed, acylic graphs (dags) 
       nodes = random variables 

         cpt    stored at each node quantifies conditional 

id203 of node   s r.v. given all its parents 
       directed arc from a to b means a    has a direct 
influence on    or    causes    b 
      evidence for a increases likelihood of b 

(deductive influence from causes to effects) 

      evidence for b increases likelihood of a 

(abductive influence from effects to causes) 
       encodes conditional independence assumptions 

      a: your alarm sounds 
      j: your neighbor john calls you 
      m: your other neighbor mary calls you 
      john and mary do not communicate (they promised 

to call you whenever they hear the alarm) 

       what kind of independence do we have? 
       what does the bayes net look like? 

2 

conditional  independence 
       random variables can be dependent, but 
conditionally independent 
       example:  your house has an alarm 

      neighbor john will call when he hears the alarm 
      neighbor mary will call when she hears the alarm 
      assume john and mary don   t talk to each other 

       is johncall independent of marycall?   

      no     if john called, it is likely the alarm went off, 

which increases the id203 of mary calling 

      p(marycall | johncall)      p(marycall) 

example 

      a: your alarm sounds 
      j: your neighbor john calls you 
      m: your other neighbor mary calls you 
      john and mary do not communicate (they promised 

to call you whenever they hear the alarm) 
       conditional independence: p(j,m|a)=p(j|a)p(m|a) 

       what kind of independence do we have? 
       what does the bayes net look like? 

conditional  independence 

alarm) 

       but, if we know the status of the alarm, johncall 
will not affect whether or not mary calls 
   p(marycall | alarm, johncall) = p(marycall | 
       we say johncall and marycall are 
conditionally independent given alarm 
       in general,    a and b are conditionally 
independent given c    means:  
p(a | b, c) = p(a | c) 
p(b | a, c) = p(b | c) 
p(a, b | c) = p(a | c) p(b | c) 

examples 

our bn: p(a,j,m) = p(a) p(j|a) p(m|a) 

      a: your alarm sounds 
chain rule: p(a,j,m) = p(a) p(j|a) p(m|a,j) 
      j: your neighbor john calls you 
our bn assumes conditional independence, 
      m: your other neighbor mary calls you 
      john and mary do not communicate (they promised 

so   p(m|a,j) = p(m|a) 

 

to call you whenever they hear the alarm) 
       conditional independence p(j,m|a)=p(j|a)p(m|a) 

       what kind of independence do we have? 
       what does the bayes net look like? 

a 

j 

m 

a 

j 

m 

              

              

3 

a simple id110 

a simple id110 

s

   

{
no

,

light

,

heavy

}

smoking 

cancer 

s

   

{
no

,

light

,

heavy

}

smoking 

cancer 

p( s=no) 
0.80 
p( s=light)  0.15 
p( s=heavy) 0.05 

c

   

{
none

,

benign

,

malignant

}

p( s=no) 
0.80 
p( s=light)  0.15 
p( s=heavy) 0.05 

c

   

{
none

,

benign

,

malignant

}

no 

light  heavy 

p(c=none|s=)  0.96  0.88 
p(c=benign|s=)  0.03  0.08 
p(c=malig|s=)  0.01  0.04 

0.60 
0.25 
0.15 

don   t need 
to store 

no 

light  heavy 

p(c=none|s=)  0.96  0.88 
p(c=benign|s=)  0.03  0.08 
p(c=malig|s=)  0.01  0.04 

0.60 
0.25 
0.15 

a id110 

a id110 

diagnostic 
variables 

flu 

allergy 

sinus 

evidence 
variables 

runny 
nose 

headache 

age 

gender 

exposure 
to toxics 

smoking 

cancer 

serum 
calcium 

lung 
tumor 

4 

applications 

       medical diagnosis systems 
       manufacturing system diagnosis 
       computer systems diagnosis 
       network systems diagnosis 
       helpdesk troubleshooting 
       information retrieval 
       customer modeling 

ricoh fixit  
       diagnostics and information retrieval 

fixit: ricoh copy machine 

online troubleshooters 

5 

pathfinder 

      pathfinder was one of the first bn systems 
      it performed diagnosis of lymph-node 
diseases 
      it dealt with over 60 diseases and 100 
symptoms and test results 
      14,000 probabilities  
      commercialized and applied to about 20 
tissue types 

pathfinder 
bayes  
net 

448 nodes, 
906 arcs 

              

conditional independence in bayes nets 
      a node is conditionally independent of its       

non-descendants, given its parents 

      a node is conditionally independent of all other 

nodes, given its    markov blanket    (i.e., its parents, 
children, and children   s parents) 

              

              

6 

conditional independence 

more conditional independence 

cancer is conditionally 
independent of age 
and gender given 
smoking 

age 

gender 

smoking 

cancer 

interpreting bayesian nets 
       2 nodes are (unconditionally) independent if 
there   s no undirected path between them 
       if there   s an undirected path between 2 nodes, then 
whether or not they are independent or dependent 
depends on what other evidence is known 

a 

b 

c 

a and b are independent 
given nothing else, but are 
dependent given c 

cancer 

serum 
calcium 

lung 
tumor 

serum calcium is 
conditionally independent of 
lung tumor, given cancer 

p(l | sc, c) = p(l | c) 

example with 5 variables 

      b: there   s burglary in your house 
      e: there   s an earthquake 
      a: your alarm sounds 
      j: your neighbor john calls you 
      m: your other neighbor mary calls you 
 

       b, e are independent 
       j is directly influenced by only a (i.e., j is 
conditionally independent of b, e, m, given a) 
       m is directly influenced by only a (i.e., m is 
conditionally independent of b, e, j, given a) 

7 

creating a bayes net 

       step 1: add variables.  choose the variables you 
want to include in the bayes net 

b 

j 

a 

e 

m 

b: there   s burglary in your house 
e: there   s an earthquake 
a: your alarm sounds 
j: your neighbor john calls you 
m: your other neighbor mary calls you 

creating a bayes net 

       step 3: add cpt   s   
       each table must list p(x | parent values) for all 
combinations of parent values 

e.g., you must specify 
p(j|a) and p(j|  a) 
since they don   t have 

to sum to 1! 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 

creating a bayes net 

       step 2: add directed edges   
       the graph must be acyclic   
       if node x is given parents q1,    , qm, you are 
promising that any variable that   s not a 
descendant of x is conditionally independent of x 
given q1,    , qm 

b 

j 

a 

e 

m 

b: there   s burglary in your house 
e: there   s an earthquake 
a: your alarm sounds 
j: your neighbor john calls you 
m: your other neighbor mary calls you 

              

creating a bayes net  

1.   choose a set of relevant variables 
2.   choose an ordering of them, call them x1,    , xn 
3.   for i = 1 to n: 

1.    add node xi to the graph 
2.    set parents(xi) to be the minimal subset of {x1   
xi-1},  such that xi is conditionally independent of 
all other members of {x1   xi-1} given parents(xi) 

3.    define the cpt   s for  

b: there   s burglary in your house 
e: there   s an earthquake 
a: your alarm sounds 
j: your neighbor john calls you 
m: your other neighbor mary calls you 

              

p(xi | assignments of parents(xi)) 

       different ordering leads to different graph, in general 
       best ordering when each variable is considered after all 

variables that directly influence it 

8 

the id110 created from a 

different variable ordering 

the id110 created from a 

different variable ordering 

          

          

compactness of bayes nets 
       a id110 is a graph structure for 
representing conditional independence relations in 
a compact way 
       a bayes net encodes the full joint distribution, often with 
far less parameters (i.e., numbers) 
       a full joint table needs kn parameters (n variables, k 
values per variable) 
      grows exponentially with n 
       if the bayes net is sparse, e.g., each node has at most 
m parents (m << n), only needs o(nkm) parameters 
      grows linearly with n 
      can   t have too many parents, though 

variable dependencies 

       directed arc from one variable to another variable 

a 

b 

       is a guaranteed to be independent of b? 

      no     information can be transmitted over 1 arc 
       example:  my knowing the alarm went off, 
increases my belief there has been a burglary, 
and similarly, my knowing there has been a 
burglary increases my belief the alarm went off 

9 

causal chain 

       this local configuration is called a    causal chain:    

causal chain 

       this local configuration is called a    causal chain:    

a 
a 

b 
b 

c 
c 

       is a guaranteed to be independent of c?   

      no     information can be transmitted between a and 

c through b if b is not observed 
       example:  not knowing alarm means that my 
knowing that a burglary has occurred increases 
my belief that mary calls, and similarly, knowing 
that mary calls increases my belief that there has 
been a burglary 

a 
a 

c 
c 
       is a independent of c given b?   

b 
b 

      yes     once b is observed, information cannot be 

transmitted between a and c through b;  b 
   blocks    the information path;    c is conditionally 
independent of a given b    
       example:  knowing that the alarm went off 
means that also knowing that a burglary has 
taken place will not increase my belief that 
mary calls   

common cause 

       this configuration is called    common cause:    

a 

common effect 

       this configuration is called    common effect:    

a 

b 

b 

c 

       is it guaranteed that b and c are independent? 

children of a if a is not observed 

      no     information can be transmitted through a to the 
       is it guaranteed that b and c are independent given a? 
      yes     observing the cause, a, blocks the influence 

between effects b and c;     b is conditionally 
independent of c given a    

 
       are a and b independent? 

c 

      yes 

       example:  burglary and earthquake cause the 
alarm to go off, but they are not correlated 

      proof:  p(a,b) =   c p(a,b,c)    by marginalization 

 
 
 
 

 
 
 
 

 
 
 
 

 =   c p(a) p(b|a) p(c|a,b)    by chain rule    
 =   c p(a) p(b) p(c|a,b)   by cond. indep. 
 = p(a) p(b)   c p(c|a,b)    
 = p(a) p(b)    since last term = 1 

10 

common effect 

       this configuration is called    common effect:    

a 

b 

c 

 
       are a and b independent given c? 

      no     information can be transmitted through c 

among the parents of c if c is observed 
       example:  if i already know that the alarm went 
off, my further knowing that there has been an 
earthquake, decreases my belief that there has 
been a burglary.  called    explaining away.      
      similarly, if c has descendant d and d is given, 

then a and b are not independent 

computing a joint entry from a bayes net 
how to compute an entry in the joint distribution? 
e.g., what is p(s,   m, l,   r, t)? 
 
p(s) = 0.3 

p(m) = 0.6 

m 

s 

p(l|m,s) = 0.05 
p(l|m,  s) = 0.1 
p(l|  m,s) = 0.1 
p(l|  m,   s) = 0.2 

l 

t 

p(t|l) = 0.3 
p(t|  l) = 0.8 

r 

p(r|m) = 0.3 
p(r|  m) = 0.6 

d-separation 

determining if two variables in a bayesian 
network are independent or conditionally 
independent given a set of observed evidence 
variables, is determined using    d-separation    
 
d-separation is covered in cs 760 

computing with bayes net 

p(s) = 0.3 

s 

p(l|m,s) = 0.05 
p(l|m,  s) = 0.1 
p(l|  m,s) = 0.1 
p(l|  m,   s) = 0.2 

l 

t 

m 

p(m) = 0.6 

p(r|m) = 0.3 
p(r|  m) = 0.6 

p(t|l) = 0.3 
p(t|  l) = 0.8 

r 

apply the chain rule + 

conditional independence! 

p(t,   r, l,   m, s) 
= p(t |   r, l,   m, s) * p(  r, l,   m, s)  
= p(t |  l) *  p(  r, l,   m, s) 
= p(t |  l) *  p(  r | l,   m, s) * p(l,   m, s) 
= p(t |  l) *  p(  r |   m) * p(l,   m, s) 
= p(t |  l) *  p(  r |   m) * p(l |   m, s) * p(  m, s) 
= p(t |  l) *  p(  r |   m) * p(l |   m, s) * p(  m | s) * p(s) 
p(t |  l) *  p(  r |   m) * p(l |   m, s) * p(  m) * p(s) 

11 

variable ordering 

the general case 

before applying chain rule, best to reorder all of 
the variables, listing first the leaf nodes, then all 
the parents of the leaves, etc.  last variables 
listed are those that have no parents, i.e., the 
root nodes.   
 
so, for previous example, 
p(s,l,m,t,r) = p(t,r,l,s,m) 

computing joint probabilities 
a

using a id110 

b

how is any joint id203 computed? 
 sum the relevant joint probabilities: 
 compute: p(a,b)
= p(a,b,c,d) + p(a,b,c,  d) + p(a,b,  c,d) + p(a,b,  c,  d)
 compute: p(c)
= p(a,b,c,d) + p(a,  b,c,d) + p(  a,b,c,d) + p(  a,  b,c,d) +!
   p(a,b,c,  d) + p(a,  b,c,  d) + p(  a,b,c,  d) + p(  a,  b,c,  d)

c
d

       a bn can answer any query (i.e., id203) about the 

domain by marginalization (   summing out   ) over the 
relevant joint probabilities 

p(x1=x1 , x2=x2 ,   ., xn-1=xn-1 , xn=xn) 
= p(xn=xn , xn-1=xn-1 ,    ., x2=x2 , x1=x1) 
= p(xn=xn | xn-1=xn-1 ,    ., x2=x2 , x1=x1) * p(xn-1=xn-1 ,    , x2=x2 , x1=x1) 
= p(xn=xn | xn-1=xn-1 ,    ., x2=x2 , x1=x1) * p(xn-1=xn-1 |    x2=x2 , x1=x1) * 

    p(xn-2=xn-2 ,   ., x2=x2 , x1=x1)  

        : 
 
= 
 

n

   
i=1

(
p xi = xi

(

)    xi   1 = xi   1

(

(

),     ,   x1 = x1

(

)

)

)

n

   
i=1

=
 

(
p xi = xi

(

)   assignments of parents xi
(

)

)

where are we now? 

       we defined a bayes net, using small number 
of parameters, to describe the joint id203 
       any joint id203 can be computed as 
p(x1,   , xn) =    i p(xi | parents(xi)) 
       the above joint id203 can be computed 
in time linear in the number of nodes, n 
       with this joint distribution, we can compute 
any id155, p(q | e); thus we 
can perform any id136 
       how? 

12 

id136 by enumeration 

              joint matching q and e p(joint) 
p(q | e) =   
                 joint matching e p(joint) 

for example:  p(b | j,   m) 
 
1.    compute p(b,j,   m) 

2.    compute p(j,   m) 
3.    return p(b,j,   m)/p(j,   m) 

p(b) = 0.001 

b 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

by def. of cond. prob. 

p(e) = 0.002 

e 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 

id136 by enumeration 
p(b, j,   m, a, e) 
p(b, j,   m, a,   e) 
p(b, j,   m,   a, e) 
p(b, j,   m,   a,   e) 

              joint matching q and e p(joint) 
p(q | e) =   
                 joint matching e p(joint) 

compute the joint (4 of them) 

for example:  p(b | j,  m) 
 
1.    compute p(b,j,  m) 

2.    compute p(j,  m) 
3.    return p(b,j,  m)/p(j,  m) 

each is o(n) for sparse graph 
p(e)=0.002 
e 

p(x1,   xn) =    i p(xi | parents(xi)) 
p(b)=0.001 

b 

sum them up 
p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(j|a) = 0.9 
p(j|  a) = 0.05 

j 

m 

p(m|a) = 0.7 
p(m|  a) = 0.01 

              

compute the joint (8 of them) 

              joint matching q and e p(joint) 
p(q | e) =   
                 joint matching e p(joint) 

id136 by enumeration 
p(j,  m,b,a,e) 
p(j,  m,b,a,  e) 
p(j,  m,b,  a,e) 
p(j,  m,b,  a,  e) 
p(j,  m,  b,a,e) 
p(j,  m,  b,a,  e) 
p(j,  m,  b,  a,e) 
p(j,  m,  b,  a,  e) 
a 

for example:  p(b | j,  m) 
 
1.    compute p(b,j,  m) 

p(b)=0.001 

b 

p(a | b, e)=0.95  
p(a | b, ~e)=0.94 
each is o(n) for sparse graph 
p(a | ~b, e)=0.29 
p(a | ~b, ~e)=0.001 
sum them up 

 p(x1,   xn) =    i p(xi | parents(xi)) 

j 

m 

2.    compute p(j,  m) 
3.    return p(b,j,  m)/p(j,~m) 

p(e)=0.002 
e 

id136 by enumeration 

              joint matching q and e p(joint) 
p(q | e) =   
                 joint matching e p(joint) 

sum up 4 joints 

for example:  p(b | j,  m) 
 
1.    compute p(b,j,  m) 

2.    compute p(j,  m) 
3.    return p(b,j,  m)/p(j,  m) 

sum up 8 joints 

p(b)=0.001 

b 

p(e) = 0.002 
e 
in general, if there are 
n boolean variables, j 
query vars, and k 
a 
evidence vars, how 
many joints to sum up? 
m 

j 

p(a | b, e)=0.95  
p(a | b, ~e)=0.94 
p(a |   b, e)=0.29 
p(a |   b, ~e)=0.001 

p(j|a)=0.9 
p(j|~a)=0.05 

p(m|a)=0.7 
p(m|~a)=0.01 

              

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 

              

13 

another example 

another example 

compute p(r | t,   s) from the following bayes net 

p(s) = 0.3 

s 

m 

p(m) = 0.6 

p(l | m, s) = 0.05 
p(l | m,   s) = 0.1 
p(l |   m, s) = 0.1 
p(l |   m,   s) = 0.2 

l 

t 

p(t | l) = 0.3 
p(t |   l) = 0.8 

r 

p(r | m) = 0.3 
p(r |   m) = 0.6 

step 1: compute p(r, t,   s) 
 
step 2: compute p(t,   s) 
 
step 3: return 

 

p(r, t,   s) 
------------------ 

p(t,   s) 

p(s)=0.3 

s 

p(l | m, s) = 0.05 
p(l | m,   s) = 0.1 
p(l |   m, s) = 0.1 
p(l |   m,   s) = 0.2 

l 

t 

m 

p(m) = 0.6 

p(r | m) = 0.3 
p(r |   m) = 0.6  

p(t | l)=0.3 
p(t |   l)=0.8 

r 

                                         compute p(r | t,   s) 

another example 

another example 

4 joint computes 

step 1: compute p(r, t,   s) 
 
step 2: compute p(t,   s) 
 
step 3: return 

 

p(r, t,   s) 
------------------ 

p(t,   s) 

sum of all the rows in the 
joint that match r     t       s 

sum of all the rows in the 
joint that match t       s 

p(s)=0.3 

s 

p(l | m, s) = 0.05 
p(l | m,   s) = 0.1 
p(l |   m, s) = 0.1 
p(l |   m,   s) = 0.2 

l 

t 

m 

p(m) = 0.6 

p(r | m) = 0.3 
p(r |   m) = 0.6 

p(t | l)=0.3 
p(t |   l)=0.8 

r 

                                        compute p(r | t,   s) 

step 1: compute p(r, t,   s) 
 
step 2: compute p(t,   s) 
 
step 3: return 

 

p(r, t,   s) 

------------------------------------- 
p(r, t,   s) + p(  r, t,   s) 

p(s)=0.3 

s 

= p(t,   s) 

m 

sum of all the rows in the 
joint that match r     t       s 

sum of all the rows in the 
joint that match t       s 

8 joint computes 

each of these obtained by 
the    computing a joint 
p(m)=0.6 
id203 entry    method in 
the earlier slides 
p(r m)=0.3 
p(r |   m) = 0.6 

p(l | m, s) = 0.05 
p(l | m,   s) = 0.1 
p(l|    m, s) = 0.1 
p(l |   m,   s) = 0.2 

l 

t 

p(t l)=0.3 
p(t | ~l)=0.8 

r 

                                        compute p(r | t,   s) 

14 

       id136 through a bayes net can go both 
   forward    and    backward    through arcs 
       causal (top-down) id136 

      given a cause, infer its effects 
      e.g., p(t | s) 

       diagnostic (bottom-up) id136 

      given effects/symptoms, infer a cause 
      e.g., p(s | t) 

the bad news 

       in general if there are n variables, while evidence contains 
j variables, and each variable has k values, how many 
joints to sum up?  k(n-j) 
       it is this summation that makes id136 by 
enumeration inefficient 
       computing conditional probabilities by enumerating all 

matching entries in the joint is expensive:  
exponential in the number of variables 

       some computation can be saved by carefully ordering the 
terms and re-using intermediate results (variable 
elimination algorithm) 
       a more complex algorithm called a join tree (junction tree) 
can save even more computation 
       but, even so, exact id136 with an arbitrary bayes 
net is np-complete 
              

the good news 

we can do id136.  that is, we can compute 

any id155: 
p( some variable | some other variable values ) 

eep
(

|

1

)

=

2

)

2

ep
(
   
1
ep
(

2

e
)

=

joint 

   
p
joint 
(
e
e
 
matching
and
 
 
entries
 1
   
p
joint 
(
e
matching
 
entries
 

 2

2

joint 

entry
)

entry
)

   id136 by enumeration    algorithm 

variable elimination algorithm 

   

   

   

general idea: 
       write query in the form 
   
p(xn,e) = !
    
       iteratively 
      move all irrelevant terms outside of innermost sum 
      perform innermost sum, getting a new term 
      insert the new term into the product 

p(xi | pai)

x2

x3

xk

i

15 

compute p(d) 

need to eliminate: v, s, x, t, l, a, b 

initial factors:  

v 

s 

t 

l 

a 

b 

x 

d 

p(v, s, t, l, a, b, x, d) =  
  p(v)p(s)p(t | v)p(l | s)p(b | s)p(a |t,l)p(x | a)p(d | a,b)
id136 by enumeration (i.e., brute force) approach: 
 
p(d) =
p(v,s,t,l,a,b,x,d)

   

   

   

   

   

   

b

   

a

x

l

t

s

v

parameter (cpt) learning for bn 
       where do you get these cpt numbers? 

      ask domain experts, or 
      learn from data 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 

       we want to compute p(d) 
       need to eliminate: v, s, x, t, l, a, b 

initial factors 

v 

s 

t 

l 

a 

b 

x 

d 

  p(v)p(s)p(t | v)p(l | s)p(b | s)p(a |t,l)p(x | a)p(d | a,b)
eliminate: v 
compute: 
      fv(t)p(s)p(l | s)p(b | s)p(a |t,l)p(x | a)p(d | a,b)

p(v)p(t | v)

fv(t) =

   

  

v

note: fv(t) = p(t) 

idea behind variable 
elimination algorithm  

parameter (cpt) learning for bn 
      learn from a data set like this: 

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

how to learn this cpt? 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

16 

parameter (cpt) learning for bn 
      learn from a data set like this: 

parameter (cpt) learning for bn 
      learn from a data set like this: 

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

count #(b) and #(  b) in dataset. 
p(b) = #(b) / [#(b) + #(  b)] 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

count #(e) and #(  e) in dataset. 
p(e) = #(e) / [#(e) + #(  e)] 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

parameter (cpt) learning for bn 
      learn from a data set like this: 

parameter (cpt) learning for bn 
      learn from a data set like this: 

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

count #(a) and #(  a) in dataset  
where b=true and e=true. 
p(a|b,e) = #(a) / [#(a) + #(  a)] 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

count #(a) and #(  a) in dataset  
where b=true and e=false. 
p(a|b,  e) = #(a) / [#(a) + #(  a)] 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

17 

parameter (cpt) learning for bn 
      learn from a data set like this: 

parameter (cpt) learning for bn 
      learn from a data set like this: 

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

count #(a) and #(  a) in dataset  
where b=false and e=true. 
p(a|  b,e) = #(a) / [#(a) + #(  a)] 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

parameter (cpt) learning for bn 
         unseen event    problem 

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
(  b,   e,   a,   j,   m) 
    

count #(a) and #(  a) in dataset  
where b=true and e=true. 
p(a|b,e) = #(a) / [#(a) + #(  a)] 
 
what if there   s no row with  
(b, e,   a, *, *) in the dataset? 
 
do you want to set 
p(a|b,e) = 1 
p(  a|b,e) = 0? 
 
why or why not? 

p 

p 

(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b,   e,   a,   j,   m) 
(b,   e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a, j,   m) 
(  b, e, a, j, m) 
(  b,   e,   a,   j,   m) 
(  b,   e,   a,   j, m) 
(  b,   e,   a,   j,   m) 
(~b, ~e,   a,   j,   m) 
(b, e, a,   j, m) 
p 
(  b,   e,   a,   j,   m) 
    

p 

count #(a) and #(  a) in dataset  
where b=false and e=false. 
p(a|  b,   e) = #(a) / [#(a) + #(  a)] 

p(b) = 0.001 

b 

e 

p(a | b, e) = 0.95  
p(a | b,   e) = 0.94 
p(a |   b, e) = 0.29 
p(a |   b,   e) = 0.001 

a 

p(e) = 0.002 

j 

m 

p(j|a) = 0.9 
p(j|  a) = 0.05 

p(m|a) = 0.7 
p(m|  a) = 0.01 
              

parameter (cpt) learning for bn 

      p(x=x | parents(x)) = (frequency of x given parents) 

is called the maximum likelihood (ml) estimate 

      ml estimate is vulnerable to the    unseen event    

problem when the dataset is small 
      flip a coin 3 times, all heads    one-sided coin? 

      simplest solution:     add one    smoothing   

              

              

18 

smoothing  cpts 
   add one    smoothing:  add 1 to all counts 

     
      in the previous example, count #(a) and #(  a) in 

dataset where b=true and e=true 
      p(a|b,e) = [#(a)+1] / [#(a)+1 + #(  a)+1] 
      if #(a)=1, #(  a)=0:  

      without smoothing p(a|b,e) = 1, p(  a|b,e) = 0 
      with smoothing p(a|b,e) = 0.67, p(  a|b,e) = 0.33 

      if #(a)=100, #(  a)=0:  

      without smoothing p(a|b,e) = 1, p(  a|b,e) = 0 
      with smoothing p(a|b,e) = 0.99, p(  a|b,e) = 0.01 

      smoothing saves you when you don   t have enough 

data, and hides away when you do 

      it   s a form of maximum a posteriori (map) estimation 
              

bn special case:  na  ve bayes 

       a special bayes net structure:  

      a    class    variable y at root, compute p(y | x1,    , xn) 
      evidence nodes xi (observed features) are all leaves 
      conditional independence between all evidence 
assumed.  usually not valid, but often empirically 
ok 

y 
    

x1 

x2 

xn 

na  ve bayes classifier 

      

class variable 

evidence variable 

a special bn:  na  ve bayes classifiers 

j 

z 

h 

c 

       what   s stored in the cpts? 

person is junior 

j 
c  brought coat to class 
z 
h  saw    hunger games 1    more 

lives in zipcode 53706 

than once 

19 

a special bn:  na  ve bayes classifiers 

a special bn:  na  ve bayes classifiers 

p(j) = 

c 

j 

z 

h 

p(c|j) = 
p(c|  j) = 

p(z|j) = 
p(z|  j) = 

p(h|j) = 
p(h|  j) = 

 

person is junior 

j 
c  brought coat to class 
z 
h  saw    hunger games 1    more 

lives in zipcode 53706 

than once 

is the person a junior? 

       input (evidence): c, z, h 
       output (query): j 

p(j|c,z,h)   
= p(j,c,z,h) / p(c,z,h) 
= p(j,c,z,h) / [p(j,c,z,h) + p(  j,c,z,h)]

 

 by def. of cond. prob. 

 by marginalization 

 where 

 
p(j,c,z,h) = p(j)p(c|j)p(z|j)p(h|j)  

conditional independence associated with bayes net 

 by chain rule and 

 
p(  j,c,z,h) = p(  j)p(c|  j)p(z|  j)p(h|  j) 
 
 

p(j) = 

c 

j 

z 

h 

p(c|j) = 
p(c|  j) = 

p(z|j) = 
p(z|  j) = 

p(h|j) = 
p(h|  j) = 

person is junior 

j 
c  brought coat to class 
z 
h  saw    hunger games 1    more 

lives in zipcode 53706 

than once 

       a new person shows up in class wearing an    i live in 
union south where i saw the    hunger games 1    
every night    coat. 
       what   s the id203 that the person is a junior? 

              

smoothing cpts for na  ve bayes 

          add 1 smoothing    ensures that each id155 > 0 
       assume c possible classes (i.e., class variable has c possible 
values) and a    bag of words    model for describing each 
example (   document   ):  if there are k distinct token types in the 
vocabulary, v1 ,    , vk, each example is represented by a vector 
of length k where the ith entry is the number of times word i 
occurs in the example 
       let nci = number of times token type vi occurs in all training 

examples in class c, including multiple occurrences in the same 
training example 

    (                  |    )=                         +1/                 +        
       note:          =1              (                    |c)=1 

       let nc = total number of tokens in all examples in class c 
       compute conditional probabilities as: 
                                   
p(vi | c) =1

nci +1
nc + k

p(vi | c) =

k
   
                          
i=1

20 

application:  id110s 

for breast cancer diagnosis 

breast  
cancer 

abnormal 
mammo 

elizabeth s. burnside 

department of radiology 

university of wisconsin hospitals 

results:  roc curves 

roc curves

 
 

e

t

a
r
 

e
v
i
t
i
s
f
o
p
t
p

 

e
u
r
t

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

radiologist 
auc = .916  

 

bayes net 
auc = .919 

 

bn 
radiologist 
combined 

0.6

0.8

1

false positive rate  

fpf

mass stability 

mass margins 

mass density 
mass shape 
mass size 

ca++ lucent 
centered 

milk of  
calcium ca++ dermal 
ca++ round 
ca++ dystrophic 

breast 
density 

mass p/a/o 

breast 
disease 

skin lesion 
tubular 
density 
architectural 
distortion 

age 

fhx 

ln  asymmetric 

density 

hrt 

ca++ popcorn 
ca++ fine/ 

linear 

ca++ eggshell 
ca++ pleomorphic 
ca++ punctate 

ca++ amorphous 

ca++ rod-like 
              

id110 properties 
       id110s compactly encode joint 
distributions 
 
       topology of a id110 is only 
guaranteed to encode conditional 
independencies 
      arcs do not necessarily represent causal 
relations 

21 

what you should know 

       id136 with joint distribution 
       problems of joint distribution 
       id110s: representation (nodes, arcs, 
cpt) and meaning 
       compute joint probabilities from bayes net 
       id136 by enumeration 
       na  ve bayes 

22 

