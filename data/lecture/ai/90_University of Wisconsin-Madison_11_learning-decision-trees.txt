supervised learning methods

    k-nearest-neighbors (id92)
    id90 (chapter 18.3)
    neural networks (ann)
    support vector machines (id166)

inductive concept learning by

learning id90

    a decision tree is a tree in which:

    each non-leaf node has associated with it an 

attribute (aka feature)

    each leaf node has associated with it a discrete
classification value (aka class label, e.g., + or    )

    each arc has associated with it one of the possible 

values of the attribute of its parent node (i.e., 
node from where the arc is directed)

inductive concept learning by

learning id90

    goal:

build a decision tree for classifying examples
as one of a discrete set of possible values
    a form of supervised learning
    uses batch processing of training examples
    uses a preference bias

    learning can be viewed as searching the hypothesis 
space h of possible h functions, y = h(x) 
    preference bias:  define a metric for comparing h   s so 
as to determine whether one is better than another

inductive concept learning by

learning id90

suit

clubs

hearts

spades

diamonds

rank

   

size

   

10

jack

   

small

large

   

size

small

large

+

   

   

+

leaf node = classification

interior node = feature

arc = value

   

9

+

1

using a decision tree

a decision tree is used as a classifier by taking a given 
input example (aka instance), which is given by its 
feature vector, and:

1. the attribute at the root node of the tree is 
interpreted as a question, and the answer is 
determined by the value of that attribute in the 
input example

2. answer determines to which child to move
3. repeat until a leaf node is reached; class label at 

leaf is the classification predicted for the input 
example

inductive concept learning by

learning id90

    what is the best decision tree?
    preference bias: ockham's razor

    the simplest hypothesis that is consistent 

with all observations is most likely

    the smallest decision tree that correctly 

classifies all of the training examples is best

    finding the provably smallest decision tree is an 
np-hard problem, so instead construct one that
is    pretty small   

ockham's razor

decision tree construction
using a greedy algorithm

    aka decision-tree-learning or   or c5.0
    top-down (greedy) construction of the 

decision tree:
1. select the "best attribute" to use for the current node

in the tree

(1287-1347)

   everything should be made 
as simple as possible, but not 
simpler.           albert einstein

2. for each possible value of the selected attribute:

a) partition the examples using the possible values

of this attribute, and assign these disjoint subsets of
the examples to the appropriate child node

b) recursively generate each child node until (ideally)

all examples for a node have same label (class)

2

building a decision tree

building a decision tree

    select an attribute 
and split the data 
into its children in a 
tree

    select an attribute 
and split the data 
into its children in a 
tree

    continue splitting 

with available 
attributes

slide by intel software

slide by intel software

building a decision tree

how long to keep splitting?

    select an attribute 
and split the data 
into its children in a 
tree

    continue splitting 

with available 
attributes

until:

    leaf node(s) are pure 

(only one class remains)

    a maximum depth is 

reached

    a performance metric 

is achieved

slide by intel software

slide by intel software

3

decision-tree-learning algorithm

buildtree(examples, attributes, default-label)
if empty(examples) then return default-label
if (examples all have same label y) then return y
if empty(attributes) then return majority-class of examples
q = best_attribute(examples, attributes) 
tree = create-node with attribute q
foreach value v of attribute q do

v-ex = subset of examples with q == v
subtree = buildtree(v-ex, attributes - {q}, majority-class(examples))
add arc from tree to subtree
return tree

building the best decision tree

what defines the best split?

slide by intel software

decision tree algorithm

information gain

    which is the    best attribute    ?

    random:  an attribute chosen at random
    least-values:  the attribute with the smallest

number of possible values

    most-values:  the attribute with the largest

number of possible values

    max-gain:  the attribute that has the largest 

expected information gain

    goal: select the attribute that will result in 
the smallest expected tree size

    how?  

use id205

4

id205

id205

    how many yes/no questions would you 

expect to ask to determine which number 
i'm thinking of in the range 1 to 100?
7

    why?

with each yes/no question at most 1/2 of 
the elements remaining can be 
eliminated

log2100 = 6.64

    given a set s of size |s|, the expected work 

required to determine a specific element is

log2 |s|

    call this value the information value of being
told the element rather than having to work
for it (by asking questions)

id205

id205

given s = p u n, where p and n are two disjoint 
sets, how hard is it to determine which element i 
am thinking of in s?

if x is in p,
then log2p questions needed, where p = |p|
if x is in n,
then log2n questions needed, where n = |n|

so, the expected number of questions that 
have to be asked is:

(pr(x in p) * log2p) + (pr(x in n) * log2n)

or, equivalently,

(p/(p+n)) log2p + (n/(p+n)) log2n

5

id178

    in general, say there are n = n1 +     + nk examples 

    n1 examples have label y1
    n2 examples have label y2
       
    nk examples have label yk

    what   s the impurity/inhomogeneity/disorder of the 

examples?

    turn it into a game:  if i put these examples in a bag, 
and pick one at random, what is the id203 the 
example has label yi? 

machine learning in the news

google   s    perspective api    uses machine learning 
to give online publishers a tool that can 
automatically score comments in their forums, 
returning a score from 0 to 100 based on how 
similar it is to things that other people have said 
are    toxic   

   perspective isn   t just a bad word filter,     [but] 
has the ability to understand context     

id178

    id203 estimated from the given samples: 
   id203 p1 = n1/n the example has label y1
   id203 p2 = n2/n the example has label y2
      
   id203 pk = nk/n the example has label yk
    p1 + p2 +     + pk = 1
    the    outcome    of the draw is a random variable y
with probabilities (p1, p2,    , pk)
    what   s the impurity/disorder of the examples?    

what   s the uncertainty of y in a random drawing?

id178 

-

y
pr(

=

y

i

log)

2

y
pr(

=

y

i

)

-

p
i

log

2

p
i

definition

yh
)(

=

=

k

  

i

1
=
k

  

i

1
=

interpretation:  the number of yes/no questions (in 
bits) needed on average to determine the value of y in 
a random drawing

6

id178:  h(y)

    h measures the information content (in bits) 

associated with a set of examples 

    h(y)      0

where 0 is no information, and 1 is maximum 
information (for a 2-class y)

    bit

    information needed to answer a yes/no question
    a real-valued scalar, not binary bits

information extremes

information extremes

    2 classes:  + and    
    perfect balance (maximum inhomogeneity):

given p+ = p    =   
h(y) =  h(  ,   ) =  -    log2           log2   

=  -   (log21     log22)        (log21     log22)
=  -   (0     1)         (0     1)
=     +   
=  1 (   the id178 is large)

a histogram of the 
frequency distribution 
of values of y is 
nearly flat
       high id178    means y is from a nearly uniform

distribution

    2 classes:  + and    
    perfect homogeneity:
given p+ = 1 and p    = 0
h(y) = h(1, 0) =  -1 log2 1       0 log2 0

=  -1 (0)       ???
=  0     0
=  0 (   no information content)

a histogram of the 
frequency 
distribution of values 
of y has many low 
values and a few 
high values

       low id178    means y is from a varied (peaks and 

valleys) distribution

7

h

id178

p+

id178

when there are k classes, id178 is defined as

h(y) =

   pi log2 pi

k

   
i=1

    pi is the proportion of y that belong to class i
    log is still base 2 because id178 is a 
measure of expected encoding length 
measured in bits

    maximum value of h is log2k

    for example, when k = 3,  0     h     1.58

id178

biased
coin

pr(head) = 0.5
pr(tail) = 0.5
h = 1

pr(head) = 0.51
pr(tail) = 0.49
h = 0.9997

example

    3 classes:  color = r, g, b
    10 examples:  3 r, 2 g, 5 b

    h(color) = h(3/10, 2/10, 5/10) 

= (-3/10) log2 (3/10) + (-2/10) log2 (2/10) + 

(-5/10) log2 (5/10)

= (-.3)(-1.74) + (-.2)(-2.32) + (-.5)(-1)
= 1.486

8

id178

conditional id178

id178 will be used as a heuristic for 
estimating the (relative) tree size rooted at 
a node given a set of examples associated 
with that node

    small id178 predicts a small tree size
    large id178 predicts a large tree size

conditional id178:  h(y | x)

    weighted sum of the entropies of each subset 
of the examples partitioned by the possible 
values of attribute x
    expected value of id178 after splitting on x
    measures the total    impurity,       disorder    or 
   inhomogeneity    of all the children nodes
    0     h(y | x)     1

vxyh
=

(

|

)

=

k

  

i

1
=

-

y
pr(

=

vxy
=
i

|

log)

2

y
pr(

=

vxy
=
i

|

)

xyh

(

|

)

=

  

pr(
x
 of 

v
values
:

vxyhvx
=

=

(

)

|

)

    y:  a label (or attribute)
    x:  an attribute  (i.e., feature or question)
    v:  a value of the attribute x
    pr(y|x=v):  id155
    textbook calls h(y|x) the remainder(x)

called 
   specific 
conditional 
id178   

specific conditional id178:  h(y|x=v)

let   s try to predict if 
someone likes a given 
movie when we know 
their major

my training data

x= college major
y= likes    gladiator   

y
x
math
yes
history no
cs
yes
no
math
no
math
cs
yes
history no
yes
math

9

specific conditional id178:  h(y|x=v)

conditional id178:  h(y|x)

x= college major
y= likes    gladiator   

y
x
math
yes
history no
cs
yes
no
math
no
math
cs
yes
history no
yes
math

definition of specific conditional id178:
h(y | x=v) = id178 of y for only those 
records in which x has value v
    h(y | x=history) = 0
    h(y | x=cs) = 0
    h(y | x=math) =
h(y | x=math) = -p(y=yes | x=math) log 
p(y=yes | x=math)     p(y=no | x=math) 
log p(y=no | x=math)
= -(2/4 * log (2/4))     ( 2/4 * log (2/4))
= (-.5 * -1) + (-.5 * -1) 
= 1

conditional id178

x= college major
y= likes    gladiator   

definition of conditional id178:

h(y | x) = total conditional id178 of y
=   j pr(x=vj) h(y | x = vj)

y
x
math
yes
history no
cs
yes
no
math
no
math
cs
yes
history no
yes
math

vj
math
history
cs

pr(x =vj) h(y| x = vj)
0.5
0.25
0.25

1
0
0

h(y |x) = 0.5 * 1 + 0.25 * 0 + 0.25 * 0 

= 0.5

x= college major
y= likes    gladiator   

y
x
math
yes
history no
cs
yes
no
math
no
math
cs
yes
history no
yes
math

definition of conditional id178:
h(y | x) = total specific conditional 
id178 of y
= if you choose a record at random what will 
be the conditional id178 of y, conditioned on 
that row   s value of x
= expected number of bits to transmit y if 
both sides know the value of x
=   j pr(x=vj) h(y | x = vj)

specific conditional id178:  h(y|x=v)

y= college major
x= likes    gladiator   

y
x
math
yes
history no
cs
yes
no
math
no
math
cs
yes
history no
yes
math

note:  we could have 
alternatively tried to 
predict someone   s 
major given whether or 
not they like the movie 
gladiator

here the class variable, y, is their 
major and the feature, x, is whether 
or not they like the movie    gladiator   

10

information gain

)

    information gain, or mutual information
)
    measures the difference in id178 of a node and 

xyhyh

xyi
;(

)(

-

=

the id178 remaining after the node   s examples are 
   split    between the children using a chosen attribute

    i(y; x) means i must transmit y. how many bits on 

average would it save me if both ends know x?

(

|

    choose the attribute (i.e., feature or question) x that         

maximizes i(y; x)

    textbook calls i(y; x) the gain(x)

using information gain to
select the best attribute

goal: construct a small decision tree that
correctly classifies the training examples

    why would high information gain be 

desirable?
    means more of the examples are the same 

class in each child node

    the id90 rooted at each child that 

are needed to differentiate between the 
classes are likely to be small

using information gain

decision boundaries

t
i
l
p
s
 
e
r
o
f
e
b

1

 
t
i
l
p
s

2
 
t
i
l

p
s

11

48

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

the training set

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

example
    attributes:  color, shape, size
    what   s the best attribute for the root?

+           -

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

h(class) = h(3/6, 3/6) = 1
h(class | color) = 3/6 h(2/3,1/3) + 2/6 h(0/2, 2/2) + 1/6 h(1/1, 0/1)

3 out of 6 
are red

2 of the red 

are +

2 out of 6 
are green

both green 

are      

blue is +

1 out of 6 
is blue

12

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

h(class) = h(3/6, 3/6) = 1
h(class | color) = 3/6 h(2/3,1/3) + 1/6 h(1/1, 0/1) + 2/6 h(0/2, 2/2)
i(class; color) = h(class)     h(class | color) = 0.54 bits

h(class) = h(3/6, 3/6) = 1
h(class | shape) = 4/6 * h(2/4, 2/4) + 2/6 * h(1/2,1/2) 
i(class; shape) = h(class)     h(class | shape) = 0 bits

shape tells us nothing 

about class!

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

h(class) = h(3/6, 3/6) = 1
h(class | size) = 4/6 * h(3/4, 1/4) + 2/6 * h(0/2, 2/2) 
i(class; size) = h(class)     h(class | size) = 0.46 bits

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

i(class; color) = h(class)     h(class | color) = 0.54 bits
i(class; shape) = h(class)     h(class | shape) = 0 bits
i(class; size) = h(class)     h(class | size) = 0.46 bits

   select color as the best attribute at the root

13

what   s the next step?

color

green

red

blue

(examples: 1, 3, 4)

(examples: 5 and 6)

(examples: 2)

    blue child is a leaf since it has only one 

example, which is +
    make its class +

    green child is a leaf since both its examples 

are    
    make its class    

    red child is not a leaf since its examples are 

2 + and 1 -

what   s the next step?

color

red

green

blue

?

which attribute for red child?
    compute i(class; size), i(class; shape), and 

i(class, color)

    h(class) = ?

= h(2/3, 1/3) = (-2/3)log 2/3 + (-1/3)log 1/3 = 0.92

14

which attribute for red child?

    h(class|size) = 2/3 h(2/2, 0/2) + 1/3 h(0/1, 1/1)

= (2/3)(0) + (1/3)(0) = 0
so, i(class; size) = 0.92     0 = 0.92

    h(class|shape) = 1/3 h(1/1, 0/1) + 2/3 h(   ,   )

= (1/3)(0) + (2/3)(1) = 0.67
so, i(class; shape) = 0.92     0.67 = 0.25

    h(class|color) = 3/3 h(2/3, 1/3) + 0/3 h(0/0, 0/0)

= h(2/3, 1/3) = 0.92
so, i(class; color) = 0

must be 0!

    so, size is the best attribute

selecting the best attribute

the best attribute for a node is the attribute 
(of those candidates available for that node, 
which are all attributes except those on path 
back to the root) with:

class 
variable

attribute

maximum information gain, i(y; x)

or, equivalently,

minimum conditional id178, h(y | x)

since at a given node h(y) is fixed

final decision tree

color

red

green

blue

size

small

big

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

the training set

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

now let color be the    class    variable    3 class problem

h(color) = ?

h(color | class) = ?

15

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

the training set

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

now let color be the    class    variable    3 class problem

h(color) = h(3/6, 2/6, 1/6) 

= (-3/6)log2(3/6) + (-2/6)log2(2/6) + (-1/6)log2(1/6)
= 1.96

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

the training set

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

i(color; class) = h(color)     h(color | class) 

= 1.96     0.918
= 1.04 bits

example color

1
2
3 
4
5
6

red
blue
red
red
green
green

the training set

shape

square
square
circle
circle
square
square

size 

big
big
big
small
small
big

class 
+
+
+
   
   
   

h(color | class) = 3/6 h(2/3, 0/3, 1/3) + 3/6 h(1/3, 2/3, 0/3)
h(2/3, 0, 1/3) = (-.667)(-.584) + 0 + (-.333)(-1.586)

= .918
h(1/3, 2/3, 0)  = .918 

so, h(color | class) = 0.918  

extensions to decision tree learning:

real-valued attributes

    what if some of the attributes, x1, x2,    , xk, 

are real-valued?

    example:  x1 = height (in inches)
    idea 1:  branch on each possible numerical 

value
    over-fragments the training data and is likely 

to    overfit   

16

extensions to decision tree learning:

real-valued attributes

    what if some of the attributes, x1, x2,    , xk, 

are real-valued?

    example:  x1 = height (in inches)
    idea 2: use questions of the form:  x1 > t1 ?

where t1 is a threshold
    how many thresholds?

    1 for each pair of consecutive examples that are 

classified differently

    how to set threshold values?

    midpoint between 2 consecutive examples    values

example

5 training examples, 2 attributes (x1 and x2) 
and 2 classes (t and f)

69

example

sort examples by values of selected attribute, x2

17

find pairs of consecutive examples that have 
different class labels, and define a candidate 
threshold as the average of these two examples    
values for x2 

    examples with x2     2.7

    examples with x2 > 2.7

compute info gain for x2:2.7

i(class) = h(3/5, 2/5)

= -3/5 log 3/5     2/5 log 2/5
= 0.9709

h(class | x2) = 2/5 h(2/2, 0/2) + 3/5 h(1/3, 2/3)

= (.4)(0) + (.6)[-1/3 log 1/3     2/3 log 2/3]
= 0.5509

i(class; x2) = 0.9709     0.5509 = 0.4199

    i(class; x2:2.7) = 0.4199 

    similarly, compute

i(class; x2:3.75)  
i(class; x2:4.25) 

    pick the threshold that has maximum 

information gain as the best threshold for x2

18

decision-tree-learning algorithm

decision tree applet

buildtree(examples, attributes, default-label)
/* examples: a list of training examples

attributes: a set of candidate questions, e.g.,    what   s the value of attribute xi?   
default-label: default label prediction, e.g., over-all majority vote */

if empty(examples) then return default-label
if (examples have same label y) then return y
if empty(attributes) then return majority vote in examples
q = best_attribute(examples, attributes) 
tree = create-node with attribute q
foreach value v of attribute q do

v-ex = subset of examples with q == v
subtree = buildtree(v-ex, attributes - {q}, majority-class(examples))
add arc from tree to subtree

return tree

you are in a basketball pool, currently betting on the 
outcome of next week's basketball game between 
the mallrats and the chinooks. you have to decide 
which team will win, then bet on that team.  

http://www.cs.ualberta.ca/~aixplore/learning/d
ecisiontrees/index.html

applications

    credit-card companies and banks use dts to 

decide whether to grant a card or loan

    diagnose breast cancer

    humans correct 65% of the time
    decision tree classified 72% correct

    bp designed a decision tree for gas-oil separation 

for offshore oil platforms

    cessna designed a flight controller using 90,000 

examples and 20 attributes per example

expressiveness of id90
    assume all attributes are boolean and all classes 

are boolean (i.e., 2 classes)

    which boolean functions can be represented by 

id90?

    answer:  all boolean functions!
proof:
1. take any boolean function
2. convert it into a truth table
3. construct a decision tree in which each row of 

the truth table corresponds to one path through 
the decision tree from root to a leaf

19

evaluating performance

training set error

how might performance be evaluated?
    speed of learner
    speed of classifier
    space requirements
    predictive accuracy of classifier

    for each example in the training set, use the 

decision tree to see what class it predicts
what % of the examples does the decision tree   s 
prediction disagree with the known true value?
    this quantity is called the training set error

    the smaller the better

    but why are we doing learning anyway?

    more important to assess how well the decision 

tree predicts output for future data

test set error

evaluating classifiers

    we hide some data away, call the test set, 

when we learn the decision tree

    after the tree is learned, check how well the 
tree predicts that held-back data:  % classified 
incorrectly

    this is a good simulation of what happens 

when we try to predict future data

    called the test set error

    during training

    during testing

    train a classifier from a training set (x1,y1), (x2,y2),    , (xn,yn)

    for new test data, xn+1,    , xn+m, your classifier generates 

predicted labels y   n+1,    , y   n+m

    test set accuracy:

    you need to know the true test labels:  yn+1,    , yn+m

    test set error:

err =

1
m

n+m

   
i=n+1

(yi        yi
)

    test set accuracy:   1     err

0-1 loss

20

evaluating performance accuracy

use separate test examples to estimate 
accuracy! 

1. randomly partition training examples

train set (say ~80% of all training examples) 
test  set (say ~20% of all training examples) 

2. generate decision tree using the train set
3. use test set to evaluate accuracy
accuracy = #correct / #total

the overfitting problem:

a regression problem example

y

x

y = f(x) + noise
can we learn f from this data?

which is best?

the overfitting problem

example:  predicting u.s. population

y

x

y

x

linear f

quadratic f

piecewise linear f

fits the data 

best, 

including 
any noise!

x=year      y=million
1900         75.99
1910         91.97
1920       105.71
1930       123.2
1940       131.67
1950       150.7
1960       179.32
1970       203.21
1980       226.51
1990       249.63
2000       281.42

    we have some 

training data 
(n=11)

    what will the 
population be 
in 2010?

21

regression: polynomial fit

overfitting

    the degree d (complexity of the model) is 

important
=

xf
)(

d

xc
d

+

c

d

1
-

d

1
-

x

+

!

+

xc
1

+

c
0

    fit (= learn) coefficients cd,     c0 to minimize 
mean squared error (mse) on training data

mse

=

1
n

n

  

i

1
=

(

y

i

-

xf
(
i

)
2)

   squared loss   
common for 
regression

    as d increases, the mse on the training data 
improves, but prediction on test data worsens

2010 pop:  308.75

degree=0 mse=4181.451643
degree=1 mse=79.600506
degree=2 mse=9.346899
degree=3 mse=9.289570
degree=4 mse=7.420147
degree=5 mse=5.310130
degree=6 mse=2.493168
degree=7 mse=2.278311
degree=8 mse=1.257978
degree=9 mse=0.001433
degree=10 mse=0.000000

overfitting a decision tree

overfitting a decision tree:  example

in general, overfitting means finding 
   meaningless    regularity in training data

especially a problem with    noisy    data 

    class associated with an example is wrong

    may mean two examples have all the same attribute 

values but different classes

    attribute values are incorrect because of 
errors getting or preprocessing the data

    irrelevant attributes

five attributes, all binary, 
are generated in all 32 
possible combinations

class y= copy of e,
except a random 25% 
of the records have y
set to the oppositeof e

s
d
r
o
c
e
r
 
2
3

a
0
0
0
0
0
:
1

b
0
0
0
0
0
:
1

c
0
0
0
0
1
:
1

d
0
0
1
1
0
:
1

e
0
1
0
1
0
:
1

y
0
0
0
1
1
:
1

training set

22

overfitting a decision tree

    the test set is constructed similarly

    y = e, but 25% the time we corrupt it by y = 1 - e
    assume the corruption in training and test sets are 

independent

    the training and test sets are the same, except
    some y   s are corrupted in training, but not in test
    some y   s are corrupted in test, but not in training

overfiting a decision tree

next, classify the test data with the tree

e 

a 

a 

overfitting a decision tree
suppose we build a full tree on the training set

e=0

e 

e=1

a 

a 

training set accuracy = 100% (all leaf nodes contain exactly 1 example)

25% of these training leaf node labels will be corrupted (i.e.,    e)

overfitting a decision tree

on average:
       training data uncorrupted

       of these are uncorrupted in test     correct labels
       of these are corrupted in test     wrong

       training data corrupted

25% of the test examples are corrupted     independent of training data

       of these are uncorrupted in test     wrong
       of these are also corrupted in test     correct labels
    test accuracy = (   *   ) + (   *   ) = 5/8 = 62.5%

23

overfitting a decision tree

but if we knew a,b,c,d are irrelevant attributes and 
don   t use them in the tree    
pretend they don   t exist

a
0
0
0
0
0
:
1

b
0
0
0
0
0
:
1

c
0
0
0
0
1
:
1

d
0
0
1
1
0
:
1

e
0
1
0
1
0
:
1

y
0
0
0
1
1
:
1

overfitting a decision tree

hence, the full tree overfit by learning meaningless 
(noise) attributes

e

a

a

overfitting a decision tree

the tree would be:

e 

in training data, about    
y   s are 0 here. majority 
vote predicts y = 0

in training data, about    
y   s are 1 here. majority 
vote predicts y = 1

in test data,    y   s are different from e because they 
were corrupted, and    y   s will be correct, so
test set accuracy = 75%, which is better than when 
using more (meaningless) attributes (= 62.5%)

extensions to

decision tree learning algorithm

    overfitting

    meaningless regularity is found in the data
    irrelevant attributes confound the true, 
important, distinguishing features
    fix by pruning some nodes in the decision 
tree

24

avoiding overfitting:  pruning

pruning with a tuning set

1. randomly split the training data into 

train and tune, say 70% and 30%

2. build a full tree using only the train set
3. prune the tree using the tune set

on training set it looks great

but not for the test set

the tree is pruned back to the red line where it gives 
more accurate results on the test set

105

pruning using a greedy algorithm
prune(tree t, tune set)
1. compute t   s accuracy on tune; call it acc(t)
2. for every internal node n in t:

a) new tree tn = copy of t, but prune (delete) the subtree

under n 

b) n becomes a leaf node in tn.  the class is the majority vote 

of train examples reaching n
c) acc(tn) = tn   s accuracy on tune

3. let t* be the tree (among the tn   s and t) with the largest acc()  

set t = t* /* prune */
if no improvement then return t else goto step 1

4.

extensions to decision tree learning:

missing data (i.e., attributes without values)

some possible approaches:
    during learning:  replace with most likely 
value
    during learning:  use unknown as a value
    during classification:  follow arcs for all 
values and weight each by the frequency of 
the examples along that arc

25

rule extraction from trees

extensions to decision tree learning:

interpreting trees as rules

generate a set of rules from a dt:

for each path from the root to a leaf
    the rule's antecedent is all the attribute values
    the consequent is the classification at leaf 
if (size=small && suit=hearts) then class = +

    constructing these rules yields an 

interpretation of the tree's meaning

extensions to decision tree learning

setting parameters

    most learning algorithms require setting various 

parameters

    they must be set without looking at the test data
    common approach: use a tuning set 

(aka validation set)

extensions to decision tree learning

use a tuning set for setting parameters:
1. partition given examples into train, tune and test sets
2. for each candidate parameter value, generate a decision 

tree using the train set

3. use the tune set to evaluate error rates and determine 

which parameter value is best

4. compute the final decision tree using the selected 

parameter values and both train and tune sets

5. use test to compute performance accuracy

26

experimental evaluation of performance

test set method

test set method

1. randomly choose say 30% of the data to 
be the test set, and the remaining 70% is 
the training set

2. build a (decision tree) classifier using the 

training set

3. estimate future performance using the 

test set

    wastes data because the test set is not used 

to construct the best classifier

    if we don   t have much data, our test set 

might be lucky or unlucky in terms of what   s 
in it, making the results on the test set a 
   high variance    estimator of the real 
performance

experimental evaluation of performance

cross-validation

often use k = 
3, 5 or 10

1. divide all examples into k disjoint subsets

e = e1, e2, ..., ek

2. for each i = 1, ..., k

let test set = ei and train set = e - ei

   
    build decision tree using train set 
    determine accuracy acci using test set 

3. compute k-fold cross-validation estimate of 

performance = mean accuracy = 
(acc1 + acc2 + ... + acck)/k

experimental evaluation of performance
leave-one-out cross validation

//  n = number of examples

for i = 1 to n do
1. let (xi, yi) be the ith example
2. remove (xi, yi) from the dataset
3. train on the remaining n-1 examples
4. compute accuracy on ith example

    accuracy = mean accuracy on all n runs
    doesn   t waste data but is expensive
    use when you have a small dataset (< ~100)

27

id90 summary

id90 summary

    one of the most widely used learning 

methods in practice

    can out-perform human experts in many 

applications

strengths
    fast
    simple to implement
    well founded in id205
    can convert result to a set of easily 

interpretable rules

    empirically valid in many commercial products
    handles noisy data
    scales well

id90 summary

weaknesses

    univariate splits/partitions using only one 
attribute at a time, which limits types of 
decision boundaries

    each split is a hyperplane orthogonal to 1 axis 

(i.e., attribute) 

    large id90 may be hard to understand
    requires fixed-length feature vectors
    non-incremental (i.e., it   s a batch method)

combining classifiers:
ensemble methods

    aggregate the predictions of multiple
classifiers with the goal of improving 
accuracy by reducing the variance of an 
estimated prediction function

       mixture of experts   
    combining multiple classifiers often produces 
higher accuracy than any individual classifier

28

example:  netflix prize competition
october 2006
    supervised learning task

    training data is a set of users and ratings (1,2,3,4,5 

stars) those users have given to movies

    construct a classifier that given a user and an unrated 
movie, correctly classifies that movie as either 1, 2, 3, 
4, or 5 stars

    $1 million prize for a 10% improvement over 

netflix   s then-current movie 
recommender/classifier   (mse = 0.9514)

just 3 weeks after it 
began, at least 40 
teams could beat the 
netflix classifier

top teams showed 

about 5% 
improvement

slide by t. holloway

slide by t. holloway

however, improvement slowed   

ensemble methods 
to the rescue   
rookies
   thanks to paul harrison's 
collaboration, a simple mix 
of our solutions improved 
our result from 6.31 to 
6.75   

slide by t. holloway

from http://www.research.att.com/~volinsky/netflix/

slide by t. holloway

29

arek paterek
   my approach is to 
combine the results of 
many methods (also two-
way interactions between 
them) using linear 
regression   

http://rainbow.mimuw.edu.pl/~ap/ap_kdd.pdf

slide by t. holloway

bellkor / korbell

the winning team was from 
at&t:

   our final solution 
(rmse=0.8712) consists of 
blending 107 individual 
results.    
an 8.5% improvement 
over netflix

slide by t. holloway

why combine classifiers?

how to combine classifiers?

    statistical:  when training data are small relative to 
size of hypothesis/classifier space, there are many 
possible classifiers that fit the data;    averaging    their 
results reduces risk of picking a poor classifier

    computational:  small training set + local search 

means hard to find    true    classifier;  ensemble 
simulates multiple random restarts to obtain multiple 
classifiers

    representational:  true classifier may not be 
representable in the hypothesis space of a single 
method, but some (weighted) combination of 
hypotheses expands the space of representable 
functions

given a test example, classify it using each 
classifier and report as the output class the 
majority (for a 2-class problem) or mode
classification (for k-class problems)

30

intuition

majority vote classifier
suppose we have 5 completely independent

classifiers
    if accuracy is 70% for each, combined accuracy 

is:  10(.73)(.32) + 5(.74)(.3) + (.75) 
    83.7% majority vote accuracy

    101 such classifiers

    99.9% majority vote accuracy

   
      

5
3

   
      

slide by t. holloway

ensemble strategies

boosting

    sequential production of classifiers, where each 

classifier is dependent on the previous one

    make examples misclassified by current classifier 

more important in the next classifier

id112 

    create classifiers using different training sets, 

where each training set is created by 
   id64,    i.e., drawing examples (with 
replacement) from all possible training examples

slide by t. holloway

when is an ensemble better?

necessary and sufficient condition for an 
ensemble to be more accurate than individual 
classifiers, is when each individual classifier is:

    accurate:  error rate is better than random 

guessing

    diverse:  classifiers make errors on different
examples, i.e., they are at least somewhat 
uncorrelated

id112

    given n training examples, generate separate 

training sets by choosing n examples with 
replacement from all n examples

    called    taking a bootstrap sample    or 

   randomizing the training set   

    construct a classifier using the n examples in 

current training set

    repeat for multiple classifiers

31

id112 example (opitz, 1999)
n = 8, n = 8

original

1 2

3 4

training set 1

2 7

8 3

training set 2

7 8

5 6

training set 3

3 6

2 7

training set 4

4 5

1 4

5

7

4

5

6

6 7 8

6 3 1

2 7 1

6 2 2

4 3 8

id79s

aka  decision forest, classification forest

2 main ideas:

1. id112:  use random samples of the 
training examples to construct each 
classifier

2. randomized node optimization:  each 

time a node is split, only a randomly 
chosen subset of the attributes is used

id112 with id90

    for each training set, build a separate 

decision tree

    take majority/mode vote from all the 
id90 to determine the output 
classification of a given testing example

id79s

for each tree,

1. choose a training set by choosing n times with 

replacement from all n available training 
examples

2. at each node of decision tree during construction, 
choose a random subset of m attributes from the 
total number, m, of possible attributes (m << m)

3. select best attribute at node using max-gain

    no tree pruning
    doesn   t overfit! 

32

test set classification error (%)

classification forests in practice

100 trees in 
forest

microsoft kinect

~5 attributes

1 attribute

breiman, leo (2001). "id79s". machine learning 45 (1), 5-32

kinect for xbox one

    aka    kinect 2     (2013)
    rgb-d camera:  time-of-flight camera + color camera
    depth resolution 2.5cm at 4m

j. shotton, a. fitzgibbon, m. cook, t. sharp, m. finocchio, r. 
moore, a. kipman, and a. blake, real-time  human pose  
recognition in parts from a single depth image, proc. ieee 
cvpr, june 2011

what the kinect does

compute depth image

estimate body parts and joint poses

application (e.g., games)

33

time-of-flight depth sensing

kinect rgb-d camera

stop-
watch

source

sensor

emitted 
light pulse 

e

d  
u l s

e

e i v
t  p

c

h

e
r
li g

time delay t

y
t
i
s
n
e
t
n

i

emitted pulse
received pulse 

time

scene

depth = c / 2t,  
where c = 
speed of light

[koechner, 1968]

goal:  estimate pose from depth image

step 1.  find body parts
step 2.  compute joint positions

finding body parts

    what should we use for features?

    difference of depths at 2 pixels

rgb

depth

part label map joint positions

    what should we use for a classifier?

    id79 / decision forest

34

features

difference of depths at two pixels

id79s

for each tree,

1. choose a training set by choosing n times with 

replacement from all n available training 
examples

2. at each node of decision tree during construction, 
choose a random subset of m attributes from the 
total number, m, of possible attributes (m << m)

3. select best attribute at node using max-gain

   = (u, v) is the offset to second pixel from first pixel

    no tree pruning
    doesn   t overfit! 

part classification with id79s

learning phase

    id79:  collection of independently-trained 

binary id90

    each tree is a classifier that predicts the likelihood of a 

pixel x belonging to body part class c in image i
    non-leaf node corresponds to a thresholded feature
    leaf node corresponds to a conjunction of several features
    at leaf node store learned distribution p(c|i, x)

1. for each tree, pick a randomly sampled subset of training data
2. randomly choose a set of features and thresholds at each node
3. pick the feature and threshold that give the largest information gain
4. recurse until a certain accuracy is achieved or depth-bound reached

35

classification

classify each pixel x in image i using all t
id90 and average the results at the 
leaves:

random  forest:  an ensemble model

tree t=1

t=2

t=3

the ensemble model

forest output id203

implementation

basic learning approach

    31 body parts (i.e., 31 classes)
    3 trees (max depth 20)
    300,000 training images per tree randomly 

selected from 1m training images

    2,000 training example pixels per image
    2,000 candidate features
    50 candidate thresholds per feature
    decision forest constructed in 1 day on 1,000 

core cluster

    very simple features

    lots of data

    flexible classifier

36

lots of training data

synthetic data for training and 

testing

    capture and sample 500k motion capture 

frames of people kicking, driving, dancing, etc.
    get 3d models for 15 bodies with a variety of 

weights, heights, etc.

    synthesize motion capture data for all 15 body 

types

results

body tracking in microsoft 
kinect for xbox 360

input depth image (bg removed)

inferred body parts posterior

(2 videos here)

37

results

classification forests in practice

automatic segmentation of 
glioblastoma brain tumors

d. zikic, b. glocker, e. konukoglu , a. criminisi, j. shotton,  c. demiralp, o. 
m. thomas, t. das, r. jena and s. j. price, decision forests for tissue-
specific segmentation of high-grade gliomas in multi-channel mr, proc. 
miccai, 2012

segmentation of brain tumors

segmentation of 
tumorous tissues:

t1-gad

t1

t2

flair

---- active cells (ac)
---- necrotic core (nc)
---- edema (e)
---- background (b)
d. zikic, b. glocker, e. konukoglu , a. criminisi, j. shotton,  c. 
demiralp, o. thomas, t. das, r. jena and s. price, decision forests for 
tissue-specific segmentation of high-grade gliomas in multi-channel 
mr, proc. miccai,  2012

dti-p

dti-q

3d mri input data

38

id79 summary

    advantages

    one of the most accurate learning algorithms
    efficient
    can handle thousands of attributes

    disadvantages

    difficult to interpret (compared to id90)

39

