supervised learning methods

    k-nearest-neighbors (id92)
    id90
    neural networks
    support vector machines (id166)

what are support vector machines 

(id166s) used for?

    classification
    regression and data-fitting
    supervised and unsupervised learning

support vector machines

chapter 18.9 and the optional paper    support 
vector machines    by m. hearst, ed., 1998

acknowledgments:  these slides combine and modify ones 
provided by andrew moore (cmu), carla gomes (cornell), 
mingyue tan (ubc), jerry zhu (wisconsin), glenn fung 
(wisconsin), and olvi mangasarian (wisconsin)

lake mendota, madison, wi
    identify areas of land cover (land, ice, 
    two methods:

water, snow) in a scene

lake mendota, wisconsin 

   
   

scientist manually-derived
support vector machine (id166)

classifier

expert 
derived

cloud
ice
land
snow
water
unclassified

45.7%

60.1%

93.6%

63.5%

84.2%

45.7%

id166

58.5%

80.4%

94.0%

71.6%

89.1%

courtesy of steve chien of nasa/jpl

visible 
image

expert 
labeled

expert 
derived

automated 

ratio

id166

1

linear classifiers

x

f 

y

denotes +1
denotes -1

linear classifiers

(aka linear discriminant functions)
    definition:

a function that is a linear combination of the components 
of the input (column vector) x:

how would you 
classify this data?

f (x) =

    xj + b = wtx + b

wj

d

j=1

where w is the weight (column vector) and bis the bias

    a 2-class classifier then uses the rule:

decide class c1 if f(x)     0 and class c2 if f(x) < 0
or, equivalently, decide c1 if wtx     -band c2 otherwise

planar decision surface 
in ddimensions is 
parameterized by (w, b)

w is the plane   s normal vector
bis the distance from the origin

linear classifiers

x

denotes +1
denotes -1

f 

y

f(x, w, b) = sign(wt x + b)

how would you 
classify this data?

w 

b  

2

linear classifiers

x

denotes +1
denotes -1

f

y

f(x, w, b) = sign(wt x + b)

how would you 
classify this data?

linear classifiers

x

denotes +1
denotes -1

f

y

f(x, w, b) = sign(wt x + b)

how would you 
classify this data?

linear classifiers

x

denotes +1
denotes -1

f 

y

f(x, w, b) = sign(wt x + b)

any of these 
would be fine    

    but which is 
best?

classifier margin

x

denotes +1
denotes -1

f 

y

f(x, w, b) = sign(wt x + b)
define the margin
of a linear classifier 
as the widththat 
the decision 
boundary could be 
expanded before 
hitting a data point

3

maximum margin

x

f

y

maximum margin

x

denotes +1
denotes -1

f(x, w, b) = sign(wt x + b)
the maximum 
margin linear 
classifier is the 
linear classifier 
with the maximum 
margin
this is the 
simplest kind of 
id166 (called an 
lid166)

linear id166

denotes +1
denotes -1

support vectors 
are those data 
points that the 
margin pushes 
against

f

y

f(x, w, b) = sign(wt x + b)

the maximum 
margin linear 
classifier is the 
linear classifier 
with the maximum 
margin
this is the 
simplest kind of 
id166 (called an 
lid166)

linear id166

why the maximum margin?

denotes +1
denotes -1

support vectors 
are those data 
points that the 
margin pushes 
against

1.
2.

intuitively this feels safest
f(x,w,b) = sign(w. x-b)
if we   ve made a small errorin the 
location of the boundary (it   s been 
the maximum 
jolted in its perpendicular direction) 
margin linear 
this gives us least chance of causing a 
misclassification
classifier is the 
linear classifier 
3. robustto outliers since the model is 
immune to change/removal of any 
with the, um, 
non-support-vector data points
maximum margin.
4. there   s some theory(using    vc 
this is the 
dimension   ) that is related to (but not 
simplest kind of 
the same as) the proposition that this 
id166 (called an 
is a good thing
lid166)

5. empirically it works very well

specifying a line and margin

1     

    p re dict  cla ss  =   +

n e

z o

    p re dict  cla ss  =  - 1     

n e

z o

plus-plane

classifier boundary

minus-plane

    how do we represent this mathematically?
        in dinput dimensions?  
    an example x = (x1,    , xd)t

4

specifying a line and margin

1     

    p re dict  cla ss  =   +

n e

z o

    p re dict  cla ss  =  - 1     

n e

z o

w x + b = 1
w x + b = 0
w x + b = -1

plus-plane

classifier boundary

minus-plane

weight vector: w = (w1 ,    , wd)t
bias or threshold:  b

w

computing the margin
    p re dict  cla ss  =   +

1     

n e

z o

m =margin (width)

    p re dict  cla ss  =  - 1     

n e

z o

how do we compute 

min terms of w
and b?

w

t x + b = 1
t x + b = 0
t x + b = -1
w
w

    plus-plane   =     wt x + b = +1
    minus-plane =     wt x + b = -1

d

wt    x =

the dot product 
wj
    xj
j=1
is a scalar: x   s 
projection onto w

classify as +1
-1
?

if wt x + b     1
if wt x + b     -1
if -1 < wt x + b < 1

computing the margin
    p re dict  cla ss  =   +
m =margin

1     

x+

n e

z o

    p re dict  cla ss  =  - 1     

x   
n e

z o

how do we compute 

min terms of w
and b?

w

w x + b = 1
w x + b = 0
w x + b = -1

    plus-plane   =     wt x + b = +1 
    minus-plane =    wt x + b = -1 

claim: the vector w is perpendicular to the plus-plane and 

the minus-plane

computing the margin
    p re dict  cla ss  =   +
m =margin

1     

x+

n e

z o

    p re dict  cla ss  =  - 1     

x   
n e

z o

how do we compute 

min terms of w
and b?

w

w x + b = 1
w x + b = 0
w x + b = -1

    plus-plane   =     wt x + b = +1 
    minus-plane =     wt x + b = -1 
    the vector w is perpendicular to the plus-plane
    let x    be any point on the minus-plane
    let x+ be the closest plus-plane-point to x   

any location in 
any location in 
rd  ; not 
(cid:0)m: not 
necessarily a 
necessarily a 
datapoint
data point

    plus-plane   =     wt x + b = +1 
    minus-plane =     wt x + b = -1 
    the vector w is perpendicular to the plus-plane
    let x    be any point on the minus-plane
    let x+ be the closest plus-plane-point to x   
    claim: x+ = x    +   w for some value of   

5

z o

n e

x+

1     

computing the margin
    p re dict  cla ss  =   +
m =margin
the line from x    to x+ is 
how do we compute 
perpendicular to the 
planes
min terms of w
and b?
so to get from  x    to x+
travel some distance in 
direction w

    p re dict  cla ss  =  - 1     

x   
n e

w x + b = 1
w x + b = 0
w x + b = -1

w

z o

    plus-plane   =     wt x + b = +1
    minus-plane =     wt x + b = -1 
    the vector w is perpendicular to the plus-plane
    let x    be any point on the minus-plane
    let x+ be the closest plus-plane-point to x   
    claim: x+ = x    +   w for some value of    why?

computing the margin
    p re dict  cla ss  =   +
m =margin

1     

x+

n e

z o

w

    p re dict  cla ss  =  - 1     

x   
n e

z o

w x + b = 1
w x + b = 0
w x + b = -1
what we know:
    wt x+ + b = +1
    wt x- + b = -1
    x+ = x    +   w
    ||x+ - x    || = m

computing the margin
    p re dict  cla ss  =   +
m =margin

1     

x+

n e

z o

w

w x + b = 1
w x + b = 0
w x + b = -1

what we know:
    wt x+ + b = +1
    wt x    + b  = -1
    x+ = x    +   w
    ||x+ - x    || = m
it   s now easy to get m
in terms of w and b

z o

   
    p re dict  cla ss  =  - 1     
   
   

x   
n e
wt (x    +   w) + b = 1 

wt x    + b+   wtw = 1

-1 +   wtw = 1
2

=l

wwt

computing the margin
    p re dict  cla ss  =   +
m =margin =

1     

x+

n e

z o

  

2

2
wtw

w

    p re dict  cla ss  =  - 1     

z o

wtw

  =
  

x   
n e
m= ||x+ - x    || = ||   w ||
  =     w   =     wtw
2
wtw

  =  

=
  

2 wtw
wtw
2
w

=

= m, margin size

w x + b = 1
w x + b = 0
w x + b = -1

what we know:
    wt x+ + b = +1
    wt x    + b = -1
    x+ = x    +   w
    ||x+ - x    || = m 

6

learning the maximum margin classifier

1     

x+

m =margin =

2
wtw

  

    p re dict  cla ss  =   +

n e

z o

w

    p re dict  cla ss  =  - 1     

x   
n e

z o

w x + b = 1
w x + b = 0
w x + b = -1

given a guess of w and b, we can
1. compute whether all data points in the correct half-planes
2. compute the margin
so now we just need to write a program to search the space 

of w   s and b   s to find the widest margin that correctly 
classifies all the data points    how?

classification with id166s

sec. 15.1

given a new point x, we can classify it by

    computing score:  wtx + b
    deciding class based on whether < 0 or > 0

    if desired, can set confidence threshold t

score > t : yes
score < -t : no
else: don   t know

1

0
-1

id166 as constrained optimization
    unknowns: w, b
    objective function: maximize the margin:               

m= 2 / ||w||

    equivalent to minimizing ||w|| or ||w||2 = wtw

    n training points: (xk, yk), yk= +1 or -1
    subject to each training point correctly classified, 

i.e.,

subject to yk(wtxk+ b)     1 for all k

nconstraints

this is a quadratic optimization problem 
(qp), which can be solved efficiently

id166s:  more than two classes
    id166s can only handle two-class problems
    k-class problem: split the task into kbinary

tasks and learn k id166s:
    class 1 vs. the rest (classes 2     k)
    class 2 vs. the rest (classes 1, 3     k)
       
    class k vs. the rest

    pick the class that puts the point farthest into 

its positive region

7

from statnikov et al.

id166s:  non linearly-separable data

two approaches:

   allow a few points on the wrong side (slack variables)
   map data to a higher dimensional space, and do linear 

classification there (kernel trick)

id166s: non linearly-separable data
what if the data are notlinearly separable?

non linearly-separable data

approach 1:  allow a few points on the wrong 
side (slack variables)

   soft margin classification   

8

what should we do?

denotes +1
denotes -1

denotes +1
denotes -1

what should we do?
idea:

minimize
||w||2 + c (# train errors)

tradeoff     penalty    

parameter

there   s a serious practical 
problem with this approach

denotes +1
denotes -1

what should we do?
idea:

minimize
||w||2 + c (# train errors)

tradeoff     penalty    

parameter

denotes +1
denotes -1

what should we do?

minimize
||w||2 + c (distance of all
   misclassified 
points    to their 
correct place)

can   t be expressed as a quadratic 

programming problem.

there   s a serious practical 
problem with this approach

so solving it may be too slow.
(also, doesn   t distinguish between 

disastrous errors and near misses)

9

choosing c

choosing c, the penalty factor
critical to choose a good value for the constant 
penalty parameter, c,because

    ctoo big means very similar to lid166, we 

have a high penalty for non-separable 
points, and we may use many support 
vectors and overfit

    ctoo small means we allow many 

misclassifications in the training data and we 
may underfit

learning maximum margin with noise

learning maximum margin with noise

from statnikov et al.

m =2
wtw

w x + b = 1
w x + b = 0
w x + b = -1

what should our optimization 

criterion be?

given guess of w, b,we can
1. compute sum of distances 

of points to their correct 
zones

2. compute the margin width
assume n examples, each  
(xk, yk) where yk= +1 / -1

m =2
wtw

  2

  11

  7

w x + b = 1
w x + b = 0
w x + b = -1

   slack variables   
what should our optimization 

criterion be?

minimize

1
2
   

   
wtw + c   k

n

k=1

given guess of w, b,we can
1. compute sum of distances 

of points to their correct 
zones

2. compute the margin width
assume n examples, each  
(xk, yk) where yk= +1 / -1

how many constraints will we 

have?  n

what should they be?
yk(wtxk+ b)     1-  kfor all k
note:    k= 0 for points in correct zone

10

learning maximum margin with noise
given guess of w, bwe can
1. compute sum of distances 

d = # input 
dimensions

  11

m =
2
  w w

  2

our original (noiseless data) qp had d +1

of points to their correct 
zones

our new (noisy data) qp has d +1+n 

variables: w1, w2,     wd, and b
e7
variables: w1, w2,     wd, b,   k,   1 ,      n

2. compute the margin width
assume rdatapoints, each 
(xk,yk) where yk= +/-1

w x + b = 1
w x + b = 0
w x + b = -1

what should our optimization 

criterion be?

minimize

1
2
   

   
wtw  +  c   k

n

k=1

how many constraints will we 

n = # examples

have? n

what should they be?
wtxk+ b     1-   k if yk= +1
wtxk+ b     -1+  k if yk= -1

learning maximum margin with noise
given guess of w , bwe can
1. compute sum of distances 

  11

  2

m =
2
wtw

  

w x + b = 1
w x + b = 0
w x + b = -1

  7

   slack variables   
what should our optimization 

criterion be?

minimize

1
2
   

   
wtw + c   k

n

k=1

of points to their correct 
zones

2. compute the margin width
assume nexamples, each 

(xk, yk) where yk= +1 / -1

how many constraints will we 

have?  2n

what should they be?
wtxk+ b     +1 -   k if yk= +1
wtxk+ b     -1 +   k if yk= -1
  k    0 for all k

non linearly-separable data

approach 2:  map data to a higher dimensional 
space, and then do linear classification there 
(called the kernel trick)

suppose we   re in 1 dimension

what would 

id166s do with 
this data?

x=0

11

suppose we   re in 1 dimension

harder 1d dataset:
notlinearly-separable

what can be done 

about this?

x=0
positive    plane   

negative    plane   

x=0

harder 1d dataset

harder 1d dataset

the kernel trick: 

preprocess the 
data, mapping x
into a higher 
dimensional 
space, z =   (x)
for example:
x =f
,

)(

(

x=0

2xx
)
here,    maps data from 1d to 2d
in general, map from d-dimensional input space to k-dimensional z space

the data islinearly 
separable in the new 
space, so use a linear 
id166 in the new space

x=0

wt   (x) + b = +1

the kernel trick: 

preprocess the 
data, mapping x
into a higher 
dimensional 
space, z =   (x)

x =f

)(

(

2xx

,

)

12

another example
x
2
f
+
1

xx
,
1

xx
,
1

=

(

)

(

,

2

2

x

2
2

)

another example                      

project examples into some higher dimensional space 
where the data islinearly separable, defined by z =   (x)

project examples into some higher dimensional space where the 
data islinearly separable, defined by z =   (x)
cs 540, university of wisconsin-madison, c. r. dyer

algorithm

1. pick a    function
2. map each training example, x, into the new 

higher-dimensional space using z =   (x)
3. solve the optimization problem using the 

nonlinearly transformed training examples, 
z, to obtain a linear id166 (with or without 
using the slack variable formulation) defined 
by w and b

4. classify a test example, e, by computing:   

sign(wt      (e) + b)

improving efficiency

    time complexity of the original optimization 

formulation depends on the dimensionality, k, 
of z  (k>> d) 

    we can convert the optimization problem into 
an equivalent form, called the dual form, with 
time complexity o(n 3) that depends on n, 
the number of training examples, notk

    dual form will also allow us to rewrite the 
mapping functions in    in terms of    kernel 
functions    instead

13

dual optimization problem
+       
yy
k

      q
k
kl

where

q
kl

  

=

n

n

n

k

l

k

1
=

k

1
=

l

1
=

xx
.
( t
k

l

)

l

maximize

dual optimization problem
+       
yy
k

dot product of 
two examples

      q
k
kl

where

q
kl

  

=

n

n

n

k

l

l

k

1
=

k

1
=

l

1
=

maximize

xx
.
( t
k

l

)

subject to these 

constraints:

  0

  k

  

c

k
"

n

  

k

1
=

   y

k

k

=

0

subject to these 

constraints:

  0

  k

  

n

  

   y

=

0

c

k
"

new variables; examples with   k> 0

1
=

k

k

k

will be the support vectors

n

   y

then define:
=   w
1
=
  
b
1(
=
-
k
k
 
where
=

k
y

k

k

x

k

then classify with:
f(x,w,b) = sign(wtx-b)

wx
)
.
-
k
k
  
max
arg
k

k

k

nexamples: (xk, yk)
where yk= +1 / -1

then define:
=   w

n

k

1
=
  
b
1(
=
-
k
k
 
where
=

y

   y

k

x

k

k

then classify with:
f(x,w,b) = sign(wtx -b)

wx
)
.
-
k
k
  
max
arg
k

k

k

nexamples: (xk, yk)
where yk= +1 / -1

algorithm

    compute nx nmatrix qby computing yi yj (xi

between allpairs of training data points

    solve the optimization problem to compute   i

for i= 1,    , n

    each non-zero   iindicates that example xiis a 
support vector
    compute w and b
    then classify test example x with:

f(x) = sign(wt x    b)

t xj)

maximize

dual optimization problem (after mapping)

n

  

k

1
=

  

k

-

1
2

n

n

    

k

1
=

l

1
=

q    
k
kl

l

where qkl = ykyl(  (xk)t  (xl))

n

  

k

1
=

k y  

k

=

0

f(x,w,b) = sign(wt  (x) - b)

then classify with:

subject to these 

constraints:

  0

  k

  

c

k
"

=

w

then define:
  
k y  
k  
0
 s.t.
 
>
  
b
1(
=
-
k
k
 
where
=

k
y

x  

(

)

k

k

wx
)
.
-
k
k
  
max
arg
k

copyright    2001, 2003, andrew w. moore

k

k

nexamples: (xk, yk)
where yk= +1 / -1

14

dual optimizzation problem (after mapping)

maximize

n

  

k

1
=

  

k

-

1
2

n

n

    

k

1
=

l

1
=

q    
k
kl

l

where qkl = ykyl(  (xk)t  (xl))

subject to these 

constraints:

  0

  k

  

n
  
c
k
"
n 2 dot products to 
compute this matrix
k
1
=

k y  

=

0

k

=

w

then define:
  
k y  
k  
0
 s.t.
 
>
  
b
1(
=
-
k
k
 
where
=

k
y

x  

(

)

k

k

wx
)
.
-
k
k
  
max
arg
k

f(x,w,b) = sign(wt  (x) - b)

then classify with:

k

k

nexamples: (xk, yk)
where yk= +1 / -1

copyright    2001, 2003, andrew w. moore

x=f x
(
2
1

)(

2,

xxx
,
21

2
2

)

)

)t        x j(
   xi(
2 + 2xi1xi2 2x j1x j2 + xi2
2x j1
= xi1
2 + 2xi1xi2x j1x j2 + xi2
= xi1
2x j1
(
= xi1x j1 + xi2x j2
(
= xi

t     x j

2 x j2
2

)2

)2

2 x j2
2

    dual formulation of the optimization problem depends on 

the input data only in dot products of the form: 
  (xi)t      (xj)   where xiand xjare two examples

    we can compute these dot products efficiently for certain 

types of      s where k(xi, xj) =   (xi)t      (xj)

    example:

x=f x
(
2
1

)(

2,

xxx
,
21

2
2

)

  (xi)t      (xj) = (xi

t    xj)2 = k(xi, xj)

    since the data onlyappears as dot products, we do not
need to map the data to higher dimensional space (using 
  (x) ) because we can use the id81 kinstead

id81s

    a kernel is a dot product in somefeature 

space

    a id81 is a function that can be 

applied to pairs of input data points to 
evaluate dot products in some corresponding 
(possibly infinite dimensional) feature space

    we do notneed to compute    explicitly

15

what   s special about a kernel?
    say 1 example is (in 2d) is:  s = (s1, s2)
    we decide to use a particular mapping into 6d space:

  (s)t = (s1

2, s2

2,    2s1s2, s1, s2, 1)

    let another example be t = (t1, t2)
    then,
  (s)t      (t) = s1

2 + s2

2 t1

2 t2

= (s1t1 + s2t2 + 1)2  
= (st  t +1)2

2 + 2s1s2t1t2 + s1t1 + s2t2 + 1

    so, define the id81 to be k(s, t) = (st  t +1)2
    we save computation by using k 

=   (s)t      (t) 

algorithm

    compute nx nmatrix qby computing 

yiyjk(xi,xj) between all pairs of training points

    solve optimization problem to compute   i

for i= 1,    , n

    each non-zero   iindicates that example xiis a 
support vector
    compute w and b
    classify test example x using:

f(x) = sign(wt x    b)

some commonly used kernels
    linear kernel:  k(xi, xj) = xi
t xj
    quadratic kernel: k(xi, xj) = (xi
    polynomial of degree d kernel: k(xi, xj) = (xi
    radial-basis function (gaussian) kernel:                                     

t xj+1)d

t xj+1)2

k(xi, xj) = exp(   ||xi-xj||2  /   2)

    many possible kernels; picking a good one is tricky
    hacking with id166s: create various kernels, hope their 
space    is meaningful, plug them into id166, pick one 
with good classification accuracy

    kernel usually combined with slack variables because 

no guarantee of linear separability in new space

applications of id166s

n bioinformatics
n machine vision
n text categorization
n ranking (e.g., google searches)
n handwritten character recognition
n time series analysis

   lots of very successful applications!

16

handwritten digit recognition

description  of  the  data

    for every paper:

    machine readable text was created using a scanner
    computed relative frequencies of 70 words that 

mosteller-wallace identified as good candidates for 
author-attribution studies

example application:

the federalist papers dispute

    written in 1787-1788 by alexander hamilton, john 
jay, and james madison to persuade the citizens of 
new york to ratify the u.s. constitution

    papers consisted of short essays, 900 to 3500 words 

in length  

    authorship of 12 of those papers have been in 

dispute ( madison or hamilton); these papers are 
referred to as the disputed federalist papers

70-word dictionary

    each document is represented as a vector containing the 

70 real numbers corresponding to the 70 word 
frequencies

    the dataset consists of 118 papers:

    50 madison papers
    56 hamilton papers
    12 disputed papers

bag of 
words

17

feature selection for classifying the 

disputed federalist papers

    apply the id166 algorithm for feature 

selection to:
    train on the 106 federalist papers with known 

authors

    find a classification hyperplane that uses as few 

words as possible

    use the hyperplane to classify the 12 

disputed papers

hyperplane classifier using 3 words

    a hyperplane depending on three words 

was found:

0.537to + 24.663upon + 2.953would = 66.616

    all disputed papers ended up on the 

madison side of the plane

results: 3d plot of hyperplane

summary

    learning linear functions

    pick separating hyperplanethat maximizes margin
    separating plane defined in terms of support vectors 

(small number of training data points) only

    learning non-linear functions

    project examples into higher dimensional space
    use id81s for efficiency
    generally avoids overfitting problem
    global optimization method; no local optima
    can be expensive to apply, especially for multi-class 

problems

    biggest drawback: the choice of id81

    there is no    set-in-stone    theory for choosing a id81 

for any given problem

    once a id81 is chosen, there is only one modifiable 

parameter, the error penalty c

18

software

    a list of id166 implementations can be found at 
http://www.kernel-
machines.org/software.html
    some implementations (such as libid166) can 
handle multi-class classification
   id166light is one of the earliest and most 
frequently used implementations of id166s
    several matlab toolboxes for id166s are 
available

19

