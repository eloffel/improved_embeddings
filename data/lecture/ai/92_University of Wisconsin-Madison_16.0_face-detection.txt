face detection and 

recognition

reading:  chapter 18.10

face detection problem

    scan window over image

    classify window as either:

    face
    non-face

window

classifier

face

non-face

face detection: motivation

face detection: motivation

    automatic camera focus

    automatic camera focus
    easier photo tagging

http://cdn.conversations.nokia.com.s3.amazonaws.com/wp-content/uploads/2013/09/nokia-pro-camera-auto-focus_half-press.jpg
http://cdn.conversations.nokia.com.s3.amazonaws.com/wp-content/uploads/2013/09/nokia-pro-camera-auto-focus_half-press.jpg

http://sphotos-d.ak.fbcdn.net/hphotos-ak-ash3/163475_10150118904661729_7246884_n.jpg

face detection: motivation

face detection: challenges

    automatic camera focus
    easier photo tagging
    first step in any face recognition algorithm

    large face shape and appearance variation
    scale and pose (yaw, roll, pitch) variation
    background clutter
    occlusion

http://images.fastcompany.com/upload/camo1.jpg

the viola-jones real-time 

face detector

p. viola and m. jones, 2004

challenges:
    each image contains 10,000     50,000 

locations and scales where a face may occur

    faces are rare: 0 - 50 per image

    > 1,000 times as many non-faces as faces

    want a very small # of false positives:  <10-6

    hair
    glasses
    hat

    lighting
    expression
    makeup

use machine learning to 
create a 2-class classifier

    training data (grayscale)

    5,000 faces (frontal)
    108 non-faces

    faces are normalized

    scale, translation

    many variations

    across individuals
    illumination
    pose (rotation both in plane and out)

use classifier at all
locations and scales

building a classifier

    compute lots of very simple features
    efficiently choose the best features
    each feature is used to define a    weak 

classifier   

    combine weak classifiers into an 

ensemble classifier based on boosting
    learn multiple ensemble classifiers and 

   cascade    them together to improve 
classification accuracy and speed

computing features

    at each position and scale, use a sub-

image (   window   ) of size 24 x 24

    compute multiple candidate features for 

each window

    want to rapidly compute these features

local features

what are local features trying to capture?

the local appearance in a region of the image

david g. lowe, "distinctive image features from scale-invariant keypoints," international 
journal of id161, 60, 2 (2004)

what types of features?

    use domain knowledge

    the eye region is darker than the forehead 

or the upper cheeks

    the nose bridge region is brighter than the 

eyes

    the mouth is darker than the chin

    encoding

    location and size: eyes, nose bridge, mouth, 

etc.

    value: darker vs. lighter

huge number of features

features

    4 feature types (similar to    haar wavelets   ):

two-rectangle

three-rectangle

four-rectangle

value =      (pixels in white 
area) -     (pixels in black 
area)

computing features efficiently:

the integral image

    intermediate representation of the image

    sum of all pixels above and to left of (x, y) in image i:

yxii
,(

)

=

  

yxx
,
'
  

'

)'

yxi
,'(
y
  

    computed in one pass over the image:

160,000 features for each window!

ii(x, y) = i(x, y) + ii(x-1, y) + ii(x, y-1)     ii(x-1, y-1)

using the integral image

(0,0)

y

x

(x,y)

s(x, y) = s(x, y-1) + i(x, y)
ii(x, y) = ii(x-1, y) + s(x, y)

    with the integral image 

representation, we can compute 
the value of any rectangular sum 
in constant time

    for example, the integral sum in 

rectangle d is computed as:
ii(4) + ii(1)     ii(2)     ii(3) 

decision stumps

each weak classifier is called a decision 
stump because it acts as a decision tree 
with a root node, containing the test ht(x),  
and two leaf nodes below it

ht

-1

+1

features as weak classifiers

given window x, feature detector ft , and 
threshold   t, construct a weak classifier, ht

a very simple 
linear classifier

threshold,   t, set to the mean value for the 
feature on both classes and then averaging the 
two values

combining classifiers:

ensemble methods

    aggregation of predictions of multiple
classifiers with the goal of improving 
accuracy by reducing the variance of 
an estimated prediction function

       mixture of experts   
    combining multiple classifiers often 
produces higher accuracy than any 
individual classifier

ensemble strategies

id112 

used for id79s

    create classifiers using different training sets, where 

each training set is created by    id64,    i.e., 
drawing examples (with replacement) from all 
possible training examples

boosting

    sequential production of classifiers, where 

each classifier is dependent on the previous 
one

    make examples misclassified by current 

classifier more important in the next classifier, 
i.e., use a weighted training set

motivation

    hard to design a single accurate classifier that 

generalizes well 

    easy to create many weak classifiers 

    a classifier is okay if it is at least slightly better 

than random guessing 

    example: if an email has word    money,    

classify it as spam; otherwise classify it as not 
spam 

    likely to be better than random guessing 

boosting

    boosting is a class of ensemble methods for 

sequentially producing multiple weak classifiers, 
where each classifier is dependent on the 
previous ones

    make examples misclassified by previous 

classifier more important in the next classifier

    combine all weak classifiers linearly into a 

   strong    classifier:

weight

weak classifier

boosting

how to select the best features?

how to learn the final classification function?

c(x) =    1 h1(x)  +    2  h2(x)  + ....

adaboost

    assume 2-class classification problem

    2 classes are +1 and -1

    adaboost computes function
g(x) =    1 h1(x)  +    2  h2(x)  + ....
where h i(x) = +1 or -1 are the weak classifiers

    final classifier is
c(x) = sign(g(x))

   

   weighted majority    combination of all the 
weak classifiers

adaboost algorithm

for each example, xi, in training set, set w(xi) = 1/n where n
is the size of the training set
for t = 1 to t do

1. find best weak classifier ht(x) using training set and 

weights w(x)

2. compute total weighted error of classifier ht:

n

  t =

   
i=1

w(xi)   i(yi     ht(xi))
3. compute weight   t for classifier ht: 
4. update training set weights:  

  t = log(1     t
  t

)

w(xi) = w(xi)   e  t   i(yi   ht (xi ))

5. normalize w so that its values sum to 1 

adaboost algorithm

given a set of training windows labelled +1 or -1, 
initially give equal weight to each training example
repeat t times

1. select best weak classifier (decision stump) 
(i.e., one with minimum total weighted error 
on all training examples)
increase weights of the examples 
misclassified by current weak classifier

2.

    each round greedily selects the best feature (i.e., decision 

stump) given all previously selected features

    final classifier weights weak classifiers by their accuracy

adaboost example

x1
x2

xt

each data 
point has a 
class label:
+1 (  )
-1 (  )

yt =

and a weight:

wt

example

a weak learner (decision stump) from the family of decision lines:

example

each data 
point has a 
class label:
+1 (  )
-1 (  )

yt =

and a weight:

wt

each data 
point has a 
class label:
+1 (  )
-1 (  )

yt =

and a weight:

wt

this is a    weak classifier   :  it performs slightly better than chance

this one is best

example

example

each data 
point has a 
class label:
+1 (  )
yt =
-1 (  )
update the 
weights:
wt wt exp{-  tytht}

ht is the weak classifier. 
yt ht returns -1 if example 
misclassified; +1 otherwise

re-weight misclassified examples

each data 
point has a 
class label:
+1 (  )
yt =
-1 (  )
update the 
weights:
wt wt exp{-  tytht}

select 2nd weak classifier

example

example

each data 
point has a 
class label:
+1 (  )
yt =
-1 (  )
update the 
weights:
wt wt exp{-  tytht}

each data 
point has a 
class label:
+1 (  )
yt =
-1 (  )
update the 
weights:
wt wt exp{-  tytht}

re-weight misclassified examples

select 3rd weak classifier

example
h1

h2

h4

h3

the final classifier is the combination of all the 
weak, linear classifiers

adaboost - adaptive boosting

    learn a single simple classifier (decision stump) 
    classify the data
    look at where it makes errors
    re-weight the training data so that instances where we 

made errors get higher weights

    now learn a 2nd simple classifier using the weighted 

data

    combine the 1st and 2nd classifiers and weight the data 

according to where they make errors

    learn a 3rd classifier based on the re-weighted data
        and so on until we learn t simple classifiers
    final classifier is the weighted combination of all t

classifiers

example classifier

learned features

    a classifier with t=200 rectangle features was learned 

    95% correct detection on test set with 1 face in 14,084

using adaboost

false positives
    not good enough

e

t

a
r
 

n
o

i
t
c
e

t

e
d

 
t
c
e
r
r
o
c

roc curve for 200-feature classifier

false positive rate

classifier error is driven down as 
t increases, but cost increases

adaboost:  generalization error
    it seems possible that boosting may overfit if 

run for too many rounds

    it has been observed empirically that 

adaboost does not overfit, even when run for 
thousands of rounds

    it has been observed that the generalization 
error (on the test set) continues to go down 
long after training set error reaches 0

adaboost
    given a training set of images

    for t = 1,    ,t (rounds of boosting)

    a weak classifier is trained using a single feature
    the error of the classifier is calculated

    the classifier with the lowest error is selected, and 
combined with the others to make a strong classifier

    after a t rounds a t-strong classifier is created

    it is the weighted linear combination of the weak 

classifiers selected

cascaded classifier

image
window

1 feature

f

50%

5 features

20%

20 features

2%

face

f

f

non-face

non-face

non-face

    a t=1 feature classifier achieves 100% detection 

rate and about 50% false positive rate

    a t=5 feature classifier achieves 100% detection 

rate and 40% false positive rate (20% 
cumulative)
    using data from previous stage

    a t=20 feature classifier achieve 100% detection 
rate with 10% false positive rate (2% cumulative)

cascading

   

improve accuracy and speed by    cascading    multiple 
classifiers together

    start with a simple (small t) classifier that rejects many 

of the negative (no face) windows while detecting 
(almost) all positive windows

    positive results from the first classifier triggers the 

evaluation of a second (more complex; larger t) 
classifier, and so on
    each classifier is trained with the false positives from 

the previous classifier

    a negative classification at any point means immediate 

rejection of the window (i.e., no face detected)

training a cascade

    user selects values for: 

    maximum acceptable false positive rate per 

node

    minimum acceptable detection rate per node
    target overall false positive rate

    user provides a set of positive and 

negative training examples

results

detection in real images

    basic classifier operates on 24 x 24 windows
    scaling

    scale the detector (rather than the images)
    features can easily be evaluated at any scale
    scale by factors of 1.25

    location

    move detector around the image (e.g., 1 pixel 

increments)
    final detection

    a real face may result in multiple nearby detections  
    post-process detected sub-windows to combine 

overlapping detections into a single detection

training data set

structure of the detector

4,916 hand-labeled face 
images (24 x 24)

9,500 non-face images

    38 layer cascade
    6,060 features

l a ye r n u m b e r
n u m b e r o f fe a u tu re s
d e te ctio n  ra te
r e je ctio n  ra te

1
2

100%
50%

2
10
100%
80%

3 to 4

50
-
-

5 to 38

-
-
-

results

    notice detection at multiple scales 

results

profile detection

profile features 

real-time demos

    http://flashfacedetection.com/

    http://flashfacedetection.com/camdemo2.html

http://vimeo.com/12774628#

face alignment and 
landmark localization

goal of face alignment: automatically align a face (usually 
non-rigidly) to a canonical reference

face image parsing

given an input face image, automatically 
segment the face into its constituent parts

goal of face landmark localization: automatically locate 
face landmarks of interests

http://www.mathworks.com/matlabcentral/fx_files/32704/4/icaam.jpg

http://homes.cs.washington.edu/~neeraj/projects/face-parts/images/teaser.png

smith, zhang, brandt, lin, and yang, exemplar-based face parsing, cvpr 2013

face image parsing: results

face image parsing: results

input

soft segments

hard segments

+

ground truth

input

soft segments

+

hard segments

ground truth

