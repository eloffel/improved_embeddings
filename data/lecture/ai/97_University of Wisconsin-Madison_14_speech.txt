id103

chapter 15.1     15.3, 23.5

   markov models and id48:  
a brief tutorial,    e. fosler-lussier, 1998

introduction

l speech is a dominant form of communication 

between humans and is becoming one for humans 
and machines

l id103: mapping an acoustic signal

into a string of words

l speech understanding: mapping what is said

to its meaning

applications

l medical transcription
l vehicle control (e.g., fighter aircraft, helicopters)
l game control
l intelligent personal assistants (e.g., siri)
l smartphone apps
l hci
l automatic translation
l telephony for the hearing impaired  

commercial software

l nuance dragon naturallyspeaking
l google cloud speech api
l microsoft bing speech api
l ibm speech to text
l wit.ai
l api.ai
l cmu sphinx
l and many more

1

introduction

international phonetic alphabet

l human languages are limited to a set of about 40 to 50 

distinct sounds called phones, e.g.,
    [ey]
    [ah]
    [oy]
    [em]
    [en]

bet
but
boy
bottom
button

l phonemes are equivalence classes of phones that can   t be 

distinguished from each other in a given language

l these phones are characterized in terms of acoustic 
features, e.g., frequency and amplitude, that can be 
extracted from the sound waves

    http://www.yorku.ca/earmstro/ipa/consonants.html
    http://www.youtube.com/watch?v=nz44witvjww&feature=fvw

id103 architecture
goal:  large vocabulary, continuous speech (words
not separated), speaker-independent

speech
waveform

feature extraction
(signal processing)

spectral
feature
vectors

phone likelihood
estimation (gaussians
or neural networks)

phone
likelihoods
p(o|q)

decoding (viterbi
or stack decoder)

words

neural net

id165 grammar

id48 lexicon

spectrograph

   i can see you   

 
 
 
 

y
c
n
e
u
q
e
r
f

time                

2

speech  recognition  task

introduction

o1, o2,    

signal

processor

match
search

w1, w2,    

analog
speech

discrete

observations

word

sequence

l why isn't this easy?

    just develop a dictionary of pronunciation

e.g., coat = [k] + [ow] + [t] = [kowt]

    but    recognize speech           wreck a nice beach   

l problems:

    homophones: different fragments sound the same

l e.g.,    rec    and    wreck   

    segmentation: determining breaks between words

l e.g.,    nize speech    and    nice beach   

    signal processing problems

mcgurk effect:  hearing with your eyes

signal processing

l sound is an analog energy source resulting from 

pressure waves striking an eardrum or microphone

l an analog-to-digital converter is used to capture the 

speech sounds
    sampling: the number of times per second that the 

sound level is measured

    quantization: the number of bits of precision for the 

sound level measurements
l telephone: 3 khz (3000 times per second)
l speech recognizer: 8 khz with 8 bits/sample

so that 1 minute takes about 500k bytes

3

signal processing

input  sequence

wave encoding

    group into ~10 msec frames (larger blocks) that

are analyzed individually

    frames overlap to ensure important acoustical 

events at frame boundaries aren't lost

    frames are represented by a set of features

l amount of energy at various frequencies
l total energy in a frame
l differences from prior frame

    vector quantization encodes signal by mapping 

frames into n-dimensional feature space

id103 model

l bayes   s rule is used break up the problem into manageable parts:
p (words | signal ) = p (signal | words ) p (words )

p (signal )

p (words ): language model
l likelihood of words being heard
l e.g.,    recognize speech    more likely than    wreck a nice 

beach   

p (signal |words ): acoustic model

l likelihood of a signal given word sequence
l accounts for differences in pronunciation of words
l e.g., given    nice,    likelihood that it is pronounced [nuys]

l vector quantization encodes each frame as one of, 
say, 256 possible observation values (aka labels):  
c1, c2,    , c256

l for example, use id116 id91 for 

unsupervised learning of k = 256 clusters in feature 
space

l input is a sequence such as

c82, c44, c63, c44, c25,      
=  o1, o2, o3, o4, o5,    

    signal = observation sequence
    words = sequence of words

,

oooo
,...,
,
1=
3
wwww
,...,
,
1=
3

2
,

to
nw

2

    best match metric: 

  
w

=

arg

max
lw  

owp
(

|

)

    bayes   s rule:

  
w

=

  

arg

max
lw
  
max
lw
  

arg

wpwop
(

(

|
)
op
(
)
)
|

)

)

wpwop
(

(

observation likelihood
(acoustic model)

prior

(language model)

4

language model

language model

l p(words) is the joint id203 that a sequence

of words = w1 w2 ... wn is likely for a specified natural 
language

l this joint id203 can be expressed using the chain rule 

(order reversed):
p (w1 w2     wn ) = p (w1 ) p (w2 |w1 ) p (w3 |w1 w2 ) ... p (wn |w1 ... wn-1 )

l collecting all these probabilities is too complex; it requires 
probabilities for mn-1 starting sequences for a sequence of 
n words in a language of m words
    simplification is necessary!

l first-order markov assumption

    id203 of a word depends only on the previous word: 

p(wi | w1 ... wi-1 )      p(wi | wi-1)

l the lm simplifies to

p (w1 w2     wn) = p (w1 ) p (w2 |w1 ) p (w3 |w2 ) ... p (wn|wn-1 )

    called the bigram model
    relates consecutive pairs of words

language model

language model

l more context could be used, such as the two words before, 

called the trigram model:

p(wi | w1 ... wi-1 )     p(wi | wi-1 wi-2)

l a weighted sum of unigram, bigram, trigram models could 
also be used in combination:
p (w1 w2     wn) =     (c1 p (wi ) + c2 p (wi | wi-1 ) + c3  p (wi | wi-1 wi-2 ))
    local context-sensitive effects
    some local grammar

l bigram and trigram models account for

l e.g., "bag of tricks" vs. "bottle of tricks"

l e.g., "we was" vs. "we were"

l probabilities are obtained by computing the 

frequency of all possible pairs of words in a large 
training set of word strings:
    if "the" appears in training data 10,000 times

and it's followed by "clock" 11 times then
p (clock | the) = 11/10000 = .0011

l these probabilities are stored in

    a id203 table
    a probabilistic finite state machine

5

language model

probabilistic finite state
machine: an almost fully 
connected directed graph:

l nodes: all possible words and 

a start state

l arcs: labeled with a id203

    from start to a word is the prior id203

of the destination word being the first word
    from one word to another is the conditional 

id203 of the destination word following a 
given source word

attack

killer

the

of

language model

probabilistic finite state
machine: an almost fully 
connected directed graph:

   

joint id203 is estimated for the
bigram model by starting at start
and multiplying the probabilities of the
arcs that are traversed for a given
sentence:

tomato

start

attack

killer

the

of

p (   attack of the killer tomato    )

= p (attack ) p (of | attack ) p (the | of ) p (killer | the ) p (tomato | killer )

tomato

start

acoustic model
    p(signal | words ) is the id155 that
a signal is likely given a sequence of spoken words for 
a particular natural language

l this is divided into two probabilities:

    p (phones | word ):  id203 of a sequence of 

phones given word

    p (signal | phones ):  id203 of a sequence of 
vector quantization values from the acoustic signal 
given phones

acoustic model
l p(phones | word ) can be specified and computed 

using a markov model (mm)

l p(signal | phones) can be specified and computed 

using a hidden markov model (id48)

6

finding  patterns
l speech is an example of a more general problem of 

finding patterns over time (or any discrete 
sequence)

l deterministic patterns have a fixed sequence of 

states, where the next state is dependent solely on 
the previous state

european
stop light

modeling a sequence of states

l id110 structure for a sequence of states 

from {red (r), red-amber (ra), green (g), amber (a)}.  
each qi is a random variable indicating the state of the 
stoplight at time i.  

q1=r

q2=ra

q3=g

q4=a

q5=r

   

markov property

the 1st order markov assumption:

state qt+1 is conditionally independent of {qt-1, qt-2,     
q1} given qt.   in other words: 

p(qt+1=sj | qt=si , qt-1=sk ,    , q1=sl )  =  p(qt+1=sj | qt=si )

non-deterministic  patterns
l assume a discrete set of states, but we can   t model 

the sequence deterministically

l example:  predicting the weather
    states:  sunny, cloudy, rainy
    arcs:  id203, called the state transition 

id203, of moving from one state to another
l nth-order markov assumption:  today   s weather 
can be predicted solely given knowledge of the 
last n days    weather

l 1st-order markov assumption:  today   s weather 

can be predicted solely given knowledge of 
yesterday   s weather

7

1st-order  markov  model

l markov process is a process that moves from state to 

state probabilistically based on a state transition 
matrix, a, associated with a graph of the possible states

l 1st-order markov model for weather prediction:

0.25

0.5
0.25
0.25

0.125
0.625
0.375

0.375
0.125
0.375

a

1st-order  markov  model
l to initialize the process, also need the prior 

probabilities of the initial state at time t = 0, called   .  
for example, if we know the first day was sunny, then 
   is a vector =

matrix properties
l sum of values in row = 1 because these are the 

probabilities of all outgoing arcs from a state

l sum of values in a column does not necessarily equal 

1 because this is the sum of probabilities on all 
incoming arcs to a state

.9

.8

.075

.15

.025

.05

.25

.25

.5

1st-order  markov  model

l markov model m = (a,   ) consists of

    discrete set of states, s1, s2,    , sn
       vector, where   i = p(q1=si)
    state transition matrix, a = {aij} 

where aij = p(qt+1=sj | qt=si )

l for simplicity, we will often assume a single, given 

state is the start state

l the state transition matrix is fixed a priori and describes 

probabilities associated with a (completely-connected) 
graph of the states

8

our language model is a markov 
model

example:  using a markov model 
for weather prediction

tomato

start

attack

killer

the

of

a =

   =

0.5
0.25
0.25

0.375
0.125
0.375

0.125
0.625
0.375

given that today is sunny, what is the id203 of the 
next two days being sunny and rainy, respectively?

weather prediction example (cont.)

p(q2= sun, q3=rain | q1=sun) = ?

p(q3=rain | q1=cloudy) = ?

weather prediction example (cont.)
conditionalized chain rule

p(q2= sun, q3=rain | q1=sun)

= p(q3=rain | q2= sun, q1=sun) p(q2= sun | q1=sun)
= p(q3=rain | q2= sun) p(q2= sun | q1=sun)
= (.125)(.5)
= 0.0625

1st order markov assumption
(conditional independence)

p(q3=rain | q1=cloudy)

= p(q2= sun, q3=rain | q1=cloudy)

+ p(q2= cloudy, q3=rain | q1=cloudy)
+ p(q2= rain, q3=rain | q1=cloudy) 

addition rule

9

acoustic model

acoustic model

p(phones |word ) can be specified as a markov model, 
describing the possible phone sequences for pronouncing 
a given word, e.g.,    tomato:   

p(phones |word ) can be specified as a markov model, 
describing the possible phone sequences for pronouncing 
a given word, e.g.,    tomato:    

start

[t]

.2

.8

[ow]

[ah]

1

1

[m]

.6

.4

[ey]

[aa]

1

1

1

[t]

[ow]

l nodes: correspond to the production of a phone

    sound slurring (coarticulation), e,g., due to quickly 

pronouncing a word

    variation in pronunciation of words, e.g., due to dialects

l arcs: id203 of transitioning from current state to another

problem

l the acoustic model also needs to compute 
p(signal | phones) but we don   t know the 
sequence of phones, we only have the observation 
sequence o1, o2, o3,    

l how do we relate the given input observation 
sequence to the    hidden    phone sequences 
associated with a word sequence?

start

[t]

.2

.8

[ow]

[ah]

1

1

[m]

.6

.4

[ey]

[aa]

1

1

1

[t]

[ow]

p(phones | word) is a path through the graph, e.g.,

    p ([towmeytow] | tomato ) = 0.2 * 1 * 0.6 * 1 * 1 = 0.12
    p ([towmaatow] | tomato ) = 0.2 * 1 * 0.4 * 1 * 1 = 0.08
    p ([tahmeytow] | tomato ) = 0.8 * 1 * 0.6 * 1 * 1 = 0.48
    p ([tahmaatow] | tomato ) = 0.8 * 1 * 0.4 * 1 * 1 = 0.32

hidden  markov  models  (id48s)

sometimes the states we want to predict are not directly 
observable; the only observations available are indirect
evidence

l example:  a cs major does not have direct access to 
the weather (sunny, cloudy, rainy) each day, but can 
only observe the condition of each day   s badger 
herald newspaper (dry, dryish, damp, soggy)

l example:  in id103 we can observe 

acoustic features, i.e., o1, o2,    , but there is no direct 
evidence of the words or phones being spoken

10

id48s

l hidden states: the states of real interest, e.g., the 
true weather or the sequence of words or phones 
spoken;  represented as a 1st-order markov model

l observable states:  a discrete set of observable 

values;  the number of observable values is not, in 
general, equal to the number of hidden states.  the 
observable values are related somehow to the hidden 
states (i.e., not 1-to-1 correspondence)

arcs  and  probabilities  in  id48s
l arcs connecting hidden states and observable states 
represent the id203 of generating an observed 
state given that the markov process is in a hidden state

l observation likelihood matrix, b, (aka output 

id203 distribution) stores probabilities associated 
with arcs from hidden states to observable states, i.e., 
p(observed | hidden)

newspaper

encodes semantic 
variations, sensor 
noise, etc.

hidden  markov  model

start

id48  summary

l an id48 contains 2 types of information:

    hidden states:  s1, s2, s3,    
    observable states

    in id103, the vector quantization values in 
the input sequence, o = o1, o2, o3,    

l an id48,    = (a, b,   ), contains 3 sets of 

probabilities: 
       vector,    = (  i)
    state transition matrix, a = (aij) 

where aij = p(qt = si | qt-1 = sj)

    observation likelihood matrix, b = bj(ok) = p(yt = ok | qt = sj)

11

id48  summary

example:  an id48

1st order markov property says observation oi is 
conditionally independent of hidden states qi-1
, qi-2 ,    , q0 given qi

in other words,

p(ot = x | qt = si, qt-1 = sj(t-1),    , q0 = sj(0))

= p(ot = x | qt = si )

example:  id48  acoustic  model

for the word    need   
a24

hidden states: phones

a11

a22

a33

a01

start0

[n]1

a12

[iy]2

a23

b1(o1) b1(o2)

b2(o3) b2(o5)

b2(o4)

observation
sequence:

   

a34

end4

[d]3

b3(o6)

   

o1 o2

o3 o4 o5

o6

acoustic model
p(signal | phone) can be specified as an id48, 
e.g., phone model for [m]:

0.3

0.7

0.9

0.1

mid

onset

0.4

0.6

end

final

possible
outputs:

o1: 0.5
o2: 0.2
o3: 0.3

o3: 0.2
o4: 0.7
o5: 0.1

o4: 0.1
o6: 0.5
o7: 0.4

    nodes: id203 distribution over a set of possible 

output values

    arcs: id203 of transitioning from current hidden 

state to next hidden state

12

generating id48 observations

l choose an initial hidden state, q1 = si , based on   
l for t = 1 to t do

1. choose output/observation state zt = ok

according to the symbol id203 distribution in 
hidden state si,  bi(k)

2. transition to a new hidden state qt+1 = sj

according to the state transition id203 
distribution for state si,  aij

l so, transition to new state using a matrix, and 

then output value at the new state using b matrix

acoustic model
p(signal | phone) can be specified as an id48, 
e.g., phone model for [m]:

0.3

0.7

1

onset

o1: 0.5
o2: 0.2
o3: 0.3

mid

o3: 0.2
o4: 0.7
o5: 0.1

0.9

0.1

0.4

0.6

end

final

o4: 0.1
o6: 0.5
o7: 0.4

l p(signal | phone) is a path through the states, i.e.,

o p([o1,o4,o6] | [m]) =  (1)(.5)(.7)(.7)(.1)(.5)(.6) =  0.00735
o p([o1,o4,o4,o6] | [m]) =  (1)(.5)(.7)(.7)(.9)(.7)(.1)(.5)(.6)

+  (1)(.5)(.7)(.7)(.1)(.1)(.4)(.5)(.6) =  0.0049245

modeling a sequence of states

id110 structure for a sequence of hidden 
states from {r, s, c}.  each qi is a hidden/   latent    random 
variable indicating the state of the weather on day i.  each 
oi is the observed state of the newspaper on day i.

q1=r

q2=s

q3=s

q4=c

q5=r

   

each horizontal arc has a matrix probs.

o1=damp

o2=damp

o3=dry

o4=dryish

o5=soggy

each vertical arc
63
has b matrix probs.

combining models:  create one 
large id48

p(phones | word )

tomato

.5

[ey]

[m]
[m]
.5
0.9

[aa]

1

1

1

1

mid

0.1

end

1

[t]

[ow]

tomato

start

0.4

0.6

final

p (words )

attack

killer

the

of

o3: 0.2
o4: 0.7
o5: 0.1

o4: 0.1
o6: 0.5
o7: 0.4

p(signal | phones)

[ow]

[ah]

0.3

0.7

[t]

.2

.8

onset

o1: 0.5
o2: 0.2
o3: 0.3

13

3 common tasks using id48s

evaluation  problem: p(o |   )

l evaluation problem
l decoding problem
l learning problem

l find the id203 of an observed sequence given an 

id48.  for example, given several id48s such as a 
   summer id48    and a    winter id48    and a sequence of 
newspaper observations, which id48 most likely 
generated the given sequence?

l in id103, use one id48 for each word, 
and an observation sequence from a spoken word.  
compute p(o |   ).  recognize the word by identifying 
the most probable id48.
l use forward algorithm

decoding  problem

learning  problem

l find the most probable sequence (i.e., path) of hidden 

states given an observation sequence

l compute  q* = argmaxq p(q | o)
l use viterbi algorithm

l generate an id48 given a sequence of observations 

and a set of known hidden states

l learn the most probable id48
l compute   * = argmax   p(o |   )
l use forward-backward algorithm or expectation-
maximization (em) algorithm

14

evaluation problem

compute p(o |   ) where o = o1, o2,    , ot is the observation 
sequence and    is an id48 model defined by (  , a, b) 

example:  an id48

let q = q1, q2,    , qt be a specific hidden state sequence

q

by conditioning rule

p(o |   ) =    p(o | q) p(q)
p(o | q) =    p(ot | qt)  = bq1(o1 ) bq2(o2 )     bqt(ot )
p(q) = p(q1)    p(qt | qt-1)  =    q1 aq1q2     aqt-1qt

t=1

t

t

t=2

so, p(o |   ) =      q1 bq1(o1) aqoq1 bq2(o2) aq1q2 bq3(o3)        aqt-1qt bqt(ot)

q

p(q1=happy, q2=sad | id48) = ?

p(q1=h, q2=s | id48) = ?

qhqp
,
(
2

=

1

=

s

)

=

hqphqs

()

=

1

|

qp
(
=
=
2
by chain rule

1

=

14.0)7)(.2(.

=

)

15

p(q2=happy | id48) = ?

p(q3=happy | id48) = ?

hqp
(

=

2

)

=

|

=

hqphqhqp
(
=
2
1
qhqp
s
(
+
1

)
(
1
qps
)
(
=
1
by conditioning rule

=
=

=

|

2

)
)

=

)7)(.8(.

+

62.0)3)(.2(.

=

p(o1=laugh, o2=frown | q1=h, q2=s) = ?

=
=
=

=
=

op
(
=
1
op
(
=
1
1
1.0)5)(.2(.

ol
|
=
2
1
ophql
(
|
2
=

qhqf
,
=

,
)

=
2
qf

|

ops
(
)
=
2
s
  )
by 
=

1

|

qhqf
s
,
=
=
markov
property
 

2

2

  )

by 

chain 

rule

3

1

2

1

)

=
|

=
(

hqhqphqhqhqp
hqp
)
,
(
(
)
|
=
=
=
=
3
qhqhqp
          
    
          
(
,
=
=
=
=
+
3
2
1
 2  
    
 more
     
conditioni
          
          
(by 
+
hqhqphqhqp
(
)
          
        
|
(
=
=
=
=
hqphqhqphqhqp
(
        
)
          
(
|
=
=
=
=
2
2
3
          
          
   
          
          
          
          

(
,
=
qhqps
,
)
=
2
rule)
 ng
)
...
+
)
(
1
1
rule)
chain 

1
cases

|
(by 

=
=

=

s

,

2

1

2

3

2

)

=

)8)(.8)(.7(.

+

)2)(.2)(.7(.

+

)8)(.2)(.3(.

+

)2)(.8)(.3(.

p(o1=laugh, o2=frown | id48) = ?
(evaluation problem)

)

+

...

16

evaluation problem

compute p(o |   ) where o = o1, o2,    , ot is the observation 
sequence and    is an id48 model defined by (  , a, b) 

let q = q1, q2,    , qt be a specific hidden state sequence

q

by conditioning rule

p(o |   ) =    p(o | q) p(q)
p(o | q) =    p(ot | qt)  = bq1(o1 ) bq2(o2 )     bqt(ot )
p(q) = p(q1)    p(qt | qt-1)  =    q1 aq1q2     aqt-1qt

t=1

t

t

t=2

so, p(o |   ) =      q1 bq1(o1) aqoq1 bq2(o2) aq1q2 bq3(o3)        aqt-1qt bqt(ot)

q

= p(o1 = l | o2 = f,q1 = h,q2 = h)
           p(o2 = f | q1 = h,q2 = h)
           p(q1 = h,q2 = h)     +...
                                           by conditionalized chain rule
= p(o1 = l | q1 = h)
         p(o2 = f | q2 = h)
         p(q2 = h | q1 = h)
         p(q1 = h)    +...
                                           by markov property
= p(q1 = h)p(o1 = l | q1 = h)
         p(q2 = h | q1 = h)
         p(o2 = f | q2 = h)    +...
)1)(.8)(.2)(.7(.
=

...

+

p(o1=laugh, o2=frown | id48) = ?

p(o1 = l,o2 = f)
           = p(o1 = l,o2 = f | q1 = h,q2 = h)p(q1 = h,q2 = h)
                       + p(o1 = l,o2 = f | q1 = h,q2 = s)p(q1 = h,q2 = s)
                       + p(o1 = l,o2 = f | q1 = s,q2 = h)p(q1 = s,q2 = h)
                       + p(o1 = l,o2 = f | q1 = s,q2 = s)p(q1 = s,q2 = s)
                                           by conditioning rule

p(o1=l, o2=f, q1=h, q2=s) = ?

=
=

ophql
(
2

)

1

|

=

op
(
=
1
)7)(.2)(.5)(.2(.

=

qf

|

2

=

qhqps
)
1

=

(

|

2

=

hqps
)

=

(

1

)

by chain rule plus markov property

in general:
p(q1,...,qt,o1,...,ot ) = p(q1)

t

   
i=2

p(qi | qi   1)

t

   
i=1

p(oi | qi)

17

p(q1=s | o1=f) = ?
(state estimation problem)

needed as part of solving the    decoding problem   

most probable path problem:  find q1, q2 such that 

p(q1 = x, q2 = y | o1 = l, o2 = f) is a maximum
possible values of x and y, and give the 
x and y

over all 
values of 

p(q3=h | o1=f, o2=l, o3=y) = ?
(decoding problem)

p(q1=s | o1=f) = ?

qp
(
1

=

os
1

|

=

f

)

=

op
(
1

=

=

=

op
(
1

|

qf
=
1
)3)(.5(.

=

qps
(
)
1

=

)3)(.5(.

+

)7)(.1(.

)

qf
qps
|
)
(
=
1
1
f
op
(
=
1
(.5)(.3)
op
s
(
)
1

=

+

|

=

s

)

   

by 

bayes'

rule
 s

hqphqf

=

=

(

)

)

1

1

conditioning rule

p(q3=h | o1=f, o2=l, o3=y) = ?

addition rule

ohqp
(
1

=

|

3

=

of
,
2

=

ol
,
3

=

y

)

3

2

,

,

hqhqhqp
(
=
=
1
qhqhqp
s
(
,
,
+
=
1
qhqp
hqs
(
,
,
+
=
1
2
qs
s
qhqp
,
(
,
+
=
1

=
=
=
=

=
=
=
=

2

3

3

2

3

|
|
|
|

...)
...)
...)
...)

where
,
=
2
of
,
=
2

,
op
(
1

3

=

ohqhqhqp
(
1
=

=
1
ol
,
=
3

|

        
=

=

=
hqhqhqphqhqhqy

=

=

=

(

,

,

)

of
,
2
|
=
1
op
(
1

ol
,
=
3
,
=
2
of
,
=
2

)

y
,
=
3
ol
,
3

=

)
y

)

=

3

2

1

          

          

        

by 

bayes'
rule
 s

op
(
1

=

=

ophqf
(
2

=

)

|

1

=

2

|

)

=

ophql
(
=
3
op
(
1

hqphqhqphqhqphqy
|
=

=
3
y
=

)
(
ol
,
3

=
3
of
,
2

=

=

=

=

=

)

(

)

(

)

|

|

1

1

2

2

)

18

how to solve id48 problems 
efficiently?

computation of p(o |   )

evaluation  using  exhaustive  search

given observation sequence (dry, damp, 
soggy),    unroll    the hidden state sequence as a 
   trellis    (matrix):

l since there are nt sequences (n hidden states 

and t observations), o(t nt ) calculations 
required!

l for n = 5, t = 100      1072 computations!

evaluation  using  exhaustive  search

l each column in the trellis (matrix) shows a possible 

state of the weather on a given day

l each state in a column is connected to each state in the 

adjacent columns

l sum the probabilities of each possible sequence of the 

hidden states;  here, 33 = 27 possible weather 
sequences

l p(dry,damp,soggy | id48   ) =

p(dry,damp,soggy | sunny,sunny,sunny) 

+ p(dry,damp,soggy | sunny,sunny,cloudy)
+        + p(dry,damp,soggy | rainy,rainy,rainy)

l not practical since the number of paths is o(nt) where

n is the number of hidden states and t is number of 
observations

19

forward algorithm intuition
l idea: compute and cache values   t(i) representing 

id203 of being in state i after seeing first t
observations, o1, o2, ..., ot

l each cell expresses the id203  

  t(i) = p(qt=i | o1, o2, ..., ot)

l qt = i means "the id203 that the t th state in the 

sequence of hidden states is state i

l compute    by summing over extensions of all paths 

leading to current cell

l an extension of a path from state i at time t-1 to state j

at t is computed by multiplying together:
i. previous path id203 from the previous cell   t-1(i)
ii. transition id203 aij from previous state i to 
current state j
iii. observation likelihood bjt that current state j
matches observation symbol t

forward algorithm

  t-1(i)

if there are multiple states leading to state j      

p(ot, j |   ) =    all i p(ot-1, i |   ) ai,j bjt
i.e., sum the probabilities

a1,j

a2,j

j

ot

1

2

ot-1

forward  algorithm

p(o, qt=sj |   ) = sum of all possible paths through 
the trellis

decoding  problem

l most probable sequence of hidden states is the state 

sequence q that maximizes p(o,q |   )

l similar to the forward algorithm except uses max

instead of sum, thereby computing the id203 of 
the most probable path to each state in the trellis

o(n2t) computation time  (i.e., linear in t, the length of 
the sequence)

20

decoding  using  viterbi  algorithm

l for each state qi and time t, compute recursively   t(i) = 
maximum id203 of all sequences ending at state 
qi and time t, and the best path to that state

l exploits the id145 invariant: 
if the ultimate best path for o includes state qi
then it includes the best path up to and including qi

an alternative approach:

deep learning id98 for id103

summary

l id103 systems work best if

    high snr (low noise and background sounds)
    small vocabulary
    good language model
    pauses between words
    trained to a specific speaker

l current systems

    vocabulary of ~200,000 words for single speaker
    vocabulary of ~5,000 words for multiple speakers
    accuracy depends on the task
       deep learning    methods now state-of-the-art

id98 results
t. sainath et al., deep convolutional neural networks 
for lvcsr, proc. icassp, 2013

word 
error 
rate

spectrogram

21

id48s vs id98s

training set

test set

microsoft on deep learning for speech 

recognition (now used in skype)

wer (word error rate) on 400 hours of english broadcast news

dnn = feedforward nn with 5 hidden layers, each with 1024 units
id98 with 2 convolutional layers and 4 fully connected layers

rick rashid, microsoft, 2012

how siri works

other applications of id48s

apple   s siri personal assistant consists of:

1. id103

limited vocabulary

2. grammar analysis

search for key phrases and use them to build a simple model of 
what user wants to do; integrated with other info on phone such 
as address book, nicknames, birthdays, gps

3. web service providers

tools for mapping to external apis for yelp, zagat, wikipedia

limited domains:  restaurants, sports, movies, 
travel, weather,    

l probabilistic robotics

    slam:  simultaneous localization and mapping
    robot control learning

l tracking objects in video
l spam deobfuscation (mis-spelling words)
l bioinformatics
l consumer decision modeling
l economics & finance
l and many more    

22

application:  tracking objects in 
video

l track the (hidden) state, xt, of a system as it changes 

over time, t

l given:  a sequence of noisy observations (i.e., 

features), zt

l goal:  compute best estimate of the hidden variables 

specifying the state, xt, of the system at each time 
step, t.  that is, compute

argmax p(xt | zt)

xt

the decoding problem! 

application:  tracking objects in video

p(zk | xk)

hidden

p(xk | xk-1)

1st order markov assumption: zk is 
conditionally independent of z1, ..., zk-1 given xk

stochastic approach:  monte carlo 
samples (particles)
l p(x | z) is often difficult or impossible to compute 

in closed form because of noise or complexity
l an alternative is to approximate p(x | z) using 

many discrete samples (aka    particles   ):
    each particle has a state value and a weight
    next state is particle with largest weight

weighted according 
to the likelihood of 
the particle

x

x

[ http://www.fulton.asu.edu/~morrell/581/ ]

23

approximate id136 by sampling

l id136 can be done approximately by sampling
l general sampling approach:

    generate many, many samples (each sample is a 

complete assignment of all variables)

    count the fraction of samples matching query and 

evidence

    as the number of samples approaches    , the 

fraction converges to the posterior: 

p(x | z)

id136 with simple sampling

l say we want to infer b, given e and m, i.e., p(b | e, m)
l we generate tons of samples
l keep those samples with e=true and m=true, and 

throw away the others

l for the ones we keep (n of them), count the ones with 

b=true, i.e., those that fit our query (n1)
b

l we return an estimate of 

p(b | e, m)     n1 / n

l the quality of this estimate improves 

as we sample more and more

a

j

e

m

id143s

monte carlo methods

also known as:
l sequential monte 

carlo filter

l condensation
l survival of the 

l bootstrap filter
    sampling-based approach rather than trying to 

fittest

calculate the exact posterior

    represent belief by a set of random samples, 
which approximates the posterior distribution

    approximate solution to an exact model (rather 
than an optimal solution to an approximate model)

l a class of numerical methods involving statistical 

sampling processes for finding approximate solutions to 
quantitative problems
l in 1864 o. fox estimated    by dropping a needle onto a 
l s. ulam used computers in 1940s to automate the 
statistical sampling process for developing nuclear 
weapons at los alamos

ruled board

l s. ulam, j. von neuman, and n. metropolis developed 
algorithms to convert non-random problems into random 
forms so that statistical sampling can be used for their 
solution; they named them    monte carlo    methods after 
the casino

24

tracking in video using a id143 
algorithm to estimate object   s state

algorithm

1. select: randomly select n particles from {st-1
(n); same particle may be 

based on weights   t-1
picked multiple times (factored sampling)

(n)} 

2. predict: move particles (i.e., update states) 

according to a deterministic motion model 
(drift), then perturb them individually (diffuse)

state samples

from isard & blake, 1998

mean of weighted 

state samples

3. measure: get a likelihood for each new sample 

by comparing it with the image   s local 
appearance, i.e., based on p(zt | xt); then 
update weight accordingly to obtain {(st

(n),   t

(n))}

dancing example

hand example

25

pointing hand example

glasses example
    6d state space of affine transformations of a spline curve
    edge detector applied along normals to the spline
    autoregressive motion model

3d model-based example

l 3d state space: image position + angle
l polyhedral model of object

26

