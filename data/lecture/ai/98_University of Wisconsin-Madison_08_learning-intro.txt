machine learning:
introduction and

unsupervised learning

what is learning?

       learning is making useful changes in our 

minds                       marvin minsky

       learning is constructing or modifying 

representations of what is being 
experienced   

    ryszard michalski

chapter 18.1, 18.2, 18.8.1

and    introduction to statistical machine learning   

       learning denotes changes in a system that 
... enable a system to do the same task more 
efficiently the next time             herbert simon

why do machine learning?

    solve classification problems
    learn models of data (   data fitting   )
    understand and improve efficiency of human 

learning (e.g., computer-aided instruction 
(cai))

    discover new things or structures that are 

unknown to humans (   data mining   )

    fill in skeletal or incomplete specifications

about a domain

major paradigms of machine learning
    rote learning
    induction
    id91
    discovery
    id107
    id23
    id21
    learning by analogy
    id72

1

inductive learning

    generalize from a given set of (training) 

examples so that accurate predictions can be 
made about future examples

    learn unknown function: f(x) = y
    x: an input example (aka instance)
    y: the desired output

    discrete or continuous scalar value

    h (hypothesis) function is learned that 
approximates  f

representing    things    in machine learning
    an example or instance, x, represents a specific 

object ((cid:1)thing(cid:2))

vector x = (x1, . . . , xd)    rd

    x often represented by a d-dimensional feature 

    each dimension is called a feature or attribute
    continuous or discrete
    x is a point in the d-dimensional feature space
    abstraction of object.  ignores all other aspects 
(e.g., two people having the same weight and 
height may be considered identical)

feature vector representation

types of features

    preprocess raw data

    extract a feature (attribute) vector, x, that describes 

all attributes relevant for an object

    each x is a list of (attribute, value) pairs

x = [(rank, queen), (suit, hearts), (size, big)] 
    number of attributes is fixed: rank, suit, size
    number of possible values for each attribute is fixed 

(if discrete)
rank:  2,    , 10, jack,  queen,  king,  ace
suit:    diamonds,  hearts,  clubs,  spades
size:    big,  small

    numerical feature has discrete or continuous values 

that are measurements, e.g., a person   s weight
    categorical feature is one that has two or more 

values (categories), but there is no intrinsic ordering 
of the values, e.g., a person   s religion  (aka nominal
feature)

    ordinal feature is similar to a categorical feature but 

there is a clear ordering of the values, 
e.g., economic status, with three values: low, 
medium and high 

2

feature vector representation

feature vector representation example

each example can be interpreted as a point in
a d-dimensional feature space, where d is the 
number of features/attributes

    text document

    vocabulary of size d (~100,000): aardvark,     , 

zulu

       bag of words   :  counts of each vocabulary entry

suit

spades
clubs
hearts
diamonds

2

4

6

8 10 j q k

rank

    to marry my true love    (3531:1 13788:1 19676:1)
    i wish that i find my soulmate this year    (3819:1 13448:1 19450:1 

20514:1)

    often remove    stopwords:     the, of, at, in,    
    special    out-of-vocabulary    (oov) entry catches all 

unknown words

more feature representations
    image

    color histogram

    software

executed

    bank account

    execution profile: the number of times each line is 

    credit rating, balance, #deposits in last day, week, 

month, year, #withdrawals,    

    bioinformatics

    medical test1, test2, test3,    

training set

    a training set (aka training sample) is a collection 
of examples (aka instances), x1, . . . , xn, which is 
the input to the learning process

    xi = (xi1, . . . , xid)
    assume these instances are all sampled 
independently from the same, unknown
(population) distribution, p(x)

    we denote this by xi   p(x), where i.i.d. stands 

for independent and identically distributed

i.i.d.

    example:  repeated throws of dice 

3

training set

    a training set is the    experience    given to a 

learning algorithm

    what the algorithm can learn from it varies
    two basic learning paradigms:

    unsupervised learning
    supervised learning

inductive learning

    supervised vs. unsupervised learning

    supervised: "teacher" gives a set of (x, y) pairs

    training examples have known outcomes

    unsupervised: only the x   s are given

    training examples have unknown outcomes

    in either case, the goal is to estimate  f so that 

it generalizes well to    correctly    deal with 
   future examples    in computing  f(x) = y
    that is, find f that minimizes some measure of the 

error over a set of samples

unsupervised learning

unsupervised learning overview

    training set is  x1, . . . , xn, that   s it!
    no (cid:1)teacher(cid:2) providing supervision as to how 

individual examples should be handled

    common tasks:

    id91:  separate the n examples into groups
    discovery:  find hidden or unknown patterns
    novelty detection:  find examples that are very 

different from the rest

    id84:  represent each example 

with a lower dimensional feature vector while 
maintaining key characteristics of the training samples

unlabeled data 

(no answers)

fit

structure

new 

unlabeled 

+

data 

model

predict

map new 
data to 
structure

slide by intel software

4

id91

    goal:  group training sample into clusters 
such that examples in the same cluster are 
similar, and examples in different clusters 
are different

    how many clusters do you see?
    many id91 algorithms

id96

+

model

fit

model

+

model

predict

predict similar 

articles

text articles 
of unknown 

topics

text articles 
of unknown 

topics

slide by intel software

oranges and lemons

(from iain murray http://homepages.inf.ed.ac.uk/imurray2/ )

google news

5

digital photo collections

    you have 1000s of digital photos stored in various 

folders

    organize them better by grouping into clusters

    simplest idea:  use image creation time (exif tag)
    more complicated:  extract image features

histogram-based image segmentation

    goal:  segment the image into k regions

    reduce the number of gray levels to k and map 

each pixel to the closest gray level

histogram-based image segmentation

three frequently used id91 methods

    goal:  segment the image into k regions

    reduce the number of gray levels to k and map 

each pixel to the closest gray level

    hierarchical agglomerative id91

    build a binary tree over the dataset by 

repeatedly merging clusters

    id116 id91

    specify the desired number of clusters and 

use an iterative algorithm to find them

    mean shift id91

6

hierarchical agglomerative id91
    initially every point is in its own cluster

hierarchical agglomerative id91
    find the pair of clusters that are the closest to 
each other

hierarchical agglomerative id91
    merge the two into a single cluster

hierarchical agglomerative id91
    repeat    

7

hierarchical agglomerative id91
    repeat    

hierarchical agglomerative id91
    repeat     until the whole dataset is one giant cluster
    you get a binary tree (not shown here)

hierarchical agglomerative id91 

algorithm

hierarchical agglomerative id91
how do you measure the closeness between 
two clusters?  
at least three ways:

    single-linkage:  the shortest distance from any 
member of one cluster to any member of the 
other cluster

    complete-linkage:  the largest distance from any 

member of one cluster to any member of the 
other cluster

    average-linkage:  the average distance between 

all pairs of members, one from each cluster

8

hierarchical linkage types

single linkage: minimum pairwise distance between clusters

hierarchical linkage types

complete linkage: maximum pairwise distance between clusters

income

income

age

age

slide by intel software

slide by intel software

hierarchical linkage types

average linkage: average pairwise distance between clusters

income

age

distance metric choice

    choice of distance metric is 

extremely important to 
id91 success

    each metric has strengths 
and most appropriate use 
cases

    but sometimes choice of 

distance metric is also based 
on empirical evaluation

slide by intel software

slide by intel software

9

distance

    how to measure the distance between a pair 
of examples, x = (x1,    , xn) and y = (y1,    , yn)?
    euclidean

d(x,y) =

(
   
    manhattan / city-block
   

d(x,y) =

i

i

    hamming

xi     yi

)2

xi     yi

    number of features that are different between the two 

examples

    and many others

hierarchical agglomerative id91 

example

    6 italian cities
    single-linkage

example created by matteo matteucci

hierarchical agglomerative id91

    the binary tree you get is often called a 

dendrogram, or taxonomy, or a hierarchy of 
data points

    the tree can be cut at any level to produce 
different numbers of clusters: if you want k
clusters, just cut the (k-1) longest links

iteration 1:  merge mi and to

recompute min
distance from 
mi/to cluster to 
all other cities

10

iteration 2:  merge na and rm

iteration 3:  merge ba and na/rm 

iteration 4:  merge fi and ba/na/rm

final dendrogram

11

issues

    when to stop / how many clusters?

    what if there are different ranges for the 

possible values of each feature? 

    how to measure distance for categorical 

features? 

    what if features are not of equal importance?

hierarchical agglomerative 

id91 applet

http://home.dei.polimi.it/matteucc/id91/
tutorial_html/appleth.html

what factors affect the outcome of 
hierarchical agglomerative 
id91?

    features used
    range of values for each feature
    linkage method
    distance metric used
    weight of each feature

agglomerative id91 stopping criteria

method 1

the correct number of clusters is 
reached

method 2

minimum average intra-cluster 
distance is greater than a threshold

slide by intel software

12

three frequently used id91 

methods

    hierarchical agglomerative id91

    build a binary tree over the dataset

    id116 id91

    specify the desired number of clusters and 

use an iterative algorithm to find them

    mean shift id91

id116 id91
    suppose i tell you the cluster centers, ci

    q:  how to determine which points to associate with each ci?
    a:  for each point x, choose closest ci

    suppose i tell you the points in each cluster

    q:  how to determine the cluster centers?
    a:   choose ci to be the mean / centroid of all points in 

the cluster

id116 algorithm

id116 algorithm

k = 2 (find two 
clusters)

income

k = 2, randomly 
assign cluster 
centers

income

age

age

slide by intel software

slide by intel software

13

id116 algorithm

id116 algorithm

k = 2, each point 
belongs to 
closest center

income

k = 2, move each 
center to cluster's 
mean

income

age

age

slide by intel software

slide by intel software

id116 algorithm

id116 algorithm

k = 2, each point 
belongs to 
closest center

income

k = 2, move each 
center to cluster's 
mean

income

age

age

slide by intel software

slide by intel software

14

id116 algorithm

id116 algorithm

k = 2, points 
don't change   
converged

income

k = 2, each point 
belongs to closest 
center

income

age

age

slide by intel software

slide by intel software

id116 algorithm

id116 algorithm

k = 3

income

k = 3, results 
depend on initial 
cluster 
assignment

income

age

age

slide by intel software

slide by intel software

15

which model is the best one?

income

age

    distortion:  sum of squared distance from 

which model is the best one?

each point (!") to its cluster (#$)
((!"   #$),
%"&'

    smaller value corresponds to tighter clusters

    other metrics can also be used

slide by intel software

slide by intel software

which model is the best one?

which model is the best one?

run multiple 
times, and
take model with 
the best score

income

distortion = 12.645

income

age

age

slide by intel software

slide by intel software

16

which model is the best one?

which model is the best one?

distortion = 12.943

distortion = 13.112 

income

income

age

age

slide by intel software

slide by intel software

id116 algorithm

id116 demo

    input:  x1,    , xn, k   where each xi is a point in a d-

dimensional feature space

    step 1: select k cluster centers, c1 ,   , ck
    step 2: for each point xi, determine its cluster:  find 

the closest center (using, say, euclidean distance)
    step 3: update all cluster centers as the centroids

1

ci =

num_ pts_in_cluster_i

x

   
x    cluster i

    repeat steps 2 and 3 until cluster centers no longer 

change

    http://home.dei.polimi.it/matteucc/id91

/tutorial_html/appletkm.html

17

example:  image segmentation

input image

clusters on intensity

clusters on color

id116 properties

    will it always terminate?

    yes (finite number of ways of partitioning 
a finite number of points into k groups)
    is it guaranteed to find an    optimal    

id91?
    no, but each iteration will reduce the 
distortion of the id91

non-optimal id91

say k=3 and you are given the following points:

non-optimal id91
given a poor choice of the initial cluster 
centers, the following result is possible:

copyright    2001, 2004, 
andrew w. moore

copyright    2001, 2004, 
andrew w. moore

18

picking starting cluster centers

smarter initialization of id116 clusters

which local optimum id116 goes to is 
determined solely by the starting cluster centers

    idea 1:  run id116 multiple times with 

different starting, random cluster centers (hill 
climbing with random restarts)

    idea 2: pick a random point x1 from the dataset

1. find a point x2 far from x1 in the dataset
2. find x3 far from both x1 and x2
3.     pick k points like this, and use them as the 

starting cluster centers for the k clusters

income

age

slide by intel software

smarter initialization of id116 clusters

smarter initialization of id116 clusters

pick one point at 
random as initial 
point

income

pick next point 
by weighting 
each by 
1/distance2

income

age

age

slide by intel software

slide by intel software

19

smarter initialization of id116 clusters

smarter initialization of id116 clusters

pick next point by 
weighting each 
by     1/distance2

income

pick next point by 
weighting each 
by     1/distance2

income

age

age

slide by intel software

slide by intel software

smarter initialization of id116 clusters

picking the number of clusters

assign clusters

income

    difficult problem
    heuristic approaches depend on the number 

of points and the number of dimensions

age

slide by intel software

20

picking the number of clusters

    sometimes the problem has a known k

    id91 similar jobs on 4 cpu cores (k = 4)

    a clothing design in 10 different sizes to cover 

most people (k = 10)

    a navigation interface for browsing scientific

papers with 20 disciplines (k = 20)

slide by intel software

how to pick the number of clusters, k?
try multiple values of k and pick the one at the 
   elbow    of the distortion curve

 
 
 
 
 
 
 
 
 
 
 
 
 

n
o
i
t
r
o
t
s
i
d

number of clusters, k

measuring cluster quality

    distortion = sum of squared distances of each 

data point to its cluster center:

    the    optimal    id91 is the one that 

minimizes distortion (over all possible cluster 
center locations and assignment of points to 
clusters)

uses of id116

    often used as an exploratory data analysis tool
    in one-dimension, a good way to quantize real-

valued variables into k non-uniform buckets

    used on acoustic data in id103 to 

convert waveforms into one of k categories 
(known as vector quantization)

    also used for choosing color palettes on 

graphical display devices

21

three frequently used id91 

methods

    hierarchical agglomerative id91

    build a binary tree over the dataset

    id116 id91

    specify the desired number of clusters and 

use an iterative algorithm to find them

    mean shift id91

mean shift id91

1. choose a search window size
2. choose the initial location of the search window
3. compute the mean location (centroid of the data) in the search 

4. center the search window at the mean location computed in 

window

step 3

5. repeat steps 3 and 4 until convergence

the mean shift 
algorithm seeks 
the mode, i.e., 
point of highest 
density of a data 
distribution:

intuitive description

intuitive description

region of
interest

centroid

mean shift

vector

objective : find the densest region

distribution of identical points

region of
interest

centroid

objective : find the densest region

distribution of identical points

mean shift

vector

22

intuitive description

intuitive description

region of
interest

centroid

region of
interest

centroid

objective : find the densest region

distribution of identical points

mean shift

vector

objective : find the densest region

distribution of identical points

mean shift

vector

intuitive description

intuitive description

region of
interest

centroid

region of
interest

centroid

objective : find the densest region

distribution of identical points

mean shift

vector

objective : find the densest region

distribution of identical points

mean shift

vector

23

intuitive description

region of
interest

centroid

results

objective : find the densest region

distribution of identical points

feature space is only 
gray level

results

results

24

supervised learning

examples:  (x1, y1), . . . , (xn, yn)

    a labeled training sample is a collection of 

    assume (xi, yi)    p(x, y) and p(x, y) is 

i.i.d.

unknown

    supervised learning learns a function h: x     y in 
some function family, h, such that h(x) predicts 
the true label y on future data, x, where

i.i.d.

(x, y)   p(x, y)

    classification:  if y discrete
    regression:  if y continuous

labels 

    examples

    predict gender (m, f) from weight, height
    predict adult, juvenile (a, j) from weight, height

    a label y is the desired prediction for an 

instance x

    discrete labels: classes

    m, f;  a, j: often encode as 0, 1 or -1, 1
    multiple classes: 1, 2, 3,    , c.  no class order 

implied.

    continuous label: e.g., blood pressure

concept learning

example:  mushroom classification

    determine if a given example is or is not an 

instance of the concept/class/category
    if it is, call it a positive example
    if not, called it a negative example

edible or 
poisonous?

http://www.usask.ca/biology/fungi/

25

mushroom features/attributes
1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, 

sunken=s 

2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s 
3. cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, 

pink=p, purple=u, red=e, white=w, yellow=y 

4. bruises?: bruises=t, no=f 
5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, 

musty=m, none=n, pungent=p, spicy=s 

6. gill-attachment: attached=a, descending=d, free=f, 

notched=n

7.     

classes:  edible=e, poisonous=p

supervised learning methods

    k-nearest-neighbors (id92) 

(chapter 18.8.1)

    id90
    neural networks (nn)
    support vector machines (id166)
    etc.

supervised concept learning by 

induction

    given a training set of positive and negative 

examples of a concept:
    {(x1, y1), (x2, y2), ..., (xn, yn)}
where each yi is either + or    

    construct a description that accurately classifies 

whether future examples are positive or 
negative:
    h(xn+1) = yn+1

where yn+1 is the + or     prediction

inductive learning

by nearest-neighbor classification

a simple approach:

    save each training example as a point in 
feature space
    classify a new example by giving it the same 
classification as its nearest neighbor in 
feature space

26

k-nearest-neighbors (id92)

k-nearest neighbors classification

    1-nn:

decision boundary

survived
did not survive

age

60

40

20

0

10

20
number of malignant nodes

slide by intel software

k-nearest neighbors classification

k-nearest neighbors classification

60

40

20

age

predict

neighbor count (k = 1):
60

0

1

age

40

20

predict

0

10

20
number of malignant nodes

0

20
number of malignant nodes

10

slide by intel software

slide by intel software

27

k-nearest neighbors classification

k-nearest neighbors classification

neighbor count (k = 2):
60

1

1

age

40

20

predict

neighbor count (k = 3):
60

2

1

age

predict

40

20

0

10

20
number of malignant nodes

0

10

20
number of malignant nodes

slide by intel software

slide by intel software

k nearest neighbors classification
neighbor count (k = 4):
60

3

1

age

predict

40

20

0

20
number of malignant nodes

10

slide by intel software

id92

    what if we want regression?

    instead of majority vote, take average of 
neighbors    y values

    how to pick k?

    split data into training and tuning sets
    classify tuning set with different values of k
    pick the k that produces the smallest 
tuning-set error

28

k-nearest neighbors decision boundary

k-nearest neighbors decision boundary

k = 1

60

40

20

age

k = all

60

40

20

age

0

10

20
number of malignant nodes

0

10

20
number of malignant nodes

slide by intel software

slide by intel software

value of k affects decision boundary

value of k affects decision boundary

k = 1

k = all

k = 1

k = all

60

40

20

age

60

40

20

60

40

20

age

60

40

20

0

10

20

0

10

20

number of malignant nodes

number of malignant nodes

0

10

20
number of malignant nodes

0

10

20

number of malignant nodes

slide by intel software

slide by intel software

29

multiclass id92 decision boundary

regression with id92

k = 5

full remission
partial remission
did not survive
age

60

40

20

0

10

20
number of malignant nodes

k = 20

k = 3

k = 1

slide by intel software

slide by intel software

characteristics of a id92 model

    fast to create model because it simply 
stores the data (the training data is the 
model)

    slow to classify a test example because 

many distance calculations required

    requires lots of memory if data set is large

slide by intel software

characteristics of a id92 model
    doesn't generalize well if the examples in 

each class are not well "clustered"

suit

spades
clubs
hearts
diamonds

2

4

6

8 10 j q k

rank

30

id92 demo

inductive bias

http://www.cs.cmu.edu/~zhuxj/courseproject/
knndemo/knn.html

    inductive learning is an inherently conjectural 

process. why?
    any knowledge created by generalization from 

specific facts cannot be proven true

    it can only be proven false

    hence, inductive id136 is    falsity 

preserving,    not    truth preserving   

inductive bias

inductive bias

    learning can be viewed as searching the 

hypothesis space h of possible h functions 

    inductive bias

    is used when one h is chosen over another
    is needed to generalize beyond the specific 

training examples

    completely unbiased inductive algorithm

    only memorizes training examples
    can't predict anything about unseen examples

biases commonly used in machine learning:

    restricted hypothesis space bias:
allow only certain types of h   s, not arbitrary 
ones
    preference bias:
define a metric for comparing h   s so as to
determine whether one is better than 
another 

31

