id107

chapter 4.1.4

introduction to id107
    another local search method
    inspired by natural evolution

living things evolved into more successful organisms
    offspring exhibit some traits of each parent

introduction to id107
    keep a population of individuals that are 

complete solutions (or partial solutions)
    explore solution space by having these 

individuals    interact    and    compete   
    interaction produces new individuals
    competition eliminates weak individuals

    after multiple generations a strong individual 

(i.e., solution) should be found

    (cid:1)simulated evolution(cid:2) via a form of 

randomized id125

introduction to id107
    mechanisms of evolutionary change:
   crossover (alteration):  the (random) 
combination of 2 parents    chromosomes during 
reproduction resulting in
offspring that have some traits of each parent

    crossover requires genetic diversity among

the parents to ensure sufficiently varied 
offspring

1

introduction to id107
    mechanisms of evolutionary change:

   mutation:  the rare occurrence of errors during
the process of copying chromosomes resulting in

    changes that are nonsensical or deadly,
producing organisms that can't survive
    changes that are beneficial, producing 
"stronger" organisms
    changes that aren't harmful or beneficial,
producing organisms that aren't improved

representation of individuals

solutions represented as a vector of values

    for example:  satisfiability problem (sat)

(p1   p2)   (p1      p3)   (p1      p4)   (  p3      p4)

    determine if a statement in id118 is 
satisfiable, for example:
    each element corresponds to a symbol having
a value of either true (i.e., 1) or false (i.e., 0)
    vector: p1 p2 p3 p4
    values: 1  0 1 1      rep. of 1 individual

    traveling salesperson problem (tsp)

    tour can be represented as a sequence of cities visited

introduction to id107
    mechanisms of evolutionary change:

   natural selection:  the fittest survive in a 
competitive environment resulting in better 
organisms
    individuals with better survival traits
generally survive for a longer period of time
    this provides a better chance for reproducing
and passing the successful traits on to offspring
    over many generations the species improves
since better traits will out-number weaker ones

genetic algorithm

create initial random population

evaluate fitness of each individual

termination criterion satisfied?

no

select parents according to fitness

combine parents to generate offspring

mutate offspring

replace population by new offspring

yes

stop

2

genetic algorithm (1 version*)

1. let s = {s1,    , sn} be the current population
2. let p[i] = f(si)/sumjf(sj) be the fitness probabilities
3. for k = 1;  k < n;  k += 2

    parent1 = randomly pick si with prob. p[i]
    parent2 = randomly pick another sj with prob. p[j]
    randomly select 1 crossover point, and swap 

strings of parents 1 and 2 to generate two children 
t[k] and t[k+1]

4. for k = 1;  k     n;  k++

    randomly mutate each position in t[k] with a 

small id203

5. new generation replaces old generation:  s = t

initialization:  seeding the population

    initialization sets the beginning population
of individuals from which future generations
are produced

    issues:

    size of the initial population

    experimentally determined for problem
    diversity of the initial population (genetic 

diversity)
    a problem resulting from lack of diversity is premature 

convergence to a non-optimal solution

*different than in book

initialization:  seeding the population
    how is a diverse initial population generated?

    uniformly random: generate individuals

randomly from a solution space with uniform distribution

    grid initialization: choose individuals

at regular "intervals" from the solution space

    non-id91: require individuals to be a predefined 

"distance" away from those already in the population
    local optimization: use another technique (e.g. hc)

to find initial population of local optima; doesn't ensure 
diversity but guarantees solution to be no worse than the 
local optima

evaluation:  ranking by fitness
    evaluation ranks the individuals using some
fitness measure that corresponds with the
quality of the individual solutions

    for example, given individual i:
(#correct(i))2
1/tour-length(i)
#ofclausessatisfied(i)

    classification:
    tsp:  
    sat:  

3

selection: finding the fittest

selection techniques

    choose which individuals survive and possibly 
    selection depends on the evaluation/fitness 

reproduce in the next generation

function
    if too dependent, then, like greedy search, a non-

optimal solution may be found

    if not dependent enough, then may not converge to 

a solution at all

    nature doesn't eliminate all "unfit" genes;

they usually become recessive for a long period
of time, and then may mutate to something 
useful

    deterministic selection

    relies on evaluation/fitness function
    converges fast

    two approaches:

    next generation contains parents and children

    parents are the best of the current generation
    parents produce children, and parents survive to next 

generation

    next generation contains only children

    parents are the best of the current generation
    parents are used to produce children
    parents don't survive (counters early convergence)

selection techniques

selection techniques

    proportional fitness selection

    each individual is selected proportionally to their 

fitness score

    even the worst individual has a chance to survive
    helps prevent    stagnation    in the population

    two approaches:

    rank selection: individual selected with a 

id203 proportional to its rank in population 
sorted by fitness
id203:   fitness(individual) /     fitness for 
all individuals

    proportional selection: individual selected with a 

proportional selection example:
    given the following fitness values for population:
l sum all the fitnesses
5 + 20 + 11 + 8 + 6 = 50
l determine id203 

individual fitness
a
b
c
d
e

5
20
11
8
6

prob.
10%
40%
22%
16%
12%

for each individual, i
fitness(i) / 50

4

selection techniques

crowding:   a potential problem associated with 
selection

    occurs when the individuals that are most-fit
quickly reproduce so that a large percentage
of the entire population looks very similar

    reduces diversity in the population
    may hinder the long-run progress of the algorithm

crossover:  producing new individuals

    1-point crossover

    pick a dividing point in the parents' vectors

and swap their segments

    example

    given parents:  1101101101 and  0001001000
    crossover point: after the 4th digit
    children produced are:

1101 + 001000 and 0001 + 101101

crossover:  producing new individuals
    crossover is used to produce new 

individuals (i.e., children)

    crossover for vector representations:

    pick pairs of individuals as parents and randomly 

swap their segments

    also known as "cut and splice"

    parameters:

    number of crossover points
    positions of the crossover points

crossover:  producing new individuals
    n-point crossover

    generalization of 1-point crossover
    pick n dividing points in the parents' vectors and 

splice together alternating segments

    uniform crossover

    the value of each element of the vector is 
randomly chosen from the values in the 
corresponding elements of the two parents

    techniques also exist for permutation 

representations

5

producing new individuals

    mutation

    randomly change an individual
    e.g. tsp: two-swap, two-interchange
    e.g. sat: bit flip

    parameters:

    mutation rate
    size of the mutation

genetic algorithm (1 version*)

1. let s = {s1,    , sn} be the current population
2. let p[i] = f(si)/sumjf(sj) be the fitness probabilities
3. for k = 1;  k < n;  k += 2

    parent1 = randomly pick si with prob. p[i]
    parent2 = randomly pick another sj with prob. p[j]
    randomly select 1 crossover point, and swap 

strings of parents 1 and 2 to generate children t[k] 
and t[k+1]

4. for k = 1;  k     n;  k++

    randomly mutate each position in t[k] with a 

small prob.

5. new generation replaces old generation:  s = t

ga solving tsp

genetic algorithm applications

*different than in book

6

id107 as search

id107 as search

the problem of local maxima

individuals get stuck at pretty good, but not 
optimal, solutions
    any small mutation gives worse fitness
    crossover can help get out of a local maximum
    mutation is a random process, so it is possible that 
we may have a sudden large mutation to get these 
individuals out of this situation

summary

    ga is a kind of hill-climbing search
    very similar to a randomized id125
    one significant difference between gas and hc
is that, it is generally a good idea in gas to (cid:1)fill
the local maxima up with individuals(cid:2)

    overall, gas have less problems with local 

maxima than hc or neural networks

    easy to apply to a wide range of problems

    optimization problems such as tsp
    inductive concept learning
    scheduling
    layout

    results can be very good on some problems and 

poor on others

    ga is very slow if only mutation is used;

crossover makes the algorithm significantly 
faster 

7

