ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

invariant theory

1

invariant theory

why are early filters wavelets?

krizhevsky

mathworks

2

invariance

consider the distribution of    natural    360 degree images.

it should be true that the id203 of a given 360 degree
image is equal to the id203 of any rotation of that image.

we say that the id203 distribution is invariant to rota-
tion.

3

invariances

translation invariance (in both space and time)

scale invariance (in both space and time)

rotation invariance (spatial rotations).

4

pca and invariance

the principal components in pca are the eigenvectors of the
covariance matrix.

the principal components of the covariance matrix of a transla-
tion invariant distribution are the fourier basis functions (sine
and cosine).

5

pca and invariance

the eigenvalues of the covariance matrix are given by the
power spectrum of the signal distribution.

this is the einstein-wiener-khinchin theorem (proved by wiener,
and independently by khinchin, in the early 1930s, but     as
only recently recognized     stated by einstein in 1914). from
   signals and systems    by oppenheim and verghese

this explains projection onto complex exponentials as a    rst
step in signal processing and signal compression (e.g., jpeg).

6

more formally

let    be a id203 density over vectors in rn.

we say    is rotation-stationary if

    e [xi] = e(cid:2)xj
    e(cid:2)xixj

(cid:3) for all i, j.
(cid:3) = f (i     j mod n)

rotation stationarity is a simpli   cation of the more widely
used notion of translation stationarity (or just stationarity).

7

more formally

the covariance matrix is given by

  i,j = e(cid:2)xixj     e [xi] e(cid:2)xj

(cid:3)(cid:3) = g(i     j mod n)

a matrix satisfying   i,j = g(i   j mod n) is called circulant.

the eigenvectors of a circulant matrix form a discrete fourier
basis.

8

wavelets

in practice we want the compressed representation to be local
to also satisfy scale invariance. this leads to wavelets.

to my knowledge scale invariance is not currently built into
deep vision architectures.

9

invariance and data augmentation

id98s build translation invariance into the architecture.

another approach to invariance is to apply invariant transfor-
mations to the training data.

for example we can apply translations, scalings, rotations, re-
   ections to generate more labeled images in mnist or ima-
genet to get a much larger training set.

10

end

