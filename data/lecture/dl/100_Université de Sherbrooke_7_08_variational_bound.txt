neural networks
deep learning - variational bound

j

abstract

|h(2))

deep learning

deep learning
d  epartement d   informatique
2
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
deep learning
october 25, 2012
universit  e de sherbrooke

math for my slides    deep learning   .

hugo larochelle

hugo larochelle

j = 1|h(2)) = sigm(b(1) + w(2)>h(2))

hugo larochelle
hugo.larochelle@usherbrooke.ca

deep belief network
deep learning

deep learning
d  epartement d   informatique
universit  e de sherbrooke

    p(h(1)
october 25, 2012
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
deep learning
    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
hugo larochelle
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
d  epartement d   informatique
math for my slides    deep learning   .
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
universit  e de sherbrooke
    x h(1) h(2) h(3)
abstract
october 25, 2012
october 25, 2012
hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca
math for my slides    deep learning   .
october 25, 2012

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

    x h(1) h(2) h(3)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
topics: deep belief network
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    x h(1) h(2) h(3)
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    this is where the rbm stacking procedure comes from
    p(h(2), h(3))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    idea: improve prior on last layer by
    p(h(1)|h(2)) =qj p(h(1)
|h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
adding another hidden layer
    p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    how do we train these additional layers?
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x) =ph(1) p(x, h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)
...
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    x h(1) h(2) h(3)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

    x h(1) h(2) h(3)

1

hugo larochelle

october 25, 2012

    x h(1) h(2) h(3)
abstract
...

    x h(1) h(2) h(3)

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

abstract

abstract

abstract

|h(2))

...

...

...

...

...

...

...

j

j

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

j

j

3

abstract

abstract

|h(2))

october 25, 2012

october 25, 2012

math for my slides    deep learning   .

math for my slides    deep learning   .

j
math for my slides    deep learning   .

    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))

    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))

    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
deep belief network
    x h(1) h(2) h(3)
    p(x) =ph(1) p(x, h(1))
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1)|h(2)) =qj p(h(1)
    p(h(2), h(3))
|h(2))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x|h(1)) =qi p(xi|h(1))
    p(x|h(1)) =qi p(xi|h(1))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
topics: concavity
    x h(1) h(2) h(3)
    p(h(1)|h(2)) =qj p(h(1)
    p(x) =ph(1) p(x, h(1))
    p(x) =ph(1) p(x, h(1))
    x h(1) h(2) h(3)
|h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(h(2), h(3))
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    we will use the fact that the logarithm function is concave:
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(h(2), h(3))
    p(x|h(1)) =qi p(xi|h(1))
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    p(x) =ph(1) p(x, h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
                                                        (where                and           )
    p(h(1)
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    ai     p(x|h(1))p(h(1))
    ai     p(x|h(1))p(h(1))
!i     q(h(1)|x)
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x|h(1)) =qi p(xi|h(1))
    ai     p(x|h(1))p(h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
q(h(1)|x)
q(h(1)|x)
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
!i     q(h(1)|x)
   
   
    p(x) =ph(1) p(x, h(1))
q(h(1)|x)
    p(h(1)|h(2)) =qj p(h(1)
log p(x) = logxh(1)
log p(x) = logxh(1)
    p(h(1)|h(2)) =qj p(h(1)
p(x|h(1))p(h(1))
    a1 a2 !1 !2
log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)
q(h(1)|x)
q(h(1)|x)
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
   
q(h(1)|x)
    p(x|h(1)) =qi p(xi|h(1))
    ai !i
    p(x|h(1)) =qi p(xi|h(1))
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x) log    p(x|h(1))p(h(1))
   
log p(x) = logxh(1)
  xh(1)
  xh(1)
    p(x) =ph(1) p(x, h(1))
    p(x) =ph(1) p(x, h(1))
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
q(h(1)|x)
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
 xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)

p(x|h(1))p(h(1))
q(h(1)|x)
p(x|h(1))p(h(1))
q(h(1)|x)
q(h(1)|x)

    a1 a2 !1 !2
    ai !i

log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)

log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)

log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)

!i     q(h(1)|x)

    a1 a2 !1 !2

    a1 a2 !1 !2

|h(2))

|h(2))

|h(2))

j

j

j

= xh(1)

 xh(1)

q(h(1)|x) log q(h(1)|x)

4

(4)

(3)

(13)

q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
deep belief network
 xh(1)
   
q(h(1)|x) log q(h(1)|x)
log p(x) = logxh(1)
topics: variational bound
(5)
q(h(1)|x)
log p(x) = logxh(1)
p(x|h(1))p(h(1))
log p(x)   xh(1)
q(h(1)|x)
    for any model               with latent variables        we can write:
(14)
q(h(1)|x) log p(x, h(1))
q(h(1)|x) p(h(1)|x)
    h(1)
q(h(1)|x)
q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x) log    p(x|h(1))p(h(1))
   
log p(x) = log xh(1)
q(h(1)|x)!
 xh(1)
  xh(1)
(15)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
q(h(1)|x) log    p(x, h(1))
q(h(1)|x)   
(16)
  xh(1)
(7)
 xh(1)
 xh(1)
= xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
(8)
q(h(1)|x) log p(x, h(1))
 xh(1)
(9)
q(h(1)|x) log q(h(1)|x)
    h(1)
q(h(1)|x) p(h(1)|x)
2

                   is any approximation of 

q(h(1)|x) p(h(1)|x)

p(x|h(1))p(h(1))

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

p(x, h(1))

(10)

(6)

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

    h(1)
q(h(1)|x) p(h(1)|x)

= xh(1)

 xh(1)

4

t

t

j

j

(4)

(3)

(13)

}

}

j
,
  

   
=

=
x

x
w

x
w

q(h(1)|x) log q(h(1)|x)

j
=
i
1
=
j
log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)
u
>i

    p(x) =ph(1) p(x, h(1))
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
deep belief network
 xh(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
   
q(h(1)|x) log q(h(1)|x)
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
log p(x) = logxh(1)
topics: variational bound
(5)
q(h(1)|x)
log p(x) = logxh(1)
p(x|h(1))p(h(1))
p
log p(x)   xh(1)
q(h(1)|x)
    for any model               with latent variables        we can write:
(14)
q(h(1)|x) log p(x, h(1))
q(h(1)|x) p(h(1)|x)
    h(1)
    a1 a2 !1 !2
q(h(1)|x)
q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x) log    p(x|h(1))p(h(1))
   
    ai !i
log p(x) = log xh(1)
q(h(1)|x)!
 xh(1)
  xh(1)
(15)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
q(h(1)|x) log    p(x, h(1))
q(h(1)|x)   
(16)
 
  xh(1)
(7)
 xh(1)
=
 xh(1)
= xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
p
(8)
q(h(1)|x) log p(x, h(1))
x
 xh(1)
(9)
q(h(1)|x) log q(h(1)|x)
    h(1)
q(h(1)|x) p(h(1)|x)
2

}
q(h(1)|x)
)
x
r
/2
x

                   is any approximation of 

q(h(1)|x) p(h(1)|x)

2
x
{
=

p(x|h(1))p(h(1))

p

x
r

2
x
{

q

w
9

q(h(1)|x)

q(h(1)|x)

(
t
e
d

=
x

 
{

 
=

h
r

h
r

0
>

u
x

   
t
,

p(x, h(1))

(10)
1

t
e

   

   

   

   

   

   

>i

=

u

u

u

u

u

x

 

 

|

|

|

)

(

)

(

(6)

,

t

i

i

i

i

i

i

i

i

i

i

i

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

w
9

   

    h(1)
q(h(1)|x) p(h(1)|x)

6
= xh(1)

4

i

t

t

j

j

j

j

(4)

(3)

(13)

)
t
(

}

}

}

}

j
,
  

j
,
  

=
x

x
w

=
x

x
w

x
w

 xh(1)

j
=
1
=

p

q(h(1)|x) log q(h(1)|x)

j
=
i
1
=
j
log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)
u
>i

    p(x) =ph(1) p(x, h(1))
    p(x) =ph(1) p(x, h(1))
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
deep belief network
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
 xh(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
x
   
q(h(1)|x) log q(h(1)|x)
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
w
    log(pi !i ai)  pi !i log(ai) pi !i = 1 !i   0
log p(x) = logxh(1)
topics: variational bound
(5)
q(h(1)|x)
log p(x) = logxh(1)
p(x|h(1))p(h(1))
p
log p(x)   xh(1)
    a1 a2 !1 !2
q(h(1)|x)
    for any model               with latent variables        we can write:
(14)
q(h(1)|x) log p(x, h(1))
q(h(1)|x) p(h(1)|x)
    h(1)
    a1 a2 !1 !2
q(h(1)|x)
q(h(1)|x) log    p(x|h(1))p(h(1))
    ai !i
  xh(1)
q(h(1)|x) log    p(x|h(1))p(h(1))
   
    ai !i
log p(x) = log xh(1)
q(h(1)|x)!
 xh(1)
  xh(1)
}
(15)
q(h(1)|x) log q(h(1)|x)
p(x, h(1))
)
x
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
r
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
u
q(h(1)|x) log    p(x, h(1))
q(h(1)|x)   
(16)
 
/2
  xh(1)
u
(7)
x
q
 
 xh(1)
=
 xh(1)
= xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
p
(8)
q(h(1)|x) log p(x, h(1))
u
x
=
x
 xh(1)
(9)
q(h(1)|x) log q(h(1)|x)
    h(1)
q(h(1)|x) p(h(1)|x)
2

w
9
p

=
p
x

}
q(h(1)|x)
)
x
r
/2
x

                   is any approximation of 

2
x
{
=

q(h(1)|x) p(h(1)|x)

2
x
{
=

log(!1 a1 + !2 a2) !1 log(a1) + !2 log(a2)

=
 
{
x
   

p(x|h(1))p(h(1))

(
t
e
d
   

p

x
r

x
r

2
x
{

2
x
{

q

|
h
r

(6)
>i

w
9

w
9

q(h(1)|x)

q(h(1)|x)

(
t
e
d

 
{

 
   

 
=

 
=

h
r

h
r

h
r

0
>

u
x

u
x

   
t
,

   
t
,

)
   
t
(

   
=

   
=

j
u

(10)
1

t
e

   

   

   

t
e

   

   

   

   

   

   

>i

>i

=

=

u

u

u

u

u

u

u

x

x

 

 

|

|

|

|

|

(

(

)

)

)

(

)

(

,

,

t

t

t

t

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

i

1

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

w
9

   

    h(1)
q(h(1)|x) p(h(1)|x)

6
6
5

   

   

   

   

   

   

   

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
deep belief network
 xh(1)
(12)
q(h(1)|x) log q(h(1)|x)
 xh(1)
q(h(1)|x) log q(h(1)|x)
!
log p(x) = log xh(1)
log p(x) = log xh(1)
!
(13)
topics: variational bound
p(x|h(1))p(h(1))
p(x|h(1))p(h(1))
(1)
q(h(1)|x)
log p(x) = log xh(1)
!
log p(x) = log xh(1)
!
p(x|h(1))p(h(1))
p(x|h(1))p(h(1))
    this is called a variational bound
q(h(1)|x)
q(h(1)|x)
q(h(1)|x) log    p(x|h(1))p(h(1))
q(h(1)|x) log    p(x|h(1))p(h(1))
   
   
  xh(1)
  xh(1)
log p(x) = log xh(1)
log p(x) = log xh(1)
!
!
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x)
log p(x)   xh(1)
  xh(1)
  xh(1)
p(x|h(1))p(h(1))
q(h(1)|x)
q(h(1)|x)
q(h(1)|x) p(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x) log p(x, h(1))
= xh(1)
= xh(1)
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
   
q(h(1)|x) log    p(x|h(1))p(h(1))
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
  xh(1)
  xh(1)
 xh(1)
 xh(1)
 xh(1)
(4)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
(15)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
 xh(1)
 xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
(5)
(16)
 xh(1)
 xh(1)
 xh(1)
    if               is equal to the true conditional              , then we have an equality
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
    the more              is different from               the less tight the bound is
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
    in fact, the difference between the left and right terms is the kl divergence 
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
 xh(1)
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
 xh(1)
 xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

q(h(1)|x) p(h(1)|x)
    h(1)
    h(1)
   
   
between               and               :
    h(1)
    h(1)
q(h(1)|x) p(h(1)|x)
   
   

    h(1)
q(h(1)|x) p(h(1)|x)
q(h(1)|x) p(h(1)|x)
   

q(h(1)|x)
p(x|h(1))p(h(1))
q(h(1)|x)
q(h(1)|x)

    h(1)
   

q(h(1)|x) p(h(1)|x)

(2)
(14)
(3)

    h(1)
   

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

(5)
(4)

(7)
(6)

(4)
(3)

(3)
(2)

(1)
(2)

(1)

(8)

(6)

(6)

(7)

(6)

(5)

(4)

(5)

(1)

(2)

(3)

    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

q(h(1)|x) p(h(1)|x)

