neural networks
deep learning - dbn pretraining

j

abstract

|h(2))

deep learning

deep learning
d  epartement d   informatique
2
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
deep learning
october 25, 2012
universit  e de sherbrooke

math for my slides    deep learning   .

hugo larochelle

hugo larochelle

j = 1|h(2)) = sigm(b(1) + w(2)>h(2))

hugo larochelle
hugo.larochelle@usherbrooke.ca

deep belief network
deep learning

deep learning
d  epartement d   informatique
universit  e de sherbrooke

    p(h(1)
october 25, 2012
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
deep learning
    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
hugo larochelle
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
d  epartement d   informatique
math for my slides    deep learning   .
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
universit  e de sherbrooke
    x h(1) h(2) h(3)
abstract
october 25, 2012
october 25, 2012
hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca
math for my slides    deep learning   .
october 25, 2012

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

    x h(1) h(2) h(3)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
topics: deep belief network
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    x h(1) h(2) h(3)
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    this is where the rbm stacking procedure comes from
    p(h(2), h(3))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    idea: improve prior on last layer by
    p(h(1)|h(2)) =qj p(h(1)
|h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
adding another hidden layer
    p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    how do we train these additional layers?
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x) =ph(1) p(x, h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)
...
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    x h(1) h(2) h(3)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

    x h(1) h(2) h(3)

1

hugo larochelle

october 25, 2012

    x h(1) h(2) h(3)
abstract
...

    x h(1) h(2) h(3)

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

abstract

abstract

abstract

|h(2))

...

...

...

...

...

...

...

j

j

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

deep learning

2

deep belief network
deep learning

topics: deep belief network
    this is where the rbm stacking procedure comes from

    idea: improve prior on last layer by

adding another hidden layer

    how do we train these additional layers?

hugo larochelle

deep learning

deep learning
d  epartement d   informatique
universit  e de sherbrooke

deep learning

hugo larochelle

hugo larochelle
hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke
abstract
october 25, 2012
hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

october 25, 2012
hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

math for my slides    deep learning   .

    x h(1) h(2) h(3)
...

october 25, 2012

abstract

october 25, 2012

abstract

math for my slides    deep learning   .

math for my slides    deep learning   .

...

    x h(1) h(2) h(3)

...

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

abstract

abstract

...

    x h(1) h(2) h(3)

...

    x h(1) h(2) h(3)

3

   

   

   

   

   

   

   

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
deep belief network
 xh(1)
(12)
q(h(1)|x) log q(h(1)|x)
 xh(1)
q(h(1)|x) log q(h(1)|x)
!
log p(x) = log xh(1)
log p(x) = log xh(1)
!
(13)
topics: variational bound
p(x|h(1))p(h(1))
p(x|h(1))p(h(1))
(1)
q(h(1)|x)
log p(x) = log xh(1)
!
log p(x) = log xh(1)
!
p(x|h(1))p(h(1))
p(x|h(1))p(h(1))
    this is called a variational bound
q(h(1)|x)
q(h(1)|x)
q(h(1)|x) log    p(x|h(1))p(h(1))
q(h(1)|x) log    p(x|h(1))p(h(1))
   
   
  xh(1)
  xh(1)
log p(x) = log xh(1)
log p(x) = log xh(1)
!
!
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x)
log p(x)   xh(1)
  xh(1)
  xh(1)
p(x|h(1))p(h(1))
q(h(1)|x)
q(h(1)|x)
q(h(1)|x) p(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x) log p(x, h(1))
= xh(1)
= xh(1)
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
   
q(h(1)|x) log    p(x|h(1))p(h(1))
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
  xh(1)
  xh(1)
 xh(1)
 xh(1)
 xh(1)
(4)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
(15)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
 xh(1)
 xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
(5)
(16)
 xh(1)
 xh(1)
 xh(1)
    if               is equal to the true conditional              , then we have an equality
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
    the more              is different from               the less tight the bound is
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
    in fact, the difference between the left and right terms is the kl divergence 
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
 xh(1)
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
 xh(1)
 xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

q(h(1)|x) p(h(1)|x)
    h(1)
    h(1)
   
   
between               and               :
    h(1)
    h(1)
q(h(1)|x) p(h(1)|x)
   
   

    h(1)
q(h(1)|x) p(h(1)|x)
q(h(1)|x) p(h(1)|x)
   

q(h(1)|x)
p(x|h(1))p(h(1))
q(h(1)|x)
q(h(1)|x)

    h(1)
   

q(h(1)|x) p(h(1)|x)

(2)
(14)
(3)

    h(1)
   

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

(5)
(4)

(7)
(6)

(4)
(3)

(3)
(2)

(1)
(2)

(1)

(8)

(6)

(6)

(7)

(6)

(5)

(4)

(5)

(1)

(2)

(3)

    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

q(h(1)|x) p(h(1)|x)

q(h(1)|x) p(h(1)|x)

(5)

(4)

    h(1)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

q(h(1)|x) p(h(1)|x)

q(h(1)|x) p(h(1)|x)

    h(1)
    h(1)
   
   

q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x) log    p(x|h(1))p(h(1))
   
!
log p(x) = log xh(1)
= xh(1)
= xh(1)
!
log p(x) = log xh(1)
  xh(1)
 xh(1)
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
  xh(1)
  xh(1)
q(h(1)|x)
 xh(1)
 xh(1)
4
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
deep belief network
= xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x) log    p(x|h(1))p(h(1))
   
q(h(1)|x)
  xh(1)
  xh(1)
(2)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
q(h(1)|x)
 xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
= xh(1)
(3)
 xh(1)
 xh(1)
topics: variational bound
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
 xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
    this is called a variational bound
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
    h(1)
q(h(1)|x) p(h(1)|x)
log p(x)   xh(1)
log p(x)   xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
q(h(1)|x) p(h(1)|x)
    h(1)
    h(1)
   
 xh(1)
 xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) p(h(1)|x)
    h(1)
   
   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
 xh(1)
   
 xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
log p(x)   xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
p(h(1)|x)   
 xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
 xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

they are the parameters of the rbm involving         and        
             is now the marginalization of the second hidden layer

    for a single hidden layer dbn (i.e. an rbm), both               and           depend on 

    when adding a second layer, we model            using a separate set of parameters

the parameters of the    rst layer

q(h(1)|x) p(h(1)|x)

q(h(1)|x) p(h(1)|x)

(6)

(7)

(8)

(7)

(4)

(5)

(6)

(8)

-

-

q(h(1)|x) p(h(1)|x)

q(h(1)|x) p(h(1)|x)

 xh(1)
q(h(1)|x) log q(h(1)|x)
deep belief network
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
 xh(1)
q(h(1)|x) log q(h(1)|x)
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))

topics: variational bound
    this is called a variational bound
   

    we can train the parameters of the new second layer by maximizing the bound

adding 2nd layer means 
untying the parameters in

this is equivalent to minimizing the following, since the other terms are constant:

-

q(h(1)|x)

(8)
(6)
p(x|h(1))p(h(1))
q(h(1)|x)
(7)
q(h(1)|x)
(8)

log p(x) = log xh(1)
q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)

!
   

(9)

(4)

5

(5)
(6)

(7)

-

this is like training an rbm on data generated from                 ! 

q(h(1)|x) p(h(1)|x)

 xh(1)

q(h(1)|x) log p(h(1))

    h(1)
   

q(h(1)|x) p(h(1)|x)

   

q(h(1)|x) log q(h(1)|x)

topics: variational bound
    this is called a variational bound

 xh(1)
deep belief network
log p(x) = log xh(1)
!
   
q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)

q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)

adding 2nd layer means 
untying the parameters in

p(x|h(1))p(h(1))

q(h(1)|x)

q(h(1)|x)

q(h(1)|x)

    for                 we use the posterior of the    rst layer rbm

q(h(1)|x) p(h(1)|x)

- equivalent to a feed-forward (sigmoidal) layer, followed by sampling     

    h(1)
   

    by initializing the weights of the second layer rbm as the transpose of the    rst 

layer weights, the bound is initially tight
- a 2 layer dbn with tied weights is equivalent to a 1 layer rbm 

q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)

6

(4)

(5)

(6)

(7)

(8)

   

deep belief network

7

q(h(1)|x)

q(h(1)|x)

p(x|h(1))p(h(1))

log p(x) = log xh(1)
q(h(1)|x) log    p(x|h(1))p(h(1))
  xh(1)
q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
= xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)

q(h(1)|x)

!
   

topics: variational bound
    this process of adding layers can be repeated recursively
    we obtain the greedy layer-wise pre-training procedure for neural networks

    we now see that this procedure corresponds to maximizing a 
bound on the likelihood of the data in a dbn
    in theory, if our approximation              is very far from the true posterior, the 

q(h(1)|x) p(h(1)|x)

    h(1)
bound might be very loose 
   

    this only means we might not be improving the true likelihood
    we might still be extracting better features!

    fine-tuning is done by the up-down algorithm

q(h(1)|x)   log p(x|h(1)) + log p(h(1))   
log p(x)   xh(1)
 xh(1)
q(h(1)|x) log q(h(1)|x)

- a fast learning algorithm for deep belief nets. 

hinton, teh, osindero, 2006.

