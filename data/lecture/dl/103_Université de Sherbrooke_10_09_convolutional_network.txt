neural networks
natural language processing - convolutional network

word tagging

2

topics: word tagging
    in many nlp applications, it is useful to augment text data 
with syntactic and semantic information
    we would like to add syntactic/semantic labels to each word

    this problem can be tackled using a conditional random    eld 
with neural network unary potentials
    we will describe the model developed by ronan collobert and jason weston in:
 a uni   ed architecture for natural language processing: deep neural networks 
with multitask learning
collobert and weston, 2008

(see natural language processing (almost) from scratch for the journal version)

sentence neural network

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

3

topics: sentence convolutional network
    how to model each label sequence
    could use a crf with neural network unary 

potentials, based on a window (context) of words
- not appropriate for id14, because 

relevant context might be very far away

    collobert and weston suggest a convolutional 

network over the whole sentence
- prediction at a given position can exploit information 

from any word in the sentence

the cat
w1
1 w1

2

wk

1 wk

2

p
a
d
d
i
n
g

d

n1

hu

sat
. . .

. . .

on the mat
w1
n

wk
n

m 1

     

n1

hu

n2

hu

input sentence

text

feature 1

...

feature k

p
a
d
d
i
n
g

lookup table

ltw 1

...

ltw k

convolution

max over time

max(  )

linear

m 2

     

hardtanh

linear

m 3

     

n3

hu = #tags

figure 2: sentence approach network.

sentence neural network

4

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

topics: sentence convolutional network
    each word can be represented by more 
than one feature
    feature of the word itself
    substring features 

input sentence

text

- pre   x:        eating                     eat       
suf   x:        eating                     ing        
-

    gazetteer features

feature 1

...

feature k

p
a
d
d
i
n
g

lookup table

- whether the word belong to a list of known locations, 

persons, etc.

ltw 1

...

ltw k

    these features are treated like word 
features, with their own lookup tables

convolution

the cat
1 w1
w1

2

wk

1 wk

2

sat
. . .

. . .

on the mat
w1
n

wk
n

p
a
d
d
i
n
g

d

m 1

     

sentence neural network

5

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

topics: sentence convolutional network
    feature must encode for which word 
we are making a prediction
    done by adding the relative
position i-posw, where posw
is the position of the current 
word

input sentence

feature 1

text

...

    this feature also has its lookup

table

feature k

lookup table

p
a
d
d
i
n
g

    for srl, must know the roles for which 
verb we are predicting
    also add the relative position of that verb i-posv

convolution

ltw k

ltw 1

...

the cat
1 w1
w1

2

wk

1 wk

2

sat
. . .

. . .

on the mat
w1
n

wk
n

p
a
d
d
i
n
g

d

m 1

     

sentence neural network

input sentence

text

the cat
1 w1
w1

2

sat
. . .

on the mat
w1
n

feature 1

6

topics: sentence convolutional network
    lookup table: 

lookup table

feature k

...

ltw 1

p
a
d
d
i
n
g

wk

1 wk

2

. . .

wk
n

p
a
d
d
i
n
g

    for each word concatenate 
the representations of its 
features

    convolution:

    at every position, compute
linear activations from a 
window of representations
    this is a convolution in 1d

    max pooling:

    obtain a    xed hidden layer
with a max across positions

...

ltw k

convolution

max over time

max(  )

linear

m 2

d

n1

hu

m 1

     

n1

hu

sentence neural network

n1

hu

7

topics: sentence convolutional network
    regular neural network:

max over time

max(  )

    the pooled representation 

serves as the input of
a regular neural network
    they proposed using a 
      hard       version of the
tanh activation function

linear

m 2

     

hardtanh

linear

m 3

     

n1

hu

n2

hu

n3

hu = #tags

figure 2: sentence approach network.

    the outputs are used as the unary potential of a chain crf 
over the labels
    no connections between the crfs of the different task (one crf per task)
dwin
    a separate neural network is used for each task
i

in the following, we will describe each layer we use in our networks shown in figure 1 and figure 2.
we adopt few notations. given a matrix a we denote [a]i, j the coef   cient at row i and column j
in the matrix. we also denote    a   
the vector obtained by concatenating the dwin column vectors
around the ith column vector of matrix a     rd1  d2:

