sparse coding
ift 725 - r  seaux neuronaux

unsupervised learning

topics: unsupervised learning
    unsupervised learning: only use the inputs       for learning

    x(t)   log p(x(t))

    automatically extract meaningful features for your data
    leverage the availability of unlabeled data
    add a data-dependent regularizer to trainings

math for my slides    restricted id82s   .

    we will see 3 neural networks for unsupervised learning

    restricted id82s
    autoencoders
    sparse coding model

2

hugo larochelle

sparse coding

d  epartement d   informatique
universit  e de sherbrooke

november 1, 2012
sparse coding
abstract

sparse coding

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

hugo larochelle

math for my slides    sparse coding   .

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

d  epartement d   informatique
november 1, 2012
universit  e de sherbrooke
hugo larochelle
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
1
2||x(t)   d h(t)||2
arg min
universit  e de sherbrooke
    we can reconstruct the original input        as much as possible
1
h(t)
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
t
h(t)
november 1, 2012
arg min
november 1, 2012

math for my slides    sparse coding   .
d  epartement d   informatique
    x(t) h(t) d
math for my slides    sparse coding   .
universit  e de sherbrooke
1
arg min
hugo.larochelle@usherbrooke.ca
t
    x(t) h(t) d
   

    x(t) h(t) d

txt=1

txt=1

abstract

d

d

sparse coding
hugo larochelle
abstract

1
2||x(t)   d h(t)||2

1
2||x(t)   d h(t)||2

min
h(t)
math for my slides    sparse coding   .

h(x(t)) = arg min
2 +  ||h(t)||1

abstract

h(t)

   

h(x(t)) = arg min

h(t)

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

min
d

1
t

txt=1
    x(t) h(t) d

h(t)

h(x(t)) = arg min

math for my slides    sparse coding   .

1
math for my slides    sparse coding   .
2||x(t)   d h(t)||2

    we also constrain the columns of        to be of norm 1
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
arg min
arg min
    sometimes the columns are constrained to be no greater than 1
t
1
d
2||x(t)   d h(t)||2
arg min
arg min
d

    x(t) h(t) d
arg min

bx(t) = d h(x(t)) = xk s.t.

d  ,k h(x(t))k
1
t

txt=1
txt=1

2 +  ||h(t)||1

1
t

d

3

txt=1

hugo larochelle

sparse coding

d  epartement d   informatique
universit  e de sherbrooke

november 1, 2012
sparse coding
abstract

sparse coding

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

hugo larochelle

math for my slides    sparse coding   .

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

d  epartement d   informatique
november 1, 2012
universit  e de sherbrooke
hugo larochelle
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
1
2||x(t)   d h(t)||2
arg min
universit  e de sherbrooke
    we can reconstruct the original input        as much as possible
1
h(t)
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
t
h(t)
november 1, 2012
arg min
november 1, 2012

math for my slides    sparse coding   .
d  epartement d   informatique
    x(t) h(t) d
math for my slides    sparse coding   .
universit  e de sherbrooke
1
arg min
hugo.larochelle@usherbrooke.ca
t
    x(t) h(t) d
   

    x(t) h(t) d

reconstruction error

txt=1

txt=1

abstract

d

d

sparse coding
hugo larochelle
abstract

1
2||x(t)   d h(t)||2

1
2||x(t)   d h(t)||2

 
min
h(t)
math for my slides    sparse coding   .

h(x(t)) = arg min
2 +  ||h(t)||1

abstract

h(t)

   

h(x(t)) = arg min

h(t)

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

min
d

1
t

txt=1
    x(t) h(t) d

h(t)

h(x(t)) = arg min

math for my slides    sparse coding   .

1
math for my slides    sparse coding   .
2||x(t)   d h(t)||2

    we also constrain the columns of        to be of norm 1
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
arg min
arg min
    sometimes the columns are constrained to be no greater than 1
t
1
d
2||x(t)   d h(t)||2
arg min
arg min
d

    x(t) h(t) d
arg min

bx(t) = d h(x(t)) = xk s.t.

d  ,k h(x(t))k
1
t

txt=1
txt=1

2 +  ||h(t)||1

1
t

d

3

txt=1

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo larochelle

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

november 1, 2012
sparse coding
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
sparse coding
sparse coding
hugo larochelle
abstract
hugo larochelle
d  epartement d   informatique
d  epartement d   informatique
november 1, 2012
universit  e de sherbrooke
hugo larochelle
universit  e de sherbrooke
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
1
hugo.larochelle@usherbrooke.ca
2||x(t)   d h(t)||2
arg min
universit  e de sherbrooke
    we can reconstruct the original input        as much as possible
1
h(t)
november 1, 2012
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
t
h(t)
november 1, 2012
arg min
november 1, 2012

math for my slides    sparse coding   .
d  epartement d   informatique
    x(t) h(t) d
math for my slides    sparse coding   .
universit  e de sherbrooke
1
arg min
hugo.larochelle@usherbrooke.ca
t
    x(t) h(t) d
   

math for my slides    sparse coding   .

hugo larochelle

    x(t) h(t) d

reconstruction error

txt=1

txt=1

abstract

d

d

1
2||x(t)   d h(t)||2

1
t

min
d

h(x(t)) = arg min
2 +  ||h(t)||1

h(x(t)) = arg min

math for my slides    sparse coding   .

txt=1
    x(t) h(t) d

 
min
h(t)
math for my slides    sparse coding   .

1
2||x(t)   d h(t)||2
h(t)
math for my slides    sparse coding   .
   
 
reconstruction
    x(t) h(t) d bx(t)
1
math for my slides    sparse coding   .
    we also constrain the columns of        to be of norm 1
2||x(t)   d h(t)||2
arg min
d
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
    x(t) h(t) d
arg min
arg min
txt=1
    sometimes the columns are constrained to be no greater than 1
t
1
d
2||x(t)   d h(t)||2
arg min
arg min
arg min
h(x(t)) = arg min
d

bx(t) = d h(x(t)) = xk s.t.

txt=1
txt=1
txt=1

d  ,k h(x(t))k
1
t

2 +  ||h(t)||1

h(x(t)) = arg min

abstract

arg min

1
t

h(t)

1
t

h(t)

   

d

3

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo larochelle

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

november 1, 2012
sparse coding
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
sparse coding
sparse coding
hugo larochelle
abstract
hugo larochelle
d  epartement d   informatique
d  epartement d   informatique
november 1, 2012
universit  e de sherbrooke
hugo larochelle
universit  e de sherbrooke
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
1
hugo.larochelle@usherbrooke.ca
2||x(t)   d h(t)||2
arg min
universit  e de sherbrooke
    we can reconstruct the original input        as much as possible
1
h(t)
november 1, 2012
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
t
h(t)
november 1, 2012
arg min
november 1, 2012

math for my slides    sparse coding   .
d  epartement d   informatique
    x(t) h(t) d
math for my slides    sparse coding   .
universit  e de sherbrooke
1
arg min
hugo.larochelle@usherbrooke.ca
t
    x(t) h(t) d
   

math for my slides    sparse coding   .

hugo larochelle

    x(t) h(t) d

reconstruction error

txt=1

txt=1

abstract

d

d

sparsity penalty
h(x(t)) = arg min
2 +  ||h(t)||1

 

1
2||x(t)   d h(t)||2

1
t

min
d

h(x(t)) = arg min

math for my slides    sparse coding   .

txt=1
    x(t) h(t) d

 
min
h(t)
math for my slides    sparse coding   .

1
2||x(t)   d h(t)||2
h(t)
math for my slides    sparse coding   .
   
 
reconstruction
    x(t) h(t) d bx(t)
1
math for my slides    sparse coding   .
    we also constrain the columns of        to be of norm 1
2||x(t)   d h(t)||2
arg min
d
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
    x(t) h(t) d
arg min
arg min
txt=1
    sometimes the columns are constrained to be no greater than 1
t
1
d
2||x(t)   d h(t)||2
arg min
arg min
arg min
h(x(t)) = arg min
d

bx(t) = d h(x(t)) = xk s.t.

txt=1
txt=1
txt=1

d  ,k h(x(t))k
1
t

2 +  ||h(t)||1

h(x(t)) = arg min

abstract

arg min

1
t

h(t)

1
t

h(t)

   

d

3

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo larochelle

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

november 1, 2012
sparse coding
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
sparse coding
sparse coding
hugo larochelle
abstract
hugo larochelle
d  epartement d   informatique
d  epartement d   informatique
november 1, 2012
universit  e de sherbrooke
hugo larochelle
universit  e de sherbrooke
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
1
hugo.larochelle@usherbrooke.ca
2||x(t)   d h(t)||2
arg min
universit  e de sherbrooke
    we can reconstruct the original input        as much as possible
1
h(t)
november 1, 2012
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
t
h(t)
november 1, 2012
arg min
november 1, 2012

math for my slides    sparse coding   .
d  epartement d   informatique
    x(t) h(t) d
math for my slides    sparse coding   .
universit  e de sherbrooke
1
arg min
hugo.larochelle@usherbrooke.ca
t
    x(t) h(t) d
   

math for my slides    sparse coding   .

hugo larochelle

    x(t) h(t) d

reconstruction error

txt=1

txt=1

abstract

d

d

sparsity penalty
h(x(t)) = arg min
2 +  ||h(t)||1

 

 

1
2||x(t)   d h(t)||2

1
t

min
d

h(x(t)) = arg min

math for my slides    sparse coding   .

txt=1
    x(t) h(t) d

 
min
h(t)
math for my slides    sparse coding   .

1
2||x(t)   d h(t)||2
h(t)
math for my slides    sparse coding   .
   
 
reconstruction
    x(t) h(t) d bx(t)
1
math for my slides    sparse coding   .
    we also constrain the columns of        to be of norm 1
2||x(t)   d h(t)||2
arg min
d
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
    x(t) h(t) d
arg min
arg min
txt=1
    sometimes the columns are constrained to be no greater than 1
t
1
d
2||x(t)   d h(t)||2
arg min
arg min
arg min
h(x(t)) = arg min
d

bx(t) = d h(x(t)) = xk s.t.

txt=1
txt=1
txt=1

d  ,k h(x(t))k
1
t

abstract
reconstruction vs.
sparsity control

2 +  ||h(t)||1

h(x(t)) = arg min

arg min

1
t

h(t)

1
t

h(t)

   

d

3

sparse coding
hugo larochelle
hugo larochelle

sparse coding

november 1, 2012
abstract

hugo larochelle

sparse coding

sparse coding

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

d  epartement d   informatique
universit  e de sherbrooke
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .
universit  e de sherbrooke
d  epartement d   informatique
sparse coding
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
    x(t) h(t) d
universit  e de sherbrooke
hugo larochelle
november 1, 2012
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
1
1
2||x(t)   d h(t)||2
d  epartement d   informatique
arg min
arg min
abstract
t
    we can reconstruct the original input        as much as possible
universit  e de sherbrooke
1
    x(t) h(t) d
november 1, 2012
november 1, 2012
arg min
hugo.larochelle@usherbrooke.ca
t
   
november 1, 2012

hugo.larochelle@usherbrooke.ca
h(t)

math for my slides    sparse coding   .

math for my slides    sparse coding   .

    x(t) h(t) d

abstract
hugo larochelle

abstract
reconstruction error

txt=1

txt=1

arg min

arg min

h(t)

d

d

math for my slides    sparse coding   .

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

sparsity penalty
h(x(t)) = arg min
2 +  ||h(t)||1
abstract

 

 

1
2||x(t)   d h(t)||2

1
t

arg min

min
d
1
t

txt=1
 
1
2||x(t)   d h(t)||2
h(t)
min
abstract
math for my slides    sparse coding   .
   
h(t)
 
txt=1
1
math for my slides    sparse coding   .
math for my slides    sparse coding   .
2||x(t)   d h(t)||2
arg min
reconstruction
    x(t) h(t) d bx(t)
d
h(t)
    x(t) h(t) d
1
h(x(t)) = arg min
2||x(t)   d h(t)||2
txt=1
    x(t) h(t) d
arg min
h(t)
txt=1
1
d
2||x(t)   d h(t)||2
h(x(t)) = arg min
arg min
arg min
bx(t) = d h(x(t)) = xk s.t.
   
   
h(t)
- encoder is the minimization
d
h(x(t)) = arg min

reconstruction vs.
2 +  ||h(t)||1
sparsity control
txt=1
math for my slides    sparse coding   .
         is equivalent to the autoencoder output weight matrix
2 +  ||h(t)||1
1
arg min
2 +  ||h(t)||1
2||x(t)   d h(t)||2
    however,             is now a complicated function of      
1
    x(t) h(t) d
2 +  ||h(t)||1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d  ,k h(x(t))k
1
h(t)
2||x(t)   d h(t)||2
2 +  ||h(t)||1

h(x(t)) = arg min

h(x(t)) = arg min

1
t
1
t

arg min

arg min

h(t)

1
t

h(t)

h(t)

d

4

txt=1

sparse coding
2 +  ||h(t)||1
hugo larochelle

h(t)

1
2||x(t)   d h(t)||2

h(x(t)) = arg min

sparse coding
bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
h(x(t))k6=0

d  ,k h(x(t))k
november 1, 2012

topics: dictionary
    can also write

=  1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 0.8

+ 1

+ 1

+ 0.8
math for my slides    sparse coding   .

+ 1

+ 1

+ 1

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
1
2||x(t)   d h(t)||2
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

    we also refer to     as the dictionary
    x(t) h(t) d
in certain applications, we know what dictionary matrix to use
-
- often however, we have to learn it

txt=1

ingly, the encoder and decoder    lter values are nearly identical up to a scale factor. after training,
id136 is extremely fast, requiring only a simple matrix-vector multiplication.

arg min

arg min

1
t

h(t)

   

d

5

+ 0.8

abstract

txt=1

sparse coding
2 +  ||h(t)||1
hugo larochelle

h(t)

1
2||x(t)   d h(t)||2

h(x(t)) = arg min

sparse coding
bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
h(x(t))k6=0

d  ,k h(x(t))k
november 1, 2012

topics: dictionary
    can also write

=  1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 0.8

+ 1

+ 1

+ 0.8
math for my slides    sparse coding   .

+ 1

+ 1

+ 1

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
1
2||x(t)   d h(t)||2
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

    we also refer to     as the dictionary
    x(t) h(t) d
in certain applications, we know what dictionary matrix to use
-
- often however, we have to learn it

txt=1

ingly, the encoder and decoder    lter values are nearly identical up to a scale factor. after training,
id136 is extremely fast, requiring only a simple matrix-vector multiplication.

arg min

arg min

1
t

h(t)

   

d

5

+ 0.8

abstract

txt=1

sparse coding
2 +  ||h(t)||1
hugo larochelle

h(t)

1
2||x(t)   d h(t)||2

h(x(t)) = arg min

sparse coding
bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
h(x(t))k6=0

d  ,k h(x(t))k
november 1, 2012

topics: dictionary
    can also write

=  1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 0.8

+ 1

+ 1

+ 0.8
math for my slides    sparse coding   .

+ 1

+ 1

+ 1

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
1
2||x(t)   d h(t)||2
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

    we also refer to     as the dictionary
    x(t) h(t) d
in certain applications, we know what dictionary matrix to use
-
- often however, we have to learn it

txt=1

ingly, the encoder and decoder    lter values are nearly identical up to a scale factor. after training,
id136 is extremely fast, requiring only a simple matrix-vector multiplication.

arg min

arg min

1
t

h(t)

   

d

5

+ 0.8

abstract

    x(t) h(t) d
math for my slides    sparse coding   .

    x(t) h(t) d bx(t)

2 +  ||h(t)||1
   
1
2||x(t)   d h(t)||2
arg min
abstract
d  ,k h(x(t))k
h(t)
2 +  ||h(t)||1

d

h(t)

1
t

arg min

abstract
1
2||x(t)   d h(t)||2
h(x(t)) = arg min
sparse coding
arg min

txt=1
bx(t) = d h(x(t)) = xk s.t.
txt=1
txt=1
bx(t) = d h(x(t)) = xk s.t.

1
1
l(x(t)) =
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
h(x(t)) = arg min
arg min

h(x(t)) = arg min

h(x(t))k6=0

h(t)

    x(t) h(t) d

d

1
t

arg min

topics: id136 of sparse codes
math for my slides    sparse coding   .
1
2||x(t)   d h(t)||2
   
h(x(t))k6=0
    given     , how do we compute
    we want to optimize                                                           w.r.t. 

1
h(x(t)) = arg min
2||x(t)   d h(t)||2
1
2||x(t)   d h(t)||2

h(t)
2 +  ||h(t)||1
2 +  ||h(t)||1
arg min

1
t

h(t)

@

d

h(t)

    h(t)
    h(x(t))k = 0
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
k )
@h(t)
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))
k
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1

d  ,k h(x(t))k

h(t)

@

l(x(t)) = ||x(t)   d h(t)||2
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
k )
    we could use a id119 method:
@h(t)
k
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))

2 +  ||h(t)||1

k (= h(t)

k       dd>(d h(t)   x(t))
k         sign(h(t)

    sign(h(t)) 6= sign(h(t)

6

    x(t) h(t) d bx(t)
    x(t) h(t) d bx(t)
   

   

   

   

   

   

   

   

@

h(t)

h(t)

   

   

h(t)

   

h(t)

   
@

min
d
t
d

d  ,k h(x(t))k

h(x(t)) = arg min

d  ,k h(x(t))k
d  ,k h(x(t))k
h(x(t))k6=0

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1
h(x(t))k6=0

2||x(t)   d h(t)||2

2 +  ||h(t)||1
2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
2 +  ||h(t)||1

bx(t) = d h(x(t)) = xk s.t.

2 +  ||h(t)||1
txt=1
txt=1
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
min
t
h(t)
h(x(t)) = arg min
   
sparse coding
bx(t) = d h(x(t)) = xk s.t.
bx(t) = d h(x(t)) = xk s.t.
   
bx(t) = d h(x(t)) = xk s.t.
d  ,k h(x(t))k
h(x(t)) = arg min
1
2||x(t)   d h(t)||2
h(x(t)) = arg min
bx(t) = d h(x(t)) = xk s.t.
h(t)
h(x(t))k6=0
bx(t) = d h(x(t)) = xk s.t.
topics: id136 of sparse codes
l(x(t)) = ||x(t)   d h(t)||2
2 +  ||h(t)||1
bx(t) = d h(x(t)) = xk s.t.
   
l(x(t)) = ||x(t)   d h(t)||2
    for a single hidden unit:
h(x(t))k6=0
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
2 +  ||h(t)||1
l(x(t)) = ||x(t)   d h(t)||2
h(x(t))k6=0
@
@h(t)
2 +  ||h(t)||1
l(x(t)) = ||x(t)   d h(t)||2
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
@
1
k
@h(t)
1
l(x(t)) =
2||x(t)   d h(t)||2
2 +  ||h(t)||1
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
@h(t)
2 +  ||h(t)||1
2||x(t)   d h(t)||2
l(x(t)) =
k )
k
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
k
@h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@h(t)
@
k
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@
k
k )
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
k )
@h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@h(t)
    h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
k
    h(t)
    issue: l1 norm not differentiable at 0
k
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
    h(t)
    h(t)
    h(t)
- very unlikely for id119 to       land       on               (even if it   s the solution)
    h(t)
    h(t)
    h(t)
k = 0
    h(t)
    solution: if       changes sign because of l1 norm gradient, clamp to 0
    h(t)
    h(t)
k = 0
    h(t)
k = 0
k
    h(t)
    each hidden unit update would be performed as follows:
    h(t)
    h(t)
k
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))
    h(t)
k (= h(t)
    h(t)
k       dd>(d h(t)   x(t))
    sign(h(t)) 6= sign(h(t)
   
-
k (= h(t)
    h(t)
k       d>(d h(t)   x(t))
k (= h(t)
    h(t)
k       d>(d h(t)   x(t))
    sign(h(t)) 6= sign(h(t)
k         sign(h(t)
if                                                    then: 
-
k ))
    h(t)
k ) 6= sign(h(t)
    sign(h(t)
k         sign(h(t)
k ))
k         sign(h(t)
    sign(h(t)) 6= sign(h(t)
k ))
- else:
    h(t)
k (= h(t)
k         sign(h(t)
    h(t)
k )
    h(t)
k (= 0
    h(t)
k (= 0
k (= h(t)
    h(t)
k         sign(h(t)
k         sign(h(t)
k (= h(t)
    h(t)

    h(t)
k = 0
    h(t)
    h(t)
k (= h(t)
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))

k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))

k (= 0
k (= h(t)

k         sign(h(t)
k )

k = 0

k = 0

@

7

k

k

k

    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))

k

    x(t) h(t) d bx(t)
    x(t) h(t) d bx(t)
   

   

   

   

   

   

   

   

@

h(t)

h(t)

   

   

h(t)

   

h(t)

   
@

min
d
t
d

d  ,k h(x(t))k

h(x(t)) = arg min

d  ,k h(x(t))k
d  ,k h(x(t))k
h(x(t))k6=0

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1
h(x(t))k6=0

2||x(t)   d h(t)||2

2 +  ||h(t)||1
2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
2 +  ||h(t)||1

bx(t) = d h(x(t)) = xk s.t.

2 +  ||h(t)||1
txt=1
txt=1
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
min
t
h(t)
h(x(t)) = arg min
   
sparse coding
bx(t) = d h(x(t)) = xk s.t.
bx(t) = d h(x(t)) = xk s.t.
   
bx(t) = d h(x(t)) = xk s.t.
d  ,k h(x(t))k
h(x(t)) = arg min
1
2||x(t)   d h(t)||2
h(x(t)) = arg min
bx(t) = d h(x(t)) = xk s.t.
h(t)
h(x(t))k6=0
bx(t) = d h(x(t)) = xk s.t.
topics: id136 of sparse codes
l(x(t)) = ||x(t)   d h(t)||2
2 +  ||h(t)||1
bx(t) = d h(x(t)) = xk s.t.
   
l(x(t)) = ||x(t)   d h(t)||2
    for a single hidden unit:
h(x(t))k6=0
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
2 +  ||h(t)||1
l(x(t)) = ||x(t)   d h(t)||2
h(x(t))k6=0
@
@h(t)
2 +  ||h(t)||1
l(x(t)) = ||x(t)   d h(t)||2
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
@
1
k
@h(t)
1
l(x(t)) =
2||x(t)   d h(t)||2
2 +  ||h(t)||1
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
@h(t)
2 +  ||h(t)||1
2||x(t)   d h(t)||2
l(x(t)) =
k )
k
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
k
@h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@h(t)
@
k
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@
k
k )
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
k )
@h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@h(t)
    h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
k
    h(t)
    issue: l1 norm not differentiable at 0
k
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
    h(t)
    h(t)
    h(t)
- very unlikely for id119 to       land       on               (even if it   s the solution)
    h(t)
    h(t)
    h(t)
k = 0
    h(t)
    solution: if       changes sign because of l1 norm gradient, clamp to 0
    h(t)
    h(t)
k = 0
    h(t)
k = 0
k
    h(t)
    each hidden unit update would be performed as follows:
    h(t)
    h(t)
k
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))
    h(t)
k (= h(t)
    h(t)
k       dd>(d h(t)   x(t))
    sign(h(t)) 6= sign(h(t)
   
-
k (= h(t)
    h(t)
k       d>(d h(t)   x(t))
k (= h(t)
    h(t)
k       d>(d h(t)   x(t))
    sign(h(t)) 6= sign(h(t)
k         sign(h(t)
if                                                    then: 
-
k ))
    h(t)
k ) 6= sign(h(t)
    sign(h(t)
k         sign(h(t)
k ))
k         sign(h(t)
    sign(h(t)) 6= sign(h(t)
k ))
- else:
    h(t)
k (= h(t)
k         sign(h(t)
    h(t)
k )
    h(t)
k (= 0
    h(t)
k (= 0
k (= h(t)
    h(t)
k         sign(h(t)
k         sign(h(t)
k (= h(t)
    h(t)

    h(t)
k = 0
    h(t)
    h(t)
k (= h(t)
k       dd>(d h(t)   x(t))
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))
 
k         sign(h(t)
update from reconstruction
k ))

k (= 0
k (= h(t)

k         sign(h(t)
k )

k = 0

k = 0

@

7

k

k

k

    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))

k

    x(t) h(t) d bx(t)
    x(t) h(t) d bx(t)
   

   

   

   

   

   

   

   

@

h(t)

h(t)

   

   

h(t)

   

h(t)

   
@

min
d
t
d

d  ,k h(x(t))k

h(x(t)) = arg min

d  ,k h(x(t))k
d  ,k h(x(t))k
h(x(t))k6=0

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1
h(x(t))k6=0

2||x(t)   d h(t)||2

2 +  ||h(t)||1
2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
2 +  ||h(t)||1

bx(t) = d h(x(t)) = xk s.t.

2 +  ||h(t)||1
txt=1
txt=1
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
min
t
h(t)
h(x(t)) = arg min
   
sparse coding
bx(t) = d h(x(t)) = xk s.t.
bx(t) = d h(x(t)) = xk s.t.
   
bx(t) = d h(x(t)) = xk s.t.
d  ,k h(x(t))k
h(x(t)) = arg min
1
2||x(t)   d h(t)||2
h(x(t)) = arg min
bx(t) = d h(x(t)) = xk s.t.
h(t)
h(x(t))k6=0
bx(t) = d h(x(t)) = xk s.t.
topics: id136 of sparse codes
l(x(t)) = ||x(t)   d h(t)||2
2 +  ||h(t)||1
bx(t) = d h(x(t)) = xk s.t.
   
l(x(t)) = ||x(t)   d h(t)||2
    for a single hidden unit:
h(x(t))k6=0
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
2 +  ||h(t)||1
l(x(t)) = ||x(t)   d h(t)||2
h(x(t))k6=0
@
@h(t)
2 +  ||h(t)||1
l(x(t)) = ||x(t)   d h(t)||2
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
@
1
k
@h(t)
1
l(x(t)) =
2||x(t)   d h(t)||2
2 +  ||h(t)||1
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
@h(t)
2 +  ||h(t)||1
2||x(t)   d h(t)||2
l(x(t)) =
k )
k
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
k
@h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@h(t)
@
k
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@
k
k )
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
k )
@h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
@h(t)
    h(t)
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
k
    h(t)
    issue: l1 norm not differentiable at 0
k
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
    h(t)
    h(t)
    h(t)
- very unlikely for id119 to       land       on               (even if it   s the solution)
    h(t)
    h(t)
    h(t)
k = 0
    h(t)
    solution: if       changes sign because of l1 norm gradient, clamp to 0
    h(t)
    h(t)
k = 0
    h(t)
k = 0
k
    h(t)
    each hidden unit update would be performed as follows:
    h(t)
    h(t)
k
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))
    h(t)
k (= h(t)
    h(t)
k       dd>(d h(t)   x(t))
    sign(h(t)) 6= sign(h(t)
   
-
k (= h(t)
    h(t)
k       d>(d h(t)   x(t))
k (= h(t)
    h(t)
k       d>(d h(t)   x(t))
    sign(h(t)) 6= sign(h(t)
k         sign(h(t)
if                                                    then: 
-
k ))
    h(t)
k ) 6= sign(h(t)
    sign(h(t)
k         sign(h(t)
k ))
k         sign(h(t)
    sign(h(t)) 6= sign(h(t)
k ))
- else:
    h(t)
k (= h(t)
k         sign(h(t)
    h(t)
k )
    h(t)
k (= 0
    h(t)
k (= 0
k (= h(t)
    h(t)
k         sign(h(t)
k         sign(h(t)
k (= h(t)
    h(t)

    h(t)
k = 0
    h(t)
    h(t)
k (= h(t)
k       dd>(d h(t)   x(t))
    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))
 
k         sign(h(t)
update from reconstruction
k ))

k (= 0
k (= h(t)

k         sign(h(t)
k )

update from sparsity

k = 0

k = 0

 

@

7

k

k

k

    h(x(t))k (= h(x(t))k       dd>(d h(t)   x(t))

k

k       dd>(d h(t)   x(t))
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))
   
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))
topics: ista (iterative shrinkage and thresholding algorithm)
k         sign(h(t)
k ))
k         sign(h(t)
k )
    this process corresponds to the ista algorithm:
k         sign(h(t)
k )

sparse coding

bx(t) = d h(x(t)) = xk s.t.

h(x(t)) = arg min

h(t)

   

h(x(t))k6=0

2||x(t)   d h(t)||2

d  ,k h(x(t))k

    initialize       (for instance to 0)
    while       has not converged

l(x(t)) = ||x(t)   d h(t)||2
h(t) (= h(t)       d>(d h(t)   x(t))
@
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
h(t) (= h(t)       d>(d h(t)   x(t))
@h(t)
 
k
h(t) (= h(t)       d>(d h(t)   x(t))
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
 
h(t) (= shrink(h(t),      )

2 +  ||h(t)||1

   

-
-

1

1

   

    return 

h(t) (= shrink(h(t),       sign(h(t)))

1

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

h(t) (= shrink(h(t),       sign(h(t)))
   
k = 0

    h(t)
    h(t)
   
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi), . . . ]
    here
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
    h(t)
    will converge if     is bigger than the largest eigenvalue of 
    1
    1
    d>d
    h(t)
k (= h(t)
    sign(h(t)) 6= sign(h(t)
txt=1

    d>d
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))

k

8

k       dd>(d h(t)   x(t))
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))
   
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))
topics: ista (iterative shrinkage and thresholding algorithm)
k         sign(h(t)
k ))
k         sign(h(t)
k )
    this process corresponds to the ista algorithm:
k         sign(h(t)
k )

sparse coding

bx(t) = d h(x(t)) = xk s.t.

h(x(t)) = arg min

h(t)

   

   

   

h(x(t))k6=0

2||x(t)   d h(t)||2

d  ,k h(x(t))k
h(t) (= shrink(h(t),       sign(h(t)))

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

1

1

   

1

-
-

   

    d>d

    initialize       (for instance to 0)
    while       has not converged

l(x(t)) = ||x(t)   d h(t)||2
h(t) (= h(t)       d>(d h(t)   x(t))
    shrink(ai, bi)
@
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
h(t) (= h(t)       d>(d h(t)   x(t))
    1
    d>d
   
@h(t)
 
k
h(t) (= h(t)       d>(d h(t)   x(t))
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
 
h(t) (= shrink(h(t),      )

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

h(t) (= shrink(h(t),       sign(h(t)))

2 +  ||h(t)||1

h(t) (= shrink(h(t),       sign(h(t)))

h(t) (= shrink(h(t),       sign(h(t)))

h(t) (= shrink(h(t),       sign(h(t)))
   
k = 0

   
    return 
    h(t)
    1
    h(t)
   
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi), . . . ]
    here
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
    h(t)
    will converge if     is bigger than the largest eigenvalue of 
    1
    1
    d>d
    h(t)
k (= h(t)
    sign(h(t)) 6= sign(h(t)
txt=1

    d>d
    d>d
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

    1

   

   

k

8

k       dd>(d h(t)   x(t))
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))
   
k       dd>(d h(t)   x(t))
k         sign(h(t)
k ))
topics: ista (iterative shrinkage and thresholding algorithm)
k         sign(h(t)
k ))
k         sign(h(t)
k )
    this process corresponds to the ista algorithm:
k         sign(h(t)
k )

sparse coding

bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

sparse coding

hugo larochelle

h(x(t)) = arg min

h(t)

   

   

   

h(x(t))k6=0

2||x(t)   d h(t)||2

d  ,k h(x(t))k
h(t) (= shrink(h(t),       sign(h(t)))

d

   

1

1

   

-
-

1
t

   

arg min

abstract

    d>d

math for my slides    sparse coding   .

2 +  ||h(t)||1

h(t) (= shrink(h(t),       sign(h(t)))

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
arg min

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

november 1, 2012
l(x(t)) = ||x(t)   d h(t)||2
h(t) (= h(t)       d>(d h(t)   x(t))
    shrink(ai, bi)
@
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
h(t) (= h(t)       d>(d h(t)   x(t))
    1
    d>d
   
@h(t)
 
k
h(t) (= h(t)       d>(d h(t)   x(t))
rh(t)l(x(t)) = d>(d h(t)   x(t)) +   sign(h(t))
 
h(t) (= shrink(h(t),      )
txt=1

    initialize       (for instance to 0)
    while       has not converged
    x(t) h(t) d bx(t)
   
    return 
    h(t)
    1
    h(t)
   
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi), . . . ]
    here
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
    h(t)
    will converge if     is bigger than the largest eigenvalue of 
    1
    1
    d>d
    h(t)
k (= h(t)
    sign(h(t)) 6= sign(h(t)
txt=1

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
    d>d
bx(t) = d h(x(t)) = xk s.t.
    d>d
h(x(t))k6=0
k       dd>(d h(t)   x(t))
l(x(t)) = ||x(t)   d h(t)||2
2 +  ||h(t)||1
k         sign(h(t)
k ))

h(t) (= shrink(h(t),       sign(h(t)))
   
2 +  ||h(t)||1
k = 0

h(t) (= shrink(h(t),       sign(h(t)))

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

   
h(x(t)) = arg min
    1

h(t) (= shrink(h(t),       sign(h(t)))

1
2||x(t)   d h(t)||2

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1

d  ,k h(x(t))k

this is 

   

1

h(t)

h(t)

   

   

k

8

sparse coding

   
topics: coordinate descent for sparse coding id136
    ista updates all hidden units simultaneously  

    this is wasteful if many hidden units have already converged

   

    idea: update only the       most promising       hidden unit

    see coordinate descent algorithm in 

gregor and lecun, 2010.

- learning fast approximations of sparse coding.

    shrink(ai, bi)
    1
    this algorithm has the advantage of not requiring a learning rate 
   

    d>d

9

november 1, 2012

h(t) (= shrink(h(t),       sign(h(t)))

sparse coding
h(t) (= shrink(h(t),       sign(h(t)))
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]

abstract

math for my slides    sparse coding   .
    shrink(ai, bi)

topics: dictionary update (algorithm 1)
    going back to our original problem

    x(t) h(t) d bx(t)

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

h(t)

1
t

1
t

min
d

min
h(t)

1
h(t)
t

txt=1

l(x(t)) = min
d

arg min
l(x(t)) = min
min
d
d
h(t)

2 +  ||h(t)||1
2 +  ||h(x(t))||1

1
    we must minimize
t

    let   s assume          doesn   t depend on     (which is false)
min
d

shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
1
2||x(t)   d h(t)||2
arg min
txt=1
1
1
2||x(t)   d h(x(t))||2
t
math for my slides    sparse coding   .

txt=1
txt=1
1
txt=1
2||x(t)   d h(t)||2
h(x(t)) = arg min
    x(t) h(t) d
1
2||x(t)   d h(x(t))||2
bx(t) = d h(x(t)) = xk s.t.
txt=1
1
2||x(t)   d h(x(t))||2
2
h(x(t))k6=0
    x(t) h(t) d
l(x(t)) = ||x(t)   d h(t)||2
d
l(x(t)) = (d  ,k)>(d h(t)   x(t)) +   sign(h(t)
k )

    we must also constrain the columns of      to be of unit norm
2 +  ||h(t)||1

math for my slides    sparse coding   .

2 +  ||h(t)||1
2 +  ||h(x(t))||1

d  ,k h(x(t))k

arg min

min
d

1
t

   

@

arg min

d

h(x(t)) = arg min

1
t
10

txt=1

sparse coding

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo larochelle
november 1, 2012

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

1
2

= min
d

txt=1
t    txt=1

= min
= min
d
d

2

1

1

1

1

1

1

1

1
2

1
t

1
t

1
t

1
1
2
t

min
d

d (= d +    

november 1, 2012

hugo.larochelle@usherbrooke.ca
1

2   d h(x(t))   > d h(x(t))
2   d h(x(t))   > d h(x(t))
txt=1
txt=1
x(t)>x(t)   x(t)>d h(x(t)) +
x(t)>x(t)   x(t)>d h(x(t)) +
1
2||x(t)   d h(x(t))||2
sparse coding
2  txt=1   d h(x(t))   > d h(x(t)!
t    txt=1
x(t)>d h(x(t))! +
2  txt=1   d h(x(t))   > d h(x(t)!
x(t)>d h(x(t))! +
2   d h(x(t))   > d h(x(t))
txt=1
x(t)>x(t)   x(t)>d h(x(t)) +
topics: dictionary update (algorithm 1)
x(t)>d h(x(t))! +
t    txt=1
2  txt=1   d h(x(t))   > d h(x(t)!
abstract
    a id119 method could be used here too
= min
math for my slides    sparse coding   .
    speci   cally, this is a projected id119 algorithm
d
txt=1
1
math for my slides    sparse coding   .
    while     hasn   t converged
(x(t)   d h(x(t))) h(x(t))>
txt=1
t
(x(t)   d h(x(t))) h(x(t))>
txt=1
    perform gradient update of
    x(t) h(t) d
arg min
arg min
2 =  txt=1
x(t) h(x(t))>!  txt=1
txt=1
1
1
d (= d +    
x(t) h(x(t))>!  txt=1
2 =  txt=1
2||x(t)   d h(x(t))||2
t
    renormalize the columns of 
    x(t) h(t) d
   
2 =  txt=1
-
1
1
2||x(t)   d h(x(t))||2
d  ,j
t
||d  ,j||2
x(t) h(x(t))>!  txt=1

1
2||x(t)   d h(t)||2
txt=1
h(x(t)) h(x(t))>! 1
arg min
h(x(t)) h(x(t))>! 1
1
2 +  ||h(t)||1
2||x(t)   d h(t)||2
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
txt=1
h(x(t)) = arg min
arg min
d
h(t)
h(x(t)) h(x(t))>! 1

1
(x(t)   d h(x(t))) h(x(t))>
t

d  ,j (=
t=1 h(x(t)) h(x(t))> d (= b a 1

d (=  txt=1

math for my slides    sparse coding   .
d

11
h(x(t)) = arg min

d  ,j (= d  ,j/||d  ,j||2

h(x(t)) = arg min

for each column          :

arg min
h(t)

abstract

abstract

arg min

1
t

1
t

h(t)

h(t)

h(t)

   

d

    x(t) h(t) d
d (= d +    
txt=1

1
t

2||x(t)   d h(x(t))||2

arg min

txt=1
t=1 x(t) h(x(t))> b (=pt

d

november 1, 2012

t=1 x(t) h(x(t))> b (=pt

1

   

1
t

1
1
t

= min
d

arg min

arg min

txt=1
2   d h(x(t))   > d h(x(t))
txt=1
txt=1
txt=1
x(t)>d h(x(t))! +
t    txt=1
2  txt=1   d h(x(t))   > d h(x(t)!
1
(x(t)   d h(x(t))) h(x(t))>
d (= d +    
2||x(t)   d h(x(t))||2
min
sparse coding
d
2 =  txt=1
txt=1
1
1
txt=1
2||x(t)   d h(x(t))||2
x(t) h(x(t))>!  txt=1
2 =  txt=1
x(t)>x(t)   x(t)>d h(x(t)) +
t
1
2||x(t)   d h(x(t))||2
txt=1
x(t)>d h(x(t))! +
t    txt=1
(x(t)   d h(x(t))) h(x(t))>
t=1 x(t) h(x(t))> b (=pt
t=1 h(x(t)) h(x(t))> d (= b a 1

1
t
   
    setting the gradient for        to zero, we have
d  ,j (= d  ,j/||d  ,j||2
(x(t)   d h(x(t))) h(x(t))j

1
2
topics: dictionary update (algorithm 2)
    an alternative is to solve for each column       in cycle:

= min
d
1
t
= min
d
d  ,j (=

t=1 h(x(t)) h(x(t))> d (= b a 1

d  ,j (= d  ,j/||d  ,j||2

txt=1
    a (=pt
t=1 x(t) h(x(t))> b (=pt

1
d  ,j
||d  ,j||2

d (= d +    

x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1

1
t

0 =

   

   

d

d

2

1
t

0 =

j =

(x(t)   d h(x(t))) h(x(t))>

b (=   b + (1    ) x(t) h(x(t))>

txt=1
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
1
txt=1
d (= d +    
b (=   b + (1    ) x(t) h(x(t))>
t
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
2 =  txt=1
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
txt=1
1
2||x(t)   d h(x(t))||2
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
d  ,j =
pt
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
t=1 x(t) h(x(t))> b (=pt

d  ,j (= d  ,j/||d  ,j||2

x(t) h(x(t))>!  txt=1

d
t=1 h(x(t))2
j

arg min
1

1
t

12

d  ,jh(x(t))2

   

txt=1
   

   

1

    a (=pt

    we don   t need to specify a learning rate to update

1

2  txt=1   d h(x(t))   > d h(x(t)!

1

1

d

   

(3)

(2)

0 =

1
t

0 =

j =

txt=1

t=1 h(x(t))2
j

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,j =
t=1 h(x(t))2
j

d  ,j =
1

arg min
j =

topics: dictionary update (algorithm 2)
t=1 h(x(t))2
j
    an alternative is to solve for each column       in cycle:

txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
txt=1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
sparse coding
txt=1
txt=1
2 =  txt=1
x(t) h(x(t))>!  txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
j =
txt=1
1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
2||x(t)   d h(x(t))||2
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
pt
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
   
txt=1
txt=1
(4)
d  ,j =
pt
d  ,j (= d  ,j/||d  ,j||2
pt
    we can rewrite
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
t=1 x(t) h(x(t))> b (=pt
    a (=pt
(5)
t=1 h(x(t)) h(x(t))> d (= b a 1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
pt
txt=1
txt=1
   
h(x(t))ih(x(t))j!1a
0@  txt=1
pt
pt
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
b (=   b + (1    ) x(t) h(x(t))>
0@  txt=1
h(x(t))ih(x(t))j!1a
0@  txt=1
h(x(t))ih(x(t))j!1a
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
pt
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
   
pt
pt
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
2 =  txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
txt=1
arg min
    this way, we only need to store:
2
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
txt=1
1
2||x(t)   d h(x(t))||2
    a (=pt
-
t=1 h(x(t)) h(x(t))> b (=pt
-
    b  ,j ai,j ai,i
   

t=1 h(x(t)) h(x(t))> b (=pt

1
2||x(t)   d h(x(t))||2

t=1 h(x(t))2
j
   

t=1 h(x(t))2
j
=

d  ,j =
t=1 h(x(t))2
j

t=1 x(t) h(x(t))>

t=1 x(t) h(x(t))>

d  ,j =
1

t=1 h(x(t))2
j

t=1 h(x(t))2
j

t=1 h(x(t))2
j

d  ,j =

arg min

1
t

1
t

(8)

(6)

(7)

=

13

=

1

1

1

1

 

d

1

1

1

   

(3)

(2)

0 =

1
t

0 =

j =

txt=1

d  ,j =

t=1 h(x(t))2
j

t=1 h(x(t))2
j

t=1 h(x(t))2
j

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,j =
t=1 h(x(t))2
j

arg min
j =
d
d  ,j =

txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
pt
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
sparse coding
txt=1
txt=1
x(t) h(x(t))>!  txt=1
2 =  txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
j =
txt=1
1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
2||x(t)   d h(x(t))||2
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
1
txt=1
topics: dictionary update (algorithm 2)
pt
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
d  ,j =
pt
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
   
t=1 h(x(t))2
h(x(t))ih(x(t))j!1a
0@  txt=1
txt=1
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
txt=1
j
    an alternative is to solve for each column       in cycle:
1
(4)
d  ,j =
1
pt
d  ,j (= d  ,j/||d  ,j||2
pt
=
pt
t=1 h(x(t))2
j
    we can rewrite
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
t=1 x(t) h(x(t))> b (=pt
    a (=pt
(5)
t=1 h(x(t)) h(x(t))> d (= b a 1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
pt
txt=1
txt=1
   
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
h(x(t))ih(x(t))j!1a
0@  txt=1
pt
pt
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
txt=1
b (=   b + (1    ) x(t) h(x(t))>
h(x(t))ih(x(t))j!1a
0@  txt=1
0@  txt=1
h(x(t))ih(x(t))j!1a
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
pt
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
   
pt
t=1 h(x(t)) h(x(t))> b (=pt
    a (=pt
pt
t=1 x(t) h(x(t))>
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
2 =  txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
txt=1
arg min
    this way, we only need to store:
2
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
txt=1
1
2||x(t)   d h(x(t))||2
    a (=pt
-
t=1 h(x(t)) h(x(t))> b (=pt
-
    b  ,j ai,j ai,i
   

b (=   b + (1    ) x(t) h(x(t))>
t=1 x(t) h(x(t))>

t=1 h(x(t)) h(x(t))> b (=pt

1
2||x(t)   d h(x(t))||2
t=1 h(x(t))2
j

    b  ,j ai,j aj,j
   

1
2||x(t)   d h(x(t))||2

t=1 h(x(t))2
j
   

t=1 h(x(t))2
j
arg min
=

d  ,j =
t=1 h(x(t))2
j

t=1 x(t) h(x(t))>

d  ,j =
1

t=1 h(x(t))2
j

t=1 h(x(t))2
j

d  ,j =

 

arg min

1
t

1
t

(8)

(6)

(7)

1
t

1
   

=

13

=

2

1

1

1

 

d

1

d

1

1

1

   

(3)

(2)

0 =

1
t

0 =

j =

txt=1

d  ,j =

t=1 h(x(t))2
j

t=1 h(x(t))2
j

t=1 h(x(t))2
j

d  ,j =

t=1 h(x(t))2
j

t=1 h(x(t))2
j

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,j =
t=1 h(x(t))2
j

arg min
j =
d
d  ,j =

txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
txt=1
pt
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
pt
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
sparse coding
txt=1
txt=1
x(t) h(x(t))>!  txt=1
2 =  txt=1
0@x(t)  0@xi6=j
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
j =
txt=1
1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
2||x(t)   d h(x(t))||2
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
pt
1
txt=1
topics: dictionary update (algorithm 2)
pt
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
d  ,j =
0@  txt=1
pt
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
x(t)h(x(t))j!  xi6=j
   
t=1 h(x(t))2
h(x(t))ih(x(t))j!1a
0@  txt=1
txt=1
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
txt=1
j
    an alternative is to solve for each column       in cycle:
1
(4)
d  ,j =
1
pt
d  ,j (= d  ,j/||d  ,j||2
pt
pt
=
pt
t=1 h(x(t))2
t=1 h(x(t))2
j
j
    we can rewrite
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
t=1 x(t) h(x(t))> b (=pt
    a (=pt
(5)
t=1 h(x(t)) h(x(t))> d (= b a 1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
pt
t=1 h(x(t))2
txt=1
txt=1
   
j
2 =  txt=1
   
h(x(t)) h(x(t))>! 1
2 =  txt=1
x(t) h(x(t))>!  txt=1
h(x(t))ih(x(t))j!1a
0@  txt=1
pt
pt
txt=1
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
txt=1
b (=   b + (1    ) x(t) h(x(t))>
1
2||x(t)   d h(x(t))||2
h(x(t))ih(x(t))j!1a
0@  txt=1
0@  txt=1
h(x(t))ih(x(t))j!1a
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
pt
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
t=1 h(x(t))2
j
t=1 h(x(t)) h(x(t))> b (=pt
    a (=pt
   
pt
t=1 h(x(t)) h(x(t))> b (=pt
    a (=pt
pt
t=1 x(t) h(x(t))>
t=1 x(t) h(x(t))>
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
x(t) h(x(t))>!  txt=1
2 =  txt=1
h(x(t)) h(x(t))>! 1
txt=1
arg min
    this way, we only need to store:
2
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
2 =  txt=1
txt=1
1
2||x(t)   d h(x(t))||2
    a (=pt
-
t=1 h(x(t)) h(x(t))> b (=pt
-
    b  ,j ai,j ai,i
   

b (=   b + (1    ) x(t) h(x(t))>
t=1 x(t) h(x(t))>

t=1 h(x(t)) h(x(t))> b (=pt

(6)
1
2||x(t)   d h(x(t))||2
(7)

    b  ,j ai,j ai,i
1
   
2||x(t)   d h(x(t))||2

    b  ,j ai,j aj,j
   

t=1 h(x(t))2
j
   

  

t=1 h(x(t))2
j
arg min
=

d  ,j =
t=1 h(x(t))2
j

t=1 x(t) h(x(t))>

d  ,j =
1

t=1 h(x(t))2
j

d  ,j =

arg min

arg min

1
t

1
t

1
t

(8)

1
t

   

1
   

=

=

13

=

1

d

2

1

1

1

 

d

1

d

b (=   b + (1    ) x(t) h(x(t))>

a (=   a + (1    ) h(x(t +1)) h(x(t +1))>

1

1

1

   

(2)

0 =

1
t

0 =

j =

txt=1

d  ,j =

t=1 h(x(t))2
j

t=1 h(x(t))2
j

t=1 h(x(t))2
j

d  ,j =

t=1 h(x(t))2
j

t=1 h(x(t))2
j

(3)
d  ,j =

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,jh(x(t))2

d  ,j =
t=1 h(x(t))2
j

arg min
j =
d
d  ,j =

txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
txt=1
pt
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
pt
d  ,i h(x(t))i1a   d  ,j h(x(t))j1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
sparse coding
txt=1
txt=1
x(t) h(x(t))>!  txt=1
2 =  txt=1
0@x(t)  0@xi6=j
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
j =
txt=1
1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
txt=1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
2||x(t)   d h(x(t))||2
txt=1
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
txt=1
pt
pt
1
txt=1
topics: dictionary update (algorithm 2)
pt
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
d  ,j =
0@  txt=1
pt
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
x(t)h(x(t))j!  xi6=j
   
t=1 h(x(t))2
h(x(t))ih(x(t))j!1a
0@  txt=1
txt=1
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
txt=1
j
    an alternative is to solve for each column       in cycle:
1
(4)
d  ,j =
1
pt
=
d  ,j (= d  ,j/||d  ,j||2
pt
pt
pt
=
pt
t=1 h(x(t))2
t=1 h(x(t))2
j
j
    we can rewrite
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
t=1 x(t) h(x(t))> b (=pt
    a (=pt
(5)
t=1 h(x(t)) h(x(t))> d (= b a 1
txt=1
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
0@x(t)  0@xi6=j
d  ,i h(x(t))i1a1a h(x(t))j
pt
t=1 h(x(t))2
txt=1
txt=1
   
j
2 =  txt=1
   
(6)
2 =  txt=1
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
h(x(t))ih(x(t))j!1a
0@  txt=1
pt
pt
txt=1
txt=1
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
txt=1
1
1
b (=   b + (1    ) x(t) h(x(t))>
1
2||x(t)   d h(x(t))||2
2||x(t)   d h(x(t))||2
h(x(t))ih(x(t))j!1a
0@  txt=1
0@  txt=1
h(x(t))ih(x(t))j!1a
t
d  ,i   txt=1
x(t)h(x(t))j!  xi6=j
pt
x(t)h(x(t))j!  xi6=j
d  ,i   txt=1
t=1 h(x(t))2
j
t=1 h(x(t)) h(x(t))> b (=pt
t=1 h(x(t)) h(x(t))> b (=pt
    a (=pt
    a (=pt
   
pt
(7)
t=1 h(x(t)) h(x(t))> b (=pt
    a (=pt
pt
t=1 x(t) h(x(t))>
t=1 x(t) h(x(t))>
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
  
    b  ,j ai,j ai,i
(8)
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
txt=1
   
arg min
    this way, we only need to store:
2
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
2 =  txt=1
b (=   b + (1    ) x(t) h(x(t))>
txt=1
1
2||x(t)   d h(x(t))||2
    a (=pt
-
t=1 h(x(t)) h(x(t))> b (=pt
-
    b  ,j ai,j ai,i
   

b (=   b + (1    ) x(t) h(x(t))>
   
t=1 x(t) h(x(t))>

t=1 h(x(t)) h(x(t))> b (=pt

    b  ,j ai,j ai,i
1
   
2||x(t)   d h(x(t))||2

    b  ,j ai,j aj,j
   

t=1 h(x(t))2
j
   

t=1 h(x(t))2
j
arg min
=

d  ,j =
t=1 h(x(t))2
j

t=1 x(t) h(x(t))>

d  ,j =
1

 

t=1 h(x(t))2
j

d  ,j =

arg min

arg min

arg min

1
t

1
t

1
t

1
t

   

   

1
   

=

=

13

=

1

d

d

2

1

1

1

 

d

1

d

a (=   a + (1    ) h(x(t +1)) h(x(t +1))>

(4)

abstract

2   d h(x(t))   > d h(x(t))
h(x(t)) h(x(t))>! 1
2  txt=1   d h(x(t))   > d h(x(t)!

1

1
2||x(t)   d h(t)||2

2

1

t

1
t

1
t

1
t

min
d

txt=1

= min
d

= min
d

d (= d +    

t    txt=1

(x(t)   d h(x(t))) h(x(t))>

topics: dictionary update (algorithm 2)
math for my slides    sparse coding   .
    while     hasn   t converged

2  txt=1   d h(x(t))   > d h(x(t)!
txt=1
txt=1
1
2||x(t)   d h(x(t))||2
(x(t)   d h(x(t))) h(x(t))>
t
sparse coding
txt=1
1
1
x(t)>x(t)   x(t)>d h(x(t)) +
2 =  txt=1
x(t) h(x(t))>!  txt=1
txt=1
2
1
d (= d +    
2||x(t)   d h(x(t))||2
t    txt=1
x(t)>d h(x(t))! +
    x(t) h(t) d
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
2 =  txt=1
txt=1
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
    for each column         perform gradient update of
1
d  ,j (= d  ,j/||d  ,j||2
2||x(t)   d h(x(t))||2
t
 
h(t)
t=1 x(t) h(x(t))> b (=pt
d  ,j (=
(b  ,j   d a  ,j + d  ,j aj,j)
t=1 h(x(t)) h(x(t))> d (= b a 1
txt=1
(x(t)   d h(x(t))) h(x(t))>
2 =  txt=1

1
aj,j
d  ,j
d (= d +    
b (=   b + (1    ) x(t) h(x(t))>
||d  ,j||2

txt=1
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
    this is referred to as a block-coordinate descent algorithm
arg min
b (=   b + (1    ) x(t) h(x(t))>
    a different block of variables are updated at each step
    the       blocks       are the columns 
d  ,j (= d  ,j/||d  ,j||2

x(t) h(x(t))>!  txt=1

t=1 h(x(t)) h(x(t))> d (= b a 1

1
2||x(t)   d h(x(t))||2

1
h(x(t)) = arg min
t

 
d  ,j (=

arg min

arg min

1
t

h(t)

d

d

-

-

    a (=pt

a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
t=1 x(t) h(x(t))> b (=pt

t=1 h(x(t)) h(x(t))> d (= b a 1

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1
h(x(t)) h(x(t))>! 1

14

2

2

d

d

d

   

1
t

1
t

1
t

h(t)

min
d

min
h(t)

arg min

    d>d

    1
txt=1

2 +  ||h(x(t))||1

    x(t) h(t) d

1
2||x(t)   d h(x(t))||2

1
arg min
t
1
2||x(t)   d h(x(t))||2

min
d
1
l(x(t)) = min
t
d
math for my slides    sparse coding   .

hugo.larochelle@usherbrooke.ca
txt=1
txt=1
1
sparse coding
l(x(t)) = min
min
t
txt=1
abstract
november 1, 2012
d
h(t)
1
2 +  ||h(x(t))||1
2||x(t)   d h(x(t))||2
min
d
txt=1
1
1
topics: learning algorithm (putting it all together)
2||x(t)   d h(x(t))||2
min
t
txt=1
d
1
    x(t) h(t) d
2||x(t)   d h(x(t))||2
    learning alternates between id136 and dictionary learning
abstract
   
txt=1
2 =  txt=1
1
1
2||x(t)   d h(t)||2
txt=1
arg min
arg min
math for my slides    sparse coding   .
1
2||x(t)   d h(x(t))||2
t
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
2 =  txt=1
txt=1
1
    while     has not converged
t
d (=  txt=1
x(t) h(x(t))>!  txt=1
txt=1
1
       nd the sparse codes              for all        in my training set with ista
2||x(t)   d h(t)||2
    x(t) h(t) d
arg min
x(t) h(x(t))>!  txt=1
d (=  txt=1
h(x(t)) h(x(t))>! 1
    update the dictionary:
t=1 x(t) h(x(t))> b (=pt
    a (=pt
 
t=1 h(x(t)) h(x(t))> d (= b a 1
t=1 x(t) h(x(t))> b (=pt
 
h(x(t)) = arg min
t=1 h(x(t)) h(x(t))> d (= b a 1
   
x(t) h(x(t) (=    txt=1
t +1xt=1
 run block-coordinate descent algorithm to update 
    x(t) h(t) d
x(t) h(x(t) (=    txt=1
x(t) h(x(t))>! + (1    )x(t +1) h(x(t +1))>
t +1xt=1
    similar to the em algorithm
h(x(t)) h(x(t) (=    txt=1
t +1xt=1
h(x(t)) h(x(t) (=    txt=1
h(x(t)) h(x(t))>! + (1    )h(x(t +1)) h(x(t +1))>
t +1xt=1

x(t) h(x(t))>!  txt=1
2 +  ||h(t)||1
math for my slides    sparse coding   .
h(x(t)) h(x(t))>! 1
1
2 +  ||h(t)||1
2||x(t)   d h(t)||2

math for my slides    sparse coding   .
1
2||x(t)   d h(t)||2

h(x(t)) = arg min

2 +  ||h(t)||1
x(t) h(x(t))>! + (1    )x(t +1) h(x(t +1))>
h(x(t)) h(x(t))>! + (1    )h(x(t +1)) h(x(t +1))>

arg min

arg min

1
t
h(t)

arg min

-
-
   
-

d
15

h(t)

h(t)

   

   

d

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

universit  e de sherbrooke

2 +  ||h(x(t))||1
hugo.larochelle@usherbrooke.ca
hugo larochelle
abstract

sparse coding

november 1, 2012

d  epartement d   informatique
universit  e de sherbrooke
abstract
arg min

hugo.larochelle@usherbrooke.ca

arg min

2

2

d

h(t)

t
1
t

min
d

min
1
d
t

txt=1

txt=1

txt=1
2||x(t)   d h(x(t))||2
1
2||x(t)   d h(x(t))||2

txt=1
txt=1
sparse coding
txt=1
1
2||x(t)   d h(x(t))||2
november 1, 2012
2 =  txt=1
h(x(t)) h(x(t))>! 1
2 =  txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
    single update of the dictionary per pass on the training set
x(t) h(x(t))>!  txt=1
math for my slides    sparse coding   .
    update      after visiting each        and id136 its code
t=1 h(x(t)) h(x(t))> d (= b a 1
t=1 h(x(t)) h(x(t))> d (= b a 1
    keep running averages of the quantities required to update     :
-
-

x(t) h(x(t))>!  txt=1
topics: online learning algorithm
txt=1
1
x(t) h(x(t))>!  txt=1
2||x(t)   d h(x(t))||2
2||x(t)   d h(x(t))||2
    this algorithm is       batch       (i.e. not online)
    x(t) h(t) d
d (=  txt=1
h(x(t)) h(x(t))>! 1
h(x(t)) h(x(t))>! 1
d (=  txt=1
    solution:
   
t=1 x(t) h(x(t))> b (=pt
    x(t) h(t) d
    x(t) h(t) d
t=1 x(t) h(x(t))> b (=pt
txt=1

 
b (=   b + (1    ) x(t) h(x(t))>
 
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>

b (=   b + (1    ) x(t) h(x(t))>
arg min
d

math for my slides    sparse coding   .

math for my slides    sparse coding   .

math for my slides    sparse coding   .

txt=1

arg min

    use current value of      as       warm start       to block-coordinate descent
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>

1
t

h(t)

1
t

   

   

d

d

    x(t) h(t) d

h(t)

november 1, 2012
h(x(t)) = arg min
1
2||x(t)   d h(t)||2

arg min
abstract

h(t)

1
h(x(t)) = arg min
2||x(t)   d h(t)||2
txt=1
1
t

h(t)
arg min

arg min

h(t)

d

16

h(x(t)) = arg min

1
2||x(t)   d h(t)||2

   

hugo.larochelle@usherbrooke.ca

november 1, 2012

2

2

2

1

(1)

1
t

min
d

1
t

min

min
d

min
h(t)

min
min
h(t)
d

arg min

min
1
d
t

txt=1

= min
d

l(x(t)) = min
d

txt=1

txt=1

abstract

abstract

2 +  ||h(x(t))||1

h(t) (= shrink(h(t),       sign(h(t)))

1
1
2||x(t)   d h(x(t))||2
november 1, 2012
t

november 1, 2012
2 +  ||h(x(t))||1

txt=1
1
1
l(x(t)) = min
2||x(t)   d h(x(t))||2
l(x(t)) = min
d
1
t
d
2||x(t)   d h(x(t))||2
min
d
1
t

txt=1
1
txt=1
txt=1
t
1
sparse coding
t
txt=1
1
2||x(t)   d h(x(t))||2
txt=1
1
2||x(t)   d h(x(t))||2
abstract
2 =  txt=1
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
topics: online learning algorithm
math for my slides    sparse coding   .
sparse coding
txt=1
1
1
2||x(t)   d h(x(t))||2
math for my slides    sparse coding   .
2 =  txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2   d h(x(t))   > d h(x(t))
txt=1
1
1
t
x(t)>x(t)   x(t)>d h(x(t)) +
txt=1
1
    initialize     (not to 0!)
math for my slides    sparse coding   .
2||x(t)   d h(x(t))||2
2
    x(t) h(t) d
txt=1
txt=1
    x(t) h(t) d
1
1
1
2 +  ||h(x(t))||1
2||x(t)   d h(x(t))||2
2  txt=1   d h(x(t))   > d h(x(t)!
x(t)>d h(x(t))! +
t    txt=1
min
x(t) h(x(t))>!  txt=1
d (=  txt=1
h(x(t)) h(x(t))>! 1
txt=1
txt=1
t
t
math for my slides    sparse coding   .
h(t)
    while     hasn   t converged
d  epartement d   informatique
1
1
1
2 +  ||h(t)||1
2||x(t)   d h(t)||2
    x(t) h(t) d
= min
arg min
arg min
2||x(t)   d h(t)||2
arg min
arg min
x(t) h(x(t))>!  txt=1
d (=  txt=1
h(x(t)) h(x(t))>! 1
universit  e de sherbrooke
d
txt=1
d
1
1
txt=1
    for each     
(1)
2||x(t)   d h(x(t))||2
1
1
d
h(t)
hugo.larochelle@usherbrooke.ca
    x(t) h(t) d
t
2||x(t)   d h(t)||2
arg min
arg min
t=1 x(t) h(x(t))> b (=pt
2   d h(x(t))   > d h(x(t))
txt=1
txt=1
t=1 h(x(t)) h(x(t))> d (= b a 1
t
x(t)>x(t)   x(t)>d h(x(t)) +
1
(2)
infer code                 
1
-
d
november 1, 2012
2||x(t)   d h(t)||2
2 +  ||h(t)||1
t=1 x(t) h(x(t))> b (=pt
arg min
txt=1
t=1 h(x(t)) h(x(t))> d (= b a 1
x(t)>d h(x(t))! +
t    txt=1
2  txt=1   d h(x(t))   > d h(x(t)!
1
1
(x(t)   d h(x(t))) h(x(t))>
t
d (= d +    
- update dictionary
h(x(t)) = arg min
2||x(t)   d h(t)||2
(3)
t
b (=   b + (1    ) x(t) h(x(t))>
1
     
b (=   b + (1    ) x(t) h(x(t))>
2||x(t)   d h(t)||2
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
   
txt=1
     
1
1
math for my slides    sparse coding   .
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
2||x(t)   d h(x(t))||2
txt=1
(x(t)   d h(x(t))) h(x(t))>
t
d (= d +    
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
    while      hasn   t converged
    x(t) h(t) d
txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
1
1
    for each column         perform gradient update
2||x(t)   d h(t)||2
d  ,j (= d  ,j/||d  ,j||2
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
1
2||x(t)   d h(x(t))||2
t
     
t=1 x(t) h(x(t))> b (=pt
(b  ,j   d a  ,j + d  ,j aj,j)
d  ,j (=
t=1 h(x(t)) h(x(t))> d (= b a 1
     
d  ,j (=

d
h(t)
h(x(t)) = arg min

h(x(t)) = arg min

hugo larochelle

h(x(t)) = arg min

2 +  ||h(t)||1

abstract

arg min

(4)
abstract

arg min

arg min

1
t

arg min

= min
d

= min
d

txt=1

h(t)

h(t)

h(t)

h(t)

   

1
t

arg min

min
d

h(t)

h(t)

(2)

(3)

(4)

h(t)

   

1
t

1
t

1
t

1
2

d

d

1

1

1

2

1
aj,j
d  ,j
||d  ,j||2
b (=   b + (1    ) x(t) h(x(t))>

h(x(t)) = arg min

h(t)

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1

17

t=1 h(x(t)) h(x(t))> d (= b a 1

t=1 x(t) h(x(t))> b (=pt

1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

1
2||x(t)   d h(t)||2

2

2

2

1

(1)

1
t

min
d

1
t

min

min
d

min
h(t)

min
min
h(t)
d

arg min

min
1
d
t

txt=1

= min
d

l(x(t)) = min
d

txt=1

txt=1

abstract

abstract

2 +  ||h(x(t))||1

h(t) (= shrink(h(t),       sign(h(t)))

hugo.larochelle@usherbrooke.ca

txt=1
1
1
l(x(t)) = min
2||x(t)   d h(x(t))||2
l(x(t)) = min
d
1
t
d
2||x(t)   d h(x(t))||2
min
d
1
t

1
1
2||x(t)   d h(x(t))||2
november 1, 2012
t

november 1, 2012

november 1, 2012
2 +  ||h(x(t))||1

txt=1
1
txt=1
txt=1
t
1
sparse coding
t
txt=1
1
2||x(t)   d h(x(t))||2
txt=1
1
2||x(t)   d h(x(t))||2
abstract
2 =  txt=1
x(t) h(x(t))>!  txt=1
h(x(t)) h(x(t))>! 1
shrink(a, b) = [. . . , sign(ai) max(|ai|   bi, 0), . . . ]
topics: online learning algorithm
math for my slides    sparse coding   .
sparse coding
txt=1
1
1
2||x(t)   d h(x(t))||2
math for my slides    sparse coding   .
2 =  txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2   d h(x(t))   > d h(x(t))
txt=1
1
1
t
x(t)>x(t)   x(t)>d h(x(t)) +
txt=1
1
    initialize     (not to 0!)
math for my slides    sparse coding   .
2||x(t)   d h(x(t))||2
2
    x(t) h(t) d
txt=1
txt=1
    x(t) h(t) d
1
1
1
2 +  ||h(x(t))||1
2||x(t)   d h(x(t))||2
2  txt=1   d h(x(t))   > d h(x(t)!
x(t)>d h(x(t))! +
t    txt=1
min
x(t) h(x(t))>!  txt=1
d (=  txt=1
h(x(t)) h(x(t))>! 1
txt=1
txt=1
t
t
math for my slides    sparse coding   .
h(t)
    while     hasn   t converged
d  epartement d   informatique
1
1
1
2 +  ||h(t)||1
2||x(t)   d h(t)||2
    x(t) h(t) d
= min
arg min
arg min
2||x(t)   d h(t)||2
arg min
arg min
x(t) h(x(t))>!  txt=1
d (=  txt=1
h(x(t)) h(x(t))>! 1
universit  e de sherbrooke
d
txt=1
d
1
1
txt=1
    for each     
(1)
2||x(t)   d h(x(t))||2
1
1
d
h(t)
hugo.larochelle@usherbrooke.ca
    x(t) h(t) d
t
online dictionary learning for sparse coding.
2||x(t)   d h(t)||2
arg min
arg min
t=1 x(t) h(x(t))> b (=pt
2   d h(x(t))   > d h(x(t))
txt=1
mairal, bach, ponce and sapiro, 2009.
txt=1
t=1 h(x(t)) h(x(t))> d (= b a 1
t
x(t)>x(t)   x(t)>d h(x(t)) +
1
(2)
infer code                 
1
1
-
d
november 1, 2012
2||x(t)   d h(t)||2
2 +  ||h(t)||1
t=1 x(t) h(x(t))> b (=pt
2||x(t)   d h(t)||2
arg min
txt=1
t=1 h(x(t)) h(x(t))> d (= b a 1
x(t)>d h(x(t))! +
t    txt=1
2  txt=1   d h(x(t))   > d h(x(t)!
1
1
(x(t)   d h(x(t))) h(x(t))>
t
d (= d +    
- update dictionary
h(x(t)) = arg min
2||x(t)   d h(t)||2
(3)
t
b (=   b + (1    ) x(t) h(x(t))>
1
     
b (=   b + (1    ) x(t) h(x(t))>
2||x(t)   d h(t)||2
2 +  ||h(t)||1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
   
txt=1
     
1
1
math for my slides    sparse coding   .
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
2||x(t)   d h(x(t))||2
txt=1
(x(t)   d h(x(t))) h(x(t))>
t
d (= d +    
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
    while      hasn   t converged
    x(t) h(t) d
txt=1
h(x(t)) h(x(t))>! 1
x(t) h(x(t))>!  txt=1
2 =  txt=1
1
1
    for each column         perform gradient update
2||x(t)   d h(t)||2
d  ,j (= d  ,j/||d  ,j||2
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
1
2||x(t)   d h(x(t))||2
t
     
t=1 x(t) h(x(t))> b (=pt
(b  ,j   d a  ,j + d  ,j aj,j)
d  ,j (=
t=1 h(x(t)) h(x(t))> d (= b a 1
     
d  ,j (=

d
h(t)
h(x(t)) = arg min

1
2||x(t)   d h(t)||2

h(x(t)) = arg min

hugo larochelle

h(x(t)) = arg min

2 +  ||h(t)||1

abstract

arg min

(4)
abstract

arg min

arg min

1
t

arg min

= min
d

= min
d

txt=1

h(t)

h(t)

h(t)

h(t)

   

1
t

arg min

min
d

h(t)

h(t)

(2)

(3)

(4)

h(t)

   

1
t

1
t

1
t

1
2

d

d

1

1

1

2

1
aj,j
d  ,j
||d  ,j||2
b (=   b + (1    ) x(t) h(x(t))>

h(x(t)) = arg min

h(t)

1
2||x(t)   d h(t)||2

2 +  ||h(t)||1

17

t=1 x(t) h(x(t))> b (=pt

t=1 h(x(t)) h(x(t))> d (= b a 1

preprocessing

   

   

topics: zca
    before running a sparse coding algorithm, it is bene   cial to 
remove       obvious       structure from the data
    normalize such that mean is 0 and covariance is the identity (whitening)
    this will remove 1st and 2nd order statistical structure
   
   
    zca preprocessing

a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
d  ,j (=
1
d  ,j (=
a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
aj,j
    let the empirical mean be     and the empirical covariance matrix
(b  ,j   d a  ,j + d  ,j aj,j)
d  ,j (=
be                    (in its eigenvalue/eigenvector representation) 
    zca transforms each input    as follows:
    x x (= u      1
   

a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
(b  ,j   d a  ,j + d  ,j aj,j)
d  ,j (=
1
aj,j

    b   b    = u   u>
    b   b    = u   u>
    x x (= u    1u>(x  b  )

    b   b    = u   u>
    b   b    = u   u>

2 u>(x  b  )

18

-

a (=   a + (1    ) h(x(t +1)) h(x(t +1))>
preprocessing
(b  ,j   d a  ,j + d  ,j aj,j)
d  ,j (=

1
aj,j

topics: zca
    after this transformation

    the empirical mean is 0

2 u>(x  b  )

1

t xt

= u      1

= u      1
= 0

u      1

2 u>(x(t)  b  )
x(t)!  b  !
2 u>   1
t xt
2 u> (b    b  )
2 u>(x(t)  b  )      u      1

t   1xt    u      1

19

2 u>(x(t)  b  )   >

topics: zca
= u      1
    after this transformation
= 0

    the empirical covariance matrix is the identity

t xt

u      1

= u      1

2 u>(x(t)  b  )
x(t)!  b  !
2 u>   1
preprocessing
t xt
2 u> (b    b  )
t   1xt    u      1
2 u>(x(t)  b  )      u      1
(x(t)  b  )(x(t)  b  )!>
2 u>  1
t   1xt
2 u> b    u     1

2 u> u     u> u      1

2 u>

2 u>

1

= u      1

= u      1
= u      1
= i

2 u>(x(t)  b  )   >

u      1

2 u>

20

1

= u      1

= u      1

universit  e de sherbrooke

2 u>(x(t)  b  )      u      1
2 u>(x(t)  b  )      u      1
2 u>  1
2 u>  1
t   1xt
t   1xt    u      1
2 u>(x(t)  b  )   >
2 u>(x(t)  b  )      u      1
t   1xt
sparse coding and v1
= u      1
2 u> b    u     1
(x(t)  b  )(x(t)  b  )!>
2 u>  1
2 u>
t   1xt
2 u> b    u     1
november 1, 2012
2 u>
= u      1
2 u> u     u> u      1
= u      1
2 u> u     u> u      1
= i
2 u> b    u     1
abstract

= u      1
hugo.larochelle@usherbrooke.ca
= u      1
= u      1
= u      1
topics: id171
= u      1
= i
    a sparse coding model can be used to extract features
2 u>
= i
    {(x(t), y(t))}
2 u> u     u> u      1
2 u>
    {( x(t), y(t)) }
    {(x(t), y(t))}
    {x(t)}
- this yield a dictionary       from which to infer sparse codes 
    {x(t)}
    {x(t)} h(x(t))
    {(h(x(t)), y(t))} x
1
1
    {( h( x(t)) , y(t)) } x
    {(h(x(t)), y(t))} x
2||x(t)   d h(t)||2
arg min
arg min
t
h(t)

= u      1
    given a labeled training set
= u      1
    {(x(t), y(t))}
    train sparse coding dictionary only on training inputs
= i
    {x(t)}
    x(t) h(t) d
    {(h(x(t)), y(t))} x
    {(x(t), y(t))}
    {(x(t), y(t))}
    {x(t)} h(x(t))
    {x(t)} h(x(t))
    {(h(x(t)), y(t))} x
   
    {(h(x(t)), y(t))} x h(x)

    when classifying test input    , must infer its sparse 
representation           rst, then feed it to the classi   er

    train favorite classi   er on transformed training set

math for my slides    sparse coding   .

h(x(t)) = arg min

txt=1

d

h(t)

1
2||x(t)   d h(t)||2

21

(
1
1
=
=
=
0
1
=
=
=
=
i
sparse coding and v1

topics: id171
    when trained on handwritten digits:

self-taught learning

figure 5. left: example images from the handwritten digit
dataset (top), the handwritten character dataset (middle)

self-taught learning: id21 from unlabeled data

raina, battle, lee, packer and ng.

22

dataset (top), the handwritten character dataset (middle)
and the font character dataset (bottom). right: example
sparse coding bases learned on handwritten digits.

sparse coding and v1

    when features trained on different input distribution

topics: self-taught learning
    self-taught learning: 

table 3. top: classi   cation accuracy on 26-way handwrit-
ten english character classi   cation, using bases trained on
handwritten digits. bottom: classi   cation accuracy on
26-way english font character classi   cation, using bases
trained on english handwritten characters. the numbers
in parentheses denote the accuracy using raw and sparse
coding features together. here, sparse coding features
alone do not perform as well as the raw features, but per-
form signi   cantly better when used in combination with
the raw features.

    train sparse coding dictionary on handwritten digits
    use codes (features) to classify handwritten characters

    example:

digits     english handwritten characters

training set size

raw

pca
39.8% 25.3%
54.8% 54.8%
61.9% 64.5%

39.7%
100
58.5%
500
65.3%
1000
handwritten characters     font characters

self-taught learning: id21 from unlabeled data

sparse coding

training set size

raina, battle, lee, packer and ng.

raw
8.2%

pca
5.7%

sparse coding
7.0% (9.2%)

100

23

a fundamental function of the visual system is to encode the build-
ing blocks of natural scenes   edges, textures and shapes   that sub-
recognition and scene
understanding. essential to this process is the formation of abstract
representations that generalize from specific instances of visual
input. a common view holds that neurons in the early visual system
signal conjunctions of image features1,2, but how these produce
invariant representations is poorly understood. here we propose that
to generalize over similar images, higher-level visual neurons encode
statistical variations that characterize local image regions. we pre-
sent a model in which neural activity encodes the id203 distri-
bution most consistent with a given image. trained on natural
images, the model generalizes by learning a compact set of dictionary
elements for image distributions typically encountered in natural
scenes. model neurons show a diverse range of properties observed
in cortical cells. these results provide a new functional explanation
for nonlinear effects in complex cells3   6 and offer insight into coding
strategies in primary visual cortex (v1) and higher visual areas.

sparse coding and v1

topics: v1 neurons vs. sparse coding
    natural image patches:

previous theoretical work has shown that neurons in the primary
visual cortex form an efficient code adapted to the statistics of natural
images16,17, but this says nothing about how neurons generalize across

    small image regions extracted from an image of nature (forest, grass, ...)

a

b

c

emergence of complex cell properties 
by learning to generalize in natural scenes.
karklin and lewicki, 2009

24

sparse coding and v1

previous theoretical work has shown that neurons in the primary
visual cortex form an efficient code adapted to the statistics of natural
images16,17, but this says nothing about how neurons generalize across

    small image regions extracted from an image of nature (forest, grass, ...)

a fundamental function of the visual system is to encode the build-
ing blocks of natural scenes   edges, textures and shapes   that sub-
recognition and scene
understanding. essential to this process is the formation of abstract
representations that generalize from specific instances of visual
input. a common view holds that neurons in the early visual system
signal conjunctions of image features1,2, but how these produce
invariant representations is poorly understood. here we propose that
to generalize over similar images, higher-level visual neurons encode
statistical variations that characterize local image regions. we pre-
sent a model in which neural activity encodes the id203 distri-
bution most consistent with a given image. trained on natural
images, the model generalizes by learning a compact set of dictionary
elements for image distributions typically encountered in natural
scenes. model neurons show a diverse range of properties observed
in cortical cells. these results provide a new functional explanation
for nonlinear effects in complex cells3   6 and offer insight into coding
strategies in primary visual cortex (v1) and higher visual areas.

signal conjunctions of image features1,2, but how these produce
representations that generalize from specific instances of visual
invariant representations is poorly understood. here we propose that
input. a common view holds that neurons in the early visual system
to generalize over similar images, higher-level visual neurons encode
signal conjunctions of image features1,2, but how these produce
statistical variations that characterize local image regions. we pre-
invariant representations is poorly understood. here we propose that
sent a model in which neural activity encodes the id203 distri-
to generalize over similar images, higher-level visual neurons encode
topics: v1 neurons vs. sparse coding
bution most consistent with a given image. trained on natural
statistical variations that characterize local image regions. we pre-
images, the model generalizes by learning a compact set of dictionary
sent a model in which neural activity encodes the id203 distri-
    natural image patches:
elements for image distributions typically encountered in natural
bution most consistent with a given image. trained on natural
scenes. model neurons show a diverse range of properties observed
images, the model generalizes by learning a compact set of dictionary
in cortical cells. these results provide a new functional explanation
elements for image distributions typically encountered in natural
for nonlinear effects in complex cells3   6 and offer insight into coding
scenes. model neurons show a diverse range of properties observed
strategies in primary visual cortex (v1) and higher visual areas.
in cortical cells. these results provide a new functional explanation
as we scan across a complex natural scene, fixations at multiple
for nonlinear effects in complex cells3   6 and offer insight into coding
locations (for example, on the trunk of a tree or along its edge)
strategies in primary visual cortex (v1) and higher visual areas.
produce a coherent percept of the underlying structure (the bark
as we scan across a complex natural scene, fixations at multiple
texture or the contour of the edge), even though individual images
locations (for example, on the trunk of a tree or along its edge)
collected at the retina are inherently highly variable. figure 1 illus-
produce a coherent percept of the underlying structure (the bark
trates the problem our brain solves so effortlessly: perceptually dis-
texture or the contour of the edge), even though individual images
tinct image regions produce response patterns that are highly
collected at the retina are inherently highly variable. figure 1 illus-
overlapping and cannot be easily distinguished using low-level, linear
trates the problem our brain solves so effortlessly: perceptually dis-
representations. what sort of computations are required to achieve
tinct image regions produce response patterns that are highly
generalization across natural stimuli?
overlapping and cannot be easily distinguished using low-level, linear
early visual neurons are typically described as linear feature detec-
representations. what sort of computations are required to achieve
tors1,2. models developed around this idea can accurately capture the

a

c

a

b

c

c

emergence of complex cell properties 
by learning to generalize in natural scenes.
karklin and lewicki, 2009

24

figure 1 | statistical patterns distinguish local regions of natural scenes.

sparse coding and v1

previous theoretical work has shown that neurons in the primary
visual cortex form an efficient code adapted to the statistics of natural
images16,17, but this says nothing about how neurons generalize across

c
    small image regions extracted from an image of nature (forest, grass, ...)

c

images, the model generalizes by learning a compact set of dictionary
sent a model in which neural activity encodes the id203 distri-
elements for image distributions typically encountered in natural
bution most consistent with a given image. trained on natural
scenes. model neurons show a diverse range of properties observed
images, the model generalizes by learning a compact set of dictionary
in cortical cells. these results provide a new functional explanation
elements for image distributions typically encountered in natural
for nonlinear effects in complex cells3   6 and offer insight into coding
scenes. model neurons show a diverse range of properties observed
in cortical cells. these results provide a new functional explanation
as we scan across a complex natural scene, fixations at multiple
for nonlinear effects in complex cells3   6 and offer insight into coding
locations (for example, on the trunk of a tree or along its edge)
produce a coherent percept of the underlying structure (the bark
as we scan across a complex natural scene, fixations at multiple
texture or the contour of the edge), even though individual images
locations (for example, on the trunk of a tree or along its edge)
collected at the retina are inherently highly variable. figure 1 illus-
produce a coherent percept of the underlying structure (the bark
trates the problem our brain solves so effortlessly: perceptually dis-
texture or the contour of the edge), even though individual images
tinct image regions produce response patterns that are highly
collected at the retina are inherently highly variable. figure 1 illus-
overlapping and cannot be easily distinguished using low-level, linear
trates the problem our brain solves so effortlessly: perceptually dis-
representations. what sort of computations are required to achieve
tinct image regions produce response patterns that are highly
overlapping and cannot be easily distinguished using low-level, linear
early visual neurons are typically described as linear feature detec-
representations. what sort of computations are required to achieve
tors1,2. models developed around this idea can accurately capture the
behaviour of neurons from the retina7 to simple cells in the cortex8
early visual neurons are typically described as linear feature detec-
but, as the examples in fig. 1 illustrate, neither individual features nor
tors1,2. models developed around this idea can accurately capture the
linear transformations can reliably discriminate images of one struc-
behaviour of neurons from the retina7 to simple cells in the cortex8
ture from another. more abstract features are presumably computed
but, as the examples in fig. 1 illustrate, neither individual features nor
in later stages of the visual system, but our knowledge of processing
linear transformations can reliably discriminate images of one struc-

a fundamental function of the visual system is to encode the build-
ing blocks of natural scenes   edges, textures and shapes   that sub-
recognition and scene
understanding. essential to this process is the formation of abstract
representations that generalize from specific instances of visual
input. a common view holds that neurons in the early visual system
signal conjunctions of image features1,2, but how these produce
invariant representations is poorly understood. here we propose that
to generalize over similar images, higher-level visual neurons encode
statistical variations that characterize local image regions. we pre-
sent a model in which neural activity encodes the id203 distri-
bution most consistent with a given image. trained on natural
images, the model generalizes by learning a compact set of dictionary
elements for image distributions typically encountered in natural
scenes. model neurons show a diverse range of properties observed
in cortical cells. these results provide a new functional explanation
for nonlinear effects in complex cells3   6 and offer insight into coding
strategies in primary visual cortex (v1) and higher visual areas.

signal conjunctions of image features1,2, but how these produce
representations that generalize from specific instances of visual
invariant representations is poorly understood. here we propose that
input. a common view holds that neurons in the early visual system
to generalize over similar images, higher-level visual neurons encode
signal conjunctions of image features1,2, but how these produce
statistical variations that characterize local image regions. we pre-
invariant representations is poorly understood. here we propose that
sent a model in which neural activity encodes the id203 distri-
to generalize over similar images, higher-level visual neurons encode
topics: v1 neurons vs. sparse coding
bution most consistent with a given image. trained on natural
statistical variations that characterize local image regions. we pre-
images, the model generalizes by learning a compact set of dictionary
sent a model in which neural activity encodes the id203 distri-
    natural image patches:
elements for image distributions typically encountered in natural
bution most consistent with a given image. trained on natural
scenes. model neurons show a diverse range of properties observed
images, the model generalizes by learning a compact set of dictionary
in cortical cells. these results provide a new functional explanation
elements for image distributions typically encountered in natural
for nonlinear effects in complex cells3   6 and offer insight into coding
scenes. model neurons show a diverse range of properties observed
strategies in primary visual cortex (v1) and higher visual areas.
in cortical cells. these results provide a new functional explanation
as we scan across a complex natural scene, fixations at multiple
for nonlinear effects in complex cells3   6 and offer insight into coding
locations (for example, on the trunk of a tree or along its edge)
strategies in primary visual cortex (v1) and higher visual areas.
produce a coherent percept of the underlying structure (the bark
as we scan across a complex natural scene, fixations at multiple
texture or the contour of the edge), even though individual images
locations (for example, on the trunk of a tree or along its edge)
collected at the retina are inherently highly variable. figure 1 illus-
produce a coherent percept of the underlying structure (the bark
trates the problem our brain solves so effortlessly: perceptually dis-
texture or the contour of the edge), even though individual images
tinct image regions produce response patterns that are highly
collected at the retina are inherently highly variable. figure 1 illus-
overlapping and cannot be easily distinguished using low-level, linear
trates the problem our brain solves so effortlessly: perceptually dis-
representations. what sort of computations are required to achieve
tinct image regions produce response patterns that are highly
generalization across natural stimuli?
overlapping and cannot be easily distinguished using low-level, linear
early visual neurons are typically described as linear feature detec-
representations. what sort of computations are required to achieve
tors1,2. models developed around this idea can accurately capture the

a

a

b

c

c

figure 1 | statistical patterns distinguish local regions of natural scenes.
a, a natural scene with four distinct regions outlined (image courtesy of
e. doi). b, the scatter plot shows the joint output of a pair of linear feature
emergence of complex cell properties 
figure 1 | statistical patterns distinguish local regions of natural scenes.
detectors (oriented gabor filters) for 20 3 20-image patches sampled from
by learning to generalize in natural scenes.
a, a natural scene with four distinct regions outlined (image courtesy of
the four regions. the outputs from different regions are highly overlapping,
karklin and lewicki, 2009
e. doi). b, the scatter plot shows the joint output of a pair of linear feature
indicating that linear features provide no means to distinguish between the
detectors (oriented gabor filters) for 20 3 20-image patches sampled from
regions. c, each column shows the joint output of a different pair of linear

c

24

figure 1 | statistical patterns distinguish local regions of natural scenes.

sparse coding and v1

topics: v1 neurons vs. sparse coding
    when trained on natural image patches

    the dictionary columns 
(      atoms      ) look like edge 
detectors
    each atom is tuned to a
particular position, orientation
and spatial frequency
    v1 neurons in the mammalian
brain have a similar behavior

    suggests that the brain
might be learning a sparse
code of visual stimulus

emergence of simple-cell receptive    eld properties 
by learning a sparse code of natural images.
olshausen and field, 1996.

25

sparse coding and v1

topics: v1 neurons vs. sparse coding

    suggests that the brain
might be learning a sparse
code of visual stimulus

    since then, many other
models have been shown
to learn similar features
    they usually all incorporate
a notion of sparsity

emergence of simple-cell receptive    eld properties 
by learning a sparse code of natural images.
olshausen and field, 1996.

26

conclusion
    we discussed the standard sparse coding model

    ista can be used to infer the sparse codes
    block-coordinate descent can be used to update the dictionary given the 
sparse codes

    the learning algorithm alternates between inferring the codes 
and updating the dictionary
    an online learning variant of this algorithm was presented

    the features extracted by sparse coding:

    can be useful for classi   cation
    are similar to the       features       extracted by real neurons in the primary 
visual cortex

27

