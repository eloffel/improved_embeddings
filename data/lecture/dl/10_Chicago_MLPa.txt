ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

multiclass id28

multilayer id88s (mlps)

stochastic id119 (sgd)

1

multiclass classi   cation

we consider the problem of taking an input x (such as an
image of a hand written digit) and classifying it into some
small number of classes (such as the digits 0 through 9).

2

multiclass classi   cation

assume a population distribution on pairs (x, y) for x     rd
and y     c.
for mnist x is a 28    28 image which we take to be a 784
dimensional vector giving x     r784.
for mnist c is the set {0, . . . , 9}.

assume a sample (x0, y0),
from the population.

. . . ,

(xn   1, yn   1) drawn iid

we want to use the sample to construct a rule for predicting
y given x when we draw new pairs from the population.

3

multiclass id28

assume a sample (x0, y0),
from the population with x     rd and y     {0, . . . , k}.

(xn   1, yn   1) drawn iid

. . . ,

for a new x we compute a score s(  y) for each possible label   y.

s = w x + b

4

multiclass id28 for mnist

j     image pixel

  y     possible image label (0 through 9)

(cid:88)

s(  y) =

w [  y, j] x[j] + b[  y]

j

note that w [  y, :] is an image.

5

softmax

softmax converts scores (or energies or logits) to probabilities.

es(  y)

1
z

(cid:88)

q(  y) =

z =

es(  y)

in vector notation

  y

q = softmax s

6

log loss and id28

let q  (  y|x) be de   ned by a model with parammeters   .
in id28    is the pair (w, b).

let n range over training instances.

w   , b    = argmin

w,b

      = argmin

  

1
n

1
n

    log qw,b(yn|xn)

    log q  (yn|xn)

n(cid:88)
n(cid:88)

n=1

n=1

7

information theoretic formulation

let    be the parameters of a probabilistc predictor q  .
      = argmin   e(x,y)   p     log q  (y|x).

we want

this is cross-id178 loss:

h(p,q) = ey   p     log q(y)

h(p) = h(p,p) = ey   p     log p(y)

h(p,q)     h(p)

e(x,y)   p     log q  (y|x) = ex   p h(p(: |x), q  (: |x))

8

multi layer id88s (mlps)

id180:

  (u) =

1

1 + e   u

relu(u) = max(u, 0)

l0 = relu(w 0x + b0)

l1 =   (w 1l0 + b1)

q   = softmax(l1)

9

explicit index notation with typed index variables

i     pixels
j     image features
  y     possible image labels

       + b0[j]
      
       + b1[  y]
      

            (cid:88)
            (cid:88)

i

l0[j] = relu

w 0[j, i] x[i]

l1[  y] =   

w 1[  y, j] l0[j]

q  (  y) =

j
el1[  y]

1
z

10

loss vs. error rate

while training (id119) is generally done on log loss,
performance is often judged by other measures such as error
rate.

the    loss    is often used as a synonym for log loss (or whatever
loss de   ned the id119 training).

hence one often reports both    loss    and    error rate   .

note that error rate is not di   erentiable.

11

train data, development data and test data

data is typically divided into a training set, a develop-
ment set and a test set each drawn iid from the popula-
tion.

a learning algorithm optimizes training loss.

one then optimizes algorithm design (and hyper-parameters)
on the development set. (graduate student descent).

ultimate performance should be done on a test set not used
for development. test data is often withheld from developers.

12

gradients with respect to systems of parameters
      (cid:96)(  , x, y) denotes the partial derivative of (cid:96)(  , x, y) with
respect to the parameter system   .

here can think of    as a single vector with

(      (cid:96)(  , x, y))i =    (cid:96)(  , x, y)/     i

but in general    can be a multi-dimensional array (an ndarray
in numpy). if    is four dimensional we can write   [i, j, k, l].
for scalar loss,       (cid:96)(  , x, y) has the same shape as   .

(      (cid:96)(  , x, y)) .shape =   .shape

13

total id119

(cid:88)

n

1
n

(cid:96)train(  ) =

(cid:96)(  , xn, yn)

we want:

      = argmin   (cid:96)train(  )

   -=        (cid:96)train(  )

14

stochastic id119 (sgd) on the training set.

repeat: select n at random.    -=          (cid:96)(  , xn, yn)

en       (cid:96)(  , xn, yn) =

n

(cid:88)
p (n)      (cid:96)train(  , xn, yn)
(cid:88)
      (cid:96)train(  , xn, yn)
(cid:88)
=      
=       (cid:96)train(  )

(cid:96)train(  , xn, yn)

1
n

1
n

=

n

n

15

epochs

in practice we cycle through the training data visiting each
training pair once.

one pass through the training data is called an epoch.

one typically imposes a random su   e of the training data
before each epoch.

16

j: feature

(cid:96): layer

  y: possible label

sgd for mlps

l0[j] = input

            (cid:88)
            (cid:88)

j

       + b(cid:96)+1[j(cid:48)]
      
       + bn   1[  y]

      

w (cid:96)+1[j(cid:48), j] l(cid:96)+1[j]

w n   1[  y, j] ln   1[j]

l(cid:96)+1[j(cid:48)] =   

ln [  y] =   

j

p (  y) =

eln [  y]

1
z

end

