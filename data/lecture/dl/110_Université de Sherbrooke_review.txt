review of fundamentals
ift 725 - r  seaux neuronaux

math for my slides    review of fundamentals   .

abstract
abstract
abstract
abstract
abstract

    vector:

topics: matrix, vector, norms, products

id202

i=1 x(1)
i=1 x(1)
i x(2)
i x(2)
i=1 x(1)
i
    product:
i=1 x(1)
i x(2)
i=1 x(1)
i x(2)
i
i x(2)
i=1 x(1)
i
    norm:                                                (euclidean) 
i

math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
id202
math for my slides    review of fundamentals   .
id202
x1
id202
id202
id202
...
xd

id202
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
x1
x1
    x = [x1, . . . , xd]> =264
375
...
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
...
x1
x1
x1
...
...
...
xd
xd
    < x(1), x(2) >= x(1)>x(2) =pd
xd
xd
    < x(1), x(2) >= x(1)>x(2) =pd
xd
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    ||x||2 = px>x =ppi x2
    ||x||2 = px>x =ppi x2
    ||x||2 = px>x =ppi x2
    ||x|| = px>x =ppi x2
    ||x|| = p< x>x > =ppi x2
    ||x|| = p< x>x > =ppi x2
    x =264
375
i
    x =264
375
. . . x1,m
    x =264
375
i
    x =264
375
375
    x =264
. . . x1,m
. . . x1,w
    x =264
375
...
...
. . . x1,w
. . . x1,w
. . . x1,w
...
...
...
...
...
...
...
...
...
...
. . . xn,m
. . . xh,w
. . . xh,w
. . . xh,w
. . . xn,m
  ,j =pk x(1)
. . . xh,w
    (x(1)x(2))i,j = x(1)
i,   x(2)
  ,j =pk x(1)
  ,j =pk x(1)
  ,j =pk x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
  ,j =pk x(1)
  ,j =pk x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    ||x||f =ptrace(x>x) =qpipj x 2
    (x(1)x(2))i,j = x(1)
i,   x(2)
    ii,j =    1 si i = j
    ii,j =    1 si i = j
    ||x||f =ptrace(x>x) =qpi x2
    ii,j =    1 si i = j
    ||x||f =ptrace(x>x) =qpi x2
0 si i 6= j
0 si i 6= j
ii,j =    1 if i = j

    product:
i,k x(2)
    norm:                                                                          (frobenius)
k,j

x1,1
x1,1
x1,1
...
x1,1
x1,1
x1,1
...
...
...
...
...
xn,1
xh,1
xh,1
xh,1
xn,1
xh,1

i,k x(2)
i,k x(2)
i,k x(2)
k,j
i,k x(2)
i,k x(2)
k,j
k,j
k,j
i,j

    matrix:

i x(2)

i
i

k,j

i,i

i

i

i

i

2

i,i

k,j

k,j

i,i
i,i
i,i

i,k x(2)

i,k x(2)
i,k x(2)
i,k x(2)
i,k x(2)

    x =264
375
    x =264
375
    x =264
375
    ||x||2 = px>x =ppi x2
...
...
...
  ,j =pk x(1)
  ,j =pk x(1)
  ,j =pk x(1)
  ,j =pk x(1)
  ,j =pk x(1)
    x =264
375
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
  ,j =pk x(1)
. . . x1,m
x1,1
. . . xn,m
xn,1
i,   x(2)
    (x(1)x(2))i,j = x(1)
xn,1
. . . xn,m
...
...
...
id202
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
  ,j =pk x(1)
  ,j =pk x(1)
    ||x||f =ptrace(x>x) =qpi x 2
i,   x(2)
    (x(1)x(2))i,j = x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
. . . xn,m
xn,1
topics: special matrices
ii,j =    1 if i = j
ii,j =    1 if i = j
ii,j =    1 if i = j
ii,j =    1 if i = j
    ||x||f =ptrace(x>x) =qpi x2
    ||x||f =ptrace(x>x) =qpi x 2
ii,j =    1 if i = j
  ,j =pk x(1)
    identity matrix   : 
ii,j =    1 if i = j
    i
    i
i,   x(2)
    (x(1)x(2))i,j = x(1)
    i
    i
0 if i 6= j
0 if i 6= j
    i
0 if i 6= j
0 if i 6= j
0 if i 6= j
    ii,j =    1 if i = j
    i
ii,j =    1 if i = j
    ||x||f =ptrace(x>x) =qpi x 2
0 if i 6= j
    x xi,j = 0 if i 6= j
    x xi,j = 0 if i 6= j
    diagonal matrix     : 
    x xi,j = 0 if i 6= j
    x xi,j = 0 if i 6= j
0 if i 6= j
    i
    x xi,j = 0 if i 6= j
0 if i 6= j
    x xi,j = 0 if i 6= j
    x xi,j = 0 if i < j
    x xi,j = 0 if i < j
ii,j =    1 if i = j
    x xi,j = 0 if i < j
    x xi,j = 0 if i < j
    xi,j = 0 if i 6= j
    x xi,j = 0 if i < j
    x xi,j = 0 if i 6= j
    i
    x xi,j = 0 if i < j
    x xi,j = xj,i (x> = x)
    x xi,j = xj,i (x> = x)
0 if i 6= j
    xi,j = 0 if i < j
    x xi,j = xj,i (x> = x)
    x xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    x xi,j = 0 if i < j
    trace(x) =pi xi,i
    x xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    trace(x) =pi xi,i
    symmetric matrix     :                            (i.e.                )
    x xi,j = xj,i (x> = x)
    x xi,j = 0 if i 6= j
    xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    x xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    trace(x) =pi xi,i
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    x xi,j = 0 if i < j
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x) =pi xi,i
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2))trace(x(2)x(3)x(1))
    x xi,j = xj,i (x> = x)
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))

    square matrix: matrix with same number of rows and columns

    lower triangular matrix     :  

i,k x(2)

i,i

i,i

1

3

    ||x||f =ptrace(x>x) =qpi x2
    x xi,j = 0 if i 6= j
    ii,j =    1 if i = j
0 if i 6= j
id202
    x xi,j = 0 if i < j
    xi,j = 0 if i 6= j
    x xi,j = xj,i (x> = x)
    xi,j = 0 if i < j
    trace(x) =pi xi,i
    xi,j = xj,i (x> = x)
    trace(x) =pi xi,i

topics: operations on matrices
    trace of matrix:
    trace of products: 

    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))

    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))

    inverse of matrix:

    doesn   t exist if determinant is 0
    inverse of product: 

    transpose of matrix:

    transpose of product:

1

x(1) 1
x(1) 1
x(1) 1

    x 1x = x x 1 = i
    x 1x = x x 1 = i
    x 1x = x x 1 = i
    (x(1)x(2)) 1 = x(2) 1
    (x(1)x(2)) 1 = x(2) 1
    (x(1)x(2)) 1 = x(2) 1
    x 1x = i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x(1)x(2))> = x(2)>x(1)>
    (x(1)x(2))> = x(2)>x(1)>
    (x(1)x(2))> = x(2)>x(1)>
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x) =qi xi,i

1

4

    of triangular matrix:

    of transpose of matrix:

    x 1x = x x 1 = i
    x 1x = x x 1 = i
    x 1x = x x 1 = i
    (x(1)x(2)) 1 = x(2) 1
x(1) 1
id202
    x 1x = x x 1 = i
    (x(1)x(2)) 1 = x(2) 1
x(1) 1
x(1) 1
    (x(1)x(2)) 1 = x(2) 1
x(1) 1
    (x(1)x(2)) 1 = x(2) 1
    (x>)i,j = xj,i
topics: operations on matrices
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x(1)x(2))> = x(2)>x(1)>
    (x>)i,j = xj,i
    determinant
    (x(1)x(2))> = x(2)>x(1)>
    (x(1)x(2))> = x(2)>x(1)>
    det (x) =qi xi,i
    (x(1)x(2))> = x(2)>x(1)>
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x>) = det (x)
    det (x>) = det (x)
    det (x>) = det (x)
    det (x 1) = det (x) 1
    det (x>) = det (x)
    det (x 1) = det (x) 1
    det (x 1) = det (x) 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x 1) = det (x) 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
    x> = x 1
    v>xv > 0
    x> = x 1
    v>xv > 0
    v>xv > 0
     
    v>xv > 0
     
     
    {x(t)}

    of product of matrix: 

    of inverse of matrix:

5

    det (x) =qi xi,i
    det (x) =qi xi,i
    (x>)i,j = xj,i
    det (x>) = det (x)
    det (x>) = det (x)
    (x(1)x(2))> = x(2)>x(1)>
id202
    det (x) =qi xi,i
    det (x 1) = det (x) 1
    det (x 1) = det (x) 1
    det (x>) = det (x)
topics: properties of matrices
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x 1) = det (x) 1
    orthogonal matrix:
    x> = x 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
    v>xv > 0
    x> = x 1
    positive de   nite matrix: 
    v>xv > 0 8v 2 r
    v>xv > 0 8v 2 r
     
    if          , then positive semi-de   nite
     
     
    {x(t)}
    {x(t)}
    9w, t    x(t   ) =pt6=t    wtx(t)
    9w, t    x(t   ) =pt6=t    wtx(t)
    {x(t)}
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    9w, t    x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    x =pi  iuiu>i
    { i, ui | xui =  iui et u>i uj = 1i=j}
    det (x) =qi  i
    {x 2 rn | x /2 r(x)}

6

    det (x 1) = det (x) 1
    det (x 1) = det (x) 1
    x> = x 1
    x> = x 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x(1)x(2)) = det (x(1)) det (x(2))
id202
    v>xv > 0 8v 2 r
    v>xv > 0 8v 2 r
    x> = x 1
    x> = x 1
     
topics: linear dependence, rank, range and nullspace
     
    v>xv > 0 8v 2 r
    v>xv > 0 8v 2 r
    set of linearly dependent vectors          :
    {x(t)}
    {x(t)}
     
     
    {x(t)}
    {x(t)}

    rank of matrix: number of linear independent columns
    range of a matrix:

    9w, t    such that x(t   ) =pt6=t    wtx(t)
    9w, t    x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    {x 2 rn | x /2 r(x)}
    {x 2 rn | x /2 r(x)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
    x =pi  iuiu>i
    x =pi  iuiu>i
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    det (x) =qi  i
    det (x) =qi  i
    x =pi  iuiu>i
    { i, ui | xui =  iui et u>i uj = 1i=j}
    x =pi  iuiu>i
    det (x) =qi  i

    nullspace of a matrix:

     i > 0

     i > 0

7

    {x(t)}

    v>xv > 0 8v 2 r
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
    x> = x 1
     
    9w, t    such that x(t   ) =pt6=t    wtx(t)
id202
    v>xv > 0 8v 2 r
    v>xv > 0 8v 2 r
    {x(t)}
     
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    9w, t    such that x(t   ) =pt6=t    wtx(t)
     
topics: eigenvalues and eigenvectors of a matrix
    {x(t)}
    {x(t)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    eigenvalues and eigenvectors
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    {x 2 rn | x /2 r(x)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    properties
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    x =pi  iuiu>i
    can write
    det (x) =qi  i
    det (x) =qi  i
    det (x) =qi  i
    determinant of any matrix: 
    det (x) =qi  i
    positive de   nite if
     i > 0 8i
     i > 0
     i > 0
    rank of matrix is the number of non-zero eigenvalues
   
   
   

     i > 0
   

@
@x

   

   

   

@
@x

@
f (x +  , y)   f (x, y)
f (x, y) = lim
f (x, y) = lim
f (x, y) = lim
@x
 !0
@
 
 !0
 !0
f (x, y) = lim
@
@x
 !0
f (x, y) = lim

f (x +  , y)   f (x, y)
f (x +  , y)   f (x, y)
f (x +  , y)   f (x, y)
f (x, y +  )   f (x, y)

8
 

 
 

    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
    x =pi  iuiu>i
    { i, ui | xui =  iui and u>i uj = 1i=j}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    det (x) =qi  i
    x =pi  iuiu>i
    det (x) =qi  i
    det (x) =qi  i

topics: derivative, partial derivative
    derivative:

     i > 0 8i
     i > 0 8i
     i > 0 8i

id128

d
dx

f (x +  )   f (x)

f (x) = lim
 !0
d
d
dx
dx
    partial derivative:

    direction and rate of increase of function

f (x +  )   f (x)
f (x +  )   f (x)
f (x) = lim
f (x) = lim
 !0
f (x +  , y)   f (x, y)
 !0

 
 
 

@
@x

 

@
@y

f (x, y) = lim
f (x, y) = lim
 !0
 !0

f (x, y) = lim
 !0
@
@
@x
@x
f (x, y) = lim
 !0
@
@
@y
@y

f (x, y) = lim
f (x, y) = lim
 !0
 !0

f (x +  , y)   f (x, y)
f (x +  , y)   f (x, y)

f (x, y +  )   f (x, y)

 
 

f (x, y +  )   f (x, y)
f (x, y +  )   f (x, y)

 

    direction and rate of increase for variable assuming others are    xed

@

@x1

    x y
    x y

    rf (x) =h @
    rf (x) =h @

@x1

f (x), . . . , @
@xd

 
 
f (x)
...

@
@
@x1

f (x)
f (x)

375

f (x)i> =264
f (x)i> =264
f (x)i> =264

9

375
375

   

   

    9w, t    x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    {x 2 rh | x /2 r(x)}
 
    { i, ui | xui =  iui et u>i uj = 1i=j}
    x =pi  iuiu>i
    det (x) =qi  i

{
x
(
t
)
}

r
x

(

)

   

   

   

x
topics: derivative, partial derivative
=
>
    example:
p
0

i
,

u

)

i

i

i

)

(

|

u

i
,

   

   

   

   

   

   

   

   

   

{
 

{
 
 
i

    9w, t    x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
id128
{
r
{
9
x
d
    {x 2 rh | x /2 r(x)}
d  riv  e partielle et gradient 
x
x
w
e
t
    { i, ui | xui =  iui et u>i uj = 1i=j}
=
x
2
2
,
(
    x =pi  iuiu>i
t
x
       exemple%de%fonc7on%  %deux%variables:%
p
   
r
r
=
    det (x) =qi  i
h
h
f(x, y) = x2
=
     i > 0
{
     i > 0
x
x
x
y
q
   
2
/2
/2
   
=
=
@
 
f (x, y) = lim
r
r
r
2x
 f(x, y)
p
@x
=  x2
 !0
 
h
=
(
   
   
i
x
x
y2
u
 y
f (x, y +  )   f (x, y)
y
@
f (x, y) = lim
9
)
)
i
@y
 !0
}
}
w
e
x
y
traite%
traite%
treat
treat
    x y
    x y
t
w
comme%une%
comme%une%
as constant
as constant
constante%
constante%
machine learning
x
(

=
{
x
2
r
h

=
 f(x, y)
 
u
 x
e
t

%       d  riv  es%par7elles:%%%

f (x +  , y)   f (x, y)

x
(
t
   
)

9
 
w

u
>i

x
u

x
u

=
t
   

u

 

hugo%larochelle%

|

|

|

|

|

(

t

t

i

i

i

i

i

i

i

i

39%

ift615%

x
=

u
>i
u

u
>i
u

x
10
=

i

 

i

u

i

u
>i

p
 

q

6
6
@x

 !0

 

f (x, y +  )   f (x, y)

@
@y

f (x, y) = lim
 !0

id128
   
topics: gradient
    x y
    gradient:
375

f (x)i> =264

@xd
       le%gradient%donne%la%direc7on%(vecteur)%ayant%le%taux%d   accroissement%de%

    rxf (x) =h @

f (x), . . . , @
descente de gradient 
@xd

f (x)
...
f (x)

@x1

@x1

 

@

@

la%fonc7on%le%plus%  lev  %

2

f(x, y)

x

y

11

3775
3775

id128

@x1,1

@

...

@
f (x)
@x1,m

@
@x1,m

f (x)

f (x)

@x1,1
f (x)
@x1,1
...
@xn,1
f (x)
@xn,1

f (x)
f (x)
. . .
...
...
. . .
f (x)
f (x)
. . .

. . .
. . .
@
@x1,m
. . .
. . .
. . .
. . .
@
@xn,m

...

@
f (x)
@xn,m

@
@xn,m

f (x)

f (x)

@

@
@

@xn,1

@2
@x2
1

    r2
    r2

@
topics: jacobian, hessian
@
    hessian: 

    rxf (x) =2664
    rxf (x) =2664
    rf (x) =2664
xf (x) =2664
xf (x) =2664
    r2f (x) =2664
    rxf (x) =264
    rf (x) =264
    rxf (x) =264

@
f (x)1
@
@x1
...
@x1
@
f (x)k
@x1
@
@x1
    (   ,f, p ),     f p ,
    (   ,f, p ),     f p ,
    (   ,f, p ),     f p ,

@x1

@2

@

. . .

@2
@2
f (x)
f (x)
f (x)
@x2
@x2
1
1
...
...
...

. . .
. . .
@2
@x1@xd
. . .
. . .
. . .
@2
. . .
@x2
d

@2
@2
f (x)
@x1@xd
@x1@xd

f (x)

f (x)

...
...
@2
@2
f (x)
f (x)
@x2
@x2
d
d

...
f (x)

@xd@x1

@2
@2
f (x)
@xd@x1
@xd@x1

. . .
f (x)
f (x)
. . .
    if                                          is a vector, the jacobian is:
    f (x) = [f (x)1, . . . , f (x)k]>
    f (x) = [f (x)1, . . . , f (x)k]>
    f (x) = [f (x)1, . . . , f (x)k]>
. . .
f (x)1
@
. . .
f (x)1
. . .
@xd
...
...
. . .
. . .
. . .
@
. . .
f (x)k
. . .
@xd
. . .
f (x)k

f (x)1
f (x)1
@
@xd
f (x)1
...
...
...
f (x)k
f (x)k
@xd
@
f (x)k

@xd

@x1

375

@

@

@

@xd

...

3775
3775
375

3775
3775
375

12

f (x)1
@
...
@xd
f (x)k
@
@xd

f (x)1
...
f (x)k

@

. . .

375

. . .
. . .

    f (x) = [f (x)1, . . . , f (x)k]>
. . .

@

@

@x1

id128

@x1

@

. . .
. . .

f (x)1
...
f (x)k
    f (x) x
f (x)
...
f (x)

    rxf (x) =264
    rxf (x) =2664

@
f (x)
...
@
f (x)

. . .
@
@x1,1
. . .
. . .
@
@xn,1

@xn,m

@x1,m

@2

topics: gradient for matrices
@x1
    if scalar function          takes a matrix      as input

@x1

f (x)

. . .

f (x)

. . .
. . .

f (x)
@
. . .

@

...

@x1,1

@x1,m

@x1,1

. . .
f (x)
. . .
@

@
f (x)
...
...
@
f (x)
f (x)
    for functions that output functions and take matrices as input, 
...
we organize into 3d tensors

    (   ,f, p ),     f p ,
. . .
    (   ,f, p ),     f p ,
. . .
       
       
. . .
f (x)
    e 2 f
    e 2 f
    f (x) = [f (x)1, . . . , f (x)k]>
        = {1, 2, 3, 4, 5, 6}

...
f (x)

@2
@x2
d

@xn,1

f (x)

@x1@xd

@xn,m

13

3775
3775

@

@xn,1

    f (x) x

    rxf (x) =264
    rxf (x) =2664
    rxf (x) =2664
xf (x) =2664
    rxf (x) =264

    r2

@2
@x2
1

@xd@x1

@2

375

. . .

f (x)1
@
f (x)1
@xd
...
...
f (x)k
@
f (x)k
@xd

. . .
. . .

@

@

@

...

@2

@x1

@x1

@x1

@x1

@xd@x1

f (x)

@
@xd@x1

@
. . .

. . .
@
. . .

@xd@x1
@2

    r2f (x) =2664
    r2f (x) =2664
    r2f (x) =2664
    rf (x) =264
    rf (x) =264
    rf (x) =264
    rf (x) =264

    r2f (x) =2664
    r2f (x) =2664
    rf (x) =264
    rf (x) =264
    rf (x) =264

3775
3775
3775
3775
3775
...
    f (x) = [f (x)1, . . . , f (x)k]>
    f (x) = [f (x)1, . . . , f (x)k]>
f (x)
. . .
    rf (x) =264
. . .
@x2
    rf (x) =264
@x2
@2
d
@
f (x)
. . .
f (x)
d
f (x)1
@2
    f (x) = [f (x)1, . . . , f (x)k]>
375
375
@x2
@xd@x1
. . .
f (x)
f (x)
id203
@x1
d
    f (x) = [f (x)1, . . . , f (x)k]>
@
@
@
@x2
...
f (x)1
. . .
f (x)1
    f (x) = [f (x)1, . . . , f (x)k]>
f (x)1
. . .
f (x)1
d
375
@xd
@x1
@xd
    f (x) = [f (x)1, . . . , f (x)k]>
...
...
@
@
...
...
375
. . .
f (x)1
f (x)1
375
    f (x) = [f (x)1, . . . , f (x)k]>
@
@
@x1
@xd
f (x)1
f (x)1
. . .
@
f (x)1
. . .
f (x)1
. . .
...
...
375
. . .
f (x)k
topics: id203 space
@x1
@xd
@
@
@xd
@x1
...
...
f (x)1
f (x)1
. . .
@x1
375
...
...
. . .
@
@
@xd
@x1
@
@
@
f (x)k
. . .
f (x)k
. . .
f (x)1
f (x)1
f (x)k
f (x)k
. . .
. . .
...
...
    id203 space: triplet
. . .
    (   ,f, p ),     f p ,
@
@
@xd
@x1
@xd
    (   ,f, p ),     f p ,
f (x)k
. . .
f (x)k
@x1
@xd
...
...
. . .
@
@
@xd
@x1
f (x)k
. . .
f (x)k
@
@
. . .
f (x)k
f (x)k
@xd
@x1
@
@
          is the space of possible outcomes
    (   ,f, p ),     f p ,
f (x)k
f (x)k
. . .
    (   ,f, p ),     f p ,
@xd
@x1
    (   ,f, p ),     f p ,
@x1
    e 2 f
    e 2 f
@
@
f (x)k
f (x)k
    (   ,f, p ),     f p ,
          is the space of possible events
    (   ,f, p ),     f p ,
@xd
@x1
    (   ,f, p ),     f p ,
    e 2 f
    e 2 f
    e 2 f
          is a id203 measure mapping an outcome to its id203 [0,1]
    (   ,f, p ),     f p ,
        = {1, 2, 3, 4, 5, 6}
        = {1, 2, 3, 4, 5, 6}
    e 2 f
    e 2 f
    example: throwing a die
        = {1, 2, 3, 4, 5, 6}
        = {1, 2, 3, 4, 5, 6}
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
        = {1, 2, 3, 4, 5, 6}
 
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
                                     (i.e. die is either 1 or 5)
    e = {1, 5} 2 f
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    e = {1, 5} 2 f
    e = {1, 5} 2 f
    e = {1, 5} 2 f
 
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    e = {1, 5} 2 f
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    properties:
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
2.
1.
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    x o s
    p!2    p ({!}) = 1

    x o s

. . .
. . .

6
6
6
6

@xd

6

14

6

6

6

6

-

-

-

6

6

6

id203

        = {1, 2, 3, 4, 5, 6}
    e 2 f
    e 2 f
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
    p ({1, 5}) = 2
    e = {1, 5} 2 f
topics: random variable
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({1, 5}) = 2
    random variable: a function on outcomes
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
    examples: 
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
          is the value of the outcome 
    p!2    p ({!}) = 1
    x o s
          is 1 if the outcome is 1, 3 or 5, otherwise it   s 0
    x o s
    p(x = x, o = o, s = s) p(x, s, o)
          is 1 if the outcome is smaller than 4, otherwise it   s 0
    x o s
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = 1, o = 1, s = 0) = 0
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = 1, o = 1, s = 0) = 0
    p(o = o, s = s)
    p(x = 1, o = 1, s = 0) = 0
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(s = s|o = o)
    p(o = 1, s = 0) = 1
    p(s = s|o = o)

6

6

6

15

6

6
6

    marginal distribution:

    p ({1, 5}) = 2
    p ({1, 5}) = 2
    e = {1, 5} 2 f
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
id203
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
topics: distributions (joint, marginal, conditional)
    x o s
    x o s
    x o s
    x o s
    x o s
    x o s
    joint distribution:                                          (              for short)
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    x o s
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    the id203 of a complete assignment of all random variables
    p(x = 1, o = 1, s = 0) = 0
    p(x = 1, o = 1, s = 0) = 0
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = 1, o = 1, s = 0) = 0
    example: 
    p(x = 1, o = 1, s = 0) = 0
    p(x = 1, o = 1, s = 0) = 0
    p(x = 1, o = 1, s = 0) = 0
    p(o, s) =px p(x, o, s)
    p(o = o, s = s)
    p(o = o, s = s)
    p(x = 1, o = 1, s = 0) = 0
    p(o = o, s = s)
    p(o = o, s = s)
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    the id203 of a partial assignment
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    example:
    p(s = s|o = o)
    p(o = 1, s = 0) = 1
6
    p(s = s|o = o)
    p(o = 1, s = 0) = 1
    conditional distribution:
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(s = 1|o = 1) = 2
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(x) =qi p(xi|x1, . . . , xi 1)
po0 p(s=s|o=o0)p(o=o0)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)

    the id203 of some variables, assuming an assignment of other variables
    example:

6

3

6

3

3

6

3

6

3

3

6

6

3

16

3

6

3

6

    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(o = o, s = s)
6
id203
    p(s = s|o = o)
    p(s = s|o = o)
    p(o = 1, s = 0) = 1
    p(s = 1|o = 1) = 2
topics: id203 chain rule, bayes rule
    p(s = 1|o = 1) = 2
    p(s = s|o = o)
    id203 chain rule:
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    in general: 
    p(s = 1|o = 1) = 2
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
po0 p(s=s|o=o0)p(o=o0)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    bayes rule:
po0 p(s=s|o=o0)p(o=o0)
    p(x1, x2) = p(x1)p(x2)
    p(x1, x2) = p(x1)p(x2)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
    p(x1, x2) = p(x1)p(x2)

3

3

3

17

3

6

3

6

3

6

    p(o, s) =px p(x, o, s)
    p(o, s) =px p(x, o, s)
    p(o, s) =px p(x, o, s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(x) =qi p(xi|x1, . . . , xi 1)
id203
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = s|o = o)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
topics: independence between variables
    p(s = 1|o = 1) = 2
po0 p(s=s|o=o0)p(o=o0)
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    independence: variables       and       are independent if
    x1 x2 x3
    x1 x2 x3
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x1, x2) = p(x1)p(x2)
    p(x) =qi p(xi|x1, . . . , xi 1)
or
    p(x1|x2) = p(x1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
po0 p(s=s|o=o0)p(o=o0)
or
    p(x2|x1) = p(x2)
    p(x1, x2) = p(x1)p(x2)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
    conditional independence: variables       and        are 
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    x1 x2 x3
    x1 x2 x3
    p(x1|x2) = p(x1)
independent given       if 
    x1 x2 x3
    p(x1|x2, x3) = p(x1|x3)
    p(x2|x1) = p(x2)
    p(x2|x1, x3) = p(x2|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    e[x + y ] = e[x] + e[y ]
    p(x2|x1, x3) = p(x2|x3)
    e[xy ] = e[x]e[y ]

    e[x] =px x p(x = x)
    e[x] =px x p(x = x)

or
or

3

3

3

18

    p(x2|x1) = p(x2)
    p(x1, x2) = p(x1)p(x2)
    p(x2|x1) = p(x2)
    p(x1|x2) = p(x1)
    p(x1, x2) = p(x1)p(x2)
    p(x1|x2) = p(x1)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2) = p(x1)
    p(x2|x1) = p(x2)
    p(x1|x2) = p(x1)
id203
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x2|x1) = p(x2)
    p(x1|x2, x3) = p(x1|x3)
    p(x2|x1) = p(x2)
    p(x1|x2, x3) = p(x1|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x2|x1) = p(x2)
    p(x1|x2, x3) = p(x1|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
topics: expectation, variance
    p(x2|x1, x3) = p(x2|x3)
    p(x2|x1, x3) = p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    e[x] =px x p(x = x)
    p(x2|x1, x3) = p(x2|x3)
    e[x] =px x p(x = x)
    p(x1|x2, x3) = p(x1|x3)
    expectation:
    p(x2|x1, x3) = p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    e[x] =px x p(x = x)
    p(x2|x1, x3) = p(x2|x3)
    e[x] =px x p(x = x)
    p(x2|x1, x3) = p(x2|x3)
    e[x + y ] = e[x] + e[y ]
    properties:
    p(x2|x1, x3) = p(x2|x3)
    e[x] =px x p(x = x)
    e[x] =px x p(x = x)
    e[x + y ] = e[x] + e[y ]
    e[x] =px x p(x = x)
    e[xy ] = e[x]e[y ]
    e[x + y ] = e[x] + e[y ]
-
    e[x + y ] = e[x] + e[y ]
    e[f (x)] =px f (x) p(x = x)
    e[xy ] = e[x]e[y ]
    e[x + y ] = e[x] + e[y ]
    e[x + y ] = e[x] + e[y ]
-
    e[xy ] = e[x]e[y ]
    e[xy ] = e[x]e[y ]
    e[x + y ] = e[x] + e[y ]
    e[f (x)] =px f (x) p(x = x)
-
    var[x] =px(x   e(x))2 p(x = x)
    e[xy ] = e[x]e[y ]
    e[f (x)] =px f (x) p(x = x)
    e[xy ] = e[x]e[y ]
    e[f (x)] =px f (x) p(x = x)
    e[xy ] = e[x]e[y ]
    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    e[f (x)] =px f (x) p(x = x)
    var[x] = e[x 2]   e[x]2
    var[x] =px(x   e(x))2 p(x = x)
    variance:
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x + y ] = var[x] + var[y ]
    var[x] = e[x 2]   e[x]2
    properties:
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
   
    var[x + y ] = var[x] + var[y ]
 
    var[x] = e[x 2]   e[x]2
-
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
if independent,
-
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
    var[x + y ] = var[x] + var[y ]
   
    var[x + y ] = var[x] + var[y ]
= xx1 xx2
   
   
   
   
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
   

 
 
if independent, 

(x1   e[x1])(x2   e[x2]) p(x1, x2)

19

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]

    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
id203
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
topics: covariance matrix
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
   
   
    covariance: 
   

(x1   e[x1])(x2   e[x2]) p(x1, x2)

(x1   e[x1])(x2   e[x2]) p(x1, x2)

= xx1 xx2

= xx1 xx2

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
(x1   e[x1])(x2   e[x2]) p(x1, x2)
(x1   e[x1])(x2   e[x2]) p(x1, x2)

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]

    cov(x1, x2) = 0
    var(x) = cov(x, x)

    if independent                          
    cov(x1, x2) = 0
    cov(x1, x2) = 0
     
    var(x) = cov(x, x)
    var(x) = cov(x, x)
    covariance matrix:
...
cov(x1, x1)

= xx1 xx2
= xx1 xx2
    cov(x1, x2) = 0
    cov(x) =264
    cov(x) =264
    var(x) = cov(x, x)
. . . cov(x1, xd)
    cov(x) =264
...
...
    cov(x) =264
. . . cov(x1, xd)
. . . cov(x1, xd)
...
...
...
...
...
cov(xd, x1)
. . . cov(xd, xd)
cov(xd, x1)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
. . . cov(xd, xd)
. . . cov(xd, xd)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx

. . . cov(x1, xd)
...
. . . cov(xd, xd)

...
cov(x1, x1)

    p (x = x)

    p (x = x)

375

20

cov(xd, x1)

cov(xd, x1)

cov(x1, x1)

cov(x1, x1)

375

375

375

...

...

...

...

...

...

cov(xd, x1)

375

cov(x1, x1)

    var(x) = cov(x, x)

= xx1 xx2
...

    cov(x1, x2) = 0
    var(x) = cov(x, x)

. . . cov(x1, xd)
...
. . . cov(xd, xd)

    cov(x1, x2) = 0
    var(x) = cov(x, x)
cov(x1, x1)
topics: continuous variables
...
    for continuous variable      ,          is a density function
cov(xd, x1)

    cov(x) =264
cov(x1, x1)
    cov(x) =264
id203
. . . cov(x1, xd)
...
375
    cov(x) =264
. . . cov(x1, xd)
cov(xd, x1)
    cov(x) =264
375
...
...
cov(xd, x1)
. . . cov(xd, xd)
cov(x1, x1)
. . . cov(x1, xd)
...
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
. . . cov(xd, xd)
. . . cov(xd, xd)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
    p (x = x)
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
p(x) =
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
p(x) =
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
    x 2 rd
1p(2   )d det(   )

     
    the id203                  is zero for continuous variables
    p (x = x)
    in previous equations, summations would be replaced by integrals
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

    p (x = x)
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

2 2     e[x] =    var[x] =  2

   cov[x] =    
p(x) =

   cov[x] =    

   cov[x] =    

p(x) =

4

   cov[x] =    

4

21

4

375

    var(x) = cov(x, x)

cov(xd, x1)
cov(x1, x1)

. . . cov(xd, xd)
. . . cov(x1, xd)
...
. . . cov(xd, xd)

    cov(x) =264
= xx1 xx2
    cov(x) =264
    var(x) = cov(x, x)
cov(x1, x1)
. . . cov(x1, xd)
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
375
    cov(x) =264
...
...
...
id203
    cov(x1, x2) = 0
. . . cov(x1, xd)
cov(x1, x1)
    p (x 2 a) =rx2a p(x)dx
(x1   e[x1])(x2   e[x2]) p(x1, x2)
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
375
...
...
...
. . . cov(x1, xd)
cov(xd, x1)
. . . cov(xd, xd)
    var(x) = cov(x, x)
(x1   e[x1])(x2   e[x2]) p(x1, x2)
...
    p (x 2 a) =rx2a p(x)dx
cov(xd, x1)
. . . cov(xd, xd)
topics: bernoulli, gaussian distributions
    p (x = x)
    cov(x) =264
375
    p (x 2 a) =rx2a p(x)dx
. . . cov(x1, xd)
cov(x1, x1)
. . . cov(xd, xd)
    bernoulli variable: 
...
...
    p (x 2 a) =rx2a p(x)dx
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
     
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
375
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
. . . cov(xd, xd)
. . . cov(x1, xd)
     
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
    p (x = x)
    p (x 2 a) =rx2a p(x)dx
...
375
. . . cov(x1, xd)
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
exp   1
     
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 rd
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
exp   1
2 (x     )>    1(x     )  e[x] =
. . . cov(xd, xd)
     
2 2     e[x] =    var[x] =  2
    x 2 rd
1p(2   )d det(   )
p(x) =
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
exp   1
2 (x     )>    1(x     )  e[x] =
   cov[x] =    
. . . cov(xd, xd)
1p(2   )d det(   )
2 2     e[x] =    var[x] =  2
   cov[x] =    
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
2 (x     )>    1(x     )  e[x] =
exp   1
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
4
exp   1
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
     
    x 2 rd
1p(2   )d det(   )
2 2     e[x] =    var[x] =  2
     
2 (x     )>    1(x     )  e[x] =
exp   1
2 (x     )>    1(x     )  e[x] =

2 (x     )>    1(x     )  e[x] =
2 (x     )>    1(x     )  e[x] =

4
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
   cov[x] =    

2 (x     )>    1(x     )  e[x] =

    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

cov(xd, x1)
...

    gaussian variable:

1p(2   )d det(   )

   cov[x] =    

cov(xd, x1)

p(x) =

p(x) =

p(x) =

     

...

4

22

...

...

375

cov(xd, x1)

...
. . . cov(xd, xd)

    cov(x) =264
375
    cov(x) =264
    cov(x) =264
375
    p (x = x)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
id203
    p (x 2 a) =rx2a p(x)dx
    p (x = x)
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
topics: multivariate gaussian distributions
2 (x     )>    1(x     ) 
exp   1
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    gaussian variable:
    x 2 rd
1p(2   )d det(   )
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
2 (x     )>    1(x     ) 
exp   1
exp   1
2 (x     )>    1(x     ) 
   
    x 2 rd
1p(2   )d det(   )
    x 2 rd
p(x) =
1p(2   )d det(   )
p(x) =
    e[x] =    cov[x] =    
2 (x     )>    1(x     ) 
exp   1
    x 2 rd
1p(2   )d det(   )
p(x) =
     
    e[x] =    cov[x] =    
    e[x] =    cov[x] =    
   
    e[x] =    cov[x] =    

    p (x = x)
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

p(x) =

4

4

4

4

23

statistics

topics: estimate of the expectation and covariance matrix
    sample mean:

    sample variance:

    sample covariance matrix:

t pt x(t)
    b   = 1
t pt x(t)
t pt x(t)
    b   = 1
t 1pt(x(t)  b  )2
    b   = 1
t pt x(t)
    b 2 = 1
    b   = 1
t 1pt(x(t)  b  )2
t 1pt(x(t)  b  )2
    b 2 = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    b 2 = 1
t 1pt(x(t)  b  )2
    b    = 1
    b 2 = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    e[b  ] =    e[b   ] =    
    b    = 1
    e[b  ] =    e[b   ] =    
    e[b  ] =    e[b   ] =    
    e[b  ] =    e[b 2] =  2 ehb   i =    
    b       2
    b       2
    b       2
       2 [ 1.96b  +b  ,b   + 1.96b ]
    b       2
       2 [ 1.96b  +b  ,b   + 1.96b ]

t

t

t

    these estimators are unbiased, i.e.:

24

topics: con   dence interval
    con   dence interval of the sample mean (1d):

    if t is big, the following estimator is approx. gaussian with mean 0 and variance 1

t 1pt(x(t)  b  )2
    b 2 = 1
t pt x(t)
t 1pt(x(t)  b  )(x(t)  b  )>
    b   = 1
    b    = 1
statistics
t 1pt(x(t)  b  )2
    b 2 = 1
    e[b  ] =    e[b 2] =  2 ehb   i =    
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
    e[b  ] =    e[b 2] =  2 ehb   i =    
b     p
   
b 2/t
b     p
b 2/t
       2 [ 1.96b  +b  ,b   + 1.96b ]
       2b       1.96pb 2/t
   

   

   

    then we have that, with 95% id203, that

b    = arg max

25

p(x(1), . . . , x(t ))

b    = arg max

t

   

t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    e[b  ] =    e[b   ] =    
t 1pt(x(t)     )2
    e[b  ] =    e[b   ] =    
statistics
    b       2
t 1pt(x(t)  b  )(x(t)  b  )>
    b       2
       2 [ 1.96b  +b  ,b   + 1.96b ]
topics: maximum likelihood, i.i.d. hypothesis
       2 [ 1.96b  +b  ,b   + 1.96b ]
    e[b  ] =    e[b   ] =    
    maximum likelihood estimator (id113):
b    = arg max
    p(x(1), . . . , x(t )) =qt p(x(t))
       2 [ 1.96b  +b  ,b   + 1.96b ]
t b    = 1
    t 1
b    = arg max
    supervised learning example: (x, y)
p(x(1), . . . , x(t )) =yt
    training set: dtrain = {(x(t), y(t)}
    supervised learning example: (x, y)
p(x(t))
    training set: dtrain = {(x(t), y(t)}

    the sample mean is the id113 for a gaussian distribution 
    the sample covariance matrix isnt, but this is
machine learning

p(x(1), . . . , x(t ))
    independent and identically distributed variables

t pt(x(t)  b  )(x(t)  b  )>

p(x(1), . . . , x(t ))

   

   

   

p(x(1), . . . , x(t ))

   

b    = arg max
p(x(1), . . . , x(t )) =yt

machine learning

p(x(t))

26

   

   

   

p(x(1), . . . , x(t ))

       2b       1.96pb 2/t
       2b       1.96pb 2/t
b 2/t
   
       2b       1.96pb 2/t
b    = arg max
   
sampling
b    = arg max
b    = arg max
p(x(1), . . . , x(t )) =yt
topics: monte carlo estimate
   
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
    monte carlo estimate:
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
t b    = 1
    t 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
    x(k) p(x)
    x(k) p(x)

    a method to approximate an expensive expectation

    the        must be sampled from 

    x(k) p(x)

p(x(1), . . . , x(t ))

p(x(t))

p(x(t))

   

   

   

p(x(1), . . . , x(t ))

p(x(t))

machine learning

machine learning

machine learning

    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    f (x;    )
    dvalid dtest

    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    dvalid dtest

27

   

   

   

   

   

   

   

   

p(x(t))

p(x(t))

p(x(t))

p(x(1), . . . , x(t ))

p(x(t))
p(x(1), . . . , x(t ))

       2b       1.96pb 2/t

b    = arg max
p(x(1), . . . , x(t )) =yt
b     p
b    = arg max
   
b 2/t
p(x(1), . . . , x(t )) =yt
       2b       1.96pb 2/t
sampling
p(x(1), . . . , x(t )) =yt
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
b    = arg max
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
topics: importance sampling
   
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
p(x(1), . . . , x(t )) =yt
    t 1
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
    importance sampling:
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
t pt(x(t)  b  )(x(t)  b  )>
    a sampling method for when           is expensive to sample from
    x(k) p(x)
    x(k) p(x)
t b    = 1
    t 1
t b    = 1
    t 1
    x(k) p(x)
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
kpk f (x(k)) p(x)
    e[f (x)] =px f (x) p(x)
    e[f (x)] =px f (x) p(x)     1
q(x) q(x)     1
kpk f (x(k)) p(x)
    e[f (x)] =px f (x) p(x)
q(x) q(x)     1
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
    supervised learning example: (x, y) x y
            is easier to sample from and should be as similar as possible to 
q(x) q(x)     1
    q(x)
    q(x)
- designing a good         is often hard to do
    training set: dtrain = {(x(t), y(t))}
machine learning
machine learning
machine learning
    f (x;    )
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
   
    f (x;    )
t xt
    f (x;    )
    dvalid dtest
    dvalid dtest
    f (x;    )
    dvalid dtest
   

t pt(x(t)  b  )(x(t)  b  )>

machine learning
q(x)

l(f (x(t);    ), y(t)) +     (   )

    x(k) p(x)

    x(k) p(x)

arg min

    q(x)

q(x)

q(x)

1

28

   

   

   

   

   

   

   

   

p(x(t))

p(x(1), . . . , x(t ))
p(x(t))

       2b       1.96pb 2/t
       2b       1.96pb 2/t
b    = arg max
b    = arg max
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
b    = arg max
b    = arg max
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
sampling
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
   
   
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
topics: id115 (mcmc)
    t 1
t b    = 1
    t 1
    x(k) p(x)
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
t b    = 1
t b    = 1
    t 1
    t 1
    mcmc:
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
q(x) q(x)     1
    iterative method to generate the sequence of
    x(k) p(x)
    x(k) p(x)
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
    the set of         will be dependent of each other
    x(k) p(x)
    x(k) p(x)
q(x) q(x)     1
    q(x)
machine learning
    q(x)
    supervised learning example: (x, y) x y
    x(1) t (x0 x)
 ! x(2) t (x0 x)
 ! x(3) t (x0 x)
 !       
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    x(1) t (x0 x)
 ! x(2) t (x0 x)
t (x0 x)
    training set: dtrain = {(x(t), y(t))}
 ! x(k)
 !       
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
                       is a transition operator, that must satisfy certain properties
machine learning
    f (x;    )
    t (x0   x)
    k must be big for the set of samples be representative of distribution
    f (x;    )
    f (x;    )
    dvalid dtest
machine learning
    supervised learning example: (x, y) x y
    usually, we drop the    rst samples, which are not reliable
    dvalid dtest
    dvalid dtest
   
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
arg min
   
   
t xt
    training set: dtrain = {(x(t), y(t))}
l(f (x(t);    ), y(t)) +     (   )
arg min
    f (x;    )
    l(f (x(t);    ), y(t))
    f (x;    )

t (x0 x)
 ! x(k)

 ! x(3) t (x0 x)

t xt
t xt

machine learning

machine learning

arg min

q(x)

q(x)

29

1

1

1

   

   

   

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

q(x)

q(x)

    x(k) p(x)

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    e[f (x)] =px f (x) p(x)     1
p(x(1), . . . , x(t )) =yt
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
    x(k) p(x)
q(x) q(x)     1
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
sampling
    t 1
q(x) q(x)     1
    x(k) p(x)
    e[f (x)] =px f (x) p(x)
    q(x)
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
q(x) q(x)     1
    q(x)
    q(x)
topics: id150
t (x0 x)
    x(1) t (x0 x)
 ! x(2) t (x0 x)
 ! x(3) t (x0 x)
    x(k) p(x)
    q(x)
 ! x(3) t (x0 x)
    x(1) t (x0 x)
 ! x(2) t (x0 x)
t (x0 x)
 ! x(k)
 !       
 ! x(k)
 !       
 ! x(2) t (x0 x)
    x(1) t (x0 x)
    id150:
kpk f (x(k)) p(x)
    e[f (x)] =px f (x) p(x)
q(x) q(x)     1
 ! x(3) t (x0 x)
    x(1) t (x0 x)
t (x0 x)
 ! x(2) t (x0 x)
 ! x(k)
 !       
    t (x0   x)
q(x)
    t (x0   x)
    mcmc method which uses the following transition operator
    t (x0   x)
    q(x)
    t (x0   x)
    xi x0
- pick a variable       
    xi x0
t (x0 x)
    x(1) t (x0 x)
 ! x(2) t (x0 x)
- obtain       by only resampling this variable according to
    xi x0
 ! x(k)
 !       
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    supervised learning example: (x, y) x y
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    t (x0   x)
machine learning
    training set: dtrain = {(x(t), y(t))}
return
    xi x0
    supervised learning example: (x, y) x y
    f (x;    )
    supervised learning example: (x, y) x y
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    often, we simply cycle through the variables, in random order
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
machine learning
    f (x;    )
   
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    supervised learning example: (x, y) x y
    dvalid dtest
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}
    f (x;    )

machine learning

 ! x(3) t (x0 x)

machine learning

arg min

30

-

p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
machine learning
t pt(x(t)  b  )(x(t)  b  )>
machine learning

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
topics: supervised learning
    learning example: 
    supervised learning example: (x, y)
    task to solve: predict target     from input
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t)}
    classi   cation: target is a class id (from 0 to nb. of class - 1)
    training set: dtrain = {(x(t), y(t)}
    training set: dtrain = {(x(t), y(t)}
    regression: target is a real number
   

machine learning

machine learning

31

t pt(x(t)  b  )(x(t)  b  )>

machine learning
machine learning
topics: unsupervised learning
    learning example: 
    supervised learning example: (x, y) x y
    no explicit target to predict
    training set: dtrain = {(x(t), y(t)}
    id91: partition data into groups
    feature extraction: learn meaningful features automatically
    id84: learning a lower-dimensional representation of input

32

   

p(x(1), . . . , x(t )) =yt

   

topics: learning algorithm, model, training set
    learning algorithm        

p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
b    = arg max
b    = arg max
t pt(x(t)  b  )(x(t)  b  )>
machine learning
   
t pt(x(t)  b  )(x(t)  b  )>
   
t pt(x(t)  b  )(x(t)  b  )>
p(x(1), . . . , x(t )) =yt
t b    = 1
p(x(1), . . . , x(t )) =yt
    t 1
t b    = 1
    t 1
t b    = 1
    t 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t b    = 1
    t 1
    t 1
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    takes as input a training set
    training set: dtrain = {(x(t), y(t))}
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    outputs a model 
    f (x;    )
   
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    training set: dtrain = {(x(t), y(t))}
    the model has learned the information present in 
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    f (x;    )
    f (x;    )

    we can now use the model            on new inputs

    we then say the model            was trained on 

machine learning

machine learning

machine learning

machine learning
machine learning

machine learning

33

machine learning

t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
machine learning
machine learning
machine learning
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    supervised learning example: (x, y) x y
topics: training, validation and test sets, generalization
    training set: dtrain = {(x(t), y(t))}
    training set            serves to train a model
    f (x;    )
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    validation set           serves to select hyper-parameters
    dvalid dtest
    f (x;    )
    test set           serves to estimate the generalization 
    dvalid dtest
performance (error)

    generalization is the behavior of the model on unseen 
examples
    this is what we care about in machine learning!

34

machine learning

topics: capacity of a model, under   tting, over   tting, hyper-
parameter, model selection
    capacity:    exibility of a model 
    hyper-parameter: a parameter of a model that is not trained 
(speci   ed before training)
    under   tting: state of model which could improve 
generalization with more training or capacity
    over   tting: state of model which could improve generalization 
with more training or capacity
    model selection: process of choosing the best hyper-
parameters on validation set

35

machine learning

topics: capacity of a model, under   tting, over   tting, hyper-
parameter, model selection

0.5

0.4

0.3

0.2

0.1

0

training

validation

under   tting

over   tting

training time or capacity

36

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:

    training error will ?
    generalization error will ?

    if training time increases:

    training error will ?
    generalization error will ?

    if training set size increases:

    generalization error will ?
    difference between the training and generalization error will ?

37

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:
    training error will ?
decrease
    generalization error will ?

    if training time increases:

    training error will ?
    generalization error will ?

    if training set size increases:

    generalization error will ?
    difference between the training and generalization error will ?

37

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:
    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training time increases:

    training error will ?
    generalization error will ?

    if training set size increases:

    generalization error will ?
    difference between the training and generalization error will ?

37

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:
    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training time increases:

    training error will ?
decrease
    generalization error will ?

    if training set size increases:

    generalization error will ?
    difference between the training and generalization error will ?

37

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:
    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training time increases:

    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training set size increases:

    generalization error will ?
    difference between the training and generalization error will ?

37

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:
    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training time increases:

    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training set size increases:

    generalization error will ?
decrease (or maybe stay the same)
    difference between the training and generalization error will ?

37

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error
    if capacity increases:
    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training time increases:

    training error will ?
decrease
    generalization error will ?

increase or decrease

    if training set size increases:

    generalization error will ?
decrease (or maybe stay the same)
    difference between the training and generalization error will ?

decrease

37

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
    training set: dtrain = {(x(t), y(t))}
t b    = 1
    t 1
machine learning
    supervised learning example: (x, y) x y
    f (x;    )
machine learning
machine learning
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
topics: empirical risk minimization, id173
    training set: dtrain = {(x(t), y(t))}
   
    f (x;    )
    f (x;    )
    empirical risk minimization
    f (x;    )
    dvalid dtest
    dvalid dtest
    framework to design learning algorithms
    dvalid dtest
   
   

t xt

arg min

1

   

   

   

1

arg min

arg min
1
arg min

t xt
t xt
t xt

1
l(f (x(t);    ), y(t)) +     (   )

    l(f (x(t);    ), y(t))
l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )
       (   )
      =   1
    l(f (x(t);    ), y(t))
                                  is a id168
    l(f (x(t);    ), y(t))
               is a regularizer (penalizes certain values of     )
              +  
       (   )
       (   )
    learning is cast as optimization

   

t pt r   l(f (x(t);    ), y(t))    r      (   )

    ideally, we   d optimize classi   cation error, but it   s not smooth
    id168 is a surrogate for what we truly should optimize (e.g. upper bound)

38

machine learning

topics: id119
    id119: procedure to minimize a function

    compute gradient
    take step in opposite direction

39

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

machine learning

topics: id119

descent 
direction

 f(x)

 x

 

40

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

t xt
t xt
t xt

1

   

   

arg min
1

   
topics: id119
arg min
    id119 for empirical risk minimization

machine learning
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )

    f (x;    )
    dvalid dtest
   
    l(f (x(t);    ), y(t))
   
       (   )
      =   1
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    initialize 
              +  
       (   )
    for n iterations
       (   )
      =   1
 
-
      =   1
              +      
-
              +  
    {x 2 rd | rxf (x) = 0}
    v>r2
xf (x)v > 0 8v
    v>r2
xf (x)v < 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))

5

41

machine learning

topics: local and global optima

42

   
   

1
1

l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )

t pt r   l(f (x(t);    ), y(t))    r      (   )
    f (x;    )
       (   )
      =   1
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
       (   )
    dvalid dtest
    dvalid dtest
t pt r   l(f (x(t);    ), y(t))    r      (   )
    training set: dtrain = {(x(t), y(t))}
machine learning
    f (x;    )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
   
              +  
      =   1
   
t xt
    f (x;    )
    dvalid dtest
t xt
arg min
arg min
    dvalid dtest
topics: critical points, local optima, saddle point, curvature
   
              +  
    {x 2 rd | rxf (x) = 0}
              +  
t xt
   
    l(f (x(t);    ), y(t))
    critical points: 
t xt
    {x 2 rd | rxf (x) = 0}
    l(f (x(t);    ), y(t))
    {x 2 rd | rxf (x) = 0}
    v>r2
xf (x)v > 0 8v
    l(f (x(t);    ), y(t))
       (   )
       (   )
    v>r2
xf (x)v > 0 8v
t pt r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
    curvature in direction     : 
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v < 0 8v
      =   1
    v>r2
       (   )
      =   1
xf (x)v < 0 8v
              +  
              +  
      =   1
    v>r2
              +  
xf (x)v < 0 8v
    {x 2 rd | rxf (x) = 0}
    types of critical points:
    {x 2 rd | rxf (x) = 0}
              +  
    {x 2 rd | rxf (x) = 0}
    v>r2
    local minima:                                           (i.e.              positive de   nite)
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v > 0 8v
    local maxima:                                          (i.e.               negative de   nite)
    v>r2
xf (x)v < 0 8v
    v>r2
5
xf (x)v > 0 8v
    v>r2
xf (x)v < 0 8v
    v>r2
xf (x)v < 0 8v
    saddle point:    curvature is positive in certain directions and negative in others
    v>r2
xf (x)v < 0 8v

t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

arg min

arg min

5

1

1

   

   

5
5

43

5
5

machine learning

topics: saddle point

44

   

   

1

t xt

l(f (x(t);    ), y(t)) +     (   )

t xt
t xt
arg min
   
    l(f (x(t);    ), y(t))
machine learning
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
       (   )
       (   )
topics: stochastic id119
t pt r   l(f (x(t);    ), y(t))    r      (   )
   
              +  
t xt
      =   1
t pt r   l(f (x(t);    ), y(t))    r      (   )
l(f (x(t);    ), y(t)) +     (   )
arg min
      =   1
    algorithm that performs updates after each example
    {x 2 rd | rxf (x) = 0}
              +  
   
    v>r2
xf (x)v > 0 8v
    initialize 
    {x 2 rd | rxf (x) = 0}
              +  
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    for n iterations
    v>r2
xf (x)v > 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    v>r2
for each training example
xf (x)v < 0 8v
    (x(t), y(t))
      =   1
     
      =  r   l(f (x(t);    ), y(t))    r      (   )
     
              +      
    {x 2 rd | rxf (x) = 0}
    v>r2
xf (x)v > 0 8v
    v>r2
xf (x)v < 0 8v
5
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))

t pt r   l(f (x(t);    ), y(t))    r      (   )

5

5

-

45

machine learning

    l(f (x(t);    ), y(t))
       (   )
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
topics: bias-variance trade-off
      =   1
       (   )
       (   )
       (   )
    l(f (x(t);    ), y(t))
    variance of trained model: does it vary a lot if the training set 
    l(f (x(t);    ), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
      =   1
      =   1
      =   1
changes 
       (   )
       (   )
    {x 2 rd | rxf (x) = 0}
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
              +  
              +  
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
    bias of trained model: is the average model close to the true 
      =   1
    v>r2
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
              +  
solution
              +  
    v>r2
xf (x)v < 0 8v
    v>r2
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v > 0 8v
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    generalization error can be seen as the sum of bias and the 
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
    v>r2
    v>r2
xf (x)v < 0 8v
xf (x)v < 0 8v
xf (x)v < 0 8v
    v>r2
xf (x)v > 0 8v
variance
    v>r2
xf (x)v > 0 8v
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
xf (x)v < 0 8v
    v>r2
xf (x)v < 0 8v
possible
    f    f
    (x(t), y(t))
    (x(t), y(t))
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    f    f
    f    f
    f    f
    (x(t), y(t))
    (x(t), y(t))
possible
    f    f
possible
    f    f

t pt r   l(f (x(t);    ), y(t))    r      (   )

t pt r   l(f (x(t);    ), y(t))    r      (   )

low variance/

high bias

good trade-off

high variance/

low bias

46

machine learning

topics: parametric vs. non-parametric
    parametric model: its capacity is    xed and does not increase 
with the amount of training data
    examples: linear classi   er, neural network with    xed number of hidden units, etc.

    non-parametric model: the capacity increases with the 
amount of training data
    examples: k nearest neighbors classi   er, neural network with adaptable hidden 

layer size, etc.

47

python

http://docs.python.org/tutorial/ (en)
http://www.dmi.usherb.ca/~larocheh/cours/tutoriel_python.html (fr)

48

mlpython

http://www.dmi.usherb.ca/~larocheh/mlpython/tutorial.html#tutorial (en)

49

