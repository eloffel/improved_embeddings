neural networks
deep learning - example

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
 xh(1)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
deep learning
   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log p(h(1))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
d =nh(l 1)(x(t))ot
 xh(1)
q(h(1)|x) log p(h(1))
 xh(1)
 xh(1)
 xh(1)
 xh(1)
q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot

 xh(1)
d =nh(l 1)(x(t))ot

    build unsupervised training set (with                     ): 
    w(l+1) b(l+1)

topics: pseudocode
    for l=1 to l

q(h(1)|x) log p(h(1))

h(0)(x) = x

q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))

(9)

t=1

t=1
t=1

t=1

   

   

t=1

   

   
   

   
   

 

t=1
t=1

q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

2

h(0)(x) = x

    w(l+1) b(l+1)

 to initialize the deep network parameters        ,

    train       greedy module       (rbm, autoencoder) on 
    use hidden layer weights and biases of greedy module
h(0)(x) = x
h(0)(x) = x
    initialize          ,           randomly (as usual)
    train the whole neural network using (supervised) 
stochastic id119 (with backprop)

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

h(0)(x) = x
h(0)(x) = x

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

   r(x)={x2rh|9wx=pjwjx  ,j}
   {x2rh|x/2r(x)}

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

h(0)(x) = x

t=1

t=1

   

   
   

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
 xh(1)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
deep learning
   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log p(h(1))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
d =nh(l 1)(x(t))ot
 xh(1)
q(h(1)|x) log p(h(1))
 xh(1)
 xh(1)
 xh(1)
 xh(1)
q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot

 xh(1)
d =nh(l 1)(x(t))ot

    build unsupervised training set (with                     ): 
    w(l+1) b(l+1)

topics: pseudocode
    for l=1 to l

q(h(1)|x) log p(h(1))

pre-training

q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))

(9)

t=1
t=1

   

   

t=1

   
   

 

t=1
t=1

q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

2

h(0)(x) = x

    w(l+1) b(l+1)

 to initialize the deep network parameters        ,

    train       greedy module       (rbm, autoencoder) on 
    use hidden layer weights and biases of greedy module
h(0)(x) = x
h(0)(x) = x
    initialize          ,           randomly (as usual)
    train the whole neural network using (supervised) 
stochastic id119 (with backprop)

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

h(0)(x) = x
h(0)(x) = x

   r(x)={x2rh|9wx=pjwjx  ,j}
   {x(t)}
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
 xh(1)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
deep learning
   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log p(h(1))
   9w,t    x(t   ) =pt6=t   wtx(t)
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
   {x2rh|x/2r(x)}
d =nh(l 1)(x(t))ot
 xh(1)
q(h(1)|x) log p(h(1))
 xh(1)
 xh(1)
 xh(1)
 xh(1)
q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))
   r(x)={x2rh|9w x=pjwjx  ,j}
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
   {x2rh|x /2r(x)}

    train       greedy module       (rbm, autoencoder) on 
    use hidden layer weights and biases of greedy module
h(0)(x) = x
h(0)(x) = x
    initialize          ,           randomly (as usual)
    train the whole neural network using (supervised) 
stochastic id119 (with backprop)

 xh(1)
d =nh(l 1)(x(t))ot

    build unsupervised training set (with                     ): 
    w(l+1) b(l+1)

topics: pseudocode
    for l=1 to l

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

 to initialize the deep network parameters        ,

    w(l+1) b(l+1)

q(h(1)|x) log p(h(1))

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

   ne-
tuning

pre-training

h(0)(x) = x

h(0)(x) = x

h(0)(x) = x
h(0)(x) = x

(9)

t=1

t=1
t=1

t=1

   

   

   

   
   

   
   

 

t=1

q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))

t=1
t=1

q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

2

deep learning

3

topics: datasets
    datasets generated with varying number of factors of variations

an empirical evaluation of deep architectures on problems with many factors of variation

variations on mnist

tall or wide?

an empirical evaluation of deep architectures on problems with many factors of variation

mnist-rotation

mnist-random-

background

mnist-image-
background

mnist-

background- 

figure 3. iterative training construction of the stacked au-

(c) predict y

an empirical evaluation of deep architectures on problems with many factors of variation

figure 5. from top to bottom, samples from rectangles and
rectangles-image.

convex shape or not?

3.2. discrimination between tall and wide

rectangles

rotation
an empirical evaluation of deep architectures on problems with many factors of variation
larochelle, erhan, courville, bergstra and bengio, 2007

in this task, a learning algorithm needs to recognize
whether a rectangle contained in an image has a larger
figure 6. samples from convex, where the    rst, fourth,    fth
width or length. the rectangle can be situated any-
and last samples correspond to convex white pixel sets.
where in the 28   28 pixel image. we generated two
datasets for this problem:

figure 4. from top to bottom, samples from mnist-rot,
mnist-back-rand, mnist-back-image, mnist-rot-back-image.

figure 5. from top to bottom, samples from rectangles and

figure 6. samples from convex, where the    rst, fourth,    fth
and last samples correspond to convex white pixel sets.

like the mnist dataset, the convex and non-convex
datasets both consist of images of 28   28 pixels. the
convex sets consist of a single convex region with pixels
of value 255 (white). candidate convex images were
constructed by taking the intersection of a random
number of half-planes whose location and orientation
were chosen uniformly at random.
candidate non-convex images were constructed by
taking the union of a random number of convex sets

deep learning

4

topics: impact of initialization

97

network

mnist-small

type

depth

classif. test error

neural network
(random initialization,
+    ne-tuning)

deep net

saa network
(autoassociator learning
+    ne-tuning)

deep net +
autoencoder

deep net +

srbm network
(cd-1 learning
+    ne-tuning)

rbm

1
2
3
4

1
2
3
4

1
2
3
4

4.14 %    0.17
4.03 %    0.17
4.24 %    0.18
4.47 %    0.18
3.87 %    0.17
3.38 %    0.16
3.37 %    0.16
3.39 %    0.16
3.17 %    0.15
2.74 %    0.14
2.71 %    0.14
2.72 %    0.14

mnist-rotation
classif. test error
15.22 %    0.31
10.63 %    0.27
11.98 %    0.28
11.73 %    0.29
11.43%    0.28
9.88 %    0.26
9.22 %    0.25
9.20 %    0.25
10.47 %    0.27
9.54 %    0.26
8.80 %    0.25
8.83 %    0.24

tableau 6.3     classi   cation performance on mnist-small and mnist-rotation of dif-

deep learning

erhan, bengio, courville, manzagol, vincent and bengio

topics: impact of initialization

5

why does unsupervised pre-training help deep learning? 
erhan, bengio, courville, manzagol, vincent and bengio, 2011

figure 9: effect of layer size on the changes brought by unsupervised pre-training, for networks
with 1, 2 or 3 hidden layers. experiments on mnist. error bars have a height of two

deep learning

erhan, bengio, courville, manzagol, vincent and bengio

topics: impact of initialization

5

acts as a regularizer:

- over   ts less with large capacity
- under   ts with small capacity

why does unsupervised pre-training help deep learning? 
erhan, bengio, courville, manzagol, vincent and bengio, 2011

figure 9: effect of layer size on the changes brought by unsupervised pre-training, for networks
with 1, 2 or 3 hidden layers. experiments on mnist. error bars have a height of two

3.5%

r
o
r
r
e
 
s
s
a
c

l

3%

3.5%

deep learning

r
o
r
r
e
 
s
s
a
c

3%

l

6

topics: choice of hidden layer size

total number of hidden units

1500

2.5%

900

3k

6k

2.5%

(a) srbm network.

rbm

srbm

900

1500

3k

6k

total number of hidden units

99

(b) saa network.

autoencoder

saa

4%

figure 6.9     classi   cation performance on mnist-small of 3-layer deep networks for
three kinds of architectures, as a function of the total number of hidden units. the three
architectures have increasing / constant / decreasing layer sizes from the bottom to the
top layers. error-bars represent 95% con   dence intervals.

3.5%

3.5%

4%

decreasing width
constant width
increasing width

decreasing width
constant width
increasing width

r
o
r
r
e
 
s
s
a
c

l

r
o
r
r
e
 
s
s
a
c

l

3%

2.5%

900

12.5%

12%

11.5%

1500

3k

6k

total number of hidden units

srbm

(a) srbm network.

decreasing width
constant width
increasing width

3%

2.5%

900

13%

12.5%

12%

1500

3k

6k

total number of hidden units

saa

(b) saa network.

decreasing width
constant width
increasing width

11%

r
o
r
r
e
 
s
s
a
c

figure 6.9     classi   cation performance on mnist-small of 3-layer deep networks for
three kinds of architectures, as a function of the total number of hidden units. the three
architectures have increasing / constant / decreasing layer sizes from the bottom to the
top layers. error-bars represent 95% con   dence intervals.

r
o
r
r
e
 
s
s
a
c

10.5%

11.5%

10.5%

10%

11%

l

l

9.5%

9%

900

1500

3k

6k

total number of hidden units

10%

9.5%

900

1500

3k

6k

total number of hidden units

(a) srbm network.

(b) saa network.

extracting and composing robust features with denoising autoencoders

extracting and composing robust features with denoising autoencoders

deep learning

7

topics: performance on different datasets

table 1. comparison of stacked denoising autoencoders (sda-3) with other models.
test error rate on all considered classi   cation problems is reported together with a 95% con   dence interval. best performer
is in bold, as well as those for which con   dence intervals overlap. sda-3 appears to achieve performance superior or
stacked
equivalent to the best other model on all problems except bg-rand. for sda-3, we also indicate the fraction   of destroyed
input components, as chosen by proper model selection. note that saa-3 is equivalent to sda-3 with   = 0%.

table 1. comparison of stacked denoising autoencoders (sda-3) with other models.
test error rate on all considered classi   cation problems is reported together with a 95% con   dence interval. best performer
is in bold, as well as those for which con   dence intervals overlap. sda-3 appears to achieve performance superior or
equivalent to the best other model on all problems except bg-rand. for sda-3, we also indicate the fraction   of destroyed
input components, as chosen by proper model selection. note that saa-3 is equivalent to sda-3 with   = 0%.

denoising autoencoders

autoencoders

stacked
rbms

stacked

id166rbf
3.03  0.15
11.11  0.28
14.58  0.31
22.61  0.37
55.18  0.44
2.15  0.13
24.04  0.37
19.13  0.34

dataset
id166poly
basic
3.69  0.17
rot
15.42  0.32
bg-rand
16.62  0.33
bg-img
24.01  0.37
rot-bg-img
56.41  0.43
rect
2.15  0.13
rect-img
24.05  0.37
convex
19.82  0.35

id166rbf
dbn-1
3.03  0.15
3.94  0.17
11.11  0.28
14.69  0.31
14.58  0.31
9.80  0.26
22.61  0.37
16.15  0.32
55.18  0.44
52.21  0.44
2.15  0.13
4.71  0.19
24.04  0.37
23.69  0.37
19.13  0.34
19.92  0.35

id166poly
saa-3
3.69  0.17
3.46  0.16
15.42  0.32
10.30  0.27
16.62  0.33
11.28  0.28
/
24.01  0.37
23.00  0.37
56.41  0.43
51.93  0.44
2.15  0.13
2.41  0.13
24.05  0.37
24.05  0.37
19.82  0.35
18.41  0.34

dbn-1
dbn-3
3.94  0.17
3.11  0.15
14.69  0.31
10.30  0.27
9.80  0.26
6.73  0.22
16.15  0.32
16.31  0.32
52.21  0.44
47.39  0.44
4.71  0.19
2.60  0.14
23.69  0.37
22.50  0.37
19.92  0.35
18.63  0.34

saa-3
sda-3 ( )
3.46  0.16
2.80  0.14 (10%)
10.30  0.27
10.29  0.27 (10%)
11.28  0.28
10.38  0.27 (40%)
23.00  0.37
16.68  0.33 (25%)
51.93  0.44
44.49  0.44 (25%)
2.41  0.13
1.99  0.12 (10%)
24.05  0.37
21.59  0.36 (25%)
18.41  0.34
19.06  0.34 (10%)

dbn-3
3.11  0.15
10.30  0.27
6.73  0.22
16.31  0.32
47.39  0.44
2.60  0.14
22.50  0.37
18.63  0.34

sda-3 ( )
2.80  0.14 (10%)
10.29  0.27 (10%)
10.38  0.27 (40%)
16.68  0.33 (25%)
44.49  0.44 (25%)
1.99  0.12 (10%)
21.59  0.36 (25%)
19.06  0.34 (10%)

extracting and composing robust features with denoising autoencoders,  
vincent, larochelle, bengio and manzagol, 2008.

