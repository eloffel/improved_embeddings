neural networks
feedforward neural network - activation function

hugo.larochelle@usherbrooke.ca
september 6, 2012

2

september 6, 2012

abstract

abstract

september 6, 2012
september 6, 2012

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

september 6, 2012
universit  e de sherbrooke
september 6, 2012
hugo.larochelle@usherbrooke.ca
artificial neuron
math for my slides    feedforward neural network   .
september 6, 2012
abstract
topics: connection weights, bias, activation function
september 6, 2012
    neuron pre-activation (or input activation):
math for my slides    feedforward neural network   .
abstract
math for my slides    feedforward neural network   .

    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    neuron (output) activation
    x1 xd b w1 wd
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    x1 xd b w1 wd
    x1 xd b w1 wd
math for my slides    feedforward neural network   .
    x1 xd b w1 wd
    x1 xd b w1 wd
...
    w
    w
    w
    x1 xd b w1 wd
    x1 xd
    x1 xd
    w
    w
         are the connection weights
    {
    {
    w
    {
    w
        is the neuron bias 
    {
    w
    g(  ) b
    {
    g(  ) b
    g(  ) b
    {
             is called the activation function 
    g(  ) b
    g(  ) b
    {
    g(  ) b
    {
    h(x) = g(a(x))
    h(x) = g(a(x))
    g(  ) b
    h(x) = g(a(x))

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
abstract

math for my slides    feedforward neural network   .

abstract
abstract

abstract

1

abstract

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

activation function

3
abstract

math for my slides    feedforward neural network   .

topics: linear activation function
    performs no input
squashing
    not very interesting...

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    w
    {
    g(a) = a
    sigm(a) =

1

1+exp( a)

topics: sigmoid activation function
    squashes the neuron   s
pre-activation between 
0 and 1
    always positive
    bounded
    strictly increasing

math for my slides    feedforward neural network   .

abstract

4

activation function
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    w
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

math for my slides    feedforward neural network   .

activation function

5

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

topics: hyperbolic tangent (      tanh      ) activation function
    squashes the neuron   s
pre-activation between 
    x1 xd b w1 wd
-1 and 1
    w
    can be positive or
negative 
    {
    bounded
    g(a) = a
    strictly increasing
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

6

    h(x) = g(a(x)) = g(b +pi wixi)
activation function

topics: recti   ed linear activation function
    bounded below by 0
(always non-negative)
    not upper bounded
    strictly increasing
    tends to give neurons
with sparse activities 

    x1 xd b w1 wd
    w
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(  ) b

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

