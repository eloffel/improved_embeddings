neural networks
id161 - convolutional rbm

convolutional rbm

2

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional id50 for scalable unsupervised learning of hierarchical representations
convolutional id50 for scalable unsupervised learning of hierarchical representations
pk (pooling layer)

(nw , nv   nh + 1); the    lter weights are shared
across all the hidden units within the group. in addi-
tion, each hidden group has a bias bk and all visible
units share a single bias c.
we de   ne the energy function e(v, h) as:

topics: convolutional rbm
(nw , nv   nh + 1); the    lter weights are shared
(nw , nv   nh + 1); the    lter weights are shared
    how about designing convolutional unsupervised networks
across all the hidden units within the group. in addi-
pk
across all the hidden units within the group. in addi-
pk
  
  
    let   s consider the case of the rbm
tion, each hidden group has a bias bk and all visible
tion, each hidden group has a bias bk and all visible
c
units share a single bias c.
units share a single bias c.
hk
hk
    could use same convolutional connectivity between input (v) and hidden layer (h)
i,j
i,j
we de   ne the energy function e(v, h) as:
we de   ne the energy function e(v, h) as:

lee et al. 2009

nh
nh

np
np

nh

np

c
c

pk
  

pk (pooling layer)
pk (pooling layer)

hk (detection layer)

hk (detection layer)
hk (detection layer)

hk
i,j

wk

rsvi+r 1,j+s 1

p (v, h) =
p (v, h) =

1
p (v, h) =
exp( e(v, h))
z
1
1
exp( e(v, h))
exp( e(v, h))
nhxi,j=1
nwxr,s=1
kxk=1
z
z
e(v, h) =  
hk
ijw k
nwxr,s=1
nhxi,j=1
kxk=1
nhxi,j=1
nwxr,s=1
kxk=1
e(v, h) =  
e(v, h) =  
hk
ijw k
hk
ijw k
rsvi+r 1,j+s 1
rsvi+r 1,j+s 1
nhxi,j=1
kxk=1
hk
ij   c
vij.
nhxi,j=1
nvxi,j=1
kxk=1
nhxi,j=1
nvxi,j=1
kxk=1
hk
hk
ij   c
ij   c
using the operators de   ned previously,
using the operators de   ned previously,
kxk=1
kxk=1
hk     (   w k     v)  
e(v, h) =  
kxk=1
kxk=1
kxk=1
kxk=1
bkxi,j
bkxi,j
hk     (   w k     v)  
hk     (   w k     v)  
e(v, h) =  
e(v, h) =  

vij.
vij.
using the operators de   ned previously,

bkxi,j
i,j   cxi,j
i,j   cxi,j

nvxi,j=1

 
bk
bk

 
 

hk
hk

hk

bk

(1)
(1)

as with standard rbms (section 2.1), we can perform

i,j   cxi,j

vij.
vij.

nv

v
v

wk
wk

v  (visible layer)

v  (visible layer)
v  (visible layer)

    hkij are the hidden units of 
v
nw
the kth feature map
    wkrs are the weights to the 
nw
nw
kth feature map
(1)
~
    wk are the weights with 
figure 1. convolutional rbm with probabilistic max-
figure 1. convolutional rbm with probabilistic max-
   ipped rows and columns 
pooling. for simplicity, only group k of the detection layer
pooling. for simplicity, only group k of the detection layer
(convolution kernel)
and the pooing layer are shown. the basic crbm corre-
and the pooing layer are shown. the basic crbm corre-
sponds to a simpli   ed structure with only visible layer and
sponds to a simpli   ed structure with only visible layer and
detection (hidden) layer. see text for details.
detection (hidden) layer. see text for details.
vij.
to simplify the notation, we consider a model with a
to simplify the notation, we consider a model with a
visible layer v , a detection layer h, and a pooling layer
visible layer v , a detection layer h, and a pooling layer

nv
nv
figure 1. convolutional rbm with probabilistic max-
pooling. for simplicity, only group k of the detection layer
and the pooing layer are shown. the basic crbm corre-
sponds to a simpli   ed structure with only visible layer and
detection (hidden) layer. see text for details.
to simplify the notation, we consider a model with a
visible layer v , a detection layer h, and a pooling layer
p , as shown in figure 1. the detection and pooling
layers both have k groups of units, and each group

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional id50 for scalable unsupervised learning of hierarchical representations

(nw , nv   nh + 1); the    lter weights are shared
across all the hidden units within the group. in addi-
tion, each hidden group has a bias bk and all visible

3

vi,j

subj. to

i,j(   w k     v)i,j + bkhk

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional rbm

increase in energy caused by turning on unit hk
 i(hk
p (hk

tion that we call probabilistic max-pooling.
if and only if a detection unit is on. equivalently, we
can consider these c2+1 units as a single random vari-
in general, higher-level feature detectors need informa-
set of shared weights   = { 1,1, . . . ,  k,k0}, where
i,j is
able which may take on one of c2 + 1 possible values:
tion from progressively larger input regions. existing
one value for each of the detection units being on, and
 k,` is a weight matrix connecting pooling unit p k to
translation-invariant representations, such as convolu-
i,j), and the id155 is given by:
one value indicating that all units are o   .
tional networks, often involve two kinds of layers in
detection unit h0`. the de   nition can be extended to
lee et al. 2009
alternation:    detection    layers, whose responses are
we formally de   ne the energy function of this simpli-
topics: convolutional rbm
i,j))
exp(i(hk
deeper networks in a straightforward way.
computed by convolving a feature detector with the
   ed probabilistic max-pooling-crbm as follows:
i,j = 1|v) =
1 +p(i0,j0)2b   
previous layer, and    pooling    layers, which shrink the
i0,j0))
exp(i(hk
    we can introduce a notion of probabilistic pooling
i,j      cxi,j
e(v, h) =  xk xi,j    hk
note that an energy function for this sub-network con-
representation of the detection layers by a constant
factor. more speci   cally, each unit in a pooling layer
1
sists of two kinds of potentials: unary terms for each
convolutional id50 for scalable unsupervised learning of hierarchical representations
    pooling unit      above is 1 only if at least one hidden unit        in neighborhood is 1
x(i,j)2b   
computes the maximum activation of the units in a
set of shared weights   = { 1,1, . . . ,  k,k0}, where
p (pk
    = 0|v) =
i,j is
increase in energy caused by turning on unit hk
increase in energy caused by turning on unit hk
i,j is
1 +p(i0,j0)2b   
i0,j0)) .
hk
i,j     1, 8k,    .
of the groups in the detection layers, and interaction
exp(i(hk
small region of the detection layer. shrinking the rep-
 k,` is a weight matrix connecting pooling unit p k to
i,j), and the id155 is given by:
 i(hk
    within a pooling neighborhood, allow at most only a single unit        equal to 1
 i(hk
i,j), and the id155 is given by:
set of shared weights   = { 1,1, . . . ,  k,k0}, where
resentation with max-pooling allows higher-layer rep-
increase in energy caused by turning on unit hk
i,j is
terms between v and h and between p and h0:
detection unit h0`. the de   nition can be extended to
resentations to be invariant to small translations of the
sampling the visible layer v given the hidden layer
 k,` is a weight matrix connecting pooling unit p k to
 i(hk
i,j), and the id155 is given by:
e(v, h, p, h0) =  xk
v     (w k     hk)  xk
input and reduces the computational burden.
i,j))
exp(i(hk
deeper networks in a straightforward way.
h can be performed in the same way as described in
detection unit h0`. the de   nition can be extended to
p (hk
i,j = 1|v) =
1 +p(i0,j0)2b   
i,j = 1|v) =
p (hk
1 +p(i0,j0)2b   
max-pooling was intended only for feed-forward archi-
exp(i(hk
i0,j0))
exp(i(hk
i,j))
i0,j0))
section 3.2.
deeper networks in a straightforward way.
note that an energy function for this sub-network con-
tectures. in contrast, we are interested in a generative
ij) , bk + (   w k     v)ij.
i(hk
(2)
p (hk
1 +p(i0,j0)2b   
pk     ( k`     h0`)  x`
 xk,`
model of images which supports both top-down and
1
sists of two kinds of potentials: unary terms for each
exp(i(hk
i0,j0))
1
set of shared weights   = { 1,1, . . . ,  k,k0}, where
note that an energy function for this sub-network con-
increase in energy caused by turning on unit hk
i,j is
3.4. training via sparsity id173
    = 0|v) =
p (pk
bottom-up id136. therefore, we designed our gen-
now, we sample each block independently as a multi-
1 +p(i0,j0)2b   
i0,j0)) .
p (pk
    = 0|v) =
implies       is 1
1 +p(i0,j0)2b   
i0,j0)) .
of the groups in the detection layers, and interaction
exp(i(hk
 k,` is a weight matrix connecting pooling unit p k to
i,j), and the id155 is given by:
 i(hk
hk
erative model so that id136 involves max-pooling-
i,j is a hid-
nomial function of its inputs. suppose hk
1
sists of two kinds of potentials: unary terms for each
i,j
our model is overcomplete in that the size of the rep-
p (pk
detection unit h0`. the de   nition can be extended to
terms between v and h and between p and h0:
1 +p(i0,j0)2b   
like behavior.
den unit contained in block     (i.e., (i, j) 2 b   ), the
i0,j0)) .
of the groups in the detection layers, and interaction
exp(i(hk
i,j))
exp(i(hk
deeper networks in a straightforward way.
resentation is much larger than the size of the inputs.
sampling the visible layer v given the hidden layer
i,j = 1|v) =
p (hk
e(v, h, p, h0) =  xk
v     (w k     hk)  xk
sampling the visible layer v given the hidden layer
1 +p(i0,j0)2b   
to sample the detection layer h and pooling layer p ,
wk
terms between v and h and between p and h0:
exp(i(hk
i0,j0))
note that an energy function for this sub-network con-
h can be performed in the same way as described in
in fact, since the    rst hidden layer of the network con-
h can be performed in the same way as described in
note that the detection layer h k receives the following
sampling the visible layer v given the hidden layer
1
sists of two kinds of potentials: unary terms for each
v     (w k     hk)  xk
e(v, h, p, h0) =  xk
section 3.2.
tains k groups of units, each roughly the size of the
    = 0|v) =
1 +p(i0,j0)2b   
i0,j0)) .
v
section 3.2.
h can be performed in the same way as described in
of the groups in the detection layers, and interaction
bottom-up signal from layer v :
nv
convolutional id50 for scalable unsupervised learning of hierarchical representations
 xk,`
pk     ( k`     h0`)  x`
image, it is overcomplete roughly by a factor of k. in
terms between v and h and between p and h0:
section 3.2.
3.4. training via sparsity id173
sampling the visible layer v given the hidden layer
3.4. training via sparsity id173
e(v, h, p, h0) =  xk
v     (w k     hk)  xk
general, overcomplete models run the risk of learning
ij) , bk + (   w k     v)ij,
 xk,`
pk     ( k`     h0`)  x`
set of shared weights   = { 1,1, . . . ,  k,k0}, where
increase in energy caused by turning on unit hk
i,j is
implies all         are 0
i(hk
h can be performed in the same way as described in
our model is overcomplete in that the size of the rep-
3.4. training via sparsity id173
trivial solutions, such as feature detectors represent-
section 3.2.
 k,` is a weight matrix connecting pooling unit p k to
our model is overcomplete in that the size of the rep-
i,j), and the id155 is given by:
 i(hk
resentation is much larger than the size of the inputs.
 xk,`
pk     ( k`     h0`)  x`

we now discuss sampling the detection layer h and
the pooling layer p given the visible layer v . group k
receives the following bottom-up signal from layer v :

figure 1. convolutional rbm with probabilistic max-
pooling. for simplicity, only group k of the detection layer
and the pooing layer are shown. the basic crbm corre-

i,j = 1|v) =
    = 0|v) =

convolutional id50 for scalable unsupervised learning of hierarchical representations

i,j))
exp(i(hk

exp(i(hk

exp(i(hk

hk (detection layer)

pk (pooling layer)

v  (visible layer)

exp(i(hk

p (pk

(1)

pk
  

nw

nh

np

c

convolutional id50 for scalable unsupervised learning of hierarchical representations

4

hk

vij.

kxk=1

bkxi,j

e(v, h) =  

i,j   cxi,j

convolutional id50 for scalable unsupervised learning of hierarchical representations

increase in energy caused by turning on unit hk
p (hk

as with standard rbms (section 2.1), we can perform
block id150 using the following conditional
distributions:

convolutional id50 for scalable unsupervised learning of hierarchical representations

kxk=1
convolutional rbm
hk     (   w k     v)  

tion that we call probabilistic max-pooling.
in general, higher-level feature detectors need informa-
tion from progressively larger input regions. existing
translation-invariant representations, such as convolu-
tional networks, often involve two kinds of layers in
alternation:    detection    layers, whose responses are
topics: convolutional rbm
computed by convolving a feature detector with the
previous layer, and    pooling    layers, which shrink the
    given the        units, we sample each input independently:
e(v, h) =  xk xi,j    hk
set of shared weights   = { 1,1, . . . ,  k,k0}, where
i,j is
representation of the detection layers by a constant
ij = 1|v) =  ((   w k     v)ij + bk)
factor. more speci   cally, each unit in a pooling layer
 k,` is a weight matrix connecting pooling unit p k to
i,j), and the id155 is given by:
p (vij = 1|h) =  ((xk
x(i,j)2b   
computes the maximum activation of the units in a
i,j is
increase in energy caused by turning on unit hk
hk
i,j     1, 8k,    .
w k     hk)ij + c),
small region of the detection layer. shrinking the rep-
detection unit h0`. the de   nition can be extended to
i,j), and the id155 is given by:
 i(hk
resentation with max-pooling allows higher-layer rep-
resentations to be invariant to small translations of the
deeper networks in a straightforward way.
sigmoid
input and reduces the computational burden.
i0,j0))
p (hk
max-pooling was intended only for feed-forward archi-
note that an energy function for this sub-network con-
i0,j0))
tectures. in contrast, we are interested in a generative
i(hk
model of images which supports both top-down and
sists of two kinds of potentials: unary terms for each
bottom-up id136. therefore, we designed our gen-
i0,j0)) .
i0,j0)) .
of the groups in the detection layers, and interaction
erative model so that id136 involves max-pooling-
exp(i(hk
like behavior.
terms between v and h and between p and h0:
i,j = 1|v) =
sampling the visible layer v given the hidden layer
sampling the visible layer v given the hidden layer
bkxij
h can be performed in the same way as described in
h can be performed in the same way as described in
hk
    = 0|v) =
section 3.2.
ij
3.4. training via sparsity id173
e(v, h, p, h0) =  xk
v     (w k     hk)  xk
b0`xij
h0`
our model is overcomplete in that the size of the rep-
pk     ( k`     h0`)  x`
 xk,`
ij

where   is the sigmoid function. id150 forms
the basis of our id136 and learning algorithms.
i,j = 1|v) =
1 +p(i0,j0)2b   
ij) , bk + (   w k     v)ij.
3.3. probabilistic max-pooling
p (pk
    = 0|v) =
in order to learn high-level representations, we stack
1 +p(i0,j0)2b   
crbms into a multilayer architecture analogous to
dbns. this architecture is based on a novel opera-
1 +p(i0,j0)2b   
tion that we call probabilistic max-pooling.
e(v, h, p, h0) =  xk
v     (w k     hk)  xk
1 +p(i0,j0)2b   
in general, higher-level feature detectors need informa-
tion from progressively larger input regions. existing
translation-invariant representations, such as convolu-
increase in energy caused by turning on unit hk
i,j is
 xk,`
pk     ( k`     h0`)  x`
tional networks, often involve two kinds of layers in
3.4. training via sparsity id173
i,j), and the id155 is given by:
 i(hk

if and only if a detection unit is on. equivalently, we
to simplify the notation, we consider a model with a
can consider these c2+1 units as a single random vari-
visible layer v , a detection layer h, and a pooling layer
able which may take on one of c2 + 1 possible values:
one value for each of the detection units being on, and
p , as shown in figure 1. the detection and pooling
one value indicating that all units are o   .
layers both have k groups of units, and each group
lee et al. 2009
we formally de   ne the energy function of this simpli-
of the pooling layer has np     np binary units. for
   ed probabilistic max-pooling-crbm as follows:
each k 2 {1, ..., k}, the pooling layer p k shrinks the
i,j      cxi,j
representation of the detection layer h k by a factor
of c along each dimension, where c is a small in-
teger such as 2 or 3.
i.e., the detection layer h k is
partitioned into blocks of size c     c, and each block
    is connected to exactly one binary unit pk
pooling layer (i.e., np = nh/c). formally, we de   ne
b    , {(i, j) : hij belongs to the block    .}.
the detection units in the block b    and the pooling
unit p    are connected in a single potential which en-
forces the following constraints: at most one of the
detection units may be on, and the pooling unit is on
if and only if a detection unit is on. equivalently, we
1
can consider these c2+1 units as a single random vari-
able which may take on one of c2 + 1 possible values:
one value for each of the detection units being on, and
set of shared weights   = { 1,1, . . . ,  k,k0}, where
one value indicating that all units are o   .
 k,` is a weight matrix connecting pooling unit p k to

1
set of shared weights   = { 1,1, . . . ,  k,k0}, where
 k,` is a weight matrix connecting pooling unit p k to
detection unit h0`. the de   nition can be extended to
deeper networks in a straightforward way.
note that an energy function for this sub-network con-
sists of two kinds of potentials: unary terms for each
of the groups in the detection layers, and interaction
nv
convolutional id50 for scalable unsupervised learning of hierarchical representations
terms between v and h and between p and h0:

(2)
increase in energy caused by turning on unit hk
i,j is
now, we sample each block independently as a multi-
 i(hk
i,j), and the id155 is given by:
i,j is a hid-
nomial function of its inputs. suppose hk
den unit contained in block     (i.e., (i, j) 2 b   ), the
i,j))
exp(i(hk
p (hk
i0,j0))
exp(i(hk
i0,j0)) .
sampling the visible layer v given the hidden layer
h can be performed in the same way as described in
section 3.2.

1 +p(i0,j0)2b   
1 +p(i0,j0)2b   

we now discuss sampling the detection layer h and
the pooling layer p given the visible layer v . group k
receives the following bottom-up signal from layer v :

(nw , nv   nh + 1); the    lter weights are shared
across all the hidden units within the group. in addi-
1
tion, each hidden group has a bias bk and all visible

figure 1. convolutional rbm with probabilistic max-
pooling. for simplicity, only group k of the detection layer
and the pooing layer are shown. the basic crbm corre-

convolutional id50 for scalable unsupervised learning of hierarchical representations

i,j))
exp(i(hk

i,j))
exp(i(hk

i,j(   w k     v)i,j + bkhk

exp(i(hk

exp(i(hk

implies all         are 0

implies       is 1

exp(i(hk

hk (detection layer)

pk (pooling layer)

v  (visible layer)

exp(i(hk

subj. to

p (pk

hk
i,j

(1)

wk

pk
  

vi,j

nw

v

nh

np

c

convolutional rbm

5

lee et al. 2009
topics: convolutional rbm
    using these adapted conditionals, we can perform contrastive 
divergence
    energy gradients involve convolutions, similar to the backprop gradients in 

convolutional network

    can stack convolutional rbms

    provides a pretraining procedure which doesn   t require the extraction of patches

    see lee et al. 2009 for more details

