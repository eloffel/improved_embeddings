ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

stochastic id119 (sgd)

the classical convergence thoerem

rmsprop, momentum and adam

scaling leaid56g rates with batch size

sgd as mcmc and mcmc as sgd

an original sgd algorithm

1

vanilla sgd

   -=     g

  g = e(x,y)   batch       loss(  , x, y)

g = e(x,y)   train       loss(  , x, y)

for theoretical analysis we will focus on the case where the
training data is very large     essentially in   nite.

2

issues

    gradient estimation. the accuracy of   g as an estimate
of g.

    gradient drift (second order structure). the fact
that g changes as the parameters change.

3

a one dimensional example

suppose that y is a scalar, and consider

loss(  , x, y) =

(       y)2

1
2

g = e(x,y)   train d loss(  , x, y)/d   =        etrain[y]

  g = e(x,y)   batch d loss(  , x, y)/d   =        ebatch[y]

4

sgd as mcmc     the sgd stationary distribution

for small batches we have that each step of sgd makes a
random move in parameter space.

even if we start at the training loss optimum, an sgd step
will move away from the optimum.

sgd de   nes an mcmc process with a stationary distribution.

to converge to a local optimum the learning rate must be
gradually reduced to zero.

5

the classical convergence theorem

for    su   ciently smooth    non-negative loss with

   -=   t      loss(  , xt, yt)
(cid:88)

  t = 0

and

lim
t   0

t

  t =    ,

  t > 0

and

we have that the training loss of    converges (in practice   
converges to a local optimum of training loss).

rigor police: one can construct cases where    converges to a saddle point or even a limit cycle.

see    neuro-id145    by bertsekas and tsitsiklis proposition 3.5.

6

physicist   s proof of the convergence theorem

since limt   0   t = 0 we will eventually get to arbitrarilly small
learning rates.

for su   ciently small learning rates any meaningful update of
the parameters will be based on an arbitrarily large sample of
gradients at essentially the same parameter value.

but since(cid:80)

an arbitrarily large sample will become arbitrarily accurate as
an estimate of the full gradient.

t   t =    , no matter how small the learning rate
gets, we still can make arbitrarily large motions in parameter
space.

7

statistical intuitions for learning rates

for intuition consider the one dimensional case.

at a    xed parameter setting we can sample gradients.

averaging together n sample gradients produces a con   dence
interval on the true gradient.

g =   g    2     
n

to have the right direction of motion this interval should not
contain zero. this gives.

n     2  2
  g2

8

statistical intuitions for learning rates

n     2  2
  g2

to average n gradients we need that n gradient updates have
a limited in   uence on the gradient.

this suggests

  t     1
n

    (gt)2
(  t)2

the constant of proportionality will depend on the rate of
change of the gradient (the second derivative of loss).

9

statistical intuitions for learning rates

  t     (gt)2
(  t)2

this is written in terms of the true (average) gradient gt at
time t and the true standard deviation   t at time t.

this formulation is of conceptual interest but is not (yet) di-
rectly implementable (more later).
as gt     0 we expect   t        > 0 and hence   t     0.

10

running averages

we can try to estimate gt and   t with a running average.
it is useful to review general running averages.
consider a time series x1, x2, x3, . . ..
suppose that we want to approximate a running average

this can be done e   ciently with the update

t(cid:88)

s=t   n +1

    t +

11

    t     1
n
(cid:18)

1     1
n

(cid:19)

xs

(cid:18) 1

(cid:19)

n

    t+1 =

xt+1

running averages

more explicitly, for     0 = 0, the update

(cid:18)

(cid:19)
(cid:18)

    t +

xt+1

(cid:18) 1
(cid:19)
(cid:19)   (t   s)

n

xs

1     1
n

(cid:19)   n

= n

gives

    t+1 =

    t =

1
n

where we have (cid:88)

1     1
n

(cid:88)
(cid:18)

1   s   t

n   0

1     1
n

12

back to learning rates

in high dimension we can apply the statistical learning rate
argument to each dimension (parameter)   [c] of the parameter
vector    giving a separate learning rate for each dimension.

  t[c]     gt[c]2
  t[c]2

  t+1[c] =   t[c]       t[c]  t[c]

13

rmsprop

rms     root mean square     was introduced by hinton and
proved e   ective in practice. we start by computing a running
average of   g[c]2.

st+1[c] =   st[c] + (1       )   g[c]2

the pytorch default for    is .99 which corresponds to a run-
ning average of 100 values of   g[c]2.
if gt[c] <<   t[c] then st[c]       t[c]2.
rmsprop:

  t[c]     1/

st[c] +  

(cid:113)

14

rmsprop

rmsprop

(cid:113)

  t[c]     1/

st[c] +  

bears some similarity to

  t[c]     gt[c]2/  t[c]2

but there is no attempt to estimate gt[c].

15

momentum

rudin   s blog

the theory of momentum is generally given in terms of gra-
dient drift (the second order structure of total training loss).

i will instead analyze momentum as a running average of   g.

16

momentum, nonstandard parameterization

  gt+1 =     gt + (1       )  g

       (0, 1) typically        .9

  t+1 =   t         gt+1

for    = .9 we have that   gt approximates a running average of
10 values of   g.

17

running averages revisited

consider any sequence yt derived from xt by

yt+1 =

yt + f (xt)

for any function f

(cid:19)

(cid:18)

1     1
n

we note that any such equation de   nes a running average of
n f (xt).

(cid:18)

(cid:19)

yt+1 =

1     1
n

(cid:18) 1

(cid:19)

n

(n f (xt))

yt +

18

momentum, standard parameterization

vt+1 =   vt +          g

       (0, 1)

  t+1 =   t     vt+1

by the preceding slide vt is a running average of (  /(1       ))  g
and hence the above de   nition is equivalent to

  gt+1 =     gt + (1       )  g

(cid:18)   

1       

(cid:19)

       (0, 1)

  gt+1

  t+1 =   t    

momentum

  gt+1 =     gt + (1       )  g

(cid:18)   

(cid:19)

  t+1 =   t    
1       
  t+1 =   t       (cid:48)  gt+1

       (0, 1), typically .9

  gt+1

standard parameterization

nonstandard parameterization

the total contribution of a gradient value   gt is   /(1       ) in
the standard parameterization and is   (cid:48) in the nonstandard
parameterization (independent of   ). this suggests that the
optimal value of   (cid:48) is independent of    and that the    does
nothing.

adam     adaptive momentum

  gt+1[c] =   1  gt[c] + (1       1)  g[c]
st+1[c] =   2st[c] + (1       2)  g[c]2

pytorch default:   1 = .9

pytorch default:   2 = .999

(cid:113)

  t+1[c] -=

(1       t

lr
2)st+1[c] +  

(1       t

1)  gt+1[c]

given the argument that momentum does nothing, this should
be equivalent to rmsprop. however, implementations of rm-
sprop and adam di   er in details such as default parameter val-
ues and, perhaps most importantly, rmsprop lacks the    initial
bias correction terms    (1       t) (see the next slide).

bias correction in adam

adam takes   g0 = s0 = 0.

for   2 = .999 we have that st is very small for t << 1000.

to make st[c] a better average of gt[c]2 we replace st[c] by
(1       t

2)st[c].

2     exp(   t/1000) and for t >> 1000

for   2 = .999 we have   t
we have (1       t
similar comments apply to replacing gt[c] by (1       t

2)     1.

1)gt[c].

22

learning rate scaling

recent work has show that by scaling the learning rate with
the batch size very large batch size can lead to very fast (highly
parallel) training.

accurate, large minibatch sgd: training ima-
genet in 1 hour, goyal et al., 2017.

23

learning rate scaling

consider two consecutive updates for a batch size of 1 with
learning rate   1.
  t+1 =   t       1     loss(  t, xt, yt)
  t+2 =   t+1       1     loss(  t+1, xt+1, yt+1)

      t+1       1     loss(  t, xt+1, yt+1)
=   t       1((     loss(  t, xt, yt)) + (     loss(  t, xt+1, yt+1)))

24

learning rate scaling

let   b be the learning rate for batch size b.

  t+2       t       1((     loss(  t, xt, yt)) + (     loss(  t, xt+1, yt+1)))

=   t     2  1   g for b = 2

hence two updates with b = 1 at learning rate   1 is the same
as one update at b = 2 and learning rate 2  1.

  2 = 2  1,

  b = b  1

25

sgd as mcmc and mcmc as sgd

    gradient estimation. the accuracy of   g as an estimate
of g.
    gradient drift (second order structure). the fact
that g changes as the parameters change.
    convergence. to converge to a local optimum the learn-
ing rate must be gradually reduced toward zero.
    exploration. since deep models are non-convex we need
to search over the parameter space. sgd can behave like
mcmc.

26

learning rate as a temperature parameter

gao huang et. al., iclr 2017

27

gradient flow

total id119:    -=   g

note this is g and not   g. gradient    ow is de   ned by

=      g

d  
dt

given   (0) we can calculate   (t) by taking the limit as n    
    of n t discrete-time total updates    -=
  
n g.
the limit n         of n t batch updates    -=
  (t).

  
n   g also gives

28

gradient flow guarantees progress

d(cid:96)
dt

= (      (cid:96)(  ))    d  
dt
=    (      (cid:96)(  ))    (      (cid:96)(  ))
=    ||      (cid:96)(  )||2
    0

if (cid:96)(  )     0 then (cid:96)(  ) must converge to a limiting value.
this does not imply that    converges.

29

an original algorithm derivation

we will derive a learning rate by maximizing a lower bound
on the rate of reduction in training loss.

we must consider
    gradient estimation. the accuracy of   g as an estimate
of g.
    gradient drift (second order structure). the fact
that g changes as the parameters change.

30

analysis plan

we will calculate a batch size b    and learning rate       by op-
timizing an improvement guarantee for a single batch update.

we then use learning rate scaling to derive the learning rate

  b for a batch size b << b   .

31

deriving learning rates

if we can calculate b    and       for optimal loss reduction in a
single batch we can calculate   b.

  b = b   1
      = b     1
     
b   
b         
b

  1 =

  b =

32

calculating b    and       in one dimension

we will    rst calculate values b    and       by optimizing the loss
reduction over a single batch update in one dimension.

g =   g    2       
b

(cid:115)

     =

e(x,y)   batch

(cid:18)d loss(  , x, y)

d  

(cid:19)2

      g

33

the second derivative of loss(  )

loss(  ) = e(x,y)   train loss(  , x, y)

d2loss(  )/d  2     l (assumption)

loss(            )     loss(  )     g      +

l     2

1
2

loss(           g)     loss(  )     g(    g) +

l(    g)2

1
2

34

a progress guarantee

loss(           g)     loss(  )     g(    g) +

l(    g)2

1
2

= loss(  )       (  g     (  g     g))  g +

l  2  g2

1
2

(cid:18)

(cid:19)

    loss(  )       

  g     2       
b

  g +

1
2

l  2  g2

optimizing b and   

(cid:18)

(cid:19)

loss(           g)     loss(  )       

  g     2       
b

  g +

1
2

l  2  g2

we optimize progress per gradient calculation by optimizing
the right hand side divided by b. the derivation at the end
of the slides gives

b    =

  b =

16    2
  g2 ,
b         =
b

      =

1
2l

b   g2
32    2l

recall this is all just in one dimension.

36

estimating   gb    and     b   

  b =

b   g2
32    2l

we are left with the problem that   g and      are de   ned in terms
of batch size b    >> b.

we can estimate   gb    and     b    using a running average with a
time constant corresponding to b   .

37

estimating   gb   

(cid:88)

  gb    =

1
b   

(x,y)   batch(b   )

d loss(  , x, y)

d  

t(cid:88)

s=t   n +1

(cid:19)

1
n

(cid:18)

=

  gt+1 =

1     b
b   

  gt +

b
b      gt+1

we are still working in just one dimension.

38

  gs

with n =

b   
b

for batch size b

a complete calculation of    (in one dimension)

(cid:19)
(cid:19)

(cid:18)
(cid:18)
(cid:113)
(cid:26) k

1     b
b   (t)
1     b
b   (t)
  st     (  gt)2

  gt+1 =

  st+1 =

    t =

b   (t) =

39

  gt +

  st +

b
b   (t)
b
b   (t)

  gt+1

(  gt+1)2

for t     k
16(    t)2/((  gt)2 +  ) otherwise

a complete calculation of    (in one dimension)

(cid:40) 0

(  gt)2

32(    t)2l

  t =

for t     k
otherwise

as t         we expect   gt     0 and     t        > 0 which implies
  t     0.

40

the high dimensional case

so far we have been considering just one dimension.

we now propose treating each dimension   [c] of a high di-
mensional parameter vector    independently using the one
dimensional analysis.
we can calculate b   [c] and      [c] for each individual pa-
rameter   [c].

of course the actual batch size b will be the same for all
parameters.

41

a complete algorithm

  gt+1[c] =

  st+1[c] =

    t[c] =

b   (t)[c] =

(cid:18)
(cid:18)
(cid:113)
(cid:26) k

1     b

b   (t)[c]

1     b

b   (t)[c]
  st[c]       gt[c]2

(cid:19)
(cid:19)

42

  gt[c] +

  st[c] +

b

b   (t)[c]

b

b   (t)[c]

  gt+1[c]

  gt+1[c]2

for t     k
  b     t[c]2/(  gt[c]2 +  ) otherwise

a complete algorithm

          0

for t     k
       gt[c]2
    t[c]2 otherwise

  t[c] =

  t+1[c] =   t[c]       t[c]  gt[c]

here we have meta-parameters k,   b,   and     .

appendix: optimizing b and   

(cid:18)

(cid:19)

loss(           g)     loss(  )         g

  g     2       
b

+

1
2

l  2  g2

optimizing    we get

(cid:18)

  g

  g     2       
b

= l    g2

(cid:19)

1
l

     (b) =

   
1     2    
b
  g
inserting this into the guarantee gives
loss(           g)     loss(  )     l
2

     (b)2  g2

(cid:19)
(cid:18)

44

optimizing b

optimizing progress per sample, or maximizing      (b)2/b, we
get

(cid:18) 1   

b
b   3

2 +

(cid:19)2

    2    
  gb
b   2

2    
  g

     (b)2

b

=

1
l2
0 =    1
2

b    =
     (b   ) =       =

16    2
  g2
1
2l

45

end

