neural networks
feedforward neural network - capacity of neural network

1

)
2
x

artificial neuron

1

1
)
2
x

)
2
x

2

,

,

,

0

topics: capacity of single neuron
    can   t solve non linearly separable problems...

0

0

1

0

1

0

(x1

0

(x1

1

(x1
xor (x1, x2)

1

)
2
x

,

0

?

0

(x1

1

xor (x1, x2)

)
2
x

,
1
x
(
d
n
a

1

0

0

1

and (x1, x2)

    ... unless the input is transformed in a better representation
figure 1.8     exemple de mod  lisation de xor par un r  seau    une couche cach  e. en
haut, de gauche    droite, illustration des fonctions bool  ennes or(x1, x2), and (x1, x2)

i

i

1

i,j

i,j

b(2)

exp(2a)+1

exp(2a)+1

exp(2a)+1

exp(2a)+1

exp(2a)+1

    h(x) = g(a(x))

exp(a)+exp( a) = exp(2a) 1

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca
1+exp( a)
september 6, 2012
exp(a)+exp( a) = exp(2a) 1
abstract

exp(a)+exp( a) = exp(2a) 1
    w (1)
universit  e de sherbrooke
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = sigm(a) =
    g(a) = reclin(a) = max(0, a)
    g(a) = a
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
1
    g(a) = tanh(a) = exp(a) exp( a)
1+exp( a)
    g(a) = sigm(a) =
    g(a) = a
3
neural network
    h(x) = g(a(x))
    g(a) = max(0, a)
    g(a) = max(0, a)
    g(  ) b
september 6, 2012
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    g(a) = max(0, a)
    g(a) = sigm(a) =
1
    g(a) = max(0, a)
i pj w (1)
    g(a) = sigm(a) =
    g(a) = max(0, a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
1+exp( a)
1+exp( a)
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = reclin(a) = max(0, a)
    w (1)
xj h(x)i w(2)
b(1)
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
b(2)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
topics: single hidden layer neural network
i
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    p(y = 1|x)
    f (x) = g(out)(b(2) + w(2)>x)
    h(x) = g(a(x))
    g(a) = max(0, a)
    hidden layer pre-activation:
    g(  ) b
    g(a) = max(0, a)
xj h(x)i w(2)
    g(a) = max(0, a)
    p(y = 1|x)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
    g(  ) b
    g(  ) b
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    g(a) = reclin(a) = max(0, a)
    g(a) = max(0, a)
xj h(x)i w(2)
    w (1)
b(1)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
math for my slides    feedforward neural network   .
    g(a) = reclin(a) = max(0, a)
i +pj w (1)
xj h(x)i w(2)
b(1)
    w (1)
b(2)
    g(  ) b
b(1)
xj h(x)i w(2)
    w (1)
math for my slides    feedforward neural network   .
i
i,j
b(2)
    f (x) = o(b(2) + w(2)>x)
i
i
...
...
xj h(x)i w(2)
b(1)
    w (1)
i
b(2)
    hidden layer activation:
    a(x) = b +pi wixi = b + w>x
1
    w (1)
i
i
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    f (x) = o(b(2) + w(2)>x)
    a(x) = b +pi wixi = b + w>x
xj h(x)i
    h(x) = g(a(x))
    h(x) = g(a(x))
    g(  ) b
i,j
1
    h(x) = g(a(x))
    h(x) = g(a(x))
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
    h(x) = g(a(x))
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    h(x) = g(a(x)) = g(b +pi wixi)
i pj w (1)
    h(x) = g(a(x)) = g(b +pi wixi)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
b(1)
    w (1)
    g(  ) b
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
i +pj w (1)
i pj w (1)
    output layer activation:
    w (1)
b(1)
i pj w (1)
i
...
...
i
    f (x) = o   b(2) + w(2)>h(1)x   
1
1
    f (x) = o   b(2) + w(2)>x   
b(1)
    w (1)
xj h(x)i
    x1 xd
    x1 xd
    h(x) = g(a(x))
    o(x) = g(out)(b(2) + w(2)>x)
    h(x) = g(a(x))
    o(x) = g(out)(b(2) + w(2)>x)
i
i,j
    o(x) = g(out)(b(2) + w(2)>x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
output activation function
    w
    w
    h(x) = g(a(x))
1
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
    {

xj h(x)i

xj h(x)i

abstract

exp(2a)+1

b(2)

b(1)
i

i,j

i,j

i,j

1

i,j

i

1

i

capacity of neural network

4

topics: single hidden layer neural network
r  eseaux de neurones

2

x2

1

z

1

0

-1

-1

0

0

-1

x1

1

zk

y1

1

0

-1

-1

0

1

0

-1

x1

x2

1

biais

.5

sortie k
wkj

y2

1

-1

0

cach  ee j
wji
entr  ee i

-1

y1

.7

-.4

-1.5

y2

1 1

1

1

x1

x2

x2

1

0

1

-1

x1

-1

0

(from pascal vincent   s slides)

x2

1

z=-1

r  eseaux de neurones

capacity of neural network

    la puissance expressive des r  eseaux de neurones
topics: single hidden layer neural network

5

z1

x2

y2

z1

x1

y3

y1

y1

y2

y3

y4

y4

x1

x2

(from pascal vincent   s slides)

deux couches

capacity of neural network

r1

6

topics: single hidden layer neural network

x1

x2

r2

trois couches

x2

...

r2

x1

x2

(from pascal vincent   s slides)

x1

x1

r1

r2
r1

capacity of neural network

7

topics: universal approximation
    universal approximation theorem (hornik, 1991):

          a single hidden layer neural network with a linear output unit can approximate 

any continuous function arbitrarily well, given enough hidden units      

    the result applies for sigmoid, tanh and many other hidden 
layer id180

    this is a good result, but it doesn   t mean there is a learning 
algorithm that can    nd the necessary parameter values!

