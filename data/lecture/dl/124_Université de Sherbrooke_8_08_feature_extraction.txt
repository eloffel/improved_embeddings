neural networks
sparse coding - feature extraction

1

= u      1

= u      1

= u      1

2 u>(x(t)  b  )      u      1
2 u>(x(t)  b  )      u      1
2 u>(x(t)  b  )   >
2 u>  1
t   1xt    u      1
2 u>(x(t)  b  )      u      1
t   1xt    u      1
2 u>(x(t)  b  )      u      1
t   1xt
2 u>  1
(x(t)  b  )(x(t)  b  )!>
(x(t)  b  )(x(t)  b  )!>
2 u>  1
t   1xt
universit  e de sherbrooke
t   1xt    u      1
2 u>(x(t)  b  )   >
2 u>(x(t)  b  )      u      1
t   1xt
(x(t)  b  )(x(t)  b  )!>
2 u>  1
(x(t)  b  )(x(t)  b  )!>
2 u>  1
feature extraction
t   1xt
t   1xt
2 u> b    u     1
= u      1
= u      1
= u      1
2 u> b    u     1
(x(t)  b  )(x(t)  b  )!>
2 u>  1
2 u>
2 u> b    u     1
t   1xt
november 1, 2012
2 u>
2 u> b    u     1
= u      1
2 u>
= u      1
2 u> b    u     1
2 u> u     u> u      1
2 u>
u      1
2 u>
= i
= u      1
2 u> u     u> u      1
= u      1
2 u> u     u> u      1
2 u>
= i
= i
2 u> b    u     1
abstract

= u      1
hugo.larochelle@usherbrooke.ca
= u      1
= u      1
= u      1
topics: id171
= u      1
= i
    a sparse coding model can be used to extract features
2 u>
= i
    {(x(t), y(t))}
2 u> u     u> u      1
2 u>
    {(x(t), y(t))}
    {(x(t), y(t))}
    {x(t)}
- this yields a dictionary       from which to infer sparse codes 
    {x(t)}
    {x(t)} h(x(t))
    {(h(x(t)), y(t))} x
1
1
    {(h(x(t)), y(t))} x
    {(h(x(t)), y(t))} x
2||x(t)   d h(t)||2
arg min
arg min
t
h(t)
    when classifying test input    , must infer its sparse 
representation           rst, then feed it to the classi   er

= u      1
    given a labeled training set
= u      1
    {(x(t), y(t))}
    train sparse coding dictionary only on training inputs
= i
    {x(t)}
    x(t) h(t) d
    {(h(x(t)), y(t))} x
    {(x(t), y(t))}
    {(x(t), y(t))}
    {x(t)} h(x(t))
    {x(t)} h(x(t))
    {(h(x(t)), y(t))} x
   
    {(h(x(t)), y(t))} x h(x)

    train favorite classi   er on transformed training set

1
2||x(t)   d h(t)||2

math for my slides    sparse coding   .

2 u> u     u> u      1

2 +  ||h(t)||1

2 +  ||h(t)||1

h(x(t)) = arg min

2 u> u     u> u      1

txt=1

u      1

2 u>

d

2

h(t)

feature extraction

3

topics: id171
    when trained on handwritten digits:

self-taught learning

figure 5. left: example images from the handwritten digit
dataset (top), the handwritten character dataset (middle)

self-taught learning: id21 from unlabeled data

raina, battle, lee, packer and ng.

table 4. accuracy on 7-way music genre classi   cation.
training set size

100
1000
5000

table 5. text bases learned on 100,000 reuters newswire
documents. top: each row represents the basis most ac-
tive on average for documents with the class label at the
left. for each basis vector, the words corresponding to the

dataset (top), the handwritten character dataset (middle)
and the font character dataset (bottom). right: example
sparse coding bases learned on handwritten digits.

feature extraction

    when features trained on different input distribution

topics: self-taught learning
    self-taught learning: 

table 3. top: classi   cation accuracy on 26-way handwrit-
ten english character classi   cation, using bases trained on
handwritten digits. bottom: classi   cation accuracy on
26-way english font character classi   cation, using bases
trained on english handwritten characters. the numbers
in parentheses denote the accuracy using raw and sparse
coding features together. here, sparse coding features
alone do not perform as well as the raw features, but per-
form signi   cantly better when used in combination with
the raw features.

    train sparse coding dictionary on handwritten digits
    use codes (features) to classify handwritten characters

    example:

digits     english handwritten characters

training set size

raw

pca
39.8% 25.3%
54.8% 54.8%
61.9% 64.5%

39.7%
100
58.5%
500
65.3%
1000
handwritten characters     font characters

self-taught learning: id21 from unlabeled data

sparse coding

training set size

raina, battle, lee, packer and ng.

raw
8.2%

pca
5.7%

sparse coding
7.0% (9.2%)

100

4

left. for each basis vector, the words corresponding to the
largest magnitude elements are displayed. bottom: each
row represents the basis that contains the largest magni-
tude element for the word at the left. the words corre-
sponding to other large magnitude elements are displayed.

design
business
vaccine
movie

design, company, product, work, market
car, sale, vehicle, motor, market, import
infect, report, virus, hiv, decline, product

share, disney, abc, release, o   ce, movie, pay

figure 5 shows example inputs from the three char-
acter datasets, and some of the learned bases. the
learned bases appear to represent    pen strokes.    in
table 4,
it is thus not surprising that sparse cod-
ing is able to use bases (   strokes   ) learned on dig-
its to signi   cantly improve performance on handwrit-
ten characters   it allows the supervised learning algo-
rithm to    see    the characters as comprising strokes,
rather than as comprising pixels.

