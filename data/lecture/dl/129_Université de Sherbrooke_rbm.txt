restricted id82s
ift 725 - r  seaux neuronaux

unsupervised learning

topics: unsupervised learning
    unsupervised learning: only use the inputs       for learning

    x(t)   log p(x(t))

    automatically extract meaningful features for your data
    leverage the availability of unlabeled data
    add a data-dependent regularizer to training (                   )
    x(t)   log p(x(t))

math for my slides    restricted id82s   .

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

math for my slides    restricted id82s   .

hugo.larochelle@usherbrooke.ca

october 10, 2012

    we will see 3 neural networks for unsupervised learning

    restricted id82s
    autoencoders
    sparse coding model

2

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

restricted id82
topics: rbm, visible layer, hidden layer, energy function

october 10, 2012

math for my slides    restricted id82s   .

    x(t)   log p(x(t))

energy function:

bj

bias

h
abstract
w connections

hidden layer
(binary units)

ck

x

visible layer
(binary units)

e(x, h) =  h>wx   c>x   b>h
wj,khjxk  xk

=  xj xk

ckxk  xj

bjhj

distribution: p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

= exp(h>wx + c>x + b>h)/z

partition function

(intractable)

3

    x(t)   log p(x(t))

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

   

{

 

,

u

|

x
u

=
 

u

e
t

u

>i

u

    h1 h2 hh 1 hh

=
1

}

topics: markov network (with vector nodes)

markov network view
ckxk  xj

math for my slides    restricted id82s   .
e(x, h) =  h>wx   c>x   b>h
wj,khjxk  xk
bjhj
abstract

=  xj xk

2

   

{

x

r

october 10, 2012

    x(t)   log p(x(t))
    h x

|

math for my slides    restricted id82s   .

x

p(x, h) = exp( e(x, h))/z

/2
= exp(h>wx + c>x + b>h)/z
r
= exp(h>wx) exp(c>x) exp(b>h)/z
x
}

p(x, h) = exp( e(x, h))/z

(

)

h

    x(t)   log p(x(t))
    h x

factors

= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

p(x, h) = exp( e(x, h))/z
    the notation based on an energy function is simply an 
alternative to the representation as the product of factors
exp(wj,khjxk)

p(x, h) =

= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

4

1

zyj yk
yk

p

6
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

abstract
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
math for my slides    restricted id82s   .
    h x

    x(t)   log p(x(t))
= exp(h>wx + c>x + b>h)/z
markov network view
    h x
= exp(h>wx) exp(c>x) exp(b>h)/z
    x(t)   log p(x(t))
    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
    h1 h2 hh 1 hh
= exp(h>wx + b>h + c>x)/z
    h x
    h x
= exp(h>wx) exp(b>h) exp(c>x)/z
    x1 x2 xd

topics: markov network (with scalar nodes)

abstract

abstract

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z
exp(wj,khjxk)

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
p(x, h) = exp( e(x, h))/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
exp(ckxk)

1

p(x, h) =

    x(t)   log p(x(t))
    h x
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
...
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

zyj yk
yk
yj

exp(bjhj)

    the scalar visualization is more informative of the structure 
within the vectors

5

p

h

}

math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

abstract
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
math for my slides    restricted id82s   .
    h x

    x(t)   log p(x(t))
= exp(h>wx + c>x + b>h)/z
markov network view
    h x
= exp(h>wx) exp(c>x) exp(b>h)/z
    x(t)   log p(x(t))
    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
    h1 h2 hh 1 hh
= exp(h>wx + b>h + c>x)/z
    h x
    h x
= exp(h>wx) exp(b>h) exp(c>x)/z
    x1 x2 xd

topics: markov network (with scalar nodes)

x
w

abstract

abstract

j
,

  

j

p

j

|

w
9

=
x

    x(t)   log p(x(t))
    h x
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
...
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

2
x
{
=

x
r

   

(

)

r

p(x, h) =

p(x, h) = exp( e(x, h))/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
pair-wise factors
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
}
1
= exp(h>wx + b>h + c>x)/z
x
exp(wj,khjxk)
r
= exp(h>wx) exp(b>h) exp(c>x)/z
/2
x

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
exp(ckxk)

|

(

)

h

r

2
x
{

exp(bjhj)

   

zyj yk
yk
yj

    the scalar visualization is more informative of the structure 
within the vectors

5

p

h

}

math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

abstract
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
math for my slides    restricted id82s   .
    h x

    x(t)   log p(x(t))
= exp(h>wx + c>x + b>h)/z
markov network view
    h x
= exp(h>wx) exp(c>x) exp(b>h)/z
    x(t)   log p(x(t))
    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
    h1 h2 hh 1 hh
= exp(h>wx + b>h + c>x)/z
    h x
    h x
= exp(h>wx) exp(b>h) exp(c>x)/z
    x1 x2 xd

topics: markov network (with scalar nodes)

x
w

abstract

j
,

  

j

p

j

|

w
9

=
x

    x(t)   log p(x(t))
    h x
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
...
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

2
x
{
=

x
r

   

(

)

r

abstract

   9w,t    x(t   )=pt6=t   wtx(t)
   r(x)={x2rh|9w x=pjwjx  ,j}
   {x2rh|x/2r(x)}
   { i,ui|xui= iuietu>iuj=1i=j}

zyj yk
yk
yj

2
x
{

r

   

exp(bjhj)

unary
factors

(

h

5

p(x, h) = exp( e(x, h))/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
pair-wise factors
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
}
1
= exp(h>wx + b>h + c>x)/z
x
exp(wj,khjxk)
r
= exp(h>wx) exp(b>h) exp(c>x)/z
/2
x

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
exp(ckxk)

|

)

p(x, h) =

    the scalar visualization is more informative of the structure 
within the vectors

math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

    x(t)   log p(x(t))
    h x

factor graph view

    x(t)   log p(x(t))
    h x
    x(t)   log p(x(t))
    h x

    x(t)   log p(x(t))
math for my slides    restricted id82s   .
    h x
topics: factor graph of an rbm
    x(t)   log p(x(t))
    h x
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
...
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
p(x, h) = exp( e(x, h))/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

6

id136

topics: conditional distributions
p(h|x) = j
p(hj = 1|x) =

h

p(hj|x)
1 + exp( (bj + wj  x))

1

= sigm(bj + wj  x)

x

h

x

p(x|h) = k
p(xk = 1|h) =

j th row of w

p(xk|h)

1

1 + exp( (ck + h   w  k))

= sigm(ck + h   w  k)

k th column of w

7

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

    ne(zi) zi xk hj

p(h|x) = p(x, h)/xh0

p(x, h0)

exp(h>wx + c>x + b>h)/z

=

=

=

=

exp(pj hjwj  x + bjhj)
qj exp(hjwj  x + bjhj)

ph02{0,1}h exp(h0>wx + c>x + b>h0)/z
ph012{0,1}       ph0h2{0,1} exp(pj h0jwj  x + bjh0j)
ph012{0,1}       ph0h2{0,1}qj exp(h0jwj  x + bjh0j)
qj exp(hjwj  x + bjhj)
   ph012{0,1} exp(h01w1  x + b1h01)    . . .   ph0h2{0,1} exp(h0hwh  x + bhh0h)   
qj   ph0j2{0,1} exp(h0jwj  x + bjh0j)   
= qj exp(hjwj  x + bjhj)
qj (1 + exp(bj + wj  x))
= yj
= yj

exp(hjwj  x + bjhj)
1 + exp(bj + wj  x)
p(hj|x)

qj exp(hjwj  x + bjhj)

=

8

p(hj = 1|x) =

=

exp(bj + wj  x)

1 + exp(bj + wj  x)

1

1 + exp( bj   wj  x)

= sigm(bj + wj  x)

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

(23)

(24)

(25)

 

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

9

p(hj = 1|x) =

=

exp(bj + wj  x)

1 + exp(bj + wj  x)

1

1 + exp( bj   wj  x)

= sigm(bj + wj  x)

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

(23)

(24)

(25)

 

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

9

p(hj = 1|x) =

=

exp(bj + wj  x)

1 + exp(bj + wj  x)

1

1 + exp( bj   wj  x)

= sigm(bj + wj  x)

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

(23)

(24)

(25)

 

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

9

p(hj = 1|x) =

=

exp(bj + wj  x)

1 + exp(bj + wj  x)

1

1 + exp( bj   wj  x)

= sigm(bj + wj  x)

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

(23)

(24)

(25)

 

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

9

= exp(h>wx) exp(c>x) exp(b>h)/z

p(x, h) = exp( e(x, h))/z
exp(ckxk)

= exp(h>wx + c>x + b>h)/z
= exp(h>wx) exp(c>x) exp(b>h)/z

p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z
p(x, h) = exp( e(x, h))/z

= exp(h>wx + c>x + b>h)/z
= exp(h>wx) exp(c>x) exp(b>h)/z

    h1 h2 hh 1 hh
    x1 x2 xd

exp(bjhj)

zyj yk
yk
local markov property
yj
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
topics: local markov property
    h1 h2 hh 1 hh
    x1 x2 xd
zyj yk
    x1 x2 xd
    x1 x2 xd
    in general, we have the following property:
    x1 x2 xd
zyj yk
yk
yk
yj
yj

pz0i
q f involving zi
pz0iq f involving zi
    ne(zi) zi xk hj
    ne(zi) zi xk hj
    ne(zi) zi
1

          is any variable in the markov network  (      or      in an rbm)
                 are the neighbors of      in the markov network
    ne(zi) zi xk hj

p(zi|z1, . . . , zv ) = p(zi|ne(zi))

 f (z0i, ne(zi))

p(x, h) =

p(z0i, ne(zi))

 f (zi, ne(zi))

p(zi, ne(zi))

p(x, h) =

and any ne(zi)

and any ne(zi)

=

=

1

1

    ne(zi) zi

exp(wj,khjxk)
p(x, h) =

exp(wj,khjxk)

exp(ckxk)

exp(ckxk)
exp(bjhj)

exp(bjhj)

p(zi|z1, . . . , zv ) = p(zi|ne(zi))
p(zi|z1, . . . , zv ) = p(zi|ne(zi))

p(zi|z1, . . . , zv ) = p(zi|ne(zi))
p(zi|z1, . . . , zv ) = p(zi|ne(zi))
p(zi|z1, . . . , zv ) = p(zi|ne(zi))
p(zi, ne(zi))

10

= yj
= yj

1 + exp(bj + wj  x)
p(hj|x)

restricted id82
topics: free energy
    what about        ?
    p(x)

1 + exp( bj   wj  x)

1 + exp(bj + wj  x)

p(hj = 1|x) =

exp(bj + wj  x)

= sigm(bj + wj  x)

=

1

=

+

+

+

+

+

+

h

x

p(x, h) = xh2{0,1}h
hxj=1

= exp( f (x))/z

p(x) = xh2{0,1}h
= exp0@c>x +
p(x) = exp0@c>x +
= exp0@c>x +

free energy

hxj=1
hxj=1

exp( e(x, h))/z

log(1 + exp(bj + wj  x))1a /z
log(1 + exp(bj + wj  x))1a /z
softplus(bj + wj  x)1a /z

11

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x) = xh2{0,1}h

exp(h>wx + c>x + b>h)/z

= exp(c>x) xh12{0,1}
= exp(c>x)0@ xh12{0,1}
= exp0@c>x +
hxj=1

exp0@xj

hjwj  x + bjhj1a /z
       xhh2{0,1}
exp(h1w1  x + b1h1)1a . . .0@ xhh2{0,1}
log(1 + exp(bj + wj  x))1a /z

= exp(c>x) (1 + exp(b1 + w1  x)) . . . (1 + exp(bh + wh  x)) /z
= exp(c>x) exp(log(1 + exp(b1 + w1  x))) . . . exp(log(1 + exp(bh + wh  x)))/z

exp(hhwh  x + bhhh)1a /z

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

@   

 

12

p(x, h) = xh2{0,1}h
hxj=1

p(x) = xh2{0,1}h
= exp0@c>x +
p(x) = exp0@c>x +
= exp0@c>x +

log(1 + exp(bj + wj  x))1a /z
restricted id82
topics: free energy
log(1 + exp(bj + wj  x))1a /z
softplus(bj + wj  x)1a /z

= exp( f (x))/z

hxj=1
hxj=1

h

x

+

+

+

+

+

+

5

4

3

2

1

0
!5

softplus(  )

2

!4

!3

!2

!1

0

1

2

3

4

5

13

p(x, h) = xh2{0,1}h
hxj=1

p(x) = xh2{0,1}h
= exp0@c>x +
p(x) = exp0@c>x +
= exp0@c>x +

log(1 + exp(bj + wj  x))1a /z
restricted id82
topics: free energy
log(1 + exp(bj + wj  x))1a /z
softplus(bj + wj  x)1a /z

= exp( f (x))/z

hxj=1
hxj=1

h

x

+

+

+

+

+

+

5

4

3

2

1

0
!5

softplus(  )

bias the prob of each xi
2

!4

!3

!2

!1

0

1

2

3

4

5

13

p(x, h) = xh2{0,1}h
hxj=1

p(x) = xh2{0,1}h
= exp0@c>x +
p(x) = exp0@c>x +
= exp0@c>x +

log(1 + exp(bj + wj  x))1a /z
restricted id82
topics: free energy
log(1 + exp(bj + wj  x))1a /z
softplus(bj + wj  x)1a /z

= exp( f (x))/z

hxj=1
hxj=1

h

x

+

+

+

+

+

+

   feature    expected in x

softplus(  )

bias the prob of each xi
2

!4

!3

!2

!1

0

1

2

3

4

5

13

5

4

3

2

1

0
!5

p(x, h) = xh2{0,1}h
hxj=1

p(x) = xh2{0,1}h
= exp0@c>x +
p(x) = exp0@c>x +
= exp0@c>x +

log(1 + exp(bj + wj  x))1a /z
restricted id82
topics: free energy
log(1 + exp(bj + wj  x))1a /z
softplus(bj + wj  x)1a /z

= exp( f (x))/z

hxj=1
hxj=1

h

x

+

+

+

+

+

+

softplus(  )

   feature    expected in x
bias of each feature
bias the prob of each xi
2

!4

!3

!2

!1

0

1

2

3

4

5

13

5

4

3

2

1

0
!5

training

topics: training objective
    to train an rbm, we   d like to minimize the average negative
   log-likelihood (nll)

1

1

1

l(f (x(t))) =

l(f (x(t))) =
1

t xt   log p(x(t))
t xt   log p(x(t))
    we   d like to proceed by stochastic id119
   x(t)    ex,h    @e(x, h)
@   log p(x(t))

t xt
t xt
= eh    @e(x(t), h)

positive phase

negative phase

@   

@   

@   

 

{{

14

training

topics: training objective
    to train an rbm, we   d like to minimize the average negative
   log-likelihood (nll)

1

1

1

l(f (x(t))) =

l(f (x(t))) =
1

t xt   log p(x(t))
t xt   log p(x(t))
    we   d like to proceed by stochastic id119
   x(t)    ex,h    @e(x, h)
@   log p(x(t))

t xt
t xt
= eh    @e(x(t), h)

positive phase

negative phase

@   

@   

@   

 

hard to
compute

{{

14

    p(x)
   

contrastive divergence (cd)
topics: contrastive divergence, negative sample

(hinton, neural computation, 2002)

1

l(f (x(t))) =

    idea: 
    p(x)
   

   
t xt
1. replace the expectation by a point estimate at
  x,   h
2. obtain the point           by id150
= eh    @e(x(t), h)
...
@   log p(x(t))
3. start sampling chain at 
    x(t)

@   

@   

@   log p(x(t))

1

1

@   

l(f (x(t))) =

t xt   log p(x(t))
  x,   h
   x(t)    ex,h    @e(x, h)
t xt
= eh    @e(x(t), h)

hk =   h

 

@   

@   

@   

@   log p(x(t))

    x(t)   h(t) = h0
   
  p(h|x)

  p(x|h)

    x(t)

x1

xk =   x

negative sample

15

1

t xt   log p(x(t))

@   log p(x(t))

1

1

@   

   
t xt
l(f (x(t))) =
contrastive divergence (cd)
t xt
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   
topics: contrastive divergence
= eh    @e(x(t), h)

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

   x(t)    ex,h    @e(x, h)

   x(t)     
@e(x(t),   h(t))
@   log p(x(t))
     
ex,h    @e(x, h)

(hinton, neural computation, 2002)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

@   

   

@   

@   

@   

@   

@   

   

@   

@   

@   

1

@   

   x(t)     
     
ex,h    @e(x, h)
@   
e(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

   

    (x(t),   h(t))

(  x,   h)

eh    @e(x(t), h)

ex,h    @e(x, h)

16

@   log p(x(t))

1

1

@   

   
t xt
l(f (x(t))) =
contrastive divergence (cd)
t xt
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   
topics: contrastive divergence
= eh    @e(x(t), h)

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

   x(t)    ex,h    @e(x, h)

   x(t)     
@e(x(t),   h(t))
@   log p(x(t))
     
ex,h    @e(x, h)

(hinton, neural computation, 2002)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

@   

   

@   

@   

@   

@   

@   

   

@   

@   

@   

@   

1

   x(t)     
     
ex,h    @e(x, h)

@   

p(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

   

    (x(t),   h(t))

(  x,   h)

eh    @e(x(t), h)

ex,h    @e(x, h)

17

@   log p(x(t))

1

1

@   

   
t xt
l(f (x(t))) =
contrastive divergence (cd)
t xt
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   
topics: contrastive divergence
= eh    @e(x(t), h)

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

   x(t)    ex,h    @e(x, h)

   x(t)     
@e(x(t),   h(t))
@   log p(x(t))
     
ex,h    @e(x, h)

(hinton, neural computation, 2002)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

@   

   

@   

@   

@   

@   

@   

   

@   

@   

@   

@   

1

   x(t)     
     
ex,h    @e(x, h)

@   

p(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

   

    (x(t),   h(t))

(  x,   h)

eh    @e(x(t), h)

ex,h    @e(x, h)

17

@   log p(x(t))

1

1

@   

   
t xt
l(f (x(t))) =
contrastive divergence (cd)
t xt
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   
topics: contrastive divergence
= eh    @e(x(t), h)

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

   x(t)    ex,h    @e(x, h)

   x(t)     
@e(x(t),   h(t))
@   log p(x(t))
     
ex,h    @e(x, h)

(hinton, neural computation, 2002)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

@   

   

@   

@   

@   

@   

@   

   

@   

@   

@   

@   

1

   x(t)     
     
ex,h    @e(x, h)

@   

p(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

   

    (x(t),   h(t))

(  x,   h)

eh    @e(x(t), h)

ex,h    @e(x, h)

17

contrastive divergence (cd)
topics: contrastive divergence

(hinton, neural computation, 2002)

    cd-k:  contrastive divergence with k 
           iterations of id150
    in general, the bigger k is, the less biased the 
estimate of the gradient will be
    in practice, k=1 works well for pre-training
    we can actually ignore the samples of the hidden 
layer    and integrate them out, conditioned on a 
h
value of 

x

18

@   log p(x(t))

derivation of the learning rule
topics: contrastive divergence
    x(t)   h(t) = h0
    derivation of             for
   e(x, h)

  = wjk

   e(x, h)

@   

    

   wjk

=    

   wjk        jk
 wjk jk

 

=  

wjkhjxk     k

wjkhjxk

@   

t xt
t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)
= eh    @e(x(t), h)
bjhj      
ckxk     j
   x(t)     
     
ex,h    @e(x, h)

eh    @e(x(t), h)

@e(x(t),   h(t))

@e(  x,   h)

@   

@   

@   

=  hjxk

    (x(t),   h(t))
    rwe(x, h) =  h x>

19

@   log p(x(t))

@   

@   

 

   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
derivation of the learning rule
eh    @e(x(t), h)
   x(t)     
topics: contrastive divergence
@e(x(t),   h(t))
eh       e(x, h)
    derivation of                       for
@   
ex,h    @e(x, h)
     
   x    = eh    hjxk   x    =    hj {0,1}
eh       e(x, h)

 hjxkp(hj|x)

   x   

  = wjk

@e(  x,   h)

@   
   wjk

@   

@   

    

=  xkp(hj = 1|x)
=   p(h1=1|x)
p(hh =1|x)   

h(x)def

...

= sigm(b + wx)

20

eh [rwe(x, h)|x ] =  h(x) x>
   

w (= w          

@ log p(x(t))

    rwe(x, h) =  h x>

ex,h    @e(x, h)

@   

@e(  x,   h)

     
@   log p(x(t))

@   

t xt
= eh    @e(x(t), h)

   

  x

@   

derivation of the learning rule
topics: contrastive divergence
    given        and     the learning rule for             becomes

  = w
eh [rwe(x, h)|x ] =  h(x) x>

    x(t)   h(t) = h0
   
eh    @e(x(t), h)
w (= w         rw   log p(x(t))   
(= w         ehhrwe(x(t), h)   x(t)i   ex,h [rwe(x, h)]   
(= w         ehhrwe(x(t), h)   x(t)i   eh [rwe(  x, h)|  x ]   
   
(= w +       h(x(t)) x(t)>   h(  x)   x>   
    (x(t),   h(t))

@   

ex,h    @e(x, h)

@   

21

   

eh [rwe(x, h)|x ] =  h(x) x>

cd-k: pseudocode
w (= w         rw   log p(x(t))   
   
(= w         ehhrwe(x(t), h)   x(t)i   ex,h [rwe(x, h)]   
topics: contrastive divergence
(= w         ehhrwe(x(t), h)   x(t)i   eh [rwe(  x, h)|  x ]   
1. for each training example
    x(t)   h(t) = h0
(= w +       h(x(t)) x(t)>   h(  x)   x>   
  x
   

i. generate a negative sample     using

k steps of id150

ii. update parameters

w (= w +       h(x(t)) x(t)>   h(  x)   x>   
b (= b +       h(x(t))   h(  x)   
   
c (= c +       x(t)     x   
2. go back to 1 until stopping criteria

    (x(t),   h(t))

@   log p(x(t))

22

persistent cd (pcd)

(tieleman, icml2008)
topics: persistent contrastive divergence

   

   

    x(t)   h(t) = h0
   
  p(h|x)

1

1

l(f (x(t))) =

@   log p(x(t))

t xt
= eh    @e(x(t), h)
...

t xt   log p(x(t))
    idea: instead of initializing the chain to       , initialize 
    x(t)   h(t) = h0
    p(x)
the chain to the negative sample of the last iteration
  x
   
   
   x(t)    ex,h    @e(x, h)
t xt
= eh    @e(x(t), h)

@   log p(x(t))

@   
hk =   h

l(f (x(t))) =

  p(x|h)

 

@   

@   

   

@   

@   

1

1

x1

    x(t)   h(t) = h0
   

xk =   x

negative sample

    (x(t),   h(t))
eh    @e(x(t), h)

23

   x(t)     

t xt   log p(x(t))

   

   

   

persistent cd (pcd)

(tieleman, icml2008)
topics: persistent contrastive divergence

1

1

l(f (x(t))) =

t xt   log p(x(t))
    idea: instead of initializing the chain to       , initialize 
    x(t)   h(t) = h0
the chain to the negative sample of the last iteration
  x
   
   x(t)    ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)
...

@   log p(x(t))

@   

@   

 

@   
hk =   h

    x(t)   h(t) = h0

  p(h|x)

  p(x|h)

x1

xk =   x

    (x(t),   h(t))

negative sample

23

persistent cd (pcd)

(tieleman, icml2008)
topics: persistent contrastive divergence

1

1

l(f (x(t))) =

t xt   log p(x(t))
    idea: instead of initializing the chain to       , initialize 
    x(t)   h(t) = h0
the chain to the negative sample of the last iteration
  x
   
   x(t)    ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)
...

@   log p(x(t))

@   

@   

 

@   
hk =   h

   

   

   

    x(t)   h(t) = h0

  p(h|x)

  p(x|h)

  x

comes from the
previous iteration

x1

xk =   x

    (x(t),   h(t))

negative sample

23

(= w         ehhrwe(x(t), h)   x(t)i   ex,h [rwe(x, h)]   
(= w         ehhrwe(x(t), h)   x(t)i   eh [rwe(  x, h)|  x ]   
(= w +       h(x(t)) x(t)>   h(  x)   x>   

w (= w +       h(x(t)) x(t)>   h(  x)   x>   
b (= b +       h(x(t))   h(  x)   
c (= c +       x(t)     x   

debugging

topics: stochastic reconstruction,    lters
    unfortunately, we can   t debug with a comparison with    nite 
difference
    we instead rely on approximate       tricks      

    we plot the average stochastic reconstruction                  and see if it tends to 

    ||x(t)     x||2

decrease:

    for inputs that correspond to image, we visualize the connection coming into each 

hidden unit as if it was an image
- gives an idea of the type of visual feature each hidden unit detects

    we can also try to approximate the partition function z and see whether the 

(approximated) nll decreases
- on the quantitative analysis of id50. 

ruslan salakhutdinov and iain murray, 2008

24

example of data set: mnist

larochelle, bengio, louradour and lamblin

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

figure 5: samples from the mnist digit recognition data set. here, a black pixel corresponds to
an input value of 0 and a white pixel corresponds to 1 (the inputs are scaled between 0

25

larochelle, bengio, louradour and lamblin

filters

(larochelle et al., jmlr2009)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

26

p(x, h) = exp( e(x, h))/z

    h1 h2 hh 1 hh
    x1 x2 xd
x

= exp(h>wx + c>x + b>h)/z
= exp(h>wx) exp(c>x) exp(b>h)/z

(= w         ehhrwe(x(t), h)   x(t)i   eh [rwe(  x, h)|  x ]   
w (= w +       h(x(t)) x(t)>   h(  x)   x>   
(= w +       h(x(t)) x(t)>   h(  x)   x>   
b (= b +       h(x(t))   h(  x)   
gaussian-bernoulli rbm
c (= c +       x(t)     x   
w (= w +       h(x(t)) x(t)>   h(  x)   x>   
b (= b +       h(x(t))   h(  x)   
topics: gaussian-bernoulli rbm
c (= c +       x(t)     x   
zyj yk
    inputs     are unbounded reals
yk
2 x>x
yj

    ||x(t)     x||2
    e(x, h) =  h>wx   c>x   b>h + 1
    ||x(t)     x||2
    only thing that changes is that            is now a gaussian distribution 
    e(x, h) =  h>wx   c>x   b>h + 1
p(x|h)
    p(x|h)    = c + wx
with mean                     and identity covariance matrix
    p(x|h)    = c + w>h
    recommended to normalize the training set by
- subtracting the mean of each input
- dividing each input       by the training set standard deviation

    add a quadratic term to the energy function

exp(wj,khjxk)

p(x, h) =

exp(ckxk)

exp(bjhj)

2 x>x

1

    ne(zi) zi xk hj

    should use a smaller learning rate than in the regular rbm
    designing rbms for different types of data is a       hot       topic

p(zi|z1, . . . , zv ) = p(zi|ne(zi))

2

=

p(zi, ne(zi))

27

2
p(z0i, ne(zi))

pz0i

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

filters

(larochelle et al., jmlr2009)

0.3

0.4

0.5

0.6

0.7

0.1

0.2

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

figure 12: input weights of a random subset of the hidden units, learned by an rbm with gaussian

28

b (= b +       h(x(t))   h(  x)   
c (= c +       x(t)     x   
id82

abstract
math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

abstract
october 10, 2012

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca
abstract
october 10, 2012

october 10, 2012

abstract

abstract

math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

2 x>x
p(x, h) = exp( e(x, h))/z

abstract
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    x(t)   log p(x(t))
    ||x(t)     x||2
topics: id82
math for my slides    restricted id82s   .
    h x
    h x
    e(x, h) =  h>wx   c>x   b>h + 1
    the original id82 has lateral connections in 
    x(t)   log p(x(t))
    x(t)   log p(x(t))
    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
    p(x|h)    = c + w>h
each layer
= exp(h>wx + b>h + c>x)/z
    h x
    h x
    h x
= exp(h>wx) exp(b>h) exp(c>x)/z
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
p(x, h) = exp( e(x, h))/z
e(x, h) =  h>wx   c>x   b>h
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
x>vx  
 

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

h>uh

1
2

1
2

2
    when only one layer has lateral connection, it   s a 
semi-restricted boltmann machine

29

b (= b +       h(x(t))   h(  x)   
c (= c +       x(t)     x   
id82

abstract
math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

abstract
october 10, 2012

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca
abstract
october 10, 2012

october 10, 2012

abstract

abstract

math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

2 x>x
p(x, h) = exp( e(x, h))/z

abstract
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    x(t)   log p(x(t))
    ||x(t)     x||2
topics: id82
math for my slides    restricted id82s   .
    h x
    h x
    e(x, h) =  h>wx   c>x   b>h + 1
    the original id82 has lateral connections in 
    x(t)   log p(x(t))
    x(t)   log p(x(t))
    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
    p(x|h)    = c + w>h
each layer
= exp(h>wx + b>h + c>x)/z
    h x
    h x
    h x
= exp(h>wx) exp(b>h) exp(c>x)/z
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
p(x, h) = exp( e(x, h))/z
e(x, h) =  h>wx   c>x   b>h
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
x>vx  
 

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

h>uh

1
2

1
2

2
    when only one layer has lateral connection, it   s a 
semi-restricted boltmann machine

29

b (= b +       h(x(t))   h(  x)   
c (= c +       x(t)     x   
id82

abstract
math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

math for my slides    restricted id82s   .

abstract
october 10, 2012

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca
abstract
october 10, 2012

october 10, 2012

abstract

abstract

math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    h x

2 x>x
p(x, h) = exp( e(x, h))/z

abstract
math for my slides    restricted id82s   .

    x(t)   log p(x(t))
    x(t)   log p(x(t))
    ||x(t)     x||2
topics: id82
math for my slides    restricted id82s   .
    h x
    h x
    e(x, h) =  h>wx   c>x   b>h + 1
    the original id82 has lateral connections in 
    x(t)   log p(x(t))
    x(t)   log p(x(t))
    x(t)   log p(x(t))
p(x, h) = exp( e(x, h))/z
    p(x|h)    = c + w>h
each layer
= exp(h>wx + b>h + c>x)/z
    h x
    h x
    h x
= exp(h>wx) exp(b>h) exp(c>x)/z
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd
...
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    h1 h2 hh 1 hh
    x1 x2 xd
    x1 x2 xd
    x1 x2 xd

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
p(x, h) = exp( e(x, h))/z
e(x, h) =  h>wx   c>x   b>h
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z

= exp(h>wx + b>h + c>x)/z
= exp(h>wx + b>h + c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
= exp(h>wx) exp(b>h) exp(c>x)/z
x>vx  
 

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

h>uh

1
2

1
2

2
    when only one layer has lateral connection, it   s a 
semi-restricted boltmann machine

29

conclusion

    we saw what is a restricted id82 and how to 
train it with contrastive divergence
    it assumes the observations are binary (bernoulli), but 
variations support other types:
    real-valued: gaussian-bernoulli rbm
    binomial observations:

- rate-coded restricted id82s for face recognition.

yee whye teh and geoffrey hinton, 2001

    multinomial observations:

- replicated softmax: an undirected topic model. 
ruslan salakhutdinov and geoffrey hinton, 2009

- training restricted id82s on word observations.

george dahl, ryan adam and hugo larochelle, 2012

    and more (see course website)

30

