ttic 31230, fundamentals of deep learning

david mcallester, april 2017

alphazero

1

alphago fan (october 2015)

alphago defeats fan hui, european go champion.

2

alphago lee (march 2016)

3

alphago zero vs. alphago lee (april 2017)

alphago lee:
    trained on both human games and self play.
    trained for months.
    run on many machines with 48 tpus for lee sedol match.
alphago zero:
    trained on self play only.
    trained for 3 days.
    run on one machine with 4 tpus.
    defeated alphago lee under match conditions 100 to 0.

4

alphazero defeats stock   sh in chess (december 2017)

alphago zero was a fundamental algorithmic advance for gen-
eral rl.

the general rl algorithm of alphazero is essentially the same
as that of alphago zero.

5

some algorithmic concepts

rollout position evaluation (bruegmann, 1993)

id169 (mcts) (bruegmann, 1993)

upper con   dence bound (ucb) bandit algorithm (lai and
robbins 1985)

upper con   dence tree search (uct) (kocsis and szepesvari,
2006)

6

rollouts and mcts (1993)

to estimate the value of a position (who is ahead and by how
much) run a cheap stochastic policy to generate a sequence of
moves (a rollout) and see who wins.

take an average value of many rollouts.

do a selective tree search using rollout averages for position
evaluation.

7

(one armed) bandit problems

consider a set of choices (di   erent slot machines).
each choice gets a stochastic reward.

we can select a choice and get a reward as often as we like.

we would like to determine which choice is best and also to
get reward as quickly as possible.

8

the ucb algorithm (1995 version)

for each choice (bandit) a construct a con   dence interval for
its average reward.

always select

   
   =         2  /

n

  (a)         (a) + u (n (a))

argmax

a

    (a) + u (n (a))

9

the uct algorithm (2006)

build a search tree by running    simulations   .

each simulation uses the ucb rule to select a child of each
node until a leaf is reached.

the leaf is then expanded and a value is computed for the leaf.

this value is    backed up    through the tree adding a value and
increment the count of each ancestor node.

10

alphago

alphago trained:
    a fast rollout policy.
    an imitation policy network.
    a self-play policy network.
    a value network trained to predict self-play rollout values.

11

alphago

competition play is done using utc search using the four
components just mentioned.

no tree search is used in training.

12

alphago policy and value networks

the layers use 5    5    lters with relu on 256 channels

[silver et al.]

13

fast rollout policy

softmax of linear combination of (hand designed) pattern fea-
tures.

an accuracy of 24.2%, using just 2  s to select an action, rather
than 3ms for the policy network.

14

imitation policy learning

a 13-layer policy network trained from from 30 million posi-
tions from the kgs go server.

15

self-play policy

run the policy network against version of itself to get an (ex-
pensive) rollout a1, b1, a2, b2, . . . , an , bn with value z.

no tree search is used here.

     += z         ln   (at|st;     )

this is just reinforce.

16

regression training of value function

using self-play of the    nal rl policy we generate a database
of 30 million pairs (s, z) where s is a board position and z    
{   1, 1} is an outcome and each pair is from a di   erent game.

we then train a value network by regression.

      = argmin

  

e(s,z) (v (s,   )     z)2

17

id169 (mcts)

competition play is then done with uct search using the four
predictors described above.

a simulation descends the tree using

argmax

a

q(s, a) + cp (s, a)

(cid:112)n (s)

1 + n (a)

where p (s, a) is the imitation learned action id203.

18

id169 (mcts)

when a leaf is expanded it is assigned value

(1       )v (s) +   z

where v (s) is from the the self-play learned value network and
z is value of a rollout from s using the fast rollout policy.

once the search is deemed complete, the most traversed edge
from the root is selected as the move.

19

alphago zero

    the self-play training is based on uct tree search rather
than rollouts.
    no rollouts are ever used     just uct trees under the learned
policy and value networks.
    no database of human games is ever used, just self-play.
    the networks are replaced with resnet.
    a single dual-head network is used for both policy and value.

20

training time

4.9 million games of self-play

0.4s thinking time per move

about 8 years of thinking time in training.

training took just under 3 days     about 1000 fold parallelism.

21

elo learning curve

22

learning curve for predicting human moves

23

ablation study for resnet and dual-head

24

learning from tree search

utc tree search is used to generate a complete self-play game.

each self-play game has a    nal outcome z and generates data
(s,   , z) for each position s in the game where    is the    nal
move id203 of that position and z is the    nal value of the
game.

this data is collected in a replay bu   er.

25

learning from tree search

learning is done from this replay bu   er using the following
objective on a single dual-head network.

                  

(v  (s)     z)2
     1 log q  (a|s)
+  2||  ||2

                  

      = argmin

  

e(s,  ,z)   replay, a     

26

exploration

exploration is maintained by selecting moves in proportion to
visit count for the    rst 30 moves rather than the maximum
visit count.

after 30 moves the max count is selected.

throughout the game noise is injected into the root move prob-
abilities for each move selection.

27

increasing blocks and training

increasing the number of resnet blocks form 20 to 40.

increasing the number of training days from 3 to 40.

gives an elo rating over 5000.

28

final elo ratings

29

alphazero     chess and shogi

essentially the same algorithm with the input image and out-
put images modi   ed to represent to game position and move
options respective.

minimal representations are used     no hand coded features.

three days of training.

tournaments played on a single machine with 4 tpus.

30

alpha vs. stock   sh

from white alpha won 25/50 and lost none.

from black alpha won 3/50 and lost none.

alpha evaluates 70 thousand positions per second.

stock   sh evaluates 80 million positions per second.

31

checkers is a draw

in 2007 jonathan schae   er at the university of alberta showed
that checkers is a draw.

using alpha-beta and end-game id145, scha-
e   er computed drawing strategies for each player.

this was listed by science magazine as one of the top 10 break-
throughs of 2007.

is chess also a draw?

32

grand uni   cation

alphazero uni   es chess and go algorithms.

this uni   cation of intuition (go) and calculation (chess) is sur-
prising.

this uni   cation grew out of go algorithms.

but are the algorithmic insights of chess algorithms really ir-
relevant?

33

chess background

the    rst min-max computer chess program was described by
claude shannon in 1950.

alpha-beta pruning was invented by various people indepen-
dently, including john mccarthy, about 1956-1960.

alpha-beta has been the cornerstone of all chess algorithms
until alphazero.

34

alpha-beta pruning

def maxvalue(s,alpha,beta):

value = alpha
for s2 in s.children():

value = max(value, minvalue(s2,value,beta))
if value >= beta: break()

return value

def minvalue(s,alpha,beta):

value = beta
for s2 in s.children():

value = min(value, maxvalue(s2,alpha,value))
if value <= alpha: break()

return value

conspiracy numbers

conspiracy numbers for min-max search, mcallester, 1988

consider a partially expanded game tree where each leaf is
labeled with a static value.

each node s has a min-max value v (s) determined by the leaf
values.

for any n de   ne an upper con   dence u (s, n ) to be the great-
est value that can be achieved for s by changing n leaf nodes.
we de   ne n (s, u ) to be the least n such that u (s, n )     u .

36

conspiracy algorithm

de   ne an upper-con   dence leaf for s and u any leaf that occurs
in a set of n (s, u ) leaves that can change v (s) to u .

algorithm:

fix a hyper-parameter n .

repeatedly expand an upper-con   dence leaf for the root s and
value u (s, n ) and a lower-con   dence leaf for s value l(s, n ).

37

simulation

to    nd an upper-con   dence leaf for the root and value u :

at a max node pick the child minimizing n (s, u ).

at a min node select any child s with v (s) < u .

38

re   nement

let the static evaluator associate leaf nodes with values u (s, n )
and l(s, n )

39

end

