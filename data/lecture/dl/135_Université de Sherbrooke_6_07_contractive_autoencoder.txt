neural networks
autoencoder - contractive autoencoder

2

hugo larochelle

overcomplete hidden layer

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: overcomplete representation
    hidden layer is overcomplete if greater than the input layer

october 17, 2012

math for my slides    autoencoders   .

    no compression in hidden layer
    each hidden unit could copy a 
different input component
    no guarantee that the 
hidden units will extract 
meaningful structure

ck

abstract

 x

w  = w 
(tied weights)

bj
h(x) = g(a(x))

= sigm(b + wx)
w

x

bx = o(ba(x))

= sigm(c + w   h(x))

hugo larochelle

contractive autoencoder

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: contractive autoencoder
    alternative approach to avoid uninteresting solutions

october 17, 2012

3

    add an explicit term in the loss that 
penalizes that solution
math for my slides    autoencoders   .

    we wish to extract features that
only re   ect variations observed
in the training set
    we   d like to be invariant to the
other variations

ck

abstract

 x

w  = w 
(tied weights)

bj
h(x) = g(a(x))

= sigm(b + wx)
w

x

bx = o(ba(x))

= sigm(c + w   h(x))

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

4

t0=1 x(t0)   

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
  ,   k v (   >    ) 1 v> v    > u> x
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
  ,   k v (   >    ) 1    > u> x
= v>
t pt
contractive autoencoder
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
k )   
k ) log(1  bx(t)

@x(t)

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

2

2

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

4

t0=1 x(t0)   

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
  ,   k v (   >    ) 1 v> v    > u> x
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
  ,   k v (   >    ) 1    > u> x
= v>
t pt
contractive autoencoder
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
  encoder keeps
k )   
k ) log(1  bx(t)

good information

@x(t)

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

2

2

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

4

t0=1 x(t0)   

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
  ,   k v (   >    ) 1 v> v    > u> x
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
  ,   k v (   >    ) 1    > u> x
= v>
t pt
contractive autoencoder
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
  encoder keeps
k )   
k ) log(1  bx(t)
  encoder throws

away all information

good information

@x(t)

2

2

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

4

t0=1 x(t0)   

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
  ,   k v (   >    ) 1 v> v    > u> x
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
  ,   k v (   >    ) 1    > u> x
= v>
t pt
contractive autoencoder
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
  encoder keeps
k )   
k ) log(1  bx(t)
  encoder throws

encoder keeps
only good 
information

away all information

good information

@x(t)

2

2

contractive autoencoder

5

topics: contractive autoencoder
    illustration:

x

x

x

x

x

x

x

x
x

contractive autoencoder

5

topics: contractive autoencoder
    illustration:

x

x

x

x

x

x

x

x
x

contractive autoencoder

5

topics: contractive autoencoder
    illustration:

x

x

x

x

x

x

x

x
x

contractive autoencoder

5

topics: contractive autoencoder
    illustration:

x

x

x

x

x
x

x

x

x
encoder must be

sensitive to this variation

to reconstruct well

contractive autoencoder

5

topics: contractive autoencoder
    illustration:

encoder doesn   t need to be 
sensitive to this variation

(not observed in training set)

x

x

x

x

x
x

x

x

x
encoder must be

sensitive to this variation

to reconstruct well

which autoencoder ?

6

topics: denoising autoencoder, contractive autoencoder
    both the denoising and contractive autoencoder perform well

    advantage of denoising autoencoder: simpler to implement
- requires adding one or two lines of code to regular autoencoder
- no need to compute jacobian of hidden layer

    advantage of contractive autoencoder: gradient is deterministic 

- can use second order optimizers (conjugate gradient, lbfgs, etc.)
- might be more stable than denoising autoencoder, which uses a sampled 

gradient

    to learn more on contractive autoencoders:

- contractive auto-encoders: explicit invariance during feature extraction.

salah rifai, pascal vincent, xavier muller, xavier glorot et yoshua bengio, 2011.

