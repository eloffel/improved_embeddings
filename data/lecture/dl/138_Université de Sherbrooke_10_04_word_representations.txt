neural networks
natural language processing - word representations

natural language processing 2

topics: one-hot encoding
    the major problem with the one-hot representation is that it 
is very high-dimensional
    the dimensionality of e(w) is the size of the vocabulary
    a typical vocabulary size is    100 000
    a window of 10 words would correspond to an input vector of at least 1 000 000 

units!

    this has 2 consequences:

    vulnerability to over   tting

- millions of inputs means millions of parameters to train in a regular neural network

    computationally expensive

- not all computations can be sparsi   ed (ex.: reconstruction in autoencoder)

word representations

3

topics: continuous word representation
    idea: learn a continuous representation of words

    each word w is associated with a real-valued vector c(w)

word
       the       
       a       

       have       
       be       
       cat       
       dog       
       car       

...

w
1
2
3
4
5
6
7
...

c(w)

[ 0.6762, -0.9607, 0.3626, -0.2410, 0.6636 ]
[ 0.6859, -0.9266, 0.3777, -0.2140, 0.6711 ]
[ 0.1656, -0.1530, 0.0310, -0.3321, -0.1342 ]
[ 0.1760, -0.1340, 0.0702, -0.2981, -0.1111 ]
[ 0.5896, 0.9137, 0.0452, 0.7603, -0.6541 ]
[ 0.5965, 0.9143, 0.0899, 0.7702, -0.6392 ]
[ -0.0069, 0.7995, 0.6433, 0.2898, 0.6359 ]

...

4

word representations

ding ~xi is obtained by metric multidimensional scaling [1, 9, 14]. the top eigenvalues of
the gram matrix measure the variance captured by the leading dimensions of this embed-
ding. thus, one can compare the eigenvalue spectra from this method and pca to ascertain
if the variance of the nonlinear embedding is concentrated in fewer dimensions. we refer
to this method of nonlinear id84 as semide   nite embedding (sde).
fig. 1 compares the eigenvalue spectra of pca and sde applied to the 2000 most frequent
words2 in the corpus described in section 4. the    gure shows that the nonlinear embedding
by sde concentrates its variance in many fewer dimensions than the linear embedding by
pca. indeed, fig. 2 shows that even the    rst two dimensions of the nonlinear embedding
preserve the neighboring relationships of many words that are semantically similar. by
contrast, the analogous plot generated by pca (not shown) reveals no such structure.

topics: continuous word representation
    idea: learn a continuous representation of words

    we would like the distance ||c(w)-c(w   )|| to re   ect meaningful similarities 

between words

   may, would, could, should,  

might, must, can, cannot, 

couldn't, won't, will

one, two, three, 

four, five, six, 

seven, eight, nine, 

ten, eleven, 

twelve, thirteen, 
fourteen, fifteen, 

sixteen, 

seventeen,

eighteen

zero

  million
billion

   monday
    tuesday

    wednesday

    thursday

    friday

    saturday

   sunday

    january
    february

    march
    april
    june
    july

    august

    september

    october
    november
    december

figure 2: projection of the normalized bigram counts of the 2000 most frequent words
onto the    rst two dimensions of the nonlinear embedding obtained by semide   nite pro-

(from blitzer et al. 2004)

word representations

5

topics: continuous word representation
    idea: learn a continuous representation of words

natural language processing

hugo larochelle

    we could then use these representations as input to a neural network
    to represent a window of 10 words [w1, ... , w10], we concatenate the 

d  epartement d   informatique
universit  e de sherbrooke

representations of each word

hugo.larochelle@usherbrooke.ca

                              x = [c(w1)   , ... , c(w10)   ]    

november 12, 2012

    we learn these representations by id119

math for my slides    natural language processing   .

    we don   t only update the neural network parameters
    we also update each representation c(w) in the input x with a gradient step

abstract

where l  is the id168 optimized by the neural network

c(w) (= c(w)      rc(w)l

word representations

6

topics: word representations as a lookup table
    let c be a matrix whose rows are the representations c(w)
    obtaining c(w) corresponds to the multiplication e(w)    c
    view differently, we are projecting e(w) onto the columns of c

-

this is a reduction of the dimensionality of the one-hot representations e(w)

    this is a continuous transformation, through which we can propagate gradients

    in practice, we implement c(w) with a lookup table, not with 
a multiplication
    c(w) returns an array pointing to the wth row of c

