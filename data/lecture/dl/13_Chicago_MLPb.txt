ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

computation graphs and id26

the educational framework (edf)

minibatching

explicit index notation

1

computation graphs

a computation graph (sometimes called a    computational graph   )
is a sequence of assignment statements.

       + b0[j]
      
      
       + b1[  y]

            (cid:88)
            (cid:88)

i

l0[j] = relu

w 0[j, i] x[i]

l1[  y] =   

w 1[  y, j] l0[j]

p (  y) =

j
el1[  y]

1
z

2

computation graphs

a simpler example:

(cid:96) =

(cid:113)

x2 + y2

u = x2
v = y2
   
r = u + v
(cid:96) =

r

a computation graph de   nes a dag (a directed acyclic graph)
where the variables are the nodes. each assignment determines
one or more directed edges from the left hand variable to the
right hand variables.

3

computation graphs

1. u = x2
2. w = y2
   
3. r = u + w
4. (cid:96) =

r

for each variable z we can consider    (cid:96)/   z.
gradients are computed in the reverse order.

r

   
(4)    (cid:96)/   r = 1
2
(3)    (cid:96)/   u =    (cid:96)/   r
(3)    (cid:96)/   w =    (cid:96)/   r
(2)    (cid:96)/   y = (   (cid:96)/   w)     (2y)
(1)    (cid:96)/   x = (   (cid:96)/   u)     (2x)

4

a more abstract example

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

for now assume all values are scalars.
we will    backpopagate    the assignments the reverse order.

5

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

   (cid:96)/   u = 1

6

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

   (cid:96)/   u = 1
   (cid:96)/   z = (   (cid:96)/   u) (   h/   z) (this uses the value of z)

7

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

   (cid:96)/   u = 1
   (cid:96)/   z = (   (cid:96)/   u) (   h/   z)
   (cid:96)/   y = (   (cid:96)/   z) (   g/   y) (this uses the value of y and x)

8

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

   (cid:96)/   u = 1
   (cid:96)/   z = (   (cid:96)/   u) (   h/   z)
   (cid:96)/   y = (   (cid:96)/   z) (   g/   y)
   (cid:96)/   x = ??? oops, we need to add up multiple occurrences.

9

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

we let x.grad be an attribute (as in python) of node x.

we will initialize x.grad to zero.

during id26 we will accumulate contributions to
   (cid:96)/   x into x.grad.

10

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

z.grad = y.grad = x.grad = 0
u.grad = 1
loop invariant: for any variable w whose de   nition has not
yet been processed we have that w.grad is    (cid:96)/   w as de   ned
by the set of assignments already processed.

11

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

z.grad = y.grad = x.grad = 0
u.grad = 1
loop invariant: for any variable w whose de   nition has not
yet been processed we have that w.grad is    (cid:96)/   w as de   ned
by the set of assignments already processed.
z.grad += u.grad        h/   z

12

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

z.grad = y.grad = x.grad = 0
u.grad = 1
loop invariant: for any variable w whose de   nition has not
yet been processed we have that w.grad is    (cid:96)/   w as de   ned
by the set of assignments already processed.
z.grad += u.grad        h/   z
y.grad += z.grad        g/   y
x.grad += z.grad        g/   x

id26

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

z.grad = y.grad = x.grad = 0
u.grad = 1
z.grad += u.grad        h/   z
y.grad += z.grad        g/   y
x.grad += z.grad        g/   x
x.grad += y.grad        f /   x

14

the vector-valued case

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

now suppose the variables can be vector-valued.
the loss (cid:96) is still a scalar.
in this case

x.grad =    x (cid:96)

these are now vectors of the same dimension with

x.grad[i] =    (cid:96)/   x[i]

15

the tensor-valued case

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

now suppose the variables can be tensor-valued (the values are
multi-dimensional arrays). the loss is still a scalar.

the computation is now a    tensor    ow   .

16

the tensor-valued case

for the tensor case we simply view tensors as a special case of
vectors. the indeces of x.grad are the same as the indeces of
x.

x.grad.shape = x.shape

the id26 equation for y = f (x) is

x.grad[i1, . . . , in] = (y.grad)[j1, . . . , jk]   xf (x)[j1, . . . , jk, i1, . . . , in]

j1, . . . , jk are indeces of y and i1, . . . , in are indeces of x.

indeces not on the left of the equation are implicitly summed.

17

the edf framework

the educational frameword (edf) is a simple python-numpy
implementation of a deep learning framework.

in edf we write

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

this is python code where variables are bound to objects.

18

the edf framework

y = f (x)
z = g(y, x)
u = h(z)
(cid:96) = u

this is python code where variables are bound to objects.

x is an object in the class input.

y is an object in the class f .

z is an object in the class g.
u and (cid:96) are the same object in the class h.

class f (compnode):

y = f (x)

def

init (self, x):

compnodes.append(self)
self.x = x

def forward(self):

self.value = f(self.x.value)

def backward(self):

self.x.addgrad(self.grad        x f (x))

#needs x.value

20

nodes of the computation graph

there are three kinds of nodes in a computation graph    
inputs, parameters and computation nodes.

class input:

def

init (self):

pass

def addgrad(self, delta):

pass

class compnode: #initialization is handled by the subclass

def addgrad(self, delta):

self.grad += delta

21

class parameter:

def

init (self,value):

parameters.append(self)
self.value = value

def addgrad(self, delta):

#sums over the minibatch
self.grad += np.sum(delta, axis = 0)

def sgd(self):

self.value -= learning rate*self.grad

22

mlp in edf

the following python code constructs the computation graph
of a multi-layer id88 (nlp) with one hidden layer.

l1 = relu(affine(phi1,x))
q = softmax(sigmoid(affine(phi2,l1))
ell = logloss(q,y)

here x and y are input computation nodes whose value have
been set. here phi1 and phi2 are    parameter packages    (a
matrix and a bias vector in this case). we have computation
node classes affine, relu, sigmoid, logloss each of which
has a forward and a backward method.

class affine(compnode):

def __init__(self,phi,x):

compnodes.append(self)
self.x = x
self.phi = phi

def forward(self):

self.value = (np.matmul(self.x.value,

self.phi.w.value)

+ self.phi.b.value)

def backward(self):

self.x.addgrad(

np.matmul(self.grad,

self.phi.w.value.transpose()))

self.phi.b.addgrad(self.grad)

self.phi.w.addgrad(self.x.value[:,:,np.newaxis]
* self.grad[:,np.newaxis,:])

25

the core of edf

def forward():

for c in compnodes: c.forward()

def backward(loss):

for c in compnodes + parameters: c.grad = 0
loss.grad = 1/nbatch
for c in compnodes[::-1]: c.backward()

def sgd():

for p in parameters:

p.sgd()

26

minibatching

ther running time of edf, and of any framework, is greatly
improved by minibatching.

minibatching: we run some number of instances together
(or in parallel) and then do a parameter update based on the
average gradients of the instances of the batch.

for numpy minibatching is not so much about parallelism as
about making the vector operations larger so that the vector
operations dominate the slowness of python. on a gpu mini-
batching allows parallelism over the batch elements.

27

minibatching

with minibatching each input value and each computed value
is actually a batch of values.

we add a batch index as an additional    rst tensor dimensionfor
each input and computed node.

for example, if a given input is a d-dimensional vector then
the value of an input node has shape (b, d) where b is size
of the minibatch.

parameters do not have a batch index.

class parameter:

def

init (self,value):

parameters.append(self)
self.value = value

def addgrad(self, delta):

#sums over the minibatch
self.grad += np.sum(delta, axis = 0)

def sgd(self):

self.value -= learning rate*self.grad

29

forward and backward must handle minibatching

the forward and backward methods must be written to handle
minibatching. we will consider some examples.

30

an mlp

l1 = relu(affine(phi1,x))
p = softmax(sigmoid(affine(phi2,l1))
ell = logloss(p,y)

31

the sigmoid and relu classes just work.

the sigmoid class

class sigmoid:

def __init__(self,x):

compnodes.append(self)
self.x = x

def forward(self):

self.value = 1. / (1. + np.exp(-self.x.value))

def backward(self):

self.x.grad += self.grad

* self.value
* (1.-self.value)

y = affine([w,b],x)

forward:

y.value[b,j] = x.value[b,i]w.value[i,j]
y.value[b,j] += b.value[j]

backward:

x.grad[b,i] += y.grad[b,j]w.value[i,j]
w.grad[i,j] += y.grad[b,j]x.value[i]
b.grad[j] += y.grad[b,j]

class affine(compnode):

def __init__(self,phi,x):

compnodes.append(self)
self.x = x
self.phi = phi

def forward(self):

# y.value[b,j] = x.value[b,i]w.value[i,j]
# y.value[b,j] += b.value[j]
self.value = (np.matmul(self.x.value,

self.phi.w.value)

+ self.phi.b.value)

def backward(self):

self.x.addgrad(

# x.grad[b,i] += y.grad[b,j]w.value[i,j]
np.matmul(self.grad,

self.phi.w.value.transpose()))

# b.grad[j] += y.grad[b,j]
self.phi.b.addgrad(self.grad)

# w.grad[i,j] += y.grad[b,j]x.value[b,i]
self.phi.w.addgrad(self.x.value[:,:,np.newaxis]
* self.grad[:,np.newaxis,:])

35

explicit index notation

a[x1, . . . , xn] += e[x1, . . . , xn, y1, . . . , yk]

abbreviates

for x1, . . . , xn, y1, . . . , yk : a[x1, . . . , xn] += e[x1, . . . , xn, y1, . . . , yk]

a[x1, . . . , xn] = e[x1, . . . , xn, y1, . . . , yk]

abbreviates

for x1, . . . , xn : a[x1, . . . , xn] = 0

a[x1, . . . , xn] += e[x1, . . . , xn, y1, . . . , yk]

36

a   ne transformation with batch index

y[b, i] = x[b, j]w [j, i]
y[b, i] += b[i]

37

the swap rule for tensor products

for backprop swap an input with the output and add    grad   .

y[b, i] = x[b, j]w [j, i]

x.grad[b, j] += y.grad[b, i]w [j, i]

w.grad[i, j] += x[b, j]y.grad[b, i]

y[b, i] += b[i]

b.grad[i] += y.grad[b, i]

the swap rule for functions of tensor products

for backprop swap an input with the output and add    grad   
and   f .

y[b, i] = f (x[b, j]w [j, i])

x.grad[b, j] += y.grad[b, i]

  f w [j, i]

w.grad[i, j] += y.grad[b, i]

  f x[b, j]

here f is scalar to scalar.

39

numpy: reshaping tensors

for an ndarray x (tensor) we have that x.shape is a tuple of
dimensions. the product of the dimensions is the number of
numbers.

in numpy an ndarray (tensor) x can be reshaped into any
shape with the same number of numbers.

40

numpy: broadcasting

shapes can contain dimensions of size 1.

dimensions of size 1 are treated as    wild card    dimensions in
operations on tensors.

x.shape = (5, 1)
y.shape = (1, 10)
z = x     y
z.shape = (5, 10)
z[i, j] = x[i, 0]     y[0, j]

class affine(compnode):

...
def backward(self):

...
# w.grad[i,j] += y.grad[b,j]x.value[b,i]
self.phi.w.addgrad(self.grad[:,np.newaxis,:]

*self.x.value[:,:,np.newaxis])

class parameter:

...
def addgrad(self, delta):

self.grad += np.sum(delta, axis = 0)

42

numpy: broadcasting

when a scalar is added to a matrix the scalar is reshaped to
shape (1, 1) so that it is added to each element of the matrix.

when a vector of shape (k) is added to a matrix the vector is
reshaped to (1, k) so that it is added to each row of the matrix.

in general when two tensors of di   erent order (number of di-
mensions) are added, unit dimensions are prepended to the
shape of the tensor of smaller order to make the orders match.

43

appendix: the jacobian matrix

consider y = f (x) in vector-valued case.

in the vector-valued case the id26 equation is

x.grad += (y.grad)(cid:62)    xf (x)

where

(   xf (x))[i, j] = j [i, j] =    f (x)[i]/   x[j]

the matrix j [i, j] is the jacobian of f .

index types: j is an x-index and i is a y-index.

44

end

