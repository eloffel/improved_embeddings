ttic 31230, fundamentals of deep learning

david mcallester, april 2017

in search of agi

universal models of computation

universal id99

1

is there a universal architecture?

noam chomsky: by the no free lunch theorem natural
language grammar is unlearnable without an in-
nate linguistic capacity. in any domain a strong prior
(a learning bias) is required.

leonid levin, andrey kolmogorov, geo    hinton
and j  urgen schmidhuber: universal learning
algorithms exist. no domain-speci   c in-
nate knowledge is required.

2

is domain-speci   c insight valuable?

fred jelinek: every time i    re a linguist our recognition
rate improves.

3

c++ as a universal architecture

let h be any c++ procedure that can be run on a problem
instance to get a loss where the loss is scaled to be in [0, 1].
let |h| be the number of bits in the zip compression of h.
free lunch theorem: with id203 at least 1       over
the draw of the sample the following holds simultaneously for
all c++ programs h and all    > 1/2.

(cid:19)(cid:19)

(cid:96)(h)     1
1     1

2  

  
n

(ln 2)|h| + ln

1
  

(cid:18)(cid:98)(cid:96)(h) +

(cid:18)

4

the turing tarpit

the choice of programming language does not matter.

for any two turing universal languages l1 and l2 there exists
a compiler c : l1     l2 such that

|c(h)|     |h| + |c|

5

(di   erentiable) universal computation

the circuit model

the von neumann architecture (   id63s   )

functional programming

logic programming

6

universal id99

id13s (freebase, wikidata)

natural language as id99

the foundations of mathematics

the concept of truth.

7

universal function representation theorems

consider continuous functions f : [0, 1]n     r

f    r

given the corner values, the interior can be    lled.

(cid:89)

n(cid:89)
(xiyi+(1   xi)(1   yi))

f (x1, . . . , xn ) =

y1,...,yn, yi   {0,1}

f (y1, . . . , yn)

i=1

hence each of the 2n corners has an independent value.

8

the kolmogorov-arnold representation theorem (1956)

for continuous f : [0, 1]n     r there exists continuous gi, hi,j :
r     r such that

f (x1, . . . , xn ) =

gi

hi,j(xj)

2n +1(cid:88)

       n(cid:88)

      

                     

i=1

j=1

g1(h1,1(x1) + h1,2(x2))
+ g2(h2,1(x1) + h2,2(x2))
...
+ g5(h5,1(x1) + h5,2(x2))

9

f (x1, x2) =

a simpler, similar theorem

for (possibly discontinuous) f : [0, 1]n     r there exists (pos-
sibly discontinuous) g, hi : r     r.

f (x1, . . . , xn ) = g

hi(xi)

that(cid:80)

proof: select hi to spread out the digits of its argument so

i hi(xi) contains all the digits of all the xi.

      (cid:88)

i

      

10

cybenko   s universal approximation theorem (1989)
for continuous f : [0, 1]n     r and    > 0 there exists

f (x) =   (cid:62)  (w x +   )

(cid:88)

      (cid:88)

=

  i  

wi,j xj +   i

      

i

j

such that for all x in [0, 1]n we have |f (x)     f (x)| <   .

11

how many hidden units?

consider boolean functions f : {0, 1}n     {0, 1}.
for boolean functions we can simply list the inputs x0, . . . , xk
where the function is true.

f (x) =

1[x = xk]

(cid:88)
      (cid:88)

k

      

1[x = xk]       

wk,ixi + bk

i

a simpler statement is that any boolean function can be put
in disjunctive normal form.

12

circuit complexity theory

building on work of ajtai, sipser and others, hastad proved
(1987) that any bounded-depth boolean circuit computing the
parity function must have exponential size.

matus telgarsky recently gave some formal conditions under
which shallow networks provably require exponentially more
parameters than deeper networks (colt 2016).

13

id63s (ntm)

id63s alex graves, greg wayne, ivo dani-
helka, 2014

(actually a di   erentiable von neumann architecture)

cpu registers of the ntm

the cpu has a system of registers each of which stores a vector
rather than a bit string.

all computation is di   erentiable.

address registers (heads) hold an attention vector over all
memory cells.

read address registers are separate from write address regis-
ters.

15

an execution cycle

the cpu receives an external input.

the    rst step is to recompute the attention vectors in the read
address registers.
the cpu computes a    key    kh for each head h and an atten-
tion   h
j

  h
j = softmax

kh    m [j]

rh =

  h
j m [j]

(cid:88)

j

j

16

reading from memory

once the read address vectors (attentions) have been computed
we read a value for each read head.

(cid:88)

vh =

  h
j m [j]

j

after reading from memory the machine computes a key and
attention for each write head.

each write operation involves    forget    and    input    operation
analogous to an lstm.

finally the controller emits an external output vector.

17

neural programmer interpreter

neural programmers-interpreter, scott reed and nando de
freitas, iclr, 2016

18

neural programmer interpreter

we will use

i     procedure pointer (integer)
p     procedure instruction (vector)
a     arguments (sequence of integers)
e     memory (array of vectors)
h     cpu state vector

to execute a procedure call run(i, a, e)

h     0
p     m prog[i]
until fend(h) :
return(e)

e, h     dostep(p, a, e, h)

19

dostep

to compute dostep(p, a, e, h)
h     flstm(p, a, e, h)
k     fprog(h)
i     argmaxi k(cid:62)m key[i]
if i = 0 : e     fenv(p, farg(h), e)
else: e     run(i, farg(h), e)
return(e, h)

20

non-di   erentiable steps

to execute a procedure call run(i, a, e)

h     0
p     m prog[i]

until fend(h) :

e, h     dostep(p, a, e, h)

return(e)

21

non-di   erentiable steps

to compute dostep(p, a, e, h)
h     flstm(p, a, e, h)
k     fprog(h)
i     argmaxi k(cid:62)m key[i]
if i = 0 : e     fenv(p, farg(h), e)
else: e     run(i, farg(h), e)
return(e, h)

22

training on execution traces

to train they use execution traces

et, it, at     it+1, at+1, fend

23

tail recursion

making neural programming architectures generalize via re-
cursion, jonathon cai, richard shin, dawn song, iclr, 2017

allowing tail recursion, and explicitly labeling tail recursions
in traces, signi   cantly improves learning.

24

bottom-up logic programming

bottom-up logic programming is distinguished by its relation-
ship to id145 algorithms.

at(x)     reachable(x)

reachable(x)     edge(x, y)     reachable(y)

this de   nes a linear time algorithm for reachability.

25

the cky algorithm for context-free parsing

consider a grammar de   ned by productions of the form s    
np vp and n     kelly.

the o(n3) cky parsing algorithm is de   ned by the following
two rules.

x     a     s[i] = a     x[i, i + 1]

x     y z     y [i, j]     z[j, k]     x[i, k]

26

datalog

a set of id136 rules each of which has antecedents and con-
clusions that are just predicates applied to variables is called
a datalog program.

it can be shown that datalog    captures the complexity class p    
    they can express all and only polynomial time decidable
relations (provided the entities are assigned a total order).

general bottom-up logic programs, including expressions (terms),
are turing complete.

n (x)     n (s(x))

27

universal id99

id13s (freebase, wikidata)

natural language as id99

the foundations of mathematics

the concept of truth.

28

natural language semantics

thousands of civilians have    ed advances by syrian govern-
ment forces in eastern ghouta as ...

29

stanford parse tree

(np (np (nns thousands))

(pp (in of) (np (nns civilians))))

(vp (vbp have)

(vp (vbn fled)

(np (nns advances))
(pp (in by) (np (np (jj syrian)

(nn government)
(nns forces))

(pp (in in) (np (jj eastern)

(nnp ghouta))))))))

stanford dependencies

root(root-0, fled-5)

aux(fled-5, have-4)
nsubj(fled-5, thousands-1)

nmod(thousands-1, civilians-3)

case(civilians-3, of-2)

dobj(fled-5, advances-6)
nmod(fled-5, forces-10)
case(forces-10, by-7)
amod(forces-10, syrian-8)
compound(forces-10, government-9)
nmod(forces-10, ghouta-13)

case(ghouta-13, in-11)
amod(ghouta-13, eastern-12)

just parantheses

(thousands of civilians)
(have fled)
(advances (by (syrian government forces))

(in eastern ghouta))

reference

thousands of civilians have    ed advances by syrian govern-
ment forces in eastern ghouta as damascus makes rapid gains
against the last major rebel enclave near the capital.
damascus     assad
rapid gains     advances-6
the last major rebel enclave ...     ghouta
the capital     damascus

33

reference vs. composition

functional programming is compositional

x = f (y, z)

the meaning of x is computed by f from the meaning of y
and z.

but in language we typically have that f (y, z) is a mention
and x is its referent.

(the last (major rebel enclave) (near (the capital)))

x = (the last q p )

34

natural language: parsing

a fast and accurate dependency parser using neural net-
works, danqi chen and christopher manning, 2014.

...
s3
s2

...
s2
s1

push   

s1    b1 b2 b3 . . .

b1    b2 b3 b4 . . .

35

arc transitions

...
s3
s2

...
s4
s3

leftarc   

s1    b1 b2 b3 . . .

s1    b1 b2 b3 . . .

...
s3
s2

...
s4
s3

leftarc   

s1    b1 b2 b3 . . .

s2    b1 b2 b3 . . .

36

emits s1

(cid:96)    s2

emits s2

(cid:96)    s1

id33 machine con   gurations

a machine con   guration

c = (s, b, a)

s     stack

b     bu   er

a     dependency arcs

37

training

construct a database of machine con   gurations labeled with
actions.

train an mlp with one hidden layer to a softmax over actions
and train on cross id178.

the input to the mlp is a concatenation of 18 word vectors
de   ned in terms of the con   guration

plus 18 corresponding part of speech vectors

and 12 parent edge label vectors.

38

the foundations of mathematics

variables, pairs

x

functions

  x :    e[x]

booleans

p (e)
    

(e1, e2)

f (e)

.
e1
= e2
  1       2

  i(e)

e1 =   e2
   x :      [x]

types

  x :       [x]

  x :       [x]

sx :      [x]

39

the base case

the base case is given by bool, set and type with bool :
type and set : type.

        

.
=      :          not in   

graph

=      : set (        )     bool
.

40

learning guiding evolution

in a 1987 paper entitled    how learning can guide evolu-
tion   , goe   rey hinton and steve nowlan brought attention to
a paper by baldwin (1896).

the basic idea is that learning facilitates modularity.

for example, longer arms are easier to evolve if arm control
is learned     arm control is then independent of arm length.
arm control and arm structure become more modular.

41

learning guiding learning

if each    module    is learning to participate in the    society of
mind    then each model can more easily accommodate (exploit)
changes (improvements) in other modules.

modularity (and abstraction) are fundamental principle of soft-
ware design.

42

levin   s universal problem solver (levin search)

leonid levin observed that one can construct a universal solver
that takes as input an oracle for testing proposed solutions and,
if a solution exists, returns it.

one can of course enumerate all candidate solutions.

however, levin   s solver is universal in the sense that it is not
more than a constant factor slower than any other solver.

43

levin   s universal solver

we time share all programs giving time slice 2   |h| to program
h where |h| is the length in bits of h.

the run time of the universal solver is at most

o(2   |h|(h(n) + t (n)))

where h(n) is the time required to run program h on a problem
of size n and t (n) is the time required to check the solution.
here 2   |h| is independent of n and is technically a constant.

44

id64 levin search

while levin search sounds like a joke, schmidhuber takes it
seriously.

he has proposed ways of accelerating a search over all programs
and has something called the optimal ordered problem solver
(oops).

the basic idea is id64     we automate a search for
methods of e   cient search.

45

end

