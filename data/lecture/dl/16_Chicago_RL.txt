ttic 31230, fundamentals of deep learning

david mcallester, april 2017

deep id23

1

de   nition of id23

rl is de   ned by the following properties:

    an environment with state.

    state changes are in   uenced by sequential decisions.

    reward (or loss) depends on making decisions that lead
to desirable states.

2

id23 examples

    board games (chess or go)

    atari games (pong)

    robot control (driving)

    dialog

    life

3

policies

a policy is a way of behaving.

formally, a (nondeterministic) policy maps a state to a prob-
ability distribution over actions.

  (at|st) id203 of action at in state st

4

imitation learning

construct a training set of state-action pairs (s, a) from ex-
perts.

de   ne stochastic policy     (s).

      = argmin

  

e(s,a)   train     ln     (a | s)

this is just cross-id178 loss.

5

dangers of imperfect imitation learning

perfect imitation learning would reproduce expert behavior.
imitation learning is o   -policy     the state distribution in
the training data is di   erent from that de   ned by the policy
being learned.

imitating experts can be dangersous.    don   t try this at home   .

also, imitating experts will never exceed expert performance
(consider alphazero).

6

id100 (mdps)

for an rl problem we work with an action policy   

st is the state at time t

rt is the reward at time t

at is the action taken at time t.

rt = r(st, at)
t (st+1|st, at)

reward at time t

state transition id203

the state space, action space, r and t de   ne a markov
decision process or mdp.

7

optimizing reward

in rl we maximize reward rather than minimize loss.

(cid:104)(cid:80)t
or e(cid:2)(cid:80)   

r(  ) = e

t=0 rt

episodic reward (go)

t=0   trt

discounted reward

r(  )

      = argmax
(cid:105)

  

(cid:3)
(cid:80)t

8

or limt       1
t

t=0 rt asymptotic average reward (driving)

for discounted reward:

the value function

(cid:34)(cid:88)

(cid:35)

  trt |   , s0 = s

t
v   (s)

v   (s) = e
v    (s) = max
     (a|s) = argmax
v    (s) = max

  

a

a

9

es(cid:48)   t (s(cid:48)|s,a) v    (s(cid:48))

r(s, a) +   es(cid:48)   t (  |s,a) v    (s(cid:48))

value iteration

suppose the state space and action space are    nite.

in that case we can do value iteration.

v0(s) = 0

vi+1(s) = max

a

r(s, a) +   es(cid:48)   t (  |s,a) vi(s(cid:48))

if all rewards are non-negative then

vi+1(s)     vi(s) vi(s)     v    (s)

10

value iteration

theorem: for discounted reward

i       vi(s) = v    (s)

lim

v    (s)     v   (s)
(cid:32)
maxa r(s, a) + es(cid:48)|a  v    (s(cid:48)))
    maxa r(s, a) + es(cid:48)|a  v   (s(cid:48))

(cid:33)

proof:

   

.
= max

s

= max

s

         

11

the q function

for discounted reward:

(cid:34)(cid:88)

(cid:35)

  trt |   , s0 = s, a0 = a

t
q  (s, a)

q  (s, a) = e
q   (s, a) = max
     (a|s) = argmax
q   (s, a) = r(s, t) +   es(cid:48)   t (  |s,a) max

q   (s, a)

  

a

a(cid:48) q   (s(cid:48), a(cid:48))

12

id24

we will assume a parameterized q function q  (s, a).

(cid:18)

(cid:18)

de   ne the bellman error

q  (s, a)    

r(s, a) +    es(cid:48)   s(  |s,a) max

a(cid:48) q  (s(cid:48), a(cid:48))

(cid:19)(cid:19)2

algorithm: run the policy argmaxa q  (st, a) and repeat

   -=         (q  (st, at)     (rt +    max

a

q  (st+1, a)))2

13

the theorem

theorem: for discounted reward, if the bellman error
is zero then the induced policy is optimal.

this id24 theorem is similar in strength to the con-
trastive divergence theorem (gradient is zero at solution).

weaker than the pseudo-likelihood theorem (optimization prob-
lem is correct assuming universal expressiveness).

14

issues with id24

problem 1: nearby states in the same run are highly correlated.

problem 2: sgd on bellman error tends to be unstable. (weak-
ness of the id24 theorem?)

to address these problems we can use a replay bu   er.

15

using a replay bu   er

we use a replay bu   er of tuples (st, at, rt, st+1).

repeat:

1. run the policy argmaxa q  (s, a) to add tuples to the replay
bu   er. remove oldest tuples to maintain a maximum bu   er
size.

2.    =   

3. for n times select a random element of the replay bu   er

and do
   -=         (q  (st, at)     (rt +    argmax

a

q  (st+1, a))2

16

replay is o   -policy

note that the replay bu   er is (slightly) o   -policy. this seems
to be important for stability.

seems related to the issue of stochastic vs. deterministic poli-
cies. more on this later.

17

multi-step id24

(cid:88)

t

     

      q  (st, at)     k(cid:88)

  =0

      2

    r(t+  )

   -=

18

deep id24 (id25)

human-level control through deep id23, mnih
et al., nature, 2015. (deep mind)

19

deep q-networks

we consider a deep q-network     a deep network with
parameters    and computing a q-value q  (s, a).

20

watch the video

https://www.youtube.com/watch?v=v1eynij0rnk

21

asynchronous id24 (simpli   ed)

no replay bu   er. many aynchronous threads each repeating:

     =    (retrieve global   )

t+k   i(cid:88)

using policy argmaxa q     (s, a) compute

st, at, rt, . . . , st+k, at+k, rt+k

ri =

update global   :

   -=   

t+k(cid:88)

i=t

  i+  r(i+  )

  =0

         (q     (si, ai)     ri)2

22

policy gradient

we assume a parameterized policy     (a|s).

    (a|s) is normalized while q  (s, a) is not.

   +=         r(  )

23

policy gradient theorem (episodic case)

e [r |   ] =

p (s0, a0, s1, a1, . . . , st , at ) r

s0,a0,s1,a1,...,st ,at

      p (. . .)r = p (s0)        (a0)p (s1)  (a1)       p (st )  (at ) r
+p (s0)  (a0)p (s1)        (a1)       p (st )  (at ) r
...
+p (s0)  (a0)p (s1)  (a1)       p (st )        (at ) r

= p (. . .)

       r

          (ai)
    (ai)

24

(cid:88)

      (cid:88)

i

policy gradient theorem (episodic case)

      p (. . .)r = p (s0)        (a0)p (s1)  (a1)       p (st )  (at ) r
+p (s0)  (a0)p (s1)        (a1)       p (st )  (at ) r
...
+p (s0)  (a0)p (s1)  (a1)       p (st )        (at ) r

= p (. . .)

(cid:34)

       r
(cid:35)

          (ai)
    (ai)

      (cid:88)
(cid:88)

i

t

25

      e [r |   ] = e

r

      ln     (at|st)

policy gradient theorem

r

(cid:35)

(cid:34)
      e [r |   ]
(cid:88)
      ln     (at|st)
(cid:88)
rt1      ln     (at2|st2)
(cid:88)
rt1      ln     (at2|st2) + e

t1,t2

t

= e

= e

= e

t1<t2

26

(cid:88)

t1   t2

rt1      ln     (at1|st2)

policy gradient theorem

= e

=

      e [r |   ]
(cid:88)
(cid:88)
(cid:88)

est1

t1<t2

t1<t2

est1

t1<t2

rt1      ln     (at2|st2) + e
(cid:88)
(cid:88)

(e rt1|st1)est2|st1
(e rt1|st1)est2|st1

at2

at2

(cid:88)
t1   t2
    (at2|st2)      ln     (at2|st2) +       

rt1      ln     (at1|st2)

          (at2|st2) = 0

policy gradient theorem

      e [r |   ]
(cid:88)
(cid:88)

t1   t2

= e

= e

      ln     (at1|s1) rt2
      ln     (at|st) r   t

t

sampling runs and computing the above sum over t is williams   
reinforce algorithm.

28

optimizing discrete decisions

the reinforce algorithm is used generally for non-di   erentiable
id168s.

for example error rate and id7 score are non-di   erentiable
    they are de   ned on the result of discrete decisions.

policy gradient theorem

(cid:88)
      e [r |   ] = e
(cid:88)

t

      ln     (at|st) r   t
(cid:88)

(          (a|s)) q    (s, a)

  (s) is the expected number of occurances of s

q  (s, a) = e  

=

  (s)

s

a

(cid:80)
t rt | s0 = s, a0 = a

30

the actor-critic algorithm

(cid:88)
(cid:88)

t

      e [r |   ] = e
    e

      ln     (at|st) q    (st, at)
      ln     (at|st) q  (st, at)

t

we can reduce variance by using an estimator q  (s, a) for
q    (s, a).

     is the    actor    and q   is the    critic   .

31

the actor-critic algorithm

      ln     (at|st) q  (st, at)

(cid:88)
(cid:0)         ln        (ai|si)(cid:1) q  (st, at)

t

         (q  (st, at)     r   t)2

      e [r |   ]     e
(cid:88)
(cid:88)

   +=   1

t

   -=   2

t

     is the    actor    and q   is the    critic   .

32

advantage-actor-critic theorem

(cid:88)

s

(cid:88)

a

  (s)

     r(  ) =

(          (a|s)) (q    (s, a)     v     (s))

v   (s) = ea     (  |s) q(s, a)

33

asynchronous advantage actor-critic (a3c)

asynchronous methods for deep id23, mnih
et al., arxiv, 2016 (deep mind)

34

asynchronous advantage actor-critic (a3c)
     =   ;      =    (retrieve global    and   )

using policy         compute st, at, rt, . . . , st+k, at+k, rt+k

ri =

  i+  r(i+  )

t+k   i(cid:88)
(cid:0)         ln        (ai|si)(cid:1)(cid:0)ri     v     (si)(cid:1)

  =0

update global    and   

t+k(cid:88)
t+k(cid:88)

i=t

   +=   

   -=   

         (v     (si)     ri)2

i=t

35

issue: policies must be exploratory

the optimal policy is deterministic     a(s) = argmaxa q(s, a).

however, a deterministic policy never samples alternative ac-
tions.

typically one forces a random action some small fraction of
the time.

36

issue: discounted reward

id25 and a3c use discounted reward on episodic or long term
problems.

presumably this is because actions have near term consequences.

this should be properly handled in the mathematics.

37

observation: continuous actions are di   erentiable

in problems like controlling an inverted pendulum, or robot
control generally, a continuous loss can be de   ned and the
gradient of loss of with respect to a deterministic policy exists.

38

more videos

https://www.youtube.com/watch?v=g59nsurxygk
https://www.youtube.com/watch?v=raai4qzcybs

39

end

