ttic 31230, fundamentals of deep learning

david mcallester, april 2017

id173

over-parameterization

weight decay

dropout

early stopping

pac-bayesian learning theory

1

over-parameterization

if we have more parameters than data then we can    t any set
of labels.

   our experiments establish that state-of-the-art con-
volutional networks for image classi   cation trained with
stochastic gradient methods easily    t a random labeling
of the training data.   

rethinking generalization, zhang et al., iclr 2017

2

over-parameterization

inception (google   s net) on cifar10 can    t random labels on
the training data.

3

using fewer parameters :)

2  256  64 + 9  64  64 = 69, 632

9  256  256 = 589, 824

[kaiming he]

4

early stopping

during sgd one should be tracking validation loss and vali-
dation error.

a typical stopping rule (on a gpu) is to stop training when the
validation error has not improved for two or three consecutive
epochs.

5

early stopping

[goodfellow et al.]

6

early stopping

7

l2 id173 (weight decay)

impose a prior id203 on parameters
2||  ||2

p (  )     e   1

this can be used to justify l2 id173 (ridge regression
is a special case).
      = argmin

e(x,y)   train loss(  , x, y) +

  ||  ||2

1
2

  

pac-bayesian theory (and other theory) can be used to show
that a small value of this regularized optimization objective
guarantees generalization independent of any truth of the prior
(more later).

8

(cid:18)

     

weight decay

e(x,y)   batch loss(  , x, y) +

(cid:19)

  ||  ||2

1
2

=   .grad +     

  t+1 =   t         .grad         t

        is called weight decay where    is the weight decay pa-
rameter.
warning: pytorch does +     rather than         and it seems
to be standard for the decay parameter to be negative :p

9

implicit id173

consider solving linear least squares regression with sgd.

sgd maintains the invariant that    is a linear combination of
input vectors.

when over-parameterized the input vectors span a proper sub-
space.

for least squares regression, sgd    nds the zero training error
solution minimizing ||  ||.

but driving the training error to zero is often a mistake.

10

dropout can be viewed as an ensemble method.

dropout

to draw a model from the ensemble we randomly select a mask
   with

            i = 0 with id203   

  i = 1 with id203 1       

then we use the model (  ,   ) with weight layers de   ned by

      (cid:88)

j

11

      

yi = relu

wi,j   jxj

dropout training

repeat:

    select a random dropout mask   

       -=       (cid:96)(  ,   )

id26 must use the same mask    used in the for-
ward computation.

12

at train time we have

yi = relu

test time scaling

      (cid:88)
      (1       )

j

      

wi,j   jxj

      

wi,j xj

(cid:88)

j

at test time we have

yi = relu

at test time we use the    average network   .

13

dropout for least squares regression

consider simple least square regression
      = argmin
(cid:104)

(   (cid:12) x)(   (cid:12) x)(cid:62)(cid:105)   1

e(x,y) e   (y           (   (cid:12) x))2
e [y(   (cid:12) x)]
(cid:88)

e(x,y)(y     (1       )      x)2 +

= e

  

= argmin

  

i

(cid:105)

(cid:104)

x2
i

  2
i

(         2)e

1
2

in this case dropout is equivalent to a form of l2 id173
    see wager et al. (2013).

14

search over id173 parameters

hyper-parameter search on penn tree bank language model-
ing with a 4 layer lstm.

loss

input drop

intra lay drop

learning rate

output drop

state drop weight decay

melis et al. 2017.

following gal and ghahrmani 2016, state dropout is an id56
parameter dropout with the same mask across the sequence.

15

early stopping

early stopping can limit ||  ||     growing a large ||  || can take
a long time.
early stopping seems more related to limiting ||         init||
theoretical guarantees work for ||         init||2 just as well as
for ||  ||2.

this suggests replacing weight decay with

  t+1 =   t         .grad       (         init)

16

learning theory: nature vs. nurture

noam chomsky: natural language grammar is unlearnable
without with an innate linguistic capacity. this position is
supported by the    no free lunch theorem   .

andrey kolmogorov, geo    hinton: universal
learning algorithms exist. this position is sup-
ported by the    free lunch theorem   .

17

the no free lunch theorem

without prior knowledge, such as universal grammar, it is im-
possible to make a prediction for an input you have not seen
in the training data.

proof: select a predictor h uniformly at random from all
functions from x to y and then take the data distribution to
draw pairs (x, h(x)) where x is drawn uniformly from x . no
learning algorithm can predict h(x) where x does not occur in
the training data.

18

the free lunch theorem

universal (knowledge-free) learning algorithms exists.
let h be a c++ procedure taking an input from x and return-
ing a value in y, where h is written using calls to prodecures in
an (arbitrarily large) code library l. let |h| be the number of
bit in a standard compression algorithm applied to the source
code for h. we are compressing only the    main    procedure h
and not the library l.

19

the free lunch theorem

consider a id168 loss(p, x, y) such that for any c pro-
gram p and pair (x, y) we have loss(p, x, y)     (0, lmax).

theorem: for any standard library l    xed before the draw
of the training data, we have that with id203 at least
1        over the draw of the training data the following holds
simultaneously for all main programs p and    > 1/2.

e(x,y)   population loss(p, x, y)

(cid:18)

(cid:18)

(cid:19)(cid:19)

    1
1     1

2  

e(x,y)   train loss(p, x, y) +

  lmax

n

20

(ln 2)|h| + ln

1
  

pac-bayesian generalization bounds

the free lunch theorem is a special case of a pac-bayesian
generalization bound.

pac-bayesian theory was introduced by me in 1999. the
bounds have evolved over time with contributions by lang-
ford, blum, shawe-taylor, catoni and others.

these bounds are of increasing interest today because of their
applicability to deep networks.

21

a more general free lunch theorem

let h be a discrete (countable) hypothesis class. h might be
the collection of all c programs.
let p be a    prior    id203 distribution on h. p (h) might
be 2   8|h| where |h| is the length of h in bytes.

e(x,y)   population loss(h, x, y)

(cid:18)

    1
1     1

2  

e(x,y)   train loss(h, x, y) +

  lmax

n

22

(cid:18)

ln

1

p (h)

+ ln

1
  

(cid:19)(cid:19)

a finite precision corollary

suppose that we parameterize a classi   er with a parameter
vector    with d parameters and use b bits per parameter.

e(x,y)   population loss(  , x, y)

(cid:18)

    1
1     1

2  

e(x,y)   train loss(  , x, y) +

  lmax

n

23

(cid:18)

(ln 2)db + ln

(cid:19)(cid:19)

1
  

proof

l(h) = e(x,y)   pop loss(h, x, y)

  l(h) = e(x,y)   train loss(h, x, y)

24

consider lmax = 1 and de   ne  (h) by

proof

(cid:118)(cid:117)(cid:117)(cid:116)2l(h)

(cid:16)

(cid:17)

.

ln 1

p (h) + ln 1
  
n

 (h) =

by the relative chernoof bound we have

(cid:16)   l(h)     l(h)      (h)

(cid:17)     e

   n  (h)2

2l(h) =   p (h).

ptrain   pop

25

ptrain   pop

proof

(cid:16)   l(h)     l(h)      (h)
(cid:17)       p (h).
(cid:16)   h   l(h)     l(h)      (h)
(cid:17)    (cid:88)
(cid:17)     1       
(cid:16)   h l(h)       l(h) +  (h)

h

ptrain   pop

ptrain   pop

  p (h) =   

26

l(h)    (cid:98)l(h) +

proof

(cid:118)(cid:117)(cid:117)(cid:117)(cid:117)(cid:116)l(h)

         2

(cid:16)

ln 1

p (h) + ln 1
  

n

(cid:17)

         

using

we get

   

ab = inf
  >0

a
2  

l(h)    (cid:98)l(h) +

  

l(h)
2  

+

27

  b
2

+

(cid:16)

ln 1

p (h) + ln 1
  

n

(cid:17)

proof

l(h)    (cid:98)l(h) +

  

l(h)
2  

+

(cid:17)

(cid:16)

ln 1

p (h) + ln 1
  

n

solving for l(h) yields

(cid:18)

l(h)     1
1     1

2  

  l(h) +

  lmax

n

28

(cid:18)

ln

1

p (h)

+ ln

1
  

(cid:19)(cid:19)

a kl divergence bound

let p be any    prior    and q be any    poterior    on any model
space.
de   ne

l(q) = eh   q l(h)

  l(q) = eh   q

  l(h)

for any p and any    > 1
the draw of the training data, the following holds simultane-
ously for all q.

2, with id203 at least 1        over
(cid:19)(cid:19)

(cid:18)

(cid:18)

  l(q) +

  lmax

kl(q, p ) + ln

l(q)     1
1     1

2  

1
  

n

29

l2 bounds

p (w) = n (0, 1)d

l(q  ) = e    n (0,1)d l(   +  )

  l(q  ) = e    n (0,1)d   l(   +  )

kl(q  , p ) =

||  ||2

1
2

l(q  )     1
1     1

2  

(cid:18)

  l(q  ) +

  lmax

n

30

(cid:18)1

2

||  ||2 + ln

1
  

(cid:19)(cid:19)

a dropout bound

kl(q  ,  , q  ,0) = e     p  ,    n (0,1)d ln
||   (cid:12)   ||2

p  (  )e   1
p  (  )e   1

2||  (cid:12) ||2

2||  (cid:12)(  + )||2

(cid:18)1       

2

||  ||2 + ln

1
  

(cid:19)(cid:19)

= e     p  
1       

1
2
||  ||2

2

=

(cid:18)

  l(q  ,  ) +

  lmax

n

31

l(q  ,  )     1
1     1

2  

l2 pac-bayesian bounds in action

computing nonvacuous generalization bounds for deep (stochas-
tic) neural networks with many more parameters than train-
ing data, (dziugaite and roy, arxiv, 2017)

the bounds are based on l2 distance of the weight
vector to the initialization.
the weight vector is retrained to minimize the
bound.

a formal de   nition of implicit prior

we now consider any learning algorithm a which takes train-
ing data and returns a id203 distribution qa(train) on
parameters.

de   ne qa(pop) = etrain   pop qa(train)

to draw    from qa(pop) i    rst draw train from pop and
then draw    from qa(train).
we will show that qa(pop) is an optimal prior for a running
on pop. we can think of qa(pop) as an implicit prior for a.

33

optimality of qa(pop)

consider an arbitrary prior p .

etrain   pop kl(qa(train), p )

= etrain   pop ln

qa(train)(h)

p (h)

= etrain   pop, h   qa(train) ln

qa(train)(h)
qa(pop)(h)

+eh   qa(pop) ln

qa(pop(h))

p (h)

optimality of qa(pop)

etrain   pop kl(qa(train), p )

= etrain   pop kl(qa(train), qa(pop)) + kl(qa(pop), p )

so qa(pop) is an optimal prior (or implicit prior) for
algorithm a.

35

an implicit prior generalization bound

for any given learning algorithm a and    > 1
2 we have the
following with id203 at least 1        over the draw of the
training data.

l(qa(train))

            l(qa(train))
(cid:16)

    1
1     1

2  

+  lmax

n

kl(qa(train), qa(pop)) + ln 1
  

         

(cid:17)

36

ensembles under square loss

we average k regression models

i=1

k(cid:88)
k(cid:88)
k(cid:88)

i=1

f (x) =

f (x)     y =

  =

1
k

1
k

1
k

fi(x)

(fi(x)     y)

 i,  i = fi     y (residuals)

i=1

37

i

e

ensembles

(cid:88)

assume that e(cid:2) 2
(cid:3) =   2 and e(cid:2) i j
(cid:3) =   2   for i (cid:54)= j.
      2          =
         
      (cid:88)
      1
            
       2
(cid:88)
(cid:18)1
(cid:18)
(cid:3) /  2 < 1 we win.

if pearson   s correlation    = e(cid:2) i j

k     1
k

  2   =   2

1
k2e

  2 +

j(cid:54)=i

i +

 i j

1
k

 i

+

=

k

k

i

i

1     1
k

(cid:19)

(cid:19)

  

38

ensembles under log loss
for log loss we average the id203 vectors.

p (y|x) =

pi(y|x)

i

1
k

(cid:88)
           1

k

(cid:88)

i

pi

(cid:96)(pi)

      1

k

(cid:96)

(cid:88)

i

    log p is a convex function of p . for any convex (cid:96)(p )
jensen   s inequality states that

this implies that the loss of the average model cannot be worse
(can only be better) than the average loss of the models.

39

l1 id173 and sparse weights

(cid:88)
|  i|
(cid:96)train(  ) +   ||  ||1

||  ||1 =

i

p(  )     e   ||  ||1

      = argmin

  

   -=         (cid:96)train(  )
  i -=      sign(  i)

(shrinkage)

at equilibrium

(sparsity is di   cult to achieve with sgd)

  i = 0

   (cid:96)/     i =      sign(  i)

40

if |   (cid:96)/     i| <   
otherwise

sparse activation

we can impose an l1 id173 on the activations of the
network (the output of the activation function of each neuron).

      = argmin

  

(cid:96)(  ) +   ||h||1

where h is the vector of neuron activation outputs.

this will tend to make activations sparse.

41

sparse coding

let w be a matrix where we view w  ,i is the ith    dictionary
vector   .

for input x we can construct a k-sparse representation h(x).

h(x) = argmin
h,||h||0=k

||x     w h||2

note

w h =

(cid:88)

i   i(x)

hi w  ,i

|i(x)| = k

we can now replace x by its sparse code h(x).

42

end

