ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

deep id114 i

exponential softmax

su   cient statistics

belief propagation

1

consider colorization

x is a black and white image.
y is a color image drawn from pop(y|x).

  y is an arbitrary color image.
q  (  y|x) is the id203 that model    assigns to the color
image y given black and white image x.

2

cross id178 training

      = argmin

  

e(x,y)   pop     log q  (y|x)

3

auto-regressive models are tractable

an auto-regressive model is locally normalized.

(cid:89)

i

qf (  y) =

qf (  y[i] |   y[parents(i)])
f (  y|  y[parents(i)])

qf (  y[i] |   y[parents(i)]) = softmax

  y

there are exponentially many possible values for   y but each
softmax is over a tractable-sized set.

general markov random fields (mrfs)

are more challenging

we can run a id98 with parameters    on the black and white
image x to get a markov random    eld (mrf) f  (x) on possible
color images.

the mrf f  (x) will determine the probabilities q  (  y|x) =
qf  (x)(  y).

5

markov random fields (mrfs)

  y[i] is the color value of pixel i in image   y.

  y[(i, j)] is the pair (  y[i],   y[j]) for neighboring pixels i and j.

6

markov random fields (mrfs)

(cid:88)

i   nodes

f (  y) =

(cid:88)

(i,j)   edges

f [i,   y[i]] +

f [(i, j),   y[(i, j)]]

node potentials

edge potentials

7

exponential softmax

qf (  y) = softmax

  y

f (  y)

(cid:88)

  y

ef (  y)

qf (  y) =

1
z

ef (  y)

(cid:88)

i   nodes

f (  y) =

z =

(cid:88)

(i,j)   edges

8

f [i,   y[i]] +

f [(i, j),   y[(i, j)]]

hyper-graphs: more general and more concise

a hyper-edge is a subset of nodes.

f [i,   y[i]] +

f [(i, j),   y[(i, j)]]

(cid:88)

i   nodes

f (  y) =

(cid:88)

(i,j)   edges

(cid:88)

     hyperedges

9

f (  y) =

f [  ,   y[  ]]

back-propagation through an exponential softmax

      = argmin

  

= argmin

  

e(x,y)   pop     log q  (y|x)
e(x,y)   pop     log qf  (x)(y)

we need to back-propagate through the softmax to get f.grad.

f is a tensor containing the numbers f [  ,   y] where   y is a pos-
sible value of   y[  ].

       log qf (y)

   f [  ,   y]

f.grad[  ,   y] =

10

back-propagation through an exponential softmax

loss(f, y) =     ln

(cid:18) 1

z(f )

(cid:19)

ef (y)

f.grad[  ,   y] =

= ln z(f )     f (y)

       1

z

(cid:88)

  y

ef (  y) (   f (  y)/   f [  ,   y])

11

           (   f (y)/   f [  ,   y])

back-propagation through an exponential softmax

       1
(cid:88)
      (cid:88)

z

  y

f.grad[  ,   y] =

=

ef (  y) (   f (  y)/   f [  ,   y])

qf (  y) (   f (  y)/   f [  ,   y])

  y

= e  y   qf

= p  y   qf

1[  y[  ] =   y]     1[y[  ] =   y]

(  y[  ] =   y)     1[y[  ] =   y]

           (   f (y)/   f [  ,   y])
           (   f (y)/   f [  ,   y])

su   cient statistics

f.grad[  ,   y] = p  y   qf

(  y[  ] =   y)     1[y[  ] =   y]

to compute f.grad it su   ces to compute p  y   qf

(  y[  ] =   y).

by (minor) abuse of terminology we will call the quantities
p  y   qf

(  y[  ] =   y) the su   cient statistics for f .

we now focus on computing the su   cient statistics for a given
mrf f .

13

an aside: features and weights

the indicators 1[  y[  ] =   y] form a 0-1 feature vector   (  y).

the tensor f [  ,   y] forms a weight vector.

f (  y) =

f [  ,   y[  ]]

f [  ,   y]1[  ,   y[  ] =   y]

(cid:88)
(cid:88)
= f(cid:62)  (  y)

  ,  y

=

  

14

an aside: features and weights

the su   cient statistics p  y   qf
(  y[  ] =   y) are just the expected
value of the features under the distribution de   ned by the
mrf.

15

belief propagation

f.grad[  ,   y] = p  y   qf

(  y[  ] =   y)     1[y[  ] =   y]

for trees we can compute p  y   qf
sage passing, aka, belief propagation.

(  y[  ] =   y) exactly by mes-

16

message passing

for each edge (i, j) there is a message zi   j and a message
zj   i.

each message is assigns a weight to each node value of the
target node.

zj   i[  y] is the partition function for the subtree attached to i
through j and with   y[i] restricted to   y.

17

message passing

(cid:88)

z(i,   y)

.
=

  y:   y[i]=  y

= ef [i,  y]

ef (  y)

       (cid:89)

j   n (i)

      

z(i,   y)

zj   i[  y]

(cid:88)

p  y   qf

(  y[i] =   y) = z(i,   y)/z,

z =

  y

18

message passing

(cid:88)

  y(cid:48)

zj   i[  y] =

ef [j,  y(cid:48)]+f [{j,i},{  y(cid:48),  y}]

       (cid:89)

k   n (j), k(cid:54)=i

      

zk   j[  y(cid:48)]

19

message passing

z({i, j},   y)

.
=

(cid:88)
(cid:89)
(cid:89)

ef (  y)

  y:   y[{i,j}]=  y

= ef [i,  y[i]]+f [j,  y[j]]+f [{i,j},  y]

zk   i[  y[i]]

zk   j[  y[j]]

k   n (i), k(cid:54)=j

k   n (j), k(cid:54)=i

p  y   qf

(  y[{i, j}] =   y) = z({i, j},   y)/z

20

loopy bp

message passing is also called belief propagation (bp).

in a graph with cycles it is common to do loopy bp.

this is done by initializing all message zi   j[  y] = 1 and then
repeating (until convergence) the updates

(cid:88)

  y(cid:48)

zj   i[  y] =

ef [j,  y(cid:48)]+f [{j,i},{  y(cid:48),  y}]

21

       (cid:89)

k   n (j), k(cid:54)=i

      

zk   j[  y(cid:48)]

end

