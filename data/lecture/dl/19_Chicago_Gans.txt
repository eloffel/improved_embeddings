ttic 31230, fundamentals of deep learning

david mcallester, april 2017

id3 (gans)

1

modeling distributions (cross id178)

      = argmin

  

ey   pop     log q  (y)

      = argmin

  

e(x,y)   pop     log q  (y|x)

2

id5

q  (  z|y)

q  (  z)

  y  (  z)

[hyeonwoo noh et al.]

      = argmax

  

ey   pop log q  (y) (cross id178)

log q  (y)     e  z   p  (  z|y) ln q  (  z, y) + h(p  (  z|y))

(elbo)

id3 (gans)

      noise

  y  ( )

[hyeonwoo noh et al.]

in a gan a distribution is modeled by a generator     there
is no encoder.

4

gans, goodfellow et al., 2014 (also schmidhuber 1992)

cross id178 loss on q   is replaced by

      = argmax

  

min
  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

   is the generator.

y is either drawn from q   or from pop (with equal probabiity)
and s is a    ag telling which.

   is the discriminator     the descriminator must predict s
from y.

5

the theorem

      = argmax

  

min
  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

theorem: if q  (y) and q  (s|y) are universally expressive
(can represent any distribution) then q      = pop.

6

generated bedrooms(dc gans, radford et al., iclr 2016)

7

interpolated faces

[ayan chakrabarti]

image arithmetic (dc gans, radford et al., iclr 2016)

9

gans on id163

10

conditional gans

all distribution modeling methods apply to conditional distri-
butions.
      = argmax

e(x,y,s)   (pop (cid:93) pop(x)q  (y|x))     log q  (s|x, y)

min
  

  

11

image-to-image translation (isola et al., 2016)

12

u-nets (ronnenberger et al. 2015)

13

image-to-image translation (isola et al., 2016)

14

arial photo to map and back

15

colorization

16

semantic segmentation

17

cycle gans (zhu et al., 2017)

18

cycle gans

19

cycle gans

cycle training of machine translation

lample et al, 2017, also artetxe et al., 2017

21

text to speech (saito et al. 2017)

minimum generation error (mge) uses perceptual distortion
    a distance between the feature vector of the generated sound
wave and the feature vector of the original.

perceptual naturalness can be enforced by a discriminator.

adversarial id20 (tzeng et al. 2017)

23

issues

jensen-shannon divergence

vanishing gradients

unstable training

mode collapse

measuring perfomance

24

jensen-shannon divergence

      = argmax

  

min
  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

     (  ) = argmin

  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

q     (  )(s = 1|y) =

p (y, s = 1)

p (y)

=

pop(y)

pop(y) + q  (y)

      = argmax

  

e(y,s)   (pop (cid:93) q  )     log q     (  )(s|y)

= argmax

  

+

1
2

e(y,1)   pop     log
    log

1
2
e(y,   1)   q  

pop(y)

pop(y) +   (y|  )

q  (y)

pop(y) + q  (y)

= argmax

  

1     1
2

kl(pop, a)     1
2

kl(q  , a)

a(y) =

1
2

(pop(y) + q  (y))

26

jensen-shannon divergence (jsd)

we have arrived at the jensen-shannon divergence.

      = argmin
(cid:18)

  

1
2

jsd(pop, q  )

(cid:19)

(cid:18)

(cid:19)

p + q

2

jsd(p, q) =

kl

p,

p + q

2

+

1
2

kl

q,

0     jsd(p, q) = jsd(q, p )     1 (in bits)

27

vanishing gradients

the discriminator typically    wins   .

the log loss goes to zero (becomes exponentially small) and
there is no gradient to guide the generator.

in this case the learning stops and the generator is blocked
from minimizing jsd(pop, q  ).

28

a heuristic fix

we continue to use

     (  ) = argmin

  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

but switch the optimization for    from

      = argmax

ey   q  

    log q  (   1|y)

to

      = argmin

ey   q  

    log q  (1|y)

  

  

it can be shown that     log q  (1|y) is essentially the margin
of the binary classi   er   .

29

converting to cross id178 (goodfellow)

     (  ) = argmin

  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

assume: q     (1|y) =

de   ne: f     (y)

.
=

=

30

pop(y)

pop(y) + q  (y)
q     (1|y)
q     (   1|y)
pop(y)
q  (y)

      ey   q  

converting to cross id178

y

=

(cid:88)
q  (y)f  (y)      ln q  (y)

q  (y)f  (y)

f  (y) =      
(cid:88)
(cid:88)
= ey   pop       ln q  (y)
=       ey   pop ln q  (y)

pop(y)      ln q  (y)

=

y

y

31

unstable training

simultaneous id119 is not the same as nested max-
min.

max

  

min
  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

vs.

min
  

max

  

e(y,s)   (pop (cid:93) q  )     log q  (s|y)

32

a synthetic example

33

another example

min

x

max

y

xy

a nash equilibrium is x = y = 0.

simultaneous gradient    ow yields a circle.

34

mode collapse a.k.a mode dropping

the generator distribution drops portions of the population.

35

measuring performance

most evaluation of gans is based on subjective judgments of
naturalness.

this is in contrast to id38 where performance
is directly measured by cross-id178 (bits per character or
perplexity).

36

summary

gans have not generally proved useful in discriminative tasks
such as image segmentation, id103, or machine
translation.

i predict that there will ultimately be better ways to model
distributions (as in id38).

i predict that in a few years discriminators will be limited to
enforcing perceptual naturalness in applications such as text
to speech and image decompression.

37

end

