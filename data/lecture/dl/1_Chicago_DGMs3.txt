ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

deep id114 iii

expectation maximization (em)

expected gradient (eg)

ctc

1

latent variable models

      = argmin

  

e(x,y)   pop     ln q  (y|x)

y ranges over a structured set such as sentences or images.

(cid:88)

q  (y|x) =

q  (  z, y | x)

  z

  z ranges over latent labels such as a word sense for each word
or a semantic label for each pixel.

2

the expected gradient (eg) identity

      ln q(y) =

=

      q(  z, y)

      q(y)
q(y)
(cid:88)
(cid:88)
= e  z   q(  z|y)      ln q(  z, y)

q(y)

q(y)

=

  z

  z

q(  z, y)       ln q(  z, y)

the eg identity

      ln q  (y|x) = e  z   q  (  z|x,y)      ln q  (  z, y|x)

it is important to note that the gradient operation only appears
inside the expectation.

this is an expected gradient.

4

su   cient statistics

(cid:16)

consider a model q  (  z,   y|x) and a tensor s computed from
x, y and   z such that
(cid:17)

e  z   q  (  z|x,y)       ln q  (  z, y|x) = f
when this equation holds we say that e  z   q  (  z|x,y) s(x, y,   z)
is a su   cient statistic for the model q  (  z,   y|x).
even when   z is a structured object (with exponentially many
possible values) it is often possible to    nd a tractable-sized suf-
   cient statistic that can be computed or estimated e   ciently.

e  z   q  (  z|x,y) s(x, y,   z)

5

example: latent variable directed id114

we assume a population of pairs (x, y) and a model q  (y|x)
to be trained by cross-id178.

      = argmin

  

    ln q  (y|x)

here y is an assignment values to observed nodes.

we must marginalize over assignments to unobserved nodes.

6

latent variable directed id114

(cid:89)

let   v denote an assignment to all nodes     both observed and
unobserved (latent).
here we consider only directed models     models satisfying

q  (  v|x) =

q  (x)(  v[i] |  v[parents(i)])

i

here q  (x) is a tensor giving the id155 tables
q[  v[i] |  v[parents(i)]].

7

observed and latent variables

let   vy be the assignment   v makes to the observed variables.

let   vz be the assignment   v makes to the unobserved variables.

let q abbreviate the tensor q  (x).

q(  v) = q(  vz,   vy)

q(  vy) =

q(  vz,   vy)

(cid:88)

  vz

8

the su   cient statistics

for each node i the tensor q  (x) speci   es the conditional
id203 table q[  vi|  vp] where   vi ranges over the possible
values of node i and   vp ranges over the possible assignments
of values to the parents of i.

q.grad =    q     ln

q(  vz,   vy)

q.grad[  vi|  vp] =

q(  vz,   vy)

  vz

   q[  vi|  vp]

(cid:88)
       ln(cid:80)

  vz

9

the su   cient statistics

   q ln q(y) = e  z   q(  z|y)   q ln q(  z, y)

(cid:88)

= e  z   q(  z|y)

   q ln q((  z, y)[i] | (  z, y)[parents(i)])

q.grad[  vi,   vp] = e  z   q(  z|y)

i

1

q[  vi,   vp]

1[(  z, y)(i, parents(i)) = (  vi,   vp)]

=

1

q[  vi|  vp]

e  z   q(  z|y)1[(  z, y)(i, parents(i)) = (  vi,   vp)]

the quantities e  z   q(  z|y)1[(  z, y)(i, parents(i)) = (  vi,   vp)] are
the su   cient statistics.

10

trees are tractable

for tree models the su   cient statics can be computed e   -
ciently by message passing (belief propagation).

loopy bp can be used for non-tree models.

expected gradient (eg)

and expectation maximization (em)

eg:

   q ln q(y) = e  z   q(  z|y)    q ln q(  z, y).

em:

qt+1 = argmaxq e  z   qt(  z|y) ln q(  z, y).

eg =    q ln q(y) equals is the gradient of the em objective.

eg and em have the same su   cient statistics (e-step).

connectionist temporal classi   cation (ctc)

phonetic transcription

a speech signal

is labeled with a phone sequence

x = x1, . . . , xt

with n << t and with yn     y for a set of phonemes y.

y = y1, . . . , yn

the length n of y is not determined by x and the alignment
between x and y is not given.

13

the model de   nes q  (  z|x, y) where   z is latent.

ctc

  z =   z1, . . . ,   zt ,

  zt     y     {   }

the sequence

y(  z) = y1, . . . , yn

is the result of removing all the occurrences of     from   z.

   , a1,   ,   ,   , a2,   ,   , a3,        a1, a2, a3

14

the ctc model

h1, . . . , ht = id56  (x1, . . . , xt )

q  (  zt|x1, . . . , xt ) = softmax

  z

e(  z)(cid:62)ht

this is a locally normalized (directed) graphical model where
  zt does not have any parent nodes.

15

the su   cient statistics

since each node has no parents the su   cient statistics are

p  z   q(  z|y)(zt =   z)

id145 (forward-backward)

x = x1, . . . , xt
  z =   z1, . . . ,   zt ,
y = y1, . . . , yn ,
y = (  z1, . . . ,   zt )        

  zt     y     {   }
yn     y, n << t

forward-backward

(cid:126)yt = (  z1, . . . ,   zt)        

f [n, t] = q((cid:126)yt = y1, . . . , yn)
b[n, t] = q(yn+1, . . . , yn|(cid:126)yt = y1, . . . , yn)

17

id145 (forward-backward)

(cid:126)yt = (  z1, . . . ,   zt)        

f [n, t] = q((cid:126)yt = y1, . . . , yn)
b[n, t] = q(yn+1, . . . , yn|(cid:126)yt = y1, . . . , yn)

f [0, 0] = 1
f [n, 0] = 0 for n > 0

f [n + 1, t + 1] = q(  zt+1 =    )f [n + 1, t] + q(  zt+1 = yn+1)f [n, t]

b[n, t ] = 1
b[n, t ] = 0,

for n < n

b[n     1, t     1] = q(  zt   1 =    )b[n     1, t] + q(  zt   1 = yn   1)b[n, t]

latent variable mrfs

(cid:80)

  z ef (  z,  y)
z

ef (  z,  y)

qf (  z,   y) =

qf (  y) =

1
z

(cid:88)

qf (  z,   y) =

  z

z(  y)

qf (  y) =
loss(y) = ln z     ln z(y)

z

the su   cient statistics

p  z   q(  z|y)(  zt =   z)

                                 

(cid:88)

n

=

1

q(y)

f [n, t     1] q(  zt) b[n, t]
f [n, t     1] q(  zt) b[n + 1, t] for   zt = yn+1

for   zt =    

0

otherwise

20

undirected latent variable mrfs

      = argmin

  

f (  z,   y)

e(x,y)   pop     ln qf  (x)(y)
(cid:88)
(cid:88)

f [  ,   z[  ],   y[  ]]

  z,  y

  

qf (  z,   y) = softmax

f (  z,   y) =

qf (  y) =

qf (  z,   y)

  z

21

latent variable mrfs

loss(y, f ) = ln z     ln z(y)

f.grad[  ,   y,   z] = p  z,  y   qf

(  y[  ] =   y,   z[  ] =   z)

   p  z   qf (  z|y)(y[  ] =   y,   z[  ] =   z)

these are the su   cient statistics for latent variable mrfs.

22

end

