 

 memory networks for 

language understanding 

                                   jason weston 

                          facebook ai research  

intelligent conversational 

agents 

end-to-end dialog agents 

while it is possible to build useful dialog agents as a set of  
separate black boxes with joining logic (google now, 
cortana, siri, .. ?) we believe a true dialog agent should: 
         be able to combine all its knowledge to fulfill complex 

tasks. 

         handle long open-ended conversations involving 

effectively tracking many latent variables.  

         be able to learn (new tasks) via conversation. 
our bet: machine learning end-to-end systems is the way 
forward in the long-run. 

memory networks 

         class of  models that combine large memory with learning 

component that can read and write to it. 

         incorporates reasoning with attention over memory (ram). 
         most ml has limited memory which is more-or-less all that   s 

needed for    low level    tasks e.g. id164. 

our motivation: long-term memory is required to read 
a story and then e.g. answer questions about it. 
 
similarly, it   s also required for dialog: to remember 
previous dialog (short- and long-term), and respond. 
 
1.    we first test this on the toy (babi) tasks. 
2.    any interesting model has to be good on real data as well. 

memory networks 

evaluating end-to-end learners  
         long term goal: a learner can be trained (from scratch?) 

to understand and use language. 

         our main interest: uncover the learning algorithms able to 

do so. 

         inspired by    a roadmap towards machine 

intelligence    (mikolov, joulin, baroni 2015) we advocate 
a set of  tasks to train & evaluate on: 
         classic id38 (id32, text8) 
         story understanding (children   s book test, news articles) 
         open id53 (webquestions, wikiqa) 
         goal-oriented dialog and chit-chat (movie dialog, ubuntu) 

what is a memory network?            
original paper description of class of models 

memnns have four component networks (which may or 
may not have shared parameters): 
         i: (input feature map) convert incoming data to the 

internal feature representation. 

         g: (generalization) update memories given new input. 
         o: produce new output (in feature              
representation space) given the memories. 

         r: (response) convert output o into                              a 

response seen by the outside world. 

some memory network- 

related publications 

 
        
         s. sukhbaatar, a. szlam, j. weston, r. fergus. end-to-end memory networks. nips 

j. weston, s. chopra, a. bordes. memory networks. iclr 2015 (and arxiv:1410.3916). 

2015 (and arxiv:1503.08895).  

        

j. weston, a. bordes, s. chopra, a. m. rush, b. van merri  nboer, a. joulin, t. mikolov. 
towards ai-complete id53: a set of  prerequisite toy tasks. arxiv:
1502.05698.  

         a. bordes, n. usunier, s. chopra, j. weston. large-scale simple id53 

with memory networks. arxiv:1506.02075.  

        

j. dodge, a. gane, x. zhang, a. bordes, s. chopra, a. miller, a. szlam, j. weston. 
evaluating prerequisite qualities for learning end-to-end id71. arxiv:
1511.06931. 

         f. hill, a. bordes, s. chopra, j. weston. the goldilocks principle: reading children's 

books with explicit memory representations. arxiv:1511.02301. 

j. weston. dialog-based language learning. arxiv:1604.06045.  

        
         a. bordes, jason weston. learning end-to-end goal-oriented dialog. arxiv:

1605.07683. 

memory network models 

implemented models.. 

supervision  
(direct or 
reward-based)  

output 

memory module 
m

r e a d  

addressing 

r e a d  

addressing 

m

l

 
e
u
d
o
m

 
r
e

l
l

o
r
t
n
o
c

q

memory 
vectors 

input 

internal state 

vector (initially: query) 

[figure by saina    
sukhbaatar]  

variants of  the class    

some options and extensions: 
         representation of inputs and memories could use all 
kinds of encodings:  bag of  words,  id56 style reading 
at word or character level, etc.  

         different possibilities for output module: e.g. multi-
class classifier or uses an id56 to output sentences.  

         if the memory is huge (e.g. wikipedia) we need to 

organize the memories. solution: hash the memories to 
store in buckets (topics). then, memory addressing and 
reading doesn   t operate on all memories. 

         if the memory is full, there could be a way of  removing 
one it thinks is most useless; i.e. it ``forgets       somehow. 
that would require a scoring function of  the utility of  
each memory.. 

task (1) factoid qa with single 

supporting fact (   where is actor   ) 

 

 

(very simple) toy reading comprehension task: 

john was in the bedroom. 
bob was in the office. 
john went to kitchen. 
bob travelled back home. 
where is john? a:kitchen 

supporting fact 

(2) factoid qa with two supporting 

facts (   where is actor+object   ) 
a harder (toy) task is to answer questions where two supporting 
statements have to be chained to answer the question: 

john is in the playground. 
bob is in the office. 
john picked up the football. 
bob went to the kitchen. 
where is the football?  a:playground 
where was bob before the kitchen? a:office 

 

 

 

 
 

(2) factoid qa with two supporting 

facts (   where is actor+object   ) 
a harder (toy) task is to answer questions where two supporting 
statements have to be chained to answer the question: 

 

 

 

john is in the playground. 
bob is in the office. 
john picked up the football. 
bob went to the kitchen. 
where is the football?  a:playground 
where was bob before the kitchen? a:office 

supporting fact 

supporting fact 

to answer the first question where is the football? both john picked up 
the football and john is in the playground are supporting facts. 
 
. 
 

memory network models 

implemented models 

supervision  
(direct or 
reward-based)  

output 

memory module 
m

r e a d  

addressing 

r e a d  

addressing 

m

l

 
e
u
d
o
m

 
r
e

l
l

o
r
t
n
o
c

q

memory 
vectors 

input 

internal state 

vector (initially: query) 

[figure by saina    
sukhbaatar]  

the first memnn implemention 

         i (input): converts to bag-of-word-embeddings x. 
         g (generalization): stores x in next available slot mn. 
         o (output): loops over all memories k=1 or 2 times: 

         1st loop max: finds best match mi with x. 
         2nd loop max: finds best match mj with (x, mi). 
         the output o is represented with (x, mi, mj). 

         r (response): ranks all words in the dictionary given 
o and returns best single word. (or: use a full id56 here) 

 

matching function 

         for a given q, we want a good match to the relevant 

memory slot(s) containing the answer, e.g.: 

match(where is the football ?,  john picked up the 
football) 
         we use a qtutud embedding model with word 
embedding features: 
         lhs features:  q:where q:is q:the q:football q:? 
         rhs features:  d:john d:picked d:up d:the d:football 

qdmatch:the qdmatch:football  

(qdmatch:football is a feature to say there   s a q&a word match, which can help.) 

the parameters u are trained with a margin ranking loss:   
supporting facts should score higher than non-supporting facts. 

matching function: 2nd hop  
         on the 2nd hop we match question & 1st hop to new 

fact: 

match( [where is the football ?, john picked up the 
football],                    j          john is in the playground) 
         we use the same qtutud embedding model: 

         lhs features:  q:where q:is q:the q:football q:? q2: 
         rhs features:  d:john d:is d:in d:the d:playground 

john q2:picked q2:up q2:the q2:football 

qdmatch:the qdmatch:is .. q2dmatch:john  

 

objective function 

minimize: 

where: so is the matching function for the output component.     
            sr is the matching function for the response component. 
            x is the input question.  
            mo1 is the first true supporting memory (fact).    
            mo2 is the first second supporting memory (fact). 
            r is the response 
            true facts and responses mo1, mo2  and r should have higher 
scores than all other facts and responses by a given margin. 
 

comparing triples 

         we also need time information for the babi tasks.  we tried 
adding absolute time as a feature: it works, but the following 
idea can be better: 

         seems to work better if  we compare triples: 
         match(q,d,d   ) returns < 0 if  d is better than d    
                          returns > 0 if  d    is better than d 

we can loop through memories, keep best mi at each step. 
now the features include relative time features: 

l.h.s:  same as before 

r.h.s:  features(d)  dbeforeq:0-or-1             

          -features(d   ) d   beforeq:0-or-1  dbefored   :0-or-1 

comparing triples: objective and id136 

similar to before, except now for both mo1 and mo2 we need to have 
two terms considering them as the second or third argument to the 
sot as they may appear on either side during id136: 
 

babi experiment 1 

       10k sentences. (actor: only ask questions about actors.) 
       difficulty: how many sentences in the past when entity mentioned. 
       fully supervised (supporting sentences are labeled). 
       compare id56 (no supervision)  
     and memnn hops k = 1 or 2, & with/without time features. 

        difficulty 5 -- max mem. sz. required: 65    average mem. sz. required: 9 

babi experiment 1 

         example test story + predictions: 
antoine went to the kitchen.  antoine got the milk. 
antoine travelled to the office. antoine dropped the 
milk. sumit picked up the football.  antoine went to 
the bathroom. sumit moved to the kitchen. 
          where is the milk now?   a: office 
           where is the football?   a: kitchen 
           where is antoine ?  a: bathroom 
           where is sumit ?  a: kitchen 
           where was antoine before the bathroom? a: office  

unsegmented setup; r module is an id56 
joe went to the garden then fred picked up the milk; joe 
moved to the bathroom and fred dropped the milk, and then 
dan moved to the living room. 
         where is dan? a: living room i believe 
         where is joe? a: the bathroom 
fred moved to the bedroom and joe went to the kitchen then 
joe took the milk there and dan journeyed to the bedroom; 
joe discarded the milk. 
         where is the milk now? a: the milk is in the kitchen 
         where is dan now? a: i think he is in the bedroom 
joe took the milk there, after that mike travelled to the office, 
then joe went to the living room, next dan went back to the 
kitchen and joe travelled to the office. 
         where is joe now? a: i think joe is in the office 

larger qa: reverb dataset in (fader et al., 13) 
         14m statements, stored as (subject, relation, 
object) triples. triples are reverb extractions 
mined from clueweb09.                                                                                       

         statements cover diverse topics:  
         (milne, authored, winnie-the-pooh) 
         (sheep, be-afraid-of, wolf),   etc... 

         weakly labeled qa pairs and 35m paraphrased 

questions from wikianswers: 
         ``who wrote the winnie the pooh books? 
         ``who is poohs creator?       

 

results: qa on reverb data     

from (fader et al.) 

       14m statements stored in the memnn memory. 
       k=1 loops memnn, 128-dim embedding. 
       r response simply outputs top scoring statement. 
       time features are not necessary, hence not used. 
       we also tried adding bag of  words (bow) features. 

 

fast qa on reverb data 

scoring all 14m candidates in the memory is slow. 

we consider speedups using hashing in s and o as 
mentioned earlier: 
         hashing via words (essentially: inverted index) 
         hashing via id116 in embedding space (k=1000) 
 

a memnn multitasked on babi 

data and reverb qa data 

the    story    told to the model after training: 

antoine went to the kitchen. antoine picked up the milk. 
antoine travelled to the office. 

memnn   s answers to some questions: 
         where is the milk?  a: office 
         where was antoine before the office? a: kitchen 
         where does milk come from?   a: milk come from cow 
         what is a cow a type of?   a: cow be female of  cattle 
         where are cattle found?   a: cattle farm become widespread in brazil 
         what does milk taste like? a: milk taste like milk 
         what does milk go well with?  a: milk go with coffee 

related memory models 

(published before or ~same time as original paper) 
         id56search (bahdanau et al.) for machine translation 
         can be seen as a memory network where memory goes 
back only one sentence (writes embedding for each word).  
         at prediction time, reads memory and performs a soft max 

to find best alignment (most useful words). 1 hop only. 

         generating sequences with id56s (graves,    13) 

         also does alignment with previous sentence to generate 
handwriting (so id56 knows what letter it   s currently on). 

         id63s (graves et al., 14)                           

[on arxiv just 5 days after memnns!] 

         has read and write operations over memory to perform 
         128 memory slots in experiments; content addressing 

tasks (e.g. copy, sort, associative recall). 

computes a score for each slot       slow for large memory? 

         earlier work by (das    92), (schmidhuber et al., 93), 

discern (miikkulainen,    90) and others... 

learning of basic algorithms using 
reasoning, attention, memory (ram)      
(e.g. addition, multiplication, sorting) 
methods include adding stacks and addressable memory to id56s: 
            neural net architectures for temporal sequence processing.    m. mozer.  
            id63s    a. graves, g. wayne, i. danihelka. 
            inferring algorithmic patterns with stack augmented recurrent nets.            

a. joulin, t. mikolov. 

            learning to transduce with unbounded memory    e. grefenstette et al. 
            neural programmer-interpreters    s. reed, n. de freitas. 
            id23 turing machine.    w. zaremba and i. sutskever. 
            learning simple algorithms from examples    w. zaremba, t. mikolov, a. 

joulin, r. fergus.  

            the neural gpu and the neural ram machine    i. sutskever. 

classic nlp tasks for ram 

classic id38: 
            long short-term memory    sepp hochreiter, j  rgen schmidhuber. 

machine translation: 
            sequence to sequence learning with neural networks    i. sutskever, o. vinyals, 

q. le. 

            id4 by jointly learning to align and translate    d. 

bahdanau, k. cho, y. bengio. 

parsing: 
            grammar as a foreign language    o. vinyals, l. kaiser, t. koo, s. petrov, i. 

sutskever, g. hinton. 

entailment: 
            reasoning about entailment with neural attention    t. rockt  schel, e. 

grefenstette, k. hermann, t. ko  isk  , p. blunsom. 

summarization: 
            a neural attention model for abstractive sentence summarization     a. m. rush, 

s. chopra, j. weston. 

reasoning with synthetic 

language 

            a roadmap towards machine intelligence    t. mikolov, a. joulin, m. baroni. 
            towards ai-complete id53: a set of prerequisite toy tasks                    

j. weston, a. bordes, s. chopra, a.. rush, b. van merri  nboer, a. joulin, t. 
mikolov. 

several new models that attempt to solve babi tasks: 
            dynamic memory networks for natural language processing    a. kumar, o. 

irsoy,      p. ondruska, m. iyyer, j. bradbury, i. gulrajani, r. socher. 

            towards neural network-based reasoning    b. peng, z. lu, h. li, k. wong. 
            end-to-end memory networks    s. sukhbaatar, a. szlam, j. weston, r. fergus. 

 

new nlp datasets for ram   

understanding news articles: 
            teaching machines to read and comprehend    k. hermann, t. ko  isk  , e. grefenstette, l. 

espeholt, w. kay, m. suleyman, p. blunsom. 

 

understanding children   s books: 
            the goldilocks principle: reading children's books with explicit memory 

representations    f. hill, a. bordes, s. chopra, j. weston. 

conducting dialog: 
            hierarchical neural network generative models for movie dialogues    i. serban, a. 

sordoni, y. bengio, a. courville, j. pineau. 

            a neural network approach to context-sensitive generation of conversational 

responses    sordoni et al. 

            neural responding machine for short-text conversation    l. shang, z. lu, h.li. 
            evaluating prerequisite qualities for learning end-to-end id71    j. dodge, a. 

gane, x. zhang, a. bordes, s. chopra, a. miller, a. szlam, j. weston. 

general id53: 
            large-scale simple id53 with memory networks     a. bordes, n. usunier, s. 

chopra, j. weston. 

what was next for memnns? 
         make the language much harder: coreference, 
conjunctions, negations, etc. etc     will it work? 

         memnns that reason with more than 2 supporting 

memories. 

         end-to-end? (doesn   t need supporting facts) 
         more useful applications on real datasets. 
         dialog: ask questions? say statements? 
         do memnn ideas extend to other ml tasks and model 
variants, .e.g. visual qa, perform actions   ? [a: yes!]. 

babi tasks: what reasoning tasks 
would we like models to work on? 
         we define 20 tasks (generated by the simulation) that 

we can test new models on. (see: http://fb.ai/babi) 

         the idea is they are a bit like software tests:           

each task checks if  an ml system has a certain skill.  

         we would like each    skill    we check to be a natural 

task for humans w.r.t. text understanding & reasoning, 
humans should be able to get 100%. 

j. weston, a. bordes, s. chopra, t. mikolov. towards ai-
complete id53: a set of  prerequisite 
toy tasks. arxiv:1502.05698. 

task (1) factoid qa with single 

supporting fact (   where is actor   ) 
our first task consists of  questions where a single supporting fact, 
previously given, provides the answer. 

we test simplest case of  this, by asking for the location of  a person. 

a small sample of  the task is thus: 

john is in the playground. 
bob is in the office. 
where is john? a:playground 

supporting fact 

we could use supporting facts for supervision at training 
time, but are not known at test time (we call this    strong 
supervision   ). however weak supervision is much better!! 

(2) factoid qa with two supporting 

facts (   where is actor+object   ) 

a harder task is to answer questions where two supporting statements 
have to be chained to answer the question: 

 

 

 

john is in the playground. 
bob is in the office. 
john picked up the football. 
bob went to the kitchen. 
where is the football?  a:playground 

supporting fact 

supporting fact 

to answer the question where is the football? both john picked up the 
football and john is in the playground are supporting facts. 
 
. 
 

(3) factoid qa with three 

supporting facts 

similarly, one can make a task with three supporting facts: 

 

 

john picked up the apple. 
john went to the office. 
john went to the kitchen. 
john dropped the apple. 
where was the apple before the kitchen? a:office 
 

the first three statements are all required to answer this. 

(4) two argument relations: 

subject vs. object 

to answer questions the ability to differentiate and recognize subjects 
and objects is crucial. 

we consider the extreme case: sentences feature re-ordered words: 

 

the office is north of  the bedroom. 
the bedroom is north of  the bathroom. 
what is north of  the bedroom? a:office 
what is the bedroom north of? a:bathroom 

note that the two questions above have exactly the same words, but in 
a different order, and different answers. 
 
so a bag-of-words will not work. 

(6) yes/no questions 

         this task tests, in the simplest case possible (with a single supporting 

fact) the ability of  a model to answer true/false type questions: 

john is in the playground. 
daniel picks up the milk. 
is john in the classroom? a:no 
does daniel have the milk? a:yes 

(7) counting 

tests ability to count sets: 

daniel picked up the football. 
daniel dropped the football. 
daniel got the milk. 
daniel took the apple. 
how many objects is daniel holding? a:two 

(8) lists/sets 

         tests ability to produce lists/sets: 

daniel picks up the football. 
daniel drops the newspaper. 
daniel picks up the milk. 
what is daniel holding? a:milk,football 

(11) basic coreference (nearest referent) 

daniel was in the kitchen. 
then he went to the studio. 
sandra was in the office. 
where is daniel? a:studio 

(13) compound coreference 

daniel and sandra journeyed to the office. 
then they went to the garden. 
sandra and john travelled to the kitchen. 
after that they moved to the hallway. 
where is daniel? a:garden 

(14) time manipulation 

         while our tasks so far have included time implicitly in the order of  the 
statements, this task tests understanding the use of  time expressions 
within the statements: 

in the afternoon julie went to the park.  
yesterday julie was at school. 
julie went to the cinema this evening. 
where did julie go after the park? a:cinema 

much harder difficulty: adapt a real time expression labeling dataset 
into a question answer format, e.g. uzzaman et al.,    12.  
 

(15) basic deduction 

         this task tests basic deduction via inheritance of  properties: 

sheep are afraid of  wolves. 
cats are afraid of  dogs. 
mice are afraid of  cats. 
gertrude is a sheep. 
what is gertrude afraid of? a:wolves 

deduction should prove difficult for memnns because it effectively 
involves search, although our setup might be simple enough for it. 
 

(17) positional reasoning 

         this task tests spatial reasoning, one of  many components of  the 

classical shrdlu system: 

the triangle is to the right of  the blue square. 
the red square is on top of  the blue square. 
the red sphere is to the right of  the blue square. 
is the red sphere to the right of  the blue square? a:yes 
is the red square to the left of  the triangle? a:yes 

(18) reasoning about size 

         this tasks requires reasoning about relative size of  objects and is 
inspired by the commonsense reasoning examples in the winograd 
schema challenge: 

the football fits in the suitcase. 
the suitcase fits in the cupboard. 
the box of  chocolates is smaller than the football. 
will the box of  chocolates fit in the suitcase? a:yes 

tasks 3 (three supporting facts) and 6 (yes/no) are prerequisites. 
 

(19) path finding 

         in this task the goal is to find the path between locations: 

the kitchen is north of  the hallway. 
the den is east of  the hallway. 
how do you go from den to kitchen?  a:west,north 

this is going to prove difficult for memnns because it effectively 
involves search. 

what models could we try? 
         classic nlp cascade e.g. id166-struct with bunch of  

features for subtasks: (not end-to-end) 

         id165 models with id166-type classifier? 
         (lstm) recurrent neural nets? 
         memory network variants      ? 
         <insert your new model here> 

end-to-end memory 
network (memn2n) 

         new end-to-end (memn2n) model (sukhbaatar    15): 

         reads from memory with soft attention 
         performs multiple lookups (hops) on memory 
         end-to-end training with id26 
         only need supervision on the final output  
 

         it is based on    memory networks    by  

[weston, chopra & bordes iclr 2015] but that had: 
         hard attention 
         requires explicit supervision of  attention during training 
         only feasible for simple tasks 
 
 

memn2n architecture 

output 

supervision 

memory 
module 

r e a d  

addressing 

r e a d  

addressing 

controller 
module 

memory vectors 
 (unordered) 

input 

internal state 

vector 

attention weights 
/ soft address 

memory module 

weighted sum 

softmax 

dot product 

memory vectors 

to controller 
(added to  
controller state) 

addressing signal 
(controller  
state vector) 

question & answering 

memory module 

weighted sum 

dot product + softmax 

answer 

kitchen 

c
o
n
t
r
o

 

l
l

e
r
 

1: sam moved 

 to garden 

2: sam went  
to kitchen 

3: sam drops 
 apple there 

input story 

where is sam? 

question 

memory vectors 

e.g.) constructing memory vectors with bag-of-words 
(bow) 
1.    embed each word  
2.    sum embedding vectors 

embedding vectors 

memory vector 

e.g.) temporal structure: special words for time and include 
them in bow 

time embedding 

positional encoding of  words 

representation of inputs and memories could 
use all kinds of encodings:  bag of  words,  
id56 style reading at word or character level, 
etc.  
 
we also built a positional encoding variant: 
words are represented by vectors as before. 
but instead of  a bag, position is modeled by a 
multiplicative term on each word vector with 
weights depending on the position in the 
sentence. 

training on 1k stories 

task 

weakly supervised 

supervised supp. facts 

id165s  lstms  memn2n  memory 
networks 

structid166
+coref+srl 

t1. single supporting fact 
t2. two supporting facts 
t3. three supporting facts 
t4. two arguments relations 
t5. three arguments relations 
t6. yes/no questions 
t7. counting 
t8. sets 
t9. simple negation  
t10. indefinite knowledge 
t11. basic coreference 
t12. conjunction 
t13. compound coreference 
t14. time reasoning 
t15. basic deduction 
t16. basic induction  
t17. positional reasoning 
t18. size reasoning 
t19. path finding 
t20. agent   s motivation 

36 
2 
7 
50 
20 
49 
52 
40 
62 
45 
29 
9 
26 
19 
20 
43 
46 
52 
0 
76 

50 
20 
20 
61 
70 
48 
49 
45 
64 
44 
72 
74 

pass 

27 
21 
23 
51 
52 
8 
91 

pass 

87 
60 

pass 

87 
92 
83 
90 
87 
85 

pass 
pass 
pass 
pass 
pass 
pass 

49 
89 
7 

pass 

pass 
pass 
pass 
pass 
pass 
pass 

85 
91 

pass 
pass 
pass 
pass 
pass 
pass 
pass 
pass 

65 

pass 

36 

pass 

pass 

74 
17 

pass 

83 

pass 

69 
70 

pass 
pass 
pass 
pass 
pass 
pass 
pass 

24 
61 
62 
49 

pass 

attention during memory 

lookups 

samples from toy qa tasks 

yes

0.00
0.00
0.37
0.60
0.01

support hop 1 hop 2 hop 3
0.03
0.00
0.00
0.96
0.00

story (1: 1 supporting fact)
daniel went to the bathroom.
0.00
mary travelled to the hallway.
0.00
john went to the bedroom.
0.02
john travelled to the bathroom.
0.98
mary went to the office.
0.00
where is john?   answer: bathroom    prediction: bathroom
story (16: basic induction)
brian is a frog.
lily is gray.
brian is yellow.
julius is green.
greg is a frog.
what color is greg?  answer: yellow    prediction: yellow

support hop 1 hop 2 hop 3
0.00
0.00
1.00
0.00
0.00

0.00
0.07
0.07
0.06
0.76

0.98
0.00
0.00
0.00
0.02

yes

yes

yes

yes

yes

hop 2
0.00
1.00
0.00
0.00
0.00

support hop 1
0.06
0.88
0.00
0.00
0.00

story (2: 2 supporting facts)
john dropped the milk.
john took the milk there.
sandra went back to the bathroom.
john moved to the hallway.
mary went back to the bedroom.
where is the milk?   answer: hallway    prediction: hallway
story (18: size reasoning)
the suitcase is bigger than the chest.
the box is bigger than the chocolate.
the chest is bigger than the chocolate.
the chest fits inside the container.
the chest fits inside the box.
does the suitcase fit in the chocolate?   answer: no    prediction: no

support hop 1
0.00
0.04
0.17
0.00
0.00

hop 2
0.88
0.05
0.07
0.00
0.00

yes

yes

hop 3
0.00
0.00
0.00
1.00
0.00

hop 3
0.00
0.10
0.90
0.00
0.00

figure 2: example predictions on the qa tasks of [21]. we show the labeled supporting facts
(support) from the dataset which memn2n does not use during training, and the probabilities p of
each hop used by the model during id136. memn2n successfully learns to focus on the correct
supporting sentences.

memnn 

93.3% 

failed 
tasks 

test acc 

lstm 

49% 

20 

4 

model
id56 [15]

# of
hidden
300

id32

# of memory valid.
perp.
hops
133

size
-

-

test
perp.
129

memn2n 
1 hop 

74.82% 

17 

text8

2 hops 

3 hops 

# of
hidden
500

84.4% 

# of memory valid.
hops
perp.

11 

11 

size
87.6.% 
-

-

-

test
perp.
184

20 babi tasks 

so we still fail on some 

tasks   . 

  .. and we could also make more tasks that we fail on! 

 

our hope is that a feedback loop of: 
1.     developing tasks that break models, and 
2.     developing models that can solve tasks 

                   leads in a fruitful research direction   . 

how about on real data? 

         toy ai tasks are important for developing innovative methods. 
         but they do not give all the answers. 

         how do these models work on real data? 

         classic id38 (id32, text8) 
         story understanding (children   s book test, news articles) 
         open id53 (webquestions, wikiqa) 
         goal-oriented dialog and chit-chat (movie dialog, ubuntu) 

id38 

the goal is to predict the next word in a text sequence given the 
previous words. results on the id32 and text8 (wikipedia-
based) corpora. 

penn tree 

text8 

id56 

lstm 

memn2n 
2 hops 

5 hops 

7 hops 

129 

115 

121 

118 

111 

184 

154 

187 

154 

147 

test perplexity 

hops vs. attention:  
average over (ptb)  

average over (text8) 

id38 

the goal is to predict the next word in a text sequence given the 
previous words. results on the id32 and text8 (wikipedia-
based) corpora. 

penn tree 

text8 

id56 

lstm 

memn2n 
2 hops 

5 hops 

7 hops 

129 

115 

121 

118 

111 

184 

154 

187 

154 

147 

test perplexity 

memnns are in the same ballpark as lstms. 
 
hypothesis: many words (e.g. syntax words) don   t actually need really long 
term context, and so memnns don   t help there. 
 
maybe memnns could eventually help more on things like nouns/entities? 

self-supervision memory 

network 

two tricks together that make things work a bit better: 

1) bypass module 

instead of  the last output module being a linear layer from 
the output of  the memory, assume the answer is one of the 
memories. sum the scores of  identical memories.  

2) self-supervision 

we know what the right answer is on the training data, so 
just directly train that memories containing the answer 
word to be supporting facts (have high id203). 

results on children   s book test 

id53 on new   s articles 

we evaluate our models on the data from: 
   teaching machines to read and comprehend    
karl moritz hermann, tom     ko  isk  , edward grefenstette, lasse 
espeholt, will kay, mustafa suleyman, phil blunsom 
 

   results on id98 qa dataset 

latest fresh results 

 

         our best results:  qaid98: 69.4    cbt-ne: 66.6   cbt-v: 63.0 
         text understanding with the attention sum reader network. kadlec et 

al. (4 mar    16)     qaid98: 75.4    cbt-ne: 71.0   cbt-cn: 68.9                 
uses id56 style encoding of words + bypass module + 1 hop 

         iterative alternating neural attention for machine reading. sordoni et 

al. (7 jun    16)     qaid98: 76.1    cbt-ne: 72.0    cbt-cn: 71.0 

         natural language comprehension with the epireader. trischler et al. 

(7 jun    16)          qaid98: 74.0    cbt-ne: 71.8    cbt-cn: 70.6 

         gated-attention readers for text comprehension. dhingra et al. (5 jun 
   16)                     qaid98: 77.4    cbt-ne: 71.9    cbt-cn: 69.             
uses id56 style encoding of words + bypass module + multiplicative 
combination of query + multiple hops 

webquestions & simplequestions 
         decent results on webquestions, a popular qa task: 

 

a. bordes, n. usunier, s. chopra j.weston. large-scale 
simple id53 with memory networks. 
arxiv:1506.02075.  

       however now beaten by many results, especially (yih et al. acl    15) that 

achieves 52.5! several hand engineered features are used in that case. 
note webquestions is very small (4k train+valid). 

recent work: new models for qa on documents 

miller et al. key-value memory networks for directly 

reading documents. arxiv:1606.03126.  

recent work: new models for qa on documents 

miller et al. key-value memory networks for directly 

reading documents. arxiv:1606.03126.  

wikiqa results 
 
 
 
 
 
 
 
 
 
 
 

how about on large scale dialog 
data? with multiple exchanges? 
         everything we showed so far was id53 

potentially with long-term context. 

         we have also built a movie dialog dataset 

closed, but large, domain about movies (75k entities, 3.5m ex). 
         ask facts about movies? 
         ask for opinions (recommendations) about movies? 
         dialog combining facts and opinions? 
         general chit-chat about movies (statements not questions)? 
 
and    combination of all above in one end-to-end model. 

recent work: combines qa with dialog tasks      

dodge et al.    evaluating prerequisite qualities for 
learning end-to-end id71.    iclr    16 

(dialog 1) qa: facts about movies 

sample input contexts and target replies (in red) from dialog task 1: 

  what movies are about open source?   revolution os 
ruggero raimondi appears in which movies? carmen 
what movies did darren mcgavin star in?  billy madison, the night 
stalker, mrs. pollifax-spy, the challenge 
can you name a film directed by stuart ortiz? grave encounters 
who directed the film white elephant?  pablo trapero 
what is the genre of  the film dial m for murder?   thriller, crime 
what language is whity in?  german 

(dialog 2) recs: movie recommendations 

sample input contexts and target replies (in red) from dialog task 2: 

  schindler's list, the fugitive, apocalypse now, pulp fiction, and 
the godfather are films i really liked. can you suggest a film?         
the hunt for red october 
 
some movies i like are heat, kids, fight club, shaun of  the dead, 
the avengers, skyfall, and jurassic park. can you suggest 
something else i might like?  ocean's eleven 

(dialog 3) qa+recs: combination dialog 

sample input contexts and target replies (in red) from dialog task 3: 

  i loved billy madison, blades of  glory, bio-dome, clue, and happy 
gilmore. i'm looking for a music movie.   school of  rock 
what else is that about?    music, musical, jack black, school, 
teacher, richard linklater, rock, guitar 
i like rock and roll movies more. do you know anything else?   
little richard 
 

(dialog 4) reddit: real dialog 

sample input contexts and target replies (in red) from dialog task 4: 

   
i think the terminator movies really suck, i mean the first one was 
kinda ok, but after that they got really cheesy. even the second one 
which people somehow think is great. and after that... 
forgeddabotit. 
c   mon the second one was still pretty cool.. arny was still so 
badass, as was sararah connor   s character.. and the way they 
blended real action and effects was perhaps the last of  its kind... 

memory network: example 

results 

ubuntu data 

dialog dataset: ubuntu irc channel logs, users ask 
questions about issues they are having with ubuntu 
and get answers by other users. (lowe et al.,    15) 

best results currently reported: 
sentence pair scoring: towards unified framework for text comprehension 
petr baudi  , jan pichl, tom     vysko  il, jan   ediv   
id56-id98 combo model: 67.2 

next steps 

artificial tasks to help design new methods: 
         new methods that succeed on all babi tasks? 
         make more babi tasks to check other skills. 
real tasks to make sure those methods are actually useful: 
         sophisticated reasoning on babi tasks doesn   t always happen as 

clearly on real data.. why? fix! 

         models that work jointly on all tasks so far built. 
dream: can learn from very weak supervision: 

we would like to learn in an environment just by communicating with other 
agents / humans, as well as seeing other agents communicating + acting 
in the environment.                                               

e.g. a baby talking to its parents, and seeing them talk to each other. 

learning from human 

responses 

mary went to the hallway. 

john moved to the bathroom. 

mary travelled to the kitchen. 

where is mary?     a:playground 

no, that's incorrect.  

where is john?     a:bathroom 

yes, that's right!  

if  you can predict this, you 
are most of  the way to 
knowing how to answer 
correctly. 

human responses give lots 

of  info 

mary went to the hallway. 

john moved to the bathroom. 

mary travelled to the kitchen. 

where is mary?     a:playground 

no, the answer is kitchen.  

where is john?     a:bathroom 

yes, that's right!  

much more signal 
than just    no    or 
zero reward. 

forward prediction 

mary went to the hallway. 

john moved to the bathroom. 

mary travelled to the kitchen. 

where is mary?  a:playground 

no, she   s in the kitchen.  

if  you can predict this, 
you are most of  the way 
to knowing how to 
answer correctly. 

candidate(
answers(

m

m

r e a d  

addressing 
r e a d  

q

addressing 

r e a d  

addressing 

memory 
module 

output 

predict 
response to 
answer  

controller 
module 

q

memory vectors 

input 

internal state 

vector (initially: query) 

see our new paper!     dialog-based language learning    arxiv:1604.06045. 

fair: paper / data / code 
         papers: 

         babi tasks: arxiv.org/abs/1502.05698 
         memory networks: http://arxiv.org/abs/1410.3916 
         end-to-end memory networks: http://arxiv.org/abs/1503.08895 
         large-scale qa with memnns: http://arxiv.org/abs/1506.02075 
         reading children   s books: http://arxiv.org/abs/1511.02301 
         evaluating end-to-end dialog:  http://arxiv.org/abs/1511.06931 
         dialog-based language learning: http://arxiv.org/abs/1604.06045 

         data: 

         babi tasks: fb.ai/babi 
         simplequestions dataset (100k questions): fb.ai/babi 
         children   s book test dataset: fb.ai/babi 
         movie dialog dataest: fb.ai/babi 

         code: 

         memory networks: https://github.com/facebook/memnn 
         simulation tasks generator: https://github.com/facebook/babi-tasks 

ram issues 

         how to decide what to write and what not to write in the memory? 
         how to represent knowledge to be stored in memories? 
         types of memory (arrays, stacks, or stored within weights of model), 

when they should be used, and how can they be learnt? 

         how to do fast retrieval of relevant knowledge from memories when 

the scale is huge? 

         how to build hierarchical memories, e.g. multiscale attention? 
         how to build hierarchical reasoning, e.g. composition of functions? 
         how to incorporate forgetting/compression of information? 
         how to evaluate reasoning models? are artificial tasks a good way? 

where do they break down and real tasks are needed? 

         can we draw inspiration from how animal or human memories work? 

          thanks!               

