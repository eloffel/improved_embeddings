deep learning on

graph-structured data 

hidden layer

hidden layer

input

output

relu

relu

   

   

   

thomas kipf, 1 december 2016

recap: deep learning on euclidean data

euclidean data: grids, sequences    

2d grid

1d grid

deep learning on graph-structured data

thomas kipf

2

recap: deep learning on euclidean data

we know how to deal with this:
convolutional neural networks (id98s)

(animation by  
vincent dumoulin)

(source: wikipedia)

or recurrent neural networks (id56s)

(source: christopher olah   s blog)

deep learning on graph-structured data

thomas kipf

3

convolutional neural networks (on grids)

single id98 layer with 3x3 filter:

   

(animation by  
vincent dumoulin)

update for a single pixel: 
    transform neighbors individually 
    add everything up

full update:

deep learning on graph-structured data

thomas kipf

4

graph-structured data

what if our data looks like this?

input

or this:

relu

   

   

real-world examples:
    social networks 
    world-wide-web 
    protein-interaction networks

    telecommunication networks 
    id13s 
       

deep learning on graph-structured data

thomas kipf

5

graphs: definitions

input

graph:

: set of nodes         ,
: set of edges

we can define:
    (adjacency matrix):

(can also be weighted)

model wish list:

    set of trainable parameters 
    trainable in            time 
    applicable even if the input graph changes

deep learning on graph-structured data

thomas kipf

6

spectral graph convolutions

main idea:

use convolution theorem to generalize convolution to graphs.

loosely speaking:
a convolution corresponds to a multiplication in the fourier domain.

graph fourier transform:

[hammond, vandergheynst, gribonval, 2009]

: eigenvectors of graph laplacian

with                                            (normalized graph laplacian)

and                            (its eigen-decomposition)

d: degree matrix

deep learning on graph-structured data

thomas kipf

7

spectral id197

graph convolution:

or:                                                  with

input

spectral id98 on graphs:

[bruna et al., iclr 2014]

limitations:

    calculating      is expensive 
    evaluating           is 
    graph structure has to be fixed

deep learning on graph-structured data

thomas kipf

8

spatial id197 (id197s)

consider this  
undirected graph:

calculate update 
for node in red:

update 
rule:

: neighbor indices
: norm. constant   
   (per edge)

how is this related to spectral id98s on graphs?  
    localized 1st-order approximation of spectral filters [kipf & welling, 2016]

deep learning on graph-structured data

thomas kipf

9

fully vectorized id197s

with                                    or

or treat self-connection in the same way:

with                                            or

is typically sparse

    we can use sparse id127s! 
    efficient               implementation in theano or tensorflow

deep learning on graph-structured data

thomas kipf

10

id197 model architecture
input: feature matrix                        , preprocessed adjacency matrix  

hidden layer

hidden layer

input

output

relu

relu

   

   

   

deep learning on graph-structured data

thomas kipf

11

what does it do? an example.

forward pass through untrained 3-layer id197 model

parameters initialized randomly

2-dim output per node

f(           ) =

[karate club network]

produces (useful?) random embeddings!

deep learning on graph-structured data

thomas kipf

12

add labels and train (semi-supervised)

video also available here: 
http://tkipf.github.io/graph-convolutional-networks

deep learning on graph-structured data

thomas kipf

13

further reading

blog post id197:  
http://tkipf.github.io/graph-convolutional-networks

code on github: 
http://github.com/tkipf/id197

paper (kipf & welling, semi-supervised classification with id197, 2016): 
https://arxiv.org/abs/1609.02907

questions? you can get in touch with me via:

hidden layer

hidden layer

    e-mail: t.n.kipf@uva.nl 
    twitter: @thomaskipf 
    web: http://tkipf.github.io

input

output

relu

relu

   

   

   

interested in thesis projects? get in touch!

deep learning on graph-structured data

thomas kipf

14

convolves,	attends	and	flows	for	action	recognition

videolstm

zhenyang	li
kirill	gavrilyuk
efstratios gavves

mihir jain
cees	snoek	

university	of	amsterdam

the	netherlands

motivation:	internet	of	things	that	video

goal:	action	recognition

understand	what is	happening	where and	when

related	work
deep	learning	for	actions

convnet

ji et	al.	icml10
karpathy et	al.,	cvpr14
simonyan &	zisserman,	nips14
tran	et	al.,	iccv15

3d	convolutions

need	large	amounts	of	data	to	learn	filters

two-stream

learn	spatial	and	temporal	filters	separately

we	propose	a	more	principled	approach	for	learning	
frame-to-frame	appearance	and	motion	transitions.	
we	localize	the	action	as	well.

baccouche et	al.	icann10
donahue	et	al.	/	ng	et	al.		cvpr15
srivastava et	al.	icml15

lstm

lstm	models	sequential	memories	in	the	long	and	short	term
use	convnet fc	vectors as	input,	no	spatial	information	encoded

rather	than	vectorizing a	video	frame,	we	rely	on	convolutions

sharma	et	al.	nips15	/	iclr16

a(ttention)lstm

look	for	best	locations	leading	to	correct	action	classification
stays	close	to	soft-attention	architecture	for	image	captioning	[xu et	al.	icml15],	
vectorizes attention	and	appearance,	and	ignores	the	motion	inside	a	video.

sharma	et	al.	nips15	/	iclr16

a(ttention)lstm

look	for	best	locations	leading	to	correct	action	classification
stays	close	to	soft-attention	architecture	for	image	captioning	[xu et	al.	icml15],	
vectorizes attention	and	appearance,	and	ignores	the	motion	inside	a	video.

we	add	convolutions and	motion for	better	action	classification
we	localize	the	action	as	well.	

our	proposal:	videolstm

model	spatiotemporal	dynamics	of	videos	by
    preserving	spatial	structure	of	the	frames	over	time
    adding	motion-based	attention
    enabling	action	localization	from	action	class	labels	only

videolstm

videolstm convolves,	attends	and	flows	for	action	recognition.

zhenyang li,	efstratios gavves,	mihir jain,	and	cees	snoek.	arxive16.

http://arxiv.org/abs/1607.01794

convolutional	(a)lstm

replace	the	fully	connected	multiplicative	operations	in	an	lstm	
unit	with	convolutional	operations

generate	attention	by	shallow	convnet instead	of	mlp

lstm
unit

mlp

alstm

convolutional	alstm
recurrent cell "   $%
=
$

convnet

$%

convolutional alstm

conv. recurrent cell "   ()
= ()
    (
&
attention &

feature map

!,
attention !

attention	map	is	generated	
by	a	two-layer	convnet

vectorization x

feature map

rgb

flow

mlp

alstm

alstm	vs convolutional	alstm
conv. recurrent cell "   ()
= ()
    (
&
attention &

recurrent cell "   $%
=
$

!,
attention !

convolutional alstm

vectorization x

feature map

convnet

$%

feature map

rgb

flow

convolutional	alstm	preserves	spatial	dimensions	over	time

motion-based	attention

motion	offers	crucial	clue	where	to	attend	in	video

   tennis	swing   

   tennis	swing   

prediction	       tennis	swing   
video	frame	   
attention	map	   

   
motion-based	attention   
   
   

   
   

   
   

   tennis	swing   

   

   

   

   

flow	image	   

motion	information	to	infer	the	attention	in	each	frame

experimental	setup

datasets:	
ucf101,	hmdb51 for	action	classification

comparison	using	similar	designs	and	training	regime:
convnet:	vgg-16	trained	for	both	rgb	frame	and	optical	flow.
lstm:	use	subsequences	of	every	30	frames,	extract	fc7	or	
pool5	features	at	each	frame	as	input.
convolutions:	3x3	kernels	for	input-to-state	and	state-to-state	
transitions	in	lstm,	and	1x1	kernels	to	generate	the	attention	map

experiments

1. what	deep	learning	architecture?

2.

influence	of	motion-based	attention

3. quality	of	action	localization

convnet

id98

id98

id98

   

y1

y2

yn

i

v
d
e
o
	
t
i

m
e

lstm

id98

id98

id98

lstm

lstm

   

lstm

y1

y2

yn

i

v
d
e
o
	
t
i

m
e

convolutional	lstm

id98

id98

clstm

clstm

   

id98

clstm

y1

y2

yn

i

v
d
e
o
	
t
i

m
e

alstm

a

id98

a

id98

a

id98

.

.

lstm

mlp

lstm

   

   

mlp

lstm

.

.

y1

y2

yn

i

v
d
e
o
	
t
i

m
e

convolutional	alstm

a

id98

a

id98

a

id98

.

.

clstm

id98

clstm

   

   

id98

clstm

.

.

y1

y2

yn

i

v
d
e
o
	
t
i

m
e

convolution,	attention	and	flow

convnet

lstm

alstm

convolutional	lstm

convolutional	alstm

77.4

77.5

77.0

77.6

79.6

rgb
flow

60 65 70 75 80 85 90

classification	accuracy	ucf101	(higher	better)

convolution,	attention	and	flow

convnet

lstm

alstm

convolutional	lstm

convolutional	alstm

77.4

75.2

77.5
78.3

77.0

79.5

77.6

80.4

79.6

82.1

rgb
flow

60 65 70 75 80 85 90

classification	accuracy	ucf101	(higher	better)

experiments

1. what	deep	learning	architecture?

2.

influence	of	motion-based	attention

3. quality	of	action	localization

recap:	motion-based	attention

flow

clstm

id98

a

rgb

a

rgb

   

flow

clstm

id98

a

rgb

.

.

clstm

.

.

clstm

   

   

clstm

y1

y2

yn

motion	attention	makes	more	sense

alstm

convolutional	alstm

77.0

78.6
79.6
79.9

appearance-based	attention
motion-based	attention

60

65

70

75

80

85

90

classification	accuracy	ucf101	(higher	better)

alstm

convolutional	alstm

40.9

42.6
43.3

44.8

30

50
classification	accuracy	hmdb51	

35

40

45

motion	attention	makes	more	sense

motion	attention	makes	more	sense

experiments

1. what	deep	learning	architecture?

2.

influence	of	motion-based	attention

3. quality	of	action	localization

temporal	smoothing

qualitative	results

http://isis-data.science.uva.nl/zhenyang/videolstm/localization_examples/

qualitative	results

http://isis-data.science.uva.nl/zhenyang/videolstm/localization_examples/

qualitative	results

http://isis-data.science.uva.nl/zhenyang/videolstm/localization_examples/

conclusions	on	videolstm

promising	deep	vision	architecture	for	action	localization
hardwires	convolutions	in	attention	lstm
derives	attention	from	what	moves	in	video

localization	from	a	video-level	action	class	label	only

http://arxiv.org/abs/1607.01794

siamese	instance	search	for	

tracking

ran	tao,	efstratios gavves,	arnold	smeulders

ran	tao,	efstratios gavves,	arnold	smeulders.	siamese	instance	search	for	tracking.	cvpr,	2016,	las	vegas.

1

(single)	visual	object	tracking

track	the	target   s	positions	over	time	in	a	video,	given	a	starting	box	in	1st	frame

...

...

...

time

2

applications

    surveillance	
    robotics
    human-computer	interaction
    autonomous	driving
    drones

3

tracking	is	hard

    start	from	1	snapshot	of	the	target
    but	the	target	may	change	its	appearance	significantly	due	to	
illumination	variation,	scale	change,	rotation,	etc.	[smeulders	et	al,	
tpami,	2014:	13	hard	aspects]

    track	the	   thing   	in	the	bounding	box	(i.e.	unknown	object)
    unknown	environment

4

how	to	handle	the	appearance	
variations	of	the	target?

5

prevalent	paradigm	in	literature

starting	from	the	1st frame,	learn	and	update	a	target	model	on-the-fly

    target	model:	target/non-target	binary	classifier,	regressor
    update	the	model	using	the	data	inferred	by	the	tracker	itself

6

prevalent	paradigm	in	literature

starting	from	the	1st frame,	learn	and	update	a	target	model	on-the-fly

    target	model:	target/non-target	binary	classifier,	regressor
    update	the	model	using	the	data	inferred	by	the	tracker	itself

the	data	inferred	by	the	tracker	itself	are	not	absolutely	reliable	   drifting

7

the	proposed	tracker:	motivation

since	the	only	reliable	data	is	the	initial	target	region	in	the	first	frame,	the	
proposed	tracker	only	relies	on	the	initial	target.	(no	update)

8

the	proposed	tracker:	motivation

since	the	only	reliable	data	is	the	initial	target	region	in	the	first	frame,	the	
proposed	tracker	only	relies	on	the	initial	target.	(no	update)

then	how	to	handle	the	appearance	variations?

9

the	proposed	tracker:	motivation

since	the	only	reliable	data	is	the	initial	target	region	in	the	first	frame,	the	
proposed	tracker	only	relies	on	the	initial	target.	(no	update)

then	how	to	handle	the	appearance	variations?

certain	objects	change	appearance	over	time	in	a	similar	way.	  
can	we	learn	a	comparison	mechanism	(similarity	metric)	a	priori,	that	is	
robust	against	typical	appearance	variations	an	object	may	have	in	videos?	

10

siamese	instance search	tracker	(sint)

initial	patch

argmax
&

candidate	patches

	    (                                               ,                   &)

predicted	patch

11

siamese	instance search	tracker	(sint)

simply	tracks	the	target	by	retrieving	in	every	frame	the	candidate	most	similar	to	the	
initial	target	in	the	first	frame

    no	online	updating
    no	occlusion	detection
    no	geometric	matching
    no	combination	of	trackers

but	still	delivers	state-of-the-art	tracking	performance	(at	the	publication	time).	

strength	is	from	the	similarity	function	    (4,4) learned	offline	using	siamese	network.

12

siamese	instance search	tracker	(sint)

learn	once on	a	rich	video	dataset	with	box	annotations following	an	
object.	

once	learned,	it	is	applied	as	is, without	any	further	adapting, to	track
any	previously	unseen	targets.	

13

similarity	function	learning

    (    &)

loss

id98

id98

f(.)

    &

f(.)

    6

    (    6)
    &6   {0,1}

marginal	contrastive	loss:

        &,    6,    &6 =12    &6    >+12 1       &6max	(0,           >)
    =         &        (    6) >

similarity	function	(after	learning):

        &,    6 =        &

4        6

14

network	architecture

    region-of-interest	(roi)	pooling	  
process	all	boxes	in	a	frame	in	one	
single	pass	through	the	network

    very	few	max	pooling	   improve	

localization	accuracy

    use	outputs	of	multiple	layers	

(conv4_3,	conv5_3,	fc6)	   to	be	robust	
in	various	situations	(unknown	
environment)

the	two	branches	share	the	parameters.

15

training	pairs

smeulders et	al, visual	tracking:	an	experimental	survey, tpami,	2014.

16

training	pairs

    60,000	pairs	of	frames	for	training,	2,000	pairs	for	validation
    128	pairs	of	boxes	per	pair	of	frames

positive

negative

17

results	on	otb

sint+:	adaptive	sampling	range	[want	et	al,	iccv15]	&	
optical	flow	to	remove	motion	inconsistent	samples

large	potential	to	improve	sint	by	
integrating	advanced	online	components
18

qualitative	results

can	handle	various	types	of	appearance	variations
the	performance	on	subsequent	frames	will	not	be	affected	by	the	mistake	made	on	the	current	frame.	

19

target	re-identification

    in	the	absent	of	any	drifting,	sint	allows	for	target	re-identification	after	the	target	

was	absent	for	a	long	period	of	time,	provided	with	a	sampling	over	the	whole	image.

https://youtu.be/knaxuljyy_q

20

summary

    siamese	instance search	tracker	(sint)

   retrieves	in	every	frame	the	patch	most	similar	to	the	1	original	patch	of	the	target,	nothing	else
   the	strength	is	from	the	matching	function,	learned	offline	generically

    allows	target	re-identification	after	the	target	was	absent	for	a	complete	shot

    establish	a	new	tracking	framework:	it	only	requires	one-time	offline	learning,	and	
once	learned,	it	is	ready	to	track	any	new,	previously	unseen,	targets,	without	any	
online	learning.

21

patrick putzky & max welling

recurrent id136 machines for 
solving inverse problems

recurrent id136 machines in practice

#4

inverse problems

quantity of interest

measurement

forward model

x

inverse model

y

forward model

y = g(x) + n

inverse model

  x = h(y)

inverse problems - examples

(1)

sky

(4)

measurement

   

forw

ard m

odel

(3)

sub-sampling

    -1

l

a
i
t
a
p
s

(2)

r
e
i
r
u
o
f

   

up to 14.4 gigapixels
with thousands of channels

inverse problems - examples

[block et.al, 2007]

inverse problems - examples

[xu et al., 2014]

inverse problems - examples

inverse problems - examples

[dosovitskiy & brox, 2016]

inverse problems - examples

and many more   

bayesian id136

p   (   |y) =

p(y|   )p   (   )

p(y)

   

   

y

iterative bayesian id136

p   (   |y) =

p(y|   )p   (   )

p(y)

choose/learn a prior
for likelihood  p(y|   )

p   (   )

choose id136 method  
iterate

iterative bayesian id136

p   (   |y) =

p(y|   )p   (   )

p(y)

choose/learn a prior
p   (   )
choose id136 method  
for likelihood  p(y|   )

iterate

iterative id136

y

p(y|   )

ry|   

p   (   ) r   

 

   t+1

maximum a posteriori (map) id136

      = arg max

   

p(y|   )p   (   )

gradient ascent

   t+1 =    t +  tr log p(   |y)

=    t +  t(r log p(y|   ) + r log p(   ))
=    t +  t(ry|    + r   )

recurrent id136 machine

y

p(y|   )

ry|   

y

p(y|   )

ry|   

p   (   ) r   

 

   t+1

st

 

   t+1

   t+1 =    t +  t(ry|    + r   )

   t+1 =    t + h (ry|   ,    t, st)

recurrent id136 machines in time

y

ry|x

p(y|x)

ry|x

p(y|x)

p(y|x)

 

x0

x

 

x1

s1

x

x2

s2

x

   

   

   

   

   

   

ry|x

p(y|x)

 

xt

st

x

objective

g( ) =

1
2

nxi=1

txt=1

(x(i)     x(i)
t )

simple super-resolution

time

natural images

200 training images, 481 x 321 pixel each, ~30 megapixel

reconstruction from random projections

y

p(y|   )

ry|   

st

 

   t+1

32 x 32 pixel image patches

fast convergence on all tasks

image denoising

denoising trained on small image patches, generalises to full-sized images 

image denoising

grayscale

rgb

super-resolution

lr

hr

bicubic interpolation

rim

super-resolution

projects: mri

[block et.al, 2007]

projects: content-aware image restoration

projects: deep visualisation

[mahendran & vedaldi, 2014]

[yosinski et al., 2015]

contact

email: patrick.putzky@gmail.com
room: c3.260
openreview: https://openreview.net/forum?id=hksolp9lg

