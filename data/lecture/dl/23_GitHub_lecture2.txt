lecture 2: learning with neural networks

deep learning @ uva

uva deep learning course - efstratios gavves  & max welling

learning with neural networks - page 1

lecture overview

o machine learning paradigm for neural networks

o the id26 algorithm for learning with a neural network

o neural networks as modular architectures

o various neural network modules

o how to implement and check your very own module

introduction on deep learning and neural networks - page 2

uva deep learning course - efstratios gavves  & max welling

the machine 
learning paradigm

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 3

forward computations

o collect annotated data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    forward propagation   

o evaluate predictions

model

data

input:
targets:

    
    

   (        ;     )

    

score/prediction/output
                   (        ;     )

objective/loss/cost/energy

   (    ;            ,    )

(                        ) 2

optimizing neural networks in theory and in practice - page 4

uva deep learning course - efstratios gavves  & max welling

forward computations

o collect annotated data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    forward propagation   

o evaluate predictions

model

data

input:
targets:

    
    

   (        ;     )

    

score/prediction/output
                   (        ;     )

objective/loss/cost/energy

   (    ;            ,    )

(                        ) 2

optimizing neural networks in theory and in practice - page 5

uva deep learning course - efstratios gavves  & max welling

forward computations

o collect annotated data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    forward propagation   

o evaluate predictions

model

data

input:
targets:

    
    

   (        ;     )

    

score/prediction/output
                   (        ;     )

objective/loss/cost/energy

   (    ;            ,    )

(                        ) 2

optimizing neural networks in theory and in practice - page 6

uva deep learning course - efstratios gavves  & max welling

forward computations

o collect annotated data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    forward propagation   

o evaluate predictions

model

data

input:
targets:

    
    

   (        ;     )

    

score/prediction/output
                   (        ;     )

objective/loss/cost/energy

   (    ;            ,    )

(                        ) 2

optimizing neural networks in theory and in practice - page 7

uva deep learning course - efstratios gavves  & max welling

backward computations

o collect gradient data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    id26   

o evaluate predictions model

score/prediction/output

objective/loss/cost/energy

data

input:
targets:

    
    

    

= 1

   (

)

(                        ) 2

optimizing neural networks in theory and in practice - page 8

uva deep learning course - efstratios gavves  & max welling

backward computations

o collect gradient data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    id26   

o evaluate predictions model

data

input:
targets:

    
    

    

score/prediction/output

objective/loss/cost/energy

       (    ;            )

                

= 1

optimizing neural networks in theory and in practice - page 9

uva deep learning course - efstratios gavves  & max welling

backward computations

o collect gradient data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    id26   

o evaluate predictions model

data

input:
targets:

    
    

    

score/prediction/output

objective/loss/cost/energy

                
       

       (    ;            )

                

optimizing neural networks in theory and in practice - page 10

uva deep learning course - efstratios gavves  & max welling

backward computations

o collect gradient data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    id26   

o evaluate predictions model

data

input:
targets:

    
    

       (        )

        

    

score/prediction/output

objective/loss/cost/energy

                
       

       (    ;            )

                

optimizing neural networks in theory and in practice - page 11

uva deep learning course - efstratios gavves  & max welling

backward computations

o collect gradient data

o define model and initialize randomly

o predict based on current model

    in neural network jargon    id26   

o evaluate predictions model

data

input:
targets:

    
    

       (        )

        

    

score/prediction/output

objective/loss/cost/energy

                
       

       (    ;            )

                

optimizing neural networks in theory and in practice - page 12

uva deep learning course - efstratios gavves  & max welling

optimization through id119

o as with many model, we optimize our neural network with id119

    (    +1) =     (    )                        

o the most important component in this formulation is the gradient

o id26 to the rescue

    the backward computations of network return the gradients
    how to make the backward computations

learning with neural networks - page 13

uva deep learning course - efstratios gavves  & max welling

id26

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 14

what is a neural network again?

o a family of parametric, non-linear and hierarchical representation learning 
functions, which are massively optimized with stochastic id119 
to encode domain knowledge, i.e. domain invariances, stationarity.

o              ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )
        :input,       : parameters for layer l,          =        (    ,       ): (non-)linear function

o given training corpus {    ,     } find optimal parameters

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

introduction on neural networks and deep learning - page 15

uva deep learning course - efstratios gavves  & max welling

neural network models

o a neural network model is a series of hierarchically connected functions

o this hierarchies can be very, very complex

forward connections (feedforward architecture)

    
    
    
    
    

   
1
(
    

    

;

    
)

   
2
(
    

    

;

    
)

   
3
(
    

    

;

    
)

   
4
(
    

    

;

    
)

   
5
(
    

    

;

    
)

    
    
    
    

optimizing neural networks in theory and in practice - page 16

uva deep learning course - efstratios gavves  & max welling

neural network models

o a neural network model is a series of hierarchically connected functions

o this hierarchies can be very, very complex

                

   5(        ;     )

   4(        ;     )

interweaved connections
(directed acyclic graph
architecture- dagnn)

   4(        ;     )

   3(        ;     )

   2(        ;     )

   2(        ;     )

   1(        ;     )

                    

optimizing neural networks in theory and in practice - page 17

uva deep learning course - efstratios gavves  & max welling

neural network models

o a neural network model is a series of hierarchically connected functions

o this hierarchies can be very, very complex

    
    
    
    
    

   
1
(
    

    

;

    
)

   
2
(
    

    

;

    
)

   
3
(
    

    

;

    
)

   
4
(
    

    

;

    
)

   
5
(
    

    

;

    
)

    
    
    
    

loopy connections
(recurrent architecture, special care needed)

optimizing neural networks in theory and in practice - page 18

uva deep learning course - efstratios gavves  & max welling

neural network models

o a neural network model is a series of hierarchically connected functions

o this hierarchies can be very, very complex

                
                

   5(        ;     )

   4(        ;     )

   4(        ;     )

   3(        ;     )

   2(        ;     )

   2(        ;     )

   1(        ;     )

                    

    
    
    
    
    

   
1
(
    

    

;

    
)

   
2
(
    

    

;

    
)

   
3
(
    

    

;

    
)

   
4
(
    

    

;

    
)

   
5
(
    

    

;

    
)

    
    
    
    
    
    
    
    

functions     modules

    
    
    
    
    

   
1
(
    

    

;

    
)

   
2
(
    

    

;

    
)

   
3
(
    

    

;

    
)

   
4
(
    

    

;

    
)

   
5
(
    

    

;

    
)

    
    
    
    
    
    
    
    

optimizing neural networks in theory and in practice - page 19

uva deep learning course - efstratios gavves  & max welling

what is a module?

o a module is a building block for our network
o each module is an object/function      =    (    ;     ) that

    contains trainable parameters (    )
    receives as an argument an input     
    and returns an output      based on the activation function        

o the activation function should be (at least)

first order differentiable (almost) everywhere

o for easier/more efficient id26     store

module input    
    easy to get module output fast
    easy to compute derivatives

                

   5(    5;     5)

   5(    5;     5)

   4(    4;     4)

   3(    3;     3)

   2(    2;     2)

   2(    2;     2)

   1(    1;     1)

                    

optimizing neural networks in theory and in practice - page 20

uva deep learning course - efstratios gavves  & max welling

anything goes or do special constraints exist?

o a neural network is a composition of modules (building blocks)

o any architecture works

o if the architecture is a feedforward cascade, no special care

o if acyclic, there is right order of computing the forward computations

o if there are loops, these form recurrent  connections (revisited later)

optimizing neural networks in theory and in practice - page 21

uva deep learning course - efstratios gavves  & max welling

forward computations for neural networks

o simply compute the activation of each module in the network
         =                 ;      , where          =         +1(or          =            1)

                

o we need to know the precise function behind

each module        (    )
o recursive operations

    one module   s output is another   s input

o steps

   5(    5;     5)

   5(    5;     5)

   4(    4;     4)

   3(    3;     3)

    visit modules one by one starting from the data input
    some modules might have several inputs from multiple modules 

   2(    2;     2)

   2(    2;     2)

o compute modules activations with the right order

    make sure all the inputs computed at the right time

   1(    1;     1)

                                     

optimizing neural networks in theory and in practice - page 22

uva deep learning course - efstratios gavves  & max welling

backward computations for neural networks

o simply compute the gradients of each module for our data

data:

                    (                    )

    we need to know the gradient formulation of each module

           (        ;         ) w.r.t. their inputs          and parameters         

       5(    5;     5)

       5(    5;     5)

o we need the forward computations first

    their result is the sum of losses for our input data

       4(    4;     4)

o then take the reverse network (reverse connections)

and traverse it backwards

       3(    3;     3)

o instead of using the id180, we use

their gradients

       2(    2;     2)

       2(    2;     2)

o the whole process can be described very neatly and concisely

with the id26 algorithm

       1(    1;     1)

optimizing neural networks in theory and in practice - page 23

uva deep learning course - efstratios gavves  & max welling

again, what is a neural network again?

o              ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

        :input,       : parameters for layer l,          =        (    ,       ): (non-)linear function

o given training corpus {    ,     } find optimal parameters

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

o to use any id119 based optimization (    (    +1) =     (    +1)             

need the gradients

       
            

,      = 1,     ,     

       
        (    )) we 

o how to compute the gradients for such a complicated function enclosing other 

functions, like         (    )?

introduction on neural networks and deep learning - page 24

uva deep learning course - efstratios gavves  & max welling

chain rule

o assume a nested function,      =           and      =          

o chain rule for scalars     ,     ,     

   

        
        

=

        
        

        
        

o when                 ,                 ,             
   

=       

        
            

            
            

        
            

    gradients from all possible paths

    

    (1)

    (2)

    (1)

    (2)

    (3)

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 25

chain rule

o assume a nested function,      =           and      =          

o chain rule for scalars     ,     ,     

   

        
        

=

        
        

        
        

o when                 ,                 ,             

   

        

             =       

        
            

            
            

    gradients from all possible paths

    

    1

    2

    1

    2

    3

        
        1 =

        
        1

        1
        1+

        
        2

        2
        1

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 26

chain rule

o assume a nested function,      =           and      =          

o chain rule for scalars     ,     ,     

   

        
        

=

        
        

        
        

o when                 ,                 ,             
   

=       

        
            

            
            

        
            

    gradients from all possible paths

    

    1

    2

    1

    2

    3

        
        2 =

        
        1

        1
        2+

        
        2

        2
        2

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 27

chain rule

o assume a nested function,      =           and      =          

o chain rule for scalars     ,     ,     

   

        
        

=

        
        

        
        

o when                 ,                 ,             
   

=       

        
            

            
            

        
            

    gradients from all possible paths

    

    (1)

    (2)

    (1)

    (2)

    (3)

        
        3 =

        
        1

        1
        3+

        
        2

        2
        3

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 28

chain rule

o assume a nested function,      =           and      =          

o chain rule for scalars     ,     ,     

   

        
        

=

        
        

        
        

o when                 ,                 ,             
   

=       

        
            

            
            

        
            

    gradients from all possible paths

    or in vector notation

   

        
        

is the jacobian

        
        

=

        
        

    

   

        
        

    

    (1)

    (2)

    (1)

    (2)

    (3)

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 29

the jacobian

o when             3,             2

               =

        
        

=

         1
         1
         2
         1

         1
         2
         2
         2

         1
         3
         2
         3

learning with neural networks - page 30

uva deep learning course - efstratios gavves  & max welling

chain rule in practice

o f y = sin      ,      =           = 0.5     2

        
        

=

     [sin(    )]

     0.5    2

        

        
= cos 0.5    2         

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 31

id26     chain rule!!!

o the id168    (    ,         ) depends on         , which depends on            1,    , 

which depends on     2: 

             ;     1,   ,l =         (          1        1     ,   1 ,     ,          1 ,       )

o gradients of parameters of layer l     chain rule

               1
               2
o when shortened, we need to two quantities

            
               1

       
            

       
            

=

   

   

           

            
            

       
            

= (

            
            

)       

       
            

gradient of a module w.r.t. its parameters

gradient of loss w.r.t. the module output

optimizing neural networks in theory and in practice - page 32

uva deep learning course - efstratios gavves  & max welling

id26     chain rule!!!

o for 

            
            

in  

       
            

= (

            
            

)       

       
            

we only need the jacobian of the     -th

module output          w.r.t. to the module   s parameters         

o very local rule, every module looks for its own

o since computations can be very local

    graphs can be very complicated
    modules can be complicated (as long as they are differentiable)

optimizing neural networks in theory and in practice - page 33

uva deep learning course - efstratios gavves  & max welling

id26     chain rule!!!

o for 

       
            

in  

       
            

= (

            
            

)       

       
            

we apply chain rule again

        +1 =        +1(        +1;         +1)

       
            

=

            +1
            

    

   

       

            +1

        +1 =         

o we can rewrite 

            +1
            

as gradient of module w.r.t. to input

         =        (        ;         )

    remember, the output of a module is the input for the next one:         =        +1

gradient w.r.t. the module input

       
            

=

            +1
            +1

    

   

       

            +1

optimizing neural networks in theory and in practice - page 34

uva deep learning course - efstratios gavves  & max welling

recursive rule (good for us)!!!

multivariate functions     (    )

o often module functions  depend on multiple input variables

    softmax!
    each output dimension depends on

multiple input dimensions

         =

             

        1 +         2 +         3 ,      = 1,2,3

o for these cases for the 

) we must compute jacobian matrix as         

            
            

(or 

            
            

depends on multiple input          (or         )
,          2
    e.g. in softmax     2 depends on all          1

and           3

, not just on           2

optimizing neural networks in theory and in practice - page 35

uva deep learning course - efstratios gavves  & max welling

diagonal jacobians

o often in modules the output depends only in a single input

    e.g. a sigmoid       =     (    ), or      = tanh(    ), or      = exp(    )

          =         =   

    1
    2
    3

=

  (    1)
  (    2)
  (    3)

o not need for full jacobian, only the diagonal: anyways 

    (    1)(1         (    1))

0

    (    2)(1         (    2))

0
0

        
        

=

        
        

=

0
0

0

    (    3)(1         (    3))

d        
             = 0,     i     j
    (    1)(1         (    1))
    (    2)(1         (    2))
    (    3)(1         (    3))

~

    can rewrite equations as inner products to save computations

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 36

dimension analysis

o to make sure everything is done correctly        dimension analysis   
o the dimensions of the gradient w.r.t.          must be equal to the dimensions 

of the respective weight         

dim

dim

       
            

       
            

= dim         

= dim         

learning with neural networks - page 37

uva deep learning course - efstratios gavves  & max welling

dimension analysis

o for  

       
            

=

            +1
            +1

    

       

            +1

dim          =         

dim          =            1            

[           1] =         +1            

         [        +1   1]

o for  

       
            

=

            
            

   

       
            

    

[           1           ] = [           1   1]     [1            ]

learning with neural networks - page 38

uva deep learning course - efstratios gavves  & max welling

id26: recursive chain rule

o step 1. compute forward propagations for all layers recursively

         =                  and         +1 =         

o step 2. once done with forward propagation, follow the reverse path. 

    start from the last layer and for each new layer compute the gradients
    cache computations when possible to avoid redundant operations

       
            

=

    

            +1
            +1
       
            

o step 3. use the gradients 

   

       
            +1

    

       
            

=

            
            

   

       
            

with stochastic gradient descend to train

id26: recursive chain rule

o step 1. compute forward propagations for all layers recursively

         =                  and         +1 =         

o step 2. once done with forward propagation, follow the reverse path. 

    start from the last layer and for each new layer compute the gradients
    cache computations when possible to avoid redundant operations

vector with dimensions [           1   1]

vector with dimensions [           1]

       
            

=

o step 3. use the gradients 

    

            +1
            +1
       
            

   

       
            +1

    

       
            

=

            
            

   

       
            

with stochastic gradient descend to train

vector with dimensions [1            ]

jacobian matrix with dimensions          +1            

    

vector with dimensions [        +1   1]

matrix with dimensions [           1           ]

dimensionality analysis: an example

o            1 = 15 (15 neurons),          = 10 (10 neurons),         +1 = 5 (5 neurons)
o let   s say          =         
o forward computations

           and         +1 =       +1

         =            1

           +1

               1     15    1 ,          : 10    1 ,         +1 : [5    1]
            : 15    1 ,         +1: 10    1
            : 15    10

o gradients

   

   

       
            
       
            

: 5    10          5    1 = 10    1

    15    1     10    1      = 15    10

intuitive
id26

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 42

id26 in practice

o things are dead simple, just compute per module

        (    ;     )

        (    ;     )

        

        

o then follow iterative procedure

       
            

=

            +1
            +1

    

   

       
            +1

    

       
            

=

            
            

   

       
            

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 43

id26 in practice

o things are dead simple, just compute per module

        (    ;     )

        (    ;     )

        

        

o then follow iterative procedure [remember:          =         +1]

derivatives from layer above

       
            

=

            +1
            +1

    

   

       
            +1

    

       
            

=

            
            

   

       
            

module derivatives

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 44

forward propagation

o for instance, let   s consider our module is the function cos          +     

o the forward computation is simply

import numpy as np
def forward(x):

return np.cos(self.theta*x)+np.pi

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 45

backward propagation

o the id26 for the function cos      +     

import numpy as np
def backward_dx(x):

return -self.theta*np.sin(self.theta*x)

import numpy as np
def backward_dtheta(x):

return -x*np.sin(self.theta*x)

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 46

id26: 
an example

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 47

id26 visualization

     = 3,     3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

1
    1

2
    1

3
    1

4
    1

optimizing neural networks in theory and in practice - page 48

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (    )

forward propagations

compute and store      1=    1(    1)

    3 =    3(    3)
    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

    1 =     (    1    1)

store!!! 

optimizing neural networks in theory and in practice - page 49

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (    )

forward propagations

compute and store      2=    2(    2)

     = 3,     3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

    1 =     (    1    1)
    2 =     (    2    2)

store!!! 

optimizing neural networks in theory and in practice - page 50

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (    )

forward propagations

compute and store      3=    3(    3)

     = 3,     3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

    1 =     (    1    1)
    2 =     (    2    2)
    3 =              3

2

store!!! 

optimizing neural networks in theory and in practice - page 51

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (    )

id26

=         direct computation

       
        3
       
        3

    3=    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example
    3 =         ,     3 =    3(    3) = 0.5              3

2

       
        3

=    (             3)

optimizing neural networks in theory and in practice - page 52

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (    )

id26

       
        2
       
        2

=

=

       
        3
       
        2

   

   

        3
        2
        2
        2

    3 =    3(    3 )

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

stored during forward computations

example

        ,     3 = 0.5              3

2

    3 =     2
    2 =     (    2    2)
       
        2

       
        3

=

=    (             3)

        (    ) =     (    )(1         (    ))

        2
        2

       
        2
       
        2

=     2    (    2    2)(1         (    2    2))

=     2    2(1         2)

=    (             3)

=

       
        2

    2    2(1         2)

optimizing neural networks in theory and in practice - page 53

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (    )

id26

       
        1
       
        1

=

=

       
        2
       
        1

   

   

        2
        1
        1
        1

    3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

        ,     3 = 0.5              3

2

=

        2
        2

    2 =     (    2    2)
    2 =     1
    1 =     (    1    1)
        2
        1
        1
        1
       
        1
       
        1

       
        2
       
        1

=

=

=     2    2(1         2)

=     1    1(1         1)

    2    2(1         2)

    1    1(1         1)

computed from the exact previous 
id26 step (remember, recursive rule)

optimizing neural networks in theory and in practice - page 54

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (     + 1)

forward propagations

compute and store      1=    1(    1)

     = 3,     3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

    1 =     (    1    1)

store!!! 

optimizing neural networks in theory and in practice - page 55

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (     + 1)

forward propagations

compute and store      2=    2(    2)

     = 3,     3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

    1 =     (    1    1)
    2 =     (    2    2)

store!!! 

optimizing neural networks in theory and in practice - page 56

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (     + 1)

forward propagations

compute and store      3=    3(    3)

     = 3,     3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

    1 =     (    1    1)
    2 =     (    2    2)
    3 =              3

2

store!!! 

optimizing neural networks in theory and in practice - page 57

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (     + 1)

id26

=         direct computation

       
        3
       
        3

    3=    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example
    3 =         ,     3 =    3(    3) = 0.5              3

2

       
        3

=    (             3)

optimizing neural networks in theory and in practice - page 58

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (     + 1)

id26

       
        2
       
        2

=

=

       
        3
       
        2

   

   

        3
        2
        2
        2

    3 =    3(    3 )

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

stored during forward computations

example

        ,     3 = 0.5              3

2

    3 =     2
    2 =     (    2    2)
       
        2

       
        3

=

=    (             3)

        (    ) =     (    )(1         (    ))

        2
        2

       
        2
       
        2

=     2    (    2    2)(1         (    2    2))

=     2    2(1         2)

=    (             3)

=

       
        2

    2    2(1         2)

optimizing neural networks in theory and in practice - page 59

uva deep learning course - efstratios gavves  & max welling

id26 visualization at epoch (     + 1)

id26

       
        1
       
        1

=

=

       
        2
       
        1

   

   

        2
        1
        1
        1

    3 =    3(    3)

    3 =    

   

    2 =    2(    2,     2)

    2

    1 =    1(    1,     1)
    1

    1

example

        ,     3 = 0.5              3

2

=

        2
        2

    2 =     (    2    2)
    2 =     1
    1 =     (    1    1)
        2
        1
        1
        1
       
        1
       
        1

       
        2
       
        1

=

=

=     2    2(1         2)

=     1    1(1         1)

    2    2(1         2)

    1    1(1         1)

computed from the exact previous 
id26 step (remember, recursive rule)

optimizing neural networks in theory and in practice - page 60

uva deep learning course - efstratios gavves  & max welling

some practical tricks of the trade

o for classification use cross-id178 loss

o use stochastic id119 on mini-batches

o shuffle training examples at each new epoch

o normalize input variables

        ,     2 = 0,1
         = 0

optimizing neural networks in theory and in practice - page 61

uva deep learning course - efstratios gavves  & max welling

everything is a
module

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 62

neural network models

o a neural network model is a series of hierarchically connected functions

o this hierarchies can be very, very complex

                

   5(        ;     )

   4(        ;     )

   4(        ;     )

   3(        ;     )

   2(        ;     )

   2(        ;     )

   1(        ;     )

                    

    
    
    
    
    

   
1
(
    

    

;

    
)

   
2
(
    

    

;

    
)

   
3
(
    

    

;

    
)

   
4
(
    

    

;

    
)

   
5
(
    

    

;

    
)

    
    
    
    

functions     modules

    
    
    
    
    

   
1
(
    

    

;

    
)

   
2
(
    

    

;

    
)

   
3
(
    

    

;

    
)

   
4
(
    

    

;

    
)

   
5
(
    

    

;

    
)

    
    
    
    

optimizing neural networks in theory and in practice - page 63

uva deep learning course - efstratios gavves  & max welling

linear module

o activation function      =         

o gradient with respect to the input 

        
        

=     

o gradient with respect to the parameters 

        
        

=     

optimizing neural networks in theory and in practice - page 64

uva deep learning course - efstratios gavves  & max welling

sigmoid module

o activation function      =           =

1

1+           

o gradient wrt the input 

=     (    )(1         (    ))

o gradient wrt the input 

=                        1                  

        
        
                 

        

o gradient wrt the parameters
                 

=              (        )(1         (        ))

        

optimizing neural networks in theory and in practice - page 65

uva deep learning course - efstratios gavves  & max welling

sigmoid module     pros and cons

+ output can be interpreted as id203

+ output bounded in [0, 1]     network cannot overshoot

- always multiply with < 1     gradients can be small in deep networks

- the gradients at the tails flat to 0     no serious sgd updates

    overconfident, but not necessarily    correct   
    neurons get stuck

optimizing neural networks in theory and in practice - page 66

uva deep learning course - efstratios gavves  & max welling

tanh module

o activation function      =                      =

                      
        +           

o gradient with respect to the input 

        
        

= 1                    2(    )

o similar to sigmoid, but with different output range

    [   1, +1] instead of  0, +1
    stronger gradients, because data is centered

around 0 (not 0.5)

    less bias to hidden layer neurons as now outputs

can be both positive and negative (more likely
to have zero mean in the end)

optimizing neural networks in theory and in practice - page 67

uva deep learning course - efstratios gavves  & max welling

rectified linear unit (relu) module (alexnet)

o activation function      =    (    ) = max 0,     
0,                   0
1,              > 0

o gradient wrt the input 

        
        

=    

o very popular in id161 and id103

o much faster computations, gradients

    no vanishing or exploding problems, only comparison, addition, multiplication 

o people claim biological plausibility

o sparse activations

o no saturation

o non-symmetric

o non-differentiable at 0

o a large gradient during training can cause a neuron to    die   . higher learning rates mitigate the problem

relu convergence rate

relu
tanh

other relus

o soft approximation (softplus):      =    (    ) = ln 1 +          

o noisy relu:      =          = max 0, x +    ,   ~    (0,   (x))

o leaky relu:      =          =    

    ,               > 0

0.01                                        

o parametric relu:      =          =    

    ,               > 0

                                            

(parameter      is trainable)

softmax module

o activation function     (    ) =                             (    (    )) =

    outputs id203 distribution,       =1
o because         +     =                 , we usually compute

        (    )
               (    )
         (    ) = 1 for      classes

    (    ) =

,      = max         (    ) because

        (    )       
               (    )       
        (    )       
               (    )       

=

                (    )
                        (    ) =

        (    )
               (    )

o avoid exponentianting  large numbers     better stability

optimizing neural networks in theory and in practice - page 71

uva deep learning course - efstratios gavves  & max welling

euclidean loss module

o activation function     (    ) = 0.5               2
    mostly used to measure the loss in regression tasks

o gradient with respect to the input 

        
        

=              

optimizing neural networks in theory and in practice - page 72

uva deep learning course - efstratios gavves  & max welling

cross-id178 loss (log-loss or log-likelihood) module

o activation function           =           =1

         (    ) log     (    ),     (    ) = {0, 1}

o gradient with respect to the input 

        
        (    ) =    

1

    (    )

o the cross-id178 loss is the most popular classification losses for 

classifiers that output probabilities (not id166)

o cross-id178 loss couples well softmax/sigmoid module
    often the modules are combined and joint gradients are computed

o generalization of id28 for more than 2 outputs

optimizing neural networks in theory and in practice - page 73

uva deep learning course - efstratios gavves  & max welling

many, many more modules out there    

o id173 modules

    dropout

o id172 modules

       2-id172,    1-id172

question: when is a id172 module needed?
answer: possibly when combining different modalities/networks (e.g. in 
siamese or multiple-branch networks)
o loss modules

    hinge loss

o and others, which we are going to discuss later in the course

learning with neural networks - page 74

uva deep learning course - efstratios gavves  & max welling

composite 
modules

or    

   make your own 

module   

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 75

id26 again

o step 1. compute forward propagations for all layers recursively

         =                  and         +1 =         

o step 2. once done with forward propagation, follow the reverse path. 

    start from the last layer and for each new layer compute the gradients
    cache computations when possible to avoid redundant operations

       
            

=

    

            +1
            +1
       
            

o step 3. use the gradients 

   

       
            +1

    

       
            

=

            
            

   

       
            

with stochastic gradient descend to train

new modules

o everything can be a module, given some ground rules

o how to make our own module?

    write a function that follows the ground rules

o needs to be (at least) first-order differentiable (almost) everywhere

o hence, we need to be able to compute the

        (    ;    )

        

and 

        (    ;    )

        

learning with neural networks - page 77

uva deep learning course - efstratios gavves  & max welling

a module of modules

o as everything can be a module, a module of modules could also be a 

module

o we can therefore make new building blocks as we please, if we expect 

them to be used frequently

o of course, the same rules for the eligibility of modules still apply

learning with neural networks - page 78

uva deep learning course - efstratios gavves  & max welling

1 sigmoid == 2 modules?

o assume the sigmoid     (    ) operating on top of         

     =     (        )

o directly computing it     complicated id26 equations

o since everything is a module, we can decompose this to 2 modules

    1 =         

    2 =     (    1)

learning with neural networks - page 79

uva deep learning course - efstratios gavves  & max welling

1 sigmoid == 2 modules?

- two id26 steps instead of one

+ but now our gradients are simpler

    algorithmic way of computing gradients
    we avoid taking more gradients than needed in a (complex) non-linearity

    1 =         

    2 =     (    1)

learning with neural networks - page 80

uva deep learning course - efstratios gavves  & max welling

network-in-network [lin et al., arxiv 2013]

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 81

resnet [he et al., cvpr 2016]

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 82

radial basis function (rbf) network module

o rbf module

     =    
    

         exp(           (                 )2)

o decompose into cascade of modules

    1 = (             )2
    2 = exp            1
    3 =         2
    4 =                 (    ,     3

     ,     )

learning with neural networks - page 83

uva deep learning course - efstratios gavves  & max welling

radial basis function (rbf) network module

o an rbf module is good for regression problems, in which cases it is 

followed by a euclidean loss module

o the gaussian centers          can be initialized externally, e.g. with id116

    1 = (             )2
    2 = exp            1
    3 =         2
    4 =                 (    ,     3

     ,     )

learning with neural networks - page 84

uva deep learning course - efstratios gavves  & max welling

an rbf visually

rbf module

    5 =              4

2

    4 =                 (    3)

(1) =     1    2
    3

(1)

(2) =     2    2
    3

(2)

(3) =     3    2
    3

(3)

(1) = exp(       1    1
    2

(1))

(2) = exp(       1    1
    2

(2))

(3) = exp(       1    1
    2

(3))

(1) = (             1)2
    1

(2) = (             1)2
    1

(3) = (             1)2
    1

    1 = (             )2        2 = exp            1         3 =         2         4 =                 (    ,     3

     ,     )

learning with neural networks - page 85

uva deep learning course - efstratios gavves  & max welling

unit tests

uva deep learning course
efstratios gavves  & max welling

optimizing neural networks in theory 
and in practice - page 86

unit test

o always check your implementations

    not only for deep learning

o does my implementation of the              function return the correct values?

    if i execute sin(    /2) does it return 1 as it should

o even more important for gradient functions

    not only our implementation can be wrong, but also our math

o slightest sign of malfunction     always recheck

    ignoring problems never solved problems

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 87

gradient check

original gradient definition: 

        (    )

        

= lim      0

    (    +   )

     

o most dangerous part for new modules     get gradients wrong

o compute gradient analytically

o compute gradient computationally

         (    )    

o compare

          +                            

2    

2

       (    ) =

        (    ;     (    ))

        (    )

             (    )

o is difference in  10   4, 10   7     thengradients are good

learning with neural networks - page 88

uva deep learning course - efstratios gavves  & max welling

gradient check

o perturb one parameter     (    ) at a time with     (    ) +     
o then check        (    )

for that one parameter only

o do not perturb the whole parameter vector      +     

    this will give wrong results (simple geometry)

o sample dimensions of the gradient vector

    if you get a few dimensions of an gradient vector good, all is good
    sample function and bias gradients equally, otherwise you might get your bias wrong

learning with neural networks - page 89

uva deep learning course - efstratios gavves  & max welling

numerical gradients

o can we replace analytical gradients with numerical gradients?

o in theory, yes!

o in practice, no!

    too slow

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 90

be creative!

o what about trigonometric modules?

o or polynomial modules?

o or new loss modules?

learning with neural networks - page 91

uva deep learning course - efstratios gavves  & max welling

summary

o machine learning paradigm for neural networks

o id26 algorithm, backbone for 

training neural networks

o neural network == modular architecture

o visited different modules, saw how to 

implement and check them

uva deep learning course
efstratios gavves  & max welling

introduction on neural networks and 
deep learning - page 92

reading material & references

o http://www.deeplearningbook.org/

    part i: chapter 2-5
    part ii: chapter 6

uva deep learning course     efstratios gavves & max welling

learning with neural networks - page 93

o optimizing deep networks

o which id168s per machine learning task

next lecture

o advanced modules

o deep learning theory

uva deep learning course
efstratios gavves  & max welling

introduction on neural networks and 
deep learning - page 94

