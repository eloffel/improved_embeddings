lecture 9: unsupervised, generative & adversarial networks

deep learning @ uva

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 1

previous lecture

o recurrent neural networks (id56) for sequences

o id26 through time

o vanishing and exploding gradients and remedies

o id56s using long short-term memory (lstm)

o applications of recurrent neural networks

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 2

lecture overview

o latent space data manifolds

o autoencoders and id5

o id82s

o adversarial networks

o self-supervised networks

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 3

the manifold 
hypothesis

uva deep learning course
efstratios gavves
unsupervised, generative
& adversarial networks - 4

data in theory

o one image:  256 256  256  3
    256 height, 256 width, 3 rgb channels, 256 pixel intensities

o each of these images is like the one in the background

o for text the equivalent would be generating random letter sequences

dakndfqblznqrnbecaojdwlzbirnxxbesjntxapkklsndtuwhc

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 5

data in theory

o one image:  256 256  256  3
    256 height, 256 width, 3 rgb channels, 256 pixel intensities

o each of these images is like the one in the background

o for text the equivalent would be generating random letter sequences

qgkhlkjijxskmbisuwephrhudskneyaeajdzhowieyqwhfnago

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 6

data in theory

o one image:  256 256  256  3
    256 height, 256 width, 3 rgb channels, 256 pixel intensities

o each of these images is like the one in the background

o for text the equivalent would be generating random letter sequences

tpxvdcxwgebouyvqaekqzgwvqfuakuhodsapxzbfsizgobtpjb

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 7

data in reality: the manifold hypothesis

o data live in manifolds

o a manifold is a latent structure of much

lower dimensionality
dim                                     dim                
o nobody    forces    the data

to be on the manifold, they
just are

o the manifold occupies only

a tiny fraction of the possible space

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 8

data in reality: the manifold hypothesis

o the trajectory defines

transformations/variances

o rotation

    e.g. the faces turns

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 9

data in reality: the manifold hypothesis

o the trajectory defines

transformations/variances

o rotation

    e.g. the faces turns

o global appearance changes

    e.g., different face

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 10

data in reality: the manifold hypothesis

o the trajectory defines

transformations/variances

o rotation

    e.g. the faces turns

o global appearance changes

    e.g., different face

o or even local appearance change

    e.g., eyes open/closed

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 11

data in reality: the manifold hypothesis

o each image is a either a set of

coordinates in the full data space

image=

0.1
   0.2
0.8
0.3
   0.5
   0.3
0.8
0.1
   0.4

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 12

data in reality: the manifold hypothesis

o or a smaller set of intrinsic

manifold coordinates

image taken from pascal vincent, deep learning summer school, 2015

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 13

so what?

o manifold     data distribution

o learning the manifold     learning data distribution and data variances

o how to learn the these variances automatically?

o unsupervised and/or generative learning

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 14

unsupervised 
and generative
learning

uva deep learning course
efstratios gavves
unsupervised, generative
& adversarial networks - 15

what is unsupervised learning? 

o latent space manifolds

o autoencoders and id5

o id82s

o adversarial networks

o self-supervised networks

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 16

why unsupervised learning?

o much more unlabeled than labeled data

    large data     better models
    ideally not pay for annotations

o what is even the correct annotation for 

learning data distribution and/or data variances

o discovering structure

    what are the important features in the dataset

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 17

what are generative models? 

o latent space manifolds

o autoencoders and id5

o id82s

o adversarial networks

o self-supervised networks

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 18

why generative?

o force models to go beyond surface statistical regularities of existing data

    go beyond direct association of observable inputs to observable outputs
    avoid silly predictions (adversarial examples)

o understand the world

o understand data variances, disentangle and recreate them

o detect unexpected events in your data

o pave the way towards reasoning and decision making

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 19

unsupervised & generative learning of the manifold 

o autoencoders and id5

o id82s

o adversarial networks

o self-supervised networks

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 20

autoencoders

uva deep learning course
efstratios gavves
unsupervised, generative
& adversarial networks - 21

standard autoencoder

o      is usually after a non-linearity

    e.g.      =              +      ,                  , tanh(   )

o reconstruction error    

        ,              

error    

    =    
    

o often error is the euclidean loss

output: reconstruction        

decoder     

latent space     

encoder     

    =                  2

input:     

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 22

standard autoencoder

o the latent space should have fewer dimensions than input

    undercomplete representation
    bottleneck architecture

o otherwise (overcomplete) autoencoder might learn the identity function

                          =              = 0

    assuming no id173
    often in practice still works though

o also, if      =          +      (linear) autoencoder learns same subspace as pca

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 23

stacking autoencoders

o stacking layers on top of each other 

o bigger depth     higher level abstractions

o slower training 

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 24

denoising autoencoder

output: reconstruction         

o add random noise to input

    dropout, gaussian

o loss includes expectation

over noise distribution

error    

    =    
    

      (        |    )        ,                 

corrupted input:         

noise     :     (        |    ,     )

input:     

decoder     

latent space     

encoder     

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 25

denoising autoencoder

o the network does not overlearn the data

    can even use overcomplete latent spaces

o model forced to learn more intelligent, robust representations

    learn to ignore noise or trivial solutions(identity)
    focus on    underlying    data generation process

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 26

stochastic contractive autoencoders

o instead of invariant to input noise, invariant output representations

    similar input with different sampled noise     similar outputs

o id173 term over noisy reconstructions

        ,                 

+           (        |    )[                          2]

    =    
    

reconstruction error

id173 error

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 27

stochastic contractive autoencoders

o instead of invariant to input noise, invariant output representations

    similar input with different sampled noise     similar outputs

o id173 term over noisy reconstructions

o trivial solution?

        ,                 

+           (        |    )[                          2]

    =    
    

reconstruction error

id173 error

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 28

stochastic contractive autoencoders

o instead of invariant to input noise, invariant output representations

    similar input with different sampled noise     similar outputs

o id173 term over noisy reconstructions

o trivial solution?

             = 0
    
    to avoid make sure the encoder and decoder weights are shared          =         

        ,                 

+           (        |    )[                          2]

    =    
    

reconstruction error

id173 error

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 29

stochastic     analytic id173

o taylor series expansion about a point      =     

         =          +                           + 0.5                              2 +    

o for      =         =      +     , where     ~    (0,     2    ) the (first order) taylor

            =          +      =          +

    h
        

    

      (        |    )[                          2] =

2

    h
        

    

        2     h
        

2

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 30

stochastic     analytic id173

o taylor series expansion about a point      =     

         =          +                           + 0.5                              2 +    

o for      =         =      +     , where     ~    (0,     2    ) the (first order) taylor

            =          +      =          +

    h
        

    

      (        |    )[                          2] =

2

    h
        

    

        2     h
        

2

o higher order expansions also possible although computationally expensive

    alternative: compute higher order derivatives stochastically
    analytic and stochastic id173

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 31

some geometric intuition

o gradients show sensitivity of function around a point
2

        ,                 

+     

    =    
    

    h
        

o id173 term penalizes sensitivity to all directions

o reconstruction term enforces sensitivity only to direction of manifold

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 32

some geometric intuition: let   s get real

o let   s try to check where the gradients are sensitive around our manifold

o gradients     jacobian 

o strongest directions of jacobian     compute svd of jacobian

       
        

=                 

o take strongest singular values in      and respective vectors from u

    these are our strongest tangents     directions of jacobian of most sensitivity

o hopefully the tangents sensitive to direction of the manifold, not random

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 33

some geometric intuition: let   s get real

o let   s try to check where the gradients are sensitive around our manifold

o gradients     jacobian 

o strongest directions of jacobian     compute svd of jacobian

       
        

=                 

o take strongest singular values in      and respective vectors from u

    these are our strongest tangents     directions of jacobian of most sensitivity

o hopefully the tangents sensitive to direction of the manifold, not random

o how to check? 

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 34

some geometric intuition: let   s get real

o let   s try to check where the gradients are sensitive around our manifold

o gradients     jacobian 

o strongest directions of jacobian     compute svd of jacobian

       
        

=                 

o take strongest singular values in      and respective vectors from u

    these are our strongest tangents     directions of jacobian of most sensitivity

o hopefully the tangents sensitive to direction of the manifold, not random

o how to check? visualize!!

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 35

some geometric intuition: let   s get real

o let   s try to check where the gradients are sensitive around our manifold

o gradients     jacobian 

o strongest directions of jacobian     compute svd of jacobian

       
        

=                 

o take strongest singular values in      and respective vectors from u

    these are our strongest tangents     directions of jacobian of most sensitivity

o hopefully the tangents sensitive to direction of the manifold, not random

o how to check? visualize!!

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 36

strongest tangents

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 37

variational autoencoder

o we want to model the data distribution

          =                                              

o posterior                    is intractable for complicated likelihood functions 

                   , e.g. a neural network               is also intractable

o introduce an id136 machine                    (e.g. another neural network) 

that learns to approximate the posterior                   
    since we cannot know                    define a variational lower bound to optimize instead

        ,     ,      =                                                    +                        (log         (    |    ))
reconstruction term

id173 term

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 38

variational autoencoder

o since we cannot know                    define a variational lower bound to 

optimize instead

o optimize id136 machine                    and likelihood machine         (    |    )
                       should follow a gaussian distribution

o reparamerization trick

    instead of      being stochastic, introduce stochastic noise variable     
    then, think of      as deterministic, where        =               +                  
    simultaneous optimization of                    and         (    |    ) now possible

        ,     ,      =                                                    +                        (log         (    |    ))
reconstruction term

id173 term

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 39

examples

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 40

adversarial 
networks

uva deep learning course
efstratios gavves
unsupervised, generative
& adversarial networks - 41

id3

o so far we were trying to express the pdf of the data     (    )

o why bother? is that really necessary? is that limiting?

o can we model directly the sampling of data?

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 42

id3

o so far we were trying to express the pdf of the data     (    )

o why bother? is that really necessary? is that limiting?

o can we model directly the sampling of data?

o yes, by modelling sampling as a    thief vs. police    game

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 43

id3

o composed of two successive networks

discriminator

    generator network (like upper half of autoencoders)
    discriminator network (like a convent)

o learning

    sample    noise    vectors     
    per      the generator produces a sample     
    make a batch where half samples are real,

half are the generated ones

    the discriminator needs to predict what is real

and what is fake

generator

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 44

noise     

   police vs thief   

o generator and discriminator networks optimized together

    the generator (thief) tries to fool the discriminator
    the discriminator (police) tries to not get fooled by the generator

o mathematically

min

    

max

    

    (    ,     ) =         ~                    (    ) log     (    ) +         ~        (    ) log(1                    )

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 45

a graphical interpretation

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 46

discriminator

adversarial network hardships

o this is (very) fresh research

o as such the theory behind adversarial

networks is by far not mature

o moreoever, gans are quite unstable

o optimal solution is a saddle point

    easy to mess up, little stability

o solutions?

    not great ones yet, although several 

researchers are actively working on that

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 47

generator

examples of generated images

bedrooms

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 48

image    arithmetics   

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 49

summary

o latent space data manifolds

o autoencoders and id5

o id82s

o adversarial networks

o self-supervised networks

uva deep learning course
efstratios gavves
unsupervised, generative
& adversarial networks - 51

reading material & references

o http://www.deeplearningbook.org/

    part ii: chapter 10

o excellent blog post on id26 through time

    http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-

id56s/

    http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-

language-model-id56-with-python-numpy-and-theano/

    http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-

through-time-and-vanishing-gradients/

o excellent blog post explaining lstms

    http://colah.github.io/posts/2015-08-understanding-lstms/

[pascanu2013] pascanu, mikolov, bengio. on the difficulty of training recurrent neural networks, jmlr, 2013

uva deep learning course     efstratios gavves                                                                                    

unsupervised, generative & adversarial networks - 52

next lecture

o memory networks

o advanced applications of recurrent and 

memory networks

uva deep learning course
efstratios gavves
unsupervised, generative
& adversarial networks - 53

