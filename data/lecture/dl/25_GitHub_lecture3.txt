lecture 3: deeper into deep learning and optimizations

deep learning @ uva

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 1

previous lecture

o machine learning paradigm for neural networks

o id26 algorithm, backbone for training neural networks

o neural network == modular architecture

o visited different modules, saw how to implement and check them

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 2

lecture overview

o how to define our model and optimize it in practice

o id174 and id172

o optimization methods

o id173s

o architectures and architectural hyper-parameters

o learning rate

o weight initializations

o good practices

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 3

deeper into
neural networks &
deep neural nets

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 4

a neural/deep network in a nutshell

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 5

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

sgd vs gd

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 6

id26 again

o step 1. compute forward propagations for all layers recursively

         =                  and         +1 =         

o step 2. once done with forward propagation, follow the reverse path. 

    start from the last layer and for each new layer compute the gradients
    cache computations when possible to avoid redundant operations

       
            

=

    

            +1
            +1
       
            

o step 3. use the gradients 

   

       
            +1

    

       
            

=

            
            

   

       
            

with stochastic gradient descend to train

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 7

still, id26 can be slow

o often loss surfaces are

    non-quadratic
    highly non-convex
    very high-dimensional

o datasets are typically really large to compute complete gradients

o no real guarantee that 

    the final solution will be good
    we converge fast to final solution
    or that there will be convergence 

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 8

stochastic gradient descend (sgd)

o stochastically sample    mini-batches    from dataset     

    the size of          can contain even just 1 sample

         =                         (    )

    (    +1) =     (    )    

        
|        |

   

                 

               

o much faster than gradient descend
o results are often better
o also suitable for datasets that change over time
o variance of gradients increases when batch size decreases

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 9

sgd is often better

current solution

full gd gradient

noisy sgd gradient

new gd solution

best gd solution

loss surface

    no guarantee that this is what

is going to always happen.

    but the noisy sgc gradients 
can help some times escaping 
local optima

best sgd solution

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 10

sgd is often better

o (a bit) noisy gradients act as id173

o gradient descend     complete gradients

o complete gradients fit optimally the (arbitrary) data we have, not the 

distribution that generates them
    all training samples are the    absolute representative    of the input distribution
    test data will be no different than training data
    suitable for traditional optimization problems:    find optimal route   
    but for ml we cannot make this assumption     test data are always different

o stochastic gradients     sampled training data sample roughly 

representative gradients
    model does not overfit to the particular training samples

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 11

sgd is faster

gradient

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 12

sgd is faster

10x

gradient

what is our 
gradient now?

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 13

sgd is faster

10x

gradient

what is our 
gradient now?

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 14

sgd is faster

o of course in real situations data do not replicate

o however, after a sizeable amount of data there are clusters of data that 

are similar

o hence, the gradient is approximately alright

o approximate alright is great, is even better in many cases actually

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 15

sgd for dynamically changed datasets

o often datasets are not    rigid   
o imagine instagram

    let   s assume 1 million of new images uploaded per week and 

we want to build a    cool picture    classifier

popular today

    should    cool pictures    from the previous year have the same as 

much influence?

    no, the learning machine should track these changes

o with gd these changes go undetected, as results are 

averaged by the many more    past    samples
    past    over-dominates   

o a properly implemented sgd can track changes much 

better and give better models
    [lecun2002]

popular in 2014

popular in 2010

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 16

shuffling examples

dataset

o applicable only with sgd
o choose samples with maximum information content
o mini-batches should contain examples from different classes

    as different as possible

o prefer samples likely to generate larger errors
    otherwise gradients will be small     slower learning
    check the errors from previous rounds and prefer    hard examples   
    don   t overdo it though :p, beware of outliers

shuffling 
at epoch t

o in practice, split your dataset into mini-batches
    each mini-batch is as class-divergent and rich as possible
    new epoch     to be safe new batches & new, randomly shuffled examples

shuffling 
at epoch t+1

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 17

advantages of gradient descend batch learning

o conditions of convergence well understood

o acceleration techniques can be applied

    second order (hessian based) optimizations are possible
    measuring not only gradients, but also curvatures of the loss surface

o simpler theoretical analysis on weight dynamics and convergence rates

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 18

in practice

o sgd is preferred to gradient descend

o training is orders faster

    in real datasets gradient descend is not even realistic

o solutions generalize better

    more efficient     larger datasets
    larger datasets     better generalization

o how many samples per mini-batch?

    hyper-parameter, trial & error
    usually between 32-256 samples

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 19

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

id174 & 
id172

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 20

data pre-processing

o center data to be roughly 0

    id180 usually    centered    around 0
    convergence usually faster
    otherwise bias on gradient direction     might slow down learning

relu    

tanh(    )    

    (    )    

   

   

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 21

data pre-processing

o scale input variables to have similar diagonal covariances          =       (        
    similar covariances     more balanced rate of learning for different weights
    rescaling to 1 is a good choice, unless some dimensions are less important

(    ))2

     =     1,     2,     3     ,      =     1,     2,     3     ,      = tanh(          )

    1,     2,     3     much different covariances

    1

    3

    2

generated gradients

d   

   

             1,    2,    3 : much different

gradient update harder:      (    +1) =     (    )             

       /      1
       /      2
       /      3

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 22

data pre-processing

o input variables should be as decorrelated as possible

    input variables are    more independent   
    network is forced to find non-trivial correlations between inputs
    decorrelated inputs     better optimization
    obviously not the case when inputs are by definition correlated (sequences)

o extreme case

    extreme correlation (linear dependency) might cause problems [caution]

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 23

id172:          ,     2 =      0, 1

o input variables follow a gaussian distribution (roughly)

o in practice: 

    from training set compute mean and standard deviation
    then subtract the mean from training samples
    then divide the result by the standard deviation

    

             

             

    

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 24

         ,     2 =      0, 1     making things faster

o instead of    per-dimension        all input dimensions simultaneously

o if dimensions have similar values (e.g. pixels in natural images)

    compute one      ,     2 instead of as many as the input variables
    or the per color channel pixel average/variance
,                         ,                         

,                     ,                     

                ,                 

2

2

2

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 25

even simpler: centering the input

o when input dimensions have similar ranges    

o     and with the right non-linearlity    

o     centering might be enough

    e.g. in images all dimensions are pixels
    all pixels have more or less the same ranges

o juse make sure images have mean 0 (     = 0)

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 26

pca whitening

o if      the covariance matrix of your dataset, compute 

eigenvalues and eigenvectors with svd 

    ,   ,          =             (    )

o decorrelate (pca-ed) dataset by

                 =             

    subset of eigenvectors         = [    1,     ,         ] to reduce data dimensions

                 =             

o scaling by square root of eigenvalues to whiten data

                =                 /   

o not used much with convolutional neural nets

    the zero mean id172 is more important

                =                 /   

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 27

example

images taken from a. karpathy course website: http://cs231n.github.io/neural-networks-2/

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 28

data augmentation [krizhevsky2012]

flip

random crop

original

contrast

tint

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 29

batch id172 [ioffe2015]

o weights change     the 

distribution of the layer inputs 
changes per round
    covariance shift

        
o normalize the layer inputs with 

batch id172
    roughly speaking, normalize          to 

    (0, 1) and rescale

   

   

        

batch id172

id26

batch id172

layer l input distribution at (t)

layer l input distribution at (t+0.5)

layer l input distribution at (t+1)

        

        

        

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 30

batch id172 - intuitively

id26

batch id172

layer l input distribution at (t)

layer l input distribution at (t+0.5)

layer l input distribution at (t+1)

        

        

        

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 31

batch id172     the algorithm

      =1

             

o            

o            

o                

1
    
1
    
                  

2+    

       

      =1

                         

[compute mini-batch mean]

2

[compute mini-batch variance]

[normalize input]

o                              +     

[scale and shift input]

trainable parameters

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 32

batch id172 - benefits

o gradients can be stronger     higher learning rates     faster training

    otherwise maybe exploding or vanishing gradients or getting stuck to local minima

o neurons get activated in a near optimal    regime   

o better model id173

    neuron activations not deterministic,

depend on the batch

    model cannot be overconfident

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 33

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

id173

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 34

id173

o neural networks typically have thousands, if not millions of parameters

    usually, the dataset size smaller than the number of parameters

o overfitting is a grave danger
o proper weight id173 is crucial to avoid overfitting

          arg min        

   (    ,              ;     1,   ,l ) +       (    )

o possible id173 methods

(    ,    )   (    ,    )

       2-id173
       1-id173
    dropout

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 35

   2-id173 

o most important (or most popular) id173

          arg min        

   (    ,              ;     1,   ,l ) +

(    ,    )   (    ,    )

    
2

   
    

2

        

o the    2-id173 can pass inside the gradient descend update rule

    (    +1) =     (    )                          +                 
         +1 = 1                                                   

o      is usually about 10   1, 10   2

   weight decay   , because 
weights get smaller

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 36

   1-id173 

o    1-id173 is one of the most important techniques

          arg min        

   (    ,              ;     1,   ,l ) +

(    ,    )   (    ,    )

    
2

   
    

        

o also    1-id173 passes inside the gradient descend update rule

         +1 =                           

o    1-id173     sparse weights
                 more weights become 0

         
|          |

                       

sign function

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 37

early stopping

o to tackle overfitting another popular technique is early stopping

o monitor performance on a separate validation set

o training the network will decrease training error, as well validation error 

(although with a slower rate usually)

o stop when validation error starts increasing
    this quite likely means the network starts to overfit

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 38

dropout [srivastava2014]

o during training setting activations randomly to 0

    neurons sampled at random from a bernoulli distribution with      = 0.5

o at test time all neurons are used

    neuron activations reweighted by     

o benefits

    reduces complex co-adaptations or co-dependencies between neurons
    no    free-rider    neurons that rely on others
    every neuron becomes more robust
    decreases significantly overfitting
    improves significantly training speed

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 39

dropout

o effectively, a different architecture at every training epoch

    similar to model ensembles

original model

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 40

dropout

o effectively, a different architecture at every training epoch

    similar to model ensembles

epoch 1

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 41

dropout

o effectively, a different architecture at every training epoch

    similar to model ensembles

epoch 1

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 42

dropout

o effectively, a different architecture at every training epoch

    similar to model ensembles

epoch 2

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 43

dropout

o effectively, a different architecture at every training epoch

    similar to model ensembles

epoch 2

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 44

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

architectural details

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 45

sigmoid-like id180

o straightforward sigmoids not a very good idea

o symmetric sigmoids converge faster

    e.g. tanh, returns a(x=0)=0

    recommended sigmoid:      =          = 1.7159 tanh(

2
3

    )

o you can add a linear term to avoid flat areas
     =          = tanh      +         

tanh      + 0.5    

tanh     

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 46

rbfs vs    sigmoids   

o rbf:      =          =                 exp                              

2

o sigmoid:      =          =           =

1

1+           

o sigmoids can cover the full feature space

o rbf   s are much more local in the feature space

    can be faster to train but with a more limited range
    can give better set of basis functions
    preferred in lower dimensional spaces

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 47

rectified linear unit (relu) module [krizhevsky2012]

o activation function      =    (    ) = max 0,     
0,                   0
1,              > 0

o gradient wrt the input 

        
        

=    

o very popular in id161 and id103

o much faster computations, gradients

    no vanishing or exploding problems, only comparison, addition, multiplication 

o people claim biological plausibility

o sparse activations

o no saturation

o non-symmetric

o non-differentiable at 0

o a large gradient during training can cause a neuron to    die   . higher learning rates mitigate the problem

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 48

relu convergence rate

relu
tanh

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 49

other relus

o soft approximation (softplus):      =    (    ) = ln 1 +          

o noisy relu:      =          = max 0, x +    ,   ~    (0,   (x))

o leaky relu:      =          =    

    ,               > 0

0.01                                        

o parametric relu:      =          =    

    ,               > 0

                                            

(parameter      is trainable)

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 50

architectural hyper-parameters

o number of hidden layers

o number of neuron in each hidden layer

o type of id180

o type and amount of id173

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 51

number of neurons, number of hidden layers

o dataset dependent hyperparameters

o tip: start small     increase complexity gradually

    e.g. start with a 2-3 hidden layers
    add more layers     does performance improve?
    add more neurons     does performance improve?

o id173 is very important, use    2
    even if with very deep or wide network
    with strong    2-id173 we avoid overfitting

n
o
i
t
a
z
i
l

a
r
e
n
e
g

model complexity

(number of neurons)

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 52

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

learning rate

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 53

learning rate

o the right learning rate          very important for fast convergence
    too strong     gradients overshoot and bounce
    too weak,     too small gradients     slow training

o learning rate per weight is often advantageous

    some weights are near convergence, others not

o rule of thumb

    learning rate of (shared) weights prop. to square root of share weight connections

o adaptive learning rates are also possible, based on the errors observed

    [sompolinsky1995]

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 54

learning rate schedules

o constant

    learning rate remains the same for all epochs

o step decay

    decrease (e.g.         /     or         /    ) every t number of epochs

    0
1+        

o inverse decay          =
o exponential decay          =     0               
o often step decay preferred

    simple, intuitive, works well and only a

single extra hyper-parameter      (     =2, 10)

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 55

learning rate in practice

o try several log-spaced values 10   1, 10   2, 10   3,     on a smaller set
    then, you can narrow it down from there around where you get the lowest error

o you can decrease the learning rate every 10 (or some other value) full 

training set epochs
    although this highly depends on your data

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 56

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

weight initialization

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 57

weight initialization

o there are few contradictory requirements

o weights need to be small enough

    around origin (    ) for symmetric functions (tanh, sigmoid)
    when training starts better stimulate id180 near their linear regime
    larger gradients     faster training

large gradients
large gradients

o weights need to be large enough

    otherwise signal is too weak for any serious learning

linear regime
linear regime

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 58

weight initialization

o weights must be initialized to preserve the variance of the activations during 

the forward and backward computations
    especially for deep learning
    all neurons operate in their full capacity
question: why similar input/output variance?

o good practice: initialize weights to be asymmetric

    don   t give save values to all weights (like all     )
    in that case all neurons generate same gradient     no learning

o generally speaking initialization depends on

    non-linearities
    data id172

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 59

weight initialization

o weights must be initialized to preserve the variance of the activations during 

the forward and backward computations
    especially for deep learning
    all neurons operate in their full capacity
question: why similar input/output variance?
answer: because the output of one module is the input to another

o good practice: initialize weights to be asymmetric

    don   t give save values to all weights (like all     )
    in that case all neurons generate same gradient     no learning

o generally speaking initialization depends on

    non-linearities
    data id172

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 60

weight initialization

o weights must be initialized to preserve the variance of the activations during 

the forward and backward computations
    especially for deep learning
    all neurons operate in their full capacity
question: why similar input/output variance?
answer: because the output of one module is the input to another

o good practice: initialize weights to be asymmetric

    don   t give save values to all weights (like all     )
    in that case all neurons generate same gradient     no learning

o generally speaking initialization depends on

    non-linearities
    data id172

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 61

one way of initializing sigmoid-like neurons

o for tanh initialize weights from     

6

           1+        

,

6

           1+        

               1 is the number of input variables to the tanh layer and          is the number of the 

output variables

o for a sigmoid     4    

6

           1+        

, 4    

6

           1+        

large gradients

linear regime

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 62

xavier initialization [glorot2010]

o for      =          the variance is

                  =           2                  + e      2                  +                                    

o since           =           = 0

                  =                                                                                             

o for                   =                                             =

1
    

o draw random weights from

    ~     0, 1/    

where      is the number of neurons in the input

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 63

[he2015] initialization for relus

o unlike sigmoids, relus ground to 0 the linear activations half the 

time

o double weight variance

    compensate for the zero flat-area    
    input and output maintain same variance
    very similar to xavier initialization

o draw random weights from

where      is the number of neurons in the input

w~     0, 2/    

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 64

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

id168s

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 65

multi-class classification

o our samples contains only one class

    there is only one correct answer per sample

o negative log-likelihood (cross id178) + softmax
        ;     ,      =           =1
o hierarchical softmax when c is very large
o hinge loss (aka id166 loss)

    
              log         

for all classes      = 1,     ,     

    

is it a cat? is it a horse?    

        ;     ,      =    
    =1
           

max(0,         

                 

     + 1)

o squared hinge loss

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 66

multi-class, multi-label classification

o each sample can have many correct answers
o hinge loss and the likes

    also sigmoids would also work

o each output neuron is independent

       does this contain a car, yes or no?   
       does this contain a person, yes or no?   
       does this contain a motorbike, yes or no?   
       does this contain a horse, yes or no?   

o instead of    is this a car, motorbike or person?   

                          ) = 0.55,          /                     ) = 0.25,                                   ) = 0.15,                              ) = 0.05
                          ) +          /                     ) +                                   ) +                              ) = 1.0

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 67

regression

o the good old euclidean loss

        ;     ,      =

1
2

2
|                 |2

o or rbf on top of euclidean loss

        ;     ,      =    
    

         exp(           (                 )2)

o or    1 distance

        ;     ,      =    

    |
|                     

    

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 68

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

even better 
optimizations

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 69

gradient + momentum

loss surface

momentum

o don   t switch gradients all the time
o maintain    momentum    from previous 

parameters

gradient

         =         (    )                        

    (    +1) =     (    ) +         

o more robust gradients and learning    

faster convergence

o nice    physics   -based interpretation

    instead of updating the position of the    ball   , we 

update the velocity, which updates the position

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 70

nesterov momentum [sutskever2013]

o use the future gradient instead of 

the current gradient

momentum

         =         (    )                        

    (    +1) =     (    ) +         

o better theoretical convergence

o generally works better with 

convolutional neural networks

momentum

look-ahead gradient 
from the next step

gradient + momentum

gradient

gradient + nesterov 
momentum

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 71

second order optimization

o normally all weights updated with same    aggressiveness   

    often some parameters could enjoy more    teaching   
    while others are already about there

o adapt learning per parameter

    (    +1) =     (    )            

   1                   

o         is the hessian matrix of    : second-order derivatives

         =

       

       

                        

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 72

second order optimization methods in practice

o inverse of hessian usually very expensive

    too many parameters

o approximating the hessian, e.g. with the l-bfgs algorithm

    keeps memory of gradients to approximate the inverse hessian

o l-bfgs works alright with gradient descend. what about sgd?

o in practice sgd with some good momentum works just fine

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 73

other per-parameter adaptive optimizations

o adagrad [duchi2011]

o rmsprop

o adam [kingma2014]

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 74

adagrad [duchi2011]

o schedule

             =       (               )2         (    +1) =     (    )             
         is a small number to avoid division with 0
    gradients become gradually smaller and smaller

           
    +    

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 75

rmsprop

o schedule

decay hyper-parameter

       1(        

             =            =1
        (    +1) =     (    )             

           
    +    

(    )       )2 + 1                  

(    )           

o moving average of the squared gradients

    compared to adagrad

o large gradients, e.g. too    noisy    loss surface

    updates are tamed

o small gradients, e.g. stuck in flat loss surface ravine

    updates become more aggressive

square rooting boosts small values 
while suppresses large values

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 76

adam [kingma2014]

o one of the most popular learning algorithms
(               )2

         =    
    

    (    +0.5) =     1    (    ) + 1         1            
    (    +0.5) =     2    (    ) + 1         2     
    (    +1) =     (    )             

    (    +0.5)

    (    +0.5) +     

o similar to rmsprop, but with momentum
o recommended values:     1 = 0.9,     2 = 0.999,      = 10   8

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 77

visual overview

picture credit: alec radford

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 78

learning    not computing    the gradients 

o learning to learn by id119 by id119

    [andrychowicz2016]
o     (    +1) =     (    ) +                     ,     
o          is an    optimizer    with its own parameters     
    implemented as a recurrent network

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 79

good practice

o preprocess the data to at least have 0 mean

o initialize weights based on activations functions

    for relu xavier or heiccv2015 initialization

o always use    2-id173 and dropout
o use batch id172

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 80

1. the neural network

             ;     1,   ,l =         (          1        1     ,   1 ,          1 ,       )

2. learning by minimizing empirical error

          arg min        

   (    ,              ;     1,   ,l )

(    ,    )   (    ,    )

3. optimizing with gradient descend based methods

    (    +1) =     (    )                        

babysitting
deep nets

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 81

babysitting deep nets

o always check your gradients if not computed automatically

o check that in the first round you get a random loss

o check network with few samples

    turn off id173. you should predictably overfit and have a 0 loss
    turn or id173. the loss should increase

o have a separate validation set

    compare the curve between training and validation sets
    there should be a gap, but not too large

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 82

o how to define our model and optimize it in practice

o id174 and id172

summary

o optimization methods

o id173s

o architectures and architectural hyper-parameters

o learning rate

o weight initializations

o good practices

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 83

reading material & references

o http://www.deeplearningbook.org/

    part ii: chapter 7, 8

[andrychowicz2016] andrychowicz, denil, gomez, hoffman, pfau, schaul, de freitas, learning to learn by id119 by gradient 
descent, arxiv, 2016
[he2015] he, zhang, ren, sun. delving deep into rectifiers: surpassing human-level performance on id163 classification, iccv, 2015
[ioffe2015] ioffe, szegedy. batch id172: accelerating deep network training by reducing internal covariate shift, arxiv, 2015
[kingma2014] kingma, ba. adam: a method for stochastic optimization, arxiv, 2014
[srivastava2014] srivastava, hinton, krizhevsky, sutskever, salakhutdinov. dropout: a simple way to prevent neural networks from 
overfitting, jmlr, 2014
[sutskever2013] sutskever, martens, dahl, hinton. on the importance of initialization and momentum in deep learning, jmlr, 2013
[bengio2012] bengio. practical recommendations for gradient-based training of deep architectures, arxiv, 2012
[krizhevsky2012] krizhevsky, hinton. id163 classification with deep convolutional neural networks, nips, 2012 
[duchi2011] duchi, hazan, singer. adaptive subgradient methods for online learning and stochastic optimization, jmlr, 2011
[glorot2010] glorot, bengio. understanding the difficulty of training deep feedforward neural networks, jmlr, 2010
[lecun2002]

uva deep learning course     efstratios gavves & max welling - deeper into deep learning and optimizations - 84

o what are the convolutional neural networks?

o why are they important in id161?

next lecture

o differences from standard neural networks

o how to train a convolutional neural network?

uva deep learning course
efstratios gavves & max welling
deeper into deep learning and optimizations - 85

