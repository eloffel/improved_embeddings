lecture 8: recurrent neural networks

deep learning @ uva

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 1

previous lecture

o word and language representations

o from id165s to neural networks

o id97

o skip-gram

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 2

lecture overview

o recurrent neural networks (id56) for sequences

o id26 through time

o vanishing and exploding gradients and remedies

o id56s using long short-term memory (lstm)

o applications of recurrent neural networks

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 3

recurrent 
neural networks

output

nn cell

state

recurrent
connections

uva deep learning course
efstratios gavves
recurrent neural networks - 4

input

sequences

o next data depend on previous data

o roughly equivalent to predicting what comes next

pr      =    

pr              1,     ,            1)

    

what 

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 5

sequences

o next data depend on previous data

o roughly equivalent to predicting what comes next

pr      =    

pr              1,     ,            1)

    

what about 

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 6

sequences

o next data depend on previous data

o roughly equivalent to predicting what comes next

pr      =    

pr              1,     ,            1)

    

what about  inputs 

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 7

sequences

o next data depend on previous data

o roughly equivalent to predicting what comes next

pr      =    

pr              1,     ,            1)

    

what about  inputs  that  appear  in 

sequences, such as  text? could  neural 

a

network  handle  such modalities?

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 8

why sequences?

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 9

why sequences?

o considering small chunks         
o generalizes well to arbitrary lengths

    fewer parameters, easier modelling

                                                        (

i think, therefore, i am!

)

   

                                                        (

everything is repeated, in a circle. history is a master because it teaches us 
that it doesn't exist. it's the permutations that matter.

)

o however, often we pick a    frame         instead of an arbitrary length

pr      =    

pr                         ,     ,            1)

    

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 10

what a sequence really is?

o data inside a sequence are non i.i.d.

    identically, independently distributed

o the next    word    depends on the previous    words   

    ideally on all of them

o we need context, and we need memory!

o how to model context and memory ?

i

am bond

,

james

mcguire

bond
bond

tired

am

!

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 11

what a sequence really is?

o data inside a sequence are non i.i.d.

    identically, independently distributed

o the next    word    depends on the previous    words   

    ideally on all of them

o we need context, and we need memory!

o how to model context and memory ?

i

am bond

,

james

bond

mcguire

bond
bond

tired

am

!

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 12

             one-hot vectors

o a vector with all zeros except for the active dimension

o 12 words in a sequence     12 one-hot vectors

o after the one-hot vectors apply an embedding (id97, glove)

vocabulary

one-hot vectors

i

am

bond

james

tired
tired

,

i

am

bond

james

tired

,

mcguire

mcguire

!

!

1

0
0

0

0

0
0
0

i

am

bond

james

tired

,

mcguire

!

0

1
0

0

0

0
0

0

i

am

bond

james

tired

,

mcguire

!

0

0
1

0

0

0
0

0

i

am

bond

james

tired

,

mcguire

!

0

0
0

1

0

0
0

0

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 13

indices instead of one-hot vectors?

o can   t we simply use indices as features?

o no, great solution, because introduces artificial bias between inputs

        =1,2,3,4 =

i

am james mcguire

1

0
0

0

0

0
0
0

0

1
0

0

0

0
0

0

0

0
0

1

0

0
0

0

0

0
0

0
0

0

1

0

i

1

am james mcguire

2

4

7

        =1,2,3,4 =

    2(    1,     4) = 3 <     2(    1,     7) = 6

is    i    closer to james than to mcguire?

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 14

memory

o a representation of the past
o project information at timestep      on a latent space          using parameters     
o re-using the projected information from      at      + 1

        +1 =    (        +1,         ;     )

o recurrent parameters      are the shared for all timesteps      = 0,    

        +1 =    (        +1,    (        ,    (           1,             1,     0;      ;     );     );     )

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 15

memory as a graph

o simplest model

    input with parameters     
    memory embedding with parameters     
    output with parameters     

output

        

output parameters

    

memory
parameters

    

        

memory

input parameters

    

input
        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 16

memory as a graph

o simplest model

    input with parameters     
    memory embedding with parameters     
    output with parameters     

output

        

        +1

output parameters

    
    

memory
parameters

    

        

memory

    

        +1

input parameters

    
    

input
        

        +1

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 17

memory as a graph

o simplest model

    input with parameters     
    memory embedding with parameters     
    output with parameters     

output

        

output parameters

    
    

        +1

    

        +2

    

        +3

    

memory
parameters

    

        

memory

    

        +1

    

        +2

    

        +3

    

input parameters

    
    

input
        

    

        +1

    

        +2

    

        +3

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 18

folding the memory

unrolled/unfolded network

folded network

    

        

    

        

    
    

        

        +1

    

        +2

    

        +1

    

        +2

    

        +1

        +2

    

    

        

        

        

    

(           1)

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 19

recurrent neural network (id56)

o only two equations

         = tanh(              +                1)
         = softmax(             )

        

        

    

    

        

    

(           1)

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 20

id56 example

o vocabulary of 5 words

o a memory of 3 units [hyperparameter that we choose like layer size]

            : 3    1 , w: [3    3]

o an input projection of 3 dimensions

    u: [3    5]

o an output projections of 10 dimensions

         = tanh(              +                1)
         = softmax(             )

    v: [10    3]

                 =4 =

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 21

rolled network vs. multi-layer network?

o what is really different?

    steps instead of layers
    step parameters shared whereas in a multi-layer network they are different

    1

    2

    3

final output

   layer/step    1

    

    

    

   layer/step    2

    

   layer/step    3

    

    

    

    

    

    2

    1

    
    
    
    
    
1

    
    
    
    
    
2

    3

    
    
    
    
    
3

3-gram unrolled recurrent network

3-layer neural network

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 22

rolled     unrolled networks 

o what is really different?

    sometimes intermediate outputs are not even needed
    removing them, we almost end up to a standard 

neural network

    steps instead of layers
    step parameters shared whereas in a multi-layer network they are different

    1

    2

    3

final output

   layer/step    1

    

    

    

   layer/step    2

    

   layer/step    3

    

    

    

    

    

    2

    1

    
    
    
    
    
1

    
    
    
    
    
2

    3

    
    
    
    
    
3

3-gram unrolled recurrent network

3-layer neural network

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 23

training recurrent networks

o cross-id178 loss

     =    
    ,    

                     =     log      =    
            
    

        =    

o id26 through time (bptt)

    again, chain rule
    only difference: gradients survive over time steps

1
    

   
    

         log         

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 24

id26 through time: an example

o

       
        

step by step explanation at:
http://www.wildml.com/2015/10/recurrent-neural-networks-
tutorial-part-3-id26-through-time-and-vanishing-
gradients/
o to make it simpler let   s focus on step 3

       
        

       
        

,

,

       3
        

,

       3
        

,

       3
        

         = tanh(              +                1)
         = softmax(            )
    =        

         log          =    
    

    

       

    1,    1

    

    2,    2

    

    

    

    

    

    1

    

    2

    3,    3

    

    

    3

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 25

id26 through time

       3
        

=

       3
        3

        3
        

=     3         3         3

         = tanh(              +                1)
         = softmax(            )
    =        

         log          =    
    

    

       

    1,    1

    

    2,    2

    

    

    

    

    

    1

    

    2

    3,    3

    

    

    3

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 26

id26 through time

o

       3
        

=

       3
        3

        3
        3

        3
        

o what is the relation between     3 and     ?
    two-fold:          = tanh(              +                1)
        
         (          ,     (    ))
        

        
        

        
        

        
        

=

+

        

o

o

        3
        

        2 +

        2
        

(

        
        

= 1)

    1,    1

    

    2,    2

    

         = tanh(              +                1)
         = softmax(            )
    =        

         log          =    
    

    

       

    

    

    

    

    1

    

    2

    3,    3

    

    

    3

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 27

recursively

o

o

o

        3
        
        2
        
        1
        

=     2 +

=     1 +

=     0 +

        2
        
        1
        
        0
        

        3
        

3

=    
    =1

        3
            

            
        

   

       3
        

3

=    
    =1

       3
        3

        3
        3

        3
            

            
        

    1,    1

    

    2,    2

    

    

    

    

    

    1

    

    2

    3,    3

    

    

    3

         = tanh(              +                1)
         = softmax(            )
    =        

         log          =    
    

    

       

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 28

what makes id56s tick?

o the latent memory space is composed of multiple dimensions

o a subspace of the memory state space can store information if multiple 

basins      of attraction in some dimensions exist

o gradients must be strong near the basin boundaries

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 29

training id56s is hard

o vanishing gradients

    after a few time steps the gradients become almost 0

o exploding gradients

    after a few time steps the gradients become huge

o can   t capture long-term dependencies

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 30

alternative formulation for id56s

o an alternative formulation to derive conclusions and intuitions

         =          tanh(           1) +                   +     

    =    

       (        )

    

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 31

another look at the gradients

o     =     (        (           1         1     1,     0;      ;      ;      ;     )

o

o

           
        

t

=       =1

           
            

       
            

            
            

=

       
    ct

   

            
        

            
            
            
    ct   1

   

               1
    ct   2

           

            +1
    c  

                               
            

                     short-term factors 

                  long-term factors 

o the id56 gradient is a recursive product of 

            
    ct   1

     determines the 
norm of the gradients

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 32

id56 gradients in 1d

o

       
            

=

       
    ct

   

            
    ct   1

   

               1
    ct   2

           

< 1

< 1

o

       
            

=

       
    ct

   

            
    ct   1

   

               1
    ct   2

           

> 1

> 1

            +1
    c        
< 1

        1
    c        
> 1

       
        

       
        

    1     vanishing gradient

    1     exploding gradient

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 33

id56 gradients in n-d

o when                     then 

            
    ct   1

is a jacobian

o

       
            

=

       
    ct

   

            
    ct   1

   

               1
    ct   2

           

            +1
    ct

< 1

< 1

< 1

o

       
            

=

       
    ct

   

            
    ct   1

   

               1
    ct   2

           

            +1
    ct

> 1

> 1

> 1

       
        

       
        

    1     vanishing gradient

    1     exploding gradient

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 34

the jacobian

            2,             3:

        
        

=

         1
         1
         2
         1

         1
         2
         2
         2

         1
         3
         2
         3

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 35

id56 gradients in n-d

o when                     then 

            
    ct   1

is a jacobian

o spectral radius    (~largest eigenvalue) of jacobian is important

o

       
            

=

       
    ct

   

            
    ct   1

   

               1
    ct   2

           

     < 1

     < 1

            +1
    c        
     < 1

o

       
            

=

       
    ct

   

            
    ct   1

   

               1
    ct   2

           

            +1
    c    

     > 1

     > 1

     > 1

       
            

       
            

    1     vanishing gradient

    1     exploding gradient

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 36

gradient clipping for exploding gradients

o scale the gradients to a threshold

pseudocode
1. g    

       
        

g

    0
    

    

2. if      >     0:

    0
    

g    

else:

    

    0

print(   do nothing   )

o simple, but works!

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 37

vanishing gradients

o the gradient of the error w.r.t. to intermediate cell
            
        

           
        

           
            

            
            

            
            

t

=    
    =1

            
            

            
               1

=    
                  

=    
                  

              tanh(           1)

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 38

vanishing gradients

o for t = 1, r = 2    

o for t = 1, r = 3    

o for t = 1, r = 4    

       2
        
       3
        
       4
        

   

   

   

        2
        1
        3
        1
        4
        1

=

=

        3
        2
        4
        3

   

   

        2
        1
        3
        2

   

        2
        1

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 39

vanishing gradients

o the gradient of the error w.r.t. to intermediate cell
            
        

           
        

           
            

            
            

            
            

t

=    
    =1

            
            

            
               1

=    
                  

=    
                  

              tanh(           1)

o long-term dependencies get exponentially smaller weights

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 40

rescaling vanishing gradients?

o not good solution

o weights are shared between timesteps     loss summed over timesteps

    =    

           

           
        

    
t

=    
    =1

           
            

            
        

       
        
    

=    
    =1

           
        

=    

    
            
            

           
            

            
        

o rescaling for one timestep (

           
        

) affects all timesteps

    the rescaling factor for one timestep does not work for another

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 41

more intuitively

       1
        

       2
        

       
        

=

       1
        

+

       2
        

       3
+
        

+

       4
        

+

       5
        

       3
        

       4
        

       5
        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 42

more intuitively

o let   s say 

o

       
        

o if 

=       
       
        

       1
        
           
        

    1,

       2
        

    1/10,

       3
        

    1/100, 

       4
        

    1/1000,

       5
        

    1/10000

= 1.1111

rescaled to 1            5
        

    10   5

       1
        

o longer-term dependencies negligible

    weak recurrent modelling
    learning focuses on the short-term only

       
        

=

       1
        

+

       2
        

       3
+
        

+

       4
        

+

       5
        

       2
        

       3
        

       4
        

       5
        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 43

recurrent networks     dynamical systems

o in the figures xt              and              f(               1 +              +     )

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 44

fixing vanishing gradients

o id173 on the recurrent weights

    force the error signal not to vanish

   =    

    

  t =    
    

2

    1

       

            +1

            +1
            

       

            +1

o advanced recurrent modules

o long-short term memory module

o gated recurrent unit module

pascanu et al., on the diculty of training recurrent neural networks, 2013

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 45

advanced id56s

           1

+

tanh

        

        

        

    

    

           
tanh

        

    

        
output

           1

        

input

uva deep learning course
efstratios gavves
recurrent neural networks - 46

how to fix the vanishing gradients?

o error signal over time must have not too large, not too small norm

o solution: have an activation function with derivative equal to 1

    identify function

o by doing so, gradients do not become too small not too large

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 47

long short-term memory (lstm: beefed up id56)

     =                  (    ) +            1    (    )

           1

     =                  (    ) +            1    (    )

     =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

+

tanh

        

        

        

    

    

           
tanh

        

    

more info at:
http://colah.github.io/posts/2015-08-understanding-lstms/

           1

        

input

        
output

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 48

cell state

o the cell state carries the essential information over time

cell state line

           1

     =                  (    ) +            1    (    )

     =                  (    ) +            1    (    )

     =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

           1

        

+

tanh

        

        

    

    

           
tanh

        

    

        

        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 49

lstm nonlinearities

o          (0, 1): control gate     something like a switch

o tanh        1, 1 : recurrent nonlinearity

     =                  (    ) +            1    (    )

           1

     =                  (    ) +            1    (    )

     =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

+

tanh

        

        

        

    

    

           
tanh

        

    

           1

        

input

        
output

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 50

lstm step-by-step: step (1)

o e.g. model the sentence    yesterday she slapped me. today she loves me.   

o decide what to forget and what to remember for the new memory

    sigmoid 1     remember everything
    sigmoid 0     forget everything

           1

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

           1

        

+

tanh

        

        

    

    

           
tanh

        

    

        

        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 51

lstm step-by-step: step (2)

o decide what new information should you add to the new memory

    modulate the input         
    generate candidate memories             

           1

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

           1

        

+

tanh

        

        

    

    

           
tanh

        

    

        

        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 52

lstm step-by-step: step (3)

o compute and update the current cell state         
    depends on the previous cell state
    what we decide to forget
    what inputs we allow
    the candidate memories

           1

+

tanh

        

        

    

    

           
tanh

        

    

        

        

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

           1

        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 53

lstm step-by-step: step (4)

o modulate the output

    does the cell state contain something relevant?     sigmoid 1

o generate the new memory

           1

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

         =                  (    ) +            1    (    )

            = tanh(                  +            1    (    ))

         =            1          +                     

         = tanh                  

           1

        

+

tanh

        

        

    

    

           
tanh

        

    

        

        

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 54

lstm unrolled network

o macroscopically very similar to standard id56s

o the engine is a bit different (more complicated)

    because of their gates lstms capture long and short term dependencies

  

+

  

tanh

  

  

+

  

tanh

  

  

+

  

tanh

  

    

    

tanh

    

    

    

tanh

    

    

    

tanh

    

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 55

beyond id56 & lstm

o lstm with peephole connections

    gates have access also to the previous cell states            1 (not only memories)
    coupled forget and input gates,          =                         1 + 1                             
    bi-directional recurrent networks

o id149 (gru)
o deep recurrent architectures
o id56s

    tree structured

lstm (2)

lstm (2)

lstm (2)

lstm (1)

lstm (1)

lstm (1)

o multiplicative interactions
o generative recurrent architectures

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 56

click to go to the video in youtube

click to go to the website

applications of 
recurrent networks

uva deep learning course
efstratios gavves
recurrent neural networks - 57

machine translation

o the phrase in the source language is one sequence 

       today the weather is good   

o the phrase in the target language is also a sequence

       vandaag is het weer goed   

o problems

    no perfect word alignment, sentence length might differ

o solution

    encoder-decoder scheme

vandaag

is

het

weer

goed

<eos>

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

today

the

weather

is

good

<eos> vandaag

is

het

weer

goed

encoder

decoder

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 58

better machine translation

o it might even pay off reversing the source sentence

    the first target words will be closer to their respective source words

o the encoder and decoder parts can be modelled with different lstms

o deep lstm

vandaag

is

het

weer

goed

<eos>

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

good

is

weather

the

today <eos> vandaag

de

weer

is

heel

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 59

image captioning

o an image is a thousand words, literally!

o pretty much the same as machine transation

o replace the encoder part with the output of a convnet

    e.g. use alexnet or a vgg16 network

o keep the decoder part to operate as a translator

today

the

weather

is

good <eos>

convnet

lstm

lstm

lstm

lstm

lstm

lstm

today

the

weather

is

good

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 60

id53

o bleeding-edge research, no real consensus

    very interesting open, research problems

o again, pretty much like machine translation

o again, encoder-decoder paradigm
    insert the question to the encoder part
    model the answer at the decoder part

o id53 with images also

    again, bleeding-edge research
    how/where to add the image?
    what has been working so far is to add the image 

only in the beginning

q: john entered the living room, where 
he met mary. she was drinking some 
wine and watching a movie. what room 
did john enter?

a: john entered the living room.

q: what are the people playing?
a: they play beach football

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 61

o recurrent neural networks (id56) for sequences

o id26 through time

summary

o vanishing and exploding gradients and 

remedies

o id56s using long short-term memory (lstm)

o applications of recurrent neural networks

uva deep learning course
efstratios gavves
recurrent neural networks - 62

reading material & references

o http://www.deeplearningbook.org/

    part ii: chapter 10

o excellent blog post on id26 through time

    http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-

id56s/

    http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-

language-model-id56-with-python-numpy-and-theano/

    http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-

through-time-and-vanishing-gradients/

o excellent blog post explaining lstms

    http://colah.github.io/posts/2015-08-understanding-lstms/

[pascanu2013] pascanu, mikolov, bengio. on the difficulty of training recurrent neural networks, jmlr, 2013

uva deep learning course     efstratios gavves                                                                                    

recurrent neural networks - 63

o memory networks

o recursive networks

next lecture

uva deep learning course
efstratios gavves
recurrent neural networks - 64

