deep learning & hpc

valeriu codreanu 
damian podareanu 
31 october 2016

about surfsara

history:
    1971: founded by the vu, uva, and cwi 
    2013: sara (stichting academisch rekencentrum a   dam) becomes part of surf 

super/cluster-computing group:
    8 consultants 
    17 members in total (including admins/system-experts) 

other activities:
    hpc cloud / virtualisation 
    data services / storage 
    visualisation

2

our systems (1/2)

cartesius (bull supercomputer):
    40.960 ivy bridge / haswell cores: 1327 tflops 
    56gbit/s in   niband 
    64 nodes with 2 gpus each: 210 tflops 

    nvidia tesla k40m gpu 
    12gb gddr5 
    gpu-direct rdma 

    accelerator island: #4 green500 (june 2014)
    broadwell & knl extension (nov 2016) 

    177 bdw and 18 knl nodes: 284tflops

    7.7 pb lustre parallel    le-system

3

our systems (2/2)

lisa (dell cluster):
    7856 cores (16 cores per node, xeon e5-2650) 
    peak performance: 149 tflops 

hpc cloud:
    virtual machines 
    up to 64 cores and 2tb ram 

the archive:
    tape-storage for long-term storage 
    virtually unlimited space 

others:
    elvis (visualisation/render cluster with 18 gpus)  
    grid 
    hadoop

4

challenges

typical challenges:
    bottleneck identi   cation 
    mpi/openmp parallelisation 
    inter-node communication 
    i/o scaling 
    gpu/xeon phi acceleration 
    algorithm optimization 
    vectorization 
approach:
    discussions, scienti   c papers, manuals 
    hot-spot detection, timing analysis (manual) 
    v-tune / scalasca / score-p / likwid / nvvp pro   ling (guided)

5

deep learning & hpc

7

convolutional neural networks 
are biologically-inspired 

the network learns from raw 
pixels 

each layer learns incrementally 
complex features 

typically implemented as a 
series of convolution and max-
pooling layers 

8

9

10

11

deep learning & gpus

12

13

14

15

16

17

18

19.6 gflops

1.5 gflops

resnet

3.6-11.6 gflops training vgg for 50 epochs on 
id163 uses more than 1 

exaflop

distributed training 

true hpc 
is needed

19

from feed forward to recurrent

from feed forward networks to recurrent neural networks

http://deeplearning4j.org/lstm.html

20

back propagation through time

u, v, w are shared across all steps

input / output are not necessary for all steps -

prediction

standard feedforward backprop

sum up gradients for w at each timestep

21

ef   ciency

duration of one epoch  on a 24 core intel xeon  e5 ~ 3000 seconds     an experiment  would take  ~ 1 day

duration of one epoch  on a tesla k40 ~ 100 seconds     an experiment  would 
take ~ 1 hour

the speedup  is in this case  ~ 30x; however, when  testing with different 
parameters,  speedups of 10x - 70x were  recorded

results

movie lens

sentiment 140

sgd

81%

78%

bdlstm

lstm

87%

84%

84%

82%

22

live demos

http://www.cs.toronto.edu/~graves/handwriting.html
http://www.inkposter.com/
http://www.cs.toronto.edu/~ilya/id56.html
http://104.131.78.120/
http://www.cs.toronto.edu/~hinton/digits.html
http://www.cs.toronto.edu/~hinton/adi/index.htm

23

machine learning @ surfsara

24

ml software on cartesius

already installed software:
    caffe 
    torch7 
    tensor   ow 
    theano/lasagne 
    cuda-convnet2 
    cntk 
    mxnet 
    scikit-learn 
    cudnn 
    nvidia digits
other software:
   
       or ask us to install it

install yourself    

digits training for googlenet

25

execution on cartesius

- create a sleep job and connect there for interactive usage 
-

create a    regular    batch job (preferred option)

2 options: 

#!/bin/bash 
#sbatch -p gpu 
#sbatch -n 1 
#sbatch -t 5:00:00 
sleep 18000

#!/bin/bash 
#sbatch -p gpu 
#sbatch -n 1 
#sbatch -t 5:00:00 
module load cuda 
module load cudnn 
module load python/2.7.9 
theano_flags=   mode=fast_run,device=gpu,   oatx=   oat32,lib.
cnmem=1' srun -u python code/convolutional_mlp.py 

sleep_job.sh

theano_job.sh

26

tensor   ow basics

- represents computations as graphs. 
- executes graphs in the context of sessions. 
- represents data as tensors. 
- maintains state with variables. 
- uses feeds and fetches to get data into and out of arbitrary operations.

https://www.tensor   ow.org/versions/r0.10/get_started/basic_usage.html

27

study 1: training cifar10

conv

pool

norm

conv

norm

pool

local

local

convolution and recti   ed linear activation.

max pooling.

local response id172.

convolution and recti   ed linear activation.

local response id172.

max pooling.

fully connected layer with recti   ed linear 
activation.

fully connected layer with recti   ed linear 
activation.

https://www.tensor   ow.org/versions/r0.11/tutorials/deep_id98/index.html

28

softmax_linea

linear transformation to produce logits.

study 1: data parallelism

tower_grads = [] 
    for i in xrange(flags.num_gpus): 
      with tf.device('/gpu:%d' % i): 
        with tf.name_scope('%s_%d' % (cifar10.tower_name, i)) as scope: 
          # calculate the loss for one tower of the cifar model. this function 
          # constructs the entire cifar model but shares the variables across 
          # all towers. 
          loss = tower_loss(scope) 

          # reuse variables for the next tower. 
          tf.get_variable_scope().reuse_variables() 

          # retain the summaries from the    nal tower. 
          summaries = tf.get_collection(tf.graphkeys.summaries, scope) 

          # calculate the gradients for the batch of data on this cifar tower. 
          grads = opt.compute_gradients(loss) 

          # keep track of the gradients across all towers. 
          tower_grads.append(grads) 

    # we must calculate the mean of each gradient. note that this is the 
    # synchronization point across all towers. 
    grads = average_gradients(tower_grads) 

29

study 2: model parallelism in tf

with tf.device('/gpu:0'): 
    a = tf.placeholder(tf.   oat32, [10000, 10000]) 
    b = tf.placeholder(tf.   oat32, [10000, 10000]) 
    # compute a^n and b^n and store results in c1 
    c1.append(matpow(a, n)) 
    c1.append(matpow(b, n)) 

# gpu:0 computes a^n 
with tf.device('/gpu:0'): 
    # compute a^n and store result in c2 
    a = tf.placeholder(tf.   oat32, [10000, 10000]) 
    c2.append(matpow(a, n)) 

# gpu:1 computes b^n 
with tf.device('/gpu:1'): 
    # compute b^n and store result in c2 
    b = tf.placeholder(tf.   oat32, [10000, 10000]) 
    c2.append(matpow(b, n)) 

with tf.device('/cpu:0'): 
  sum = tf.add_n(c1) #addition of all elements in c1, i.e. a^n + b^n 

with tf.device('/cpu:0'): 
  sum = tf.add_n(c2) #addition of all elements in c2, i.e. a^n + b^n 

t1_1 = datetime.datetime.now() 
with tf.session(con   g=tf.con   gproto( \ 

log_device_placement=log_device_placement)) as sess: 

    # run the op. 
    sess.run(sum, {a:a, b:b}) 
t2_1 = datetime.datetime.now()

t1_2 = datetime.datetime.now() 
with tf.session(con   g=tf.con   gproto( \ 

log_device_placement=log_device_placement)) as sess: 

    # run the op. 
    sess.run(sum, {a:a, b:b}) 
t2_2 = datetime.datetime.now()

30

study 3: simple keras example

general repository https://github.com/kjw0612/awesome-id56
keras example
model = sequential()
model.add(lstm(512, return_sequences=true, input_shape=(maxlen, len(chars))))
model.add(dropout(0.2))
model.add(lstm(512, return_sequences=false))
model.add(dropout(0.2))
model.add(dense(len(chars)))
model.add(activation('softmax'))
model.compile(loss='categorical_crossid178', optimizer='rmsprop')

31

questions?

