christof monz

deep learning
language models and id27s

today   s class

(cid:73) id165 id38
(cid:73) feed-forward neural language model

    architecture
    final layer computations

(cid:73) id27s

    continuous bag-of-words model
    skip-gram
    negative sampling

christof monz
language models and id27s

1

the role of lm in smt

(cid:73) translation models map source phrases to target phrases
    translation probabilities should re   ect the degree to which the
meaning of the source phrase is preserved by the target phrase
(adequacy)

    source:    der mann hat einen hund gekauft.   

monotone translation:    the man has a dog bought.   
translation preserves the meaning but is not    uent

(cid:73) language models compute the id203 of a string

    p(the man has a dog bought.) < p(the man has bought a dog.)
    language model probabilities do not necessarily correlate with
grammaticality: p(green ideas sleep furiously.) is likely to be
small

    during translation language model scores of translation

hypotheses are compared to each other

christof monz
language models and id27s

2

the role of lm in smt

(cid:73) the language model is one of the most important models

in smt

(cid:73) substantial improvements in translation quality can be

gained from carefully trained language models

(cid:73) decades of research (and engineering) in language
modeling for automated id103 (asr)
    many insights can be transferred to smt
    types of causes for dis   uencies di   er between both areas

asr: p(we won   t i scream) < p(we want ice cream)
smt: p(get we ice cream) < p(we want ice cream)

    reordering does not play a role in asr

christof monz
language models and id27s

3

id165 id38

(cid:73) id165 language model compute the id203 of a

string as the product of probabilities of the consecutive
id165s:

    p(<s> the man has a dog bought . </s>)

= p(<s> the)    p(<s> the man)    p(the man has)    p(man
has a)    p(has a dog)    p(a dog bought)    p(dog bought .)   
p(bought . </s>)

    generally: p(wn
i   n+1), for order n
    problem: if one id165 id203 is zero, e.g.,

i=1 p(wi|wi   1

1 ) =    n

p(dog bought .) = 0, then the id203 of the entire product
is zero

    solution: smoothing

christof monz
language models and id27s

4

language model smoothing

(cid:73) a number of smoothing approaches have been developed

for id38

(cid:73) jelinek-mercer smoothing

    weighted linear interpolation of conditional probabilities of

di   erent orders
(cid:73) katz smoothing

(cid:73) witten-bell smoothing

    back-o    to lower-order probabilities and counts are discounted

    linear interpolation where lower-order probabilities are

weighted by the number of contexts of the history

(cid:73) kneser-ney smoothing

    weight lower-order probabilities by the number of contexts in

which they occur

christof monz
language models and id27s

5

kneser-ney smoothing

pkn(wi|wi   1

i   n+1) =

                max{c(wi

  (wi   1

i   n+1)   d(c(wi
   wi c(wi
i   n+1)

i   n+1)),0}

i   n+1)pkn(wi|wi   1

i   n+2)

if c(wi

i   n+1) > 0

if c(wi

i   n+1) = 0

(cid:73) original backo   -style formulation of kneser-ney

smoothing

    closer to representation found in arpa style language models
    can be re-formulated as linear interpolation (see chen and

goodman 1999)

christof monz
language models and id27s

6

lm smoothing in smt

(cid:73) does the choice of smoothing method matter for smt?
    kneser-ney smoothing typically yields results with the lowest
    correlation between perplexity and mt metrics (such a id7)
    few comparative studies, but kneser-ney smoothing yields

perplexity

is low

small gains over witten-bell smoothing

(cid:73) kneser-ney smoothing is the de facto standard for smt

(and asr)

(cid:73) recent smt research combines witten-bell smoothing

with kneser-ney smoothing

christof monz
language models and id27s

7

size matters

christof monz
language models and id27s

8

17more data is better data   five-gram language model, no count-cutoff, integrated into search:47.548.549.550.551.552.553.575m150m300m600m1.2b2.5b5b10b18b+219b webae id7[%]probabilistic neural network lms
(cid:73) both word- and class-based models use discrete

parameters as elements of the event space

(cid:73) the current word+history id165 has not been seen
    smoothing results in a more relaxed matching criterion

during training or it has not been seen (binary decision)

(cid:73) probabilistic neural network lms (bengio et al. jmlr

2003) use a distributed real-valued representation of
words and contexts

(cid:73) each word in the vocabulary is mapped to a

m-dimensional real-valued vector

    c(w)     rm, typical values for m are 50, 100, 150
    a hidden layer capture the contextual dependencies between
    the output layer is a |v|-dimensional vector describing the

words in an id165
id203 distribution of p(wi|wi   1

i   n+1)

christof monz
language models and id27s

9

probabilistic neural network lms

christof monz
language models and id27s

10

probabilistic neural network lms

(cid:73) layer-1 (projection layer)

c(wt   i) = c wt   i
where

    c is a m   v matrix
(cid:73) layer-2 (context layer)

    wt   i is a v-dimensional 1-hot vector, i.e., a zero-vector where
only the index corresponding the word occurring at position
t    i is 1

h = tanh(d + h x)
where

    x = [c(wt   n+1); . . .;c wt   1]
    h is a n   (l    1)m matrix

christof monz
language models and id27s

([   ;   ] = vector concatenation)

11

probabilistic neural network lms

(cid:73) layer-3 (output layer)
  y = softmax(b + u h)
where

    u is a v    n matrix
    softmax(v) = exp(vi)

(cid:73) optional: skip-layer connections

   i exp(vi) (turns activations into probs)

  y = softmax(b + w x + u h)
where

    w is a v    (l    1)m matrix (skipping the non-linear context

layer)

christof monz
language models and id27s

12

training pnlms

(cid:73) id168 is cross-id178: l(y,   y) =    log(  yi), where

i = argmax(y)

(cid:73) optimize with respect to

   l(y,  y)

     

where    = {c,h,d,u,b} using stochastic gradient
descent (sgd)

(cid:73) update all parameters, including c (the projections)
(cid:73) what does c capture?

    maps discrete words to continuous, low dimensional vectors
    c is shared across all contexts
    c is position-independent
    if c(white)     c(red) then

p(drives|a white car)     p(drives|a red car)

christof monz
language models and id27s

13

pnlm variant

(cid:73) previous architecture directly connects hidden context

layer to full vocabulary output layer

(cid:73) alternative: introduce output projection layer in between:

(cid:73) sometimes also referred to as    deep output layer   

christof monz
language models and id27s

14

how useful are pnlms?

(cid:73) advantages:

    pnlms outperform id165 based language models (in terms
    use limited amount of memory

of perplexity)

    nplm:    100m    oats     400m ram
    id165 model:    10-40g ram

(cid:73) disadvantages:

    computationally expensive

    mostly due to large output layer (size of vocabulary): u h can
    we want to know p(w|c) for a speci   c w, but to do so we

involve hundreds of millions of operations!

need softmax over entire output layer

christof monz
language models and id27s

15

speeding up pnlms

(cid:73) slow training

    annoys developpers/scientists/phd students
    slows down development cycles

(cid:73) slow id136
    annoys users
    can cause products to become impractical

(cid:73) speeding things up

    mini-batching (training)
    using gpus (training)
    parallelization (training)
    short-lists (training + id136)
    class-based structured output layers (training + id136)
    hierarchical softmax (training + id136)
    noise contrastive estimation (training + id136)
    self-id172 (id136)

christof monz
language models and id27s

16

mini-batching

(cid:73) instead of computing p(w|c) compute p(w|c )

where w is an ordered set of words, and c is ordered set
of contexts
(cid:73)     matrix-id127s instead of matrix-vector

multiplications
allows to use low-level libraries such as blas to exploit
memory-layout

(cid:73)   y = softmax(b + u tanh(d + h x) becomes

  y = softmax(b + u tanh(d + h x)

(cid:73) advantage: mini-batching is very gpu friendly
(cid:73) disadvantage: fewer parameter updates (depends on

mini-batch size)

(cid:73) disadvantage: not really applicable during id136

christof monz
language models and id27s

17

short-lists

(cid:73) in nlp, the size of the vocabulary can easily reach 200k

(english) to 1m (russian) words

(cid:73) quick-   x: short-lists

    ignore rare words and keep only the n most frequent words
    all rare words are mapped to a special token: <unk>

(cid:73) typical sizes of short-lists vary between 10k, 50k, 100k,

and sometimes 200k words

(cid:73) disadvantage: all rare words receive equal id203 (in

a given context)

christof monz
language models and id27s

18

class-based output layer

(cid:73) partition vocabulary into n non-overlapping classes (c)

    using id91 (brown id91)
       xed categories (pos tags)

(cid:73) instead of   y = softmax(b + u h)

compute   c = softmax(b + u h), where |c| (cid:28) |v|
then choose   ci = argmax(  c) and
compute   yci = softmax(b + uci h)
where uci is a |vci|  |h| matrix, where |vci| (cid:28) |v|
(cid:73) advantage: leads to signi   cant speed improvements
(cid:73) disadvantage: not very mini-batch friendly (matrix uci

can vary across instances in the same batch)

christof monz
language models and id27s

19

self-id172

(cid:73) during id136 (i.e., when applying a trained model to
unseen data) we are interested in p(w|c) and not p(w(cid:48)|c),
where w(cid:48) (cid:54)= w

(cid:73) unfortunately b + u h does not yield probabilities and

softmax requires summation over the entire output layer
(cid:73)    encourage    the neural network to produce id203-like
values (devlin et al., acl-2014) without applying softmax

christof monz
language models and id27s

20

self-id172

(cid:73) softmax log likelihood:

log(p(x)) = log( exp(ur(x))
where

z(x)

)

    ur(x) is the output layer score for x
|v|
    z(x) =    
r(cid:48)=1 ur(cid:48)(x)

log(p(x)) = log(ur(x))    log(z(x))

(cid:73) if we could ensure that log(z(x)) = 0 then we could use

log(ur(x)) directly

(cid:73) strictly speaking not possible, but we can encourage the

model by augmenting the id168:
l =    i[log(p(xi))       (log(z(xi))2]

christof monz
language models and id27s

21

self-id172

(cid:73) self-id172 included during training; for id136,

log(p(x)) = log(ur(x))

(cid:73)    regulates the importance of id172

(hyper-parameter):

(cid:73) initialize output layer bias to log(1/|v|)
(cid:73) devlin et al. report speed-ups of around 15x during

id136

(cid:73) no speed-up during training

christof monz
language models and id27s

22

reminder: pnlm architecture

christof monz
language models and id27s

23

projections = embeddings?

(cid:73) are projections the same as id27s?
(cid:73) what are (good) id27s? c(w)     c(w(cid:48)) i   

    w and w(cid:48) mean the same thing
    w and w(cid:48) exhibit the same syntactic behavior

(cid:73) for pnlms the projections/embeddings are by-products

    main objective is to optimize next word prediction
    projections are    ne-tuned to achieve this objective

(cid:73) representation learning: if the main objective is to learn

good projections/embeddings

christof monz
language models and id27s

24

word meanings

(cid:73) what does a word mean?
(cid:73) often de   ned in terms of relationship between words

    synonyms: purchase ::
    hyponyms: car :: vehicle (is-a)
    meronyms: wheel :: car (part-whole)
    antonyms: small :: large (opposites)

acquire (same meaning)

(cid:73) explicit, qualitative relations require hand-crafted

resources (dictionaries, such as id138)

    expensive
    incomplete
    language-speci   c

(cid:73) what about

    learning relations automatically?
    quantifying relations between words, e.g.,

sim(car,vehicle) > sim(car,tree) ?

christof monz
language models and id27s

25

id65

(cid:73)    you shall know a word by the company it keeps.    (firth,

1957)

(cid:73) in id65 all words w are represented as

a v-dimensional context vector cw

(cid:73) cw[i] = f where f is the frequency of word i occurring

within the (   xed-size) context of w

christof monz
language models and id27s

26

id65

(cid:73) word similarity as cosine similarity in the context vector

space:

(cid:73) in id65 context vectors are

high-dimensional, discrete, and sparse

christof monz
language models and id27s

27

id27s

(cid:73) similar underlying intuition to id65,

but word vectors are

    low dimensional (e.g., 100 vs. |v|)
    dense (no zeros)
    continuous (cw     rm)
    learned by performing a task (predict)

(cid:73) popular approach: id97 (mikolov et al.)
(cid:73) id97 consists of two approaches:

    continuous bag of words (cbow)
    skip-gram

christof monz
language models and id27s

28

continuous bag of words (cbow)

(cid:73) task: given a position t in a sentence, and the n words

occurring to its the left ({wt   n, . . . ,wt   1}) and m its right
({wt+1, . . . ,wt+n}) predict the word in position t
the man x the road, with x =?
n = lm order    1 and m = 0

(cid:73) seemingly similar to id165 id38 where

(cid:73) use feed-forward neural network

    focus on learning embeddings themselves
    simpler network (compared to pnlm)
    bring embedding/projection layer closer to output
    typically n = m, and n     {2,5,10}

christof monz
language models and id27s

29

cbow model architecture

christof monz
language models and id27s

30

cbow model
(cid:73) no non-linearities
(cid:73) one hidden layer:

h = 1

2nw wc, where

    w is a |h|  |v| matrix
    wc =
    wi is a 1-hot vector for the word occurring in position i

i=t   n,i(cid:54)=t

t+n
   

wi

(cid:73) output layer:

  y = softmax(w(cid:48) h)

    w(cid:48) is a |v|  |h| matrix
    w(cid:48) and w are not (necessarily) shared, i.e., w(cid:48) (cid:54)= wt

(cid:73) id168: cross id178 (see pnlm)
(cid:73) trained with sgd

christof monz
language models and id27s

31

cbow embeddings

(cid:73) where do the embeddings live?

    column i in w (|h|  |v| matrix) represents the embedding for
    row i in w(cid:48) (|v|  |h| matrix) represents the embedding for

word i

word i

(cid:73) which one of the two?

    typically w or
    ws = wt + w(cid:48) (combining both into one)

christof monz
language models and id27s

32

skip-gram model architecture

(cid:73) alternative to cbow
(cid:73) task: given a word at position t in a sentence, predict the

words occurring between positions t    n and t    1 and
between t + 1 and t + n

christof monz
language models and id27s

33

skip-gram model

(cid:73) one hidden layer:
h = w wi, where

    wi is the 1-hot vector for word at position t

(cid:73) 2n output layers:

p(wt   n . . .wt   1wt+1 . . .wt+n|wi)
   
  yi = softmax(w(cid:48) h) (t    n     i     t + n and i (cid:54)= t)

p(wi|wi)

i=t   n,i(cid:54)=t

t+n
   

    w(cid:48) is a |v|  |h| matrix
    w(cid:48) and w are not (necessarily) shared, i.e., w(cid:48) (cid:54)= wt

(cid:73) id168: cross id178 (see pnlm)
(cid:73) trained with sgd

christof monz
language models and id27s

34

negative sampling

(cid:73) both cbow and skip-gram bene   t from large amounts

of data

(cid:73) computing activations for the full output layer becomes

an issue

(cid:73) negative sampling: try to distinguish between words that
do and and words that do not occur in the context of the
input word

    classi   cation task
    1 positive example (from the ground truth)
    k negative examples (from a random noise distribution

christof monz
language models and id27s

35

negative sampling

(cid:73) given the input word w and a context word c we want to
p(d = 0|c,w;  )

argmax
where d represents the observed data and d(cid:48) a noise
distribution

p(d = 1|c,w;  )    

(w,c)   d(cid:48)

(w,c)   d

   

  

(cid:73) we compute p(d = 1|c,w;  ) as   (vc    vw)

where vw = w w and vc = w(cid:48)t c

(cid:73) p(d = 0|c,w;  ) = 1    p(d = 1|c,w;  )
(cid:73) since 1      (x) =   (   x):

argmax

  

   

(w,c)   d

argmax

  

   

(w,c)   d

  (   vc    vw)

  (vc    vw)    
log  (vc    vw) +    

(w,c)   d(cid:48)

(w,c)   d(cid:48)

log  (   vc    vw)

christof monz
language models and id27s

36

id97 practical considerations

(cid:73) skip-gram:

to data

    for each observer occurrence (w,c) add 5-20 negative samples
    draw c from uni-gram distribution p(w)
    scale uni-gram distribution: p(w)0.75 to bias rarer words

(cid:73) context size typically around 2-5
(cid:73) the more data the smaller the context and the negative

sample set

(cid:73) exclude very rare words (less than 10 occurrences)
(cid:73) removing stop words: better topical modeling, less

sensitive to syntactical patterns

christof monz
language models and id27s

37

evaluation of id27s

(cid:73) word similarity tasks

(cid:73) analogy task

    rank list of word pairs, e.g., (car,bicycle), by similarity
    spearman correlation with human judgements
    benchmarks: ws-353, siid113x-999, ...
    mixes all kinds of similarities (synonyms, topical, unrelated...)

    paris is to france as berlin is to x
    evaluated by accuracy
    also includes syntactic analogy: acquired is to acquire as tried
    arithmetic magic: x = vking     vman + vwoman

is to x

christof monz
language models and id27s

38

applicability of id27s

(cid:73) word similarity
(cid:73) to initialize projection layers in deep networks

    if training data is small
    if number of output classes is small
    task-speci   c    ne-tuning still useful in many cases

christof monz
language models and id27s

39

recap

(cid:73) feed-forward neural language model

    projection layers
    cross-id178 loss
    final layer computations

    mini-batching
    short-lists
    class-based structured output layer
    self-id172

(cid:73) id27s

    continuous bag-of-words model
    skip-gram
    negative sampling

christof monz
language models and id27s

40

