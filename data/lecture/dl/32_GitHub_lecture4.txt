lecture 4: convolutional neural networks for id161

deep learning @ uva

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 1

previous lecture

o how to define our model and optimize it in practice

o id174 and id172

o optimization methods

o id173s

o architectures and architectural hyper-parameters

o learning rate

o weight initializations

o good practices

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 2

lecture overview

o what are the convolutional neural networks?

o why are they important in id161?

o differences from standard neural networks

o how to train a convolutional neural network?

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 3

convolutional 
neural networks

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 4

what makes images different?

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 5

what makes images different?

depth

height

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 6

width

what makes images different?

1920x1080x3 = 6,220,800 input variables

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 7

what makes images different?

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 8

what makes images different?

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 9

what makes images different?

image has shifted a bit to the up and the left!

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 10

what makes images different?

o an image has spatial structure

o huge dimensionality

    a 256x256 rgb image amounts to ~200k input variables
    1-layered nn with 1,000 neurons     200 million parameters

o images are stationary signals     they share features

    after variances images are still meaningful
    small visual changes (often invisible to naked eye)     big changes to input vector
    still, semantics remain
    basic natural image statistics are the same

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 11

input dimensions are correlated

traditional task: predict my salary!

shift 1 dimension

level of education

   higher   

level of education

spain

age
28

age
   higher   

years of experience

6

previous job
researcher

nationality

spain

years of experience

previous job

28

6

nationality
researcher

vision task: predict the picture!

first 5x5 values

first 5x5 values

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 12

convolutional neural networks

o question: spatial structure?

    answer: convolutional filters

o question: huge input dimensionalities?
    answer: parameters are shared between filters

o question: local variances?

    answer: pooling

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 13

preserving 
spatial structure

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 14

why spatial?

o images are 2-d

    k-d if you also count the extra channels
    rgb, hyperspectral, etc.

o what does a 2-d input really mean?

    neighboring variables are locally correlated

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 15

example filter when k=1

e.g. sobel 2-d filter

   1 0 1
   2 0 2
   1 0 1

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 16

learnable filters

o several, handcrafted filters in 

id161
    canny, sobel, gaussian blur, smoothing, low-

level segmentation, morphological filters, 
gabor filters

o are they optimal for recognition?

o can we learn them from our data?

o are they going resemble the 

handcrafted filters?

    1     2     3
    4     5     6
    7     8     9

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 17

2-d filters (parameters)

o if images are 2-d, parameters should also be organized in 2-d
    that way they can learn the local correlations between input variables
    that way they can    exploit    the spatial nature of images

grayscale

filters

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 18

k-d filters (parameters)

o similarly, if images are k-d, parameters should also be k-d

grayscale

rgb

multiple channels

3  dims

k dims

filters

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 19

what would a k-d filter look like?

e.g. sobel 2-d filter

   1 0 1
   2 0 2
   1 0 1

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 20

think in space

rgb

3  dims

  

7

7

7

=

7

3

how many weights for this neuron?

7     7     3 = 147

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 21

think in space

the activations of a hidden layer form a volume of neurons, not a 1-d    chain   
this volume has a depth 5, as we have 5 filters

rgb

3 dims

7

7

7

  

7

3

=

how many weights for these 5 neurons?

5     7     7     3 = 735

depth=5  dims

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 22

local connectivity

o the weight connections are surface-wise local!

    local connectivity

o the weights connections are depth-wise global

o for standard neurons no local connectivity

    everything is connected to everything

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 23

output

filters vs
convolutional
k-d filters

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 24

input

again, think in space

rgb

3 dims

7

7

7

  

7

3

=

735 weights

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 25

what about cover the full image with filters?

weight filters

rgb

3 dims

  

7

7

24

5

=

24

assume the image is 30x30x3.
1 filter every pixel (stride =1)
how many parameters in total?

24 filters along the      axis
24 filters along the      axis
depth of 5
7     7     3 parameters per filter

  

423     parameters in total

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 26

problem!

o clearly, too many parameters

o with a only 30    30 pixels image and a single hidden layer of depth 5 we 

would need 85k parameters
    with a 256    256 image we would need 46     106 parameters

o problem 1: fitting a model with that many parameters is not easy

o problem 2: finding the data for such a model is not easy

o problem 3: are all these weights necessary?

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 27

hypothesis

o imagine

    with the right amount of data    
        and if we connect all inputs of layer      with all 

outputs of layer      + 1,    

        and if we would visualize the (2d) filters 

(local connectiveity     2d)    

        we would see very similar filters no matter 

their location

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 28

hypothesis

o imagine

    with the right amount of data    
        and if we connect all inputs of layer      with all 

outputs of layer      + 1,    

        and if we would visualize the (2d) filters 

(local connectiveity     2d)    

        we would see very similar filters no matter 

their location

o why?

    natural images are stationary
    visual features are common for different parts 

of one or multiple image

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 29

solution? share!

o so, if we are anyways going to compute the same filters, why not share?

    sharing is caring

3 dims

  

7

7

=

assume the image is 30x30x3.
1 column of filters common across the image.
how many parameters in total?

depth of 5
7     7     3 parameters per filter

  

735 parameters in total

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 30

shared 2-d filters     convolutions

original image

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 31

shared 2-d filters     convolutions

original image

1 1 1
0 1 1
0 0 1
0 0 1
0 1 1

0 0
1 0
1 1
1 0
0 0

convolutional filter 1

1 0 1
1 0
0
1
0 1

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 32

shared 2-d filters     convolutions

original image

convolving the image

result

1 1 1
0 1 1
0 0 1
0 0 1
0 1 1

0 0
1 0
1 1
1 0
0 0

convolutional filter 1

1 0 1
1 0
0
1
0 1

1
1
1
1
x1
x1

0
0
0
0
x0
x0
x1

0
0
0
0
x1
x1
x0

0
0
0
0
x1

0
0
0
0

1
1
1
1
x0
x0

1
1
1
1
x1
x1
x0

0
0
0
0
x0
x0
x1

0
0
0
0
x0

1
1
1
1

1
1
1
1
x1
x1
x1

1
1
1
1
x0
x0
x0
x1

x1

1

1
1

1
1
1

0
0
0
0
x0

1
1
1
1
x1

1
1
1
x0

1
1
1

0
0
0

0
0
0
0
x1

0
0
0
0
x0

1
1
1
x1

0
0
0

0
0
0

    

    

         ,              =    
    =       

   
    =       

44
4

inner product

                  ,                      (    ,     )

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 33

shared 2-d filters     convolutions

original image

convolving the image

result

1 1 1
0 1 1
0 0 1
0 0 1
0 1 1

0 0
1 0
1 1
1 0
0 0

convolutional filter 1

1 0 1
1 0
0
1
0 1

1
1
1
1
x1

0
0
0
0
x0
x1

0
0
0
0
x1
x0

0
0
0
0
x1

0
0
0
0

1
1
1
1
x0
x1

1
1
1
1
x0
x1
x0

0
0
0
0
x1
x0
x1

0
0
0
0
x0

1
1
1
1

1
1
1
1
x1
x0
x1

1
1
1
1
x1
x0
x0
x1

x0

1

1
1

1
1
1

0
0
0
0
x1
x0

1
1
1
1
x0
x1

1
1
1
x1
x0

1
1
1

0
0
0

0
0
0
0
x1

0
0
0
0
x0

1
1
1
x1

0
0
0

0
0
0

    

    

         ,              =    
    =       

   
    =       

44
4

3

inner product

                  ,                      (    ,     )

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 34

shared 2-d filters     convolutions

original image

convolving the image

result

1 1 1
0 1 1
0 0 1
0 0 1
0 1 1

0 0
1 0
1 1
1 0
0 0

convolutional filter 1

1 0 1
1 0
0
1
0 1

1
1
1
1
1
x1

0
0
0
0
0
x0
x1

0
0
0
0
0
x1
x0

0
0
0
0
0
x1

0
0
0
0
0

1
1
1
1
1
x0
x1

1
1
1
1
1
x0
x1
x0

0
0
0
0
0
x1
x0
x1

0
0
0
0
0
x0

1
1
1
1
1

1
1
1
1
1
x1
x0
x1

1
1
1
1
1
x1
x0
x0
x1

1
1
1
1
1
x0
x1
x1
x1
x0

1
1
1
1
1
x0
x1

1
1
1
1
1
x1

0
0
0
0
0
x1
x0

1
1
1
1
1
x0
x1

1
1
1
1
1
x1
x0
x0

1
1
1
1
1
x1

0
0
0
0
0
x0

0
0
0
0
0
x1

0
0
0
0
0
x0

1
1
1
1
1
x1
x1

0
0
0
0
0
x0

0
0
0
0
0

x1

    

    

         ,              =    
    =       

   
    =       

4
4
4
4
4

2
2

2

3
3
3
3

4

3

4
4
4

3

4

inner product

                  ,                      (    ,     )

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 35

output dimensions?

            
            

    

           
           

            

                

=

               

        

        

       

        

                

                =

                 =

                       

    

+ 1

                         

    

+ 1

                 =         

         =             

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 36

why call them convolutions?

definition the convolution of two functions      and      is denoted by     as the 
integral of the product of the two functions after one is reversed and shifted

   

   

                          

                                      =    

                                     

      

      

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 37

problem, again :s

o our images get smaller and smaller

o not too deep architectures

o details are lost

image

after conv 1 after conv 2

1
x1
0
x0
0
x1
0

1
x0
1
x1
0
x0
0

1
x1
1
x0
1
x1
1

0

1

1

0

1

1

1

0

0

0

1

0

0

4

2

2

3

4

3

4

3

4

18

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 38

solution? zero-padding!

o for s = 1, surround the image with (          1)/2 and (           1)/2 layers of 0

0

0

0

0

0

0

0

0

1

0

0

0

0

0

0

1

1

0

0

1

0

0

1

1

1

1

1

0

0

0

1

1

1

0

0

0

0

0

1

0

0

0

0

0

0

0

0

0

0

   

0 0 1
0
1 1
1 1
1

=

1

0

0

1

0

1

1

0

0

1

2

1

1

2

1

0

1

2

1

3

0

0

1

0

0

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 39

convolutional module (new module!!!)

o activation function

    
             =    
    =       

    

   
    =       

               ,                            

o essentially a dot product, similar to  linear layer

            ~                            

    

        

o gradient w.r.t. the parameters

                
                

       2    

       2    

=    
    =0

   
    =0

               ,           

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 40

convolutional module in tensorflow

import tensorflow as tf

tf.nn.conv2d(input, filter, strides, padding)

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 41

good practice

o resize the image to have a size in the power of 2
o use stride      = 1
o a filter of         ,          = [3    3] works quite alright with deep architectures
o add 1 layer of zero padding
o in general avoid combinations of hyper-parameters that do not click

    e.g.      = 1
    [                   ] = [3    3] and 
    image size                              = 6    6
                                        = [2.5    2.5]
    programmatically worse, and worse accuracy because borders are ignored

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 42

p.s. sometimes convolutional filters are not preferred

o when images are registered and each pixel has a particular significance

    e.g. after face alignment specific pixels hold specific types of inputs, like eyes, nose, etc.

o in these cases maybe better every spatial filter to have different parameters

    network learns particular weights for particular image locations [taigman2014]

eye location filters

nose location filters

mouth location filters

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 43

pooling

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 44

pooling

o aggregate multiple values into a single value

    invariance to small transformations
    reduces the size of the layer output/input to next layer     faster computations
    keeps most important information for the next layer

o max pooling

o average pooling

a b

c

d

z

                 (

) ==

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 45

max pooling (new module!)

o run a sliding window of size [       ,         ]
o at each location keep the maximum value
o activation function:  imax, jmax = arg max
    ,         (    ,    )
              = imax, j = jmax

o gradient w.r.t. input 

                
                

=    

1,
0,

o the preferred choice of pooling

                             =     [imax, jmax]

                                   

1

2

2

5

4

1

2

3

3

0

7

3

6

9

7

6

4

5

9

7

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 46

average pooling (new module!)

o run a sliding window of size [       ,         ]
o at each location keep the maximum value

o activation function:             =
                
                

o gradient w.r.t. input 

=

      ,         (    ,    )             
1
           

1
           

1

2

1

4

4

3

2

1

1

0

7

0

6

9

1

2

5

4

8

5

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 47

convnets for 

object recognition

this is the grumpy cat!

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 48

standard neural network vs convnets

neural network

l
a
y
e
r
 
1

l
a
y
e
r
 
2

l
a
y
e
r
 
3

l
a
y
e
r
 
4

l
a
y
e
r
 
5

l
o
s
s

convolutional neural network

conv. 
conv. 
layer 1
layer 1

relu
layer 1

max 
pool. 1

conv. 
layer 2

relu
layer 2

max 
pool. 2

l
a
y
e
r
 
5

l
a
y
e
r
 
6

l
o
s
s

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 49

convets in practice

o several convolutional layers

    5 or more

o after the convolutional layers non-linearities are added

    the most popular one is the relu

o after the relu usually some pooling

    most often max pooling

o after 5 rounds of cascading, vectorize last convolutional layer and connect 

it to a fully connected layer

o then proceed as in a usual neural network

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 50

id98 case study i: 
alexnet

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 51

architectural details

18.2% error in id163

227

227

11    11

5    5

3    3

3    3

3    3

max 
pool. 2

l
a
y
e
r
 
6

d
r
o
p
o
u
t

l
a
y
e
r
 
7

d
r
o
p
o
u
t

l
o
s
s

4,096

4,096

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 52

removing layer 7

1.1% drop in performance, 16 million less parameters

227

227

max 
pool. 2

l
a
y
e
r
 
6

d
r
o
p
o
u
t

l
a
y
e
r
 
7

d
r
o
p
o
u
t

l
o
s
s

4,096

4,096

11    11

5    5

3    3

3    3

3    3

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 53

removing layer 6, 7

5.7% drop in performance, 50 million less parameters

227

227

max 
pool. 2

l
a
y
e
r
 
6

d
r
o
p
o
u
t

l
a
y
e
r
 
7

d
r
o
p
o
u
t

l
o
s
s

4,096

4,096

11    11

5    5

3    3

3    3

3    3

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 54

removing layer 3, 4

3.0% drop in performance, 1 million less parameters. why?

227

227

max 
pool. 2

l
a
y
e
r
 
6

d
r
o
p
o
u
t

l
a
y
e
r
 
7

d
r
o
p
o
u
t

l
o
s
s

4,096

4,096

11    11

5    5

3    3

3    3

3    3

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 55

removing layer 3, 4, 6, 7

33.5% drop in performance. conclusion? depth!

227

227

max 
pool. 2

l
a
y
e
r
 
6

d
r
o
p
o
u
t

l
a
y
e
r
 
7

d
r
o
p
o
u
t

l
o
s
s

4,096

4,096

11    11

5    5

3    3

3    3

3    3

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 56

translation invariance

credit: r. fergus slides in deep learning summer school 2016

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 57

scale invariance

credit: r. fergus slides in deep learning summer school 2016

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 58

rotation invariance

credit: r. fergus slides in deep learning summer school 2016

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 59

id98 case study ii:
vggnet

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 60

differences from alexnet

o much more accurate

    6.8% vs 18.2% top-5 error

o about twice as many layers

    16 vs 7 layers

o filters are much smaller

    3x3 vs 7x7 filters

o harder/slower to train

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 61

id98 case study iii:
resnet [he2015]

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 62

how does resnet work?

o instead of modelling directly     (    ), 

model the residual                   

o adding identity layers should lead to 

larger networks that have at least
lower training error

o if not maybe optimizers cannot 
approximate identity mappings

o modelling the residual, the optimizer 

can simply set the weights to 0

id98 a

id98 b

id98

id98

why error b > error a since 
identity layer 1 just copies 
previous layer activations?

1

shortcut/skip connections

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 63

o without residual connections deeper 

networks are untrainable

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 64

other convnets

o google inception module

o two-stream network
    moving images (videos)

o network in network

o deep fried network

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 65

more cases

o two-stream network
    moving images (videos)

o network in network

o deep fried network

o resnet

    winner of ilsvrc 2016

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 66

o what are the convolutional neural networks?

o why are they so important for computer 

summary

vision?

o how do they differ from standard neural 

networks?

o how can we train a convolutional neural 

network?

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 67

reading material & references

o http://www.deeplearningbook.org/

    part ii: chapter 9

[he2016] he, zhang, ren, sun. deep residual learning for image recognition, cvpr, 2016
[simonyan2014] simonyan, zisserman/ very deep convolutional, networks for large-scale image recognition, arxiv, 2014
[taigman2014] taigman, yang, ranzato, wolf. deepface: closing the gap to human-level performance in face 
verification, cvpr, 2014
[zeiler2014] zeiler, fergus. visualizing and understanding convolutional networks, eccv, 2014
[krizhevsky2012] krizhevsky, hinton. id163 classification with deep convolutional neural networks, nips, 2012
[lecun1998] lecun, bottou, bengio, haffner. gradient-based learning applied to document recognition, ieee, 1998

uva deep learning course     efstratios gavves                                                                                    

deeper into deep learning and optimizations - 68

o what do convolutions look like?

o build on the visual intuition behind convnets

next lecture

o deep learning feature maps

o id21

uva deep learning course
efstratios gavves
deeper into deep learning and optimizations - 69

