neural networks
autoencoder - denoising autoencoder

2

hugo larochelle

overcomplete hidden layer

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: overcomplete representation
    hidden layer is overcomplete if greater than the input layer

october 17, 2012

math for my slides    autoencoders   .

    no compression in hidden layer
    each hidden unit could copy a 
different input component
    no guarantee that the 
hidden units will extract 
meaningful structure

ck

abstract

 x

w  = w 
(tied weights)

bj
h(x) = g(a(x))

= sigm(b + wx)
w

x

bx = o(ba(x))

= sigm(c + w   h(x))

math for my slides    autoencoders   .

topics: denoising autoencoder
    idea: representation should be 
robust to introduction of noise:
    random assignment of subset of 
inputs to 0, with id203
    gaussian additive noise

    reconstruction     computed 
from the corrupted input
    id168 compares 
reconstruction with the 
noiseless input

ck

abstract

october 17, 2012

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

= i   k,       1 (   >) 1    > u> x
= i   k,   (   >    ) 1    > u> x
= i   k,       1 u> x
3
= i   k,       1 (   >) 1    > u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
   k,   k (u  ,   k)>    x w    w
   k,   k (u  ,   k)>    x w    w

denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
 x
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    p(ex|x)     ex
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    bx = sigm(c + w   h(ex))
 x
    p(ex|x)     ex
 x
f =xj xk   @h(x(t))j
bx = o(ba(x))
||rx(t)h(x(t))||2
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
2pk(bxk   xk)2
    l(f (x(t))) =  pk   x(t)
k )   
k ) log(1  bx(t)

    l(f (x(t))) +  ||rx(t)h(x(t))||2
   

k log(bx(t)

= sigm(b + wx)
w

k ) + (1   x(t)

w  = w 
(tied weights)

= sigm(c + w   h(x))

bj
h(x) = g(a(x))

@x(t)

x

x

f

    f (x)    bx l(f (x)) = 1

math for my slides    autoencoders   .

topics: denoising autoencoder
    idea: representation should be 
robust to introduction of noise:
    random assignment of subset of 
inputs to 0, with id203
    gaussian additive noise

    reconstruction     computed 
from the corrupted input
    id168 compares 
reconstruction with the 
noiseless input

ck

abstract

october 17, 2012

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

= i   k,       1 (   >) 1    > u> x
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= i   k,       1 u> x
3
denoising autoencoder
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= v>
  ,   k v (   >    ) 1    > u> x
=     1
   k,   k (u  ,   k)> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
= i   k,       1 u> x
 x
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
=     1
   k,   k (u  ,   k)> x
t pt
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    p(ex|x)     ex
    bx = sigm(c + w   h(ex))
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
 x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t pt
    l(f (x(t))) +  ||rx(t)h(x(t))||2
    p(ex|x)     ex
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
0
   
 x
f =xj xk   @h(x(t))j
    p(ex|x)     ex
bx = o(ba(x))
||rx(t)h(x(t))||2
    p(ex|x)     ex
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
2pk(bxk   xk)2
    l(f (x(t))) =  pk   x(t)
k )   
k ) log(1  bx(t)
k ) + (1   x(t)

   k,   k (u  ,   k)>    x w    w

k log(bx(t)

= sigm(b + wx)
w

w  = w 
(tied weights)

= sigm(c + w   h(x))

bj
h(x) = g(a(x))

noise process

@x(t)

x

0

0

x

x

f

    f (x)    bx l(f (x)) = 1

math for my slides    autoencoders   .

topics: denoising autoencoder
    idea: representation should be 
robust to introduction of noise:
    random assignment of subset of 
inputs to 0, with id203
    gaussian additive noise

    reconstruction     computed 
from the corrupted input
    id168 compares 
reconstruction with the 
noiseless input

ck

abstract

october 17, 2012

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

= i   k,       1 (   >) 1    > u> x
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= i   k,       1 u> x
3
denoising autoencoder
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= v>
  ,   k v (   >    ) 1    > u> x
=     1
   k,   k (u  ,   k)> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
= i   k,       1 u> x
 x
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
=     1
   k,   k (u  ,   k)> x
t pt
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    p(ex|x)     ex
    bx = sigm(c + w   h(ex))
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
 x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t pt
    l(f (x(t))) +  ||rx(t)h(x(t))||2
    p(ex|x)     ex
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
0
   
 x
f =xj xk   @h(x(t))j
    p(ex|x)     ex
bx = o(ba(x))
||rx(t)h(x(t))||2
    p(ex|x)     ex
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
2pk(bxk   xk)2
    l(f (x(t))) =  pk   x(t)
k )   
k ) log(1  bx(t)
k ) + (1   x(t)

   k,   k (u  ,   k)>    x w    w

k log(bx(t)

= sigm(b + wx)
w

w  = w 
(tied weights)

= sigm(c + w   h(x))

bj
h(x) = g(a(x))

noise process

@x(t)

x

0

0

x

x

f

    f (x)    bx l(f (x)) = 1

= i   k,       1 u> x
  ,   k v (   >    ) 1 v> v    > u> x
= arg max
4
=     1
   k,   k (u  ,   k)> x
  ,   k v (   >    ) 1    > u> x
   ,   0
   k,   k (u  ,   k)>    x w    w

= arg max

(7)

   ,   0

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

x
x

  x

  x

= arg max

   ,   0

= arg min

   ,   0

eeq0(ex)[l(q0, ex)]
eeq0(x,ex,y )[log[p(y )p(x|y )p(ex|x)]]
eeq0(x,ex,y )[log p(x|y )]
eeq0(x,ex)[log p(x|y = f   (ex))]
eeq0(x,ex)hlih   x, g   0(f   (ex))   i

x
x

where the third line is obtained because (   ,    0)

we chose p(y ) uniform,

have no in   uence on eeq0(x,ex,y )[log p(y )] because
eeq0(x,ex)[log p(ex|x)], and the last line is obtained
p(x|y = f   (ex)) is a bg   0 (f   (ex)).

by inspection of the de   nition of lih in eq. 2, when

2

figure 2. manifold learning perspective.

suppose

ifold. corrupted examples (.) obtained by applying cor-

2

= i   k,       1 u> x
  ,   k v (   >    ) 1 v> v    > u> x
= arg max
4
=     1
   k,   k (u  ,   k)> x
  ,   k v (   >    ) 1    > u> x
   ,   0
   k,   k (u  ,   k)>    x w    w

= arg max

(7)

   ,   0

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

x
x

  x

  x

= arg max

   ,   0

= arg min

   ,   0

eeq0(ex)[l(q0, ex)]
eeq0(x,ex,y )[log[p(y )p(x|y )p(ex|x)]]
eeq0(x,ex,y )[log p(x|y )]
eeq0(x,ex)[log p(x|y = f   (ex))]
eeq0(x,ex)hlih   x, g   0(f   (ex))   i

x
x

where the third line is obtained because (   ,    0)

we chose p(y ) uniform,

have no in   uence on eeq0(x,ex,y )[log p(y )] because
eeq0(x,ex)[log p(ex|x)], and the last line is obtained
p(x|y = f   (ex)) is a bg   0 (f   (ex)).

by inspection of the de   nition of lih in eq. 2, when

2

figure 2. manifold learning perspective.

suppose

ifold. corrupted examples (.) obtained by applying cor-

2

= i   k,       1 u> x
  ,   k v (   >    ) 1 v> v    > u> x
= arg max
4
=     1
   k,   k (u  ,   k)> x
  ,   k v (   >    ) 1    > u> x
   ,   0
   k,   k (u  ,   k)>    x w    w

= arg max

(7)

   ,   0

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

x
x

  x

  x

= arg max

   ,   0

= arg min

   ,   0

eeq0(ex)[l(q0, ex)]
eeq0(x,ex,y )[log[p(y )p(x|y )p(ex|x)]]
eeq0(x,ex,y )[log p(x|y )]
eeq0(x,ex)[log p(x|y = f   (ex))]
eeq0(x,ex)hlih   x, g   0(f   (ex))   i

x
x

where the third line is obtained because (   ,    0)

we chose p(y ) uniform,

have no in   uence on eeq0(x,ex,y )[log p(y )] because
eeq0(x,ex)[log p(ex|x)], and the last line is obtained
p(x|y = f   (ex)) is a bg   0 (f   (ex)).

by inspection of the de   nition of lih in eq. 2, when

2

figure 2. manifold learning perspective.

suppose

ifold. corrupted examples (.) obtained by applying cor-

2

= i   k,       1 u> x
  ,   k v (   >    ) 1 v> v    > u> x
= arg max
4
=     1
   k,   k (u  ,   k)> x
  ,   k v (   >    ) 1    > u> x
   ,   0
   k,   k (u  ,   k)>    x w    w

= arg max

(7)

   ,   0

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

x
x

  x

  x

= arg max

   ,   0

= arg min

   ,   0

eeq0(ex)[l(q0, ex)]
eeq0(x,ex,y )[log[p(y )p(x|y )p(ex|x)]]
eeq0(x,ex,y )[log p(x|y )]
eeq0(x,ex)[log p(x|y = f   (ex))]
eeq0(x,ex)hlih   x, g   0(f   (ex))   i

x
x

where the third line is obtained because (   ,    0)

we chose p(y ) uniform,

have no in   uence on eeq0(x,ex,y )[log p(y )] because
eeq0(x,ex)[log p(ex|x)], and the last line is obtained
p(x|y = f   (ex)) is a bg   0 (f   (ex)).

by inspection of the de   nition of lih in eq. 2, when

2

figure 2. manifold learning perspective.

suppose

ifold. corrupted examples (.) obtained by applying cor-

2

filters (denoising autoencoder)
    no corrupted inputs (cross-id178 loss)

(vincent, larochelle, bengio and manzagol, icml 2008)

5

rect-img
convex

24.04  0.37
19.13  0.34

24.05  0.37
18.41  0.34
filters (denoising autoencoder)
    25% corrupted inputs

(vincent, larochelle, bengio and manzagol, icml 2008)

23.69  0.37
19.92  0.35

24.05  0.37
19.82  0.35

22.50  0.37
18.63  0.34
6

(a) no destroyed inputs

(b) 25% destruction

rect-img
convex

24.04  0.37
19.13  0.34

24.05  0.37
filters (denoising autoencoder)
18.41  0.34
    50% corrupted inputs

(vincent, larochelle, bengio and manzagol, icml 2008)

23.69  0.37
19.92  0.35

24.05  0.37
19.82  0.35

22.50  0.37
18.63  0.34
7

(a) no destroyed inputs

(b) 25% destruction

vincent, larochelle, lajoie, bengio, manzagol
8

squared error loss

    training on natural image patches, with squared-difference loss

    pca is not the best solution

data

filters

figure 5: regular autoencoder trained on natural image patches. left: some of the 12 12

vincent, larochelle, lajoie, bengio, manzagol
9

squared error loss

    training on natural image patches, with squared-difference loss

    not equivalent to weight decay

data

filters

figure 5: regular autoencoder trained on natural image patches. left: some of the 12 12

