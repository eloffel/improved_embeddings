neural networks
training crfs - general conditional random    eld

    v w(l+1) b(l+1)

p(y|x) = exp  kxk=1
kxk=2

a(l+1,0)(xk)yk +

general crf

vyk,yk+1+

a(l+1,+1)(xk+1)yk! /z(x)
topics: crfs in general
    we don   t have to restrict the crf structure to linear chains

a(l+1, 1)(xk 1)yk +

k 1xk=1
k 1xk=1

    w(l+1, 1) w(l+1,+1) w(l+1,0) b(l+1)

2

p(y|x) =

1

z(x)yf

 f (y, x)

grid structure
(pixels in image)

     f (y, x) =  f (yk, xk 1) = exp a(l+1, 1)(xk 1)c 1yk=c 
     f (y, x) =  f (yk, xk+1) = exp a(l+1,+1)(xk+1)c 1yk=c 
     f (y, x) =  f (yk, xk) = exp a(l+1,0)(xk)c 1yk=c 
     f (y, x) =  f (yk, yk+1) = exp vc,c0 1yk=c 1yk+1=c0 

    we could also have n-ary factors, with n>2

general pair-wise structure
(webpages sharing a link)

p

exp(au(y1) + ap(y1, y2)) ( =    1(y2) )

j
=

i

}

exp(au(y2) + ap(y2, y3))   1(y2) ( =    2(y3) )

1
=

j

  f0!s(i)

3

}

j
,

  

j

j

    1 exp(au(y1)) exp(au(y1)) py1

9

|

r

       k(yk+1)  k(yk 1)

2

{

  s!f (i) = yf02ne(s)\f
general crf

  f0!s(i)

1 exp(au(y2))

{

   

  s!f (i) = yf02ne(s)\f
    y1 y2 y3 x1 x2
    1 exp(au(y1)) exp(au(y1)) py1
topics: crfs in general
exp(au(y2))   1(y2) py2
   
    gradients in general crfs always take the form:
exp(au(y1) + ap(y1, y2)) ( =    1(y2) )
       k(yk+1)  k(yk 1)
exp(au(y2))   1(y2) py2
=     pf
    @ log p(y(t)|x(t))
=     pf

@    log  f (y(t), x(t))   eyhpf

exp(au(y2) + ap(y2, y3))   1(y2) ( =    2(y3) )
}
x
r
/2
x

@    log  f (y(t), x(t))   eyhpf
@    log  f (y, x(t))  x(t)i   

/2
r

2

2
x
{

x

}

r

make everything less likely

x

x

make y(t) more likely

|

(

)

r

|

h

(

)

h

@   

@

@

@

@

    the expectation over y will often need to be approximated, 
using loopy belief propagation
    it will often involve only a few of the yk variables

   

e
t

>i

r

u

   

>i

r

u

t
e

@    log  f (y, x(t))  x(t)i   

u

i

i

 
=

u
x

i

{

2

|

|

9

u

i

,

 

i

{

  

}

   

p

   

6
exp(au(y1) + ap(y1, y2)) ( =    1(y2) )

4

exp(au(y2) + ap(y2, y3))   1(y2) ( =    2(y3) )

1 exp(au(y2))

    1 exp(au(y1)) exp(au(y1)) py1

(loopy) belief propagation

exp(au(y2))   1(y2) py2
   
       k(yk+1)  k(yk 1)
topics: crfs in general
=     pf
    marginals can be approximated with:
    @ log p(y(t)|x(t))
exp(log  f (yk)+pf02ne(k)\f log   f0!k(yk))
    p(yk|x) =
exp(log  f (y0k)+pf02ne(k)\f log   f0!k(y0k))
py0k

@    log  f (y(t), x(t))   eyhpf

@   

@

    in general, an approximated marginal is computed by
1. summing all the log-factors that involve only the yk variables of interest
2. summing all the log-messages coming into the yk variables from other factors
3. exponentiating
4. renormalizing

@

@    log  f (y, x(t))  x(t)i   

