neural networks
training neural networks - activation function derivative

feedforward neural network
hugo larochelle

@

2

=

=

exp(ac )

exp(ac )

g0(a(k)(x)j)

@h(k)(x)j
@a(k)(x)j

ra(k)(x)   log f (x)y

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)
gradient computation

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
a(k)(x)j   log f (x)y
    x(t) y(t)
    w
@   log f (x)y
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    p(y = c|x)
@h(k)(x)j
    p(y = c|x)
    {
              pre-activation
@   log f (x)y
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
@h(k)(x)j
pc exp(ac) . . .
   
    g(a) = a
    gradient:
    p(y = c|x)
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=  rh(k)(x)   log f (x)y > ra(k)(x)h(k)(x)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=  rh(k)(x)   log f (x)y    [. . . , g0(a(k)(x)j), . . . ]
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
element-wise
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
product
@
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
  log f (x)y
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
w (k)
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
i,j
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@   log f (x)y
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
=
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
@a(k)(x)i
reminder
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x)j = g(a(k)(x)j)
@   log f (x)y
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
b(1)
    w (1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@a(k)(x)i
i
i,j
    h(k)(x) = g(a(k)(x))
@
    h(k)(x) = g(a(k)(x))
a(k)(x)j   log f (x)y

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

@a(k)(x)i
@w (k)
i,j
h(k 1)
j

september 6, 2012

    p(y = c|x)

abstract

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

exp(ac )

(x)

...

...

=

1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

f (x)c   log f (x)y =  1(y=c)

    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))

topics: linear activation function gradient

    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y

activation function

    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    a(x) = b +pi wixi = b + w>x
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    h(x) = g(a(x)) = g(b +pi wixi)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k 1)(x)
    partial derivative:
    g0(a) = 1

math for my slides    feedforward neural network   .

3
abstract

    x1 xd b w1 wd
    w
    {
    g(a) = a
    sigm(a) =

1

1+exp( a)

3

4

    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))

abstract

math for my slides    feedforward neural network   .

    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
topics: sigmoid activation function gradient

    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
activation function
    a(x) = b +pi wixi = b + w>x
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    h(x) = g(a(x)) = g(b +pi wixi)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    partial derivative:
    g0(a) = a
    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2

    x1 xd b w1 wd
    w
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)

3

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

5

math for my slides    feedforward neural network   .

topics: tanh activation function gradient

    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
activation function
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    x1 xd b w1 wd
    g0(a) = a
    partial derivative:
    w
    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

3

1

