ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

deep id114 ii

algorithms for approximate sgd

1

consider colorization

x is a black and white image.
y is a color image drawn from pop(y|x).

  y is an arbitrary color image.
q  (  y|x) is the id203 that model    assigns to the color
image y given black and white image x.

2

exponential softmax

(cid:88)

f (  y) =

f [  ,   y[  ]]

     hyperedges

      = argmin

  

e(x,y)   pop     log qf  (x)(y)

f.grad[  ,   y] = p  y   qf

(  y[  ] =   y)     1[y[  ] =   y]

the quantities p  y   qf

(  y[  ] =   y) are su   cient statistics.

3

estimation by sampling

we can estimate p  y   qf

(  y[  ] =   y) by sampling   y

hinton has proposed that this is the function of dreaming.

4

monte carlo markov chain (mcmc) sampling

metropolis algorithm

pick an initial graph label   y and then repeat:
1. pick a    neighbor      y(cid:48) of   y uniformly at random. the neigh-
bor relation must be symmetric. perhaps hamming distance
one.
2. if f (  y(cid:48)) > f (  y) update   y =   y(cid:48)
3. if f (  y(cid:48))     f (  y) then update   y =   y(cid:48) with id203 e   (f (  y)   f (  y(cid:48)))

5

markov processes and stationary distributions

a markov process is a process de   ned by a    xed state transi-

tion id203 p (  y(cid:48)|  y) = m  y(cid:48),  y.
let p t the id203 distribution for time t.

p t+1 = m p t

if every state can be reached form every state (ergodic process)
then p t converges to a unique stationary distribution p   

p    = m p   

6

metropolis correctness

to verify that the metropolis process has the correct stationary
distribution we simply verify that m p = p where p is the
desired distribution.

this can be done by checking that under the desired distribu-
tion the    ow from   y to   y(cid:48) equals the    ow from   y(cid:48) to   y (detailed
balance).

7

metropolis correctness

for f (  y)     f (  y(cid:48))

   ow(  y(cid:48)       y) =
   ow(  y       y(cid:48)) =

ef (  y(cid:48)) 1
n
e      f =
ef (  y) 1
n

1
z
1
z

ef (  y(cid:48)) 1
n

1
z

but detailed balance is not required in general (see hamilto-
nian mcmc).

8

id150

the metropolis algorithm wastes time by rejecting proposed
moves.

id150 avoids this move rejection.

in id150 we select a node i at random and change
that node by drawing a new node value conditioned on the
current values of the other nodes.

9

id150

qf (i =   y |   y)

= qf (  y[i] =   y |   y[1], . . . ,   y[i     1],   y[i + 1], . . . ,   y[i])
.

markov blanket property:

qf (i =   y |   y) = qf (i =   y |   y[n (i)])

id150, repeat:
    select i at random
    draw   y from qf (i =   y |   y)
      y[i] =   y

10

id150

let   y[i =   y] be the assignment   y(cid:48) equal to   y except   y(cid:48)[i] =   y.

qf (i =   y |   y) =

=

qf (  y[i] =   y)
  y qf (  y[i] =   y)

ef (  y[i=  y])
  y ef (  y[i=  y])

(cid:80)
(cid:80)

11

correctness proof

qf (  y) is a stationary distribution of id150.

    select i at random
    draw   y from qf (i =   y |   y)
      y[i] =   y

the distribution before the update equals the distribution after
the update.

12

pseudolikelihood

in pseudolikelihood we replace the objective     log qf (  y) with
the objective     log   qf (  y) where

  qf (  y)

.
=

qf (i =   y[i] |   y)

loss(f )

=     log   q(y)
.

(cid:89)
(cid:88)

i

i

13

f.grad[  ,   y] =

       log qf [i =   y[i] |   y]/   f [  ,   y]

pseudolikelihood consistency

ey   pop     log   q(y) = pop

argmin

q

14

proof of consistency i

we have

min
q

ey   pop     log   q(y)     ey   pop     log(cid:103)pop(y)

if we can show

ey   pop     log   q(y)     ey   pop     log(cid:103)pop(y)

min
q

then the minimizer (the argmin) is pop as desired.

15

proof of consistency ii

we will prove the case of two nodes.

min
q

    min
q1,q2

ey   pop    log q(y[1]|y[2]) q(y[2]|y[1])
ey   pop    log q1(y[1]|y[2]) q2(y[2]|y[1])
ey   pop    log q1(y[1]|y[2]) + min
q2

= min
q1
= ey   pop    log pop(y[1]|y[2]) + ey   pop    log pop(y[2]|y[1])
= ey   pop    log(cid:103)pop(y|x)

ey   pop    log q2(y[2]|y[1])

contrastive divergence

algorithm (cdk): run k steps of mcmc for qf (  y) start-
ing from y to get   y.

then set

f.grad[  ,   y] = 1[  y[  ] =   y]     1[y[  ] =   y]

theorem: if qf (  y) = pop then

ey   pop 1[  y[  ] =   y]     1[y[  ] =   y] = 0

here we can take k = 1     no mixing time required.

17

end

