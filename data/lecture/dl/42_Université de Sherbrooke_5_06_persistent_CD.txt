neural networks
restricted id82 - persistent cd

   

    p(x)
eh [rwe(x, h)|x ] =  h(x) x>
   

cd-k: pseudocode

i. generate a negative sample     using
k steps of id150, starting at 

w (= w         rw   log p(x(t))   
   
(= w         ehhrwe(x(t), h)   x(t)i   ex,h [rwe(x, h)]   
topics: contrastive divergence
   
(= w         ehhrwe(x(t), h)   x(t)i   eh [rwe(  x, h)|  x ]   
1. for each training example
    x(t)   h(t) = h0
(= w +       h(x(t)) x(t)>   h(  x)   x>   
  x
   
    x(t)   h(t) = h0
   
w (= w +       h(x(t)) x(t)>   h(  x)   x>   
b (= b +       h(x(t))   h(  x)   
   
c (= c +       x(t)     x   
   
2. go back to 1 until stopping criteria

ii. update parameters

2

1

l(f (x(t))) =

l(f (x(t))) =
1

t xt
t xt
= eh    @e(x(t), h)
= eh    @e(x(t), h)

(14)
@   log p(x(t))
(15)

@   
@   log p(x(t))

(16)
@   
(17)
(18)

(20)

(21)
(22)

eh    @e(x(t), h)

@   

(19)

eh    @e(x(t), h)
ex,h    @e(x, h)

@   

@   

ex,h    @e(x, h)

    (x(t),   h(t))

    (x(t),   h(t))

persistent cd (pcd)

(tieleman, icml 2008)

   

   

    p(x)

topics: persistent contrastive divergence

1

1

l(f (x(t))) =

@   log p(x(t))

t xt
= eh    @e(x(t), h)
...

t xt   log p(x(t))
    idea: instead of initializing the chain to       , initialize 
    x(t)   h(t) = h0
    p(x)
the chain to the negative sample of the last iteration
   
   
   x(t)    ex,h    @e(x, h)
t xt
= eh    @e(x(t), h)

@   log p(x(t))

l(f (x(t))) =

  p(x|h)

 

@   

@   

   

@   

@   

@   

1

    x(t)   h(t) = h0
   
  p(h|x)

x1

    x(t)   h(t) = h0
   

xk =   x

negative sample

    (x(t),   h(t))
eh    @e(x(t), h)

   x(t)     

3

1

t xt
= eh    @e(x(t), h)

@   log p(x(t))

@   

1

eh    @e(x(t), h)
t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

ex,h    @e(x, h)

   

   

   

persistent cd (pcd)

(tieleman, icml 2008)

    p(x)

topics: persistent contrastive divergence

1

1

l(f (x(t))) =

t xt   log p(x(t))
    idea: instead of initializing the chain to       , initialize 
    x(t)   h(t) = h0
the chain to the negative sample of the last iteration
   
   x(t)    ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)
...

@   log p(x(t))

@   

@   

@   

    x(t)   h(t) = h0

  p(h|x)

  p(x|h)

x1

xk =   x

    (x(t),   h(t))

negative sample

3

1

t xt
= eh    @e(x(t), h)

@   log p(x(t))

@   

 

eh    @e(x(t), h)

ex,h    @e(x, h)

   

   

   

persistent cd (pcd)

(tieleman, icml 2008)

    p(x)

topics: persistent contrastive divergence

1

1

l(f (x(t))) =

t xt   log p(x(t))
    idea: instead of initializing the chain to       , initialize 
    x(t)   h(t) = h0
the chain to the negative sample of the last iteration
   
   x(t)    ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)
...

@   log p(x(t))

@   

@   

@   

    x(t)   h(t) = h0

  p(h|x)

  p(x|h)

  x

comes from the
previous iteration

x1

xk =   x

    (x(t),   h(t))

negative sample

3

1

t xt
= eh    @e(x(t), h)

@   log p(x(t))

@   

 

eh    @e(x(t), h)

ex,h    @e(x, h)

