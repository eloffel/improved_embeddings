neural networks
id161 - convolutional network

id161

2

topics: id161
    we can design neural networks that are speci   cally adapted 
for such problems
    must deal with very high-dimensional inputs 

- 150 x 150 pixels = 22500 inputs, or 3 x 22500 if rgb pixels

    can exploit the 2d topology of pixels (or 3d for video data)
    can build in invariance to certain variations we can expect 

-

translations, illumination, etc.

    convolutional networks leverage these ideas

    local connectivity
    parameter sharing
    pooling / subsampling hidden units

convolutional network

3

topics: convolutional network
    convolutional neural network alternates between the 
convolutional and pooling layers

r  seau de neurones    convolution: 

r  seau complet 

ift%615%

(from yann lecun)

hugo%larochelle%

51%

{

fully

connected

convolutional network

4

topics: convolutional network
    output layer is a regular, fully connected layer with softmax 
non-linearity
    output provides an estimate of the id155 of each class

    the network is trained by stochastic id119

    id26 is used similarly as in a fully connected network
    we have seen how to pass gradients through element-wise activation function
    we also need to pass gradients through the convolution operation and the 

pooling operation

convolutional network

5

topics: gradient of convolution layer
    let l be the id168

    for convolution operation yj = xi * kij the gradient for xi is

                 x l =    (   y l) * (wij)

j

i

j

and the gradient for wij is 

                          w  l = (   y l) * xi
~

ij

where * is the convolution with zero padding and xi is the row/column    ipped 
version of xi 

~

hugo larochelle

convolutional network

hugo larochelle
d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

topics: gradient of pooling layer
    let l be the id168
november 8, 2012

november 8, 2012

6

math for my slides    id161   .

    for max pooling operation yijk = max xi,j+p,k+q the gradient for xijk is
              x   l = 0 except for      x         l =    y  l  

abstract

abstract
p,q

i,j+p   ,k+q   

ijk

ijk

math for my slides    id161   .

    h x    

where p   , q    = argmax xi,j+p,k+q
-

yijk = max
p,q

xi,j+p,k+q

in words, only the       winning       units in layer x get the gradient from the pooled layer

1

xi,j+p,k+q

yijk = max
p,q
yijk =

m2xp,q
    for average pooling operation                                    the gradient for xijk is
                x l =    upsample(   y l)
m2xp,q
where upsample inverts subsampling

xi,j+p,k+q

xi,j+p,k+q

yijk =

1

    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
     jk = (pipq wpqv2

    yijk = vijk/ max(c,  jk)

i,j+p,k+q)1/2

