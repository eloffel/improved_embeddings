neural networks
training crfs - discriminative vs. generative learning

 (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))

@   log p(y|x)
@ap(y0k, y0k+1)

generative vs. discriminative

=  (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))

2

 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1)  

=  (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))
=

p(yk, yk+1|x)!
k 1xk=1
k 1xk=1
 (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))

topics: discriminative learning, generative learning
    in discriminative learning, we optimize the conditional 
likelihood
    crfs are discriminative 

@   log p(y|x)

@vy0k,y0k+1

      log p(y, x) =   log (p(y|x)p(x)) =   log p(y|x)   log p(x)   log p(y|x)
rv   log p(y|x) =

 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1)  

    in generative learning, we optimize the joint log-likelihood:

p(yk, yk+1|x)!

2

      log p(y, x) =   log (p(y|x)p(x)) =   log p(y|x)   log p(x)   log p(y|x)
    id48s are usually trained generatively
                       is similar to a regularizer

k 1xk=1

      log p(y, x) =   log (p(y|x)p(x)) =   log p(y|x)   log p(x)   log p(y|x)

 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1)  

p(yk, yk+1|x)!

k 1xk=1

k 1xk=1

 (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))

2

2

generative vs. discriminative

3

topics: generative learning, discriminative learning
    it can be shown that:

    if model is well-speci   ed (i.e. is the true model) generative learning is better

0.5
0.4
0.3
0.2
0.1
0

generative

discriminative

training set size

generative vs. discriminative

4

topics: generative learning, discriminative learning
    it can be shown that:

    if model is not well-speci   ed (i.e. most of the time), it depends:

generative

discriminative

0.5
0.4
0.3
0.2
0.1
0

    see these papers for more details:

training set size

- on discriminative vs. generative classi   ers: a comparison of id28 and naive 

bayes. andrew ng and michael jordan, 2001

