neural networks
id161 - example

convolutional network

2

topics: convolutional network
    these operations are inserted after the convolutions and 
before the pooling

jarret et al. 2009

    images should also be preprocessed by
figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
    converting to grayscale (if appropriate)
through a non-linear    lterbank, followed by recti   cation, local
    resizing images to 150 x 150 pixels (use zero padding for non-square images)
contrast id172 and spatial pooling/sub-sampling.
    removing (intra image) mean and dividing by standard deviation of the image 
    applying local contrast id172
layer with 4x4 down-sampling is denoted: p 4  4
a .
max-pooling and subsampling layer - pm : building lo-

convolutional network

3

topics: initialization of parameters
    initialization of parameters:

jarret et al. 2009

    can do as in regular neural network and initialize them randomly
    can also use unsupervised pretraining approach

-

to use unsupervised neural networks we   ve seen so far, we have to convert pretraining as a 
patch-wise learning problem
    extract patches of the same as the receptive    elds of the hidden units, at random positions
    train an unsupervised neural network (rbm, autoencoder, sparse coding) on those patches
    use weights connecting an input patch to each hidden unit to initialize each feature map parameters
    map images through all feature maps and repeat previous steps, for as many layers as desired

    we will compare:

    using random initialization (r) or unsupervised pretraining (u)
    using    ne-tuning of whole network (+) or only training output layer (no +)

convolutional network

4

topics: convolutional network
    results on caltech:

fid19 = convolution layer
r = recti   cation layer
n = local contrast id172 layer
pm = max pooling layer, pa = average pooling layer

jarret et al. 2009

single stage system: [64.f9  9

id19     r/n/p5  5] - log reg

rabs     n     pa rabs     pa

n     pm

50.0%
47.0%

43.3%(  1.6)

31.7%

44.3%
38.0%
44.0%
32.1%

n     pa
18.5%
16.3%
17.2%
15.3%

pa

14.5%
14.3%
13.4%

12.1%(  2.2)

54.2%
54.8%
52.2%
53.3%
52.3%

g
two stage system: [64.f9  9

id19     r/n/p5  5]     [256.f9  9

id19     r/n/p4  4] - log reg

rabs     n     pa rabs     pa

n     pm

60.5%
59.5%
46.7%

65.5%
64.7%
63.7%
62.9%
55.8%
single stage: [64.f9  9
64.0%

33.7%(  1.5)

61.0%
60.0%
56.0%

37.6%(  1.9)

n     pa
34.0%
31.0%
23.1%
19.6%

pa

32.0%
29.7%
9.1%
8.8%

id19     rabs/n/p5  5

a ] - pmk-id166

u+
r+
u

r

u+u+
r+r+
uu

rr

gt

u

convolutional network

5

topics: random    lters
    results on caltech:

jarret et al. 2009

    random    lters are surprisingly good
    turns out that random    lters give units that are still sensitive to a particular 

frequency
- can analyze this by    nding input which maximizes the activation of a given hidden unit (with 

gradient ascent applied in input space)

random    lters

optimal input

learned    lters

optimal input

figure 4. left: random stage-1    lters, and corresponding optimal inputs that maximize the response of each corresponding complex cell in
a fid19     rabs     n     pa architecture. the small asymmetry in the random    lters is suf   cient to make them orientation selective. middle:

convolutional network

6

topics: importance of architecture
    results on caltech:

jarret et al. 2009

    choice of right architecture can be more important than learning algorithm

-

-

the use of recti   cation and local contrast id172 layers is important
this is particularly true if little training data 

    results on norb:

    architecture makes less of a

substitute for unsupervised training.
5. it is clear that abs recti   cation is a crucial component for
achieving good performance, as shown with the u +u + pro-
difference with lots of 
tocol by comparing columns n     pa (31.0%), rabs     pa
training data per class
(60.0%), and rabs     n     pa (65.5%). however, using
    random    lters are also
max pooling seems to alleviate the need for abs recti   ca-
tion, con   rming the hypothesis that average pooling with-
out recti   cation falls victim to cancellation effects between
neighboring    lter outputs. this is an extremely important
fact for users of convolutional networks, in which recti   ca-
tion has not been traditionally used.
6. a single-stage system with pmk-id166 reaches the same

not as good

e

t
a
r
 
r
o
r
r
e

50

40
35
30
25

20

15

10
9
8
7
6

 

4
20

 

f

f

f

id19

id19

id19

     p

 (r+ r+)
a
     r

     n     p

abs

abs

     r

     n     p

 (rr)

a

 (r+ r+)
a

50

100

200

500

1000

2000

4860

figure 3. test error rate vs. number of training samples per class

number of training samples per class

