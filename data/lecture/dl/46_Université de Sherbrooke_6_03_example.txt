neural networks
autoencoder - example

hugo larochelle

math for my slides    autoencoders   .
d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

autoencoder
d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: autoencoder, encoder, decoder, tied weights
hugo.larochelle@usherbrooke.ca
    feed-forward neural network trained to reproduce its input at 
= sigm(b + wx)
october 17, 2012
the output layer

h(x) = g(a(x))

october 16, 2012
decoder

2

ck

 x

abstract

= w 
w 
(tied weights)

math for my slides    autoencoders   .

 
 
    f (x)    bx l(f (x)) =pk(bxk   xk)2

= sigm(c + w   h(x))

= sigm(b + wx)

h(x) = g(a(x))

bx = o(ba(x))

w

x

bj

abstract
= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

 
for binary inputs
encoder
h(x) = g(a(x))

= sigm(b + wx)

example of data set: mnist

3

larochelle, bengio, louradour and lamblin

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

figure 5: samples from the mnist digit recognition data set. here, a black pixel corresponds to
an input value of 0 and a white pixel corresponds to 1 (the inputs are scaled between 0

of the    rst hidden layer is obtained by a dot product of such a weight    image    with the
input image. in these images, a black pixel corresponds to a weight smaller than    3 and
a white pixel to a weight larger than 3, with the different shades of gray corresponding
to different weight values uniformly between    3 and 3.

filters (autoencoder)

(larochelle et al., jmlr2009)

93

4

1
1

0.9
0.9

0.8
0.8

0.7
0.7

0.6
0.6

0.5
0.5

0.4
0.4

0.3
0.3

0.2
0.2

0.1
0.1

0
0

0
0

0.1
0.1

0.2
0.2

0.3
0.3

0.4
0.4

0.5
0.5

0.6
0.6

0.7
0.7

0.8
0.8

0.9
0.9

1
1

figure 6.7     input weights of a random subset of the hidden units, learned by an autoas-
figure 6.6     display of the input weights of a random subset of the hidden units, learned

