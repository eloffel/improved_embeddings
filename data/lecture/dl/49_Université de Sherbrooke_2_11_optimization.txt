neural networks
training neural networks - optimization

optimization

2

topics: local optimum, global optimum, plateau
    notes on the optimization problem

    there isn   t a single global optimum (non-id76)

- we can permute the hidden units (with their connections) and get the same function
- we say that the hidden unit parameters are not identi   able

    optimization can get stuck in local minimum or plateaus

optimization

3

topics: local optimum, global optimum, plateau

neural network training demo 

(by andrej karpathy)

http://cs.stanford.edu/~karpathy/id166js/demo/demonn.html

4

2   

2   

2   
2   

id119

    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    b(3) b(2) b(1)
@x     f (x+   ) f (x    )
    @f (x)
    w(3) w(2) w(1) x f (x)
    w(3) w(2) w(1) x f (x)
    h(2)(x) = g(a(2)(x))
@x     f (x+   ) f (x    )
    w(3) w(2) w(1) x f (x)
    @f (x)
    w(3) w(2) w(1) x f (x)
    h(1)(x) = g(a(1)(x))
@x     f (x+   ) f (x    )
    @f (x)
    f (x) x    
@x     f (x+   ) f (x    )
    @f (x)
@x     f (x+   ) f (x    )
    @f (x)
2   
    f (x) x    
topics: convergence conditions, decrease constant
2   
    b(3) b(2) b(1)
@x     f (x+   ) f (x    )
    @f (x)
    f (x) x    
    f (x +    ) f (x      )
    stochastic id119 will converge if
    f (x) x    
    f (x +    ) f (x      )
    f (x) x    
    w(3) w(2) w(1) x f (x)
    f (x +    ) f (x      )
    p1t=1    t = 1
    f (x) x    
    p1t=1    t = 1
   
@x     f (x+   ) f (x    )
    f (x +    ) f (x      )
    @f (x)
    p1t=1    t = 1
    f (x +    ) f (x      )
 
    p1t=1    2
    f (x +    ) f (x      )
    p1t=1    2
    f (x) x    
    p1t=1    2
   
t < 1
    p1t=1    t = 1
t < 1
    p1t=1    t = 1
t < 1    t
    f (x +    ) f (x      )
where        is the learning rate of the tth update
t < 1    t
       t =    
    p1t=1    2
    p1t=1    t = 1
    p1t=1    2
t < 1    t
t < 1    t
    decreasing strategies:      (    is the decrease constant)
       t =    
t  0.5 <       1  
    p1t=1    2
t < 1    t
     
       t =    
       t =    
1+ t
1+ t
       t =    
       
                           (o                      )
       t =    
      1  
       t =    
t  0.5 <       1  
t 
    better to use a    xed learning rate for the    rst few updates

    p1t=1    t = 1
    p1t=1    2

1+ t

1+ t

t 

 

2   

5

    w(3) w(2) w(1) x f (x)
@x     f (x+   ) f (x    )
    @f (x)
    f (x) x    
topics: mini-batch, momentum
    f (x +    ) f (x      )
    can update based on a mini-batch of example (instead of 1 example): 

id119

2   

    the gradient is the average regularized loss for that mini-batch
    can give a more accurate estimate of the risk gradient
    can leverage matrix/matrix operations, which are more ef   cient

t < 1    t

    p1t=1    t = 1
    p1t=1    2
       t =    
       t =    
    r(t)

1+ t

    can use an exponential average of previous gradients:

t  0.5 <       1  

    = r   l(f (x(t)), y(t)) +  r(t 1)
    can get through plateaus more quickly, by       gaining momentum      

   

   

   

   

6

1+ t

1+ t

t < 1    t

)
t
(

t  0.5 <       1  

t  0.5 <       1  

 

r

 

id119

    f (x) x    
    p1t=1    t = 1
t < 1    t
    f (x) x    
    f (x +    ) f (x      )
    p1t=1    2
    f (x +    ) f (x      )
    p1t=1    t = 1
    = r   l(f (x(t)), y(t)) +  r(t 1)
    p1t=1    t = 1
       t =    
    p1t=1    2
    p1t=1    2
t < 1    t
topics: newton   s method
       t =    
t  0.5 <       1  
t < 1    t
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
       t =    
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
    if we locally approximate the loss through taylor expansion:
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
=
       t =    
       t =    
t  0.5 <       1  
1+ t
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  (         (t))
r
)
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
       t =    
1
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
t  0.5 <       1  
+
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
(r   l(f (x;    (t)), y))
   
t
   l(f (x;    (t)), y)    (         (t))
+0.5(         (t))>   r2
(
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
    {    (t)     l(f (x;    ), y)
true
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
approx
hessian
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
   
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2
(r   l(f (x;    (t)), y))
       (t+1) =    (t)   r2
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
+0.5(         (t))>   r2
    {    (t)     l(f (x;    (t)), y)
    we could minimize that approximation, by solving:
    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2

   l(f (x;    (t)), y)  (         (t))
   l(f (x;    (t)), y)  1
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
   l(f (x;    (t)), y)    (         (t))
4
   l(f (x;    (t)), y)  (         (t))

   l(f (x;    (t)), y)  1

(r   l(f (x;    (t)), y))

    {    (t)     l(f (x;    (t)), y)

(r   l(f (x;    (t)), y))

   

   

   

{

(r   l(f (x;    (t)), y))

   

4

   

4

4

   

   

id119
+0.5(         (t))>   r2

7
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
   l(f (x;    (t)), y)    (         (t))

topics: newton   s method
    we can show that the minimum is: 

    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2

   l(f (x;    (t)), y)  1

    {
    only practical if:
    few parameters (so we can invert hessian)
    locally convex (so the hessian is invertible)

   l(f (x;    (t)), y)  (         (t))

(r   l(f (x;    (t)), y))

4

    see recommended readings for more on optimization of 
neural networks

