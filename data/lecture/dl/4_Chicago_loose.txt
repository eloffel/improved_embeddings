ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

gradients as dual vectors

hessian-vector products

information geometry

1

coordinates

for a vector space we can make an arbitrary choice of basis
vectors b1, . . ., bn that are linearly independent and span the
space.

a basis de   nes coordinates x[i] for each vector x.

x = x[1]b1 +        + x[n ]bn

the basis, and the induced coordinate system, is arbitrary.

2

orthogonality is coordinate-relative

in two dimensions the vectors represented by (1, 0) and (0, 1)
need not be orthogonal.

(1, 0) represents b1 and (0, 1) represents b2.

we only require that b1 and b2 are independent.

hence inner product is coordinate-relative.

inner products in taylor expansions

are coordinate-independent

f (   +      )     f (  ) + [      f (  )] (     )

4

what is a gradient?

the gradient       f (  ) is the change in f per change in   .

more formally,       f (  ) is a linear function from       to    f .

f (   +      )     f (  ) + [      f (  )] (     )

5

coordinate-free de   nition of the gradient

f (   +      )     f (  ) + [      f (  )] (     )
f (   +       )     f (  ) + [      f (  )] (      )
f (   +       )     f (  )

    [      f (  )] (     )

 

[      f (  )] (     )

.
= lim
    0

f (   +       )     f (  )

 

no coordinates required.

6

dual vectors

a dual vector is a linear function from vectors to scalars.

the gradient is a dual vector.

7

coordinates

we calculate

using coordinates.

[     f (  )]      
(cid:88)

(cid:20)    f

     [i]

i

(cid:21)

     [i]

[     f (  )]       =

but this calculation is coordinate-independent.

[      f (  )] (     )     lim
    0

f (   +       )     f (  )

 

8

strange coordinate systems
consider any gradient      f (  ) at any value of   .
for any such situation, and any vector       with [     f (  )]       >
0, and any learning rate    > 0, there exists a coordinate system
in which     .grad[c] =      [c] and hence

  t+1 =   t          

note that gradient decent always yields [     f (  )]       > 0.

9

de   ne the basis vectors b1, . . ., bn by

proof

.
=

b1

[     f (  )] bi = 0 for i > 1

in this coordinate system we have

   =   [1]b1 + . . . +   [n ]bn

     

(cid:112)  [     f (  )]     
(cid:112)[     f (  )]     

=

   

  

  .grad[i]  [i]bi =

i

10

   f (  )
     [1]

(cid:88)

= 0 for j > 0

(cid:112)[     f (  )]     

   f (  )
     [j]
   

  

b1 =

     
  

coordinate-free versions of sgd

11

newton   s method

we can make a second order approximation to the id168

f (   +      )     f (  ) + (      f (  ))      +

     (cid:62)h     

1
2

where h is the second derivative of f , the hessian, equal to
           f (  ).
again, no coordinates are needed     we can de   ne the operator
      generally indpendent of coordinates.

     (cid:62)

1 h      2 = (      ((      ft(  ))         1))         2

12

newton   s method

we consider the    rst order expansion of the gradient.
      f (  ) |  +          (      f (  ) |  ) + h     

we approximate       by setting this gradient approximation to
zero.

0 =       f (  ) + h     
      =    h   1       f (  )

this gives newton   s method (without coordinates)

   -= h   1       f (  )

newton updates

it seems safer to take smaller steps. so it is common to use

   -=    h   1       f (  )

for        (0, 1) where    is naturally dimensionless.

most second order methods attempt to approximate making
updates in the newton direction.

14

the gradient covariance martix

.

= et (  gt     g)(  gt     g)(cid:62)

  

  t+1 =   t            1     loss(  , xt, yy)

this is related to rmsprop.

15

information geometry and the natural gradient

we consider the case where loss is determined by a id203
distribution. for example     log p (y).

the set of all distributions p forms a manifold.

for a given point (distribution) p we can consider the ball

b (p ) = {q | kl(p, q)      }

   p = argmin
   p   b (p )

f (p +    p )

16

distance functions de   ne a point-wise inner product

for any smooth (doubly di   erentiable) function d(x, y) with
d(x, y)     0 and d(x, x) = 0 we must have

      x d(x, x +    x)|   x=0 = 0
d(x, x +    x)        x(cid:62)h   x
=       x      xd(x, x +    x)|   x=0
.

h

the coordinate-independent gradient direction de   ned by d(x, y)
is then

h   1   xf (x)

information geometry and id119

for kl divergence h is diagonal with

   p(cid:62)h   p =

   p (y)2
p (y)

(cid:88)

y

although kl is not symmetric, h happens to be the same
for kl(p, p +    p ) and kl(p +    p, p ).

18

hessian-vector products

(cid:16)

h      =      

(      f t(  ))         

(cid:17)

this is supported in pytorch     in pytorch   .grad is a vari-
able while   .grad.data is a tensor.

19

hessian-vector products

for id26 to be e   cient it is important that the
value of the graph is a scalar (like a loss). but note that for v
   xed we have that

(      f t(  ))    v

is a scalar and hence its gradient with respect to   , which is
hv, can be computed e   ciently.

20

complex-step di   erentiation

consider a function f : r     r de   ned by a computer pro-
gram.

for c code on a cpu we can run program the program on
complex numbers simply by changing the data type of x.

james lyness and cleve moler, numerical di   erentiation of
analytic functions siam j. of numerical analysis, 1967.

21

complex-step di   erentiation

consider f (x + i ) at real input x and consider the    rst order
taylor expansion.

f (x + i ) = f (x) + i(df /dx) 

note that f (x) and df /dx must both be real. therefore

im(f (x + i )) =  (df /dx)

im(f (x + i ))

 

df
dx

=

22

complex-step di   erentiation

df
dx

=

im(f (x + i ))

 

this is vastly better than

    f (x +  )     f (x)

 

df
dx

the point is that in complex arithmetic the real and imaginary
parts have independent    oating point representations.
in 64 bit    oating point arithmetic   can be taken to be 2   50.
for   = 2   50, division by   simply changes the exponent of the
   oating point representation leaving the mantissa unchanged.

23

first order polynomial arithmetic

numerically, complex-step di   erentiation is equivalent to    rst
order polynomial arithmetic.

(a + b )(a(cid:48) + b(cid:48) ) = (a + a(cid:48)) + (ab(cid:48) + a(cid:48)b) 

di   erentiation based on    rst order polynomial arithmetic is
exact.

24

equivalence to polynomial arithmetic

(a + ib )(a(cid:48) + ib(cid:48) ) = (a + a(cid:48)     bb(cid:48) 2) + i(ab(cid:48) + a(cid:48)b) 

  = 2   50

here the  2 term is below the precision of a + a(cid:48).

numerically, complex-step arithmetic and    rst order polyno-
mial arithmetic are the same.

25

hessian-vector products

we are interested in computing htv for v = (   (cid:12)   g).

htv =

im(     f (  )|  +i v)

 

  = 2   50

26

second order sgd

focusing on the hessian.

rudin   s blog

27

quasi-id77s

it is often faster and more e   ective to approximate the hessian.
maintain an approximation m     h   1.
repeat:
       -=   m      f (  ) (   is often optimized in this step).
    restimate m .

the restimation of m typically involves a    nite di   erence

(cid:16)      f (  ) |  t+1

(cid:17)    (cid:0)      f (  ) |  t

(cid:1)

as a numerical approximation of h     .

28

quasi-id77s

conjugate gradient

bfgs

limited memory bfgs

29

issues with quasi-id77s

in sgd the gradients are random even when    does not change.

we cannot use

(cid:16)      ft+1(  )|  t+1

(cid:17)    (cid:0)      ft(  ) |  t

(cid:1)

as an estimate of h     .

30

issues

    gradient estimation. the accuracy of   g as an estimate
of g.

    gradient drift (second order structure). the fact
that g changes as the parameters change.

    convergence. to converge to a local optimum the learn-
ing rate must be gradually reduced toward zero.

31

the classical convergence theorem

  t+1 =   t       t      loss(  , xt, yt)

for su   ciently smooth id168s, and holding the coordi-
nate system constant through time, and for

(cid:88)

t

  t =    ,

  t > 0

and

t         t = 0
lim

and

the loss value of sgd converges.

32

structure from motion

see the videos

https://www.youtube.com/watch?v=i7iervkxya8
http://www.mada.org.il/brain/shape/shape.html

33

the levenberg-marquart algorithm (bundle adjustment)

loss = e(x,y)   train

||f  (x)     y||2

1
2

f  +     (xt)     f  (xt) + jt     

jt =       f  (xt)
  gt = (f  (xt)     yt)jt = rtjt
rt = f  (xt)     yt

34

the levenberg-marquart algorithm

loss(   +      )     et

||rt + jt     ||2

1
2

minimizing this squared error over the choice of       is a least
squares regression problem.

0 = et (rt + jt     )jt
0 = (et rtjt) + et j(cid:62)

t jt     

(et j(cid:62)

t jt)      =    et   gt
      =    (et j(cid:62)

t jt)   1(et   gt)

a general id168

t       + (jt     )(cid:62)ht jt     

losst(f (   +      ))     lt +   g(cid:62)
=       f (  )
.
=    f   f losst(f )
.

ht

jt

setting the gradient to zero and solving for      :

     t =    (et j(cid:62)

t ht jt)   1  gt

36

the levenberg-marquart algorithm for log loss

  t+1 =   t       (et j(cid:62)

t ht jt)   1  gt

loss(  , x, y) =     log qf  (x)(y)

qf (y) = softmax

y

f (y)

ht = ey   qft

(  y     qft)(  y     qft)(cid:62)

= diag(qft)     qftqt

ft

37

end

