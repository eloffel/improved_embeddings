neural networks
training neural networks - hidden layer gradient

   

   

   

arg min

abstract

t xt

l(f (x(t);    ), y(t)) +     (   )

feedforward neural network

math for my slides    feedforward neural network   .

september 13, 2012
hugo larochelle
hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke
d  epartement d   informatique
universit  e de sherbrooke

machine learning

    9w,t    x(t   ) =pt6=t    wtx(t)
    r(x) ={x2rh|9w x =pj wjx  ,j}
    {x2rh| x /2r(x)}
    { i,ui| xui =  iui et u>i uj = 1i=j}

t xt
t xt
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
       (   )
       (   )
topics: stochastic id119 (sgd)
t pt r   l(f (x(t);    ), y(t))    r      (   )
   
              +  
t xt
      =   1
1
t pt r   l(f (x(t);    ), y(t))    r      (   )
l(f (x(t);    ), y(t)) +     (   )
arg min
      =   1
    algorithm that performs updates after each example
    {x 2 rd | rxf (x) = 0}
    f (x)
              +  
    v>r2
xf (x)v > 0 8v
    initialize           (                                                                    )
    {x 2 rd | rxf (x) = 0}
            {w(1), b(1), . . . , w(l+1), b(l+1)}
              +  
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    for n iterations
    v>r2
xf (x)v > 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
       (   )
    v>r2
for each training example
xf (x)v < 0 8v
    (x(t), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
training epoch 
    r   l(f (x(t);    ), y(t))
      =   1
    f (x)
     
=
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
iteration over all examples
     
              +      
       (   )
5
    l(f (x(t);    ), y(t))
    f (x)
    f (x)
    f (x)
    {x 2 rd | rxf (x) = 0}
    r      (   )
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    the id168
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
    f (x)c = p(y = c|x)
    r   l(f (x(t);    ), y(t))
       (   )
    a procedure to compute the parameter gradients
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    the regularizer             (and the gradient                 )
    x(t) y(t)
    r      (   )
       (   )
5
       (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    initialization method
    f (x)c = p(y = c|x)
    r      (   )
    r      (   )
    (x(t), y(t))
    r      (   )

    to apply this algorithm to neural network training, we need

hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

september 13, 2012
september 13, 2012

math for my slides    feedforward neural network   .

abstract
abstract

5

   

-

d  epartement d   informatique
hugo larochelle
universit  e de sherbrooke

2

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

september 13, 2012
hugo.larochelle@usherbrooke.ca

september 13, 2012
abstract

abstract

feedforward neural network
hugo larochelle

3

exp(ac )

exp(ac )

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)
gradient computation

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
    x(t) y(t)
    w
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layer
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    ... this is getting complicated!!
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

september 6, 2012

    p(y = c|x)

abstract

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

exp(ac )

...

...

1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

f (x)c   log f (x)y =  1(y=c)

4

=

 1
=

a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)
hugo larochelle

ra(l+1)(x)c   log f (x)y

=   1(y=c)   f (x)c 
f (x)y0@
1a
=   1(y=c)   f (x)c 
a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
exp(a(l+1)(x)y)   
a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
exp(a(l+1)(x)y)   
f (x)y0@
f (x)y0@
    h(x) = g(a(x)) = g(b +pi wixi)
pc0 exp(a(l+1)(x)c0)  
 pc0 exp(a(l+1)(x)c0) 2
    r      (   )
 1
feedforward neural network
pc0 exp(a(l+1)(x)c0)  
=
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)  
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
gradient computation
    f (x)c = p(y = c|x)
feedforward neural network
    x1 xd b w1 wd
exp(a(l+1)(x)c)
exp(a(l+1)(x)y)
 1
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
ra(l+1)(x)c   log f (x)y
exp(a(l+1)(x)y)
exp(a(l+1)(x)c)
hugo larochelle
 1
    x(t) y(t)
ra(l+1)(x)c   log f (x)y
 1
    w
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)
=
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)
=
=   (e(y)   f (x))
 1
d  epartement d   informatique
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: chain rule
    p(y = c|x)
=   (e(y)   f (x))
=
    p(y = c|x)
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
    {
universit  e de sherbrooke
 1
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
 1
    if a function        can be written as 
=
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    p(a) qi(a)
=
pc exp(ac) . . .
pc exp(ac) . . .
   
=
    g(a) = a
a function of intermediate results
    p(a) qi(a)
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    p(y = c|x)
=   1(y=c)   f (x)c 
then we have:
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
   
=
    f (x)
 1
    p(y = c|x)
    f (x)
f (x)c   log f (x)y =  1(y=c)
september 6, 2012
=xi
    g(a) = sigm(a) =
=
@p(a)
@qi(a)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=   1(y=c)   f (x)c 
1+exp( a)
    p(y = c|x)
   
=xi
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
@p(a)
@qi(a)
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
=xi
=   1(y=c)   f (x)c 
@a
@a
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
@p(a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
@a
@qi(a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
exp(2a)+1
    p(y = c|x)
@a
    f (x)
   
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
abstract
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    f (x)
ra(l+1)(x)c   log f (x)y
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
math for my slides    feedforward neural network   .
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=   (e(y)   f (x))
ra(l+1)(x)c   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
   
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    we can invoke it by setting
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
ra(l+1)(x)c   log f (x)y
    a(x) = b +pi wixi = b + w>x
=   (e(y)   f (x))
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
=   (e(y)   f (x))
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
=xi
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
@qi(a)
@p(a)
@p(a)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i,j
i
@a
@qi(a)
@a
@qi(a)
@p(a)
@p(a)
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))

        to a unit in layer 
             to a pre-activation in the layer above
             is the id168
    p(a) qi(a) a
   

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

=   (e(y)   f (x))

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

september 6, 2012

@p(a)
@qi(a)

abstract

@p(a)

exp(ac )

exp(ac )

exp(ac )

@a

exp(ac )

exp(ac )

exp(ac )

...

...

1

=xi

    p(a) qi(a) a

    p(a) qi(a) a

exp(a(l+1)(x)c)
feedforward neural network
hugo larochelle

5

@

=

=

@a

exp(ac )

@p(a)

@qi(a)

exp(a(l+1)(x)y)

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

 pc0 exp(a(l+1)(x)c0) 2
    h(x) = g(a(x)) = g(b +pi wixi)
    r      (   )
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
 1
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
gradient computation
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
=xi
@p(a)
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
    x(t) y(t)
    w
@qi(a)
@a
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
    p(y = c|x)
 1
    p(y = c|x)
    {
=
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    partial derivative:
=   1(y=c)   f (x)c 
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    f (x)
@h(k)(x)j   log f (x)y
    f (x)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
@a(k+1)(x)i
@   log f (x)y
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
ra(l+1)(x)   log f (x)y
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
@h(k)(x)j
@a(k+1)(x)i
    p(y = c|x)
=   (e(y)   f (x))
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
@   log f (x)y
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
w (k+1)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@a(k+1)(x)i
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=xi
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
@qi(a)
@p(a)
@p(a)
  ,j )>(rak+1(x)   log f (x)y)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
@qi(a)
@a
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
= w(k+1)> ra(k+1)(x)   log f (x)y 
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))

    p(a) qi(a) a k
   
rh(k)(x)   log f (x)y
reminder
    a(k)(x)i = b(k)

hugo.larochelle@usherbrooke.ca

= xi
= xi

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

i +pj w (k)

september 6, 2012

september 6, 2012

    p(y = c|x)

i,j h(k 1)(x)j

= (wk+1

    p(y = c|x)

abstract

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

...

...

i,j

1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

f (x)c   log f (x)y =  1(y=c)

feedforward neural network
hugo larochelle

6

i,j

exp(ac )

exp(ac )

w (k+1)

    p(y = c|x)

hugo larochelle

@a(k+1)(x)i
@h(k)(x)j

= xi
= xi

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
=
@   log f (x)y
gradient computation
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
@a(k+1)(x)i
=
    x(t) y(t)
    w
@   log f (x)y
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    p(y = c|x)
=
    p(y = c|x)
@a(k+1)(x)i
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    gradient:
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
rh(k)(x)   log f (x)y
    f (x)
    f (x)
= w(k+1)> ra(k+1)(x)   log f (x)y 
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    p(a) qi(a) a k
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
   
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
2
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

reminder
reminder
    a(k)(x)i = b(k)

math for my slides    feedforward neural network   .

i +pj w (k)

september 6, 2012

september 6, 2012

i,j h(k 1)(x)j

    p(y = c|x)

abstract

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

...

...

1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

f (x)c   log f (x)y =  1(y=c)

feedforward neural network
hugo larochelle

7

@

exp(ac )

exp(ac )

    p(y = c|x)

    p(y = c|x)

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)
gradient computation

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
    x(t) y(t)
    w
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    p(y = c|x)
    {
              pre-activation
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    partial derivative:
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
@a(k)(x)j   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
@h(k)(x)j
...
...
@   log f (x)y
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
@h(k)(x)j
@a(k)(x)j
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
g0(a(k)(x)j)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
@h(k)(x)j
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=  rh(k)(x)   log f (x)y > ra(k)(x)h(k)(x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
@
    h(k)(x) = g(a(k)(x))
=  rh(k)(x)   log f (x)y    [. . . , g0(a(k)(x)j), . . . ]
a(k)(x)j   log f (x)y

reminder
    h(k)(x)j = g(a(k)(x)j)

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

ra(k)(x)   log f (x)y

september 6, 2012

september 6, 2012

    p(y = c|x)

abstract

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

exp(ac )

=

=

...

...

1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

f (x)c   log f (x)y =  1(y=c)

feedforward neural network
hugo larochelle

@

8

=

=

exp(ac )

exp(ac )

g0(a(k)(x)j)

@h(k)(x)j
@a(k)(x)j

ra(k)(x)   log f (x)y

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)
gradient computation

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
a(k)(x)j   log f (x)y
    x(t) y(t)
    w
@   log f (x)y
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    p(y = c|x)
@h(k)(x)j
    p(y = c|x)
    {
              pre-activation
@   log f (x)y
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
@h(k)(x)j
pc exp(ac) . . .
   
    g(a) = a
    gradient:
    p(y = c|x)
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=  rh(k)(x)   log f (x)y > ra(k)(x)h(k)(x)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=  rh(k)(x)   log f (x)y    [. . . , g0(a(k)(x)j), . . . ]
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
element-wise
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
product
@
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
  log f (x)y
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
w (k)
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
i,j
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@   log f (x)y
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
=
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
@a(k)(x)i
reminder
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x)j = g(a(k)(x)j)
@   log f (x)y
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
b(1)
    w (1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@a(k)(x)i
i
i,j
    h(k)(x) = g(a(k)(x))
@
    h(k)(x) = g(a(k)(x))
a(k)(x)j   log f (x)y

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

@a(k)(x)i
@w (k)
i,j
h(k 1)
j

september 6, 2012

    p(y = c|x)

abstract

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

exp(ac )

(x)

...

...

=

1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

f (x)c   log f (x)y =  1(y=c)

