natural language processing:
word representations
ift 725 - r  seaux neuronaux

natural language processing
topics: natural language processing (nlp)
    natural language processing is concerned with tasks involving 
language data 
    we will focus on text data nlp
    speech processing is also nlp, though it has its own dedicated research 

community

    much like for id161, we can design neural networks 
speci   cally adapted to the processing of text data
    main issue: text data is inherently high dimensional

2

natural language processing
topics: id121
    typical preprocessing steps of text data

    tokenize text (from a long string to a list of token strings)

       he   s spending 7 days in san 

francisco.       

       he       
          s       
       spending       
       7       
       days       
       in       
       san francisco       
       .       

    for many datasets, this has already been done for you
    splitting into tokens based on spaces and separating punctuation is good enough 

in english or french

3

natural language processing
topics: lemmatization
    typical preprocessing steps of text data

    lemmatize tokens (put into standard form)

       he       
          s       
       spending       
       7       
       days       
       in       
       san francisco       
       .       

       he       
       be       
       spend       
       number       
       day       
       in       
       san francisco       
       .       

    the speci   c lemmatization will depend on the problem we want to solve

- we can remove variations of words that are not relevant to the task at hand

4

natural language processing
topics: vocabulary
    typical preprocessing steps of text data

    form vocabulary of words that maps lemmatized words to a unique id 

(position of word in vocabulary)

    different criteria can be used to select which words are part of the vocabulary

- pick most frequent words
-

ignore uninformative words from a user-de   ned short list (ex.:        the       ,        a       , etc.)

    all words not in the vocabulary will be mapped to a special       out-of-vocabulary       id

    typical vocabulary sizes will vary between 10 000 
and 250 000

5

natural language processing
topics: vocabulary
    example:

vocabulary
word
w
1
       the       
2
       and       
3
       dog       
4
       .       
5
       oov       

1
5
2
1
3
5
4

       the       
       cat       
       and       
       the       
       dog       
       play       
       .       

    we will note word ids with the symbol w

    can think of w as a categorical feature for the original word
    we will sometimes refer to w as a word, for simplicity

6

natural language processing
topics: one-hot encoding
    from its word id, we get a basic representation of a word 
through the one-hot encoding of the id
    the one-hot vector of an id is a vector    lled with 0s, except for a 1 at the position 

associated with the id
- ex.:  for vocabulary size d=10, the one-hot vector of word id w=4 is

                                      e(w) = [ 0 0 0 1 0 0 0 0 0 0 ]

    a one-hot encoding makes no assumption about word similarity

-

-

||e(w) - e(w   )||2 = 0 if w = w   
||e(w) - e(w   )||2 = 2 if w     w   

- all words are equally different from each other

    this is a natural representation to start with, though a poor one

7

natural language processing
topics: one-hot encoding
    the major problem with the one-hot representation is that it 
is very high-dimensional
    the dimensionality of e(w) is the size of the vocabulary
    a typical vocabulary size is    100 000
    a window of 10 words would correspond to an input vector of at least 1 000 000 

units!

    this has 2 consequences:

    vulnerability to over   tting

- millions of inputs means millions of parameters to train in a regular neural network

    computationally expensive

- not all computations can be sparsi   ed (ex.: reconstruction in autoencoder)

8

word representations

topics: continuous word representation
    idea: learn a continuous representation of words

    each word w is associated with a real-valued vector c(w)

word
       the       
       a       

       have       
       be       
       cat       
       dog       
       car       

...

w
1
2
3
4
5
6
7
...

c(w)

[ 0.6762, -0.9607, 0.3626, -0.2410, 0.6636 ]
[ 0.6859, -0.9266, 0.3777, -0.2140, 0.6711 ]
[ 0.1656, -0.1530, 0.0310, -0.3321, -0.1342 ]
[ 0.1760, -0.1340, 0.0702, -0.2981, -0.1111 ]
[ 0.5896, 0.9137, 0.0452, 0.7603, -0.6541 ]
[ 0.5965, 0.9143, 0.0899, 0.7702, -0.6392 ]
[ -0.0069, 0.7995, 0.6433, 0.2898, 0.6359 ]

...

9

word representations

ding ~xi is obtained by metric multidimensional scaling [1, 9, 14]. the top eigenvalues of
the gram matrix measure the variance captured by the leading dimensions of this embed-
ding. thus, one can compare the eigenvalue spectra from this method and pca to ascertain
if the variance of the nonlinear embedding is concentrated in fewer dimensions. we refer
to this method of nonlinear id84 as semide   nite embedding (sde).
fig. 1 compares the eigenvalue spectra of pca and sde applied to the 2000 most frequent
words2 in the corpus described in section 4. the    gure shows that the nonlinear embedding
by sde concentrates its variance in many fewer dimensions than the linear embedding by
pca. indeed, fig. 2 shows that even the    rst two dimensions of the nonlinear embedding
preserve the neighboring relationships of many words that are semantically similar. by
contrast, the analogous plot generated by pca (not shown) reveals no such structure.

topics: continuous word representation
    idea: learn a continuous representation of words

    we would like the distance ||c(w)-c(w   )|| to re   ect meaningful similarities 

between words

   may, would, could, should,  

might, must, can, cannot, 

couldn't, won't, will

one, two, three, 

four, five, six, 

seven, eight, nine, 

ten, eleven, 

twelve, thirteen, 
fourteen, fifteen, 

sixteen, 

seventeen,

eighteen

zero

  million
billion

   monday
    tuesday

    wednesday

    thursday

    friday

    saturday

   sunday

    january
    february

    march
    april
    june
    july

    august

    september

    october
    november
    december

figure 2: projection of the normalized bigram counts of the 2000 most frequent words
onto the    rst two dimensions of the nonlinear embedding obtained by semide   nite pro-

(from blitzer et al. 2004)

10

word representations

natural language processing

topics: continuous word representation
    idea: learn a continuous representation of words

hugo larochelle

    we can then use these representations as input to a neural network
    to represent a window of 10 words [w1, ... , w10], we concatenate the 

d  epartement d   informatique
universit  e de sherbrooke

representations of each word

hugo.larochelle@usherbrooke.ca

                              x = [c(w1)   , ... , c(w10)   ]    

november 12, 2012

    we learn these representations by id119

math for my slides    natural language processing   .

    we don   t only update the neural network parameters
    we also update each representation c(w) in the input x with a gradient step

abstract

where l  is the id168 optimized by the neural network

c(w) (= c(w)      rc(w)l

11

word representations

topics: word representations as a lookup table
    let c be a matrix whose rows are the representations c(w)
    obtaining c(w) corresponds to the multiplication e(w)    c
    in words, we are projecting e(w) onto the columns of c

-

this is a reduction of the dimensionality of the one-hot representations e(w)

    this is a continuous transformation, through which we can propagate gradients

    in practice, we implement c(w) with a lookup table, not with 
a multiplication
    c(w) returns an array pointing to the wth row of c

12

conclusion

    we looked at the preprocessing steps of text data

    id121, lemmatization and vocabulary extraction

    we discussed the challenge that text data presents

    data is very high-dimensional
    requires large quantity of data to learn in such a setting

    we looked at the idea of learning word representations

    these representations are trained within a neural network, with gradient 
descent

13

