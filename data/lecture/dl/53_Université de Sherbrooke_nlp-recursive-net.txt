natural language processing:
id56
ift 725 - r  seaux neuronaux

from words to phrases
topics: word phrase representations
    we   ve seen how to learn representations for single words

    how could we learn representations for phrases of arbitrary 
length?
    can we model relationships between words and multiword expressions

- ex.:        consider                   take into account       

    can we extract a representation of full sentences that preserves some of its 

semantic meaning
- ex.: 

        word representations
 were learned from a
 corpus        

   

        we trained word 
 representations on 
 a text data set       

2

id56
topics: id56 (id56)
    idea: recursively merge pairs of word/phrase representations

word representations{

    we need 2 things

figure 1. illustration of our id56 ar-
socher, lin, ng and manning, 2011
chitecture which parses images and natural language sen-
    a model that merges pairs of representations
tences. segment features and word indices (orange) are
    a model that determines the tree structure
   rst mapped into semantic feature space (blue) and then
recursively merged by the same neural network until they

3

id56
topics: id56 (id56)
    given two input representations c1 and c2, the recursive 
network computes the merged representation p as follows:

parsing natural scenes and natural language with id56s

s = w scorep
(9)
p = f (w [c1; c2] + b)

socher, lin, 
ng and manning, 2011

of the tree. the    nal score that we need for structure
prediction is simply the sum of all the local decisions:

s(id56(  , xi,   y)) =!d   n (  y)

to    nish the example, assume the next highest score
was s((4,5),3), so we merge the (4, 5) super segment
with segment 3, so c = {[a1, a2], [a1, p((4,5),3)], [a2, a1],
[a2, p((4,5),3)], [p((4,5),3), a1], [p((4,5),3), a2]}. if we then
merge segments (1, 2), we get c = {[p(1,2), p((4,5),3)],
[p((4,5),3), p(1,2)]}, leaving us with only the last choice
of merging the di   erently labeled super segments. this
results in the bottom tree in fig. 2.

4

    the network also computes a score s

figure 3. one id56 which is replicated
for each pair of possible input vectors. this network is
di   erent to the original id56 formulation in that it predicts
    it estimates the quality of the merge
a score for being a correct merging decision.
    it will be used to decide which pairs of representations to merge    rst
neural network. the network computes the potential
parent representation for these possible child nodes:

id56
topics: id56 (id56)
    the score of the full tree is the sum of all merging scores
recursive network

parse tree

s((1,2),(3,4))
wscore

w

s(3,4)

wscore

w

s(1,2)

wscore

w

       the       

       cat       

       is       

       here       

       cat       
score: s(1,2) + s(3,4) + s((1,2),(3,4))

       the       

       is       

       here       

5

id56
topics: id56 (id56)
    the score of the full tree is the sum of all merging scores
recursive network

parse tree

s(((1,2)3),4)
wscore

w

s((1,2),3)
wscore

w

s(1,2)

wscore

w

       the       

       cat       

       is       

       here       

       the       

       cat       

       is       

       here       

score: s(1,2) + s((1,2),3) + s(((1,2),3),4)

6

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

a[1,3] 

a[2,4] 

a[1,2] 

a[2,3] 

a[3,4] 

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

a[1,3] 

a[2,4] 

a[1,2] 

s(1,2)

a[2,3] 

a[3,4] 

s(2,3)

s(3,4)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

a[1,3] 

a[2,4] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

a[1,2] 

s(1,2)

a[2,3] 

a[3,4] 

s(2,3)

s(3,4)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

a[1,3] 

a[2,4] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

<

a[1,2] 

s(1,2)

a[2,3] 

a[3,4] 

s(2,3)

s(3,4)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

a[1,3] 

a[2,4] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

<

s((2,3),4) + a[2,3] 
s(2,(3,4)) + a[3,4] 

a[2,3] 

s(2,3)

a[3,4] 

s(3,4)

a[1,2] 

s(1,2)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

a[1,3] 

a[2,4] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

<

s((2,3),4) + a[2,3] 
s(2,(3,4)) + a[3,4] 

>

a[2,3] 

s(2,3)

a[3,4] 

s(3,4)

a[1,2] 

s(1,2)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

s((1,(2,3)),4) + a[1,3]

s((1,2),(3,4))+a[1,2]+ a[3,4] 

s(1,((2,3),4)) + a[2,4]

a[2,4] 

a[1,3] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

<

s((2,3),4) + a[2,3] 
s(2,(3,4)) + a[3,4] 

>

a[2,3] 

s(2,3)

a[3,4] 

s(3,4)

a[1,2] 

s(1,2)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

s((1,(2,3)),4) + a[1,3]

s((1,2),(3,4))+a[1,2]+ a[3,4] 

>

s(1,((2,3),4)) + a[2,4]

<

a[2,4] 

a[1,3] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

<

s((2,3),4) + a[2,3] 
s(2,(3,4)) + a[3,4] 

>

a[2,3] 

s(2,3)

a[3,4] 

s(3,4)

a[1,2] 

s(1,2)

       the       

       cat       

       is       

       here       

7

id56
topics: id56 (id56)
    approximate best tree by locally maximizing each subtree

a[1,4] 

s((1,(2,3)),4) + a[1,3]

s((1,2),(3,4))+a[1,2]+ a[3,4] 

>

s(1,((2,3),4)) + a[2,4]

<

a[2,4] 

a[1,3] 

s((1,2),3) + a[1,2] 
s(1,(2,3)) + a[2,3] 

<

s((2,3),4) + a[2,3] 
s(2,(3,4)) + a[3,4] 

>

a[2,3] 

s(2,3)

a[1,2] 

s(1,2)

       the       

       cat       

       is       

       here       

a[3,4] 

s(3,4)

       the       

       cat       

       is       

       here       

7

   

id56
topics: training algorithm
    let y be the true parse tree and    be the predicted parse tree

rc(w)l =

    to update the recursive network

    we would like the score s(y) of y to be higher than the score s(  ) of   

    w1 w2 wn 1
    rc(3)l = w>3 ra(x)l
    rc(14)l = w>2 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(w)l = 0
    r   s(y)   r   s(  y)

    infer the predicted parse tree   
    increase the score s(y) and decrease the score s(  ) by doing an update in the 

direction of the gradient

-

these gradient can be computed by backpropagating through the recursive network 
structured according to the parse trees y and   

8

id56
topics: training algorithm
    the nodes of a parse tree are also labeled

- noun phrase (np), verb phrase (vp), etc.
- can add softmax layer that predict the label from each node representation
-

this is an additional gradient to backpropagate, for the true parse tree y
s((1,2),(3,4))
wscore

s

np

vp

det

n

v

adv

s(1,2)

wscore

w

w

s(3,4)

wscore

w

       the       

       cat       

       is       

       here       

       the       

       cat       

       is       

       here       

9

id56
topics: training algorithm
    the nodes of a parse tree are also labeled

- noun phrase (np), verb phrase (vp), etc.
- can add softmax layer that predict the label from each node representation
-

this is an additional gradient to backpropagate, for the true parse tree y
s((1,2),(3,4))
wscore

s

np

vp

s

s(1,2)

wscore

np

w

s(3,4)

wscore

vp

det

n

v

adv

det

w

n

v

w

       the       

       cat       

       is       

       here       

       the       

       cat       

       is       

       here       

adv

9

id56
topics: training algorithm
    other details

    word representations are pre-trained using collobert and weston   s approach and 

   ne-tuned while training the recursive autoencoder

    training is actually based on a margin criteria: s(y) > s(y*) +    (y,y*)

-

score of the true parse tree y trained to be larger than score of any other tree y* plus its 
number of incorrect spans    (y,y*) 

span    (y,y*)  = 1
number of incorrect 

       here       

       is       

       here       

       the       

       the       

       cat       

y

       cat       

       is       

y*

- a simple modi   cation to the id125    nding the best tree (see socher et al. for details)

10

id56
topics: training algorithm
    other details

    word representations are pre-trained using collobert and weston   s approach and 

   ne-tuned while training the recursive autoencoder

    training is actually based on a margin criteria: s(y) > s(y*) +    (y,y*)

-

score of the true parse tree y trained to be larger than score of any other tree y* plus its 
number of incorrect spans    (y,y*) 

span    (y,y*)  = 1
number of incorrect 

       here       

       is       

       here       

       the       

       the       

       cat       

y

       cat       

       is       

y*

- a simple modi   cation to the id125    nding the best tree (see socher et al. for details)

10

id56
topics: experimental comparison
    parsing f1 performance

all the    gures are adjusted for seasonal variations
1. all the numbers are adjusted for seasonal    uctuations
2. all the    gures are adjusted to remove usual seasonal
patterns
3. all nasdaq industry indexes    nished lower , with    -
nancial issues hit the hardest
knight-ridder would n   t comment on the o   er
1. harsco declined to say what country placed the order
2. coastal would n   t disclose the terms
3. censorship is n   t a marxist invention
sales grew almost 7% to $unk m. from $unk m.
1. sales rose more than 7% to $94.9 m. from $88.3 m.
2. sales surged 40% to unk b. yen from unk b.
3. revenues declined 1% to $4.17 b. from$ 4.19 b.

    nearest neighbor phrases based on id56 representation

    id56: 90.29% 
    berkeley parser: 91.63%

figure 5. nearest neighbor image region trees (of the    rst

fujisawa gained 50 to unk
1. mead gained 1 to 37 unk
2. ogden gained 1 unk to 32
3. kellogg surged 4 unk to 7

the dollar dropped
1. the dollar retreated
2. the dollar gained
3. bond prices rallied

figure 6. nearest neighbors phrase trees. the learned fea-
ture representations of higher level nodes capture interest-

socher, lin, ng and manning, 2011

11

conclusion

    we introduced id56s

    they allow the extraction of representations for arbitrary length phrases
    they can also extract a labeled parse tree for sentences

    id56s can be applied to visual scene segmentation

    words are replaced by (super-)pixel features
    the span of an internal node in the parse tree de   nes a region
    the label of a node de   nes its semantic label

12

