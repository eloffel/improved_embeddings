neural networks
training crfs - id168

2

hugo larochelle

hugo larochelle

training crfs

training crfs

training crfs
linear chain crf
training crfs
hugo larochelle
d  epartement d   informatique
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
universit  e de sherbrooke
september 26, 2012

topics: reminder of notation 
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    then we have:
d  epartement d   informatique
    p(y|x) = exp   pk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
k=1 au(yk) +pk 1
universit  e de sherbrooke
    p(y|x) = exp   pk
k=1 ap(yk, yk+1)    /z(x)
k=1 au(yk) +pk 1
hugo.larochelle@usherbrooke.ca
where
exp   pk
    pz(x) =py01py02       py0k
exp   pk
    z(x) =py01py02       py0k
k=1 au(y0k) +pk 1

k=1 ap(yk, yk+1)    /z(x)
k=1 au(y0k) +pk 1
k=1 ap(y0k, y0k+1)   

september 26, 2012
    two types of (log-)factors:

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

september 26, 2012

september 26, 2012

hugo larochelle

k=1 ap(y0k, y0k+1)   

hugo.larochelle@usherbrooke.ca

abstract

abstract

abstract

math for my slides    training crfs   .

    unary:

math for my slides    training crfs   .
abstract
math for my slides    training crfs   .

math for my slides    training crfs   .

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    ap(yk, yk+1) = 11   k<k vyk,yk+1

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1

    pairwise:

abstract

t pt(x(t)  b  )(x(t)  b  )>

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)

t b    = 1
    training set: dtrain = {(x(t), y(t))}
machine learning
    f (x;    )
    supervised learning example: (x, y) x y
math for my slides    training crfs   .
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}
topics: empirical risk minimization, id173
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
   
    f (x;    )
    empirical risk minimization
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    dvalid dtest
    framework to design learning algorithms
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
   
   

machine learning
t xt

    l(f (x(t);    ), y(t))
1
l(f (x(t);    ), y(t)) +     (   )
t xt
l(f (x(t);    ), y(t)) +     (   )
arg min
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
    l(f (x(t);    ), y(t))
                                  is a id168
    l(f (x(t);    ), y(t))
               is a regularizer (penalizes certain values of     )
              +  
       (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    learning is cast as optimization
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))
    ideally, we   d optimize classi   cation error, but it   s not smooth
    l(f (x(t);    ), y(t))
    id168 is a surrogate for what we truly should optimize
    r   l(f (x(t);    ), y(t))

t xt

l(f (x(t);    ), y(t)) +     (   )

t xt

arg min

1
arg min

arg min

1

   

1

   

   

   

3

l(f (x(t);    ), y(t)) +     (   )

   

4

arg min

abstract

math for my slides    training crfs   .

t xt

hugo larochelle

abstract
math for my slides    training crfs   .

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

machine learning

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
math for my slides    training crfs   .
d  epartement d   informatique
    l(f (x(t);    ), y(t))
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
universit  e de sherbrooke
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
hugo.larochelle@usherbrooke.ca
       (   )
topics: stochastic id119 (sgd)
    ap(yk, yk+1) = 11   k<k vyk,yk+1
   
t xt
1
l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
arg min
      =   1
    algorithm that performs updates after each example
september 13, 2012
   
    initialize     
t xt
              +  
t xt
1
    for n iterations

    9w,t    x(t   ) =pt6=t    wtx(t)
    r(x) ={x2rh|9w x =pj wjx  ,j}
    {x2rh| x /2r(x)}
    { i,ui| xui =  iui et u>i uj = 1i=j}

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    ap(yk, yk+1) = 11   k<k vyk,yk+1
   
1
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
   
   
t xt
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
arg min
arg min
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
       (   )
    l(f (x(t);    ), y(t))
for each training example
    (x(t), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
training epoch 
    l(f (x(t);    ), y(t))
      =   1
    f (x)
     
=
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
iteration over all examples
      =  r   l(f (x(t);    ), y(t))    r      (   )
     
              +      
    (x(t), y(t))
    f (x)
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    to apply this algorithm to a crf, we need
    (x(t), y(t))
    (x(t), y(t))
    {x 2 rd | rxf (x) = 0}
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    the id168
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
    r   l(f (x(t);    ), y(t))
       (   )
    a procedure to compute the parameter gradients
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    the regularizer             (and the gradient                 )
    r      (   )
       (   )
5
      =  r   l(f (x(t);    ), y(t))    r      (   )
    initialization method
    f (x)c = p(y = c|x)
    r      (   )
    (x(t), y(t))

t pt r   l(f (x(t);    ), y(t))    r      (   )

math for my slides    feedforward neural network   .

september 13, 2012

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

t xt

abstract

abstract

arg min

1

1

   

   

   

   

   

-

t xt

   

1

arg min

t xt

      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    ap(yk, yk+1) = 11   k<k vyk,yk+1
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))
id168
      =  r   l(f (x(t);    ), y(t))    r      (   )
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    l(f (x(t);    ), y(t))
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    (x(t), y(t))
    l(f (x(t);    ), y(t))
    (x(t), y(t))
   
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
topics: id168 for sequential classi   cation with crf
t xt
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
l(f (x(t);    ), y(t)) +     (   )
arg min
    crf estimates
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    p(y|x) l(f (x), y) =   log p(y|x)
    (x(t), y(t))
    we could maximize the probabilities of         given          in the training set
    p(y|x) y(t) x(t)
    p(y|x) y(t) x(t)
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    to frame as minimization, we minimize the 
negative log-likelihood
    r   l(f (x(t);    ), y(t))
    (x(t), y(t))
    l(f (x(t);    ), y(t))
    p(y|x) y(t) x(t)
    r   l(f (x(t);    ), y(t))
    unlike for non-sequential classi   cation, we never explicitly compute the value of
@   log p(y|x)
               for all values of 
    p(y|x) l(f (x), y) =   log p(y|x)
l(f (x), y) =   log p(y|x) y
@au(yk)
@   log p(y|x)

l(f (x), y) =   log p(y|x)
=
@au(yk)
@   log p(y|x)

l(f (x), y) =   log p(y|x)

l(f (x), y) =   log p(y|x)

l(f (x(t);    ), y(t)) +     (   )

@   log p(y|x)

=

=

@   log p(y|x)

=

1

   

@au(yk)

@au(yk)

@   log p(y|x)
=
@au(yk)

@au(yk)

5

=

