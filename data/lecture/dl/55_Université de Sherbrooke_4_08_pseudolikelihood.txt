neural networks
training crfs - pseudolikelihood

p

exp(au(y1) + ap(y1, y2)) ( =    1(y2) )

j
=

i

}

exp(au(y2) + ap(y2, y3))   1(y2) ( =    2(y3) )

1
=

j

  f0!s(i)

2

}

j
,

  

j

j

    1 exp(au(y1)) exp(au(y1)) py1

9

|

r

       k(yk+1)  k(yk 1)

2

{

  s!f (i) = yf02ne(s)\f
general crf

  f0!s(i)

1 exp(au(y2))

{

   

  s!f (i) = yf02ne(s)\f
    y1 y2 y3 x1 x2
    1 exp(au(y1)) exp(au(y1)) py1
topics: crfs in general
exp(au(y2))   1(y2) py2
   
    gradients in general crfs always take the form:
exp(au(y1) + ap(y1, y2)) ( =    1(y2) )
       k(yk+1)  k(yk 1)
exp(au(y2))   1(y2) py2
=     pf
    @ log p(y(t)|x(t))
=     pf

@    log  f (y(t), x(t))   eyhpf

exp(au(y2) + ap(y2, y3))   1(y2) ( =    2(y3) )
}
x
r
/2
x

@    log  f (y(t), x(t))   eyhpf
@    log  f (y, x(t))  x(t)i   

/2
r

2

2
x
{

x

}

r

make everything less likely

x

x

make y(t) more likely

|

(

)

r

|

h

(

)

h

@   

@

@

@

@

    the expectation over y will often need to be approximated, 
using loopy belief propagation
    it will often involve only a few of the yk variables

   

e
t

>i

r

u

   

>i

r

u

t
e

@    log  f (y, x(t))  x(t)i   

u

i

i

 
=

u
x

i

{

2

|

|

9

u

i

,

 

i

{

  

}

   

p

   

6
3

@

@    log  f (y, x(t))  x(t)i   

exp(au(y2))   1(y2) py2
       k(yk+1)  k(yk 1)
general crf
@    log  f (y(t), x(t))   eyhpf
=     pf
    @ log p(y(t)|x(t))
topics: pseudolikelihood
exp(log  f (yk)+pf02ne(k)\f log   f0!k(yk))
    p(yk|x) =
exp(log  f (y0k)+pf02ne(k)\f log   f0!k(y0k))
py0k
    why not just change the id168 to a tractable one
k=1 log p(yk|y1, . . . , yk 1, yk+1, . . . , yk, x)

@   

@

     pk

    predict, in turn, each yk not just from x, but also all the other elements of y
    can compute the exact gradients

-

the probabilities only require normalizing yk individually, like in a regular softmax

- each conditional often only depend on few variables (local markov property)

    however, often tends to work less well
    we still need to compute p(yk|x) to do predictions anyways

