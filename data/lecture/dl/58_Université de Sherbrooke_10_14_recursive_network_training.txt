neural networks
natural language processing - recursive network training

id56

2

topics: id56 (id56)
    idea: recursively merge pairs of word/phrase representations

word representations{

    we need 2 things

figure 1. illustration of our id56 ar-
socher, lin, ng and manning, 2011
chitecture which parses images and natural language sen-
    a model that merges pairs of representations
tences. segment features and word indices (orange) are
    a model that determines the tree structure
   rst mapped into semantic feature space (blue) and then
recursively merged by the same neural network until they

3

1(wt i=w) w>i ra(x)l

   

id56
n 1xi=1

rc(w)l =

topics: training algorithm
    let y be the true parse tree and    be the predicted parse tree

    we would like the score s(y) of y to be higher than the score s(  ) of    

(unless    is actually y)

    to update the recursive network

    w1 w2 wn 1
    rc(3)l = w>3 ra(x)l
    rc(14)l = w>2 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(w)l = 0
    r   s(y)   r   s(  y)

    infer the predicted parse tree   
    increase the score s(y) and decrease the score s(  ) by doing an update in the 

direction of the gradient

-

these gradient can be computed by backpropagating through the recursive network 
structured according to the parse trees y and   

id56

4

topics: training algorithm
    the nodes of a parse tree are also labeled

- noun phrase (np), verb phrase (vp), etc.
- can add softmax layer that predict the label from each node representation
-

this is an additional gradient to backpropagate, for the true parse tree y
s((1,2),(3,4))
wscore

s

np

vp

det

n

v

adv

s(1,2)

wscore

w

w

s(3,4)

wscore

w

       the       

       cat       

       is       

       here       

       the       

       cat       

       is       

       here       

id56

4

topics: training algorithm
    the nodes of a parse tree are also labeled

- noun phrase (np), verb phrase (vp), etc.
- can add softmax layer that predict the label from each node representation
-

this is an additional gradient to backpropagate, for the true parse tree y
s((1,2),(3,4))
wscore

s

np

vp

s

s(1,2)

wscore

np

w

s(3,4)

wscore

vp

det

n

v

adv

det

w

n

v

w

adv

       the       

       cat       

       is       

       here       

       the       

       cat       

       is       

       here       

id56

5

topics: training algorithm
    other details

    word representations are pre-trained using collobert and weston   s approach and 

   ne-tuned while training the recursive network

    training is actually based on a margin criteria: s(y) > s(y*) +    (y,y*)

-

score of the true parse tree y trained to be larger than score of any other tree y* plus its 
number of incorrect spans    (y,y*) 

span    (y,y*)  = 1
number of incorrect 

       here       

       is       

       here       

       the       

       the       

       cat       

y

       cat       

       is       

y*

- a simple modi   cation to the id125    nding the best tree (see socher et al. for details)

6

id56

topics: experimental comparison
    parsing f1 performance

all the    gures are adjusted for seasonal variations
1. all the numbers are adjusted for seasonal    uctuations
2. all the    gures are adjusted to remove usual seasonal
patterns
3. all nasdaq industry indexes    nished lower , with    -
nancial issues hit the hardest
knight-ridder would n   t comment on the o   er
1. harsco declined to say what country placed the order
2. coastal would n   t disclose the terms
3. censorship is n   t a marxist invention
sales grew almost 7% to $unk m. from $unk m.
1. sales rose more than 7% to $94.9 m. from $88.3 m.
2. sales surged 40% to unk b. yen from unk b.
3. revenues declined 1% to $4.17 b. from$ 4.19 b.

    id56: 90.29% 
    berkeley parser: 91.63%

    nearest neighbor phrases based on id56 representation

figure 5. nearest neighbor image region trees (of the    rst

fujisawa gained 50 to unk
1. mead gained 1 to 37 unk
2. ogden gained 1 unk to 32
3. kellogg surged 4 unk to 7

the dollar dropped
1. the dollar retreated
2. the dollar gained
3. bond prices rallied

figure 6. nearest neighbors phrase trees. the learned fea-
ture representations of higher level nodes capture interest-

socher, lin, ng and manning, 2011

