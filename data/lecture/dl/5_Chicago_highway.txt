ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

controlling gradients

vanishing and exploding gradients

initialization

batch id172

residual networks

gated id56s

1

vanishing and exploding gradients

causes of vanishing and exploding gradients:

activation function saturation

repeated multiplication by network weights

2

activation function saturation

consider the sigmoid activation function 1/(1 + e   x).

the gradient of this function is quite small for |x| > 4.
in deep networks id26 can go through many sig-
moids and the gradient can    vanish   

3

activation function saturation

relu(x) = max(x, 0)

the relu does not saturate at positive inputs (good) but is
completely saturated at negative inputs (bad).
alternate variations of relu still have small gradients at neg-
ative inputs.

4

repeated multiplication by network weights

consider a deep id98.

li+1 = relu(conv(  i, li))

for i large, li has been multiplied by many weights.

if the weights are small then the neuron values, and hence the
weight gradients, decrease exponentially with depth.

if the weights are large, and the id180 do not sat-
urate, then the neuron values, and hence the weight gradients,
increase exponentially with depth.

5

methods for maintaining gradients

initialization

batch id172

highway architectures (skip connections)

6

methods for maintaining gradients

kaiming he

7

initialization

8

xavier initialization

initialize a weight matrix (or tensor) to preserve zero-mean
unit variance distributions.
if we assume xi has unit mean and zero variance then we want

n   1(cid:88)

j=0

yj =

xiwi,j

to have zero mean and unit variance.
xavier initialization randomly sets wi,j to be uniform in the

(cid:18)

   (cid:113) 3

n ,

(cid:113) 3

n

(cid:19)

.

interval

assuming independence this gives zero mean and unit variance
for yj.

9

edf implementation

def xavier(shape):

sq = np.sqrt(3.0/np.prod(shape[:-1]))
return np.random.uniform(-sq,sq,shape)

this assumes that we sum over all but the last index.

for example, an image convolution    lter has shape (w, w, c1, c2)
and we sum over the    rst three indices.

10

he initialization

a relu nonlinearity reduces the variance.

(cid:18)

   (cid:113) 6

n ,

(cid:19)

(cid:113) 6

n

interval

before a relu nonlinearity it seems better to use the larger

.

11

batch id172

12

given a tensor x[b, c] we de   ne   x[b, c] as follows.

id172

x[b, c]         [cx]

    [cx]

  x[b, c] =

    [cx] =

    [cx] =

1
b

(cid:88)
(cid:115) 1

b

b     1

x[b, c]

(cid:88)

b

(x[b, c]         [cx])2

at test time a single    xed estimate of   [cx] and   [cx] is used.

13

spatial batch id172

given a spatial tensor x[b, i, j, cx] we de   ne   x[b, i, j, cx] as fol-
lows.

x[b, i, j, cx]         [cx]

  x[b, i, j, cx] =

    [cx] =

    [cx] =

    [cx]

1

bij

(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1

b,i,j

bij     1

x[b, i, j, cx]

(cid:88)

b,i,j

(x[b, i, j, cx]         [cx])2

id26 through id172

  x[b, i, j, cx] = (x[b, i, j, cx]         [cx])/    [cx]

x.grad[b, i, j, cx] +=   x.grad[b, i, j, cx]/    [cx]

    .grad[cx] -=   x.grad[b, i, j, cx]/    [cx]
    .grad[cx] -=   x.grad[b, i, j, cx](x[b, i, j, cx]         [cx])/    [cx]2

15

id26 through id172

    [cx] = x[b, i, j, cx]/(bij)

x.grad[b, i, j, cx] +=     .grad[cx]/(bij)

16

id26 through id172

    [cx] = (cid:112)  s[cx]

  s.grad[cx] +=     .grad[cx]/(2    [cx])

  s[cx] =

1

bij     1

(x[b, i, j, cx]         [cx])2

x.grad[b, i, j, cx] +=   s.grad[cx]

    .grad[cx] -=   s.grad[cx]

1

bij     1

1

bij     1

2(x[b, i, j, cx]         [cx])

2(x[b, i, j, cx]         [cx])

17

adding an a   ne transformation

  x[b, i, j, cx] =   [cx]  x[b, i, j, cx] +   [cx]

here   [cx] and   [cx] are parameters of the batch id172
operation.

this allows the batch normlization to learn an arbitrary a   ne
transformation (o   set and scaling).

it can even undo the normaliztion.

18

batch id172

batch id172 is is empirically successful in id98s.

not so successful in id56s.

it is typically used just prior to a nonlinear activation function.

it is intuitively justi   ed in terms of    internal covariate shift   :
as the inputs to a layer change the zero mean unit variance
property underlying xavier initialization are maintained.

highway architectures (skip connections)

20

deep residual networks (resnets) by kaiming he 2015

here we have a    highway   
with    diversions   .

the highway path connects
input to outputs and pre-
serves gradients.

resnets were introduced in
late 2015 (kaiming he et al.)
and revolutionized computer
vision.

the resnet that won the im-
agenet competition in 2015
had 152 diversions.

residual skip connections

or

li+1 = li + di

l += di

here di       ts the residual of the identity function   

22

resnet32

[kaiming he]

deep residual networks

as with most of deep learn-
ing, not much is known about
what resnets are actually do-
ing.

for example, di   erent di-
versions might update dis-
joint channels making the
networks shallower than they
look.

they are capable of repre-
senting very general circuit
topologies.

a bottleneck diversion

this reduction in the num-
ber of channels used in the di-
version suggests a modest up-
date of the highway informa-
tion.

[kaiming he]

25

expressive power

this architecture can express
fairly arbitrary state updates.

each layer has data-   ow pa-
rameters.

[kaiming he]

gating gives data-dependent data flow

residual: y(cid:96)+1 = y(cid:96) + d(cid:96)

lstm:

gru:

ht+1 = f t (cid:12) ht + it (cid:12) dt
ht+1 = f t (cid:12) ht + (1     f t) (cid:12) dt

resnet has data-   ow parameters at each layer.

gated id56s use the same parameters at each time step but
use gating for data-   ow control.

27

recurrent neural networks (id56s)

id103

machine translation

reading comprehesion

id38

28

vanilla id56s

[christopher olah]

h[b, t+1, ch] = tanh(w [c(cid:48)

h, ch]h[b, t, c(cid:48)

h]+w [cx, ch]x[b, t, cx]+  [ch])

or

or

ht+1 = tanh(w h,hht + w x,hxt +   )

ht+1 = tanh(w h[ht, xt] +   )

where [x, y] denotes concatenation.

exploding and vanishing gradients

an id56 uses the same weights at every time step.

if we avoid saturation of the id180 then we get
exponentially growing or shrinking eigenvectors of the weight
matrix.

note that if the forward values are bounded by sigmoids or
tanh then they cannot explode.

however the gradients can still explode.

30

exploding gradients: gradient clipping

we can dampen the e   ect of exploding gradients by clipping
them before applying sgd.

                     

w.grad =

w.grad if ||w.grad||     nmax

nmax w.grad/||w.grad|| otherwise

see torch.nn.utils.clip grad norm

31

vanishing gradients: highway paths (skip connections)

modern id56s have highway paths (skip connections).

unlike deep id98 layers such as resnet, id56s use the same
parameters at each layer.

probably for this reason, pure residual connections are not
used.

instead gating is used for data-dependent weighting.

skip connections

residual:

li+1 = li + di

forget gates (lstm):

li+1 = fi (cid:12) li + ii (cid:12) di

convex gates (gru): li+1 = fi (cid:12) li + (1     fi) (cid:12) di

(cid:12) is hadamard product. this is the same as numpy element-
wise product. however, the symbol (cid:12) is commonly used in
the literature.

33

long short term memory (lstm)

[   gure: christopher olah]

[lstm: hochreiter&shmidhuber, 1997]

ct+1 = f t (cid:12) ct + it (cid:12) dt

note that if f t = it = 1 then we have a residual connection.

long short term memory (lstm)

[christopher olah]

ct+1 = f t (cid:12) ct + it (cid:12) dt

f t =   (w f [xt, ht] + bf )
it =   (w i[xt, ht] + bi)
ot =   (w o[xt, ht] + b0)

dt = tanh(w d[xt, ht] + bd)

ht+1 = ot (cid:12) tanh(ct+1)

gated recurrent unity (gru) by cho et al. 2014

[christopher olah]

ht+1 = f t (cid:12) ht + (1     f t) (cid:12) dt

f t =   (w f [xt, ht] + bf )
rt =   (w r[xt, ht] + br)

dt = tanh(w x,dxt + rt (cid:12) (w h,dht + bh,d) + bd)

36

grus vs. lstms

the gru is simpler than the lstm.

in ttic31230 class projects grus consistently outperformed
lstms.

a systematic study [collins, dickstein and sussulo 2016] states:

our results point to the gru as being the most learn-
able of gated id56s for shallow architectures, followed by
the ugid56.

update gate id56 (ugid56)

ht+1 = f t (cid:12) ht + (1     f t) (cid:12) dt

f t =   (w f [xt, ht] + bf )

dt = tanh(w d[xt, ht] + bd)

38

end

