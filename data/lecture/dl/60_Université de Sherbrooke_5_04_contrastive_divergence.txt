neural networks
restricted id82 - contrastive divergence

d  epartement d   informatique
universit  e de sherbrooke

restricted id82

hugo.larochelle@usherbrooke.ca

topics: rbm, visible layer, hidden layer, energy function

october 10, 2012

math for my slides    restricted id82s   .

    x(t)   log p(x(t))

energy function:

bj

bias

h
abstract
w connections

hidden layer
(binary units)

ck

x

visible layer
(binary units)

e(x, h) =  h>wx   c>x   b>h
wj,khjxk  xk

=  xj xk

ckxk  xj

bjhj

distribution: p(x, h) = exp( e(x, h))/z

p(x, h) = exp( e(x, h))/z

= exp(h>wx + c>x + b>h)/z

partition function

(intractable)

2

(1)
(2)

(3)
(4)

training

3

topics: training objective
    to train an rbm, we   d like to minimize the average negative
   log-likelihood (nll)

1

1

1

l(f (x(t))) =

l(f (x(t))) =
1

t xt   log p(x(t))
t xt   log p(x(t))
    we   d like to proceed by stochastic id119
   x(t)    ex,h    @e(x, h)
@   log p(x(t))

t xt
t xt
= eh    @e(x(t), h)

positive phase

negative phase

@   

@   

@   

 

{{

training

3

topics: training objective
    to train an rbm, we   d like to minimize the average negative
   log-likelihood (nll)

1

1

1

l(f (x(t))) =

l(f (x(t))) =
1

t xt   log p(x(t))
t xt   log p(x(t))
    we   d like to proceed by stochastic id119
   x(t)    ex,h    @e(x, h)
@   log p(x(t))

t xt
t xt
= eh    @e(x(t), h)

positive phase

negative phase

@   

@   

@   

 

hard to
compute

{{

contrastive divergence (cd)

    p(x)
   
(hinton, neural computation, 2002)

1

4

1

t xt   log p(x(t))

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

@   log p(x(t))

@   

topics: contrastive divergence, negative sample

   

1. replace the expectation by a point estimate at
2. obtain the point     by id150

  x

  x

    idea: 
    p(x)
   

3. start sampling chain at  ...

    x(t)

   
  p(h|x)

  p(x|h)

@   log p(x(t))

@   

    x(t)

x1

xk =   x

negative sample

1

l(f (x(t))) =

t xt
= eh    @e(x(t), h)

@   

1

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

@   log p(x(t))

   

1

1

@   

   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

t xt
contrastive divergence (cd)
l(f (x(t))) =
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
1
   x(t)    ex,h    @e(x, h)
(hinton, neural computation, 2002)
   x(t)     
topics: contrastive divergence, negative sample
@e(x(t),   h(t))
   
@   log p(x(t))
     
ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

 

@   

@   

@   

@   

@   

@   

@   

   

@   

@   

@   

1

5

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

@   

   x(t)     
     
ex,h    @e(x, h)
@   
e(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

   

    (x(t),   h(t))

(  x,   h)

@   log p(x(t))

   

1

1

@   

   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

t xt
contrastive divergence (cd)
l(f (x(t))) =
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
1
   x(t)    ex,h    @e(x, h)
(hinton, neural computation, 2002)
   x(t)     
topics: contrastive divergence, negative sample
@e(x(t),   h(t))
   
@   log p(x(t))
     
ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

 

@   

@   

@   

@   

@   

@   

@   

   

@   

@   

@   

@   

1

6

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

   x(t)     
     
ex,h    @e(x, h)

@   

p(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

   

    (x(t),   h(t))

(  x,   h)

@   log p(x(t))

   

1

1

@   

   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

t xt
contrastive divergence (cd)
l(f (x(t))) =
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
1
   x(t)    ex,h    @e(x, h)
(hinton, neural computation, 2002)
   x(t)     
topics: contrastive divergence, negative sample
@e(x(t),   h(t))
   
@   log p(x(t))
     
ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

 

@   

@   

@   

@   

@   

@   

@   

   

@   

@   

@   

@   

1

6

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

   x(t)     
     
ex,h    @e(x, h)

@   

p(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

   

    (x(t),   h(t))

(  x,   h)

@   log p(x(t))

   

1

1

@   

   x(t)    ex,h    @e(x, h)

= eh    @e(x(t), h)
 

t xt
contrastive divergence (cd)
l(f (x(t))) =
    x(t)   h(t) = h0
= eh    @e(x(t), h)
   
@   

@   log p(x(t))
    p(x)
t xt   log p(x(t))
@   
   
1
   x(t)    ex,h    @e(x, h)
(hinton, neural computation, 2002)
   x(t)     
topics: contrastive divergence, negative sample
@e(x(t),   h(t))
   
@   log p(x(t))
     
ex,h    @e(x, h)

t xt
= eh    @e(x(t), h)

eh    @e(x(t), h)

eh    @e(x(t), h)

l(f (x(t))) =

@e(x(t),   h(t))

@e(  x,   h)

 

@   

@   

@   

@   

@   

@   

@   

   

@   

@   

@   

@   

1

6

t xt   log p(x(t))
   x(t)    ex,h    @e(x, h)

   x(t)     
     
ex,h    @e(x, h)

@   

p(x, h)

@e(  x,   h)

    x(t)   h(t) = h0
   

@   

@   

eh    @e(x(t), h)

   x(t)     
     
ex,h    @e(x, h)

@   

@e(x(t),   h(t))

@   

@e(  x,   h)

@   

   

    (x(t),   h(t))

(  x,   h)

