neural networks
deep learning - unsupervised pre-training

deep learning

2

topics: why training is hard
    depending on the problem, one or the other situation will 
tend to prevail

    if    rst hypothesis (under   tting): use better optimization

    this is an active area of research

    if second hypothesis (over   tting): use better id173

    unsupervised learning
    stochastic   dropout   training

unsupervised pre-training

3

topics: unsupervised pre-training
    solution: initialize hidden layers using unsupervised learning

    force network to represent latent structure of input distribution

character image

random image

    encourage hidden layers to encode that structure

unsupervised pre-training

3

topics: unsupervised pre-training
    solution: initialize hidden layers using unsupervised learning

    force network to represent latent structure of input distribution

why is one
a character
and the other

is not ?

character image

random image

    encourage hidden layers to encode that structure

unsupervised pre-training

4

topics: unsupervised pre-training
    solution: initialize hidden layers using unsupervised learning

    this is a harder task than supervised learning (classi   cation)

why is one
a character
and the other

is not ?

character image
    hence we expect less over   tting

random image

5

feedforward neural network

feedforward neural network
hugo larochelle

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

hugo larochelle

hugo larochelle

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

hugo.larochelle@usherbrooke.ca

    x1 xd b w1 wd
    w

d  epartement d   informatique
universit  e de sherbrooke

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

unsupervised pre-training
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
feedforward neural network
feedforward neural network
topics: unsupervised pre-training
feedforward neural network
feedforward neural network
    x1 xd b w1 wd
    x1 xd b w1 wd
    we will use a greedy, layer-wise procedure
hugo larochelle
hugo larochelle
    w
    w
d  epartement d   informatique
    train one layer at a time, from    rst to last, with unsupervised criterion
d  epartement d   informatique
universit  e de sherbrooke
    {
    {
universit  e de sherbrooke
       x the parameters of previous hidden layers
    g(a) = a
    g(a) = a
    g(a) = a
hugo.larochelle@usherbrooke.ca
...
...
    previous layers viewed as feature extraction
september 6, 2012
september 6, 2012
    g(a) = sigm(a) =
    g(a) = sigm(a) =
    g(a) = sigm(a) =
september 6, 2012
september 6, 2012
1+exp( a)
1+exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = tanh(a) = exp(a) exp( a)
...
...
...
...
abstract
abstract
abstract
abstract
    g(a) = max(0, a)
    g(a) = max(0, a)
    g(a) = max(0, a)
math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .
...
...
...
...
...
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
1
1
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    g(  ) b
    g(  ) b
    g(  ) b
...
...
...
...
...
    w (1)
    w (1)
b(1)
    w (1)
b(1)
b(1)
    x1 xd
    x1 xd
    x1 xd
    x1 xd
    x1 xd
xj h(x)i
xj h(x)i
i
i
i
    w
    w
    w
    w
    w
    h(x) = g(a(x))
    h(x) = g(a(x))
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    {
    {

...
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
...
    x1 xd
xj h(x)i
    w
    {

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

i,j xj   
i pj w (1)

i,j xj   
i pj w (1)

i,j xj   
i pj w (1)

abstract

exp(2a)+1

exp(2a)+1

exp(2a)+1

i,j

i,j

i,j

1

1

1

1

1

1

1

math for my slides    feedforward neural network   .

abstract

unsupervised pre-training

6

topics: unsupervised pre-training
    we call this procedure unsupervised pre-training

       rst layer:    nd hidden unit features that are more common in 
training inputs than in random inputs
    second layer:    nd combinations of hidden unit features that are 
more common than random hidden unit features
    third layer:    nd combinations of combinations of ...
    etc.

    pre-training initializes the parameters in a region such that the near 
local optima over   t less the data

topics:    ne-tuning
    once all layers are pre-trained

    add output layer
    train the whole network using supervised learning
    supervised learning is performed as in
a regular feed-forward network
    forward propagation, id26 and update

    we call this last phase    ne-tuning

    all parameters are       tuned       for the supervised task

at hand

    representation is adjusted to be more discriminative

math for my slides    feedforward neural network   .

7

fine-tuning

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

september 6, 2012

feedforward neural network

feedforward neural network
hugo larochelle

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

1

1

...

...

september 6, 2012

    x1 xd b w1 wd
    w
    {
    g(a) = a
...
    g(a) = sigm(a) =
1+exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
...
    g(a) = max(0, a)
1
    g(a) = reclin(a) = max(0, a)
    g(  ) b
...
    w (1)
b(1)
    x1 xd
i
    w
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)

...
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
...
    x1 xd
xj h(x)i
    w
    {

...
math for my slides    feedforward neural network   .
...

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

i,j xj   
i pj w (1)

abstract

exp(2a)+1

i,j

1

1

abstract

math for my slides    feedforward neural network   .

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
 xh(1)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
deep learning
   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log p(h(1))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
d =nh(l 1)(x(t))ot
 xh(1)
q(h(1)|x) log p(h(1))
 xh(1)
 xh(1)
 xh(1)
 xh(1)
q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot

 xh(1)
d =nh(l 1)(x(t))ot

    build unsupervised training set (with                     ): 
    w(l+1) b(l+1)

topics: pseudocode
    for l=1 to l

q(h(1)|x) log p(h(1))

h(0)(x) = x

q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))

(9)

t=1

t=1
t=1

t=1

   

   

t=1

   

   
   

   
   

 

t=1
t=1

q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

8

h(0)(x) = x

    w(l+1) b(l+1)

 to initialize the deep network parameters        ,

    train       greedy module       (rbm, autoencoder) on 
    use hidden layer weights and biases of greedy module
h(0)(x) = x
h(0)(x) = x
    initialize          ,           randomly (as usual)
    train the whole neural network using (supervised) 
stochastic id119 (with backprop)

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

h(0)(x) = x
h(0)(x) = x

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

   r(x)={x2rh|9wx=pjwjx  ,j}
   {x2rh|x/2r(x)}

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

h(0)(x) = x

t=1

t=1

   

   
   

    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
 xh(1)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
deep learning
   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log p(h(1))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
d =nh(l 1)(x(t))ot
 xh(1)
q(h(1)|x) log p(h(1))
 xh(1)
 xh(1)
 xh(1)
 xh(1)
q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot

 xh(1)
d =nh(l 1)(x(t))ot

    build unsupervised training set (with                     ): 
    w(l+1) b(l+1)

topics: pseudocode
    for l=1 to l

q(h(1)|x) log p(h(1))

pre-training

q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))

(9)

t=1
t=1

   

   

t=1

   
   

 

t=1
t=1

q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

8

h(0)(x) = x

    w(l+1) b(l+1)

 to initialize the deep network parameters        ,

    train       greedy module       (rbm, autoencoder) on 
    use hidden layer weights and biases of greedy module
h(0)(x) = x
h(0)(x) = x
    initialize          ,           randomly (as usual)
    train the whole neural network using (supervised) 
stochastic id119 (with backprop)

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

h(0)(x) = x
h(0)(x) = x

   r(x)={x2rh|9wx=pjwjx  ,j}
   {x(t)}
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
 xh(1)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
deep learning
   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
 xh(1)
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
    kl(q||p) =ph(1) q(h(1)|x) log    q(h(1)|x)
p(h(1)|x)   
q(h(1)|x) log p(h(1))
   9w,t    x(t   ) =pt6=t   wtx(t)
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
    p(x|h(1)) p(h(1)) h(1) h(2) p(h(1)) =ph(2) p(h(1), h(2))
   {x2rh|x/2r(x)}
d =nh(l 1)(x(t))ot
 xh(1)
q(h(1)|x) log p(h(1))
 xh(1)
 xh(1)
 xh(1)
 xh(1)
q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))
   r(x)={x2rh|9w x=pjwjx  ,j}
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
d =nh(l 1)(x(t))ot
   {x2rh|x /2r(x)}

    train       greedy module       (rbm, autoencoder) on 
    use hidden layer weights and biases of greedy module
h(0)(x) = x
h(0)(x) = x
    initialize          ,           randomly (as usual)
    train the whole neural network using (supervised) 
stochastic id119 (with backprop)

 xh(1)
d =nh(l 1)(x(t))ot

    build unsupervised training set (with                     ): 
    w(l+1) b(l+1)

topics: pseudocode
    for l=1 to l

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

 to initialize the deep network parameters        ,

    w(l+1) b(l+1)

q(h(1)|x) log p(h(1))

    w(l) b(l) w(l+1) b(l+1)
    w(l) b(l) w(l+1) b(l+1)

   ne-
tuning

pre-training

h(0)(x) = x

h(0)(x) = x

h(0)(x) = x
h(0)(x) = x

(9)

t=1

t=1
t=1

t=1

   

   

   

   
   

   
   

 

t=1

q(h(1)|x) log p(h(1))
q(h(1)|x) log p(h(1))

t=1
t=1

q(h(1)|x) log q(h(1)|x)
q(h(1)|x) log q(h(1)|x)

8

what kind of unsupervised learning ? 9
topics: stacked rbms, stacked autoencoders
    stacked restricted id82s:

    hinton, teh and osindero suggested this procedure with rbms

- a fast learning algorithm for deep belief nets. 

hinton, teh, osindero., 2006.

- to recognize shapes,    rst learn to generate images. 

hinton, 2006.

    stacked autoencoders:

    bengio, lamblin, popovici and larochelle studied and generalized the procedure to 

autoencoders
- greedy layer-wise training of deep networks. 
bengio, lamblin, popovici and larochelle, 2007.

    ranzato, poultney, chopra and lecun also generalized it to sparse autoencoders

- ef   cient learning of sparse representations with an energy-based model. 

ranzato, poultney, chopra and lecun, 2007.

what kind of unsupervised learning ? 10
topics: stacked rbms, stacked autoencoders
    stacked denoising autoencoders:

    proposed by vincent, larochelle, bengio and manzagol

- extracting and composing robust features with denoising autoencoders, 
vincent, larochelle, bengio and manzagol, 2008.

    and more:

    stacked semi-supervised embeddings

- deep learning via semi-supervised embedding. weston, ratle and collobert, 2008.

    stacked kernel pca

- kernel methods for deep learning. 

cho and saul, 2009.

    stacked independent subspace analysis

- learning hierarchical invariant spatio-temporal features for action recognition with 

independent subspace analysis. 
le, zou, yeung and ng, 2011.

