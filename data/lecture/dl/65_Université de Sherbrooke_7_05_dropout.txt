neural networks
deep learning - dropout

deep learning

2

topics: why training is hard
    depending on the problem, one or the other situation will 
tend to prevail

    if    rst hypothesis (under   tting): use better optimization

    this is an active area of research

    if second hypothesis (over   tting): use better id173

    unsupervised learning
    stochastic   dropout   training

feedforward neural network
hugo larochelle

3

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

1

...

exp(ac )

exp(ac )

exp(ac )

id203 0.5

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

september 6, 2012

    x1 xd b w1 wd
dropout
    w
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
topics: dropout
pc exp(ac) . . .
pc exp(ac) . . .
    g(a) = a
    idea:   cripple   neural network by
    f (x)
    p(y = c|x)
    f (x)
removing hidden units stochastically
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
    each hidden unit is set to 0 with
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    hidden units cannot co-adapt to other
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    hidden units must be more generally 
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    could use a different dropout
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
id203, but 0.5 usually
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
works well
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

abstract

exp(2a)+1

useful

units

exp(ac )

exp(ac )

exp(ac )

...

i,j xj   

math for my slides    feedforward neural network   .

abstract

feedforward neural network
hugo larochelle

3

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

1

...

exp(ac )

exp(ac )

exp(ac )

id203 0.5

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

september 6, 2012

    x1 xd b w1 wd
dropout
    w
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
topics: dropout
pc exp(ac) . . .
pc exp(ac) . . .
    g(a) = a
    idea:   cripple   neural network by
    f (x)
    p(y = c|x)
    f (x)
removing hidden units stochastically
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
    each hidden unit is set to 0 with
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    hidden units cannot co-adapt to other
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    hidden units must be more generally 
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    could use a different dropout
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
id203, but 0.5 usually
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
works well
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

abstract

exp(2a)+1

useful

units

exp(ac )

exp(ac )

exp(ac )

...

i,j xj   

math for my slides    feedforward neural network   .

abstract

feedforward neural network
hugo larochelle

3

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

1

...

exp(ac )

exp(ac )

exp(ac )

id203 0.5

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

september 6, 2012

    x1 xd b w1 wd
dropout
    w
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
topics: dropout
pc exp(ac) . . .
pc exp(ac) . . .
    g(a) = a
    idea:   cripple   neural network by
    f (x)
    p(y = c|x)
    f (x)
removing hidden units stochastically
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
    each hidden unit is set to 0 with
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    hidden units cannot co-adapt to other
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    hidden units must be more generally 
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    could use a different dropout
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
id203, but 0.5 usually
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
works well
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

abstract

exp(2a)+1

useful

units

exp(ac )

exp(ac )

exp(ac )

...

i,j xj   

math for my slides    feedforward neural network   .

abstract

feedforward neural network
hugo larochelle

4

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

1

...

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

september 6, 2012

    x1 xd b w1 wd
dropout
    w
    p(y = c|x)
    p(y = c|x)
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
topics: dropout
pc exp(ac) . . .
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    g(a) = a
    f (x)
    f (x)
    use random binary masks m(k) 
pc exp(ac) . . .
    p(y = c|x)
    p(y = c|x)
    f (x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    layer pre-activation for k>0
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
pc exp(ac) . . .
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    h(k)(x) = g(a(k)(x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    hidden layer activation (k from 1 to l):
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
...
    h(k)(x) = g(a(k)(x))
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    output layer activation (k=l+1):
    h(k)(x) = g(a(k)(x))
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

i,j xj   

math for my slides    feedforward neural network   .

abstract

feedforward neural network
hugo larochelle

4

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

1

...

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

september 6, 2012

    x1 xd b w1 wd
dropout
hugo larochelle
    w
    p(y = c|x)
d  epartement d   informatique
    p(y = c|x)
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
universit  e de sherbrooke
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
hugo.larochelle@usherbrooke.ca
topics: dropout
pc exp(ac) . . .
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    g(a) = a
    f (x)
    f (x)
    use random binary masks m(k) 
september 28, 2013
pc exp(ac) . . .
    p(y = c|x)
    p(y = c|x)
    f (x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    layer pre-activation for k>0
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
pc exp(ac) . . .
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
abstract
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    h(k)(x) = g(a(k)(x))
exp(ac )
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
    p(y = c|x)
    f (x)
math for my slides    deep learning   .
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    hidden layer activation (k from 1 to l):
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x)   m(k))
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
   m(k)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
...
    h(k)(x) = g(a(k)(x))
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)   m(l+1)) = f (x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    x h(1) h(2) h(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    output layer activation (k=l+1):
    h(k)(x) = g(a(k)(x))
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    p(h(2), h(3))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

abstract

exp(2a)+1

exp(ac )

exp(ac )

i,j xj   

math for my slides    feedforward neural network   .

abstract

5

=

   

=

=

@a(k)(x)i
@a(k)(x)i
@a(k)(x)i
@b(k)
@b(k)
@b(k)
i
i
i

  log f (x)y
  log f (x)y
=  ra(k)(x)   log f (x)y  h(k 1)(x)>
  log f (x)y
b(k)
b(k)
b(k)
i
@a(k)(x)i
i
@   log f (x)y
dropout
i
@   log f (x)y
@   log f (x)y
@a(k)(x)i
@b(k)
@   log f (x)y
=
=
i
=
@a(k)(x)i
@a(k)(x)i
@   log f (x)y
@a(k)(x)i
@
  log f (x)y
@   log f (x)y
@a(k)(x)i
@   log f (x)y
b(k)
@   log f (x)y
=
topics: dropout id26
=
i
=
@a(k)(x)i
@a(k)(x)i
@a(k)(x)i
@   log f (x)y
@a(k)(x)i
    this assumes a forward propagation has been made before
rb(k)   log f (x)y
@a(k)(x)i
@b(k)
= ra(k)(x)   log f (x)y
rb(k)   log f (x)y
@   log f (x)y
rb(k)   log f (x)y
rb(k)   log f (x)y
@a(k)(x)i
= ra(k)(x)   log f (x)y
= ra(k)(x)   log f (x)y
= ra(k)(x)   log f (x)y

    compute output gradient (before activation)
=
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    for k from l+1 to 1
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
- compute gradients of hidden layer parameter 
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
- compute gradient of hidden layer below
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    [. . . , g0(a(k 1)(x)j), . . . ]
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
- compute gradient of hidden layer below (before activation)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    [. . . , g0(a(k 1)(x)j), . . . ]

= ra(k)(x)   log f (x)y

rb(k)   log f (x)y

3

i

3
3

3

i

=

   

=

=

@a(k)(x)i
@a(k)(x)i
@a(k)(x)i
@b(k)
@b(k)
@b(k)
i
i
i

  log f (x)y
  log f (x)y
=  ra(k)(x)   log f (x)y  h(k 1)(x)>
  log f (x)y
b(k)
b(k)
b(k)
i
@a(k)(x)i
i
@   log f (x)y
dropout
i
@   log f (x)y
@   log f (x)y
@a(k)(x)i
@b(k)
@   log f (x)y
=
=
i
=
@a(k)(x)i
@a(k)(x)i
@   log f (x)y
@a(k)(x)i
@
  log f (x)y
@   log f (x)y
@a(k)(x)i
@   log f (x)y
b(k)
@   log f (x)y
=
topics: dropout id26
=
i
=
@a(k)(x)i
@a(k)(x)i
@a(k)(x)i
@   log f (x)y
@a(k)(x)i
    this assumes a forward propagation has been made before
rb(k)   log f (x)y
@a(k)(x)i
@b(k)
= ra(k)(x)   log f (x)y
rb(k)   log f (x)y
@   log f (x)y
rb(k)   log f (x)y
rb(k)   log f (x)y
@a(k)(x)i
= ra(k)(x)   log f (x)y
= ra(k)(x)   log f (x)y
includes the 
= ra(k)(x)   log f (x)y
mask m(k   1) 

    compute output gradient (before activation)
=
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    for k from l+1 to 1
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
- compute gradients of hidden layer parameter 
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
- compute gradient of hidden layer below
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    [. . . , g0(a(k 1)(x)j), . . . ]
    h(k)(x) = g(a(k)(x)   m(k))
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
- compute gradient of hidden layer below (before activation)
                             m(k   1)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    [. . . , g0(a(k 1)(x)j), . . . ]
    h(l+1)(x) = o(a(l+1)(x)   m(l+1)) = f (x)
    x h(1) h(2) h(3)

math for my slides    deep learning   .

= ra(k)(x)   log f (x)y

rb(k)   log f (x)y

3
3

3

3

5

deep learning

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

september 28, 2013

abstract

dropout

6

topics: test time classi   cation
    at test time, we replace the masks by their expectation

    this is simply the constant vector 0.5 if dropout id203 is 0.5
    for single hidden layer, can show this is equivalent to taking the geometric average 

of all neural networks, with all possible binary masks

    can be combined with unsupervised pre-training

    beats regular id26 on many datasets

    improving neural networks by preventing co-adaptation of feature detectors. 

hinton, srivastava, krizhevsky, sutskever and salakhutdinov, 2012.

