autoencoders
ift 725 - r  seaux neuronaux

unsupervised learning

topics: unsupervised learning
    unsupervised learning: only use the inputs       for learning

    x(t)   log p(x(t))

    automatically extract meaningful features for your data
    leverage the availability of unlabeled data
    add a data-dependent regularizer to trainings

math for my slides    restricted id82s   .

    we will see 3 neural networks for unsupervised learning

    restricted id82s
    autoencoders
    sparse coding model

2

hugo larochelle

hugo larochelle

math for my slides    autoencoders   .
d  epartement d   informatique
universit  e de sherbrooke

autoencoder
topics: autoencoder, encoder, decoder, tied weights
hugo.larochelle@usherbrooke.ca
    feed-forward neural network trained to reproduce its input at 
= sigm(b + wx)
october 17, 2012
the output layer

d  epartement d   informatique
universit  e de sherbrooke

h(x) = g(a(x))

hugo.larochelle@usherbrooke.ca

october 16, 2012
decoder

ck

w 

 x

abstract

math for my slides    autoencoders   .

 
 
    f (x)    bx l(f (x)) =pk(bxk   xk)2

= sigm(c + w   h(x))

= sigm(b + wx)

h(x) = g(a(x))

bx = o(ba(x))

w

x

bj

abstract
= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

encoder
h(x) = g(a(x))

= sigm(b + wx)

3

hugo larochelle

hugo larochelle

math for my slides    autoencoders   .
d  epartement d   informatique
universit  e de sherbrooke

autoencoder
topics: autoencoder, encoder, decoder, tied weights
hugo.larochelle@usherbrooke.ca
    feed-forward neural network trained to reproduce its input at 
= sigm(b + wx)
october 17, 2012
the output layer

d  epartement d   informatique
universit  e de sherbrooke

h(x) = g(a(x))

hugo.larochelle@usherbrooke.ca

october 16, 2012
decoder

ck

 x

abstract

= w 
w 
(tied weights)

math for my slides    autoencoders   .

 
 
    f (x)    bx l(f (x)) =pk(bxk   xk)2

= sigm(c + w   h(x))

= sigm(b + wx)

h(x) = g(a(x))

bx = o(ba(x))

w

x

bj

abstract
= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

encoder
h(x) = g(a(x))

= sigm(b + wx)

3

hugo larochelle

hugo larochelle

math for my slides    autoencoders   .
d  epartement d   informatique
universit  e de sherbrooke

autoencoder
topics: autoencoder, encoder, decoder, tied weights
hugo.larochelle@usherbrooke.ca
    feed-forward neural network trained to reproduce its input at 
= sigm(b + wx)
october 17, 2012
the output layer

d  epartement d   informatique
universit  e de sherbrooke

h(x) = g(a(x))

hugo.larochelle@usherbrooke.ca

october 16, 2012
decoder

ck

 x

abstract

= w 
w 
(tied weights)

math for my slides    autoencoders   .

 
 
    f (x)    bx l(f (x)) =pk(bxk   xk)2

= sigm(c + w   h(x))

= sigm(b + wx)

h(x) = g(a(x))

bx = o(ba(x))

w

x

bj

abstract
= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

 
for binary inputs
encoder
h(x) = g(a(x))

= sigm(b + wx)

3

math for my slides    autoencoders   .
   

   
autoencoder

= sigm(c + w   h(x))

    f (x)    bx l(f (x)) =pk(bxk   xk)2

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

h(x) = g(b + wx)

topics: id168
    for real-valued inputs:

= sigm(b + wx)

- sum of squared differences
- squared euclidean distance

2pk(bxk   xk)2

    f (x)    bx l(f (x)) = 1
    rba(x(t))l(f (x(t))) =bx(t)   x(t)
bx = o(c + w   h(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

- cross-id178 (more precisely: sum of bernoulli cross-entropies)

= sigm(c + w   h(x))

    for binary inputs:

a(x(t)) (= b + wx(t)
h(x(t)) (= sigm(a(x(t)))

ba(x(t)) (= c + w>h(x(t))
bx(t) (= sigm(ba(x(t)))

4

    f (x)    bx l(f (x)) =pk(bxk   xk)2

   

   

   

= sigm(c + w   h(x))

    f (x)    bx l(f (x)) =pk(bxk   xk)2

topics: id168 gradient
    for both cases, the gradient  
has a very simple form:

    rba(x(t))l(f (x(t))) =bx(t)   x(t)
bx = o(ba(x))
   
h(x(t)) (= sigm(a(x(t)))
a(x(t)) (= b + wx(t)
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
    f (x)    bx l(f (x)) =pk(bxk   xk)2
ba(x(t)) (= c + w>h(x(t))
autoencoder
h(x(t)) (= sigm(a(x(t)))
    rba(x(t))l(f (x(t))) =bx(t)   x(t)
bx(t) (= sigm(ba(x(t)))
ba(x(t)) (= c + w>h(x(t))
bx = o(ba(x))
a(x(t)) (= b + wx(t)
bx = o(ba(x))
bx(t) (= sigm(ba(x(t)))
h(x(t)) (= sigm(a(x(t)))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
    f (x)    bx l(f (x)) =pk(bxk   xk)2
= sigm(c + w   h(x))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
ba(x(t)) (= c + w>h(x(t))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
bx(t) (= sigm(ba(x(t)))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
    f (x)    bx l(f (x)) =pk(bxk   xk)2
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
a(x(t)) (= b + wx(t)
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
    rba(x(t))l(f (x(t))) =bx(t)   x(t)
h(x(t)) (= sigm(a(x(t)))
rbl(f (x(t))) (= ra(x(t))l(f (x(t)))
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
ba(x(t)) (= c + w>h(x(t))
rbl(f (x(t))) (= ra(x(t))l(f (x(t)))
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
rbl(f (x(t))) (= ra(x(t))l(f (x(t)))
bx(t) (= sigm(ba(x(t)))
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
a(x(t)) (= b + wx(t)
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
h(x(t)) (= sigm(a(x(t)))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
ba(x(t)) (= c + w>h(x(t))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
bx(t) (= sigm(ba(x(t)))
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]

    parameter gradients are obtained by backpropagating the 
gradient                      like in a regular network
    important: when using tied weights (             ),                    is the 
    w    = w>
sum of two gradients !
- this is because        is present in the encoder and in the decoder
    rwl(f (x(t))) w

    w    = w>
    rwl(f (x(t)))

rbl(f (x(t))) (= ra(x(t))l(f (x(t)))

    w    = w>

1

5

   
autoencoder

    f (x)    bx l(f (x)) =pk(bxk   xk)2

-

topics: adaptation to the type of input
    recipe to adapt an autoencoder to a new
type of input
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    choose a joint distribution             over the inputs
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    p(x|  )   
    is the vector of parameters of that distribution
    p(x|  )   
    rwl(f (x(t))) w
    p(x|  )   
    p(x|  )   
       h(x)
    rwl(f (x(t))) w
    rwl(f (x(t))) w
       h(x)
       h(x)
    p(x|  )   
       h(x)
    l(f (x(t))) =   log p(x(t))
    p(x|  )   
    l(f (x)) =   log p(x|  )
       h(x)
    l(f (x(t))) =   log p(x(t))
    l(f (x(t))) =   log p(x(t))
    p(x|  )   
2pk(xk     k)2)    = c + w   h(x)
    example: we get the sum of squared distance by
       h(x)
    rwl(f (x(t))) w
    l(f (x(t))) =   log p(x(t))
    p(x|  ) =
       h(x)
    choosing a gaussian distribution with mean     and identity covariance 
    l(f (x)) =   log p(x|   = h(x))
    p(x|  )   
2pk(xk     k)2)    = c + w   h(x)
for
    l(f (x)) =   log p(x|   = h(x))
(2   )d/2 exp(  1
    p(x|  ) =
       h(x)
    choosing
       = c + w   h(x)
    l(f (x(t))) =   log p(x(t))

    choose the relationship between    and the hidden layer
    use                                  as the id168

(2   )d/2 exp(  1

1

1

6

example of data set: mnist

larochelle, bengio, louradour and lamblin

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

figure 5: samples from the mnist digit recognition data set. here, a black pixel corresponds to
an input value of 0 and a white pixel corresponds to 1 (the inputs are scaled between 0

7

of the    rst hidden layer is obtained by a dot product of such a weight    image    with the
input image. in these images, a black pixel corresponds to a weight smaller than    3 and
a white pixel to a weight larger than 3, with the different shades of gray corresponding
to different weight values uniformly between    3 and 3.
(larochelle et al., jmlr2009)

filters (autoencoder)

93

1
1

0.9
0.9

0.8
0.8

0.7
0.7

0.6
0.6

0.5
0.5

0.4
0.4

0.3
0.3

0.2
0.2

0.1
0.1

0
0

0
0

0.1
0.1

0.2
0.2

0.3
0.3

0.4
0.4

0.5
0.5

0.6
0.6

0.7
0.7

0.8
0.8

0.9
0.9

1
1

figure 6.7     input weights of a random subset of the hidden units, learned by an autoas-
figure 6.6     display of the input weights of a random subset of the hidden units, learned

8

1

1

1

1

2pk(xk     k)2)    = c + w   h(x)

autoencoder

    rwl(f (x(t))) w
    rwl(f (x(t))) w
    rwl(f(x(t))) w
    p(x|  )   
    p(x|  )   
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    p(x|  )   
       h(x)
       h(x)
    p(x|  )   
    p(x|  )   
       h(x)
topics: optimality of a linear autoencoder
    l(f (x)) =   log p(x|  )
    l(f (x)) =   log p(x|  )
    rwl(f (x(t))) w
       h(x)
       h(x)
    l(f(x)) =   log p(x|  )
    to do the proof, we need the following theorem:
    p(x|  ) =
    p(x|  )   
    p(x|  ) =
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    l(f (x)) =   log p(x|  )
    l(f (x)) =   log p(x|  )
(2   )d/2 exp(  1
    p(x|  ) =
    let     be any matrix, with singular value decomposition                        
    p(x|  )   
    a a = u     v> u  ,   k       k,   k v>
    a a = u     v> u  ,   k       k,   k v>
    p(x|  )   
(2   )d/2 exp(  1
(2   )d/2 exp(  1
    p(x|  ) =
    p(x|  ) =
    l(f (x)) =   log p(x|  )
     is a diagonal matrix
-
    a a = u     v> u  ,   k       k,   k v>
       h(x)
   
   
2pk(xk     k)2)    = c + w   h(x)
       h(x)
2pk(xk     k)2)    = c + w   h(x)
(2   )d/2 exp(  1
   ,      are orthonormal matrices (columns/rows are orthonormal vectors)
    p(x|  ) =
b    = arg min
    a a = u     v> u  ,   k       k,   k v>
    a a = u     v> u  ,   k       k,   k v>
-
(2   )d/2 exp(  1
    l(f (x)) =   log p(x|  )
    l(f (x)) =   log p(x|  )
    let                                be the decomposition where we keep only 
b    = arg min
||a   b||f
    a a = u     v> u  ,   k       k,   k v>
  ,   k b
2pk(xk     k)2)    = c + w   h(x)
2pk(xk     k)2)    = c + w   h(x)
b    = u  ,   k       k,   k v>
b    = u  ,   k       k,   k v>
the k largest singular values
(2   )d/2 exp(  1
    p(x|  ) =
    p(x|  ) =
b    = arg min
b    = arg min
  ,   k
    a a = u     v> u  ,   k       k,   k v>
||a   b||f
||a   b||f
  ,   k b
b    = u  ,   k       k,   k v>
b
    then, the matrix     of rank k that is closest to     :
b    =
  ,   k
    a a = u     v> u  ,   k       k,   k v>
b s.t. rank(b)=k ||a   b||f
arg min
  ,   k b
    a a = u     v> u  ,   k       k,   k v>
    a a = u     v> u  ,   k       k,   k v>
  ,   k b
b    = u  ,   k       k,   k v>
b    = u  ,   k       k,   k v>
  ,   k
  ,   k
   
b    =
   
b s.t. rank(b)=k ||a   b||f
b    =
  ,   k
b    = arg min
||a   b||f
2xk
1
k )2   arg min
2xk

(2   )d/2 exp(  1
2pk(xk     k)2)    = c + w   h(x)
2pk(xk     k)2)    = c + w   h(x)
2pk(xk     k)2)    = c + w   h(x)

b s.t. rank(b)=k ||a   b||f
w   ,h(x)||x   w   h(x)||f
b    = u  ,   k       k,   k v>
  ,   k
9
k  bx(t)
k )2   arg min

||a   b||f
(2   )d/2 exp(  1

is 
b    = u  ,   k       k,   k v>
  ,   k

b    = u  ,   k       k,   k v>

w   ,h(x)||x   w   h(x)||f

k  bx(t)
    xt

    l(f (x)) =   log p(x|  )

b    = u  ,   k       k,   k v>

b    = u  ,   k       k,   k v>

    xt

(2   )d/2 exp(  1

(2   )d/2 exp(  1

  ,   k b
  ,   k b

b
(x(t)
  ,   k

arg min

  ,   k b

  ,   k b

arg min

arg min

arg min

  ,   k

(x(t)

1

b

b

b

1

1

1

1

2pk(xk     k)2)    = c + w   h(x)

sketch of proof

b    = u  ,   k       k,   k v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

f

1

1

1

min

  ,   k

(x(t)

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
1
(x(t)
k  bx(t)
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
2xk
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

sketch of proof

b    = u  ,   k       k,   k v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
2xk
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

sketch of proof

b    = u  ,   k       k,   k v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
2xk
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
2xk
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

sketch of proof

b    = u  ,   k       k,   k v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k

    x(t) x = u     v>

  ,   k

b    = u  ,   k       k,   k v>
arg min

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 

f

1

1

1

min

  ,   k

w   ,h(x)

w   ,h(x)

arg min
1

min
(x(t)

arg min
w   ,h(x)

arg min
w   ,h(x)

1
2||x   w   h(x)||2

1
2||x   w   h(x)||2

    x(t) x = u     v>

matrix of all hidden layers
(could be any encoder)

based on previous theorem, where
and k is the hidden layer size

    xt
2xk
matrix where columns are 
(x(t)
k  bx(t)
k )2   arg min
    xt
2xk
    xt
2xk
 
1
(x(t)
k  bx(t)
k  bx(t)
(x(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
2||x   w   h(x)||2
k )2  
f
2xk
 
 based on 
k  bx(t)
w   ,h(x)||x   w   h(x)||f
k )2   arg min
linear encoder
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
1
f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
2||x   w   h(x)||2
w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>
  ,   k 
    x = u     v>
h(x) = v>
h(x) = v>
  ,   k
  ,   k
let   s show            is a linear encoder:
  ,   k (x> x) 1 (x> x)
= v>
= v>
  ,   k (x> x) 1 (x> x)
h(x) = v>
  ,   k
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
h(x) = v>
= v>
  ,   k (x> x) 1 (x> x)
  ,   k
= v>
  ,   k (v    >     v>) 1 v    > u> x
multiplying by identity
  ,   k (v    >     v>) 1 v    > u> x
= v>
= v>
  ,   k (x> x) 1 (x> x)
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
  ,   k v (   >    ) 1 v> v    > u> x
= v>
replace with svd
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= v>
  ,   k (v    >     v>) 1 v    > u> x
= v>
  ,   k (v    > u> u     v>) 1 (v    > u> x)
= v>
  ,   k v (   >    ) 1    > u> x
ut u = i (orthonormal)
  ,   k v (   >    ) 1    > u> x
= v>
= v>
  ,   k v (   >    ) 1 v> v    > u> x
= v>
  ,   k (v    >     v>) 1 v    > u> x
= i   k,   (   >    ) 1    > u> x
v(  t  )-1vt v  t  vt = i 
= i   k,   (   >    ) 1    > u> x
= v>
  ,   k v (   >    ) 1    > u> x
  ,   k v (   >    ) 1 v> v    > u> x
= v>
= i   k,       1 (   >) 1    > u> x
vt v = i (orthonormal)
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
= i   k,   (   >    ) 1    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,       1 u> x
=     1
   k,   k (u  ,   k)> x
= i   k,       1 (   >) 1    > u> x
idem
= i   k,   (   >    ) 1    > u> x
=     1
(  t  )-1=   -1(  t)-1
= i   k,       1 u> x
= i   k,       1 (   >) 1    > u> x
   k,   k (u  ,   k)>    x w    w
     (they are pseudo-inverses) 
=     1
   k,   k u>
   k,   x
= i   k,       1 u> x
   k,   k (u  ,   k)>    x w    w
multiplying by i   k,   selects 
=     1
   k,   k (u  ,   k)> x
the k    rst rows this is a linear encoder

   k,   k (u  ,   k)> x

10

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t pt
    x(t)   1pt    x(t)   1
t0=1 x(t0)   

t0=1 x(t0)   
t pt

w   ,h(x)||x   w   h(x)||f =  w      u  ,   k       k,   k, h(x)   v>

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

  ,   k v (   >    ) 1    > u> x

= i   k,   (   >    ) 1    > u> x
autoencoder
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1
   k,   x

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
topics: optimality of a linear autoencoder
   k,   k (u  ,   k)> x
= i   k,       1 u> x
    so an optimal pair of encoder and decoder is
=     1
   k,   k u>
   k,   k u>
   k,   x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,      x w    w
   k,      x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
t pt
   k,   k u>
   k,   k u>
    for the sum of squared difference error
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    for an autoencoder with a linear decoder
    where optimality means       has the lowest training reconstruction error      
    bx = u  ,   k       k,   kx h(x) =     1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt

    encoder corresponds to principal component analysis (pca)

    if inputs are normalized as follows: 

  

t0=1 x(t0)   

   k,   k u   k,  x

- singular values and (left) vectors = the eigenvalues/vectors of covariance matrix
11

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

undercomplete hidden layer
topics: undercomplete representation
    hidden layer is undercomplete if smaller than the input layer

hugo.larochelle@usherbrooke.ca

october 17, 2012

    hidden layer       compresses       the input
    will compress well only for the 
math for my slides    autoencoders   .
training distribution
    hidden units will be 
    good features for the 
training distribution
    but bad for other 
types of input

abstract

ck

 x

w  = w 
(tied weights)

h(x) = g(a(x))

bj

= sigm(b + wx)

w

x

bx = o(ba(x))

= sigm(c + w   h(x))

12

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

overcomplete hidden layer
topics: overcomplete representation
    hidden layer is overcomplete if greater than the input layer

hugo.larochelle@usherbrooke.ca

october 17, 2012

math for my slides    autoencoders   .

    no compression in hidden layer
    each hidden unit could copy a 
different input component
    no guarantee that the 
hidden units will extract 
meaningful structure

ck

abstract

 x

w  = w 
(tied weights)

bj
h(x) = g(a(x))

= sigm(b + wx)
w

x

bx = o(ba(x))

= sigm(c + w   h(x))

13

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

math for my slides    autoencoders   .

   k,   k (u  ,   k)>    x w    w
   k,   k (u  ,   k)>    x w    w

ck

abstract

october 17, 2012

w  = w 
(tied weights)

topics: denoising autoencoder
    idea: representation should be 
robust to introduction of noise:
    random assignment of subset of 
inputs to 0, with id203
    gaussian additive noise

denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
 x
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
    p(ex|x)     ex
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    bx = sigm(c + w   h(ex))
 x
    p(ex|x)     ex
 x
bx = o(ba(x))
||rx(t)h(x(t))||2
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
2pk(bxk   xk)2
    l(f (x(t))) =  pk   x(t)
k ) log(1  bx(t)

    reconstruction     computed 
from the corrupted input
    id168 compares 
reconstruction with the 
noiseless input

    l(f (x(t))) +  ||rx(t)h(x(t))||2
   

k log(bx(t)

= sigm(b + wx)
w

k ) + (1   x(t)

= sigm(c + w   h(x))

bj
h(x) = g(a(x))

x

x

f

14

    f (x)    bx l(f (x)) = 1

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

math for my slides    autoencoders   .

ck

abstract

october 17, 2012

w  = w 
(tied weights)

topics: denoising autoencoder
    idea: representation should be 
robust to introduction of noise:
    random assignment of subset of 
inputs to 0, with id203
    gaussian additive noise

denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
 x
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    p(ex|x)     ex
    bx = sigm(c + w   h(ex))
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
 x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t pt
    l(f (x(t))) +  ||rx(t)h(x(t))||2
    p(ex|x)     ex
    x(t)   1pt    x(t)   1
0
   
 x
    p(ex|x)     ex
bx = o(ba(x))
||rx(t)h(x(t))||2
    p(ex|x)     ex
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
2pk(bxk   xk)2
    l(f (x(t))) =  pk   x(t)
k ) + (1   x(t)
k ) log(1  bx(t)

    reconstruction     computed 
from the corrupted input
    id168 compares 
reconstruction with the 
noiseless input

k log(bx(t)

= sigm(b + wx)
w

= sigm(c + w   h(x))

bj
h(x) = g(a(x))

noise process

x

0

0

x

x

f

14

    f (x)    bx l(f (x)) = 1

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

math for my slides    autoencoders   .

ck

abstract

october 17, 2012

w  = w 
(tied weights)

topics: denoising autoencoder
    idea: representation should be 
robust to introduction of noise:
    random assignment of subset of 
inputs to 0, with id203
    gaussian additive noise

denoising autoencoder
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
 x
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
t pt
    p(ex|x)     ex
    bx = sigm(c + w   h(ex))
    x(t)   1pt    x(t)   1
t0=1 x(t0)   
 x
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t pt
    l(f (x(t))) +  ||rx(t)h(x(t))||2
    p(ex|x)     ex
    x(t)   1pt    x(t)   1
0
   
 x
    p(ex|x)     ex
bx = o(ba(x))
||rx(t)h(x(t))||2
    p(ex|x)     ex
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
2pk(bxk   xk)2
    l(f (x(t))) =  pk   x(t)
k ) + (1   x(t)
k ) log(1  bx(t)

    reconstruction     computed 
from the corrupted input
    id168 compares 
reconstruction with the 
noiseless input

k log(bx(t)

= sigm(b + wx)
w

= sigm(c + w   h(x))

bj
h(x) = g(a(x))

noise process

x

0

0

x

x

f

14

    f (x)    bx l(f (x)) = 1

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

   k,   k (u  ,   k)>    x w    w

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
denoising autoencoder
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

(7)

x
x

  x

  x

x
x

figure 2. manifold learning perspective.

suppose

15

ifold. corrupted examples (.) obtained by applying cor-

2

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

   k,   k (u  ,   k)>    x w    w

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
denoising autoencoder
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

(7)

x
x

  x

  x

x
x

figure 2. manifold learning perspective.

suppose

15

ifold. corrupted examples (.) obtained by applying cor-

2

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

   k,   k (u  ,   k)>    x w    w

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
denoising autoencoder
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

(7)

x
x

  x

  x

x
x

figure 2. manifold learning perspective.

suppose

15

ifold. corrupted examples (.) obtained by applying cor-

2

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

   k,   k (u  ,   k)>    x w    w

consequently rewrite equation 6 as

q?(x, y |ex) = p(x, y |ex), where the idkl becomes 0.
we can thus write log p(ex) = maxq? l(q?, ex), and
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
denoising autoencoder
q? l(q?, ex)]}
   0 {eeq0(ex)[max
h = max
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
   0,q?{eeq0(ex)[l(q?, ex)]}
= max
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
   k,   k (u  ,   k)> x
    p(ex|x)     ex
    p(ex|x)     ex
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
p( x|x)
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
t pt
qd(  x|x)
    p(ex|x)     ex

= v>
= v>
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
= i   k,       1 u> x
=     1

t pt

g   0(f   (  x))

(7)

x
x

  x

  x

x
x

figure 2. manifold learning perspective.

suppose

15

ifold. corrupted examples (.) obtained by applying cor-

2

filters (denoising autoencoder)
    no corrupted inputs (cross-id178 loss)

(vincent, larochelle, bengio and manzagol, icml 2008)

16

rect-img
convex

24.04  0.37
19.13  0.34

24.05  0.37
19.82  0.35

23.69  0.37
19.92  0.35

filters (denoising autoencoder)
    25% corrupted inputs

(vincent, larochelle, bengio and manzagol, icml 2008)

(a) no destroyed inputs

17

(b) 25% destruction

rect-img
convex

24.04  0.37
19.13  0.34

24.05  0.37
19.82  0.35

23.69  0.37
19.92  0.35

filters (denoising autoencoder)
    50% corrupted inputs

(vincent, larochelle, bengio and manzagol, icml 2008)

(a) no destroyed inputs

18

(b) 25% destruction

vincent, larochelle, lajoie, bengio, manzagol

squared error loss

    training on natural image patches, with squared-difference loss

    pca is not the best solution

data

filters

figure 5: regular autoencoder trained on natural image patches. left: some of the 12 12

19

vincent, larochelle, lajoie, bengio, manzagol

squared error loss

    training on natural image patches, with squared-difference loss

    not equivalent to weight decay

data

filters

figure 5: regular autoencoder trained on natural image patches. left: some of the 12 12

20

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

contractive autoencoder
topics: contractive autoencoder
    alternative approach to avoid uninteresting solutions

hugo.larochelle@usherbrooke.ca

october 17, 2012

    add an explicit term in the loss that 
penalizes that solution
math for my slides    autoencoders   .

    we wish to extract features that
only re   ect the structure in
the training set
    we   d like to be invariant to the
other variations

ck

abstract

 x

w  = w 
(tied weights)

bj
h(x) = g(a(x))

= sigm(b + wx)
w

x

bx = o(ba(x))

= sigm(c + w   h(x))

21

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

t0=1 x(t0)   

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
= v>
t pt
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
contractive autoencoder
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
k )   
k ) log(1  bx(t)

@x(t)

2

22

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

t0=1 x(t0)   

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
= v>
t pt
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
contractive autoencoder
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
  encoder keeps
k )   
k ) log(1  bx(t)

good information

@x(t)

2

22

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

t0=1 x(t0)   

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
= v>
t pt
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
contractive autoencoder
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
  encoder keeps
k )   
k ) log(1  bx(t)
  encoder throws

away all information

good information

@x(t)

2

22

    bx = (u  ,   k       k,   k) h(x) h(x) =       1

t0=1 x(t0)   

   k,   k (u  ,   k)>    x w    w

t0=1 x(t0)   

  ,   k v (   >    ) 1 v> v    > u> x
  ,   k v (   >    ) 1    > u> x

    bx = (u  ,   k       k,   k) h(x) h(x) =       1
   k,   k (u  ,   k)> x
    x(t)   1pt    x(t)   1
= v>
t pt
   k,   k (u  ,   k)>    x w    w
    bx = (u  ,   k       k,   k) h(x) h(x) =       1
contractive autoencoder
= i   k,   (   >    ) 1    > u> x
= i   k,       1 (   >) 1    > u> x
t0=1 x(t0)   
    x(t)   1pt    x(t)   1
    p(ex|x)     ex
t pt
= i   k,       1 u> x
topics: contractive autoencoder
=     1
   k,   k (u  ,   k)> x
    bx = sigm(c + w   h(ex))
    new id168:
    p(ex|x)     ex
   k,   k (u  ,   k)>    x w    w
    bx = sigm(c + w   h(ex))
    l(f (x(t))) +  ||rx(t)h(x(t))||2
   
    where, for binary observations:

    l(f (x(t))) +  ||rx(t)h(x(t))||2
  autoencoder
    ||rx(t)h(x(t))||2

jacobian of 
encoder

reconstruction

f

f

    l(f (x(t))) =  pk   x(t)

k ) + (1   x(t)
k log(bx(t)
f =xj xk   @h(x(t))j
k !2

@x(t)

||rx(t)h(x(t))||2

||rx(t)h(x(t))||2

f =pjpk    @h(x)j
@xk    2
f =xj xk   @h(x(t))j
k !2
  encoder keeps
k )   
k ) log(1  bx(t)
  encoder throws

encoder keeps
only good 
information

away all information

good information

@x(t)

2

22

contractive autoencoder
topics: contractive autoencoder
    illustration:

x

x

x

x

x

x

x

x
x

23

contractive autoencoder
topics: contractive autoencoder
    illustration:

x

x

x

x

x

x

x

x
x

23

contractive autoencoder
topics: contractive autoencoder
    illustration:

x

x

x

x

x

x

x

x
x

23

contractive autoencoder
topics: contractive autoencoder
    illustration:

x

x

x

x

x
x

x

x

x
encoder must be

sensitive to this variation

to reconstruct well

23

contractive autoencoder
topics: contractive autoencoder
    illustration:

encoder doesn   t need to be 
sensitive to this variation

(not observed in training set)

x

x

x

x

x
x

x

x

x
encoder must be

sensitive to this variation

to reconstruct well

23

which autoencoder ?
topics: denoising autoencoder, contractive autoencoder
    both the denoising and contractive autoencoder perform well

    advantage of denoising autoencoder: simpler to implement
- requires adding one or two lines of code to regular autoencoder
- no need to compute jacobian of hidden layer

    advantage of contractive autoencoder: gradient is deterministic 

- can use second order optimizers (conjugate gradient, lbfgs, etc.)
- might be more stable than denoising autoencoder, which uses a sampled 

gradient

    to learn more on contractive autoencoders:

- contractive auto-encoders: explicit invariance during feature extraction.

salah rifai, pascal vincent, xavier muller, xavier glorot et yoshua bengio, 2011.

24

conclusion

    we saw the autoencoder, a type of unsupervised learning 
neural network
    we discussed problematic cases, where it tends to extract a 
representation which is not very interesting
    linear decoder, sum of squared differences: equivalent to pca
    other losses, but with overcomplete hidden layer: learns trivial solution

    we saw how we could avoid these issues with two new 
autoencoders
    denoising autoencoder: add noise to input and learn to remove it
    contractive autoencoder: add penalty to encourage invariant hidden layer

25

assignment 3

    you must implement a denoising autoencoder that uses the 
      random assignment to 0       noise

    i   ve only given you the gradient at the input of the output layer

    you   ll need to    gure out the rest of id26
    the report must contain the backprop equations you implemented

26

