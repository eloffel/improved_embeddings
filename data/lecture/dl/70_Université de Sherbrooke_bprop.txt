training neural networks
ift 725 - r  seaux neuronaux

feedforward neural network
hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

1

...

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

    p(y = c|x)

review

september 6, 2012

math for my slides    feedforward neural network   .

    x1 xd b w1 wd
neural network
    w
    p(y = c|x)
    p(y = c|x)
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
topics: multilayer neural network
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    g(a) = a
    f (x)
    could have l hidden layers:
    f (x)
    f (x)
pc exp(ac) . . .
    p(y = c|x)
    p(y = c|x)
    f (x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    g(a) = sigm(a) =
    l(f (x(t);    ), y(t))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    layer input activation for k>0
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    r   l(f (x(t);    ), y(t))
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    h(k)(x) = g(a(k)(x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
    p(y = c|x)
    f (x)
       (   )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    hidden layer activation (k from 1 to l):
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    r      (   )
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
...
    h(k)(x) = g(a(k)(x))
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)c = p(y = c|x)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    x(t) y(t)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    output layer activation (k=l+1):
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
   
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

exp(ac )

exp(ac )

exp(ac )

2

math for my slides    feedforward neural network   .

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
    training set: dtrain = {(x(t), y(t))}
t b    = 1
    t 1
machine learning
    supervised learning example: (x, y) x y
    f (x;    )
machine learning
machine learning
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
topics: empirical risk minimization, id173
    training set: dtrain = {(x(t), y(t))}
   
    f (x;    )
    f (x;    )
    empirical risk minimization
    f (x;    )
    dvalid dtest
    dvalid dtest
    framework to design learning algorithms
    dvalid dtest
   
   

t xt

review

arg min

1

   

   

   

1

arg min

arg min
1
arg min

t xt
t xt
t xt

1
l(f (x(t);    ), y(t)) +     (   )

    l(f (x(t);    ), y(t))
l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )
       (   )
      =   1
    l(f (x(t);    ), y(t))
                                  is a id168
    l(f (x(t);    ), y(t))
               is a regularizer (penalizes certain values of     )
              +  
       (   )
       (   )
    learning is cast as optimization

   

t pt r   l(f (x(t);    ), y(t))    r      (   )

    ideally, we   d optimize classi   cation error, but it   s not smooth
    id168 is a surrogate for what we truly should optimize (e.g. upper bound)

3

hugo.larochelle@usherbrooke.ca

   

   

1

t xt

l(f (x(t);    ), y(t)) +     (   )

feedforward neural network

math for my slides    feedforward neural network   .

september 13, 2012
review
hugo larochelle
hugo larochelle
abstract

d  epartement d   informatique
universit  e de sherbrooke
d  epartement d   informatique
universit  e de sherbrooke

topics: stochastic id119 (sgd)
l(f (x(t);    ), y(t)) +     (   )
    algorithm that performs updates after each example

    9w,t    x(t   ) =pt6=t    wtx(t)
    r(x) ={x2rh|9w x =pj wjx  ,j}
    {x2rh| x /2r(x)}
    { i,ui| xui =  iui et u>i uj = 1i=j}

t xt
t xt
arg min
   
    l(f (x(t);    ), y(t))
machine learning
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
       (   )
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
   
              +  
t xt
      =   1
t pt r   l(f (x(t);    ), y(t))    r      (   )
arg min
      =   1
    {x 2 rd | rxf (x) = 0}
    f (x)
              +  
   
    v>r2
xf (x)v > 0 8v
    initialize           (                                                                    )
    {x 2 rd | rxf (x) = 0}
            {w(1), b(1), . . . , w(l+1), b(l+1)}
              +  
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    for n iterations
    v>r2
xf (x)v > 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
       (   )
    v>r2
for each training example
xf (x)v < 0 8v
    (x(t), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
training epoch 
math for my slides    feedforward neural network   .
    r   l(f (x(t);    ), y(t))
      =   1
    f (x)
     
=
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .
iteration over all examples
     
              +      
       (   )
5
    l(f (x(t);    ), y(t))
    f (x)
    f (x)
    f (x)
    {x 2 rd | rxf (x) = 0}
5
    r      (   )
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    the id168
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
    f (x)c = p(y = c|x)
    r   l(f (x(t);    ), y(t))
       (   )
    a procedure to compute the parameter gradients
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    the regularizer             (and the gradient                 )
    x(t) y(t)
    r      (   )
       (   )
5
       (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    initialization method
    f (x)c = p(y = c|x)
    r      (   )
    r      (   )
    (x(t), y(t))
    r      (   )

    to apply this algorithm to neural network training, we need

hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca

september 13, 2012
september 13, 2012

math for my slides    feedforward neural network   .

abstract
abstract

4

-

    l(f (x(t);    ), y(t))
            {w(1), b(1), . . . , w(l+1), b(l+1)}
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
id168
    l(f (x(t);    ), y(t))
       (   )
       (   )
       (   )
    r   l(f (x(t);    ), y(t))
    r      (   )
topics: id168 for classi   cation
    r      (   )
    r      (   )
    f (x)c = p(y = c|x)
    neural network estimates
       (   )
    f (x)c = p(y = c|x)
    f (x)c = p(y = c|x)
    we could maximize the probabilities of         given         in the training set
    x(t) y(t)
    x(t) y(t)
    r      (   )
    x(t) y(t)
    f (x)c = p(y = c|x)
    to frame as minimization, we minimize the 
negative log-likelihood
   
    x(t) y(t)

   
natural log (ln)

   

    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =

    we take the log to simplify for numerical stability and math simplicity
   
    sometimes referred to as cross-id178 

    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =

    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =

@

f (x)c   log f (x)y =  1(y=c)

rf (x)   log f (x)y =
rf (x)   log f (x)y =

5

    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y

feedforward neural network
hugo larochelle

@

@

exp(ac )

exp(ac )

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

d  epartement d   informatique
universit  e de sherbrooke

    h(x) = g(a(x)) = g(b +pi wixi)

feedforward neural network

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
    x(t) y(t)
    w
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at output
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    partial derivative:
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
f (x)c   log f (x)y =  1(y=c)
@f (x)c   log f (x)y =  1(y=c)
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    gradient:
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
rf (x)   log f (x)y
rf (x)   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
375
f (x)y264
    h(k)(x) = g(a(k)(x))
f (x)y264
375
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
1(y=0)
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
1(y=0)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
 1
exp(ac )
...
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
 1
    h(k)(x) = g(a(k)(x))
...
    f (x)
    a(x) = b +pi wixi = b + w>x
=
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
1(y=c 1)
    h(x) = g(a(x)) = g(b +pi wixi)
1(y=c 1)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
=  e(y)
    h(k)(x) = g(a(k)(x))
=  e(y)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
f (x)y
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
f (x)y
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

    p(y = c|x)
f (x)y
f (x)y

exp(ac )

exp(ac )

exp(ac )

exp(ac )

=

...

...

1

6

feedforward neural network
hugo larochelle

@

=

=

=

 1

a(l+1)(x)c

    p(y = c|x)

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)
exp(a(l+1)(x)y)   
f (x)y0@
    h(x) = g(a(x)) = g(b +pi wixi)
a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
exp(a(l+1)(x)y)   
    r      (   )
1a
feedforward neural network
pc0 exp(a(l+1)(x)c0)  
 pc0 exp(a(l+1)(x)c0) 2
    f (x)c = p(y = c|x)
pc0 exp(a(l+1)(x)c0)  
    x1 xd b w1 wd
 pc0 exp(a(l+1)(x)c0) 2
id168
f (x)y  1(y=c) exp(a(l+1)(x)y)
    x(t) y(t)
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
    w
 1
exp(a(l+1)(x)y)
exp(a(l+1)(x)c)
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at output 
    p(y = c|x)
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)
    {
             (before activation)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
pc exp(ac) . . .
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
pc exp(ac) . . .
   
    g(a) = a
    partial derivative:
    p(y = c|x)
    f (x)
    f (x)
@
    g(a) = sigm(a) =
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
@a(l+1)(x)c   log f (x)y
1+exp( a)
    p(y = c|x)
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
=   1(y=c)   f (x)c 
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
 1
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
a(l+1)(x)c
f (x)y
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
 1
@
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
softmax(a(l+1)(x))y
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
=
    a(x) = b +pi wixi = b + w>x
    gradient:
1
...
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
a(l+1)(x)c
f (x)y
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
exp(a(l+1)(x)y)
    h(k)(x) = g(a(k)(x))
 1
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc0 exp(a(l+1)(x)c0)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
a(l+1)(x)c
f (x)y
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
=   (e(y)   f (x))
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
exp(a(l+1)(x)y)
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

    p(y = c|x)

a(l+1)(x)c

exp(ac )

exp(ac )

exp(ac )

exp(ac )

=

=

exp(ac )

=

exp(ac )

...

@

@

@

1

7

exp(a(l+1)(x)y)   

a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   

 1

f (x)y0@

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

 g(x)h(x)

 x

  g(x)
h(x)
 x

=  g(x)
 x
=  g(x)
 x

h(x) + g(x)  h(x)
 x

1
h(x)  

g(x)
h(x)2

 h(x)

 x

hugo%larochelle%

33%

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

ift615%

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

 g(x)h(x)

 x

  g(x)
h(x)
 x

=  g(x)
 x
=  g(x)
 x

h(x) + g(x)  h(x)
 x

1
h(x)  

g(x)
h(x)2

 h(x)

 x

hugo%larochelle%

33%

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

ift615%

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

 g(x)h(x)

 x

  g(x)
h(x)
 x

=  g(x)
 x
=  g(x)
 x

h(x) + g(x)  h(x)
 x

1
h(x)  

g(x)
h(x)2

 h(x)

 x

hugo%larochelle%

33%

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

ift615%

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

 g(x)h(x)

 x

  g(x)
h(x)
 x

=  g(x)
 x
=  g(x)
 x

h(x) + g(x)  h(x)
 x

1
h(x)  

g(x)
h(x)2

 h(x)

 x

hugo%larochelle%

33%

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

ift615%

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

 g(x)h(x)

 x

  g(x)
h(x)
 x

=  g(x)
 x
=  g(x)
 x

h(x) + g(x)  h(x)
 x

1
h(x)  

g(x)
h(x)2

 h(x)

 x

hugo%larochelle%

33%

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

ift615%

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

 g(x)h(x)

 x

  g(x)
h(x)
 x

=  g(x)
 x
=  g(x)
 x

h(x) + g(x)  h(x)
 x

1
h(x)  

g(x)
h(x)2

 h(x)

 x

hugo%larochelle%

33%

@

@

@a(l+1)(x)c

@a(l+1)(x)c   log f (x)y
 1
f (x)y
f (x)y
 1
f (x)y
 1
f (x)y

@a(l+1)(x)c

@a(l+1)(x)c

@

@

=

=

=

softmax(a(l+1)(x))y

ift615%

exp(a(l+1)(x)y)

pc0 exp(a(l+1)(x)c0)

@

@

=

=

 

 1

 1

@a(l+1)(x)c

exp(a(l+1)(x)y)

exp(a(l+1)(x)y)   

@a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
f (x)y0@
pc0 exp(a(l+1)(x)c0)
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)  
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
=   1(y=c)   f (x)c 

pc0 exp(a(l+1)(x)c0)

exp(a(l+1)(x)y)

exp(a(l+1)(x)c)

 1
 1

=

=

1a

ra(l+1)(x)   log f (x)y

8

feedforward neural network
hugo larochelle

exp(ac )

exp(ac )

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

d  epartement d   informatique
universit  e de sherbrooke

    h(x) = g(a(x)) = g(b +pi wixi)

feedforward neural network

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
    x(t) y(t)
    w
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layer
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    ... this is getting complicated!!
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

...

...

1

9

=

 1
=

a(l+1)(x)c

exp(a(l+1)(x)y)

=   1(y=c)   f (x)c 
f (x)y0@
=   1(y=c)   f (x)c 
a(l+1)(x)cpc0 exp(a(l+1)(x)c0)   
exp(a(l+1)(x)y)   
exp(a(l+1)(x)y)   
f (x)y0@
f (x)y0@
    h(x) = g(a(x)) = g(b +pi wixi)
pc0 exp(a(l+1)(x)c0)  
 pc0 exp(a(l+1)(x)c0) 2
    r      (   )
 1
feedforward neural network
pc0 exp(a(l+1)(x)c0)  
=
 pc0 exp(a(l+1)(x)c0) 2
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)!
f (x)y  1(y=c) exp(a(l+1)(x)y)
    f (x)c = p(y = c|x)
feedforward neural network
    x1 xd b w1 wd
exp(a(l+1)(x)c)
exp(a(l+1)(x)y)
 1
id168
f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
f (x)y  1(y=c) exp(a(l+1)(x)y)
ra(l+1)(x)c   log f (x)y
exp(a(l+1)(x)y)
exp(a(l+1)(x)c)
hugo larochelle
 1
    x(t) y(t)
ra(l+1)(x)c   log f (x)y
 1
    w
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)
=
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
pc0 exp(a(l+1)(x)c0)  
pc0 exp(a(l+1)(x)c0)
=
=   (e(y)   f (x))
 1
d  epartement d   informatique
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: chain rule
    p(y = c|x)
=   (e(y)   f (x))
=
    p(y = c|x)
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
    {
universit  e de sherbrooke
 1
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
 1
    if a function        can be written as 
=
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    p(a) qi(a)
=
pc exp(ac) . . .
pc exp(ac) . . .
   
=
    g(a) = a
a function of intermediate results
    p(a) qi(a)
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    p(y = c|x)
=   1(y=c)   f (x)c 
then we have:
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
   
=
    f (x)
 1
    p(y = c|x)
    f (x)
=xi
    g(a) = sigm(a) =
=
@p(a)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=   1(y=c)   f (x)c 
1+exp( a)
    p(y = c|x)
   
=xi
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
@p(a)
@qi(a)
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
=   1(y=c)   f (x)c 
@a
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
@a
@qi(a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
   
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    f (x)
ra(l+1)(x)c   log f (x)y
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
math for my slides    feedforward neural network   .
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=   (e(y)   f (x))
ra(l+1)(x)c   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
   
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    we can invoke it by setting
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
ra(l+1)(x)c   log f (x)y
    a(x) = b +pi wixi = b + w>x
=   (e(y)   f (x))
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
        to a unit in layer 
    f (x)
=   (e(y)   f (x))
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
             to input of units in the layer above
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
=xi
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
             is the id168
@qi(a)
@p(a)
@p(a)
    w (1)
b(1)
    p(a) qi(a) a
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i,j
i
@a
@qi(a)
@a
@qi(a)
@p(a)
@p(a)
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
   

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

september 6, 2012

@p(a)

exp(ac )

exp(ac )

exp(ac )

@a

exp(ac )

exp(ac )

exp(ac )

...

...

10

1

=xi

    p(a) qi(a) a

    p(a) qi(a) a

feedforward neural network
hugo larochelle

@

=

=

@a

exp(ac )

exp(ac )

@p(a)

@qi(a)

    p(y = c|x)

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)

f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
=xi
 1
@p(a)
    x(t) y(t)
    w
@qi(a)
@a
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    p(y = c|x)
=
    p(y = c|x)
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    partial derivative:
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    f (x)
@h(k)(x)j   log f (x)y
    f (x)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc exp(ac) . . .
@a(k+1)(x)i
@   log f (x)y
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
@h(k)(x)j
@a(k+1)(x)i
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
@   log f (x)y
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
w (k+1)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@a(k+1)(x)i
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

reminder
rh(k)(x)   log f (x)y
    a(k)(x)i = b(k)

hugo.larochelle@usherbrooke.ca

= w(k+1)> ra(k+1)(x)   log f (x)y 

= xi
= xi

    p(a) qi(a) a k
   

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

i +pj w (k)

september 6, 2012

i,j h(k 1)(x)j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

...

...

i,j

11

1

feedforward neural network
hugo larochelle

i,j

exp(ac )

exp(ac )

w (k+1)

    p(y = c|x)

@a(k+1)(x)i
@h(k)(x)j

= xi
= xi

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)

f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
=
@   log f (x)y
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
@a(k+1)(x)i
=
    x(t) y(t)
    w
@   log f (x)y
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    p(y = c|x)
=
    p(y = c|x)
@a(k+1)(x)i
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    gradient:
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
rh(k)(x)   log f (x)y
    f (x)
    f (x)
= w(k+1)> ra(k+1)(x)   log f (x)y 
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    p(a) qi(a) a k
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
   
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
2
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

reminder
reminder
    a(k)(x)i = b(k)

math for my slides    feedforward neural network   .

i +pj w (k)

september 6, 2012

i,j h(k 1)(x)j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

...

...

12

1

feedforward neural network
hugo larochelle

@

exp(ac )

exp(ac )

    p(y = c|x)

    p(y = c|x)

    p(y = c|x)

    h(x) = g(a(x)) = g(b +pi wixi)

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
    x(t) y(t)
    w
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
    {
              (before activation)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    partial derivative:
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
@a(k)(x)j   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
@h(k)(x)j
...
...
@   log f (x)y
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
@h(k)(x)j
@a(k)(x)j
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
g0(a(k)(x)j)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
@h(k)(x)j
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=  rh(k)(x)   log f (x)y > ra(k)(x)h(k)(x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
@
    h(k)(x) = g(a(k)(x))
=  rh(k)(x)   log f (x)y    [. . . , g0(a(k)(x)j), . . . ]
a(k)(x)j   log f (x)y

reminder
    h(k)(x)j = g(a(k)(x)j)

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

ra(k)(x)   log f (x)y

september 6, 2012

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

=

=

...

...

13

1

feedforward neural network
hugo larochelle

@

=

=

exp(ac )

exp(ac )

g0(a(k)(x)j)

ra(k)(x)   log f (x)y

d  epartement d   informatique
universit  e de sherbrooke

    h(x) = g(a(x)) = g(b +pi wixi)

a(k)(x)j   log f (x)y
@   log f (x)y
    p(y = c|x)
@h(k)(x)j
    p(y = c|x)
@   log f (x)y
@h(k)(x)j

feedforward neural network

    r      (   )
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
    x(t) y(t)
    w
@h(k)(x)j
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient at hidden layers
@a(k)(x)j
    {
              (before activation)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    gradient:
    p(y = c|x)
    f (x)
    f (x)
    g(a) = sigm(a) =
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=  rh(k)(x)   log f (x)y > ra(k)(x)h(k)(x)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
=  rh(k)(x)   log f (x)y    [. . . , g0(a(k)(x)j), . . . ]
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
element-wise
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
product
@
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
  log f (x)y
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
w (k)
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
i,j
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@   log f (x)y
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
=
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
@a(k)(x)i
reminder
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x)j = g(a(k)(x)j)
@   log f (x)y
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
b(1)
    w (1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@a(k)(x)i
i
i,j
    h(k)(x) = g(a(k)(x))
@
    h(k)(x) = g(a(k)(x))
a(k)(x)j   log f (x)y

hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

september 6, 2012

@a(k)(x)i
@w (k)
i,j
h(k 1)
j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

(x)

...

...

=

14

1

feedforward neural network
hugo larochelle

@

=

=

exp(ac )

=
    p(y = c|x)

ra(k)(x)   log f (x)y

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)

f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
=  rh(k)(x)   log f (x)y > ra(k)(x)h(k)(x)
    x(t) y(t)
    w
=  rh(k)(x)   log f (x)y    [. . . , g0(a(k)(x)j), . . . ]
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient of parameters
    p(y = c|x)
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    partial derivative (weights):
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    f (x)
    f (x)
  log f (x)y
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@w (k)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
i,j
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
@a(k)(x)i
@   log f (x)y
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
=
    p(y = c|x)
    f (x)
@a(k)(x)i
@w (k)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
i,j
    p(a) qi(a) a k
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
h(k 1)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
=
(x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
   
    a(x) = b +pi wixi = b + w>x
j
1
...
@a(k)(x)i
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

=  ra(k)(x)   log f (x)y  h(k 1)(x)>

hugo.larochelle@usherbrooke.ca

reminder
rw(k)   log f (x)y
    a(k)(x)i = b(k)

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

i +pj w (k)

september 6, 2012

    p(y = c|x)

i,j h(k 1)(x)j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

...

...

15

1

feedforward neural network
hugo larochelle

=

=

(x)

exp(ac )

exp(ac )

    p(y = c|x)

@a(k)(x)i
@w (k)
i,j
h(k 1)
j

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)

f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
w (k)
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
=
i,j
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
@   log f (x)y
id168
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
=
@a(k)(x)i
    x(t) y(t)
    w
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
@   log f (x)y
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient of parameters
    p(y = c|x)
=
    p(y = c|x)
    {
@a(k)(x)i
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    gradient (weights):
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
rw(k)   log f (x)y
    f (x)
    f (x)
=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
@
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
  log f (x)y
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
b(k)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
i
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
@a(k)(x)i
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
@   log f (x)y
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
@a(k)(x)i
@b(k)
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
i +pj w (k)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@   log f (x)y
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
@a(k)(x)i
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

hugo.larochelle@usherbrooke.ca

    p(a) qi(a) a k
   

math for my slides    feedforward neural network   .

reminder
    a(k)(x)i = b(k)

math for my slides    feedforward neural network   .

september 6, 2012

i,j h(k 1)(x)j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

=

=

...

...

16

1

i

feedforward neural network
hugo larochelle

=

exp(ac )

exp(ac )

    p(y = c|x)

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)

f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
@a(k)(x)i
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
=
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
rw(k)   log f (x)y
    x(t) y(t)
    w
=  ra(k)(x)   log f (x)y  h(k 1)(x)>
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient of parameters
    p(y = c|x)
=
    p(y = c|x)
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    partial derivative (biases):
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
@
    f (x)
    f (x)
  log f (x)y
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@b(k)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@a(k)(x)i
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
@   log f (x)y
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
@a(k)(x)i
@b(k)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
i
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
@   log f (x)y
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
@a(k)(x)i
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
rb(k)   log f (x)y
i +pj w (k)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

hugo.larochelle@usherbrooke.ca

    p(a) qi(a) a k
   

math for my slides    feedforward neural network   .

= ra(k)(x)   log f (x)y

reminder
    a(k)(x)i = b(k)

math for my slides    feedforward neural network   .

september 6, 2012

i,j h(k 1)(x)j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

=

=

...

...

17

1

i

feedforward neural network
hugo larochelle

i

=

=

=

exp(ac )

exp(ac )

@b(k)

@a(k)(x)i

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

    h(x) = g(a(x)) = g(b +pi wixi)

f (x)y  1(y=c) exp(a(l+1)(x)y)
pc0 exp(a(l+1)(x)c0)!
    r      (   )
 1
pc0 exp(a(l+1)(x)c0)
pc0 exp(a(l+1)(x)c0)  
@   log f (x)y
    f (x)c = p(y = c|x)
    x1 xd b w1 wd
id168
f (x)y   1(y=c)softmax(a(l+1)(x))y   softmax(a(l+1)(x))y softmax(a(l+1)(x))c   
 1
@a(k)(x)i
    x(t) y(t)
    w
@   log f (x)y
f (x)y 1(y=c)f (x)y   f (x)y f (x)c 
 1
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
topics: loss gradient of parameters
    p(y = c|x)
=
=
    p(y = c|x)
    {
@a(k)(x)i
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
=   1(y=c)   f (x)c 
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    gradient (biases):
pc exp(ac) . . .
pc exp(ac) . . .
   
    g(a) = a
    p(y = c|x)
    f (x)
rb(k)   log f (x)y
    f (x)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
= ra(k)(x)   log f (x)y
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
ra(l+1)(x)   log f (x)y
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
=   (e(y)   f (x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
=xi
    a(x) = b +pi wixi = b + w>x
@qi(a)
@p(a)
@p(a)
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
@qi(a)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
@

hugo.larochelle@usherbrooke.ca

    p(a) qi(a) a k
   

math for my slides    feedforward neural network   .

reminder
    a(k)(x)i = b(k)

math for my slides    feedforward neural network   .

i +pj w (k)

september 6, 2012

i,j h(k 1)(x)j

    p(y = c|x)

exp(ac )

exp(ac )

exp(ac )

exp(ac )

@a

@a

...

...

18

1

   

id26

=

@a(k)(x)i
@a(k)(x)i
@b(k)
@b(k)
@b(k)
i
i
i

@   log f (x)y
@   log f (x)y
@a(k)(x)i
@b(k)
@   log f (x)y
=
=
i
=
@a(k)(x)i
@a(k)(x)i
@   log f (x)y
@a(k)(x)i
@
  log f (x)y
@   log f (x)y
@a(k)(x)i
@   log f (x)y
b(k)
@   log f (x)y
=
=
i
=
@a(k)(x)i
@a(k)(x)i
@a(k)(x)i
@   log f (x)y
@a(k)(x)i
rb(k)   log f (x)y
@a(k)(x)i
@b(k)
= ra(k)(x)   log f (x)y
rb(k)   log f (x)y
@   log f (x)y
rb(k)   log f (x)y
rb(k)   log f (x)y
@a(k)(x)i
= ra(k)(x)   log f (x)y
= ra(k)(x)   log f (x)y
= ra(k)(x)   log f (x)y

i

=

topics: id26 algorithm
    compute output gradient (before activation)
=
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    for k from l+1 to 1
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
- compute gradients of hidden layer parameter 
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
- compute gradient of hidden layer below
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    [. . . , g0(a(k 1)(x)j), . . . ]
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
- compute gradient of hidden layer below (before activation)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    [. . . , g0(a(k 1)(x)j), . . . ]

    this assumes a forward propagation has been made before

= ra(k)(x)   log f (x)y

rb(k)   log f (x)y

3

3

3
3
3

19

topics: linear activation function gradient

    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))
    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k 1)(x)>
id180
    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
    a(x) = b +pi wixi = b + w>x
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    h(x) = g(a(x)) = g(b +pi wixi)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k 1)(x)
    partial derivative:
    g0(a) = 1

math for my slides    feedforward neural network   .

    x1 xd b w1 wd
    w
    {
    g(a) = a
    sigm(a) =

1

20

1+exp( a)

    ra(l+1)(x)   log f (x)y (=   (e(y)   f (x))

abstract

math for my slides    feedforward neural network   .

    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y
topics: sigmoid activation function gradient

id180

    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    a(x) = b +pi wixi = b + w>x
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    h(x) = g(a(x)) = g(b +pi wixi)
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    partial derivative:
    g0(a) = a
    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2

    x1 xd b w1 wd
    w
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)

3

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

21

math for my slides    feedforward neural network   .

topics: tanh activation function gradient

    rb(k)   log f (x)y (= ra(k)(x)   log f (x)y

id180

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    rw(k)   log f (x)y (=  ra(k)(x)   log f (x)y  h(k)(x)>
    rh(k 1)(x)   log f (x)y (= w(k)> ra(k)(x)   log f (x)y 
    ra(k 1)(x)   log f (x)y (=  rh(k 1)(x)   log f (x)y    ra(k 1)(x)h(k)(x)
    x1 xd b w1 wd
    g0(a) = a
    partial derivative:
    w
    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

3

22

1

id173
    g0(a) = g(a)(1   g(a))
    g0(a) = g(a)(1   g(a))
topics: l2 id173
    g0(a) = 1   g(a)2
    g0(a) = 1   g(a)2
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
       (   ) =pkpipj   w (k)
i,j    2
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj |w (k)
i,j |
       (   ) =pkpipj |w (k)
i,j |
    rw(k)   (   ) = sign(w(k))
    rw(k)   (   ) = sign(w(k))
    sign(w(k))i,j = 1 0
    sign(w(k))i,j = 1 0

    only applied on weights, not on biases (weight decay)
    can be interpreted as having a gaussian prior over the 
weights

    rw(k)   (   ) = 2w(k)

    gradient:

f

=pk ||w(k)||2

f

23

=pk ||w(k)||2

f

    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2
       (   ) =pkpipj   w (k)
i,j    2
id173
    g0(a) = 1   g(a)2
       (   ) =pkpipj   w (k)
i,j    2
    g0(a) = g(a)(1   g(a))
=pk ||w(k)||2
topics: l1 id173
    g0(a) = 1   g(a)2
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
       (   ) =pkpipj |w (k)
    rw(k)   (   ) = 2w(k)
i,j |
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj |w (k)
i,j |
       (   ) =pkpipj |w (k)
    rw(k)   (   ) = sign(w(k))
i,j |
    rw(k)   (   ) = sign(w(k))
    rw(k)   (   ) = sign(w(k))
    sign(w(k))i,j = 1 0
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    sign(w(k))i,j = 1 0

    also only applied on weights
    unlike l2, l1 will push certain weights to be exactly 0
    can be interpreted as having a laplacian prior over the 
weights

    gradient:

    where 

i,j <0

f

24

review

machine learning

    l(f (x(t);    ), y(t))
       (   )
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
topics: bias-variance trade-off
      =   1
       (   )
       (   )
       (   )
    l(f (x(t);    ), y(t))
    variance of trained model: does it vary a lot if the training set 
    l(f (x(t);    ), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
      =   1
      =   1
      =   1
changes 
       (   )
       (   )
    {x 2 rd | rxf (x) = 0}
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
              +  
              +  
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
    bias of trained model: is the average model close to the true 
      =   1
    v>r2
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
              +  
solution
              +  
    v>r2
xf (x)v < 0 8v
    v>r2
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v > 0 8v
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    generalization error can be seen as the sum of bias and the 
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
    v>r2
    v>r2
xf (x)v < 0 8v
xf (x)v < 0 8v
xf (x)v < 0 8v
    v>r2
xf (x)v > 0 8v
variance
    v>r2
xf (x)v > 0 8v
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
xf (x)v < 0 8v
    v>r2
xf (x)v < 0 8v
possible
    f    f
    (x(t), y(t))
    (x(t), y(t))
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    f    f
    f    f
    f    f
    (x(t), y(t))
    (x(t), y(t))
possible
    f    f
possible
    f    f

t pt r   l(f (x(t);    ), y(t))    r      (   )

t pt r   l(f (x(t);    ), y(t))    r      (   )

low variance/

high bias

good trade-off

high variance/

low bias

25

    initialize all to 0
    for weights

topics: initialization
    for biases

    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2

initialization
    g0(a) = g(a)(1   g(a))
    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2
    g0(a) = 1   g(a)2
    g0(a) = g(a)(1   g(a))
i,j    2
       (   ) =pkpipj   w (k)
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
=pk ||w(k)||2
    g0(a) = 1   g(a)2
i,j    2
       (   ) =pkpipj   w (k)
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
=pk ||w(k)||2
    rw(k)   (   ) = 2w(k)
    rw(k)   (   ) = 2w(k)
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj |w (k)
    can   t initialize weights to 0 with tanh activation
       (   ) =pkpipj |w (k)
       (   ) =pkpipj |w (k)
i,j |
i,j |
i,j |
       (   ) =pkpipj |w (k)
i,j |
    rw(k)   (   ) = sign(w(k))
    rw(k)   (   ) = sign(w(k))
    can   t initialize all weights to the same value
    rw(k)   (   ) = sign(w(k))
    rw(k)   (   ) = sign(w(k))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
- we can show that all hidden units in a layer will always behave the same
    sign(w(k))i,j = 1w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
i,j >0   1w(k)
p6phk+hk 1
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
i,j <0
    w(k)
i,j <0
i,j u [ b, b] b =
- need to break symmetry
p6phk+hk 1
p6phk+hk 1
p6phk+hk 1
    recipe: sample          from                   where
    w(k)
    w(k)
    w(k)
i,j u [ b, b] b =
i,j u [ b, b] b =
hk
i,j u [ b, b] b =
the idea is to sample around 0 but break symmetry  

- we can show that all gradients would then be 0 (saddle point)

    rw(k)   (   ) = 2w(k)

i,j <0
size of 

hk h(k)(x)

hk

i,j <0

hk

f

f

f

-
- other values of b could work well (not an exact science)

( see glorot & bengio, 2010)

26

f

optimization
topics: local optimum, global optimum, plateau
    notes on the optimization problem

    there isn   t a single global optimum (non-id76)

- we can permute the hidden units (with their connections) and get the same function
- we say that the hidden unit parameters are not identi   able

    optimization can get stuck in local minimum or plateaus

27

optimization
topics: local optimum, global optimum, plateau

neural network training demo

http://cs.stanford.edu/~karpathy/id166js/demo/demonn.html

28

topics:    ow graph
    forward propagation can be
representated as an acyclic 
   ow graph
    it   s a nice way of implementing
forward propagation in a modular
way
    each box is an object with an fprop method,

that outputs the value of the box given its
children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

29

    calling the fprop method of each box in the

right order yield forward propagation

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

children

p6phk+hk 1
    w(k)
i,j    2
       (   ) =pkpipj   w (k)
    w(k)
i,j u [ b, b] b =
p6phk+hk 1
i,j u [ b, b] b =
    rw(k)   (   ) = sign(w(k))
    r      (   )
    w(k)
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
       (   ) =pkpipj |w (k)
i,j u [ b, b] b =
    h(3)(x) = o(a(3)(x))
i,j |
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(1) = b(1) + w(1)x
    f (x)c = p(y = c|x)
    a(3) = b(3) + w(3)h(2)
p6phk+hk 1
flow graph
    w(k)
    rw(k)   (   ) = 2w(k)
    h(2)(x) = g(a(2)(x))
    a(3) = b(3) + w(3)h(2)
i,j u [ b, b] b =
    a(3) = b(3) + w(3)h(2)
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    h(3) = o(a(3)(x))
    a(2) = b(2) + w(2)h(1)
    x(t) y(t)
       (   ) =pkpipj |w (k)
    w(k)
i,j u [ b, b] b =
    a(2) = b(2) + w(2)h(1)
i,j |
    h(1)(x) = g(a(1)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
topics: automatic differentiation
    a(3)(x) = b(3) + w(3)h(2)
    a(2) = b(2) + w(2)h(1)
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y
    h(2) = g(a(2)(x))
    a(1) = b(1) + w(1)x
    a(1) = b(1) + w(1)x
    each object also has a bprop method
    rw(k)   (   ) = sign(w(k))
p6phk+hk 1
    a(3) = b(3) + w(3)h(2)
    b(3) b(2) b(1)
    a(2)(x) = b(2) + w(2)h(1)
    w(k)
    h(1) = g(a(1)(x))
    h(3) = o(a(3)(x))
i,j u [ b, b] b =
    a(1) = b(1) + w(1)x
   
    it outputs the gradient of the loss given each
    h(3) = o(a(3)(x))
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    a(2) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    a(3)(x) = b(3) + w(3)h(2)
    h(3) = o(a(3)(x))
    h(2) = g(a(2)(x))
p6phk+hk 1
    a(1) = b(1) + w(1)x
    w(3) w(2) w(1) x
    h(3)(x) = o(a(3)(x))
    w(k)
    h(1) = g(a(1)(x))
i,j u [ b, b] b =
    a(2)(x) = b(2) + w(2)h(1)
    h(1) = g(a(1)(x))
    h(2) = g(a(2)(x))
    h(3) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    b(3) b(2) b(1)
    h(2) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x
    h(1) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    a(1)(x) = b(1) + w(1)x
    h(2)(x) = g(a(2)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x
    w(3) w(2) w(1) x
    h(2)(x) = g(a(2)(x))
    b(3) b(2) b(1)

    by calling bprop in the reverse order,
we get id26
    only need to reach the parameters

    fprop depends on the fprop of a box   s children,

while bprop depends the bprop of a box   s parents

30

    a(3)(x) = b(3) + w(3)h(2)
    a(3)(x) = b(3) + w(3)h(2)
    a(2)(x) = b(2) + w(2)h(1)
    a(3)(x) = b(3) + w(3)h(2)
    a(1)(x) = b(1) + w(1)x
    h(1)(x) = g(a(1)(x))
    a(3)(x) = b(3) + w(3)h(2)
    a(2)(x) = b(2) + w(2)h(1)
    a(2)(x) = b(2) + w(2)h(1)
gradient checking
    a(1)(x) = b(1) + w(1)x
    a(2)(x) = b(2) + w(2)h(1)
    h(3)(x) = o(a(3)(x))
    a(2)(x) = b(2) + w(2)h(1)
    a(1)(x) = b(1) + w(1)x
    a(1)(x) = b(1) + w(1)x
    a(1)(x) = b(1) + w(1)x
    h(3)(x) = o(a(3)(x))
    b(3) b(2) b(1)
    h(2)(x) = g(a(2)(x))
topics:    nite difference approximation
    h(3)(x) = o(a(3)(x))
    a(1)(x) = b(1) + w(1)x
    h(3)(x) = o(a(3)(x))
    h(3)(x) = o(a(3)(x))
    h(2)(x) = g(a(2)(x))
    to debug your implementation of fprop/bprop, you can 
    h(1)(x) = g(a(1)(x))
    h(2)(x) = g(a(2)(x))
    h(3)(x) = o(a(3)(x))
    w(3) w(2) w(1) x f (x)
    h(2)(x) = g(a(2)(x))
compare with a    nite-difference approximation of the gradient
    h(2)(x) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    h(1)(x) = g(a(1)(x))
    b(3) b(2) b(1)
    h(2)(x) = g(a(2)(x))
    h(1)(x) = g(a(1)(x))
    h(1)(x) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    w(3) w(2) w(1) x f (x)
@x     f (x+   ) f (x    )
    @f (x)
    h(1)(x) = g(a(1)(x))
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    w(3) w(2) w(1) x f (x)
    w(3) w(2) w(1) x f (x)
2   
@x     f (x+   ) f (x    )
    @f (x)
    b(3) b(2) b(1)
    w(3) w(2) w(1) x f (x)
    w(3) w(2) w(1) x f (x)
@x     f (x+   ) f (x    )
    @f (x)
@x     f (x+   ) f (x    )
    @f (x)
              would be the loss
    f (x) x    
    w(3) w(2) w(1) x f (x)
@x     f (x+   ) f (x    )
    @f (x)
@x     f (x+   ) f (x    )
    @f (x)
         would be a parameter
    f (x) x    
    f (x) x    
@x     f (x+   ) f (x    )
    @f (x)
    f (x) x    
                   would be the loss if you add    to the parameter
    f (x) x    
    f (x +    ) f (x      )
                   would be the loss if you subtract    to the parameter
    f (x) x    
    f (x +    ) f (x      )

2   

2   

2   

2   

2   

2   

31

knowing when to stop
topics: early stopping
    to select the number of epochs, stop training when validation 
set increases (with some look ahead)
validation

training

0.5

0.4

0.3

0.2

0.1

0

under   tting

over   tting

number of epochs

32

2   

2   

2   
2   

    h(1)(x) = g(a(1)(x))
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    b(3) b(2) b(1)
    w(3) w(2) w(1) x f (x)
    b(3) b(2) b(1)
    h(3)(x) = o(a(3)(x))
    b(3) b(2) b(1)
id119
@x     f (x+   ) f (x    )
    @f (x)
    w(3) w(2) w(1) x f (x)
    w(3) w(2) w(1) x f (x)
    h(2)(x) = g(a(2)(x))
@x     f (x+   ) f (x    )
    w(3) w(2) w(1) x f (x)
    @f (x)
    w(3) w(2) w(1) x f (x)
    h(1)(x) = g(a(1)(x))
@x     f (x+   ) f (x    )
    @f (x)
    f (x) x    
@x     f (x+   ) f (x    )
    @f (x)
@x     f (x+   ) f (x    )
    @f (x)
2   
    f (x) x    
topics: convergence conditions, decrease constant
2   
    b(3) b(2) b(1)
@x     f (x+   ) f (x    )
    @f (x)
    f (x) x    
    f (x +    ) f (x      )
    stochastic id119 will converge if
    f (x) x    
    f (x +    ) f (x      )
    f (x) x    
    w(3) w(2) w(1) x f (x)
    f (x +    ) f (x      )
    p1t=1    t = 1
    f (x) x    
    p1t=1    t = 1
   
@x     f (x+   ) f (x    )
    f (x +    ) f (x      )
    @f (x)
    p1t=1    t = 1
    f (x +    ) f (x      )
 
    p1t=1    2
    f (x +    ) f (x      )
    p1t=1    2
    f (x) x    
    p1t=1    2
   
t < 1
    p1t=1    t = 1
t < 1
    p1t=1    t = 1
t < 1    t
    f (x +    ) f (x      )
where        is the learning rate of the tth update
t < 1    t
       t =    
    p1t=1    2
    p1t=1    t = 1
    p1t=1    2
t < 1    t
t < 1    t
    decreasing strategies:      (    is the decrease constant)
       t =    
t  0.5 <       1  
    p1t=1    2
t < 1    t
     
       t =    
       t =    
1+ t
1+ t
       t =    
       
                           (o                      )
       t =    
      1  
       t =    
t  0.5 <       1  
t 
    better to use a    xed learning rate for the    rst few updates

    p1t=1    t = 1
    p1t=1    2

1+ t

1+ t

t 

 

2   

33

    w(3) w(2) w(1) x f (x)
@x     f (x+   ) f (x    )
    @f (x)
id119
    f (x) x    
topics: mini-batch, momentum
    f (x +    ) f (x      )
    can update based on a mini-batch of example (instead of 1 example): 

2   

    the gradient is the average regularized loss for that mini-batch
    can give a more accurate estimate of the risk gradient
    can leverage matrix/matrix operations, which are more ef   cient

t < 1    t

    p1t=1    t = 1
    p1t=1    2
       t =    
       t =    
    r(t)

1+ t

    can use an exponential average of previous gradients:

t  0.5 <       1  

    = r   l(f (x(t)), y(t)) +  r(t 1)
    can get through plateaus more quickly, by       gaining momentum      

   

34

   

   

   

1+ t

1+ t

t < 1    t

)
t
(

t  0.5 <       1  

t  0.5 <       1  

 

r

 

    f (x) x    
    p1t=1    t = 1
t < 1    t
    f (x) x    
    f (x +    ) f (x      )
    p1t=1    2
id119
    f (x +    ) f (x      )
    p1t=1    t = 1
    = r   l(f (x(t)), y(t)) +  r(t 1)
    p1t=1    t = 1
       t =    
    p1t=1    2
    p1t=1    2
t < 1    t
topics: newton   s method
       t =    
t  0.5 <       1  
t < 1    t
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
       t =    
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
    if we locally approximate the loss through taylor expansion:
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
=
       t =    
       t =    
t  0.5 <       1  
1+ t
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  (         (t))
r
)
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
       t =    
1
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
t  0.5 <       1  
+
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
(r   l(f (x;    (t)), y))
   
t
   l(f (x;    (t)), y)    (         (t))
+0.5(         (t))>   r2
(
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
    {    (t)     l(f (x;    ), y)
true
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
approx
hessian
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
   
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2
(r   l(f (x;    (t)), y))
       (t+1) =    (t)   r2
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
+0.5(         (t))>   r2
    {    (t)     l(f (x;    (t)), y)
    we could minimize that approximation, by solving:
    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2

   l(f (x;    (t)), y)  (         (t))
   l(f (x;    (t)), y)  1
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
4
   l(f (x;    (t)), y)  (         (t))

   l(f (x;    (t)), y)  1

(r   l(f (x;    (t)), y))

    {    (t)     l(f (x;    (t)), y)

(r   l(f (x;    (t)), y))

   

   

   

{

(r   l(f (x;    (t)), y))

   

4

35

   

4

4

   

r

   

   

   

   

1+ t

1+ t

t < 1    t

t < 1    t

)
t
(

t  0.5 <       1  

)
1
+
t
(

 

    p1t=1    2
    f (x) x    
r
    f (x +    ) f (x      )
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
    p1t=1    t = 1
       t =    
    f (x +    ) f (x      )
id119
    p1t=1    2
   
 
    p1t=1    t = 1
t < 1    t
       t =    
t  0.5 <       1  
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
       t =    
   l(f (x;    (t)), y)    (         (t))
+0.5(         (t))>   r2
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
    p1t=1    2
topics: newton   s method
       t =    
t  0.5 <       1  
t < 1    t
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  (         (t))
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
    if we locally approximate the loss through taylor expansion:
=
       t =    
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
1+ t
   
       t =    
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
t  0.5 <       1  
    {    (t)     l(f (x;    ), y)
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
    0 = r   l(f (x;    (t)), y) + r2
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
hessian
   
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2
    {    (t)     l(f (x;    (t)), y)
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
    {    (t)     l(f (x;    (t)), y)
+0.5(         (t))>   r2
    we could minimize that approximation, by solving:
4
    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2

   l(f (x;    (t)), y)  (         (t))

+0.5(         (t))>   r2
   l(f (x;    (t)), y)  (         (t))

   l(f (x;    (t)), y)  1

l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))

(r   l(f (x;    (t)), y))
true
approx

(r   l(f (x;    (t)), y))

(r   l(f (x;    (t)), y))

(r   l(f (x;    (t)), y))

   

   

{

(r   l(f (x;    (t)), y))

   

35

   

4

4

   

r

   

    f (x) x    
    f (x +    ) f (x      )

r

t < 1    t

)
t
(

 

)
1
+
t
(

 

(r   l(f (x;    (t)), y))
true
approx

   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
id119
    p1t=1    t = 1
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
   
t  0.5 <       1  
    p1t=1    2
topics: newton   s method
t < 1    t
   
    = r   l(f (x(t)), y(t)) +  r(t 1)
    {    (t)     l(f (x;    ), y)
    if we locally approximate the loss through taylor expansion:
=
       t =    
1+ t
       t =    
l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
t  0.5 <       1  
    0 = r   l(f (x;    (t)), y) + r2
+0.5(         (t))>   r2
   l(f (x;    (t)), y)    (         (t))
    = r   l(f (x(t)), y(t)) +  r(t 1)
    r(t)
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2
hessian
   
       (t+1) =    (t)   r2
   l(f (x;    (t)), y)  1
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2
+0.5(         (t))>   r2
    {    (t)     l(f (x;    (t)), y)
    {    (t)     l(f (x;    (t)), y)
    we could minimize that approximation, by solving:
   l(f (x;    (t)), y)  (         (t))
    0 = r   l(f (x;    (t)), y) + r2
   l(f (x;    (t)), y)  1
       (t+1) =    (t)   r2

(r   l(f (x;    (t)), y))

(r   l(f (x;    (t)), y))

   

   

{

   

4

35

   

   l(f (x;    (t)), y)  (         (t))
   l(f (x;    (t)), y)  1

l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
4

   

l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))

   

l(f (x;    ), y)     l(f (x;    (t)), y) + r   l(f (x;    (t)), y)>(         (t))
   l(f (x;    (t)), y)    (         (t))

+0.5(         (t))>   r2

topics: newton   s method
    we can show that the minimum is: 

id119
   l(f (x;    (t)), y)  (         (t))

    0 = r   l(f (x;    (t)), y) + r2
       (t+1) =    (t)   r2

   l(f (x;    (t)), y)  1

(r   l(f (x;    (t)), y))

    {
    only practical if:
    few parameters (so we can invert hessian)
    locally convex (so the hessian is invertible)

4

    see recommended readings for more on optimization of 
neural networks

36

machine learning
review

t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
machine learning
machine learning
machine learning
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    supervised learning example: (x, y) x y
topics: training, validation and test sets, generalization
    training set: dtrain = {(x(t), y(t))}
    training set            serves to train a model
    f (x;    )
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    validation set           serves to select hyper-parameters
    dvalid dtest
    f (x;    )
    test set           serves to estimate the generalization 
    dvalid dtest
performance (error)

    generalization is the behavior of the model on unseen 
examples
    this is what we care about in machine learning!

37

model selection

topics: grid search
    to search for the best con   guration of the hyper-parameters:

    you can perform a grid search

-

-

specify a set of values you want to test for each hyper-parameter
try all possible con   gurations of these values

    you can perform a random search

-

-

specify a distribution over the values of each hyper-parameters (e.g. uniform in some range)
sample independently each hyper-parameter to get a con   guration, and repeat as many 
times as wanted

    use a validation set performance to select the best 
con   guration
    you can go back and re   ne the grid/distributions if needed

38

conclusion

    we have seen how to train a feed forward neural network:

    stochastic id119 on the empirical risk 

(i.e. the average regularized negative log-likelihood)

    computing gradients requires

forward pass to compute the output and regularized loss (forward propagation)

-
- backward pass to compute the parameter gradients (id26)

    finite-difference approximation can be used to debug
    hyper-parameter selection can be performed with a grid 
search
    we will see later how to do better than a random initialization 
(deep learning)

39

assignment 1

    regularized negative log-likelihood:

    it   s the negative log-likelihood + l2 weight decay + l1 weight decay
    the weight decays are weighted by their hyper-parameters

    the bprop function that computes the parameter gradients:

    it must compute the gradient of the regularized loss, not just the loss
    use run_verify_gradients.py to check your gradients:
    for the report:

    use latex with the nips paper format
    in the description of the approach section, give enough details to convince me you 

understand what a neural network is
- you should mention all neural network concepts that are involved in the assignment (what is 
the fprop, the bprop, the id168, how you optimize it, what are the hyper-parameters)

40

