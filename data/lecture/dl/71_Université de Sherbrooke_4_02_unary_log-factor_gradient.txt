neural networks
training crfs - unary log-factor gradient

   

2

arg min

abstract

math for my slides    training crfs   .

t xt

hugo larochelle

abstract
math for my slides    training crfs   .

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

machine learning

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
math for my slides    training crfs   .
d  epartement d   informatique
    l(f (x(t);    ), y(t))
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
universit  e de sherbrooke
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
hugo.larochelle@usherbrooke.ca
       (   )
topics: stochastic id119 (sgd)
    ap(yk, yk+1) = 11   k<k vyk,yk+1
   
t xt
1
l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
arg min
      =   1
    algorithm that performs updates after each example
september 13, 2012
   
    initialize     
t xt
              +  
t xt
1
    for n iterations

    9w,t    x(t   ) =pt6=t    wtx(t)
    r(x) ={x2rh|9w x =pj wjx  ,j}
    {x2rh| x /2r(x)}
    { i,ui| xui =  iui et u>i uj = 1i=j}

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    ap(yk, yk+1) = 11   k<k vyk,yk+1
   
1
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
   
   
t xt
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
arg min
arg min
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
       (   )
    l(f (x(t);    ), y(t))
for each training example
    (x(t), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
training epoch 
    l(f (x(t);    ), y(t))
      =   1
    f (x)
     
=
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
iteration over all examples
      =  r   l(f (x(t);    ), y(t))    r      (   )
     
              +      
    (x(t), y(t))
    f (x)
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    to apply this algorithm to a crf, we need
    (x(t), y(t))
    (x(t), y(t))
    {x 2 rd | rxf (x) = 0}
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    the id168
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
    r   l(f (x(t);    ), y(t))
       (   )
    a procedure to compute the parameter gradients
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    the regularizer             (and the gradient                 )
    r      (   )
       (   )
5
      =  r   l(f (x(t);    ), y(t))    r      (   )
    initialization method
    f (x)c = p(y = c|x)
    r      (   )
    (x(t), y(t))

t pt r   l(f (x(t);    ), y(t))    r      (   )

math for my slides    feedforward neural network   .

september 13, 2012

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

t xt

abstract

abstract

arg min

1

1

   

   

   

   

   

-

t xt

   

1

arg min

t xt

      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    ap(yk, yk+1) = 11   k<k vyk,yk+1
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))
id168
      =  r   l(f (x(t);    ), y(t))    r      (   )
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    l(f (x(t);    ), y(t))
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    (x(t), y(t))
    l(f (x(t);    ), y(t))
    (x(t), y(t))
   
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
topics: id168 for sequential classi   cation with crf
t xt
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
l(f (x(t);    ), y(t)) +     (   )
arg min
    crf estimates
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    p(y|x) l(f (x), y) =   log p(y|x)
    (x(t), y(t))
    we could maximize the probabilities of         given          in the training set
    p(y|x) y(t) x(t)
    p(y|x) y(t) x(t)
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    to frame as minimization, we minimize the 
negative log-likelihood
    r   l(f (x(t);    ), y(t))
    (x(t), y(t))
    l(f (x(t);    ), y(t))
    p(y|x) y(t) x(t)
    r   l(f (x(t);    ), y(t))
    unlike for non-sequential classi   cation, we never explicitly compute the value of
@   log p(y|x)
               for all values of 
    p(y|x) l(f (x), y) =   log p(y|x)
l(f (x), y) =   log p(y|x) y
@au(yk)
@   log p(y|x)

l(f (x), y) =   log p(y|x)
=
@au(yk)
@   log p(y|x)

l(f (x), y) =   log p(y|x)

l(f (x), y) =   log p(y|x)

l(f (x(t);    ), y(t)) +     (   )

@   log p(y|x)

=

=

1

   

@   log p(y|x)

=

@au(yk)

@au(yk)

@   log p(y|x)
=
@au(yk)

@au(yk)

3

=

      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )

parameter gradients

4

   

   

   

   

   

topics: loss gradient at unary log-factors
    partial derivative wrt au(yk   ):

   

l(f (x), y) =   log p(y|x) y
l(f (x), y) =   log p(y|x) y
@   log p(y|x)
@au(y0k)

@   log p(y|x)
@au(y0k)

d
e
t
(

x
=

{
 

=  (1yk=y0k   p(y0k|x))

=  (1yk=y0k   p(yk = y0k|x))

{
x
2

,

i

x
p
    gradient for each unary (log-)factors:
=

)

 

i

i

|

u

r
h

|

i

i

i

i

 

u

u

ra(l+1,0)(xk)   log p(y|x) =  (e(yk)   p(yk|x))
ra(l+1,0)(xk)   log p(y|x) =  (e(yk)   p(yk|x))

x
/2
ra(l+1, 1)(xk 1)   log p(y|x) =  1k>1 (e(yk)   p(yk|x))
ra(l+1, 1)(xk 1)   log p(y|x) =  1k>1 (e(yk)   p(yk|x))
r
ra(l+1,+1)(xk+1)   log p(y|x) =  1k<k (e(yk)   p(yk|x))
x
ra(l+1,+1)(xk+1)   log p(y|x) =  1k<k (e(yk)   p(yk|x))
)
}
vector of all
marginal probabilities

=
 

e
t

>i

u

u

(

i

i

i

x
u

q

@   log p(y|x)
@   log p(y|x)
@au(y0k)
@au(y0k)

=
=

@
@

@au(y0k)     kxk=1
@au(y0k)     kxk=1
=     1yk=y0k  

au(yk) +
au(yk) +

=

>i

u
j

ap(yk, yk+1)!   log z(x)!
k 1xk=1
ap(yk, yk+1)!   log z(x)!
k 1xk=1
log z(x)   

p

j

r
x

(

)

=
{
x
2

9
w

,
t
   

x

(
t
   
)

=

r
h

p

|

9
w

x
=

t

=
   

t

t

w
x

(
t
)

6
@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

@   log p(y|x)

@au(y0k)

@

=

@au(y0k)     kxk=1
=     1yk=y0k  

@

@au(y0k)

k 1xk=1
log z(x)   

au(yk) +

ap(yk, yk+1)!   log z(x)!

5

@

@au(y0k)

log z(x) =

1

@

z(x)

@au(y0k)

1
z(x)

      xy00k

@

@au(y0k)

exp  kxk=1
exp  kxk=1
exp  kxk=1

au(y00k ) +

au(y00k ) +

ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1
ap(y00k , y00k+1)!
k 1xk=1

1y0k=y00k

au(y00k ) +

1y0k=y00k

p(y001 , . . . , y00k|x)

=

1

z(x)

@

@au(y0k)xy001 xy002
      xy00k
      xy00k

1

1

=

=

z(x)xy001 xy002
z(x)xy001 xy002
      xy00k
= xy001 xy002
= p(y0k|x)

6

gradients, from every
position, into parameters

math for my slides    conditional random    elds   .

    l(f (x(t);    ), y(t))
    a(l+1)(x(t 1)) a(l+1)(x(t)) a(l+1)(x(t+1))
parameter gradients
      =  r   l(f (x(t);    ), y(t))    r      (   )
    a(l+1)(x(t 1)) a(l+1)(x(t)) a(l+1)(x(t+1))
    (x(t), y(t))
    x(t 1) x(t) x(t+1)
    a(l+1)(x(t 1)) a(l+1)(x(t)) a(l+1)(x(t+1))
    x(t 1) x(t) x(t+1)
    l(f (x(t);    ), y(t))
topics: loss gradient at unary log-factor parameters
    x(t 1) x(t) x(t+1)
    p(y(t 1)|x(t 1)) p(y(t)|x(t)) p(y(t+1)|x(t+1))
    v w(l+1) b(l+1)
    p(y(t 1)|x(t 1)) p(y(t)|x(t)) p(y(t+1)|x(t+1))
    v w(l+1) b(l+1)
    v w(l+1) b(l+1)
    v w(l+1) b(l+1)
    r   l(f (x(t);    ), y(t))
    use regular backprop
p(y|x) = exp  kxk=1
p(y|x) = exp  kxk=1
p(y|x) = exp  kxk=1
p(y|x) = exp  kxk=1
    p(y(t 1)|x(t 1)) p(y(t)|x(t)) p(y(t+1)|x(t+1))
k 1xk=1
    p(y(1), . . . , y(t )|x(1), . . . , x(t ))
k 1xk=1
k 1xk=1
    p(y(1), . . . , y(t )|x(1), . . . , x(t ))
    p(y|x) y(t) x(t)
a(l+1,0)(xk)yk +
l(f (x), y) =   log p(y|x) y
a(l+1,0)(xk)yk +
a(l+1,0)(xk)yk +
    backprop at all positions k
a(l+1,0)(xk)yk +
vyk,yk+1+
    p(y(1), . . . , y(t )|x(1), . . . , x(t ))
...
...
@   log p(y|x)
    y(t) = [y(t)
1 , . . . , y(t)
a(l+1,+1)(xk+1)yk! /z(x)
a(l+1,+1)(xk+1)yk! /z(x)
    accumulate all
a(l+1,+1)(xk+1)yk! /z(x)
    y(t) = [y(t)
1 , . . . , y(t)
]
kxk=2
]
k 1xk=1
k 1xk=1
kxk=2
kxk=2
k 1xk=1
kxk=2
kt
@au(y0k)
kt
a(l+1, 1)(xk 1)yk +
a(l+1, 1)(xk 1)yk +
    y(t) = [y(t)
1 , . . . , y(t)
a(l+1, 1)(xk 1)yk +
]
kt
1 , . . . , x(t)
    x(t) = [x(t)
    x(t) = [x(t)
1 , . . . , x(t)
]
]
kt
kt
1 , . . . , x(t)
    x(t) = [x(t)
    w(l+1, 1) w(l+1,+1) w(l+1,0) b(l+1)
    w(l+1, 1) w(l+1,+1) w(l+1,0) b(l+1)
ra(l+1,0)(xk)   log p(y|x) =  (e(yk)   p(yk|x))
    w(l+1, 1) w(l+1,+1) w(l+1,0) b(l+1)
kt
    w(l+1, 1) w(l+1,+1) w(l+1,0) b(l+1)
...
    kt
    kt
    kt
...
    x(t)
    x(t)
    x(t)
    a(l+1)(xk 1) a(l+1)(xk) a(l+1)(xk+1)
    a(l+1)(xk 1) a(l+1)(xk) a(l+1)(xk+1)
@au(y0k)     kxk=1
    a(l+1)(xk 1) a(l+1)(xk) a(l+1)(xk+1)
k 1xk=1
@   log p(y|x)
    xk 1 xk xk+1
    xk 1 xk xk+1
    xk 1 xk xk+1
log z(x)   
=     1yk=y0k  

ra(l+1, 1)(xk 1)   log p(y|x) =  1k>1 (e(yk)   p(yk|x))
ra(l+1,+1)(xk+1)   log p(y|x) =  1k<k (e(yk)   p(yk|x))

=  (1yk=y0k   p(yk = y0k|x))

a(l+1, 1)(xk 1)yk +

vyk,yk+1+
vyk,yk+1+

@au(y0k)

au(yk) +

.
.
.

...

=

@

k

k

k

]

ap(yk, yk+1)!   log z(x)!

parameter gradients

7

topics: loss gradient at unary log-factor parameters
    for linear log-factors:

    logpy0k

exp(  ) maxy0k

(  )
    the log-factors are directly connected to the input:
    maxy    p(y|x) (= maxy0k
    a(1,0)(xk) = b(1) + w(1,0)xk
    a(1, 1)(xk) = w(1, 1)xk
    a(1,+1)(xk) = w(1,+1)xk

au(y0k)) + log    k 1(y0k)

@   log p(y|x)
@ap(y0k, y0k+1)

=  (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))

parameter gradients

8

rv   log p(y|x) = e(yk)e(yk+1)>

topics: loss gradient at unary log-factor parameters
ra(l+1, 1)(xk 1)yk   log p(y|x) =  1k>1 (e(yk)   p(yk|x))
ra(l+1,+1)(xk+1)yk   log p(y|x) =  1k<k (e(yk)   p(yk|x))
    for linear log-factors:

    the gradients are:

rb(1)   log p(y|x) =

rw(1,0)   log p(y|x) =

rw(1, 1)   log p(y|x) =

rw(1,+1)   log p(y|x) =

kxk=1 ra(1,0)(xk)   log p(y|x)  =
kxk=1 ra(1,0)(xk)   log p(y|x)  x>k =
kxk=2 ra(1, 1)(xk)   log p(y|x)  x>k 1 =
k 1xk=1 ra(1,+1)(xk)   log p(y|x)  x>k+1 =

 (e(yk)   p(yk|x))

 (e(yk)   p(yk|x)) x>k

kxk=1
kxk=1
kxk=2
  (e(yk)   p(yk|x)) x>k 1
k 1xk=1

  (e(yk)   p(yk|x)) x>k+1

