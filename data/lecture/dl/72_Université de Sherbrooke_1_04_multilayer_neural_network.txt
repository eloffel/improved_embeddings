neural networks
feedforward neural network - multilayer neural network

1

)
2
x

artificial neuron

1

1
)
2
x

)
2
x

2

,

,

,

0

topics: capacity of single neuron
    can   t solve non linearly separable problems...

0

0

1

0

1

0

(x1

0

(x1

1

(x1
xor (x1, x2)

1

)
2
x

,

0

?

0

(x1

1

xor (x1, x2)

)
2
x

,
1
x
(
d
n
a

1

0

0

1

and (x1, x2)

    ... unless the input is transformed in a better representation
figure 1.8     exemple de mod  lisation de xor par un r  seau    une couche cach  e. en
haut, de gauche    droite, illustration des fonctions bool  ennes or(x1, x2), and (x1, x2)

i

i

1

i,j

i,j

b(2)

exp(2a)+1

exp(2a)+1

exp(2a)+1

exp(2a)+1

exp(2a)+1

    h(x) = g(a(x))

exp(a)+exp( a) = exp(2a) 1

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca
1+exp( a)
september 6, 2012
exp(a)+exp( a) = exp(2a) 1
abstract

exp(a)+exp( a) = exp(2a) 1
    w (1)
universit  e de sherbrooke
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = sigm(a) =
    g(a) = reclin(a) = max(0, a)
    g(a) = a
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
1
    g(a) = tanh(a) = exp(a) exp( a)
1+exp( a)
    g(a) = sigm(a) =
    g(a) = a
3
neural network
    h(x) = g(a(x))
    g(a) = max(0, a)
    g(a) = max(0, a)
    g(  ) b
september 6, 2012
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    g(a) = max(0, a)
    g(a) = sigm(a) =
1
    g(a) = max(0, a)
i pj w (1)
    g(a) = sigm(a) =
    g(a) = max(0, a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
1+exp( a)
1+exp( a)
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = reclin(a) = max(0, a)
    w (1)
xj h(x)i w(2)
b(1)
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
b(2)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
topics: single hidden layer neural network
i
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    p(y = 1|x)
    f (x) = g(out)(b(2) + w(2)>x)
    h(x) = g(a(x))
    g(a) = max(0, a)
    hidden layer pre-activation:
    g(  ) b
    g(a) = max(0, a)
xj h(x)i w(2)
    g(a) = max(0, a)
    p(y = 1|x)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
    g(  ) b
    g(  ) b
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    g(a) = reclin(a) = max(0, a)
    g(a) = max(0, a)
xj h(x)i w(2)
    w (1)
b(1)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
math for my slides    feedforward neural network   .
    g(a) = reclin(a) = max(0, a)
i +pj w (1)
xj h(x)i w(2)
b(1)
    w (1)
b(2)
    g(  ) b
b(1)
xj h(x)i w(2)
    w (1)
math for my slides    feedforward neural network   .
i
i,j
b(2)
    f (x) = o(b(2) + w(2)>x)
i
i
...
...
xj h(x)i w(2)
b(1)
    w (1)
i
b(2)
    hidden layer activation:
    a(x) = b +pi wixi = b + w>x
1
    w (1)
i
i
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    f (x) = o(b(2) + w(2)>x)
    a(x) = b +pi wixi = b + w>x
xj h(x)i
    h(x) = g(a(x))
    h(x) = g(a(x))
    g(  ) b
i,j
1
    h(x) = g(a(x))
    h(x) = g(a(x))
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
    h(x) = g(a(x))
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    h(x) = g(a(x)) = g(b +pi wixi)
i pj w (1)
    h(x) = g(a(x)) = g(b +pi wixi)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
b(1)
    w (1)
    g(  ) b
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
i +pj w (1)
i pj w (1)
    output layer activation:
    w (1)
b(1)
i pj w (1)
i
...
...
i
    f (x) = o   b(2) + w(2)>h(1)x   
1
1
    f (x) = o   b(2) + w(2)>x   
b(1)
    w (1)
xj h(x)i
    x1 xd
    x1 xd
    h(x) = g(a(x))
    o(x) = g(out)(b(2) + w(2)>x)
    h(x) = g(a(x))
    o(x) = g(out)(b(2) + w(2)>x)
i
i,j
    o(x) = g(out)(b(2) + w(2)>x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
output activation function
    w
    w
    h(x) = g(a(x))
1
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
    {

xj h(x)i

xj h(x)i

abstract

exp(2a)+1

b(2)

b(1)
i

i,j

i,j

i,j

1

i,j

i

1

i

i

i

i,j

i,j

b(2)

b(2)

b(1)
i

b(1)
i

xj h(x)i w(2)

neural network

xj h(x)i w(2)
topics: softmax activation function
    for multi-class classi   cation:

    g(a) = reclin(a) = max(0, a)
    g(  ) b
    w (1)
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    f (x) = o   b(2) + w(2)>x   
    o(a) = softmax(a) =h exp(a1)

    we need multiple outputs (1 output per class)
    we would like to estimate the id155 

    w (1)
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    f (x) = o   b(2) + w(2)>x   
i,j xj   
i +pj w (1)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>

pc exp(ac) . . .

    we use the softmax activation function at the output:

pc exp(ac) . . .

    p(y = c|x)

    p(y = c|x)

exp(ac )

1

4

i +pj w (1)

exp(ac )

pc exp(ac)i>

    strictly positive
    sums to one

    predicted class is the one with highest estimated id203

1

feedforward neural network
hugo larochelle

5

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

september 6, 2012

1

...

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

september 6, 2012

neural network

    x1 xd b w1 wd
    w
    p(y = c|x)
    p(y = c|x)
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
topics: multilayer neural network
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    g(a) = a
    f (x)
    could have l hidden layers:
    f (x)
pc exp(ac) . . .
    p(y = c|x)
    p(y = c|x)
    f (x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    layer pre-activation for k>0
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    h(k)(x) = g(a(k)(x))
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    hidden layer activation (k from 1 to l):
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
...
    h(k)(x) = g(a(k)(x))
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    output layer activation (k=l+1):
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

abstract

exp(2a)+1

exp(ac )

exp(ac )

exp(ac )

i,j xj   

math for my slides    feedforward neural network   .

abstract

