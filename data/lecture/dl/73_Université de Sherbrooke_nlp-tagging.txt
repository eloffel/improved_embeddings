natural language processing:
syntactic and semantic tagging
ift 725 - r  seaux neuronaux

word tagging

topics: word tagging
    in many nlp applications, it is useful to augment text data 
with syntactic and semantic information
    we would like to add syntactic/semantic labels to each word

    this problem can be tackled using a conditional random    eld 
with neural network unary potentials
    we will describe the model developed by ronan collobert and jason weston in:
 a uni   ed architecture for natural language processing: deep neural networks 
with multitask learning
collobert and weston, 2008

(see natural language processing (almost) from scratch for the journal version)

2

word tagging

pos (part-of-speech) 
topics: part-of-speech tagging
     tag each word with its part of speech category

r  les syntaxiques. 

!      tiqueter les mots d'une phrase en fonction de leurs 
    noun, verb, adverb, etc.
    might want to distinguish between singular/plural, present tense/past tense, etc.
!    entre 50 et 150 classes pour l   anglais. 
    see id32 pos tags set for an example
!    ex  : nom, adverbe 

    example:

the 

little 

yellow 

dog 

barked 

dt  

jj 

jj 

nn 

vbd 

at 

in 

the 

dt 

cat 

nn 

(from stanislas lauly)

3

word tagging

topics: chunking
    segment phrases into syntactic phrases

    noun phrase, verb phrase, etc.

chunking 

    segments are identi   ed with iobes encoding

!      tiqueter syntaxiquement des segments de phrase. 

    single word phrase (s- pre   x). ex.: s-np
    multiword phrase (b-, i-, e- pre   xes). ex.: b-vp i-vp i-vp e-vp  
    words outside of syntactic phrases: o

!    noun phrase (np), verb phrase (vp). 
!    encodage iobes:  

!    ex.: np-s pour un seul mot, np-b/np-i/np-e pour plusieurs. 

he 

reckons 

the 

current 

account 

deficit 

s-np 

s-vp 

b-np 

i-np 

i-np 

e-np 

(from stanislas lauly)

4

word tagging

topics: chunking
    segment phrases into syntactic phrases

    noun phrase, verb phrase, etc.

chunking 

    segments are identi   ed with iobes encoding

!      tiqueter syntaxiquement des segments de phrase. 

    single word phrase (s- pre   x). ex.: s-np
    multiword phrase (b-, i-, e- pre   xes). ex.: b-vp i-vp i-vp e-vp  
    words outside of syntactic phrases: o

!    noun phrase (np), verb phrase (vp). 
!    encodage iobes:  

!    ex.: np-s pour un seul mot, np-b/np-i/np-e pour plusieurs. 

np

he 

reckons 

the 

current 

account 

deficit 

s-np 

s-vp 

b-np 

i-np 

i-np 

e-np 

(from stanislas lauly)

4

word tagging

topics: chunking
    segment phrases into syntactic phrases

    noun phrase, verb phrase, etc.

chunking 

    segments are identi   ed with iobes encoding

!      tiqueter syntaxiquement des segments de phrase. 

    single word phrase (s- pre   x). ex.: s-np
    multiword phrase (b-, i-, e- pre   xes). ex.: b-vp i-vp i-vp e-vp  
    words outside of syntactic phrases: o

!    noun phrase (np), verb phrase (vp). 
!    encodage iobes:  

!    ex.: np-s pour un seul mot, np-b/np-i/np-e pour plusieurs. 

np

he 

vb

reckons 

the 

current 

account 

deficit 

s-np 

s-vp 

b-np 

i-np 

i-np 

e-np 

(from stanislas lauly)

4

word tagging

topics: chunking
    segment phrases into syntactic phrases

    noun phrase, verb phrase, etc.

chunking 

    segments are identi   ed with iobes encoding

!      tiqueter syntaxiquement des segments de phrase. 

    single word phrase (s- pre   x). ex.: s-np
    multiword phrase (b-, i-, e- pre   xes). ex.: b-vp i-vp i-vp e-vp  
    words outside of syntactic phrases: o

!    noun phrase (np), verb phrase (vp). 
!    encodage iobes:  

!    ex.: np-s pour un seul mot, np-b/np-i/np-e pour plusieurs. 

np

he 

vb

np

reckons 

the 

current 

account 

deficit 

s-np 

s-vp 

b-np 

i-np 

i-np 

e-np 

(from stanislas lauly)

4

word tagging

topics: id39 (ner)
    identify phrases referring to a named entity

ner (named-entity reconition) 

!    identifier les entit  s nomm  es d   une phrase. 
!    exemple  : emplacement, personne, endroit, etc. 
 
    example:

    person
    location
    organization

u.n. 

official 

ekeus 

heads 

for 

baghdad 

s-org 

o 

s-per 

o 

o 

s-loc 

(from stanislas lauly)

5

word tagging

!    l     tiquetage s  mantique est important pour r  pondre 

srl (id14) 

topics: id14 (srl)
    for each verb, identify the role of other words with respect to 
that verb
    example:
    v: verb
    a0: acceptor
!    v: verb 
    a1: thing accepted
!    a0: acceptor  
!    a1: thing accepted  

    a2: accepted from
 
    a3: attribute
!    a2: accepted-from  
    am-mod: modal
!    a3: attribute 
    am-neg: negation
!    am-mod: modal 
!    am-neg: negation  

aux questions     qui    ,     quand    ,     o      , etc. 

!    exemple d     tiquettes. 

he 

would 

n   t 

accept 

anything 

of 

value 

s-a0 

s-am-mod 

s-am-neg 

v 

b-a1 

i-a1 

e-a1 

(from stanislas lauly)

6

word tagging

topics: labeled corpus
    the raw data looks like this:

the          dt     b-np   o      b-a0   b-a0
$            $      i-np   o      i-a0   i-a0
1.4          cd     i-np   o      i-a0   i-a0
billion      cd     i-np   o      i-a0   i-a0
robot        nn     i-np   o      i-a0   i-a0
spacecraft   nn     e-np   o      e-a0   e-a0
faces        vbz    s-vp   o      s-v    o
a            dt     b-np   o      b-a1   o
six-year     jj     i-np   o      i-a1   o
journey      nn     e-np   o      i-a1   o
to           to     b-vp   o      i-a1   o
explore      vb     e-vp   o      i-a1   s-v
jupiter      nnp    s-np   s-org  i-a1   b-a1
and          cc     o      o      i-a1   i-a1
its          prp$   b-np   o      i-a1   i-a1
16           cd     i-np   o      i-a1   i-a1
known        jj     i-np   o      i-a1   i-a1
moons        nns    e-np   o      e-a1   e-a1
.            .      o      o      o      o

7

sentence neural network
topics: sentence convolutional network
    how to model each label sequence
    could use a crf with neural network unary 

on the mat
w1
n

the cat
w1
1 w1

input sentence

lookup table

feature k

feature 1

1 wk

p
a
d
d
i
n
g

p
a
d
d
i
n
g

sat
. . .

ltw 1

text

wk
n

wk

. . .

...

2

2

d

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

...

potentials, based on a window (context) of words
- not appropriate for id14, because 

relevant context might be very far away

    collobert and weston suggest a convolutional 

network over the whole sentence
- prediction at a given position can exploit information 

from any word in the sentence

ltw k

convolution

max over time

max(  )

linear

m 2

     

hardtanh

linear

m 3

     

n1

hu

m 1

     

n1

hu

n2

hu

n3

hu = #tags

figure 2: sentence approach network.

8

sentence neural network
topics: sentence convolutional network
    each word can be represented by more 
then one feature
    feature of the word itself
    substring features 

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

input sentence

text

- pre   x:        eating                     eat       
suf   x:        eating                     ing        
-

    gazetteer features

feature 1

...

feature k

p
a
d
d
i
n
g

lookup table

- whether the word belong to a list of known locations, 

persons, etc.

ltw 1

...

ltw k

    these features are treated like word 
features, with their own lookup tables

convolution

the cat
1 w1
w1

2

sat
. . .

on the mat
w1
n

wk

1 wk

2

. . .

wk
n

p
a
d
d
i
n
g

d

m 1

     

9

sentence neural network
topics: sentence convolutional network
    feature must encode for which word 
we are making a prediction
    done by adding the relative
position i-posw, where posw
is the position of the current 
word

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

on the mat
w1
n

the cat
1 w1
w1

input sentence

feature 1

sat
. . .

text

2

    this feature also has its lookup

table

p
a
d
d
i
n
g

...

feature k

lookup table

    for srl, must know the roles for which 
verb we are predicting
    also add the relative position of that verb i-posv

convolution

ltw k

ltw 1

...

wk

1 wk

2

. . .

wk
n

m 1

     

p
a
d
d
i
n
g

d

10

input sentence

sentence neural network
topics: sentence convolutional network
    lookup table: 

on the mat
w1
n

the cat
1 w1
w1

lookup table

feature k

feature 1

1 wk

p
a
d
d
i
n
g

p
a
d
d
i
n
g

sat
. . .

text

wk
n

wk

. . .

...

2

2

ltw 1

    for each word concatenate 
the representations of its 
features

    convolution:

    at every position, compute
linear activations from a 
window of representations
    this is a convolution in 1d

    max pooling:

    obtain a    xed hidden layer
with a max across positions

...

ltw k

convolution

max over time

max(  )

linear

m 2

m 1

     

d

n1

hu

n1

hu

11

sentence neural network
topics: sentence convolutional network
    regular neural network:

max over time

max(  )

n1

n1

hu

hu

    the pooled representation 

serves as the input of
a regular neural network
    they proposed using a 
      hard       version of the
tanh activation function

linear

m 2

     

hardtanh

linear

m 3

     

n2

hu

n3

hu = #tags

figure 2: sentence approach network.

    the outputs are used as the unary potential of a chain crf 
over the labels
    no connections between the crfs of the different task (one crf per task)
dwin
    a separate neural network is used for each task
i

in the following, we will describe each layer we use in our networks shown in figure 1 and figure 2.
we adopt few notations. given a matrix a we denote [a]i, j the coef   cient at row i and column j
in the matrix. we also denote !a"
the vector obtained by concatenating the dwin column vectors
around the ith column vector of matrix a     rd1  d2:

12

sentence neural network
topics: multitask learning
    could share vector representations of the features across 
tasks
    simply use the same lookup

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

lookup table

lookup table

ltw 1

.

.

.

tables across tasks

    the other parameters of the

neural networks are not
tied

linear

n1

hu

ltw k

m 1      

linear

n1

hu

hardtanh

hardtanh

linear

linear

m 2

     

(t1)

    this is referred to as
multitask learning
    the idea is to transfer knowledge learned within the word representations across 

figure 5: example of multitasking with nn. task 1 and task 2 are two tasks trained with the
window approach architecture presented in figure 1. lookup tables as well as the    rst
hidden layer are shared. the last layer is task speci   c. the principle is the same with
more than two tasks.

task 1

task 2

= #tags

= #tags

hu,(t1)

hu,(t2)

(t2)

     

n2

n2

m 2

the different task

13

sentence neural network
topics: language model
    we can design other tasks without any labeled data

set id178 may require prohibitively large training sets.16 the id178 criterion lacks dynamical
range because its numerical value is largely determined by the most frequent phrases. in order to
learn syntax, rare but legal phrases are no less signi   cant than common phrases.
it is therefore desirable to de   ne alternative training criteria. we propose here to use a pairwise
ranking approach (cohen et al., 1998). we seek a network that computes a higher score when
given a legal phrase than when given an incorrect phrase. because the ranking literature often deals
with information retrieval applications, many authors de   ne complex ranking criteria that give more
weight to the ordering of the best ranking instances (see burges et al., 2007; cl  emenc  on and vayatis,
2007). however, in our case, we do not want to emphasize the most common phrase over the rare
but legal phrases. therefore we use a simple pairwise criterion.
we consider a window approach network, as described in section 3.3.1 and figure 1, with
parameters    which outputs a score f  (x) given a window of text x = [w]
. we minimize the
ranking criterion with respect to   :

      cat sat on the mat          vs          cat sat think the mat      
    can generate impostor examples from unlabeled text (wikipedia)

    train a neural network (with word representations) to assign a higher score to the 

- pick a window of words from unlabeled corpus
-

    identify whether the middle word of a window of text is an       impostor      

replace middle word with a different, randomly chosen word

dwin1

original window
   !       x   x
    similar to id38, except we predict the word in the middle

max!0 , 1     f  (x) + f  (x(w))" ,

original window

   
w   d

with word w

impostor window 

where x is the set of all possible text windows with dwin words coming from our training corpus, d
is the dictionary of words, and x(w) denotes the text window obtained by replacing the central word

14

natural language processing (almost) from scratch
natural language processing (almost) from scratch

sentence neural network
topics: experimental comparison
    from natural language processing (almost) from scratch 
by collobert et al.
approach
approach
benchmark systems
benchmark systems
nn+wll
nn+sll
nn+sll+lm2
nn+wll+lm1
nn+sll+lm2+mtl
nn+sll+lm1
nn+wll+lm2
nn+sll+lm2
nn+sll+lm2
nn+sll+lm2+mtl

chunk ner srl
chunk ner srl
(f1)
(f1)
(f1)
(f1)
94.29
77.92
77.92
94.29
89.13
55.40
70.99
90.33
93.63
58.18
91.91
94.10
93.65
73.84
92.04
58.34
74.15
93.37
93.63
74.15
93.75
74.29

(f1)
(f1)
89.31
89.31
79.53
81.47
window approach
88.67
85.68
88.62
87.58
sentence approach
86.96
88.78
88.67
88.27

pos
pos
(pwa)
(pwa)
97.24
97.24
96.31
96.37
97.20
97.05
97.22
97.10
97.14
97.12
97.20
97.22

   
   

table 8: comparison in generalization performance of benchmark nlp systems with our (nn) ap-
table 9: effect of multi-tasking on our neural architectures. we trained pos, chunk ner in a
proach on pos, chunking, ner and srl tasks. we report results with both the word-level
mtl way, both for the window and sentence network approaches. srl was only included
log-likelihood (wll) and the sentence-level log-likelihood (sll). we report with (lmn)
in the sentence approach joint training. as a baseline, we show previous results of our
performance of the networks trained from the language model embeddings (table 7). gen-

15

sentence neural network
natural language processing (almost) from scratch
topics: experimental comparison
natural language processing (almost) from scratch
    from natural language processing (almost) from scratch 
natural language processing (almost) from scratch
chunk ner srl
approach
by collobert et al.
(f1)
(f1)
(f1)
approach
chunk ner srl
benchmark systems
89.31
94.29
77.92
chunk ner srl
approach
(f1)
(f1)
(f1)
window approach
(f1)
(f1)
(f1)
benchmark systems
89.31
94.29
77.92
   
nn+sll+lm2
88.67
93.63
benchmark systems
77.92
89.31
94.29
nn+wll
79.53
89.13
55.40
nn+sll+lm2+mtl
   
88.62
94.10
70.99
nn+sll
81.47
90.33
window approach
sentence approach
88.67
93.63
nn+sll+lm2
   
58.18
nn+wll+lm1
85.68
91.91
74.15
nn+sll+lm2
88.78
93.37
nn+sll+lm2+mtl
   
88.62
94.10
73.84
nn+sll+lm1
93.65
87.58
74.29
nn+sll+lm2+mtl
93.75
88.27
sentence approach
58.34
nn+wll+lm2
86.96
92.04
74.15
88.78
93.37
nn+sll+lm2
74.15
nn+sll+lm2
88.67
93.63
74.29
nn+sll+lm2+mtl
88.27
93.75

pos
(pwa)
pos
97.24
pos
(pwa)
(pwa)
97.24
97.20
97.24
96.31
97.22
96.37
97.20
97.05
97.12
97.22
97.10
97.22
97.14
97.12
97.20
97.22

table 9: effect of multi-tasking on our neural architectures. we trained pos, chunk ner in a
mtl way, both for the window and sentence network approaches. srl was only included
in the sentence approach joint training. as a baseline, we show previous results of our
table 8: comparison in generalization performance of benchmark nlp systems with our (nn) ap-
table 9: effect of multi-tasking on our neural architectures. we trained pos, chunk ner in a
window approach system, as well as additional results for our sentence approach system,
proach on pos, chunking, ner and srl tasks. we report results with both the word-level
mtl way, both for the window and sentence network approaches. srl was only included
when trained separately on each task. benchmark system performance is also given for
log-likelihood (wll) and the sentence-level log-likelihood (sll). we report with (lmn)
in the sentence approach joint training. as a baseline, we show previous results of our
comparison.
performance of the networks trained from the language model embeddings (table 7). gen-

15

nn+sll+lm2
nn+sll+lm2+mtl

97.12
97.22

93.37
93.75

88.78
88.27

74.15
74.29

sentence neural network
table 9: effect of multi-tasking on our neural architectures. we trained pos, chunk ner in a
natural language processing (almost) from scratch
mtl way, both for the window and sentence network approaches. srl was only included
topics: experimental comparison
in the sentence approach joint training. as a baseline, we show previous results of our
natural language processing (almost) from scratch
window approach system, as well as additional results for our sentence approach system,
    from natural language processing (almost) from scratch 
natural language processing (almost) from scratch
when trained separately on each task. benchmark system performance is also given for
chunk ner srl
approach
by collobert et al.
comparison.
(f1)
(f1)
(f1)
approach
chunk ner srl
benchmark systems
89.31
94.29
77.92
chunk ner srl
approach
(f1)
(f1)
(f1)
window approach
(f1)
(f1)
(f1)
benchmark systems
89.31
94.29
77.92
   
nn+sll+lm2
88.67
93.63
chunk ner srl
approach
benchmark systems
77.92
89.31
94.29
nn+wll
79.53
89.13
55.40
nn+sll+lm2+mtl
   
88.62
94.10
(f1)
(f1)
nn+sll
70.99
90.33
81.47
window approach
sentence approach
benchmark systems
77.92
94.29
89.31
88.67
93.63
nn+sll+lm2
   
nn+wll+lm1
85.68
91.91
58.18
74.15
nn+sll+lm2
88.78
93.37
88.67
93.63
74.15
nn+sll+lm2
nn+sll+lm2+mtl
   
88.62
94.10
73.84
nn+sll+lm1
93.65
87.58
74.29
nn+sll+lm2+mtl
93.75
88.27
nn+sll+lm2+suf   x2
   
sentence approach
58.34
92.04
nn+wll+lm2
86.96
   
nn+sll+lm2+gazetteer
89.59
74.15
88.78
93.37
nn+sll+lm2
74.15
nn+sll+lm2
88.67
93.63
nn+sll+lm2+pos
88.67
94.32
   
74.29
nn+sll+lm2+mtl
88.27
93.75
74.72
nn+sll+lm2+chunk

table 9: effect of multi-tasking on our neural architectures. we trained pos, chunk ner in a
mtl way, both for the window and sentence network approaches. srl was only included
in the sentence approach joint training. as a baseline, we show previous results of our
table 8: comparison in generalization performance of benchmark nlp systems with our (nn) ap-
table 9: effect of multi-tasking on our neural architectures. we trained pos, chunk ner in a
window approach system, as well as additional results for our sentence approach system,
proach on pos, chunking, ner and srl tasks. we report results with both the word-level
table 10: comparison in generalization performance of benchmark nlp systems with our neural
mtl way, both for the window and sentence network approaches. srl was only included
when trained separately on each task. benchmark system performance is also given for
log-likelihood (wll) and the sentence-level log-likelihood (sll). we report with (lmn)
networks (nns) using increasing task-speci   c engineering. we report results obtained
in the sentence approach joint training. as a baseline, we show previous results of our
comparison.
performance of the networks trained from the language model embeddings (table 7). gen-

pos
(pwa)
pos
97.24
pos
(pwa)
(pwa)
97.24
97.20
pos
97.24
96.31
97.22
(pwa)
96.37
97.24
97.20
97.05
97.12
97.20
97.22
97.10
97.22
97.29
97.14
97.12
97.20
97.22

   
   
   

   
   
   

   

   

15

sentence neural network
topics: experimental comparison
collobert, weston, bottou, karlen, kavukcuoglu and kuksa
    nearest neighbors in word representation space:

france

454

austria
belgium
germany

italy
greece
sweden
norway
europe
hungary

switzerland

jesus
1973
god
sati
christ
satan
kali
indra
vishnu
ananda
parvati
grace

xbox
6909
amiga

playstation

msx
ipod
sega

hd

psnumber

dreamcast

geforce
capcom

reddish
11724
greenish
bluish
pinkish
purplish
brownish
greyish
grayish
whitish
silvery

yellowish

scratched

megabits

87025
octets
mb/s
bit/s
baud
carats
kbit/s

29869
nailed
smashed
punched
popped
crimped
scraped
screwed
megahertz
sectioned megapixels
slashed
ripped

gbit/s
amperes

table 7: id27s in the word lookup table of the language model neural network lm1
trained with a dictionary of size 100,000. for each column the queried word is followed
by its index in the dictionary (higher means more rare) and its 10 nearest neighbors (using
the euclidean metric, which was chosen arbitrarily).

    for a 2d visualization: http://www.cs.toronto.edu/~hinton/turian.png

16

conclusion

    we saw a particular architecture for tagging words with 
syntactic and semantic information
    it exploits the idea of learning vector representations of words
    it uses a convolutional architecture, to use the whole sentence as context
    it demonstrates that unsupervised learning can help a lot in learning 
good representations
    it can incorporate additional features that are known to work well in 
certain nlp problems
    even without them, it almost reaches state of the art performances

17

