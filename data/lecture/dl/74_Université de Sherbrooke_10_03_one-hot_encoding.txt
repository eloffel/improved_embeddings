neural networks
natural language processing - one-hot encoding

natural language processing 2

topics: one-hot encoding
    from its word id, we get a basic representation of a word 
through the one-hot encoding of the id
    the one-hot vector of an id is a vector    lled with 0s, except for a 1 at the position 

associated with the id
- ex.:  for vocabulary size d=10, the one-hot vector of word id w=4 is

                                      e(w) = [ 0 0 0 1 0 0 0 0 0 0 ]

    a one-hot encoding makes no assumption about word similarity

-

-

||e(w) - e(w   )||2 = 0 if w = w   
||e(w) - e(w   )||2 = 2 if w     w   

- all words are equally different from each other

    this is a natural representation to start with, though a poor one

natural language processing 3

topics: one-hot encoding
    the major problem with the one-hot representation is that it 
is very high-dimensional
    the dimensionality of e(w) is the size of the vocabulary
    a typical vocabulary size is    100 000
    a window of 10 words would correspond to an input vector of at least 1 000 000 

units!

    this has 2 consequences:

    vulnerability to over   tting

- millions of inputs means millions of parameters to train in a regular neural network

    computationally expensive

- not all computations can be sparsi   ed (ex.: reconstruction in autoencoder)

