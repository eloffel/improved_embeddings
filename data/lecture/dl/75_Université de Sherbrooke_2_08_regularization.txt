neural networks
training neural networks - id173

   

   

   

arg min

abstract

t xt

l(f (x(t);    ), y(t)) +     (   )

feedforward neural network

math for my slides    feedforward neural network   .

september 13, 2012
hugo larochelle
hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke
d  epartement d   informatique
universit  e de sherbrooke

machine learning

    9w,t    x(t   ) =pt6=t    wtx(t)
    r(x) ={x2rh|9w x =pj wjx  ,j}
    {x2rh| x /2r(x)}
    { i,ui| xui =  iui et u>i uj = 1i=j}

t xt
t xt
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
       (   )
       (   )
topics: stochastic id119 (sgd)
t pt r   l(f (x(t);    ), y(t))    r      (   )
   
              +  
t xt
      =   1
1
t pt r   l(f (x(t);    ), y(t))    r      (   )
l(f (x(t);    ), y(t)) +     (   )
arg min
      =   1
    algorithm that performs updates after each example
    {x 2 rd | rxf (x) = 0}
    f (x)
              +  
    v>r2
xf (x)v > 0 8v
    initialize           (                                                                    )
    {x 2 rd | rxf (x) = 0}
            {w(1), b(1), . . . , w(l+1), b(l+1)}
              +  
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    for n iterations
    v>r2
xf (x)v > 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
       (   )
    v>r2
for each training example
xf (x)v < 0 8v
    (x(t), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
training epoch 
    r   l(f (x(t);    ), y(t))
      =   1
    f (x)
     
=
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
iteration over all examples
     
              +      
       (   )
5
    l(f (x(t);    ), y(t))
    f (x)
    f (x)
    f (x)
    {x 2 rd | rxf (x) = 0}
    r      (   )
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    the id168
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
    f (x)c = p(y = c|x)
    r   l(f (x(t);    ), y(t))
       (   )
    a procedure to compute the parameter gradients
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    the regularizer             (and the gradient                 )
    x(t) y(t)
    r      (   )
       (   )
5
       (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    initialization method
    f (x)c = p(y = c|x)
    r      (   )
    r      (   )
    (x(t), y(t))
    r      (   )

    to apply this algorithm to neural network training, we need

hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

september 13, 2012
september 13, 2012

math for my slides    feedforward neural network   .

abstract
abstract

5

   

-

d  epartement d   informatique
hugo larochelle
universit  e de sherbrooke

2

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

september 13, 2012
hugo.larochelle@usherbrooke.ca

september 13, 2012
abstract

abstract

3

id173

    rw(k)   (   ) = 2w(k)

    g0(a) = g(a)(1   g(a))
    g0(a) = g(a)(1   g(a))
topics: l2 id173
    g0(a) = 1   g(a)2
    g0(a) = 1   g(a)2
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
       (   ) =pkpipj   w (k)
i,j    2
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj |w (k)
i,j |
       (   ) =pkpipj |w (k)
i,j |
    rw(k)   (   ) = sign(w(k))
    rw(k)   (   ) = sign(w(k))
    sign(w(k))i,j = 1 0
    sign(w(k))i,j = 1 0

    only applied on weights, not on biases (weight decay)
    can be interpreted as having a gaussian prior over the 
weights

    gradient:

f

=pk ||w(k)||2

f

4

=pk ||w(k)||2

f

f

    g0(a) = g(a)(1   g(a))
    g0(a) = 1   g(a)2
id173
       (   ) =pkpipj   w (k)
i,j    2
    g0(a) = 1   g(a)2
       (   ) =pkpipj   w (k)
i,j    2
    g0(a) = g(a)(1   g(a))
=pk ||w(k)||2
topics: l1 id173
    g0(a) = 1   g(a)2
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
       (   ) =pkpipj |w (k)
    rw(k)   (   ) = 2w(k)
i,j |
    rw(k)   (   ) = 2w(k)
       (   ) =pkpipj |w (k)
i,j |
       (   ) =pkpipj |w (k)
    rw(k)   (   ) = sign(w(k))
i,j |
    rw(k)   (   ) = sign(w(k))
    rw(k)   (   ) = sign(w(k))
    sign(w(k))i,j = 1 0
    sign(w(k))i,j = 1w(k)
i,j >0   1w(k)
    sign(w(k))i,j = 1 0

    also only applied on weights
    unlike l2, l1 will push certain weights to be exactly 0
    can be interpreted as having a laplacian prior over the 
weights

    gradient:

    where 

i,j <0

f

5

machine learning

t pt r   l(f (x(t);    ), y(t))    r      (   )

    l(f (x(t);    ), y(t))
       (   )
topics: bias-variance trade-off
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
      =   1
       (   )
       (   )
       (   )
    variance of trained model: does it vary a lot if the training set 
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
      =   1
      =   1
      =   1
changes 
       (   )
       (   )
    {x 2 rd | rxf (x) = 0}
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
              +  
              +  
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
    bias of trained model: is the average model close to the true 
      =   1
    v>r2
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
solution
              +  
              +  
    v>r2
xf (x)v < 0 8v
    v>r2
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v > 0 8v
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    generalization error can be seen as the sum of the (squared) 
    {x 2 rd | rxf (x) = 0}
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
    v>r2
    v>r2
xf (x)v < 0 8v
xf (x)v < 0 8v
xf (x)v < 0 8v
    v>r2
xf (x)v > 0 8v
bias and the variance
    v>r2
xf (x)v > 0 8v
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
xf (x)v < 0 8v
    v>r2
xf (x)v < 0 8v
possible
    f    f
    (x(t), y(t))
    (x(t), y(t))
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    f    f
    f    f
    f    f
    (x(t), y(t))
    (x(t), y(t))
possible
    f    f
possible
    f    f

t pt r   l(f (x(t);    ), y(t))    r      (   )

low variance/

high bias

good trade-off

high variance/

low bias

