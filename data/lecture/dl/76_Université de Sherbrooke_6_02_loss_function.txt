neural networks
autoencoder - id168

hugo larochelle

math for my slides    autoencoders   .
d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

autoencoder
d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: autoencoder, encoder, decoder, tied weights
hugo.larochelle@usherbrooke.ca
    feed-forward neural network trained to reproduce its input at 
= sigm(b + wx)
october 17, 2012
the output layer

h(x) = g(a(x))

october 16, 2012
decoder

2

ck

 x

abstract

= w 
w 
(tied weights)

math for my slides    autoencoders   .

 
 
    f (x)    bx l(f (x)) =pk(bxk   xk)2

= sigm(c + w   h(x))

= sigm(b + wx)

h(x) = g(a(x))

bx = o(ba(x))

w

x

bj

abstract
= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

 
for binary inputs
encoder
h(x) = g(a(x))

= sigm(b + wx)

    f (x)    bx l(f (x)) =pk(bxk   xk)2

   

autoencoder

h(x) = g(a(x))

3

= sigm(b + wx)

    f (x)    bx l(f (x)) =pk(bxk   xk)2

   

    for real-valued inputs:

= sigm(c + w   h(x))

- cross-id178 (more precisely: sum of bernoulli cross-entropies)

topics: id168
    for binary inputs:

bx = o(c + w   h(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
    f (x)    bx l(f (x)) = 1
    rba(x(t))l(f (x(t))) =bx(t)   x(t)

2pk(bxk   xk)2

- sum of squared differences (squared euclidean distance)
- we use a linear activation function at the output

= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

a(x(t)) (= b + wx(t)

4

   

   

   

= sigm(c + w   h(x))

    f (x)    bx l(f (x)) =pk(bxk   xk)2

ba(x(t)) (= c + w>h(x(t))
bx(t) (= sigm(ba(x(t)))

topics: id168 gradient
    for both cases, the gradient  
has a very simple form:

    rba(x(t))l(f (x(t))) =bx(t)   x(t)
bx = o(ba(x))
   
h(x(t)) (= sigm(a(x(t)))
a(x(t)) (= b + wx(t)
a(x(t)) (= b + wx(t)
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
    f (x)    bx l(f (x)) =pk(bxk   xk)2
autoencoder
ba(x(t)) (= c + w>h(x(t))
h(x(t)) (= sigm(a(x(t)))
h(x(t)) (= sigm(a(x(t)))
    rba(x(t))l(f (x(t))) =bx(t)   x(t)
bx(t) (= sigm(ba(x(t)))
ba(x(t)) (= c + w>h(x(t))
bx = o(ba(x))
a(x(t)) (= b + wx(t)
bx = o(ba(x))
bx(t) (= sigm(ba(x(t)))
h(x(t)) (= sigm(a(x(t)))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
    f (x)    bx l(f (x)) =pk(bxk   xk)2
= sigm(c + w   h(x))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
ba(x(t)) (= c + w>h(x(t))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
bx(t) (= sigm(ba(x(t)))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
    f (x)    bx l(f (x)) =pk(bxk   xk)2
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
a(x(t)) (= b + wx(t)
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
    rba(x(t))l(f (x(t))) =bx(t)   x(t)
h(x(t)) (= sigm(a(x(t)))
rbl(f (x(t))) (= ra(x(t))l(f (x(t)))
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]
ba(x(t)) (= c + w>h(x(t))
rbl(f (x(t))) (= ra(x(t))l(f (x(t)))
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
rbl(f (x(t))) (= ra(x(t))l(f (x(t)))
bx(t) (= sigm(ba(x(t)))
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
a(x(t)) (= b + wx(t)
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
rwl(f (x(t))) (=    ra(x(t))l(f (x(t)))    x(t)> + h(x(t))   rba(x(t))l(f (x(t)))   >
h(x(t)) (= sigm(a(x(t)))
rba(x(t))l(f (x(t))) (= bx(t)   x(t)
ba(x(t)) (= c + w>h(x(t))
rcl(f (x(t))) (= rba(x(t))l(f (x(t)))
rh(x(t))l(f (x(t))) (= w   rba(x(t))l(f (x(t)))   
bx(t) (= sigm(ba(x(t)))
ra(x(t))l(f (x(t))) (=    rh(x(t))l(f (x(t)))      [. . . , h(x(t))j(1   h(x(t))j), . . . ]

    parameter gradients are obtained by backpropagating the 
gradient                      like in a regular network
    important: when using tied weights (             ),                    is the 
    w    = w>
sum of two gradients !
- this is because        is present in the encoder and in the decoder
    rwl(f (x(t))) w

    w    = w>
    rwl(f (x(t)))

rbl(f (x(t))) (= ra(x(t))l(f (x(t)))

    w    = w>

1

1

1

   

autoencoder

5

    f (x)    bx l(f (x)) =pk(bxk   xk)2

-

topics: adaptation to the type of input
    recipe to adapt an autoencoder to a new
type of input
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    choose a joint distribution             over the inputs
    rwl(f (x(t))) w
    rwl(f (x(t))) w
    p(x|  )   
    is the vector of parameters of that distribution
    p(x|  )   
    rwl(f (x(t))) w
    p(x|  )   
    p(x|  )   
       h(x)
    rwl(f (x(t))) w
    rwl(f (x(t))) w
       h(x)
       h(x)
    p(x|  )   
       h(x)
    l(f (x(t))) =   log p(x(t))
    p(x|  )   
    l(f (x)) =   log p(x|  )
       h(x)
    l(f (x(t))) =   log p(x(t))
    l(f (x(t))) =   log p(x(t))
    p(x|  )   
2pk(xk     k)2)    = c + w   h(x)
    example: we get the sum of squared distance by
       h(x)
    rwl(f (x(t))) w
    l(f (x(t))) =   log p(x(t))
    p(x|  ) =
       h(x)
    choosing a gaussian distribution with mean     and identity covariance 
    l(f (x)) =   log p(x|   = h(x))
    p(x|  )   
2pk(xk     k)2)    = c + w   h(x)
for
    l(f (x)) =   log p(x|   = h(x))
(2   )d/2 exp(  1
    p(x|  ) =
       h(x)
    choosing
       = c + w   h(x)
    l(f (x(t))) =   log p(x(t))

    choose the relationship between    and the hidden layer
    use                                  as the id168

(2   )d/2 exp(  1

1

1

