neural networks
sparse coding - de   nition

unsupervised learning

2

math for my slides    restricted id82s   .

topics: unsupervised learning
    unsupervised learning: only use the inputs       for learning

    x(t)   log p(x(t))

    automatically extract meaningful features for your data
    leverage the availability of unlabeled data
    add a data-dependent regularizer to trainings

    we will see 3 neural networks for unsupervised learning

    restricted id82s
    autoencoders
    sparse coding model

hugo larochelle

sparse coding

november 1, 2012
sparse coding
abstract

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

3

november 1, 2012

abstract

hugo.larochelle@usherbrooke.ca

sparse coding
hugo larochelle
abstract

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

    x(t) h(t) d

hugo larochelle

math for my slides    sparse coding   .

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

d  epartement d   informatique
abstract
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
d  epartement d   informatique
hugo larochelle
    x(t) h(t) d
txt=1
math for my slides    sparse coding   .
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
universit  e de sherbrooke
    it is sparse: the vector        has many zeros
1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
arg min
arg min
arg min
arg min
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
t
    we can reconstruct the original input        as well as possible
1
1
d
h(t)
h(t)
    x(t) h(t) d
2 +  ||h(t)||1
2||x(t)   d h(t)||2
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
txt=1
t
1
1
   
h(t)
november 1, 2012
2||x(t)   d h(t)||2
arg min
arg min
1
1
november 1, 2012
t
h(x(t)) = arg min
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d
h(t)
1
abstract
h(t)
2||x(t)   d h(t)||2
h(x(t)) = arg min

min
h(t)
math for my slides    sparse coding   .

h(x(t)) = arg min
2 +  ||h(t)||1

1
2||x(t)   d h(t)||2

h(x(t)) = arg min

txt=1

txt=1

abstract

abstract

abstract

min
d

1
t

h(t)

h(t)

1
t

   

d

d

2 +  ||h(t)||1
1
2||x(t)   d h(t)||2

h(t)

h(x(t)) = arg min

math for my slides    sparse coding   .

1
math for my slides    sparse coding   .
2||x(t)   d h(t)||2

    we also constrain the columns of        to be of norm 1
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
arg min
arg min
    sometimes the columns are constrained to be no greater than 1
t
1
d
h(t)
2||x(t)   d h(t)||2
arg min
arg min
arg min
d
h(t)

    x(t) h(t) d
arg min

bx(t) = d h(x(t)) = xk s.t.

d  ,k h(x(t))k
1
t

txt=1
txt=1

2 +  ||h(t)||1

1
t

d

txt=1

1
2||x(t)   d h(t)||2
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

txt=1
    x(t) h(t) d

h(t)

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo larochelle

sparse coding

november 1, 2012
sparse coding
abstract

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

3

november 1, 2012

abstract

hugo.larochelle@usherbrooke.ca

sparse coding
hugo larochelle
abstract

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

    x(t) h(t) d

hugo larochelle

math for my slides    sparse coding   .

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

d  epartement d   informatique
abstract
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
d  epartement d   informatique
hugo larochelle
    x(t) h(t) d
txt=1
math for my slides    sparse coding   .
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
universit  e de sherbrooke
    it is sparse: the vector        has many zeros
1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
arg min
arg min
arg min
arg min
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
t
    we can reconstruct the original input        as well as possible
1
1
d
h(t)
h(t)
    x(t) h(t) d
2 +  ||h(t)||1
2||x(t)   d h(t)||2
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
txt=1
t
1
1
   
h(t)
november 1, 2012
2||x(t)   d h(t)||2
arg min
arg min
1
1
november 1, 2012
t
h(x(t)) = arg min
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d
h(t)
1
abstract
h(t)
2||x(t)   d h(t)||2
h(x(t)) = arg min

 
min
h(t)
math for my slides    sparse coding   .

h(x(t)) = arg min
2 +  ||h(t)||1

1
2||x(t)   d h(t)||2

h(x(t)) = arg min

reconstruction error

txt=1

txt=1

abstract

abstract

abstract

min
d

1
t

h(t)

h(t)

1
t

   

d

d

2 +  ||h(t)||1
1
2||x(t)   d h(t)||2

h(t)

h(x(t)) = arg min

math for my slides    sparse coding   .

1
math for my slides    sparse coding   .
2||x(t)   d h(t)||2

    we also constrain the columns of        to be of norm 1
- otherwise,      could grow big while        becomes small to satisfy the prior
1
    x(t) h(t) d
arg min
arg min
    sometimes the columns are constrained to be no greater than 1
t
1
d
h(t)
2||x(t)   d h(t)||2
arg min
arg min
arg min
d
h(t)

    x(t) h(t) d
arg min

bx(t) = d h(x(t)) = xk s.t.

d  ,k h(x(t))k
1
t

txt=1
txt=1

2 +  ||h(t)||1

1
t

d

txt=1

1
2||x(t)   d h(t)||2
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

txt=1
    x(t) h(t) d

h(t)

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo.larochelle@usherbrooke.ca

3

abstract

abstract

hugo larochelle

    x(t) h(t) d

november 1, 2012

hugo larochelle

math for my slides    sparse coding   .

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

november 1, 2012
sparse coding
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
sparse coding
sparse coding
hugo larochelle
abstract
hugo larochelle
d  epartement d   informatique
d  epartement d   informatique
abstract
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
d  epartement d   informatique
hugo larochelle
    x(t) h(t) d
universit  e de sherbrooke
txt=1
math for my slides    sparse coding   .
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
universit  e de sherbrooke
    it is sparse: the vector        has many zeros
1
1
hugo.larochelle@usherbrooke.ca
2||x(t)   d h(t)||2
2 +  ||h(t)||1
arg min
arg min
arg min
arg min
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
t
    we can reconstruct the original input        as well as possible
1
1
d
h(t)
h(t)
    x(t) h(t) d
november 1, 2012
2 +  ||h(t)||1
2||x(t)   d h(t)||2
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
txt=1
t
1
1
   
h(t)
november 1, 2012
2||x(t)   d h(t)||2
arg min
arg min
1
1
november 1, 2012
t
h(x(t)) = arg min
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d
h(t)
abstract
1
abstract
h(t)
2||x(t)   d h(t)||2
h(x(t)) = arg min

1
2||x(t)   d h(t)||2
h(t)
math for my slides    sparse coding   .
   
 
reconstruction
    x(t) h(t) d bx(t)
1
math for my slides    sparse coding   .
    we also constrain the columns of        to be of norm 1
1
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
arg min
d
- otherwise,      could grow big while        becomes small to satisfy the prior
1
1
    x(t) h(t) d
    x(t) h(t) d
2||x(t)   d h(t)||2
arg min
arg min
txt=1
    sometimes the columns are constrained to be no greater than 1
1
t
1
1
d
2||x(t)   d h(t)||2
h(t)
2 +  ||h(t)||1
2||x(t)   d h(t)||2
arg min
arg min
arg min
arg min
2||x(t)   d h(t)||2
h(x(t)) = arg min
d
h(t)

 
min
h(t)
math for my slides    sparse coding   .

txt=1
    x(t) h(t) d

2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

bx(t) = d h(x(t)) = xk s.t.

h(x(t)) = arg min
2 +  ||h(t)||1

math for my slides    sparse coding   .

txt=1
txt=1
txt=1

d  ,k h(x(t))k
1
t

h(x(t)) = arg min

2 +  ||h(t)||1

2 +  ||h(t)||1

h(x(t)) = arg min

reconstruction error

txt=1

txt=1

abstract

abstract

arg min

min
d

1
t

1
t

h(t)

h(t)

1
t

1
t

h(t)

h(t)

   

d

d

d

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo.larochelle@usherbrooke.ca

3

abstract

abstract

hugo larochelle

    x(t) h(t) d

november 1, 2012

hugo larochelle

math for my slides    sparse coding   .

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

november 1, 2012
sparse coding
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
sparse coding
sparse coding
hugo larochelle
abstract
hugo larochelle
d  epartement d   informatique
d  epartement d   informatique
abstract
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
d  epartement d   informatique
hugo larochelle
    x(t) h(t) d
universit  e de sherbrooke
txt=1
math for my slides    sparse coding   .
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
universit  e de sherbrooke
    it is sparse: the vector        has many zeros
1
1
hugo.larochelle@usherbrooke.ca
2||x(t)   d h(t)||2
2 +  ||h(t)||1
arg min
arg min
arg min
arg min
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
t
    we can reconstruct the original input        as well as possible
1
1
d
h(t)
h(t)
    x(t) h(t) d
november 1, 2012
2 +  ||h(t)||1
2||x(t)   d h(t)||2
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
txt=1
t
1
1
   
h(t)
november 1, 2012
2||x(t)   d h(t)||2
arg min
arg min
1
1
november 1, 2012
t
h(x(t)) = arg min
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d
h(t)
abstract
1
abstract
h(t)
2||x(t)   d h(t)||2
h(x(t)) = arg min

1
2||x(t)   d h(t)||2
h(t)
math for my slides    sparse coding   .
   
 
reconstruction
    x(t) h(t) d bx(t)
1
math for my slides    sparse coding   .
    we also constrain the columns of        to be of norm 1
1
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
arg min
d
- otherwise,      could grow big while        becomes small to satisfy the prior
1
1
    x(t) h(t) d
    x(t) h(t) d
2||x(t)   d h(t)||2
arg min
arg min
txt=1
    sometimes the columns are constrained to be no greater than 1
1
t
1
1
d
2||x(t)   d h(t)||2
h(t)
2 +  ||h(t)||1
2||x(t)   d h(t)||2
arg min
arg min
arg min
arg min
2||x(t)   d h(t)||2
h(x(t)) = arg min
d
h(t)

 
min
h(t)
math for my slides    sparse coding   .

txt=1
    x(t) h(t) d

2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

bx(t) = d h(x(t)) = xk s.t.

sparsity penalty
h(x(t)) = arg min
2 +  ||h(t)||1

math for my slides    sparse coding   .

txt=1
txt=1
txt=1

d  ,k h(x(t))k
1
t

h(x(t)) = arg min

2 +  ||h(t)||1

2 +  ||h(t)||1

h(x(t)) = arg min

reconstruction error

txt=1

txt=1

abstract

abstract

arg min

 

min
d

1
t

1
t

h(t)

h(t)

1
t

1
t

h(t)

h(t)

   

d

d

d

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

hugo.larochelle@usherbrooke.ca

3

abstract

abstract

hugo larochelle

    x(t) h(t) d

november 1, 2012

hugo larochelle

math for my slides    sparse coding   .

hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .

d  epartement d   informatique
universit  e de sherbrooke

sparse coding

november 1, 2012
sparse coding
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
sparse coding
sparse coding
hugo larochelle
abstract
hugo larochelle
d  epartement d   informatique
d  epartement d   informatique
abstract
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
d  epartement d   informatique
hugo larochelle
    x(t) h(t) d
universit  e de sherbrooke
txt=1
math for my slides    sparse coding   .
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
universit  e de sherbrooke
    it is sparse: the vector        has many zeros
1
1
hugo.larochelle@usherbrooke.ca
2||x(t)   d h(t)||2
2 +  ||h(t)||1
arg min
arg min
arg min
arg min
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
t
    we can reconstruct the original input        as well as possible
1
1
d
h(t)
h(t)
    x(t) h(t) d
november 1, 2012
2 +  ||h(t)||1
2||x(t)   d h(t)||2
november 1, 2012
arg min
arg min
hugo.larochelle@usherbrooke.ca
txt=1
t
1
1
   
h(t)
november 1, 2012
2||x(t)   d h(t)||2
arg min
arg min
1
1
november 1, 2012
t
h(x(t)) = arg min
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d
h(t)
abstract
1
abstract
h(t)
2||x(t)   d h(t)||2
h(x(t)) = arg min

1
2||x(t)   d h(t)||2
h(t)
math for my slides    sparse coding   .
   
 
reconstruction
    x(t) h(t) d bx(t)
1
math for my slides    sparse coding   .
    we also constrain the columns of        to be of norm 1
1
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
arg min
d
- otherwise,      could grow big while        becomes small to satisfy the prior
1
1
    x(t) h(t) d
    x(t) h(t) d
2||x(t)   d h(t)||2
arg min
arg min
txt=1
    sometimes the columns are constrained to be no greater than 1
1
t
1
1
d
2||x(t)   d h(t)||2
h(t)
2 +  ||h(t)||1
2||x(t)   d h(t)||2
arg min
arg min
arg min
arg min
2||x(t)   d h(t)||2
h(x(t)) = arg min
d
h(t)

 
min
h(t)
math for my slides    sparse coding   .

txt=1
    x(t) h(t) d

2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

bx(t) = d h(x(t)) = xk s.t.

sparsity penalty
h(x(t)) = arg min
2 +  ||h(t)||1

math for my slides    sparse coding   .

txt=1
txt=1
txt=1

d  ,k h(x(t))k
1
t

abstract
reconstruction vs.
sparsity control

h(x(t)) = arg min

2 +  ||h(t)||1

2 +  ||h(t)||1

h(x(t)) = arg min

reconstruction error

txt=1

txt=1

abstract

arg min

 

min
d

1
t

1
t

h(t)

h(t)

1
t

1
t

 

h(t)

h(t)

   

d

d

d

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

4

sparse coding
hugo larochelle
hugo larochelle

hugo.larochelle@usherbrooke.ca

sparse coding

november 1, 2012

hugo larochelle

sparse coding

math for my slides    sparse coding   .

november 1, 2012
abstract

topics: sparse coding
    for each          nd a latent representation       such that:
    x(t) h(t) d

sparse coding
d  epartement d   informatique
universit  e de sherbrooke
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
math for my slides    sparse coding   .
universit  e de sherbrooke
d  epartement d   informatique
sparse coding
d  epartement d   informatique
hugo.larochelle@usherbrooke.ca
abstract
math for my slides    sparse coding   .
november 1, 2012
universit  e de sherbrooke
d  epartement d   informatique
    x(t) h(t) d
universit  e de sherbrooke
hugo larochelle
txt=1
november 1, 2012
hugo.larochelle@usherbrooke.ca
    it is sparse: the vector        has many zeros
universit  e de sherbrooke
1
1
hugo.larochelle@usherbrooke.ca
2 +  ||h(t)||1
2||x(t)   d h(t)||2
d  epartement d   informatique
arg min
arg min
arg min
arg min
abstract
hugo.larochelle@usherbrooke.ca
t
    we can reconstruct the original input        as well as possible
universit  e de sherbrooke
1
1
d
h(t)
h(t)
    x(t) h(t) d
november 1, 2012
2 +  ||h(t)||1
2||x(t)   d h(t)||2
november 1, 2012
arg min
hugo.larochelle@usherbrooke.ca
txt=1
t
1
   
2||x(t)   d h(t)||2
arg min
november 1, 2012
1
1
h(x(t)) = arg min
2||x(t)   d h(t)||2
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d
abstract
1
2||x(t)   d h(t)||2
h(x(t)) = arg min

sparsity penalty
h(x(t)) = arg min
2 +  ||h(t)||1
abstract

sparse coding
abstract

math for my slides    sparse coding   .

    x(t) h(t) d

abstract
hugo larochelle

abstract
reconstruction error

txt=1

txt=1

arg min

arg min

h(t)
h(t)

 

1
t

1
t

h(t)

1
t

 

d

d

arg min

min
d
1
t

txt=1
 
1
2||x(t)   d h(t)||2
h(t)
min
abstract
math for my slides    sparse coding   .
   
h(t)
 
txt=1
1
math for my slides    sparse coding   .
math for my slides    sparse coding   .
2||x(t)   d h(t)||2
arg min
reconstruction
    x(t) h(t) d bx(t)
d
h(t)
    x(t) h(t) d
1
h(x(t)) = arg min
2||x(t)   d h(t)||2
txt=1
    x(t) h(t) d
arg min
h(t)
txt=1
1
d
2||x(t)   d h(t)||2
h(x(t)) = arg min
arg min
arg min
bx(t) = d h(x(t)) = xk s.t.
   
   
h(t)
- encoder is the minimization
d
h(x(t)) = arg min

reconstruction vs.
2 +  ||h(t)||1
sparsity control
txt=1
math for my slides    sparse coding   .
         is equivalent to the autoencoder output weight matrix
2 +  ||h(t)||1
1
arg min
2 +  ||h(t)||1
2||x(t)   d h(t)||2
    however,             is now a complicated function of      
1
    x(t) h(t) d
2 +  ||h(t)||1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
d  ,k h(x(t))k
1
h(t)
2||x(t)   d h(t)||2
2 +  ||h(t)||1

h(x(t)) = arg min

h(x(t)) = arg min

1
t
1
t

arg min

arg min

h(t)

1
t

h(t)

h(t)

h(t)

d

2 +  ||h(t)||1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1

h(t)

1
2||x(t)   d h(t)||2

1
1
2||x(t)   d h(t)||2
2 +  ||h(t)||1
t

arg min

txt=1

math for my slides    sparse coding   .

math for my slides    sparse coding   .
    more formally:
   

    x(t) h(t) d bx(t)

   

txt=1
sparse coding

1
2||x(t)   d h(t)||2

sparse coding
2 +  ||h(t)||1
hugo larochelle

h(t)

h(x(t)) = arg min

topics: dictionary
    can also write

bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
h(x(t))k6=0

d  ,k h(x(t))k
november 1, 2012

5

=  1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 0.8

+ 0.8

+ 1

+ 1

+ 0.8
math for my slides    sparse coding   .

+ 1

+ 1

+ 1

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
1
2||x(t)   d h(t)||2
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.
2 +  ||h(t)||1

    we also refer to     as the dictionary
    x(t) h(t) d
in certain applications, we know what dictionary matrix to use
-
- often however, we have to learn it

txt=1

ingly, the encoder and decoder    lter values are nearly identical up to a scale factor. after training,
id136 is extremely fast, requiring only a simple matrix-vector multiplication.

arg min

arg min

1
t

h(t)

   

d

+ 0.8

abstract

txt=1
sparse coding

1
2||x(t)   d h(t)||2

sparse coding
2 +  ||h(t)||1
hugo larochelle

h(t)

h(x(t)) = arg min

topics: dictionary
    can also write

bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
h(x(t))k6=0

d  ,k h(x(t))k
november 1, 2012

5

=  1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 0.8

+ 0.8

+ 1

+ 1

+ 0.8
math for my slides    sparse coding   .

+ 1

+ 1

+ 1

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
1
2||x(t)   d h(t)||2
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.
2 +  ||h(t)||1

    we also refer to     as the dictionary
    x(t) h(t) d
in certain applications, we know what dictionary matrix to use
-
- often however, we have to learn it

txt=1

ingly, the encoder and decoder    lter values are nearly identical up to a scale factor. after training,
id136 is extremely fast, requiring only a simple matrix-vector multiplication.

arg min

arg min

1
t

h(t)

   

d

+ 0.8

abstract

txt=1
sparse coding

1
2||x(t)   d h(t)||2

sparse coding
2 +  ||h(t)||1
hugo larochelle

h(t)

h(x(t)) = arg min

topics: dictionary
    can also write

bx(t) = d h(x(t)) = xk s.t.

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
h(x(t))k6=0

d  ,k h(x(t))k
november 1, 2012

5

=  1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 1

+ 0.8

+ 0.8

+ 1

+ 1

+ 0.8
math for my slides    sparse coding   .

+ 1

+ 1

+ 1

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
1
2||x(t)   d h(t)||2
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.

figure 4: top: a randomly selected subset of encoder    lters learned by our energy-based model
when trained on the mnist handwritten digit dataset. bottom: an example of reconstruction of a
digit randomly extracted from the test data set. the reconstruction is made by adding    parts   : it is
the additive linear combination of few basis functions of the decoder with positive coef   cients.
2 +  ||h(t)||1

    we also refer to     as the dictionary
    x(t) h(t) d
in certain applications, we know what dictionary matrix to use
-
- often however, we have to learn it

txt=1

ingly, the encoder and decoder    lter values are nearly identical up to a scale factor. after training,
id136 is extremely fast, requiring only a simple matrix-vector multiplication.

arg min

arg min

1
t

h(t)

   

d

+ 0.8

abstract

