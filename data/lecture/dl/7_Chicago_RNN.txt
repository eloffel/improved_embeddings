ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

id38

machine translation

attention

id125

error-based training

1

id38

we are given a sequence x1, . . ., xt and we want to compute
a model id203.

q  (x1, . . . , xt )

here xt can be either characters or words.
we represent the id203 as a product of conditionals.

q  (x1, . . . , xt ) =

q  (xt | x1, . . . , xt   1)

t

a model of this form is called autoregressive     a model
(regression) is used to predict the next element from the pre-
vious elements.

2

(cid:89)

id56 id38

[christopher olah]

q  (xt+1 | x1, . . . , xt) = softmax(w o ht)

for word language models this softmax is the computation
bottleneck in training.

3

standard measures of performance

bits per character: for character language models per-
formance is measured in bits per character. typical numbers
are slightly over one bit per character.

perplexity: it would be natural to measure word language
models in bits per word. however, it is traditional to measure
then in perplexity which is de   ned to be 2b where b is bits per
world. perplexities of about 60 are typical.

according to quora there are 4.79 letters per word. 1 bit
per character (including space characters) gives a perplexity of
25.79 or 55.3.

4

sampling from an autoregressive model

to sample a sentence

we sample xt from

x1, . . . , xt , <eos>

q  (xt|x1, . . . , xt   1)

until we get <eos>.

5

machine translation

[figure from luong et al.]

dcba     xy x

translation is a sequence to sequence (id195) task.

sequence to sequence learning with neural net-
works, sutskever, vinyals and le, nips 2014, arxiv sept 10,
2014.

machine translation

[figure from luong et al.]

the input sentence is represented by a    thought vector    pro-
duced by an id56.

7

machine translation

[figure from luong et al.]

the thought vector is the initial state vector used in    sam-
pling    a translation.

8

machine translation

[figure from luong et al.]

for the training pair dcba     xy x the training loss is

    log q  (xy z | dcba)

9

machine translation

[figure from luong et al.]

in sutskever et al. (2014) the lstms are layered 4 deep.

10

attention-based translation

translation is improved by aligning input words with output
words.

this is done with    attention   .

id4 by jointly learning to
align and translate dzmitry bahdanau, kyunghyun cho,
yoshua bengio, iclr 2015 (arxiv sept. 1, 2014)

11

attention

the input sentence is no longer represented by just one thought
vector.

instead the entire sequence of hidden vectors produced by the
id56 is used during generation of the translation.

12

attention

[bahdanau, cho, bengio (2014)]

13

a    rst step: biid56s

(cid:126)ht+1 = id56cell  ((cid:126)ht, xt)

t   1
(cid:126)h

t
= id56cell  ( (cid:126)h

, xt)

(cid:104)(cid:126)ht, (cid:126)h
t(cid:105)

   
t
h

=

([x, y] denotes vector concatenation)

14

basic sequence to sequence model

s0 = (cid:126)ht
y0 = <eos>

q  (:

|x, y1, . . . , yi) = softmax

  y

w y si+1

si+1 = id56cell  (si, e(yi)))

15

adding attention

t

   
c0 =
h
s0 = (cid:126)ht
y0 = <eos>

q  (:

|x, y1, . . . , yi) = softmax

  y

w y si+1

si+1 = id56cell  (si, [e(yi), ci])

(cid:88)

ci =

  i,t    

h

t

t

  i,t = softmax

t

tanh(w a [si,

   
t
h

])

attention

[bahdanau, cho, bengio (2014)]

17

bilinear attention is more popular today

t

   
c0 =
h
s0 = (cid:126)ht
y0 = <eos>

q  (:

|x, y1, . . . , yi) = softmax

  y

w y si+1

si+1 = id56cell  (si, [e(yi), ci])

t(cid:88)

t

ci =

  i,t    
  i,t = softmax

h

t

(si)(cid:62)w a    

h

t

attention in image captioning

xu et al. icml 2015

19

greedy decoding vs. id125

we would like

y    = argmax

y

q  (y|x)

a greedy algorithm may do well

yt+1 = argmax

  y

q  (  y | x, y1, . . . , yt)

but these are not the same.

21

example

y    = argmax

y

q  (y|x)

yt+1 = argmax

  y

q  (  y | x, y1, . . . , yt)

   those apples are good    vs.    apples are good   

q  (apples are good <eos>) > q  (those apples are good <eos>)

q  (those|  ) > q  (apples|  )

22

id125

at each time step we maintain a list of k word-vector pairs:

y t = ((yt,1, ht,1), . . . , (yt,k, ht,k)

y t+1 = kbest
(  y,  h)   ct

q  (y | x, y 1, . . . , y t)

         (yt+1,i,j, ht+1,i) :

ct =

yt+1,i,j     kbest  y q  (  y|(yt,i, ht,i))

ht+1,i,j = id56cell(ht,i, e(yt,i,j))

         

error-based training

systems are often evaluated by error rather than loss (log loss).

should we train directly to minimize error?

should translation be trained directly on id7 score?

should segmentation be trained on intersection over union?

should cancer screening be trained on recall?

if the model provides probabilities we can do bayesian infer-
ence. but the model might not be su   ciently expressive.

24

label adjustment

we can consider an arbitrary error function err(y,   y) assigning
an error value the true label is y and the system guesses   y.

f [  ,   y[  ]]

  

(cid:88)
f (  y) =
  y   (f ) = argmax
  y   (f ) = argmax
(cid:16)

  y

  y

error(y, f ) = err(y,   y   (f ))

f (  y)

f (  y)      err(y,   y)

(adjusted label)

f  (x).grad[  ,   y] -=   

1[  y   

f  (x)[  ] =   y]     1[  y   

f  (x)[  ] =   y]

(cid:17)

(cid:16)

label adjustment theorem

f  (x).grad[  ,   y] -=   

1[  y   

f  (x)[  ] =   y]     1[  y   

f  (x)[  ] =   y]

theorem: for a continuous and smooth population distri-
bution

(cid:17)

      e(x,y)   pop err(y,   y   (f  (x)))
(cid:88)

e(x,y)   pop

f  (x)[  ] =   y]     1[  y   

1[  y   

(cid:16)

1
 

= lim
    0

  ,  y

26

(cid:17)     f  (x)[  ,   y]

f  (x)[  ] =   y]

intersection over union

in visual detection problems one is typically evaluated by in-
tersection over union.

iou =

|true positives     false positives|
|true positives     false positives| =

p     f n
p + f p

   iou
   fp

   iou
   fn

=

=

   (p     f n )
(p + f p )2 =

   iou
p + f p

   1

p + f p

phrase based id151 (smt)

step i: learn a phrase table     a set of triples (p, q, s) where
    p is a (short) sequence of source words.
    q is a (short) sequence of target words.
    s is a score.

(   au   ,    to the   , .5)

(   au banque   ,    the the bank   , .01)

for a phrase p we will write p.source for the source phrase,
p.target for the target phrase, and p.score for the score.

28

derivations

consider an input sentence x of length t .
we will write x[s : t] for the substring x[s], . . ., x[t     1].
a derivation d from x is a sequence (p1, s1, t1, ), . . ., (pk, sk, tk)
where pk.source = x[sk : tk].

the substrings x[sk : tk] should be disjoint and    cover    x.

for d = [(p1, s1, t1, ), . . ., (pl, sk, tk)] we de   ne

y(d)     p1.target        pk.target
we let d(x) be the set of derivations from x.

29

scoring
for d     d(x) we de   ne a score s(d)

(cid:88)

s(d) =    ln plm(y(d)) +   

pk.score +    distortion(d)

k

where plm(y) is the id203 assigned to string y under a
language model for the target language

and distortion(d) is a measure of consistency of word ordering
between source and target strings as de   ned by the indeces
(s1, t1), . . ., (sk, tk).

30

translation

y(x) = y(d   (x))

d   (x) = argmax
d   d(x)

s(d)

31

end

