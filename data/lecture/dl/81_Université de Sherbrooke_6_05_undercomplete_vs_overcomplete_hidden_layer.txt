neural networks
autoencoder - undercomplete vs. overcomplete hidden layer

hugo larochelle

math for my slides    autoencoders   .
d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

autoencoder
d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: autoencoder, encoder, decoder, tied weights
hugo.larochelle@usherbrooke.ca
    feed-forward neural network trained to reproduce its input at 
= sigm(b + wx)
october 17, 2012
the output layer

h(x) = g(a(x))

october 16, 2012
decoder

2

ck

 x

abstract

= w 
w 
(tied weights)

math for my slides    autoencoders   .

 
 
    f (x)    bx l(f (x)) =pk(bxk   xk)2

= sigm(c + w   h(x))

= sigm(b + wx)

h(x) = g(a(x))

bx = o(ba(x))

w

x

bj

abstract
= sigm(c + w   h(x))

bx = o(ba(x))
l(f (x)) =  pk (xk log(bxk) + (1   xk) log(1  bxk))

 
for binary inputs
encoder
h(x) = g(a(x))

= sigm(b + wx)

undercomplete hidden layer

d  epartement d   informatique
universit  e de sherbrooke

3

hugo larochelle

hugo.larochelle@usherbrooke.ca

topics: undercomplete representation
    hidden layer is undercomplete if smaller than the input layer

october 17, 2012

    hidden layer       compresses       the input
    will compress well only for the 
math for my slides    autoencoders   .
training distribution
    hidden units will be 
    good features for the 
training distribution
    but bad for other 
types of input

abstract

ck

 x

w  = w 
(tied weights)

h(x) = g(a(x))

bj

= sigm(b + wx)

w

x

bx = o(ba(x))

= sigm(c + w   h(x))

4

hugo larochelle

overcomplete hidden layer

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

topics: overcomplete representation
    hidden layer is overcomplete if greater than the input layer

october 17, 2012

math for my slides    autoencoders   .

    no compression in hidden layer
    each hidden unit could copy a 
different input component
    no guarantee that the 
hidden units will extract 
meaningful structure

ck

abstract

 x

w  = w 
(tied weights)

bj
h(x) = g(a(x))

= sigm(b + wx)
w

x

bx = o(ba(x))

= sigm(c + w   h(x))

