feedforward neural network
ift 725 - r  seaux neuronaux

hugo.larochelle@usherbrooke.ca

september 6, 2012
september 6, 2012

d  epartement d   informatique
universit  e de sherbrooke

september 6, 2012
universit  e de sherbrooke
september 6, 2012
artificial neuron
math for my slides    feedforward neural network   .
september 6, 2012
abstract
topics: connection weights, bias, activation function
september 6, 2012
    neuron input activation:

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

abstract

abstract

math for my slides    feedforward neural network   .

    x1 xd b w1 wd
abstract

math for my slides    feedforward neural network   .
abstract
math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    neuron (output) activation
    x1 xd b w1 wd
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    x1 xd b w1 wd
    x1 xd b w1 wd
math for my slides    feedforward neural network   .
    x1 xd b w1 wd
    x1 xd b w1 wd
...
    w
    w
    w
    x1 xd b w1 wd
    x1 xd
    x1 xd
    w
    w
         are the connection weights
    {
    {
    w
    {
    w
        is the neuron bias 
    {
    w
    g(  ) b
    {
    g(  ) b
    g(  ) b
    {
             is called the activation function 
    g(  ) b
    g(  ) b
    {
    g(  ) b
    {
    h(x) = g(a(x))
    h(x) = g(a(x))
    g(  ) b
    h(x) = g(a(x))

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

1

2

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

-1
math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

-1

0

    a(x)=bipiwixi =bi+w>x
    h(x)=g(a(x))=g(bipiwixi)

artificial neuron
topics: connection weights, bias, activation function

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

abstract

0

    w
    {
range determined 
by 
    g(  ) b

    w
    {

1

y1

0

-1

    a(x) = bipi wixi = bi + w>x
    h(x) = g(a(x)) = g(bipi wixi)

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    w

1

-1

x2

    w
    {
bias     only 
    g(  ) b
biais
changes the 
position of 
.5
the riff

y1

1 1

3

-1

0

1

0

-1

x1

(from pascal vincent   s slides)

artificial neuron

math for my slides    feedforward neural network   .

topics: linear activation function
    performs no input
squashing
    not very interesting...

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    w
    {
    g(a) = a
    sigm(a) =

1

4

1+exp( a)

artificial neuron

math for my slides    feedforward neural network   .

abstract

topics: sigmoid activation function
    squashes the neuron   s
input between 0 and 1
    always positive
    bounded
    strictly increasing

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    w
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

5

math for my slides    feedforward neural network   .

artificial neuron
    a(x) = b +pi wixi = b + w>x
topics: hyperbolic tangent (      tanh      ) activation function
    h(x) = g(a(x)) = g(b +pi wixi)
    squashes the neuron   s
input between -1 and 1
    x1 xd b w1 wd
    can be positive or
    w
negative 
    {
    bounded
    strictly increasing
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b

1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

6

    h(x) = g(a(x)) = g(b +pi wixi)

topics: recti   ed linear activation function
    bounded below by 0
(always positive)
    not upper bounded
    strictly increasing
    tends to give neurons
with sparse activities 

artificial neuron
    x1 xd b w1 wd
    w
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(  ) b

1

7

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

1

    with sigmoid, can interpret neuron as estimating 
    also known as id28 classi   er
    if greater than 0.5,

    g(a) = a
    g(a) = sigm(a) =
artificial neuron
    g(a) = tanh(a) = exp(a) exp( a)
r  eseaux de neurones
topics: capacity, decision boundary of neuron
    g(a) = max(0, a)
    could do binary classi   cation:
    g(a) = reclin(a) = max(0, a)
    p(y = 1|x)
    la puissance expressive des r  eseaux de neurones
    g(  ) b
    w (1)
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    f (x) = o   b(2) + w(2)>x   
    o(a) = softmax(a) =h exp(a1)

decision boundary is linear
xj h(x)i w(2)
x2
i,j

pc exp(ac) . . .

(similar idea can
 apply with tanh)

deux couches

trois couches

    otherwise, predict

    p(y = c|x)

(from pascal vincent   s slides)

predict class 1

class 0

b(1)
i

r2

r1

x1

x2

x1

x2

8

i

artificial neuron

topics: capacity of single neuron
    can solve linearly separable problems

or (x1, x2)

and (x1, x2)

and (x1, x2)

)
2
x

,

1

0

1
)
2
x

,

0

)
2
x

,

1

0

0

1

(x1
xor (x1, x2)

0

(x1

1

0

(x1

1

xor (x1, x2)

1

)

)
2
x

,

1

21

9

1

)
2
x

,

artificial neuron

1
)
2
x

)
2
x

1

,

,

0

topics: capacity of single neuron
    can   t solve non linearly separable problems...

0

0

1

0

1

0

(x1

0

(x1

1

(x1
xor (x1, x2)

1

)
2
x

,

0

?

0

(x1

1

xor (x1, x2)

)
2
x

,
1
x
(
d
n
a

1

0

0

1

and (x1, x2)

    ... unless the input is transformed in a better representation
figure 1.8     exemple de mod  lisation de xor par un r  seau    une couche cach  e. en
haut, de gauche    droite, illustration des fonctions bool  ennes or(x1, x2), and (x1, x2)

10

i

i

1

i,j

i,j

b(2)

exp(2a)+1

exp(2a)+1

exp(2a)+1

exp(2a)+1

    h(x) = g(a(x))

exp(a)+exp( a) = exp(2a) 1

hugo.larochelle@usherbrooke.ca

exp(a)+exp( a) = exp(2a) 1
    w (1)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = sigm(a) =
    g(a) = reclin(a) = max(0, a)
    g(a) = a
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = sigm(a) =
    g(a) = a
    h(x) = g(a(x))
    g(a) = max(0, a)
neural network
    g(a) = max(0, a)
    g(  ) b
september 6, 2012
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    g(a) = max(0, a)
    g(a) = sigm(a) =
1
    g(a) = max(0, a)
    g(a) = sigm(a) =
    g(a) = max(0, a)
    g(a) = tanh(a) = exp(a) exp( a)
1+exp( a)
1+exp( a)
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = reclin(a) = max(0, a)
    w (1)
xj h(x)i w(2)
b(1)
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
b(2)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
topics: single hidden layer neural network
i
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    p(y = 1|x)
    f (x) = g(out)(b(2) + w(2)>x)
    h(x) = g(a(x))
    g(a) = max(0, a)
    hidden layer input activation:
    g(  ) b
    g(a) = max(0, a)
xj h(x)i w(2)
    g(a) = max(0, a)
    p(y = 1|x)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
    g(  ) b
    g(  ) b
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    g(a) = reclin(a) = max(0, a)
    g(a) = max(0, a)
xj h(x)i w(2)
    w (1)
b(1)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
math for my slides    feedforward neural network   .
    g(a) = reclin(a) = max(0, a)
i +pj w (1)
xj h(x)i w(2)
b(1)
    w (1)
b(2)
    g(  ) b
b(1)
xj h(x)i w(2)
    w (1)
math for my slides    feedforward neural network   .
i
i,j
b(2)
    f (x) = o(b(2) + w(2)>x)
i
i
...
...
xj h(x)i w(2)
b(1)
    w (1)
i
b(2)
    hidden layer activation:
    a(x) = b +pi wixi = b + w>x
1
    w (1)
i
i
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    f (x) = o(b(2) + w(2)>x)
    a(x) = b +pi wixi = b + w>x
xj h(x)i
    h(x) = g(a(x))
    h(x) = g(a(x))
    g(  ) b
i,j
    h(x) = g(a(x))
    h(x) = g(a(x))
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    h(x) = g(a(x)) = g(b +pi wixi)
i pj w (1)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
b(1)
    w (1)
    g(  ) b
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i +pj w (1)
i pj w (1)
    output layer activation:
    w (1)
b(1)
i pj w (1)
i
...
...
i
    f (x) = o   b(2) + w(2)>h(1)x   
1
1
    f (x) = o   b(2) + w(2)>x   
b(1)
    w (1)
xj h(x)i
    x1 xd
    x1 xd
    h(x) = g(a(x))
    o(x) = g(out)(b(2) + w(2)>x)
    h(x) = g(a(x))
    o(x) = g(out)(b(2) + w(2)>x)
i
i,j
    o(x) = g(out)(b(2) + w(2)>x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
output activation function
    w
    w
    h(x) = g(a(x))
1
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    {

xj h(x)i

b(2)

b(1)
i

i,j

i,j

i,j

1

i,j

11

i

1

i

neural network

topics: single hidden layer neural network
r  eseaux de neurones

x2

1

z

1

0

-1

-1

0

0

-1

x1

1

zk

y1

1

0

-1

-1

0

1

0

-1

x1

x2

1

biais

.5

sortie k
wkj

y2

1

-1

0

cach  ee j
wji
entr  ee i

-1

y1

.7

-.4

-1.5

y2

1 1

1

1

x1

x2

x2

1

0

1

-1

x1

-1

0

(from pascal vincent   s slides)

x2

1

z=-1

2

12

deux couches

neural network

r1

topics: single hidden layer neural network

x1

x2

x2

trois couches

...

r2

x1

x2

r2

r1

r2
r1

x1

x1

(from pascal vincent   s slides)

13

r  eseaux de neurones

neural network
    la puissance expressive des r  eseaux de neurones
topics: single hidden layer neural network

z1

x2

y2

z1

x1

y3

y1

y1

y2

y3

y4

y4

x1

x2

(from pascal vincent   s slides)

14

neural network

topics: universal approximation
    universal approximation theorem (hornik, 1991):

          a single hidden layer neural network with a linear output unit can approximate 

any continuous function arbitrarily well, given enough hidden units      

    the result applies for sigmoid, tanh and many other hidden 
layer id180

    this is a good result, but it doesn   t mean there is a learning 
algorithm that can    nd the necessary parameter values!

15

i

i,j

i,j

b(2)

b(1)
i

b(1)
i

xj h(x)i w(2)

neural network

xj h(x)i w(2)
topics: softmax activation function
    for multi-class classi   cation:

    g(a) = reclin(a) = max(0, a)
    g(  ) b
    w (1)
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    f (x) = o   b(2) + w(2)>x   
    o(a) = softmax(a) =h exp(a1)

    we need multiple outputs (1 output per class)
    we would like to estimate the id155 

    w (1)
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    f (x) = o   b(2) + w(2)>x   
i,j xj   
i +pj w (1)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>

    we use the softmax activation function at the output:

pc exp(ac) . . .

    p(y = c|x)

    p(y = c|x)

exp(ac )

    strictly positive
    sums to one

    predicted class is the one with highest estimated id203

1

16

feedforward neural network
hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

1

...

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

exp(ac )

    p(y = c|x)

september 6, 2012

    x1 xd b w1 wd
neural network
    w
    p(y = c|x)
    p(y = c|x)
    p(y = c|x)
    {
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
topics: multilayer neural network
pc exp(ac) . . .
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    g(a) = a
    f (x)
    could have l hidden layers:
    f (x)
pc exp(ac) . . .
    p(y = c|x)
    p(y = c|x)
    f (x)
    p(y = c|x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    g(a) = sigm(a) =
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    layer input activation for k>0
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
1+exp( a)
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
exp(ac )
pc exp(ac) . . .
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
pc exp(ac) . . .
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
1
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
...
...
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
    p(y = c|x)
    f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    h(k)(x) = g(a(k)(x))
    p(y = c|x)
    h(k)(x) = g(a(k)(x))
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
    f (x)
    h(k)(x) = g(a(k)(x))
    g(a) = max(0, a)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    hidden layer activation (k from 1 to l):
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
pc exp(ac) . . .
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
exp(ac )
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    f (x)
    a(x) = b +pi wixi = b + w>x
1
...
...
    h(k)(x) = g(a(k)(x))
    g(a) = reclin(a) = max(0, a)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(x) = b +pi wixi = b + w>x
    f (x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    g(  ) b
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    output layer activation (k=l+1):
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
1
...
...
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    w (1)
b(1)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
xj h(x)i
    x1 xd
    x1 xd
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
i
i,j
    h(k)(x) = g(a(k)(x))
    h(k)(x) = g(a(k)(x))
    w
    w
    h(x) = g(a(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

math for my slides    feedforward neural network   .

exp(ac )

exp(ac )

exp(ac )

17

math for my slides    feedforward neural network   .

neural network
le syst  me visuel humain 
topics: parallel with the visual cortex
       pourquoi%ne%pas%s   inspirer%du%cerveau%pour%faire%de%la%vision!%

edges

...

nose

mouth

eyes

face

18

biological neurons

topics: synapse, axon, dendrite
    we estimate around 1010 and 1011 the number of neurons in 
the human brain:
    they receive information from other neurons through their dendrites
    the       process       the information in their cell body (soma)
    they send information through a       cable       called an axon
    the point of connection between the axon branches and other neurons    dendrites 

are called synapses

19

biological neurons

topics: synapse, axon, dendrite
52

3 outline of the visual system

signal
transmission

c

o

m

p

signal
reception

u

t

a

t

i

o

n

synapses

axon

cell
body

dendrites

other neurons

fig. 3.1: a schematic diagram of information-processing in a neuron. flow of information is from
right to left.

(from hyv  rinen, hurri and hoyer   s book)

20

biological neurons

topics: action potential,    ring rate
    an action potential is an electrical impulse that travels through 
the axon:
    this is how neurons communicate
    it generates a       spike       in the electric potential (voltage) of the axon
    an action potential is generated at neuron only if it receives enough (over some 

threshold) of the       right       pattern of spikes from other neurons

    neurons can generate several such spikes every seconds:

    the frequency of the spikes, called    ring rate, is what characterizes the activity of a 

neuron
- neurons are always    ring a little bit, (spontaneous    ring rate), but they will    re more, given 

the right stimulus

21

biological neurons

topics: action potential,    ring rate
    firing rates of different input neurons combine to in   uence 
the    ring rate of other neurons:
    depending on the dendrite and axon, a neuron can either work to increase 

(excite) or descrease (inhibit) the    ring rate of another neuron

    this is what arti   cial neurons approximate:

    the activation corresponds to a       sort of          ring rate
    the weights between neurons model whether neurons excite or inhibit each other
    the activation function and bias model the thresholded behavior of action 

potentials

22

biological neurons

hubel & wiesel experiment

http://www.youtube.com/watch?v=8vdff3egwfg&feature=related

23

conclusion

    we have seen the most common: 

    id180
    network topologies (layer-wise)

    we could easily have designed more complicated activation 
functions and topologies:
    could get more inspiration from neuroscience...

    however, those discussed here tend to work    ne

24

