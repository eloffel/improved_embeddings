natural language processing:
neural network language model
ift 725 - r  seaux neuronaux

id38

topics: id38
    a language model is a probabilistic model that assigns 
probabilities to any sequence of words
                             p(w1, ... ,wt)
    id38 is the task of learning a language model that assigns high 

probabilities to well formed sentences

    plays a crucial role in id103 and machine translation systems

       une personne intelligente       

?

       a person smart       

       a smart person       

2

id38

topics: id38
    an assumption frequently made is the nth order markov 
assumption
            p(w1, ... ,wt) =     p(wt | wt   (n   1) , ... ,wt   1)
    the tth word was generated based only on the n   1 previous words
    we will refer to wt   (n   1) , ... ,wt   1 as the context

t=1

t

3

id38

topics: id165 model
    an id165 is a sequence of n words 

    unigrams (n=1):       is      ,       a      ,       sequence      , etc.
    bigrams (n=2):  [      is      ,       a       ],  [      a      ,       sequence       ], etc.
    trigrams (n=3):  [      is      ,       a      ,       sequence      ], [       a      ,       sequence      ,       of      ], etc.

    id165 models estimate the conditional from id165s counts
p(wt | wt   (n   1) , ... ,wt   1) = count(wt   (n   1) , ... ,wt   1, wt)
                                    count(wt   (n   1) , ... ,wt   1,    )
    the counts are obtained from a training corpus (a data set of word text)

4

id38

topics: id165 model
    issue: data sparsity

    we want n to be large, for the model to be realistic
    however, for large values of n, it is likely that a given id165 will not have been 

observed in the training corpora
    smoothing the counts can help

- combine count(w1 , w2 , w3 , w4), count(w2 , w3 , w4), count(w3 , w4), and count(w4) to 

estimate p(w4 |w1, w2, w3)

    this only partly solves the problem

5

bengio, ducharme, vincent and jauvin

neural network language model
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

index for

wt 1

6

bengio, ducharme, vincent and jauvin

neural network language model
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

index for

wt 1

6

bengio, ducharme, vincent and jauvin

neural network language model
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

index for

wt 1

6

bengio, ducharme, vincent and jauvin

neural network language model
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

index for

wt 1

6

neural network language model
topics: neural network language model
    can potentially generalize to contexts not seen in training set

    example: p(       eating        |        the       ,        cat       ,        is       ) 

-

-

imagine 4-gram [       the       ,        cat       ,        is       ,         eating        ] is not in training corpus, 
but [       the       ,        dog       ,        is       ,         eating        ] is
if the word representations of        cat        and        dog        are similar, then the neural network will 
be able to generalize to the case of        cat        

- neural network could learn similar word representations for those words based on other 

4-grams:
                          [       the       ,        cat       ,        was       ,         sleeping        ] 
                          [       the       ,        dog       ,        was       ,         sleeping        ]

7

math for my slides    natural language processing   .

math for my slides    natural language processing   .

november 13, 2012
hugo larochelle

abstract

d  epartement d   informatique
universit  e de sherbrooke

neural network language model
topics: word representation gradients
hugo.larochelle@usherbrooke.ca
    we know how to propagate gradients 
in such a network
    we know how to compute the gradient for the 
n 1xi=1

linear activation of the hidden layer

c(w) (= c(w)      rc(w)l

november 13, 2012

1(wt i=w) w>i ra(x)l

bengio, ducharme, vincent and jauvin

rc(w)l =

i-th output = p(wt = i | context)

abstract

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

    let   s note the submatrix connecting wt   i and the 

hidden layer as wi

    the gradient wrt c(w) for any w is

c(w) (= c(w)      rc(w)l

wn-1

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

c(wt 1)

w2 w1
c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

. . .

c

index for

rc(w)l =

n 1xi=1

1(wt i=w) w>i ra(x)l

bengio, ducharme,
neural network and c(i) is the i-th word feature vector.
vincent and jauvin, 2003

figure 1: neural architecture: f (i,wt 1,       ,wt n+1) = g(i,c(wt 1),       ,c(wt n+1)) where g is the
parameters of the mapping c are simply the feature vectors themselves, represented by a |v|    m
matrix c whose row i is the feature vector c(i) for word i. the function g may be implemented by a
feed-forward or recurrent neural network or another parametrized function, with parameters   . the
overall parameter set is    = (c,  ).
training is achieved by looking for    that maximizes the training corpus penalized log-likelihood:

l =

1
t    t

log f (wt,wt 1,       ,wt n+1;  ) +r(  ),

8

abstract
abstract
abstract
abstract

math for my slides    natural language processing   .
math for my slides    natural language processing   .
math for my slides    natural language processing   .
math for my slides    natural language processing   .

neural network language model
   
   
topics: word representation gradients
   
   
    example:    [       the       ,        dog       ,        and       ,         the       ,         cat        ] 
   
   
w7
w3
w6
   
n 1xi=1
   
n 1xi=1
n 1xi=1
1(wt i=w) w>i ra(x)l
n 1xi=1
rc(w)l =
1(wt i=w) w>i ra(x)l
=21 =3 =14 =21
rc(w)l =
1(wt i=w) w>i ra(x)l
rc(w)l =
1(wt i=w) w>i ra(x)l
rc(w)l =

c(w) (= c(w)      rc(w)l
c(w) (= c(w)      rc(w)l
c(w) (= c(w)      rc(w)l
c(w) (= c(w)      rc(w)l

w4

w5

    w1 w2 wn 1
    w1 w2 wn 1
    the loss is l =     log p(       cat        |        the       ,        dog       ,        and       ,         the       )
    w1 w2 wn 1
    w1 w2 wn 1
    rc(3)l = w>3 ra(x)l
    rc(3)l = w>3 ra(x)l
     
    rc(3)l = w>3 ra(x)l
    rc(3)l = w>3 ra(x)l
    rc(14)l = w>2 ra(x)l
     
    rc(14)l = w>2 ra(x)l
    rc(14)l = w>2 ra(x)l
    rc(14)l = w>2 ra(x)l
     
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
                       for all other words w 
    rc(w)l = 0
    rc(w)l = 0
    rc(w)l = 0
    rc(w)l = 0
    only need to update the representations c(3), c(14) and 
c(21),

9

neural network language model
topics: performance evaluation
    in id38, a common evaluation metric is the 
perplexity
    it is simply the exponential of the average negative log-likelihood

    evaluation on brown corpus

    id165 model (kneser-ney smoothing): 321
    neural network language model: 276
    neural network + id165: 252

bengio, ducharme,
vincent and jauvin, 2003

10

neural network language model
topics: performance evaluation
    a more interesting (and less straightforward) way of 
evaluating a language model is within a particular application
    does a language model improve the performance of a machine translation or 

id103 system

    later work has shown improvements in both cases

    connectionist id38 for large vocabulary continuous speech 

recognition
schwenk and gauvain, 2002 

    continuous-space language models for id151

schwenk, 2010

11

neural network language model
topics: hierarchical output layer
    issue: output layer is huge

    we are dealing with vocabularies with a size d in the hundred thousands
    computing all output layer units is very computationally expensive

    solution: use a hierarchical (tree) output layer

    de   ne a tree where each leaf is a word 
    neural network assigns probabilities of branching from a parent to any child
    the id203 of a word is thus the product of each branching probabilities from 

the root to the word   s leaf

    if the tree is binary and balanced, computing word 
probabilities is in o(log2 d)

12

neural network language model
topics: hierarchical output layer
    example: [       the       ,        dog       ,        and       ,         the       ,         cat        ]

       the       
       dog       
        and       
        the        

.
.
.

.
.
.

p(       cat        | context) = 

1

2

3

4

5

6

7

       dog       

       the       

       and       

       cat       

       he       

       have       

       be       

       oov       

13

neural network language model
topics: hierarchical output layer
    example: [       the       ,        dog       ,        and       ,         the       ,         cat        ]

       the       
       dog       
        and       
        the        

p(       cat        | context) = 

.
.
.

.
.
.

v

1

2

3

4

5

6

7

       dog       

       the       

       and       

       cat       

       he       

       have       

       be       

       oov       

13

neural network language model
topics: hierarchical output layer
    example: [       the       ,        dog       ,        and       ,         the       ,         cat        ]

       the       
       dog       
        and       
        the        

p(       cat        | context) =  p(branch left at 1| context) 

x p(branch right at 2| context) 
x p(branch right at 3| context) 

.
.
.

.
.
.

v

1

2

3

4

5

6

7

       dog       

       the       

       and       

       cat       

       he       

       have       

       be       

       oov       

14

neural network language model
topics: hierarchical output layer
    example: [       the       ,        dog       ,        and       ,         the       ,         cat        ]

       the       
       dog       
        and       
        the        

.
.
.

.
.
.

p(       cat        | context) = (1-p(branch right at 1| context)) 
x p(branch right at 2| context) 
x p(branch right at 3| context) 

v

1

2

3

4

5

6

7

       dog       

       the       

       and       

       cat       

       he       

       have       

       be       

       oov       

15

neural network language model
topics: hierarchical output layer
    example: [       the       ,        dog       ,        and       ,         the       ,         cat        ]

       the       
       dog       
        and       
        the        

p(       cat        | context) =  (1 - sigm(b1 + v1,   h(x)))

x sigm(b2 + v2,   h(x))
x sigm(b5 + v5,   h(x))

.
.
.

.
.
.

v

1

2

3

4

5

6

7

       dog       

       the       

       and       

       cat       

       he       

       have       

       be       

       oov       

16

neural network language model
topics: hierarchical output layer
    how to de   ne the word hierarchy

    can use a randomly generated tree

-

this is likely to be suboptimal

    can use existing linguistic resources, such as id138
- hierarchical probabilistic neural network language model

morin and bengio, 2005
they report a speedup of 258x, with a slight decrease in performance
    can learn the hierarchy using a recursive partitioning strategy

-

- a scalable hierarchical distributed language model

mnih and hinton, 2008
similar speedup factors are reported, without a performance
decrease

-

17

neural network language model
topics: hierarchical output layer
    how to de   ne the word hierarchy

    can use a randomly generated tree

-

this is likely to be suboptimal

    can use existing linguistic resources, such as id138
- hierarchical probabilistic neural network language model

morin and bengio, 2005
they report a speedup of 258x, with a slight decrease in performance
    can learn the hierarchy using a recursive partitioning strategy

-

- a scalable hierarchical distributed language model

mnih and hinton, 2008
similar speedup factors are reported, without a performance
decrease

-

17

neural network language model
topics: hierarchical output layer
    how to de   ne the word hierarchy

    can use a randomly generated tree

-

this is likely to be suboptimal

    can use existing linguistic resources, such as id138
- hierarchical probabilistic neural network language model

morin and bengio, 2005
they report a speedup of 258x, with a slight decrease in performance
    can learn the hierarchy using a recursive partitioning strategy

-

- a scalable hierarchical distributed language model

mnih and hinton, 2008
similar speedup factors are reported, without a performance
decrease

-

17

neural network language model
topics: hierarchical output layer
    how to de   ne the word hierarchy

    can use a randomly generated tree

-

this is likely to be suboptimal

    can use existing linguistic resources, such as id138
- hierarchical probabilistic neural network language model

morin and bengio, 2005
they report a speedup of 258x, with a slight decrease in performance
    can learn the hierarchy using a recursive partitioning strategy

-

- a scalable hierarchical distributed language model

mnih and hinton, 2008
similar speedup factors are reported, without a performance
decrease

-

17

conclusion
    we discussed the task of id38 

    we saw how to tackle this problem with a neural network that 
learning word representations
    word representations can help the neural network to generalize to new 
contexts

    we discussed ways of speeding up computations using a 
hierarchical output layer

18

