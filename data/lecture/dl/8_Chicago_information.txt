ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

id205

1

id28 as a major advance

      = argmin

  

e(x,y)   train (y     f  (x))2

switched to

q  (y|x) = softmax
      = argmin

  y

  

f  (  y|x)
e(x,y)   train     log q  (y|x)

2

binary classi   cation

we have a population distribution over (x, y) with y     {   1, 1}.

we compute a single number f  (x) where

for f  (x)     0 predict y = 1

for f  (x) < 0 predict y =    1

3

softmax for binary classi   cation

q  (y|x) =

1
z

eyf (x)

=

=

=

eyf (x)

eyf (x) + e   yf (x)

1

1 + e   2yf (x)

1

1 + e   m(y)

m(y|x) = 2yf (x) is the margin

4

id28 for binary classi   cation

      = argmin

e(x,y)   train ln 1/q  (y|x)

  

(cid:16)

1 + e   m(y|x)(cid:17)

(cid:16)
(cid:16)

ln

ln

e(x,y)   train ln

= argmin

1 + e   m(y|x)(cid:17)     0 for m(y|x) >> 1
1 + e   m(y|x)(cid:17)        m(y|x)

  

for m(y|x) <<    1

5

log loss vs. hinge loss

log loss:
      = argmin

  

e(x,y)   train     ln q  (y|x)

(cid:16)

1 + e   m(y|x)(cid:17)

= argmin

  

e(x,y)   train ln

binary case

e(x,y)   train max(0, 1     m(y|x))

hinge loss:

      = argmin

  

m(y|x) = min
  y(cid:54)=y

f (y|x)     f (  y|x)

6

log loss vs. hinge loss

we will show that log loss is a consistent id203 estimator.
for log loss, and for in   nite training data, q   (y|x) is the true
population id155.

hinge loss is a consistent classi   er but not a consistent proba-
bility estimator.

f   (y   |x) = 1
f   (  y|x) = 0 for   y (cid:54)= y   

7

id178

consider a id203 distribution pop on a    nite set s.

consider a code c assigning a bit string code word c(y1, . . . , yb)
to each possible batch of b elements with yi     pop.

source coding theorem: as b         the optimal coding uses
exactly

h(pop) = ey   pop     log2 pop(y)

bits per batch element.

8

pre   x free codes

let s be a    nite set.

let c be assignment of a bit string c(y) to each y     s.

c is called pre   x-free if for x (cid:54)= y we have that c(x) is not a
pre   x of c(y).

a concatenation of sequence of pre   x-free code words can be
uniquely segmented (parsed) back into a sequence of code
words.

9

pre   x-free codes as trees and as probabilities

a pre   x-free code de   nes a binary branching tree     branch
on the    rst code bit, then the second, and so on.

the leaves of this tree are labeled with the elements of s.

the code de   nes a id203 distribution on s by randomly
selecting branches.

we have qc(y) = 2   |c(y)|.

10

the source coding theorem

(1) there exists a pre   x-free code c such that
|c(y)| <= (    log2 pop(y)) + 1
ey   pop|c(y)|     h(pop) + 1

and hence

(2) for any pre   x-free code c

ey   pop |c(y)|     h(pop)

11

code construction

we construct a code by iterating over y     s in order of de-
creasing id203 (most likely    rst).

for each y select a code word c(y) (a tree leaf) with length
(depth)

|c(y)| = (cid:100)    log2 pop(y)(cid:101)

and where c(y) is not an extension of (under) any previously
selected code word.

12

at any point before coding all elements of s we have

code existence proof

(cid:88)

2   |c(y)|     (cid:88)

y   de   ned

y   de   ned

pop(y) < 1

therefore there exists an in   nite descent into the tree that
misses all previous code words.

hence there exists a code word c(x) not under any previous
code word with |c(x)| = (cid:100)    log2 pop(y)(cid:101).
furthermore c(x) is at least as long as all previous code words
and hence c(x) is not a pre   x of any previously selected code
word.

13

hu   man coding

maintain a list of trees t1, . . . , tn .

inititally each tree is just one root node labeled with an element
of s.

each tree ti has a weight equal to the sum of the probabilities
of the nodes on the leaves of that tree.

repeatedly merge the two trees of lowest weight into a single
tree until all trees are merged.

14

optimality of hu   man coding

theorem: the hu   man code t for pop is optimal     for
any other tree t(cid:48) we have d(t ; pop)     d(t(cid:48); pop).
proof: the algorithm maintains the invariant that there ex-
ists an optimal tree including all the subtrees on the list.

to prove that a merge operation maintains this invariant we
consider any tree containing the given subtrees.
consider the two subtrees ti and tj of minimal weight. with-
out loss of generality we can assume that ti is at least as deep
as tj.
swapping the sibling of ti for tj brings ti and tj together
and can only improve the average depth.

15

optimality of hu   man coding

why the swap operation cannot increase id178. ...

16

log loss has both a conditional and an unconditional version.

back to log loss

      = argmin

  

      = argmin

  

e(x,y)   pop     log q  (y|x)

ey   pop     log q  (y)

conditional test loss can often be made small (mnist, speech
recognition) but can also be inherently large (image coloriza-
tion).
unconditional log loss is typically inherently large (language
modeling).

17

log loss is cross id178

h(pop)

h(pop, q)

= ey   pop     log pop(y)
.
= ey   pop     log q(y)
.

ey   pop     log q  (y) = h(pop, q  )

e(x,y)   pop     log q  (y|x) = ex   pop h(pop(y|x), q  (y|x))

18

kl divergence

kl(pop, q)

pop(y)
q(y)

.
= ey   pop log2
= ey   pop log pop(y)     log q(y)

= (cid:0)ey   pop     log q(y)(cid:1)    (cid:0)ey   pop     log pop(y)(cid:1)

= h(pop, q)     h(p )

19

jensen   s inequality

for f convex (upward curving) we have

e[f (x)]     f (e[x])

20

kl divergence

q(y)
p (y)

q(y)
p (y)

kl(p, q) = ey   p     log
        log ex   p
(cid:88)
(cid:88)

=     log

y

=     log

q(y)

p (y)

q(y)
p (y)

y

= 0

21

fundamentals

kl(pop, q)     0

h(pop, q) = h(pop) + kl(pop, q)

    h(pop)

argmin

h(pop, q) = pop

q

ex   pop h(pop(y|x), q(y|x)) = pop(y|x)

argmin
q(y|x)

22

asymmetry of cross id178

consider

      = argmin

  

      = argmin

  

h(p, q  )

(1)

h(q  , p )

(2)

for (1) q   must cover all of the support of p .

for (2) q   concentrates all mass on the point maximizing p .

23

asymmetry of kl divergence

consider

  

      = argmin
= argmin
      = argmin
= argmin

  

  

  

kl(p, q  )

h(p, q  )

(1)

kl(q  , p )
h(q  , p )     h(q  )

(2)

if q   is not universally expressive we have that (1) still forces
q   to cover all of p (or else the kl divergence is in   nite)
while (2) allows q   to be restricted to a single mode of p (a
common outcome).

unsupervised learning

unsupervised learning is sometimes equated with uncondi-
tional log loss (density estimation).

      = argmin

  

ey   pop     log q  (y)

25

unsupervised learning

26

unsupervised learning

by    unsupervised learning    we will mean learning from mas-
sively available data. this is not a mathematical de   nition.

massive: images, audio, text, video, click-through data.

less massive: car control data, stereo image pairs, closed
captioned video, captioned images.

big: manually annotated images or audio.

small: manually annotated text     parse trees, named enti-
ties, semantic roles, coreference, entailment.

smallest: manually annotated text in an obscure language.

27

colorization

      = argmin

  

e(x,y)   pop     log q  (y|x)

we have massive data for colorization.

but any colorization is inevitably a guess.

28

di   erential id178

consider a continuous density p(x). for example

p(x) =

   x2
2  2

e

1   
2     

di   erential id178 is often de   ned as

h(p)

.
=

p(x)dx

(cid:90) (cid:18)

(cid:19)

ln

1

p(x)

29

finite di   erential id178 is not meaningful

(cid:90) (cid:32)

   

(cid:33)

x2
2  2
1
2

h(n (0,   )) = +

ln(

2    ) +
   
= ln(  ) + ln(

2  ) +

p(x)dx

but if we take y

= x/2 we get h(y) = h(x)     ln 2.
.

also for    << 1, we get h(p) < 0

hence di   erential id178 then depends on the choice of units
    a distributions on lengths will have a di   erent id178 when
measuring in inches than when measuring in feet.

30

di   erential id178 is always in   nite

consider quantizing the the real numbers into bins.

a continuous id203 densisty p assigns a id203 p(b)
to each bin.

as the bin size decreases toward zero the id178 of the bin
distribution increases toward    .

a meaningful convention is that h(p) = +    for any contin-
uous density p.

31

di   erential kl-divergence is meaningful

(cid:90) (cid:18)

(cid:19)

ln

p(x)
q(x)

kl(p, q) =

p(x)dx

this integral can be computed by dividing the real numbers
into bins and computing the kl divergence between the dis-
tributions on bins.

the kl divergence between the bin distribution typically ap-
proaches a    nite limit as the bin size goes to zero.

32

kl-divergence can also be in   nite

kl(p, q) = ex   p log

p(x)
q(x)

in either the discrete or continuous case, if a set is assigned
nonzero id203 by p but zero id203 by q then kl(p, q) =
+   .

if every set assigned nonzero id203 by p is also assigned
nonzero id203 by q then we say that p is absolutely con-
tinuous with respect to q.

33

random variables

we consider variables where a single draw form the population
determines a value for each variable.

this is the formal de   nition of a    random variable   .

each random variable has a id203 distribution de   ned by
the distribution on the population.

we write h(x) for the id178 of the distribution on x.

34

mutual information

for two random variables x and y there is a distribution on
pairs (x, y) determined by the population distribution.

mutual information concerns the relationship between the dis-
tribution on (x, y) and the marginal distributions on x and
y.

for the discrete case we can write.

i(x, y)

= h(x) + h(y)     h(x, y)
.

this can be viewed as a quantity of non-independence     in-
dependent variables have zero mutual information.

35

conditional id178

for the discrete case conditional id178 h(y|x) is de   ned by

h(y|x)

pop(y|x)     log pop(y|x)

.
=

pop(x)

(cid:88)

(cid:88)
= ex   pop ey   pop|x     log pop(y|x)
= ex   pop h(pop(y|x))

x

y

36

more identities

for the discrete case we have.

i(x, y) = h(x)     h(x|y)

= h(y)     h(y|x)

= kl(pop(x, y), pop(x)    pop(y))

the last identity can be taken as a de   nition of i(x, y) in the
continuous case.

37

end

