neural networks
deep learning - deep belief network

deep learning
d  epartement d   informatique
2
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

deep learning

deep belief network

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle

hugo larochelle
d  epartement d   informatique
deep learning
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

october 25, 2012

hugo larochelle
october 25, 2012

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
universit  e de sherbrooke

topics: deep belief network
    the idea of pre-training came from work on deep belief 
networks (dbns)
abstract
math for my slides    deep learning   .
hugo.larochelle@usherbrooke.ca
    it is a generative model that mixes undirected
...
and directed connections between variables
    top 2 layers    distribution                 is an rbm
    other layers form a id110:
math for my slides    deep learning   .

    x h(1) h(2) h(3)
math for my slides    deep learning   .
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)

dbn   s graphical model
    x h(1) h(2) h(3)

    x h(1) h(2) h(3)
...

math for my slides    deep learning   .

october 25, 2012

abstract

abstract

math for my slides    deep learning   .

    x h(1) h(2) h(3)
    p(h(2), h(3))
the conditional distributions of a layers given the one
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    x h(1) h(2) h(3)
above it are  
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)

this is referred to as a sigmoid belief network (sbn)

-

-

    a dbn is not a feed-forward network

math for my slides    deep learning   .

...

    x h(1) h(2) h(3)

math for my slides    deep learning   .

...

    x h(1) h(2) h(3)

deep learning
d  epartement d   informatique
2
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo larochelle
october 25, 2012

topics: deep belief network
    the idea of pre-training came from work on deep belief 
networks (dbns)
abstract
math for my slides    deep learning   .
hugo.larochelle@usherbrooke.ca
    it is a generative model that mixes undirected
...
and directed connections between variables
    top 2 layers    distribution                 is an rbm
    other layers form a id110:
math for my slides    deep learning   .

    x h(1) h(2) h(3)
math for my slides    deep learning   .
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)

dbn   s graphical model
    x h(1) h(2) h(3)

    x h(1) h(2) h(3)
...

math for my slides    deep learning   .

october 25, 2012

abstract

math for my slides    deep learning   .

    x h(1) h(2) h(3)
    p(h(2), h(3))
the conditional distributions of a layers given the one
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    x h(1) h(2) h(3)
above it are  
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)

this is referred to as a sigmoid belief network (sbn)

-

-

    a dbn is not a feed-forward network

math for my slides    deep learning   .

    x h(1) h(2) h(3)

math for my slides    deep learning   .

    x h(1) h(2) h(3)

deep belief network

hugo larochelle

rbm
abstract

october 25, 2012

deep learning

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle
d  epartement d   informatique
deep learning
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
universit  e de sherbrooke

   { i,ui|xui= iuietu>iuj=1i=j}
   {x2rh|x/2r(x)}
   r(x)={x2rh|9wx=pjwjx  ,j}
   9w,t   x(t   )=pt6=t   wtx(t)

...

...

deep learning
d  epartement d   informatique
2
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo larochelle
october 25, 2012

topics: deep belief network
    the idea of pre-training came from work on deep belief 
networks (dbns)
abstract
math for my slides    deep learning   .
hugo.larochelle@usherbrooke.ca
    it is a generative model that mixes undirected
...
and directed connections between variables
    top 2 layers    distribution                 is an rbm
    other layers form a id110:
math for my slides    deep learning   .

    x h(1) h(2) h(3)
math for my slides    deep learning   .
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)

dbn   s graphical model
    x h(1) h(2) h(3)

    x h(1) h(2) h(3)
...

math for my slides    deep learning   .

october 25, 2012

abstract

math for my slides    deep learning   .

    x h(1) h(2) h(3)
    p(h(2), h(3))
the conditional distributions of a layers given the one
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    x h(1) h(2) h(3)
above it are  
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)

this is referred to as a sigmoid belief network (sbn)

-

-

    a dbn is not a feed-forward network

math for my slides    deep learning   .

    x h(1) h(2) h(3)

math for my slides    deep learning   .

    x h(1) h(2) h(3)

deep belief network

hugo larochelle

rbm
abstract

october 25, 2012

deep learning

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

hugo larochelle
d  epartement d   informatique
deep learning
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
universit  e de sherbrooke

   { i,ui|xui= iuietu>iuj=1i=j}
   { i,ui|xui= iuietu>iuj=1i=j}
   {x2rh|x/2r(x)}
   {x2rh|x/2r(x)}
   r(x)={x2rh|9wx=pjwjx  ,j}
   r(x)={x2rh|9wx=pjwjx  ,j}
   9w,t   x(t   )=pt6=t   wtx(t)

sbn

...

...

3

abstract
abstract
abstract

math for my slides    deep learning   .
math for my slides    deep learning   .
math for my slides    deep learning   .

deep belief network

    x h(1) h(2) h(3)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    x h(1) h(2) h(3)
    x h(1) h(2) h(3)
    x h(1) h(2) h(3)
topics: deep belief network
    p(h(2), h(3))
    p(h(2), h(3))
    p(h(2), h(3))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    the full distribution of a dbn is as follows
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    where:
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
-
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(x|h(1)) =qi p(xi|h(1))
    p(x|h(1)) =qi p(xi|h(1))
    p(x|h(1)) =qi p(xi|h(1))
    http://www.cs.toronto.edu/~hinton/adi/index.htm

    to observe a dbn trained on mnist in action:

|h(2))

|h(2))
|h(2))
|h(2))

 
 
 

j
j
j

j

-

-

    as in a deep feed-forward network, training a dbn is hard

    initialization will play a crucial role on the results

j

abstract

|h(2))

deep learning

deep learning
d  epartement d   informatique
4
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
deep learning
october 25, 2012
universit  e de sherbrooke

math for my slides    deep learning   .

hugo larochelle

hugo larochelle

j = 1|h(2)) = sigm(b(1) + w(2)>h(2))

hugo larochelle
hugo.larochelle@usherbrooke.ca

deep belief network
deep learning

deep learning
d  epartement d   informatique
universit  e de sherbrooke

    p(h(1)
october 25, 2012
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
deep learning
    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
hugo larochelle
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
d  epartement d   informatique
math for my slides    deep learning   .
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
universit  e de sherbrooke
    x h(1) h(2) h(3)
abstract
october 25, 2012
october 25, 2012
hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca
math for my slides    deep learning   .
october 25, 2012

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

    x h(1) h(2) h(3)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
topics: deep belief network
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    x h(1) h(2) h(3)
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    this is where the rbm stacking procedure comes from
    p(h(2), h(3))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    idea: improve prior on last layer by
    p(h(1)|h(2)) =qj p(h(1)
|h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
adding another hidden layer
    p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    how do we train these additional layers?
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x) =ph(1) p(x, h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)
...
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    x h(1) h(2) h(3)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

    x h(1) h(2) h(3)

1

hugo larochelle

october 25, 2012

    x h(1) h(2) h(3)
abstract
...

    x h(1) h(2) h(3)

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

abstract

abstract

abstract

|h(2))

...

...

...

...

...

...

...

j

j

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

j

abstract

|h(2))

deep learning

deep learning
d  epartement d   informatique
4
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
deep learning
october 25, 2012
universit  e de sherbrooke

math for my slides    deep learning   .

hugo larochelle

hugo larochelle

j = 1|h(2)) = sigm(b(1) + w(2)>h(2))

hugo larochelle
hugo.larochelle@usherbrooke.ca

deep belief network
deep learning

deep learning
d  epartement d   informatique
universit  e de sherbrooke

    p(h(1)
october 25, 2012
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
deep learning
    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
hugo larochelle
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
d  epartement d   informatique
math for my slides    deep learning   .
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
universit  e de sherbrooke
    x h(1) h(2) h(3)
abstract
october 25, 2012
october 25, 2012
hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca
math for my slides    deep learning   .
october 25, 2012

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

    x h(1) h(2) h(3)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
topics: deep belief network
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    x h(1) h(2) h(3)
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    this is where the rbm stacking procedure comes from
    p(h(2), h(3))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    idea: improve prior on last layer by
    p(h(1)|h(2)) =qj p(h(1)
|h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
adding another hidden layer
    p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    how do we train these additional layers?
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x) =ph(1) p(x, h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)
...
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    x h(1) h(2) h(3)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

    x h(1) h(2) h(3)

1

hugo larochelle

october 25, 2012

    x h(1) h(2) h(3)
abstract
...

    x h(1) h(2) h(3)

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

abstract

abstract

abstract

|h(2))

...

...

...

...

...

...

...

j

j

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

j

abstract

|h(2))

deep learning

deep learning
d  epartement d   informatique
4
universit  e de sherbrooke
hugo larochelle

hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
deep learning
october 25, 2012
universit  e de sherbrooke

math for my slides    deep learning   .

hugo larochelle

hugo larochelle

j = 1|h(2)) = sigm(b(1) + w(2)>h(2))

hugo larochelle
hugo.larochelle@usherbrooke.ca

deep belief network
deep learning

deep learning
d  epartement d   informatique
universit  e de sherbrooke

    p(h(1)
october 25, 2012
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
deep learning
    p(h(1)|h(2)) =qj p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
hugo larochelle
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
d  epartement d   informatique
math for my slides    deep learning   .
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
universit  e de sherbrooke
    x h(1) h(2) h(3)
abstract
october 25, 2012
october 25, 2012
hugo.larochelle@usherbrooke.ca
hugo.larochelle@usherbrooke.ca
math for my slides    deep learning   .
october 25, 2012

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
october 25, 2012
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

    x h(1) h(2) h(3)
    p(h(2), h(3))
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
topics: deep belief network
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
    p(h(1)
    x h(1) h(2) h(3)
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    this is where the rbm stacking procedure comes from
    p(h(2), h(3))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(xi = 1|h(1)) = sigm(b(0) + w(1)>h(1))
    idea: improve prior on last layer by
    p(h(1)|h(2)) =qj p(h(1)
|h(2))
j = 1|h(2)) = sigm(b(1) + w(2)>h(2))
adding another hidden layer
    p(h(1)
    p(x|h(1)) =qi p(xi|h(1))
    how do we train these additional layers?
    p(x, h(1), h(2), h(3)) = p(h(2), h(3)) p(h(1)|h(2)) p(x|h(1))
    p(x) =ph(1) p(x, h(1))
    p(h(2), h(3)) = exp   h(2)>w(3)h(3) + b(2)>h(2) + b(3)>h(3)    /z
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    p(h(1)|h(2)) =qj p(h(1)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)
...
    p(x|h(1)) =qi p(xi|h(1))
    p(x) =ph(1) p(x, h(1))
    p(x, h(1)) = p(x|h(1))ph(2) p(h(1), h(2))
    x h(1) h(2) h(3)
    p(h(1), h(2)) = p(h(1)|h(2))ph(3) p(h(2), h(3))
    x h(1) h(2) h(3)

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

    x h(1) h(2) h(3)

1

hugo larochelle

october 25, 2012

    x h(1) h(2) h(3)
abstract
...

    x h(1) h(2) h(3)

    x h(1) h(2) h(3)

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

math for my slides    deep learning   .

abstract

abstract

abstract

|h(2))

...

...

...

...

...

...

...

j

j

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

