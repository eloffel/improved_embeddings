neural networks
training crfs - pairwise log-factor gradient

   

2

arg min

abstract

math for my slides    training crfs   .

t xt

hugo larochelle

abstract
math for my slides    training crfs   .

hugo.larochelle@usherbrooke.ca

d  epartement d   informatique
universit  e de sherbrooke

machine learning

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
math for my slides    training crfs   .
d  epartement d   informatique
    l(f (x(t);    ), y(t))
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
universit  e de sherbrooke
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
hugo.larochelle@usherbrooke.ca
       (   )
topics: stochastic id119 (sgd)
    ap(yk, yk+1) = 11   k<k vyk,yk+1
   
t xt
1
l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
arg min
      =   1
    algorithm that performs updates after each example
september 13, 2012
   
    initialize     
t xt
              +  
t xt
1
    for n iterations

    9w,t    x(t   ) =pt6=t    wtx(t)
    r(x) ={x2rh|9w x =pj wjx  ,j}
    {x2rh| x /2r(x)}
    { i,ui| xui =  iui et u>i uj = 1i=j}

    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    au(yk) = a(l+1,0)(xk)yk + 1k>1 a(l+1, 1)(xk 1)yk + 1k<k a(l+1,+1)(xk+1)yk
    ap(yk, yk+1) = 11   k<k vyk,yk+1
    ap(yk, yk+1) = 11   k<k vyk,yk+1
   
1
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
   
   
t xt
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
arg min
arg min
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
       (   )
    l(f (x(t);    ), y(t))
for each training example
    (x(t), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
training epoch 
    l(f (x(t);    ), y(t))
      =   1
    f (x)
     
=
      =  r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
iteration over all examples
      =  r   l(f (x(t);    ), y(t))    r      (   )
     
              +      
    (x(t), y(t))
    f (x)
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    to apply this algorithm to a crf, we need
    (x(t), y(t))
    (x(t), y(t))
    {x 2 rd | rxf (x) = 0}
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    the id168
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
    r   l(f (x(t);    ), y(t))
       (   )
    a procedure to compute the parameter gradients
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    the regularizer             (and the gradient                 )
    r      (   )
       (   )
5
      =  r   l(f (x(t);    ), y(t))    r      (   )
    initialization method
    f (x)c = p(y = c|x)
    r      (   )
    (x(t), y(t))

t pt r   l(f (x(t);    ), y(t))    r      (   )

math for my slides    feedforward neural network   .

september 13, 2012

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

t xt

abstract

abstract

arg min

1

1

   

   

   

   

   

-

rw(1, 1)   log p(y|x) =

rw(1, 1)   log p(y|x) =

rw(1, 1)   log p(y|x) =

kxk=1 ra(1,0)(xk)   log p(y|x)  x>k =
kxk=2 ra(1, 1)(xk)   log p(y|x)  x>k 1 =
kxk=2 ra(1, 1)(xk)   log p(y|x)  x>k 1 =
k 1xk=1 ra(1,+1)(xk)   log p(y|x)  x>k+1 =
k 1xk=1 ra(1,+1)(xk)   log p(y|x)  x>k+1 =

kxk=1 ra(1,0)(xk)   log p(y|x)  x>k =
kxk=1
kxk=1
kxk=1
kxk=2
kxk=2 ra(1, 1)(xk)   log p(y|x)  x>k 1 =
kxk=2
kxk=2
 (e(yk)   p(yk|x)) x>k
  (e(yk)   p(yk|x)) x>k 1
  (e(yk)   p(yk|x)) x>k 1
  (e(yk)   p(yk|x)) x>k 1
kxk=2
k 1xk=1
k 1xk=1 ra(1,+1)(xk)   log p(y|x)  x>k+1 =
k 1xk=1
k 1xk=1
  (e(yk)   p(yk|x)) x>k 1
  (e(yk)   p(yk|x)) x>k+1
  (e(yk)   p(yk|x)) x>k+1
topics: loss gradient at pairwise log-factor and parameters
k 1xk=1
    partial derivative for log-factor:
  (e(yk)   p(yk|x)) x>k+1
   

kxk=1 ra(1,0)(xk)   log p(y|x)  x>k =
kxk=2 ra(1, 1)(xk)   log p(y|x)  x>k 1 =
k 1xk=1 ra(1,+1)(xk)   log p(y|x)  x>k+1 =

parameter gradients

rw(1,+1)   log p(y|x) =

  (e(yk)   p(yk|x)) x>k+1

}

rw(1,+1)   log p(y|x) =

rw(1,+1)   log p(y|x) =

3

@   log p(y|x)
@   log p(y|x)
@ap(y0k, y0k+1)
@ap(y0k, y0k+1)

    partial derivative of log-factor parameters:

j
,
  

@   log p(y|x)
=  (1yk=y0k,yk+1=y0k+1   p(y0k, y0k+1|x))
=  (1yk=y0k,yk+1=y0k+1   p(y0k, y0k+1|x))
x
@ap(y0k, y0k+1)
w
   

j

=  (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))

}

j
=

i

   

1
   
=
9
 (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))
w

j
u

(

,
t

=

i

i

t

j

,

|

   

)
t
(

=
@vy0k,y0k+1

x
w

@   log p(y|x)
@vy0k,y0k+1

r
x

{
@   log p(y|x)
x
=
=
2
x

p
{
@   log p(y|x)
 
@vy0k,y0k+1
u

=  (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))
k 1xk=1
    gradient of log-factor parameters 
k 1xk=1

k 1xk=1
k 1xk=1
 (1yk=y0k,yk+1=y0k+1   p(y0k, y0k+1|x))
 (1yk=y0k,yk+1=y0k+1   p(y0k, y0k+1|x))
 (1yk=y0k,yk+1=y0k+1   p(yk = y0k, yk+1 = y0k+1|x))
p(yk, yk+1|x)!
 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1)  
x
 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1) +
p(yk, yk+1|x)!
 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1)  
k 1xk=1
k 1xk=1
k 1xk=1
k 1xk=1
k 1xk=1
u
rv   log p(y|x) =
=
p(yk, yk+1|x)!
 
u
matrix of all pairwise
label frequencies
u

=
{
}
x
)
2
x
r
r
h
/2
9
w
x
z(yk 1,x) exp (au(yk) + ap(yk 1, yk))
z(yk 1,x) exp (au(yk) + ap(yk 1, yk))

w
x
9
/2
r
x
2
)
}
x
{
1

 (e(yk) e(yk+1)>   p(yk, yk+1|x)) =   freq(yk, yk+1)  

k=1 p(yk|yk 1, x) p(yk|yk 1, x) =

k=1 p(yk|yk 1, x) p(yk|yk 1, x) =

matrix of all pairwise
marginal probabilities

h
r

r
h

   
=

rv   log p(y|x) =

rv   log p(y|x) =
p

      log p(y, x) =   log (p(y|x)p(x)) =   log p(y|x)   log p(x)   log p(y|x)
      log p(y, x) =   log (p(y|x)p(x)) =   log p(y|x)   log p(x)   log p(y|x)

)
   
t
(

=

e
t

k 1xk=1

x

|

|

|

|

(

)

(

t

t

i

i

i

1

 
=

t

=
i
   

t

u
x

w

t

u

=

i

t
e

k 1xk=1

u

x

i

>i

   

(
t
   
)

p(yk, yk+1|x)!

p

>i

u

i

u

i

 

   

{

}

p

6
6
abstract

id173

4

    au(yk) = a(l+1,0)(xk)yk+ 1k>1 a(l+1, 1)(xk 1)yk+ 1k<k a(l+1,+1)(xk+1)yk

topics: id173
    for id173, we can use the same regularizers as for a 
non-sequential neural network
    exp(a(l+1,0)(x3)y3) exp(a(l+1, 1)(x2)y3) exp(a(l+1,+1)(x4)y3) exp(vy3,y4)
    add a regularizing term for all connection matrices
    do not regularize the bias vectors

l(f (x(t);    ), y(t)) +     (   )

    we could scale    by the sequence size

    with the loss and id173 gradients, we have all the 
ingredients to perform stochastic id119

