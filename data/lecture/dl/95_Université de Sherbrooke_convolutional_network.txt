id161
ift 725 - r  seaux neuronaux

id161

topics: id161, object recognition
    id161 is the design of computers that can process 
visual data and accomplish some given task 
    we will focus on object recognition: given some input image, identify which object 
caltech 101 dataset

it contains

i

s
l
e
x
p
 
2
1
1

      sun    ower      

150 pixels

2

id161

topics: id161
    we can design neural networks that are speci   cally adapted 
for such problems
    must deal with very high-dimensional inputs 

- 150 x 150 pixels = 22500 inputs, or 3 x 22500 if rgb pixels

    can exploit the 2d topology of pixels (or 3d for video data)
    can build in invariance to certain variations we can expect 

-

translations, illumination, etc.

3

id161

topics: local connectivity
    first idea: use a local 
connectivity of hidden units
    each hidden unit is connected only to a 

subregion (patch) of the input image

.
.
.

...

...

    it is connected to all channels

- 1 if greyscale image
- 3 (r, g, b) for color image

    solves the following problems:
    fully connected hidden layer would have 
an unmanageable number of parameters
    computing the linear activations of the 
hidden units would be very expensive

r

= receptive    eld

4

id161

topics: local connectivity
    units are connected to all channels:

    1 channel if grayscale image, 3 channels (r, g, b) if color image

.
.
.

...

...

5

id161

topics: parameter sharing
    second idea: share matrix of parameters across certain units

    units organized into the same       feature map       share parameters
    hidden units within a feature map cover different positions in the image 

feature map 1
...
...

feature map 2
...
...

feature map 3
...
...

same color

= 

same matrix
of connections

6

id161

topics: parameter sharing
    second idea: share matrix of parameters across certain units

    units organized into the same       feature map       share parameters
    hidden units within a feature map cover different positions in the image 

feature map 1
...
...

feature map 2
...
...

feature map 3
...
...

same color

= 

same matrix
of connections

6

id161

topics: parameter sharing
    second idea: share matrix of parameters across certain units

    units organized into the same       feature map       share parameters
    hidden units within a feature map cover different positions in the image 

feature map 1
...
...

feature map 2
...
...

feature map 3
...
...

same color

= 

same matrix
of connections

6

id161

topics: parameter sharing
    second idea: share matrix of parameters across certain units

    units organized into the same       feature map       share parameters
    hidden units within a feature map cover different positions in the image 

feature map 1
...
...

feature map 2
...
...

feature map 3
...
...

same color

= 

same matrix
of connections

6

id161

topics: parameter sharing
    second idea: share matrix of parameters across certain units

    units organized into the same       feature map       share parameters
    hidden units within a feature map cover different positions in the image 

feature map 1
...
...

feature map 2
...
...

feature map 3
...
...

same color

= 

same matrix
of connections

    wij is the matrix 
connecting the ith 
input channel with 
the jth feature map
6

id161

topics: parameter sharing
    solves the following problems:

    reduces even more the number of parameters
    will extract the same features at every position (features are       equivariant      )

feature map 1
...
...

feature map 2
...
...

feature map 3
...
...

same color

= 

same matrix
of connections

    wij is the matrix 
connecting the ith 
input channel with 
the jth feature map
7

id161

topics: parameter sharing
    each feature map forms a 2d grid of features

math for my slides    id161   .

jarret et al. 2009

    h x    

    can be computed with a discrete convolution (   ) of a kernel matrix kij which is

the hidden weights matrix wij with its rows and columns    ipped

filter bank layer - fid19: the input of a    lter bank
layer is a 3d array with n1 2d feature maps of size n2    n3.
each component is denoted xijk, and each feature map is
denoted xi. the output is also a 3d array, y composed of
m1 feature maps of size m2    m3. a    lter in the    lter bank
kij has size l1    l2 and connects input feature map xi to
output feature map yj. the module computes:

    xi is the ith channel of input
    kij is the convolution kernel
    gj is a learned scaling factor
    yj is the hidden layer 

   (could have added a bias)

r
e 

at
u

p
s

fe

a

m

figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
through a non-linear    lterbank, followed by recti   cation, local
contrast id172 and spatial pooling/sub-sampling.

yj = gj tanh(!
i

kij     xi)

figure 1. a example of feature extraction stage of the type fid19   

layer with 4x4 down-sampling is denoted: p 4  4
max-pooling and subsampling layer - pm : building lo-

8

(1)

id161

topics: discrete convolution
    the convolution of an image x with a kernel k is computed 
as follows:

                   (x * k)ij =     xi+p,j+q km-p,m-q 
    example:

pq

=

*

k

x

9

0804020400004000.250.51id161

topics: discrete convolution
    the convolution of an image x with a kernel k is computed 
as follows:

                   (x * k)ij =     xi+p,j+q km-p,m-q 
    example:

k~ = k  with rows and columns    ipped

pq

=

*

k

x

10

0804020400004000.250.5110.50.250id161

topics: discrete convolution
    the convolution of an image x with a kernel k is computed 
as follows:

                   (x * k)ij =     xi+p,j+q km-p,m-q 
    example:

1	
   x	
   0	
   +	
   0.5	
   x	
   80	
   +	
   0.25	
   x	
   20	
   +	
   0	
   x	
   40

pq

=

*

k

x

11

00.250.51450804020400004010.50.250id161

topics: discrete convolution
    the convolution of an image x with a kernel k is computed 
as follows:

                   (x * k)ij =     xi+p,j+q km-p,m-q 
    example:

pq

1	
   x	
   80	
   +	
   0.5	
   x	
   40	
   +	
   0.25	
   x	
   40	
   +	
   0	
   x	
   0

=

*

k

x

12

00.250.51451100804020400004010.50.250id161

topics: discrete convolution
    the convolution of an image x with a kernel k is computed 
as follows:

                   (x * k)ij =     xi+p,j+q km-p,m-q 
    example:

pq

1	
   x	
   20	
   +	
   0.5	
   x	
   40	
   +	
   0.25	
   x	
   0	
   +	
   0	
   x	
   0

=

*

k

x

13

00.250.5145110400804020400004010.50.250id161

topics: discrete convolution
    the convolution of an image x with a kernel k is computed 
as follows:

                   (x * k)ij =     xi+p,j+q km-p,m-q 
    example:

pq

1	
   x	
   40	
   +	
   0.5	
   x	
   0	
   +	
   0.25	
   x	
   0	
   +	
   0	
   x	
   40

=

*

k

x

14

00.250.514511040400804020400004010.50.250id161

topics: discrete convolution
    linear activations from channel xi  into feature map yj can be 
computed by:
    getting the convolution kernel where kij =wij from the connection matrix wij
    applying the convolution yj = xi * kij 

~

    this is equivalent to computing the correlation 
of xi with wij

15

id161

example 
topics: discrete convolution
~
    simple illustration: xi * kij where wij =wij
       premi  re%  tape%:%calcul%de%la%convolu7on%%

example 
       calcul%d   une%couche%  %simple%cell%  %
       premi  re%  tape%:%calcul%de%la%convolu7on%%

       calcul%d   une%couche%  %simple%cell%  %

0%

0% 0.5%
0%

0.5% 0%
0%
0%

0%

0%

0%

0%

255%

0% 0.5%
0%
0%
255%
0%
0.5% 0%
0%
0%
255%
0%
0%
0%
255%
0%

0%
255%
0%

255%
0%
255%
0%
255%
0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

255%

0%
255%
0%
xi
%%%%%
x
% 

0%
0%
%%%%%
x
% 
couche)d   entr  e)

couche)d   entr  e)

0.5%
connexions%
connexions%
vers%les%neurones%
vers%les%neurones%
cach  s%
cach  s%

0%

0%

0.5%

0.5%
w

0%

0.5%
w
128%

0%

128%

0%

0%

128%

128%

255%

0%

0%

0%

0%

128%

128%

255%

255%

0%

255%

0%

0%

0%

0%

0%

0%

128%
0%
128%
0%

0%

0%

0%

0%

xi * kij
 x w
%%%%%
 x w
%%%%%
% 
% 
couche)  )simple)cell)  )

couche)  )simple)cell)  )

16

id161

example 

topics: discrete convolution
    with a non-linearity, we get a detector of a feature at any 
       calcul%d   une%couche%  %simple%cell%  %
position in the image

       premi  re%  tape%:%calcul%de%la%convolu7on%%
       deuxi  me%  tape%:%calcul%de%la%nonnlin  arit  %(%ex.:%logis6c(%(xn200)/50%)%)%

0%

0%

0%

0%

0%

0%

0%

255%

255%

255%

255%

0%

255%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%
xi
%%%%%
x
% 

couche)d   entr  e)

0.02% 0.19% 0.19% 0.02%
0%

128%

128%

0%

0.02% 0.19% 0.19% 0.02%
0%

128%

128%

0%

0.02% 0.75% 0.02% 0.02%
0%

255%

0%

0%

0.75% 0.02% 0.02% 0.02%
255%

0%

0%

0%

sigm(0.02 xi * kij -4)
logis6c(%(%%%%%%%%%%%%%n%200%)%/%50%)%

 x w
%%%%%
% 

couche)  )simple)cell)  )

17

id161

topics: discrete convolution
connexions%
    can use       zero padding       to allow going over the borders ( * )
       convolu7on%avec%zero,padding%%
vers%les%neurones%
cach  s%

example 

0.5%

0.5%

0%

0%

0% 0.5%
0%

0%

0%

0.5%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

255%

0%

255%

255%

255%

0%

0%

255%

0%

0%

0%

%%%%%
% 

0%

xi

couche)d   entr  e)

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

255%

128%

0%

w
0%
128%

128%

128%

128%

128%

255%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

xi * kij
%%%%%
% 

couche)  )simple)cell)  )

0%

0%

0%

0%

0%

0%

18

hugo larochelle
id161

d  epartement d   informatique
universit  e de sherbrooke
topics: pooling and subsampling
hugo.larochelle@usherbrooke.ca
    third idea: pool hidden units in same neighborhood

    pooling is performed in non-overlapping neighborhoods (subsampling)

november 8, 2012

math for my slides    id161   .

abstract

yijk = max
p,q

xi,j+p,k+q

jarret et al. 2009

    xi,j,k is value of the ith 
feature map at position j,k
    p is vertical index in local 
neighborhood
    q is horizontal index in 
local neighborhood
    yijk is pooled and 
subsampled layer

19

universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

id161
november 8, 2012
topics: pooling and subsampling
    third idea: pool hidden units in same neighborhood

math for my slides    id161   .

    an alternative to       max       pooling is       average       pooling 

abstract

yijk = max
m
p,q

xi,j+p,k+q

yijk =

1

m2xp,q

xi,j+p,k+q

jarret et al. 2009

    xi,j,k is value of the ith 
feature map at position j,k
    p is vertical index in local 
neighborhood
    q is horizontal index in 
local neighborhood
    yijk is pooled and 
subsampled layer
    m is the neighborhood 
height/width

20

example 

id161

       calcul%d   une%couche%  %complex%cell%  %
topics: pooling and subsampling
       maximum%dans%plusieurs%segments%
    illustration of pooling/subsampling operation

0.02% 0.19% 0.19% 0.02%

0.02% 0.19% 0.19% 0.02%

0.02% 0.75% 0.02% 0.02%

0.75% 0.02% 0.02% 0.02%

max%

max%

max%

max%

0.19% 0.19%

0.75% 0.02%

    solves the following problems:
logis6c(%(%%%%%%%%%%%%%n%200%)%/%50%)%

%%%%%
% 
couche)  )simple)cell)  )

 x w
%%%%%
% 

    introduces invariance to local translations
couche)  )complex)cell)  )
    reduces the number of hidden units in hidden layer                                                               
ift%615%

hugo%larochelle%

49%

21

id161

topics: pooling and subsampling
    illustration of local translation invariance

example 

       la%couche%  %complex%cell%  %est%invariante%aux%transla7ons%locales%
    both images given the same feature map after pooling/subsampling

   

0%

0%

0%

0%

0%

0%

0%

255%

255%

0%

255%

255%

255%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

255%

0%

255%

255%

255%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0%

0.19% 0.19%

0.75% 0.02%

couche)  )complex)cell)  )

couches)d   entr  e)

couches)d   entr  e)

ift%615%

hugo%larochelle%

22

50%

convolutional network
topics: convolutional network
    convolutional neural network alternates between the 
convolutional and pooling layers

r  seau de neurones    convolution: 

r  seau complet 

ift%615%

(from yann lecun)

hugo%larochelle%

{

fully

connected

23

51%

convolutional network
topics: convolutional network
    output layer is a regular, fully connected layer with softmax 
non-linearity
    output provides an estimate of the id155 of each class

    the network is trained by stochastic id119

    id26 is used similarly as in a fully connected network
    we have seend how to pass gradient through element-wise activation function
    we also need to pass gradients through the convolution operation and the 

pooling operation

24

convolutional network
topics: gradient of convolution layer
    let l be the id168

    for convolution operation yj = xi * kij the gradient for xi is

                 x l =    (   y l) * (wij)

j

i

j

and the gradient for wij is 

                          w  l = (   y l) * xi
~

ij

where * is the convolution with zero padding and xi is the row/column    ipped 
version of xi 

~

25

hugo larochelle

hugo larochelle
d  epartement d   informatique
universit  e de sherbrooke

convolutional network
topics: gradient of pooling layer
    let l be the id168
november 8, 2012

d  epartement d   informatique
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

november 8, 2012

math for my slides    id161   .

    for max pooling operation yijk = max xi,j+p,k+q the gradient for xijk is
              x   l = 0 except for      x         l =    y  l  

abstract

abstract
p,q

i,j+p   ,k+q   

ijk

ijk

math for my slides    id161   .

    h x    

where p   , q    = argmax xi,j+p,k+q
-

yijk = max
p,q

xi,j+p,k+q

in words, only the       winning       units in layer x get the gradient from the pooled layer

1

xi,j+p,k+q

yijk = max
p,q
yijk =

    for average pooling operation                                    the gradient for xijk is
                x l =    upsample(   y l)
where upsample inverts subsampling with       nearest neighbor       interpolation

m2xp,q

xi,j+p,k+q

xi,j+p,k+q

yijk =

m2xp,q

1

    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
     jk = (pipq wpqv2

    yijk = vijk/ max(c,  jk)

i,j+p,k+q)1/2

26

convolutional network
topics: convolutional network
    this architecture works well for handwritten character 
recognition

r  seau de neurones    convolution: 

r  seau complet 

{

fully

connected

ift%615%

(from yann lecun)

hugo%larochelle%

51%

    it performs poorly on object recognition in general

    we need to introduce other operations between 

27

filter bank layer - fid19: the input of a    lter bank
layer is a 3d array with n1 2d feature maps of size n2    n3.
each component is denoted xijk, and each feature map is
denoted xi. the output is also a 3d array, y composed of
m1 feature maps of size m2    m3. a    lter in the    lter bank
kij has size l1    l2 and connects input feature map xi to

convolutional network
jarret et al. 2009
topics: recti   cation layer
    recti   cation layer:  yijk = |xijk|

    introduces invariance to the sign of the unit in the 

previous layer
-

for instance, lose information of whether an edge is 
black-to-white or white-to-black

figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
through a non-linear    lterbank, followed by recti   cation, local
contrast id172 and spatial pooling/sub-sampling.

28

    h x    
    h x    
    h x    
   
   
   

convolutional network
jarret et al. 2009
topics: local contrast id172 layer
1
1
xi,j+p,k+q
1
    local contrast id172:
xi,j+p,k+q
xi,j+p,k+q

yijk = max
yijk = max
p,q
yijk = max
p,q
p,q

xi,j+p,k+q
xi,j+p,k+q
xi,j+p,k+q

yijk =
yijk =
yijk =

   
   
   

m xp,q
m xp,q
m xp,q

    yijk = vijk/ max(c,  jk)
    yijk = vijk/ max(c,  jk)
    yijk = vijk/ max(c,  jk)

    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
     jk = (pipq wpqv2
     jk = (pipq wpqv2
     jk = (pipq wpqv2

i,j+p,k+q)1/2
i,j+p,k+q)1/2
i,j+p,k+q)1/2

   pq wpq = 1

figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
through a non-linear    lterbank, followed by recti   cation, local
contrast id172 and spatial pooling/sub-sampling.

where c is a small constant to prevent division by 0
    reduces unit   s activation if neighbors are also active
    creates competition between feature maps

29

    h x    
    h x    
    h x    
   
   
   

convolutional network
jarret et al. 2009
topics: local contrast id172 layer
1
1
xi,j+p,k+q
1
    local contrast id172:
xi,j+p,k+q
xi,j+p,k+q

yijk = max
yijk = max
p,q
yijk = max
p,q
p,q

xi,j+p,k+q
xi,j+p,k+q
xi,j+p,k+q

yijk =
yijk =
yijk =

   
   
   

local average

m xp,q
m xp,q
m xp,q

    yijk = vijk/ max(c,  jk)
    yijk = vijk/ max(c,  jk)
    yijk = vijk/ max(c,  jk)

    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
     jk = (pipq wpqv2
     jk = (pipq wpqv2
     jk = (pipq wpqv2

i,j+p,k+q)1/2
i,j+p,k+q)1/2
i,j+p,k+q)1/2

   pq wpq = 1

figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
through a non-linear    lterbank, followed by recti   cation, local
contrast id172 and spatial pooling/sub-sampling.

where c is a small constant to prevent division by 0
    reduces unit   s activation if neighbors are also active
    creates competition between feature maps

29

    h x    
    h x    
    h x    
   
   
   

convolutional network
jarret et al. 2009
topics: local contrast id172 layer
1
1
xi,j+p,k+q
1
    local contrast id172:
xi,j+p,k+q
xi,j+p,k+q

yijk = max
yijk = max
p,q
yijk = max
p,q
p,q

xi,j+p,k+q
xi,j+p,k+q
xi,j+p,k+q

yijk =
yijk =
yijk =

   
   
   

local average

m xp,q
m xp,q
m xp,q

    yijk = vijk/ max(c,  jk)
    yijk = vijk/ max(c,  jk)
    yijk = vijk/ max(c,  jk)

    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
    vijk = xijk  pipq wpqxi,j+p,k+q
     jk = (pipq wpqv2
     jk = (pipq wpqv2
     jk = (pipq wpqv2

i,j+p,k+q)1/2
i,j+p,k+q)1/2
i,j+p,k+q)1/2

   pq wpq = 1

local std dev.

figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
through a non-linear    lterbank, followed by recti   cation, local
contrast id172 and spatial pooling/sub-sampling.

where c is a small constant to prevent division by 0
    reduces unit   s activation if neighbors are also active
    creates competition between feature maps

29

convolutional network
jarret et al. 2009
topics: convolutional network
    these operations are inserted after the convolutions and 
before the pooling

    images should also be preprocessed by
figure 1. a example of feature extraction stage of the type fid19   
rabs     n     pa. an input image (or a feature map) is passed
    converting to grayscale (if appropriate)
through a non-linear    lterbank, followed by recti   cation, local
    resizing images to 150 x 150 pixels (use zero padding for non-square images)
contrast id172 and spatial pooling/sub-sampling.
    removing (intra image) mean and dividing by standard deviation of the image 
    applying local contrast id172
layer with 4x4 down-sampling is denoted: p 4  4
a .
max-pooling and subsampling layer - pm : building lo-

30

convolutional network
jarret et al. 2009
topics: initialization of parameters
    initialization of parameters:

    can do as in regular neural network and initialize them randomly
    can also use unsupervised pretraining approach

-

to use unsupervised neural networks we   ve seen so far, we have to convert pretraining as a 
patch-wise learning problem
    extract patches of the same as the receptive    elds of the hidden units, at random positions
    train an unsupervised neural network (rbm, autoencoder, sparse coding) on those patches
    use weights connecting an input patch to each hidden unit to initialize each feature map parameters
    map images through all feature maps and repeat previous steps, for as many layers as desired

    we will compare:

    using random initialization (r) or unsupervised pretraining (u)
    using    ne-tuning of whole network (+) or only training output layer (no +)

31

convolutional network
jarret et al. 2009
topics: convolutional network
    results on caltech:

fid19 = convolution layer
r = recti   cation layer
n = local contrast id172 layer
pm = max pooling layer, pa = average pooling layer

single stage system: [64.f9  9

id19     r/n/p5  5] - log reg

rabs     n     pa rabs     pa

n     pm

50.0%
47.0%

43.3%(  1.6)

31.7%

44.3%
38.0%
44.0%
32.1%

n     pa
18.5%
16.3%
17.2%
15.3%

pa

14.5%
14.3%
13.4%

12.1%(  2.2)

54.2%
54.8%
52.2%
53.3%
52.3%

g
two stage system: [64.f9  9

id19     r/n/p5  5]     [256.f9  9

id19     r/n/p4  4] - log reg

rabs     n     pa rabs     pa

n     pm

61.0%
60.0%
56.0%

37.6%(  1.9)

n     pa
34.0%
31.0%
23.1%
19.6%

pa

32.0%
29.7%
9.1%
8.8%

60.5%
59.5%
46.7%

65.5%
64.7%
63.7%
62.9%
55.8%
single stage: [64.f9  9
64.0%

33.7%(  1.5)

u+
r+
u

r

u+u+
r+r+
uu

rr

gt

u

id19     rabs/n/p5  5

a ] - pmk-id166

32

convolutional network
jarret et al. 2009
topics: importance of architecture
    results on caltech:

    choice of right architecture can be more important than learning algorithm

-

-

the use of recti   cation and local contrast id172 layers is important
this is particularly true if little training data 

    results on norb:

    architecture makes less of a

substitute for unsupervised training.
5. it is clear that abs recti   cation is a crucial component for
achieving good performance, as shown with the u +u + pro-
difference with lots of 
tocol by comparing columns n     pa (31.0%), rabs     pa
training data per class
(60.0%), and rabs     n     pa (65.5%). however, using
    random    lters are also
max pooling seems to alleviate the need for abs recti   ca-
tion, con   rming the hypothesis that average pooling with-
out recti   cation falls victim to cancellation effects between
neighboring    lter outputs. this is an extremely important
fact for users of convolutional networks, in which recti   ca-
tion has not been traditionally used.
6. a single-stage system with pmk-id166 reaches the same

not as good

e

t
a
r
 
r
o
r
r
e

50

40
35
30
25

20

15

10
9
8
7
6

 

4
20

 

f

f

f

id19

id19

id19

     p

 (r+ r+)
a
     r

     n     p

abs

abs

     r

     n     p

 (rr)

a

 (r+ r+)
a

50

100

200

500

1000

2000

4860

figure 3. test error rate vs. number of training samples per class

number of training samples per class

33

convolutional network
jarret et al. 2009
topics: random    lters
    results on caltech:

    random    lters are surprisingly good
    turns out that random    lters give units that are still sensitive to a particular 

frequency
- can analyze this by    nding input which maximizes the activation of a given hidden unit (with 

gradient ascent applied in input space)

random    lters

optimal input

learned    lters

optimal input

figure 4. left: random stage-1    lters, and corresponding optimal inputs that maximize the response of each corresponding complex cell in
a fid19     rabs     n     pa architecture. the small asymmetry in the random    lters is suf   cient to make them orientation selective. middle:

34

invariance by data set expansion
topics: generating additional examples
    invariances built-in in convolutional network:

    small translations: due to convolution and max pooling
    small illumination changes: due to local contrast id172

    it is not invariant to other important variations such as 
rotations and scale changes
    however, it   s easy to arti   cially generate data with such 
transformations
    could use such data as additional training data
    neural network will learn to be invariant to such transformations

35

invariance by data set expansion
topics: generating additional examples

original
ti o
sl a
n
a

r

t

n

rotation

sc
alin
g

crop

crop

crop

crop

undo

undo

undo

undo

36

invariance by data set expansion
topics: generating additional examples, distortion    eld
    can add       elastic       deformations (useful in character recognition)
    we do this by applying a       distortion    eld       to the image

    a distortion    eld speci   es where to displace each pixel value

random distortion

see simard et al.
for more detail

(from bishop   s book)

37

invariance by data set expansion
topics: generating additional examples, distortion    eld
    can add       elastic       deformations (useful in character recognition)
    we do this by applying a       distortion    eld       to the image

    a distortion    eld speci   es where to displace each pixel value

smoothed 

random distortion

see simard et al.
for more detail

(from bishop   s book)

38

invariance by data set expansion
topics: generating additional examples, distortion    eld
    can add       elastic       deformations (useful in character recognition)
    we do this by applying a       distortion    eld       to the image

    a distortion    eld speci   es where to displace each pixel value

smoothed 

random distortion

see simard et al.
for more detail

(from bishop   s book)

39

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional id50 for scalable unsupervised learning of hierarchical representations
convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional rbm
(nw , nv   nh + 1); the    lter weights are shared
across all the hidden units within the group. in addi-
tion, each hidden group has a bias bk and all visible
units share a single bias c.
we de   ne the energy function e(v, h) as:

topics: convolutional rbm
(nw , nv   nh + 1); the    lter weights are shared
(nw , nv   nh + 1); the    lter weights are shared
    how about designing convolutional unsupervised networks
across all the hidden units within the group. in addi-
pk
across all the hidden units within the group. in addi-
pk
  
  
    let   s consider the case of the rbm
tion, each hidden group has a bias bk and all visible
tion, each hidden group has a bias bk and all visible
c
units share a single bias c.
units share a single bias c.
hk
hk
    could use same convolutional connectivity between input (v) and hidden layer (h)
i,j
i,j
we de   ne the energy function e(v, h) as:
we de   ne the energy function e(v, h) as:

lee et al. 2009

nh
nh

np
np

nh

np

c
c

pk
  

hk

wk

rsvi+r 1,j+s 1

p (v, h) =
p (v, h) =

1
p (v, h) =
exp( e(v, h))
z
1
1
exp( e(v, h))
exp( e(v, h))
nhxi,j=1
nwxr,s=1
kxk=1
z
z
e(v, h) =  
hk
ijw k
nwxr,s=1
nhxi,j=1
kxk=1
nhxi,j=1
nwxr,s=1
kxk=1
e(v, h) =  
e(v, h) =  
hk
ijw k
hk
ijw k
rsvi+r 1,j+s 1
rsvi+r 1,j+s 1
nhxi,j=1
kxk=1
hk
ij   c
vij.
nhxi,j=1
nvxi,j=1
kxk=1
nhxi,j=1
nvxi,j=1
kxk=1
hk
hk
ij   c
ij   c
using the operators de   ned previously,
using the operators de   ned previously,
kxk=1
kxk=1
hk     (   w k     v)  
e(v, h) =  
kxk=1
kxk=1
kxk=1
kxk=1
bkxi,j
bkxi,j
hk     (   w k     v)  
hk     (   w k     v)  
e(v, h) =  
e(v, h) =  

vij.
vij.
using the operators de   ned previously,

bkxi,j
i,j   cxi,j
i,j   cxi,j

nvxi,j=1

 
bk
bk

 
 

hk
hk

hk

bk

(1)
(1)

as with standard rbms (section 2.1), we can perform

i,j   cxi,j

vij.
vij.

nv

v
v

wk
wk

    hkij are the hidden units of 
v
nw
the kth feature map
    wkrs are the weights to the 
nw
nw
kth feature map
(1)
~
    wk are the weights with 
figure 1. convolutional rbm with probabilistic max-
figure 1. convolutional rbm with probabilistic max-
   ipped rows and columns 
pooling. for simplicity, only group k of the detection layer
pooling. for simplicity, only group k of the detection layer
(convolution kernel)
and the pooing layer are shown. the basic crbm corre-
and the pooing layer are shown. the basic crbm corre-
sponds to a simpli   ed structure with only visible layer and
sponds to a simpli   ed structure with only visible layer and
detection (hidden) layer. see text for details.
detection (hidden) layer. see text for details.
vij.
to simplify the notation, we consider a model with a
to simplify the notation, we consider a model with a
visible layer v , a detection layer h, and a pooling layer
visible layer v , a detection layer h, and a pooling layer

nv
nv
figure 1. convolutional rbm with probabilistic max-
pooling. for simplicity, only group k of the detection layer
and the pooing layer are shown. the basic crbm corre-
sponds to a simpli   ed structure with only visible layer and
detection (hidden) layer. see text for details.
to simplify the notation, we consider a model with a
visible layer v , a detection layer h, and a pooling layer
p , as shown in figure 1. the detection and pooling
layers both have k groups of units, and each group

40

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional id50 for scalable unsupervised learning of hierarchical representations

(nw , nv   nh + 1); the    lter weights are shared
across all the hidden units within the group. in addi-
tion, each hidden group has a bias bk and all visible

subj. to

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional id50 for scalable unsupervised learning of hierarchical representations

tion that we call probabilistic max-pooling.
if and only if a detection unit is on. equivalently, we
can consider these c2+1 units as a single random vari-
in general, higher-level feature detectors need informa-
convolutional rbm
able which may take on one of c2 + 1 possible values:
tion from progressively larger input regions. existing
one value for each of the detection units being on, and
translation-invariant representations, such as convolu-
set of shared weights   = { 1,1, . . . ,  k,k0}, where
increase in energy caused by turning on unit hk
i,j is
one value indicating that all units are o   .
tional networks, often involve two kinds of layers in
lee et al. 2009
 k,` is a weight matrix connecting pooling unit p k to
alternation:    detection    layers, whose responses are
we formally de   ne the energy function of this simpli-
 i(hk
i,j), and the id155 is given by:
topics: convolutional rbm
computed by convolving a feature detector with the
   ed probabilistic max-pooling-crbm as follows:
detection unit h0`. the de   nition can be extended to
previous layer, and    pooling    layers, which shrink the
    we can introduce a notion of probabilistic pooling
i,j))
exp(i(hk
e(v, h) =  xk xi,j    hk
i,j      cxi,j
convolutional id50 for scalable unsupervised learning of hierarchical representations
deeper networks in a straightforward way.
representation of the detection layers by a constant
i,j(   w k     v)i,j + bkhk
p (hk
i,j = 1|v) =
1 +p(i0,j0)2b   
factor. more speci   cally, each unit in a pooling layer
i0,j0))
exp(i(hk
    within a pooling neighborhood, allow at most only a single unit        equal to 1
note that an energy function for this sub-network con-
x(i,j)2b   
computes the maximum activation of the units in a
set of shared weights   = { 1,1, . . . ,  k,k0}, where
increase in energy caused by turning on unit hk
i,j is
increase in energy caused by turning on unit hk
hk
i,j     1, 8k,    .
small region of the detection layer. shrinking the rep-
1
sists of two kinds of potentials: unary terms for each
 k,` is a weight matrix connecting pooling unit p k to
i,j), and the id155 is given by:
 i(hk
    pooling unit      above is 1 only if at least one hidden unit        in neighborhood is 1
 i(hk
i,j), and the id155 is given by:
set of shared weights   = { 1,1, . . . ,  k,k0}, where
resentation with max-pooling allows higher-layer rep-
    = 0|v) =
p (pk
i,j is
increase in energy caused by turning on unit hk
1 +p(i0,j0)2b   
i0,j0)) .
of the groups in the detection layers, and interaction
detection unit h0`. the de   nition can be extended to
exp(i(hk
resentations to be invariant to small translations of the
we now discuss sampling the detection layer h and
 k,` is a weight matrix connecting pooling unit p k to
i,j), and the id155 is given by:
 i(hk
input and reduces the computational burden.
i,j))
exp(i(hk
the pooling layer p given the visible layer v . group k
terms between v and h and between p and h0:
deeper networks in a straightforward way.
detection unit h0`. the de   nition can be extended to
p (hk
i,j = 1|v) =
receives the following bottom-up signal from layer v :
1 +p(i0,j0)2b   
p (hk
i,j = 1|v) =
sampling the visible layer v given the hidden layer
1 +p(i0,j0)2b   
max-pooling was intended only for feed-forward archi-
e(v, h, p, h0) =  xk
i0,j0))
exp(i(hk
i,j))
deeper networks in a straightforward way.
note that an energy function for this sub-network con-
tectures. in contrast, we are interested in a generative
ij) , bk + (   w k     v)ij.
h can be performed in the same way as described in
i(hk
(2)
i,j = 1|v) =
p (hk
1 +p(i0,j0)2b   
pk
model of images which supports both top-down and
1
  
sists of two kinds of potentials: unary terms for each
i0,j0))
exp(i(hk
section 3.2.
note that an energy function for this sub-network con-
increase in energy caused by turning on unit hk
i,j is
p (pk
    = 0|v) =
bottom-up id136. therefore, we designed our gen-
now, we sample each block independently as a multi-
1 +p(i0,j0)2b   
i0,j0)) .
p (pk
    = 0|v) =
implies       is 1
1 +p(i0,j0)2b   
of the groups in the detection layers, and interaction
exp(i(hk
i,j), and the id155 is given by:
 i(hk
hk
erative model so that id136 involves max-pooling-
i,j is a hid-
nomial function of its inputs. suppose hk
1
sists of two kinds of potentials: unary terms for each
i,j
3.4. training via sparsity id173
terms between v and h and between p and h0:
    = 0|v) =
p (pk
1 +p(i0,j0)2b   
like behavior.
den unit contained in block     (i.e., (i, j) 2 b   ), the
i0,j0)) .
of the groups in the detection layers, and interaction
exp(i(hk
i,j))
exp(i(hk
sampling the visible layer v given the hidden layer
i,j = 1|v) =
p (hk
e(v, h, p, h0) =  xk
sampling the visible layer v given the hidden layer
1 +p(i0,j0)2b   
our model is overcomplete in that the size of the rep-
terms between v and h and between p and h0:
exp(i(hk
i0,j0))
h can be performed in the same way as described in
h can be performed in the same way as described in
resentation is much larger than the size of the inputs.
sampling the visible layer v given the hidden layer
1
e(v, h, p, h0) =  xk
to sample the detection layer h and pooling layer p ,
section 3.2.
    = 0|v) =
1 +p(i0,j0)2b   
i0,j0)) .
v
section 3.2.
h can be performed in the same way as described in
in fact, since the    rst hidden layer of the network con-
nv
convolutional id50 for scalable unsupervised learning of hierarchical representations
note that the detection layer h k receives the following
section 3.2.
3.4. training via sparsity id173
tains k groups of units, each roughly the size of the
sampling the visible layer v given the hidden layer
bottom-up signal from layer v :
3.4. training via sparsity id173
set of shared weights   = { 1,1, . . . ,  k,k0}, where
increase in energy caused by turning on unit hk
i,j is
image, it is overcomplete roughly by a factor of k. in
h can be performed in the same way as described in
our model is overcomplete in that the size of the rep-
3.4. training via sparsity id173
section 3.2.
 k,` is a weight matrix connecting pooling unit p k to
our model is overcomplete in that the size of the rep-
i,j), and the id155 is given by:
 i(hk
general, overcomplete models run the risk of learning
resentation is much larger than the size of the inputs.

exp(i(hk
convolutional id50 for scalable unsupervised learning of hierarchical representations

figure 1. convolutional rbm with probabilistic max-
pooling. for simplicity, only group k of the detection layer
and the pooing layer are shown. the basic crbm corre-

implies all         are 0

exp(i(hk

hk (detection layer)

pk (pooling layer)

v  (visible layer)

exp(i(hk

p (pk

(1)

wk

vi,j

41

nw

nh

np

c

convolutional id50 for scalable unsupervised learning of hierarchical representations

hk

vij.

kxk=1

kxk=1

bkxi,j

hk     (   w k     v)  

i,j   cxi,j

convolutional id50 for scalable unsupervised learning of hierarchical representations

increase in energy caused by turning on unit hk
p (hk

as with standard rbms (section 2.1), we can perform
block id150 using the following conditional
distributions:

convolutional id50 for scalable unsupervised learning of hierarchical representations

e(v, h) =  
convolutional rbm

tion that we call probabilistic max-pooling.
in general, higher-level feature detectors need informa-
tion from progressively larger input regions. existing
translation-invariant representations, such as convolu-
tional networks, often involve two kinds of layers in
alternation:    detection    layers, whose responses are
topics: convolutional rbm
computed by convolving a feature detector with the
previous layer, and    pooling    layers, which shrink the
    given the        units, we sample each input independently:
e(v, h) =  xk xi,j    hk
set of shared weights   = { 1,1, . . . ,  k,k0}, where
i,j is
representation of the detection layers by a constant
ij = 1|v) =  ((   w k     v)ij + bk)
factor. more speci   cally, each unit in a pooling layer
 k,` is a weight matrix connecting pooling unit p k to
i,j), and the id155 is given by:
p (vij = 1|h) =  ((xk
x(i,j)2b   
computes the maximum activation of the units in a
increase in energy caused by turning on unit hk
hk
i,j     1, 8k,    .
w k     hk)ij + c),
small region of the detection layer. shrinking the rep-
detection unit h0`. the de   nition can be extended to
i,j), and the id155 is given by:
 i(hk
resentation with max-pooling allows higher-layer rep-
resentations to be invariant to small translations of the
deeper networks in a straightforward way.
sigmoid
input and reduces the computational burden.
i0,j0))
p (hk
max-pooling was intended only for feed-forward archi-
note that an energy function for this sub-network con-
tectures. in contrast, we are interested in a generative
i(hk
model of images which supports both top-down and
sists of two kinds of potentials: unary terms for each
bottom-up id136. therefore, we designed our gen-
i0,j0)) .
of the groups in the detection layers, and interaction
erative model so that id136 involves max-pooling-
like behavior.
terms between v and h and between p and h0:
sampling the visible layer v given the hidden layer
h can be performed in the same way as described in

where   is the sigmoid function. id150 forms
the basis of our id136 and learning algorithms.
i,j = 1|v) =
1 +p(i0,j0)2b   
ij) , bk + (   w k     v)ij.
3.3. probabilistic max-pooling
p (pk
    = 0|v) =
in order to learn high-level representations, we stack
1 +p(i0,j0)2b   
crbms into a multilayer architecture analogous to
dbns. this architecture is based on a novel opera-
1 +p(i0,j0)2b   
tion that we call probabilistic max-pooling.
e(v, h, p, h0) =  xk
v     (w k     hk)  xk
1 +p(i0,j0)2b   
in general, higher-level feature detectors need informa-
tion from progressively larger input regions. existing
translation-invariant representations, such as convolu-
increase in energy caused by turning on unit hk
i,j is
 xk,`
pk     ( k`     h0`)  x`
tional networks, often involve two kinds of layers in
3.4. training via sparsity id173
i,j), and the id155 is given by:
 i(hk

if and only if a detection unit is on. equivalently, we
to simplify the notation, we consider a model with a
can consider these c2+1 units as a single random vari-
visible layer v , a detection layer h, and a pooling layer
able which may take on one of c2 + 1 possible values:
one value for each of the detection units being on, and
p , as shown in figure 1. the detection and pooling
one value indicating that all units are o   .
layers both have k groups of units, and each group
lee et al. 2009
we formally de   ne the energy function of this simpli-
of the pooling layer has np     np binary units. for
   ed probabilistic max-pooling-crbm as follows:
each k 2 {1, ..., k}, the pooling layer p k shrinks the
i,j      cxi,j
representation of the detection layer h k by a factor
of c along each dimension, where c is a small in-
teger such as 2 or 3.
partitioned into blocks of size c     c, and each block
    is connected to exactly one binary unit pk
pooling layer (i.e., np = nh/c). formally, we de   ne
b    , {(i, j) : hij belongs to the block    .}.
the detection units in the block b    and the pooling
unit p    are connected in a single potential which en-
forces the following constraints: at most one of the
detection units may be on, and the pooling unit is on
if and only if a detection unit is on. equivalently, we
1
can consider these c2+1 units as a single random vari-
able which may take on one of c2 + 1 possible values:
one value for each of the detection units being on, and
set of shared weights   = { 1,1, . . . ,  k,k0}, where
one value indicating that all units are o   .
 k,` is a weight matrix connecting pooling unit p k to

(2)
increase in energy caused by turning on unit hk
i,j is
now, we sample each block independently as a multi-
 i(hk
i,j), and the id155 is given by:
i,j is a hid-
nomial function of its inputs. suppose hk
den unit contained in block     (i.e., (i, j) 2 b   ), the
i,j))
exp(i(hk
p (hk
i0,j0))
exp(i(hk
i0,j0)) .
sampling the visible layer v given the hidden layer
h can be performed in the same way as described in
section 3.2.

i,j = 1|v) =
sampling the visible layer v given the hidden layer
h can be performed in the same way as described in
    = 0|v) =
section 3.2.
3.4. training via sparsity id173
our model is overcomplete in that the size of the rep-

1 +p(i0,j0)2b   
1 +p(i0,j0)2b   

nv
convolutional id50 for scalable unsupervised learning of hierarchical representations

we now discuss sampling the detection layer h and
the pooling layer p given the visible layer v . group k
receives the following bottom-up signal from layer v :

(nw , nv   nh + 1); the    lter weights are shared
across all the hidden units within the group. in addi-
1
tion, each hidden group has a bias bk and all visible

figure 1. convolutional rbm with probabilistic max-
pooling. for simplicity, only group k of the detection layer
and the pooing layer are shown. the basic crbm corre-

i,j))
exp(i(hk

i,j(   w k     v)i,j + bkhk

exp(i(hk

exp(i(hk

implies all         are 0

implies       is 1

exp(i(hk

hk (detection layer)

pk (pooling layer)

v  (visible layer)

exp(i(hk

subj. to

p (pk

hk
i,j

(1)

wk

pk
  

vi,j

42

nw

v

nh

np

c

convolutional id50 for scalable unsupervised learning of hierarchical representations

convolutional rbm

lee et al. 2009
topics: convolutional rbm
    using these adapted conditionals, we can perform contrastive 
divergence
    energy gradients involve convolutions, similar to the backprop gradients in 

convolutional network

    can stack convolutional rbms

    provides a pretraining procedure which doesn   t require the extraction of patches

    see lee et al. 2009 for more details

43

conclusion

{ makes hidden units

    we presented how to tackle id161 problems with 
convolutional neural networks
    locally connected units
    parameter sharing across feature maps
    max pooling of feature maps

more invariant to
small translations
    discussed how to improve their performance on object 
recognition problems
    described a procedure to augment training set and improve 
invariance to other transformations
    presented a convolutional variant of rbms

44

