neural networks
natural language processing - neural network language model

id38

2

topics: id165 model
    issue: data sparsity

    we want n to be large, for the model to be realistic
    however, for large values of n, it is likely that a given id165 will not have been 

observed in the training corpora
    smoothing the counts can help

- combine count(w1 , w2 , w3 , w4), count(w2 , w3 , w4), count(w3 , w4), and count(w4) to 

estimate p(w4 |w1, w2, w3)

    this only partly solves the problem

bengio, ducharme, vincent and jauvin

neural network language model 3
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

index for

bengio, ducharme, vincent and jauvin

neural network language model 3
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

index for

bengio, ducharme, vincent and jauvin

neural network language model 3
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

index for

bengio, ducharme, vincent and jauvin

neural network language model 3
topics: neural network language model
    solution: model the conditional 
p(wt | wt   (n   1) , ... ,wt   1) 
with a neural network
    learn word representations
to allow transfer to id165s
not observed in training corpus 

i-th output = p(wt = i | context)

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

bengio, ducharme,
vincent and jauvin, 2003

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

index for

neural network language model 4
topics: neural network language model
    can potentially generalize to contexts not seen in training set

    example: p(       eating        |        the       ,        cat       ,        is       ) 

-

-

imagine 4-gram [       the       ,        cat       ,        is       ,         eating        ] is not in training corpus, 
but [       the       ,        dog       ,        is       ,         eating        ] is
if the word representations of        cat        and        dog        are similar, then the neural network will 
be able to generalize to the case of        cat        

- neural network could learn similar word representations for those words based on other 

4-grams:
                          [       the       ,        cat       ,        was       ,         sleeping        ] 
                          [       the       ,        dog       ,        was       ,         sleeping        ]

math for my slides    natural language processing   .

math for my slides    natural language processing   .

abstract

november 13, 2012
hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

neural network language model 5
topics: word representation gradients
hugo.larochelle@usherbrooke.ca
    we know how to propagate gradients 
in such a network
    we know how to compute the gradient for the 
n 1xi=1

linear activation of the hidden layer

c(w) (= c(w)      rc(w)l

november 13, 2012

1(wt i=w) w>i ra(x)l

bengio, ducharme, vincent and jauvin

rc(w)l =

i-th output = p(wt = i | context)

abstract

most  computation here

softmax

tanh

. . .

. . .

. . .

. . .

    let   s note the submatrix connecting wt   i and the 

hidden layer as wi

    the gradient wrt c(w) for any w is

c(w) (= c(w)      rc(w)l

wn-1

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

c(wt 1)

w2 w1
c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

. . .

c

index for

rc(w)l =

n 1xi=1

1(wt i=w) w>i ra(x)l

bengio, ducharme,
neural network and c(i) is the i-th word feature vector.
vincent and jauvin, 2003

figure 1: neural architecture: f (i,wt 1,       ,wt n+1) = g(i,c(wt 1),       ,c(wt n+1)) where g is the
parameters of the mapping c are simply the feature vectors themselves, represented by a |v|    m
matrix c whose row i is the feature vector c(i) for word i. the function g may be implemented by a
feed-forward or recurrent neural network or another parametrized function, with parameters   . the
overall parameter set is    = (c,  ).
training is achieved by looking for    that maximizes the training corpus penalized log-likelihood:

l =

1
t    t

log f (wt,wt 1,       ,wt n+1;  ) +r(  ),

abstract
abstract
abstract
abstract

neural network language model 6
math for my slides    natural language processing   .
math for my slides    natural language processing   .
math for my slides    natural language processing   .
math for my slides    natural language processing   .
   
   
topics: word representation gradients
   
   
    example:    [       the       ,        dog       ,        and       ,         the       ,         cat        ] 
   
   
w7
w3
w6
   
n 1xi=1
   
n 1xi=1
n 1xi=1
1(wt i=w) w>i ra(x)l
n 1xi=1
rc(w)l =
1(wt i=w) w>i ra(x)l
=21 =3 =14 =21
rc(w)l =
1(wt i=w) w>i ra(x)l
rc(w)l =
1(wt i=w) w>i ra(x)l
rc(w)l =

c(w) (= c(w)      rc(w)l
c(w) (= c(w)      rc(w)l
c(w) (= c(w)      rc(w)l
c(w) (= c(w)      rc(w)l

w4

w5

    w1 w2 wn 1
    w1 w2 wn 1
    the loss is l =     log p(       cat        |        the       ,        dog       ,        and       ,         the       )
    w1 w2 wn 1
    w1 w2 wn 1
    rc(3)l = w>3 ra(x)l
    rc(3)l = w>3 ra(x)l
     
    rc(3)l = w>3 ra(x)l
    rc(3)l = w>3 ra(x)l
    rc(14)l = w>2 ra(x)l
     
    rc(14)l = w>2 ra(x)l
    rc(14)l = w>2 ra(x)l
    rc(14)l = w>2 ra(x)l
     
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
    rc(21)l = w>1 ra(x)l + w>4 ra(x)l
                       for all other words w 
    rc(w)l = 0
    rc(w)l = 0
    rc(w)l = 0
    rc(w)l = 0
    only need to update the representations c(3), c(14) and 
c(21),

neural network language model 7
topics: performance evaluation
    in id38, a common evaluation metric is the 
perplexity
    it is simply the exponential of the average negative log-likelihood

    evaluation on brown corpus

    id165 model (kneser-ney smoothing): 321
    neural network language model: 276
    neural network + id165: 252

bengio, ducharme,
vincent and jauvin, 2003

neural network language model 8
topics: performance evaluation
    a more interesting (and less straightforward) way of 
evaluating a language model is within a particular application
    does a language model improve the performance of a machine translation or 

id103 system

    later work has shown improvements in both cases

    connectionist id38 for large vocabulary continuous speech 

recognition
schwenk and gauvain, 2002 

    continuous-space language models for id151

schwenk, 2010

