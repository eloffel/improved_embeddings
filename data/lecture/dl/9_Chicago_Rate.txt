ttic 31230, fundamentals of deep learning

david mcallester, winter 2018

rate-distortion autoencoders

1

rate-distortion autoencoders

given an image or sound wave y we can compress it into a
coded form c  (y).
let |c  (y)| denote the number of bits in the compressed form
c  (y).

we let   y  (c  (y)) be the decompression of c  (y).

we let d(y,   y) be a measure of the di   erence between y and
  y (the loss or    distortion   ).

2

rate-distortion autoencoders

      = argmin

  

ey   pop |c  (y)| +   d(y,   y  (c  (y)))

it is common to take

d(y,   y) = ||y       y||2

or

d(y,   y) = ||y       y||1

3

(l2)

(l1)

conditional rate-distortion autoencoders

      = argmin

  

e(x,y)   pop

|c  (y|x)| +   d(y |   y  (x, c  (y|x)))

4

colorization

  

      = argmin

e(x,y)   pop |c  (y|x)| +

  ||y       y  (x, c  (y|x))||2
if the image can be segmented based on x then c  (y|x) can
be a speci   cation of color of each segment     this would be
very compact.

1
2

5

a case study in image compression

end-to-end optimized image compression, balle,
laparra, simoncelli, iclr 2017.

      = argmin

  

ey   pop |c  (y)| +

  ||y       y  (c  (y))||2

1
2

6

jpeg at 4283 bytes or .121 bits per pixel

7

jpeg 2000 at 4004 bytes or .113 bits per pixel

8

proposed method at 3986 bytes or .113 bits per pixel

9

the encoder c  (y)

this paper uses a three layer id98 for the encoder.

the    rst layer is computed stride 4.

the two remaining layers are computed stride 2.

they use a generilized divisive id172 (gdn) layer
rather than an activation function.

y(cid:48)[i, j, c] =

(cid:16)

  c +(cid:80)

y[i, j, c]

c(cid:48)   c,c(cid:48) y[i, j, c(cid:48)]2(cid:17)1/2

  c and   c,c(cid:48) =   c(cid:48),c are trained.

10

the number of numbers

the    rst layer is computed stride 4.

the next two layers are computed stride 2.

final image dimension is reduced by a factor of 16 with 192
channels per pixel (192 channels is for color images).

192 < 16    16    3 = 768

these 192 numbers are rounded to integers.

the 192 integers are coded losslessly using p code
   .

11

increasing spatial dimension in decoding

[hyeonwoo noh et al.]

12

increasing spatial dimensions in deconvolution

consider a stride 2 convolution

y[i, j, cy] = w [   i,    j, cx, cy]x[2i +    i, 2j +    j, cx]
y[i, j, cy] += b[cy]

for deconvolution we use stride 1 with 4 times the channels.

  x[i, j, c  x] = w(cid:48)[   i,    j, c  y, c  x]  y[i +    i, j +    j, c  x]
  x[i, j, c  x] += b[c  x]

the channels at each lower resolution pixel   x[i, j] are divided
among four higher resolution pixels.

this is done by a simple reshaping of   x.

13

the decoder

this is a deconvolution network of the same architecture with
independent parameters.

there is a special parameterization of the    inverter    for the
id172 layer.

14

rounding the numbers

we let c  (x) be the unrounded numerical representation and
  c  (x) be the result of rounding.

  c  (x)i = round(c  (x)i) = (cid:98)c  (x)i + 1/2(cid:99)

each integer channel of the    nal layer is coded independently.

context-based adaptive binary arithmetic coding framework
(cabac; marpe, schwarz, and wiegand, 2003).

15

training

we now have the optimization problem
      = argmin

ey   pop    log2 p  (   c  (y)) +   ||y      y  (   c  (y))||2

  

issue: the rounding causes the gradients for    to be zero.

16

modeling rounding with noise

      = argmin

  

ey   pop    log2 p  (   c  (y)) +   ||y      y  (   c  (y))||2

at train time (but not test time) the rounding is replaced with
additive noise.

      = argmin

  

ey,    log2 p  (c  (y) +  )+  ||y     y  (c  (y) +  )||2

 i drawn uniformly from [   1/2, 1/2]

p   de   nes a piecewise linear density for each coordinate of z.

17

noise vs. rounding

18

di   erential id178 vs. discrete id178

19

varying the level of compression

      = argmin

  

ey   pop   log2 p  (   c  (y))+

  ||y     y  (   c  (y))||2

1
2

di   erent levels of compression correspond to di   erent values
of   .

in all levels of compression we replace 768 numbers by 192
numbers.

higher levels of compression result in more compact distribu-
tions on the 192 numbers.

20

end

