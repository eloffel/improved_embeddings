cs6200	
   

informa.on	
   retrieval	
   

david	
   smith	
   

college	
   of	
   computer	
   and	
   informa.on	
   science	
   

northeastern	
   university	
   

query	
   process	
   

retrieval	
   models	
   

       provide	
   a	
   mathema.cal	
   framework	
   for	
   

de   ning	
   the	
   search	
   process	
   
      includes	
   explana.on	
   of	
   assump.ons	
   
      basis	
   of	
   many	
   ranking	
   algorithms	
   
      can	
   be	
   implicit	
   

       retrieval	
   model	
   developed	
   by	
   trial	
   and	
   error	
   

       progress	
   in	
   retrieval	
   models	
   has	
   corresponded	
   
       theories	
   about   i.e.,	
   models	
   of   relevance	
   

with	
   improvements	
   in	
   e   ec.veness	
   

relevance	
   

       complex	
   concept	
   that	
   has	
   been	
   studied	
   for	
   

some	
   .me	
   
      many	
   factors	
   to	
   consider	
   	
   
      people	
   omen	
   disagree	
   when	
   making	
   relevance	
   
judgments	
   

       retrieval	
   models	
   make	
   various	
   assump.ons	
   

about	
   relevance	
   to	
   simplify	
   problem	
   
      e.g.,	
   topical	
   vs.	
   user	
   relevance	
   
      e.g.,	
   binary	
   vs.	
   mul1-     valued	
   relevance	
   

topical	
   vs.	
   user	
   relevance	
   

       topical	
   relevance	
   

      document	
   and	
   query	
   are	
   on	
   the	
   same	
   topic	
   
      query:	
      u.s.	
   presidents   	
   
      document:	
   	
   wikipedia	
   ar.cle	
   on	
   abraham	
   lincoln	
   

       user	
   relevance	
   

      incorporate	
   factors	
   beside	
   document	
   topic	
   

       document	
   freshness	
   
       style	
   	
   	
   
       content	
   presenta.on	
   

binary	
   vs.	
   mul.-     valued	
   relevance	
   
       binary	
   relevance	
   

      the	
   document	
   is	
   either	
   relevant	
   or	
   not	
   	
   

       mul.-     valued	
   relevance	
   

      makes	
   the	
   evalua.on	
   task	
   easier	
   for	
   the	
   judges	
   
      not	
   as	
   important	
   for	
   retrieval	
   models	
   
      many	
   retrieval	
   models	
   calculate	
   the	
   id203	
   of	
   
relevance	
   	
   

retrieval	
   model	
   overview	
   

       older	
   models	
   

      boolean	
   retrieval	
   
      vector	
   space	
   model	
   
       probabilis.c	
   models	
   

      bm25	
   
      language	
   models	
   

       combining	
   evidence	
   
      id136	
   networks	
   
      learning	
   to	
   rank	
   

boolean	
   retrieval	
   

       two	
   possible	
   outcomes	
   for	
   query	
   processing	
   

      true	
   and	
   false	
   
         exact-     match   	
   retrieval;	
      set   	
   retrieval	
   
      simplest	
   form	
   of	
   ranking	
   

       query	
   usually	
   speci   ed	
   using	
   boolean	
   

operators	
   
      and,	
   or,	
   not	
   
      proximity	
   operators	
   and	
   wildcards	
   also	
   used	
   

boolean	
   retrieval	
   

       advantages	
   

      results	
   are	
   predictable,	
   rela.vely	
   easy	
   to	
   explain	
   
      many	
   di   erent	
   features	
   can	
   be	
   incorporated	
   
      e   cient	
   processing	
   since	
   many	
   documents	
   can	
   be	
   
eliminated	
   from	
   search	
   

       disadvantages	
   

      e   ec.veness	
   depends	
   en.rely	
   on	
   user	
   
      simple	
   queries	
   usually	
   don   t	
   work	
   well	
   
      complex	
   queries	
   are	
   di   cult	
   

searching	
   by	
   numbers	
   
       sequence	
   of	
   queries	
   driven	
   by	
   number	
   of	
   

lincoln	
   

retrieved	
   documents	
   
1.   
2.    president	
   and	
   lincoln	
   
3.    president	
   and	
   lincoln	
   and	
   not	
   (automobile	
   or	
   car)	
   
4.    president	
   and	
   lincoln	
   and	
   biography	
   and	
   life	
   and	
   

birthplace	
   and	
   geeysburg	
   and	
   not	
   (automobile	
   
or	
   car)	
   

5.    president	
   and	
   lincoln	
   and	
   (biography	
   or	
   life	
   or	
   

birthplace	
   or	
   geeysburg)	
   and	
   not	
   (automobile	
   or	
   
car)	
   

vector	
   space	
   model	
   

       documents	
   and	
   query	
   represented	
   by	
   a	
   vector	
   

of	
   term	
   weights	
   

       collec.on	
   represented	
   by	
   a	
   matrix	
   of	
   term	
   

weights	
   

vector	
   space	
   model	
   

vector	
   space	
   model	
   

       query:	
      tropical	
      sh   	
   
	
   

term	
   

aquarium	
   
bowl	
   	
   
care	
   
   sh	
   
freshwater	
   
gold   sh	
   
homepage	
   
keep	
   
setup	
   
tank	
   
tropical	
   

query	
   

0	
   
0	
   
0	
   
1	
   
0	
   
0	
   
0	
   
0	
   
0	
   
0	
   
1	
   

vector	
   space	
   model	
   

       3-     d	
   pictures	
   useful,	
   but	
   can	
   be	
   misleading	
   for	
   

high-     dimensional	
   space	
   

vector	
   space	
   model	
   

       documents	
   ranked	
   by	
   distance	
   between	
   
points	
   represen.ng	
   query	
   and	
   documents	
   
      similarity	
   measure	
   more	
   common	
   than	
   a	
   distance	
   
or	
   dissimilarity	
   measure	
   
      e.g.	
   cosine	
   correla.on	
   

similarity	
   calcula.on	
   

      consider	
   two	
   documents	
   d1,	
   d2	
   and	
   a	
   query	
   q	
   

       d1	
   =	
   (0.5,	
   0.8,	
   0.3),	
   d2	
   =	
   (0.9,	
   0.4,	
   0.2),	
   q	
   =	
   (1.5,	
   1.0,	
   0)	
   

di   erence	
   from	
   boolean	
   retrieval	
   
       similarity	
   calcula.on	
   has	
   two	
   factors	
   that	
   

dis.nguish	
   it	
   from	
   boolean	
   retrieval	
   
      number	
   of	
   matching	
   terms	
   a   ects	
   similarity	
   
      weight	
   of	
   matching	
   terms	
   a   ects	
   similarity	
   

       documents	
   can	
   be	
   ranked	
   by	
   their	
   similarity	
   

scores	
   

term	
   weights	
   

       =.idf	
   weight	
   

      term	
   frequency	
   weight	
   measures	
   importance	
   in	
   
document:	
   
	
   
      inverse	
   document	
   frequency	
   measures	
   
importance	
   in	
   collec.on:	
   
      heuris.c	
   combina.on	
   

relevance	
   feedback	
   

       rocchio	
   algorithm	
   
       op1mal	
   query	
   	
   

      maximizes	
   the	
   di   erence	
   between	
   the	
   average	
   
vector	
   represen.ng	
   the	
   relevant	
   documents	
   and	
   
the	
   average	
   vector	
   represen.ng	
   the	
   non-     relevant	
   
documents	
   

       modi   es	
   query	
   according	
   to	
   

        ,	
     ,	
   and	
     	
   are	
   parameters	
   

       typical	
   values	
   8,	
   16,	
   4	
   

vector	
   space	
   model	
   

       advantages	
   

      simple	
   computa.onal	
   framework	
   for	
   ranking	
   
      any	
   similarity	
   measure	
   or	
   term	
   weigh.ng	
   scheme	
   
could	
   be	
   used	
   
       disadvantages	
   

      assump.on	
   of	
   term	
   independence	
   
      no	
   predic1ons	
   about	
   techniques	
   for	
   e   ec.ve	
   
ranking	
   

id203	
   ranking	
   principle	
   

       robertson	
   (1977)	
   

          if	
   a	
   reference	
   retrieval	
   system   s	
   response	
   to	
   each	
   

request	
   is	
   a	
   ranking	
   of	
   the	
   documents	
   in	
   the	
   collec.on	
   
in	
   order	
   of	
   decreasing	
   id203	
   of	
   relevance	
   to	
   the	
   
user	
   who	
   submieed	
   the	
   request,	
   	
   
       where	
   the	
   probabili.es	
   are	
   es.mated	
   as	
   accurately	
   as	
   

possible	
   on	
   the	
   basis	
   of	
   whatever	
   data	
   have	
   been	
   
made	
   available	
   to	
   the	
   system	
   for	
   this	
   purpose,	
   	
   

       the	
   overall	
   e   ec.veness	
   of	
   the	
   system	
   to	
   its	
   user	
   will	
   

be	
   the	
   best	
   that	
   is	
   obtainable	
   on	
   the	
   basis	
   of	
   those	
   
data.   	
   

ir	
   as	
   classi   ca.on	
   

bayes	
   classi   er	
   

       bayes	
   decision	
   rule	
   

      a	
   document	
   d	
   is	
   relevant	
   if	
   p(r|d)	
   >	
   p(nr|d)	
   

       es.ma.ng	
   probabili.es	
   

      use	
   bayes	
   rule	
   

      classify	
   a	
   document	
   as	
   relevant	
   if	
   

       this	
   is	
   likelihood	
   ra1o	
   

es.ma.ng	
   p(d|r)	
   

       assume	
   independence	
   

       binary	
   independence	
   model	
   

      document	
   represented	
   by	
   a	
   vector	
   of	
   binary	
   
features	
   indica.ng	
   term	
   occurrence	
   (or	
   non-     
occurrence)	
   
      pi	
   is	
   id203	
   that	
   term	
   i	
   occurs	
   (i.e.,	
   has	
   value	
   
1)	
   in	
   relevant	
   document,	
   si	
   	
   is	
   id203	
   of	
   
occurrence	
   in	
   non-     relevant	
   document	
   

binary	
   independence	
   model	
   

binary	
   independence	
   model	
   

       scoring	
   func.on	
   is	
   

       query	
   provides	
   informa.on	
   about	
   relevant	
   

documents	
   
       if	
   we	
   assume	
   pi	
   constant,	
   si	
   approximated	
   by	
   
en.re	
   collec.on,	
   get	
   idf-     like	
   weight	
   	
   

con.ngency	
   table	
   

gives	
   scoring	
   func.on:	
   

bm25	
   

       popular	
   and	
   e   ec.ve	
   ranking	
   algorithm	
   based	
   

on	
   binary	
   independence	
   model	
   
      adds	
   document	
   and	
   query	
   term	
   weights	
   

      k1,	
   k2	
   	
   and	
   k	
   are	
   parameters	
   whose	
   values	
   are	
   set	
   
empirically	
   
      	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   dl	
   is	
   doc	
   length	
   
      typical	
   trec	
   value	
   for	
   k1	
   is	
   1.2,	
   k2	
   	
   varies	
   from	
   0	
   to	
   
1000,	
   b	
   =	
   0.75	
   

bm25	
   example	
   

       query	
   with	
   two	
   terms,	
      president	
   lincoln   ,	
   (qf	
   =	
   1)	
   
       no	
   relevance	
   informa.on	
   (r	
   and	
   r	
   are	
   zero)	
   
       n	
   =	
   500,000	
   documents	
   
          president   	
   occurs	
   in	
   40,000	
   documents	
   (n1	
   =	
   40,	
   000)	
   
          lincoln   	
   occurs	
   in	
   300	
   documents	
   (n2	
   =	
   300)	
   
          president   	
   occurs	
   15	
   .mes	
   in	
   doc	
   (f1	
   =	
   15)	
   
          lincoln   	
   occurs	
   25	
   .mes	
   (f2	
   =	
   25)	
   
       document	
   length	
   is	
   90%	
   of	
   the	
   average	
   length	
   (dl/avdl	
   
       k1	
   =	
   1.2,	
   b	
   =	
   0.75,	
   and	
   k2	
   =	
   100	
   
       k	
   =	
   1.2	
        	
   (0.25	
   +	
   0.75	
        	
   0.9)	
   =	
   1.11	
   

=	
   .9)	
   	
   

bm25	
   example	
   

bm25	
   example	
   

       e   ect	
   of	
   term	
   frequencies	
   

language	
   model	
   

       language	
   model	
   
       unigram	
   language	
   model	
   

      id203	
   distribu.on	
   over	
   strings	
   of	
   text	
   

      genera.on	
   of	
   text	
   consists	
   of	
   pulling	
   words	
   out	
   of	
   
a	
      bucket   	
   according	
   to	
   the	
   id203	
   
distribu.on	
   and	
   replacing	
   them	
   

       n-     gram	
   language	
   model	
   

      some	
   applica.ons	
   use	
   bigram	
   and	
   trigram	
   
language	
   models	
   where	
   probabili.es	
   depend	
   on	
   
previous	
   words	
   

language	
   model	
   

       a	
   topic	
   in	
   a	
   document	
   or	
   query	
   can	
   be	
   
represented	
   as	
   a	
   language	
   model	
   
      i.e.,	
   words	
   that	
   tend	
   to	
   occur	
   omen	
   when	
   
discussing	
   a	
   topic	
   will	
   have	
   high	
   probabili.es	
   in	
   
the	
   corresponding	
   language	
   model	
   

       mul1nomial	
   distribu.on	
   over	
   words	
   

      text	
   is	
   modeled	
   as	
   a	
      nite	
   sequence	
   of	
   words,	
   
where	
   there	
   are	
   t	
   possible	
   words	
   at	
   each	
   point	
   in	
   
the	
   sequence	
   
      commonly	
   used,	
   but	
   not	
   only	
   possibility	
   
      doesn   t	
   model	
   burs1ness	
   

lms	
   for	
   retrieval	
   

       3	
   possibili.es:	
   

      id203	
   of	
   genera.ng	
   the	
   query	
   text	
   from	
   a	
   
document	
   language	
   model	
   
      id203	
   of	
   genera.ng	
   the	
   document	
   text	
   from	
   
a	
   query	
   language	
   model	
   
      comparing	
   the	
   language	
   models	
   represen.ng	
   the	
   
query	
   and	
   document	
   topics	
   
       models	
   of	
   topical	
   relevance	
   

query-     likelihood	
   model	
   

       rank	
   documents	
   by	
   the	
   id203	
   that	
   the	
   
query	
   could	
   be	
   generated	
   by	
   the	
   document	
   
model	
   (i.e.	
   same	
   topic)	
   

       given	
   query,	
   start	
   with	
   p(d|q)	
   
       using	
   bayes   	
   rule	
   	
   

       assuming	
   prior	
   is	
   uniform,	
   unigram	
   model	
   

es.ma.ng	
   probabili.es	
   

       obvious	
   es.mate	
   for	
   unigram	
   probabili.es	
   is	
   	
   

       maximum	
   likelihood	
   es1mate	
   

      makes	
   the	
   observed	
   value	
   of	
   fqi;d	
   most	
   likely	
   

       if	
   query	
   words	
   are	
   missing	
   from	
   document,	
   

score	
   will	
   be	
   zero	
   
      missing	
   1	
   out	
   of	
   4	
   query	
   words	
   same	
   as	
   missing	
   3	
   
out	
   of	
   4	
   

smoothing	
   

       document	
   texts	
   are	
   a	
   sample	
   from	
   the	
   language	
   

model	
   
       missing	
   words	
   should	
   not	
   have	
   zero	
   id203	
   of	
   

occurring	
   

       smoothing	
   is	
   a	
   technique	
   for	
   es.ma.ng	
   

words	
   that	
   are	
   seen	
   in	
   the	
   document	
   text	
   

probabili.es	
   for	
   missing	
   (or	
   unseen)	
   words	
   
       lower	
   (or	
   discount)	
   the	
   id203	
   es.mates	
   for	
   
       assign	
   that	
      lem-     over   	
   id203	
   to	
   the	
   es.mates	
   for	
   
       what	
   does	
   this	
   do	
   to	
   the	
   likelihood	
   of	
   the	
   document?	
   

the	
   words	
   that	
   are	
   not	
   seen	
   in	
   the	
   text	
   

es.ma.ng	
   probabili.es	
   
       es.mate	
   for	
   unseen	
   words	
   is	
     dp(qi|c)	
   
      p(qi|c)	
   is	
   the	
   id203	
   for	
   query	
   word	
   i	
   in	
   the	
   
collec1on	
   language	
   model	
   for	
   collec.on	
   c	
   
(background	
   id203)	
   
        d	
   is	
   a	
   parameter	
   

       es.mate	
   for	
   words	
   that	
   occur	
   is	
   
	
   	
   	
   	
   	
   	
   	
   (1	
      	
     d)	
   p(qi|d)	
   +	
     d	
   p(qi|c)	
   
       di   erent	
   forms	
   of	
   es.ma.on	
   come	
   from	
   

di   erent	
     d	
   

jelinek-     mercer	
   smoothing	
   

         d	
   is	
   a	
   constant,	
     	
   
       gives	
   es.mate	
   of	
   

       ranking	
   score	
   
	
   
       use	
   logs	
   for	
   convenience	
   	
   

      accuracy	
   problems	
   mul.plying	
   small	
   numbers	
   

where	
   is	
   =.idf	
   weight?	
   

-     	
   propor.onal	
   to	
   the	
   term	
   frequency,	
   inversely	
   	
   	
   	
   

propor.onal	
   to	
   the	
   collec.on	
   frequency	
   

dirichlet	
   smoothing	
   

         d	
   depends	
   on	
   document	
   length	
   
	
   
       gives	
   id203	
   es.ma.on	
   of	
   	
   

	
   

       and	
   document	
   score	
   

query	
   likelihood	
   example	
   

       for	
   the	
   term	
      president   	
   
       fqi,d	
   =	
   15,	
   cqi	
   =	
   160,000	
   
       for	
   the	
   term	
      lincoln   	
   

       fqi,d	
   =	
   25,	
   cqi	
   =	
   2,400	
   

d|	
   is	
   assumed	
   to	
   be	
   1,800	
   

       number	
   of	
   word	
   occurrences	
   in	
   the	
   document	
   |
       number	
   of	
   word	
   occurrences	
   in	
   the	
   collec.on	
   is	
   
109	
   	
   
       500,000	
   documents	
   .mes	
   an	
   average	
   of	
   2,000	
   words	
   

         	
   =	
   2,000	
   

query	
   likelihood	
   example	
   

      	
   	
   nega.ve	
   number	
   because	
   summing	
   logs	
   

	
   of	
   small	
   numbers	
   

query	
   likelihood	
   example	
   

relevance	
   models	
   
       relevance	
   model	
      	
   language	
   model	
   

represen.ng	
   informa.on	
   need	
   
      query	
   and	
   relevant	
   documents	
   are	
   samples	
   from	
   
this	
   model	
   
       p(d|r)	
   -     	
   id203	
   of	
   genera.ng	
   the	
   text	
   in	
   a	
   

document	
   given	
   a	
   relevance	
   model	
   
      document	
   likelihood	
   model	
   
      less	
   e   ec.ve	
   than	
   query	
   likelihood	
   due	
   to	
   
di   cul.es	
   comparing	
   across	
   documents	
   of	
   
di   erent	
   lengths	
   

pseudo-     relevance	
   feedback	
   
       es.mate	
   relevance	
   model	
   from	
   query	
   and	
   

top-     ranked	
   documents	
   

       rank	
   documents	
   by	
   similarity	
   of	
   document	
   

model	
   to	
   relevance	
   model	
   

       kullback-     leibler	
   divergence	
   (kl-     divergence)	
   is	
   

a	
   well-     known	
   measure	
   of	
   the	
   di   erence	
   
between	
   two	
   id203	
   distribu.ons	
   

kl-     divergence	
   

       given	
   the	
   true	
   id203	
   distribu.on	
   p	
   and	
   

another	
   distribu.on	
   q	
   that	
   is	
   an	
   
approxima1on	
   to	
   p,	
   

      use	
   nega.ve	
   kl-     divergence	
   for	
   ranking,	
   and	
   
assume	
   relevance	
   model	
   r	
   is	
   the	
   true	
   distribu.on	
   
(not	
   symmetric),	
   

kl-     divergence	
   

       given	
   a	
   simple	
   maximum	
   likelihood	
   es.mate	
   

for	
   p(w|r),	
   based	
   on	
   the	
   frequency	
   in	
   the	
   
query	
   text,	
   ranking	
   score	
   is	
   

      rank-     equivalent	
   to	
   query	
   likelihood	
   score	
   

       query	
   likelihood	
   model	
   is	
   a	
   special	
   case	
   of	
   

retrieval	
   based	
   on	
   relevance	
   model	
   

es.ma.ng	
   the	
   relevance	
   model	
   

       id203	
   of	
   pulling	
   a	
   word	
   w	
   out	
   of	
   the	
   

   bucket   	
   represen.ng	
   the	
   relevance	
   model	
   
depends	
   on	
   the	
   n	
   query	
   words	
   we	
   have	
   just	
   
pulled	
   out	
   

       by	
   de   ni.on	
   

es.ma.ng	
   the	
   relevance	
   model	
   

       joint	
   id203	
   is	
   

	
   

       assume	
   

       gives	
   

es.ma.ng	
   the	
   relevance	
   model	
   

       p(d)	
   usually	
   assumed	
   to	
   be	
   uniform	
   
       p(w,	
   q1	
   .	
   .	
   .	
   qn)	
   is	
   simply	
   a	
   weighted	
   average	
   of	
   
the	
   language	
   model	
   probabili.es	
   for	
   w	
   in	
   a	
   set	
   
of	
   documents,	
   where	
   the	
   weights	
   are	
   the	
   
query	
   likelihood	
   scores	
   for	
   those	
   documents	
   
       formal	
   model	
   for	
   pseudo-     relevance	
   feedback	
   

      query	
   expansion	
   technique	
   

pseudo-     feedback	
   algorithm	
   

example	
   from	
   top	
   10	
   docs	
   

example	
   from	
   top	
   50	
   docs	
   

combining	
   evidence	
   

       e   ec.ve	
   retrieval	
   requires	
   the	
   combina.on	
   of	
   
many	
   pieces	
   of	
   evidence	
   about	
   a	
   document   s	
   
poten.al	
   relevance	
   
      have	
   focused	
   on	
   simple	
   word-     based	
   evidence	
   
      many	
   other	
   types	
   of	
   evidence	
   

       structure,	
   id95,	
   metadata,	
   even	
   scores	
   from	
   
di   erent	
   models	
   

       id136	
   network	
   model	
   is	
   one	
   approach	
   to	
   

combining	
   evidence	
   
      uses	
   bayesian	
   network	
   formalism	
   

id136	
   network	
   

id136	
   network	
   

       document	
   node	
   (d)	
   corresponds	
   to	
   the	
   event	
   
that	
   a	
   document	
   is	
   observed	
   
       representa1on	
   nodes	
   (ri)	
   are	
   document	
   
features	
   (evidence)	
   
      probabili.es	
   associated	
   with	
   those	
   features	
   are	
   
based	
   on	
   language	
   models	
     	
   es.mated	
   using	
   the	
   
parameters	
     	
   
      one	
   language	
   model	
   for	
   each	
   signi   cant	
   
document	
   structure	
   
      ri	
   	
   nodes	
   can	
   represent	
   proximity	
   features,	
   or	
   other	
   
types	
   of	
   evidence	
   (e.g.	
   date)	
   

id136	
   network	
   
       query	
   nodes	
   (qi)	
   are	
   used	
   to	
   combine	
   
evidence	
   from	
   representa.on	
   nodes	
   and	
   
other	
   query	
   nodes	
   
      represent	
   the	
   occurrence	
   of	
   more	
   complex	
   
evidence	
   and	
   document	
   features	
   
      a	
   number	
   of	
   combina.on	
   operators	
   are	
   available	
   

       informa1on	
   need	
   node	
   (i)	
   is	
   a	
   special	
   query	
   
node	
   that	
   combines	
   all	
   of	
   the	
   evidence	
   from	
   
the	
   other	
   query	
   nodes	
   
      network	
   computes	
   p(i|d,	
     )	
   

example:	
   and	
   combina.on	
   

a	
   and	
   b	
   are	
   parent	
   nodes	
   for	
   q	
   

example:	
   and	
   combina.on	
   

       combina.on	
   must	
   consider	
   all	
   possible	
   states	
   

       some	
   combina.ons	
   can	
   be	
   computed	
   

of	
   parents	
   

e   ciently	
   

id136	
   network	
   operators	
   

galago	
   query	
   language	
   

       a	
   document	
   is	
   viewed	
   as	
   a	
   sequence	
   of	
   text	
   

that	
   may	
   contain	
   arbitrary	
   tags	
   

       a	
   single	
   context	
   is	
   generated	
   for	
   each	
   unique	
   

tag	
   name	
   

       an	
   extent	
   is	
   a	
   sequence	
   of	
   text	
   that	
   appears	
   
within	
   a	
   single	
   begin/end	
   tag	
   pair	
   of	
   the	
   same	
   
type	
   as	
   the	
   context	
   

galago	
   query	
   language	
   

galago	
   query	
   language	
   

texpoint	
   display	
   

galago	
   query	
   language	
   

galago	
   query	
   language	
   

galago	
   query	
   language	
   

galago	
   query	
   language	
   

galago	
   query	
   language	
   

galago	
   query	
   language	
   

web	
   search	
   

       most	
   important,	
   but	
   not	
   only,	
   search	
   
       major	
   di   erences	
   to	
   trec	
   news	
   

applica.on	
   

      size	
   of	
   collec.on	
   
      connec.ons	
   between	
   documents	
   
      range	
   of	
   document	
   types	
   
      importance	
   of	
   spam	
   
      volume	
   of	
   queries	
   
      range	
   of	
   query	
   types	
   

search	
   taxonomy	
   

       informa1onal	
   

      finding	
   informa.on	
   about	
   some	
   topic	
   which	
   may	
   
be	
   on	
   one	
   or	
   more	
   web	
   pages	
   
      topical	
   search	
   

       naviga1onal	
   

       transac1onal	
   

         nding	
   a	
   par.cular	
   web	
   page	
   that	
   the	
   user	
   has	
   
either	
   seen	
   before	
   or	
   is	
   assumed	
   to	
   exist	
   

         nding	
   a	
   site	
   where	
   a	
   task	
   such	
   as	
   shopping	
   or	
   
downloading	
   music	
   can	
   be	
   performed	
   

web	
   search	
   

       for	
   e   ec.ve	
   naviga.onal	
   and	
   transac.onal	
   

search,	
   need	
   to	
   combine	
   features	
   that	
   re   ect	
   
user	
   relevance	
   

       commercial	
   web	
   search	
   engines	
   combine	
   

evidence	
   from	
   hundreds	
   of	
   features	
   to	
   
generate	
   a	
   ranking	
   score	
   for	
   a	
   web	
   page	
   
      page	
   content,	
   page	
   metadata,	
   anchor	
   text,	
   links	
   
(e.g.,	
   id95),	
   and	
   user	
   behavior	
   (click	
   logs)	
   
      page	
   metadata	
      	
   e.g.,	
      age   ,	
   how	
   omen	
   it	
   is	
   
updated,	
   the	
   url	
   of	
   the	
   page,	
   the	
   domain	
   name	
   
of	
   its	
   site,	
   and	
   the	
   amount	
   of	
   text	
   content	
   

search	
   engine	
   op.miza.on	
   

       seo:	
   understanding	
   the	
   rela.ve	
   importance	
   of	
   
features	
   used	
   in	
   search	
   and	
   how	
   they	
   can	
   be	
   
manipulated	
   to	
   obtain	
   beeer	
   search	
   rankings	
   
for	
   a	
   web	
   page	
   
      e.g.,	
   improve	
   the	
   text	
   used	
   in	
   the	
   .tle	
   tag,	
   
improve	
   the	
   text	
   in	
   heading	
   tags,	
   make	
   sure	
   that	
   
the	
   domain	
   name	
   and	
   url	
   contain	
   important	
   
keywords,	
   and	
   try	
   to	
   improve	
   the	
   anchor	
   text	
   and	
   
link	
   structure	
   
      some	
   of	
   these	
   techniques	
   are	
   regarded	
   as	
   not	
   
appropriate	
   by	
   search	
   engine	
   companies	
   

web	
   search	
   

       in	
   trec	
   evalua.ons,	
   most	
   e   ec.ve	
   features	
   
for	
   naviga.onal	
   search	
   are:	
   
      text	
   in	
   the	
   .tle,	
   body,	
   and	
   heading	
   (h1,	
   h2,	
   h3,	
   
and	
   h4)	
   parts	
   of	
   the	
   document,	
   the	
   anchor	
   text	
   of	
   
all	
   links	
   poin.ng	
   to	
   the	
   document,	
   the	
   id95	
   
number,	
   and	
   the	
   inlink	
   count	
   
       given	
   size	
   of	
   web,	
   many	
   pages	
   will	
   contain	
   all	
   

query	
   terms	
   
      ranking	
   algorithm	
   focuses	
   on	
   discrimina.ng	
   
between	
   these	
   pages	
   
      word	
   proximity	
   is	
   important	
   

term	
   proximity	
   

       many	
   models	
   have	
   been	
   developed	
   
       n-     grams	
   are	
   commonly	
   used	
   in	
   commercial	
   

web	
   search	
   

       dependence	
   model	
   based	
   on	
   id136	
   net	
   has	
   

been	
   e   ec.ve	
   in	
   trec	
   -     	
   e.g.	
   

example	
   web	
   query	
   

machine	
   learning	
   and	
   ir	
   

       considerable	
   interac.on	
   between	
   these	
      elds	
   

      rocchio	
   algorithm	
   (60s)	
   is	
   a	
   simple	
   learning	
   
approach	
   
      80s,	
   90s:	
   learning	
   ranking	
   algorithms	
   based	
   on	
   
user	
   feedback	
   
      2000s:	
   text	
   categoriza.on	
   

       limited	
   by	
   amount	
   of	
   training	
   data	
   
       web	
   query	
   logs	
   have	
   generated	
   new	
   wave	
   of	
   

research	
   
      e.g.,	
      learning	
   to	
   rank   	
   
	
   

genera.ve	
   vs.	
   discrimina.ve	
   

       all	
   of	
   the	
   probabilis.c	
   retrieval	
   models	
   
presented	
   so	
   far	
   fall	
   into	
   the	
   category	
   of	
   
genera1ve	
   models	
   
      a	
   genera.ve	
   model	
   assumes	
   that	
   documents	
   
were	
   generated	
   from	
   some	
   underlying	
   model	
   (in	
   
this	
   case,	
   usually	
   a	
   mul.nomial	
   distribu.on)	
   and	
   
uses	
   training	
   data	
   to	
   es.mate	
   the	
   parameters	
   of	
   
the	
   model	
   
      id203	
   of	
   belonging	
   to	
   a	
   class	
   (i.e.	
   the	
   
relevant	
   documents	
   for	
   a	
   query)	
   is	
   then	
   es.mated	
   
using	
   bayes   	
   rule	
   and	
   the	
   document	
   model	
   

genera.ve	
   vs.	
   discrimina.ve	
   

       a	
   discrimina1ve	
   model	
   es.mates	
   the	
   

id203	
   of	
   belonging	
   to	
   a	
   class	
   directly	
   
from	
   the	
   observed	
   features	
   of	
   the	
   document	
   
based	
   on	
   the	
   training	
   data	
   

       genera.ve	
   models	
   perform	
   well	
   with	
   low	
   

numbers	
   of	
   training	
   examples	
   

       discrimina.ve	
   models	
   usually	
   have	
   the	
   
advantage	
   given	
   enough	
   training	
   data	
   
      can	
   also	
   easily	
   incorporate	
   many	
   features	
   

discrimina.ve	
   models	
   for	
   ir	
   
       discrimina.ve	
   models	
   can	
   be	
   trained	
   using	
   
explicit	
   relevance	
   judgments	
   or	
   click	
   data	
   in	
   
query	
   logs	
   

       large	
   class	
   of	
   algorithms	
   called	
   learning	
   to	
   

rank	
   
      learns	
   weights	
   on	
   a	
   linear	
   (or	
   non-     linear)	
   
combina.on	
   of	
   features	
   that	
   is	
   used	
   to	
   rank	
   
documents	
   
      best	
   weights	
   op.mize	
   a	
   performance	
   metric	
   

ranking	
   id166	
   

       training	
   data	
   is	
   

      r	
   is	
   par.al	
   rank	
   informa.on	
   

       if	
   document	
   da	
   should	
   be	
   ranked	
   higher	
   than	
   db,	
   then	
   
(da,	
   db)	
      	
   ri	
   
      par.al	
   rank	
   informa.on	
   comes	
   from	
   relevance	
   
judgments	
   (allows	
   mul.ple	
   levels	
   of	
   relevance)	
   or	
   
click	
   data	
   
       e.g.,	
   d1,	
   d2	
   	
   and	
   d3	
   are	
   the	
   documents	
   in	
   the	
      rst,	
   second	
   
and	
   third	
   rank	
   of	
   the	
   search	
   output,	
   only	
   d3	
   clicked	
   on	
   
   	
   (d3,	
   d1)	
   and	
   (d3,	
   d2)	
   will	
   be	
   in	
   desired	
   ranking	
   for	
   this	
   
query	
   

ranking	
   id166	
   

       learning	
   a	
   linear	
   ranking	
   func.on	
   	
   

      where	
   	
   w	
   	
   is	
   a	
   weight	
   vector	
   that	
   is	
   adjusted	
   by	
   
learning	
   
      da	
   is	
   the	
   vector	
   representa.on	
   of	
   the	
   features	
   of	
   
document	
   
      non-     linear	
   func.ons	
   also	
   possible	
   

       weights	
   represent	
   importance	
   of	
   features	
   

      learned	
   using	
   training	
   data	
   
      e.g.,	
   

ranking	
   id166	
   

       learn	
   w	
   that	
   sa.s   es	
   as	
   many	
   of	
   the	
   following	
   

condi.ons	
   as	
   possible:	
   

       can	
   be	
   formulated	
   as	
   an	
   op1miza1on	
   problem	
   

ranking	
   id166	
   

        ,	
   known	
   as	
   a	
   slack	
   variable,	
   allows	
   for	
   
misclassi   ca.on	
   of	
   di   cult	
   or	
   noisy	
   training	
   
examples,	
   and	
   c	
   is	
   a	
   parameter	
   that	
   is	
   used	
   to	
   
prevent	
   over   }ng	
   

ranking	
   id166	
   

       somware	
   available	
   to	
   do	
   op.miza.on	
   
       each	
   pair	
   of	
   documents	
   in	
   our	
   training	
   data	
   can	
   

be	
   represented	
   by	
   the	
   vector:	
   

       score	
   for	
   this	
   pair	
   is:	
   

       id166	
   classi   er	
   will	
      nd	
   a	
   w	
   that	
   makes	
   the	
   

smallest	
   score	
   as	
   large	
   as	
   possible	
   
       make	
   the	
   di   erences	
   in	
   scores	
   as	
   large	
   as	
   possible	
   for	
   

the	
   pairs	
   of	
   documents	
   that	
   are	
   hardest	
   to	
   rank	
   

topic	
   models	
   

       improved	
   representa.ons	
   of	
   documents	
   
       can	
   also	
   be	
   viewed	
   as	
   improved	
   smoothing	
   
       improve	
   es.mates	
   for	
   words	
   that	
   are	
   related	
   to	
   the	
   

techniques	
   

topic(s)	
   of	
   the	
   document	
   
       instead	
   of	
   just	
   using	
   background	
   probabili.es	
   

       approaches	
   
       heuris.c	
   
       latent	
   seman.c	
   indexing	
   (lsi)	
   
       probabilis.c	
   and	
   genera.ve	
   
       probabilis.c	
   latent	
   seman.c	
   indexing	
   (plsi)	
   
       latent	
   dirichlet	
   alloca.on	
   (lda)	
   

text	
   reuse	
   

topical	
   similarity	
   

parallel	
   bitext	
   

genehmigung	
   des	
   protokolls	
   
das	
   protokoll	
   der	
   sitzung	
   vom	
   
donnerstag,	
   den	
   28.	
   m  rz	
   1996	
   
wurde	
   verteilt.	
   
gibt	
   es	
   einw  nde?	
   
die	
   punkte	
   3	
   und	
   4	
   widersprechen	
   
sich	
   jetzt,	
   obwohl	
   es	
   bei	
   der	
   
abs.mmung	
   anders	
   aussah.	
   
das	
   mu  	
   ich	
   erst	
   einmal	
   kl  ren,	
   
frau	
   oomen-     ruijten.	
   

approval	
   of	
   the	
   minutes	
   
the	
   minutes	
   of	
   the	
   si}ng	
   of	
   
thursday,	
   28	
   march	
   1996	
   have	
   
been	
   distributed.	
   
are	
   there	
   any	
   comments?	
   
points	
   3	
   and	
   4	
   now	
   contradict	
   one	
   
another	
   whereas	
   the	
   vo.ng	
   
showed	
   otherwise.	
   
i	
   will	
   have	
   to	
   look	
   into	
   that,	
   mrs	
   
oomen-     ruijten.	
   	
   

koehn	
   (2005):	
   european	
   parliament	
   corpus	
   

mul.lingual	
   topical	
   similarity	
   

what	
   representa.on?	
   

       bag	
   of	
   words,	
   n-     grams,	
   etc.?	
   

      vocabulary	
   mismatch	
   within	
   language:	
   

       jobless	
   vs.	
   unemployed	
   

      what	
   about	
   between	
   languages?	
   
       translate	
   everything	
   into	
   english?	
   

       represent	
   documents/passages	
   as	
   
id203	
   distribu.ons	
   over	
   hidden	
   
   topics   	
   

modeling	
   text	
   with	
   topics	
   

latent	
   dirichlet	
   alloca1on	
   (blei,	
   ng,	
   jordan	
   2003)	
   
       let	
   the	
   text	
   talk	
   about	
   t	
   topics	
   
       each	
   topic	
   is	
   a	
   id203	
   dist   n	
   over	
   all	
   

words	
   

       for	
   d	
   documents	
   each	
   with	
   nd	
   words:	
   

prior	
   

  	
   

z	
   

w	
   

  	
   

prior	
   

80%	
   economy	
   
20%	
   pres.	
   elect.	
   

economy	
   

   jobs   	
   

  	
   

n	
   

d	
   

top	
   words	
   by	
   topic	
   

topics	
      	
   

1	
   

2	
   

3	
   

4	
   

5	
   

6	
   

7	
   

8	
   

gri   ths	
   et	
   al.	
   

top	
   words	
   by	
   topic	
   

topics	
      	
   

1	
   

2	
   

3	
   

4	
   

5	
   

6	
   

7	
   

8	
   

gri   ths	
   et	
   al.	
   

modeling	
   text	
   with	
   topics	
   

latent	
   dirichlet	
   alloca1on	
   (blei,	
   ng,	
   jordan	
   2003)	
   

prior	
   

  	
   

z	
   

w	
   

  	
   

prior	
   

  	
   

n	
   

d	
   

mul1ple	
   languages?	
   

lda	
   

       model	
   document	
   as	
   being	
   generated	
   from	
   a	
   

mixture	
   of	
   topics	
   

lda	
   

       gives	
   language	
   model	
   probabili.es	
   

       used	
   to	
   	
   smooth	
   the	
   document	
   representa.on	
   

by	
   mixing	
   them	
   with	
   the	
   query	
   likelihood	
   
id203	
   as	
   follows:	
   

lda	
   

       if	
   the	
   lda	
   probabili.es	
   are	
   used	
   directly	
   as	
   the	
   

document	
   representa.on,	
   the	
   e   ec.veness	
   
will	
   be	
   signi   cantly	
   reduced	
   because	
   the	
   
features	
   are	
   too	
   smoothed	
   
      e.g.,	
   in	
   typical	
   trec	
   experiment,	
   only	
   400	
   topics	
   
used	
   for	
   the	
   en1re	
   collec.on	
   
      genera.ng	
   lda	
   topics	
   is	
   expensive	
   

       when	
   used	
   for	
   smoothing,	
   e   ec.veness	
   is	
   

improved	
   

lda	
   example	
   

      top	
   words	
   from	
   4	
   lda	
   topics	
   from	
   trec	
   news	
   

summary	
   

and	
   data	
   available	
   

       best	
   retrieval	
   model	
   depends	
   on	
   applica.on	
   
       evalua.on	
   corpus	
   (or	
   test	
   collec.on),	
   training	
   
data,	
   and	
   user	
   data	
   are	
   all	
   cri.cal	
   resources	
   
       open	
   source	
   search	
   engines	
   can	
   be	
   used	
   to	
   
   nd	
   e   ec.ve	
   ranking	
   algorithms	
   
      galago	
   query	
   language	
   makes	
   this	
   par.cularly	
   
easy	
   

       language	
   resources	
   (e.g.,	
   thesaurus)	
   can	
   make	
   

a	
   big	
   di   erence	
   

