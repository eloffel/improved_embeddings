experiences with mapreduce,

an abstraction for large-scale computation

jeff dean
google, inc.

problem: lots of data

    example: 20+ billion web pages x 20kb = 400+ terabytes
    one computer can read 30-35 mb/sec from disk

    ~four months to read the web

    ~1,000 hard drives just to store the web
    even more to do something with the data

3

solution: spread the work over many machines

    good news: same problem with 1000 machines, < 3 hours
    bad news: programming work

    communication and coordination
    recovering from machine failure
    status reporting
    debugging
    optimization
    locality

    bad news ii: repeat for every problem you want to solve

4

mapreduce

    a simple programming model that applies to many large-scale 

computing problems

    hide messy details in mapreduce runtime library:

    automatic parallelization
    load balancing
    network and disk transfer optimization
    handling of machine failures
    robustness
    improvements to core library benefit all users of library!

8

typical problem solved by mapreduce

    read a lot of data
    map: extract something you care about from each record
    shuffle and sort
    reduce: aggregate, summarize, filter, or transform
    write the results

outline stays the same,

map and reduce change to fit the problem

9

more specifically   

    programmer specifies two primary methods:

    map(k, v)     <k', v'>*
    reduce(k', <v'>*)     <k', v      >*

    all v' with same k' are reduced together, in order.

    usually also specify:

    partition(k   , total partitions) -> partition for k   

    often a simple hash of the key
    allows reduce operations for different k    to be parallelized

10

mapreduce: scheduling

    one master, many workers 

    input data split into m map tasks (typically 64 mb in size)
    reduce phase partitioned into r reduce tasks
    tasks are assigned to workers dynamically
    often: m=200,000; r=4,000; workers=2,000

    master assigns each map task to a free worker 

    considers locality of data to worker when assigning task
    worker reads task input (often from local disk!)
    worker produces r local files containing intermediate k/v pairs

    master assigns each reduce task to a free worker 
    worker reads intermediate k/v pairs from map workers
    worker sorts & applies user   s reduce op to produce the output 

20

parallel mapreduce

map

map

map

map

shuffle
reduce

shuffle
reduce

shuffle
reduce

21

input
data

master

partitioned 

output

task granularity and pipelining
    fine granularity tasks: many more map tasks than machines

    minimizes time for fault recovery
    can pipeline shuffling with map execution
    better dynamic load balancing

    often use 200,000 map/5000 reduce tasks w/ 2000 machines

22

conclusion

    mapreduce has proven to be a remarkably-useful abstraction
    greatly simplifies large-scale computations at google
   

fun to use: focus on problem, let library deal with messy details
    many thousands of parallel programs written by hundreds of different 

programmers in last few years

    many had no prior parallel or distributed programming experience

further info:

mapreduce: simplified data processing on large clusters, jeffrey dean 
and sanjay ghemawat, osdi   04
http://labs.google.com/papers/mapreduce.html
(or search google for [mapreduce])

43

