introduction to information retrieval
introduction to information retrieval

introduction to information retrieval
introduction to information retrieval

ch. 2

recap of the previous lecture

  the type/token distinction

  terms are normalized types put in the dictionary

  id121 problems:

  hyphens, apostrophes, compounds, cjk

  term equivalence classing:

  numbers, case folding, id30, lemmatization

  skip pointers

  encoding a tree-like structure in a postings list

  biword indexes for phrases

  positional indexes for phrases/proximity queries

2

introduction to information retrieval
introduction to information retrieval

sec. 3.1

dictionary data structures for inverted 
indexes

  the dictionary data structure stores the term 

vocabulary, document frequency, pointers to each 
postings list     in what data structure?

introduction to
information retrieval

cs276: information retrieval and web search

pandu nayak and prabhakar raghavan

lecture 3: dictionaries and tolerant retrieval

introduction to information retrieval
introduction to information retrieval

ch. 3

this lecture

  dictionary data structures

     tolerant    retrieval

  wild-card queries

  id147

  soundex

3

introduction to information retrieval
introduction to information retrieval

sec. 3.1

introduction to information retrieval
introduction to information retrieval

sec. 3.1

a na  ve dictionary

  an array of struct:

dictionary data structures

  two main choices:

  hashtables

  trees

  some ir systems use hashtables, some trees

char[20]   int                   postings *

20 bytes   4/8 bytes        4/8 bytes  

  how do we store a dictionary in memory efficiently?

  how do we quickly look up elements at query time?

5

4

6

1

introduction to information retrieval
introduction to information retrieval

sec. 3.1

introduction to information retrieval
introduction to information retrieval

sec. 3.1

hashtables

  each vocabulary term is hashed to an integer

  (we assume you   ve seen hashtables before)

  pros:

  lookup is faster than for a tree: o(1)

  cons:

  no easy way to find minor variants:

  judgment/judgement

  no prefix search

[tolerant  retrieval]

  if vocabulary keeps growing, need to occasionally do the 

expensive operation of rehashing everything

tree: binary tree

root

a-m

n-z

a-hu

hy-m

n-sh

si-z

7

8

introduction to information retrieval
introduction to information retrieval

sec. 3.1

introduction to information retrieval
introduction to information retrieval

sec. 3.1

tree: b-tree

a-hu

hy-m

n-z

  definition: every internal nodel has a number of children 

in the interval [a,b] where a, b are appropriate natural 
numbers, e.g., [2,4].

introduction to information retrieval
introduction to information retrieval

wild-card queries

trees

  simplest: binary tree

  more usual: b-trees

  trees require a standard ordering of characters and hence 

strings     but we typically have one

  pros:

  solves the prefix problem (terms starting with hyp)

  cons:

  slower: o(log m)  [and this requires balanced tree]

  rebalancing binary trees is expensive

  but b-trees mitigate the rebalancing problem

10

introduction to information retrieval
introduction to information retrieval

sec. 3.2

wild-card queries: *

  mon*: find all docs containing any word beginning 

with    mon   .

  easy with binary tree (or b-tree) lexicon: retrieve all 

words in range: mon     w < moo

  *mon: find words ending in    mon   : harder

  maintain an additional b-tree for terms backwards.

can retrieve all words in range: nom     w < non.

exercise: from this, how can we enumerate all terms
meeting the wild-card query pro*cent ?

12

9

11

2

introduction to information retrieval
introduction to information retrieval

sec. 3.2

query processing

introduction to information retrieval
introduction to information retrieval

b-trees handle *   s at the end of a 
query term

sec. 3.2

  at this point, we have an enumeration of all terms in 

  how can we handle *   s in the middle of query term?

the dictionary that match the wild-card query.

  we still have to look up the postings for each 

enumerated term.

  e.g., consider the query:

se*ate and fil*er

this may result in the execution of many boolean 
and queries.

  co*tion

  we could look up co* and *tion in a b-tree and 

intersect the two term sets
  expensive

  the solution: transform wild-card queries so that the 

*   s occur at the end

  this gives rise to the permuterm index.

13

14

introduction to information retrieval
introduction to information retrieval

sec. 3.2.1

introduction to information retrieval
introduction to information retrieval

sec. 3.2.1

permuterm index

  for term hello, index under:

  hello$, ello$h, llo$he, lo$hel, o$hell, $hello

where $ is a special symbol.

  queries:

  x lookup on x$

x*   lookup on   $x*

  *x   lookup on x$*

*x* lookup on   x*

  x*y lookup on y$x*

x*y*z

??? exercise!

query = hel*o

x=hel, y=o

lookup o$hel*

permuterm query processing

  rotate query wild-card to the right

  now use b-tree lookup as before.

  permuterm problem:     quadruples lexicon size

empirical observation for english.

15

16

introduction to information retrieval
introduction to information retrieval

sec. 3.2.2

introduction to information retrieval
introduction to information retrieval

sec. 3.2.2

bigram (k-gram) indexes

bigram index example

  enumerate all k-grams (sequence of k chars) 

  the k-gram index finds terms based on a query 

occurring in any term

consisting of k-grams (here k=2).

  e.g., from text    april is the cruelest month    we get 

the 2-grams (bigrams)

$a,ap,pr,ri,il,l$,$i,is,s$,$t,th,he,e$,$c,cr,ru,
ue,el,le,es,st,t$, $m,mo,on,nt,h$

  $ is a special word boundary symbol

  maintain a second inverted index from bigrams to

dictionary terms that match each bigram.

$m

mo

on

mace

madden

among

amortize

along

among

17

18

3

introduction to information retrieval
introduction to information retrieval

sec. 3.2.2

introduction to information retrieval
introduction to information retrieval

sec. 3.2.2

processing wild-cards

  query mon* can now be run as

  $m and mo and on

processing wild-card queries

  as before, we must execute a boolean query for each 

enumerated, filtered term.

  gets terms that match and version of our wildcard 

  wild-cards can result in expensive query execution 

query.

  but we   d enumerate moon.

  must post-filter these terms against query.

  surviving enumerated terms are then looked up in 

the term-document inverted index.

  fast, space efficient (compared to permuterm).

(very large disjunctions   )
  pyth* and prog*

  if you encourage    laziness    people will respond!

type your search terms, use    *    if you need to.
e.g., alex* will match alexander.

search

19

  which web search engines allow wildcard queries?

20

introduction to information retrieval
introduction to information retrieval

introduction to information retrieval
introduction to information retrieval

sec. 3.3

id147

spell correction

  two principal uses

  correcting document(s) being indexed

  correcting user queries to retrieve    right    answers

  two main flavors:

  isolated word

  check each word on its own for misspelling

  will not catch typos resulting in correctly spelled words

  e.g., from             form

  context-sensitive

  look at surrounding words, 

  e.g., i flew form heathrow to narita.

21

22

introduction to information retrieval
introduction to information retrieval

sec. 3.3

introduction to information retrieval
introduction to information retrieval

sec. 3.3

document correction

  especially needed for ocr   ed documents

  correction algorithms are tuned for this: rn/m

  can use domain-specific knowledge

  e.g., ocr can confuse o and d more often than it would confuse o 

and i (adjacent on the qwerty keyboard, so more likely 
interchanged in typing).

  but also: web pages and even printed material have 

typos

  goal: the dictionary contains fewer misspellings

  but often we don   t change the documents and 

instead fix the query-document mapping

query mis-spellings

  our principal focus here

  e.g., the query alanis morisett

  we can either

  retrieve documents indexed by the correct spelling, or

  return several suggested alternative queries with the 

correct spelling

  did you mean     ?

23

24

4

introduction to information retrieval
introduction to information retrieval

sec. 3.3.2

introduction to information retrieval
introduction to information retrieval

sec. 3.3.2

isolated word correction

isolated word correction

  fundamental premise     there is a lexicon from which 

  given a lexicon and a character sequence q, return 

the correct spellings come

  two basic choices for this
  a standard lexicon such as
  webster   s english dictionary

  an    industry-specific    lexicon     hand-maintained

  the lexicon of the indexed corpus

  e.g., all words on the web

  all names, acronyms etc.

  (including the mis-spellings)

the words in the lexicon closest to q

  what   s    closest   ?

  we   ll study several alternatives

  id153 (levenshtein distance)

  weighted id153

  id165 overlap

25

26

introduction to information retrieval
introduction to information retrieval

sec. 3.3.3

introduction to information retrieval
introduction to information retrieval

sec. 3.3.3

id153

weighted id153

  given two strings s1 and s2, the minimum number of 

operations to convert one to the other

  operations are typically character-level

  insert, delete, replace, (transposition)

  e.g., the id153 from dof to dog is 1
  from cat to act is 2 (just 1 with transpose.)

  from cat to dog is 3.

  generally found by id145.

  see http://www.merriampark.com/ld.htm for a nice 

example plus an applet.

  as above, but the weight of an operation depends on 

the character(s) involved
  meant to capture ocr or keyboard errors

example: m more likely to be mis-typed as n than as q

  therefore, replacing m by n is a smaller id153 than 

by q

  this may be formulated as a id203 model

  requires weight matrix as input

  modify id145 to handle weights

27

28

introduction to information retrieval
introduction to information retrieval

sec. 3.3.4

introduction to information retrieval
introduction to information retrieval

sec. 3.3.4

using id153s

id153 to all dictionary terms?

  given query, first enumerate all character sequences 

  given a (mis-spelled) query     do we compute its edit 

within a preset (weighted) id153 (e.g., 2)

  intersect this set with list of    correct    words

  show terms you found to user as suggestions

  alternatively, 

  we can look up all possible corrections in our inverted 

index and return all docs     slow

  we can run with a single most likely correction

  the alternatives disempower the user, but save a 

round of interaction with the user

distance to every dictionary term?
  expensive and slow

  alternative?

  how do we cut the set of candidate dictionary 

terms?

  one possibility is to use id165 overlap for this

  this can also be used by itself for id147.

29

30

5

introduction to information retrieval
introduction to information retrieval

sec. 3.3.4

introduction to information retrieval
introduction to information retrieval

sec. 3.3.4

id165 overlap

example with trigrams

  enumerate all the id165s in the query string as well 

  suppose the text is november

as in the lexicon

  use the id165 index (recall wild-card search) to 

retrieve all lexicon terms matching any of the query 
id165s

  threshold by number of matching id165s

  variants     weight by keyboard layout, etc.

  trigrams are nov, ove, vem, emb, mbe, ber.

  the query is december

  trigrams are dec, ece, cem, emb, mbe, ber.

  so 3 trigrams overlap (of 6 in each term)

  how can we turn this into a normalized measure of 

overlap?

31

32

introduction to information retrieval
introduction to information retrieval

sec. 3.3.4

introduction to information retrieval
introduction to information retrieval

sec. 3.3.4

one option     jaccard coefficient

matching trigrams

  a commonly-used measure of overlap

  let x and y be two sets; then the j.c. is

  consider the query lord     we wish to identify words 

matching 2 of its 3 bigrams (lo, or, rd)

yxyx

    /

   

  equals 1 when x and y have the same elements and 

zero when they are disjoint

  x and y don   t have to be of the same size

  always assigns a number between 0 and 1
  now threshold to decide if you have a match

  e.g., if j.c. > 0.8, declare a match 

lo

or

rd

alone

lore

sloth

border

lore

morbid

ardent

border card

standard postings    merge    will enumerate     

33

adapt this to using jaccard (or another) measure.

34

introduction to information retrieval
introduction to information retrieval

sec. 3.3.5

introduction to information retrieval
introduction to information retrieval

sec. 3.3.5

context-sensitive spell correction

context-sensitive correction

  text: i flew from heathrow to narita.

  consider the phrase query    flew form heathrow   

  we   d like to respond

did you mean    flew from heathrow   ?

because no docs matched the query phrase.

  need surrounding context to catch this.
  first idea: retrieve dictionary terms close (in 

weighted id153) to each query term

  now try all possible resulting phrases with one word 

   fixed    at a time
  flew from heathrow 
  fled form heathrow
  flea form heathrow

  hit-based id147: suggest the 

alternative that has lots of hits.

35

36

6

introduction to information retrieval
introduction to information retrieval

sec. 3.3.5

introduction to information retrieval
introduction to information retrieval

sec. 3.3.5

exercise

another approach

  suppose that for    flew form heathrow     we have 7 
alternatives for flew, 19 for form and 3 for heathrow.

  break phrase query into a conjunction of biwords 

(lecture 2).

how many    corrected    phrases will we enumerate in 

  look for biwords that need only one term corrected.

this scheme?

  enumerate only phrases containing    common    

biwords.

37

introduction to information retrieval
introduction to information retrieval

sec. 3.3.5

introduction to information retrieval
introduction to information retrieval

general issues in spell correction

  we enumerate multiple alternatives for    did you 

mean?   

  need to figure out which to present to the user

  the alternative hitting most docs

  query log analysis

  more generally, rank alternatives probabilistically

argmaxcorr p(corr | query)

  from bayes rule, this is equivalent to

argmaxcorr p(query | corr) * p(corr)

noisy channel

language model

39

soundex

38

40

introduction to information retrieval
introduction to information retrieval

sec. 3.4

introduction to information retrieval
introduction to information retrieval

sec. 3.4

soundex

soundex     typical algorithm

  class of heuristics to expand a query into phonetic

  turn every token to be indexed into a 4-character 

equivalents
  language specific     mainly for names

  e.g., chebyshev     tchebycheff

  invented for the u.s. census     in 1918

reduced form

  do the same with query terms

  build and search an index on the reduced forms

  (when the query calls for a soundex match)

 

http://www.creativyst.com/doc/articles/soundex1/soundex1.htm#top

41

42

7

introduction to information retrieval
introduction to information retrieval

sec. 3.4

introduction to information retrieval
introduction to information retrieval

sec. 3.4

soundex     typical algorithm

soundex continued

1. retain the first letter of the word. 
2. change all occurrences of the following letters to '0' 

(zero):
'a', e', 'i', 'o', 'u', 'h', 'w', 'y'. 

3. change letters to digits as follows: 
  b, f, p, v     1
  c, g, j, k, q, s, x, z     2
  d,t     3
 

l     4

  m, n     5
  r     6

4. remove all pairs of consecutive digits.

5. remove all zeros from the resulting string.

6. pad the resulting string with trailing zeros and 

return the first four positions, which will be of the 
form <uppercase letter> <digit> <digit> <digit>. 

e.g., herman becomes h655.

will hermann generate the same code?

43

44

introduction to information retrieval
introduction to information retrieval

sec. 3.4

introduction to information retrieval
introduction to information retrieval

soundex

what queries can we process?

  soundex is the classic algorithm, provided by most 

  we have

databases (oracle, microsoft,    )

  how useful is soundex?

  not very     for information retrieval

  okay for    high recall    tasks (e.g., interpol), though 

biased to names of certain nationalities

  zobel and dart (1996) show that other algorithms for 

phonetic matching perform much better in the 
context of ir

  positional inverted index with skip pointers

  wild-card index

  spell-correction

  soundex

  queries such as

(spell(moriset) /3 toron*to) or soundex(chaikofski)

45

46

introduction to information retrieval
introduction to information retrieval

introduction to information retrieval
introduction to information retrieval

sec. 3.5

exercise

  draw yourself a diagram showing the various indexes 
in a search engine incorporating all the functionality 
we have talked about

  identify some of the key design choices in the index 

pipeline:
  does id30 happen before the soundex index?

  what about id165s?

resources

  iir 3, mg 4.2

  efficient spell retrieval:

  k. kukich. techniques for automatically correcting words in text. acm 

computing surveys 24(4), dec 1992.

  j. zobel and p. dart. finding approximate matches in large 

lexicons. software - practice and experience 25(3), march 1995. 
http://citeseer.ist.psu.edu/zobel95finding.html

  mikael tillenius: efficient generation and ranking of spelling error 

corrections. master   s thesis at sweden   s royal institute of technology. 
http://citeseer.ist.psu.edu/179155.html

  given a query, how would you parse and dispatch 

  nice, easy reading on spell correction:

sub-queries to the various indexes?

  peter norvig: how to write a spelling corrector 

http://norvig.com/spell-correct.html

47

48

8

