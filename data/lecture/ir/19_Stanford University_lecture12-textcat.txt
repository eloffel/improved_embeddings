introduction to
information retrieval
cs276: information retrieval and web 

search

text classification 1

chris manning and pandu nayak

introduction to information retrieval

ch. 13

standing queries
   the path from ir to text classification:

   you have an information need to monitor, say:
   you want to rerun an appropriate query periodically to 
   you will be sent new documents that are found 

   unrest in the niger delta region
find new news items on this topic

   i.e., it  s not ranking but classification (relevant vs. not 

relevant)

   such queries are called standing queries
   long used by    information professionals   
   a modern mass instantiation is google alerts

   standing queries are (hand-written) text classifiers

introduction to information retrieval

3

introduction to information retrieval

spam filtering
another text classification task

ch. 13

from: "" <takworlld@hotmail.com>
subject: real estate is the only way... gem  oalvgkay
anyone can buy real estate with no money down
stop paying rent today !
there is no need to spend hundreds or even thousands for similar courses
i am 22 years old and i have already purchased 6 properties using the
methods outlined in this truly incredible ebook.
change your life now !
=================================================
click below to order:
http://www.wholesaledaily.com/sales/nmd.htm
=================================================

introduction to information retrieval

sec. 13.1

categorization/classification
   given:

   a representation of a document d

   issue: how to represent text documents. 
   usually some type of high-dimensional space     bag of 
words

   a fixed set of classes:
c = {c1, c2,   , cj}

   determine:

   the category of d:   (d)    c, where   (d)is a 
classification function
   we want to build classification functions 
(   classifiers   ).

introduction to information retrieval

sec. 13.1

document classification

test
data:

classes:

training
data:

   planning
language
proof
intelligence   

(ai)

(programming)

(hci)

ml
learning
intelligence
algorithm
reinforcement
network...

planning
planning
temporal
reasoning
plan
language...

multimedia gui
...

...

semantics
programming
semantics
language
proof...

garb.coll.
garbage
collection
memory
optimization
region...

introduction to information retrieval

ch. 13

classification methods (1)
   manual classification

   used by the original yahoo! directory
   looksmart, about.com, odp, pubmed
   accurate when job is done by experts
   consistent when the problem size and team is 
small
   difficult and expensive to scale

   means we need automatic classification methods for 
big problems

introduction to information retrieval

ch. 13

classification methods (2)
   hand-coded rule-based classifiers
   one technique used by news agencies, 
intelligence agencies, etc.
   widely deployed in government and 
enterprise
   vendors provide    ide    for writing such 
rules

introduction to information retrieval

ch. 13

classification methods (2)
   hand-coded rule-based classifiers

   commercial systems have complex query 
languages
   accuracy can be high if a rule has been 
carefully refined over time by a subject 
expert
   building and maintaining these rules is 
expensive

introduction to information retrieval

a verity topic 
a complex classification rule: art

ch. 13

   note:

   maintenance issues 
   hand-weighting of 

(author, etc.)
terms

[verity was bought by 
autonomy in 2005, 
which was bought 
by hp in 2011     a 
mess; i think it no 
longer exists ...]

11

introduction to information retrieval

classification methods (3):
supervised learning
   given:

sec. 13.1

   a document d
   a fixed set of classes:
c = {c1, c2,   , cj}
   a training set dof documents each with a label 
in c

   determine:

   a learning method or algorithm which will 
enable us to learn a classifier   
   for a test document d, we assign it the class
  (d)    c

introduction to information retrieval

ch. 13

classification methods (3)
   supervised learning

   naive bayes (simple, common)     see video, cs229
   k-nearest neighbors (simple, powerful)
   support-vector machines (newer, generally more 
powerful)
   id90    id79s   
gradient-boosted id90 (e.g., xgboost)
       plus many other methods
   no free lunch: need hand-classified training data
   but data can be built up by amateurs

   many commercial systems use a mix of methods

introduction to information retrieval

features
   supervised learning classifiers can use any 
sort of feature
   url, email address, punctuation, capitalization, 
dictionaries, network features

   in the simplest bag of words view of 

documents
   we use only word features 
   we use all of the words in the text (not a subset)

introduction to information retrieval

the bag of words representation

  (

i love this movie! it's sweet, 
but with satirical humor. the 
dialogue is great and the 
adventure scenes are fun     it 
manages to be whimsical and 
romantic while laughing at the 
conventions of the fairy tale 
genre. i would recommend it to 
just about anyone. i've seen it 
several times, and i'm always 
happy to see it again whenever 
i have a friend who hasn't seen 
it yet.

)=c

introduction to information retrieval

the bag of words representation

  (

2
great
2
love
recommend 1
1
laugh
1
happy
...

...

)=c

introduction to information retrieval

sec.13.5

   10,000     1,000,000 unique words     and more

feature selection: why?
   text collections have a large number of features
   selection may make a particular classifier feasible
   reduces training time
the number of features

   training time for some methods is quadratic or worse in 

   some classifiers can  t deal with 1,000,000 features

   makes runtime models smaller and faster
   can improve generalization (performance)

   eliminates noise features
   avoids overfitting

introduction to information retrieval

feature selection: frequency
   the simplest feature selection 
method:
   just use the commonest terms
   no particular foundation
   but it make sense why this works

   they  re the words that can be well-estimated 
and are most often available as evidence
   in practice, this is often 90% as good as 
better methods

   smarter feature selection: 

   chi-squared, etc.

introduction	to	information	retrieval

na  ve	bayes:	see	iir	13	or	cs124	
lecture	on	coursera	or	cs229
   classify	based	on	prior	weight	of	class	and	

conditional	parameter	for	what	each	word	says:
$ 
cnb = argmax
logp(c j) +
& 
cj    c
& 
% 

' 
logp(xi |c j)
) 
) 
( 

i   positions

   

   training	is	done	by	counting	and	dividing:

p(c j)    

nc j
n

p(xk |c j)    

tc jxk +  

xi    v   

[tc jxi +  ]

   don   t	forget	to	smooth

    

19

introduction to information retrieval

spamassassin
   na  ve bayes has found a home in 
spam filtering
   paul graham  s a plan for spam
   widely used in spam filters 
   but many features beyond words:
   black hole lists, etc.
   particular hand-crafted text patterns

introduction to information retrieval

spamassassin features:

   basic (na  ve) bayes spam id203
   mentions: generic viagra
   regex: millions of (dollar) ((dollar) nn,nnn,nnn.nn)
   phrase: impress ... girl
   phrase:    prestigious non-accredited universities  
   from: starts with many numbers
   subject is all capitals
   html has a low ratio of text to image area
   relay in rbl, http://www.mail-
abuse.com/enduserinfo_rbl.html
   rcvd line looks faked
   http://spamassassin.apache.org/tests_3_3_x.html

introduction to information retrieval

naive bayes is not so naive
  very fast learning and testing (basically 
just count words)
  low storage requirements
  very good in domains with many 
equally important features
  more robust to irrelevant features than 
many learning methods
irrelevant features cancel out without 
affecting results

introduction to information retrieval

naive bayes is not so naive
  more robust to concept drift (changing class 
definition over time)
  naive bayes won 1st and 2nd place in kdd-
cup 97 competition out of 16 systems
goal: financial services industry direct mail 
response prediction: predict if the recipient of 
mail will actually respond to the advertisement    
750,000 records.

  a good dependable baseline for text 
classification (but not the best)!

introduction to information retrieval

sec.13.6

evaluating categorization
   evaluation must be done on test data that 
are independent of the training data
   sometimes use cross-validation (averaging 
results over multiple training and test splits of 
the overall data)
   easy to get good performance on a test set 

that was available to the learner during 
training (e.g., just memorize the test set)

introduction to information retrieval

sec.13.6

evaluating categorization
   measures: precision, recall, f1, 
classification accuracy
   classification accuracy: r/n where n is 
the total number of test docs and ris 
the number of test docs correctly 
classified

introduction to information retrieval

sec.13.6

webkb experiment (1998)
   classify webpages from cs departments 
into:
   student, faculty, course, project 

   train on ~5,000 hand-labeled web pages
   crawl and classify a new site (cmu) using 

   cornell, washington, u.texas, wisconsin
na  ve bayes

   results

introduction to information retrieval

introduction	to	information	retrieval

sec.14.1

remember:	vector	space	representation
   each	document	is	a	vector,	one	component	for	each	

term	(=	word).

   normally	normalize	vectors	to	unit	length.
   high-dimensional	vector	space:

   terms	are	axes
   10,000+	dimensions,	or	even	100,000+
   docs	are	vectors	in	this	space

   how	can	we	do	classification	in	this	space?

28

introduction to information retrieval

classification	using	vector	spaces
   in vector space classification, training set 

corresponds to a labeled set of points 
(equivalently, vectors)
form a contiguous region of space
classes don   t overlap (much)
delineate classes in the space

   premise 1: documents in the same class 
   premise 2: documents from different 
   learning a classifier: build surfaces to 

documents	in	a	vector	space

sec.14.1

government

science

arts

30

test	document	of	what	class?

sec.14.1

government

science

arts

31

test	document	=	government

sec.14.1

government

science

arts

our focus: how to find good separators

32

sec.14.2

definition	of	centroid

    
   (c) =
  

1
| dc |

    v (d)

   
d    dc

   where	dc is	the	set	of	all	documents	that	belong	to	
class	c	and	v(d)	is	the	vector	space	representation	of	
d.
    

   note	that	centroid	will	in	general	not	be	a	unit	vector	

even	when	the	inputs	are	unit	vectors.

33

sec.14.2

rocchio	classification
   rocchio	forms	a	simple	representative	for	

each	class:	the	centroid/prototype	

   classification:	nearest	prototype/centroid
   it	does	not	guarantee	that	classifications	are	

consistent	with	the	given	training	data

why not?

34

introduction	to	information	retrieval

sec.14.2

two-class	rocchio	as	a	linear	classifier
   line	or	hyperplane	defined	by:

m
   
i=1

widi =  

   for	rocchio,	set:
! 
! 
! w =
   (c1)    
   (c2)
   (c1) |2     | ! 
   = 0.5   (| ! 
  

    

   (c2) |2)

    

35

introduction	to	information	retrieval

sec.14.4

linear	classifier:	example

   class:	   interest   	(as	in	interest	rate)
   example	features	of	a	linear	classifier

wi

ti

    0.70 prime
    0.67 rate
    0.63 interest
    0.60 rates
    0.46 discount
    0.43 bundesbank

wi

ti

       0.71 dlrs
       0.35 world
       0.33 sees
       0.25 year
       0.24 group
       0.24 dlr

   to	classify,	find	dot	product	of	feature	vector	and	

weights

36

sec.14.2

rocchio	classification
   a	simple	form	of	fisher   s	linear	discriminant
   little	used	outside	text	classification

   it	has	been	used	quite	effectively	for	text	

classification

   but	in	general	worse	than	na  ve	bayes

   again,	cheap	to	train	and	test	documents

37

introduction	to	information	retrieval

sec.14.3

k nearest	neighbor	classification
   knn	=	k nearest	neighbor

   to	classify	a	document	d:
   define	k-neighborhood	as	the	k nearest	

neighbors	of	d

   pick	the	majority	class	label	in	the	k-

neighborhood

   for	larger	k can	roughly	estimate	p(c|d)	as	

#(c)/k

38

test	document	=	science

voronoi diagram

sec.14.1

government

science

arts

39

sec.14.3

nearest-neighbor	learning
   learning:	just	store	the	labeled	training	examples	d
   testing	instance	x	(under	1nn):

   compute	similarity	between	x and	all	examples	in	d.
   assign	x the	category	of	the	most	similar	example	in	d.
   does	not	compute	anything	beyond	storing	the	examples
   also	called:

   case-based	learning
   memory-based	learning
   lazy	learning

   rationale	of	knn:	contiguity	hypothesis

40

sec.14.3

k	nearest	neighbor
   using	only	the	closest	example	(1nn)	is	

subject	to	errors	due	to:
   a	single	atypical	example.	
   noise	(i.e.,	an	error)	in	the	category	label	of	

a	single	training	example.

   more	robust:	find	the	k examples	and	
return	the	majority	category	of	these	k
   k is	typically	odd	to	avoid	ties;	3	and	5	

are	most	common

41

introduction	to	information	retrieval

sec.14.3

nearest	neighbor	with	inverted	index
   naively	finding	nearest	neighbors	requires	a	linear	

search	through	|d|	documents	in	collection

   but	determining	k nearest	neighbors	is	the	same	as	

determining	the	k	best	retrievals	using	the	test	
document	as	a	query	to	a	database	of	training	
documents.

   use	standard	vector	space	inverted	index	methods	to	

find	the	k nearest	neighbors.

   testing	time:	o(b|vt|)									where	b is	the	average	

number	of	training	documents	in	which	a	test-document	word	
appears.
   typically	b	<<	|d|

42

introduction	to	information	retrieval

sec.14.3

knn:	discussion
   no	feature	selection	necessary
   no	training	necessary
   scales	well	with	large	number	of	classes
   don   t	need	to	train	n classifiers	for	n classes

   classes	can	influence	each	other

   small	changes	to	one	class	can	have	ripple	effect

   done	naively,	very	expensive	at	test	time
   in	most	cases	it   s	more	accurate	than	nb	or	rocchio
   as	the	amount	of	data	goes	to	infinity,	it	has	to	be	a	great	

classifier!	    it   s	   bayes	optimal   

43

let   s test our intuition
   can a bag of words always be viewed as a 
vector space?
   what about a bag of features?
   can we always view a standing query as a 
contiguous region in a vector space?
   do far away points influence classification 
in a knn classifier?  in a rocchio classifier?
   can a rocchio classifier handle disjunctive 
classes?
   why do linear classifiers actually work well 
for text?

44

introduction	to	information	retrieval

sec.14.2

rocchio	anomaly			
   prototype	models	have	problems	with	polymorphic	

(disjunctive)	categories.

45

introduction	to	information	retrieval

3	nearest	neighbor	vs.	rocchio
   nearest	neighbor	tends	to	handle	polymorphic	

categories	better	than	rocchio/nb.	

46

introduction	to	information	retrieval

bias	vs.	capacity	    notions	and	
terminology
   consider	asking	a	botanist:	is	an	object	a	tree?

   too	much	capacity,	low	bias

sec.14.6

   botanist	who	memorizes
   will	always	say	   no   	to	new	object	(e.g.,	different	#	of	

leaves)

   not	enough	capacity,	high	bias

   lazy	botanist
   says	   yes   	if	the	object	is	green
   you	want	the	middle	ground

(example due to c. burges)

47

introduction	to	information	retrieval

sec.14.6

knn	vs.	naive	bayes
   bias/variance	tradeoff
   variance	   	capacity

   knn has	high	variance	and	low	bias.

   infinite	memory

   rocchio/nb	has	low	variance	and	high	bias.
   linear	decision	surface	between	classes

48

bias	vs.	variance:	
choosing	the	correct	model	capacity

sec.14.6

49

introduction	to	information	retrieval

summary:	representation	of
text	categorization	attributes
   representations	of	text	are	usually	very	

high	dimensional
      the	curse	of	dimensionality   

   high-bias	algorithms	should	generally	

work	best	in	high-dimensional	space
   they	prevent	overfitting
   they	generalize	more

   for	most	text	categorization	tasks,	there	are	

many	relevant	features	&	many	irrelevant	ones

50

introduction	to	information	retrieval

which	classifier	do	i	use	for	a	given	
text	classification	problem?
   is	there	a	learning	method	that	is	optimal	for	all	text	

classification	problems?

   no,	because	there	is	a	tradeoff	between	bias	and	

variance.

   factors	to	take	into	account:

   how	much	training	data	is	available?
   how	simple/complex	is	the	problem?	(linear	vs.	nonlinear	

decision	boundary)

   how	noisy	is	the	data?
   how	stable	is	the	problem	over	time?

   for	an	unstable	problem,	it   s	better	to	use	a	simple	and	robust	

classifier.

51

