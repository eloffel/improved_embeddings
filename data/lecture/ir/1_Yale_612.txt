nlp
introduction to nlp
search engine architecture
search engine architecture
decide what to index
collect it
index it (efficiently)
keep the index up to date
provide user-friendly query facilities
search engine architecture
id194s
term-document matrix (m x n)
document-document matrix (n x n)
typical example in a medium-sized collection
n=3,000,000 documents
m=50,000 terms
typical example on the web
n=30,000,000,000
m=1,000,000
boolean vs. integer-valued matrices
storage issues
example
imagine a medium-sized collection with n=3,000,000 and m=50,000
how large a term-document matrix will be needed?
is there any way to do better? 
any heuristic?

inverted index
instead of an incidence vector, use a posting table
vermont:  d1, d2, d6
massachusetts: d1, d5, d6, d7
use linked lists to be able to insert new document postings in order and to remove existing postings.
can be used to compute document frequency
keep everything sorted! this gives you a logarithmic improvement in access.
basic operations on inverted indexes
conjunction (and)
iterative merge of the two postings: o(x+y)
disjunction (or)
very similar
negation (not)
can we still do it in o(x+y)? 
example: vermont and not massachusetts
example: massachusetts or not vermont
recursive operations
optimization
start with the smallest sets
the vector model
queries as documents
advantages:
mathematically easier to manage
problems:
different lengths
syntactic differences
repetitions of words (or lack thereof)

vector queries
each document is represented as a vector
non-efficient representation
dimensional compatibility
the matching process
document space
matching is done between a document and a query (or between two documents)
distance vs. similarity measures.
euclidean distance (define)
manhattan distance (define)
word overlap
jaccard coefficient
similarity measures
the cosine measure (normalized dot product)

the jaccard coefficient
exercise
compute the cosine scores 
    (d1,d2)
    (d1,d3) 
for the documents
d1 = <1,3>
d2 = <100,300>
d3 = <3,1>
compute the corresponding euclidean distances, manhattan distances, and jaccard coefficients.

phrase-based queries
examples
   new york city   
   ann arbor   
   barack obama   
we don   t want to match
york is a city in new hampshire
positional indexing
keep track of all words and their positions in the documents
to find a multi-word phrase, look for the matching words appearing next to each other
document ranking
compute the similarity between the query and each of the documents
use cosine similarity
use tf*idf weighting
return the top k matches to the user
idf: inverse document frequency
motivation
example
n: number of documents
dk: number of documents containing term k
fik: absolute frequency of term k in document i
wik: weight of term k in document i
 idfk =  log2(n/dk) + 1 = log2n - log2dk + 1
nlp
