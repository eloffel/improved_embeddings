introduc)on	
   to	
   informa)on	
   retrieval	
   

introduc)on	
   to	
   
informa(on	
   retrieval	
   

cs276	
   

informa)on	
   retrieval	
   and	
   web	
   search	
   
pandu	
   nayak	
   and	
   prabhakar	
   raghavan	
   

lecture	
   8:	
   evalua)on	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 6.2 

this	
   lecture	
   
       how	
   do	
   we	
   know	
   if	
   our	
   results	
   are	
   any	
   good?	
   	
   

       evalua)ng	
   a	
   search	
   engine	
   

       benchmarks	
   
       precision	
   and	
   recall	
   
       results	
   summaries:	
   

       making	
   our	
   good	
   results	
   usable	
   to	
   a	
   user	
   

2	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

evaluating	
   search	
   engines	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6 

measures	
   for	
   a	
   search	
   engine	
   
       how	
   fast	
   does	
   it	
   index	
   

       number	
   of	
   documents/hour	
   
       (average	
   document	
   size)	
   
       how	
   fast	
   does	
   it	
   search	
   

       latency	
   as	
   a	
   func)on	
   of	
   index	
   size	
   
       expressiveness	
   of	
   query	
   language	
   

       ability	
   to	
   express	
   complex	
   informa)on	
   needs	
   
       speed	
   on	
   complex	
   queries	
   

       uncluuered	
   ui	
   
       is	
   it	
   free?	
   

4	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6 

measures	
   for	
   a	
   search	
   engine	
   
       all	
   of	
   the	
   preceding	
   criteria	
   are	
   measurable:	
   we	
   can	
   

quan)fy	
   speed/size	
   
       we	
   can	
   make	
   expressiveness	
   precise	
   
       the	
   key	
   measure:	
   user	
   happiness	
   

       what	
   is	
   this?	
   
       speed	
   of	
   response/size	
   of	
   index	
   are	
   factors	
   
       but	
   blindingly	
   fast,	
   useless	
   answers	
   won   t	
   make	
   a	
   user	
   

happy	
   

       need	
   a	
   way	
   of	
   quan)fying	
   user	
   happiness	
   

5	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6.2 

measuring	
   user	
   happiness	
   
       issue:	
   who	
   is	
   the	
   user	
   we	
   are	
   trying	
   to	
   make	
   happy?	
   
       web	
   engine:	
   

       depends	
   on	
   the	
   sezng	
   

       can	
   measure	
   rate	
   of	
   return	
   users	
   

       user	
      nds	
   what	
   s/he	
   wants	
   and	
   returns	
   to	
   the	
   engine	
   
       user	
   completes	
   task	
      	
   search	
   as	
   a	
   means,	
   not	
   end	
   
       see	
   russell	
   hup://dmrussell.googlepages.com/jcdl-     talk-     

june-     2007-     short.pdf	
   

       ecommerce	
   site:	
   user	
      nds	
   what	
   s/he	
   wants	
   and	
   buys	
   
       is	
   it	
   the	
   end-     user,	
   or	
   the	
   ecommerce	
   site,	
   whose	
   happiness	
   
       measure	
   )me	
   to	
   purchase,	
   or	
   frac)on	
   of	
   searchers	
   who	
   

we	
   measure?	
   

become	
   buyers?	
   

6	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6.2 

measuring	
   user	
   happiness	
   
       enterprise	
   (company/govt/academic):	
   care	
   about	
   

   user	
   produc)vity   	
   
       how	
   much	
   )me	
   do	
   my	
   users	
   save	
   when	
   looking	
   for	
   

informa)on?	
   

       many	
   other	
   criteria	
   having	
   to	
   do	
   with	
   breadth	
   of	
   access,	
   

secure	
   access,	
   etc.	
   

7	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.1 

happiness:	
   elusive	
   to	
   measure	
   
       most	
   common	
   proxy:	
   relevance	
   of	
   search	
   results	
   
       but	
   how	
   do	
   you	
   measure	
   relevance?	
   
       we	
   will	
   detail	
   a	
   methodology	
   here,	
   then	
   examine	
   

its	
   issues	
   

       relevance	
   measurement	
   requires	
   3	
   elements:	
   

1.    a	
   benchmark	
   document	
   collec)on	
   
2.    a	
   benchmark	
   suite	
   of	
   queries	
   
3.    a	
   usually	
   binary	
   assessment	
   of	
   either	
   relevant	
   or	
   
nonrelevant	
   for	
   each	
   query	
   and	
   each	
   document	
   
       some	
   work	
   on	
   more-     than-     binary,	
   but	
   not	
   the	
   standard	
   

8	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.1 

evalua)ng	
   an	
   ir	
   system	
   
       note:	
   the	
   informa(on	
   need	
   is	
   translated	
   into	
   a	
   query	
   
       relevance	
   is	
   assessed	
   rela)ve	
   to	
   the	
   informa(on	
   

need	
   not	
   the	
   query	
   

       e.g.,	
   informa)on	
   need:	
   i'm	
   looking	
   for	
   informa)on	
   on	
   

whether	
   drinking	
   red	
   wine	
   is	
   more	
   e   ec)ve	
   at	
   
reducing	
   your	
   risk	
   of	
   heart	
   a<acks	
   than	
   white	
   wine.	
   

       query:	
   wine	
   red	
   white	
   heart	
   a+ack	
   e   ec/ve	
   
       evaluate	
   whether	
   the	
   doc	
   addresses	
   the	
   informa)on	
   

need,	
   not	
   whether	
   it	
   has	
   these	
   words	
   

9	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.2 

standard	
   relevance	
   benchmarks	
   
       trec	
   -     	
   na)onal	
   ins)tute	
   of	
   standards	
   and	
   

technology	
   (nist)	
   has	
   run	
   a	
   large	
   ir	
   test	
   bed	
   for	
   
many	
   years	
   

       reuters	
   and	
   other	
   benchmark	
   doc	
   collec)ons	
   used	
   
          retrieval	
   tasks   	
   speci   ed	
   

       some)mes	
   as	
   queries	
   

       human	
   experts	
   mark,	
   for	
   each	
   query	
   and	
   for	
   each	
   

doc,	
   relevant	
   or	
   nonrelevant	
   
       or	
   at	
   least	
   for	
   subset	
   of	
   docs	
   that	
   some	
   system	
   returned	
   

for	
   that	
   query	
   

10	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

unranked	
   retrieval	
   evalua)on:	
   
precision	
   and	
   recall	
   
       precision:	
   frac)on	
   of	
   retrieved	
   docs	
   that	
   are	
   relevant	
   

=	
   p(relevant|retrieved)	
   

       recall:	
   frac)on	
   of	
   relevant	
   docs	
   that	
   are	
   retrieved	
   
	
   =	
   p(retrieved|relevant)	
   

retrieved 
not retrieved 

relevant 
tp 
fn 

nonrelevant 
fp 
tn 

       precision	
   p	
   =	
   tp/(tp	
   +	
   fp)	
   
       recall	
   	
   	
   	
   	
   	
   r	
   =	
   tp/(tp	
   +	
   fn)	
   

11	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

should	
   we	
   instead	
   use	
   the	
   accuracy	
   
measure	
   for	
   evalua)on?	
   
       given	
   a	
   query,	
   an	
   engine	
   classi   es	
   each	
   doc	
   as	
   

   relevant   	
   or	
      nonrelevant   	
   

       the	
   accuracy	
   of	
   an	
   engine:	
   the	
   frac)on	
   of	
   these	
   

classi   ca)ons	
   that	
   are	
   correct	
   
       (tp	
   +	
   tn)	
   /	
   (	
   tp	
   +	
   fp	
   +	
   fn	
   +	
   tn)	
   

       accuracy	
   is	
   a	
   commonly	
   used	
   evalua)on	
   measure	
   in	
   

machine	
   learning	
   classi   ca)on	
   work	
   

       why	
   is	
   this	
   not	
   a	
   very	
   useful	
   evalua)on	
   measure	
   in	
   

ir?	
   

12	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

why	
   not	
   just	
   use	
   accuracy?	
   
       how	
   to	
   build	
   a	
   99.9999%	
   accurate	
   search	
   engine	
   on	
   a	
   

low	
   budget   .	
   

search for:  
0 matching results found. 

       people	
   doing	
   informa)on	
   retrieval	
   want	
   to	
      nd	
   
something	
   and	
   have	
   a	
   certain	
   tolerance	
   for	
   junk.	
   

13	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

precision/recall	
   
       you	
   can	
   get	
   high	
   recall	
   (but	
   low	
   precision)	
   by	
   

retrieving	
   all	
   docs	
   for	
   all	
   queries!	
   

       recall	
   is	
   a	
   non-     decreasing	
   func)on	
   of	
   the	
   number	
   

of	
   docs	
   retrieved	
   

       in	
   a	
   good	
   system,	
   precision	
   decreases	
   as	
   either	
   the	
   

number	
   of	
   docs	
   retrieved	
   or	
   recall	
   increases	
   
       this	
   is	
   not	
   a	
   theorem,	
   but	
   a	
   result	
   with	
   strong	
   empirical	
   

con   rma)on	
   

14	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

di   cul)es	
   in	
   using	
   precision/recall	
   
       should	
   average	
   over	
   large	
   document	
   collec)on/

query	
   ensembles	
   

       need	
   human	
   relevance	
   assessments	
   

       people	
   aren   t	
   reliable	
   assessors	
   
       assessments	
   have	
   to	
   be	
   binary	
   

       nuanced	
   assessments?	
   

       heavily	
   skewed	
   by	
   collec)on/authorship	
   

       results	
   may	
   not	
   translate	
   from	
   one	
   domain	
   to	
   another	
   

15	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

a	
   combined	
   measure:	
   f	
   
       combined	
   measure	
   that	
   assesses	
   precision/recall	
   
tradeo   	
   is	
   f	
   measure	
   (weighted	
   harmonic	
   mean):	
   

       	
   	
   i.e.,	
   with	
     	
   =	
   1	
   or	
     	
   =	
     	
   

       people	
   usually	
   use	
   balanced	
   f1	
   measure	
   
       harmonic	
   mean	
   is	
   a	
   conserva)ve	
   average	
   
       see	
   cj	
   van	
   rijsbergen,	
   informa)on	
   retrieval	
   

16	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.3 

f1	
   and	
   other	
   averages	
   

17	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

evalua)ng	
   ranked	
   results	
   
       evalua)on	
   of	
   ranked	
   results:	
   

       the	
   system	
   can	
   return	
   any	
   number	
   of	
   results	
   
       by	
   taking	
   various	
   numbers	
   of	
   the	
   top	
   returned	
   documents	
   

(levels	
   of	
   recall),	
   the	
   evaluator	
   can	
   produce	
   a	
   precision-     
recall	
   curve	
   

18	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

a	
   precision-     recall	
   curve	
   

19	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

averaging	
   over	
   queries	
   
       a	
   precision-     recall	
   graph	
   for	
   one	
   query	
   isn   t	
   a	
   very	
   

sensible	
   thing	
   to	
   look	
   at	
   

       you	
   need	
   to	
   average	
   performance	
   over	
   a	
   whole	
   

bunch	
   of	
   queries.	
   

       but	
   there   s	
   a	
   technical	
   issue:	
   	
   

       precision-     recall	
   calcula)ons	
   place	
   some	
   points	
   on	
   the	
   

graph	
   

       how	
   do	
   you	
   determine	
   a	
   value	
   (interpolate)	
   between	
   the	
   

points?	
   

20	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

interpolated	
   precision	
   
       idea:	
   if	
   locally	
   precision	
   increases	
   with	
   increasing	
   

recall,	
   then	
   you	
   should	
   get	
   to	
   count	
   that   	
   

       so	
   you	
   take	
   the	
   max	
   of	
   precisions	
   to	
   right	
   of	
   value	
   

21	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

evalua)on	
   
       graphs	
   are	
   good,	
   but	
   people	
   want	
   summary	
   measures!	
   

       precision	
   at	
      xed	
   retrieval	
   level	
   

       precision-     at-     k:	
   precision	
   of	
   top	
   k	
   results	
   
       perhaps	
   appropriate	
   for	
   most	
   of	
   web	
   search:	
   all	
   people	
   want	
   are	
   

good	
   matches	
   on	
   the	
      rst	
   one	
   or	
   two	
   results	
   pages	
   

       but:	
   averages	
   badly	
   and	
   has	
   an	
   arbitrary	
   parameter	
   of	
   k	
   

       11-     point	
   interpolated	
   average	
   precision	
   

       the	
   standard	
   measure	
   in	
   the	
   early	
   trec	
   compe))ons:	
   you	
   take	
   

the	
   precision	
   at	
   11	
   levels	
   of	
   recall	
   varying	
   from	
   0	
   to	
   1	
   by	
   tenths	
   of	
   
the	
   documents,	
   using	
   interpola)on	
   (the	
   value	
   for	
   0	
   is	
   always	
   
interpolated!),	
   and	
   average	
   them	
   

       evaluates	
   performance	
   at	
   all	
   recall	
   levels	
   

22	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

typical	
   (good)	
   11	
   point	
   precisions	
   
       sabir/cornell	
   8a1	
   11pt	
   precision	
   from	
   trec	
   8	
   (1999)	
   	
   

23	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

yet	
   more	
   evalua)on	
   measures   	
   
       mean	
   average	
   precision	
   (map)	
   

       average	
   of	
   the	
   precision	
   value	
   obtained	
   for	
   the	
   top	
   k	
   

documents,	
   each	
   )me	
   a	
   relevant	
   doc	
   is	
   retrieved	
   

       avoids	
   interpola)on,	
   use	
   of	
      xed	
   recall	
   levels	
   
       map	
   for	
   query	
   collec)on	
   is	
   arithme)c	
   ave.	
   

       macro-     averaging:	
   each	
   query	
   counts	
   equally	
   

       r-     precision	
   

       if	
   we	
   have	
   a	
   known	
   (though	
   perhaps	
   incomplete)	
   set	
   of	
   

relevant	
   documents	
   of	
   size	
   rel,	
   then	
   calculate	
   precision	
   of	
   
the	
   top	
   rel	
   docs	
   returned	
   

       perfect	
   system	
   could	
   score	
   1.0.	
   

24	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.4 

variance	
   
       for	
   a	
   test	
   collec)on,	
   it	
   is	
   usual	
   that	
   a	
   system	
   does	
   
crummily	
   on	
   some	
   informa)on	
   needs	
   (e.g.,	
   map	
   =	
   
0.1)	
   and	
   excellently	
   on	
   others	
   (e.g.,	
   map	
   =	
   0.7)	
   
       indeed,	
   it	
   is	
   usually	
   the	
   case	
   that	
   the	
   variance	
   in	
   
performance	
   of	
   the	
   same	
   system	
   across	
   queries	
   is	
   
much	
   greater	
   than	
   the	
   variance	
   of	
   di   erent	
   systems	
   
on	
   the	
   same	
   query.	
   

       that	
   is,	
   there	
   are	
   easy	
   informa)on	
   needs	
   and	
   hard	
   

ones!	
   

25	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

creating	
   test	
   collections	
   
for	
   ir	
   evaluation	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.5 

test	
   collec)ons	
   

27	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

from	
   document	
   collec)ons	
   	
   
to	
   test	
   collec)ons	
   
       s)ll	
   need	
   

       test	
   queries	
   
       relevance	
   assessments	
   

       test	
   queries	
   

       must	
   be	
   germane	
   to	
   docs	
   available	
   
       best	
   designed	
   by	
   domain	
   experts	
   
       random	
   query	
   terms	
   generally	
   not	
   a	
   good	
   idea	
   

       relevance	
   assessments	
   

       human	
   judges,	
   )me-     consuming	
   
       are	
   human	
   panels	
   perfect?	
   

sec. 8.5 

28	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.5 

kappa	
   measure	
   for	
   inter-     judge	
   (dis)
agreement	
   

       kappa	
   measure	
   

       agreement	
   measure	
   among	
   judges	
   
       designed	
   for	
   categorical	
   judgments	
   
       corrects	
   for	
   chance	
   agreement	
   

       kappa	
   =	
   [	
   p(a)	
      	
   p(e)	
   ]	
   /	
   [	
   1	
      	
   p(e)	
   ]	
   
       p(a)	
      	
   propor)on	
   of	
   )me	
   judges	
   agree	
   
       p(e)	
      	
   what	
   agreement	
   would	
   be	
   by	
   chance	
   
       kappa	
   =	
   0	
   for	
   chance	
   agreement,	
   1	
   for	
   total	
   agreement.	
   

29	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.5 
p(a)? p(e)? 

kappa	
   measure:	
   example	
   

number of docs 

judge 1 

judge 2 

300 

70 

20 

10 

relevant 

relevant 

nonrelevant 

nonrelevant 

relevant 

nonrelevant 

nonrelevant 

relevant 

30	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.5 

kappa	
   example	
   

       p(a)	
   =	
   370/400	
   =	
   0.925	
   
       p(nonrelevant)	
   =	
   (10+20+70+70)/800	
   =	
   0.2125	
   
       p(relevant)	
   =	
   (10+20+300+300)/800	
   =	
   0.7878	
   
       p(e)	
   =	
   0.2125^2	
   +	
   0.7878^2	
   =	
   0.665	
   
       kappa	
   =	
   (0.925	
      	
   0.665)/(1-     0.665)	
   =	
   0.776	
   

       kappa	
   >	
   0.8	
   =	
   good	
   agreement	
   
       0.67	
   <	
   kappa	
   <	
   0.8	
   -     >	
      tenta)ve	
   conclusions   	
   (carleua	
   	
   	
      96)	
   
       depends	
   on	
   purpose	
   of	
   study	
   	
   
       for	
   >2	
   judges:	
   average	
   pairwise	
   kappas	
   	
   

31	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.2 

trec	
   
       trec	
   ad	
   hoc	
   task	
   from	
      rst	
   8	
   trecs	
   is	
   standard	
   ir	
   task	
   

       50	
   detailed	
   informa)on	
   needs	
   a	
   year	
   
       human	
   evalua)on	
   of	
   pooled	
   results	
   returned	
   
       more	
   recently	
   other	
   related	
   things:	
   web	
   track,	
   hard	
   

       a	
   trec	
   query	
   (trec	
   5)	
   

<top>	
   
<num>	
   number:	
   	
   225	
   
<desc>	
   descrip)on:	
   
what	
   is	
   the	
   main	
   func)on	
   of	
   the	
   federal	
   emergency	
   management	
   

agency	
   (fema)	
   and	
   the	
   funding	
   level	
   provided	
   to	
   meet	
   emergencies?	
   	
   
also,	
   what	
   resources	
   are	
   available	
   to	
   fema	
   such	
   as	
   people,	
   
equipment,	
   facili)es?	
   

</top>	
   

32	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.2 

standard	
   relevance	
   benchmarks:	
   
others	
   
       gov2	
   

       another	
   trec/nist	
   collec)on	
   
       25	
   million	
   web	
   pages	
   
       largest	
   collec)on	
   that	
   is	
   easily	
   available	
   
       but	
   s)ll	
   3	
   orders	
   of	
   magnitude	
   smaller	
   than	
   what	
   google/

yahoo/msn	
   index	
   

       ntcir	
   

       east	
   asian	
   language	
   and	
   cross-     language	
   informa)on	
   retrieval	
   

       cross	
   language	
   evalua)on	
   forum	
   (clef)	
   

       this	
   evalua)on	
   series	
   has	
   concentrated	
   on	
   european	
   languages	
   

and	
   cross-     language	
   informa)on	
   retrieval.	
   

       many	
   others	
   

33	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.5 

impact	
   of	
   inter-     judge	
   agreement	
   

      

impact	
   on	
   absolute	
   performance	
   measure	
   can	
   be	
   signi   cant	
   
(0.32	
   vs	
   0.39)	
   

       liule	
   impact	
   on	
   ranking	
   of	
   di   erent	
   systems	
   or	
   rela)ve	
   

performance	
   

       suppose	
   we	
   want	
   to	
   know	
   if	
   algorithm	
   a	
   is	
   beuer	
   than	
   

algorithm	
   b	
   

       a	
   standard	
   informa)on	
   retrieval	
   experiment	
   will	
   give	
   us	
   a	
   

reliable	
   answer	
   to	
   this	
   ques)on.	
   

34	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.5.1 

cri)que	
   of	
   pure	
   relevance	
   
       relevance	
   vs	
   marginal	
   relevance	
   

       a	
   document	
   can	
   be	
   redundant	
   even	
   if	
   it	
   is	
   highly	
   relevant	
   
       duplicates	
   
       the	
   same	
   informa)on	
   from	
   di   erent	
   sources	
   
       marginal	
   relevance	
   is	
   a	
   beuer	
   measure	
   of	
   u)lity	
   for	
   the	
   

user.	
   

       using	
   facts/en))es	
   as	
   evalua)on	
   units	
   more	
   directly	
   

measures	
   true	
   relevance.	
   

       but	
   harder	
   to	
   create	
   evalua)on	
   set	
   
       see	
   carbonell	
   reference	
   

35	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6.3 

can	
   we	
   avoid	
   human	
   judgment?	
   
       no	
   
       makes	
   experimental	
   work	
   hard	
   

       especially	
   on	
   a	
   large	
   scale	
   

       in	
   some	
   very	
   speci   c	
   sezngs,	
   can	
   use	
   proxies	
   
       e.g.:	
   for	
   approximate	
   vector	
   space	
   retrieval,	
   we	
   can	
   

compare	
   the	
   cosine	
   distance	
   closeness	
   of	
   the	
   closest	
   docs	
   
to	
   those	
   found	
   by	
   an	
   approximate	
   retrieval	
   algorithm	
   

       but	
   once	
   we	
   have	
   test	
   collec)ons,	
   we	
   can	
   reuse	
   them	
   

(so	
   long	
   as	
   we	
   don   t	
   overtrain	
   too	
   badly)	
   

36	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6.3 

evalua)on	
   at	
   large	
   search	
   engines	
   
       search	
   engines	
   have	
   test	
   collec)ons	
   of	
   queries	
   and	
   hand-     ranked	
   

results	
   

       recall	
   is	
   di   cult	
   to	
   measure	
   on	
   the	
   web	
   
       search	
   engines	
   o|en	
   use	
   precision	
   at	
   top	
   k,	
   e.g.,	
   k	
   =	
   10	
   
      

.	
   .	
   .	
   or	
   measures	
   that	
   reward	
   you	
   more	
   for	
   gezng	
   rank	
   1	
   right	
   than	
   
for	
   gezng	
   rank	
   10	
   right.	
   
       ndcg	
   (normalized	
   cumula)ve	
   discounted	
   gain)	
   

       search	
   engines	
   also	
   use	
   non-     relevance-     based	
   measures.	
   

       clickthrough	
   on	
      rst	
   result	
   

       not	
   very	
   reliable	
   if	
   you	
   look	
   at	
   a	
   single	
   clickthrough	
      	
   but	
   preuy	
   

reliable	
   in	
   the	
   aggregate.	
   

       studies	
   of	
   user	
   behavior	
   in	
   the	
   lab	
   
       a/b	
   tes)ng	
   

37	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.6.3 

system	
   that	
   includes	
   the	
   innova)on	
   

a/b	
   tes)ng	
   
       purpose:	
   test	
   a	
   single	
   innova)on	
   
       prerequisite:	
   you	
   have	
   a	
   large	
   search	
   engine	
   up	
   and	
   running.	
   
       have	
   most	
   users	
   use	
   old	
   system	
   
       divert	
   a	
   small	
   propor)on	
   of	
   tra   c	
   (e.g.,	
   1%)	
   to	
   the	
   new	
   
       evaluate	
   with	
   an	
      automa)c   	
   measure	
   like	
   clickthrough	
   on	
   
       now	
   we	
   can	
   directly	
   see	
   if	
   the	
   innova)on	
   does	
   improve	
   user	
   
       probably	
   the	
   evalua)on	
   methodology	
   that	
   large	
   search	
   
      

engines	
   trust	
   most	
   
in	
   principle	
   less	
   powerful	
   than	
   doing	
   a	
   mul)variate	
   regression	
   
analysis,	
   but	
   easier	
   to	
   understand	
   

happiness.	
   

   rst	
   result	
   

38	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.7 

results	
   presentation	
   

39	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.7 

result	
   summaries	
   
       having	
   ranked	
   the	
   documents	
   matching	
   a	
   query,	
   we	
   

wish	
   to	
   present	
   a	
   results	
   list	
   

       most	
   commonly,	
   a	
   list	
   of	
   the	
   document	
   )tles	
   plus	
   a	
   

short	
   summary,	
   aka	
      10	
   blue	
   links   	
   

40	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.7 

summaries	
   
       the	
   )tle	
   is	
   o|en	
   automa)cally	
   extracted	
   from	
   document	
   

metadata.	
   what	
   about	
   the	
   summaries?	
   
       this	
   descrip)on	
   is	
   crucial.	
   
       user	
   can	
   iden)fy	
   good/relevant	
   hits	
   based	
   on	
   descrip)on.	
   

       two	
   basic	
   kinds:	
   

       sta)c	
   
       dynamic	
   
	
   a	
   sta(c	
   summary	
   of	
   a	
   document	
   is	
   always	
   the	
   same,	
   
regardless	
   of	
   the	
   query	
   that	
   hit	
   the	
   doc	
   

      

       a	
   dynamic	
   summary	
   is	
   a	
   query-     dependent	
   auempt	
   to	
   explain	
   

why	
   the	
   document	
   was	
   retrieved	
   for	
   the	
   query	
   at	
   hand	
   

41	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.7 

sta)c	
   summaries	
   
       in	
   typical	
   systems,	
   the	
   sta)c	
   summary	
   is	
   a	
   subset	
   of	
   
       simplest	
   heuris)c:	
   the	
      rst	
   50	
   (or	
   so	
      	
   this	
   can	
   be	
   

the	
   document	
   

varied)	
   words	
   of	
   the	
   document	
   
       summary	
   cached	
   at	
   indexing	
   )me	
   

       more	
   sophis)cated:	
   extract	
   from	
   each	
   document	
   a	
   

set	
   of	
      key   	
   sentences	
   
       simple	
   nlp	
   heuris)cs	
   to	
   score	
   each	
   sentence	
   
       summary	
   is	
   made	
   up	
   of	
   top-     scoring	
   sentences.	
   
       most	
   sophis)cated:	
   nlp	
   used	
   to	
   synthesize	
   a	
   
summary	
   
       seldom	
   used	
   in	
   ir;	
   cf.	
   text	
   summariza)on	
   work	
   

42	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.7 

dynamic	
   summaries	
   
       present	
   one	
   or	
   more	
      windows   	
   within	
   the	
   document	
   that	
   

contain	
   several	
   of	
   the	
   query	
   terms	
   
          kwic   	
   snippets:	
   keyword	
   in	
   context	
   presenta)on	
   

43	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

sec. 8.7 

techniques	
   for	
   dynamic	
   summaries	
   
       find	
   small	
   windows	
   in	
   doc	
   that	
   contain	
   query	
   terms	
   

       requires	
   fast	
   window	
   lookup	
   in	
   a	
   document	
   cache	
   

       score	
   each	
   window	
   wrt	
   query	
   

       use	
   various	
   features	
   such	
   as	
   window	
   width,	
   posi)on	
   in	
   

document,	
   etc.	
   

       combine	
   features	
   through	
   a	
   scoring	
   func)on	
      	
   

methodology	
   to	
   be	
   covered	
   nov	
   12th	
   

       challenges	
   in	
   evalua)on:	
   judging	
   summaries	
   

       easier	
   to	
   do	
   pairwise	
   comparisons	
   rather	
   than	
   binary	
   

relevance	
   assessments	
   

44	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

quicklinks	
   
       for	
   a	
   naviga)onal	
   query	
   such	
   as	
   united	
   airlines	
   
user   s	
   need	
   likely	
   sa)s   ed	
   on	
   www.united.com	
   

       quicklinks	
   provide	
   naviga)onal	
   cues	
   on	
   that	
   home	
   

page	
   

45	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

46	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

alterna)ve	
   results	
   presenta)ons?	
   

47	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

resources	
   for	
   this	
   lecture	
   
       iir	
   8	
   
       mir	
   chapter	
   3	
   
       mg	
   4.5	
   
       carbonell	
   and	
   goldstein	
   1998.	
   the	
   use	
   of	
   mmr,	
   

diversity-     based	
   reranking	
   for	
   reordering	
   documents	
   
and	
   producing	
   summaries.	
   sigir	
   21.	
   

48	
   

