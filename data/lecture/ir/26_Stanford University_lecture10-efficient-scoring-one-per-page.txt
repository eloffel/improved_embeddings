introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc*on	
   to	
   
informa(on	
   retrieval	
   

cs276	
   

informa*on	
   retrieval	
   and	
   web	
   search	
   

chris	
   manning	
   and	
   pandu	
   nayak	
   

e   cient	
   scoring	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

today   s	
   focus	
   
         retrieval	
      	
   get	
   docs	
   matching	
   query	
   from	
   inverted	
   

index	
   

         scoring+ranking	
   

         assign	
   a	
   score	
   to	
   each	
   doc	
   
         pick	
   k	
   highest	
   scoring	
   docs	
   

         our	
   emphasis	
   today	
   will	
   be	
   on	
   doing	
   this	
   e   ciently,	
   

rather	
   than	
   on	
   the	
   quality	
   of	
   the	
   ranking	
   

	
   

2	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

background	
   
         score	
   computa*on	
   is	
   a	
   large	
   (10s	
   of	
   %)	
   frac*on	
   of	
   

the	
   cpu	
   work	
   on	
   a	
   query	
   
         generally,	
   we	
   have	
   a	
   *ght	
   budget	
   on	
   latency	
   (say,	
   250ms)	
   
         cpu	
   provisioning	
   doesn   t	
   permit	
   exhaus*vely	
   scoring	
   

every	
   document	
   on	
   every	
   query	
   

         today	
   we   ll	
   look	
   at	
   ways	
   of	
   cuwng	
   cpu	
   usage	
   for	
   

scoring,	
   without	
   compromising	
   the	
   quality	
   of	
   results	
   
(much)	
   

         basic	
   idea:	
   avoid	
   scoring	
   docs	
   that	
   won   t	
   make	
   it	
   into	
   

the	
   top	
   k	
   

3	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

recap:	
   queries	
   as	
   vectors	
   
         vector	
   space	
   scoring	
   

         we	
   have	
   a	
   weight	
   for	
   each	
   term	
   in	
   each	
   doc	
   
         represent	
   queries	
   as	
   vectors	
   in	
   the	
   space	
   
         rank	
   documents	
   according	
   to	
   their	
   cosine	
   similarity	
   to	
   the	
   

query	
   in	
   this	
   space	
   
         or	
   something	
   more	
   complex:	
   bm25,	
   proximity,	
      	
   

         vector	
   space	
   scoring	
   is	
   
         en*rely	
   query	
   dependent	
   
         addi*ve	
   on	
   term	
   contribu*ons	
      	
   no	
   condi*onals	
   etc.	
   
         context	
   insensi*ve	
   (no	
   interac*ons	
   between	
   query	
   terms)	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

taat	
   vs	
   daat	
   techniques	
   
         taat	
   =	
      term	
   at	
   a	
   time   	
   

         scores	
   for	
   all	
   docs	
   computed	
   concurrently,	
   one	
   query	
   term	
   

at	
   a	
   *me	
   

         daat	
   =	
      document	
   at	
   a	
   time   	
   

         total	
   score	
   for	
   each	
   doc	
   (incl	
   all	
   query	
   terms)	
   computed,	
   

before	
   proceeding	
   to	
   the	
   next	
   

         each	
   has	
   implica*ons	
   for	
   how	
   the	
   retrieval	
   index	
   is	
   

structured	
   and	
   stored	
   

5	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 7.1 

e   cient	
   cosine	
   ranking	
   
         find	
   the	
   k	
   docs	
   in	
   the	
   collec*on	
      nearest   	
   to	
   the	
   

query	
      	
   k	
   largest	
   query-     doc	
   cosines.	
   

         e   cient	
   ranking:	
   

         choosing	
   the	
   k	
   largest	
   cosine	
   values	
   e   ciently.	
   

         can	
   we	
   do	
   this	
   without	
   compu*ng	
   all	
   n	
   cosines?	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

safe	
   vs	
   non-     safe	
   ranking	
   
         the	
   terminology	
      safe	
   ranking   	
   is	
   used	
   for	
   methods	
   

that	
   guarantee	
   that	
   the	
   k	
   docs	
   returned	
   are	
   the	
   k	
   
absolute	
   highest	
   scoring	
   documents	
   
         (not	
   necessarily	
   just	
   under	
   cosine	
   similarity)	
   

         is	
   it	
   ok	
   to	
   be	
   non-     safe?	
   
         if	
   it	
   is	
      	
   then	
   how	
   do	
   we	
   ensure	
   we	
   don   t	
   get	
   too	
   far	
   

from	
   the	
   safe	
   solu*on?	
   
         how	
   do	
   we	
   measure	
   if	
   we	
   are	
   far?	
   

7	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

non-     safe	
   ranking	
   
         covered	
   in	
   depth	
   in	
   coursera	
   video	
   (number	
   7)	
   
         non-     safe	
   ranking	
   may	
   be	
   okay	
   

         ranking	
   func*on	
   is	
   only	
   a	
   proxy	
   for	
   user	
   happiness	
   
         documents	
   close	
   to	
   top	
   k	
   may	
   be	
   just	
      ne	
   

         index	
   elimina*on	
   

         only	
   consider	
   high-     idf	
   query	
   terms	
   
         only	
   consider	
   docs	
   containing	
   many	
   query	
   terms	
   

         champion	
   lists	
   
         high/low	
   lists,	
   *ered	
   indexes	
   
         order	
   pos*ngs	
   by	
   g(d)	
   (query-     indep.	
   quality	
   score)	
   

8	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

safe	
   ranking	
   

9	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

safe	
   ranking	
   
         when	
   we	
   output	
   the	
   top	
   k	
   docs,	
   we	
   have	
   a	
   proof	
   

that	
   these	
   are	
   indeed	
   the	
   top	
   k	
   

         does	
   this	
   imply	
   we	
   always	
   have	
   to	
   compute	
   all	
   n	
   

cosines?	
   
         we   ll	
   look	
   at	
   pruning	
   methods	
   
         so	
   we	
   only	
   fully	
   score	
   some	
   j	
   documents	
   
         do	
   we	
   have	
   to	
   sort	
   the	
   j	
   cosine	
   scores?	
   

10	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

compu*ng	
   the	
   k	
   largest	
   cosines:	
   
selec*on	
   vs.	
   sor*ng	
   
         typically	
   we	
   want	
   to	
   retrieve	
   the	
   top	
   k	
   docs	
   (in	
   the	
   

	
   	
   
sec. 7.1 

cosine	
   ranking	
   for	
   the	
   query)	
   
         not	
   to	
   totally	
   order	
   all	
   docs	
   in	
   the	
   collec*on	
   

         can	
   we	
   pick	
   o   	
   docs	
   with	
   k	
   highest	
   cosines?	
   
         let	
   j	
   =	
   number	
   of	
   docs	
   with	
   nonzero	
   cosines	
   

         we	
   seek	
   the	
   k	
   best	
   of	
   these	
   j	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 7.1 

use	
   heap	
   for	
   selec*ng	
   top	
   k	
   
         binary	
   tree	
   in	
   which	
   each	
   node   s	
   value	
   >	
   the	
   values	
   

of	
   children	
   

         takes	
   2j	
   opera*ons	
   to	
   construct,	
   then	
   each	
   of	
   k	
   

   winners   	
   read	
   o   	
   in	
   o(log	
   j)	
   steps.	
   

         for	
   j=1m,	
   k=100,	
   this	
   is	
   about	
   10%	
   of	
   the	
   cost	
   of	
   

sor*ng.	
   

10 

.9 

.8 

.3 

.1 

.2 

.3 

.1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

wand	
   scoring	
   
         an	
   instance	
   of	
   daat	
   scoring	
   
         basic	
   idea	
   reminiscent	
   of	
   branch	
   and	
   bound	
   

         we	
   maintain	
   a	
   running	
   threshold	
   score	
      	
   e.g.,	
   the	
   kth	
   

highest	
   score	
   computed	
   so	
   far	
   

         we	
   prune	
   away	
   all	
   docs	
   whose	
   cosine	
   scores	
   are	
   

guaranteed	
   to	
   be	
   below	
   the	
   threshold	
   

         we	
   compute	
   exact	
   cosine	
   scores	
   for	
   only	
   the	
   un-     pruned	
   

docs	
   

broder et al. ef   cient query evaluation using a two-level retrieval process. 

13	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

index	
   structure	
   for	
   wand	
   
         pos*ngs	
   ordered	
   by	
   docid	
   
         assume	
   a	
   special	
   iterator	
   on	
   the	
   pos*ngs	
   of	
   the	
   form	
   

   go	
   to	
   the	
      rst	
   docid	
   greater	
   than	
   or	
   equal	
   to	
   x   	
   
         typical	
   state:	
   we	
   have	
   a	
         nger   	
   at	
   some	
   docid	
   in	
   

the	
   pos*ngs	
   of	
   each	
   query	
   term	
   
         each	
      nger	
   moves	
   only	
   to	
   the	
   right,	
   to	
   larger	
   docids	
   
         invariant	
      	
   all	
   docids	
   lower	
   than	
   any	
      nger	
   have	
   

already	
   been	
   processed,	
   meaning	
   
         these	
   docids	
   are	
   either	
   pruned	
   away	
   or	
   
         their	
   cosine	
   scores	
   have	
   been	
   computed	
   

14	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

upper	
   bounds	
   
         at	
   all	
   *mes	
   for	
   each	
   query	
   term	
   t,	
   we	
   maintain	
   an	
   
upper	
   bound	
   ubt	
   on	
   the	
   score	
   contribu*on	
   of	
   any	
   
doc	
   to	
   the	
   right	
   of	
   the	
      nger	
   
         max	
   (over	
   docs	
   remaining	
   in	
   t   s	
   pos*ngs)	
   of	
   wt(doc)	
   

finger 

t 

3	
   

7	
   

11	
   

17	
   

29	
   

38	
   

57	
   

79	
   

ubt = wt(38)  

as finger moves right, ub drops 

15	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

pivo*ng	
   
         query:	
   catcher	
   in	
   the	
   rye	
   
         let   s	
   say	
   the	
   current	
      nger	
   posi*ons	
   are	
   as	
   below	
   

catcher 

273	
   

304	
   

rye 

in 

the 

threshold = 6.8 

ubcatcher = 2.3 

ubrye = 1.8 

ubin = 3.3 

762	
   

ubthe = 4.3 

16	
   

589	
   

pivot 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

prune	
   docs	
   that	
   have	
   no	
   hope	
   
         terms	
   sorted	
   in	
   order	
   of	
      nger	
   posi*ons	
   
         move	
      ngers	
   to	
   589	
   or	
   right	
   

catcher 

273	
   

hopeless	
   docs	
   

304	
   

hopeless	
   docs	
   

rye 

in 

the 

threshold = 6.8 

ubcatcher = 2.3 

ubrye = 1.8 

ubin = 3.3 

762	
   

ubthe = 4.3 

update ub   s 

17	
   

589	
   

pivot 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

compute	
   589   s	
   score	
   if	
   need	
   be	
   
         if	
   589	
   is	
   present	
   in	
   enough	
   pos*ngs,	
   compute	
   its	
   full	
   

cosine	
   score	
      	
   else	
   some	
      ngers	
   to	
   right	
   of	
   589	
   

         pivot	
   again	
      	
   

catcher 

rye 

in 

the 

589	
   

589	
   

589	
   

762	
   

18	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

wand	
   summary	
   
         in	
   tests,	
   wand	
   leads	
   to	
   a	
   90+%	
   reduc*on	
   in	
   score	
   

computa*on	
   
         berer	
   gains	
   on	
   longer	
   queries	
   

         nothing	
   we	
   did	
   was	
   speci   c	
   to	
   cosine	
   ranking	
   

         we	
   need	
   scoring	
   to	
   be	
   addi)ve	
   by	
   term	
   

         wand	
   and	
   variants	
   give	
   us	
   safe	
   ranking	
   

         possible	
   to	
   devise	
      careless   	
   variants	
   that	
   are	
   a	
   bit	
   faster	
   

but	
   not	
   safe	
   (see	
   summary	
   in	
   ding+suel	
   2011)	
   
         ideas	
   combine	
   some	
   of	
   the	
   non-     safe	
   scoring	
   we	
   

considered	
   

19	
   

