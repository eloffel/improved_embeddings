introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc*on	
   to	
   
informa(on	
   retrieval	
   

cs276:	
   informa*on	
   retrieval	
   and	
   web	
   search	
   

pandu	
   nayak	
   and	
   prabhakar	
   raghavan	
   

lecture	
   4:	
   index	
   construc*on	
   

plan	
   
         last	
   lecture:	
   

         dic*onary	
   data	
   structures	
   
         tolerant	
   retrieval	
   

         wildcards	
   
         spell	
   correc*on	
   
         soundex	
   

         this	
   *me:	
   

         index	
   construc*on	
   

a-hu 

n-z 

hy-m 

$m 

mo 

on 

mace 

madden 

among 

amortize 

abandon 

among 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.1 

index	
   construc*on	
   
         how	
   do	
   we	
   construct	
   an	
   index?	
   
         what	
   strategies	
   can	
   we	
   use	
   with	
   limited	
   main	
   

memory?	
   

hardware	
   basics	
   
         many	
   design	
   decisions	
   in	
   informa*on	
   retrieval	
   are	
   

based	
   on	
   the	
   characteris*cs	
   of	
   hardware	
   
         we	
   begin	
   by	
   reviewing	
   hardware	
   basics	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.1 

hardware	
   basics	
   
         access	
   to	
   data	
   in	
   memory	
   is	
   much	
   faster	
   than	
   access	
   

to	
   data	
   on	
   disk.	
   

         disk	
   seeks:	
   no	
   data	
   is	
   transferred	
   from	
   disk	
   while	
   the	
   

disk	
   head	
   is	
   being	
   posi*oned.	
   

         therefore:	
   transferring	
   one	
   large	
   chunk	
   of	
   data	
   from	
   
disk	
   to	
   memory	
   is	
   faster	
   than	
   transferring	
   many	
   small	
   
chunks.	
   

         disk	
   i/o	
   is	
   block-     based:	
   reading	
   and	
   wri*ng	
   of	
   en*re	
   

blocks	
   (as	
   opposed	
   to	
   smaller	
   chunks).	
   

         block	
   sizes:	
   8kb	
   to	
   256	
   kb.	
   

hardware	
   basics	
   
         servers	
   used	
   in	
   ir	
   systems	
   now	
   typically	
   have	
   several	
   

gb	
   of	
   main	
   memory,	
   some*mes	
   tens	
   of	
   gb.	
   	
   
         available	
   disk	
   space	
   is	
   several	
   (2   3)	
   orders	
   of	
   

magnitude	
   larger.	
   

         fault	
   tolerance	
   is	
   very	
   expensive:	
   it  s	
   much	
   cheaper	
   
to	
   use	
   many	
   regular	
   machines	
   rather	
   than	
   one	
   fault	
   
tolerant	
   machine.	
   

1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

	
   

	
   sta(s(c	
    	
   

hardware	
   assump*ons	
   for	
   this	
   lecture	
   
         symbol	
   
         s
	
   
         b	
   
	
   
         	
   	
   	
   	
   	
   	
   	
   
         p 	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   
         	
   	
   	
   	
   	
   	
   	
   
         	
   	
   	
   	
   	
   	
   	
   

	
   
	
   
	
   average	
   seek	
   *me	
   	
   
	
   
	
   transfer	
   *me	
   per	
   byte	
   
	
   processor  s	
   clock	
   rate	
   
	
   low-     level	
   opera*on	
   
	
   
	
   (e.g.,	
   compare	
   &	
   swap	
   a	
   word)	
   
	
   size	
   of	
   main	
   memory	
   	
   
	
   size	
   of	
   disk	
   space 	
   	
   
	
   

	
   value	
   
	
   5	
   ms	
   =	
   5	
   x	
   10   3	
   s	
   
	
   0.02	
     s	
   =	
   2	
   x	
   10   8	
   s	
   
	
   109	
   s   1	
   
	
   0.01	
     s	
   =	
   10   8	
   s	
   

	
   several	
   gb	
   
	
   1	
   tb	
   or	
   more	
   

rcv1:	
   our	
   collec*on	
   for	
   this	
   lecture	
   
         shakespeare  s	
   collected	
   works	
   de   nitely	
   aren  t	
   

large	
   enough	
   for	
   demonstra*ng	
   many	
   of	
   the	
   points	
   
in	
   this	
   course.	
   

         the	
   collec*on	
   we  ll	
   use	
   isn  t	
   really	
   large	
   enough	
   
either,	
   but	
   it  s	
   publicly	
   available	
   and	
   is	
   at	
   least	
   a	
   
more	
   plausible	
   example.	
   

         as	
   an	
   example	
   for	
   applying	
   scalable	
   index	
   

construc*on	
   algorithms,	
   we	
   will	
   use	
   the	
   reuters	
   
rcv1	
   collec*on.	
   

         this	
   is	
   one	
   year	
   of	
   reuters	
   newswire	
   (part	
   of	
   1995	
   

and	
   1996)	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

a	
   reuters	
   rcv1	
   document	
   

	
   
	
   

reuters	
   rcv1	
   sta*s*cs	
   
         symbol
	
   
	
   sta(s(c	
    	
   
	
   
	
   
         n	
   
	
   documents 	
   
	
   
	
   
	
   	
   
	
   
         l	
   
	
   avg.	
   #	
   tokens	
   per	
   doc	
   	
   
	
   
	
   
         m 	
   
	
   terms	
   (=	
   word	
   types)	
   	
   
	
   
         	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
    	
   avg.	
   #	
   bytes	
   per	
   token	
   
         	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
    	
   avg.	
   #	
   bytes	
   per	
   token	
   
         	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
    	
   avg.	
   #	
   bytes	
   per	
   term 	
   
         	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
    	
   non-     posi*onal	
   pos*ngs 	
   100,000,000	
   

	
   value	
   
	
   800,000	
   
	
   200	
   
	
   400,000	
   
	
   6	
   

	
   (incl.	
   spaces/punct.)	
   

	
   (without	
   spaces/punct.)	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   4.5	
   

	
   7.5	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

4.5 bytes per word token vs. 7.5 bytes per word type: why? 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

recall	
   iir	
   1	
   index	
   construc*on	
   
         documents	
   are	
   parsed	
   to	
   extract	
   words	
   and	
   these	
   

are	
   saved	
   with	
   the	
   document	
   id.	
   

doc 1 

doc 2 

i did enact julius 
caesar i was killed  
i' the capitol;  
brutus killed me. 

so let it be with 
caesar. the noble 
brutus hath told you 
caesar was ambitious 

term
i
did
enact 
julius
caesar
i
was
killed
i'
the
capitol
brutus
killed
me
so
let
it
be
with
caesar
the
noble 
brutus
hath 
told 
you
caesar 
was
ambitious

doc #
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

	
   key	
   step	
   
         ager	
   all	
   documents	
   have	
   been	
   

parsed,	
   the	
   inverted	
      le	
   is	
   
sorted	
   by	
   terms.	
   	
   

we focus on this sort step. 
we have 100m items to sort. 

term
i
did
enact 
julius
caesar
i
was
killed
i'
the
capitol
brutus
killed
me
so
let
it
be
with
caesar
the
noble 
brutus
hath 
told 
you
caesar 
was
ambitious

doc #
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

term
ambitious
be
brutus
brutus 
capitol
caesar
caesar
caesar
did
enact
hath
i
i 
i'
it
julius
killed
killed
let
me
noble
so
the
the 
told
you
was
was
with

doc #
2
2
1
2
1
1
2
2
1
1
1
1
1
1
2
1
1
1
2
1
2
2
1
2
2
2
1
2
2

2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

scaling	
   index	
   construc*on	
   
         in-     memory	
   index	
   construc*on	
   does	
   not	
   scale	
   

         can  t	
   stu   	
   en*re	
   collec*on	
   into	
   memory,	
   sort,	
   then	
   write	
   

         how	
   can	
   we	
   construct	
   an	
   index	
   for	
   very	
   large	
   

back	
   

collec*ons?	
   

         taking	
   into	
   account	
   the	
   hardware	
   constraints	
   we	
   just	
   

learned	
   about	
   .	
   .	
   .	
   

         memory,	
   disk,	
   speed,	
   etc.	
   

sort-     based	
   index	
   construc*on	
   
         as	
   we	
   build	
   the	
   index,	
   we	
   parse	
   docs	
   one	
   at	
   a	
   *me.	
   
         while	
   building	
   the	
   index,	
   we	
   cannot	
   easily	
   exploit	
   
compression	
   tricks	
   	
   	
   (you	
   can,	
   but	
   much	
   more	
   complex)	
   

         the	
      nal	
   pos*ngs	
   for	
   any	
   term	
   are	
   incomplete	
   un*l	
   the	
   end.	
   
         at	
   12	
   bytes	
   per	
   non-     posi*onal	
   pos*ngs	
   entry	
   (term,	
   doc,	
   freq),	
   

demands	
   a	
   lot	
   of	
   space	
   for	
   large	
   collec*ons.	
   

         t	
   =	
   100,000,000	
   in	
   the	
   case	
   of	
   rcv1	
   

         so	
      	
   we	
   can	
   do	
   this	
   in	
   memory	
   in	
   2009,	
   but	
   typical	
   

collec*ons	
   are	
   much	
   larger.	
   	
   e.g.,	
   the	
   new	
   york	
   times	
   
provides	
   an	
   index	
   of	
   >150	
   years	
   of	
   newswire	
   

         thus:	
   we	
   need	
   to	
   store	
   intermediate	
   results	
   on	
   disk.	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

sort	
   using	
   disk	
   as	
      memory   ?	
   
         can	
   we	
   use	
   the	
   same	
   index	
   construc*on	
   algorithm	
   
for	
   larger	
   collec*ons,	
   but	
   by	
   using	
   disk	
   instead	
   of	
   
memory?	
   

         no:	
   sor*ng	
   t	
   =	
   100,000,000	
   records	
   on	
   disk	
   is	
   too	
   

slow	
      	
   too	
   many	
   disk	
   seeks.	
   

         we	
   need	
   an	
   external	
   sor*ng	
   algorithm.	
   

boid113neck	
   
         parse	
   and	
   build	
   pos*ngs	
   entries	
   one	
   doc	
   at	
   a	
   *me	
   
         now	
   sort	
   pos*ngs	
   entries	
   by	
   term	
   (then	
   by	
   doc	
   

within	
   each	
   term)	
   

         doing	
   this	
   with	
   random	
   disk	
   seeks	
   would	
   be	
   too	
   slow	
   

   	
   must	
   sort	
   t=100m	
   records	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

bsbi:	
   blocked	
   sort-     based	
   indexing	
   
(sor*ng	
   with	
   fewer	
   disk	
   seeks)	
   
         12-     byte	
   (4+4+4)	
   records	
   (term,	
   doc,	
   freq).	
   
         these	
   are	
   generated	
   as	
   we	
   parse	
   docs.	
   
         must	
   now	
   sort	
   100m	
   such	
   12-     byte	
   records	
   by	
   term.	
   
         de   ne	
   a	
   block	
   ~	
   10m	
   such	
   records	
   
         can	
   easily	
      t	
   a	
   couple	
   into	
   memory.	
   
         will	
   have	
   10	
   such	
   blocks	
   to	
   start	
   with.	
   

         basic	
   idea	
   of	
   algorithm:	
   

         accumulate	
   pos*ngs	
   for	
   each	
   block,	
   sort,	
   write	
   to	
   disk.	
   
         then	
   merge	
   the	
   blocks	
   into	
   one	
   long	
   sorted	
   order.	
   

if every comparison took 2 disk seeks, and n items could be 
sorted with n log2n comparisons, how long would this take? 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   sec. 4.2 

3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   sec. 4.2 

sor*ng	
   10	
   blocks	
   of	
   10m	
   records	
   
         first,	
   read	
   each	
   block	
   and	
   sort	
   within:	
   	
   	
   

         quicksort	
   takes	
   2n	
   ln	
   n	
   expected	
   steps	
   
         in	
   our	
   case	
   2	
   x	
   (10m	
   ln	
   10m)	
   steps	
   

         exercise:	
   es)mate	
   total	
   )me	
   to	
   read	
   each	
   block	
   from	
   

disk	
   and	
   and	
   quicksort	
   it.	
   

         10	
   *mes	
   this	
   es*mate	
      	
   gives	
   us	
   10	
   sorted	
   runs	
   of	
   

         done	
   straighqorwardly,	
   need	
   2	
   copies	
   of	
   data	
   on	
   disk	
   

10m	
   records	
   each.	
   

         but	
   can	
   op*mize	
   this	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.2 

how	
   to	
   merge	
   the	
   sorted	
   runs?	
   
         can	
   do	
   binary	
   merges,	
   with	
   a	
   merge	
   tree	
   of	
   log210	
   =	
   4	
   layers.	
   
         during	
   each	
   layer,	
   read	
   into	
   memory	
   runs	
   in	
   blocks	
   of	
   10m,	
   

merge,	
   write	
   back.	
   

1 

3 

2 

4 

runs being 
merged. 

disk 

merged run. 

1 

2 

3 

4 

how	
   to	
   merge	
   the	
   sorted	
   runs?	
   
         but	
   it	
   is	
   more	
   e   cient	
   to	
   do	
   a	
   mul*-     way	
   merge,	
   where	
   you	
   

are	
   reading	
   from	
   all	
   blocks	
   simultaneously	
   

         providing	
   you	
   read	
   decent-     sized	
   chunks	
   of	
   each	
   block	
   into	
   
memory	
   and	
   then	
   write	
   out	
   a	
   decent-     sized	
   output	
   chunk,	
   
then	
   you  re	
   not	
   killed	
   by	
   disk	
   seeks	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.3 

remaining	
   problem	
   with	
   sort-     based	
   
algorithm	
   
         our	
   assump*on	
   was:	
   we	
   can	
   keep	
   the	
   dic*onary	
   in	
   

memory.	
   

         we	
   need	
   the	
   dic*onary	
   (which	
   grows	
   dynamically)	
   in	
   

order	
   to	
   implement	
   a	
   term	
   to	
   termid	
   mapping.	
   

         actually,	
   we	
   could	
   work	
   with	
   term,docid	
   pos*ngs	
   

instead	
   of	
   termid,docid	
   pos*ngs	
   .	
   .	
   .	
   

         .	
   .	
   .	
   but	
   then	
   intermediate	
      les	
   become	
   very	
   large.	
   
(we	
   would	
   end	
   up	
   with	
   a	
   scalable,	
   but	
   very	
   slow	
   
index	
   construc*on	
   method.)	
   

spimi:	
   	
   
single-     pass	
   in-     memory	
   indexing	
   
         key	
   idea	
   1:	
   generate	
   separate	
   dic*onaries	
   for	
   each	
   
block	
      	
   no	
   need	
   to	
   maintain	
   term-     termid	
   mapping	
   
across	
   blocks.	
   

         key	
   idea	
   2:	
   don  t	
   sort.	
   accumulate	
   pos*ngs	
   in	
   

pos*ngs	
   lists	
   as	
   they	
   occur.	
   

         with	
   these	
   two	
   ideas	
   we	
   can	
   generate	
   a	
   complete	
   

inverted	
   index	
   for	
   each	
   block.	
   

         these	
   separate	
   indexes	
   can	
   then	
   be	
   merged	
   into	
   one	
   

big	
   index.	
   

4 

	
   	
   
sec. 4.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.3 

spimi:	
   compression	
   
         compression	
   makes	
   spimi	
   even	
   more	
   e   cient.	
   

         compression	
   of	
   terms	
   
         compression	
   of	
   pos*ngs	
   

         see	
   next	
   lecture	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   
spimi-     invert	
   

	
   	
   

         merging	
   of	
   blocks	
   is	
   analogous	
   to	
   bsbi.	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

distributed	
   indexing	
   
         for	
   web-     scale	
   indexing	
   (don  t	
   try	
   this	
   at	
   home!):	
   

must	
   use	
   a	
   distributed	
   compu*ng	
   cluster	
   
         individual	
   machines	
   are	
   fault-     prone	
   

         can	
   unpredictably	
   slow	
   down	
   or	
   fail	
   

         how	
   do	
   we	
   exploit	
   such	
   a	
   pool	
   of	
   machines?	
   

web	
   search	
   engine	
   data	
   centers	
   
         web	
   search	
   data	
   centers	
   (google,	
   bing,	
   baidu)	
   mainly	
   

contain	
   commodity	
   machines.	
   

         data	
   centers	
   are	
   distributed	
   around	
   the	
   world.	
   
         es*mate:	
   google	
   ~1	
   million	
   servers,	
   3	
   million	
   

processors/cores	
   (gartner	
   2007)	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

massive	
   data	
   centers	
   
         if	
   in	
   a	
   non-     fault-     tolerant	
   system	
   with	
   1000	
   nodes,	
   

each	
   node	
   has	
   99.9%	
   up*me,	
   what	
   is	
   the	
   up*me	
   of	
   
the	
   system?	
   
         answer:	
   63%	
   
         exercise:	
   calculate	
   the	
   number	
   of	
   servers	
   failing	
   per	
   

minute	
   for	
   an	
   installa*on	
   of	
   1	
   million	
   servers.	
   

distributed	
   indexing	
   
         maintain	
   a	
   master	
   machine	
   direc*ng	
   the	
   indexing	
   job	
   

   	
   considered	
      safe   .	
   

         break	
   up	
   indexing	
   into	
   sets	
   of	
   (parallel)	
   tasks.	
   
         master	
   machine	
   assigns	
   each	
   task	
   to	
   an	
   idle	
   machine	
   

from	
   a	
   pool.	
   

5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

parallel	
   tasks	
   
         we	
   will	
   use	
   two	
   sets	
   of	
   parallel	
   tasks	
   

         parsers	
   
         inverters	
   

         break	
   the	
   input	
   document	
   collec*on	
   into	
   splits	
   
         each	
   split	
   is	
   a	
   subset	
   of	
   documents	
   (corresponding	
   to	
   

blocks	
   in	
   bsbi/spimi)	
   

parsers	
   
         master	
   assigns	
   a	
   split	
   to	
   an	
   idle	
   parser	
   machine	
   
         parser	
   reads	
   a	
   document	
   at	
   a	
   *me	
   and	
   emits	
   (term,	
   

doc)	
   pairs	
   

         parser	
   writes	
   pairs	
   into	
   j	
   par**ons	
   
         each	
   par**on	
   is	
   for	
   a	
   range	
   of	
   terms  	
      rst	
   lemers	
   

         (e.g.,	
   a-     f,	
   g-     p,	
   q-     z)	
      	
   here	
   j	
   =	
   3.	
   

         now	
   to	
   complete	
   the	
   index	
   inversion	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

inverters	
   
         an	
   inverter	
   collects	
   all	
   (term,doc)	
   pairs	
   (=	
   pos*ngs)	
   

for	
   one	
   term-     par**on.	
   

         sorts	
   and	
   writes	
   to	
   pos*ngs	
   lists	
   

data	
      ow	
   

assign 

master 

assign 

postings 

parser 

a-f  g-p  q-z 

inverter 

a-f 

parser 

a-f  g-p  q-z 

splits 

parser 

a-f  g-p  q-z 

inverter 

g-p 

inverter 

q-z 

map 
phase 

segment files 

reduce 
phase 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

mapreduce	
   
         the	
   index	
   construc*on	
   algorithm	
   we	
   just	
   described	
   is	
   

an	
   instance	
   of	
   mapreduce.	
   

         mapreduce	
   (dean	
   and	
   ghemawat	
   2004)	
   is	
   a	
   robust	
   
and	
   conceptually	
   simple	
   framework	
   for	
   distributed	
   
compu*ng	
      	
   

            	
   without	
   having	
   to	
   write	
   code	
   for	
   the	
   distribu*on	
   

part.	
   

         they	
   describe	
   the	
   google	
   indexing	
   system	
   (ca.	
   2002)	
   

as	
   consis*ng	
   of	
   a	
   number	
   of	
   phases,	
   each	
   
implemented	
   in	
   mapreduce.	
   

mapreduce	
   
         index	
   construc*on	
   was	
   just	
   one	
   phase.	
   
         another	
   phase:	
   transforming	
   a	
   term-     par**oned	
   

index	
   into	
   a	
   document-     par**oned	
   index.	
   
         term-     par))oned:	
   one	
   machine	
   handles	
   a	
   subrange	
   of	
   

terms	
   

documents	
   

         document-     par))oned:	
   one	
   machine	
   handles	
   a	
   subrange	
   of	
   

         as	
   we  ll	
   discuss	
   in	
   the	
   web	
   part	
   of	
   the	
   course,	
   most	
   
search	
   engines	
   use	
   a	
   document-     par**oned	
   index	
      	
   
bemer	
   load	
   balancing,	
   etc.	
   

6 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.4 

schema	
   for	
   index	
   construc*on	
   in	
   
mapreduce	
   
         schema	
   of	
   map	
   and	
   reduce	
   func(ons	
   
         map:	
   input	
      	
   list(k,	
   v)	
   	
   	
   	
   	
   reduce:	
   (k,list(v))	
      	
   output	
   
         instan(a(on	
   of	
   the	
   schema	
   for	
   index	
   construc(on	
   
         map:	
   collec*on	
      	
   list(termid,	
   docid)	
   
         reduce:	
   (<termid1,	
   list(docid)>,	
   <term ,	
   list(docid)>,	
      )	
      	
   

(pos*ngs	
   list1,	
   pos*ngs	
   list2,	
      )	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

example	
   for	
   index	
   construc(on	
   
         map:	
   
         d1	
   :	
   c	
   came,	
   c	
   c  ed.	
   	
   
         d2	
   :	
   c	
   died.	
      	
   
         <c,d1>,	
   <came,d1>,	
   <c,d1>,	
   <c  ed,	
   d1>,	
   <c,	
   d2>,	
   

<died,d2>	
   
         reduce:	
   
         (<c,(d1,d2,d1)>,	
   <died,(d2)>,	
   <came,(d1)>,	
   <c  ed,(d1)
>)	
   	
      	
   	
   (<c,(d1:2,d2:1)>,	
   <died,(d2:1)>,	
   <came,(d1:1)>,	
   
<c  ed,(d1:1)>)	
   

38	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

dynamic	
   indexing	
   
         up	
   to	
   now,	
   we	
   have	
   assumed	
   that	
   collec*ons	
   are	
   

sta*c.	
   

         they	
   rarely	
   are:	
   	
   

         documents	
   come	
   in	
   over	
   *me	
   and	
   need	
   to	
   be	
   inserted.	
   
         documents	
   are	
   deleted	
   and	
   modi   ed.	
   

         this	
   means	
   that	
   the	
   dic*onary	
   and	
   pos*ngs	
   lists	
   have	
   

to	
   be	
   modi   ed:	
   
         pos*ngs	
   updates	
   for	
   terms	
   already	
   in	
   dic*onary	
   
         new	
   terms	
   added	
   to	
   dic*onary	
   

simplest	
   approach	
   
         maintain	
      big   	
   main	
   index	
   
         new	
   docs	
   go	
   into	
      small   	
   auxiliary	
   index	
   
         search	
   across	
   both,	
   merge	
   results	
   
         dele*ons	
   

         invalida*on	
   bit-     vector	
   for	
   deleted	
   docs	
   
         filter	
   docs	
   output	
   on	
   a	
   search	
   result	
   by	
   this	
   invalida*on	
   

bit-     vector	
   

         periodically,	
   re-     index	
   into	
   one	
   main	
   index	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

issues	
   with	
   main	
   and	
   auxiliary	
   indexes	
   
         problem	
   of	
   frequent	
   merges	
      	
   you	
   touch	
   stu   	
   a	
   lot	
   
         poor	
   performance	
   during	
   merge	
   
         actually:	
   

         merging	
   of	
   the	
   auxiliary	
   index	
   into	
   the	
   main	
   index	
   is	
   e   cient	
   if	
   we	
   

keep	
   a	
   separate	
      le	
   for	
   each	
   pos*ngs	
   list.	
   

         merge	
   is	
   the	
   same	
   as	
   a	
   simple	
   append.	
   
         but	
   then	
   we	
   would	
   need	
   a	
   lot	
   of	
      les	
      	
   ine   cient	
   for	
   os.	
   

         assump*on	
   for	
   the	
   rest	
   of	
   the	
   lecture:	
   the	
   index	
   is	
   one	
   big	
   

        

   le.	
   
in	
   reality:	
   use	
   a	
   scheme	
   somewhere	
   in	
   between	
   (e.g.,	
   split	
   
very	
   large	
   pos*ngs	
   lists,	
   collect	
   pos*ngs	
   lists	
   of	
   length	
   1	
   in	
   one	
   
   le	
   etc.)	
   

logarithmic	
   merge	
   
         maintain	
   a	
   series	
   of	
   indexes,	
   each	
   twice	
   as	
   large	
   as	
   

the	
   previous	
   one	
   
         at	
   any	
   *me,	
   some	
   of	
   these	
   powers	
   of	
   2	
   are	
   instan*ated	
   

         keep	
   smallest	
   (z0)	
   in	
   memory	
   
         larger	
   ones	
   (i0,	
   i1,	
      )	
   on	
   disk	
   
         if	
   z0	
   gets	
   too	
   big	
   (>	
   n),	
   write	
   to	
   disk	
   as	
   i0	
   
         or	
   merge	
   with	
   i0	
   (if	
   i0	
   already	
   exists)	
   as	
   z1	
   
         either	
   write	
   merge	
   z1	
   to	
   disk	
   as	
   i1	
   (if	
   no	
   i1)	
   
         or	
   merge	
   with	
   i1	
   to	
   form	
   z2	
   

7 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   sec. 4.5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

logarithmic	
   merge	
   
         auxiliary	
   and	
   main	
   index:	
   index	
   construc*on	
   *me	
   is	
   

o(t2)	
   as	
   each	
   pos*ng	
   is	
   touched	
   in	
   each	
   merge.	
   

         logarithmic	
   merge:	
   each	
   pos*ng	
   is	
   merged	
   o(log	
   t)	
   

*mes,	
   so	
   complexity	
   is	
   o(t	
   log	
   t)	
   

         so	
   logarithmic	
   merge	
   is	
   much	
   more	
   e   cient	
   for	
   index	
   

construc*on	
   

         but	
   query	
   processing	
   now	
   requires	
   the	
   merging	
   of	
   o

(log	
   t)	
   indexes	
   
         whereas	
   it	
   is	
   o(1)	
   if	
   you	
   just	
   have	
   a	
   main	
   and	
   auxiliary	
   

index	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

further	
   issues	
   with	
   mul*ple	
   indexes	
   
         collec*on-     wide	
   sta*s*cs	
   are	
   hard	
   to	
   maintain	
   
         e.g.,	
   when	
   we	
   spoke	
   of	
   spell-     correc*on:	
   which	
   of	
   

several	
   corrected	
   alterna*ves	
   do	
   we	
   present	
   to	
   the	
   
user?	
   
         we	
   said,	
   pick	
   the	
   one	
   with	
   the	
   most	
   hits	
   

         how	
   do	
   we	
   maintain	
   the	
   top	
   ones	
   with	
   mul*ple	
   

indexes	
   and	
   invalida*on	
   bit	
   vectors?	
   
         one	
   possibility:	
   ignore	
   everything	
   but	
   the	
   main	
   index	
   for	
   

such	
   ordering	
   

         will	
   see	
   more	
   such	
   sta*s*cs	
   used	
   in	
   results	
   ranking	
   

dynamic	
   indexing	
   at	
   search	
   engines	
   
         all	
   the	
   large	
   search	
   engines	
   now	
   do	
   dynamic	
   indexing	
   
         their	
   indices	
   have	
   frequent	
   incremental	
   changes	
   

         news	
   items,	
   blogs,	
   new	
   topical	
   web	
   pages	
   

         sarah	
   palin,	
      	
   

         but	
   (some*mes/typically)	
   they	
   also	
   periodically	
   

reconstruct	
   the	
   index	
   from	
   scratch	
   
         query	
   processing	
   is	
   then	
   switched	
   to	
   the	
   new	
   index,	
   and	
   

the	
   old	
   index	
   is	
   deleted	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   sec. 4.5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 4.5 

other	
   sorts	
   of	
   indexes	
   
         posi*onal	
   indexes	
   

         same	
   sort	
   of	
   sor*ng	
   problem	
      	
   just	
   larger	
   

         building	
   character	
   n-     gram	
   indexes:	
   
         as	
   text	
   is	
   parsed,	
   enumerate	
   n-     grams.	
   
         for	
   each	
   n-     gram,	
   need	
   pointers	
   to	
   all	
   dic*onary	
   terms	
   

why? 

containing	
   it	
      	
   the	
      pos*ngs   .	
   

         note	
   that	
   the	
   same	
      pos*ngs	
   entry   	
   will	
   arise	
   repeatedly	
   
in	
   parsing	
   the	
   docs	
      	
   need	
   e   cient	
   hashing	
   to	
   keep	
   track	
   
of	
   this.	
   
         e.g.,	
   that	
   the	
   trigram	
   uou	
   occurs	
   in	
   the	
   term	
   deciduous	
   will	
   be	
   

discovered	
   on	
   each	
   text	
   occurrence	
   of	
   deciduous	
   

         only	
   need	
   to	
   process	
   each	
   term	
   once	
   

8 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 4 

resources	
   for	
   today  s	
   lecture	
   
         chapter	
   4	
   of	
   iir	
   
         mg	
   chapter	
   5	
   
         original	
   publica*on	
   on	
   mapreduce:	
   dean	
   and	
   

ghemawat	
   (2004)	
   

         original	
   publica*on	
   on	
   spimi:	
   heinz	
   and	
   zobel	
   (2003)	
   

9 

