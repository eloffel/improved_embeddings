introduction	to	information	retrieval

introduction	to
information	retrieval

cs276

information	retrieval	and	web	search

chris	manning	and	pandu	nayak

personalization

introduction	to	information	retrieval

ambiguity
   unlikely	that	a	short	query	can	unambiguously	

describe	a	user   s	information	need

   for	example,	the	query	[chi]	can	mean

   calamos	convertible	opportunities	&	income	fund	quote
   the	city	of	chicago
   balancing	one   s	natural	energy	(or	ch   i)
   computer-human	interactions	

2

introduction	to	information	retrieval

personalization
   ambiguity	means	that	a	single	ranking	is	unlikely	to	be	

optimal	for	all	users

   personalized	ranking	is	the	only	way	to	bridge	the	gap
   personalization	can	use

   long	term	behavior	to	identify	user	interests,	

e.g.,	a	long	term	interest	in	user	interface	research

   short	term	session	to	identify	current	task,

e.g.,	checking	on	a	series	of	stock	tickers

   user	location,	e.g.,	mta	in	new	york	vs	baltimore
   social	network
      

3

introduction	to	information	retrieval

potential	for	personalization
[teevan,	dumais,	horvitz	2010]
  how	much	can	personalization	improve	ranking?		how	
can	we	measure	this?

  ask	raters	to	explicitly	rate	a	set	of	queries

   but	rather	than	asking	them	to	guess	what	a	

user   s	information	need	might	be	   

   ...	ask	which	results	they	would	personally	

consider	relevant

   use	self-generated	and	pre-generated	queries

4

introduction	to	information	retrieval

computing	potential	for	personalization
   for	each	query	q

   compute	average	rating	for	each	result
   let	rq be	the	optimal	ranking	according	to	the	average	

rating

   compute	the	ndcg	value	of	ranking	rq for	the	ratings	of	

each	rater	i

   let	avgq be	the	average	of	the	ndcg	values	for	each	rater

   let	avg	be	the	average	avgq over	all	queries
   potential	for	personalization	is	(1	    avg)

5

introduction	to	information	retrieval

example:	ndcg	values	for	a	query

result

d1
d2
d3
d4
d5
d6
d7
d8
d9
d10
ndcg

rater	a

rater	b

average	rating

1
1
0
0
0
1
1
0
0
0
0.88

0
1
1
0
0
0
2
0
0
0
0.65

0.5
1
0.5
0
0
0.5
1.5
0
0
0

average ndcg for raters: 0.77

6

introduction	to	information	retrieval

example:	ndcg	values	for	optimal	
ranking	for	average	ratings

result

d7
d2
d1
d3
d6
d4
d5
d8
d9
d10
ndcg

rater	a

rater	b

average	rating

1
1
1
0
1
0
0
0
0
0
0.98

2
1
0
1
0
0
0
0
0
0
0.96

1.5
1
0.5
0.5
0.5
0
0
0
0
0

average ndcg for raters: 0.97

7

introduction	to	information	retrieval

example:	potential	for	personalization

result

d7
d2
d1
d3
d6
d4
d5
d8
d9
d10
ndcg

rater	a

rater	b

average	rating

1
1
1
0
1
0
0
0
0
0
0.98

2
1
0
1
0
0
0
0
0
0
0.96

1.5
1
0.5
0.5
0.5
0
0
0
0
0

potential for personalization: 0.03

8

introduction	to	information	retrieval

potential	for	personalization	graph

potential for 
personalization

g
c
d
n

number of raters

9

introduction	to	information	retrieval

personalizing	search

10

introduction	to	information	retrieval

personalizing	search
[pitkow	et	al.	2002]
  two	general	ways	of	personalizing	search

   query	expansion

   modify	or	augment	user	query
   e.g.,	query	term	   ir    can	be	augmented	with	either	   information	

retrieval    or	   ingersoll-rand    depending	on	user	interest

   ensures	that	there	are	enough	personalized	results

   reranking

   issue	the	same	query	and	fetch	the	same	results	   
      	but	rerank	the	results	based	on	a	user	profile
   allows	both	personalized	and	globally	relevant	results

11

introduction	to	information	retrieval

user	interests
   explicitly	provided	by	the	user

   sometimes	useful,	particularly	for	new	users
      	but	generally	doesn   t	work	well

   inferred	from	user	behavior	and	content

   previously	issued	search	queries
   previously	visited	web	pages
   personal	documents
   emails

   ensuring	privacy	and	user	control	is	very	important 12

introduction	to	information	retrieval

relevance	feedback	perspective
[teevan,	dumais,	horvitz	2005]

query

personalized	
reranking

results

search
engine

personalized
results

user model
(source of relevant
documents)

13

introduction	to	information	retrieval

binary	independence	model

    estimating	rsv	coefficients	in	theory
    for	each	term	i look	at	this	table	of	document	counts:

c
i

=

log

p
r
1(
-
i
i
p
r
1(
-
i
i

)
)

relevant  non-relevant  total 

documents 
 
xi=1 
xi=0 
total 

ni 
n-ni 
n 

si 
s-si 
s 
si
s

ri    

ni-si 

n-ni-s+si 

n-s 
(ni    si)
(n    s)

si (s     si)

(ni     si) (n    ni     s + si)

for now,
assume no
zero terms.
see later
lecture.

  

    estimates:

pi    

ci     k(n,ni,s,si) = log

introduction	to	information	retrieval

personalization	as	relevance	feedback

n

si

s

ni

n

ni

documents
containing
term i

all

documents

relevant 
documents

  n = n + s
  ni = ni + si

s

si

user

content

15

introduction	to	information	retrieval

reranking
   bm25	scoring

ci      tfi

   use	updated	weight	ci in	bm25

(           "+0.5)(           "+0.5)
(    "+0.5)
(    "+0.5)    log

    "=log

where	we	have	used

  n = n + s
  ni = ni + si

(    "+0.5)
(           "+0.5)+            "

16

introduction	to	information	retrieval

corpus	representation
   estimating	n and	ni

   many	possibilities

   n:	all	documents,	query	relevant	documents,	result	set
   ni:	full	text,	only	titles	and	snippets

   practical	strategy

   approximate	corpus	statistics	from	result	set
      	and	just	the	title	and	snippets
   empirically	seems	to	work	the	best!

17

introduction	to	information	retrieval

user	representation
   estimating	s and	si

   estimated	from	a	local	search	index	containing

   web	pages	the	user	has	viewed
   email	messages	that	were	viewed	or	sent
   calendar	items
   documents	stored	on	the	client	machine

   best	performance	when

   s is	the	number	of	local	documents	matching	the	query
   si is	the	number	that	also	contains	term	i

18

introduction	to	information	retrieval

document	and	query	representation
   document	represented	by	the	title	and	snippets

   query	is	expanded	to	contain	words	near	query	

terms	(in	titles	and	snippets)
   for	the	query	[cancer]	add	underlined	terms

the	american cancer society is	dedicated	to	eliminating cancer as	a	
major health	problem	by	preventing cancer,	saving lives,	and	
diminishing	suffering	through	   

   this	combination	of	corpus,	user,	document,	and	

query	representations	seem	to	work	well

19

introduction	to	information	retrieval

location

20

introduction	to	information	retrieval

user	location
   user	location	is	one	of	the	most	important	features	

for	personalization
   country

   query	[football]	in	the	us	vs	the	uk

   state/metro/city

   queries	like	[zoo],	[craigslist],	[giants]

   fine-grained	location

   queries	like	[pizza],	[restaurants],	[coffee	shops]

21

introduction	to	information	retrieval

challenges
   not	all	queries	are	location	sensitive

   [facebook]	is	not	asking	for	the	closest	facebook	office
   [seaworld]	is	not	necessarily	asking	for	the	closest	seaworld

   different	parts	of	a	site	may	be	more	or	less	location	

sensitive
   nytimes	home	page	vs	nytimes	local	section

   addresses	on	a	page	don   t	always	tell	us	how	location	

sensitive	the	page	is
   stanford	home	page	has	address,	but	not	location	sensitive22

introduction	to	information	retrieval

key	idea
[bennett	et	al.	2011]
   usage	statistics,	rather	than	locations	mentioned	in	a	

document,	best	represent	where	it	is	relevant
   i.e.,	if	users	in	a	location	tend	to	click	on	that	document,	

then	it	is	relevant	in	that	location

   user	location	data	is	acquired	from	anonymized logs	

(with	user	consent,	e.g.,	from	a	widely	distributed	
browser	extension)
   user	ip	addresses	are	resolved	into	geographic	location	

information

23

introduction	to	information	retrieval

location	interest	model
   use	the	logs	data	to	estimate	the	id203	of	the	

location	of	the	user	given	they	viewed	this	url

p(location = x |url)

24

introduction	to	information	retrieval

location	interest	model
   use	the	logs	data	to	estimate	the	id203	of	the	

location	of	the	user	given	they	viewed	this	url

p(location = x |url)

25

introduction	to	information	retrieval

learning	the	location	interest	model
   for	compactness,	represent	location	interest	model	

as	a	mixture	of	5-25	2-d	gaussians	(x is	[lat,	long])

p(location = x |url) = win(x;  i,    i

)

n
   
i=1
e   

=

n
   
i=1

wi

(2  )2 |   i |1/2

1
2(x     i)t

  i   1(x     i)

   learn	gaussian	mixture	model	using	em

   expectation	step:	estimate	id203	that	each	point	

belongs	to	each	gaussian

   maximization	step:	estimate	most	likely	mean,	covariance,	

weight

26

introduction	to	information	retrieval

more	location	interest	models
   learn	a	location-interest	model	for	queries

   using	location	of	users	who	issued	the	query

   learn	a	background	model	showing	the	overall	

density	of	users

27

introduction	to	information	retrieval

topics	in	urls	with	high	
p(user	location	|	url)

28

introduction	to	information	retrieval

location	sensitive	features
   non-contextual	features	(user-independent)

   is	the	query	location	sensitive?		what	about	the	urls?
   feature:	id178	of	the	location	distribution

   low	id178	means	distribution	is	peaked	and	location	is	important

   feature:	kl-divergence	between	location	model	and	

background	model
   high	kl-divergence	suggests	that	it	is	location	sensitive

   feature:	kl-divergence	between	query	and	url	models
   low	kl-divergence	suggests	url	is	more	likely	to	be	relevant	to	

users	issuing	the	query

29

introduction	to	information	retrieval

more	location	sensitive	features
   contextual	features	(user-dependent)

   feature:	user   s	location	(naturally!)
   feature:	id203	of	the	user   s	location	given	the	url
   computed	by	evaluating	url   s	location	model	at	user	location
   feature	is	high	when	user	is	at	a	location	where	url	is	popular
   downside:	large	population	centers	tend	to	higher	probabilities	for	

all	urls

   feature:	use	bayes	rule	to	compute	p(url	|	user	location)
   feature:	also	create	a	normalized	version	of	the	above	

feature	by	normalizing	with	the	background	model

   features:	versions	of	the	above	with	query	instead	of	url

30

introduction	to	information	retrieval

learning	to	rank
   add	location	features	(in	addition	to	standard	

features)	for	machine	learned	ranking
   training	data	derived	from	logs
   p(url	|	user	location)	turns	out	to	be	an	important	feature
   kl	divergence	of	the	url	model	from	the	background	

model	also	plays	an	important	role

31

introduction	to	information	retrieval

query	model	for	[rta	bus	schedule]

user in new orleans

32

introduction	to	information	retrieval

url	model	for	top	original	result

user in new orleans

33

introduction	to	information	retrieval

url	model	for	promoted	url

user in new orleans

34

introduction	to	information	retrieval

personalized	id95

35

introduction	to	information	retrieval

id95	review
   let	a be	the	stochastic	matrix	corresponding	to	the	

web	graph	g over	n nodes
   no	teleportation	links	(but	assume	no	deadends	in	g)
   if	node	i has	oi outlinks,	and	there	is	an	edge	from	node	i

to	node	j,	then	aij =	1/oi

   let	p be	the	teleportation	probabilities

   (n x	1)	column	vector	with	each	entry	being	1/n

   id95	vector	r is	defined	by	the	following

r = (1     )ar +  p

36

introduction	to	information	retrieval

personalized	id95
[haveliwala	2003]	[jeh	and	widom	2003]
  in	the	basic	id95	computation,	teleportation	
id203	vector	p is	uniform	over	all	pages
  but	if	the	user	has	preferences	on	which	pages	to	
teleport	to,	that	preference	can	be	represented	in	p

   p could	be	uniform	over	user   s	bookmarks
   or	it	could	be	non-zero	on	just	pages	on	topics	of	interest	

to	the	user

  id95	would	be	personalized	to	user   s	interests

  but	computing	personalized	id95	is	expensive

37

introduction	to	information	retrieval

linearity	theorem
   for	any	preference	vectors	u1 and	u2,	if	v1 and	v2 are	

the	corresponding	personalized	id95	vectors,	
then	for	any	non-negative	constants	a1 and	a2 such	
that	a1+	a2 =	1,	we	have

a1v1 + a2v2 = (1     )a(a1v1 + a2v2) +  (a1u1 + a2u2)

   proof

a1v1 + a2v2

= a1((1     )av1 +  u1) + a2((1     )av2 +  u2)
= a1(1     )av1 + a1  u1 + a2(1     )av2 + a2  u2
= (1     )a(a1v1 + a2v2) +  (a1u1 + a2u2)

38

introduction	to	information	retrieval

topic-sensitive	id95
   compute	personalized	id95	vector	per	topic

   16	top-level	topics	from	the	open	directory	project
   each	odp	topic	has	a	set	of	pages	(hand-)classified	

into	that	topic

   preference	vector	for	the	topic	is	uniform	over	

pages	in	that	topic,	and	0	elsewhere

   note:	[jeh	and	widom	2003]	provide	a	more	general	

treatment

39

introduction	to	information	retrieval

query-time	processing
   construct	a	distribution	over	topics	for	the	query

   user	profile	can	provide	a	distribution	over	topics
   query	can	be	classified	into	the	different	topics
   any	other	context	information	can	be	used	to	inform	topic	

distributions

   use	the	topic	preferences	to	compute	a	weighted	

linear	combination	of	topic	id95	vectors	to	use	
in	place	of	id95

40

introduction	to	information	retrieval

social	networks

41

introduction	to	information	retrieval

unicorn
[curtiss	et	al	2013]
  primary	backend	for	facebook	graph	search

  facebook	social	graph

   nodes	represent	people	and	things	(entities)
   each	entity	has	a	unique	64-bit	id
   edges	represent	relationships	between	nodes
   there	are	many	thousands	of	edge-types

   examples:	friend,	likes,	likers,	   

42

introduction	to	information	retrieval

data	model
   billions	of	nodes,	but	graph	is	sparse

   represent	graph	using	adjacency	list
   postings	sorted	by	sort-key (importance)	and	

then	id

   index	sharded	by	result-id

43

introduction	to	information	retrieval

basic	set	operations
   query	language	includes	basic	set	operations

   and, or, difference

   friends	of	either	jon	jones	(id 5)	and	lea	lin	(id 6)

(or(friend:5 friend:6))

   female	friends	of	jon	jones	who	are	not	friend	of	lea	lin
(difference (and friend:5 gender:1)

friend:6)

44

introduction	to	information	retrieval

typeahead
   find	users	by	typing	first	few	characters	of	their	name
   index	servers	contain	postings	lists	for	every	name	

prefix	up	to	a	predefined	character	limit
   simple	typeahead	implementation	would	simply	return	ids	

in	the	corresponding	postings	lists

   simple	solution	doesn   t	ensure	social	relevance
   alternate	solution:	use	a	conjunctive	query

(and mel* friend:3)
   misses	people	who	are	not	friends
   issuing	two	queries	is	expensive

45

introduction	to	information	retrieval

weakand operator
   provides	a	mechanism	for	some	fraction	of	results	to	

possess	a	trait	without	requiring	trait	for	all	results
   weakand allows	missing	terms	from	some	results

   these	optional	terms	can	have	an	optional	count	or	weight
   once	the	optional	count	is	met,	the	term	is	required

(weak-and (term friend:3 :optional-hits 2) (term melanie) (term mars*))

46

introduction	to	information	retrieval

graph	search
   graph	search	results	are	often	more	than	one	edge	

away	from	source	nodes
   example:	pages	liked	by	friends	of	melanie	who	like	emacs

   unicorn	provides	additional	operators	to	support	

graph	search
   apply

   extract

(apply likes: (and friend:7 likers:42))

   extract	and	return	(denormalized)	ids	stored	in	hitdata

47

introduction	to	information	retrieval

references
   j.	teevan,	s.	dumais,	e.	horvitz.	potential	for	personalization.	

2010

   j.	pitkow	et	al.	personalized	search.	2002
   j.	teevan,	s.	dumais,	e.	horvitz.	personalizing	search	via	

automated	analysis	of	interests	and	activities.	2005

   p.	bennett	et	al.	inferring	and	using	location	metadata	to	

personalize	web	search.	2011

   t.	haveliwala.	topic-sensitive	id95.	2002.
   g.	jeh	and	j.	widom.	scaling	personalized	web	search.	2003
   m.	curtiss	et	al.	unicorn:	a	system	for	searching	the	social	

graph.	2013

48

