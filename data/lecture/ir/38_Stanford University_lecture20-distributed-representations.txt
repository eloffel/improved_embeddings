introduction	to	information	retrieval

introduction	to
information	retrieval

cs276:	information	retrieval	and	web	search

christopher	manning	and	pandu	nayak

lecture	13:	distributed	word	representations	

for	information	retrieval

introduction	to	information	retrieval

sec. 9.2.2

how	can	we	more	robustly	match	a	
user   s	search	intent?
we	want	to	understand	the	query,	not	just	do	string	equals()
   if	user	searches	for	[dell	notebook	battery	size],	we	would	like	
to	match	documents	discussing	   dell	laptop	battery	capacity   

   if	user	searches	for	[seattle	motel],	we	would	like	to	match	

documents	containing	   seattle	hotel   

a	na  ve	information	retrieval	system	does	nothing	to	help
simple	facilities	that	we	have	already	discussed	do	a	bit	to	help
   spelling	correction
   id30	/	case	folding
but	we   d	like	to	better	understand when	query/document	match

introduction	to	information	retrieval

sec. 9.2.2

how	can	we	more	robustly	match	a	
user   s	search	intent?
   use	of	anchor	text	may	solve	this	by	providing	human	

authored	synonyms,	but	not	for	new	or	less	popular	web	
pages,	or	non-hyperlinked	collections

   relevance	feedback	could	allow	us	to	capture	this	if	we	get	

near	enough	to	matching	documents	with	these	words

   we	can	also	fix	this	with	information	on	word	similarities:

   a	manual	thesaurus of	synonyms
   a	measure	of	word	similarity

   calculated	from	a	big	document	collection
   calculated	by	query	log	mining	(common	on	the	web)

introduction	to	information	retrieval

sec. 9.2.2

example	of	manual	thesaurus	

introduction	to	information	retrieval

search	log	query	expansion
   context-free	query	expansion	ends	up	problematic

   [light	hair]	   	[fair	hair]										at	least	in	u.k./australia?	   	blonde

   so	expand	[light]	    [light	fair]

   but	[outdoor	light	price]	   	[outdoor	fair	price]

   you	can	learn	query	context-specific	rewritings	from	
search	logs	by	attempting	to	identify	the	same	user	
making	a	second	attempt	at	the	same	user	need
   [hinton	word	vector]
   [hinton	word	embedding]

   in	this	context,	[vector]	   	[embedding]

   but	not	when	talking	about	a	disease	vector	or	c++!

introduction	to	information	retrieval

sec. 9.2.3

automatic	thesaurus	generation
   attempt	to	generate	a	thesaurus	automatically	by	

analyzing	a	collection	of	documents

   fundamental	notion:	similarity	between	two	words
   definition	1:	two	words	are	similar	if	they	co-occur	with	

similar	words.

   definition	2:	two	words	are	similar	if	they	occur	in	a	

given	grammatical	relation	with	the	same	words.

   you	can	harvest,	peel,	eat,	prepare,	etc.	apples	and	

pears,	so	apples	and	pears	must	be	similar.

   co-occurrence	based	is	more	robust,	grammatical	

relations	are	more	accurate.

why?

introduction	to	information	retrieval

sec. 9.2.3

simple	co-occurrence	thesaurus

   simplest	way	to	compute	one	is	based	on	term-term	similarities	

in	c	=	aat	where	a is	term-document	matrix.

   wi,j =	(normalized)	weight	for	(ti ,dj)

dj

n

a

ti

m
   for	each	ti,	pick	terms	with	high	values	in	c

what does c
contain if a
is a term-doc 
incidence 
(0/1) matrix?

introduction	to	information	retrieval

automatic	thesaurus	generation	
example	    sort	of	works

word
absolutely
bottomed
captivating
doghouse
makeup
mediating
keeping
lithographs
pathogens
senses

nearest	neighbors
absurd,	whatsoever,	totally,	exactly,	nothing
dip,	copper,	drops,	topped,	slide,	trimmed
shimmer,	stunningly,	superbly, plucky,	witty
dog,	porch,	crawling,	beside,	downstairs
repellent,	lotion,	glossy,	sunscreen,	skin,	gel
reconciliation, negotiate,	cease,	conciliation
hoping,	bring,	wiping,	could, some,	would
drawings, picasso,	dali,	sculptures,	gauguin
toxins,	bacteria,	organisms, bacterial,	parasites
grasp,	psyche, truly,	clumsy,	na  ve,	innate	

but data is  too sparse in this form 100,000 words = 1010 entries in c. 

introduction	to	information	retrieval

sec. 9.2.2

how	can	we	represent	term	relations?
   with	the	standard	symbolic	encoding	of	terms,	each	term	is	a	

dimension
   different	terms	have	no	inherent	similarity
   motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]t
hotel  [0 0 0 0 0 0 0 3 0 0 0 0 0 0 0] = 0
   if	query	on	hotel and	document	has	motel,	then	our	query	

and	document	vectors	are	orthogonal

introduction	to	information	retrieval

can	you	directly	learn	term	relations?
   basic	ir	is	scoring	on	qtd
   no	treatment	of	synonyms;	no	machine	learning
   can	we	learn	parameters	w to	rank	via	qtwd ?

   problem	is	again	sparsity     w is	huge	>	1010

introduction	to	information	retrieval

is	there	a	better	way?
   idea:

word	in	   dsuch	that	dot	products utv express	word	

   can	we	learn	a	dense	low-dimensional	representation	of	a	

similarity?

   we	could	still	if	we	want	to	include	a	   translation   	matrix	

between	vocabularies	(e.g.,	cross-language):	utwv
   but	now	w is	small!

   supervised	semantic	indexing	(bai et	al.	journal	of	
information	retrieval	2009)	shows	successful	use	of	
learning	w	for	information	retrieval

   but	we   ll	develop	direct	similarity	in	this	class

introduction	to	information	retrieval

distributional	similarity	based	
representations
   you	can	get	a	lot	of	value	by	representing	a	word	by	

means	of	its	neighbors

      you	shall	know	a	word	by	the	company	it	keeps   

  

(j.	r.	firth	1957:	11)

   one	of	the	most	successful	ideas	of	modern	

statistical	nlp

government debt problems turning into banking crises as has happened in

saying that europe needs unified banking regulation to replace the hodgepodge

   these	words	will	represent	banking	  

12

introduction	to	information	retrieval

solution:	low	dimensional	vectors
   the	number	of	topics	that	people	talk	about	is	small	

(in	some	sense)

   clothes,	movies,	politics,	   

    idea:	store	   most   	of	the	important	information	in	a	

fixed,	small	number	of	dimensions:	a	dense	vector

    usually	25	    1000	dimensions

    how	to	reduce	the	dimensionality?

    go	from	big,	sparse	co-occurrence	count	vector	to	low	

dimensional	   word	embedding   	

13

introduction	to	information	retrieval

sec. 18.2

traditional	way:
latent	semantic	indexing/analysis
   use	singular	value	decomposition	(svd)	    kind	of	like	
principal	components	analysis	(pca)	for	an	arbitrary	
rectangular	matrix	    or	just	random	projection	to	find	a	low-
dimensional	basis	or	orthogonal	vectors

   theory	is	that	similarity	is	preserved	as	much	as	possible
   you	can	actually	gain	in	ir	(slightly)	by	doing	lsa,		as	   noise   	

of	term	variation	gets	replaced	by	semantic	   concepts   

   popular	in	the	1990s	[deerwester et	al.	1990,	etc.]

   results	were	always	somewhat	iffy	(   	it	worked	sometimes)
   hard	to	implement	efficiently	in	an	ir	system	(dense	vectors!)

   discussed	in	iir chapter	18,	but	not	discussed	further	here

   and	not	on	the	exam	(!!!)

introduction	to	information	retrieval

   neural	embeddings   

introduction	to	information	retrieval

word	meaning	is	defined	in	terms	of	
vectors
   we	will	build	a	dense	vector	for	each	word	type,	

chosen	so	that	it	is	good	at	predicting	other	words	
appearing	in	its	context
    those	other	words	also	being	represented	by	vectors	    it	all	gets	a	bit	recursive

linguistics =

0.286
0.792
   0.177
   0.107
0.109
   0.542
0.349
0.271

introduction	to	information	retrieval

neural	word	embeddings	- visualization

17

introduction	to	information	retrieval

basic	idea	of	learning	neural	network	word	
embeddings

we	define	a	model	that	aims	to	predict	between	a	center	
word	wt and	context	words	in	terms	of	word	vectors

p(context|wt)	=	   

which	has	a	loss	function,	e.g.,

j =	1	   	p(w   t	|wt)	

we	look	at	many	positions	t	in	a	big	language	corpus
we	keep	adjusting	the	vector	representations	of	words	to	
minimize	this	loss

introduction	to	information	retrieval

idea:	directly	learn	low-dimensional	word	
vectors	based	on	ability	to	predict
    old	idea.	relevant	for	this	lecture	&	deep	learning:
learning	representations	by	back-propagating	errors.	
(rumelhart et	al.,	1986)

   

    a	neural	probabilistic	language	model	(bengio et	al.,	

2003)		

    nlp	(almost)	from	scratch	(collobert &	weston,	2008)
    a	recent,	even	simpler	and	faster	model:	

   

id97	(mikolov et	al.	2013)	   intro	now
the	glove	model	from	stanford	(pennington,	socher,	and	
manning	2014)	connects	back	to	matrix	factorization

   

initial	models	were	quite	non-linear	and	slow;	recent	work	
has	used	fast,	bilinear	models

19

introduction	to	information	retrieval

id97	is	a	family	of	algorithms
[mikolov et	al.	2013]

predict	between	every	word	and	its	context	words!

two	algorithms

1. skip-grams	(sg)

predict	context	words	given	target	(position	independent)

2. continuous	bag	of	words	(cbow)

predict	target	word	from	bag-of-words	context

two	(moderately	efficient)	training	methods

1. hierarchical	softmax
2. negative	sampling
na  ve	softmax

introduction	to	information	retrieval

skip-gram	prediction

introduction	to	information	retrieval

details	of	id97
for	each	word	t =	1	    t,	predict	surrounding	words	in	a	
window	of	   radius   	m of	every	word.
objective	function:	maximize	the	id203	of	any	
context	word	given	the	current	center	word:

where	   represents	all	variables	we	will	optimize

training time. the basic skip-gram formulation de   nes p(wt+j|wt) using the softmax function:

introduction	to	information	retrieval

details	of	id97
predict	surrounding	words	in	a	window	of	radius	m of	
every	word
for																						the	simplest	first	formulation	is	
exp!v   
w=1 exp!v   
#w
where	o is	the	outside	(or	output)	word	index,	c is	the	
center	word	index,	vc and	uo are	   center   	and	   outside   	
vectors	of	indices	c and	o
softmax using	word	c to	obtain	id203	of	word	o

   vwi"
   vwi"

(2)

wo

w

w are the    input    and    output    vector representations of w, and w is the num-
ber of words in the vocabulary. this formulation is impractical because the cost of computing
    log p(wo|wi ) is proportional to w, which is often large (105   107 terms).

a computationally ef   cient approximation of the full softmax is the hierarchical softmax. in the
context of neural network language models, it was    rst introduced by morin and bengio [12]. the

introduction	to	information	retrieval

softmax	function:	standard	map
from	   v to	a	id203	distribution

exponentiate	to
make	positive

normalize	to
give	id203

introduction	to	information	retrieval

skip	gram	model	structure

introduction	to	information	retrieval

to	learn	good	word	vectors:
compute	all vector	gradients!
   we	often	define	the	set	of	all parameters	in	a	model	

in	terms	of	one	long	vector	

   in	our	case	with	

d-dimensional	vectors
and
v many	words:

   we	then	optimize
these	parameters

note: every word has two vectors! makes it simpler!

introduction	to	information	retrieval

intuition	of	how	to	minimize	loss	for	a	
simple	function	over	two	parameters

we	start	at	a	random	point	and	walk	in	the	steepest	
direction,	which	is	given	by	the	derivative	of	the	function

contour	lines	show	
points	of	equal	value	
of	objective	function

introduction	to	information	retrieval

descending	by	using	derivatives

we	will	minimize	a	cost	function	by
gradient	descent

trivial	example:	(from	wikipedia)
find	a	local	minimum	of	the	function	
f(x)	=	x4   3x3+2,	
with	derivative	f'(x)	=	4x3   9x2

subtracting	a	fraction	
of	the	gradient	moves	

you	towards	the	

minimum!

introduction	to	information	retrieval

vanilla	gradient	descent	code

introduction	to	information	retrieval

stochastic	gradient	descent
   but	corpus	may	have	40b	tokens	and	windows
   you	would	wait	a	very	long	time	before	making	a	single	

update!

   very bad	idea	for	pretty	much	all	neural	nets!
   instead:	we	will	update	parameters	after	each	window	t	

   stochastic	gradient	descent	(sgd)

introduction	to	information	retrieval

working	out	how	to	optimize	a	neural	
network	is	really	all	the	chain	rule!

chain	rule!	if	y =	f(u)	and	u =	g(x),	i.e.	y	=	f(g(x)),	then:

simple	example:	

introduction	to	information	retrieval

introduction	to	information	retrieval

introduction	to	information	retrieval

introduction	to	information	retrieval

introduction	to	information	retrieval

36

introduction	to	information	retrieval

linear	relationships	in	id97
these	representations	are	very	good	at	encoding	
similarity and	dimensions	of	similarity!
   analogies	testing	dimensions	of	similarity	can	be	

solved	quite	well	just	by	doing	vector	subtraction	in	
the	embedding	space
syntactically
   xapple    	xapples    	xcar    	xcars     xfamily    	xfamilies
   similarly	for	verb	and	adjective	morphological	forms
semantically	(semeval 2012	task	2)
   xshirt    	xclothing     xchair    	xfurniture
   xking    	xman    	xqueen    	xwoman

37

introduction	to	information	retrieval

word	analogies

test for linear relationships, examined by mikolov et al.

a:b :: c:?
a:b :: c:?

man:woman :: king:?

+

   

+

king

man

[ 0.30 0.70 ]

[ 0.20 0.20 ]

woman

[ 0.60 0.30 ]

queen

king

woman

queen

[ 0.70 0.80 ]

man

introduction	to	information	retrieval

glove visualizations

http://nlp.stanford.edu/projects/glove/

39

introduction	to	information	retrieval

glove	visualizations:	company	- ceo

40

introduction	to	information	retrieval

glove	visualizations:	superlatives

5/21/17

41

introduction	to	information	retrieval

application	to	information	retrieval
application	is	just	beginning	    there   s	little	to	go	on
   google   s	rankbrain     almost	nothing	is	publicly	known

   bloomberg	article	by	jack	clark	(oct	26,	2015):	

http://www.bloomberg.com/news/articles/2015-10-26/google-turning-its-
lucrative-web-search-over-to-ai-machines

   a	result	reranking	system
   even	though	more	of	the	value	is	in	the	tail?

   new	sigir	neu-ir	workshop	series	(2016	and	2017)

introduction	to	information	retrieval

final	thoughts
from	chris	manning	sigir	2016	keynote

2011														2013										2015											2017
speech							vision						nlp										ir

you	are
here

introduction	to	information	retrieval

an	application	to	information	retrieval
nalisnick,	mitra,	craswell &	caruana.	2016.	improving	document	
ranking	with	dual	word	embeddings.	www	2016	companion.	
http://research.microsoft.com/pubs/260867/pp1291-nalisnick.pdf
mitra,	nalisnick,	craswell &	caruana.	2016.	a	dual	embedding	
space	model	for	document	ranking.	arxiv:1602.01137 [cs.ir]

builds	on	bm25	model	idea	of	   aboutness   
   not	just	term	repetition	indicating	aboutness
   relationship	between	query	terms	and	all	terms	in	the	

document	indicates	aboutness	(bm25	uses	only	query	terms)

makes	clever	argument	for	different	use	of	word	and	context	
vectors	in	id97   s	cbow/sgns	or	glove

introduction	to	information	retrieval

modeling	document	aboutness:	
results	from	a	search	for	albuquerque

d1

d2

introduction	to	information	retrieval

using	2	word	embeddings

id97 model with 1 word of context

win
embeddings
for focus
words

wout
embeddings
for context
words

context
word

focus
word

we can gain by using these
two embeddings differently

introduction	to	information	retrieval

using	2	word	embeddings

introduction	to	information	retrieval

dual	embedding	space	model	(desm)
   simple	model
   a	document	is	represented	by	the	centroid	of	its	

word	vectors

   query-document	similarity	is	average	over	query	

words	of	cosine	similarity

introduction	to	information	retrieval

dual	embedding	space	model	(desm)
   what	works	best	is	to	use	the	out	vectors	for	the	

document	and	the	in	vectors	for	the	query

   this	way	similarity	measures	aboutness     words	that	
appear	with	this	word	    which	is	more	useful	in	this	
context	than	(distributional)	semantic	similarity

introduction	to	information	retrieval

experiments
   train	id97	from	either

   600	million	bing	queries
   342	million	web	document	sentences

   test	on	7,741	randomly	sampled	bing	queries

   5	level	eval (perfect,	excellent,	good,	fair,	bad)

   two	approaches

1. use	desm	model	to	rerank top	results	from	bm25
2. use	desm	alone	or	a	mixture	model	of	it	and	bm25

introduction	to	information	retrieval

results	    reranking	k-best	list	

pretty	decent	gains	    e.g.,	2%	for	ndcg@3
gains	are	bigger	for	model	trained	on	queries	than	docs

introduction	to	information	retrieval

results	    whole	ranking	system

introduction	to	information	retrieval

a	possible	explanation

in-out	has	some	ability	to	prefer	relevant	to	close-by	
(judged)	non-relevant,	but	it   s	scores	induce	too	much	
noise	vs.	bm25	to	be	usable	alone

introduction	to	information	retrieval

desm	conclusions
   desm	is	a	weak	ranker	but	effective	at	finding	subtler	

similarities/aboutness

   it	is	effective	at,	but	only	at,	ranking	at	least	

somewhat	relevant	documents

   for	example,	desm	can	confuse	oxford	and	cambridge
   bing	rarely	makes	the	oxford-cambridge	mistake

introduction	to	information	retrieval

global	vs.	local	embedding	[diaz	2016]

introduction	to	information	retrieval

global	vs.	local	embedding	[diaz	2016]

train w2v on documents from 
first round of retrieval

fine-grained word sense 
disambiguation

introduction	to	information	retrieval

ad-hoc	retrieval	using	local	and	
distributed	representation	[mitra et	al.	2017]
   argues	both	   lexical   	and	

   semantic   	matching	is	
important	for	document	
ranking

   duet	model	is	a	linear	

combination	of	two	dnns	
using	local	and	distributed	
representations	of	query/	
document	as	inputs,	and	
jointly	trained	on	labelled	data

introduction	to	information	retrieval

summary:	embed	all	the	things!
word	embeddings	are	the	hot	new	technology	(again!)

lots	of	applications	wherever	knowing	word	context	or	
similarity	helps	prediction:
   synonym	handling	in	search
   document	aboutness
   ad	serving
   language	models:	from	spelling	correction	to	email	response
   machine	translation
   sentiment	analysis
      

introduction	to	information	retrieval

introduction	to	information	retrieval

sec. 9.2.2

thesaurus-based	query	expansion
   for	each	term	t in	a	query,	expand	the	query	with	synonyms	and	

related	words	of	t from	the	thesaurus
   feline	   	feline	cat

   may	weight	added	terms	less	than	original	query	terms.
   generally	increases	recall
   widely	used	in	many	science/engineering	fields
   may	significantly	decrease	precision,	particularly	with	ambiguous	

terms.
      interest	rate   	      interest	rate	fascinate	evaluate   

   there	is	a	high	cost	of	manually	producing	a	thesaurus

   and	for	updating	it	for	scientific	changes

introduction	to	information	retrieval

sec. 9.2.3

automatic	thesaurus	generation	issues

n quality of associations is usually a problem
n sparsity

100,000

100,000

c

1010 entries

n term ambiguity may introduce irrelevant statistically 

correlated terms.
n    planet earth facts          planet earth soil ground facts   

n since terms are highly correlated anyway, expansion 

may not retrieve many additional documents.

introduction	to	information	retrieval

coals	model	(count-modified	lsa)
[rohde,	gonnerman &	plaut,	ms.,	2005]

introduction	to	information	retrieval

count	based	vs.	direct	prediction

lsa, hal (lund & burgess), 
coals (rohde et al), 
hellinger-pca (lebret & collobert)

    fast training
    efficient usage of statistics

    primarily used to capture word 
similarity
    disproportionate importance 
given to small counts

    nnlm, hlbl, id56, id97 
skip-gram/cbow, (bengio et al; 
collobert & weston; huang et al; mnih & 
hinton; mikolov et al; mnih & kavukcuoglu)

    scales with corpus size
    inefficient usage of statistics
    generate improved performance 
on other tasks

    can capture complex patterns 
beyond word similarity 

63

introduction	to	information	retrieval

encoding	meaning	in	vector	differences
[pennington,	socher,	and	manning,	emnlp	2014]

crucial	insight:	

ratios	of	co-occurrence	probabilities	can	encode	
meaning	components

x =	solid

x =	gas

x =	water			

x =	random			

large

small

small

large

large

large

large

small

~1

small

small

~1

introduction	to	information	retrieval

encoding	meaning	in	vector	differences
[pennington,	socher,	and	manning,	emnlp	2014]

crucial	insight:	

ratios	of	co-occurrence	probabilities	can	encode	meaning	
components

x =	solid

x =	gas

x =	water			

x =	fashion

1.9	x	10-4

6.6	x	10-5

3.0	x	10-3

1.7	x	10-5

2.2	x	10-5

7.8	x	10-4

2.2	x	10-3

1.8	x	10-5

8.9

8.5	x	10-2

1.36

0.96

introduction	to	information	retrieval

glove:	a	new	model	for	learning	word	representations
[pennington,	socher,	and	manning,	emnlp	2014]

introduction	to	information	retrieval

word	similarities

nearest	words	to frog:

1.	frogs
2.	toad
3.	litoria
4.	leptodactylidae
5.	rana
6.	lizard
7.	eleutherodactylus

litoria

leptodactylidae

http://nlp.stanford.edu/projects/glove/

rana

eleutherodactylus

introduction	to	information	retrieval

word	analogy	task			[mikolov,	yih &	zweig	2013a]

model

dimensions corpus	size

performance
(syn +	sem)

cbow (mikolov et	al.	2013b)

glove (this	work)

cbow	(m	et	al.	2013b,	by	us)

glove (this	work)

cbow (mikolov et	al.	2013b)

glove (this	work)

300

300

300

300

1.6	billion

1.6	billion

6	billion

6	billion

1000

6	billion

300

42	billion

36.1

70.3

65.7

71.7

63.7

75.0

