nlp
introduction to nlp
information retrieval
introduction
people search the web daily
search engines
google
bing
baidu
yandex
information retrieval is about search engines
yahoo search
amazon search
library of congress search
examples of search engines
conventional (library catalog)
search by keyword, title, author, etc.
text-based (lexis-nexis, google, yahoo!)
search by keywords. limited search using queries in natural language.
image-based
shapes, colors, keywords
id53 systems (ask.com)
search in (restricted) natural language
id91 systems (viv  simo, clusty)
research systems (lemur, nutch)
sample queries
how to get rid of stretch marks
dodge
kourtney kardashian
how many calories are in pumpkn pie
angelina jolie and brad pitt
how to vote
derek jeter
interstellar trailer
what is ebola?

https://www.google.com/trends/topcharts
the size of the world wide web
the size of the indexed world wide web pages (by nov 2015)
indexed by google: about 48 billion pages
indexed by bing: about 25 billion pages
http://www.worldwidewebsize.com/ 
web statistics
twitter hits 400 million tweets per day
june, 2012. dick costolo, ceo at twitter
over 2.5 billion photos uploaded to facebook each month (2010)
blog.facebook.com
google   s clusters process a total of more than 20 petabytes of data per day. 
2008. jeffrey dean from google

challenges
dynamically generated content
new pages get added all the time
the size of the blogosphere doubles every 6 months
characteristics of user queries
sessions
users revisit their queries
very short queries
typically 2 words long
a large number of typos
a small number of popular queries
a long tail of infrequent ones
almost no use of advanced query operators
with the exception of double quotes
information retrieval
baseline process
given a collection of documents
and a user   s query
find the most relevant documents
key terms used in ir
query
a representation of what the user is looking for - can be a list of words or a phrase.
document
an information entity that the user wants to retrieve
collection
a set of documents
index
a representation of information that makes querying easier
term
word or concept that appears in a document or a query
documents
not just printed paper
can be records, pages, sites, images, people, movies
document encoding (unicode)
id194
document preprocessing (e.g., removing metadata)
words, terms, types, tokens
nlp
information retrieval
models of information retrieval
sample queries (from excite)
in what year did baseball become an offical sport?
play station codes . com
birth control and depression
government
"workability i"+conference
kitchen appliances
where can i find a chines rosewood
tiger electronics
58 plymouth fury
how does the character seyavash in ferdowsi's shahnameh exhibit characteristics of a hero?
emeril lagasse
hubble
m.s subalaksmi
running
key terms used in ir
query: a representation of what the user is looking for - can be a list of words or a phrase.
document: an information entity that the user wants to retrieve
collection: a set of documents
index: a representation of information that makes querying easier
term: word or concept that appears in a document or a query
mappings and abstractions




reality
data
information need
query




from robert korfhage   s book
search engine architecture
decide what to index
collect it
index it (efficiently)
keep the index up to date
provide user-friendly query facilities
search engine architecture
documents
not just printed paper
can be records, pages, sites, images, people, movies
document encoding (unicode)
id194
document preprocessing (e.g., removing metadata)
words, terms, types, tokens
sample query sessions (from aol)
toley spies grames
tolley spies games
totally spies games
tajmahal restaurant brooklyn ny
taj mahal restaurant brooklyn ny
taj mahal restaurant brooklyn ny 11209
do you love me like you say
do you love me like you say lyrics
do you love me like you say lyrics marvin gaye
characteristics of user queries
sessions
users revisit their queries
very short queries
typically 2 words long
a large number of typos
a small number of popular queries
a long tail of infrequent ones
almost no use of advanced query operators
with the exception of double quotes
queries as documents
advantages:
mathematically easier to manage
problems:
different lengths
syntactic differences
repetitions of words (or lack thereof)

id194s
term-document matrix (m x n)
document-document matrix (n x n)
typical example in a medium-sized collection: 3,000,000 documents (n) with 50,000 terms (m)
typical example on the web: n=30,000,000,000, m=1,000,000
boolean vs. integer-valued matrices
storage issues
imagine a medium-sized collection with n=3,000,000 and m=50,000
how large a term-document matrix will be needed?
is there any way to do better? any heuristic?

tokenizing text
(id98) -- a tropical storm has strengthened into hurricane leslie in the atlantic ocean, forecasters said wednesday.
the slow-moving storm could affect bermuda this weekend, according to the national hurricane center in miami.
the category 1 hurricane was churning wednesday afternoon about 465 miles (750 kilometers) south-southeast of the british territory and moving north at 2 mph (4 kph), the hurricane center said.

http://www.id98.com/2012/09/05/world/americas/bermuda-hurricane-leslie/index.html
 
inverted index
instead of an incidence vector, use a posting table
cleveland:  d1, d2, d6
ohio: d1, d5, d6, d7
use linked lists to be able to insert new document postings in order and to remove existing postings.
can be used to compute document frequency
keep everything sorted! this gives you a logarithmic improvement in access.
basic operations on inverted indexes
conjunction (and)
iterative merge of the two postings: o(x+y)
disjunction (or)
very similar
negation (not)
can we still do it in o(x+y)? 
example: vermont and not massachusetts
example: massachusetts or not vermont
recursive operations
optimization
start with the smallest sets
major ir models
boolean
vector
probabilistic
id38
fuzzy retrieval
id45

information retrieval
the boolean model
the boolean model



x
w
y
z
d1
d2


venn diagrams
boolean queries
operators: and, or, not, parentheses
example:
cleveland and not ohio
(michigan and indiana) or (texas and oklahoma)
ambiguous uses of and and or in human language
exclusive vs. inclusive or
restrictive operator: and or or?
canonical forms of queries
de morgan   s laws: 
not (a and b) = (not a) or (not b)
not (a or b) = (not a) and (not b)
normal forms
conjunctive normal form (cnf)
disjunctive normal form (dnf) 
some people swear by cnf - why?
evaluating boolean queries
incidence vectors:
cleveland: 1100010
ohio: 1000111
examples:
cleveland and ohio
cleveland and not ohio
clevaland or ohio
information retrieval
indexing and ranking (1)
revisit: what is text retrieval (tr)?
there exists a collection of text documents
user gives a query to express the information need
a retrieval system returns relevant documents to users
more often called    information retrieval    (ir) , but ir is actually much broader
may include non-textual information
may include text categorization or summarization   
known as    search technology    in industry 
typical ir system architecture

user


query




judgments
documents
results

query rep

doc rep
ranking
feedback






indexing
searching
query modification
interface




text retrieval vs. database retrieval
database retrieval     library search system
text retrieval     web search engine
information
unstructured/free text vs. structured data
ambiguous vs. well-defined semantics
query 
ambiguous vs. well-defined semantics
incomplete vs. complete specification
answers
relevant documents vs. matched records
tr is an empirically defined problem!

text retrieval is hard!
under/over-specified query
ambiguous:    buying cds    (money or music?) 
incomplete: what kind of cds?
what if    cd    is never mentioned in document? 
vague semantics of documents
ambiguity: e.g., word-sense, structural
incomplete: id136s required
hard even for people! 
80% agreement in human judgments
text retrieval is    easy   !
text retrieval can be easy in a particular case
ambiguity in query/document is relative to the database
so, if the query is specific enough, just one keyword may get all the relevant documents
perceived text retrieval performance is usually better than the actual performance
users can not judge the completeness of an answer
history of tr on one slide
birth of tr
1945: v.  bush   s article    as we may think   
1957:  h. p. luhn   s idea of word counting and matching
indexing & evaluation methodology (1960   s)
smart system (g. salton   s group)
cranfield test collection (c. cleverdon   s group)
indexing: automatic can be as good as manual 
tr models (1970   s & 1980   s)    
large-scale evaluation & applications (1990   s-present)
trec (d. harman & e. voorhees, nist)
web search (google, yahoo!, bing)
pubmed (biomedical literature search)

short vs. long term info need
short-term information need (ad hoc retrieval)
   temporary need   , e.g., info about used cars
information source is relatively static 
user    pulls    information
application example: library search, web search
long-term information need (filtering)
   stable need   , e.g., new data mining algorithms
information source is dynamic
system    pushes    information to user
applications:  news filter
importance of ad hoc retrieval
directly manages any existing large collection of information
there are many many    ad hoc    information needs
a long-term information need can be satisfied through frequent ad hoc retrieval
basic techniques of ad hoc retrieval can be used for filtering and other    non-retrieval    tasks, such as id54. 
formal formulation of tr
vocabulary v={w1, w2,    , wn} of language
query q = q1,   ,qm, where qi     v
document di = di1,   ,dil, where dij     v
collection c= {d1,    , dk}
set of relevant documents r(q)     c
generally unknown and user-dependent
query is a    hint    on which doc is in r(q)
task =  compute r   (q), an    approximate r(q)   
because we never know true r(q)   
formal formulation of tr

+
+
+
+
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+
-
-
r   (q)


true r(q)


+
+
+
+
-
-
+

c

di


computing r(q)
strategy 1: document selection
r(q)={d   c|f(d,q)=1}, where f(d,q)    {0,1} is an indicator function or classifier
system must decide if a doc is relevant or not (   absolute relevance   )
strategy 2: document ranking
r(q) = {d   c|f(d,q)>   }, where f(d,q)        is a relevance measure function;     is a cutoff
system must decide if one doc is more likely to be relevant than another (   relative relevance   )

document selection vs. ranking

+
+
+
+
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+
-
-
doc selection
f(d,q)=?



-
-
-
doc ranking
f(d,q)=?

1
0
0.98 d1 +
0.95 d2 +
0.83 d3 -
0.80 d4 +
0.76 d5 -
0.56 d6 -
0.34 d7 -
0.21 d8 +
0.21 d9 -


r   (q)


true r(q)

   r   (q)
problems of doc selection
the classifier is unlikely accurate
   over-constrained    query (terms are too specific): no relevant documents found
   under-constrained    query (terms are too general): over delivery
it is extremely hard to find the right position between these two extremes
even if it is accurate, all relevant documents are not equally relevant
relevance is a matter of degree!
ranking is often preferred
relevance is a matter of degree
a user can stop browsing anywhere, so the boundary is controlled by the user
high recall users would view more items
high precision users would view only a few
theoretical justification: id203 ranking principle [robertson 77]
id203 ranking principle
[robertson 77]
as stated by cooper




robertson provides two formal justifications
assumptions: independent relevance and sequential browsing (not necessarily all hold in reality)
   if a reference retrieval system   s response to each request is a ranking of the documents in the collections in order of decreasing id203 of usefulness to the user who submitted the request, where the probabilities are estimated as accurately a possible on the basis of whatever data made available to the system for this purpose, then the overall effectiveness of the system to its users  will be the best that is obtainable on the basis of that data.   
according to the prp, all we need is  

   a relevance measure function f   

which satisfies

for all q, d1, d2,  
f(q,d1) > f(q,d2) iff  p(relevance|q,d1) >p(relevance|q,d2)

54
we will talk about many different ranking methods later   
55
next topic
what is text retrieval (tr) ?
document selection vs. document ranking
how do we compare tr systems?
indexing
evaluation criteria
effectiveness/accuracy
precision, recall

efficiency
space and time complexity 

usability
how useful for real user tasks?
methodology: cranfield tradition
laboratory testing of system components
precision, recall
comparative testing
test collections
set of documents
set of questions
relevance judgments
evaluation: document selection vs. ranking

+
+
+
+
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+
-
-
doc selection
f(d,q)=?



-
-
-
1
0
r   (q)


true r(q)

this is easy: just count the + and     in r   (q) and    r   (q)
   r   (q)


the contingency table
relevant retrieved
irrelevant retrieved
irrelevant rejected
relevant rejected

relevant
not relevant



retrieved


not retrieved

doc
action
r   (q)
   r   (q)
+
-
evaluation: document selection vs. ranking

+
+
+
+
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+
-
-
doc ranking
f(d,q)=?

0.98 d1 +
0.95 d2 +
0.83 d3 -
0.80 d4 +
0.76 d5 -
0.56 d6 -
0.34 d7 -
0.21 d8 +
0.21 d9 -



true r(q)

ranking is much harder to evaluate   
how to measure ranking quality?
compute the precision at every recall point (the position where each relevant document is retrieved)
d1 +
d2 +
d3 -
d4 +
d5 -
d6 -
d7 -
d8 +
d9 -

recall = 0.25, precision = 1
recall = 0.5, precision = 1
not a recall point, no calculation
recall = 0.75, precision = 0.75 
not a recall point, no calculation
recall = 1, precision = 0.5
                  when computing map, precision = 0
      
      
this relevant document is never retrieved 
(not in r   (q)).. 
how to measure the goodness of a ranking?
compute the precision at every recall point
plot a precision-recall (pr) curve

precision
recall
x
x
x
x


precision
recall
x
x
x
x
which is better?
summarize a ranking: map
given that n docs are retrieved
compute the precision (at rank) where each (new) relevant document is retrieved => p(1),   ,p(k), if we have k rel. docs
e.g., if the first rel. doc is at the 2nd rank, then p(1)=1/2. 
if a relevant document never gets retrieved, we assume the precision corresponding to that rel. doc to be zero 
compute the average over all the relevant documents
average precision = (p(1)+   p(k))/k
this gives us an average precision, which captures both precision and recall and is sensitive to the rank of each relevant document
mean average precision (map)
 map = arithmetic mean average precision over a set of queries
gmap = geometric mean average precision over a set of queries (more affected by difficult queries)
computing map
d1 +
d2 +
d3 -
d4 +
d5 -
d6 -
d7 -
d8 +
d9 -

1st rel. doc, precision = 1
2nd rel. doc, precision = 1
3rd rel. doc, precision = 0.75 
4th rel. doc, precision = 0.5
                   precision = 0
      
      
this relevant document is never retrieved 
(not in r   (q)).. 
map = (1 + 1 + 0.75 + 0) / 4 = 0.6875 
there are 4 relevant doc in total
summarize a ranking: dcg
what if relevance judgments are on a scale of [1,r]? r>2
cumulative gain (cg) at rank n
let the ratings of the n documents be r1, r2,    rn (in ranked order)
cg = r1+r2+   rn
discounted cumulative gain (dcg) at rank n
dcg = r1 + r2/log22 + r3/log23 +     rn/log2n
we may use any base for the logarithm, e.g., base=b 
for rank positions above b, do not discount
summarize a ranking: ndcg
normalized discounted cumulative gain (ndcg) at rank n
normalize dcg at rank n by the dcg value at rank n of the ideal ranking
the ideal ranking would first return the documents with the highest relevance level, then the next highest relevance level, etc.
compute the precision (at rank) where each  (new) relevant document is retrieved => p(1),   ,p(k), if we have k rel. docs
ndcg is now quite popular in evaluating web search

other measures
precision at k documents (e.g., prec@10doc):
more meaningful than map (why?)
also called breakeven precision when k is the same as the number of relevant documents
mean reciprocal rank (mrr): 
same as map  when there   s only 1 relevant document
reciprocal rank = 1/rank-of-the-relevant-doc
f-measure (f1): harmonic mean of precision and recall




p: precision
r: recall
   : parameter
69
precision-recall curve
mean avg. precision (map)

recall=3212/4728

breakeven precision
 (precision when precision=recall)


out of 4728 rel docs,
 we   ve got 3212
d1 +
d2 +
d3    
d4    
d5 +
d6 -
total # rel docs = 4
system returns 6 docs
average prec = (1/1+2/2+3/5+0)/4
about 5.5 docs
in the top 10 docs
are relevant
precision@10docs


typical trec evaluation result
what query averaging hides
slide from doug oard   s presentation, originally from ellen voorhees    presentation
70
next topic
what is text retrieval (tr) ?
document selection vs. document ranking
how do we compare tr systems?
indexing
71
information retrieval
indexing and ranking (2)
typical ir system architecture

user


query




judgments
documents
results

query rep

doc rep
ranking
feedback






indexing
searching
query modification
interface




let   s start from here   



query




documents
results

query rep

doc rep
ranking


indexing
searching




vector representation is powerful!
revisit: text representation/indexing
making it easier to match a query with a document
query and document should be represented using the same units/terms
controlled vocabulary vs. full text indexing
full-text indexing is more practically useful and has proven to be as effective as manual indexing with controlled vocabulary
handling large collections
life is good when every document is mapped into a vector of words, but    
consider n = 1 million documents, each with about 1000 words.
avg. 6 bytes/word including spaces/punctuation 
6gb of data in the documents.
say there are m = 500k distinct terms among these.
storage issue
500k x 1m matrix has half-a-trillion elements.
4 bytes for an integer
500k x 1m x 4 = 2t (your laptop would fail)
500k x 100b x 4 = 2*105 t (challenging even for google)
but it has no more than one billion positive numbers.
matrix is extremely sparse.
1000 x 1m x 4 = 4g
what   s a better representation?

another motivation   
let   s look at the simplest case (boolean retrieval)
query =    school    +    information   
the only thing we need to do is to return all the documents containing both the term    school    and the term    information   
simple strategy:
scan each document and output anyone who contains both words.
can we afford to scan all the documents for each query?
any faster way to do it?

indexing
indexing = convert documents to data structures that enable fast search 
inverted index is the dominating indexing method (used by all search engines)
other indices (e.g., document index) may be needed for feedback

instead of an incidence vector, use a posting table
cleveland:  d1, d2, d6
ohio: d1, d5, d6, d7
use linked lists to be able to insert new document postings in order and to remove existing postings.
more efficient than scanning docs (why?)
inverted iindex
inverted index
fast access to all docs containing a given term (along with frequency and position information) 
for each term, we get a list of tuples 
(docid, freq, pos).
given a query, we can fetch the lists for all query terms and work on the involved documents.
boolean query:  set operation
natural language query: term weight summing 
keep everything sorted! this gives you a logarithmic improvement in access.
inverted index - example
for each term t, we must store a list of all documents that contain t.
identify each by a docid, a document serial number
dictionary
postings
posting

brutus
calpurnia
caesar


2

31
54
101
- from chris manning   s slides


inverted index - example
this is a sample 
document
with one sample
sentence
doc 1
this is another 
sample document
doc 2




dictionary
postings




- from chengxiang zhai   s slides
conjunction (and)     iterative merge of the two postings: o(x+y)
disjunction (or)     very similar
negation (not)     can we still do it in o(x+y)? 
example: michigan and not ohio
example: michigan or not ohio
recursive operations
optimization: start with the smallest sets
basic operations on inverted indexes

data structures for inverted index
dictionary: modest size
needs fast random access
preferred to be in memory
hash table, b-tree, trie,    
postings: huge
sequential access is expected 
can stay on disk
may contain docid, term freq., term pos, etc
compression is desirable
constructing inverted index
the main difficulty is to build a huge index with limited memory
memory-based methods: not usable for large collections 
sort-based methods: 
step 1: collect local (termid, docid, freq) tuples
step 2: sort local tuples (to make    runs   )
step 3: pair-wise merge runs
step 4: output inverted file
sort-based inversion



...
term lexicon:

the 1
cold 2
days 3
a 4
...
docid
lexicon:

doc1 1
doc2 2
doc3 3
...
doc1
doc1
doc300
87
searching
given a query, how to score documents efficiently?
boolean query
fetch the inverted list for all query terms
perform set operations to get the subset of docs that satisfy the boolean condition
e.g., q1=   info    and    security    , q2=   info    or    security   
info: d1, d2, d3, d4
security: d2, d4, d6
results: {d2,d4} (q1) {d1,d2,d3,d4,d6} (q2)
ranking documents
assumption:score(d,q)=f[g(w(d,q,t1),   w(d,q,tn)), w(d),w(q)], where, ti   s are the matched terms
maintain a score accumulator for each doc to compute function g
for each query term ti
fetch the inverted list {(d1,f1),   ,(dn,fn)}
for each entry (dj,fj), compute  w(dj,q,ti), and update score accumulator for doc di
adjust the score to compute f, and sort 
ranking documents: example
s(d,q)=g(t1)+   +g(tn) [sum of freq of matched terms]
query =    info security   
    s(d, q) = g(   info   ) + g(   security   )

info: (d1, 3), (d2, 4), (d3, 1), (d4, 5)
security: (d2, 3), (d4,1), (d5, 3)
accumulators:   d1      d2      d3     d4      d5
                           0         0        0       0        0
        (d1,3)  =>   3         0        0       0        0
        (d2,4)  =>   3         4        0       0        0
        (d3,1)  =>   3         4        1       0        0
        (d4,5)  =>   3         4        1       5        0
        (d2,3)  =>   3         7        1       5        0
        (d4,1)  =>   3         7        1       6        0
        (d5,3)  =>   3         7        1        6        3



info
security
further improving efficiency
keep only the most promising accumulators
sort the inverted list in decreasing order of weights and fetch only n entries with the highest weights
pre-compute as much as possible 
scaling up to the web-scale (more about this later)
inverted index compression
compress the postings
observations
inverted list is sorted (e.g., by docid or termfq)
small numbers tend to occur more frequently
implications
   d-gap    (store difference): d1, d2-d1, d3-d2-d1,   
exploit skewed frequency distribution: fewer bits for small (high frequency) integers
 binary code, unary code,    -code,    -code 
integer compression
in general, to exploit skewed distribution
binary: equal-length coding
unary: x   1 is coded as x-1 one bits followed by 0, e.g., 3=> 110; 5=>11110
   -code: x=> unary code for 1+   log x    followed by  uniform code for x-2    log x    in    log x     bits, e.g., 3=>101, 5=>11001
   -code: same as    -code ,but replace the unary prefix with    -code. e.g., 3=>1001, 5=>10101
what you should know
how tr  is different from db retrieval
why ranking is generally preferred to document selection (justified by prp)
how to compute the major evaluation measure (precision, recall, map, ndcg, f1)
what is an inverted index
why does an inverted index help make search fast
how to construct a large inverted index
how to compress an index (optional)
phrase-based queries
examples
   new york city   
   ann arbor   
   barack obama   
we don   t want to match
york is a city in new hampshire
positional indexing
keep track of all words and their positions in the documents
to find a multi-word phrase, look for the matching words appearing next to each other
document ranking
compute the similarity between the query and each of the documents
use cosine similarity
use tf*idf weighting
return the top k matches to the user
typical ir system architecture

user


query




judgments
documents
results

query rep

doc rep
ranking
feedback






indexing
searching
query modification
interface




the basic question of ranking
given a query, how do we know if document a is more relevant than b?
document ranking

+
+
+
+
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+
-
-

0.98 d1 +
0.95 d2 +
0.83 d3 -
0.80 d4 +
0.76 d5 -
0.56 d6 -
0.34 d7 -
0.21 d8 +
0.21 d9 -



true r(q)

retrieval function
f(d,q)=?
measures the degree of relevance

relevance = similarity
assumptions
query and document are represented in a similar way
a query can be regarded as a    document   
relevance(d, q)     similarity(d, q)
r   (q) = {d   c | f(d,q)>   }, f(q,d)=   (rep(q), rep(d)) 
key issues
how to represent a query/document?
how to define the similarity measure    ?
revisit: vector space model
represent a doc/query by a term vector
term: basic concept, e.g., word or phrase
each term defines one dimension
n terms define a high-dimensional space
element of vector corresponds to term weight
e.g., d=(x1,   ,xn), xi is    importance    of term i
measure relevance by the similarity/distance (e.g., the cosine similarity) between the query vector and document vector in the vector space
vs model: illustration

what the vs model doesn   t say
how to define/select the    basic concept   
concepts are assumed to be orthogonal
we talked about how to select index terms
how to assign weights
weight in query indicates importance of term
weight in doc indicates how well the term characterizes the document
we talked about simple presence/absence
how to define the similarity/distance measure
what   s a good    basic concept   ?
orthogonal
linearly independent basis vectors
   non-overlapping    in meaning
no ambiguity
weights can be assigned automatically and hopefully accurately
many possibilities: words, stemmed words, phrases,    latent concept   ,      
why is being    orthogonal    important?
query = {laptop computer pc sale}
document 1 = {computer sale}
document 2 = {laptop computer pc}
ambiguity is the killer   
query = {jaguar band}
document 1 = {jaguar car}
document 2 = {jaguar cat}

how to assign weights?
very, very important!
why weighting
query side: not all terms are equally important
doc side: some terms carry more information about contents
how? 
two basic heuristics
tf (term frequency) = within-doc-frequency
idf (inverse document frequency)
tf id172
weighting is very important    
query = {text information management}
document 1 = {text information}
document 2 = {information management}
document 3 = {text management}


what   s a reasonable term weight?
how do we weight the multiple occurrences of a term in a document?


how do we weight different terms differently?
what   s a reasonable term weight?
how do we weight the multiple occurrences of a term in a document?
by term frequency (tf)

how do we weight different terms differently?
term frequency (tf) weighting
tf id172
tf id172 can be even more complicated:
tf in okapi/bm25:
	
where doclen is the length of d and avg_dl is the average length of documents in the collection

question: why do we need to normalize tf?
tf id172
why? 
document length variation
   repeated occurrences    are less informative than the    first occurrence   
two views of document length
a doc is long because it uses more words
a doc is long because it has more contents
generally we want to penalize long documents, but avoid over-penalizing (pivoted id172)
tf id172 (cont.)


normalized
 tf
raw tf

   pivoted id172   : using avg. doc length to regularize id172

 1-b+b*doclen / avg_doclen

b varies from 0 to 1
id172 interacts with the similarity measure
what   s a reasonable term weight?
how do we weight the multiple occurrences of a term in a document?
by term frequency (tf)

how do we weight different terms differently?
by inverted document frequency (idf)
inverted document frequency (idf) weighting
idea: a term is more discriminative if it occurs only in fewer documents
why this is true?
formula:
	
	n     total number of documents in collection
	k     number of documents with term t 
	      (document frequency)
note that idf is document independent while tf is document dependent!
a formulation of idf
n: number of documents
dk: number of documents containing term k
fik: absolute frequency of term k in document i
wik: weight of term k in document i
 idfk =  log2(n/dk) + 1 = log2n - log2dk + 1
tf-idf weighting
tf-idf weighting: 
weight(t, d)=tf(t, d)*idf(t)
term is common in doc     high tf     high weight
term is rare in collection    high idf    high weight
imagine a word count profile, what kind of terms would have high weights? 
how is this related to luhn   s study of term frequency and zipf   s law?

how to measure similarity?
vs example: raw tf & dot product 
query=   information retrieval   

how to implement this efficiently?
what works the best?
(singhal 2001)
use single words 
use stat. phrases
remove stop words
id30
others(?)



[          ]
pivoted id172     the typical vsm
123
why do we only care about terms in both q and d?

this part is idf

this part is (normalized) tf

this part is tf in query, or qtf

s is a parameter between 0 to 1,  that has to be set empirically. 
note the tf and idf part     how are they similar with/different from what we discussed earlier? 

# docs that contain t

number of words in d

average doc length
okapi/bm25
124
a variant form of idf
variant form of (normalized) tf
normalized qtf



k1, k3, and b are all parameters that have to be set empirically. 
the best performer in trec, the core feature of bing
   extensions    of vector space model
alternative similarity measures 
many other choices (tend not to be very effective)
p-norm (extended boolean): matching a boolean query with a tf-idf document vector
alternative representation
many choices (performance varies a lot)
id45 (lsi) [trec performance tends to be average]
generalized vector space model
theoretically interesting, not seriously evaluated

advantages of vs model
empirically effective! (top trec performance)
intuitive
easy to implement
well-studied/most evaluated
the smart system
developed at cornell: 1960-1999
still widely used 
warning: many variants of tf-idf!

disadvantages of vs model
assume term independence
which is never true!
assume query and document to be the same
lack of    predictive adequacy    
arbitrary term weighting
arbitrary similarity measure
very sensitive to the selection of weighting. 
lots of parameter tuning!
no way to predict performance
128
   k1, b and k3 are parameters which depend on the nature 
of the queries and possibly on the database; 
k1 and b default to 1.2 and 0.75 respectively,  
but smaller values of b are sometimes advantageous; 
in long queries k3 is often set to 7 or 1000.      
sophisticated parameter tuning
[robertson et al. 1999]
129
 high parameter sensitivity

130
how to find well performing retrieval functions?
simple approach: try all the variations, and evaluate their performance
time consuming; can we really try    all    variations?

if only we can find some formal guidance on how to find good retrieval functions!
axiomatic approach helps to do that
a good retrieval function must satisfy a series of axioms;
and if it doesn   t    
basic idea of axiomatic approach
132
define a set of retrieval constraints that any reasonable retrieval function should satisfy
an axiomatic framework for 
retrieval functions
component 1: constraints to be satisfied by an effective retrieval function (fang et al. 2004)

component 2: function space that allows us to efficiently search for an effective function
we won   t cover in this lecture (fang and zhai. 2005)
133
what should a good retrieval function do?
a document that matches more query words?
a document that matches more discriminative query words?
a document that is longer?
a document that covers more query words vs. a document that covers more unique query words?
   
another way to think about it
what do the good performers have in common?
135


term frequency


inverted document frequency


document length id172

1+ln(c(w,d))

alternative tf transformation
term frequency constraints (tfc1)
tfc1
tf weighting heuristic i:    				give a higher score to a document with more 	occurrences of a query term.
if	  and           ,      
then
let q be a query and d be a document. 
136
term frequency constraints (tfc2)
tfc2
tf weighting heuristic ii:   
	require that the amount of increase in the score due to   	adding a query term must decrease as we add more terms.
then
let q be a query with only one query term q.
let d1 be a document. 
137
length id172 constraints (lncs)
document length id172 heuristic:		penalize long documents(lnc1);                          	avoid over-penalizing long documents (lnc2) .
let q be a query and d be a document.
if t is a non-query term,  
then
lnc1	
138
tf-length constraint (tf-lnc)
tf-lnc
tf-ln heuristic:						regularize the interaction of tf and document length.
let q be a query and d be a document. 
then
if q is a query term, 
139
is constraint analysis related to the performance of a retrieval function? 
140
violation of constraints     poor performance
okapi method
141
conditional satisfaction of constraints     parameter bounds
pivoted id172 method	
lnc2     s<0.4
142
constraints analysis     guidance for improving a retrieval function

make okapi satisfy more constraints; expected to help verbose queries
modified okapi method
143
implementation of  component 2:

question: 
how can we define a function space that can be searched efficiently?
(we won   t cover in the lecture)  
144
what you should know
vector space model is a typical (and well-performing) way to define retrieval functions
tf-idf weighting
advantage and disadvantage of vsm
typical open source ir toolkits (another set of slides)
