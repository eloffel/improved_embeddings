introduction	to	information	retrieval

introduction	to
information	retrieval

bm25,	bm25f,	and	user	behavior
chris	manning	and	pandu	nayak

introduction	to	information	retrieval

introduction	to	information	retrieval

summary	    bim					[robertson	&	sp  rck-jones	1976]
   boils	down	to	
bim;
ci
   
xi=qi=1

bim = log pi(1   ri)
ci
(1    pi)ri

log	odds	

ratio

rsv bim =
where

term	present
term	absent
   with	constant	pi =	0.5,	simplifies	to	idf	weighting:

relevant	(r=1)
pi
(1	    pi)

not	relevant	(r=0)
ri
(1     ri)

document
xi =	1
xi =	0

rsv =

   
xi=qi=1

log n
ni

introduction	to	information	retrieval

graphical	model	for	bim	    bernoulli	nb

binary
variables
xi = (tfi     0)

i     q

introduction	to	information	retrieval

a	key	limitation	of	the	bim
   bim	    like	much	of	original	ir	    was	designed	for	

titles	or	abstracts,	and	not	for	modern	full	text	
search

   we	want	to	pay	attention	to	term	frequency	and	
document	lengths,	just	like	in	other	models	we		
discuss

   want

ci = log ptfr0
p0rtf

   want	some	model	of	how	often	terms	occur	in	docs

introduction	to	information	retrieval

1.	okapi	bm25	
   bm25	   best	match	25   	(they	had	a	bunch	of	tries!)

[robertson	et	al.	1994,	trec	city	u.]

   developed	in	the	context	of	the	okapi	system
   started	to	be	increasingly	adopted	by	other	teams	during	

the	trec	competitions

   it	works	well

   goal:	be	sensitive	to	term	frequency	and	document	

length	while	not	adding	too	many	parameters
   (robertson	and	zaragoza	2009;	sp  rck jones	et	al.	2000)

introduction	to	information	retrieval

generative	model	for	documents
   words	are	drawn	independently	from	the	vocabulary	

using	a	multinomial	distribution

... the draft is that each team is given a position in the draft     

basic

team

the

given

each

draft
nfl
design

football

of

football

draft

annual

nfl

is

team
that

   

introduction	to	information	retrieval

generative	model	for	documents
   distribution	of	term	frequencies	(tf)	follows	a	

binomial	distribution	    approximated	by	a	poisson

... the draft is that each team is given a position in the draft     

draft

   

   

introduction	to	information	retrieval

poisson	distribution
   the	poisson	distribution	models	the	id203	of	k,	
the	number	of	events	occurring	in	a	fixed	interval	of	
time/space,	with	known	average	rate	   (	=	cf/t),	
independent	of	the	last	event

p(k) =

   examples

  k
k! e     

   number	of	cars	arriving	at	the	toll	booth	per	minute
   number	of	typos	on	a	page

introduction	to	information	retrieval

poisson	distribution
   if	t	is	large	and	p	is	small,	we	can	approximate	a	
binomial	distribution	with	a	poisson	where	   =	tp

p(k) =

  k
k! e     

   mean	=	variance	=	   =	tp.	
   example	p	= 0.08,	t	=	20.	chance	of	1	occurrence	is:

   binomial	

p(1) =

$
&(.08)1(.92)19 =.3282
%

20
1

!
#
"
[(20)(.08)]1

1!

1.6
1 e   1.6 = 0.3230

   poisson																																																																																	   	already	close

e   (20)(.08) =

p(1) =

introduction	to	information	retrieval

poisson	model
   assume	that	term	frequencies	in	a	document	(tfi)	

follow	a	poisson	distribution
      fixed	interval   	implies	fixed	document	length	   	
think	roughly	constant-sized	document	abstracts
      	will	fix	later

introduction	to	information	retrieval

poisson	distributions

introduction	to	information	retrieval

(one)	poisson	model
   is	a	reasonable	fit	for	   general   	words
   is	a	poor	fit	for	topic-specific	words
   get	higher	p(k)	than	predicted	too	often

freq word
53
52
53
55
51

expected
based
conditions
cathexis
comic

7

6

5

4

3

9

8

documents containing	k occurrences	of	word	(   =	53/650)
0
599
600
604
619
642

1
49
48
39
22
3

2
2
2
7
3
0

2
0

1
0

2
1

1
0

0
0

1

1

0

0

10 11 12

2

harter,    a probabilistic approach to automatic keyword indexing   , jasist, 1975 

introduction	to	information	retrieval

eliteness	(   aboutness   )
   model	term	frequencies	using	eliteness
   what	is	eliteness?

   hidden	variable	for	each	document-term	pair,	

denoted	as	ei for	term	i

   represents	aboutness:	a	term	is	elite	in	a	

document	if,	in	some	sense,	the	document	is	
about	the	concept	denoted	by	the	term

   eliteness	is	binary
   term	occurrences	depend	only	on	eliteness   
      	but	eliteness	depends	on	relevance	

introduction	to	information	retrieval

elite	terms
text	from	the	wikipedia	page	on	the	nfl	draft	showing	
elite	terms

the national football league draft 
is an annual event in which the 
national football league (nfl) 
teams select eligible college 
football players.  it serves as the 
league   s most common source of 
player recruitment.  the basic design 
of the draft is that each team is given 
a position in the draft order in 
reverse order relative to its record    

introduction	to	information	retrieval

graphical	model	with	eliteness

binary
variables

frequencies
(not	binary)

i     q

introduction	to	information	retrieval

retrieval	status	value
   similar	to	the	bim	derivation,	we	have

rsv elite =

ci
elite

(tfi);

   
i   q,tfi>0

where
elite(tfi) = log p(tfi = tfi r =1)p(tfi = 0 r = 0)
ci
p(tfi = 0 r =1)p(tfi = tfi r = 0)

and	using	eliteness,	we	have:
p(tfi = tfi r) = p(tfi = tfi ei = elite)p(ei = elite r)

+p(tfi = tfi ei = elite)(1    p(ei = elite r))

introduction	to	information	retrieval

2-poisson	model
   the	problems	with	the	1-poisson	model	suggests	

fitting	two	poisson	distributions

   in	the	   2-poisson	model   ,	the	distribution	is	different	

depending	on	whether	the	term	is	elite	or	not

p(tfi = ki r) =

   where	  	is	id203	that	document	is	elite	for	term
   but,	unfortunately,	we	don   t	know	  ,	  ,	  

introduction	to	information	retrieval

let   s	get	an	idea:	graphing																		for	
different	parameter	values	of	the	2-poisson

elite(tfi)
ci

introduction	to	information	retrieval

qualitative	properties

  

  

elite(0) = 0
ci

elite(tfi)
ci

increases	monotonically	with	tfi

      	but	asymptotically	approaches	a	maximum	value	

as																											[not	true	for	simple	scaling	of	tf]

tfi        

       with	the	asymptotic	limit	being	

ci
bim

weight	of
eliteness
feature

introduction	to	information	retrieval

approximating	the	saturation	function
   estimating	parameters	for	the	2-poisson	model	is	not	

easy

      	so	approximate	it	with	a	simple	parametric	curve	

that	has	the	same	qualitative	properties

tf
k1 +tf

introduction	to	information	retrieval

saturation	function

   for	high	values	of	k1,	increments	in	tfi continue	to	

contribute	significantly	to	the	score

   contributions	tail	off	quickly	for	low	values	of	k1

introduction	to	information	retrieval

   early   	versions	of	bm25
   version	1:	using	the	saturation	function

bm 25v1(tfi) = ci
ci

bim

tfi
k1 +tfi

   version	2:	bim	simplification	to	idf

bm 25v2(tfi) = log n
ci
dfi

(k1 +1)tfi
k1 +tfi

  

   (k1+1) factor	doesn   t	change	ranking,	but	makes	

term	score	1 when	tfi = 1

   similar	to	tf-idf,	but	term	scores	are	bounded

introduction	to	information	retrieval

document	length	id172
   longer	documents	are	likely	to	have	larger	tfi values

   why	might	documents	be	longer?

   verbosity:	suggests	observed	tfi too	high
   larger	scope:	suggests	observed	tfi may	be	right

   a	real	document	collection	probably	has	both	effects	
      	so	should	apply	some	kind	of	partial	id172

introduction	to	information	retrieval

document	length	id172
   document	length:
dl =

tfi

   
i   v

   avdl:	average	document	length	over	collection
   length	id172	component

b = (1   b) +b dl
avdl

"
$
#

%
',
&

0     b    1

   b = 1  full	document	length	id172	
   b = 0  no	document	length	id172

introduction	to	information	retrieval

document	length	id172

introduction	to	information	retrieval

okapi	bm25
   normalize	tf using	document	length

  

t !fi =

tfi
b
bm 25(tfi) = log n
ci
dfi
= log n
dfi
   bm25	ranking	function
ci
bm 25
   
i   q

rsv bm 25 =

  

(tfi);

(k1 +1)t "fi
k1 +t "fi
(k1 +1)tfi
k1((1   b) +b dl

avdl) +tfi

introduction	to	information	retrieval

okapi	bm25

rsv bm 25 =

log n
dfi

   
i   q

   

(k1 +1)tfi
k1((1   b) +b dl

avdl) +tfi

   k1 controls	term	frequency	scaling

   k1 = 0 is	binary	model;	k1 large	is	raw	term	frequency

   b controls	document	length	id172

   b = 0 is	no	length	id172;	b = 1 is	relative	

frequency	(fully	scale	by	document	length)

   typically,	k1 is	set	around	1.2   2	and	b around	0.75	
   iir	sec.	11.4.3	discusses	incorporating	query	term	

weighting	and	(pseudo)	relevance	feedback

introduction	to	information	retrieval

why	is	bm25	better	than	vsm	tf-idf?
   suppose	your	query	is	[machine	learning]
   suppose	you	have	2	documents	with	term	counts:

   doc1:	learning	1024;	machine	1
   doc2:	learning	16;	machine	8

   tf-idf:	log2 tf *	log2 (n/df)

   doc1:	11	*	7	+	1	*	10									 = 87
   doc2:	5	*	7	+	4	*	10												=	75

   bm25:	k1 =	2

   doc1:	7	*	3	+	10	*	1													=	31
   doc2:	7	*	2.67	+	10	*	2.4				=	42.7

introduction	to	information	retrieval

2.	ranking	with	features
   textual	features

   zones:	title,	author,	abstract,	body,	anchors,	   
   proximity
      

   non-textual	features

   file	type
   file	age
   page	rank
      

introduction	to	information	retrieval

ranking	with	zones
   straightforward	idea:	

   apply	your	favorite	ranking	function	(bm25)	to	

each	zone	separately

   combine	zone	scores	using	a	weighted	linear	

combination

   but	that	seems	to	imply	that	the	eliteness	properties	

of	different	zones	are	different	and	independent	of	
each	other
      which	seems	unreasonable

introduction	to	information	retrieval

ranking	with	zones
   alternate	idea

   assume	eliteness	is	a	term/document	property	

shared	across	zones

      	but	the	relationship	between	eliteness	and	term	

frequencies	are	zone-dependent
   e.g.,	denser	use	of	elite	topic	words	in	title	

   consequence

   first	combine	evidence	across	zones	for	each	term
   then	combine	evidence	across	terms

introduction	to	information	retrieval

bm25f	with	zones
   calculate	a	weighted	variant	of	total	term	frequency
      	and	a	weighted	variant	of	document	length

vztfzi

t   fi =

z
   
z=1
where	

d   l =

z
   
z=1

vzlenz

d   l

avd   l = average
across	all
documents

vz is	zone	weight
tfzi is	term	frequency	in	zone	z
lenz is	length	of	zone	z
z is	the		number	of	zones

introduction	to	information	retrieval

simple	bm25f	with	zones

rsv simplebm 25f =

log n
dfi

   
i   q

   

(k1 +1)t   fi
k1((1   b) +b d   l

avd   l ) +t   fi

   simple	interpretation:	zone	z is	   replicated   	vz times

   but	we	may	want	zone-specific	parameters	(k1, b,

idf)

introduction	to	information	retrieval

bm25f
   empirically,	zone-specific	length	id172	(i.e.,	

zone-specific	b)	has	been	found	to	be	useful

t   fi =

z
   
z=1

bz = (1   bz) +bz

"
$
#

rsv bm 25f =

   
i   q

vz

tfzi
bz
lenz
avlenz
log n
dfi

   

%
',
&
(k1 +1)t   fi
k1 +t   fi

0     bz    1

see robertson and zaragoza (2009: 364)

introduction	to	information	retrieval

ranking	with	non-textual	features
   assumptions

   usual	independence	assumption

   independent	of	each	other	and	of	the	textual	features
   allows	us	to	factor	out																																			in	bim-style	

p(fj = fj r =1)
p(fj = fj r = 0)

derivation

   relevance	information	is	query	independent

   usually	true	for	features	like	page	rank,	age,	type,	   
   allows	us	to	keep	all	non-textual	features	in	the	bim-

style	derivation	where	we	drop	non-query	terms

introduction	to	information	retrieval

ranking	with	non-textual	features

where

f
   
j=1

i   q

ci

    (tfi) +   jvj( fj)
rsv =
vj( fj) = log p(fj = fj r =1)
p(fj = fj r = 0)

  j

and						is	an	artificially	added	free	parameter	to	account	
for	rescalings in	the	approximations
   care	must	be	taken	in	selecting	vj depending	on	fj.	e.g.	

log( !  j + fj)

fj

!  j + fj

1

!  j +exp(    fj

!!  j)

   explains	why																																														works	well

rsv bm 25 +log(id95)

introduction	to	information	retrieval

taken with slight adaptation from fan guo and 
chao liu   s 2009/2010 cikm tutorial: statistical 
models for web search: click log analysis

user	behavior
   search results for    cikm    (in 2010!)

# of clicks received

38

introduction	to	information	retrieval

user		behavior
   adapt ranking to user clicks?

# of clicks received

39

introduction	to	information	retrieval

user	behavior
   tools needed for non-trivial cases

# of clicks received

40

introduction	to	information	retrieval

web	search	click	log

an 
example

41

introduction	to	information	retrieval

web	search	click	log
   how large is the click log?
search logs: 10+ tb/day   

  

   in existing publications:

   [silverstein+99]: 285m sessions
   [craswell+08]: 108k sessions
   [dupret+08] : 4.5m sessions (21 subsets * 216k sessions)
   [guo +09a] : 8.8m sessions from 110k unique queries
   [guo+09b]: 8.8m sessions from 110k unique queries
   [chapelle+09]: 58m sessions from 682k unique queries
   [liu+09a]: 0.26pb data from 103m unique queries

42

introduction	to	information	retrieval

interpret	clicks:	an	example

   clicks are good   

   are these two clicks 

equally    good   ?

   non-clicks may have 

excuses:
   not relevant
   not examined

43

introduction	to	information	retrieval

eye-tracking	user	study

44

introduction	to	information	retrieval

click	position-bias

e
g
a
t
n
e
c
r
e
p

e
g
a
t
n
e
c
r
e
p

normal	position

reversed	impression

   higher positions receive 
more user attention (eye 
fixation) and clicks than 
lower positions.

   this is true even in the 
extreme setting where 
the order of positions is 
reversed.

      clicks are informative 

but biased   .

[joachims+07]

45

introduction	to	information	retrieval

user	behavior
   user	behavior	is	an	intriguing	source	of	relevance	data

   users	make	(somewhat)	informed	choices	when	

they	interact	with	search	engines

   potentially	a	lot	of	data	available	in	search	logs

   but	there	are	significant	caveats

   user	behavior	data	can	be	very	noisy
   interpreting	user	behavior	can	be	tricky
   spam	can	be	a	significant	problem
   not	all	queries	will	have	user	behavior

introduction	to	information	retrieval

features	based	on	user	behavior
from	[agichtein,	brill,	dumais 2006;	joachims 2002]
   click-through	features

   click	frequency,	click	id203,	click	deviation
   click	on	next	result?	previous result?	above?	below>?

   browsing	features

   cumulative	and	average	time	on	page,	on	domain,	on	url	

prefix;	deviation	from	average	times

   browse	path	features
   query-text	features

   query	overlap	with	title,	snippet,	url,	domain,	next	query
   query	length

introduction	to	information	retrieval

incorporating	user	behavior	into	
ranking	algorithm
   incorporate	user	behavior	features	into	a	ranking	

function	like	bm25f
   but	requires	an	understanding	of	user	behavior	

features	so	that	appropriate	vj functions	are	used

   incorporate	user	behavior	features	into	learned	

ranking	function

   either	of	these	ways	of	incorporating	user	behavior	

signals	improve	ranking

introduction	to	information	retrieval

resources
   s.	e.	robertson	and	h.	zaragoza.	2009.	the	probabilistic	

relevance	framework:	bm25	and	beyond.	foundations	and	
trends	in	information	retrieval 3(4):	333-389.

   k.	sp  rck jones,	s.	walker,	and	s.	e.	robertson.	2000.	a	

probabilistic	model	of	information	retrieval:	development	and	
comparative	experiments.	part	1.	information	processing	and	
management	779   808.

   t.	joachims.	optimizing	search	engines	using	clickthrough

data.	2002.	sigkdd.

   e.	agichtein,	e.	brill,	s.	dumais.	2006.	improving	web	search	
ranking	by	incorporating	user	behavior	information.	2006.	
sigir.

