introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc*on	
   to	
   
informa(on	
   retrieval	
   

cs276:	
   informa*on	
   retrieval	
   and	
   web	
   search	
   

pandu	
   nayak	
   and	
   prabhakar	
   raghavan	
   

lecture	
   6:	
   scoring,	
   term	
   weigh*ng	
   and	
   the	
   

vector	
   space	
   model	
   

recap	
   of	
   lecture	
   5	
   
         collec*on	
   and	
   vocabulary	
   sta*s*cs:	
   heaps  	
   and	
   zipf  s	
   laws	
   
         dic*onary	
   compression	
   for	
   boolean	
   indexes	
   

         dic*onary	
   string,	
   blocks,	
   front	
   coding	
   

         pos*ngs	
   compression:	
   gap	
   encoding,	
   pre   x-     unique	
   codes	
   

         variable-     byte	
   and	
   gamma	
   codes	
   

mb 

collection (text, xml markup etc) 
collection (text) 
term-doc incidence matrix 
postings, uncompressed (32-bit words) 
postings, uncompressed (20 bits) 
postings, variable byte encoded 
postings,   -encoded 

3,600.0 
960.0 
40,000.0 
400.0 
250.0 
116.0 
101.0 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

this	
   lecture;	
   iir	
   sec*ons	
   6.2-     6.4.3	
   
         ranked	
   retrieval	
   
         scoring	
   documents	
   
         term	
   frequency	
   
         collec*on	
   sta*s*cs	
   
         weigh*ng	
   schemes	
   
         vector	
   space	
   scoring	
   

ranked	
   retrieval	
   
         thus	
   far,	
   our	
   queries	
   have	
   all	
   been	
   boolean.	
   

         documents	
   either	
   match	
   or	
   don  t.	
   

         good	
   for	
   expert	
   users	
   with	
   precise	
   understanding	
   of	
   

their	
   needs	
   and	
   the	
   collec*on.	
   
         also	
   good	
   for	
   applica*ons:	
   applica*ons	
   can	
   easily	
   

consume	
   1000s	
   of	
   results.	
   

         not	
   good	
   for	
   the	
   majority	
   of	
   users.	
   

         most	
   users	
   incapable	
   of	
   wri*ng	
   boolean	
   queries	
   (or	
   they	
   

are,	
   but	
   they	
   think	
   it  s	
   too	
   much	
   work).	
   

         most	
   users	
   don  t	
   want	
   to	
   wade	
   through	
   1000s	
   of	
   results.	
   

         this	
   is	
   par*cularly	
   true	
   of	
   web	
   search.	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

problem	
   with	
   boolean	
   search:	
   
feast	
   or	
   famine	
   
         boolean	
   queries	
   o]en	
   result	
   in	
   either	
   too	
   few	
   (=0)	
   or	
   

	
   	
   
ch. 6 

too	
   many	
   (1000s)	
   results.	
   

         query	
   1:	
      standard	
   user	
   dlink	
   650   	
      	
   200,000	
   hits	
   
         query	
   2:	
      standard	
   user	
   dlink	
   650	
   no	
   card	
   found   :	
   0	
   

hits	
   

         it	
   takes	
   a	
   lot	
   of	
   skill	
   to	
   come	
   up	
   with	
   a	
   query	
   that	
   

produces	
   a	
   manageable	
   number	
   of	
   hits.	
   
         and	
   gives	
   too	
   few;	
   or	
   gives	
   too	
   many	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

ranked	
   retrieval	
   models	
   
         rather	
   than	
   a	
   set	
   of	
   documents	
   sa*sfying	
   a	
   query	
   

expression,	
   in	
   ranked	
   retrieval,	
   the	
   system	
   returns	
   an	
   
ordering	
   over	
   the	
   (top)	
   documents	
   in	
   the	
   collec*on	
   
for	
   a	
   query	
   

         free	
   text	
   queries:	
   rather	
   than	
   a	
   query	
   language	
   of	
   
operators	
   and	
   expressions,	
   the	
   user  s	
   query	
   is	
   just	
   
one	
   or	
   more	
   words	
   in	
   a	
   human	
   language	
   

         in	
   principle,	
   there	
   are	
   two	
   separate	
   choices	
   here,	
   but	
   

in	
   prac*ce,	
   ranked	
   retrieval	
   has	
   normally	
   been	
   
associated	
   with	
   free	
   text	
   queries	
   and	
   vice	
   versa	
   

	
   

6	
   

1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

feast	
   or	
   famine:	
   not	
   a	
   problem	
   in	
   
ranked	
   retrieval	
   
         when	
   a	
   system	
   produces	
   a	
   ranked	
   result	
   set,	
   large	
   

result	
   sets	
   are	
   not	
   an	
   issue	
   
         indeed,	
   the	
   size	
   of	
   the	
   result	
   set	
   is	
   not	
   an	
   issue	
   
         we	
   just	
   show	
   the	
   top	
   k	
   (	
      	
   10)	
   results	
   
         we	
   don  t	
   overwhelm	
   the	
   user	
   

         premise:	
   the	
   ranking	
   algorithm	
   works	
   

scoring	
   as	
   the	
   basis	
   of	
   ranked	
   retrieval	
   
         we	
   wish	
   to	
   return	
   in	
   order	
   the	
   documents	
   most	
   likely	
   

to	
   be	
   useful	
   to	
   the	
   searcher	
   

         how	
   can	
   we	
   rank-     order	
   the	
   documents	
   in	
   the	
   

collec*on	
   with	
   respect	
   to	
   a	
   query?	
   

         assign	
   a	
   score	
      	
   say	
   in	
   [0,	
   1]	
      	
   to	
   each	
   document	
   
         this	
   score	
   measures	
   how	
   well	
   document	
   and	
   query	
   

   match   .	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

query-     document	
   matching	
   scores	
   
         we	
   need	
   a	
   way	
   of	
   assigning	
   a	
   score	
   to	
   a	
   query/

document	
   pair	
   

         let  s	
   start	
   with	
   a	
   one-     term	
   query	
   
         if	
   the	
   query	
   term	
   does	
   not	
   occur	
   in	
   the	
   document:	
   

score	
   should	
   be	
   0	
   

         the	
   more	
   frequent	
   the	
   query	
   term	
   in	
   the	
   document,	
   

the	
   higher	
   the	
   score	
   (should	
   be)	
   

         we	
   will	
   look	
   at	
   a	
   number	
   of	
   alterna*ves	
   for	
   this.	
   

take	
   1:	
   jaccard	
   coe   cient	
   
         recall	
   from	
   lecture	
   3:	
   a	
   commonly	
   used	
   measure	
   of	
   

overlap	
   of	
   two	
   sets	
   a	
   and	
   b	
   

         jaccard(a,b)	
   =	
   |a	
      	
   b|	
   /	
   |a	
      	
   b|	
   
         jaccard(a,a)	
   =	
   1	
   
         jaccard(a,b)	
   =	
   0	
   if	
   a	
      	
   b	
   =	
   0	
   
         a	
   and	
   b	
   don  t	
   have	
   to	
   be	
   the	
   same	
   size.	
   
         always	
   assigns	
   a	
   number	
   between	
   0	
   and	
   1.	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

jaccard	
   coe   cient:	
   scoring	
   example	
   
         what	
   is	
   the	
   query-     document	
   match	
   score	
   that	
   the	
   
jaccard	
   coe   cient	
   computes	
   for	
   each	
   of	
   the	
   two	
   
documents	
   below?	
   
         query:	
   ides	
   of	
   march	
   
         document	
   1:	
   caesar	
   died	
   in	
   march	
   
         document	
   2:	
   the	
   long	
   march	
   

issues	
   with	
   jaccard	
   for	
   scoring	
   
         it	
   doesn  t	
   consider	
   term	
   frequency	
   (how	
   many	
   *mes	
   

a	
   term	
   occurs	
   in	
   a	
   document)	
   

         rare	
   terms	
   in	
   a	
   collec*on	
   are	
   more	
   informa*ve	
   than	
   

frequent	
   terms.	
   jaccard	
   doesn  t	
   consider	
   this	
   
informa*on	
   

         we	
   need	
   a	
   more	
   sophis*cated	
   way	
   of	
   normalizing	
   for	
   

length	
   

         later	
   in	
   this	
   lecture,	
   we  ll	
   use	
   	
   
         .	
   .	
   .	
   instead	
   of	
   |a	
      	
   b|/|a	
      	
   b|	
   (jaccard)	
   for	
   length	
   

 b a|

 b a|

/|

   

   

|

normaliza*on.	
   

2 

	
   	
   
sec. 6.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

recall	
   (lecture	
   1):	
   binary	
   term-     
document	
   incidence	
   matrix	
   

antony and cleopatra

julius caesar

the tempest

haid113t

othello

macbeth

antony
brutus
caesar
calpurnia
cleopatra

mercy
worser

1
1
1
0
1
1
1

1
1
1
1
0
0
0

0
0
0
0
0
1
1

0
1
1
0
0
1
1

0
0
1
0
0
1
1

1
0
1
0
0
1
0

each document is represented by a binary vector     {0,1}|v| 

term-     document	
   count	
   matrices	
   
         consider	
   the	
   number	
   of	
   occurrences	
   of	
   a	
   term	
   in	
   a	
   

document:	
   	
   
         each	
   document	
   is	
   a	
   count	
   vector	
   in	
      v:	
   a	
   column	
   below	
   	
   

antony and cleopatra

julius caesar

the tempest

haid113t

othello

macbeth

antony
brutus
caesar
calpurnia
cleopatra

mercy
worser

157
4
232
0
57
2
2

73
157
227
10
0
0
0

0
0
0
0
0
3
1

0
1
2
0
0
5
1

0
0
1
0
0
5
1

0
0
1
0
0
1
0

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

bag	
   of	
   words	
   model	
   
         vector	
   representa*on	
   doesn  t	
   consider	
   the	
   ordering	
   

of	
   words	
   in	
   a	
   document	
   

         john	
   is	
   quicker	
   than	
   mary	
   and	
   mary	
   is	
   quicker	
   than	
   

john	
   have	
   the	
   same	
   vectors	
   

         this	
   is	
   called	
   the	
   bag	
   of	
   words	
   model.	
   
         in	
   a	
   sense,	
   this	
   is	
   a	
   step	
   back:	
   the	
   posi*onal	
   index	
   

was	
   able	
   to	
   dis*nguish	
   these	
   two	
   documents.	
   

         we	
   will	
   look	
   at	
      recovering   	
   posi*onal	
   informa*on	
   

later	
   in	
   this	
   course.	
   

         for	
   now:	
   bag	
   of	
   words	
   model	
   

term	
   frequency	
   o	
   
         the	
   term	
   frequency	
   ot,d	
   of	
   term	
   t	
   in	
   document	
   d	
   is	
   
de   ned	
   as	
   the	
   number	
   of	
   *mes	
   that	
   t	
   occurs	
   in	
   d.	
   
         we	
   want	
   to	
   use	
   o	
   when	
   compu*ng	
   query-     document	
   

match	
   scores.	
   but	
   how?	
   

         raw	
   term	
   frequency	
   is	
   not	
   what	
   we	
   want:	
   

         a	
   document	
   with	
   10	
   occurrences	
   of	
   the	
   term	
   is	
   more	
   

relevant	
   than	
   a	
   document	
   with	
   1	
   occurrence	
   of	
   the	
   term.	
   

         but	
   not	
   10	
   *mes	
   more	
   relevant.	
   

         relevance	
   does	
   not	
   increase	
   propor*onally	
   with	
   

term	
   frequency.	
   

nb:	
   frequency	
   =	
   count	
   in	
   ir	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.1 

log-     frequency	
   weigh*ng	
   
         the	
   log	
   frequency	
   weight	
   of	
   term	
   t	
   in	
   d	
   is	
   

t,dw

 1
      
 
 
=
      
      

+

log 
10
0,

 tf

t,d

,

 tf

if
0  
>
otherwise
 

t,d

         0	
      	
   0,	
   1	
      	
   1,	
   2	
      	
   1.3,	
   10	
      	
   2,	
   1000	
      	
   4,	
   etc.	
   
         score	
   for	
   a	
   document-     query	
   pair:	
   sum	
   over	
   terms	
   t	
   in	
   

both	
   q	
   and	
   d:	
   

         score	
   

=

          

dqt

log  (1

+

 tf

dt )
,

document	
   frequency	
   
         rare	
   terms	
   are	
   more	
   informa*ve	
   than	
   frequent	
   terms	
   

         recall	
   stop	
   words	
   

         consider	
   a	
   term	
   in	
   the	
   query	
   that	
   is	
   rare	
   in	
   the	
   

collec*on	
   (e.g.,	
   arachnocentric)	
   

         a	
   document	
   containing	
   this	
   term	
   is	
   very	
   likely	
   to	
   be	
   

relevant	
   to	
   the	
   query	
   arachnocentric	
   

            	
   we	
   want	
   a	
   high	
   weight	
   for	
   rare	
   terms	
   like	
   

arachnocentric.	
   

         the	
   score	
   is	
   0	
   if	
   none	
   of	
   the	
   query	
   terms	
   is	
   present	
   in	
   

the	
   document.	
   

3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.1 

document	
   frequency,	
   con*nued	
   
         frequent	
   terms	
   are	
   less	
   informa*ve	
   than	
   rare	
   terms	
   
         consider	
   a	
   query	
   term	
   that	
   is	
   frequent	
   in	
   the	
   

collec*on	
   (e.g.,	
   high,	
   increase,	
   line)	
   

         a	
   document	
   containing	
   such	
   a	
   term	
   is	
   more	
   likely	
   to	
   

be	
   relevant	
   than	
   a	
   document	
   that	
   doesn  t	
   
         but	
   it  s	
   not	
   a	
   sure	
   indicator	
   of	
   relevance.	
   
            	
   for	
   frequent	
   terms,	
   we	
   want	
   high	
   posi*ve	
   weights	
   

for	
   words	
   like	
   high,	
   increase,	
   and	
   line	
   
         but	
   lower	
   weights	
   than	
   for	
   rare	
   terms.	
   
         we	
   will	
   use	
   document	
   frequency	
   (df)	
   to	
   capture	
   this.	
   

idf	
   weight	
   
         dft	
   is	
   the	
   document	
   frequency	
   of	
   t:	
   the	
   number	
   of	
   

documents	
   that	
   contain	
   t	
   
         dft	
   is	
   an	
   inverse	
   measure	
   of	
   the	
   informa*veness	
   of	
   t	
   
         dft	
   	
      	
   n	
   

         we	
   de   ne	
   the	
   idf	
   (inverse	
   document	
   frequency)	
   of	
   t	
   

by	
   

	
   

idf

log  
=
10

t

( 

n
/df

)

t

         we	
   use	
   log	
   (n/dft)	
   instead	
   of	
   n/dft	
   to	
      dampen   	
   the	
   e   ect	
   

of	
   idf.	
   

will turn out the base of the log is immaterial. 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

idf	
   example,	
   suppose	
   n	
   =	
   1	
   million	
   

term 
calpurnia 
animal 
sunday 
fly 
under 
the 

dft 

idft 

1 
100 
1,000 
10,000 
100,000 
1,000,000 

idf

log  
=
10

t

( 

n
/df

)

t

there is one idf value for each term t in a collection. 

e   ect	
   of	
   idf	
   on	
   ranking	
   
         does	
   idf	
   have	
   an	
   e   ect	
   on	
   ranking	
   for	
   one-     term	
   

queries,	
   like	
   
         iphone	
   

         idf	
   has	
   no	
   e   ect	
   on	
   ranking	
   one	
   term	
   queries	
   

         idf	
   a   ects	
   the	
   ranking	
   of	
   documents	
   for	
   queries	
   with	
   at	
   

least	
   two	
   terms	
   

         for	
   the	
   query	
   capricious	
   person,	
   idf	
   weigh*ng	
   makes	
   

occurrences	
   of	
   capricious	
   count	
   for	
   much	
   more	
   in	
   the	
      nal	
   
document	
   ranking	
   than	
   occurrences	
   of	
   person.	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.1 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.2 

22	
   

collec*on	
   vs.	
   document	
   frequency	
   
         the	
   collec*on	
   frequency	
   of	
   t	
   is	
   the	
   number	
   of	
   

occurrences	
   of	
   t	
   in	
   the	
   collec*on,	
   coun*ng	
   
mul*ple	
   occurrences.	
   

         example:	
   
word 

collection frequency 

document frequency 

insurance 

10440 

3997 

try 

8760 
	
   
         which	
   word	
   is	
   a	
   beqer	
   search	
   term	
   (and	
   should	
   

10422 

get	
   a	
   higher	
   weight)?	
   

o-     idf	
   weigh*ng	
   
         the	
   o-     idf	
   weight	
   of	
   a	
   term	
   is	
   the	
   product	
   of	
   its	
   o	
   

weight	
   and	
   its	
   idf	
   weight.	
   

	
   

w

dt
,

log(
1

+

tf

=

dt
,

)

  

log
10

(

n

)df/
t

         best	
   known	
   weigh*ng	
   scheme	
   in	
   informa*on	
   retrieval	
   

         note:	
   the	
      -        	
   in	
   o-     idf	
   is	
   a	
   hyphen,	
   not	
   a	
   minus	
   sign!	
   
         alterna*ve	
   names:	
   o.idf,	
   o	
   x	
   idf	
   

         increases	
   with	
   the	
   number	
   of	
   occurrences	
   within	
   a	
   

document	
   

         increases	
   with	
   the	
   rarity	
   of	
   the	
   term	
   in	
   the	
   collec*on	
   

4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.2.2 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

score	
   for	
   a	
   document	
   given	
   a	
   query	
   
tf.idft,d

score(q,d) =

t "q#d$

         there	
   are	
   many	
   variants	
   

         how	
      o   	
   is	
   computed	
   (with/without	
   logs)	
   
         whether	
   the	
   terms	
   in	
   the	
   query	
   are	
   also	
   weighted	
   
            	
   	
   

! 

25	
   

binary	
      	
   count	
      	
   weight	
   matrix	
   

antony and cleopatra

julius caesar

the tempest

haid113t

othello

macbeth

antony
brutus
caesar
calpurnia
cleopatra

mercy
worser

5.25
1.21
8.59

0

2.85
1.51
1.37

3.18
6.1
2.54
1.54

0
0
0

0
0
0
0
0
1.9
0.11

0
1

0
0

1.51

0.25

0
0

0.12
4.15

0
0

5.25
0.25

0.35

0
0
0
0

0.88
1.95

each document is now represented by a real-valued 
vector of tf-idf weights     r|v| 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

documents	
   as	
   vectors	
   
         so	
   we	
   have	
   a	
   |v|-     dimensional	
   vector	
   space	
   
         terms	
   are	
   axes	
   of	
   the	
   space	
   
         documents	
   are	
   points	
   or	
   vectors	
   in	
   this	
   space	
   
         very	
   high-     dimensional:	
   tens	
   of	
   millions	
   of	
   dimensions	
   

when	
   you	
   apply	
   this	
   to	
   a	
   web	
   search	
   engine	
   

         these	
   are	
   very	
   sparse	
   vectors	
   -     	
   most	
   entries	
   are	
   zero.	
   

queries	
   as	
   vectors	
   
         key	
   idea	
   1:	
   do	
   the	
   same	
   for	
   queries:	
   represent	
   them	
   

as	
   vectors	
   in	
   the	
   space	
   

         key	
   idea	
   2:	
   rank	
   documents	
   according	
   to	
   their	
   

proximity	
   to	
   the	
   query	
   in	
   this	
   space	
   

         proximity	
   =	
   similarity	
   of	
   vectors	
   
         proximity	
      	
   inverse	
   of	
   distance	
   
         recall:	
   we	
   do	
   this	
   because	
   we	
   want	
   to	
   get	
   away	
   from	
   

the	
   you  re-     either-     in-     or-     out	
   boolean	
   model.	
   

         instead:	
   rank	
   more	
   relevant	
   documents	
   higher	
   than	
   

less	
   relevant	
   documents	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

formalizing	
   vector	
   space	
   proximity	
   
         first	
   cut:	
   distance	
   between	
   two	
   points	
   

         (	
   =	
   distance	
   between	
   the	
   end	
   points	
   of	
   the	
   two	
   vectors)	
   

         euclidean	
   distance?	
   
         euclidean	
   distance	
   is	
   a	
   bad	
   idea	
   .	
   .	
   .	
   
         .	
   .	
   .	
   because	
   euclidean	
   distance	
   is	
   large	
   for	
   vectors	
   of	
   

di   erent	
   lengths.	
   

why	
   distance	
   is	
   a	
   bad	
   idea	
   

the	
   euclidean	
   distance	
   
between	
   q	
   
and	
   d2	
   is	
   large	
   even	
   
though	
   the	
   
distribu*on	
   of	
   terms	
   
in	
   the	
   query	
   q	
   and	
   the	
   
distribu*on	
   of	
   
terms	
   in	
   the	
   
document	
   d2	
   are	
   
very	
   similar.	
   

5 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

use	
   angle	
   instead	
   of	
   distance	
   
         thought	
   experiment:	
   take	
   a	
   document	
   d	
   and	
   append	
   

it	
   to	
   itself.	
   call	
   this	
   document	
   d     .	
   

            seman*cally   	
   d	
   and	
   d     	
   have	
   the	
   same	
   content	
   
         the	
   euclidean	
   distance	
   between	
   the	
   two	
   documents	
   

can	
   be	
   quite	
   large	
   

         the	
   angle	
   between	
   the	
   two	
   documents	
   is	
   0,	
   

corresponding	
   to	
   maximal	
   similarity.	
   

from	
   angles	
   to	
   cosines	
   
         the	
   following	
   two	
   no*ons	
   are	
   equivalent.	
   

         rank	
   documents	
   in	
   decreasing	
   order	
   of	
   the	
   angle	
   between	
   

query	
   and	
   document	
   

         rank	
   documents	
   in	
   increasing	
   order	
   	
   of	
   cosine

         cosine	
   is	
   a	
   monotonically	
   decreasing	
   func*on	
   for	
   the	
   

(query,document)	
   

interval	
   [0o,	
   180o]	
   

         key	
   idea:	
   rank	
   documents	
   according	
   to	
   angle	
   with	
   

query.	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

from	
   angles	
   to	
   cosines	
   

length	
   normaliza*on	
   
         a	
   vector	
   can	
   be	
   (length-     )	
   normalized	
   by	
   dividing	
   each	
   
of	
   its	
   components	
   by	
   its	
   length	
      	
   for	
   this	
   we	
   use	
   the	
   
l2	
   norm:	
   

   
x

   =

i

ix
2

2

         but	
   how	
      	
   and	
   why	
      	
   should	
   we	
   be	
   compu*ng	
   cosines?	
   

         dividing	
   a	
   vector	
   by	
   its	
   l2	
   norm	
   makes	
   it	
   a	
   unit	
   
(length)	
   vector	
   (on	
   surface	
   of	
   unit	
   hypersphere)	
   
         e   ect	
   on	
   the	
   two	
   documents	
   d	
   and	
   d     	
   (d	
   appended	
   to	
   

itself)	
   from	
   earlier	
   slide:	
   they	
   have	
   iden*cal	
   vectors	
   
a]er	
   length-     normaliza*on.	
   
         long	
   and	
   short	
   documents	
   now	
   have	
   comparable	
   weights	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

cosine(query,document)	
   

dot product 
   
   
dq
   
      
dq

      
dq
,

=

)

=

cos(

unit vectors 
   
q
   
q

   
d
   
d

=

   

v
   
i
1
=
q
2
i

1
=

v
i

i

dq
i
   

v
i

1
=

d

2
i

   

qi is the tf-idf weight of term i in the query 
di is the tf-idf weight of term i in the document 
 
cos(q,d) is the cosine similarity of q and d     or, 
equivalently, the cosine of the angle between q and d. 

! 

cosine	
   for	
   length-     normalized	
   vectors	
   
         for	
   length-     normalized	
   vectors,	
   cosine	
   similarity	
   is	
   

simply	
   the	
   dot	
   product	
   (or	
   scalar	
   product):	
   

cos(! q , ! d ) =
  

! q    

! d =

v"

i=1

qidi

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   for	
   q,	
   d	
   length-     normalized.	
   
	
   

36	
   

6 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

cosine	
   similarity	
   illustrated	
   

sas 

term 

cosine	
   similarity	
   amongst	
   3	
   documents	
   
how	
   similar	
   are	
   
the	
   novels	
   
sas:	
   sense	
   and	
   
sensibility	
   
pap:	
   pride	
   and	
   
prejudice,	
   and	
   
wh:	
   wuthering	
   
heights?	
   

wuthering 

affection 

jealous 

gossip 

pap 

115 

58  

10 

7 

2 

0 

0 

0 

term frequencies (counts) 

wh 

20 

11 

6 

38 

37	
   

note: to simplify this example, we don  t do idf weighting. 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.3 

3	
   documents	
   example	
   contd.	
   

compu*ng	
   cosine	
   scores	
   

log	
   frequency	
   weigh(ng	
   

a9er	
   length	
   normaliza(on	
   

term 
affection 
jealous 
gossip 
wuthering 

sas 
3.06 
2.00 
1.30 
0 

pap 

wh 

2.76 
1.85 
0 
0 

2.30 
2.04 
1.78 
2.58 

term 
affection 
jealous 
gossip 
wuthering 

sas 
0.789 
0.515 
0.335 
0 

pap 
0.832 
0.555 
0 
0 

wh 
0.524 
0.465 
0.405 
0.588 

cos(sas,pap)     
0.789    0.832 + 0.515    0.555 + 0.335    0.0 + 0.0    0.0 
    0.94 
cos(sas,wh)     0.79 
cos(pap,wh)     0.69 

why do we have cos(sas,pap) > cos(sas,wh)? 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.4 

o-     idf	
   weigh*ng	
   has	
   many	
   variants	
   

columns headed   n   are acronyms for weight schemes. 

why is the base of the log in idf immaterial? 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.4 

weigh*ng	
   may	
   di   er	
   in	
   queries	
   vs	
   
documents	
   
         many	
   search	
   engines	
   allow	
   for	
   di   erent	
   weigh*ngs	
   

for	
   queries	
   vs.	
   documents	
   

         smart	
   nota*on:	
   denotes	
   the	
   combina*on	
   in	
   use	
   in	
   

an	
   engine,	
   with	
   the	
   nota*on	
   ddd.qqq,	
   using	
   the	
   
acronyms	
   from	
   the	
   previous	
   table	
   

         a	
   very	
   standard	
   weigh*ng	
   scheme	
   is:	
   lnc.ltc	
   
         document:	
   logarithmic	
   o	
   (l	
   as	
      rst	
   character),	
   no	
   idf	
   

and	
   cosine	
   normaliza*on	
   

         query:	
   logarithmic	
   o	
   (l	
   in	
   le]most	
   column),	
   idf	
   (t	
   in	
   

second	
   column),	
   no	
   normaliza*on	
      	
   

a bad idea? 

7 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
sec. 6.4 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   

o-     idf	
   example:	
   lnc.ltc	
   

document: car insurance auto insurance 
query: best car insurance 

term 

auto 
best 
car 
insurance 

tf-
raw 
0 
1 
1  
1 

tf-wt 

query 
df 

idf  wt  n  liz
e 

document 
tf-wt 
wt 

tf-raw 

prod 

n  liz
e 

0 

5000  2.3  

0 
0 
1  50000  1.3  1.3  0.34 
1  10000  2.0  2.0  0.52 
1 
1000  3.0  3.0  0.78 

1 
0 
1 
2 

1 
0 
1 
1.3 

0 
1  0.52 
0 
0 
0 
1  0.52  0.27 
1.3  0.68  0.53 

summary	
      	
   vector	
   space	
   ranking	
   
         represent	
   the	
   query	
   as	
   a	
   weighted	
   o-     idf	
   vector	
   
         represent	
   each	
   document	
   as	
   a	
   weighted	
   o-     idf	
   vector	
   
         compute	
   the	
   cosine	
   similarity	
   score	
   for	
   the	
   query	
   

vector	
   and	
   each	
   document	
   vector	
   

         rank	
   documents	
   with	
   respect	
   to	
   the	
   query	
   by	
   score	
   
         return	
   the	
   top	
   k	
   (e.g.,	
   k	
   =	
   10)	
   to	
   the	
   user	
   

exercise: what is n, the number of docs? 

doc length = 

12 + 02 +12 +1.32 "1.92

score = 0+0+0.27+0.53 = 0.8 

! 

introduc)on	
   to	
   informa)on	
   retrieval	
   

	
   	
   

	
   	
   
ch. 6 

resources	
   for	
   today  s	
   lecture	
   
         iir	
   6.2	
      	
   6.4.3	
   

         hqp://www.miislita.com/informa*on-     retrieval-     

tutorial/cosine-     similarity-     tutorial.html	
   
         term	
   weigh*ng	
   and	
   cosine	
   similarity	
   tutorial	
   for	
   seo	
   folk!	
   

8 

