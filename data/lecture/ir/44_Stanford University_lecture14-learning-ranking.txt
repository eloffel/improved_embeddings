introduction	to	information	retrieval

introduction	to
information	retrieval

cs276:	information	retrieval	and	web	search

christopher	manning	and	pandu	nayak

lecture	14:	learning	to	rank

introduction	to	information	retrieval

sec.	15.4

machine	learning	for	ir	ranking?
   we   ve	looked	at	methods	for	ranking	documents	in	ir
   cosine	similarity,	inverse	document	frequency,	bm25,	proximity,	pivoted	

document	length	id172,	(will	look	at)	id95,	   

   we   ve	looked	at	methods	for	classifying	documents	

using	supervised	machine	learning	classifiers
   rocchio,	knn,	id166s,	etc.

   surely	we	can	also	use	machine	learning	to	rank	the	

documents	displayed	in	search	results?
   sounds	like	a	good	idea
   a.k.a.	   machine-learned	relevance   	or	   learning	to	rank   

introduction	to	information	retrieval

introduction	to	information	retrieval

machine	learning	for	ir	ranking
   this	   good	idea   	has	been	actively	researched	    and	
actively	deployed	by	major	web	search	engines	    in	
the	last	7   10	years

   why	didn   t	it	happen	earlier?		

   modern	supervised	ml	has	been	around	for	about	20	

years   

   na  ve	bayes	has	been	around	for	about	50	years   

introduction	to	information	retrieval

machine	learning	for	ir	ranking
   there   s	some	truth	to	the	fact	that	the	ir	community	

wasn   t	very	connected	to	the	ml	community
   but	there	were	a	whole	bunch	of	precursors:

   wong,	s.k.	et	al.	1988.	linear	structure	in	information	

retrieval.	sigir	1988.

   fuhr,	n.	1992.	probabilistic	methods	in	information	

retrieval.	computer	journal.

   gey,	f.	c.	1994.	inferring	id203	of	relevance	using	the	

method	of	logistic	regression.	sigir	1994.

   herbrich,	r.	et	al.	2000.	large	margin	rank	boundaries	for	
ordinal	regression.	advances	in	large	margin	classifiers.

introduction	to	information	retrieval

why	weren   t	early	attempts	very	
successful/influential?
   sometimes	an	idea	just	takes	time	to	be	appreciated   
   limited	training	data

   especially	for	real	world	use	(as	opposed	to	writing	

academic	papers),	it	was	very	hard	to	gather	test	collection	
queries	and	relevance	judgments	that	are	representative	of	
real	user	needs	and	judgments	on	documents	returned
   this	has	changed,	both	in	academia	and	industry

   poor	machine	learning	techniques
   insufficient	customization	to	ir	problem
   not	enough	features	for	ml	to	show	value

introduction	to	information	retrieval

why	wasn   t	ml	much	needed?
   traditional	ranking	functions	in	ir	used	a	very	small	

number	of	features,	e.g.,
   term	frequency
   inverse	document	frequency
   document	length

   it	was	easy possible	to	tune	weighting	coefficients	by	

hand
   and	people	did
   you	students	do	it	in	pa3	

introduction	to	information	retrieval

why	is	ml	needed	now?
   modern	systems	    especially	on	the	web	    use	a	great	

number	of	features:

   arbitrary	useful	features	    not	a	single	unified	model

   log	frequency	of	query	word	in	anchor	text?
   query	word	in	color	on	page?
   #	of	images	on	page?
   #	of	(out)	links	on	page?
   id95	of	page?
   url	length?
   url	contains	   ~   ?
   page	edit	recency?
   page	loading	speed

   the	new	york	times	in	2008-06-03	quoted	amit	singhal as	
saying	google	was	using	over	200	such	features	(   signals   )

introduction	to	information	retrieval

sec.	15.4.1

simple	example:
using	classification	for	ad	hoc	ir
   collect	a	training	corpus	of	(q,	d,	r)	triples

   relevance	r	is	here	binary	 (but	may	be	multiclass,	with	3   7	values)
   document	is	represented	by	a	feature	vector

   x =	(  ,	  )   	is	cosine	similarity,	   is	minimum	query	window	size
      is	the	the	shortest	text	span	that	includes	all	query	words
   query	term	proximity	is	an	important new	weighting	factor
   train	a	machine	learning	model	to	predict	the	class	r	of	a	document-

query	pair	

introduction	to	information	retrieval

sec.	15.4.1

simple	example:
using	classification	for	ad	hoc	ir
   a	linear	score	function	is	then	

   and	the	linear	classifier	is

score(d,	q)	=	score(  ,	  )	=	a  	+	b  	+	c
decide	relevant	if	score(d,	q)	>	  

      	just	like	when	we	were	doing	text	classification

introduction	to	information	retrieval

sec.	15.4.1

simple	example:
using	classification	for	ad	hoc	ir

decision	surface

0.05

	

a
e
r
o
c
s
	
e
n
i
s
o
c

0.025

r

0

2

r

r
r
n

n

r
r

r
n
n
n

r
n

n

n

r
r
r

n

n

3

4

5
term	proximity	w

introduction	to	information	retrieval

more	complex	example	of	using	classification	for	
search	ranking		[nallapati	2004]
   we	can	generalize	this	to	classifier	functions	over	

more	features

   we	can	use	methods	we	have	seen	previously	for	

learning	the	linear	classifier	weights

introduction	to	information	retrieval

an	id166	classifier	for	information	retrieval		
[nallapati	2004]
   let	relevance	score	g(r|d,q)	=	w  f(d,q)	+	b
   id166	training:	want	g(r|d,q)	   	   1	for	nonrelevant	

documents	and	g(r|d,q)	   	1	for	relevant	documents

   id166	testing:	decide	relevant	iff g(r|d,q)	   	0

   features	are	not word	presence	features	(how	would	you	

deal	with	query	words	not	in	your	training	data?)	but	
scores	like	the	summed	(log)	tf of	all	query	terms

   unbalanced	data	(which	can	result	in	trivial	always-say-
nonrelevant	classifiers)	is	dealt	with	by	undersampling
nonrelevant	documents	during	training	(just	take	some	
at	random)					[there	are	other	ways	of	doing	this	    cf.	cao	et	al.	later]

introduction	to	information	retrieval

an	id166	classifier	for	information	retrieval		
[nallapati	2004]
   experiments:

   4	trec	data	sets
   comparisons	with	lemur,	a	state-of-the-art	open	source	ir	

engine	(language	model	(lm)-based	    see	iir	ch.	12)

   linear	kernel	normally	best	or	almost	as	good	as	quadratic	

kernel,	and	so	used	in	reported	results

   6	features,	all	variants	of	tf,	idf,	and	tf.idf	scores

introduction	to	information	retrieval

an	id166	classifier	for	information	retrieval		
[nallapati	2004]

train	\ test
trec	disk	3

disk	4-5

lemur
id166
lemur
id166

disk	3
0.1785
0.1728
0.1773
0.1646

disk	4-5
0.2503
0.2432
0.2516
0.2355

wt10g	(web)
0.2666
0.2750
0.2656
0.2675

   at	best	the	results	are	about	equal	to	lemur

   actually	a	little	bit	below

   paper   s	advertisement:	easy	to	add	more	features

   this	is	illustrated	on	a	homepage	finding	task	on	

wt10g:
   baseline	lemur	52%	success@10,	baseline	id166	58%
   id166	with	url-depth,	and	in-link	features:	78%	success@10

introduction	to	information	retrieval

sec.	15.4.2

   learning	to	rank   
   classification	probably	isn   t	the	right	way	to	think	

about	approaching	ad	hoc	ir:
   classification	problems:	map	to	an	unordered	set	of	classes
   regression	problems:	map	to	a	real	value	[start	of	pa4]
   ordinal	regression	problems:	map	to	an	ordered set	of	

classes
   a	fairly	obscure	sub-branch	of	statistics,	but	what	we	want	here

   this	formulation	gives	extra	power:

   relations	between	relevance	levels	are	modeled
   documents	are	good	versus	other	documents	for	query	

given	collection;	not	an	absolute	scale	of	goodness

introduction	to	information	retrieval

   learning	to	rank   
   assume	a	number	of	categories	c of	relevance	exist

   these	are	totally	ordered:	c1 <	c2 <	   	<	cj
   this	is	the	ordinal	regression	setup

   assume	training	data	is	available	consisting	of	document-

query	pairs	represented	as	feature	vectors	  i and	
relevance	ranking	ci

   we	could	do	point-wise	learning,	where	we	try	to	map	
items	of	a	certain	relevance	rank	to	a	subinterval	(e.g,	
crammer	et	al.	2002	prank)

   but	most	work	does	pair-wise	learning,	where	the	input	

is	a	pair	of	results	for	a	query,	and	the	class	is	the	
relevance	ordering	relationship	between	them

introduction	to	information	retrieval

point-wise	learning
   goal	is	to	learn	a	threshold	to	separate	each	rank

introduction	to	information	retrieval

sec.	15.4.2

pairwise	learning:	the	ranking	id166	
[herbrich	et	al.	1999,	2000;	joachims	et	al.	2002]
   aim	is	to	classify	instance	pairs	as	correctly	ranked	or	

incorrectly	ranked
   this	turns	an	ordinal	regression	problem	back	into	a	binary	

classification	problem	in	an	expanded	space

   we	want	a	ranking	function	f	such	that

ci >	ck iff f(  i)	>	f(  k)

      	or	at	least	one	that	tries	to	do	this	with	minimal	

error

   suppose	that	f is	a	linear	function	

f(  i)	=	w    i

introduction	to	information	retrieval

sec.	15.4.2

the	ranking	id166	
[herbrich	et	al.	1999,	2000;	joachims	et	al.	2002]
   ranking	model:	f(  i)

f (  i)

    

introduction	to	information	retrieval

sec.	15.4.2

the	ranking	id166	
[herbrich	et	al.	1999,	2000;	joachims	et	al.	2002]
   then	(combining	ci >	ck iff f(  i)	>	f(  k) and	f(  i)	=	w    i):

ci >	ck iff w  (  i    	  k) >	0

   let	us	then	create	a	new	instance	space	from	such	

pairs:

  u =	  (di,	dk,	q)	=	  i    	  k
zu =	+1,	0,	   1	as	ci >,=,<	ck

   we	can	build	model	over	just	cases	for	which	zu =	   1
   from	training	data	s =	{  u},	we	train	an	id166

introduction	to	information	retrieval

two	queries	in	the	original	space

introduction	to	information	retrieval

two	queries	in	the	pairwise	space

introduction	to	information	retrieval

sec.	15.4.2

the	ranking	id166	
[herbrich	et	al.	1999,	2000;	joachims	et	al.	2002]
   the	id166	learning	task	is	then	like	other	examples	

that	we	saw	before

   find	w and	  u    	0	such	that

     wtw	+	c	     u is	minimized,	and
   for	all	  u such	that		zu <	0,			w    u    	1	   	  u

   we	can	just	do	the	negative	zu,	as	ordering	is	

antisymmetric

   you	can	again	use	libid166 or	id166light (or	other	id166	

libraries)	to	train	your	model	(id166rank specialization)

introduction	to	information	retrieval

aside:	the	id166	loss	function
   the	minimization

minw   wtw	+	c   	  u

and	for	all	  u	such	that		zu <	0,	w    u	    	1	   	  u

   can	be	rewritten	as

minw (1/2c)wtw	+	  	  u

and	for	all	  u	such	that		zu <	0,			  u    	1	   	(w    u)

   now,	taking	  	=	1/2c,	we	can	reformulate	this	as	

minw   	[1	   	(w    u)]+ +	  wtw

   where	[]+ is	the	positive	part	(0	if	a	term	is	negative)

introduction	to	information	retrieval

aside:	the	id166	loss	function
   the	reformulation

hinge	loss

regularizer of	   w   

minw    [1	   	(w    u)]+ +	  wtw

   shows	that	an	id166	can	be	thought	of	as	having	an	

empirical	   hinge   	loss combined	with	a	weight	
regularizer (smaller	weights	are	preferred)

loss

1												w    u

introduction	to	information	retrieval

adapting	the	ranking	id166	for	(successful)	
information	retrieval
[yunbo cao,	jun	xu,	tie-yan	liu,	hang	li,	yalou huang,	hsiao-

wuen hon	sigir	2006]

   a	ranking	id166	model	already	works	well

   using	things	like	vector	space	model	scores	as	features	
   as	we	shall	see,	it	outperforms	standard	ir	in	evaluations
   but	it	does	not	model	important	aspects	of	practical	

ir	well	

   this	paper	addresses	two	customizations	of	the	

ranking	id166	to	fit	an	ir	utility	model

introduction	to	information	retrieval

the	ranking	id166	fails	to	model	the	ir	problem	
well	   
1. correctly	ordering	the	most	relevant	documents	is	

crucial	to	the	success	of	an	ir	system,	while	
misordering less	relevant	results	matters	little
   the	ranking	id166	considers	all	ordering	violations	as	the	

same

2. some	queries	have	many	(somewhat)	relevant	

documents,	and	other	queries	few.		if	we	treat	all	
pairs	of	results	for	queries	equally,	queries	with	
many	results	will	dominate	the	learning
  

but	actually	queries	with	few	relevant	results	are	at	least	
as	important	to	do	well	on

introduction	to	information	retrieval

experiments	use	letor	test	collection

  

https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/
   from	microsoft	research	asia
   an	openly	available	standard	test	collection	with	pregenerated

features,	baselines,	and	research	results	for	learning	to	rank
   it   s	availability	has	really	driven	research	in	this	area
   ohsumed,	medline	subcollection for	ir

   350,000	articles
   106	queries
   16,140	query-document	pairs
   3	class	judgments:	definitely	relevant	(dr),	partially	relevant	(pr),	

   trec	gov	collection	(predecessor	of	gov2,	cf.	iir	p.	142)

non-relevant	(nr)

   1	million	web	pages
   125	queries

introduction	to	information	retrieval

principal	components	projection	of	2	queries
[solid	=	q12,	open	=	q50;	circle	=	dr,	square	=	pr,	triangle	=	nr]	

introduction	to	information	retrieval

ranking	scale	importance	discrepancy
[r3	=	definitely	relevant,	r2	=	partially	relevant,	r1	=	nonrelevant]

introduction	to	information	retrieval

number	of	training	documents	per	query	
discrepancy			[solid	=	q12,	open	=	q50]

introduction	to	information	retrieval

ir	evaluation	measures
   some	evaluation	measures	strongly	weight	doing	well	

in	highest	ranked	results:
   map	(mean	average	precision)
   ndcg	(normalized	discounted	cumulative	gain)

   ndcg	has	been	especially	popular	in	machine	learned	

relevance	research
   it	handles	multiple	levels	of	relevance	(map	doesn   t)
   it	seems	to	have	the	right	kinds	of	properties	in	how	it	scores	

system	rankings

introduction	to	information	retrieval

sec.	8.4

normalized	discounted	cumulative	gain	(ndcg)	
evaluation	measure
   query:	
   1)/log(1+ j)
   dcg	at	position	m:	
   ndcg	at	position	m:	average	over	queries
   example

ni = z i

m
   
j=1

(2r( j)

iq

    
   (3,	3,	2,	2,	1,	1,	1)
2r( j)    1
   (7,	7,	3,	3,	1,	1,	1)	
   (1,	0.63,	0.5,	0.43,	0.39,	0.36,	0.33)
   (7,	11.41,	12.91,	14.2,	14.59,	14.95,	15.28)

rank	r
gain

discount

1/ log(1+ j)

m
   
j=1

(2r ( j )

   1) / log(1 + j)

   zi normalizes	against	best	possible	result	for	query,	the	

above,	versus	lower	scores	for	other	rankings
   necessarily:	high	ranking	number	is	good	(more	relevant)

introduction	to	information	retrieval

recap:	two	problems	with	direct	application	of	
the	ranking	id166

   cost	sensitivity:	negative	effects	of	making	errors	on	top	ranked	

documents	

d:	definitely relevant,	p:	partially relevant,	n:	not relevant
ranking	1:	p d p n n n n
ranking	2:	d p n p n n n

   query	id172:	number	of	instance	pairs	varies	according	to	

query

q1:	d p p n n n n
q2:	d d p p p n n n n n
q1	pairs:	2*(d,	p)	+	4*(d,	n)	+	8*(p,	n)	=	14
q2	pairs:	6*(d,	p)	+	10*(d,	n)	+	15*(p,	n)	=	31

introduction	to	information	retrieval

these	problems	are	solved	with	a	new	loss	
function
min   w
     	weights	for	type	of	rank	difference

l(    w) =   k (i)  q(i) 1    zi

   w,    xi

(1)    

   

+  

#
$
+

   xi

!
"

i=1

(2)

l

   w 2

   estimated	empirically	from	effect	on	ndcg

     	weights	for	size	of	ranked	result	set
   linearly	scaled	versus	biggest	result	set	

introduction	to	information	retrieval

optimization	(gradient	descent)

introduction	to	information	retrieval

optimization	(quadratic	programming)

min   w

l =   k (i)  q(i) 1    zi

   

!
"

   

i=1

   w,    xi

(1)    

(2)

   xi

   w 2 ,

+  

#
$+

   

1
2

   w 2

m (    w) =

min   w
subject to  i     0,
where ci =

+ ci      i
   
   w,    xi
zi
(1)    
  k (i)  q(i)

i=1

2  

   xi
(2)    1     i

i =1,   ,   

   

  

ld =   i
       

max   
   
i=1
subject to 0      i     ci

i=1

   

1
2

   

  i  i'zizi'
   
i'=1
i =1,   ,   

   xi
(1)    

   xi
(2),    xi'

(1)    

(2)

   xi'

introduction	to	information	retrieval

experiments
   ohsumed	(from	letor)	
   features:

   6	that	represent	versions	of	tf,	idf,	and	tf.idf factors
   bm25	score	(iir	sec.	11.4.3)

introduction	to	information	retrieval

experimental	results	(ohsumed)

introduction	to	information	retrieval

msn	search	[now	bing]
   second	experiment	with	msn	search
   collection	of	2198	queries
   6	relevance	levels	rated:

   definitive
   excellent
   good
   fair
   bad
   detrimental

8990
4403
3735
20463
36375
310

introduction	to	information	retrieval

experimental	results	(msn	search)

introduction	to	information	retrieval

alternative:	optimizing	rank-based	measures
[yue	et	al.	sigir	2007]
   if	we	think	that	ndcg	is	a	good	approximation	of	the	

user   s	utility	function	from	a	result	ranking
   then,	let   s	directly	optimize	this	measure

   as	opposed	to	some	proxy	(weighted	pairwise	prefs)

   but,	there	are	problems	   

   objective	function	no	longer	decomposes

   pairwise	prefs	decomposed	into	each	pair

   objective	function	is	flat	or	discontinuous

introduction	to	information	retrieval

discontinuity	example
   ndcg	computed	using	rank	positions
   ranking	via	retrieval	scores
   slight	changes	to	model	parameters	

   slight	changes	to	retrieval	scores
   no	change	to	ranking
   no	change	to	ndcg

ndcg	=	0.63

ndcg	discontinuous	w.r.t	
model	parameters!

retrieval score
rank
relevance

d2

d1
d3
0.9 0.6 0.3
1
0

2
1

3
0

introduction	to	information	retrieval

structural	id166s				[tsochantaridis	et	al.,	2007]
   structural	id166s	are	a	generalization	of	id166s	where	the	output	

classification	space	is	not	binary	or	one	of	a	set	of	classes,	but	some	
complex	object	(such	as	a	sequence	or	a	parse	tree)

   here,	it	is	a	complete	(weak)	ranking	of	documents	for	a	query
   the	structural	id166	attempts	to	predict	the	complete	ranking	for	

the	input	query	and	document	set

   the	true	labeling is	a	ranking	where	the	relevant	documents	are	all	

ranked	in	the	front,	e.g.,

   an	incorrect	labeling would	be	any	other	ranking,	e.g.,

   there	are	an	intractable	number	of	rankings,	thus	an	intractable	

number	of	constraints!

introduction	to	information	retrieval

structural	id166	training	
[tsochantaridis	et	al.,	2007]
structural	id166	training	proceeds	incrementally	by	starting	with	a	working	set	of	
constraints,	and	adding	in	the	most	violated	constraint	at	each	iteration

original	id166	problem
   exponential	constraints
   most	are	dominated	by	a	small	
set	of	   important    constraints

structural	id166	approach
  

repeatedly	finds	the	next	most	violated	
constraint   

      until	a	set	of	constraints	which	is	a	

good	approximation	is	found

introduction	to	information	retrieval

other	machine	learning	methods	for	learning	to	
rank
   of	course!
   i   ve	only	presented	the	use	of	id166s	for	machine	

learned	relevance,	but	other	machine	learning	
methods	have	also	been	used	successfully
   boosting:	rankboost
   ordinal	regression	loglinear models
   neural	nets:	ranknet,	rankbrain
   (gradient-boosted)	decision	trees

introduction	to	information	retrieval

the	limitations	of	machine	learning
   everything	that	we	have	looked	at	(and	most	work	in	

this	area)	produces	linear models	over	features	
   this	contrasts	with	most	of	the	clever	ideas	of	
traditional	ir,	which	are	nonlinear scalings and	
combinations	(products,	etc.)	of	basic	measurements
   log	term	frequency,	idf,	tf.idf,	pivoted	length	id172
   at	present,	ml	is	good	at	weighting	features,	but	not	

as	good	at	coming	up	with	nonlinear	scalings
   designing	the	basic	features	that	give	good	signals	for	

ranking	remains	the	domain	of	human	creativity

   or	maybe	we	can	do	it	with	deep	learning	j

introduction	to	information	retrieval

http://www.quora.com/why-is-machine-learning-used-heavily-for-googles-ad-ranking-and-less-for-their-search-ranking

introduction	to	information	retrieval

summary
   the	idea	of	learning	ranking	functions	has	been	around	

for	about	20	years

   but	only	more	recently	have	ml	knowledge,	availability	

of	training	datasets,	a	rich	space	of	features,	and	massive	
computation	come	together	to	make	this	a	hot	research	
area

   it   s	too	early	to	give	a	definitive	statement	on	what	

methods	are	best	in	this	area	   	it   s	still	advancing	rapidly

   but	machine-learned	ranking	over	many	features	now	

easily	beats	traditional	hand-designed	ranking	functions	
in	comparative	evaluations		[in	part	by	using	the	hand-designed	functions	as	features!]
   there	is	every	reason	to	think	that	the	importance	of	

machine	learning	in	ir	will	grow	in	the	future.

introduction	to	information	retrieval

resources
   iir secs	6.1.2   3	and	15.4
   letor	benchmark	datasets

   website	with	data,	links	to	papers,	benchmarks,	etc.
   http://research.microsoft.com/users/letor/
   everything	you	need	to	start	research	in	this	area!

   nallapati,	r.	discriminative	models	for	information	

retrieval.	sigir	2004.

   cao,	y.,	xu,	j.	liu,	t.-y.,	li,	h.,	huang,	y.	and	hon,	h.-w.	
adapting	ranking	id166	to	document	retrieval,	sigir	
2006.	

   y.	yue,	t.	finley,	f.	radlinski,	t.	joachims.	a	support	vector	

method	for	optimizing	average	precision.	sigir	2007.

