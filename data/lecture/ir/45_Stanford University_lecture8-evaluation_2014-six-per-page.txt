introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

introduc*on	to	
informa(on	retrieval	

cs276	

informa*on	retrieval	and	web	search	

chris	manning	and	pandu	nayak	

evalua*on	

situa*on	
      thanks	to	your	stellar	performance	in	cs276,	you	
quickly	rise	to	vp	of	search	at	internet	retail	giant	
nozama.com.	your	boss	brings	in	her	nephew	sergey,	
who	claims	to	have	built	a	beler	search	engine	for	
nozama.	do	you	
      laugh	derisively	and	send	him	to	rival	tramlaw	labs?	
      counsel	sergey	to	go	to	stanford	and	take	cs276?	
      try	a	few	queries	on	his	engine	and	say	   not	bad   ?	
         	?	

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.6 

introduc)on	to	informa)on	retrieval	

		

		

what	could	you	ask	sergey?	
      how	fast	does	it	index?	

      number	of	documents/hour	
      incremental	indexing	   	nozama	adds	10k	products/day	

      how	fast	does	it	search?	

      latency	and	cpu	needs	for	nozama   s	5	million	products	

      does	it	recommend	related	products?	
      this	is	all	good,	but	it	says	nothing	about	the	quality	

of	sergey   s	search	
      you	want	nozama   s	users	to	be	happy	with	the	search	

experience	

how	do	you	tell	if	users	are	happy?	
      search	returns	products	relevant	to	users	

      how	do	you	assess	this	at	scale?	
      search	results	get	clicked	a	lot	

      misleading	*tles/summaries	can	cause	users	to	click	

      users	buy	a^er	using	the	search	engine	

      or,	users	spend	a	lot	of	$	a^er	using	the	search	engine	

      repeat	visitors/buyers	

      do	users	leave	soon	a^er	searching?	
      do	they	come	back	within	a	week/month/   	?	

3	

2	

4	

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.1 

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.1 

happiness:	elusive	to	measure	
      most	common	proxy:	relevance	of	search	results	

      but	how	do	you	measure	relevance?	

      pioneered	by	cyril	cleverdon	in	the	cran   eld	

experiments	

measuring	relevance	
      three	elements:	

1.    a	benchmark	document	collec*on		
2.    a	benchmark	suite	of	queries	
3.    an	assessment	of	either	relevant	or	nonrelevant	for	

each	query	and	each	document	

5	

6	

1 

introduc)on	to	informa)on	retrieval	

		

		

so	you	want	to	measure	the	quality	of	
a	new	search	algorithm	
      benchmark	documents	   	nozama   s	products	
      benchmark	query	suite	   	more	on	this	
      judgments	of	document	relevance	for	each	query	

5 million nozama.com products 

relevance	
judgement	

50000 
sample  
queries 

introduc)on	to	informa)on	retrieval	

		

		

relevance	judgments	
      binary	(relevant	vs.	non-relevant)	in	the	simplest	

case,	more	nuanced	(0,	1,	2,	3	   )	in	others	

      what	are	some	issues	already?	
      5	million	*mes	50k	takes	us	into	the	range	of	a	

quarter	trillion	judgments	
      if	each	judgment	took	a	human	2.5	seconds,	we   d	s*ll	

need	1011	seconds,	or	nearly	$300	million	if	you	pay	
people	$10	per	hour	to	assess	

      10k	new	products	per	day	

7	

8	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.1 

crowd	source	relevance	judgments?	
      present	query-document	pairs	to	low-cost	labor	on	

online	crowd-sourcing	plalorms	
      hope	that	this	is	cheaper	than	hiring	quali   ed	assessors	
      lots	of	literature	on	using	crowd-sourcing	for	such	

tasks	

      main	takeaway	   	you	get	some	signal,	but	the	
variance	in	the	resul*ng	judgments	is	very	high	

evalua*ng	an	ir	system	
      note:	user	need	is	translated	into	a	query	
      relevance	is	assessed	rela*ve	to	the	user	need,	not	

the	query	

      e.g.,	informa*on	need:	my	swimming	pool	bo;om	is	

becoming	black	and	needs	to	be	cleaned.	

      query:	pool	cleaner	
      assess	whether	the	doc	addresses	the	underlying	

need,	not	whether	it	has	these	words	

9	

10	

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.5 

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.5 

some	public	test	collec*ons	

what	else?	
      s*ll	need	test	queries	

      must	be	germane	to	docs	available	
      must	be	representa*ve	of	actual	user	needs	
      random	query	terms	from	the	documents	generally	not	a	

good	idea	

      sample	from	query	logs	if	available	

      classically	(non-web)	

      low	query	rates	   	not	enough	query	logs	
      experts	hand-cra^	   user	needs   	

11	

typical	
trec	

12	

2 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.3 

now	we	have	the	basics	of	a	benchmark	
      let   s	review	some	evalua*on	measures	

      precision	
      recall	
      ndcg	
         		

13	

unranked	retrieval	evalua*on:	
precision	and	recall	   	recap	from	iir	8/video	
      binary	assessments	
precision:	frac*on	of	retrieved	docs	that	are	relevant	=	

p(relevant|retrieved)	

recall:	frac*on	of	relevant	docs	that	are	retrieved	

	=	p(retrieved|relevant)	

	
	

retrieved 
not retrieved 

relevant 
tp 
fn 

nonrelevant 
fp 
tn 

      precision	p	=	tp/(tp	+	fp)	
      recall						r	=	tp/(tp	+	fn)	

14	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

rank-based measures 

      binary relevance 

      precision@k (p@k) 
      mean average precision (map) 
      mean reciprocal rank (mrr) 

      multiple levels of relevance 

      normalized discounted cumulative gain (ndcg) 

precision@k 
      set a rank threshold k 
      compute % relevant in top k 
      ignores documents ranked lower than k 
      ex:                   

      prec@3 of 2/3  
      prec@4 of 2/4 
      prec@5 of 3/5 

      in similar fashion we have recall@k 

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.4 

introduc)on	to	informa)on	retrieval	

		

a	precision-recall	curve	

i

i

n
o
s
c
e
r
p

1.0

0.8

0.6

0.4

0.2

0.0

lots more detail on this in the 
coursera video 

0.0

0.2

0.4

0.6

0.8

1.0

recall

17	

mean average precision 
      consider rank position of each relevant doc 

      k1, k2,     kr 

      compute precision@k for each k1, k2,     kr 
      average precision = average of p@k 

      ex:                    has avgprec of 

1
3

   

1
1

++

2
3

3
5

   
   
   

   
      
   

      map is average precision across multiple queries/

rankings 

		

		

76.0

3 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

average precision 

map 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

mean	average	precision	

      if a relevant document never gets retrieved, we 

assume the precision corresponding to that relevant 
doc to be zero  

      map is macro-averaging: each query counts equally 
      now perhaps most commonly used measure in 

research papers 

      good for web search? 
      map assumes user is interested in finding many 

relevant documents for each query 

      map requires many relevance judgments in text 

collection 

beyond	binary	relevance	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

		

		

22	

fair	

fair	

good	

discounted cumulative gain 
      popular measure for evaluating web search and 

related tasks 

      two assumptions: 

      highly relevant documents are more useful 
than marginally relevant documents 
      the lower the ranked position of a relevant 
document, the less useful it is for the user, 
since it is less likely to be examined 

4 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

discounted cumulative gain 
      uses graded relevance as a measure of  

usefulness, or gain, from examining a document 

      gain is accumulated starting at the top of the 
ranking and may be reduced, or discounted, at 
lower ranks 

      typical discount is 1/log (rank) 

      with base 2, the discount at rank 4 is 1/2, and 
at rank 8 it is 1/3 

summarize a ranking: dcg 

      what if relevance judgments are in a scale of [0,r]? 

r>2 

      cumulative gain (cg) at rank n 

      let the ratings of the n documents be r1, r2,    rn 
(in ranked order) 
      cg = r1+r2+   rn 

      discounted cumulative gain (dcg) at rank n 
      dcg = r1 + r2/log22 + r3/log23 +     rn/log2n 

      we may use any base for the logarithm 

26 
 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

discounted cumulative gain 
      dcg is the total gain accumulated at a particular 

rank p: 

      alternative formulation: 

      used by some web search companies 
      emphasis on retrieving highly relevant documents 

dcg example 
      10 ranked documents judged on 0-3 relevance 

scale:  
3, 2, 3, 0, 0, 1, 2, 2, 3, 0 

      discounted gain:  

3, 2/1, 3/1.59, 0, 0, 1/2.59, 2/2.81, 2/3, 3/3.17, 0  
= 3, 2, 1.89, 0, 0, 0.39, 0.71, 0.67, 0.95, 0 

      dcg: 

3, 5, 6.89, 6.89, 6.89, 7.28, 7.99, 8.66, 9.61, 9.61 

 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

summarize a ranking: ndcg 

ndcg - example 

      normalized discounted cumulative gain (ndcg) 
at rank n 
      normalize dcg at rank n by the dcg value at 
rank n of the ideal ranking 
      the ideal ranking would first return the 
documents with the highest relevance level, 
then the next highest relevance level, etc 

      id172 useful for contrasting queries 

with varying numbers of relevant results 
 
      ndcg is now quite popular in evaluating web 

search 

29 
 

maxdcg

4 documents: d1, d2, d3, d4 

ground	truth	

ranking	func*on1	

ranking	func*on2	

document	

order	

ri	

document	

order	

ri	

document	

order	

ri	

2	
d4	
2	
d3	
1	
d2	
d1	
0	
ndcggt=1.00	

2	
d3	
2	
d4	
1	
d2	
d1	
0	
ndcgrf1=1.00	

2	
d3	
1	
d2	
2	
d4	
d1	
0	
ndcgrf2=0.9203	

i	

1	
2	
3	
4	

dcg

+=gt

2

dcg

+=rf

1

2

dcg

+=rf

2

2

2
log
2
2
log
2
1
log

   
      
   
   
      
   
   
      
   

2

+

2

+

2

+

2

=

1
log
2
1
log
2
2
log

2

+

3

+

3

+

3

4

4

0
log
2
0
log
2
0
log

   
=      
   
   
=      
   
   
=      
   
6309

4

2

dcg

gt

.4=

.4

6309

.4

6309

.4

2619

5 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

what	if	the	results	are	not	in	a	list?	
      suppose	there   s	only	one	relevant	document	
      scenarios:		

      known-item	search	
      naviga*onal	queries	
      looking	for	a	fact	

      search	dura*on	~	rank	of	the	answer		

      measures	a	user   s	e   ort	

mean reciprocal rank 

      consider rank position, k, of first relevant doc 

      could be     only clicked doc 

      reciprocal rank score = 
 
      mrr is the mean rr across multiple queries    

1
k

	

31	
	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

human	judgments	are	
      expensive	
      inconsistent	

      between	raters	
      over	*me	

      decay	in	value	as	documents/query	mix	evolves	
      not	always	representa*ve	of	   real	users   	

      ra*ng	vis-  -vis	query,	vs	underlying	need	

      so	   	what	alterna*ves	do	we	have?	

using	user	clicks	

33	
	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

what	do	clicks	tell	us?	

rela*ve	vs	absolute	ra*ngs	

#	of	clicks	received	

		

		

34	

user   s click 
sequence 

strong position bias, so absolute click rates unreliable 

35	

hard to conclude result1 > result3 
probably can conclude result3 > result2 

36	

6 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		
sec. 8.6.3 

pairwise	rela*ve	ra*ngs	
      pairs	of	the	form:	doca	beler	than	docb	for	a	query	

      doesn   t	mean	that	doca	relevant	to	query	

      now,	rather	than	assess	a	rank-ordering	wrt	per-doc	

relevance	assessments	

      assess	in	terms	of	conformance	with	historical	
pairwise	preferences	recorded	from	user	clicks	

a/b	tes*ng	at	web	search	engines	
      purpose:	test	a	single	innova*on	
      prerequisite:	you	have	a	large	search	engine	up	and	

	

running.	

      have	most	users	use	old	system	
      divert	a	small	propor*on	of	tra   c	(e.g.,	1%)	to	an	

experiment	to	evaluate	an	innova*on	
      full	page	experiment	
      interleaved	experiment	

37	

		

introduc)on	to	informa)on	retrieval	

		

interleave	the	two	rankings	

introduc)on	to	informa)on	retrieval	

		

comparing	two	rankings	via	clicks	
(joachims	2002)	

query: [support vector machines] 

ranking a 

kernel	machines	

id166-light

		
lucent	id166	demo	
royal	holl.	id166	
id166	so^ware	
id166	tutorial	

ranking b 

kernel	machines	

id166s	

intro	to	id166s	
archives	of	id166	

id166-light	

id166	so^ware	

39	

this interleaving 
starts with b 

kernel	machines	
kernel	machines	

id166s	
id166-light

		

intro	to	id166s	

lucent	id166	demo	
archives	of	id166	
royal	holl.	id166	

id166-light	

    

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

remove	duplicate	results	

count	user	clicks	

38	

40	

		

		

kernel	machines	
kernel	machines	

id166s	
id166-light

		

intro	to	id166s	

lucent	id166	demo	
archives	of	id166	
royal	holl.	id166	

id166-light	

    

ranking a: 3 
ranking b: 1 

kernel	machines	
kernel	machines	

id166s	
id166-light

		

intro	to	id166s	

lucent	id166	demo	
archives	of	id166	
royal	holl.	id166	

id166-light	

    

a, b 

clicks 

a 

a 

42	

41	

7 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

interleaved	ranking		
      present	interleaved	ranking	to	users	

      start	randomly	with	ranking	a	or	ranking	b	to	evens	out	

presenta*on	bias	

facts/en**es	(what	happens	to	clicks?)	

      count	clicks	on	results	from	a	versus	results	from	b	

	

      beler	ranking	will	(on	average)	get	more	clicks	

44	

46	

43	

introduc)on	to	informa)on	retrieval	

		

		

kendall	tau	distance	
      let	x	be	the	number	of	agreements	between	a	

ranking	(say	a)	and	p	

      let	y	be	the	number	of	disagreements	
      then	the	kendall	tau	distance	between	a	and	p	is	
	(x-y)/(x+y)	
      say	p	=	{(1,2),	(1,3),	(1,4),	(2,3),	(2,4),	(3,4))}	and	

a=(1,3,2,4)	

      then	x=5,	y=1	   	
      (what	are	the	minimum	and	maximum	possible	

values	of	the	kendall	tau	distance?)	

introduc)on	to	informa)on	retrieval	

		

		

comparing	two	rankings	to	a	baseline	
ranking	
      given	a	set	of	pairwise	preferences	p	
      we	want	to	measure	two	rankings	a	and	b	
      de   ne	a	proximity	measure	between	a	and	p	

      and	likewise,	between	b	and	p	

      want	to	declare	the	ranking	with	beler	proximity	to	

be	the	winner	

      proximity	measure	should	reward	agreements	with	p	

and	penalize	disagreements	

45	

introduc)on	to	informa)on	retrieval	

		

		

recap	
      benchmarks	consist	of	
      document	collec*on	
      query	set	
      assessment	methodology	

      assessment	methodology	can	use	raters,	user	clicks,	

or	a	combina*on	
      these	get	quan*zed	into	a	goodness	measure	   	precision/

ndcg	etc.	

      di   erent	engines/algorithms	compared	on	a	benchmark	

together	with	a	goodness	measure	

47	

8 

