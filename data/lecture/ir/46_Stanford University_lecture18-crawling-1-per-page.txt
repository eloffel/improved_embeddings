introduction	to	information	retrieval

introduction	to
information	retrieval

cs276

information	retrieval	and	web	search

chris	manning	and	pandu	nayak

crawling	and	duplicates

introduction	to	information	retrieval

today   s	lecture
   web	crawling
   (near)	duplicate	detection

2

introduction	to	information	retrieval

sec. 20.2

basic	crawler	operation
   begin	with	known	   seed    urls
   fetch	and	parse	them

   extract	urls	they	point	to
   place	the	extracted	urls	on	a	queue

   fetch	each	url	on	the	queue	and	

repeat

3

introduction	to	information	retrieval

sec. 20.2

crawling	picture

urls crawled
and parsed

unseen web

urls frontier

seed
pages

web

4

introduction	to	information	retrieval

sec. 20.1.1

simple	picture	    complications
   web	crawling	isn   t	feasible	with	one	machine

   all	of	the	above	steps	distributed

   malicious	pages

   spam	pages	
   spider	traps	    incl	dynamically	generated

   even	non-malicious	pages	pose	challenges

   latency/bandwidth	to	remote	servers	vary
   webmasters    stipulations

   how	   deep    should	you	crawl	a	site   s	url	hierarchy?

   site	mirrors	and	duplicate	pages

   politeness	    don   t	hit	a	server	too	often

5

introduction	to	information	retrieval

sec. 20.1.1

what	any	crawler	must do
   be	robust:	be	immune	to	spider	traps	and	
other	malicious	behavior	from	web	servers

   be	polite:	respect	implicit	and	explicit	

politeness	considerations

6

introduction	to	information	retrieval

sec. 20.2

explicit	and	implicit	politeness
   explicit	politeness:	specifications	from	

webmasters	on	what	portions	of	site	can	be	
crawled
   robots.txt

   implicit	politeness:	even	with	no	

specification,	avoid	hitting	any	site	too	
often

7

introduction	to	information	retrieval

sec. 20.2.1

robots.txt
   protocol	for	giving	spiders	(   robots   )	limited	

access	to	a	website,	originally	from	1994
   www.robotstxt.org/robotstxt.html

   website	announces	its	request	on	what	can(not)	

be	crawled
   for	a	server,	create	a	file	/robots.txt
   this	file	specifies	access	restrictions

8

introduction	to	information	retrieval

sec. 20.2.1

robots.txt	example
   no	robot	should	visit	any	url	starting	with	
"/yoursite/temp/",	except	the	robot	called	
   searchengine":	

user-agent: *
disallow: /yoursite/temp/ 

user-agent: searchengine
disallow:

9

introduction	to	information	retrieval

sec. 20.1.1

what	any	crawler	should do
   be	capable	of	distributed operation:	designed	to	

run	on	multiple	distributed	machines

   be	scalable:	designed	to	increase	the	crawl	rate	

by	adding	more	machines

   performance/efficiency:	permit	full	use	of	
available	processing	and	network	resources

10

introduction	to	information	retrieval

sec. 20.1.1

what	any	crawler	should do
   fetch	pages	of	   higher	quality    first
   continuous operation:	continue	fetching	
fresh	copies	of	a	previously	fetched	page
   extensible:	adapt	to	new	data	formats,	

protocols

11

introduction	to	information	retrieval

sec. 20.1.1

updated	crawling	picture

urls crawled
and parsed

seed
pages

unseen web

url frontier

crawling thread

12

introduction	to	information	retrieval

sec. 20.2

url	frontier
   can	include	multiple	pages	from	the	same	

host

   must	avoid	trying	to	fetch	them	all	at	the	

same	time

   must	try	to	keep	all	crawling	threads	busy

13

introduction	to	information	retrieval

sec. 20.2.1

processing	steps	in	crawling
   pick	a	url	from	the	frontier
   fetch	the	document	at	the	url
   parse	the	url

which one?

   extract	links	from	it	to	other	docs	(urls)
   check	if	url	has	content	already	seen

   if	not,	add	to	indexes

   for	each	extracted	url

e.g., only crawl .edu, 
obey robots.txt, etc.

   ensure	it	passes	certain	url	filter	tests
   check	if	it	is	already	in	the	frontier	(duplicate	url	

elimination)

14

introduction	to	information	retrieval

sec. 20.2.1

basic	crawl	architecture

dns

doc
fp   s

robots
filters

url
set

www

parse

fetch

content

seen?

url
filter

dup
url
elim

url frontier

15

introduction	to	information	retrieval

sec. 20.2.2

dns	(domain	name	server)
   a	lookup	service	on	the	internet
   given	a	url,	retrieve	its	ip	address
   service	provided	by	a	distributed	set	of	servers	    thus,	

lookup	latencies	can	be	high	(even	seconds)

   common	os	implementations	of	dns	lookup	are	
blocking:	only	one	outstanding	request	at	a	time

   solutions

   dns	caching
   batch	dns	resolver	    collects	requests	and	sends	them	out	

together

16

introduction	to	information	retrieval

sec. 20.2.1

parsing:	url	id172

   when	a	fetched	document	is	parsed,	some	of	the	

extracted	links	are	relative urls

   e.g.,	http://en.wikipedia.org/wiki/main_page has	a	
relative	link	to	/wiki/wikipedia:general_disclaimer	
which	is	the	same	as	the	absolute	url	
http://en.wikipedia.org/wiki/wikipedia:general_disclaimer

   during	parsing,	must	normalize	(expand)	such	relative	

urls

17

introduction	to	information	retrieval

sec. 20.2.1

content	seen?
   duplication	is	widespread	on	the	web
   if	the	page	just	fetched	is	already	in	
the	index,	do	not	further	process	it

   this	is	verified	using	document	

fingerprints	or	shingles
   second	part	of	this	lecture

18

introduction	to	information	retrieval

sec. 20.2.1

filters	and	robots.txt	
   filters     regular	expressions	for	urls	to	

be	crawled/not

   once	a	robots.txt	file	is	fetched	from	a	

site,	need	not	fetch	it	repeatedly
   doing	so	burns	bandwidth,	hits	web	

server

   cache	robots.txt	files

19

introduction	to	information	retrieval

sec. 20.2.1

duplicate	url	elimination
   for	a	non-continuous	(one-shot)	crawl,	test	

to	see	if	an	extracted+filtered	url	has	
already	been	passed	to	the	frontier

   for	a	continuous	crawl	    see	details	of	

frontier	implementation

20

introduction	to	information	retrieval

sec. 20.2.1

distributing	the	crawler
   run	multiple	crawl	threads,	under	different	

processes	    potentially	at	different	nodes
   geographically	distributed	nodes

   partition	hosts	being	crawled	into	nodes

   hash	used	for	partition

   how	do	these	nodes	communicate	and	share	

urls?

21

introduction	to	information	retrieval

sec. 20.2.1

communication	between	nodes
   output	of	the	url	filter	at	each	node	is	sent	to	the	

dup	url	eliminator	of	the	appropriate	node

dns

doc
fp   s

robots
filters

to
other
nodes

www

parse

fetch

content

seen?

url
filter

url frontier

host

splitter

from
other
nodes

url
set

dup
url
elim

22

introduction	to	information	retrieval

sec. 20.2.3

url	frontier:	two	main	considerations

   politeness:	do	not	hit	a	web	server	too	frequently
   freshness:	crawl	some	pages	more	often	than	

others
   e.g.,	pages	(such	as	news	sites)	whose	content	

changes	often

these	goals	may	conflict	with	each	other.
(e.g.,	simple	priority	queue	fails	    many	links	out	of	

a	page	go	to	its	own	site,	creating	a	burst	of	
accesses	to	that	site.)

23

introduction	to	information	retrieval

sec. 20.2.3

politeness	    challenges
   even	if	we	restrict	only	one	thread	to	fetch	

from	a	host,	can	hit	it	repeatedly

   common	heuristic:	insert	time	gap	between	
successive	requests	to	a	host	that	is	>>	time	
for	most	recent	fetch	from	that	host

24

introduction	to	information	retrieval

sec. 20.2.3

url	frontier:	mercator	scheme

urls

prioritizer

k front queues

biased front queue selector

back queue router

b back queues

single host on each

back queue selector

crawl thread requesting url

25

introduction	to	information	retrieval

sec. 20.2.3

mercator	url	frontier
   urls	flow	in	from	the	top	into	the	frontier
   front	queues manage	prioritization
   back	queues enforce	politeness
   each	queue	is	fifo

26

introduction	to	information	retrieval

sec. 20.2.3

front	queues

prioritizer

1

biased front queue selector

back queue router

k

27

introduction	to	information	retrieval

sec. 20.2.3

front	queues
   prioritizer	assigns	to	url	an	integer	priority	

between	1	and	k
   appends	url	to	corresponding	queue

   heuristics	for	assigning	priority

   refresh	rate	sampled	from	previous	crawls
   application-specific	(e.g.,	   crawl	news	sites	more	

often   )

28

introduction	to	information	retrieval

sec. 20.2.3

biased	front	queue	selector
   when	a	back	queue requests	a	url	(in	a	

sequence	to	be	described):	picks	a	front	queue
from	which	to	pull	a	url

   this	choice	can	be	round	robin	biased	to	queues	

of	higher	priority,	or	some	more	sophisticated	
variant
   can	be	randomized

29

introduction	to	information	retrieval

sec. 20.2.3

back	queues

biased front queue selector

back queue router

1

b

back queue selector

heap

30

introduction	to	information	retrieval

sec. 20.2.3

back	queue	invariants

   each	back	queue	is	kept	non-empty	while	the	

crawl	is	in	progress

   each	back	queue	only	contains	urls	from	a	

single	host
   maintain	a	table	from	hosts	to	back	queues

host name

back queue

    

3

1

b

31

introduction	to	information	retrieval

sec. 20.2.3

back	queue	heap
   one	entry	for	each	back	queue
   the	entry	is	the	earliest	time	te at	which	the	host	
corresponding	to	the	back	queue	can	be	hit	again

   this	earliest	time	is	determined	from

   last	access	to	that	host
   any	time	buffer	heuristic	we	choose

32

introduction	to	information	retrieval

sec. 20.2.3

back	queue	processing

   a	crawler	thread	seeking	a	url	to	crawl:
   extracts	the	root	of	the	heap
   fetches	url	at	head	of	corresponding	back	queue	q

(look	up	from	table)

   checks	if	queue	q is	now	empty	    if	so,	pulls	a	url	v

from	front	queues
   if	there   s	already	a	back	queue	for	v   s	host,	append	v to	it	

and	pull	another	url	from	front	queues,	repeat

   else	add	v to	q

   when	q is	non-empty,	create	heap	entry	for	it

33

introduction	to	information	retrieval

sec. 20.2.3

number	of	back	queues	b
   keep	all	threads	busy	while	respecting	politeness
   mercator	recommendation:	three	times	as	many	

back	queues	as	crawler	threads

34

introduction	to	information	retrieval

introduction	to
information	retrieval

near	duplicate	

document	detection

35

introduction	to	information	retrieval

sec. 19.6

duplicate	documents
   the	web	is	full	of	duplicated	content
   strict	duplicate	detection	=	exact	match

   not	as	common

   but	many,	many	cases	of	near	duplicates

   e.g.,	last	modified	date	the	only	difference	

between	two	copies	of	a	page

introduction	to	information	retrieval

sec. 19.6

duplicate/near-duplicate	detection

   duplication:	exact	match		can	be	detected	with	

fingerprints

   near-duplication:	approximate	match

   overview

   compute	syntactic	similarity	with	an	edit-distance	

measure

   use	similarity	threshold	to	detect	near-duplicates

   e.g.,		similarity	>	80%	=>	documents	are	   near	duplicates   
   not	transitive	though	sometimes	used	transitively

introduction	to	information	retrieval

sec. 19.6

computing	similarity
   features:

   segments	of	a	document	(natural	or	artificial	breakpoints)
   shingles (word	id165s)
   a	rose	is	a	rose	is	a	rose    	4-grams	are

a_rose_is_a

rose_is_a_rose

is_a_rose_is	

a_rose_is_a

   similarity	measure	between	two	docs	(=	sets	of	shingles)

   jaccard	cooefficient:	(size_of_intersection	/	size_of_union)

introduction	to	information	retrieval

sec. 19.6

shingles	+	set	intersection
   computing	exact set	intersection	of	shingles	
between	all pairs	of	documents	is	expensive

  approximate	using	a	cleverly	chosen	subset	of	
shingles	from	each	(a	sketch)
   estimate	(size_of_intersection	/	size_of_union)
based	on	a	short	sketch	

doc 
a

doc 
b

shingle set a

sketch a

shingle set b

sketch b

jaccard

introduction	to	information	retrieval

sec. 19.6

sketch	of	a	document
   create	a	   sketch	vector    (of	size	~200)	for	

each	document
   documents	that	share	    t (say	80%)	

corresponding	vector	elements	are	deemed	
near	duplicates
   for	doc	d,	sketchd[	i	]	is	as	follows:

   let	f	map	all	shingles	in	the	universe	to	1..2m
(e.g.,	f	=	fingerprinting)
   let	pi be	a	random	permutation on	1..2m
   pick	min	{pi(f(s))}		over	all	shingles	s in	d

introduction	to	information	retrieval

sec. 19.6

computing	sketch[i]	for	doc1

document 1

264
264
264
264

start with 64-bit f(shingles)
permute on the number line
with pi

pick the min value

introduction	to	information	retrieval

sec. 19.6

test	if	doc1.sketch[i]	=	doc2.sketch[i]	

document 1

document 2

264
264
264
264

b

264
264
264
264

a

are these equal?

test for 200 random permutations: p1, p2,    p200

introduction	to	information	retrieval

sec. 19.6

however   

document 1

document 2

a

264
264
264
264

b

264
264
264
264

a = b iff the shingle with the min value in the union of 
doc1 and doc2 is common to both (i.e., lies in the 
intersection)

claim: this happens with id203

size_of_intersection / size_of_union

why?

introduction	to	information	retrieval

sec. 19.6

set	similarity	of	sets	ci ,	cj
cc
"
cc
!

jaccard(c

)c,
j

=

i

i

i

j

j

   view sets as columns of a matrix a; one row for each 
element in the universe.  aij = 1 indicates presence of 
item i  in set j

   example

c1 c2
0     1
1    0
1    1        jaccard(c1,c2) = 2/5 = 0.4
0    0
1    1
0    1

introduction	to	information	retrieval

sec. 19.6

key	observation
   for	columns	ci,	cj, four	types	of	rows

ci cj
a 1 1
b
1 0
c
0 1
d 0 0

   overload	notation: a	=	#	of	rows	of	type	a
   claim

jaccard(c

)c,
j

i

=

a
++

cba

introduction	to	information	retrieval

sec. 19.6

   min    hashing
   randomly permute rows
   hash h(ci) = index of first row with 1 in column 

ci

   surprising property
h(c

[
h(c  p

=

)

i

   why?

]

 )

j

=

jaccard

(
c,c

i

)j

   both are a/(a+b+c)
   look down columns ci, cj until first non-type-d row
   h(ci) = h(cj)      type a row

introduction	to	information	retrieval

random	permutations
   random	permutations	are	expensive	to	compute

   linear	permutations	work	well	in	practice

   for	a	large	prime	p,	consider	permutations	over	{0,    , p     1}	

drawn	from	the	set:

fp =	{pa,b :	1    a     p     1, 0     b     p     1} where

pa,b(x) = ax + b mod p

47

introduction	to	information	retrieval

final	notes
   shingling	is	a	randomized	algorithm

   our	analysis	did	not	presume	any	id203	model	on	the	

inputs

   it	will	give	us	the	right	(wrong)	answer	with	some	

id203	on	any	input

   we   ve	described	how	to	detect	near	duplication	in	a	

pair	of	documents

   in	   real	life    we   ll	have	to	concurrently	look	at	many	

pairs
   see	text	book	for	details

48

