introduction	to	information	retrieval

introduction	to
information	retrieval

cs276:	information	retrieval	and	web	search

christopher	manning	and	pandu	nayak

lecture	13:	support	vector	machines	and	

machine	learning	on	documents

[borrows slides from ray mooney]

introduction	to	information	retrieval

text	classification
   last	lecture:	basic	algorithms	for	text	classification

   naive	bayes	classifier

   simple,	cheap,	high	bias,	linear

   k	nearest	neighbor	classification

   simple,	expensive	at	test	time,	high	variance,	non-linear

   vector	space	classification:	rocchio

   simple	linear	discriminant	classifier;	perhaps	too	simple*

   today

   support	vector	machines	(id166s)

   including	soft	margin	id166s	and	kernels	for	non-linear	classifiers

   some	empirical	evaluation	and	comparison
   text-specific	issues	in	classification

2

introduction	to	information	retrieval

linear	classifiers
a	hyperplane	decision	boundary

ch. 15

   lots	of	possible	choices	for	a,	b,	c.
   some	methods	find	a	separating	hyperplane,	

but	not	the	optimal	one	[according	to	some	
criterion	of	expected	goodness]
   e.g.,	id88

   a	support	vector	machine	(id166)	finds	an	

optimal* solution.
   maximizes	the	distance	between	the	

hyperplane and	the	   difficult	points   	close	to	
decision	boundary

this line 

represents the 

decision 
boundary:

ax + by     c = 0

   one	intuition:	if	there	are	no	points	near	the	

decision	surface,	then	there	are	no	very	
uncertain	classification	decisions
   the	decision	boundary	has	a	   margin   

3

introduction	to	information	retrieval

sec. 15.1

another	intuition
   if	you	have	to	place	a	fat	separator	between	classes,	

you	have	less	choices,	and	so		the	capacity	of	the	
model	has	been	decreased

4

introduction	to	information	retrieval

sec. 15.1

support	vector	machine	(id166)

   id166s	maximize	the	margin around	

the	separating	hyperplane.

support	vectors

   a.k.a.	large	margin	classifiers
   the	decision	function	is	fully	

specified	by	a	subset	of	training	
samples,	the	support	vectors.
   solving	id166s	is	a	quadratic	

programming problem

   seen	by	many	as	the	most	

successful	current	text	
classification	method*

maximizes
margin

narrower
margin

*but other discriminative methods 
often perform very similarly

5

introduction	to	information	retrieval

sec. 15.1

maximum	margin:	formalization

   w:	decision	hyperplane normal	vector
   xi:	data	point	i
   yi:	class	of	data	point	i (+1	or	-1)					nb:	not	1/0
   classifier	is:
   functional	margin	of	xi is:
   the	functional	margin	of	a	dataset	is	twice	the	minimum	

f(xi)	=	 sign(wtxi +	b)

yi (wtxi +	b)

functional	margin	for	any	point
   the	factor	of	2	comes	from	measuring	the	whole	width	of	the	

margin

   problem:	we	can	increase	this	margin	simply	by	scaling	w,	b   .

6

introduction	to	information	retrieval

sec. 15.1

geometric	margin

xw
t +
   distance	from	example	to	the	separator	is	
w
   examples	closest	to	the	hyperplane	are	support	vectors.	
   margin    of	the	separator	is	the	width	of	separation	between	support	vectors	

b

=

y

r

of	classes.

x

r

  

x  

derivation	of	finding	r:
dotted	line	x   	   	x is	perpendicular	to
decision	boundary	so	parallel	to	w.
unit	vector	is	w/|w|,	so	line	is	rw/|w|.
x    =	x     yrw/|w|.	
x    satisfies	wtx   	+	b	=	0.
so	wt(x    yrw/|w|)	+	b	=	0
recall	that	|w|	=	sqrt(wtw).
so	wtx    yr|w|	+	b	=	0
so,	solving	for	r	gives:
r	=	y(wtx +	b)/|w|

7

w

introduction	to	information	retrieval

sec. 15.1

linear	id166	mathematically
a	different	way	of	looking	at	things	    constrain	functional	margin
   assume	that	the	functional	margin	of	each	data	item	is	at	least	1,	then	the	

following	two	constraints	follow	for	a	training	set	{(xi ,yi)}	

wtxi + b     1    if yi = 1
wtxi + b        1   if yi =    1

   for	support	vectors,	the	inequality	becomes	an	equality
   then,	since	each	example   s	distance	from	the	hyperplane	is

   the	margin	is:

r

=

y

b

xw
t +
w

=  

2
w

8

introduction	to	information	retrieval

sec. 15.1

linear	support	vector	machine	(id166)

  

  

  

hyperplane
wt x +	b	=	0

extra	scale	constraint:
mini=1,   ,n |wtxi +	b|	=	1

this	implies:
wt(xa   xb)	=	2
   =	   xa   xb   2 =	2/   w   2

  

wtxa + b = 1

wtxb + b = -1

wt x + b = 0

9

introduction	to	information	retrieval

worked	example:	geometric	margin

   maximum	margin	weight	

extra	margin

vector	is	parallel	to	line	
from	(1,	1)	to	(2,	3).	so	
weight	vector	is	(1,	2).
   decision	boundary	is	

normal	(   perpendicular   )	
to	it	halfway	between.

   it	passes	through	(1.5,	2)
   so	y =	x1 +2x2    	5.5
   geometric	margin	is	   5

10

introduction	to	information	retrieval

worked	example:	functional	margin
   let   s	minimize	w given	that

yi(wtxi +	b)	   	1

   constraint	has	=	at	svs;
w =	(a,	2a)	for	some	a

   a+2a+b =	   1						2a+6a+b =	1
   so,	a =	2/5	and	b =	   11/5

optimal	hyperplane	is:
w =	(2/5,	4/5)	and	b =	   11/5

   margin	  	is	2/|w|	
=	2/   (4/25+16/25)
=	2/(2   5/5)	=	   5

11

introduction	to	information	retrieval

sec. 15.1

linear	id166s	mathematically	(cont.)
   we	can	therefore	formulate	the	quadratic	optimization	problem:	

find w and b such that

2
w

is maximized; and for all {(xi , yi)}
=  
wtxi + b     1 if yi=1;   wtxi + b        1   if yi =    1

   a	better	formulation	(min	   w   =	max	1/   w    ):	

find w and b such that
  (w) =   wtw is minimized; 
and for all {(xi ,yi)}:    yi (wtxi + b)     1

12

introduction	to	information	retrieval

sec. 15.1

solving	the	optimization	problem

find w and b such that
  (w) =   wtw is minimized; 
and for all {(xi ,yi)}:  yi (wtxi + b)     1

   this	is	now	optimizing	a	quadratic	function	subject	to	linear	constraints
   quadratic	optimization	problems	are	a	well-known	class	of	mathematical	

programming	problem,	and	many	(intricate)	algorithms	exist	for	solving	them	
(with	many	special	ones	built	for	id166s:	smo,	pegasos,	   )

   the	solution	usually	involves	constructing	a	dual	problem	where	a	lagrange	

multiplier   i is	associated	with	every	constraint	in	the	primary	problem:

find   1     n such that
q(  ) =    i -         i  jyiyjxi
(1)     iyi = 0
(2)   i     0 for all   i

txj is maximized and 

13

introduction	to	information	retrieval

sec. 15.1

the	optimization	problem	solution
   the	solution	has	the	form:	

w =     iyixi

b = yk- wtxk for any xk such that   k     0

   each	non-zero	  i indicates	that	corresponding	xi is	a	support	vector.
   then	the	classifying	function	will	have	the	form:

f(x) =     iyixi

tx + b

   notice	that	it	relies	on	an	inner	product between	the	test	point	x and	the	

support	vectors	xi
   we	will	return	to	this	later.

   also	keep	in	mind	that	solving	the	optimization	problem	involved	

computing	the	inner	products	xi

txj	between	all	pairs	of	training	points.

14

introduction	to	information	retrieval

three	dimensions	   	almost	separating

15

introduction	to	information	retrieval

sec. 15.2.1

soft	margin	classification		

   if	the	training	data	is	not	
linearly	separable,	slack	
variables   i can	be	added	to	
allow	misclassification	of	
difficult	or	noisy	examples.

   allow	some	errors

   let	some	points	be	moved	
to	where	they	belong,	at	a	
cost

   still,	try	to	minimize	training	

set	errors,	and	to	place	
hyperplane	   far   	from	each	
class	(large	margin)

  i

  j

16

sec. 15.2.1

introduction	to	information	retrieval

soft	margin	classification	
mathematically
   the	old	formulation:

find w and b such that
  (w) =   wtw is minimized and for all {(xi ,yi)}
yi (wtxi + b)     1

   the	new	formulation	incorporating	slack	variables:

find w and b such that
  (w) =   wtw + c    i
yi (wtxi + b)     1       i

is minimized and for all {(xi ,yi)}
and      i     0 for all i

   parameter	c can	be	viewed	as	a	way	to	control	overfitting

   a	id173	term

17

introduction	to	information	retrieval

sec. 15.2.1

soft	margin	classification	    solution

  

the	dual	problem	for	soft	margin	classification:

find   1     n such that
q(  ) =    i -         i  jyiyjxi
(1)     iyi = 0
(2)  0       i     c for all   i

txj is maximized and 

   neither	slack	variables	  i nor	their	lagrange	multipliers	appear	in	the	dual	

problem!

   again,	xi	with	non-zero	  i will	be	support	vectors.
  

solution	to	the	dual	problem	is:

w =     iyixi             
b = yk(1-   k) - wtxk where k = argmax   k   

k   

w is not needed explicitly 
for classification!
f(x) =     iyixi

tx + b

18

introduction	to	information	retrieval

sec. 15.1

classification	with	id166s
   given	a	new	point	x,	we	can	score	its	projection	

onto	the	hyperplane	normal:
   i.e.,	compute	score:	wtx +	b =	    iyixi
   decide	class	based	on	whether	<	or	>	0

tx +	b

   can	set	confidence	threshold	t.

score > t: yes
score < -t: no
else: don   t know

1

0

-1

19

introduction	to	information	retrieval

sec. 15.2.1

linear	id166s:		summary
   the	classifier	is	a	separating	hyperplane.

   the	   important   	training	points	are	the	support	vectors;	they	define	the	

hyperplane.

   quadratic	optimization	algorithms	can	identify	which	training	points	xi	are	

support	vectors	with	non-zero	lagrangian multipliers	  i.

   both	in	the	dual	formulation	of	the	problem	and	in	the	solution,	training	

points	appear	only	inside	inner	products:	

find   1     n such that
q(  ) =    i -         i  jyiyjxi
(1)     iyi = 0
(2)  0       i     c for all   i

txj is maximized and 

f(x) =     iyixi

tx + b

20

introduction	to	information	retrieval

sec. 15.2.3

non-linear	id166s
   datasets	that	are	linearly	separable	(with	some	noise)	work	out	great:

0

x

   but	what	are	we	going	to	do	if	the	dataset	is	just	too	hard?	

   how	about	   	mapping	data	to	a	higher-dimensional	space:

0

x

x2

0

x

21

introduction	to	information	retrieval

sec. 15.2.3

non-linear	id166s:		feature	spaces
   general	idea:			the	original	feature	space	can	be	

mapped	to	some	higher-dimensional	feature	space	
where	the	training	set	is	separable:

  :  x       (x)

22

introduction	to	information	retrieval

sec. 15.2.3

the	   kernel	trick   

   the	linear	classifier	relies	on	an	inner	product	between	vectors	k(xi,xj)=xi
   if	every	datapoint is	mapped	into	a	high-dimensional	space	via	some	

txj

transformation	  :		x       (x),	the	inner	product	becomes:

   a	kernel	function is	some	function	k	that	corresponds	to	an	inner	product	in	

k(xi,xj)=	  (xi) t  (xj)

some	expanded	feature	space.

   example:	

2-dimensional	vectors	x=[x1			x2];		let	k(xi,xj)=(1	+	xi
need	to	show	that	k(xi,xj)=	  (xi) t  (xj):
k(xi,xj)	=	(1	+	xi

2	+	2	xi1xj1 xi2xj2+	xi2

,=	1+	xi1

txj)2

2xj1

2xj2

txj)2
,

2		   2	xi1xi2		 xi2

=	[1		xi1
=	  (xi) t  (xj)				where	  (x)	=	 [1		x1

2		   2xi1		   2xi2]t	[1		xj1

2		   2	x1x2		 x2

2			   2x1		   2x2]

2	+	2xi1xj1	+	2xi2xj2	=
2		   2xj1		   2xj2]	

2		   2	xj1xj2		 xj2

23

introduction	to	information	retrieval

sec. 15.2.3

kernels
   why	use	kernels?

   make	non-separable	problem	separable.
   map	data	into	a	better	representational	space

   common	kernels

   linear
   polynomial	k(x,z)	=	(1+xtz)d

   gives	feature	conjunctions

   radial	basis	function	(balls	    infinite	dimensional	space)

   haven   t	been	very	useful	in	text	classification

24

sec. 15.2.4

introduction	to	information	retrieval

text	classification	evaluation:	
classic	reuters-21578	data	set	
   most	(over)used	data	set
   21578	documents
   9603	training,	3299	test	articles	(modapte/lewis	split)
   118	categories

   an	article	can	be	in	more	than	one	category
   learn	118	binary	category	distinctions

   average	document:	about	90	types,	200	tokens
   average	number	of	classes	assigned

   1.24	for	docs	with	at	least	one	category

   only	about	10	out	of	118	categories	are	large

common categories
(#train, #test)

    earn (2877, 1087) 
    acquisitions (1650, 179)
    money-fx (538, 179)
    grain (433, 149)
    crude (389, 189)

    trade (369,119)
    interest (347, 131)
    ship (197, 89)
    wheat (212, 71)
    corn (182, 56)

25

introduction	to	information	retrieval

reuters	text	categorization	data	set	
(reuters-21578)	document

sec. 15.2.4

<reuters topics="yes" lewissplit="train" cgisplit="training-set" 
oldid="12981" newid="798">
<date> 2-mar-1987 16:51:43.42</date>
<topics><d>livestock</d><d>hog</d></topics>
<title>american pork congress kicks off tomorrow</title>
<dateline>    chicago, march 2 - </dateline><body>the american pork congress 
kicks off tomorrow, march 3, in indianapolis with 160 of the nations pork producers from 44 
member states determining industry positions on a number of issues, according to the national pork 
producers council, nppc.

delegates to the three day congress will be considering 26 resolutions concerning various issues, 

including the future direction of farm policy and the tax law as it applies to the agriculture sector. 
the delegates will also debate whether to endorse concepts of a national prv (pseudorabies virus) 
control and eradication program, the nppc said.

a large trade show, in conjunction with the congress, will feature the latest in technology in all 

areas of the industry, the nppc added. reuter
&#3;</body></text></reuters>

26

introduction	to	information	retrieval

sec. 15.2.4

per	class	evaluation	measures

   recall:	fraction	of	docs	in	class	i

classified	correctly:

   precision:	fraction	of	docs	assigned	
class	i that	are	actually	about	class	i:

    

   accuracy:	(1	- error	rate)	fraction	of	

    

docs	classified	correctly:

cii
cij
   
j

cii
c ji
   
j

cii
   
i
cij
   
   
j
i

27

introduction	to	information	retrieval

sec. 15.2.4

micro- vs.	macro-averaging
   if	we	have	more	than	one	class,	how	do	we	combine	

multiple	performance	measures	into	one	quantity?

   macroaveraging:	compute	performance	for	each	

class,	then	average.

   microaveraging:	collect	decisions	for	all	classes,	

compute	contingency	table,	evaluate.

28

introduction	to	information	retrieval

sec. 15.2.4

micro- vs.	macro-averaging:	example

class 1

class 2

micro ave. table

truth: 
yes
10

truth: 
no
10

10

970

classifi
er: yes
classifi
er: no

truth: 
yes
90

truth: 
no
10

10

890

classifi
er: yes
classifi
er: no

classifier: 
yes
classifier: 
no

truth: 
yes
100

truth: 
no
20

20

1860

n macroaveraged precision: (0.5 + 0.9)/2 = 0.7
n microaveraged precision: 100/120 = .83

n microaveraged score is dominated by score 

on common classes

29

introduction	to	information	retrieval

sec. 15.2.4

30

introduction	to	information	retrieval

precision-recall	for	category:	crude

recall

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

lid166
decision tree
na  ve bayes
rocchio

0

0.2

0.4
0.6
precision

0.8

1

dumais 
(1998)

introduction	to	information	retrieval

sec. 15.2.4

precision-recall	for	category:	ship

recall

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

lid166
decision tree
na  ve bayes
rocchio

0

0.2

0.6
0.4
precision

0.8

1

dumais 
(1998)

32

introduction	to	information	retrieval

sec. 15.2.4

yang&liu:	id166	vs.	other	methods

33

introduction	to	information	retrieval

good	practice	department:
make	a	confusion	matrix

sec. 15.2.4

this (i, j) entry means 53 of the docs actually in
class i were put in class j by the classifier.

class assigned by classifier

s
s
a
l
c

 
l
a
u
t
c
a

53

   in	a	perfect	classification,	only	the	diagonal	has	non-zero	entries
   look	at	common	confusions	and	how	they	might	be	addressed

34

introduction	to	information	retrieval

sec. 15.3

the	real	world

p.	jackson	and	i.	moulinier.	2002.	natural	language	processing	for	online	applications
      there	is	no	question	concerning	the	commercial	value	of	
being	able	to	classify	documents	automatically	by	content.	
there	are	myriad	potential	applications	of	such	a	capability	
for	corporate	intranets,	government	departments,	and	
internet	publishers   

      understanding	the	data	is	one	of	the	keys	to	successful	

categorization,	yet	this	is	an	area	in	which	most	categorization	
tool	vendors	are	extremely	weak.	many	of	the	   one	size	fits	
all   	tools	on	the	market	have	not	been	tested	on	a	wide	range	
of	content	types.   

35

introduction	to	information	retrieval

sec. 15.3.1

the	real	world
   gee,	i   m	building	a	text	classifier	for	real,	now!
   what	should	i	do?

   how	much	training	data	do	you	have?

   none
   very	little
   quite	a	lot
   a	huge	amount	and	its	growing

36

introduction	to	information	retrieval

sec. 15.3.1

manually	written	rules
   no	training	data,	adequate	editorial	staff?
   never	forget	the	hand-written	rules	solution!
   if	(wheat	or	grain)	and	not	(whole	or	bread)	then

   categorize	as	grain

   in	practice,	rules	get	a	lot	bigger	than	this
   can	also	be	phrased	using	tf or	tf.idf weights

   with	careful	crafting	(human	tuning	on	development	

data)	performance	is	high:
   construe:	94%	recall,	84%	precision	over	675	categories	

(hayes	and	weinstein	iaai	1990)

   amount	of	work	required	is	huge

   estimate	2	days	per	class	   	plus	maintenance

37

introduction	to	information	retrieval

sec. 15.3.1

very	little	data?
   if	you   re	just	doing	supervised	classification,	you	

should	stick	to	something	high	bias
   there	are	theoretical	results	that	na  ve	bayes	should	do	

well	in	such	circumstances	(ng	and	jordan	2002	nips)

   the	interesting	theoretical	answer	is	to	explore	semi-

supervised	training	methods:
   id64,	em	over	unlabeled	documents,	   

   the	practical	answer	is	to	get	more	labeled	data	as	

soon	as	you	can
   how	can	you	insert	yourself	into	a	process	where	humans	

will	be	willing	to	label	data	for	you??

38

introduction	to	information	retrieval

sec. 15.3.1

a	reasonable	amount	of	data?
   perfect!
   we	can	use	all	our	clever	classifiers
   roll	out	the	id166!

   but	if	you	are	using	an	id166/nb	etc.,	you	should	
probably	be	prepared	with	the	   hybrid   	solution	
where	there	is	a	boolean	overlay
   or	else	to	use	user-interpretable	boolean-like	models	like	

decision	trees

   users	like	to	hack,	and	management	likes	to	be	able	to	

implement	quick	fixes	immediately

39

introduction	to	information	retrieval

sec. 15.3.1

a	huge	amount	of	data?
   this	is	great	in	theory	for	doing	accurate	

classification   

   but	it	could	easily	mean	that	expensive	methods	like	
id166s	(train	time)	or	knn (test	time)	are	less	practical

   na  ve	bayes	can	come	back	into	its	own	again!

   or	other	methods	with	linear	training/test	complexity	like	

(regularized)	logistic	regression	(though	much	more	
expensive	to	train)

40

introduction	to	information	retrieval

sec. 15.3.1

accuracy	as	a	function	of	data	size

   with	enough	data	the	choice	
of	classifier	may	not	matter	
much,	and	the	best	choice	
may	be	unclear
   data:	brill	and	banko	on	
context-sensitive	spelling	
correction

   but	the	fact	that	you	have	to	

keep	doubling	your	data	to	
improve	performance	is	a	
little	unpleasant

41

introduction	to	information	retrieval

sec. 15.3.2

how	many	categories?
   a	few	(well	separated	ones)?

   easy!

   a	zillion	closely	related	ones?

   think:	yahoo!	directory,	library	of	congress	classification,	

legal	applications

   quickly	gets	difficult!

   classifier	combination	is	always	a	useful	technique
   voting,	id112,	or	boosting	multiple	classifiers

   much	literature	on	hierarchical	classification

   mileage	fairly	unclear,	but	helps	a	bit	(tie-yan	liu	et	al.	2005)
   definitely	helps	for	scalability,	even	if	not	in	accuracy

   may	need	a	hybrid	automatic/manual	solution

42

introduction	to	information	retrieval

sec. 15.3.2

how	can	one	tweak	performance?
   aim	to	exploit	any	domain-specific	useful	features	

that	give	special	meanings	or	that	zone	the	data
   e.g.,	an	author	byline	or	mail	headers

   aim	to	collapse	things	that	would	be	treated	as	

different	but	shouldn   t	be.
   e.g.,	part	numbers,	chemical	formulas

   does	putting	in	   hacks   	help?

   you	bet!

   feature	design	and	non-linear	weighting	is	very	important	in	the	

performance	of	real-world	systems

43

introduction	to	information	retrieval

sec. 15.3.2

upweighting
   you	can	get	a	lot	of	value	by	differentially	weighting	

contributions	from	different	document	zones:

   that	is,	you	count	as	two	instances	of	a	word	when	

you	see	it	in,	say,	the	abstract
   upweighting	title	words	helps		(cohen	&	singer	1996)

   doubling	the	weighting	on	the	title	words	is	a	good	rule	of	thumb

   upweighting	the	first	sentence	of	each	paragraph	helps	

(murata,	1999)

   upweighting	sentences	that	contain	title	words	helps	(ko	

et	al, 2002)

44

introduction	to	information	retrieval

sec. 15.3.2

two	techniques	for	zones
1. have	a	completely	separate	set	of	

features/parameters	for	different	zones	like	the	title

2. use	the	same	features	(pooling/tying	their	
parameters)	across	zones,	but	upweight	the	
contribution	of	different	zones

   commonly	the	second	method	is	more	successful:	it	

costs	you	nothing	in	terms	of	sparsifying	the	data,	
but	can	give	a	very	useful	performance	boost

   which	is	best	is	a	contingent	fact	about	the	data

45

introduction	to	information	retrieval

sec. 15.3.2

text	summarization	techniques	in	text	
classification
   text	summarization:	process	of	extracting	key	pieces	

from	text,	normally	by	features	on	sentences	
reflecting	position	and	content

   much	of	this	work	can	be	used	to	suggest	weightings	

for	terms	in	text	categorization

   see:	kolcz,	prabakarmurthi,	and	kalita,	cikm	2001:	summarization	

as	feature	selection	for	text	categorization	

   categorizing	with	title,
   categorizing	with	first	paragraph	only
   categorizing	with	paragraph	with	most	keywords
   categorizing	with	first	and	last	paragraphs,	etc.

46

introduction	to	information	retrieval

sec. 15.3.2

does	id30/lowercasing/   	help?
   as	always,	it   s	hard	to	tell,	and	empirical	evaluation	is	

normally	the	gold	standard

   but	note	that	the	role	of	tools	like	id30	is	rather	

different	for	textcat	vs.	ir:
   for	ir,	you	often	want	to	collapse	forms	of	the	verb	

oxygenate and	oxygenation,	since	all	of	those	documents	
will	be	relevant	to	a	query	for	oxygenation

   for	textcat,	with	sufficient	training	data,	id30	does	

no	good.	it	only	helps	in	compensating	for	data	sparseness	
(which	can	be	severe	in	textcat	applications).	overly	
aggressive	id30	can	easily	degrade	performance.

47

introduction	to	information	retrieval

measuring	classification
figures	of	merit
   not	just	accuracy;	in	the	real	world,	there	are	

economic	measures:
   your	choices	are:

   do	no	classification

   that	has	a	cost	(hard	to	compute)

   do	it	all	manually

   has	an	easy-to-compute	cost	if	you   re	doing	it	like	that	now

   do	it	all	with	an	automatic	classifier

   mistakes	have	a	cost

   do	it	with	a	combination	of	automatic	classification	and	manual	

review	of	uncertain/difficult/   new   	cases

   commonly	the	last	method	is	cost	efficient	and	is	adopted

   with	more	theory	and	turkers:	werling,	chaganty,	liang,	and	
manning	(2015).	on-the-job	learning	with	bayesian	decision	
theory.	http://arxiv.org/abs/1506.03140

48

introduction	to	information	retrieval

a	common	problem:	concept	drift
   categories	change	over	time
   example:	   president	of	the	united	states   

   1999:	clinton	is	great	feature
   2010:	clinton	is	bad	feature

   one	measure	of	a	text	classification	system	is	how	

well	it	protects	against	concept	drift.
   favors	simpler	models	like	na  ve	bayes

   feature	selection:	can	be	bad	in	protecting	against	

concept	drift

49

introduction	to	information	retrieval

summary

   support	vector	machines	(id166)

   choose	hyperplane based	on	support	vectors

   support	vector	=	   critical   	point	close	to	decision	boundary

   (degree-1)	id166s	are	linear	classifiers.
   kernels:	powerful	and	elegant	way	to	define	similarity	metric
   perhaps	best	performing	text	classifier

   but	there	are	other	methods	that	perform	about	as	well	as	id166,	such	

as	regularized	logistic	regression	(zhang	&	oles 2001)

   partly	popular	due	to	availability	of	good	software

   id166light is	accurate	and	fast	    and	free	(for	research)
   now	lots	of	good	software:	libid166,	tinyid166,	scikit-learn,	   .

   comparative	evaluation	of	methods
   real	world:	exploit	domain	specific	structure!

50

introduction	to	information	retrieval

ch. 15

resources	for	today   s	lecture

   christopher	j.	c.	burges.	1998.	a	tutorial	on	support	vector	machines	for	pattern	

  

  

  

  

  
  

  

  

recognition
s.	t.	dumais.	1998.	using	id166s	for	text	categorization,	ieee	intelligent	systems,	
13(4)
yiming yang,	xin liu.	1999.	a	re-examination	of	text	categorization	methods.	22nd	
annual	international	sigir
tong	zhang,	frank	j.	oles.	2001.	text	categorization	based	on	regularized	linear	
classification	methods.	information	retrieval	4(1):	5-31	
trevor	hastie,	robert	tibshirani and	jerome	friedman.	elements	of	statistical	
learning:	data	mining,	id136	and	prediction. springer-verlag,	new	york.	
t.	joachims,	learning	to	classify	text	using	support	vector	machines.	kluwer,	2002.
fan	li,	yiming yang.	2003.	a	loss	function	analysis	for	classification	methods	in	
text	categorization.	icml	2003:	472-479.
tie-yan	liu,	yiming yang,	hao wan,	et	al.	2005.	support	vector	machines	
classification	with	very	large	scale	taxonomy,	sigkdd	explorations,	7(1):	36-43.
   classic   	reuters-21578	data	set:	
http://www.daviddlewis.com/resources/testcollections/reuters21578/

