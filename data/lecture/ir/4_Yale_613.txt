introduction to nlp
evaluation of ir
evaluation
different criteria
size of index
speed of indexing
speed of retrieval
accuracy
timeliness
ease of use
expressiveness of search language
contingency table
w=tp
x=fn
y=fp
z=tn
n2 = w + y
n1 = w + x
n
precision and recall
recall:
precision:
issues
why not use accuracy a=(w+z)/n?
average precision
report when p=r
f measure: 
f=(b2+1)pr/(b2p+r)
f1 measure: 
f1 = 2/(1/r+1/p)
harmonic mean of p and r
sample trec query
<top>
<num> number: 305
<title> most dangerous vehicles 

<desc> description: 
which are the most crashworthy, and least crashworthy, 
passenger vehicles?
 
<narr> narrative: 
a relevant document will contain information on the crashworthiness of a given vehicle or vehicles that can be used to draw a comparison with other vehicles.  the document will have to describe/compare vehicles, not drivers.  for instance, it should be expected that vehicles preferred by 16-25 year-olds would be involved in more crashes, because that age group is involved in more crashes.  i would view number of fatalities per 100 crashes to be more revealing of a vehicle's crashworthiness than the number of crashes per 100,000 miles, for example.
</top>
<docno> la031689-0177 </docno>
<docid> 31701 </docid>
<date><p>march 16, 1989, thursday, home edition </p></date>
<section><p>business; part 4; page 1; column 5; financial desk </p></section>
<length><p>586 words </p></length>
<headline><p>agency to launch study of ford bronco ii after high rate of roll-over accidents </p></headline>
<byline><p>by linda williams, times staff writer </p></byline>
<text>
<p>the federal government's highway safety watchdog said wednesday that the ford bronco ii appears to be involved in more fatal roll-over
accidents than other vehicles in its class and that it will seek to determine if the vehicle itself contributes to the accidents. </p>
<p>the decision to do an engineering analysis of the ford motor co. utility-sport vehicle grew out of a federal accident study of the
suzuki samurai, said tim hurd, a spokesman for the national highway traffic safety administration. nhtsa looked at samurai accidents after
consumer reports magazine charged that the vehicle had basic design flaws. </p>
<p>several fatalities </p>
<p>however, the accident study showed that the "ford bronco ii appears to have a higher number of single-vehicle, first event roll-overs,
particularly those involving fatalities," hurd said. the engineering analysis of the bronco, the second of three levels of investigation
conducted by nhtsa, will cover the 1984-1989 bronco ii models, the agency said. </p>
<p>according to a fatal accident reporting system study included in the september report on the samurai, 43 bronco ii single-vehicle
roll-overs caused fatalities, or 19 of every 100,000 vehicles. there were eight samurai fatal roll-overs, or 6 per 100,000; 13 involving
the chevrolet s10 blazers or gmc jimmy, or 6 per 100,000, and six fatal jeep cherokee roll-overs, for 2.5 per 100,000. after the
accident report, nhtsa declined to investigate the samurai. </p>
...
</text>
<graphic><p> photo, the ford bronco ii "appears to have a higher
number of single-vehicle, first event roll-overs," a federal official
said. </p></graphic>
<subject>
<p>traffic accidents; ford motor corp; national highway traffic safety administration; vehicle inspections;
recreational vehicles; suzuki motor co; automobile safety </p>
</subject>
</doc>
trec (cont   d)
http://trec.nist.gov/tracks.html
http://trec.nist.gov/presentations/presentations.html


most used reference collections
generic retrieval
ohsumed, cranfield, cacm
text classification
reuters, 20newsgroups
id53
trec-qa
web
dotgov, wt100g
blogs
buzzmetrics datasets
trec ad hoc collections, 2-6 gb
trec web collections, 2-100gb

comparing two systems
comparing a and b
one query?
average performance?
need: a to consistently outperform b



[example from james allan]
the sign test
example 1:
a > b (12 times)
a = b (25 times)
a < b (3 times)
p < 0.035 (significant at the 5% level)
example 2:
a > b (18 times)
a < b (9 times)
p < 0.122 (not significant at the 5% level)
external link:
http://www.fon.hum.uva.nl/service/statistics/sign_test.html   
other tests
student t-test
takes into account the actual performances, not just which system is better
http://www.fon.hum.uva.nl/service/statistics/student_t_test.html 
http://www.socialresearchmethods.net/kb/stat_t.php
wilcoxon matched-pairs signed-ranks test
http://www.fon.hum.uva.nl/service/statistics/signed_rank_test.html 
nlp
information retrieval
evaluation of ir
evaluation
size of index
speed of indexing
speed of retrieval
accuracy
timeliness
ease of use
expressiveness of search language
contingency table
w=tp
x=fn
y=fp
z=tn
n2 = w + y
n1 = w + x
n
precision and recall
recall:
precision:
exercise
go to http://www.google.com  and search for documents on tolkien   s    lord of the rings   . 
note! before starting the exercise, have a clear idea of what a relevant document for your query should look like. try different information needs.
try different ways of phrasing the query: e.g., tolkien,    jrr tolkien   , +   jrr tolkien    +   lord of the rings   , etc. for each query, compute the precision (p) based on the first 10 documents returned by google. 
later, try different queries.
[from salton   s book]
interpolated average precision (e.g., 11pt)
interpolation     what is precision at recall=0.5?



issues
why not use accuracy a=(w+z)/n?
average precision
report when p=r
f measure: 
f=(b2+1)pr/(b2p+r)
f1 measure: 
f1 = 2(pr)/(1*p + r) = 2/(1/r+1/p) : harmonic mean of p and r
kappa     interannotator agreement

n: number of items  (index i)
n: number of categories (index j)
k: number of annotators

kappa example
kappa (cont   d)
p(a) = 370/400 = 0.925
p (-) = (10+20+70+70)/800 = 0.2125
p (+) = (10+20+300+300)/800 = 0.7875
p (e) = 0.2125 * 0.2125 + 0.7875 * 0.7875 = 0.665
k = (0.925-0.665)/(1-0.665) = 0.776
kappa higher than 0.67 is tentatively acceptable; higher than 0.8 is good 
comparing two systems
comparing a and b
one query?
average performance?
need: a to consistently outperform b



[example from james allan]
the sign test
example 1:
a > b (12 times)
a = b (25 times)
a < b (3 times)
p < 0.035 (significant at the 5% level)
example 2:
a > b (18 times)
a < b (9 times)
p < 0.122 (not significant at the 5% level)
external link:
http://www.fon.hum.uva.nl/service/statistics/sign_test.html   
other tests
student t-test: takes into account the actual performances, not just which system is better
http://www.fon.hum.uva.nl/service/statistics/student_t_test.html 
http://www.socialresearchmethods.net/kb/stat_t.php
wilcoxon matched-pairs signed-ranks test
http://www.fon.hum.uva.nl/service/statistics/signed_rank_test.html 
information retrieval
system evaluation
how do i evaluate my system if   
the task is new?
no benchmark data is available?
no real users/real queries yet?
no ground truth?
no well-established competitors?
the cranfield paradigm
build test collections
start with creating a document collection that is representative of the task
usually a sample of the real domain (or the whole thing if affordable)
size varies from thousands of documents to billions (trec clueweb)
from document collections 
to test collections
still need
test queries
relevance assessments
test queries
must be germane to docs available
best designed by domain experts
random query terms generally not a good idea
relevance assessments
human judges, time-consuming
are human panels perfect?
- slide from pandu nayak and prabhakar raghavan
two issues
too many documents for human annotation
is sampling the solution?
too many irrelevant documents
waste of human annotation effort
random sampling doesn   t solve this problem   
trec   s solution: pooling
pooling
before evaluation: every participating team submit a ranked list of results per query
for a query, select top k results of each team 
pool the top k results from all teams together, remove duplicates
this is called a document    pool.   
human annotators judge the documents
if the pool is large, sampling from the pool
evaluate each team (and incoming teams) using the judgments.
why it works
top-ranked results of a candidate system ensures a good presence of relevant documents in the sample
multiple (diverse) candidate systems are needed so that the sample isn   t biased.
in your project, you don   t have participating teams    
employ different (diversity is important!) ir approaches and treat them as candidate teams!
e.g., boolean & tf-idf; w or w/o feedback; synonym,    
information retrieval
evaluation collections
sample trec query
<top>
<num> number: 305
<title> most dangerous vehicles 

<desc> description: 
which are the most crashworthy, and least crashworthy, 
passenger vehicles?
 
<narr> narrative: 
a relevant document will contain information on the crashworthiness of a given vehicle or vehicles that can be used to draw a comparison with other vehicles.  the document will have to describe/compare vehicles, not drivers.  for instance, it should be expected that vehicles preferred by 16-25 year-olds would be involved in more crashes, because that age group is involved in more crashes.  i would view number of fatalities per 100 crashes to be more revealing of a vehicle's crashworthiness than the number of crashes per 100,000 miles, for example.
</top>
<docno> la031689-0177 </docno>
<docid> 31701 </docid>
<date><p>march 16, 1989, thursday, home edition </p></date>
<section><p>business; part 4; page 1; column 5; financial desk </p></section>
<length><p>586 words </p></length>
<headline><p>agency to launch study of ford bronco ii after high rate of roll-over accidents </p></headline>
<byline><p>by linda williams, times staff writer </p></byline>
<text>
<p>the federal government's highway safety watchdog said wednesday that the ford bronco ii appears to be involved in more fatal roll-over
accidents than other vehicles in its class and that it will seek to determine if the vehicle itself contributes to the accidents. </p>
<p>the decision to do an engineering analysis of the ford motor co. utility-sport vehicle grew out of a federal accident study of the
suzuki samurai, said tim hurd, a spokesman for the national highway traffic safety administration. nhtsa looked at samurai accidents after
consumer reports magazine charged that the vehicle had basic design flaws. </p>
<p>several fatalities </p>
<p>however, the accident study showed that the "ford bronco ii appears to have a higher number of single-vehicle, first event roll-overs,
particularly those involving fatalities," hurd said. the engineering analysis of the bronco, the second of three levels of investigation
conducted by nhtsa, will cover the 1984-1989 bronco ii models, the agency said. </p>
<p>according to a fatal accident reporting system study included in the september report on the samurai, 43 bronco ii single-vehicle
roll-overs caused fatalities, or 19 of every 100,000 vehicles. there were eight samurai fatal roll-overs, or 6 per 100,000; 13 involving
the chevrolet s10 blazers or gmc jimmy, or 6 per 100,000, and six fatal jeep cherokee roll-overs, for 2.5 per 100,000. after the
accident report, nhtsa declined to investigate the samurai. </p>
...
</text>
<graphic><p> photo, the ford bronco ii "appears to have a higher
number of single-vehicle, first event roll-overs," a federal official
said. </p></graphic>
<subject>
<p>traffic accidents; ford motor corp; national highway traffic safety administration; vehicle inspections;
recreational vehicles; suzuki motor co; automobile safety </p>
</subject>
</doc>
trec (cont   d)
http://trec.nist.gov/tracks.html
http://trec.nist.gov/presentations/presentations.html


common reference collections
generic retrieval
ohsumed, cranfield, cacm
text classification
reuters, 20newsgroups
id53
trec-qa
web
dotgov, wt100g
blogs
buzzmetrics datasets
trec ad hoc collections, 2-6 gb
trec web collections, 2-100gb

