introduc)on  to  informa)on  retrieval  

introduc)on   to   
informa(on   retrieval   

cs276:   informa)on   retrieval   and   web   search   

pandu   nayak   and   prabhakar   raghavan   

lecture   5:   index   compression   

introduc)on  to  informa)on  retrieval  

course   work   
       problem   set   1   due   thursday   
       programming   exercise   1   will   be   handed   out   today   

2   

introduc)on  to  informa)on  retrieval  

last   lecture         index   construc)on   
       sort   based   indexing   

       na  ve   in   memory   inversion   
       blocked   sort   based   indexing   

       merge   sort   is   e   ec)ve   for   disk   based   sor)ng   (avoid   seeks!)   

       single   pass   in   memory   indexing   

       no   global   dic)onary   

       generate   separate   dic)onary   for   each   block   

       don   t   sort   pos)ngs   

       accumulate   pos)ngs   in   pos)ngs   lists   as   they   occur   

       distributed   indexing   using   mapreduce   
       dynamic   indexing:   mul)ple   indices,   logarithmic   merge   

3   

introduc)on  to  informa)on  retrieval  

ch. 5 

today   

       collec)on   sta)s)cs   in   more   detail   (with   rcv1)   

       how   big   will   the   dic)onary   and   pos)ngs   be?   

       dic)onary   compression   
       pos)ngs   compression   

4   

introduc)on  to  informa)on  retrieval  

ch. 5 

why   compression   (in   general)?   
       use   less   disk   space   
       saves   a   li]le   money   

       keep   more   stu      in   memory   

       increases   speed   

       increase   speed   of   data   transfer   from   disk   to   memory   

       [read   compressed   data   |   decompress]   is   faster   than               

[read   uncompressed   data]   

       premise:   decompression   algorithms   are   fast   

       true   of   the   decompression   algorithms   we   use   

5   

introduc)on  to  informa)on  retrieval  

ch. 5 

why   compression   for   inverted   indexes?   
       dic)onary   

       make   it   small   enough   to   keep   in   main   memory   
       make   it   so   small   that   you   can   keep   some   pos)ngs   lists   in   

main   memory   too   

       pos)ngs      le(s)   

       reduce   disk   space   needed   
       decrease   )me   needed   to   read   pos)ngs   lists   from   disk   
       large   search   engines   keep   a   signi   cant   part   of   the   pos)ngs   

in   memory.   
       compression   lets   you   keep   more   in   memory   

       we   will   devise   various   ir   speci   c   compression   schemes   

6   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

recall   reuters   rcv1   
       symbol
   
   sta(s(c       
   
   
       n   
      
   documents    
   
   
   
       l   
   avg.   #   tokens   per   doc      
   
   
       m    
   terms   (=   word   types)      
   
                                                           avg.   #   bytes   per   token   

   
   

                                                         

   (incl.   spaces/punct.)   

                                                           avg.   #   bytes   per   token   

   value   
   800,000   
   200   
   ~400,000   
   6   

   4.5   

                                    

   (without   spaces/punct.)   

                                                           avg.   #   bytes   per   term    
                                                           non   posi)onal   pos)ngs    100,000,000   

   7.5   

7   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

index   parameters   vs.   what   we   index   
(details   iir  table   5.1,   p.80)   

size of 

word types (terms)  non-positional 

dictionary 

postings 
non-positional index  

positional postings 

positional index 

size 
(k) 

   %  cumul 

% 

size (k)      
% 

cumul 
% 

size (k)      
% 

cumul 
% 

unfiltered 
no numbers 
case folding 
30 stopwords 
150 stopwords 
id30 

484 
474 
-2 
392  -17 
391 
-0 
391 
-0 
322  -17 

109,971 
-8 
-2  100,680 
-3 
-19 
96,969 
83,390  -14 
-19 
-19 
67,002  -30 
-4 
63,812 
-33 

197,879 
-9 
-8  179,158 
0 
-12  179,158 
-24  121,858  -31 
-39 
94,517  -47 
0 
94,517 
-42 

-9 
-9 
-38 
-52 
-52 

exercise: give intuitions for all the    0    entries. why do some 
zero entries correspond to big deltas in other columns?  

8   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

lossless   vs.   lossy   compression   
       lossless   compression:   all   informa)on   is   preserved.   

       what   we   mostly   do   in   ir.   

       lossy   compression:   discard   some   informa)on   
       several   of   the   preprocessing   steps   can   be   viewed   as   

lossy   compression:   case   folding,   stop   words,   
id30,   number   elimina)on.   

       chap/lecture   7:   prune   pos)ngs   entries   that   are   
unlikely   to   turn   up   in   the   top   k   list   for   any   query.   
       almost   no   loss   quality   for   top   k   list.   

9   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

vocabulary   vs.   collec)on   size   
       how   big   is   the   term   vocabulary?   

       that   is,   how   many   dis)nct   words   are   there?   

       can   we   assume   an   upper   bound?   

       not   really:   at   least   7020   =   1037   di   erent   words   of   length   20   
       in   prac)ce,   the   vocabulary   will   keep   growing   with   the   

collec)on   size   
       especially   with   unicode         

10   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

vocabulary   vs.   collec)on   size   
       heaps      law:   m  =  ktb  
       m   is   the   size   of   the   vocabulary,   t   is   the   number   of   

tokens   in   the   collec)on   

       typical   values:   30         k         100   and   b         0.5   
       in   a   log   log   plot   of   vocabulary   size   m   vs.   t,   heaps      

law   predicts   a   line   with   slope   about        
       it   is   the   simplest   possible   rela)onship   between   the   two   in   

log   log   space   

       an   empirical      nding   (   empirical   law   )   

11   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

heaps      law   

fig 5.1 p81 

for   rcv1,   the   dashed   line   
log10m   =   0.49   log10t   +   1.64   
is   the   best   least   squares      t.   
thus,   m   =   101.64t0.49   so   k   =   
101.64         44   and   b   =   0.49.   

good   empirical      t   for   
reuters   rcv1   !   

for      rst   1,000,020   tokens,   
law   predicts   38,323   terms;   
actually,   38,365   terms   

12   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

exercises   
       what   is   the   e   ect   of   including   spelling   errors,   vs.   
automa)cally   correc)ng   spelling   errors   on   heaps      
law?   

       compute   the   vocabulary   size   m  for   this   scenario:  
       looking   at   a   collec)on   of   web   pages,   you      nd   that   there   
are   3000   di   erent   terms   in   the      rst   10,000   tokens   and   
30,000   di   erent   terms   in   the      rst   1,000,000   tokens.   

       assume   a   search   engine   indexes   a   total   of   20,000,000,000   

(2        1010)   pages,   containing   200   tokens   on   average   

       what   is   the   size   of   the   vocabulary   of   the   indexed   collec)on   

as   predicted   by   heaps      law?   

13   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

zipf   s   law   
       heaps      law   gives   the   vocabulary   size   in   collec)ons.   
       we   also   study   the   rela)ve   frequencies   of   terms.   
       in   natural   language,   there   are   a   few   very   frequent   

terms   and   very   many   very   rare   terms.   

       zipf   s   law:   the   ith   most   frequent   term   has   frequency   

propor)onal   to   1/i   .   

       cfi         1/i  =  k/i  where   k   is   a   normalizing   constant  
       cfi   is   collec)on   frequency:   the   number   of   
occurrences   of   the   term   ti   in   the   collec)on.   

14   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

zipf   consequences   
       if   the   most   frequent   term   (the)   occurs   cf1   )mes      

       then   the   second   most   frequent   term   (of)   occurs   cf1/2   )mes   
       the   third   most   frequent   term   (and)   occurs   cf1/3   )mes            
       equivalent:   cfi   =   k/i   where   k   is   a   normalizing   factor,   

so   
       log   cfi   =   log   k         log   i  
       linear   rela)onship   between   log   cfi   and   log   i  

       another   power   law   rela)onship   

15   

introduc)on  to  informa)on  retrieval  

sec. 5.1 

zipf   s   law   for   reuters   rcv1   

16   

introduc)on  to  informa)on  retrieval  

ch. 5 

compression   
       now,   we   will   consider   compressing   the   space   

for   the   dic)onary   and   pos)ngs   
      basic   boolean   index   only   
      no   study   of   posi)onal   indexes,   etc.   
      we   will   consider   compression   schemes   

17   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

dictionary   compression   

18   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

why   compress   the   dic)onary?   
       search   begins   with   the   dic)onary   
       we   want   to   keep   it   in   memory   
       memory   footprint   compe))on   with   other   

applica)ons   

       embedded/mobile   devices   may   have   very   li]le   

memory   

       even   if   the   dic)onary   isn   t   in   memory,   we   want   it   to   

be   small   for   a   fast   search   startup   )me   

       so,   compressing   the   dic)onary   is   important   

19   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

dic)onary   storage            rst   cut   
       array   of      xed   width   entries   

       ~400,000   terms;   28   bytes/term   =   11.2   mb.   

dictionary search 

structure 

20 bytes 

4 bytes each 

20   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

fixed   width   terms   are   wasteful   
       most   of   the   bytes   in   the   term   column   are   wasted         

we   allot   20   bytes   for   1   le]er   terms.   
       and   we   s)ll   can   t   handle   supercalifragilis)cexpialidocious  or   
       wri]en   english   averages   ~4.5   characters/word.   

hydrochloro   uorocarbons.  

       exercise:   why   is/isn   t   this   the   number   to   use   for   es)ma)ng   

the   dic)onary   size?   

       ave.   dic)onary   word   in   english:   ~8   characters   
       how   do   we   use   ~8   characters   per   dic)onary   term?   

       short   words   dominate   token   counts   but   not   type   

average.   

21   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

compressing   the   term   list:      
dic)onary   as   a   string   
      store dictionary as a (long) string of characters: 
      pointer to next word shows end of current word 
      hope to save up to 60% of dictionary space. 

   .systilesyzygeticsyzygialsyzygyszaibelyiteszczecinszomo   . 

total string length = 
400k x 8b = 3.2mb 

pointers resolve 3.2m 
positions: log23.2m = 

22bits = 3bytes 

22   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

space   for   dic)onary   as   a   string   
       4   bytes   per   term   for   freq.   
       4   bytes   per   term   for   pointer   to   pos)ngs.   
       3   bytes   per   term   pointer   
       avg.   8   bytes   per   term   in   term   string   
       400k   terms   x   19         7.6   mb   (against   11.2mb   for      xed   

    now avg. 11 
    bytes/term, 
    not 20. 

width)   

23   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

blocking   
       store   pointers   to   every   kth   term   string.   

       example   below:   k=4.   

       need   to   store   term   lengths   (1   extra   byte)   

   .7systile9syzygetic8syzygial6syzygy11szaibelyite8szczecin9szomo   . 

    save 9 bytes 
    on 3 
    pointers. 

lose 4 bytes on 
term lengths. 

24   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

net   
       example   for   block   size   k   =   4   
       where   we   used   3   bytes/pointer   without   blocking   

       3   x   4   =   12   bytes,   

now   we   use   3   +   4   =   7   bytes.   

shaved   another   ~0.5mb.   this   reduces   the   size   of   the   
dic)onary   from   7.6   mb   to   7.1   mb.   
we   can   save   more   with   larger   k.   

why not go with larger k? 

25   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

exercise   
       es)mate   the   space   usage   (and   savings   compared   to   
7.6   mb)   with   blocking,   for   block   sizes   of   k  =  4,  8  and  
16.  

26   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

dic)onary   search   without   blocking   

       assuming   each   

dic)onary   term   equally   
likely   in   query   (not   really   
so   in   prac)ce!),   average   
number   of   comparisons   
=   (1+2   2+4   3+4)/8   ~2.6   

exercise:   what   if   the   frequencies   
of   query   terms   were   non   uniform   
but   known,   how   would   you   
structure   the   dic)onary   search   
tree?   

27   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

dic)onary   search   with   blocking   

       binary   search   down   to   4   term   block;   

       then   linear   search   through   terms   in   block.   

       blocks   of   4   (binary   tree),   avg.   =   

(1+2   2+2   3+2   4+5)/8   =   3   compares   

28   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

exercise   
       es)mate   the   impact   on   search   performance   (and   

slowdown   compared   to   k=1)   with   blocking,   for   block   
sizes   of   k  =  4,  8  and  16.  

29   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

front   coding   
       front   coding:   

       sorted   words   commonly   have   long   common   pre   x         store   

di   erences   only   

       (for   last   k   1   in   a   block   of   k)   
8automata8automate9automa   c10automa   on  

   8automat*a1   e2   ic3   ion 

encodes automat 

extra length 
beyond automat. 

begins to resemble general string compression. 

30   

introduc)on  to  informa)on  retrieval  

sec. 5.2 

rcv1   dic)onary   compression   summary   

technique   

fixed   width   

dic)onary   as   string   with   pointers   to   every   term   

also,   blocking   k  =   4   

also,   blocking   +   front   coding   

size   in   mb   

11.2   

7.6   

7.1   

5.9   

31   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

postings   compression   

32   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

pos)ngs   compression   
       the   pos)ngs      le   is   much   larger   than   the   dic)onary,   

factor   of   at   least   10.   

       key   desideratum:   store   each   pos)ng   compactly.   
       a   pos)ng   for   our   purposes   is   a   docid.   
       for   reuters   (800,000   documents),   we   would   use   32   

bits   per   docid   when   using   4   byte   integers.   

       alterna)vely,   we   can   use   log2   800,000         20   bits   per   
       our   goal:   use   far   fewer   than   20   bits   per   docid.   

docid.   

33   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

pos)ngs:   two   con   ic)ng   forces   
       a   term   like   arachnocentric  occurs   in   maybe   one   doc   
out   of   a   million         we   would   like   to   store   this   pos)ng   
using   log2   1m   ~   20   bits.   

       a   term   like   the   occurs   in   virtually   every   doc,   so   20   

bits/pos)ng   is   too   expensive.   
       prefer   0/1   bitmap   vector   in   this   case      

34   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

pos)ngs      le   entry   
       we   store   the   list   of   docs   containing   a   term   in   

increasing   order   of   docid.   
       computer:   33,47,154,159,202         

       consequence:   it   su   ces   to   store   gaps.   

       33,14,107,5,43         

       hope:   most   gaps   can   be   encoded/stored   with   far   

fewer   than   20   bits.   

35   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

three   pos)ngs   entries   

36   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

variable   length   encoding   
       aim:   

       for   arachnocentric,   we   will   use   ~20   bits/gap   entry.   
       for   the,   we   will   use   ~1   bit/gap   entry.   

       if   the   average   gap   for   a   term   is   g,   we   want   to   use   

~log2g   bits/gap   entry.   

       key   challenge:   encode   every   integer   (gap)   with   about   

as   few   bits   as   needed   for   that   integer.   

       this   requires   a   variable  length  encoding  
       variable   length   codes   achieve   this   by   using   short   

codes   for   small   numbers   

37   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

variable   byte   (vb)   codes   
       for   a   gap   value   g,  we   want   to   use   close   to   the   fewest   

bytes   needed   to   hold   log2   g   bits   

       begin   with   one   byte   to   store   g   and   dedicate   1   bit   in   it   

to   be   a   con)nua)on   bit   c   

       if   g      127,   binary   encode   it   in   the   7   available   bits   and   

set   c  =1   

       else   encode   g   s   lower   order   7   bits   and   then   use   
addi)onal   bytes   to   encode   the   higher   order   bits   
using   the   same   algorithm   

       at   the   end   set   the   con)nua)on   bit   of   the   last   byte   to   

1   (c   =1)         and   for   the   other   bytes   c   =   0.   

38   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

example   

docids 
gaps 
vb code 

824  

00000110 
10111000  

829  
5 
10000101  

215406 
214577 
00001101 
00001100 
10110001 

postings stored as the byte concatenation 
000001101011100010000101000011010000110010110001 

key property: vb-encoded postings are 
uniquely prefix-decodable. 

for a small gap (5), vb 
uses a whole byte. 

39   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

other   variable   unit   codes   
       instead   of   bytes,   we   can   also   use   a   di   erent      unit   of   
alignment   :   32   bits   (words),   16   bits,   4   bits   (nibbles).   

       variable   byte   alignment   wastes   space   if   you   have   
many   small   gaps         nibbles   do   be]er   in   such   cases.   

       variable   byte   codes:   

       used   by   many   commercial/research   systems   
       good   low   tech   blend   of   variable   length   coding   and   

sensi)vity   to   computer   memory   alignment   matches   (vs.   
bit   level   codes,   which   we   look   at   next).   

       there   is   also   recent   work   on   word   aligned   codes   that   

pack   a   variable   number   of   gaps   into   one   word   

40   

introduc)on  to  informa)on  retrieval  

unary   code   
       represent   n   as   n   1s   with   a      nal   0.   
       unary   code   for   3   is   1110.   
       unary   code   for   40   is   

11111111111111111111111111111111111111110   .   

       unary   code   for   80   is:   

111111111111111111111111111111111111111111
111111111111111111111111111111111111110   

       this   doesn   t   look   promising,   but   .   

41   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

gamma   codes   
       we   can   compress   be]er   with   bit   level   codes   

       the   gamma   code   is   the   best   known   of   these.   

       represent   a   gap   g   as   a   pair   length   and   o   set   
       o   set   is   g   in   binary,   with   the   leading   bit   cut   o      

       for   example   13         1101         101   
       length   is   the   length   of   o   set   
       for   13   (o   set   101),   this   is   3.   

       we   encode   length   with   unary  code:   1110.   
       gamma   code   of   13   is   the   concatena)on   of   length   

and   o   set:   1110101   

42   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

gamma   code   examples   

number 

length  

offset  

  -code 
none 

0 
1 
2 
3 
4 
9 
13 
24 
511 
1025 

0 
10 
10 
110  
1110 
1110 
11110 
111111110 

0 
1 
00 
001 
101 
1000 
11111111 
11111111110  0000000001 

0 
10,0 
10,1 
110,00 
1110,001 
1110,101 
11110,1000 
111111110,11111111 
11111111110,0000000001 

43   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

gamma   code   proper)es   
       g   is   encoded   using   2      log   g      +   1   bits   

       length   of   o   set   is      log   g      bits   
       length   of   length   is      log   g      +   1   bits   

       all   gamma   codes   have   an   odd   number   of   bits   
       almost   within   a   factor   of   2   of   best   possible,   log2   g   

       gamma   code   is   uniquely   pre   x   decodable,   like   vb   
       gamma   code   can   be   used   for   any   distribu)on   
       gamma   code   is   parameter   free   

44   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

gamma   seldom   used   in   prac)ce   
       machines   have   word   boundaries         8,   16,   32,   64   bits   

       opera)ons   that   cross   word   boundaries   are   slower   

       compressing   and   manipula)ng   at   the   granularity   of   

bits   can   be   slow   

       variable   byte   encoding   is   aligned   and   thus   poten)ally   

more   e   cient   

       regardless   of   e   ciency,   variable   byte   is   conceptually   

simpler   at   li]le   addi)onal   space   cost   

45   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

rcv1   compression   

data structure  
dictionary, fixed-width 
dictionary, term pointers into string 
with blocking, k = 4 
with blocking & front coding 
collection (text, xml markup etc) 
collection (text) 
term-doc incidence matrix 
postings, uncompressed (32-bit words) 
postings, uncompressed (20 bits) 
postings, variable byte encoded 
postings,   -encoded 

size in mb 
11.2 
7.6 
7.1 
5.9 
3,600.0 
960.0 
40,000.0 
400.0 
250.0 
116.0 
101.0 

46   

introduc)on  to  informa)on  retrieval  

sec. 5.3 

index   compression   summary   
       we   can   now   create   an   index   for   highly   e   cient   

boolean   retrieval   that   is   very   space   e   cient   

       only   4%   of   the   total   size   of   the   collec)on   
       only   10   15%   of   the   total   size   of   the   text   in   the   

collec)on   

       however,   we   ve   ignored   posi)onal   informa)on   
       hence,   space   savings   are   less   for   indexes   used   in   

prac)ce   
       but   techniques   substan)ally   the   same.   

47   

introduc)on  to  informa)on  retrieval  

ch. 5 

resources   for   today   s   lecture   
       iir   5   
       mg   3.3,   3.4.   
       f.   scholer,   h.e.   williams   and   j.   zobel.   2002.   

compression   of   inverted   indexes   for   fast   query   
evalua)on.   proc.  acm   sigir  2002.   
       variable   byte   codes   

       v.   n.   anh   and   a.   mo   at.   2005.   inverted   index   

compression   using   word   aligned   binary   codes.   
informa)on  retrieval  8:   151   166.         
       word   aligned   codes   

48   

