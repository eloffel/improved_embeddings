introduction	to	information	retrieval

introduction	to
information	retrieval

probabilistic	information	retrieval
christopher	manning and	pandu	nayak

introduction	to	information	retrieval

from	boolean	to	ranked	
retrieval	   	in	two	steps

introduction	to	information	retrieval

ch. 6

ranked	retrieval
   thus	far,	our	queries	have	all	been	boolean

   documents	either	match	or	don   t

   can	be	good	for	expert	users	with	precise	

understanding	of	their	needs	and	the	collection
   can	also	be	good	for	applications:	applications	can	easily	

consume	1000s	of	results

   not	good	for	the	majority	of	users

   most	users	incapable	of	writing	boolean	queries	

   or	they	are,	but	they	think	it   s	too	much	work

   most	users	don   t	want	to	wade	through	1000s	of	results.

   this	is	particularly	true	of	web	search

introduction	to	information	retrieval

problem	with	boolean	search:
feast	or	famine
   boolean	queries	often	result	in	either	too	few	(=0)	or	

ch. 6

too	many	(1000s)	results.

   query	1:	   standard	user	dlink 650   	   	200,000	hits
   query	2:	   standard	user	dlink 650	no	card	found   :	0	

hits

   it	takes	a	lot	of	skill	to	come	up	with	a	query	that	

produces	a	manageable	number	of	hits.
   and	gives	too	few;	or	gives	too	many

introduction	to	information	retrieval

who are these people?

karen sp  rck jones

stephen robertson

keith van rijsbergen

introduction	to	information	retrieval

why	probabilities	in	ir?

user 

information need

query

representation

understanding
of user need is
uncertain

how to match?

documents

document

representation

uncertain guess of
whether document 
has relevant content

in traditional ir systems, matching between each document and
query is attempted in a semantically imprecise space of index terms.
probabilities provide a principled foundation for uncertain reasoning.
can we use probabilities to quantify our uncertainties?

introduction	to	information	retrieval

probabilistic	ir	topics

1. classical	probabilistic	retrieval	model

   id203	ranking	principle,	etc.
   binary	independence	model	(   	na  ve	bayes	text	cat)
   (okapi)	bm25

2. bayesian	networks	for	text	retrieval
3. language	model	approach	to	ir

   an	important	emphasis	in	recent	work

probabilistic	methods	are	one	of	the	oldest	but	also	
one	of	the	currently	hot	topics	in	ir

   traditionally:	neat	ideas,	but	didn   t	win	on	performance
   it	seems	to	be	different	now

introduction	to	information	retrieval

the	document	ranking	problem
   we	have	a	collection	of	documents
   user	issues	a	query
   a	list	of	documents	needs	to	be	returned
   ranking	method	is	the	core	of	modern	ir	systems:

   in	what	order	do	we	present	documents	to	the	user?
   we	want	the	   best   	document	to	be	first,	second	best	

second,	etc   .

   idea:	rank	by	id203	of	relevance	of	the	

document	w.r.t.	information	need
   p(r=1|documenti,	query)

introduction	to	information	retrieval

recall	a	few	id203	basics
   for	events	a	and	b:
p(a,b) = p(a    b) = p(a | b)p(b) = p(b | a)p(a)
   bayes    rule
p(a | b) =

p(b | a)p(a)

p(b | a)p(a)

prior

=

x=a,a   

p(b | x)p(x)

p(b)

posterior

   odds:

o(a) =

p(a)
p(a) =

p(a)
1    p(a)

introduction	to	information	retrieval

the	id203	ranking	principle
   if	a	reference	retrieval	system   s	response	to	each	request	is	a	
ranking	of	the	documents	in	the	collection	in	order	of	decreasing	
id203	of	relevance	to	the	user	who	submitted	the	request,	
where	the	probabilities	are	estimated	as	accurately	as	possible	on	
the	basis	of	whatever	data	have	been	made	available	to	the	system	
for	this	purpose,	the	overall	effectiveness	of	the	system	to	its	user	
will	be	the	best	that	is	obtainable	on	the	basis	of	those	data.   

   [1960s/1970s]	s.	robertson,	w.s.	cooper,	m.e.	maron;	

van rijsbergen (1979:113);	manning	&	sch  tze (1999:538)

introduction	to	information	retrieval

id203	ranking	principle

let	x represent	a	document	in	the	collection.	
let	r represent	relevance	of	a	document	w.r.t.	given	(fixed)	
query	and	let	r=1 represent	relevant	and	r=0 not	relevant.

need	to	find	p(r=1|x)     id203	that	a	document	x is	relevant.

p(r =1| x) =

p(r = 0 | x) =

p(x | r =1)p(r =1)

p(x | r = 0)p(r = 0)

p(x)

p(x)

p(r = 0 | x) + p(r =1| x) =1

p(r=1),p(r=0) - prior id203
of retrieving a relevant or non-relevant
document
p(x|r=1), p(x|r=0) - id203 that if a 
relevant (not relevant) document is 
retrieved, it is x.

introduction	to	information	retrieval

id203	ranking	principle	(prp)
   simple	case:	no	selection	costs	or	other	utility	
concerns	that	would	differentially	weight	errors

   prp	in	action:	rank	all	documents	by	p(r=1|x)

   theorem:	using	the	prp	is	optimal,	in	that	it	
minimizes	the	loss	(bayes	risk)	under	1/0	loss
   provable	if	all	probabilities	correct,	etc.		[e.g.,	ripley	1996]

introduction	to	information	retrieval

id203	ranking	principle

   more	complex	case:	retrieval	costs.

   let	d be	a	document
   c	    cost	of	not	retrieving	a	relevant document
   c        cost	of	retrieving	a	non-relevant document

   id203	ranking	principle:	if
!c     p(r = 0|d)   c     p(r =1|d)     !c     p(r = 0|
!d )
for	all	d   	not	yet	retrieved,	then	d is	the	next	document	

!d )   c     p(r =1|

to	be	retrieved

   we	won   t	further	consider	cost/utility	from	now	on

introduction	to	information	retrieval

id203	ranking	principle

   how	do	we	compute	all	those	probabilities?

   do	not	know	exact	probabilities,	have	to	use	estimates	
   binary	independence	model	(bim)	    which	we	discuss	

next	    is	the	simplest	model
   questionable	assumptions

      relevance   	of	each	document	is	independent	of	

relevance	of	other	documents.
   really,	it   s	bad	to	keep	on	returning	duplicates

   boolean	model	of	relevance
   that	one	has	a	single	step	information	need

   seeing	a	range	of	results	might	let	user	refine	query

introduction	to	information	retrieval

probabilistic	retrieval	strategy

   estimate	how	terms	contribute	to	relevance

   how	do	other	things	like	term	frequency	and	document	

length	influence	your	judgments	about	document	
relevance?	
   not	at	all	in	bim
   a	more	nuanced	answer	is	the	okapi	(bm25)	formulae	[next	time]

   sp  rck jones	/	robertson

   combine	to	find	document	relevance	id203

   order	documents	by	decreasing	id203	

introduction	to	information	retrieval

probabilistic	ranking

basic concept:
   for a given query, if we know some documents that are 
relevant, terms that occur in those documents should be 
given greater weighting in searching for other relevant 
documents.
by making assumptions about the distribution of terms 
and applying id47, it is possible to derive 
weights theoretically.   

van rijsbergen

introduction	to	information	retrieval

binary	independence	model
   traditionally	used	in	conjunction	with	prp
      binary   	=	boolean:	documents	are	represented	as	binary	

incidence	vectors	of	terms	(cf.	iir	chapter	1):
  
  

x
( 1
,
!
iff term	i is	present	in	document	x.

" =
x
1=ix

nx

)

,

      independence   : terms	occur	in	documents	independently		
   different	documents	can	be	modeled	as	the	same	vector

introduction	to	information	retrieval

binary	independence	model
   queries:	binary	term	incidence	vectors
   given	query	q,	

   for	each	document	d need	to	compute	p(r|q,d).
   replace	with	computing	p(r|q,x) where x is	binary	term	

incidence	vector	representing	d.

   interested	only	in	ranking

   will	use	odds	and	bayes   	rule:

o(r | q,    x) =

p(r =1| q,    x)
p(r = 0 | q,    x) =

p(r =1| q)p(   x | r =1,q)
p(r = 0 | q)p(   x | r = 0,q)

p(   x | q)
p(   x | q)

introduction	to	information	retrieval

binary	independence	model

o(r | q,    x) =

p(r =1| q,    x)
p(r = 0 | q,    x) =
constant for a 
given query

p(r =1| q)
p(r = 0 | q)    

p(   x | r =1,q)
p(   x | r = 0,q)

needs estimation

    using independence assumption:
p(xi | r =1,q)
p(xi | r = 0,q)

p(   x | r =1,q)
p(   x | r = 0,q) =

n
   
i=1

o(r | q,    x) = o(r | q)   

n
   
i=1

p(xi | r =1,q)
p(xi | r = 0,q)

introduction	to	information	retrieval

binary	independence	model

o(r | q,    x) = o(r | q)   

is either 0 or 1:
    since xi
o(r|q,    x) =o(r|q)   
   
xi=1

p(xi | r =1,q)
n
   
p(xi | r = 0,q)
i=1
p(xi =1| r =1,q)
p(xi =1| r = 0,q)

   

p(xi = 0| r =1,q)
p(xi = 0| r = 0,q)

   
xi=0

    let  pi = p(xi =1| r =1,q); ri = p(xi =1| r = 0,q);

    assume, for all terms not occurring in the query (qi=0)

p =
i

r
i

o(r|q,    x) =o(r|q)   

pi
       
ri
xi=1
qi=1

   
xi=0
qi=1

(1    pi)
(1   ri)

introduction	to	information	retrieval

term	present
term	absent

document
xi =	1
xi =	0

relevant	(r=1)
pi
(1	    pi)

not	relevant	(r=0)
ri
(1     ri)

introduction	to	information	retrieval

binary	independence	model

o(r | q,    x) = o(r | q)   

all matching terms

pi
       
ri
xi=qi=1

   
xi=0
qi=1

1    pi
1   ri

o(r | q,    x) = o(r | q)   

o(r | q,    x) = o(r | q)   

all matching terms

   

1   ri
1    pi

$
&
%

pi
       
ri
xi=1
qi=1
   
xi=qi=1

   
xi=1
qi=1
pi(1   ri)
ri(1    pi)

   

   
qi=1

1    pi
1   ri

'
   
)
(
xi=0
qi=1

non-matching 
query terms
1    pi
1   ri
1    pi
1   ri

all query terms

introduction	to	information	retrieval

binary	independence	model

qroxqro
|

=

(

(

|

!
),

)

  

  

qx
i
i

1
==

p
r
1(
-
i
i
r
p
1(
-
i
i

)
)

  

1
-
1
-

p
i
r
i

  

q
i

1
=

constant for
each query

retrieval	status	value:
p
r
1(
-
rsv
i
i
r
p
1(
-
i
i

  

log

1
==

=

qx
i
i

)
)

=

only quantity to be estimated 

for rankings

  

qx
i
i

1
==

log

p
r
1(
-
i
i
p
r
1(
-
i
i

)
)

introduction	to	information	retrieval

binary	independence	model

=

log

rsv

all	boils	down	to	computing	rsv.
  
qx
1
==
i
i
p
r
1(
-
i
i
p
r
1(
-
i
i

p
r
1(
-
i
i
p
r
1(
-
i
i
c
log
i

qx
1
==
i
i
ic
;
1
==

  

rsv

  

i qx

)
)

=

=

=

i

p
r
1(
-
i
i
r
p
1(
-
i
i

)
)

log

)
)

the	ci are	log	odds	ratios
they	function	as	the	term	weights	in	this	model

so,	how	do	we	compute	ci   s from	our	data	?

introduction	to	information	retrieval

binary	independence	model

    estimating	rsv	coefficients in	theory
    for	each	term	i look	at	this	table	of	document	counts:

c
i

=

log

p
r
1(
-
i
i
p
r
1(
-
i
i

)
)

documents 
 
xi=1 
xi=0 
total 

relevant  non-relevant  total 

s 
s-s 
s 

n-s 

n-n-s+s 

n-s 

n 
n-n 
n 

  

    estimates:

ssnnkci
),

  

(

,

,

pi   

s
s
log
=

  

sn
(
)
-
sn
(
)
-
sss
(
-
ssnnsn
()
+--

-

)

ri

(

for now,
assume no
zero terms.
remember
smoothing.

)

introduction	to	information	retrieval

estimation	    key	challenge

   if	non-relevant	documents	are	approximated	by	
the	whole	collection,	then	ri (prob.	of	occurrence	
in	non-relevant	documents	for	query)	is	n/n	and

log1   ri
ri

= log n    n     s + s

n     s

    log n    n
n

    log n

n = idf!

introduction	to	information	retrieval

sec. 6.2.1

collection	vs.	document	frequency
   collection	frequency	of	t is	the	total number	of	
occurrences	of	t in	the	collection	(incl.	multiples)

   document	frequency	is	number	of	docs	t	is	in
   example:

word

collection frequency

document frequency

insurance

try

10440

10422

3997

8760

   which	word	is	a	better	search	term	(and	should	

get	a	higher	weight)?

introduction	to	information	retrieval

estimation	    key	challenge

   pi (id203	of	occurrence	in	relevant	

documents)	cannot	be	approximated	as	easily

   pi can	be	estimated	in	various	ways:

   from	relevant	documents	if	you	know	some

   relevance	weighting	can	be	used	in	a	feedback	loop

   constant	(croft	and	harper	combination	match)	    then	

just	get	idf weighting	of	terms	(with	pi=0.5)

rsv =

log n
ni

   
xi=qi=1

   proportional	to	prob.	of	occurrence	in	collection

   greiff (sigir	1998)	argues	for	1/3	+	2/3	dfi/n

introduction	to	information	retrieval

probabilistic	relevance	feedback
1. guess	a	preliminary	probabilistic	description	of	r=1

2.

documents;	use	it	to	retrieve	a	set	of	documents
interact	with	the	user	to	refine	the	description:	
learn	some	definite	members	with	r	=	1	and	r	=	0

3. re-estimate	pi and	ri on	the	basis	of	these

if	i appears	in	vi within	set	of	documents	v:	pi =	|vi|/|v|
  
   or	can	combine	new	information	with	original	guess	(use	

bayesian	prior):

v
p
|
)1(
k
+
i
i
v
|
|
k
+
4. repeat,	thus	generating	a	succession	of	
approximations	to	relevant	documents	

p
)2(
i

=

|

   is 
prior
weight

introduction	to	information	retrieval

iteratively	estimating	pi and ri
(=	pseudo-relevance	feedback)
1. assume	that	pi is	constant	over	all	xi in	query	and	ri

as	before
  

model

pi =	0.5	(even	odds)	for	any	given	doc

2. determine	guess	of	relevant	document	set:

   v is	fixed	size	set	of	highest	ranked	documents	on	this	

3. we	need	to	improve	our	guesses	for	pi and	ri,	so

   use	distribution	of	xi in	docs	in	v.	let	vi be	set	of	

documents	containing	xi
  

pi =	|vi|	/	|v|

   assume	if	not	retrieved	then	not	relevant	

  

ri =	(ni     |vi|)	/	(n	    |v|)

4. go	to	2.	until	converges	then	return	ranking

30

introduction	to	information	retrieval

prp	and	bim

   getting	reasonable	approximations	of	probabilities	

is	possible.

   requires	restrictive	assumptions:

   term	independence
   terms	not	in	query	don   t	affect	the	outcome
   boolean	representation	of	

documents/queries/relevance

   document	relevance	values	are	independent

   some	of	these	assumptions	can	be	removed
   problem:	either	require	partial	relevance	information	or	

seemingly	only	can	derive	somewhat	inferior	term	weights

introduction	to	information	retrieval

removing	term	independence

   in	general,	index	terms	aren   t	

independent

   dependencies	can	be	complex
   van	rijsbergen (1979)	proposed	

simple	model	of	dependencies	as	
a	tree
   exactly	friedman	and	

goldszmidt   s tree	augmented	
naive		bayes	(aaai	13,	1996)
   each	term	dependent	on	one	

other

   in	1970s,	estimation	problems	
held	back	success	of	this	model

introduction	to	information	retrieval

second	step:	term	frequency
   right	in	the	first	lecture,	we	said	that	a	page	should	

rank	higher	if	it	mentions	a	word	more
   perhaps	modulated	by	things	like	page	length

   we	might	want	a	model	with	term	frequency	in	it.

   we   ll	see	a	probabilistic	one	next	time	    bm25

   quick	summary	of	vector	space	model

introduction	to	information	retrieval

summary	    vector	space	ranking

   represent	the	query	as	a	weighted	term	

frequency/inverse	document	frequency	(tf-idf)	vector
   represent	each	document	as	a	weighted	tf-idf vector
   compute	the	cosine	similarity	score	for	the	query	

vector	and	each	document	vector

   rank	documents	with	respect	to	the	query	by	score
   return	the	top	k (e.g.,	k =	10)	to	the	user

introduction	to	information	retrieval

introduction	to	information	retrieval

sec. 6.3

cosine	similarity

introduction	to	information	retrieval

sec. 6.4

tf-idf	weighting	has	many	variants

introduction	to	information	retrieval

resources

s.	e.	robertson	and	k.	sp  rck jones.	1976.	relevance	weighting	of	search	
terms.	journal	of	the	american	society	for	information	sciences	27(3):	
129   146.

c.	j.	van	rijsbergen.	1979.	information	retrieval. 2nd	ed.	london:	

butterworths,	chapter	6.		[most	details	of	math]	
http://www.dcs.gla.ac.uk/keith/preface.html

n.	fuhr.	1992.	probabilistic	models	in	information	retrieval.	the	computer	

journal,	35(3),243   255.		[easiest	read,	with	bns]

f.	crestani,	m.	lalmas,	c.	j.	van	rijsbergen,	and	i.	campbell.	1998.	is	this	
document	relevant?	   	probably:	a	survey	of	probabilistic	models	in	
information	retrieval.	acm	computing	surveys 30(4):	528   552.	
http://www.acm.org/pubs/citations/journals/surveys/1998-30-4/p528-crestani/
[adds	very	little	material	that	isn   t	in	van	rijsbergen or	fuhr ]

