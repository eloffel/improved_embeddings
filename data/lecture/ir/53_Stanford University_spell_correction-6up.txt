introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

introduc*on	to	
informa(on	retrieval	

cs276:	informa*on	retrieval	and	web	search	

christopher	manning	and	pandu	nayak	

spelling	correc*on	

the	course	thus	far	   	
index	construc*on	
index	compression	
e   cient	boolean	querying	

	chapters	1,	2,	4,	5	
	coursera	lectures	1,	2,	3,	4	

spelling	correc*on	

	chapter	3	
	coursera	lecture	5	(mainly	some	parts)	
	this	lecture	(pa	#2!)	

	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

applica*ons	for	spelling	correc*on	

word	processing	

phones	

rates	of	spelling	errors	

		

		

2	

depending	on	the	applica*on,	~1   20%	
error	rates	

26%: 	web	queries		wang	et	al.	2003		
13%: 	retyping,	no	backspace:	whitelaw	et	al.	english&german	
7%:	words	corrected	retyping	on	phone-sized	organizer	
2%:	words	uncorrected	on	organizer	soukore   	&mackenzie	
2003	
1-2%:		retyping:	kane	and	wobbrock	2007,	gruden	et	al.	1983	
	

4	

web	search	

3	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

spelling	tasks	
      spelling	error	detec*on	
      spelling	error	correc*on:	

      autocorrect				
      hte  the	

      suggest	a	correc*on	
      sugges*on	lists	

types	of	spelling	errors	
      non-word	errors	
      gra   e	  gira   e	
      real-word	errors	

      typographical	errors	

      three	  there	

      cogni*ve	errors	(homophones)	

      piece  peace,		
      too	  	two	
      your	  you   re	

5	

      non-word	correc*on	was	historically	mainly	context	insensi*ve	
      real-word	correc*on	almost	needs	to	be	context	sensi*ve	

6	

1 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

non-word	spelling	errors	
      non-word	spelling	error	detec*on:	
      any	word	not	in	a	dic$onary	is	an	error	
      the	larger	the	dic*onary	the	becer	   	up	to	a	point	
      (the	web	is	full	of	mis-spellings,	so	the	web	isn   t	

necessarily	a	great	dic*onary	   )	

      non-word	spelling	error	correc*on:	

      generate	candidates:	real	words	that	are	similar	to	error	
      choose	the	one	which	is	best:	
      shortest	weighted	edit	distance	
      highest	noisy	channel	id203	

real	word	&	non-word	spelling	errors	
      for	each	word	w,	generate	candidate	set:	

      find	candidate	words	with	similar	pronuncia$ons	
      find	candidate	words	with	similar	spellings	
      include	w	in	candidate	set	

      choose	best	candidate	

      noisy	channel	view	of	spell	errors	
      context-sensi*ve	   	so	have	to	consider	whether	the	

surrounding	words	   make	sense   	

      flying	form	heathrow	to	lax	  	flying	from	heathrow	to	

lax	

7	

8	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

terminology	
      these	are	character	bigrams:	

      st,	pr,	an	   	

      these	are	word	bigrams:	

      palo	alto,	   ying	from,	road	repairs	

similarly	
trigrams,	
k-grams	etc		

      in	today   s	class,	we	will	generally	deal	with	word	

bigrams	

      in	the	accompanying	coursera	lecture,	we	mostly	

deal	with	character	bigrams	(because	we	cover	stu   	
complementary	to	what	we   re	discussing	here)	

the	noisy	channel	model	of	spelling	
independent word 
id147 

9	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

		

noisy	channel	intui*on	

noisy	channel	=	bayes   	rule	
      we	see	an	observa*on	x	of	a	misspelled	word	
      find	the	correct	word	  		

  w = argmax
w   v
= argmax
w   v
= argmax
w   v

p(w | x)
p(x | w)p(w)

p(x)

p(x | w)p(w)

bayes	

11	

12	

2 

introduc)on	to	informa)on	retrieval	

		

history:	noisy	channel	for	spelling	
proposed	around	1990	
      ibm	

      mays,	eric,	fred	j.	damerau	and	robert	l.	mercer.	1991.	
context	based	spelling	correc*on.	informa)on	processing	
and	management,	23(5),	517   522	

      at&t	bell	labs	

      kernighan,	mark	d.,	kenneth	w.	church,	and	william	a.	

gale.	1990.	
a	spelling	correc*on	program	based	on	a	noisy	channel	
model.	proceedings	of	coling	1990,	205-210	

		

introduc)on	to	informa)on	retrieval	

		

		

non-word	spelling	error	example	

acress

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

candidate	genera*on	
      words	with	similar	spelling	
      small	edit	distance	to	error	

      words	with	similar	pronuncia*on	

      small	distance	of	pronuncia*on	to	error	

      in	this	class	lecture	we	mostly	won   t	dwell	on	

e   cient	candidate	genera*on	

      a	lot	more	about	candidate	genera*on	in	the	

accompanying	coursera	material	

introduc)on	to	informa)on	retrieval	

		

words	within	1	of	acress
type	

candidate	
correc(on	

correct	
leder	

error	
leder	

error	

acress

actress t

acress
acress
acress
acress
acress

cress
caress
access
across
acres

-
ca
c
o
-

-

a
ac
r
e
s

dele*on	

inser*on	

transposi*on	

subs*tu*on	

subs*tu*on	

inser*on	

17	

14	

16	

candidate	tes*ng:	
damerau-levenshtein	edit	distance	
      minimal	edit	distance	between	two	strings,	where	

edits	are:	
      inser*on	
      dele*on	
      subs*tu*on	
      transposi*on	of	two	adjacent	lecers	

      see	iir	sec	3.3.3	for	edit	distance	

15	

		

introduc)on	to	informa)on	retrieval	

		

		

candidate	genera*on	
      80%	of	errors	are	within	edit	distance	1	
      almost	all	errors	within	edit	distance	2	

      also	allow	inser*on	of	space	or	hyphen	

      thisidea   		this idea
      inlaw    in-law

      can	also	allow	merging	words	
      data base   		database
      for	short	texts	like	a	query,	can	just	regard	whole	string	as	

one	item	from	which	to	produce	edits

18	

3 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

how	do	you	generate	the	candidates?	
1.    run	through	dic*onary,	check	edit	distance	with	each	

word	

2.    generate	all	words	within	edit	distance	   	k	(e.g.,	k	=	1	

or	2)	and	then	intersect	them	with	dic*onary	

3.    use	a	character	k-gram	index	and	   nd	dic*onary	

words	that	share	   most   	k-grams	with	word	(e.g.,	by	
jaccard	coe   cient)	
     

see	iir	sec	3.3.4	

4.    compute	them	fast	with	a	levenshtein	   nite	state	

5.    have	a	precomputed	map	of	words	to	possible	

transducer	

correc*ons	

a	paradigm	   	
      we	want	the	best	spell	correc*ons	
      instead	of	   nding	the	very	best,	we	
      find	a	subset	of	precy	good	correc*ons	

      (say,	edit	distance	at	most	2)	
      find	the	best	amongst	them	

      these	may	not	be	the	actual	best	
      this	is	a	recurring	paradigm	in	ir	including	   nding	
the	best	docs	for	a	query,	best	answers,	best	ads	   	
      find	a	good	candidate	set	
      find	the	top	k	amongst	them	and	return	them	as	the	best	

19	

20	

introduc)on	to	informa)on	retrieval	

		

		

let   s	say	we   ve	generated	candidates:	
now	back	to	bayes   	rule	
      we	see	an	observa*on	x	of	a	misspelled	word	
      find	the	correct	word	  		

p(w | x)
p(x | w)p(w)

p(x)

  w = argmax
w   v

= argmax
w   v
= argmax
w   v

p(x | w)p(w) what   s	p(w)?	

21	

introduc)on	to	informa)on	retrieval	

		

		

language	model	
      take	a	big	supply	of	words	(your	document	collec*on	

with	t	tokens);	let	c(w)	=	#	occurrences	of	w	

p(w) =

c(w)
t

      in	other	applica*ons	   	you	can	take	the	supply	to	be	

typed	queries	(suitably	   ltered)	   	when	a	sta*c	
dic*onary	is	inadequate	
	
	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

unigram	prior	id203	

counts	from	404,253,213	words	in	corpus	of	contemporary	english	(coca)	
	

word	

actress	
cress	
caress	
access	
across	
acres	

frequency	of	
word	

p(w)	

9,321 .0000230573
220 .0000005442
686 .0000016969
37,038 .0000916207
120,844 .0002989314
12,874 .0000318463

channel	model	id203	
      error	model	id203,	edit	id203	
      kernighan,	church,	gale		1990	

      misspelled	word	x	=	x1,	x2,	x3   	xm	
      correct	word	w	=	w1,	w2,	w3,   ,	wn	

      p(x|w)	=	id203	of	the	edit		

      (dele*on/inser*on/subs*tu*on/transposi*on)	

	

23	

22	

24	

4 

introduc)on	to	informa)on	retrieval	

		

		

compu*ng	error	id203:	
confusion	   matrix   	
del[x,y]:    count(xy typed as x)
ins[x,y]:    count(x typed as xy)
sub[x,y]:    count(y typed as x)
trans[x,y]:  count(xy typed as yx)

introduc)on	to	informa)on	retrieval	

		

		

confusion	matrix	for	subs*tu*on	

inser*on	and	dele*on	condi*oned	on	previous	
character	

25	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

nearby	keys	

genera*ng	the	confusion	matrix	
      peter	norvig   s	list	of	errors	
      peter	norvig   s	list	of	counts	of	single-edit	errors	

      all	peter	norvig   s	ngrams	data	links:	hcp://norvig.com/ngrams/		

28	

introduc)on	to	informa)on	retrieval	

		

		

channel	model		

kernighan,	church,	gale	1990	

p(x|w) =

8>>>>>>>><>>>>>>>>:

del[wi 1,wi]
count[wi 1wi] , if deletion
ins[wi 1,xi]
if insertion
count[wi 1] ,
sub[xi,wi]
if substitution
count[wi] ,
trans[wi,wi+1]
count[wiwi+1] , if transposition

29	

introduc)on	to	informa)on	retrieval	

		

		

smoothing	probabili*es:	add-1	
smoothing	
      but	if	we	use	the	confusion	matrix	example,	unseen	

errors	are	impossible!	

      they   ll	make	the	overall	id203	0.	that	seems	

too	harsh	
      e.g.,	in	kernighan   s	chart	q  a	and	a  q	are	both	0,	even	

though	they   re	adjacent	on	the	keyboard!	

      a	simple	solu*on	is	to	add	1	to	all	counts	and	then	if	

there	is	a	|a|	character	alphabet,	to	normalize	
appropriately:	

if substitution, p(x|w) =

sub[x,w]+1
count[w]+a

30	

5 

introduc)on	to	informa)on	retrieval	

		

		

channel	model	for	acress

candidate	
correc(on	

correct	
leder	

error	
leder	

x|w	

p(x|w)	

actress

cress
caress

access
across
acres
acres

t

-
ca

c
o
-
-

-

a
ac

r
e
s
s

c|ct

.000117

a|#
ac|ca

.00000144
.00000164

r|c
e|o
es|e
ss|s

.000000209
.0000093
.0000321
.0000342

31	

		
x|w	

introduc)on	to	informa)on	retrieval	
error	
candidate	
correc(on	
leder	

correct	
leder	

109	*	
noisy	channel	id203	for	acress
p(x|w)*	
p(w)	
actress t
2.7

c|ct .000117

.0000231

p(x|w)	

p(w)	

-

		

cress

-

a

a|#

.00000144

.000000544 .00078

caress

ca

ac

access

across

acres
acres

c

o

-
-

r

e

s
s

ac|
ca
r|c

e|o

.00000164

.00000170

.0028

.000000209 .0000916

.019

.0000093

.000299

2.8

es|e .0000321
ss|s .0000342

.0000318
.0000318

1.0
1.032	

correct	
leder	

introduc)on	to	informa)on	retrieval	
error	
candidate	
correc(on	
leder	

noisy	channel	id203	for	acress
actress t
2.7

.0000231

p(x|w)	

p(w)	

x|w	
		

-

		

109	*p(x|
w)p(w)	

.000117

c|
ct
a|# .00000144

.000000544 .00078

cress

-

caress

ca

access

across

acres

acres

c

o

-

-

a

ac

r

e

s

s

.00000170

.00000164

ac|
ca
r|c .000000209 .0000916

e|o .0000093

.000299

.0000321

.0000318

.0028

.019

2.8

1.0

.0000342

.0000318

1.033	

es|
e
ss|
s

introduc)on	to	informa)on	retrieval	

		

evalua*on	
      some	spelling	error	test	sets	

      wikipedia   s	list	of	common	english	misspelling	
      aspell	   ltered	version	of	that	list	
      birkbeck	spelling	error	corpus	
      peter	norvig   s	list	of	errors	(includes	wikipedia	and	

birkbeck,	for	training	or	tes*ng)	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

		

34	

real-word	spelling	errors	

         leaving in about fifteen minuets to go to her house.
      the design an construction of the system   
      can they lave him my messages?
      the study was conducted mainly be john black.

      25-40%	of	spelling	errors	are	real	words					kukich	1992	

context-sensi*ve	spelling	correc*on	
id147 with 
the noisy channel 

36	

6 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

context-sensi*ve	spelling	error	   xing	
      for	each	word	in	sentence	(phrase,	query	   )	

      generate	candidate	set	

      the	word	itself		
      all	single-lecer	edits	that	are	english	words	
      words	that	are	homophones	
      (all	of	this	can	be	pre-computed!)	

      choose	best	candidates	
      noisy	channel	model	

noisy	channel	for	real-word	spell	correc*on	
      given	a	sentence	w1,w2,w3,   ,wn	
      generate	a	set	of	candidates	for	each	word	wi	

      candidate(w1)	=	{w1,	w   1	,	w      1	,	w         1	,   }	
      candidate(w2)	=	{w2,	w   2	,	w      2	,	w         2	,   }	
      candidate(wn)	=	{wn,	w   n	,	w      n	,	w         n	,   }	

      choose	the	sequence	w	that	maximizes	p(w)	

37	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

incorpora*ng	context	words:	
context-sensi*ve	spelling	correc*on	
      determining	whether	actress	or	across	is	

appropriate	will	require	looking	at	the	context	of	use	

      we	can	do	this	with	a	becer	language	model	

      you	learned/can	learn	a	lot	about	language	models	in	

      here	we	present	just	enough	to	be	dangerous/do	the	

cs124	or	cs224n	

assignment	

      a	bigram	language	model	condi*ons	the	id203	

of	a	word	on	(just)	the	previous	word	

p(w1   wn)	=	p(w1)p(w2|w1)   p(wn|wn   1)		

incorpora*ng	context	words	
      for	unigram	counts,	p(w)	is	always	non-zero	

      if	our	dic*onary	is	derived	from	the	document	collec*on	
      this	won   t	be	true	of	p(wk|wk   1).	we	need	to	smooth	
      we	could	use	add-1	smoothing	on	this	condi*onal	

      but	here   s	a	becer	way	   	interpolate	a	unigram	and	

distribu*on	

a	bigram:		

pli(wk|wk   1)	=	  puni(wk)	+	(1     )pbi(wk|wk   1)			

     pbi(wk|wk   1)	=	c(wk   1,	wk)	/	c(wk   1)	

39	

40

		

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

all	the	important	   ne	points	
      note	that	we	have	several	id203	distribu*ons	for	

words	
      keep	them	straight!	

      you	might	want/need	to	work	with	log	probabili*es:			

     
log	p(w1   wn)	=	log	p(w1)	+	log	p(w2|w1)	+	   	+	log	p(wn|wn   1)		
      otherwise,	be	very	careful	about	   oa*ng	point	under   ow	
      our	query	may	be	words	anywhere	in	a	document	

es*mate	

      we   ll	start	the	bigram	es*mate	of	a	sequence	with	a	unigram	
      o~en,	people	instead	condi*on	on	a	start-of-sequence	symbol,	
      because	of	this,	the	unigram	and	bigram	counts	have	di   erent	

but	not	good	here	

totals	   	not	a	problem	

using	a	bigram	language	model	
         a stellar and versatile acress whose 

combination of sass and glamour      

      counts	from	the	corpus	of	contemporary	american	

english	with	add-1	smoothing	

      p(actress|versatile)=.000021 p(whose|actress) = .0010
      p(across|versatile) =.000021 p(whose|across) = .000006

      p(   versatile actress whose   ) = .000021*.0010 = 210 x10-10
      p(   versatile across whose   )  = .000021*.000006 = 1 x10-10

41	

42	

7 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

using	a	bigram	language	model	
         a stellar and versatile acress whose 

combination of sass and glamour      

      counts	from	the	corpus	of	contemporary	american	

english	with	add-1	smoothing	

      p(actress|versatile)=.000021 p(whose|actress) = .0010
      p(across|versatile) =.000021 p(whose|across) = .000006

      p(   versatile actress whose   ) = .000021*.0010 = 210 x10-10
      p(   versatile across whose   )  = .000021*.000006 = 1 x10-10

noisy	channel	for	real-word	spell	
correc*on	

two

to

tao

too

two

of

off

on

of

thew

...

threw

thaw

the

thaw

introduc)on	to	informa)on	retrieval	

		

noisy	channel	for	real-word	spell	
correc*on	
two

thew

of

to

tao

too

two

threw

thaw

the

thaw

off

on

of

43	

44	

		

introduc)on	to	informa)on	retrieval	

		

		

simpli   ca*on:	one	error	per	sentence	

      out	of	all	possible	sentences	with	one	word	replaced	

      w1,	w      2,w3,w4							two	o   	thew						
      w1,w2,w   3,w4													two	of	the	
      w         1,w2,w3,w4										too	of	thew		
         	

      choose	the	sequence	w	that	maximizes	p(w)	

...

45	

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

where	to	get	the	probabili*es	
      language	model	

      unigram	
      bigram	
      etc.	

      channel	model	

      same	as	for		non-word	spelling	correc*on	
      plus	need	id203	for	no	error,	p(w|w)	

id203	of	no	error	
      what	is	the	channel	id203	for	a	correctly	typed	

word?	

      p(   the   |   the   )	

correct	

      if	you	have	a	big	corpus,	you	can	es*mate	this	percent	

      but	this	value	depends	strongly	on	the	applica*on	

      .90	(1	error	in	10	words)	
      .95	(1	error	in	20	words)	
      .99	(1	error	in	100	words)	

47	

48	

8 

introduc)on	to	informa)on	retrieval	

		

		

introduc)on	to	informa)on	retrieval	

		

		

peter	norvig   s	   thew   	example	

x	
thew	

thew	

thew	
thew	

thew	

w	
the	

x|w	 p(x|w)	
p(w)	
ew|e	 0.000007 0.02

0.95
thew	
thaw	 e|a	 0.001
threw	h|hr	 0.000008 0.000004

109	p(x|
w)p(w)	
144
0.00000009 90
0.0000007 0.7
0.03

ew|
we	

thwe	

0.000003 0.00000004 0.0001

introduc)on	to	informa)on	retrieval	

		

		

improvements	to	channel	model	
      allow	richer	edits				(brill	and	moore	2000)	

      ent  ant	
      ph  f	
      le  al	

      incorporate	pronuncia*on	into	channel	(toutanova	

and	moore	2002)	

      incorporate	device	into	channel	

      not	all	android	phones	need	have	the	same	error	model	
      but	spell	correc*on	may	be	done	at	the	system	level	

state	of	the	art	noisy	channel	
      we	never	just	mul*ply	the	prior	and	the	error	model	
      independence	assump*ons  probabili*es	not	

commensurate	

      instead:	weight	them	
  w =argmax
w   v

p(x|w)p(w)  
      learn	  	from	a	development	test	set	

49	

51	

50	

9 

