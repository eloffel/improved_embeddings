introduction to information retrieval

http://informationretrieval.org

iir 3: dictionaries and tolerant retrieval

hinrich sch  utze

center for information and language processing, university of munich

2014-04-10

1 / 114

overview

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

2 / 114

outline

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

3 / 114

type/token distinction

token     an instance of a word or term occurring in a
document

type     an equivalence class of tokens

in june, the dog likes to chase the cat in the barn.

12 word tokens, 9 word types

4 / 114

problems in id121

what are the delimiters? space? apostrophe? hyphen?

for each of these: sometimes they delimit, sometimes they
don   t.

no whitespace in many languages! (e.g., chinese)

no whitespace in dutch, german, swedish compounds
(lebensversicherungsgesellschaftsangestellter)

5 / 114

problems with equivalence classing

a term is an equivalence class of tokens.

how do we de   ne equivalence classes?

numbers (3/20/91 vs. 20/3/91)

case folding

id30, porter stemmer

morphological analysis: in   ectional vs. derivational
equivalence classing problems in other languages

more complex morphology than in english
finnish: a single verb may have 12,000 di   erent forms
accents, umlauts

6 / 114

skip pointers

16

28

72

brutus

2 4 8 16 19 23 28 43

5

51

98

caesar

1 2 3 5 8 41 51 60 71

7 / 114

positional indexes

postings lists in a nonpositional index: each posting is just a docid
postings lists in a positional index: each posting is a docid and a list of
positions
example query:    to1 be2 or3 not4 to5 be6   

to, 993427:

h 1: h 7, 18, 33, 72, 86, 231i;

2: h1, 17, 74, 222, 255i;
4: h 8, 16, 190, 429, 433i;
5: h363, 367i;
7: h13, 23, 191i; . . . i

be, 178239:

h 1: h 17, 25i;

4: h 17, 191, 291, 430, 434i;
5: h14, 19, 101i; . . . i document 4 is a match!

8 / 114

positional indexes

with a positional index, we can answer phrase queries.

with a positional index, we can answer proximity queries.

9 / 114

take-away

tolerant retrieval: what to do if there is no exact match
between query term and document term

wildcard queries

id147

10 / 114

outline

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

11 / 114

inverted index

for each term t, we store a list of all documents that contain t.

brutus

       1

caesar

       1

2

2

4

4

11

31

45

173

174

5

6

16

57

132

. . .

calpurnia        2

31

54

101

...

{z

}
|
dictionary

|

{z

postings

}

12 / 114

dictionaries

the dictionary is the data structure for storing the term
vocabulary.

term vocabulary: the data

dictionary: the data structure for storing the term vocabulary

13 / 114

dictionary as array of    xed-width entries

for each term, we need to store a couple of items:

document frequency
pointer to postings list
. . .

assume for the time being that we can store this information
in a    xed-length entry.

assume that we store these entries in an array.

14 / 114

dictionary as array of    xed-width entries

term

a
aachen
. . .
zulu
20 bytes

document
frequency
656,265
65
. . .
221
4 bytes

pointer
to
postings list
      
      
. . .
      
4 bytes

how do

space needed:

we look up a query term qi in this array at query time? that is:
which data structure do we use to locate the entry (row) in the
array where qi is stored?

15 / 114

data structures for looking up term

two main classes of data structures: hashes and trees

some ir systems use hashes, some use trees.
criteria for when to use hashes vs. trees:

is there a    xed number of terms or will it keep growing?
what are the relative frequencies with which various keys will
be accessed?
how many terms are we likely to have?

16 / 114

hashes

each vocabulary term is hashed into an integer, its row
number in the array

at query time: hash query term, locate entry in    xed-width
array
pros: lookup in a hash is faster than lookup in a tree.

lookup time is constant.

cons

no way to    nd minor variants (resume vs. r  esum  e)
no pre   x search (all terms starting with automat)
need to rehash everything periodically if vocabulary keeps
growing

17 / 114

trees

trees solve the pre   x problem (   nd all terms starting with
automat).

simplest tree: binary tree

search is slightly slower than in hashes: o(log m), where m is
the size of the vocabulary.

o(log m) only holds for balanced trees.

rebalancing binary trees is expensive.

b-trees mitigate the rebalancing problem.

b-tree de   nition: every internal node has a number of
children in the interval [a, b] where a, b are appropriate
positive integers, e.g., [2, 4].

18 / 114

binary tree

19 / 114

b-tree

20 / 114

outline

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

21 / 114

wildcard queries

mon*:    nd all docs containing any term beginning with mon

easy with b-tree dictionary: retrieve all terms t in the range:
mon     t < moo
*mon:    nd all docs containing any term ending with mon

maintain an additional tree for terms backwards
then retrieve all terms t in the range: nom     t < non

result: a set of terms that are matches for wildcard query

then retrieve documents that contain any of these terms

22 / 114

how to handle * in the middle of a term

example: m*nchen

we could look up m* and *nchen in the b-tree and intersect
the two term sets.

expensive

alternative: permuterm index

basic idea: rotate every wildcard query, so that the * occurs
at the end.

store each of these rotations in the dictionary, say, in a b-tree

23 / 114

permuterm index

for term hello: add hello$, ello$h, llo$he, lo$hel, o$hell, and
$hello to the b-tree where $ is a special symbol

24 / 114

permuterm     term mapping

25 / 114

permuterm index

for hello, we   ve stored: hello$, ello$h, llo$he, lo$hel, o$hell,
$hello
queries

for x, look up x$
for x*, look up $x*
for *x, look up x$*
for *x*, look up x*
for x*y, look up y$x*
example: for hel*o, look up o$hel*

permuterm index would better be called a permuterm tree.

but permuterm index is the more common name.

26 / 114

processing a lookup in the permuterm index

rotate query wildcard to the right

use b-tree lookup as before

problem: permuterm more than quadruples the size of the
dictionary compared to a regular b-tree. (empirical number)

27 / 114

k-gram indexes

more space-e   cient than permuterm index

enumerate all character k-grams (sequence of k characters)
occurring in a term

2-grams are called bigrams.

example: from april is the cruelest month we get the bigrams:
$a ap pr ri il l$ $i is s$ $t th he e$ $c cr ru ue el le es st t$ $m
mo on nt h$

$ is a special word boundary symbol, as before.

maintain an inverted index from bigrams to the terms that
contain the bigram

28 / 114

postings list in a 3-gram inverted index

etr

   

beetroot

   

metric

   

petrify

   

retrieval

29 / 114

k-gram (bigram, trigram, . . . ) indexes

note that we now have two di   erent types of inverted indexes

the term-document inverted index for    nding documents
based on a query consisting of terms

the k-gram index for    nding terms based on a query
consisting of k-grams

30 / 114

processing wildcarded terms in a bigram index

query mon* can now be run as:
$m and mo and on

gets us all terms with the pre   x mon . . .

. . . but also many    false positives    like moon.

we must post   lter these terms against query.

surviving terms are then looked up in the term-document
inverted index.
k-gram index vs. permuterm index

k-gram index is more space e   cient.
permuterm index doesn   t require post   ltering.

31 / 114

exercise

google has very limited support for wildcard queries.
for example, this query doesn   t work very well on google:
[gen* universit*]

intention: you are looking for the university of geneva, but
don   t know which accents to use for the french words for
university and geneva.

according to google search basics, 2010-04-29:    note that
the * operator works only on whole words, not parts of words.   

but this is not entirely true. try [pythag*] and [m*nchen]

exercise: why doesn   t google fully support wildcard queries?

32 / 114

processing wildcard queries in the term-document index

problem 1: we must potentially execute a large number of
boolean queries.

most straightforward semantics: conjunction of disjunctions

for [gen* universit*]: genevauniversity orgenevauniversit  e
orgen`eveuniversity orgen`eveuniversit  e orgeneraluniversities
or . . .

very expensive

problem 2: users hate to type.

if abbreviated queries like [pyth* theo*] for [pythagoras   
theorem] are allowed, users will use them a lot.

this would signi   cantly increase the cost of answering queries.

somewhat alleviated by google suggest

33 / 114

outline

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

34 / 114

id153

the id153 between string s1 and string s2 is the
minimum number of basic operations that convert s1 to s2.
levenshtein distance: the admissible basic operations are
insert, delete, and replace

levenshtein distance dog-do: 1

levenshtein distance cat-cart: 1

levenshtein distance cat-cut: 1

levenshtein distance cat-act: 2

damerau-levenshtein distance cat-act: 1

damerau-levenshtein includes transposition as a fourth
possible operation.

35 / 114

levenshtein distance: computation

f
1
1
2
3
4

a
2
2
1
2
3

s
3
3
2
2
2

t
4
4
3
2
3

0
1
2
3
4

c
a
t
s

36 / 114

levenshtein distance: algorithm

levenshteindistance(s1, s2)

1 for i     0 to |s1|
2 do m[i , 0] = i
3 for j     0 to |s2|
4 do m[0, j] = j
5 for i     1 to |s1|
6 do for j     1 to |s2|
7
do if s1[i ] = s2[j]
8
9
10 return m[|s1|, |s2|]
operations: insert (cost 1), delete (cost 1), replace (cost 1), copy
(cost 0)

then m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]}
else m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]+1}

37 / 114

levenshtein distance: algorithm

levenshteindistance(s1, s2)

1 for i     0 to |s1|
2 do m[i , 0] = i
3 for j     0 to |s2|
4 do m[0, j] = j
5 for i     1 to |s1|
6 do for j     1 to |s2|
7
do if s1[i ] = s2[j]
8
9
10 return m[|s1|, |s2|]
operations: insert (cost 1), delete (cost 1), replace (cost 1), copy
(cost 0)

then m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]}
else m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]+1}

38 / 114

levenshtein distance: algorithm

levenshteindistance(s1, s2)

1 for i     0 to |s1|
2 do m[i , 0] = i
3 for j     0 to |s2|
4 do m[0, j] = j
5 for i     1 to |s1|
6 do for j     1 to |s2|
7
do if s1[i ] = s2[j]
8
9
10 return m[|s1|, |s2|]
operations: insert (cost 1), delete (cost 1), replace (cost 1), copy
(cost 0)

then m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]}
else m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]+1}

39 / 114

levenshtein distance: algorithm

levenshteindistance(s1, s2)

1 for i     0 to |s1|
2 do m[i , 0] = i
3 for j     0 to |s2|
4 do m[0, j] = j
5 for i     1 to |s1|
6 do for j     1 to |s2|
7
do if s1[i ] = s2[j]
8
9
10 return m[|s1|, |s2|]
operations: insert (cost 1), delete (cost 1), replace (cost 1), copy
(cost 0)

then m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]}
else m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]+1}

40 / 114

levenshtein distance: algorithm

levenshteindistance(s1, s2)

1 for i     0 to |s1|
2 do m[i , 0] = i
3 for j     0 to |s2|
4 do m[0, j] = j
5 for i     1 to |s1|
6 do for j     1 to |s2|
7
do if s1[i ] = s2[j]
8
9
10 return m[|s1|, |s2|]
operations: insert (cost 1), delete (cost 1), replace (cost 1), copy
(cost 0)

then m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]}
else m[i , j] = min{m[i -1, j]+1, m[i , j-1]+1, m[i -1, j-1]+1}

41 / 114

levenshtein distance: example

f

a

s

t

0

1
1

2
2

3
3

4
4

1 1

2
1
1
2
2 2
3
2
3 3
4
3
4 4
5
4

2 2

2 3
2 2

1
3
3
1
3 2
4 2
4 3
5 3

3 3

3 4
3 3

4
3
2
2
3
2
3
2
2 3
4
2

4 4

4 5
4 4

4
5
3 3

2
3
3
3

4
2
3
3

c

a

t

s

42 / 114

each cell of levenshtein matrix

cost of getting here from
my upper left neighbor
(copy or replace)

cost of getting here from
my left neighbor (insert)

of

getting

cost
here
from my upper neighbor
(delete)
the minimum of
the
three possible    move-
ments   ;
cheapest
way of getting here

the

43 / 114

levenshtein distance: example

f

a

s

t

0

1
1

2
2

3
3

4
4

1 1

2
1
1
2
2 2
3
2
3 3
4
3
4 4
5
4

2 2

2 3
2 2

1
3
3
1
3 2
4 2
4 3
5 3

3 3

3 4
3 3

4
3
2
2
3
2
3
2
2 3
4
2

4 4

4 5
4 4

4
5
3 3

2
3
3
3

4
2
3
3

c

a

t

s

44 / 114

id145 (cormen et al.)

optimal substructure: the optimal solution to the problem
contains within it subsolutions, i.e., optimal solutions to
subproblems.

overlapping subsolutions: the subsolutions overlap. these
subsolutions are computed over and over again when
computing the global optimal solution in a brute-force
algorithm.

subproblem in the case of id153: what is the edit
distance of two pre   xes

overlapping subsolutions: we need most distances of pre   xes
3 times     this corresponds to moving right, diagonally, down.

45 / 114

weighted id153

as above, but weight of an operation depends on the
characters involved.

meant to capture keyboard errors, e.g., m more likely to be
mistyped as n than as q.

therefore, replacing m by n is a smaller id153 than by
q.

we now require a weight matrix as input.

modify id145 to handle weights

46 / 114

using id153 for id147

given query,    rst enumerate all character sequences within a
preset (possibly weighted) id153

intersect this set with our list of    correct    words

then suggest terms in the intersection to the user.

    exercise in a few slides

47 / 114

exercise

1 compute levenshtein distance matrix for oslo     snow

2 what are the levenshtein editing operations that transform

cat into catcat?

48 / 114

s

n

o

w

1 1

2

2

3 3

4 4

0

1
1

2
2

3
3

4
4

o

s

l

o

49 / 114

s

n

o

w

1 1

2

2

3 3

4 4

1
2

2
?

0

1
1

2
2

3
3

4
4

o

s

l

o

50 / 114

s

n

o

w

2

2

3 3

4 4

1 1

1 2
2
1

0

1
1

2
2

3
3

4
4

o

s

l

o

51 / 114

s

n

o

w

1 1

1 2
2
1

2

2
2

2

3
?

3 3

4 4

0

1
1

2
2

3
3

4
4

o

s

l

o

52 / 114

s

n

o

w

1 1

1 2
2
1

2

2
2

2

3
2

3 3

4 4

0

1
1

2
2

3
3

4
4

o

s

l

o

53 / 114

s

n

o

w

1 1

1 2
2
1

2

2
2

2

3
2

3 3

4 4

2
3

4
?

0

1
1

2
2

3
3

4
4

o

s

l

o

54 / 114

s

n

o

w

1 1

1 2
2
1

2

2
2

2

3
2

3 3

2 4
3
2

4 4

0

1
1

2
2

3
3

4
4

o

s

l

o

55 / 114

s

n

o

w

1 1

1 2
2
1

2

2
2

2

3
2

3 3

2 4
3
2

4 4

4
3

5
?

0

1
1

2
2

3
3

4
4

o

s

l

o

56 / 114

s

n

o

w

1 1

1 2
2
1

2

2
2

2

3
2

3 3

2 4
3
2

4 4

4
3

5
3

0

1
1

2
2

3
3

4
4

o

s

l

o

57 / 114

s

n

o

w

2

2
2

2

3
2

3 3

2 4
3
2

4 4

4
3

5
3

1 1

1 2
2
1
1
3

2
?

0

1
1

2
2

3
3

4
4

o

s

l

o

58 / 114

s

n

o

w

2

2
2

2

3
2

3 3

2 4
3
2

4 4

4
3

5
3

1 1

1 2
2
1
1 2
3
1

0

1
1

2
2

3
3

4
4

o

s

l

o

59 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1

2

2
2

2
2

2

3
2

3
?

3 3

2 4
3
2

4 4

4
3

5
3

0

1
1

2
2

3
3

4
4

o

s

l

o

60 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2

4 4

4
3

5
3

0

1
1

2
2

3
3

4
4

o

s

l

o

61 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2
3
3

3
?

4 4

4
3

5
3

0

1
1

2
2

3
3

4
4

o

s

l

o

62 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

4 4

4
3

5
3

0

1
1

2
2

3
3

4
4

o

s

l

o

63 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

4 4

4
3

3
4

5
3

4
?

0

1
1

2
2

3
3

4
4

o

s

l

o

64 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

4 4

4
3

3
4

5
3

4
3

0

1
1

2
2

3
3

4
4

o

s

l

o

65 / 114

s

n

o

w

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

4 4

4
3

3
4

5
3

4
3

1 1

1 2
2
1
1 2
3
1
3
4

2
?

0

1
1

2
2

3
3

4
4

o

s

l

o

66 / 114

s

n

o

w

2

2
2

2
2

2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

4 4

4
3

3
4

5
3

4
3

1 1

1 2
2
1
1 2
3
1
3 2
4 2

0

1
1

2
2

3
3

4
4

o

s

l

o

67 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1
3 2
4 2

2

2
2

2
2

2
3

2

3
2

3
2

3
?

3 3

2 4
3
2
3 3
3 3

4 4

4
3

3
4

5
3

4
3

0

1
1

2
2

3
3

4
4

o

s

l

o

68 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1
3 2
4 2

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

4 4

4
3

3
4

5
3

4
3

0

1
1

2
2

3
3

4
4

o

s

l

o

69 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1
3 2
4 2

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

3
3

4
?

4 4

4
3

3
4

5
3

4
3

0

1
1

2
2

3
3

4
4

o

s

l

o

70 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1
3 2
4 2

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4

5
3

4
3

0

1
1

2
2

3
3

4
4

o

s

l

o

71 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1
3 2
4 2

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4
4
4

5
3

4
3

4
?

0

1
1

2
2

3
3

4
4

o

s

l

o

72 / 114

s

n

o

w

1 1

1 2
2
1
1 2
3
1
3 2
4 2

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4
4
4

5
3

4
3

4
4

0

1
1

2
2

3
3

4
4

o

s

l

o

73 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
3
4
5
?

o

s

l

o

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4
4
4

5
3

4
3

4
4

74 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

o

s

l

o

2

2
2

2
2

2
3

2

3
2

3
2

3
2

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4
4
4

5
3

4
3

4
4

75 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
?

o

s

l

o

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4
4
4

5
3

4
3

4
4

76 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

o

s

l

o

3 3

2 4
3
2
3 3
3 3

3 4
3 3

4 4

4
3

3
4
4
4

5
3

4
3

4
4

77 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2
4

4
?

4 4

4
3

3
4
4
4

5
3

4
3

4
4

o

s

l

o

78 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4 4

4
3

3
4
4
4

5
3

4
3

4
4

o

s

l

o

79 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4 4

4
3

3
4
4
4

4
3

5
3

4
3

4
4

5
?

o

s

l

o

80 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4 4

4
3

3
4
4
4

4
3

5
3

4
3

4
4

5
3

o

s

l

o

81 / 114

s

n

o

w

o

s

l

o

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2

4 3
5 3

2

2
2

2
2

2
3

3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4

4
3

3
4
4
4

4

5
3

4
3

4
4

4
5
3 3

82 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4 4

4
3

3
4
4
4

4
3

5
3

4
3

4
4

5
3

o

s

l

o

how do

i read out the editing operations that transform oslo into snow?

83 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2

3
2

3
2

3
2

3
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4

4
3

3
4
4
4

4
3

4

5
3

4
3

4
4

5
3

o

s

l

o

cost
1

operation
insert

input
*

output
w

84 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1
1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2
3
2

3
2

3
2

3
3

3 3
2 4
3
2
3 3
3 3

3 4
3 3

2 4
4
2

4 4
4
5
3 3

3 4
4
3
4 4
4 4

4
3

5
3

o

s

l

o

cost
0
1

operation
(copy)
insert

input
o
*

output
o
w

85 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1
1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2

2
2

2
2

2
3
3
4

2
3
2
3
2
3
2

3
3

3

2
3
3
3

3
3

2
4

3
4
2

3
3
4
3

4
2

4 4
4
5
3 3
3 4
4
3
4 4
4 4

4
5
3 3

o

s

l

o

cost
1
0
1

operation
replace
(copy)
insert

input
l
o
*

output
n
o
w

86 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

1 1

1 2
2
1
1 2
3
1
3 2
4 2
4 3
5 3

2 2

2 3
2 2

2 3
2 2
2 3
3
2
3 3
4
3

3

2
3
3
3

3
3

2
4

3

4
2

3
3
4
3
4
2

4

4
3

4

5
3

4
3

3
4
4
4
4
4
4
5
3 3

o

s

l

o

cost
0
1
0
1

operation
(copy)
replace
(copy)
insert

input
s
l
o
*

output
s
n
o
w

87 / 114

s

n

o

w

0

1
1

2
2

3
3

4
4

o

s

l

o

1

1

2
1

2
1

1
2
1
3
3 2
4 2
4 3
5 3

2 2

2 3
2 2

2 3
2 2

2 3
3
2
3 3
4
3

3 3

2 4
3
2
3 3
3 3

3 4
3 3

2
4

4
2

4 4

4
3

3
4
4
4

4
3

5
3

4
3

4
4

5
3

cost
1
0
1
0
1

operation
delete
(copy)
replace
(copy)
insert

input
o
s
l
o
*

output
*
s
n
o
w

88 / 114

c

a

t

c

a

t

0

1
1

2
2

3
3

1 1

0 2
2
0
2 1
3 1
3 2
4 2

2

2
1

2

3
1

2
0

0
2
2 1
3 1

3 3

4
3
2 2

2
3
1 1

0 2
2
0

4

3
3

3
2

2
1

4

5
3

4
2

3
1

5 5

6
5
4 4

3 5
3 3

4
3
2 2

6

6
5

5
4

3
3

6

7
5

6
4

5
3

c

a

t

89 / 114

c

a

t

c

a

t

1 1

2 2

3

3

0

1
1

2
2

3
3

c

a

t

2
0

0
2
2 1
3 1
3 2
4 2

cost
1
1
1
0
0
0

operation
insert
insert
insert
(copy)
(copy)
(copy)

input
*
*
*
c
a
t

4
3
2 2

2
3
1 1

0 2
2
0

3
2
1 1

0 2
2
0
2 1
3 1

output
c
a
t
c
a
t

4 4

3 5
3 3

3
2

2
1

4
2

3
1

5

5
4

3
3

3
2

5

6
4

5
3

4
2

6 6

7
6
5 5

5
6
4 4

3 5
3 3

90 / 114

c

a

t

c

a

t

3 3

4
3
2 2

2
3
1 1

0 2
2
0

4

4

3 5
3 3

3
2

2
1

4
2

3
1

5 5

5
4

3
3

3
2

6
4

5
3

4
2

6

6

7
6
5 5

5
6
4 4

3 5
3 3

0

1
1

2
2

3
3

1 1

0 2
2
0
2 1
3 1
3 2
4 2

c

a

t

cost
0
1
1
1
0
0

operation
(copy)
insert
insert
insert
(copy)
(copy)

input
c
*
*
*
a
t

2 2

3
2
1 1

0 2
2
0
2 1
3 1

output
c
a
t
c
a
t

91 / 114

c

a

t

c

a

t

3 3

4
3
2 2

2
3
1 1

0 2
2
0

4

3
3

4

5
3

3
4
2 2

2
1

3
1

5 5

5
4

3
3

3
2

6
4

5
3

4
2

6

6

7
6
5 5

5
6
4 4

3 5
3 3

0

1
1

2
2

3
3

1 1

0 2
2
0
2 1
3 1
3 2
4 2

c

a

t

cost
0
0
1
1
1
0

operation
(copy)
(copy)
insert
insert
insert
(copy)

input
c
a
*
*
*
t

2 2

3
2
1 1

0 2
2
0
2 1
3 1

output
c
a
t
c
a
t

92 / 114

c

a

t

c

a

t

3 3

4
3
2 2

2
3
1 1

0 2
2
0

4

3
3

3
2

4

5
3

4
2

3
2
1 1

5 5

5
4

3
3

3
2

6
4

5
3

4
2

6

6

7
6
5 5

5
6
4 4

3 5
3 3

0

1
1

2
2

3
3

1 1

0 2
2
0
2 1
3 1
3 2
4 2

c

a

t

cost
0
0
0
1
1
1

operation
(copy)
(copy)
(copy)
insert
insert
insert

input
c
a
t
*
*
*

2 2

3
2
1 1

0 2
2
0
2 1
3 1

output
c
a
t
c
a
t

93 / 114

outline

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

94 / 114

id147

two principal uses

correcting documents being indexed
correcting user queries

two di   erent methods for id147
isolated word id147

check each word on its own for misspelling
will not catch typos resulting in correctly spelled words, e.g.,
an asteroid that fell form the sky
context-sensitive id147

look at surrounding words
can correct form/from error above

95 / 114

correcting documents

we   re not interested in interactive id147 of
documents (e.g., ms word) in this class.

in ir, we use document correction primarily for ocr   ed
documents. (ocr = id42)

the general philosophy in ir is: don   t change the documents.

96 / 114

correcting queries

first: isolated word id147

premise 1: there is a list of    correct words    from which the
correct spellings come.

premise 2: we have a way of computing the distance between
a misspelled word and a correct word.

simple id147 algorithm: return the    correct   
word that has the smallest distance to the misspelled word.

example: informaton     information

for the list of correct words, we can use the vocabulary of all
words that occur in our collection.

why is this problematic?

97 / 114

alternatives to using the term vocabulary

a standard dictionary (webster   s, oed etc.)

an industry-speci   c dictionary (for specialized ir systems)

the term vocabulary of the collection, appropriately weighted

98 / 114

distance between misspelled word and    correct    word

several alternatives

id153 and levenshtein distance

weighted id153

k-gram overlap

99 / 114

id147

now that we can compute id153: how to use it for
isolated word id147     this is the last slide in this
section.

k-gram indexes for isolated word id147.

context-sensitive id147

general issues

100 / 114

k-gram indexes for id147

enumerate all k-grams in the query term

example: bigram index, misspelled word bordroom

bigrams: bo, or, rd, dr, ro, oo, om

use the k-gram index to retrieve    correct    words that match
query term k-grams

threshold by number of matching k-grams

e.g., only vocabulary terms that di   er by at most 3 k-grams

101 / 114

k-gram indexes for id147: bordroom

bo

   

aboard

   

about

   

boardroom

   

border

or

rd

   

border

   

lord

   

morbid

   

sordid

   

aboard

   

ardent

   

boardroom

   

border

102 / 114

context-sensitive id147

our example was: an asteroid that fell form the sky

how can we correct form here?
one idea: hit-based id147

retrieve    correct    terms close to each query term
for    ew form munich:    ea for    ew, from for form, munch for
munich
now try all possible resulting phrases as queries with one word
      xed    at a time
try query       ea form munich   
try query       ew from munich   
try query       ew form munch   
the correct query       ew from munich    has the most hits.

suppose we have 7 alternatives for    ew, 20 for form and 3 for
munich, how many    corrected    phrases will we enumerate?

103 / 114

context-sensitive id147

the    hit-based    algorithm we just outlined is not very
e   cient.

more e   cient alternative: look at    collection    of queries, not
documents

104 / 114

general issues in id147

user interface

automatic vs. suggested correction
did you mean only works for one suggestion.
what about multiple possible corrections?
tradeo   : simple vs. powerful ui

cost

id147 is potentially expensive.
avoid running on every query?
maybe just on queries that match few documents.
guess: id147 of major search engines is e   cient
enough to be run on every query.

105 / 114

exercise: understand peter norvig   s spelling corrector

import re, collections
def words(text): return re.findall(   [a-z]+   , text.lower())
def train(features):
model = collections.defaultdict(lambda: 1)
for f in features:
model[f] += 1
return model
nwords = train(words(file(   big.txt   ).read()))
alphabet =    abcdefghijklmnopqrstuvwxyz   
def edits1(word):
splits
deletes
transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b) gt 1]
replaces
inserts
return set(deletes + transposes + replaces + inserts)
def known_edits2(word):
return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)
def known(words): return set(w for w in words if w in nwords)
def correct(word):
candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]
return max(candidates, key=nwords.get)

= [a + c + b[1:] for a, b in splits for c in alphabet if b]
= [a + c + b

= [(word[:i], word[i:]) for i in range(len(word) + 1)]
= [a + b[1:] for a, b in splits if b]

for a, b in splits for c in alphabet]

106 / 114

outline

1 recap

2 dictionaries

3 wildcard queries

4 id153

5 id147

6 soundex

107 / 114

soundex

soundex is the basis for    nding phonetic (as opposed to
orthographic) alternatives.

example: chebyshev / tchebysche   
algorithm:

turn every token to be indexed into a 4-character reduced form
do the same with query terms
build and search an index on the reduced forms

108 / 114

soundex algorithm

1 retain the    rst letter of the term.
2 change all occurrences of the following letters to    0    (zero): a, e, i,

o, u, h, w, y

3 change letters to digits as follows:

b, f, p, v to 1
c, g, j, k, q, s, x, z to 2
d,t to 3
l to 4
m, n to 5
r to 6

4 repeatedly remove one out of each pair of consecutive identical digits
5 remove all zeros from the resulting string; pad the resulting string

with trailing zeros and return the    rst four positions, which will
consist of a letter followed by three digits

109 / 114

example: soundex of herman

retain h

erman     0rm0n

0rm0n     06505

06505     06505

06505     655

return h655

note: hermann will generate the same code

110 / 114

how useful is soundex?

not very     for information retrieval

ok for    high recall    tasks in other applications (e.g., interpol)

zobel and dart (1996) suggest better alternatives for phonetic
matching in ir.

111 / 114

exercise

compute soundex code of your last name

112 / 114

take-away

tolerant retrieval: what to do if there is no exact match
between query term and document term

wildcard queries

id147

113 / 114

resources

chapter 3 of iir
resources at http://cislmu.org

trie vs hash vs ternary tree
soundex demo
id153 demo
peter norvig   s spelling corrector
google: wild card search, id147 gone wrong, a
misspelling that is more frequent that the correct spelling

114 / 114

