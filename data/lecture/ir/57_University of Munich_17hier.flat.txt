introduction to information retrieval

http://informationretrieval.org

iir 17: hierarchical id91

hinrich sch  utze & lucia d. krisnawati

center for information and language processing, university of munich

2013-07-03

1 / 66

overview

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

2 / 66

outline

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

3 / 66

applications of id91 in ir

application

search result id91

scatter-gather

collection id91

what is
clustered?
search
results

col-

(subsets
of)
lection
collection

bene   t

example

more e   ective infor-
mation
presentation
to user
alternative user inter-
face:    search without
typing   
e   ective
information
presentation for ex-
ploratory browsing

mckeown et al. 2002,
news.google.com

cluster-based retrieval

collection

higher
faster search

e   ciency:

salton 1971

4 / 66

k -means algorithm

k -means({~x1, . . . , ~xn }, k )

1 (~s1,~s2, . . . ,~sk )     selectrandomseeds({~x1, . . . , ~xn }, k )
2 for k     1 to k
3 do ~  k     ~sk
4 while stopping criterion has not been met
5 do for k     1 to k
6
7
8
9
10
11
12 return {~  1, . . . , ~  k }

do   k     {}
for n     1 to n
do j     arg minj     |~  j         ~xn|

  j       j     {~xn} (reassignment of vectors)

for k     1 to k
do ~  k     1

~x (recomputation of centroids)

|  k | p~x     k

5 / 66

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.
better heuristics:

select seeds not randomly, but using some heuristic (e.g.,    lter
out outliers or    nd a set of seeds that has    good coverage    of
the document space)
use hierarchical id91 to    nd good seeds (next class)
select i (e.g., i = 10) di   erent sets of seeds, do a k -means
id91 for each, select the id91 with lowest rss

6 / 66

take-away today

introduction to hierarchical id91

single-link and complete-link id91

centroid and group-average agglomerative id91 (gaac)

bisecting id116

how to label clusters automatically

7 / 66

outline

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

8 / 66

hierarchical id91

our goal in hierarchical id91 is to
create a hierarchy like the one we saw earlier
in reuters:

top

regions

industries

kenya

china

uk

france

co   ee

poultry

oil & gas

want to create this hierarchy automatically.
we can do this either top-down or
bottom-up. the best known bottom-up
method is hierarchical agglomerative
id91.

w

9 / 66

hierarchical agglomerative id91 (hac)

hac creates a hierachy in the form of a binary tree.

assumes a similarity measure for determining the similarity of
two clusters.

up to now, our similarity measures were for documents.

we will look at four di   erent cluster similarity measures.

10 / 66

hac: basic algorithm

start with each document in a separate cluster

then repeatedly merge the two clusters that are most similar

until there is only one cluster.

the history of merging is a hierarchy in the form of a binary
tree.

the standard way of depicting this history is a
dendrogram.

11 / 66

a
d
e
n
d
r
o
g
r
a
m

1.0

0.8

0.6

0.4

0.2

0.0

ag trade reform.
back   to   school spending is up
lloyd   s ceo questioned
lloyd   s chief / u.s. grilling
viag stays positive
chrysler / latin america
ohio blue cross
japanese prime minister / mexico
compuserve reports loss
sprint / internet access service
planet hollywood
trocadero: tripling of revenues
german unions split
war hero colin powell
war hero colin powell
oil prices slip
chains may raise prices
clinton signs law
lawsuit against tobacco companies
suits against tobacco firms
indiana tobacco lawsuit
most active stocks
mexican markets
hog prices tumble
nyse closing averages
british ftse index
fed holds interest rates steady
fed to keep interest rates steady
fed keeps interest rates steady
fed keeps interest rates steady

d
e
n
d
r
o
g
r
a
m
a
t

a

w

e

c
a
n

c
u
t

t
h
e

t
h
e
m
e
r
g
e
r

w
a
s
.

   
a
t

c
l
u
s
t
e
r
i
n
g
.

a
t

0
.
1

o
r

0
.
4
)

t
o

g
e
t

a

p
a
r
t
i
c
u
a
r

l

p
o
n
t

i

(
e
.
g
.
,

b
o
t
t
o
m

t
o

t
o
p
.

e
a
c
h
m
e
r
g
e
r

t
e
l
l
s

u
s

w
h
a
t

t
h
e

s
i

m

i
l

a
r
i
t
y

o
f

t
h
e

h
o
r
i
z
o
n
t
a

l

l
i

n
e

o
f

c
a
n

b
e

r
e
a
d

o
   

f
r
o
m

t
h
e

h
i
s
t
o
r
y

o
f

m
e
r
g
e
r
s

1
2
/
6
6

divisive id91

divisive id91 is top-down.

alternative to hac (which is bottom up).
divisive id91:

start with all docs in one big cluster
then recursively split clusters
eventually each node forms a cluster on its own.

    bisecting k -means at the end

for now: hac (= bottom-up)

13 / 66

naive hac algorithm

simplehac(d1, . . . , dn )

do c [n][i ]     sim(dn, di )
i [n]     1 (keeps track of active clusters)

1 for n     1 to n
2 do for i     1 to n
3
4
5 a     [] (collects id91 as a sequence of merges)
6 for k     1 to n     1
7 do hi , mi     arg max{hi ,mi:i 6=m   i [i ]=1   i [m]=1} c [i ][m]
8
9
10
11
12
13
14 return a

a.append(hi , mi) (store merge)
for j     1 to n
do (use i as representative for < i , m >)

c [i ][j]     sim(< i , m >, j)
c [j][i ]     sim(< i , m >, j)
i [m]     0 (deactivate cluster)

14 / 66

computational complexity of the naive algorithm

first, we compute the similarity of all n    n pairs of
documents.
then, in each of n iterations:

we scan the o(n    n) similarities to    nd the maximum
similarity.
we merge the two clusters with maximum similarity.
we compute the similarity of the new cluster with all other
(surviving) clusters.

there are o(n) iterations, each performing a o(n    n)
   scan    operation.
overall complexity is o(n3).
we   ll look at more e   cient algorithms later.

15 / 66

key question: how to de   ne cluster similarity

single-link: maximum similarity

maximum similarity of any two documents

complete-link: minimum similarity

minimum similarity of any two documents

centroid: average    intersimilarity   

average similarity of all document pairs (but excluding pairs of
docs in the same cluster)
this is equivalent to the similarity of the centroids.

group-average: average    intrasimilarity   

average similary of all document pairs, including pairs of docs
in the same cluster

16 / 66

cluster similarity: example

4

3

2

1

0

0

1

2

3

4

5

6

7

17 / 66

b
b
b
b
single-link: maximum similarity

4

3

2

1

0

0

1

2

3

4

5

6

7

18 / 66

b
b
b
b
complete-link: minimum similarity

4

3

2

1

0

0

1

2

3

4

5

6

7

19 / 66

b
b
b
b
centroid: average intersimilarity

intersimilarity = similarity of two documents in di   erent clusters

4

3

2

1

0

0

1

2

3

4

5

6

7

20 / 66

b
b
b
b
group average: average intrasimilarity

intrasimilarity = similarity of any pair, including cases where the
two documents are in the same cluster

4

3

2

1

0

0

1

2

3

4

5

6

7

21 / 66

b
b
b
b
cluster similarity: larger example

4

3

2

1

0

0

1

2

3

4

5

6

7

22 / 66

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
single-link: maximum similarity

4

3

2

1

0

0

1

2

3

4

5

6

7

23 / 66

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
complete-link: minimum similarity

4

3

2

1

0

0

1

2

3

4

5

6

7

24 / 66

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
centroid: average intersimilarity

4

3

2

1

0

0

1

2

3

4

5

6

7

25 / 66

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
group average: average intrasimilarity

4

3

2

1

0

0

1

2

3

4

5

6

7

26 / 66

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
outline

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

27 / 66

single link hac

the similarity of two clusters is the maximum intersimilarity    
the maximum similarity of a document from the    rst cluster
and a document from the second cluster.

once we have merged two clusters, how do we update the
similarity matrix?

this is simple for single link:

sim(  i , (  k1       k2)) = max(sim(  i ,   k1), sim(  i ,   k2))

28 / 66

t
h
i
s

d
e
n
d
r
o
g
r
a
m
w
a
s

p
r
o
d
u
c
e
d

b
y

s
i
n
g
l
e
-
l
i

n
k

1.0

0.8

0.6

0.4

0.2

0.0

ag trade reform.
back   to   school spending is up
lloyd   s ceo questioned
lloyd   s chief / u.s. grilling
viag stays positive
chrysler / latin america
ohio blue cross
japanese prime minister / mexico
compuserve reports loss
sprint / internet access service
planet hollywood
trocadero: tripling of revenues
german unions split
war hero colin powell
war hero colin powell
oil prices slip
chains may raise prices
clinton signs law
lawsuit against tobacco companies
suits against tobacco firms
indiana tobacco lawsuit
most active stocks
mexican markets
hog prices tumble
nyse closing averages
british ftse index
fed holds interest rates steady
fed to keep interest rates steady
fed keeps interest rates steady
fed keeps interest rates steady

d
e
n
d
r
o
g
r
a
m

.

d
e
r
i
v
e
d

b
y

c
u
t
t
i
n
g

t
h
e

c
l
u
s
t
e
r
i
n
g

t
h
a
t

c
a
n

b
e

2
-
c
l
u
s
t
e
r

o
r

3
-
c
l
u
s
t
e
r

t
h
e
r
e

i
s

n
o

b
a

l

a
n
c
e
d

t
o

t
h
e
m
a
n

i

c
l
u
s
t
e
r

m
e
m
b
e
r
s
)

b
e
i
n
g

a
d
d
e
d

c
l
u
s
t
e
r
s

(
1

o
r

2

n
o
t
i
c
e
:

m
a
n
y

s

m
a

l
l

2
9
/
6
6

complete link hac

the similarity of two clusters is the minimum intersimilarity    
the minimum similarity of a document from the    rst cluster
and a document from the second cluster.

once we have merged two clusters, how do we update the
similarity matrix?

again, this is simple:

sim(  i , (  k1       k2)) = min(sim(  i ,   k1), sim(  i ,   k2))

we measure the similarity of two clusters by computing the
diameter of the cluster that we would get if we merged
them.

30 / 66

1.0

0.8

0.6

0.4

0.2

0.0

nyse closing averages
hog prices tumble
oil prices slip
ag trade reform.
chrysler / latin america
japanese prime minister / mexico
fed holds interest rates steady
fed to keep interest rates steady
fed keeps interest rates steady
fed keeps interest rates steady
mexican markets
british ftse index
war hero colin powell
war hero colin powell
lloyd   s ceo questioned
lloyd   s chief / u.s. grilling
ohio blue cross
lawsuit against tobacco companies
suits against tobacco firms
indiana tobacco lawsuit
viag stays positive
most active stocks
compuserve reports loss
sprint / internet access service
planet hollywood
trocadero: tripling of revenues
back   to   school spending is up
german unions split
chains may raise prices
clinton signs law

c
o
m
p
l
e
t
e
-
l
i

n
k

d
e
n
d
r
o
g
r
a
m

s
i
z
e
.

a
b
o
u
t

t
h
e

s
a
m
e

w

e

c
a
n

c
r
e
a
t
e

a

w
i
t
h

t
w
o

c
l
u
s
t
e
r
s

o
f

2
-
c
l
u
s
t
e
r

c
l
u
s
t
e
r
i
n
g

t
h
e

s
i
n
g
l
e
-
l
i

n
k

o
n
e
.

m
o
r
e

b
a

l

a
n
c
e
d

t
h
a
n

d
e
n
d
r
o
g
r
a
m

i
s

m
u
c
h

n
o
t
i
c
e

t
h
a
t

t
h
i
s

3
1
/
6
6

exercise: compute single and complete link id91s

d1

d2

d3

d4

d5

d6

d7

d8

0

1

2

3

4

3

2

1

0

32 / 66

  
  
  
  
  
  
  
  
single-link id91

d1

d2

d3

d4

d5

d6

d7

d8

0

1

2

3

4

3

2

1

0

33 / 66

  
  
  
  
  
  
  
  
complete link id91

d1

d2

d3

d4

d5

d6

d7

d8

0

1

2

3

4

3

2

1

0

34 / 66

  
  
  
  
  
  
  
  
single-link vs. complete link id91

d1

d2

d3

d4

d1

d2

d3

d4

d5

d6

d7

d8

3

2

1

0

3

2

1

0

d5

d6

d7

d8

0

1

2

3

4

0

1

2

3

4

35 / 66

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
single-link: chaining

2
1
0

0 1 2 3 4 5 6 7 8 9 10 11 12

single-link id91 often produces long,

straggly clusters. for most applications, these are undesirable.

36 / 66

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
what 2-cluster id91 will complete-link produce?

d1

d2

d3

d4

d5

1
0

0 1 2 3 4 5 6 7

1 + 2      , 4, 5 + 2      , 6, 7       .

coordinates:

37 / 66

  
  
  
  
  
complete-link: sensitivity to outliers

d1

d2

d3

d4

d5

1
0

0 1 2 3 4 5 6 7

the complete-link id91 of this set splits d2 from its right
neighbors     clearly undesirable.
the reason is the outlier d1.
this shows that a single outlier can negatively a   ect the
outcome of complete-link id91.

single-link id91 does better in this case.

38 / 66

  
  
  
  
  
outline

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

39 / 66

centroid hac

the similarity of two clusters is the average intersimilarity    
the average similarity of documents from the    rst cluster with
documents from the second cluster.

a naive implementation of this de   nition is ine   cient
(o(n2)), but the de   nition is equivalent to computing the
similarity of the centroids:

sim-cent(  i ,   j ) = ~  (  i )    ~  (  j )

hence the name: centroid hac

note: this is the dot product, not cosine similarity!

40 / 66

exercise: compute centroid id91

5

4

3

2

1

0

d3

d1

d2

d4

d5

d6

0

1

2

3

4

5

6

7

41 / 66

  
  
  
  
  
  
centroid id91

d1

c   2

d2

5

4

3

2

1

0

d3

d4

d6

c  3

  1

d5

0

1

2

3

4

5

6

7

42 / 66

  
  
  
  
  
  
b
c
b
b
inversion in centroid id91

in an inversion, the similarity increases during a merge
sequence. results in an    inverted    dendrogram.
below: similarity of the    rst merger (d1     d2) is -4.0,
similarity of second merger ((d1     d2)     d3) is        3.5.

5
4
3
2
1
0

d3

d1

d2

0 1 2 3 4 5

   4
   3
   2
   1
0

d1

d2

d3

43 / 66

  
  
  
b
c
inversions

hierarchical id91 algorithms that allow inversions are
inferior.

the rationale for hierarchical id91 is that at any given
point, we   ve found the most coherent id91 for a given k .

intuitively: smaller id91s should be more coherent than
larger id91s.

an inversion contradicts this intuition: we have a large cluster
that is more coherent than one of its subclusters.

the fact that inversions can occur in centroid id91 is a
reason not to use it.

44 / 66

group-average agglomerative id91 (gaac)

gaac also has an    average-similarity    criterion, but does not
have inversions.

the similarity of two clusters is the average intrasimilarity    
the average similarity of all document pairs (including those
from the same cluster).

but we exclude self-similarities.

45 / 66

group-average agglomerative id91 (gaac)

again, a naive implementation is ine   cient (o(n2)) and there
is an equivalent, more e   cient, centroid-based de   nition:

sim-ga(  i ,   j ) =

1

(ni + nj )(ni + nj     1)

[( x
dm     i      j

~dm)2     (ni + nj )]

again, this is the dot product, not cosine similarity.

46 / 66

which hac id91 should i use?

don   t use centroid hac because of inversions.

in most cases: gaac is best since it isn   t subject to chaining
and sensitivity to outliers.

however, we can only use gaac for vector representations.

for other types of id194s (or if only
pairwise similarities for documents are available): use
complete-link.

there are also some applications for single-link (e.g., duplicate
detection in web search).

47 / 66

flat or hierarchical id91?

for high e   ciency, use    at id91 (or perhaps bisecting
id116)

for deterministic results: hac

when a hierarchical structure is desired: hierarchical algorithm

hac also can be applied if k cannot be predetermined (can
start without knowing k )

48 / 66

outline

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

49 / 66

major issue in id91     labeling

after a id91 algorithm    nds a set of clusters: how can
they be useful to the end user?

we need a pithy label for each cluster.

for example, in search result id91 for    jaguar   , the
labels of the three clusters could be    animal   ,    car   , and
   operating system   .

topic of this section: how can we automatically    nd good
labels for clusters?

50 / 66

exercise

come up with an algorithm for labeling clusters

input: a set of documents, partitioned into k clusters (   at
id91)

output: a label for each cluster

part of the exercise: what types of labels should we consider?
words?

51 / 66

discriminative labeling

to label cluster   , compare    with all other clusters

find terms or phrases that distinguish    from the other
clusters

we can use any of the feature selection criteria we introduced
in text classi   cation to identify discriminating terms: mutual
information,   2 and frequency.
(but the latter is actually not discriminative)

52 / 66

non-discriminative labeling

select terms or phrases based solely on information from the
cluster itself

e.g., select terms with high weights in the centroid (if we are
using a vector space model)

non-discriminative methods sometimes select frequent terms
that do not distinguish clusters.

for example, monday, tuesday, . . . in newspaper text

53 / 66

using titles for labeling clusters

terms and phrases are hard to scan and condense into a
holistic idea of what the cluster is about.

alternative: titles

for example, the titles of two or three documents that are
closest to the centroid.

titles are easier to scan than a list of phrases.

54 / 66

cluster labeling: example

# docs

4

622

9

1017

10

1259

centroid
plant mexico
oil
production
crude
power000re   nerygas
bpd
police
security rus-
sian people military
peace
told
groznycourt
00 000 tonnes traders
futures wheat prices
centsseptember
tonne

killed

labeling method
mutual information
plant oil production
crude bpd
barrels
mexico dolly capaci-
typetroleum
police killed military
security peace
told
troops
forcesrebels
people
delivery
tures
desk
000 00

traders fu-
tonnes
wheat prices

tonne

title

mexico: hurricane
dolly heads for mex-
ico coast

russia:
lebed meets
chief in chechnya

russia   s
rebel

usa: export business
- grain/oilseeds com-
plex

three methods: most prominent terms in centroid, di   erential labeling using
mi, title of doc closest to centroid
all three methods do a pretty good job.

55 / 66

outline

1 recap

2

introduction

3 single-link/complete-link

4 centroid/gaac

5 labeling clusters

6 variants

56 / 66

bisecting k -means: a top-down algorithm

start with all documents in one cluster

split the cluster into 2 using k -means

of the clusters produced so far, select one to split (e.g. select
the largest one)

repeat until we have produced the desired number of
clusters

57 / 66

bisecting k -means

bisectingkmeans(d1, . . . , dn )
1   0     {~d1, . . . , ~dn }
2 leaves     {  0}
3 for k     1 to k     1
4 do   k     pickclusterfrom(leaves)
5
6
7 return leaves

{  i ,   j }     kmeans(  k , 2)
leaves     leaves \ {  k }     {  i ,   j }

58 / 66

bisecting k -means

if we don   t generate a complete hierarchy, then a top-down
algorithm like bisecting k -means is much more e   cient than
hac algorithms.

but bisecting k -means is not deterministic.

there are deterministic versions of bisecting k -means (see
resources at the end), but they are much less e   cient.

59 / 66

e   cient single link id91

singlelinkid91(d1, . . . , dn , k )

i [n]     n
nbm[n]     arg maxx    {c [n][i ]:n6=i } x .sim

c [n][i ].index     i

do c [n][i ].sim     sim(dn, di )

1 for n     1 to n
2 do for i     1 to n
3
4
5
6
7 a     []
8 for n     1 to n     1
9 do i1     arg max{i :i [i ]=i } nbm[i ].sim
10
11
12
13
14
15
16
17
18 return a

i2     i [nbm[i1].index]
a.append(hi1, i2i)
for i     1 to n
do if i [i ] = i     i 6= i1     i 6= i2

then i [i ]     i1

if i [i ] = i2

nbm[i1]     arg maxx    {c [i1][i ]:i [i ]=i    i 6=i1} x .sim

then c [i1][i ].sim     c [i ][i1].sim     max(c [i1][i ].sim, c [i2][i ].sim)

60 / 66

time complexity of hac

the single-link algorithm we just saw is o(n2).
much more e   cient than the o(n3) algorithm we looked at
earlier!
there are also o(n2) algorithms for complete-link, centroid
and gaac.

61 / 66

combination similarities of the four algorithms

id91 algorithm sim(   , k1, k2)
single-link
complete-link
centroid
group-average

max(sim(   , k1), sim(   , k2))
min(sim(   , k1), sim(   , k2))
( 1
nm
(nm+n   )(nm+n      1) [(~vm + ~v   )2     (nm + n   )]

~vm)    ( 1
n   

~v   )

1

62 / 66

comparison of hac algorithms

method
single-link
complete-link min intersimilarity of any 2 docs   (n2 log n)

combination similarity
max intersimilarity of any 2 docs   (n2)

time compl.

group-average

average of all sims

centroid

average intersimilarity

  (n2 log n)

  (n2 log n)

optimal?
yes

comment
chaining e   ect

no

no

no

sensitive to outliers

best choice for
most applications

inversions can occur

63 / 66

what to do with the hierarchy?

use as is (e.g., for browsing as in yahoo hierarchy)

cut at a predetermined threshold
cut to get a predetermined number of clusters k

ignores hierarchy below and above cutting line.

64 / 66

take-away today

introduction to hierarchical id91

single-link and complete-link id91

centroid and group-average agglomerative id91 (gaac)

bisecting id116

how to label clusters automatically

65 / 66

resources

chapter 17 of iir
resources at http://cislmu.org

columbia newsblaster (a precursor of google news):
mckeown et al. (2002)
bisecting k -means id91: steinbach et al. (2000)
pddp (similar to bisecting k -means; deterministic, but also
less e   cient): saravesi and boley (2004)

66 / 66

