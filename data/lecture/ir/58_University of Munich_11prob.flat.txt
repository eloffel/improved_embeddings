introduction to information retrieval

http://informationretrieval.org

iir 11: probabilistic information retrieval

hinrich sch  utze

center for information and language processing, university of munich

2011-05-23

1 / 51

overview

1 recap

2 probabilistic approach to ir

3 basic id203 theory

4 id203 ranking principle

5 appraisal&extensions

2 / 51

outline

1 recap

2 probabilistic approach to ir

3 basic id203 theory

4 id203 ranking principle

5 appraisal&extensions

3 / 51

relevance feedback: basic idea

the user issues a (short, simple) query.

the search engine returns a set of documents.

user marks some docs as relevant, some as nonrelevant.

search engine computes a new representation of the
information need     should be better than the initial query.

search engine runs new query and returns new results.

new results have (hopefully) better recall.

4 / 51

rocchio illustrated

~  r     ~  nr
x

x
x

x
x

x

~qopt

~  r

~  nr

5 / 51

types of id183

manual thesaurus (maintained by editors, e.g., pubmed)

automatically derived thesaurus (e.g., based on co-occurrence
statistics)

query-equivalence based on query log mining (common on the
web as in the    palm    example)

6 / 51

id183 at search engines

main source of id183 at search engines: query logs
example 1: after issuing the query [herbs], users frequently
search for [herbal remedies].

       herbal remedies    is potential expansion of    herb   .

example 2: users searching for [   ower pix] frequently click on
the url photobucket.com/   ower. users searching for [   ower
clipart] frequently click on the same url.

          ower clipart    and       ower pix    are potential expansions of
each other.

7 / 51

take-away today

probabilistically grounded approach to ir

id203 ranking principle

models: bim, bm25

assumptions these models make

8 / 51

outline

1 recap

2 probabilistic approach to ir

3 basic id203 theory

4 id203 ranking principle

5 appraisal&extensions

9 / 51

relevance feedback from last lecture

previous lecture: in relevance feedback, the user marks
documents as relevant/nonrelevant

given some known relevant and nonrelevant documents, we
compute weights for non-query terms that indicate how likely
they will occur in relevant documents

today: develop a probabilistic approach for relevance
feedback and also a general probabilistic model for ir

10 / 51

probabilistic approach to retrieval

given a user information need (represented as a query) and a
collection of documents (transformed into document
representations), a system must determine how well the
documents satisfy the query

an ir system has an uncertain understanding of the user
query, and makes an uncertain guess of whether a document
satis   es the query

id203 theory provides a principled foundation for such
reasoning under uncertainty

probabilistic models exploit this foundation to estimate how
likely it is that a document is relevant to a query

11 / 51

probabilistic ir models at a glance

classical probabilistic retrieval model

id203 ranking principle

binary independence model, bestmatch25 (okapi)

id110s for text retrieval
language model approach to ir

important recent work, will be covered in the next lecture

probabilistic methods are one of the oldest but also one of the
currently hottest topics in ir

12 / 51

exercise: probabilistic model vs. other models

boolean model

probabilistic models support ranking and thus are better than
the simple boolean model.

vector space model

the vector space model is also a formally de   ned model that
supports ranking.
why would we want to look for an alternative to the vector
space model?

13 / 51

probabilistic vs. vector space model

vector space model: rank documents according to similarity
to query.

the notion of similarity does not translate directly into an
assessment of    is the document a good document to give to
the user or not?   

the most similar document can be highly relevant or
completely nonrelevant.

id203 theory is arguably a cleaner formalization of what
we really want an ir system to do: give relevant documents
to the user.

14 / 51

outline

1 recap

2 probabilistic approach to ir

3 basic id203 theory

4 id203 ranking principle

5 appraisal&extensions

15 / 51

basic id203 theory

for events a and b

joint id203 p(a     b) of both events occurring
id155 p(a|b) of event a occurring given that
event b has occurred

chain rule gives fundamental relationship between joint and
conditional probabilities:

p(ab) = p(a     b) = p(a|b)p(b) = p(b|a)p(a)

similarly for the complement of an event p(a):

p(ab) = p(b|a)p(a)

partition rule: if b can be divided into an exhaustive set of
disjoint subcases, then p(b) is the sum of the probabilities of
the subcases. a special case of this rule gives:

p(b) = p(ab) + p(ab)

16 / 51

basic id203 theory

bayes    rule for inverting conditional probabilities:

p(a|b) =

p(b|a)p(a)

p(b)

="

p(b|a)

px    {a,a} p(b|x )p(x )# p(a)

can be thought of as a way of updating probabilities:

start o    with prior id203 p(a) (initial estimate of how
likely event a is in the absence of any other information)

derive a posterior id203 p(a|b) after having seen the
evidence b, based on the likelihood of b occurring in the two
cases that a does or does not hold

odds of an event provide a kind of multiplier for how probabilities
change:

odds:

o(a) =

p(a)
p(a)

=

p(a)

1     p(a)

17 / 51

outline

1 recap

2 probabilistic approach to ir

3 basic id203 theory

4 id203 ranking principle

5 appraisal&extensions

18 / 51

the document ranking problem

ranked retrieval setup: given a collection of documents, the
user issues a query, and an ordered list of documents is
returned
assume binary notion of relevance: rd,q is a random
dichotomous variable, such that

rd ,q = 1 if document d is relevant w.r.t query q
rd ,q = 0 otherwise

probabilistic ranking orders documents decreasingly by their
estimated id203 of relevance w.r.t. query: p(r = 1|d, q)

assume that the relevance of each document is independent
of the relevance of other documents

19 / 51

id203 ranking principle (prp)

prp in brief

if the retrieved documents (w.r.t a query) are ranked
decreasingly on their id203 of relevance, then the
e   ectiveness of the system will be the best that is obtainable

prp in full

if [the ir] system   s response to each [query] is a ranking of the
documents [...] in order of decreasing id203 of relevance
to the [query], where the probabilities are estimated as
accurately as possible on the basis of whatever data have been
made available to the system for this purpose, the overall
e   ectiveness of the system to its user will be the best that is
obtainable on the basis of those data

20 / 51

binary independence model (bim)

traditionally used with the prp

assumptions:

   binary    (equivalent to boolean): documents and queries
represented as binary term incidence vectors

e.g., document d represented by vector ~x = (x1, . . . , xm ),
where xt = 1 if term t occurs in d and xt = 0 otherwise
di   erent documents may have the same vector representation

   independence   : no association between terms (not true, but
practically works -    naive    assumption of naive bayes models)

21 / 51

binary incidence matrix

anthony

and

cleopatra

julius
caesar tempest

the

haid113t othello macbeth

. . .

anthony

brutus

caesar

calpurnia

cleopatra

mercy

1
1
1
0
1
1
1

1
1
1
1
0
0
0

0
0
0
0
0
1
1

0
1
1
0
0
1
1

worser
. . .
each document is represented as a binary vector     {0, 1}|v |.

0
0
1
0
0
1
1

1
0
1
0
0
1
0

22 / 51

binary independence model

to make a probabilistic retrieval strategy precise, need to estimate
how terms in documents contribute to relevance

find measurable statistics (term frequency, document
frequency, document length) that a   ect judgments about
document relevance

combine these statistics to estimate the id203 p(r|d, q)
of document relevance

next: how exactly we can do this

23 / 51

binary independence model

p(r|d, q) is modeled using term incidence vectors as p(r|~x, ~q)

p(r = 1|~x, ~q) =

p(r = 0|~x, ~q) =

p(~x|r = 1, ~q)p(r = 1|~q)

p(~x|~q)

p(~x|r = 0, ~q)p(r = 0|~q)

p(~x|~q)

p(~x|r = 1, ~q) and p(~x|r = 0, ~q): id203 that if a
relevant or nonrelevant document is retrieved, then that
document   s representation is ~x

use statistics about the document collection to estimate these
probabilities

24 / 51

binary independence model

p(r|d, q) is modeled using term incidence vectors as p(r|~x, ~q)

p(r = 1|~x, ~q) =

p(r = 0|~x, ~q) =

p(~x|r = 1, ~q)p(r = 1|~q)

p(~x|~q)

p(~x|r = 0, ~q)p(r = 0|~q)

p(~x|~q)

p(r = 1|~q) and p(r = 0|~q): prior id203 of retrieving a
relevant or nonrelevant document for a query ~q
estimate p(r = 1|~q) and p(r = 0|~q) from percentage of
relevant documents in the collection

since a document is either relevant or nonrelevant to a query,
we must have that:

p(r = 1|~x, ~q) + p(r = 0|~x, ~q) = 1

25 / 51

deriving a ranking function for query terms (1)

given a query q, ranking documents by p(r = 1|d, q) is
modeled under bim as ranking them by p(r = 1|~x, ~q)

easier: rank documents by their odds of relevance (gives same
ranking)

o(r|~x, ~q) =

p(r = 1|~x, ~q)
p(r = 0|~x, ~q)

=

p(r=1|~q)p(~x |r=1,~q)

p(~x |~q)

p(r=0|~q)p(~x |r=0,~q)

p(~x |~q)

=

p(r = 1|~q)
p(r = 0|~q)

  

p(~x|r = 1, ~q)
p(~x|r = 0, ~q)

p(r=1|~q)
p(r=0|~q) is a constant for a given query - can be ignored

26 / 51

deriving a ranking function for query terms (2)

it is at this point that we make the naive bayes conditional
independence assumption that the presence or absence of a word in
a document is independent of the presence or absence of any other
word (given the query):

so:

p(~x|r = 1, ~q)
p(~x|r = 0, ~q)

=

p(xt|r = 1, ~q)
p(xt|r = 0, ~q)

m

yt=1

o(r|~x, ~q) = o(r|~q)   

p(xt|r = 1, ~q)
p(xt|r = 0, ~q)

m

yt=1

27 / 51

exercise

naive bayes conditional independence assumption: the presence or
absence of a word in a document is independent of the presence or
absence of any other word (given the query). why is this wrong?
good example? prp assumes that the relevance of each
document is independent of the relevance of other documents.
why is this wrong? good example?

28 / 51

deriving a ranking function for query terms (3)

since each xt is either 0 or 1, we can separate the terms:

o(r|~x, ~q) = o(r|~q)   yt:xt =1

p(xt = 1|r = 1, ~q)
p(xt = 1|r = 0, ~q)

   yt:xt =0

p(xt = 0|r = 1, ~q)
p(xt = 0|r = 0, ~q)

29 / 51

deriving a ranking function for query terms (4)

let pt = p(xt = 1|r = 1, ~q) be the id203 of a term
appearing in relevant document
let ut = p(xt = 1|r = 0, ~q) be the id203 of a term
appearing in a nonrelevant document

can be displayed as contingency table:

document

relevant (r = 1)

nonrelevant (r = 0)

term present
term absent

xt = 1
xt = 0

pt

1     pt

ut

1     ut

30 / 51

deriving a ranking function for query terms

additional simplifying assumption: terms not occurring in the
query are equally likely to occur in relevant and nonrelevant
documents

if qt = 0, then pt = ut

now we need only to consider terms in the products that appear in
the query:

o(r|~x, ~q) = o(r|~q)    yt:xt =qt =1

pt
ut

   yt:xt =0,qt =1

1     pt
1     ut

the left product is over query terms found in the document
and the right product is over query terms not found in the
document

31 / 51

deriving a ranking function for query terms

including the query terms found in the document into the right
product, but simultaneously dividing by them in the left product,
gives:

o(r|~x, ~q) = o(r|~q)    yt:xt =qt =1

pt(1     ut)
ut(1     pt)

1     pt
1     ut

   yt:qt =1

the left product is still over query terms found in the
document, but the right product is now over all query terms,
hence constant for a particular query and can be ignored.
    the only quantity that needs to be estimated to rank
documents w.r.t a query is the left product
hence the retrieval status value (rsv) in this model:

rsvd = log yt:xt =qt =1

pt(1     ut )
ut(1     pt )

= xt:xt =qt =1

log

pt(1     ut)
ut(1     pt)

32 / 51

deriving a ranking function for query terms

equivalent: rank documents using the log odds ratios for the terms
in the query ct :

ct = log

pt(1     ut )
ut(1     pt )

= log

pt

(1     pt)

    log

ut

1     ut

the odds ratio is the ratio of two odds: (i) the odds of the
term appearing if the document is relevant (pt/(1     pt)), and
(ii) the odds of the term appearing if the document is
nonrelevant (ut/(1     ut))
ct = 0: term has equal odds of appearing in relevant and
nonrelevant docs
ct positive: higher odds to appear in relevant documents
ct negative: higher odds to appear in nonrelevant documents

33 / 51

term weight ct in bim

ct = log

pt

(1   pt )     log ut
1   ut

functions as a term weight.

retrieval status value for document d: rsvd =pxt =qt =1 ct .

so bim and vector space model are identical on an
operational level . . .

. . . except that the term weights are di   erent.

in particular: we can use the same data structures (inverted
index etc) for the two models.

34 / 51

how to compute id203 estimates

for each term t in a query, estimate ct in the whole collection
using a contingency table of counts of documents in the collection,
where df t is the number of documents that contain term t:

documents

relevant

nonrelevant

term present
term absent

xt = 1
xt = 0
total

s

df t     s

s     s

(n     df t )     (s     s) n     df t

s

n     s

n

total
df t

pt = s/s

ut = (df t     s)/(n     s)

ct = k (n, df t , s, s) = log

s/(s     s)

(df t     s)/((n     df t)     (s     s))

35 / 51

avoiding zeros

if any of the counts is a zero, then the term weight is not
well-de   ned.

maximum likelihood estimates do not work for rare events.

to avoid zeros: add 0.5 to each count (expected likelihood
estimation = ele)

for example, use s     s + 0.5 in formula for s     s

36 / 51

exercise

query: obama health plan

doc1: obama rejects allegations about his own bad
health

doc2: the plan is to visit obama

doc3: obama raises concerns with us health plan
reforms

estimate the id203 that the above documents are relevant to
the query. use a contingency table. these are the only three
documents in the collection

37 / 51

simplifying assumption

assuming that relevant documents are a very small
percentage of the collection, approximate statistics for
nonrelevant documents by statistics from the whole collection
hence, ut (the id203 of term occurrence in nonrelevant
documents for a query) is df t /n and

log[(1     ut )/ut ] = log[(n     df t)/df t ]     log n/df t

this should look familiar to you . . .

the above approximation cannot easily be extended to
relevant documents

38 / 51

id203 estimates in relevance feedback

statistics of relevant documents (pt ) in relevance feedback
can be estimated using id113 or ele
(add 0.5).

use the frequency of term occurrence in known relevant
documents.

this is the basis of probabilistic approaches to relevance
feedback weighting in a feedback loop

the exercise we just did was a probabilistic relevance feedback
exercise since we were assuming the availability of relevance
judgments.

39 / 51

id203 estimates in adhoc retrieval

ad-hoc retrieval: no user-supplied relevance judgments
available
in this case: assume that pt is constant over all terms xt in
the query and that pt = 0.5
each term is equally likely to occur in a relevant document,
and so the pt and (1     pt) factors cancel out in the expression
for rsv
weak estimate, but doesn   t disagree violently with
expectation that query terms appear in many but not all
relevant documents
combining this method with the earlier approximation for ut ,
the document ranking is determined simply by which query
terms occur in documents scaled by their idf weighting
for short documents (titles or abstracts) in one-pass retrieval
situations, this estimate can be quite satisfactory

40 / 51

outline

1 recap

2 probabilistic approach to ir

3 basic id203 theory

4 id203 ranking principle

5 appraisal&extensions

41 / 51

history and summary of assumptions

among the oldest formal models in ir

maron & kuhns, 1960: since an ir system cannot predict with
certainty which document is relevant, we should deal with
probabilities

assumptions for getting reasonable approximations of the
needed probabilities (in the bim):

boolean representation of documents/queries/relevance
term independence
out-of-query terms do not a   ect retrieval
document relevance values are independent

42 / 51

how di   erent are vector space and bim?

they are not that di   erent.

in either case you build an information retrieval scheme in the
exact same way.

for probabilistic ir, at the end, you score queries not by
cosine similarity and tf-idf in a vector space, but by a slightly
di   erent formula motivated by id203 theory.

next: how to add term frequency and length id172 to
the probabilistic model.

43 / 51

okapi bm25: overview

okapi bm25 is a probabilistic model that incorporates term
frequency (i.e., it   s nonbinary) and length id172.

bim was originally designed for short catalog records of fairly
consistent length, and it works reasonably in these contexts

for modern full-text search collections, a model should pay
attention to term frequency and document length

bestmatch25 (a.k.a bm25 or okapi) is sensitive to these
quantities

bm25 is one of the most widely used and robust retrieval
models

44 / 51

okapi bm25: starting point

the simplest score for document d is just idf weighting of the
query terms present in the document:

rsvd =xt   q

log

n
df t

45 / 51

okapi bm25 basic weighting

improve idf term [log n/df] by factoring in term frequency
and document length.

rsvd =xt   q

log(cid:20) n
df t(cid:21)   

(k1 + 1)tf td

k1((1     b) + b    (ld /lave)) + tf td

tf td : term frequency in document d
ld (lave): length of document d (average document length in
the whole collection)
k1: tuning parameter controlling the document term
frequency scaling

b: tuning parameter controlling the scaling by document
length

46 / 51

exercise

interpret bm25 weighting formula for k1 = 0
interpret bm25 weighting formula for k1 = 1 and b = 0
interpret bm25 weighting formula for k1 7        and b = 0
interpret bm25 weighting formula for k1 7        and b = 1

47 / 51

okapi bm25 weighting for long queries

for long queries, use similar weighting for query terms

rsvd =xt   q(cid:20)log

n

df t(cid:21)  

(k1 + 1)tf td

k1((1     b) + b    (ld /lave)) + tf td

  

(k3 + 1)tf tq

k3 + tf tq

tf tq: term frequency in the query q
k3: tuning parameter controlling term frequency scaling of the
query

no length id172 of queries (because retrieval is being
done with respect to a single    xed query)

the above tuning parameters should ideally be set to optimize
performance on a development test collection. in the absence
of such optimization, experiments have shown reasonable
values are to set k1 and k3 to a value between 1.2 and 2 and
b = 0.75

48 / 51

which ranking model should i use?

i want something basic and simple     use vector space with
tf-idf weighting.

i want to use a state-of-the-art ranking model with excellent
performance     use language models or bm25 with tuned
parameters

in between: bm25 or language models with no or just one
tuned parameter

49 / 51

take-away today

probabilistically grounded approach to ir

id203 ranking principle

models: bim, bm25

assumptions these models make

50 / 51

resources

chapter 11 of iir

resources at http://cislmu.org

51 / 51

