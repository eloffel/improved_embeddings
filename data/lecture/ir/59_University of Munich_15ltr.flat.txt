introduction to information retrieval

http://informationretrieval.org

iir 15-2: learning to rank

hinrich sch  utze

center for information and language processing, university of munich

2011-06-05

1 / 53

overview

1 recap

2 zone scoring

3 machine-learned scoring

4 ranking id166s

2 / 53

outline

1 recap

2 zone scoring

3 machine-learned scoring

4 ranking id166s

3 / 53

a linear classi   er in 3d

a linear classi   er in 3d is
a plane described by the
equation
w1d1 + w2d2 + w3d3 =   
example for a 3d linear
classi   er
points (d1 d2 d3) with
w1d1 + w2d2 + w3d3       
are in the class c.
points (d1 d2 d3) with
w1d1 + w2d2 + w3d3 <   
are in the complement
class c.

4 / 53

linear classi   ers

many common text classi   ers are linear classi   ers: naive
bayes, rocchio, id28, least squares regression,
linear support vector machines etc.
each method has a di   erent way of selecting the separating
hyperplane

huge di   erences in performance on test documents

5 / 53

support vector machines

binary classi   cation
problem

simple id166s are
linear classi   ers.

criterion: being
maximally far away
from any data point
    determines
classi   er margin

linear separator
position de   ned by
support vectors

maximum
margin
decision
hyperplane

support vectors

margin is
maximized

6 / 53

b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
optimization problem solved by id166s
find ~w and b such that:

1

2 ~w t ~w is minimized (because |~w| =    ~w t ~w ), and
for all {(~xi , yi )}, yi (~w t~xi + b)     1

7 / 53

which machine learning method to choose

is there a learning method that is optimal for all text
classi   cation problems?

no, because there is a tradeo    between bias and variance.
factors to take into account:

how much training data is available?
how simple/complex is the problem? (linear vs. nonlinear
decision boundary)
how noisy is the problem?
how stable is the problem over time?

for an unstable problem, it   s better to use a simple and robust
classi   er.

8 / 53

take-away today

basic idea of learning to rank (ltr): we use machine learning
to learn the relevance score (retrieval status value) of a
document with respect to a query.

zone scoring: a particularly simple instance of ltr

machine-learned scoring as a general approach to ranking

ranking id166s

9 / 53

outline

1 recap

2 zone scoring

3 machine-learned scoring

4 ranking id166s

10 / 53

main idea

the aim of term weights (e.g., tf-idf) is to measure term
salience.

the sum of term weights is a measure of the relevance of a
document to a query and the basis for ranking.

now we view this ranking problem as a machine learning
problem     we will learn the weighting or, more generally, the
ranking.

term weights can be learned using training examples that have
been judged.

this methodology falls under a general class of approaches
known as machine learned relevance or learning to rank.

11 / 53

learning weights

main methodology

given a set of training examples, each of which is a tuple of:
a query q, a document d, a relevance judgment for d on q

simplest case: r(d , q) is either relevant (1) or nonrelevant (0)
more sophisticated cases: graded relevance judgments

learn weights from these examples, so that the learned scores
approximate the relevance judgments in the training examples

12 / 53

binary independence model (bim)

is what bim does a form of learning to rank?
recap bim:

estimate classi   er of id203 of relevance on training set
apply to all documents
rank documents according to id203 of relevance

13 / 53

learning to rank vs. text classi   cation

both are machine learning approaches
text classi   cation, bim and relevance feedback (if solved by
text classi   cation) are query-speci   c.

we need a query-speci   c training set to learn the ranker.
we need to learn a new ranker for each query.

learning to rank usually refers to query-independent ranking.

we learn a single classi   er.

we can then rank documents for a query that we don   t have
any relevance judgments for.

14 / 53

learning to rank: exercise

one approach to learning to rank is to represent each
query-document pair as a data point, represented as a vector.
we have two classes.

class 1: the document is relevant to the query.
class 2: the document is not relevant to the query.

this is a standard classi   cation problem, except that the data
points are query-document pairs (as opposed to documents).

documents are ranked according to id203 of relevance of
corresponding query-document pairs.

what features/dimensions would you use to represent a
query-document pair?

15 / 53

simple form of learning to rank: zone scoring

given: a collection where documents have three zones (a.k.a.
   elds): author, title, body

weighted zone scoring requires a separate weight for each
zone, e.g. g1, g2, g3
not all zones are equally important:
e.g. author < title < body
    g1 = 0.2, g2 = 0.3, g3 = 0.5 (so that they add up to 1)
score for a zone = 1 if the query term occurs in that zone, 0
otherwise (boolean)

example
query term appears in title and body only
document score: (0.3    1) + (0.5    1) = 0.8.

16 / 53

general form of weighted zone scoring

given query q and document d, weighted zone scoring assigns to
the pair (q, d) a score in the interval [0,1] by computing a linear
combination of document zone scores, where each zone contributes
a value.

consider a set of documents, which have l zones
let g1, ..., gl     [0, 1], such that pl
for 1     i     l , let si be the boolean score denoting a match
(or non-match) between q and the i th zone

i =1 gi = 1

si = 1 if a query term occurs in zone i, 0 otherwise

weighted zone score a.k.a ranked boolean retrieval
rank documents according to pl

i =1 gi si

17 / 53

learning weights in weighted zone scoring

weighted zone scoring may be viewed as learning a linear
function of the boolean match scores contributed by the
various zones.
no free lunch: labor-intensive assembly of user-generated
relevance judgments from which to learn the weights
especially in a dynamic collection (such as the web)
major search engines put considerable resources into creating
large training sets for learning to rank.

good news: once you have a large enough training set, the
problem of learning the weights gi reduces to a simple
optimization problem.

18 / 53

learning weights in weighted zone scoring: simple case

let documents have two zones: title, body

the weighted zone scoring formula we saw before:

l

x
i =1

gi si

given q, d, st (d , q) = 1 if a query term occurs in title, 0
otherwise; sb (d , q) = 1 if a query term occurs in body, 0
otherwise

we compute a score between 0 and 1 for each (d , q) pair
using st (d , q) and sb(d , q) by using a constant g     [0, 1]:

score(d , q) = g    st (d , q) + (1     g )    sb (d , q)

19 / 53

learning weights: determine g from training examples

example

  j
  1
  2
  3
  4
  5
  6
  7

dj
37
37
238
238
1741
2094
3194

qj
linux
penguin
system
penguin
kernel
driver
driver

st
1
0
0
0
1
0
1

r (dj , qj )
sb
1 relevant
1 nonrelevant
1 relevant
0 nonrelevant
1 relevant
1 relevant
0 nonrelevant

training examples: triples of the form   j = (dj , qj , r (dj , qj ))
a given training document dj and a given training query qj
are assessed by a human who decides r (dj , qj ) (either relevant
or nonrelevant)

20 / 53

learning weights: determine g from training examples

example

example docid query

  1
  2
  3
  4
  5
  6
  7

37
37
238
238
1741
2094
3194

linux
penguin
system
penguin
kernel
driver
driver

st
1
0
0
0
1
0
1

judgment

sb
1 relevant
1 nonrelevant
1 relevant
0 nonrelevant
1 relevant
1 relevant
0 nonrelevant

for each training example   j we have boolean values
st (dj , qj ) and sb (dj , qj ) that we use to compute a score:

score(dj , qj ) = g    st (dj , qj ) + (1     g )    sb (dj , qj )

21 / 53

learning weights

we compare this score score(dj , qj ) with the human relevance
judgment for the same query-document pair (dj , qj ).
we de   ne the error of the scoring function with weight g as

  (g ,   j ) = (r (dj , qj )     score(dj , qj ))2

then, the total error of a set of training examples is given by

  (g ,   j )

x

j

the problem of learning the constant g from the given
training examples then reduces to picking the value of g that
minimizes the total error.

22 / 53

minimizing the total error   : example (1)

training examples

example docid query

  1
  2
  3
  4
  5
  6
  7

37
37
238
238
1741
2094
3194

linux
penguin
system
penguin
kernel
driver
driver

st
1
0
0
0
1
0
1

sb
1
1
1
0
1
1
0

judgment
1 (relevant)
0 (nonrelevant)
1 (relevant)
0 (nonrelevant)
1 (relevant)
1 (relevant)
0 (nonrelevant)

compute score:
score(dj , qj ) = g    st (dj , qj ) + (1     g )    sb(dj , qj )
compute total error: pj   (g ,   j ), where
  (g ,   j ) = (r (dj , qj )     score(dj , qj ))2
pick the value of g that minimizes the total error

23 / 53

minimizing the total error   : example (2)

compute score score(dj , qj )
score(d1, q1) = g    1 + (1     g )    1 = g + 1     g = 1
score(d2, q2) = g    0 + (1     g )    1 = 0 + 1     g = 1     g
score(d3, q3) = g    0 + (1     g )    1 = 0 + 1     g = 1     g
score(d4, q4) = g    0 + (1     g )    0 = 0 + 0 = 0
score(d5, q5) = g    1 + (1     g )    1 = g + 1     g = 1
score(d6, q6) = g    0 + (1     g )    1 = 0 + 1     g = 1     g
score(d7, q7) = g    1 + (1     g )    0 = g + 0 = g
compute total error pj   (g ,   j )
(1   1)2 +(0   1+g )2 +(1   1+g )2 +(0   0)2 +(1   1)2 +(1   1+
g )2+(0   g )2 = 0+(   1+g )2+g 2+0+0+g 2+g 2 = 1   2g +4g 2
pick the value of g that minimizes the total error
setting derivative to 0, gives you a minimum of g = 1
4 .

24 / 53

weight g that minimizes error in the general case

g =

n10r + n01n

n10r + n10n + n01r + n01n

n... are the counts of rows of the training set table with the
corresponding properties:
sb = 0
sb = 0
sb = 1
sb = 1

document relevant
document nonrelevant
document relevant
document nonrelevant

st = 1
st = 1
st = 0
st = 0

n10r
n10n
n01r
n01n

derivation: see book

note that we ignore documents that have 0 match scores for
both zones or 1 match scores for both zones     the value of g
does not change their    nal score.

25 / 53

exercise: compute g that minimizes the error

docid query

37
37
238
238
238
1741
2094
3194
3194

linux
penguin
system
penguin
redmond
kernel
driver
driver
redmond

st
0
1
1
1
0
0
1
0
0

judgment

sb
0 relevant
1 nonrelevant
0 relevant
1 nonrelevant
1 nonrelevant
0 relevant
0 relevant
1 nonrelevant
0 nonrelevant

  1
  2
  3
  4
  5
  6
  7
  8
  9

26 / 53

solution

2
0
0
2

n10r
n10n
n01r
n01n

st = 1
st = 1
st = 0
st = 0

sb = 0
sb = 0
sb = 1
sb = 1

document relevant
document nonrelevant
document relevant
document nonrelevant

g =

n10r + n01n

n10r + n10n + n01r + n01n

=

2 + 2

2 + 0 + 2 + 0

= 1

27 / 53

outline

1 recap

2 zone scoring

3 machine-learned scoring

4 ranking id166s

28 / 53

more general setup of machine learned scoring

so far, we have considered a case where we combined match
scores (boolean indicators of relevance).

now consider more general factors that go beyond boolean
functions of query term presence in document zones.

29 / 53

two examples of typical features

the vector space cosine similarity between query and
document (denoted   )
the minimum window width within which the query terms lie
(denoted   )

query term proximity is often indicative of topical relevance.

thus, we have one feature that captures overall
query-document similarity and one features that captures
proximity of query terms in the document.

30 / 53

learning to rank setup for these two features

example

example docid query

  1
  2
  3
  4
  5
  6
  7

37
37
238
238
1741
2094
3191

  
0.032
linux
penguin
0.02
operating system 0.043
0.004
runtime
0.022
kernel layer
device driver
0.03
0.027
device driver

   judgment
3
4
2
2
3
2
5

relevant
nonrelevant
relevant
nonrelevant
relevant
relevant
nonrelevant

   is the cosine score.    is the window width. this is exactly the

same setup as for zone scoring except we now have more complex
features that capture whether a document is relevant to a query.

31 / 53

graphic representation of the training set

this should look familiar.

32 / 53

in this case: ltr approach learns a linear classi   er in 2d

a linear classi   er in 2d is
a line described by the
equation w1d1 + w2d2 =   
example for a 2d linear
classi   er
points (d1 d2) with
w1d1 + w2d2        are in
the class c.
points (d1 d2) with
w1d1 + w2d2 <    are in
the complement class c.

33 / 53

learning to rank setup for two features

again, two classes: relevant = 1 and nonrelevant = 0

we now seek a scoring function that combines the values of
the features to generate a value that is (close to) 0 or 1.

we wish this function to be in agreement with our set of
training examples as much as possible.

a linear classi   er is de   ned by an equation of the form:

score(d , q) = score(  ,   ) = a   + b   + c,

where we learn the coe   cients a, b, c from training data.
regression vs. classi   cation

we have only covered binary classi   cation so far.
we can also cast the problem as a regression problem.
this is what we did for zone scoring just now.

34 / 53

di   erent geometric interpretation of what   s happening

the function score(  ,   )
represents a plane
   hanging above    the
   gure.

ideally this plane assumes
values close to 1 above
the points marked r, and
values close to 0 above
the points marked n.

0.05 

! 
 
e
r
o
c
s
 
e
n
s
o
c

i

0.025 

r!

0 

2 

r!

r!
r!

n!

n!

r!

r!

r!

n!

n!

n!

r!
n!

r!
r!

r!

n!

n!

n!

n!

3 

4 

5 

term proximity " 

35 / 53

linear classi   cation in this case

we pick a threshold   .

if score(  ,   ) >   , we
declare the document
relevant, otherwise we
declare it nonrelevant.

as before, all points that
satisfy score(  ,   ) =   
form a line (dashed here)
    linear classi   er that
separates relevant from
nonrelevant instances.

0.05 

! 
 
e
r
o
c
s
 
e
n
s
o
c

i

0.025 

r!

0 

2 

r!

r!
r!

n!

n!

r!

r!

r!

n!

n!

n!

r!
n!

r!
r!

r!

n!

n!

n!

n!

3 

4 

5 

term proximity " 

36 / 53

summary

the problem of making a binary relevant/nonrelevant
judgment is cast as a classi   cation or regression problem,
based on a training set of query-document pairs and
associated relevance judgments.

in the example: the classi   er corresponds to a line
score(  ,   ) =    in the   -   plane.

in principle, any method learning a linear classi   er (including
least squares regression) can be used to    nd this line.

big advantage of learning to rank: we can avoid hand-tuning
scoring functions and simply learn them from training data.

bottleneck of learning to rank: maintaining a representative
set of training examples whose relevance assessments must be
made by humans.

37 / 53

learning to rank for more than two features

the approach can be readily generalized to a large number of
features.

in addition to cosine similarity and query term window, there
are lots of other indicators of relevance: id95-style
measures, document age, zone contributions, document
length, etc.

if these measures can be calculated for a training document
collection with relevance judgments, any number of such
measures can be used to machine-learn a classi   er.

38 / 53

ltr features used by microsoft research (1)

zones: body, anchor, title, url, whole document

features derived from standard ir models: query term
number, query term ratio, length, idf, sum of term frequency,
min of term frequency, max of term frequency, mean of term
frequency, variance of term frequency, sum of length
normalized term frequency, min of length normalized term
frequency, max of length normalized term frequency, mean of
length normalized term frequency, variance of length
normalized term frequency, sum of tf-idf, min of tf-idf, max of
tf-idf, mean of tf-idf, variance of tf-idf, boolean model, bm25

39 / 53

ltr features used by microsoft research (2)

language model features: lmir.abs, lmir.dir, lmir.jm

web-speci   c features: number of slashes in url, length of url,
inlink number, outlink number, id95, siterank

spam features: qualityscore

usage-based features: query-url click count, url click count,
url dwell time

see link in resources for more information

40 / 53

shortcoming of our ltr approach so far

approaching ir ranking like we have done so far is not
necessarily the right way to think about the problem.

statisticians normally    rst divide problems into classi   cation
problems (where a categorical variable is predicted) versus
regression problems (where a real number is predicted).

in between is the specialized    eld of ordinal regression where a
ranking is predicted.

machine learning for ad hoc retrieval is most properly thought
of as an ordinal regression problem, where the goal is to rank
a set of documents for a query, given training data of the
same sort.

next up: ranking id166s, a machine learning method that
learns an ordering directly.

41 / 53

exercise

example

example docid query

  1
  2
  3
  4
  5
  6

37
37
238
238
518
518

linux
linux
operating system
operating system
runtime
runtime

cosine
0.051
0.04
0.3
0.12
0.04
0.005

   judgment
3
5
2
3
2
10

relevant
nonrelevant
relevant
relevant
relevant
nonrelevant

give parameters a, b, c of a line a   + b   + c that separates
relevant from nonrelevant.

42 / 53

outline

1 recap

2 zone scoring

3 machine-learned scoring

4 ranking id166s

43 / 53

basic setup for ranking id166s

as before we begin with a set of judged query-document pairs.

but we do not represent them as query-document-judgment
triples.

instead, we ask judges, for each training query q, to order the
documents that were returned by the search engine with
respect to relevance to the query.
we again construct a vector of features   j =   (dj , q) for each
query-document pair     exactly as we did before.
for two documents di and dj , we then form the vector of
feature di   erences:

  (di , dj , q) =   (di , q)       (dj , q)

44 / 53

training a ranking id166

vector of feature di   erences:   (di , dj , q) =   (di , q)       (dj , q)
by hypothesis, one of di and dj has been judged more
relevant.
notation: we write di     dj for    di precedes dj in the results
ordering   .
if di is judged more relevant than dj , then we will assign the
vector   (di , dj , q) the class yijq = +1; otherwise    1.
this gives us a training set of pairs of vectors and
   precedence indicators   . each of the vectors is computed as
the di   erence of two query-document vectors.

we can then train an id166 on this training set with the goal
of obtaining a classi   er that returns

~w t  (di , dj , q) > 0 i    di     dj

45 / 53

advantages of ranking id166s vs. classi   cation/regression

documents can be evaluated relative to other candidate
documents for the same query, rather than having to be
mapped to a global scale of goodness.

this often is an easier problem to solve since just a ranking is
required rather than an absolute measure of relevance.

especially germane in web search, where the ranking at the
very top of the results list is exceedingly important.

46 / 53

why simple ranking id166s don   t work that well

ranking id166s treat all ranking violations alike.

but some violations are minor problems, e.g., getting the order
of two relevant documents wrong.
other violations are big problems, e.g., ranking a nonrelevant
document ahead of a relevant document.

some queries have many relevant documents, others few.

depending on the training regime, too much emphasis may be
put on queries with many relevant documents.

in most ir settings, getting the order of the top documents
right is key.

in the simple setting we have described, top and bottom ranks
will not be treated di   erently.

    learning-to-rank frameworks actually used in ir are more
complicated than what we have presented here.

47 / 53

example for superior performance of ltr

id166 algorithm that directly optimizes map (as opposed to
ranking). proposed by: yue, finley, radlinski, joachims, acm
sigir 2007. performance compared to state-of-the-art models:
cosine, tf-idf, bm25, language models (dirichlet and
jelinek-mercer)

to-rank clearly better than non-machine-learning
approaches

learning-

48 / 53

optimizing scaling/representation of features

both of the methods that we   ve seen treat the features as
given and do not attempt to modify the basic representation
of the query-document pairs.

much of traditional ir weighting involves nonlinear scaling of
basic measurements (such as log-weighting of term frequency,
or idf).

at the present time, machine learning is very good at
producing optimal weights for features in a linear
combination, but it is not good at coming up with good
nonlinear scalings of basic measurements.

this area remains the domain of human feature engineering.

49 / 53

assessment of learning to rank

the idea of learning to rank is old.

early work by norbert fuhr and william s. cooper

but it is only very recently that su   cient machine learning
knowledge, training document collections, and computational
power have come together to make this method practical and
exciting.

while skilled humans can do a very good job at de   ning
ranking functions by hand, hand tuning is di   cult, and it has
to be done again for each new document collection and class
of users.

the more features are used in ranking, the more di   cult it is
to manually integrate them into one ranking function.
web search engines use a large number of features     web
search engines need some form of learning to rank.

50 / 53

exercise

write down the training set from the last exercise as a training set
for a ranking id166. recall: vector of feature di   erences:

  (di , dj , q) =   (di , q)       (dj , q),
~w t  (di , dj , q) > 0
i    di     dj

example

example docid query

cosine    judgment

  1
  2
  3
  4

37
37
238
238

linux
penguin
operating system
runtime

0.03
0.04
0.04
0.02

3
5
2
3

relevant
nonrelevant
relevant
nonrelevant

51 / 53

take-away today

basic idea of learning to rank (ltr): we use machine learning
to learn the relevance score (retrieval status value) of a
document with respect to a query.

zone scoring: a particularly simple instance of ltr

machine-learned scoring as a general approach to ranking

ranking id166s

52 / 53

resources

chapters 6 and 15 of iir
resources at http://cislmu.org

references to ranking id166 results
microsoft learning to rank datasets

53 / 53

