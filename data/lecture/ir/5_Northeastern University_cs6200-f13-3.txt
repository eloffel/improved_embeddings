cs6200	
   

informa.on	
   retrieval	
   

david	
   smith	
   

college	
   of	
   computer	
   and	
   informa.on	
   science	
   

northeastern	
   university	
   

indexing	
   process	
   

web	
   crawler	
   

       finds	
   and	
   downloads	
   web	
   pages	
   automa.cally	
   

      provides	
   the	
   collec.on	
   for	
   searching	
   

       web	
   is	
   huge	
   and	
   constantly	
   growing	
   
       web	
   is	
   not	
   under	
   the	
   control	
   of	
   search	
   engine	
   

providers	
   

       web	
   pages	
   are	
   constantly	
   changing	
   
       crawlers	
   also	
   used	
   for	
   other	
   types	
   of	
   data	
   

retrieving	
   web	
   pages	
   

       every	
   page	
   has	
   a	
   unique	
   uniform	
   resource	
   

locator	
   (url)	
   

       web	
   pages	
   are	
   stored	
   on	
   web	
   servers	
   that	
   use	
   

http	
   to	
   exchange	
   informa.on	
   with	
   client	
   
somware	
   

       e.g.,	
   

retrieving	
   web	
   pages	
   

       web	
   crawler	
   client	
   program	
   connects	
   to	
   a	
   

domain	
   name	
   system	
   (dns)	
   server	
   

       dns	
   server	
   translates	
   the	
   hostname	
   into	
   an	
   

internet	
   protocol	
   (ip)	
   address	
   

       crawler	
   then	
   apempts	
   to	
   connect	
   to	
   server	
   

host	
   using	
   speci   c	
   port	
   

       amer	
   connec.on,	
   crawler	
   sends	
   an	
   http	
   

request	
   to	
   the	
   web	
   server	
   to	
   request	
   a	
   page	
   
      usually	
   a	
   get	
   request	
   

crawling	
   the	
   web	
   

web	
   crawler	
   

queue	
   

urls	
   given	
   to	
   it	
   as	
   parameters	
   

       starts	
   with	
   a	
   set	
   of	
   seeds,	
   which	
   are	
   a	
   set	
   of	
   
       seeds	
   are	
   added	
   to	
   a	
   url	
   request	
   queue	
   
       crawler	
   starts	
   fetching	
   pages	
   from	
   the	
   request	
   
       downloaded	
   pages	
   are	
   parsed	
   to	
      nd	
   link	
   tags	
   
that	
   might	
   contain	
   other	
   useful	
   urls	
   to	
   fetch	
   
       new	
   urls	
   added	
   to	
   the	
   crawler   s	
   request	
   
       con.nue	
   un.l	
   no	
   more	
   new	
   urls	
   or	
   disk	
   full	
   

queue,	
   or	
   fron2er	
   

web	
   crawling	
   

responses	
   to	
   requests	
   

       web	
   crawlers	
   spend	
   a	
   lot	
   of	
   .me	
   wai.ng	
   for	
   
       to	
   reduce	
   this	
   ine   ciency,	
   web	
   crawlers	
   use	
   
threads	
   and	
   fetch	
   hundreds	
   of	
   pages	
   at	
   once	
   
       crawlers	
   could	
   poten.ally	
      ood	
   sites	
   with	
   
       to	
   avoid	
   this	
   problem,	
   web	
   crawlers	
   use	
   

requests	
   for	
   pages	
   

politeness	
   policies	
   
      e.g.,	
   delay	
   between	
   requests	
   to	
   same	
   web	
   server	
   

controlling	
   crawling	
   

       even	
   crawling	
   a	
   site	
   slowly	
   will	
   anger	
   some	
   

web	
   server	
   administrators,	
   who	
   object	
   to	
   any	
   
copying	
   of	
   their	
   data	
   

       robots.txt	
      le	
   can	
   be	
   used	
   to	
   control	
   crawlers	
   

simple	
   crawler	
   thread	
   

freshness	
   

       web	
   pages	
   are	
   constantly	
   being	
   added,	
   

deleted,	
   and	
   modi   ed	
   

       web	
   crawler	
   must	
   con.nually	
   revisit	
   pages	
   it	
   

has	
   already	
   crawled	
   to	
   see	
   if	
   they	
   have	
   
changed	
   in	
   order	
   to	
   maintain	
   the	
   freshness	
   of	
   
the	
   document	
   collec.on	
   
      stale	
   copies	
   no	
   longer	
   re   ect	
   the	
   real	
   contents	
   of	
   
the	
   web	
   pages	
   

freshness	
   

       http	
   protocol	
   has	
   a	
   special	
   request	
   type	
   

called	
   head	
   that	
   makes	
   it	
   easy	
   to	
   check	
   for	
   
page	
   changes	
   
      returns	
   informa.on	
   about	
   page,	
   not	
   page	
   itself	
   

freshness	
   

       not	
   possible	
   to	
   constantly	
   check	
   all	
   pages	
   
      must	
   check	
   important	
   pages	
   and	
   pages	
   that	
   
change	
   frequently	
   

       freshness	
   is	
   the	
   propor.on	
   of	
   pages	
   that	
   are	
   

fresh	
   

       op.mizing	
   for	
   this	
   metric	
   can	
   lead	
   to	
   bad	
   

decisions,	
   such	
   as	
   not	
   crawling	
   popular	
   sites	
   

       age	
   is	
   a	
   beper	
   metric	
   

freshness	
   vs.	
   age	
   

age	
   

       expected	
   age	
   of	
   a	
   page	
   t	
   days	
   amer	
   it	
   was	
   last	
   

crawled:	
   

       web	
   page	
   updates	
   follow	
   the	
   poisson	
   

distribu.on	
   on	
   average	
   
      .me	
   un.l	
   the	
   next	
   update	
   is	
   governed	
   by	
   an	
   
exponen.al	
   distribu.on	
   

age	
   

       older	
   a	
   page	
   gets,	
   the	
   more	
   it	
   costs	
   not	
   to	
   

crawl	
   it	
   
      e.g.,	
   expected	
   age	
   with	
   mean	
   change	
   frequency	
   	
   	
   	
   
  	
   =	
   1/7	
   (one	
   change	
   per	
   week)	
   

focused	
   crawling	
   

       apempts	
   to	
   download	
   only	
   those	
   pages	
   that	
   

are	
   about	
   a	
   par.cular	
   topic	
   
      used	
   by	
   ver2cal	
   search	
   applica.ons	
   

       rely	
   on	
   the	
   fact	
   that	
   pages	
   about	
   a	
   topic	
   tend	
   
to	
   have	
   links	
   to	
   other	
   pages	
   on	
   the	
   same	
   topic	
   
      popular	
   pages	
   for	
   a	
   topic	
   are	
   typically	
   used	
   as	
   
seeds	
   

       crawler	
   uses	
   text	
   classi   er	
   to	
   decide	
   whether	
   

a	
   page	
   is	
   on	
   topic	
   

deep	
   web	
   

       sites	
   that	
   are	
   di   cult	
   for	
   a	
   crawler	
   to	
      nd	
   are	
   
collec.vely	
   referred	
   to	
   as	
   the	
   deep	
   (or	
   hidden)	
   
web	
   
       much	
   larger	
   than	
   conven.onal	
   web	
   

       three	
   broad	
   categories:	
   

       private	
   sites	
   
       form	
   results	
   

       scripted	
   pages	
   

       no	
   incoming	
   links,	
   or	
   may	
   require	
   log	
   in	
   with	
   a	
   valid	
   account	
   

       sites	
   that	
   can	
   be	
   reached	
   only	
   amer	
   entering	
   some	
   data	
   into	
   
a	
   form	
   

       pages	
   that	
   use	
   javascript,	
   flash,	
   or	
   another	
   client-     side	
   
language	
   to	
   generate	
   links	
   

sitemaps	
   

       sitemaps	
   contain	
   lists	
   of	
   urls	
   and	
   data	
   about	
   

those	
   urls,	
   such	
   as	
   modi   ca.on	
   .me	
   and	
   
modi   ca.on	
   frequency	
   

       generated	
   by	
   web	
   server	
   administrators	
   
       tells	
   crawler	
   about	
   pages	
   it	
   might	
   not	
   

       gives	
   crawler	
   a	
   hint	
   about	
   when	
   to	
   check	
   a	
   

otherwise	
      nd	
   

page	
   for	
   changes	
   

sitemap	
   example	
   

distributed	
   crawling	
   

       three	
   reasons	
   to	
   use	
   mul.ple	
   computers	
   for	
   

crawling	
   
       helps	
   to	
   put	
   the	
   crawler	
   closer	
   to	
   the	
   sites	
   it	
   crawls	
   
       reduces	
   the	
   number	
   of	
   sites	
   the	
   crawler	
   has	
   to	
   

remember	
   

       reduces	
   compu.ng	
   resources	
   required	
   

       distributed	
   crawler	
   uses	
   a	
   hash	
   func.on	
   to	
   assign	
   

urls	
   to	
   crawling	
   computers	
   
       hash	
   func.on	
   should	
   be	
   computed	
   on	
   the	
   host	
   part	
   of	
   

each	
   url	
   

desktop	
   crawls	
   

       used	
   for	
   desktop	
   search	
   and	
   enterprise	
   search	
   
       di   erences	
   to	
   web	
   crawling:	
   
      much	
   easier	
   to	
      nd	
   the	
   data	
   
      responding	
   quickly	
   to	
   updates	
   is	
   more	
   important	
   
      must	
   be	
   conserva.ve	
   in	
   terms	
   of	
   disk	
   and	
   cpu	
   
usage	
   
      many	
   di   erent	
   document	
   formats	
   
      data	
   privacy	
   very	
   important	
   

document	
   feeds	
   

       many	
   documents	
   are	
   published	
   

      created	
   at	
   a	
      xed	
   .me	
   and	
   rarely	
   updated	
   again	
   
      e.g.,	
   news	
   ar.cles,	
   blog	
   posts,	
   press	
   releases,	
   
email	
   

       published	
   documents	
   from	
   a	
   single	
   source	
   can	
   

be	
   ordered	
   in	
   a	
   sequence	
   called	
   a	
   document	
   
feed	
   
      new	
   documents	
   found	
   by	
   examining	
   the	
   end	
   of	
   
the	
   feed	
   

document	
   feeds	
   

       two	
   types:	
   

      a	
   push	
   feed	
   alerts	
   the	
   subscriber	
   to	
   new	
   
documents	
   
      a	
   pull	
   feed	
   requires	
   the	
   subscriber	
   to	
   check	
   
periodically	
   for	
   new	
   documents	
   

       most	
   common	
   format	
   for	
   pull	
   feeds	
   is	
   called	
   

rss	
   
      really	
   simple	
   syndica.on,	
   rdf	
   site	
   summary,	
   rich	
   
site	
   summary,	
   or	
   ...	
   

rss	
   example	
   

rss	
   example	
   

rss	
   

       ttl	
   tag	
   (.me	
   to	
   live)	
   

      amount	
   of	
   .me	
   (in	
   minutes)	
   contents	
   should	
   be	
   
cached	
   

       rss	
   feeds	
   are	
   accessed	
   like	
   web	
   pages	
   

      using	
   http	
   get	
   requests	
   to	
   web	
   servers	
   that	
   host	
   
them	
   

       easy	
   for	
   crawlers	
   to	
   parse	
   
       easy	
   to	
      nd	
   new	
   informa.on	
   

conversion	
   

       text	
   is	
   stored	
   in	
   hundreds	
   of	
   incompa.ble	
      le	
   

formats	
   
       e.g.,	
   raw	
   text,	
   rtf,	
   html,	
   xml,	
   microsom	
   word,	
   odf,	
   

pdf	
   

       other	
   types	
   of	
      les	
   also	
   important	
   

       e.g.,	
   powerpoint,	
   excel	
   

       typically	
   use	
   a	
   conversion	
   tool	
   

       converts	
   the	
   document	
   content	
   into	
   a	
   tagged	
   text	
   

format	
   such	
   as	
   html	
   or	
   xml	
   

       retains	
   some	
   of	
   the	
   important	
   formaeng	
   informa.on	
   

character	
   encoding	
   

       a	
   character	
   encoding	
   is	
   a	
   mapping	
   between	
   
bits	
   and	
   glyphs	
   
      i.e.,	
   geeng	
   from	
   bits	
   in	
   a	
      le	
   to	
   characters	
   on	
   a	
   
screen	
   
      can	
   be	
   a	
   major	
   source	
   of	
   incompa.bility	
   

       ascii	
   is	
   basic	
   character	
   encoding	
   scheme	
   for	
   
english	
   
      encodes	
   128	
   lepers,	
   numbers,	
   special	
   characters,	
   
and	
   control	
   characters	
   in	
   7	
   bits,	
   extended	
   with	
   an	
   
extra	
   bit	
   for	
   storage	
   in	
   bytes	
   

character	
   encoding	
   

       other	
   languages	
   can	
   have	
   many	
   more	
   glyphs	
   

       e.g.,	
   chinese	
   has	
   more	
   than	
   40,000	
   characters,	
   with	
   

over	
   3,000	
   in	
   common	
   use	
   

       many	
   languages	
   have	
   mul.ple	
   encoding	
   schemes	
   

       e.g.,	
   cjk	
   (chinese-     japanese-     korean)	
   family	
   of	
   east	
   

asian	
   languages,	
   hindi,	
   arabic	
   

       must	
   specify	
   encoding	
   
       can   t	
   have	
   mul.ple	
   languages	
   in	
   one	
      le	
   

       unicode	
   developed	
   to	
   address	
   encoding	
   

problems	
   

unicode	
   

       single	
   mapping	
   from	
   numbers	
   to	
   glyphs	
   that	
   
apempts	
   to	
   include	
   all	
   glyphs	
   in	
   common	
   use	
   
in	
   all	
   known	
   languages	
   

       unicode	
   is	
   a	
   mapping	
   between	
   numbers	
   and	
   

glyphs	
   
      does	
   not	
   uniquely	
   specify	
   bits	
   to	
   glyph	
   mapping!	
   
      e.g.,	
   utf-     8,	
   utf-     16,	
   utf-     32	
   

unicode	
   

       prolifera.on	
   of	
   encodings	
   comes	
   from	
   a	
   need	
   
for	
   compa.bility	
   and	
   to	
   save	
   space	
   
      utf-     8	
   uses	
   one	
   byte	
   for	
   english	
   (ascii),	
   as	
   many	
   
as	
   4	
   bytes	
   for	
   some	
   tradi.onal	
   chinese	
   characters	
   
      variable	
   length	
   encoding,	
   more	
   di   cult	
   to	
   do	
   
string	
   opera.ons	
   
      utf-     32	
   uses	
   4	
   bytes	
   for	
   every	
   character	
   

       many	
   applica.ons	
   use	
   utf-     32	
   for	
   internal	
   text	
   
encoding	
   (fast	
   random	
   lookup)	
   and	
   utf-     8	
   for	
   
disk	
   storage	
   (less	
   space)	
   

unicode	
   

      e.g.,	
   greek	
   leper	
   pi	
   (  )	
   is	
   unicode	
   symbol	
   number	
   
960	
   
      in	
   binary,	
   00000011	
   11000000	
   (3c0	
   in	
   
hexadecimal)	
   
      final	
   encoding	
   is	
   11001111	
   10000000	
   (cf80	
   in	
   
hexadecimal)	
   

storing	
   the	
   documents	
   

       many	
   reasons	
   to	
   store	
   converted	
   document	
   

text	
   
      saves	
   crawling	
   .me	
   when	
   page	
   is	
   not	
   updated	
   
      provides	
   e   cient	
   access	
   to	
   text	
   for	
   snippet	
   
genera.on,	
   informa.on	
   extrac.on,	
   etc.	
   

       database	
   systems	
   can	
   provide	
   document	
   

storage	
   for	
   some	
   applica.ons	
   
      web	
   search	
   engines	
   use	
   customized	
   document	
   
storage	
   systems	
   

storing	
   the	
   documents	
   

       requirements	
   for	
   document	
   storage	
   system:	
   

      random	
   access	
   

       request	
   the	
   content	
   of	
   a	
   document	
   based	
   on	
   its	
   url	
   
       hash	
   func.on	
   based	
   on	
   url	
   is	
   typical	
   

      compression	
   and	
   large	
      les	
   

       reducing	
   storage	
   requirements	
   and	
   e   cient	
   access	
   
       many	
   documents	
   per	
      le	
   

      update	
   

       handling	
   large	
   volumes	
   of	
   new	
   and	
   modi   ed	
   
documents	
   
       adding	
   new	
   anchor	
   text	
   

large	
   files	
   

       store	
   many	
   documents	
   in	
   large	
      les,	
   rather	
   

than	
   each	
   document	
   in	
   a	
      le	
   
      avoids	
   overhead	
   in	
   opening	
   and	
   closing	
      les	
   
      reduces	
   seek	
   .me	
   rela.ve	
   to	
   read	
   .me	
   

       compound	
   documents	
   formats	
   

      used	
   to	
   store	
   mul.ple	
   documents	
   in	
   a	
      le	
   
      e.g.,	
   trec	
   web	
   

trec	
   web	
   format	
   

compression	
   

       text	
   is	
   highly	
   redundant	
   (or	
   predictable)	
   
       compression	
   techniques	
   exploit	
   this	
   redundancy	
   

to	
   make	
      les	
   smaller	
   without	
   losing	
   any	
   of	
   the	
   
content	
   

       compression	
   of	
   indexes	
   covered	
   later	
   
       popular	
   algorithms	
   can	
   compress	
   html	
   and	
   xml	
   

text	
   by	
   80%	
   
       e.g.,	
   deflate	
   (zip,	
   gzip)	
   and	
   lzw	
   (unix	
   compress,	
   
       may	
   compress	
   large	
      les	
   in	
   blocks	
   to	
   make	
   access	
   

pdf)	
   

faster	
   

bigtable	
   

       google   s	
   document	
   storage	
   system	
   

      customized	
   for	
   storing,	
      nding,	
   and	
   upda.ng	
   web	
   
pages	
   
      handles	
   large	
   collec.on	
   sizes	
   using	
   inexpensive	
   
computers	
   

bigtable	
   

op.mize	
   

       no	
   query	
   language,	
   no	
   complex	
   queries	
   to	
   
       only	
   row-     level	
   transac.ons	
   
       tablets	
   are	
   stored	
   in	
   a	
   replicated	
      le	
   system	
   that	
   
       any	
   changes	
   to	
   a	
   bigtable	
   tablet	
   are	
   recorded	
   to	
   
a	
   transac.on	
   log,	
   which	
   is	
   also	
   stored	
   in	
   a	
   shared	
   
   le	
   system	
   

is	
   accessible	
   by	
   all	
   bigtable	
   servers	
   

       if	
   any	
   tablet	
   server	
   crashes,	
   another	
   server	
   can	
   

immediately	
   read	
   the	
   tablet	
   data	
   and	
   transac.on	
   
log	
   from	
   the	
      le	
   system	
   and	
   take	
   over	
   

bigtable	
   
       logically	
   organized	
   into	
   rows	
   
       a	
   row	
   stores	
   data	
   for	
   a	
   single	
   web	
   page	
   

       combina.on	
   of	
   a	
   row	
   key,	
   a	
   column	
   key,	
   and	
   
a	
   .mestamp	
   point	
   to	
   a	
   single	
   cell	
   in	
   the	
   row	
   

bigtable	
   

       bigtable	
   can	
   have	
   a	
   huge	
   number	
   of	
   columns	
   

per	
   row	
   
      all	
   rows	
   have	
   the	
   same	
   column	
   groups	
   
      not	
   all	
   rows	
   have	
   the	
   same	
   columns	
   
      important	
   for	
   reducing	
   disk	
   reads	
   to	
   access	
   
document	
   data	
   

       rows	
   are	
   par..oned	
   into	
   tablets	
   based	
   on	
   

their	
   row	
   keys	
   
      simpli   es	
   determining	
   which	
   server	
   is	
   appropriate	
   

detec.ng	
   duplicates	
   

       duplicate	
   and	
   near-     duplicate	
   documents	
   

occur	
   in	
   many	
   situa.ons	
   
      copies,	
   versions,	
   plagiarism,	
   spam,	
   mirror	
   sites	
   
      30%	
   of	
   the	
   web	
   pages	
   in	
   a	
   large	
   crawl	
   are	
   exact	
   or	
   
near	
   duplicates	
   of	
   pages	
   in	
   the	
   other	
   70%	
   

       duplicates	
   consume	
   signi   cant	
   resources	
   

during	
   crawling,	
   indexing,	
   and	
   search	
   
      liple	
   value	
   to	
   most	
   users	
   

duplicate	
   detec.on	
   

       exact	
   duplicate	
   detec.on	
   is	
   rela.vely	
   easy	
   
       checksum	
   techniques	
   

       a	
   checksum	
   is	
   a	
   value	
   that	
   is	
   computed	
   based	
   on	
   the	
   

content	
   of	
   the	
   document	
   
       e.g.,	
   sum	
   of	
   the	
   bytes	
   in	
   the	
   document	
      le	
   

       possible	
   for	
      les	
   with	
   di   erent	
   text	
   to	
   have	
   same	
   
       func.ons	
   such	
   as	
   a	
   cyclic	
   redundancy	
   check	
   

checksum	
   

(crc),	
   have	
   been	
   developed	
   that	
   consider	
   the	
   
posi.ons	
   of	
   the	
   bytes	
   

near-     duplicate	
   detec.on	
   

       more	
   challenging	
   task	
   

      are	
   web	
   pages	
   with	
   same	
   text	
   context	
   but	
   
di   erent	
   adver.sing	
   or	
   format	
   near-     duplicates?	
   
       a	
   near-     duplicate	
   document	
   is	
   de   ned	
   using	
   a	
   
threshold	
   value	
   for	
   some	
   similarity	
   measure	
   
between	
   pairs	
   of	
   documents	
   
      e.g.,	
   document	
   d1	
   is	
   a	
   near-     duplicate	
   of	
   
document	
   d2	
   if	
   more	
   than	
   90%	
   of	
   the	
   words	
   in	
   
the	
   documents	
   are	
   the	
   same	
   

near-     duplicate	
   detec.on	
   

       search:	
   	
   

       discovery:	
   	
   

         nd	
   near-     duplicates	
   of	
   a	
   document	
   d	
   
      o(n)	
   comparisons	
   required	
   

         nd	
   all	
   pairs	
   of	
   near-     duplicate	
   documents	
   in	
   the	
   
collec.on	
   
      o(n2)	
   comparisons	
   

       ir	
   techniques	
   are	
   e   ec.ve	
   for	
   search	
   scenario	
   
       for	
   discovery,	
   other	
   techniques	
   used	
   to	
   

generate	
   compact	
   representa.ons	
   

fingerprints	
   

fingerprint	
   example	
   

simhash	
   

       similarity	
   comparisons	
   using	
   word-     based	
   

representa.ons	
   more	
   e   ec.ve	
   at	
      nding	
   near-     
duplicates	
   
       problem	
   is	
   e   ciency	
   

       simhash	
   combines	
   the	
   advantages	
   of	
   the	
   word-     based	
   
similarity	
   measures	
   with	
   the	
   e   ciency	
   of	
      ngerprints	
   
based	
   on	
   hashing	
   

       similarity	
   of	
   two	
   pages	
   as	
   measured	
   by	
   the	
   cosine	
   

correla.on	
   measure	
   is	
   propor.onal	
   to	
   the	
   number	
   of	
   
bits	
   that	
   are	
   the	
   same	
   in	
   the	
   simhash	
      ngerprints	
   
       other	
   locality	
   sensi2ve	
   hashing	
   schemes	
   for	
   euclidean	
   
and	
   other	
   l-     norms,	
   hamming	
   distance,	
   kl	
   divergence	
   

simhash	
   

simhash	
   example	
   

removing	
   noise	
   

       many	
   web	
   pages	
   contain	
   text,	
   links,	
   and	
   

pictures	
   that	
   are	
   not	
   directly	
   related	
   to	
   the	
   
main	
   content	
   of	
   the	
   page	
   

       this	
   addi.onal	
   material	
   is	
   mostly	
   noise	
   that	
   

could	
   nega.vely	
   a   ect	
   the	
   ranking	
   of	
   the	
   page	
   

       techniques	
   have	
   been	
   developed	
   to	
   detect	
   

the	
   content	
   blocks	
   in	
   a	
   web	
   page	
   
      non-     content	
   material	
   is	
   either	
   ignored	
   or	
   reduced	
   
in	
   importance	
   in	
   the	
   indexing	
   process	
   

noise	
   example	
   

finding	
   content	
   blocks	
   

       cumula.ve	
   distribu.on	
   of	
   tags	
   in	
   the	
   example	
   

web	
   page	
   

       main	
   text	
   content	
   of	
   the	
   page	
   corresponds	
   to	
   the	
   

   plateau   	
   in	
   the	
   middle	
   of	
   the	
   distribu.on	
   

finding	
   content	
   blocks	
   

       represent	
   a	
   web	
   page	
   as	
   a	
   sequence	
   of	
   bits,	
   
where	
   bn	
   =	
   1	
   indicates	
   that	
   the	
   nth	
   token	
   is	
   a	
   
tag	
   

       op.miza.on	
   problem	
   where	
   we	
      nd	
   values	
   of	
   

i	
   and	
   j	
   to	
   maximize	
   both	
   the	
   number	
   of	
   tags	
   
below	
   i	
   and	
   above	
   j	
   and	
   the	
   number	
   of	
   non-     
tag	
   tokens	
   between	
   i	
   and	
   j	
   

       i.e.,	
   maximize	
   

finding	
   content	
   blocks	
   

       other	
   approaches	
   
use	
   dom	
   structure	
   
and	
   visual	
   (layout)	
   
features	
   

