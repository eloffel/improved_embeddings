introduction to information retrieval

http://informationretrieval.org

iir 16: flat id91

hinrich sch  utze

center for information and language processing, university of munich

2014-06-11

1 / 86

overview

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

2 / 86

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

3 / 86

learning to rank for zone scoring

given query q and document d, weighted zone scoring assigns to
the pair (q, d) a score in the interval [0,1] by computing a linear
combination of document zone scores, where each zone contributes
a value.

consider a set of documents, which have l zones

let g1, ..., gl     [0, 1], such that pl

for 1     i     l , let si be the boolean score denoting a match
(or non-match) between q and the i th zone

i =1 gi = 1

si = 1 if a query term occurs in zone i, 0 otherwise

weighted zone scoring aka ranked boolean retrieval

rank documents according to pl

i =1 gi si

learning to rank approach: learn the weights gi from training data

4 / 86

training set for learning to rank

  j
  1
  2
  3
  4
  5
  6
  7

dj
37
37
238
238
1741
2094
3194

qj
linux
penguin
system
penguin
kernel
driver
driver

st
1
0
0
0
1
0
1

r (dj , qj )
sb
1 relevant
1 nonrelevant
1 relevant
0 nonrelevant
1 relevant
1 relevant
0 nonrelevant

5 / 86

summary of learning to rank approach

the problem of making a binary relevant/nonrelevant
judgment is cast as a classi   cation or regression problem,
based on a training set of query-document pairs and
associated relevance judgments.

in principle, any method learning a classi   er (including least
squares regression) can be used to    nd this line.

big advantage of learning to rank: we can avoid hand-tuning
scoring functions and simply learn them from training data.

bottleneck of learning to rank: the cost of maintaining a
representative set of training examples whose relevance
assessments must be made by humans.

6 / 86

ltr features used by microsoft research (1)

zones: body, anchor, title, url, whole document

features derived from standard ir models: query term
number, query term ratio, length, idf, sum of term frequency,
min of term frequency, max of term frequency, mean of term
frequency, variance of term frequency, sum of length
normalized term frequency, min of length normalized term
frequency, max of length normalized term frequency, mean of
length normalized term frequency, variance of length
normalized term frequency, sum of tf-idf, min of tf-idf, max of
tf-idf, mean of tf-idf, variance of tf-idf, boolean model, bm25

7 / 86

ltr features used by microsoft research (2)

language model features: lmir.abs, lmir.dir, lmir.jm

web-speci   c features: number of slashes in url, length of url,
inlink number, outlink number, id95, siterank

spam features: qualityscore

usage-based features: query-url click count, url click count,
url dwell time

8 / 86

ranking id166s

vector of feature di   erences:   (di , dj , q) =   (di , q)       (dj , q)
by hypothesis, one of di and dj has been judged more
relevant.
notation: we write di     dj for    di precedes dj in the results
ordering   .
if di is judged more relevant than dj , then we will assign the
vector   (di , dj , q) the class yijq = +1; otherwise    1.
this gives us a training set of pairs of vectors and
   precedence indicators   . each of the vectors is computed as
the di   erence of two document-query vectors.

we can then train an id166 on this training set with the goal
of obtaining a classi   er that returns

~w t  (di , dj , q) > 0 i    di     dj

9 / 86

take-away today

what is id91?

applications of id91 in information retrieval

k -means algorithm

evaluation of id91

how many clusters?

10 / 86

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

11 / 86

id91: de   nition

(document) id91 is the process of grouping a set of
documents into clusters of similar documents.

documents within a cluster should be similar.

documents from di   erent clusters should be dissimilar.

id91 is the most common form of unsupervised learning.

unsupervised = there are no labeled or annotated data.

12 / 86

data set with clear cluster structure

5

.

2

0

.

2

5

.

1

0
1

.

5
.
0

0

.

0

propose
algorithm
for    nding
the cluster
structure in
this
example

0.0

0.5

1.0

1.5

2.0

13 / 86

classi   cation vs. id91

classi   cation: supervised learning

id91: unsupervised learning

classi   cation: classes are human-de   ned and part of the
input to the learning algorithm.
id91: clusters are inferred from the data without human
input.

however, there are many ways of in   uencing the outcome of
id91: number of clusters, similarity measure,
representation of documents, . . .

14 / 86

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

15 / 86

the cluster hypothesis

cluster hypothesis. documents in the same cluster behave
similarly with respect to relevance to information needs. all

applications of id91 in ir are based (directly or indirectly) on
the cluster hypothesis. van rijsbergen   s original wording (1979):

   closely associated documents tend to be relevant to the same
requests   .

16 / 86

applications of id91 in ir

application

search result id91

scatter-gather

what is
clustered?
search
results

(subsets of)
collection

collection id91

collection

cluster-based retrieval

collection

bene   t

more e   ective infor-
mation
presentation
to user
alternative user inter-
face:    search without
typing   
e   ective
information
presentation for ex-
ploratory browsing
higher
faster search

e   ciency:

17 / 86

search result id91 for better navigation

18 / 86

scatter-gather

19 / 86

global navigation: yahoo

20 / 86

global navigation: mesh (upper level)

21 / 86

global navigation: mesh (lower level)

22 / 86

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

but they are well known examples for using a global hierarchy
for navigation.
some examples for global navigation/exploration based on
id91:
cartia
themescapes
google news

23 / 86

global navigation combined with visualization (1)

24 / 86

global navigation combined with visualization (2)

25 / 86

global id91 for navigation: google news

http://news.google.com

26 / 86

id91 for improving recall

to improve search recall:

cluster docs in collection a priori
when a query matches a doc d, also return other docs in the
cluster containing d

hope: if we do this: the query    car    will also return docs
containing    automobile   

because the id91 algorithm groups together docs
containing    car    with those containing    automobile   .
both types of documents contain words like    parts   ,    dealer   ,
   mercedes   ,    road trip   .

27 / 86

data set with clear cluster structure

5

.

2

0

.

2

5

.

1

0
1

.

5
.
0

0

.

0

propose
algorithm
for    nding
the cluster
structure in
this
example

0.0

0.5

1.0

1.5

2.0

28 / 86

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.
later: semiautomatic methods for determining k

secondary goals in id91

avoid very small and very large clusters
de   ne clusters that are easy to explain to the user
many others . . .

29 / 86

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively
main algorithm: k -means

hierarchical algorithms

create a hierarchy
bottom-up, agglomerative
top-down, divisive

30 / 86

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel
shoes

you can only do that with a soft id91 approach.

this class:    at, hard id91

next time: hierarchical, hard id91

next week: id45, a form of soft
id91

31 / 86

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

given: a set of documents and the number k

find: a partition into k clusters that optimizes the chosen
partitioning criterion
global optimization: exhaustively enumerate partitions, pick
optimal one

not tractable

e   ective heuristic method: k -means algorithm

32 / 86

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

33 / 86

k -means

perhaps the best known id91 algorithm

simple, works well in many cases

use as default / baseline for id91 documents

34 / 86

id194s in id91

vector space model

as in vector space classi   cation, we measure relatedness
between vectors by euclidean distance . . .

. . . which is almost equivalent to cosine similarity.

almost: centroids are not length-normalized.

35 / 86

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

objective/partitioning criterion: minimize the average squared
di   erence from the centroid

recall de   nition of centroid:

~  (  ) =

1

|  | x~x     

~x

where we use    to denote a cluster.
we try to    nd the minimum average squared di   erence by
iterating two steps:

reassignment: assign each vector to its closest centroid
recomputation: recompute each centroid as the average of the
vectors that were assigned to it in reassignment

36 / 86

k -means pseudocode (  k is centroid of   k)

k -means({~x1, . . . , ~xn }, k )

1 (~s1,~s2, . . . ,~sk )     selectrandomseeds({~x1, . . . , ~xn }, k )
2 for k     1 to k
3 do ~  k     ~sk
4 while stopping criterion has not been met
5 do for k     1 to k
6
7
8
9
10
11
12 return {~  1, . . . , ~  k }

do   k     {}
for n     1 to n
do j     arg minj     |~  j         ~xn|

  j       j     {~xn} (reassignment of vectors)

for k     1 to k
do ~  k     1

~x (recomputation of centroids)

|  k | p~x     k

37 / 86

worked example : set of points to be clustered

exercise: (i) guess what the

optimal id91 into two clusters is in this case; (ii) compute the
centroids of the clusters

38 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
worked example: random selection of initial centroids

39 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assign points to closest center

40 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

1

1

2 2
2
1
1

1
1
1

1

1

2

1

1

1

1

1

1

41 / 86

  
  
worked example: recompute cluster centroids

2

1

1

2 2
2
1
1

1
1
1

1

1

2

1

1

1

1

1

1

42 / 86

  
  
  
  
worked example: assign points to closest centroid

43 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

1

1

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

44 / 86

  
  
worked example: recompute cluster centroids

2

1

1

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

45 / 86

  
  
  
  
worked example: assign points to closest centroid

46 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

1

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

47 / 86

  
  
worked example: recompute cluster centroids

2

1

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

48 / 86

  
  
  
  
worked example: assign points to closest centroid

49 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

2

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

50 / 86

  
  
worked example: recompute cluster centroids

2

2

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

51 / 86

  
  
  
  
worked example: assign points to closest centroid

52 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

2

2

2 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

53 / 86

  
  
worked example: recompute cluster centroids

2

2

2

2 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

54 / 86

  
  
  
  
worked example: assign points to closest centroid

55 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

56 / 86

  
  
worked example: recompute cluster centroids

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

57 / 86

  
  
  
  
worked example: assign points to closest centroid

58 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
worked example: assignment

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

1

2

1

1

1

59 / 86

  
  
worked example: recompute cluster centroids

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

1

2

1

1

1

60 / 86

  
  
  
  
worked ex.: centroids and assignments after convergence

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

1

2

1

1

1

61 / 86

  
  
k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

see next slide

there is only a    nite number of id91s.

thus: we must reach a    xed point.

assumption: ties are broken consistently.

finite set & monotonically decreasing     convergence

62 / 86

recomputation decreases average distance

rss = pk

measure)

k=1 rssk     the residual sum of squares (the    goodness   

rssk (~v ) = x~x     k
= x~x     k

   vm

   rssk (~v )

m

k~v     ~xk2 = x~x     k

(vm     xm)2

xm=1

2(vm     xm) = 0

vm =

1

|  k | x~x     k

xm

the last line is the componentwise de   nition of the centroid! we
minimize rssk when the old centroid is replaced with the new
centroid. rss, the sum of the rssk , must then also decrease
during recomputation.

63 / 86

k -means is guaranteed to converge

but we don   t know how long convergence will take!

if we don   t care about a few docs switching back and forth,
then convergence is usually fast (< 10-20 iterations).

however, complete convergence can take many more
iterations.

64 / 86

optimality of k -means

convergence 6= optimality

convergence does not mean that we converge to the optimal
id91!

this is the great weakness of k -means.

if we start with a bad set of seeds, the resulting id91 can
be horrible.

65 / 86

exercise: suboptimal id91

3

2

1

0

d1

d2

d4

d5

0

1

2

3

d3

d6

4

what is the optimal id91 for k = 2?

do we converge on this id91 for arbitrary seeds
di , dj ?

66 / 86

  
  
  
  
  
  
initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.
better ways of computing initial centroids:

select seeds not randomly, but using some heuristic (e.g.,    lter
out outliers or    nd a set of seeds that has    good coverage    of
the document space)
use hierarchical id91 to    nd good seeds
select i (e.g., i = 10) di   erent random sets of seeds, do a
k -means id91 for each, select the id91 with lowest
rss

67 / 86

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

recomputation step: o(nm) (we need to add each of the
document   s < m values to one of the centroids)

assume number of iterations bounded by i

overall complexity: o(iknm)     linear in all important
dimensions

however: this is not a real worst-case analysis.

in pathological cases, complexity can be worse than linear.

68 / 86

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

69 / 86

what is a good id91?

internal criteria

example of an internal criterion: rss in k -means

but an internal criterion often does not evaluate the actual
utility of a id91 in the application.
alternative: external criteria

evaluate with respect to a human-de   ned classi   cation

70 / 86

external criteria for id91 quality

based on a gold standard data set, e.g., the reuters collection
we also used for the evaluation of classi   cation

goal: id91 should reproduce the classes in the gold
standard

(but we only want to reproduce how documents are divided
into groups, not the class labels.)

first measure for how well we were able to reproduce the
classes: purity

71 / 86

external criterion: purity

purity(   , c ) =

1

n xk

max

j

|  k     cj |

    = {  1,   2, . . . ,   k } is the set of clusters and
c = {c1, c2, . . . , cj } is the set of classes.
for each cluster   k :    nd class cj with most members nkj in   k
sum all nkj and divide by total number of points

72 / 86

example for computing purity

cluster 1

cluster 2

cluster 3

x

x
x

o

x x

x

o

o
o    

o

x

   
       

x

to compute

purity: 5 = maxj |  1     cj | (class x, cluster 1); 4 = maxj |  2     cj |
(class o, cluster 2); and 3 = maxj |  3     cj | (class    , cluster 3).
purity is (1/17)    (5 + 4 + 3)     0.71.

73 / 86

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

tp+fn+fp+tn is the total number of pairs.

tp+fn+fp+tn = (cid:0)n
example: (cid:0)17

2(cid:1) = 136 in o/   /x example

2(cid:1) for n documents.

each pair is either positive or negative (the id91 puts the
two documents in the same or in di   erent clusters) . . .

. . . and either    true    (correct) or    false    (incorrect): the
id91 decision is correct or incorrect.

74 / 86

rand index: example

as an example, we compute ri for the o/   /x example. we    rst
compute tp + fp. the three clusters contain 6, 6, and 5 points,
respectively, so the total number of    positives    or pairs of
documents that are in the same cluster is:

tp + fp = (cid:18) 6

2 (cid:19) +(cid:18) 6

2 (cid:19) +(cid:18) 5

2 (cid:19) = 40

of these, the x pairs in cluster 1, the o pairs in cluster 2, the    
pairs in cluster 3, and the x pair in cluster 3 are true positives:

tp = (cid:18) 5

2 (cid:19) +(cid:18) 4

2 (cid:19) +(cid:18) 3

2 (cid:19) +(cid:18) 2

2 (cid:19) = 20

thus, fp = 40     20 = 20. fn and tn are computed similarly.

75 / 86

rand measure for the o/   /x example

same cluster

di   erent clusters

same class
di   erent classes

tp = 20
fp = 20

fn = 24
tn = 72

ri is then

(20 + 72)/(20 + 20 + 24 + 72)     0.68.

76 / 86

two other external evaluation measures

two other measures
normalized mutual information (nmi)

how much information does the id91 contain about the
classi   cation?
singleton clusters (number of clusters = number of docs) have
maximum mi
therefore: normalize by id178 of clusters and classes

f measure

like rand, but    precision    and    recall    can be weighted

77 / 86

evaluation results for the o/   /x example

lower bound
maximum
value for example

purity nmi ri
0.0
0.0
1.0
1.0
0.71
0.68

0.0
1.0
0.36

f5
0.0
1.0
0.46

all four

measures range from 0 (really bad id91) to 1 (perfect
id91).

78 / 86

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

79 / 86

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

what if there is no external constraint? is there a    right   
number of clusters?
one way to go: de   ne an optimization criterion

given docs,    nd k for which the optimum is reached.
what optimization criterion can we use?
we can   t use rss or average squared distance from centroid
as criterion: always chooses k = n clusters.

80 / 86

exercise

your job is to develop the id91 algorithms for a
competitor to news.google.com

you want to use k -means id91.

how would you determine k ?

81 / 86

simple objective function for k : basic idea

start with 1 cluster (k = 1)

keep adding clusters (= keep increasing k )

add a penalty for each new cluster

then trade o    cluster penalties against average squared
distance from centroid

choose the value of k with the best tradeo   

82 / 86

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

then: penalize each cluster with a cost   

thus for a id91 with k clusters, total cluster penalty is
k   

de   ne the total cost of a id91 as distortion plus total
cluster penalty: rss(k) + k   

select k that minimizes (rss(k) + k   )

still need to determine good value for    . . .

83 / 86

finding the    knee    in the curve

s
e
r
a
u
q
s
 
f

 

o
m
u
s
 
l

a
u
d
s
e
r

i

0
5
9
1

0
0
9
1

0
5
8
1

0
0
8
1

0
5
7
1

2

4

6

8

10

number of clusters

pick the number of clusters where

curve       attens   . here: 4 or 9.

84 / 86

take-away today

what is id91?

applications of id91 in information retrieval

k -means algorithm

evaluation of id91

how many clusters?

85 / 86

resources

chapter 16 of iir
resources at http://cislmu.org

keith van rijsbergen on the cluster hypothesis (he was one of
the originators)
bing/carrot2/clusty: search result id91 systems
stirling number: the number of distinct k-id91s of n
items

86 / 86

