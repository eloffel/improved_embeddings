introduction to information retrieval

http://informationretrieval.org

iir 14: vector space classi   cation

hinrich sch  utze

center for information and language processing, university of munich

2013-05-28

1 / 68

overview

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

2 / 68

outline

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

3 / 68

feature selection: mi for poultry/export

goal of feature selection: eleminate noise and useless features for
better e   ectiveness and e   ciency

et = eexport = 1
et = eexport = 0

ec = epoultry = 1
n11 = 49
n01 = 141

ec = epoultry = 0
n10 = 27,652
n00 = 774,106

plug

these values into formula:

i (u; c ) =

log2

801,948    49

(49+27,652)(49+141)

49

801,948
141

+

+

801,948
27,652
801,948
774,106
801,948
    0.000105

+

log2

log2

log2

801,948    141

(141+774,106)(49+141)

801,948    27,652

(49+27,652)(27,652+774,106)

801,948    774,106

(141+774,106)(27,652+774,106)

4 / 68

feature selection for reuters classes co   ee and sports

class: co   ee
mi
0.0111
0.0042
0.0025
0.0019
0.0018
0.0016
0.0014
0.0013
0.0013
0.0012

term
coffee
bags
growers
kg
colombia
brazil
export
exporters
exports
crop

class: sports
mi
0.0681
0.0515
0.0441
0.0408
0.0388
0.0386
0.0301
0.0299
0.0284
0.0264

term
soccer
cup
match
matches
played
league
beat
game
games
team

5 / 68

using language models (lms) for ir

lm = language model

we view the document as a generative model that generates
the query.

what we need to do:

de   ne the precise generative model we want to use

estimate parameters (di   erent parameters for each
document   s model)

smooth to avoid zeros

apply to query and    nd document most likely to have
generated the query

present most likely document(s) to user

6 / 68

jelinek-mercer smoothing

p(t|d) =   p(t|md ) + (1       )p(t|mc )
mixes the id203 from the document with the general
collection frequency of the word.

high value of   :    conjunctive-like    search     tends to retrieve
documents containing all query words.

low value of   : more disjunctive, suitable for long queries

correctly setting    is very important for good performance.

7 / 68

take-away today

vector space classi   cation: basic idea of doing text
classi   cation for documents that are represented as vectors

rocchio classi   er: rocchio relevance feedback idea applied to
text classi   cation

k nearest neighbor classi   cation

linear classi   ers

more than two classes

8 / 68

outline

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

9 / 68

recall vector space representation

each document is a vector, one component for each term.

terms are axes.

high dimensionality: 100,000s of dimensions

normalize vectors (documents) to unit length

how can we do classi   cation in this space?

10 / 68

basic text classi   cation setup

regions

industries

subject areas

classes:

training
set:

uk

china

poultry

co   ee

elections

sports

congestion

olympics

feed

roasting

recount

diamond

london

beijing

chicken

beans

votes

baseball

  (d    ) =china

d    

first

private

chinese
airline

test
set:

parliament

tourism

big ben

great wall

pate

ducks

arabica

robusta

seat

forward

run-off

soccer

windsor

mao

bird flu

kenya

tv ads

team

the queen

communist

turkey

harvest

campaign

captain

11 / 68

vector space classi   cation

as before, the training set is a set of documents, each labeled
with its class.

in vector space classi   cation, this set corresponds to a labeled
set of points or vectors in the vector space.

premise 1: documents in the same class form a contiguous
region.

premise 2: documents from di   erent classes don   t overlap.

we de   ne lines, surfaces, hypersurfaces to divide regions.

12 / 68

classes in the vector space

   

   

   

   

   

   
uk

   

china

x

x

x

x

kenya

should the document     be assigned to china, uk or kenya? find
separators between the classes based on these separators:     should
be assigned to china how do we    nd separators that do a good
job at classifying new documents like    ?     main topic of today

13 / 68

aside: 2d/3d graphs can be misleading

x2

x3

x4

e

tr u

d

x    
2

x    
3

x    
4

d projected

x1

x    
1

x5

x    
5

x    
1

x    
2

x    
3

x    
4

x    
5

left: a projection of the 2d semicircle to 1d. for the points
x1, x2, x3, x4, x5 at x coordinates    0.9,    0.2, 0, 0.2, 0.9 the distance
|x2x3|     0.201 only di   ers by 0.5% from |x    
|x1x3|/|x    
3| = d true/d projected     1.06/0.9     1.18 is an example of
a large distortion (18%) when projecting a large area. right: the
corresponding projection of the 3d hemisphere to 2d.

3| = 0.2; but

2x    

1x    

14 / 68

outline

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

15 / 68

relevance feedback

in relevance feedback, the user marks documents as
relevant/nonrelevant.

relevant/nonrelevant can be viewed as classes or categories.

for each document, the user decides which of these two
classes is correct.

the ir system then uses these class assignments to build a
better query (   model   ) of the information need . . .

. . . and returns better documents.

relevance feedback is a form of text classi   cation.

16 / 68

using rocchio for vector space classi   cation

the principal di   erence between relevance feedback and text
classi   cation:

the training set is given as part of the input in text
classi   cation.
it is interactively created in relevance feedback.

17 / 68

rocchio classi   cation: basic idea

compute a centroid for each class

the centroid is the average of all documents in the class.

assign each test document to the class of its closest centroid.

18 / 68

recall de   nition of centroid

~  (c) =

1
|dc | x

d   dc

~v (d)

where dc is the set of all documents that belong to class c and

~v (d) is the vector space representation of d.

19 / 68

rocchio illustrated : a1 = a2, b1 = b2, c1 = c2

   

   

   

a1

a2

   

   

uk

c1

c2

   

b1

b2

   

x

x

x

x

kenya

china

20 / 68

rocchio algorithm

trainrocchio(c, d)
1 for each cj     c
2 do dj     {d : hd, cj i     d}
~v (d)
3
|dj | pd   dj
4 return {~  1, . . . , ~  j }

~  j     1

applyrocchio({~  1, . . . , ~  j }, d)
1 return arg minj |~  j     ~v(d)|

21 / 68

rocchio properties

rocchio forms a simple representation for each class: the
centroid

we can interpret the centroid as the prototype of the class.

classi   cation is based on similarity to / distance from
centroid/prototype.

does not guarantee that classi   cations are consistent with the
training data!

22 / 68

time complexity of rocchio

time complexity

mode
training   (|d|lave + |c||v |)       (|d|lave)
testing   (la + |c|m a)       (|c|m a)

23 / 68

rocchio vs. naive bayes

in many cases, rocchio performs worse than naive bayes.

one reason: rocchio does not handle nonconvex, multimodal
classes correctly.

24 / 68

rocchio cannot handle nonconvex, multimodal classes

a

a
a

aa
aa
a
a
a
x
aa
a
a
a
a
a
a
a

a

a
a
a

a
a

a
a

a
a
a
a a
a
x
a
a
a
a
a

a

a

o
b

b

b
b
b
b
b
b bb
b
b

b

b

b
bb
b

b

b

a

exercise: why is rocchio
not expected to do well for
the classi   cation task a vs.
b here?

a is centroid of the
a   s, b is centroid of
the b   s.

the point o is closer
to a than to b.

but o is a better    t for
the b class.

a is a multimodal class
with two prototypes.

but in rocchio we only
have one prototype.

25 / 68

outline

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

26 / 68

knn classi   cation

knn classi   cation is another vector space classi   cation
method.

it also is very simple and easy to implement.

knn is more accurate (in most cases) than naive bayes and
rocchio.

if you need to get a pretty accurate classi   er up and running
in a short time . . .

. . . and you don   t care about e   ciency that much . . .

. . . use knn.

27 / 68

knn classi   cation

knn = k nearest neighbors

knn classi   cation rule for k = 1 (1nn): assign each test
document to the class of its nearest neighbor in the training
set.

1nn is not very robust     one document can be mislabeled or
atypical.

knn classi   cation rule for k > 1 (knn): assign each test
document to the majority class of its k nearest neighbors in
the training set.
rationale of knn: contiguity hypothesis

we expect a test document d to have the same label as the
training documents located in the local region surrounding d.

28 / 68

probabilistic knn

probabilistic version of knn: p(c|d) = fraction of k neighbors
of d that are in c

knn classi   cation rule for probabilistic knn: assign d to class
c with highest p(c|d)

29 / 68

knn is based on voronoi tessellation

   

   

   

x

x

x

x x

x

x

x

x

   

x

x

   

   

   
   

   
   

   

   

30 / 68

knn algorithm

train-knn(c, d)
1 d        preprocess(d)
2 k     select-k(c, d   )
3 return d   , k

apply-knn(d   , k, d)
1 sk     computenearestneighbors(d   , k, d)
2 for each cj     c(d   )
3 do pj     |sk     cj |/k
4 return arg maxj pj

31 / 68

exercise

o
o
o
o

o

   

x

x

x

x

x

x

x

x

how is star classi   ed by:

(i) 1-nn (ii) 3-nn (iii) 9-nn (iv) 15-nn (v) rocchio?

x

x

32 / 68

time complexity of knn

knn with preprocessing of training set

training   (|d|lave)
testing   (la + |d|m avem a) =   (|d|m avem a)

knn test time proportional to the size of the training set!

the larger the training set, the longer it takes to classify a
test document.

knn is ine   cient for very large training sets.

question: can we divide up the training set into regions, so
that we only have to search in one region to do knn
classi   cation for a given test document? (which perhaps
would give us better than linear time complexity)

33 / 68

curse of dimensionality

our intuitions about space are based on the 3d world we live
in.

intuition 1: some things are close by, some things are distant.

intuition 2: we can carve up space into areas such that: within
an area things are close, distances between areas are large.

these two intuitions don   t necessarily hold for high
dimensions.

in particular: for a set of k uniformly distributed points, let
dmin be the smallest distance between any two points and
dmax be the largest distance between any two points.

then

lim
d      

dmax     dmin

dmin

= 0

34 / 68

curse of dimensionality: simulation

simulate

lim
d      

dmax     dmin

dmin

= 0

pick a dimensionality d

generate 10 random points in the d-dimensional hypercube
(uniform distribution)

compute all 45 distances
compute dmax   dmin
we see that intuition 1 (some things are close, others are
distant) is not true for high dimensions.

dmin

35 / 68

intuition 2: space can be carved up

intuition 2: we can carve up space into areas such that: within
an area things are close, distances between areas are large.

if this is true, then we have a simple and e   cient algorithm
for knn.

to    nd the k closest neighbors of data point
< x1, x2, . . . , xd > do the following.
using binary search    nd all data points whose    rst dimension
is in [x1       , x1 +   ]. this is o(log n) where n is the number of
data points.

do this for each dimension, then intersect the d subsets.

36 / 68

intuition 2: space can be carved up

size of data set n = 100
again, assume uniform distribution in hypercube
set    = 0.05: we will look in an interval of length 0.1 for
neighbors on each dimension.
what is the id203 that the nearest neighbor of a new
data point ~x is in this neighborhood in d = 1 dimension?
for d = 1: 1     (1     0.1)100     0.99997
in d = 2 dimensions?
for d = 2: 1     (1     0.12)100     0.63
in d = 3 dimensions?
for d = 3: 1     (1     0.13)100     0.095
in d = 4 dimensions?
for d = 4: 1     (1     0.14)100     0.0095
in d = 5 dimensions?
for d = 5: 1     (1     0.15)100     0.0009995

37 / 68

intuition 2: space can be carved up

in d = 5 dimensions?
for d = 5: 1     (1     0.15)100     0.0009995
in other words: with enough dimensions, there is only one
   local    region that will contain the nearest neighbor with high
certainty: the entire search space.

we cannot carve up high-dimensional space into neat
neighborhoods . . .

. . . unless the    true    dimensionality is much lower than d.

38 / 68

knn: discussion

no training necessary

but linear preprocessing of documents is as expensive as
training naive bayes.
we always preprocess the training set, so in reality training
time of knn is linear.

knn is very accurate if training set is large.

optimality result: asymptotically zero error if bayes rate is
zero.

but knn can be very inaccurate if training set is small.

39 / 68

outline

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

40 / 68

linear classi   ers

de   nition:

a linear classi   er computes a linear combination or weighted
sum pi wi xi of the feature values.
classi   cation decision: pi wi xi >   ?
. . . where    (the threshold) is a parameter.

(first, we only consider binary classi   ers.)

geometrically, this corresponds to a line (2d), a plane (3d) or
a hyperplane (higher dimensionalities), the separator.

we    nd this separator based on training set.

methods for    nding separator: id88, rocchio, naive
bayes     as we will explain on the next slides

assumption: the classes are linearly separable.

41 / 68

a linear classi   er in 1d

a linear classi   er in 1d is
a point described by the
equation w1d1 =   
the point at   /w1
points (d1) with w1d1       
are in the class c.
points (d1) with w1d1 <   
are in the complement
class c.

42 / 68

a linear classi   er in 2d

a linear classi   er in 2d is
a line described by the
equation w1d1 + w2d2 =   
example for a 2d linear
classi   er
points (d1 d2) with
w1d1 + w2d2        are in
the class c.
points (d1 d2) with
w1d1 + w2d2 <    are in
the complement class c.

43 / 68

a linear classi   er in 3d

a linear classi   er in 3d is
a plane described by the
equation
w1d1 + w2d2 + w3d3 =   
example for a 3d linear
classi   er
points (d1 d2 d3) with
w1d1 + w2d2 + w3d3       
are in the class c.
points (d1 d2 d3) with
w1d1 + w2d2 + w3d3 <   
are in the complement
class c.

44 / 68

rocchio as a linear classi   er

rocchio is a linear classi   er de   ned by:

m

x
i =1

wi di = ~w~d =   

where ~w is the normal vector ~  (c1)     ~  (c2) and
   = 0.5     (|~  (c1)|2     |~  (c2)|2).

45 / 68

naive bayes as a linear classi   er

multinomial naive bayes is a linear classi   er (in log space) de   ned
by:

m

x
i =1

wi di =   

where wi = log[   p(ti |c)/   p(ti |  c)], di = number of occurrences of ti
in d, and    =     log[   p(c)/   p(  c)]. here, the index i , 1     i     m,
refers to terms of the vocabulary (not to positions in d as k did in
our original de   nition of naive bayes)

46 / 68

knn is not a linear classi   er

   

   

   

x

x
x

x x

x

x

x
x

   

x

x

   

   

   
   

   
   

   

   

classi   cation decision
based on majority of
k nearest neighbors.

the decision
boundaries between
classes are piecewise
linear . . .

. . . but they are in
general not linear
classi   ers that can be
described as
pm

i =1 wi di =   .

47 / 68

example of a linear two-class classi   er

d2i
1
0
0
0
0
0

d1i
0
1
0
0
1
0

d2i
1
0
0
0
0
0

ti
prime
rate
interest
rates
discount
bundesbank

wi
0.70
0.67
0.63
0.60
0.46
0.43

wi
-0.71
-0.35
-0.33
-0.25
-0.24
-0.24

ti
dlrs
world
sees
year
group
dlr

d1i
1
1
0
0
0
0
this is for the class interest in reuters-21578.
for simplicity: assume a simple 0/1 vector representation
d1:    rate discount dlrs world   
d2:    prime dlrs   
   = 0
exercise: which class is d1 assigned to? which class is d2 assigned to?
we assign document ~d1    rate discount dlrs world    to interest since
~w t ~d1 = 0.67    1 + 0.46    1 + (   0.71)    1 + (   0.35)    1 = 0.07 > 0 =   .
we assign ~d2    prime dlrs    to the complement class (not in interest) since
~w t ~d2 =    0.01       .

48 / 68

which hyperplane?

49 / 68

learning algorithms for vector space classi   cation

in terms of actual computation, there are two types of
learning algorithms.
(i) simple learning algorithms that estimate the parameters of
the classi   er directly from the training data, often in one
linear pass.

naive bayes, rocchio, knn are all examples of this.

(ii) iterative algorithms

support vector machines
id88 (example available as pdf on website:
http://cislmu.org)

the best performing learning algorithms usually require
iterative learning.

50 / 68

id88 update rule

randomly initialize linear separator ~w
do until convergence:
pick data point ~x
if sign(~w t~x) is correct class (1 or -1): do nothing
otherwise: ~w = ~w     sign(~w t~x)~x

51 / 68

id88 (class of ~x is yes)

no

yes

~x

~w

s

52 / 68

id88 (class of ~x is yes)

no

yes

~x

~x

~w

s

53 / 68

id88 (class of ~x is yes)

noyes

no

yes

~x

~w + ~x

~x

~w

s    

s

54 / 68

id88 (class of ~x is yes)

noyes

~x

~w + ~x

s    

55 / 68

which hyperplane?

56 / 68

which hyperplane?

for linearly separable training sets: there are in   nitely many
separating hyperplanes.

they all separate the training set perfectly . . .

. . . but they behave di   erently on test data.

error rates on new data are low for some, high for others.

how do we    nd a low-error separator?

id88: generally bad; naive bayes, rocchio: ok; linear
id166: good

57 / 68

linear classi   ers: discussion

many common text classi   ers are linear classi   ers: naive
bayes, rocchio, id28, linear support vector
machines etc.
each method has a di   erent way of selecting the separating
hyperplane

huge di   erences in performance on test documents

can we get better performance with more powerful nonlinear
classi   ers?

not in general: a given amount of training data may su   ce
for estimating a linear boundary, but not for estimating a
more complex nonlinear boundary.

58 / 68

a nonlinear problem

0

.

1

8

.

0

6

.

0

4

.

0

2
0

.

0

.

0

0.0

0.2

0.4

0.6

0.8

1.0

linear classi   er like rocchio does badly on this task.

knn will do well (assuming enough training data)

59 / 68

which classi   er do i use for a given tc problem?

is there a learning method that is optimal for all text
classi   cation problems?

no, because there is a tradeo    between bias and variance.
factors to take into account:

how much training data is available?
how simple/complex is the problem? (linear vs. nonlinear
decision boundary)
how noisy is the problem?
how stable is the problem over time?

for an unstable problem, it   s better to use a simple and robust
classi   er.

60 / 68

outline

1 recap

2

intro vector space classi   cation

3 rocchio

4 knn

5 linear classi   ers

6 > two classes

61 / 68

how to combine hyperplanes for > 2 classes?

?

62 / 68

one-of problems

one-of or multiclass classi   cation
classes are mutually exclusive.
each document belongs to exactly one class.
example: language of a document (assumption: no document
contains multiple languages)

63 / 68

one-of classi   cation with linear classi   ers

combine two-class linear classi   ers as follows for one-of
classi   cation:

run each classi   er separately
rank classi   ers (e.g., according to score)
pick the class with the highest score

64 / 68

any-of problems

any-of or multilabel classi   cation

a document can be a member of 0, 1, or many classes.
a decision on one class leaves decisions open on all other
classes.
a type of    independence    (but not statistical independence)
example: topic classi   cation
usually: make decisions on the region, on the subject area, on
the industry and so on    independently   

65 / 68

any-of classi   cation with linear classi   ers

combine two-class linear classi   ers as follows for any-of
classi   cation:

simply run each two-class classi   er separately on the test
document and assign document accordingly

66 / 68

take-away today

vector space classi   cation: basic idea of doing text
classi   cation for documents that are represented as vectors

rocchio classi   er: rocchio relevance feedback idea applied to
text classi   cation

k nearest neighbor classi   cation

linear classi   ers

more than two classes

67 / 68

resources

chapter 13 of iir (feature selection)

chapter 14 of iir
resources at http://cislmu.org

id88 example
general overview of text classi   cation: sebastiani (2002)
text classi   cation chapter on decision tress and id88s:
manning & sch  utze (1999)
one of the best machine learning textbooks: hastie, tibshirani
& friedman (2003)

68 / 68

