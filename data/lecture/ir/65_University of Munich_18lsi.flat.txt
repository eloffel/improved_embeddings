introduction to information retrieval

http://informationretrieval.org

iir 18: id45

hinrich sch  utze

center for information and language processing, university of munich

2013-07-10

1 / 43

overview

1 recap

2 id45

3 id84

4 lsi in information retrieval

5 id91

2 / 43

outline

1 recap

2 id45

3 id84

4 lsi in information retrieval

5 id91

3 / 43

indexing anchor text

anchor text is often a better description of a page   s content
than the page itself.

anchor text can be weighted more highly than the text on the
page.
a google bomb is a search with    bad    results due to
maliciously manipulated anchor text.

[dangerous cult] on google, bing, yahoo

4 / 43

id95

model: a web surfer doing a random walk on the web

formalization: markov chain

id95 is the long-term visit rate of the random surfer or
the steady-state distribution.

need teleportation to ensure well-de   ned id95
power method to compute id95

id95 is the principal left eigenvector of the transition
id203 matrix.

5 / 43

computing id95: power method

x1
pt (d1) pt (d2)

x2

t0
t1
t2
t3

0
0.3
0.24
0.252

1
0.7
0.76
0.748

p11 = 0.1 p12 = 0.9
p21 = 0.3 p22 = 0.7
0.3
0.24
0.252
0.2496

0.7
0.76
0.748
0.7504

t    0.25

0.75

0.25

. . .
0.75

id95 vector = ~   = (  1,   2) = (0.25, 0.75)

pt (d1) = pt   1(d1)     p11 + pt   1(d2)     p21

pt (d2) = pt   1(d1)     p12 + pt   1(d2)     p22

= ~xp
= ~xp 2
= ~xp 3
= ~xp 4

= ~xp    

6 / 43

hits: hubs and authorities

hubs

authorities

www.bestfares.com

www.airlinesquality.com

blogs.usatoday.com/sky

aviationblog.dallasnews.com

www.aa.com

www.delta.com

www.united.com

7 / 43

hits update rules

a: link matrix
~h: vector of hub scores
~a: vector of authority scores
hits algorithm:

compute ~h = a~a
compute ~a = at ~h
iterate until convergence
output (i) list of hubs ranked according to hub score and (ii)
list of authorities ranked according to authority score

8 / 43

take-away today

id45 (lsi) / singular value
decomposition: the math

svd used for id84

lsi: svd in information retrieval

lsi as id91

9 / 43

outline

1 recap

2 id45

3 id84

4 lsi in information retrieval

5 id91

10 / 43

recall: term-document matrix

anthony

and

cleopatra

julius
caesar tempest

the

haid113t othello macbeth

5.25
1.21
8.59
0.0
2.85
1.51
1.37

anthony
brutus
caesar
calpurnia
cleopatra
mercy
worser
. . .
this matrix is the basis for computing the similarity between

0.0
1.0
1.51
0.0
0.0
0.12
4.15

0.0
0.0
0.25
0.0
0.0
5.25
0.25

3.18
6.10
2.54
1.54
0.0
0.0
0.0

0.0
0.0
0.0
0.0
0.0
1.90
0.11

0.35
0.0
0.0
0.0
0.0
0.88
1.95

documents and queries. today: can we transform this matrix, so

that we get a better measure of similarity between documents and
queries?

11 / 43

id45: overview

we will decompose the term-document matrix into a product
of matrices.

the particular decomposition we   ll use: singular value
decomposition (svd).
svd: c = u  v t (where c = term-document matrix)
we will then use the svd to compute a new, improved
term-document matrix c    .
we   ll get better similarity values out of c     (compared to c ).

using svd for this purpose is called id45
or lsi.

12 / 43

example of c = u  v t : the matrix c

c
ship
boat
ocean
wood
tree

d1
1
0
1
1
0

d2
0
1
1
0
0

d3
1
0
0
0
0

d4
0
0
0
1
1

d5
0
0
0
1
0

d6
0
0
0
0
1

this is a standard

term-document matrix. actually, we use a non-weighted matrix

here to simplify the example.

13 / 43

example of c = u  v t : the matrix u

1

3
2
u
0.57
   0.44    0.30
ship
boat
   0.13    0.33    0.59
ocean    0.48    0.51    0.37
wood    0.70
   0.26
tree

0.35
0.65    0.41

5
4
0.25
0.58
0.00
0.73
0.00    0.61
0.16
0.58    0.09

0.15    0.58

one row per

term, one column per min(m, n) where m is the number of terms
and n is the number of documents. this is an orthonormal matrix:

(i) row vectors have unit length. (ii) any two distinct row vectors
are orthogonal to each other. think of the dimensions as

   semantic    dimensions that capture distinct topics like politics,
sports, economics. 2 = land/water each number uij in the matrix

indicates how strongly related term i is to the topic represented by
semantic dimension j.

14 / 43

example of c = u  v t : the matrix   

   1
1
2
3
4
5

2.16
0.00
0.00
0.00
0.00

2
0.00
1.59
0.00
0.00
0.00

3
0.00
0.00
1.28
0.00
0.00

4
0.00
0.00
0.00
1.00
0.00

5
0.00
0.00
0.00
0.00
0.39

this is a square, diagonal

matrix of dimensionality min(m, n)    min(m, n). the diagonal

consists of the singular values of c . the magnitude of the singular

value measures the importance of the corresponding semantic
dimension. we   ll make use of this by omitting unimportant

dimensions.

15 / 43

example of c = u  v t : the matrix v t

v t
1
2
3
4
5

d4

d2

d1

d5

d3

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
0.41
   0.29    0.53    0.19
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.28    0.75
0.00
0.00
   0.53
0.29

0.00    0.58
0.19

one

column per document, one row per min(m, n) where m is the
number of terms and n is the number of documents. again: this

is an orthonormal matrix: (i) column vectors have unit length. (ii)
any two distinct column vectors are orthogonal to each other.
these are again the semantic dimensions from matrices u and   

that capture distinct topics like politics, sports, economics. each

number vij in the matrix indicates how strongly related document i
is to the topic represented by semantic dimension j.

16 / 43

example of c = u  v t : all four matrices

d1
1
0
1
1
0

d3
1
0
0
0
0

d5
0
0
0
1
0

d2
0
1
1
0
0
1

d4
c
0
ship
boat
0
0
ocean
1
wood
tree
1
3
2
u
   0.44    0.30
0.57
ship
   0.13    0.33    0.59
boat
ocean    0.48    0.51    0.37
wood    0.70
   0.26
tree
2
0.00
1.59
0.00
0.00
0.00

0.35
0.65    0.41
3
0.00
0.00
1.28
0.00
0.00
d2

4
0.00
0.00
0.00
1.00
0.00
d3

2.16
0.00
0.00
0.00
0.00

d1

d6
0
0
0
0
1

=

  

5
0.00
0.00
0.00
0.00
0.39
d4

5
4
0.25
0.58
0.00
0.73
0.00    0.61
0.16
0.58    0.09

  

0.15    0.58

   1
1
2
3
4
5
v t
1
2
3
4
5

d5

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
   0.29    0.53    0.19
0.41
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.28    0.75
0.00
0.00
   0.53
0.29

0.00    0.58
0.19

lsi is

decomposition of c into a representation of the terms, a
representation of the documents and a representation of the
importance of the    semantic    dimensions.

17 / 43

lsi: summary

we   ve decomposed the term-document matrix c into a
product of three matrices: u  v t .
the term matrix u     consists of one (row) vector for each
term
the document matrix v t     consists of one (column) vector
for each document

the singular value matrix        diagonal matrix with singular
values, re   ecting importance of each dimension

next: why are we doing this?

18 / 43

exercise

v t
1
2
3
4
5

d4

d2

d1

d5

d3

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
   0.29    0.53    0.19
0.41
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.28    0.75
0.00
0.00
   0.53
0.29

0.00    0.58
0.19

verify

that the    rst document has unit length. verify that the    rst two

documents are orthogonal.
0.752 + 0.292 + 0.282 + 0.002 + 0.532 = 1.0059    0.75        0.28 +

   0.29        0.53 + 0.28        0.75 + 0.00     0.00 +    0.53     0.29 = 0

19 / 43

outline

1 recap

2 id45

3 id84

4 lsi in information retrieval

5 id91

20 / 43

how we use the svd in lsi

key property: each singular value tells us how important its
dimension is.

by setting less important dimensions to zero, we keep the
important information, but get rid of the    details   .
these details may

be noise     in that case, reduced lsi is a better representation
because it is less noisy.
make things dissimilar that should be similar     again, the
reduced lsi representation is a better representation because it
represents similarity better.

analogy for    fewer details is better   

image of a blue    ower
image of a yellow    ower
omitting color makes is easier to see the similarity

21 / 43

reducing the dimensionality to 2

3
0.00
0.00
0.00
0.00
0.00

4
0.00
0.00
0.00
0.00
0.00

5
0.00
0.00
0.00
0.00
0.00

4
0.00
0.00
0.00
0.00
0.00
d3

5
0.00
0.00
0.00
0.00
0.00
d4

1

2
u
ship
   0.44    0.30
   0.13    0.33
boat
ocean    0.48    0.51
0.35
wood    0.70
tree
   0.26
0.65
  2
2
0.00
1
2
1.59
0.00
3
0.00
4
5
0.00
v t
1
2
3
4
5

1
2.16
0.00
0.00
0.00
0.00

0.00
0.00
0.00

d2

3
0.00
0.00
0.00
0.00
0.00

d5

d1

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.41
   0.29    0.53    0.19
0.00
0.00
0.00
0.00
0.00
0.00

0.63
0.00
0.00
0.00

0.22
0.00
0.00
0.00

0.00
0.00
0.00

actually, we
only zero out
singular values
in   . this has
the e   ect of
setting the
corresponding
dimensions in
u and v t to
zero when
computing the
product c =
u  v t .

22 / 43

reducing the dimensionality to 2

d5

d4
0.13

d2
0.52
0.36
0.72
0.12

d6
d3
d1
0.28
0.21    0.08
0.85
0.16    0.20    0.02    0.18
0.36
0.36    0.04
0.16    0.21
1.01
0.41
0.62
1.03
0.97
0.20
0.12    0.39    0.08
0.90
0.41
0.49

1

c2
ship
boat
ocean
wood
tree
3
2
u
   0.44    0.30
0.57
ship
boat
   0.13    0.33    0.59
ocean    0.48    0.51    0.37
wood    0.70
   0.26
tree
2
  2
1
0.00
2
1.59
3
0.00
0.00
4
5
0.00
v t
1
2
3
4
5

0.28    0.75
0.00
0.00
   0.53
0.29

4
0.00
0.00
0.00
0.00
0.00
d3

3
0.00
0.00
0.00
0.00
0.00

1
2.16
0.00
0.00
0.00
0.00

0.35
0.65    0.41

d2

5
4
0.25
0.58
0.73
0.00
0.00    0.61
0.16
0.58    0.09

  

0.15    0.58

  

5
0.00
0.00
0.00
0.00
0.00
d4

d5

d1

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
   0.29    0.53    0.19
0.41
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.00    0.58
0.19

=

23 / 43

example of c = u  v t : all four matrices

d1
1
0
1
1
0

d3
1
0
0
0
0

d5
0
0
0
1
0

d2
0
1
1
0
0
1

d4
c
0
ship
boat
0
0
ocean
1
wood
tree
1
3
2
u
   0.44    0.30
0.57
ship
   0.13    0.33    0.59
boat
ocean    0.48    0.51    0.37
wood    0.70
   0.26
tree
2
0.00
1.59
0.00
0.00
0.00

0.35
0.65    0.41
3
0.00
0.00
1.28
0.00
0.00
d2

4
0.00
0.00
0.00
1.00
0.00
d3

2.16
0.00
0.00
0.00
0.00

d1

d6
0
0
0
0
1

=

  

5
0.00
0.00
0.00
0.00
0.39
d4

5
4
0.25
0.58
0.00
0.73
0.00    0.61
0.16
0.58    0.09

  

0.15    0.58

   1
1
2
3
4
5
v t
1
2
3
4
5

d5

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
   0.29    0.53    0.19
0.41
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.28    0.75
0.00
0.00
   0.53
0.29

0.00    0.58
0.19

lsi is

decomposition of c into a representation of the terms, a
representation of the documents and a representation of the
importance of the    semantic    dimensions.

24 / 43

original matrix c vs. reduced c2 = u  2v t

c
ship
boat
ocean
wood
tree
c2
ship
boat
ocean
wood
tree

d6
0
0
0
0
1

d4
0
0
0
1
1

d2
0
1
1
0
0

d1
1
0
1
1
0

d3
1
0
0
0
0
d2
0.52
0.36
0.72
0.12

d5
0
0
0
1
0
d6
d3
d1
0.28
0.21    0.08
0.85
0.16    0.20    0.02    0.18
0.36
0.16    0.21
0.36    0.04
1.01
0.41
0.62
1.03
0.97
0.20
0.12    0.39    0.08
0.90
0.41
0.49

d4
0.13

d5

we can view
c2 as a two-
dimensional
representation
of the matrix
c . we have
performed a
dimensionality
reduction to
two
dimensions.

25 / 43

exercise

c
ship
boat
ocean
wood
tree
c2
ship
boat
ocean
wood
tree

d6
0
0
0
0
1

d4
0
0
0
1
1

d2
0
1
1
0
0

d1
1
0
1
1
0

d3
1
0
0
0
0
d2
0.52
0.36
0.72
0.12

d5
0
0
0
1
0
d6
d3
d1
0.28
0.21    0.08
0.85
0.16    0.20    0.02    0.18
0.36
0.16    0.21
0.36    0.04
1.01
0.41
0.62
1.03
0.97
0.20
0.12    0.39    0.08
0.90
0.41
0.49

d4
0.13

d5

compute the
similarity between
d2 and d3 for the
original matrix
and for the
reduced matrix.

26 / 43

why the reduced matrix c2 is better than c

c
ship
boat
ocean
wood
tree
c2
ship
boat
ocean
wood
tree

d6
0
0
0
0
1

d4
0
0
0
1
1

d2
0
1
1
0
0

d1
1
0
1
1
0

d3
1
0
0
0
0
d2
0.52
0.36
0.72
0.12

d5
0
0
0
1
0
d6
d3
d1
0.28
0.21    0.08
0.85
0.16    0.20    0.02    0.18
0.36
0.16    0.21
0.36    0.04
1.01
0.41
0.62
1.03
0.97
0.20
0.12    0.39    0.08
0.90
0.41
0.49

d4
0.13

d5

similarity of
d2 and d3 in
the original
space: 0.
similarity of
d2 and d3 in
the reduced
space:
0.52     0.28 +
0.36     0.16 +
0.72     0.36 +
0.12     0.20 +
   0.39    
   0.08     0.52
   boat    and
   ship    are
semantically
similar. the
   reduced   
similarity
measure
re   ects this.

27 / 43

exercise: compute matrix product

???????=

d4
-0.19
-0.21
-0.32
0.22
0.41

d6
d5
-0.12
-0.07
-0.14
-0.07
-0.21
-0.11
0.14
0.08
0.27
0.14
5
4
0.25
0.58
0.00
0.73
0.00    0.61
0.16
0.58    0.09

  

0.15    0.58

d2
0.16
0.17
0.27
-0.19
-0.34

d3
0.06
0.06
0.10
-0.07
-0.12

d1
0.09
0.10
0.15
-0.10
-0.19
1

c2
ship
boat
ocean
wood
tree
3
2
u
   0.44    0.30
0.57
ship
   0.13    0.33    0.59
boat
ocean    0.48    0.51    0.37
wood    0.70
   0.26
tree
  2
2
0.00
1
1.59
2
3
0.00
0.00
4
0.00
5
v t
1
2
3
4
5

4
0.00
0.00
0.00
0.00
0.00
d3

3
0.00
0.00
0.00
0.00
0.00

1
0.00
0.00
0.00
0.00
0.00

0.35
0.65    0.41

d1

  

5
0.00
0.00
0.00
0.00
0.00
d4

d2

d5

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
   0.29    0.53    0.19
0.41
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.28    0.75
0.00
0.00
0.29
   0.53

0.00    0.58
0.19

28 / 43

outline

1 recap

2 id45

3 id84

4 lsi in information retrieval

5 id91

29 / 43

why we use lsi in information retrieval

lsi takes documents that are semantically similar (= talk
about the same topics), . . .

. . . but are not similar in the vector space (because they use
di   erent words) . . .

. . . and re-represents them in a reduced vector space . . .

. . . in which they have higher similarity.

thus, lsi addresses the problems of synonymy and semantic
relatedness.

standard vector space: synonyms contribute nothing to
document similarity.

desired e   ect of lsi: synonyms contribute strongly to
document similarity.

30 / 43

how lsi addresses synonymy and semantic relatedness

the id84 forces us to omit a lot of
   detail   .

we have to map di   erents words (= di   erent dimensions of
the full space) to the same dimension in the reduced space.

the    cost    of mapping synonyms to the same dimension is
much less than the cost of collapsing unrelated words.

svd selects the    least costly    mapping (see below).

thus, it will map synonyms to the same dimension.

but it will avoid doing that for unrelated words.

31 / 43

lsi: comparison to other approaches

recap: relevance feedback and id183 are used to
increase recall in information retrieval     if query and
documents have no terms in common.

(or, more commonly, too few terms in common for a high
similarity score)

lsi increases recall and hurts precision.

thus, it addresses the same problems as (pseudo) relevance
feedback and id183 . . .

. . . and it has the same problems.

32 / 43

implementation

compute svd of term-document matrix

reduce the space and compute reduced document
representations
map the query into the reduced space ~qk =      1
this follows from: ck = uk   k v t
compute similarity of qk with all reduced documents in vk.
output ranked list of documents as usual

k ~q.
k u t c = v t

k          1

k u t

k

exercise: what is the fundamental problem with this
approach?

33 / 43

optimality

svd is optimal in the following sense.

keeping the k largest singular values and setting all others to
zero gives you the optimal approximation of the original
matrix c . eckart-young theorem

optimal: no other matrix of the same rank (= with the same
underlying dimensionality) approximates c better.

measure of approximation is frobenius norm:

||c ||f = qpi pj c 2

ij

so lsi uses the    best possible    matrix.

there is only one best possible matrix     unique solution
(modulo signs).

caveat: there is only a tenuous relationship between the
frobenius norm and cosine similarity between documents.

34 / 43

data for graphical illustration of lsi

human machine interface for lab abc computer applications
a survey of user opinion of computer system response time
the eps user interface management system
system and human system engineering testing of eps
relation of user perceived response time to error measurement

c1
c2
c3
c4
c5
m1 the generation of random binary unordered trees
m2 the intersection graph of paths in trees
m3 graph minors iv widths of trees and well quasi ordering
m4 graph minors a survey

human
interface
computer
user
system
response
time
eps
survey
trees
graph
minors

c1
1
1
1
0
0
0
0
0
0
0
0
0

c2
0
0
1
1
1
1
1
0
1
0
0
0

the matrix c
c3
0
1
0
1
1
0
0
1
0
0
0
0

c4
1
0
0
0
2
0
0
1
0
0
0
0

c5 m1 m2 m3 m4
0
0
0
1
0
1
1
0
0
0
0
0

0
0
0
0
0
0
0
0
0
1
1
0

0
0
0
0
0
0
0
0
0
1
1
1

0
0
0
0
0
0
0
0
1
0
1
1

0
0
0
0
0
0
0
0
0
1
0
0

35 / 43

graphical illustration of lsi: plot of c2

2-dimensional plot of
c2 (scaled dimensions).
circles = terms. open
squares = documents
(component
in
parentheses). q = query
   human computer inter-
action   .

terms

the dotted cone represents the region whose points are within a cosine of
.9 from q . all documents about human-computer documents (c1-c5) are
near q, even c3/c5 although they share no terms. none of the id207
documents (m1-m4) are near q.

36 / 43

exercise

what happens when we rank documents according to cosine
similarity in the original vector space? what happens when we
rank documents according to cosine similarity in the reduced vector
space?

37 / 43

lsi performs better than vector space on med collection

lsi-100 = lsi reduced to

100 dimensions; smart = smart implementation of vector
space model

38 / 43

outline

1 recap

2 id45

3 id84

4 lsi in information retrieval

5 id91

39 / 43

example of c = u  v t : all four matrices

d1
1
0
1
1
0

d3
1
0
0
0
0

d5
0
0
0
1
0

d2
0
1
1
0
0
1

d4
c
0
ship
boat
0
0
ocean
1
wood
tree
1
3
2
u
   0.44    0.30
0.57
ship
   0.13    0.33    0.59
boat
ocean    0.48    0.51    0.37
wood    0.70
   0.26
tree
2
0.00
1.59
0.00
0.00
0.00

0.35
0.65    0.41
3
0.00
0.00
1.28
0.00
0.00
d2

4
0.00
0.00
0.00
1.00
0.00
d3

2.16
0.00
0.00
0.00
0.00

d1

d6
0
0
0
0
1

=

  

5
0.00
0.00
0.00
0.00
0.39
d4

5
4
0.25
0.58
0.00
0.73
0.00    0.61
0.16
0.58    0.09

  

0.15    0.58

   1
1
2
3
4
5
v t
1
2
3
4
5

d5

d6
   0.75    0.28    0.20    0.45    0.33    0.12
0.22
   0.29    0.53    0.19
0.41
0.12    0.33
0.58
0.41    0.22

0.63
0.45    0.20
0.58
0.63

0.28    0.75
0.00
0.00
   0.53
0.29

0.00    0.58
0.19

lsi is

decomposition of c into a representation of the terms, a
representation of the documents and a representation of the
importance of the    semantic    dimensions.

40 / 43

why lsi can be viewed as soft id91

each of the k dimensions of the reduced space is one cluster.
if the value of the lsi representation of document d on
dimension k is x, then x is the soft membership of d in topic
k.
this soft membership can be positive or negative.
example: dimension 2 in our svd decomposition

this dimension/cluster corresponds to the water/earth
dichotomy.
   ship   ,    boat   ,    ocean    have negative values.
   wood   ,    tree    have positive values.
d1, d2, d3 have negative values (most of their terms are water
terms).
d4, d5, d6 have positive values (all of their terms are earth
terms).

41 / 43

take-away today

id45 (lsi) / singular value
decomposition: the math

svd used for id84

lsi: svd in information retrieval

lsi as id91

42 / 43

resources

chapter 18 of iir
resources at http://cislmu.org

original paper on id45 by deerwester et al.
paper on probabilistic lsi by thomas hofmann
word space: lsi for words

43 / 43

