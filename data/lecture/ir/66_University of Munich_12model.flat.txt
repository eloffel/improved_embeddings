introduction to information retrieval

http://informationretrieval.org

iir 12: language models for ir

hinrich sch  utze

center for information and language processing, university of munich

2013-05-21

1 / 50

overview

1 recap

2 feature selection

3 language models

4 language models for ir

5 discussion

2 / 50

outline

1 recap

2 feature selection

3 language models

4 language models for ir

5 discussion

3 / 50

naive bayes classi   cation rule

c map = arg max

c   c

[log   p(c) + x
1   k   nd

log   p(tk |c)]

each conditional parameter log   p(tk |c) is a weight that
indicates how good an indicator tk is for c.
the prior log   p(c) is a weight that indicates the relative
frequency of c.

the sum of log prior and term weights is then a measure of
how much evidence there is for the document being in the
class.

we select the class with the most evidence.

4 / 50

parameter estimation

prior:

  p(c) =

nc
n

where nc is the number of docs in class c and n the total
number of docs

conditional probabilities:

  p(t|c) =

tct + 1

pt       v (tct     + 1)

where tct is the number of tokens of t in training documents
from class c (includes multiple occurrences)

5 / 50

add-one smoothing to avoid zeros

c =china

x1=beijing

x2=and

x3=taipei

x4=join

x5=wto

without add-one smoothing: if there are no occurrences of wto in
documents in class china, we get a zero estimate for the corresponding
parameter:

  p(wto|china) =

tchina,
wto
pt       v tchina,t    

= 0

with this estimate:
we must smooth to get a better estimate p(china|d) > 0.

[d contains wto]     [p(china|d) = 0].

6 / 50

naive bayes generative model

c =china

x1=beijing

x2=and

x3=taipei

x4=join

x5=wto

p(c|d )     p(c) q1   k   nd

p(tk |c)

generate a class with id203 p(c)
generate each of the words (in their respective positions), conditional
on the class, but independent of each other, with id203 p(tk |c)

7 / 50

take-away today

feature selection for text classi   cation: how to select a subset
of available dimensions

statistical language models: introduction

statistical language models in ir

discussion: properties of di   erent probabilistic models in use
in ir

8 / 50

outline

1 recap

2 feature selection

3 language models

4 language models for ir

5 discussion

9 / 50

feature selection

in text classi   cation, we usually represent documents in a
high-dimensional space, with each dimension corresponding to
a term.

in this lecture: axis = dimension = word = term = feature

many dimensions correspond to rare words.

rare words can mislead the classi   er.

rare misleading features are called noise features.

eliminating noise features from the representation increases
e   ciency and e   ectiveness of text classi   cation.

eliminating features is called feature selection.

10 / 50

example for a noise feature

let   s say we   re doing text classi   cation for the class china.

suppose a rare term, say arachnocentric, has no
information about china . . .

. . . but all instances of arachnocentric happen to occur in
china documents in our training set.

then we may learn a classi   er that incorrectly interprets
arachnocentric as evidence for the class china.

such an incorrect generalization from an accidental property
of the training set is called over   tting.

feature selection reduces over   tting and improves the
accuracy of the classi   er.

11 / 50

basic feature selection algorithm

selectfeatures(d, c, k)
1 v     extractvocabulary(d)
2 l     []
3 for each t     v
4 do a(t, c)     computefeatureutility(d, t, c)
5
6 return featureswithlargestvalues(l, k)
how do we compute a, the feature utility?

append(l, ha(t, c), ti)

12 / 50

di   erent feature selection methods

a feature selection method is mainly de   ned by the feature
utility measure it employs
feature utility measures:

frequency     select the most frequent terms
mutual information     select the terms with the highest mutual
information
mutual information is also called information gain in this
context.
chi-square (see book)

13 / 50

mutual information

compute the feature utility a(t, c) as the mutual information
(mi) of term t and class c.

mi tells us    how much information    the term contains about
the class and vice versa.

for example, if a term   s occurrence is independent of the class
(same proportion of docs within/without class contain the
term), then mi is 0.

de   nition:

i (u; c ) = x

x

p(u = et, c = ec ) log2

et    {1,0}

ec    {1,0}

p(u = et, c = ec )
p(u = et)p(c = ec )

14 / 50

how to compute mi values

based on maximum likelihood estimates, the formula we
actually use is:

i (u; c ) =

n11
n
n10
n

+

log2

nn11
n1.n.1

+

log2

nn10
n1.n.0

n01
n
n00
n

+

log2

nn01
n0.n.1

log2

nn00
n0.n.0

n10: number of documents that contain t (et = 1) and are
not in c (ec = 0); n11: number of documents that contain t
(et = 1) and are in c (ec = 1); n01: number of documents
that do not contain t (et = 1) and are in c (ec = 1); n00:
number of documents that do not contain t (et = 1) and are
not in c (ec = 1); n = n00 + n01 + n10 + n11.

15 / 50

how to compute mi values (2)

alternative way of computing mi:

i (u; c ) = x

x

p(u = et, c = ec ) log2

et    {1,0}

ec    {1,0}

n(u = et , c = ec )
e (u = et)e (c = ec )

n(u = et , c = ec ) is the count of documents with values et
and ec .
e (u = et, c = ec ) is the expected count of documents with
values et and ec if we assume that the two random variables
are independent.

16 / 50

mi example for poultry/export in reuters

et = e export = 1
et = e export = 0

ec = epoultry = 1
n11 = 49
n01 = 141

ec = epoultry = 0
n10 = 27,652
n00 = 774,106

plug

these values into formula:

i (u; c ) =

log2

801,948    49

(49+27,652)(49+141)

49

801,948
141

+

+

801,948
27,652
801,948
774,106
801,948
    0.000105

+

log2

log2

log2

801,948    141

(141+774,106)(49+141)

801,948    27,652

(49+27,652)(27,652+774,106)

801,948    774,106

(141+774,106)(27,652+774,106)

17 / 50

mi feature selection on reuters

term
coffee

bags

class: co   ee
mi
0.0111
0.0042
0.0025
0.0019
0.0018
0.0016
0.0014
0.0013
0.0013
0.0012

growers

kg

colombia

brazil

export

exports

crop

exporters

term
soccer

cup

match

class: sports
mi
0.0681
0.0515
0.0441
0.0408
0.0388
0.0386
0.0301
0.0299
0.0284
0.0264

beat

game

games

team

matches

played

league

18 / 50

naive bayes: e   ect of feature selection

e
r
u
s
a
e
m
1
f

 

8

.

0

6

.

0

4

.

0

2
0

.

0

.
0

#
b

o

x

#

o
b

x

o

#

b

x

o

b
x
#

bb

b

b

x

x x

b

x

#

x

o
#

x
#
o
b

x
b
o
#

x
o
#

b

xx
oo
##

bb

#
o
x
b

multinomial, mi
multinomial, chisquare
multinomial, frequency
binomial, mi

o o oo
#
# #

o
x
#
b

1

100

10
number of features selected

1000

10000

(multinomial
= multino-
mial naive
bi-
bayes,
nomial
=
bernoulli
naive
bayes)

19 / 50

feature selection for naive bayes

in general, feature selection is necessary for naive bayes to
get decent performance.

also true for many other learning methods in text
classi   cation: you need feature selection for optimal
performance.

20 / 50

exercise

(i) compute the    export   /poultry contingency table for the
   kyoto   /japan in the collection given below. (ii) make up a
contingency table for which mi is 0     that is, term and class are
independent of each other.    export   /poultry table:

et = e export = 1
et = e export = 0

collection:

ec = epoultry = 1
n11 = 49
n01 = 141

ec = epoultry = 0
n10 = 27,652
n00 = 774,106

training set

docid words in document
1
2
3
4
5

kyoto osaka taiwan
japan kyoto
taipei taiwan
macao taiwan shanghai
london

in c = japan?
yes
yes
no
no
no

21 / 50

outline

1 recap

2 feature selection

3 language models

4 language models for ir

5 discussion

22 / 50

using language models (lms) for ir

1 lm = language model

2 we view the document as a generative model that generates

the query.

3 what we need to do:

4 de   ne the precise generative model we want to use

5 estimate parameters (di   erent parameters for each

document   s model)

6 smooth to avoid zeros

7 apply to query and    nd document most likely to have

generated the query

8 present most likely document(s) to user

9 note that 4   7 is very similar to what we did in naive bayes.

23 / 50

what is a language model?

we can view a    nite state automaton as a deterministic language

i

wish

model.

i wish i wish i wish i wish . . .

cannot generate:    wish i wish   

or    i wish i    our basic model: each document was generated by a

di   erent automaton like this except that these automata are
probabilistic.

24 / 50

a probabilistic language model

q1

p(w |q1) w

w
stop 0.2
0.2
the
0.1
a
frog
0.01

p(w |q1)
0.01
0.03
0.02
0.04
. . .

this

toad
said
likes
that
. . .

is a one-state probabilistic    nite-state automaton     a unigram
language model     and the state emission distribution for its one
state q1. stop is not a word, but a special symbol indicating that

the automaton stops. frog said that toad likes frog stop

p(string) = 0.01   0.03   0.04   0.01   0.02   0.01   0.2

= 0.0000000000048

25 / 50

a di   erent language model for each document

language model of d1
p(w |.) w

w
stop .2
.2
the
a
.1
.01
frog

p(w |.)
.01
.03
.02
.04
. . .

toad
said
likes
that
. . .

language model of d2
p(w |.) w

w
stop .2
.15
the
a
.08
.01
frog

p(w |.)
.02
.03
.02
.05
. . .

toad
said
likes
that
. . .

query: frog said that toad likes frog stop p(query|md1) = 0.01

  0.03   0.04   0.01   0.02   0.01   0.2 = 0.0000000000048 = 4.8    10   12
p(query|md2) = 0.01   0.03   0.05   0.02   0.02   0.01   0.2
= 0.0000000000120 = 12    10   12 p(query|md1) < p(query|md2)

thus, document d2 is    more relevant    to the query    frog said that
toad likes frog stop    than d1 is.

26 / 50

outline

1 recap

2 feature selection

3 language models

4 language models for ir

5 discussion

27 / 50

using language models in ir

each document is treated as (the basis for) a language model.

given a query q

rank documents based on p(d|q)

p(d|q) =

p(q|d)p(d)

p(q)

p(q) is the same for all documents, so ignore
p(d) is the prior     often treated as the same for all d

but we can give a higher prior to    high-quality    documents,
e.g., those with high id95.

p(q|d) is the id203 of q given d .

for uniform prior: ranking documents according according to
p(q|d) and p(d|q) is equivalent.

28 / 50

where we are

in the lm approach to ir, we attempt to model the query
generation process.

then we rank documents by the id203 that a query
would be observed as a random sample from the respective
document model.

that is, we rank according to p(q|d).

next: how do we compute p(q|d)?

29 / 50

how to compute p(q|d )

we will make the same conditional independence assumption
as for naive bayes.

p(q|md ) = p(ht1, . . . , t|q|i|md ) = y

p(tk |md )

1   k   |q|

(|q|: length of q; tk : the token occurring at position k in q)
this is equivalent to:

p(q|md ) =

y

p(t|md )tf t ,q

distinct term t in q

tf t ,q: term frequency (# occurrences) of t in q
multinomial model (omitting constant factor)

30 / 50

parameter estimation

missing piece: where do the parameters p(t|md ) come from?
start with maximum likelihood estimates (as we did for naive
bayes)

  p(t|md ) =

tf t ,d
|d|

(|d|: length of d; tf t ,d : # occurrences of t in d)
as in naive bayes, we have a problem with zeros.
a single t with p(t|md ) = 0 will make
p(q|md ) = q p(t|md ) zero.
we would give a single term    veto power   .
for example, for query [michael jackson top hits] a document
about    top songs    (but not using the word    hits   ) would have
p(q|md ) = 0.     thats   s bad.
we need to smooth the estimates to avoid zeros.

31 / 50

smoothing

key intuition: a nonoccurring term is possible (even though it
didn   t occur), . . .

. . . but no more likely than would be expected by chance in
the collection.
notation: mc : the collection model; cf t : the number of
occurrences of t in the collection; t = pt
cf t: the total
number of tokens in the collection.

  p(t|mc ) =

cf t
t

we will use   p(t|mc) to    smooth    p(t|d) away from zero.

32 / 50

jelinek-mercer smoothing

p(t|d) =   p(t|md ) + (1       )p(t|mc )
mixes the id203 from the document with the general
collection frequency of the word.

high value of   :    conjunctive-like    search     tends to retrieve
documents containing all query words.

low value of   : more disjunctive, suitable for long queries

correctly setting    is very important for good performance.

33 / 50

jelinek-mercer smoothing: summary

p(q|d)     y

(  p(tk |md ) + (1       )p(tk |mc ))

1   k   |q|

what we model: the user has a document in mind and
generates the query from this document.

the equation represents the id203 that the document
that the user had in mind was in fact this one.

34 / 50

example

collection: d1 and d2
d1: jackson was one of the most talented entertainers of all
time
d2: michael jackson anointed himself king of pop
query q: michael jackson

use mixture model with    = 1/2
p(q|d1) = [(0/11 + 1/18)/2]    [(1/11 + 2/18)/2]     0.003
p(q|d2) = [(1/7 + 1/18)/2]    [(1/7 + 2/18)/2]     0.013
ranking: d2 > d1

35 / 50

exercise: compute ranking

collection: d1 and d2
d1: xerox reports a pro   t but revenue is down
d2: lucene narrows quarter loss but revenue decreases further
query q: revenue down

use mixture model with    = 1/2
p(q|d1) = [(1/8 + 2/16)/2]    [(1/8 + 1/16)/2] = 1/8    3/32 =
3/256
p(q|d2) = [(1/8 + 2/16)/2]    [(0/8 + 1/16)/2] = 1/8    1/32 =
1/256
ranking: d1 > d2

36 / 50

dirichlet smoothing

  p(t|d) =

tf t ,d +      p(t|mc)

ld +   

the background distribution   p(t|mc ) is the prior for   p(t|d).
intuition: before having seen any part of the document we
start with the background distribution as our estimate.

as we read the document and count terms we update the
background distribution.

the weighting factor    determines how strong an e   ect the
prior has.

37 / 50

jelinek-mercer or dirichlet?

dirichlet performs better for keyword queries, jelinek-mercer
performs better for verbose queries.

both models are sensitive to the smoothing parameters     you
shouldn   t use these models without parameter tuning.

38 / 50

sensitivity of dirichlet to smoothing parameter

smoothing parameter (called    on the previous slides)

   is the dirichlet

39 / 50

outline

1 recap

2 feature selection

3 language models

4 language models for ir

5 discussion

40 / 50

language models are generative models

we have assumed that queries are generated by a probabilistic
process that looks like this: (as in naive bayes)

c =china

x1=beijing

x2=and

x3=taipei

x4=join

x5=wto

41 / 50

naive bayes and lm generative model s

we want to classify document d.
we want to classify a query q.

classes: e.g., geographical regions like china, uk, kenya.
each document in the collection is a di   erent class.

assume that d was generated by the generative model.
assume that q was generated by a generative model
key question: which of the classes is most likely to have
generated the document? which document (=class) is most
likely to have generated the query q?

or: for which class do we have the most evidence? for which
document (as the source of the query) do we have the most
evidence?

42 / 50

naive bayes multinomial model / ir language models

c =china

x1=beijing

x2=and

x3=taipei

x4=join

x5=wto

43 / 50

naive bayes bernoulli model / binary independence model

c =china

ualaska=0 ubeijing=1

uindia=0

ujoin=1

utaipei=1

uwto=1

44 / 50

comparison of the two models

event model
random variable(s)
doc. representation

multinomial model / ir language model
generation of (multi)set of tokens
x = t i    t occurs at given pos
d = ht1 , . . . , tk , . . . , tnd i, tk     v

bernoulli model / bim
generation of subset of vocabula
ut = 1 i    t occurs in doc
d = he1 , . . . , ei , . . . , em i,

parameter estimation
dec. rule: maximize
multiple occurrences
length of docs
# features
estimate for the

  p(x = tk |c)

  p(x = t|c)
  p(c) q1   k   nd
taken into account
can handle longer docs
can handle more
  p(x = the|c)     0.05

  p(ui = ei |c)

ei     {0, 1}
  p(ui = e|c)
  p(c) qti    v
ignored
works best for short docs
works best with fewer
  p(uthe = 1|c)     1.0

45 / 50

vector space (tf-idf) vs. lm

rec.
0.0
0.1
0.2
0.4
0.6
0.8
1.0
11-point average

tf-idf
0.7439
0.4521
0.3514
0.2093
0.1024
0.0160
0.0028
0.1868

precision
%chg
lm
+2.0
0.7590
+8.6
0.4910
+15.1
0.4045
+22.9
0.2572
0.1405
+37.1
0.0432 +169.6
0.0050
+76.9
+19.6
0.2233

signi   cant

*
*
*
*

*

the

id38 approach always does better in these
experiments . . .
signi   cant gains is at higher levels of recall.

. . . but note that where the approach shows

46 / 50

vector space vs bm25 vs lm

bm25/lm: based on id203 theory
vector space: based on similarity, a geometric/id202
notion
term frequency is directly used in all three models.

lms: raw term frequency, bm25/vector space: more complex

length id172

vector space: cosine or pivot id172
lms: probabilities are inherently length normalized
bm25: tuning parameters for optimizing length id172

idf: bm25/vector space use it directly.
lms: mixing term and collection frequencies has an e   ect
similar to idf.

terms rare in the general collection, but common in some
documents will have a greater in   uence on the ranking.

collection frequency (lms) vs. document frequency (bm25,
vector space)

47 / 50

language models for ir: assumptions

simplifying assumption: queries and documents are objects of
the same type. not true!

there are other lms for ir that do not make this assumption.
the vector space model makes the same assumption.

simplifying assumption: terms are conditionally independent.
again, vector space model (and naive bayes) make the same
assumption.

cleaner statement of assumptions than vector space
thus, better theoretical foundation than vector space

. . . but    pure    lms perform much worse than    tuned    lms.

48 / 50

take-away today

feature selection for text classi   cation: how to select a subset
of available dimensions

statistical language models: introduction

statistical language models in ir

discussion: properties of di   erent probabilistic models in use
in ir

49 / 50

resources

chapter 13 of iir (feature selection)

chapter 12 of iir (language models)
resources at http://cislmu.org

ponte and croft   s 1998 sigir paper (one of the    rst on lms
in ir)
zhai and la   erty: a study of smoothing methods for language
models applied to information retrieval. acm trans. inf. syst.
(2004).
lemur toolkit (good support for lms in ir)

50 / 50

