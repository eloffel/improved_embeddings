introduction to information retrieval

http://informationretrieval.org

iir 9: relevance feedback & id183

hinrich sch  utze

center for information and language processing, university of munich

2014-05-14

1 / 63

overview

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

2 / 63

outline

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

3 / 63

relevance

we will evaluate the quality of an information retrieval system
and, in particular, its ranking algorithm with respect to
relevance.

a document is relevant if it gives the user the information she
was looking for.
to evaluate relevance, we need an evaluation benchmark with
three elements:

a benchmark document collection
a benchmark suite of queries
an assessment of the relevance of each query-document pair

4 / 63

relevance: query vs. information need

the notion of    relevance to the query    is very problematic.

information need i : you are looking for information on
whether drinking red wine is more e   ective at reducing your
risk of heart attacks than white wine.

query q: wine and red and white and heart and
attack

consider document d    : he then launched into the heart of his
speech and attacked the wine industry lobby for downplaying
the role of red and white wine in drunk driving.

d     is relevant to the query q, but d     is not relevant to the
information need i .

user happiness/satisfaction (i.e., how well our ranking
algorithm works) can only be measured by relevance to
information needs, not by relevance to queries.

5 / 63

precision and recall

precision (p) is the fraction of retrieved documents that are
relevant

precision =

#(relevant items retrieved)

#(retrieved items)

= p(relevant|retrieved)

recall (r) is the fraction of relevant documents that are
retrieved

recall =

#(relevant items retrieved)

#(relevant items)

= p(retrieved|relevant)

6 / 63

a combined measure: f

f allows us to trade o    precision against recall.

balanced f :

f1 =

2pr
p + r

this is a kind of soft minimum of precision and recall.

7 / 63

averaged 11-point precision/recall graph

i

i

n
o
s
c
e
r
p

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

this curve is typical of performance levels for the trec benchmark.
70% chance of getting the    rst document right (roughly)
when we want to look at at least 50% of all relevant documents, then for each
relevant document we    nd, we will have to look at about two nonrelevant
documents.
that   s not very good.
high-recall retrieval is an unsolved problem.

8 / 63

google dynamic summaries for [vegetarian diet running]

9 / 63

take-away today

interactive relevance feedback: improve initial retrieval results
by telling the ir system which docs are relevant / nonrelevant

best known relevance feedback method: rocchio feedback
id183: improve retrieval results by adding
synonyms / related terms to the query

sources for related terms: manual thesauri, automatic
thesauri, query logs

10 / 63

overview

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

11 / 63

outline

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

12 / 63

how can we improve recall in search?

main topic today: two ways of improving recall: relevance
feedback and id183

as an example consider query q: [aircraft] . . .

. . . and document d containing    plane   , but not containing
   aircraft   

a simple ir system will not return d for q.

even if d is the most relevant document for q!
we want to change this:

return relevant documents even if there is no term match with
the (original) query

13 / 63

recall

loose de   nition of recall in this lecture:    increasing the
number of relevant documents returned to user   
this may actually decrease recall on some measures, e.g.,
when expanding    jaguar    to    jaguar and panthera   

. . . which eliminates some relevant documents, but increases
relevant documents returned on top pages

14 / 63

options for improving recall

local: do a    local   , on-demand analysis for a user query

main local method: relevance feedback
part 1

global: do a global analysis once (e.g., of collection) to
produce thesaurus

use thesaurus for id183
part 2

15 / 63

google used to expose id183 in ui

     ights -   ight

  dogs -dog

no longer available:
http://searchenginewatch.com/article/2277383/google-kills-

16 / 63

outline

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

17 / 63

relevance feedback: basic idea

the user issues a (short, simple) query.

the search engine returns a set of documents.

user marks some docs as relevant, some as nonrelevant.

search engine computes a new representation of the
information need. hope: better than the initial query.

search engine runs new query and returns new results.

new results have (hopefully) better recall.

we will use the term ad hoc retrieval to refer to regular
retrieval without relevance feedback.

18 / 63

relevance feedback: examples

we will now look at three di   erent examples of relevance
feedback that highlight di   erent aspects of the process.

19 / 63

relevance feedback: example 1

20 / 63

results for initial query

21 / 63

user feedback: select what is relevant

22 / 63

results after relevance feedback

23 / 63

vector space example: query    canine    (1)

source:

fernando d    az

24 / 63

similarity of docs to query    canine   

source:

fernando d    az

25 / 63

user feedback: select relevant documents

source:

fernando d    az

26 / 63

results after relevance feedback

source:

fernando d    az

27 / 63

example 3: a real (non-image) example

initial query:
[new space satellite applications] results for initial query: (r = rank)

r
+ 1
+ 2
3

4

5

6

7

0.539 nasa hasn   t scrapped imaging spectrometer
0.533 nasa scratches environment gear from satellite plan
0.528

science panel backs nasa satellite plan, but urges launches
of smaller probes

0.526 a nasa satellite project accomplishes incredible feat: staying

0.525

within budget
scientist who exposed global warming proposes satellites for
climate research

0.524 report provides support for the critics of using big satellites

to study climate

0.516 arianespace receives satellite launch pact from telesat

canada

+ 8

0.509 telecommunications tale of two companies

user then marks relevant documents with    +   .

28 / 63

expanded query after relevance feedback

2.074
30.816
5.991
4.196
3.516
3.004
2.790
2.003
0.836

new
satellite
nasa
launch
instrument
bundespost
rocket
broadcast
oil

15.106
5.660
5.196
3.972
3.446
2.806
2.053
1.172
0.646 measure

space
application
eos
aster
arianespace
ss
scientist
earth

query: [new space satellite applications]

compare to original

29 / 63

results for expanded query (old ranks in parens)

*

*

*

r
1 (2)

0.513 nasa scratches environment gear from satellite

plan

2 (1)
3

0.500 nasa hasn   t scrapped imaging spectrometer
0.493 when the pentagon launches a secret satellite,

space sleuths do some spy work of their own

4

0.493 nasa uses    warm    superconductors for fast cir-

cuit

5 (8)
6

7

8

0.492 telecommunications tale of two companies
0.491

soviets may adapt parts of ss-20 missile for com-
mercial use

0.490 gaping gap: pentagon lags in race to match the

soviets in rocket launchers

0.490 rescue of satellite by space agency to cost $90

million

30 / 63

outline

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

31 / 63

key concept for relevance feedback: centroid

the centroid is the center of mass of a set of points.

recall that we represent documents as points in a
high-dimensional space.

thus: we can compute centroids of documents.

de   nition:

~  (d) =

1
|d| x

d    d

~v(d)

where d is a set of documents and ~v (d) = ~d is the vector we
use to represent document d.

32 / 63

centroid: examples

   

   

   

   

   

   

x

x

x

x

33 / 63

rocchio algorithm

the rocchio algorithm implements relevance feedback in the
vector space model.
rocchio chooses the query ~qopt that maximizes

~qopt = arg max

[sim(~q,   (dr ))     sim(~q,   (dnr ))]

~q

dr : set of relevant docs; dnr : set of nonrelevant docs
intent: ~qopt is the vector that separates relevant and
nonrelevant docs maximally.
making some additional assumptions, we can rewrite ~qopt as:

~qopt =   (dr ) + [  (dr )       (dnr )]

34 / 63

rocchio algorithm

the optimal query vector is:

~qopt =   (dr ) + [  (dr )       (dnr )]
1
|dr | x

1
|dr | x

~dj + [

=

~dj    

~dj    dr

~dj    dr

1
|dnr | x

~dj    dnr

~dj ]

we move the centroid of the relevant documents by the
di   erence between the two centroids.

35 / 63

exercise: compute rocchio vector

x

x
x

x
x

x

documents, xs: nonrelevant documents compute:
~qopt =   (dr ) + [  (dr )       (dnr )]

circles: relevant

36 / 63

rocchio illustrated

~  r     ~  nr
x

x
x

x
x

x

~qopt

~  r

~  nr

circles: relevant documents, xs: nonrelevant documents ~  r :
centroid of relevant documents ~  r does not separate
relevant/nonrelevant. ~  nr : centroid of nonrelevant documents
~  r     ~  nr : di   erence vector add di   erence vector to ~  r . . .
get ~qopt ~qopt separates relevant/nonrelevant perfectly.

. . . to

37 / 63

terminology

so far, we have used the name rocchio for the theoretically
better motivated original version of rocchio.

the implementation that is actually used in most cases is the
smart implementation     this smart version of rocchio is
what we will refer to from now on.

38 / 63

rocchio 1971 algorithm (smart)

used in practice:

~qm =   ~q0 +     (dr )         (dnr )
~dj       

=   ~q0 +   

1
|dr | x

~dj    dr

1
|dnr | x

~dj    dnr

~dj

qm: modi   ed query vector; q0: original query vector; dr and
dnr : sets of known relevant and nonrelevant documents
respectively;   ,   , and   : weights
new query moves towards relevant documents and away from
nonrelevant documents.
tradeo       vs.   /  : if we have a lot of judged documents, we
want a higher   /  .
set negative term weights to 0.
   negative weight    for a term doesn   t make sense in the vector
space model.

39 / 63

positive vs. negative relevance feedback

positive feedback is more valuable than negative feedback.

for example, set    = 0.75,    = 0.25 to give higher weight to
positive feedback.

many systems only allow positive feedback.

40 / 63

relevance feedback: assumptions

when can relevance feedback enhance recall?

assumption a1: the user knows the terms in the collection
well enough for an initial query.

assumption a2: relevant documents contain similar terms
(so i can    hop    from one relevant document to a di   erent one
when giving relevance feedback).

41 / 63

violation of a1

assumption a1: the user knows the terms in the collection
well enough for an initial query.

violation: mismatch of searcher   s vocabulary and collection
vocabulary

example: cosmonaut / astronaut

42 / 63

violation of a2

assumption a2: relevant documents are similar.

example for violation: [contradictory government policies]
several unrelated    prototypes   

subsidies for tobacco farmers vs. anti-smoking campaigns
aid for developing countries vs. high tari   s on imports from
developing countries

relevance feedback on tobacco docs will not help with    nding
docs on developing countries.

43 / 63

relevance feedback: assumptions

when can relevance feedback enhance recall?

assumption a1: the user knows the terms in the collection
well enough for an initial query.

assumption a2: relevant documents contain similar terms
(so i can    hop    from one relevant document to a di   erent one
when giving relevance feedback).

44 / 63

relevance feedback: evaluation

pick an evaluation measure, e.g., precision in top 10: p@10
compute p@10 for original query q0
compute p@10 for modi   ed relevance feedback query q1
in most cases: q1 is spectacularly better than q0!
is this a fair evaluation?

45 / 63

relevance feedback: evaluation

fair evaluation must be on    residual    collection: docs not yet
judged by user.

studies have shown that relevance feedback is successful when
evaluated this way.

empirically, one round of relevance feedback is often very
useful. two rounds are marginally useful.

46 / 63

evaluation: caveat

true evaluation of usefulness must compare to other methods
taking the same amount of time.

alternative to relevance feedback: user revises and resubmits
query.

users may prefer revision/resubmission to having to judge
relevance of documents.

there is no clear evidence that relevance feedback is the    best
use    of the user   s time.

47 / 63

exercise

do search engines use relevance feedback?

why?

48 / 63

relevance feedback: problems

relevance feedback is expensive.

relevance feedback creates long modi   ed queries.
long queries are expensive to process.

users are reluctant to provide explicit feedback.

it   s often hard to understand why a particular document was
retrieved after applying relevance feedback.

the search engine excite had full relevance feedback at one
point, but abandoned it later.

49 / 63

pseudo-relevance feedback

pseudo-relevance feedback automates the    manual    part of
true relevance feedback.
pseudo-relevance feedback algorithm:

retrieve a ranked list of hits for the user   s query
assume that the top k documents are relevant.
do relevance feedback (e.g., rocchio)

works very well on average
but can go horribly wrong for some queries.

because of query drift
if you do several iterations of pseudo-relevance feedback, then
you will get query drift for a large proportion of queries.

50 / 63

pseudo-relevance feedback at trec4

cornell smart system

results show number of relevant documents out of top 100
for 50 queries (so total number of documents is 5000):

method
number of relevant documents
3210
lnc.ltc
3634
lnc.ltc-psrf
lnu.ltu
3709
lnu.ltu-psrf 4350

results contrast two length id172 schemes (l vs. l)
and pseudo-relevance feedback (psrf).

the pseudo-relevance feedback method used added only 20
terms to the query. (rocchio will add many more.)

this demonstrates that pseudo-relevance feedback is e   ective
on average.

51 / 63

outline

1 recap

2 motivation

3 relevance feedback: basics

4 relevance feedback: details

5 id183

52 / 63

id183: example

53 / 63

types of user feedback

user gives feedback on documents.

more common in relevance feedback

user gives feedback on words or phrases.

more common in id183

54 / 63

id183

id183 is another method for increasing recall.

we use    global id183    to refer to    global methods
for query reformulation   .

in global id183, the query is modi   ed based on
some global resource, i.e. a resource that is not
query-dependent.

main information we use: (near-)synonymy

55 / 63

   global    resources used for id183

a publication or database that collects (near-)synonyms is
called a thesaurus.

manual thesaurus (maintained by editors, e.g., pubmed)

automatically derived thesaurus (e.g., based on co-occurrence
statistics)

query-equivalence based on query log mining (common on the
web as in the    palm    example)

56 / 63

thesaurus-based id183

for each term t in the query, expand the query with words the
thesaurus lists as semantically related with t.

example from earlier: hospital     medical

generally increases recall
may signi   cantly decrease precision, particularly with
ambiguous terms

interest rate     interest rate fascinate

widely used in specialized search engines for science and
engineering

it   s very expensive to create a manual thesaurus and to
maintain it over time.

57 / 63

example for manual thesaurus: pubmed

58 / 63

automatic thesaurus generation

attempt to generate a thesaurus automatically by analyzing
the distribution of words in documents

fundamental notion: similarity between two words
de   nition 1: two words are similar if they co-occur with
similar words.

   car           motorcycle    because both occur with    road   ,    gas   
and    license   , so they must be similar.

de   nition 2: two words are similar if they occur in a given
grammatical relation with the same words.

you can harvest, peel, eat, prepare, etc. apples and pears, so
apples and pears must be similar.

co-occurrence is more robust, grammatical relations are more
accurate.

59 / 63

co-occurence-based thesaurus: examples

word
absolutely
bottomed
captivating
doghouse
makeup
mediating
keeping
lithographs
pathogens
senses

nearest neighbors
absurd whatsoever totally exactly nothing
dip copper drops topped slide trimmed
shimmer stunningly superbly plucky witty
dog porch crawling beside downstairs
repellent lotion glossy sunscreen skin gel
reconciliation negotiate case conciliation
hoping bring wiping could some would
drawings picasso dali sculptures gauguin
toxins bacteria organisms bacterial parasite
grasp psyche truly clumsy naive innate

wordspace demo on web

60 / 63

id183 at search engines

main source of id183 at search engines: query logs
example 1: after issuing the query [herbs], users frequently
search for [herbal remedies].

       herbal remedies    is potential expansion of    herb   .

example 2: users searching for [   ower pix] frequently click on
the url photobucket.com/   ower. users searching for [   ower
clipart] frequently click on the same url.

          ower clipart    and       ower pix    are potential expansions of
each other.

61 / 63

take-away today

interactive relevance feedback: improve initial retrieval results
by telling the ir system which docs are relevant / nonrelevant

best known relevance feedback method: rocchio feedback
id183: improve retrieval results by adding
synonyms / related terms to the query

sources for related terms: manual thesauri, automatic
thesauri, query logs

62 / 63

resources

chapter 9 of iir
resources at http://cislmu.org

salton and buckley 1990 (original relevance feedback paper)
spink, jansen, ozmultu 2000: relevance feedback at excite
justin bieber: related searches fail
word space
sch  utze 1998: automatic word sense discrimination (describes
a simple method for automatic thesaurus generation)

63 / 63

