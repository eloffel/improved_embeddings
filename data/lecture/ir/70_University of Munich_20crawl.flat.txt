introduction to information retrieval

http://informationretrieval.org

iir 20: crawling

hinrich sch  utze

center for information and language processing, university of munich

2009.07.14

1 / 36

outline

1 recap

2 a simple crawler

3 a real crawler

2 / 36

search engines rank content pages and ads

3 / 36

google   s second price auction

advertiser
a
b
c
d

bid
$4.00
$3.00
$2.00
$1.00

ctr ad rank
0.01
0.03
0.06
0.08

0.04
0.09
0.12
0.08

rank
4
2
1
3

paid
(minimum)
$2.68
$1.51
$0.51

bid: maximum bid for a click by advertiser

ctr: click-through rate: when an ad is displayed, what
percentage of time do users click on it? ctr is a measure of
relevance.

ad rank: bid    ctr: this trades o    (i) how much money the
advertiser is willing to pay against (ii) how relevant the ad is

paid: second price auction: the advertiser pays the minimum
amount necessary to maintain their position in the auction
(plus 1 cent).

4 / 36

what   s great about search ads

users only click if they are interested.

the advertiser only pays when a user clicks on an ad.

searching for something indicates that you are more likely to
buy it . . .

. . . in contrast to radio and newpaper ads.

5 / 36

near duplicate detection: minimum of permutation

document 1: {sk }

document 2: {sk }

1

1

1

x3   

x3   

s

s1

s

s2

s

s3

s

s4

xk =   (sk )
s
s
s

x1   

x4   

xk

x1   

x4   

s

x2   

x2   

   

2m

   

2m

   

2m

1

1

1

x3   

x3   

s

s1

s

s5

s

s3

s

s4

xk =   (sk )
s
s

s
x4   

x1   

s

xk

x1   

x5   

minsk

  (sk )

minsk

  (sk )

1

x3   
roughly: we use mins    d1
and d2 near-duplicates?

   

2m

x3   
  (s) = mins    d2

1

x5   

x2   

   

2m

   

2m

   

2m

   

2m

  (s) as a test for: are d1

6 / 36

outline

1 recap

2 a simple crawler

3 a real crawler

7 / 36

how hard can crawling be?

web search engines must crawl their documents.
getting the content of the documents is easier for many other
ir systems.

e.g., indexing all    les on your hard disk: just do a recursive
descent on your    le system

ok: for web ir, getting the content of the documents takes
longer . . .

. . . because of latency.

but is that really a design/systems challenge?

8 / 36

basic crawler operation

initialize queue with urls of known seed pages
repeat

take url from queue
fetch and parse page
extract urls from page
add urls to queue

fundamental assumption: the web is well linked.

9 / 36

exercise: what   s wrong with this crawler?

urlqueue := (some carefully selected set of seed urls)
while urlqueue is not empty:
myurl := urlqueue.getlastanddelete()
mypage := myurl.fetch()
fetchedurls.add(myurl)
newurls := mypage.extracturls()
for myurl in newurls:
if myurl not in fetchedurls and not in urlqueue:
urlqueue.add(myurl)
addtoinvertedindex(mypage)

10 / 36

what   s wrong with the simple crawler

scale: we need to distribute.

we can   t index everything: we need to subselect. how?

duplicates: need to integrate duplicate detection

spam and spider traps: need to integrate spam detection

politeness: we need to be    nice    and space out all requests
for a site over a longer period (hours, days)
freshness: we need to recrawl periodically.

because of the size of the web, we can do frequent recrawls
only for a small subset.
again, subselection problem or prioritization

11 / 36

magnitude of the crawling problem

to fetch 20,000,000,000 pages in one month . . .

. . . we need to fetch almost 8000 pages per second!

actually: many more since many of the pages we attempt to
crawl will be duplicates, unfetchable, spam etc.

12 / 36

what a crawler must do

be polite

don   t hit a site too often

only crawl pages you are allowed to crawl: robots.txt

be robust

be immune to spider traps, duplicates, very large pages, very
large websites, dynamic pages etc

13 / 36

robots.txt

protocol for giving crawlers (   robots   ) limited access to a
website, originally from 1994
examples:

user-agent: *
disallow: /yoursite/temp/
user-agent: searchengine
disallow: /

important: cache the robots.txt    le of each site we are
crawling

14 / 36

example of a robots.txt (nih.gov)

user-agent: picosearch/1.0
disallow: /news/information/knight/
disallow: /nidcd/
...
disallow: /news/research_matters/secure/
disallow: /od/ocpl/wag/
user-agent: *
disallow: /news/information/knight/
disallow: /nidcd/
...
disallow: /news/research_matters/secure/
disallow: /od/ocpl/wag/
disallow: /ddir/
disallow: /sdminutes/

15 / 36

what any crawler should do

be capable of distributed operation

be scalable: need to be able to increase crawl rate by adding
more machines

fetch pages of higher quality    rst

continuous operation: get fresh version of already crawled
pages

16 / 36

outline

1 recap

2 a simple crawler

3 a real crawler

17 / 36

url frontier

urls crawled

and parsed

url frontier:

found, but

not yet crawled

unseen urls

18 / 36

url frontier

the url frontier is the data structure that holds and manages
urls we   ve seen, but that have not been crawled yet.

can include multiple pages from the same host

must avoid trying to fetch them all at the same time

must keep all crawling threads busy

19 / 36

basic crawl architecture

       

dns

   
   

   

www

   

parse

   

fetch

   

doc
fps

robots

templates

   
   
      
   
   
   
   
   

   
   
      
   
   
   
   
   

content
seen?

url
   lter

   

url frontier

   

url
set

   
   
      
   
   
   
   

dup
url
elim

20 / 36

url id172

some urls extracted from a document are relative urls.
e.g., at http://mit.edu, we may have aboutsite.html
this is the same as: http://mit.edu/aboutsite.html

during parsing, we must normalize (expand) all relative urls.

21 / 36

content seen

for each page fetched: check if the content is already in the
index

check this using document    ngerprints or shingles

skip documents whose content has already been indexed

22 / 36

distributing the crawler

run multiple crawl threads, potentially at di   erent nodes

usually geographically distributed nodes

partition hosts being crawled into nodes

23 / 36

google data centers (wayfaring.com)

24 / 36

distributed crawler

      

dns
   
   

   

www

   

   

fetch

   

parse

doc
fps

   
   
       
   
   
      
   

content
seen?

to

other
nodes
         

url
   lter

   

host

splitter

   
         

url
set

   
   
       
   
   
      

dup
url
elim

   

url frontier

   

from
other
nodes

25 / 36

url frontier: two main considerations

politeness: don   t hit a web server too frequently

e.g., insert a time gap between successive requests to the
same server

freshness: crawl some pages (e.g., news sites) more often
than others

not an easy problem: simple priority queue fails.

26 / 36

mercator url frontier

   

prioritizer

f front queues

1

                     

                     
   
   
                  
                  
                     
                     
   
                     

   
                     

1

f

b

   

ppppppq
   
                  
                     
   

   
   
                     
      

b back queues:

single host on each

b. queue selector

heap

f. queue selector & b. queue router

urls    ow in from the top
into the frontier.

front queues manage
prioritization.

back queues enforce
politeness.

each queue is fifo.

   

27 / 36

mercator url frontier: front queues

   

prioritizer

1

                        

                        
q
                        

q
                        

f front queues

q

f

pppppppq
q
                        

f. queue selector & b. queue router

prioritizer assigns
to url an integer
priority between 1
and f .

then appends url
to corresponding
queue

heuristics for
assigning priority:
refresh rate,
id95 etc

selection from front
queues is initiated
by back queues

pick a front queue

28 / 36

mercator url frontier: back queues

f. queue selector & b. queue router

                           
q

q

b back queues

single host on each

1

                           

                           
q
                        

q
                        

b

b. queue selector

   

                        

   

   

heap

29 / 36

mercator url frontier: back queues

f. queue selector & b. queue router

                           
q

q

b back queues

single host on each

1

                           

                           
q
                        

q
                        

                        

   

   

heap

b

b. queue selector

   

invariant 1. each
back queue is kept
non-empty while the
crawl is in progress.

invariant 2. each
back queue only
contains urls from a
single host.

maintain a table from
hosts to back queues.

30 / 36

mercator url frontier: back queues

f. queue selector & b. queue router

                           
q

q

b back queues

single host on each

1

                           

                           
q
                        

q
                        

                        

   

   

heap

b. queue selector

   

in the heap:

b

one entry for each
back queue

the entry is the
earliest time te at
which the host
corresponding to the
back queue can be
hit again.

the earliest time te is
determined by (i) last
access to that host
(ii) time gap heuristic

31 / 36

mercator url frontier: back queues

f. queue selector & b. queue router

                           
q

q

b back queues

single host on each

1

                           

                           
q
                        

q
                        

                        

   

   

heap

b

b. queue selector

   

how fetcher interacts
with back queue:

repeat (i) extract
current root q of the
heap (q is a back
queue)

and (ii) fetch url u
at head of q . . .

. . . until we empty the
q we get.

(i.e.: u was the last
url in q)

32 / 36

mercator url frontier: back queues

f. queue selector & b. queue router

                           
q

q

b back queues

single host on each

1

                           

                           
q
                        

q
                        

                        

   

   

heap

b. queue selector

   

when we have
emptied a back queue
q:

b

repeat (i) pull urls
u from front queues
and (ii) add u to its
corresponding back
queue . . .

. . . until we get a u
whose host does not
have a back queue.

then put u in q and
create heap entry for
it.

33 / 36

mercator url frontier

   

prioritizer

f front queues

1

                     

                     
   
   
                  
                  
                     
                     
   
                     

   
                     

1

f

b

   

ppppppq
   
                  
                     
   

   
   
                     
      

b back queues:

single host on each

b. queue selector

heap

f. queue selector & b. queue router

urls    ow in from the top
into the frontier.

front queues manage
prioritization.

back queues enforce
politeness.

each queue is fifo.

   

34 / 36

spider trap

malicious server that generates an in   nite sequence of linked
pages

sophisticated spider traps generate pages that are not easily
identi   ed as dynamic.

35 / 36

resources

chapter 20 of iir
resources at http://cislmu.org

paper on mercator by heydon et al.
robot exclusion standard

36 / 36

