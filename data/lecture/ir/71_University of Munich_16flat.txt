recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

introduction to information retrieval

http://informationretrieval.org

iir 16: flat id91

hinrich sch  utze

center for information and language processing, university of munich

2014-06-11

sch  utze: flat id91

1 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

overview

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

2 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

3 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

learning to rank for zone scoring

given query q and document d, weighted zone scoring assigns to
the pair (q, d) a score in the interval [0,1] by computing a linear
combination of document zone scores, where each zone contributes
a value.

consider a set of documents, which have l zones

let g1, ..., gl     [0, 1], such that pl

for 1     i     l , let si be the boolean score denoting a match
(or non-match) between q and the i th zone

i =1 gi = 1

si = 1 if a query term occurs in zone i, 0 otherwise

weighted zone scoring aka ranked boolean retrieval

rank documents according to pl

i =1 gi si

learning to rank approach: learn the weights gi from training data

sch  utze: flat id91

4 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

training set for learning to rank

  j
  1
  2
  3
  4
  5
  6
  7

dj
37
37
238
238
1741
2094
3194

qj
linux
penguin
system
penguin
kernel
driver
driver

st
1
0
0
0
1
0
1

r (dj , qj )
sb
1 relevant
1 nonrelevant
1 relevant
0 nonrelevant
1 relevant
1 relevant
0 nonrelevant

sch  utze: flat id91

5 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

summary of learning to rank approach

the problem of making a binary relevant/nonrelevant
judgment is cast as a classi   cation or regression problem,
based on a training set of query-document pairs and
associated relevance judgments.

in principle, any method learning a classi   er (including least
squares regression) can be used to    nd this line.

big advantage of learning to rank: we can avoid hand-tuning
scoring functions and simply learn them from training data.

bottleneck of learning to rank: the cost of maintaining a
representative set of training examples whose relevance
assessments must be made by humans.

sch  utze: flat id91

6 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

ltr features used by microsoft research (1)

zones: body, anchor, title, url, whole document

features derived from standard ir models: query term
number, query term ratio, length, idf, sum of term frequency,
min of term frequency, max of term frequency, mean of term
frequency, variance of term frequency, sum of length
normalized term frequency, min of length normalized term
frequency, max of length normalized term frequency, mean of
length normalized term frequency, variance of length
normalized term frequency, sum of tf-idf, min of tf-idf, max of
tf-idf, mean of tf-idf, variance of tf-idf, boolean model, bm25

sch  utze: flat id91

7 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

ltr features used by microsoft research (2)

language model features: lmir.abs, lmir.dir, lmir.jm

web-speci   c features: number of slashes in url, length of url,
inlink number, outlink number, id95, siterank

spam features: qualityscore

usage-based features: query-url click count, url click count,
url dwell time

sch  utze: flat id91

8 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

ranking id166s

vector of feature di   erences:   (di , dj , q) =   (di , q)       (dj , q)
by hypothesis, one of di and dj has been judged more
relevant.
notation: we write di     dj for    di precedes dj in the results
ordering   .
if di is judged more relevant than dj , then we will assign the
vector   (di , dj , q) the class yijq = +1; otherwise    1.
this gives us a training set of pairs of vectors and
   precedence indicators   . each of the vectors is computed as
the di   erence of two document-query vectors.
we can then train an id166 on this training set with the goal
of obtaining a classi   er that returns

~w t  (di , dj , q) > 0 i    di     dj

sch  utze: flat id91

9 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

sch  utze: flat id91

10 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

what is id91?

sch  utze: flat id91

10 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

what is id91?

applications of id91 in information retrieval

sch  utze: flat id91

10 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

what is id91?

applications of id91 in information retrieval

k -means algorithm

sch  utze: flat id91

10 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

what is id91?

applications of id91 in information retrieval

k -means algorithm

evaluation of id91

sch  utze: flat id91

10 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

what is id91?

applications of id91 in information retrieval

k -means algorithm

evaluation of id91

how many clusters?

sch  utze: flat id91

10 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

11 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91: de   nition

sch  utze: flat id91

12 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91: de   nition

(document) id91 is the process of grouping a set of
documents into clusters of similar documents.

sch  utze: flat id91

12 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91: de   nition

(document) id91 is the process of grouping a set of
documents into clusters of similar documents.

documents within a cluster should be similar.

sch  utze: flat id91

12 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91: de   nition

(document) id91 is the process of grouping a set of
documents into clusters of similar documents.

documents within a cluster should be similar.

documents from di   erent clusters should be dissimilar.

sch  utze: flat id91

12 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91: de   nition

(document) id91 is the process of grouping a set of
documents into clusters of similar documents.

documents within a cluster should be similar.

documents from di   erent clusters should be dissimilar.

id91 is the most common form of unsupervised learning.

sch  utze: flat id91

12 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91: de   nition

(document) id91 is the process of grouping a set of
documents into clusters of similar documents.

documents within a cluster should be similar.

documents from di   erent clusters should be dissimilar.

id91 is the most common form of unsupervised learning.

unsupervised = there are no labeled or annotated data.

sch  utze: flat id91

12 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

data set with clear cluster structure

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0

.

0

0.0

0.5

1.0

1.5

2.0

sch  utze: flat id91

13 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

classi   cation vs. id91

sch  utze: flat id91

14 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

classi   cation vs. id91

classi   cation: supervised learning

sch  utze: flat id91

14 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

classi   cation vs. id91

classi   cation: supervised learning

id91: unsupervised learning

sch  utze: flat id91

14 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

classi   cation vs. id91

classi   cation: supervised learning

id91: unsupervised learning

classi   cation: classes are human-de   ned and part of the
input to the learning algorithm.

sch  utze: flat id91

14 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

classi   cation vs. id91

classi   cation: supervised learning

id91: unsupervised learning

classi   cation: classes are human-de   ned and part of the
input to the learning algorithm.
id91: clusters are inferred from the data without human
input.

sch  utze: flat id91

14 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

classi   cation vs. id91

classi   cation: supervised learning

id91: unsupervised learning

classi   cation: classes are human-de   ned and part of the
input to the learning algorithm.
id91: clusters are inferred from the data without human
input.

however, there are many ways of in   uencing the outcome of
id91: number of clusters, similarity measure,
representation of documents, . . .

sch  utze: flat id91

14 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

15 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

the cluster hypothesis

sch  utze: flat id91

16 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

the cluster hypothesis

cluster hypothesis. documents in the same cluster behave
similarly with respect to relevance to information needs.

sch  utze: flat id91

16 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

the cluster hypothesis

cluster hypothesis. documents in the same cluster behave
similarly with respect to relevance to information needs.

all applications of id91 in ir are based (directly or indirectly)
on the cluster hypothesis.

sch  utze: flat id91

16 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

the cluster hypothesis

cluster hypothesis. documents in the same cluster behave
similarly with respect to relevance to information needs.

all applications of id91 in ir are based (directly or indirectly)
on the cluster hypothesis.

van rijsbergen   s original wording (1979):    closely associated
documents tend to be relevant to the same requests   .

sch  utze: flat id91

16 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

applications of id91 in ir

sch  utze: flat id91

17 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

applications of id91 in ir

application

search result id91

scatter-gather

what is
clustered?
search
results

(subsets of)
collection

collection id91

collection

cluster-based retrieval

collection

bene   t

more e   ective infor-
mation
presentation
to user
alternative user inter-
face:    search without
typing   
e   ective
information
presentation for ex-
ploratory browsing
higher
faster search

e   ciency:

sch  utze: flat id91

17 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

search result id91 for better navigation

sch  utze: flat id91

18 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

search result id91 for better navigation

sch  utze: flat id91

18 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

scatter-gather

sch  utze: flat id91

19 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

scatter-gather

sch  utze: flat id91

19 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation: yahoo

sch  utze: flat id91

20 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation: yahoo

sch  utze: flat id91

20 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation: mesh (upper level)

sch  utze: flat id91

21 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation: mesh (upper level)

sch  utze: flat id91

21 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation: mesh (lower level)

sch  utze: flat id91

22 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation: mesh (lower level)

sch  utze: flat id91

22 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

but they are well known examples for using a global hierarchy
for navigation.

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

but they are well known examples for using a global hierarchy
for navigation.
some examples for global navigation/exploration based on
id91:

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

but they are well known examples for using a global hierarchy
for navigation.
some examples for global navigation/exploration based on
id91:
cartia

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

but they are well known examples for using a global hierarchy
for navigation.
some examples for global navigation/exploration based on
id91:
cartia
themescapes

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

navigational hierarchies: manual vs. automatic creation

note: yahoo/mesh are not examples of id91.

but they are well known examples for using a global hierarchy
for navigation.
some examples for global navigation/exploration based on
id91:
cartia
themescapes
google news

sch  utze: flat id91

23 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation combined with visualization (1)

sch  utze: flat id91

24 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation combined with visualization (1)

sch  utze: flat id91

24 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation combined with visualization (2)

sch  utze: flat id91

25 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global navigation combined with visualization (2)

sch  utze: flat id91

25 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global id91 for navigation: google news

sch  utze: flat id91

26 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

global id91 for navigation: google news

http://news.google.com

sch  utze: flat id91

26 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

to improve search recall:

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

to improve search recall:

cluster docs in collection a priori

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

to improve search recall:

cluster docs in collection a priori
when a query matches a doc d, also return other docs in the
cluster containing d

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

to improve search recall:

cluster docs in collection a priori
when a query matches a doc d, also return other docs in the
cluster containing d

hope: if we do this: the query    car    will also return docs
containing    automobile   

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

to improve search recall:

cluster docs in collection a priori
when a query matches a doc d, also return other docs in the
cluster containing d

hope: if we do this: the query    car    will also return docs
containing    automobile   

because the id91 algorithm groups together docs
containing    car    with those containing    automobile   .

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id91 for improving recall

to improve search recall:

cluster docs in collection a priori
when a query matches a doc d, also return other docs in the
cluster containing d

hope: if we do this: the query    car    will also return docs
containing    automobile   

because the id91 algorithm groups together docs
containing    car    with those containing    automobile   .
both types of documents contain words like    parts   ,    dealer   ,
   mercedes   ,    road trip   .

sch  utze: flat id91

27 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise: data set with clear cluster structure

propose
algorithm
for    nding
the cluster
structure in
this
example

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0

.

0

0.0

0.5

1.0

1.5

2.0

sch  utze: flat id91

28 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.
later: semiautomatic methods for determining k

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.
later: semiautomatic methods for determining k

secondary goals in id91

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.
later: semiautomatic methods for determining k

secondary goals in id91

avoid very small and very large clusters

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.
later: semiautomatic methods for determining k

secondary goals in id91

avoid very small and very large clusters
de   ne clusters that are easy to explain to the user

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

desiderata for id91

general goal: put related docs in the same cluster, put
unrelated docs in di   erent clusters.

we   ll see di   erent ways of formalizing this.

the number of clusters should be appropriate for the data set
we are id91.

initially, we will assume the number of clusters k is given.
later: semiautomatic methods for determining k

secondary goals in id91

avoid very small and very large clusters
de   ne clusters that are easy to explain to the user
many others . . .

sch  utze: flat id91

29 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively
main algorithm: k -means

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively
main algorithm: k -means

hierarchical algorithms

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively
main algorithm: k -means

hierarchical algorithms

create a hierarchy

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively
main algorithm: k -means

hierarchical algorithms

create a hierarchy
bottom-up, agglomerative

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat vs. hierarchical id91

flat algorithms

usually start with a random (partial) partitioning of docs into
groups
re   ne iteratively
main algorithm: k -means

hierarchical algorithms

create a hierarchy
bottom-up, agglomerative
top-down, divisive

sch  utze: flat id91

30 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel
shoes

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel
shoes

you can only do that with a soft id91 approach.

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel
shoes

you can only do that with a soft id91 approach.

this class:    at, hard id91

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel
shoes

you can only do that with a soft id91 approach.

this class:    at, hard id91

next time: hierarchical, hard id91

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

hard vs. soft id91

hard id91: each document belongs to exactly one
cluster.

more common and easier to do

soft id91: a document can belong to more than one
cluster.

makes more sense for applications like creating browsable
hierarchies
you may want to put sneakers in two clusters:

sports apparel
shoes

you can only do that with a soft id91 approach.

this class:    at, hard id91

next time: hierarchical, hard id91

next week: id45, a form of soft
id91

sch  utze: flat id91

31 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

given: a set of documents and the number k

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

given: a set of documents and the number k

find: a partition into k clusters that optimizes the chosen
partitioning criterion

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

given: a set of documents and the number k

find: a partition into k clusters that optimizes the chosen
partitioning criterion
global optimization: exhaustively enumerate partitions, pick
optimal one

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

given: a set of documents and the number k

find: a partition into k clusters that optimizes the chosen
partitioning criterion
global optimization: exhaustively enumerate partitions, pick
optimal one

not tractable

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

flat algorithms

flat algorithms compute a partition of n documents into a
set of k clusters.

given: a set of documents and the number k

find: a partition into k clusters that optimizes the chosen
partitioning criterion
global optimization: exhaustively enumerate partitions, pick
optimal one

not tractable

e   ective heuristic method: k -means algorithm

sch  utze: flat id91

32 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

33 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means

sch  utze: flat id91

34 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means

perhaps the best known id91 algorithm

sch  utze: flat id91

34 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means

perhaps the best known id91 algorithm

simple, works well in many cases

sch  utze: flat id91

34 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means

perhaps the best known id91 algorithm

simple, works well in many cases

use as default / baseline for id91 documents

sch  utze: flat id91

34 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id194s in id91

sch  utze: flat id91

35 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id194s in id91

vector space model

sch  utze: flat id91

35 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id194s in id91

vector space model

as in vector space classi   cation, we measure relatedness
between vectors by euclidean distance . . .

sch  utze: flat id91

35 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id194s in id91

vector space model

as in vector space classi   cation, we measure relatedness
between vectors by euclidean distance . . .

. . . which is almost equivalent to cosine similarity.

sch  utze: flat id91

35 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

id194s in id91

vector space model

as in vector space classi   cation, we measure relatedness
between vectors by euclidean distance . . .

. . . which is almost equivalent to cosine similarity.

almost: centroids are not length-normalized.

sch  utze: flat id91

35 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

objective/partitioning criterion: minimize the average squared
di   erence from the centroid

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

objective/partitioning criterion: minimize the average squared
di   erence from the centroid

recall de   nition of centroid:

~  (  ) =

1

|  | x~x     

~x

where we use    to denote a cluster.

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

objective/partitioning criterion: minimize the average squared
di   erence from the centroid

recall de   nition of centroid:

~  (  ) =

1

|  | x~x     

~x

where we use    to denote a cluster.
we try to    nd the minimum average squared di   erence by
iterating two steps:

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

objective/partitioning criterion: minimize the average squared
di   erence from the centroid

recall de   nition of centroid:

~  (  ) =

1

|  | x~x     

~x

where we use    to denote a cluster.
we try to    nd the minimum average squared di   erence by
iterating two steps:

reassignment: assign each vector to its closest centroid

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means: basic idea

each cluster in k -means is de   ned by a centroid.

objective/partitioning criterion: minimize the average squared
di   erence from the centroid

recall de   nition of centroid:

~  (  ) =

1

|  | x~x     

~x

where we use    to denote a cluster.
we try to    nd the minimum average squared di   erence by
iterating two steps:

reassignment: assign each vector to its closest centroid
recomputation: recompute each centroid as the average of the
vectors that were assigned to it in reassignment

sch  utze: flat id91

36 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means pseudocode (  k is centroid of   k)

sch  utze: flat id91

37 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means pseudocode (  k is centroid of   k)

k -means({~x1, . . . , ~xn }, k )

1 (~s1,~s2, . . . ,~sk )     selectrandomseeds({~x1, . . . , ~xn }, k )
2 for k     1 to k
3 do ~  k     ~sk
4 while stopping criterion has not been met
5 do for k     1 to k
6
7
8
9
10
11
12 return {~  1, . . . , ~  k }

do   k     {}
for n     1 to n
do j     arg minj     |~  j         ~xn|

  j       j     {~xn} (reassignment of vectors)

for k     1 to k
do ~  k     1

~x (recomputation of centroids)

|  k | p~x     k

sch  utze: flat id91

37 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: set of points to be clustered

sch  utze: flat id91

38 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example

exercise: (i) guess what the optimal id91 into two clusters is
in this case; (ii) compute the centroids of the clusters

sch  utze: flat id91

38 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: random selection of initial centroids

sch  utze: flat id91

39 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest center

sch  utze: flat id91

40 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

1

1

2 2
2
1
1

1
1
1

1

1

2

1

1

1

1

1

1

sch  utze: flat id91

41 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

1

1

2 2
2
1
1

1
1
1

1

1

2

1

1

1

1

1

1

sch  utze: flat id91

42 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest centroid

sch  utze: flat id91

43 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

1

1

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

sch  utze: flat id91

44 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

1

1

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

sch  utze: flat id91

45 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest centroid

sch  utze: flat id91

46 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

1

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

sch  utze: flat id91

47 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

1

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

sch  utze: flat id91

48 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest centroid

sch  utze: flat id91

49 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

2

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

sch  utze: flat id91

50 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

2

2

2 2
2
1
1

1
1
1

1

1

2

2

2

1

1

1

1

sch  utze: flat id91

51 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest centroid

sch  utze: flat id91

52 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

2

2

2 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

sch  utze: flat id91

53 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

2

2

2 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

sch  utze: flat id91

54 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest centroid

sch  utze: flat id91

55 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

sch  utze: flat id91

56 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

2

2

1

1

1

sch  utze: flat id91

57 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assign points to closest centroid

sch  utze: flat id91

58 / 86

b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: assignment

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

1

2

1

1

1

sch  utze: flat id91

59 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked example: recompute cluster centroids

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

1

2

1

1

1

sch  utze: flat id91

60 / 86

  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

worked ex.: centroids and assignments after convergence

2

2

2

1 1
1
1
1

1
1
1

1

1

2

2

1

2

1

1

1

sch  utze: flat id91

61 / 86

  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

see next slide

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

see next slide

there is only a    nite number of id91s.

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

see next slide

there is only a    nite number of id91s.

thus: we must reach a    xed point.

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

see next slide

there is only a    nite number of id91s.

thus: we must reach a    xed point.

assumption: ties are broken consistently.

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge: proof

rss = sum of all squared distances between document vector
and closest centroid
rss decreases during each reassignment step.

because each vector is moved to a closer centroid

rss decreases during each recomputation step.

see next slide

there is only a    nite number of id91s.

thus: we must reach a    xed point.

assumption: ties are broken consistently.

finite set & monotonically decreasing     convergence

sch  utze: flat id91

62 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

recomputation decreases average distance

sch  utze: flat id91

63 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

recomputation decreases average distance

rss = pk

measure)

k=1 rssk     the residual sum of squares (the    goodness   

rssk (~v ) = x~x     k
= x~x     k

   vm

   rssk (~v )

m

k~v     ~xk2 = x~x     k

(vm     xm)2

xm=1

2(vm     xm) = 0

vm =

1

|  k | x~x     k

xm

the last line is the componentwise de   nition of the centroid!

sch  utze: flat id91

63 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

recomputation decreases average distance

rss = pk

measure)

k=1 rssk     the residual sum of squares (the    goodness   

rssk (~v ) = x~x     k
= x~x     k

   vm

   rssk (~v )

m

k~v     ~xk2 = x~x     k

(vm     xm)2

xm=1

2(vm     xm) = 0

vm =

1

|  k | x~x     k

xm

the last line is the componentwise de   nition of the centroid!
we minimize rssk when the old centroid is replaced with the new
centroid.

sch  utze: flat id91

63 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

recomputation decreases average distance

rss = pk

measure)

k=1 rssk     the residual sum of squares (the    goodness   

rssk (~v ) = x~x     k
= x~x     k

   vm

   rssk (~v )

m

k~v     ~xk2 = x~x     k

(vm     xm)2

xm=1

2(vm     xm) = 0

vm =

1

|  k | x~x     k

xm

the last line is the componentwise de   nition of the centroid!
we minimize rssk when the old centroid is replaced with the new
centroid. rss, the sum of the rssk , must then also decrease during
recomputation.

sch  utze: flat id91

63 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge

sch  utze: flat id91

64 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge

but we don   t know how long convergence will take!

sch  utze: flat id91

64 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge

but we don   t know how long convergence will take!

if we don   t care about a few docs switching back and forth,
then convergence is usually fast (< 10-20 iterations).

sch  utze: flat id91

64 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

k -means is guaranteed to converge

but we don   t know how long convergence will take!

if we don   t care about a few docs switching back and forth,
then convergence is usually fast (< 10-20 iterations).

however, complete convergence can take many more
iterations.

sch  utze: flat id91

64 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

optimality of k -means

convergence 6= optimality

sch  utze: flat id91

65 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

optimality of k -means

convergence 6= optimality

convergence does not mean that we converge to the optimal
id91!

sch  utze: flat id91

65 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

optimality of k -means

convergence 6= optimality

convergence does not mean that we converge to the optimal
id91!

this is the great weakness of k -means.

sch  utze: flat id91

65 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

optimality of k -means

convergence 6= optimality

convergence does not mean that we converge to the optimal
id91!

this is the great weakness of k -means.

if we start with a bad set of seeds, the resulting id91 can
be horrible.

sch  utze: flat id91

65 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise: suboptimal id91

sch  utze: flat id91

66 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise: suboptimal id91

3

2

1

0

d1

d2

d4

d5

0

1

2

3

d3

d6

4

what is the optimal id91 for k = 2?

do we converge on this id91 for arbitrary seeds
di , dj ?

sch  utze: flat id91

66 / 86

  
  
  
  
  
  
recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.
better ways of computing initial centroids:

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.
better ways of computing initial centroids:

select seeds not randomly, but using some heuristic (e.g.,    lter
out outliers or    nd a set of seeds that has    good coverage    of
the document space)

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.
better ways of computing initial centroids:

select seeds not randomly, but using some heuristic (e.g.,    lter
out outliers or    nd a set of seeds that has    good coverage    of
the document space)
use hierarchical id91 to    nd good seeds

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

initialization of k -means

random seed selection is just one of many ways k -means can
be initialized.

random seed selection is not very robust: it   s easy to get a
suboptimal id91.
better ways of computing initial centroids:

select seeds not randomly, but using some heuristic (e.g.,    lter
out outliers or    nd a set of seeds that has    good coverage    of
the document space)
use hierarchical id91 to    nd good seeds
select i (e.g., i = 10) di   erent random sets of seeds, do a
k -means id91 for each, select the id91 with lowest
rss

sch  utze: flat id91

67 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

recomputation step: o(nm) (we need to add each of the
document   s < m values to one of the centroids)

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

recomputation step: o(nm) (we need to add each of the
document   s < m values to one of the centroids)

assume number of iterations bounded by i

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

recomputation step: o(nm) (we need to add each of the
document   s < m values to one of the centroids)

assume number of iterations bounded by i

overall complexity: o(iknm)     linear in all important
dimensions

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

recomputation step: o(nm) (we need to add each of the
document   s < m values to one of the centroids)

assume number of iterations bounded by i

overall complexity: o(iknm)     linear in all important
dimensions

however: this is not a real worst-case analysis.

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

time complexity of k -means

computing one distance of two vectors is o(m).

reassignment step: o(knm) (we need to compute kn
document-centroid distances)

recomputation step: o(nm) (we need to add each of the
document   s < m values to one of the centroids)

assume number of iterations bounded by i

overall complexity: o(iknm)     linear in all important
dimensions

however: this is not a real worst-case analysis.

in pathological cases, complexity can be worse than linear.

sch  utze: flat id91

68 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

69 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

what is a good id91?

sch  utze: flat id91

70 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

what is a good id91?

internal criteria

sch  utze: flat id91

70 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

what is a good id91?

internal criteria

example of an internal criterion: rss in k -means

sch  utze: flat id91

70 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

what is a good id91?

internal criteria

example of an internal criterion: rss in k -means

but an internal criterion often does not evaluate the actual
utility of a id91 in the application.

sch  utze: flat id91

70 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

what is a good id91?

internal criteria

example of an internal criterion: rss in k -means

but an internal criterion often does not evaluate the actual
utility of a id91 in the application.
alternative: external criteria

sch  utze: flat id91

70 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

what is a good id91?

internal criteria

example of an internal criterion: rss in k -means

but an internal criterion often does not evaluate the actual
utility of a id91 in the application.
alternative: external criteria

evaluate with respect to a human-de   ned classi   cation

sch  utze: flat id91

70 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criteria for id91 quality

sch  utze: flat id91

71 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criteria for id91 quality

based on a gold standard data set, e.g., the reuters collection
we also used for the evaluation of classi   cation

sch  utze: flat id91

71 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criteria for id91 quality

based on a gold standard data set, e.g., the reuters collection
we also used for the evaluation of classi   cation

goal: id91 should reproduce the classes in the gold
standard

sch  utze: flat id91

71 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criteria for id91 quality

based on a gold standard data set, e.g., the reuters collection
we also used for the evaluation of classi   cation

goal: id91 should reproduce the classes in the gold
standard

(but we only want to reproduce how documents are divided
into groups, not the class labels.)

sch  utze: flat id91

71 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criteria for id91 quality

based on a gold standard data set, e.g., the reuters collection
we also used for the evaluation of classi   cation

goal: id91 should reproduce the classes in the gold
standard

(but we only want to reproduce how documents are divided
into groups, not the class labels.)

first measure for how well we were able to reproduce the
classes: purity

sch  utze: flat id91

71 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criterion: purity

sch  utze: flat id91

72 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criterion: purity

purity(   , c ) =

1

n xk

max

j

|  k     cj |

    = {  1,   2, . . . ,   k } is the set of clusters and
c = {c1, c2, . . . , cj } is the set of classes.

sch  utze: flat id91

72 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criterion: purity

purity(   , c ) =

1

n xk

max

j

|  k     cj |

    = {  1,   2, . . . ,   k } is the set of clusters and
c = {c1, c2, . . . , cj } is the set of classes.
for each cluster   k :    nd class cj with most members nkj in   k

sch  utze: flat id91

72 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

external criterion: purity

purity(   , c ) =

1

n xk

max

j

|  k     cj |

    = {  1,   2, . . . ,   k } is the set of clusters and
c = {c1, c2, . . . , cj } is the set of classes.
for each cluster   k :    nd class cj with most members nkj in   k
sum all nkj and divide by total number of points

sch  utze: flat id91

72 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

example for computing purity

sch  utze: flat id91

73 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

example for computing purity

cluster 1

cluster 2

cluster 3

x

x
x

o

x x

x

o

o
o    

o

x

   
       

x

to compute purity: 5 = maxj |  1     cj | (class x, cluster 1); 4 =
maxj |  2     cj | (class o, cluster 2); and 3 = maxj |  3     cj | (class    ,
cluster 3). purity is (1/17)    (5 + 4 + 3)     0.71.

sch  utze: flat id91

73 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

tp+fn+fp+tn is the total number of pairs.

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

tp+fn+fp+tn is the total number of pairs.

tp+fn+fp+tn = (cid:0)n

2(cid:1) for n documents.

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

tp+fn+fp+tn is the total number of pairs.

tp+fn+fp+tn = (cid:0)n
example: (cid:0)17

2(cid:1) = 136 in o/   /x example

2(cid:1) for n documents.

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

tp+fn+fp+tn is the total number of pairs.

tp+fn+fp+tn = (cid:0)n
example: (cid:0)17

2(cid:1) = 136 in o/   /x example

2(cid:1) for n documents.

each pair is either positive or negative (the id91 puts the
two documents in the same or in di   erent clusters) . . .

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

another external criterion: rand index

purity can be increased easily by increasing k     a measure
that does not have this problem: rand index.

de   nition: ri =

tp+tn

tp+fp+fn+tn

based on 2x2 contingency table of all pairs of documents:

same cluster

di   erent clusters

same class
di   erent classes

true positives (tp)
false positives (fp)

false negatives (fn)
true negatives (tn)

tp+fn+fp+tn is the total number of pairs.

tp+fn+fp+tn = (cid:0)n
example: (cid:0)17

2(cid:1) = 136 in o/   /x example

2(cid:1) for n documents.

each pair is either positive or negative (the id91 puts the
two documents in the same or in di   erent clusters) . . .
. . . and either    true    (correct) or    false    (incorrect): the
id91 decision is correct or incorrect.

sch  utze: flat id91

74 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

rand index: example

sch  utze: flat id91

75 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

rand index: example

as an example, we compute ri for the o/   /x example. we    rst
compute tp + fp. the three clusters contain 6, 6, and 5 points,
respectively, so the total number of    positives    or pairs of
documents that are in the same cluster is:

tp + fp = (cid:18) 6

2 (cid:19) +(cid:18) 6

2 (cid:19) +(cid:18) 5

2 (cid:19) = 40

of these, the x pairs in cluster 1, the o pairs in cluster 2, the    
pairs in cluster 3, and the x pair in cluster 3 are true positives:

tp = (cid:18) 5

2 (cid:19) +(cid:18) 4

2 (cid:19) +(cid:18) 3

2 (cid:19) +(cid:18) 2

2 (cid:19) = 20

thus, fp = 40     20 = 20.
fn and tn are computed similarly.

sch  utze: flat id91

75 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

rand measure for the o/   /x example

sch  utze: flat id91

76 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

rand measure for the o/   /x example

same cluster

di   erent clusters

same class
di   erent classes

tp = 20
fp = 20

fn = 24
tn = 72

ri is then (20 + 72)/(20 + 20 + 24 + 72)     0.68.

sch  utze: flat id91

76 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures
normalized mutual information (nmi)

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures
normalized mutual information (nmi)

how much information does the id91 contain about the
classi   cation?

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures
normalized mutual information (nmi)

how much information does the id91 contain about the
classi   cation?
singleton clusters (number of clusters = number of docs) have
maximum mi

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures
normalized mutual information (nmi)

how much information does the id91 contain about the
classi   cation?
singleton clusters (number of clusters = number of docs) have
maximum mi
therefore: normalize by id178 of clusters and classes

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures
normalized mutual information (nmi)

how much information does the id91 contain about the
classi   cation?
singleton clusters (number of clusters = number of docs) have
maximum mi
therefore: normalize by id178 of clusters and classes

f measure

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

two other external evaluation measures

two other measures
normalized mutual information (nmi)

how much information does the id91 contain about the
classi   cation?
singleton clusters (number of clusters = number of docs) have
maximum mi
therefore: normalize by id178 of clusters and classes

f measure

like rand, but    precision    and    recall    can be weighted

sch  utze: flat id91

77 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

evaluation results for the o/   /x example

sch  utze: flat id91

78 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

evaluation results for the o/   /x example

lower bound
maximum
value for example

purity nmi ri
0.0
0.0
1.0
1.0
0.71
0.68

0.0
1.0
0.36

f5
0.0
1.0
0.46

all four measures range from 0 (really bad id91) to 1 (perfect
id91).

sch  utze: flat id91

78 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

outline

1 recap

2 id91: introduction

3 id91 in ir

4 k -means

5 evaluation

6 how many clusters?

sch  utze: flat id91

79 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

what if there is no external constraint? is there a    right   
number of clusters?

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

what if there is no external constraint? is there a    right   
number of clusters?
one way to go: de   ne an optimization criterion

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

what if there is no external constraint? is there a    right   
number of clusters?
one way to go: de   ne an optimization criterion

given docs,    nd k for which the optimum is reached.

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

what if there is no external constraint? is there a    right   
number of clusters?
one way to go: de   ne an optimization criterion

given docs,    nd k for which the optimum is reached.
what optimization criterion can we use?

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

how many clusters?

number of clusters k is given in many applications.

e.g., there may be an external constraint on k . example: in
the case of scatter-gather, it was hard to show more than
10   20 clusters on a monitor in the 90s.

what if there is no external constraint? is there a    right   
number of clusters?
one way to go: de   ne an optimization criterion

given docs,    nd k for which the optimum is reached.
what optimization criterion can we use?
we can   t use rss or average squared distance from centroid
as criterion: always chooses k = n clusters.

sch  utze: flat id91

80 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise

sch  utze: flat id91

81 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise

your job is to develop the id91 algorithms for a
competitor to news.google.com

sch  utze: flat id91

81 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise

your job is to develop the id91 algorithms for a
competitor to news.google.com

you want to use k -means id91.

sch  utze: flat id91

81 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

exercise

your job is to develop the id91 algorithms for a
competitor to news.google.com

you want to use k -means id91.

how would you determine k ?

sch  utze: flat id91

81 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : basic idea

sch  utze: flat id91

82 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : basic idea

start with 1 cluster (k = 1)

sch  utze: flat id91

82 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : basic idea

start with 1 cluster (k = 1)

keep adding clusters (= keep increasing k )

sch  utze: flat id91

82 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : basic idea

start with 1 cluster (k = 1)

keep adding clusters (= keep increasing k )

add a penalty for each new cluster

sch  utze: flat id91

82 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : basic idea

start with 1 cluster (k = 1)

keep adding clusters (= keep increasing k )

add a penalty for each new cluster

then trade o    cluster penalties against average squared
distance from centroid

sch  utze: flat id91

82 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : basic idea

start with 1 cluster (k = 1)

keep adding clusters (= keep increasing k )

add a penalty for each new cluster

then trade o    cluster penalties against average squared
distance from centroid

choose the value of k with the best tradeo   

sch  utze: flat id91

82 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

then: penalize each cluster with a cost   

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

then: penalize each cluster with a cost   

thus for a id91 with k clusters, total cluster penalty is
k   

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

then: penalize each cluster with a cost   

thus for a id91 with k clusters, total cluster penalty is
k   

de   ne the total cost of a id91 as distortion plus total
cluster penalty: rss(k) + k   

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

then: penalize each cluster with a cost   

thus for a id91 with k clusters, total cluster penalty is
k   

de   ne the total cost of a id91 as distortion plus total
cluster penalty: rss(k) + k   

select k that minimizes (rss(k) + k   )

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

simple objective function for k : formalization

given a id91, de   ne the cost for a document as
(squared) distance to centroid

de   ne total distortion rss(k) as sum of all individual
document costs (corresponds to average distance)

then: penalize each cluster with a cost   

thus for a id91 with k clusters, total cluster penalty is
k   

de   ne the total cost of a id91 as distortion plus total
cluster penalty: rss(k) + k   

select k that minimizes (rss(k) + k   )

still need to determine good value for    . . .

sch  utze: flat id91

83 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

finding the    knee    in the curve

sch  utze: flat id91

84 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

finding the    knee    in the curve

s
e
r
a
u
q
s
 
f

 

o
m
u
s
 
l

a
u
d
s
e
r

i

0
5
9
1

0
0
9
1

0
5
8
1

0
0
8
1

0
5
7
1

2

4

6

8

10

number of clusters

pick the number of clusters where curve       attens   . here: 4 or
9.

sch  utze: flat id91

84 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

take-away today

what is id91?

applications of id91 in information retrieval

k -means algorithm

evaluation of id91

how many clusters?

sch  utze: flat id91

85 / 86

recap

id91: introduction

id91 in ir

k -means

evaluation

how many clusters?

resources

chapter 16 of iir
resources at http://cislmu.org

keith van rijsbergen on the cluster hypothesis (he was one of
the originators)
bing/carrot2/clusty: search result id91 systems
stirling number: the number of distinct k-id91s of n
items

sch  utze: flat id91

86 / 86

