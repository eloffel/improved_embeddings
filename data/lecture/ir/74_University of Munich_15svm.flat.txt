introduction to information retrieval

http://informationretrieval.org

iir 15-1: support vector machines

hinrich sch  utze

center for information and language processing, university of munich

2014-06-04

1 / 49

overview

1 recap

2 id166 intro

3 id166 details

4 classi   cation in the real world

2 / 49

outline

1 recap

2 id166 intro

3 id166 details

4 classi   cation in the real world

3 / 49

rocchio, a simple vector space classi   er

~  j     1

trainrocchio(c, d)
1 for each cj     c
2 do dj     {d : hd, cji     d}
|dj | pd   dj
~v (d)
3
4 return {~  1, . . . , ~  j}
applyrocchio({~  1, . . . , ~  j}, d)
1 return arg minj |~  j     ~v(d)|

4 / 49

a linear classi   er in 1d

a linear classi   er in 1d is
a point described by the
equation w1d1 =   
the point at   /w1
points (d1) with w1d1       
are in the class c.
points (d1) with w1d1 <   
are in the complement
class c.

5 / 49

a linear classi   er in 2d

a linear classi   er in 2d is
a line described by the
equation w1d1 + w2d2 =   
example for a 2d linear
classi   er
points (d1 d2) with
w1d1 + w2d2        are in
the class c.
points (d1 d2) with
w1d1 + w2d2 <    are in
the complement class c.

6 / 49

a linear classi   er in 3d

a linear classi   er in 3d is
a plane described by the
equation
w1d1 + w2d2 + w3d3 =   
example for a 3d linear
classi   er
points (d1 d2 d3) with
w1d1 + w2d2 + w3d3       
are in the class c.
points (d1 d2 d3) with
w1d1 + w2d2 + w3d3 <   
are in the complement
class c.

7 / 49

learning algorithms for vector space classi   cation

in terms of actual computation, there are two types of
learning algorithms.
(i) simple learning algorithms that estimate the parameters of
the classi   er directly from the training data, often in one
linear pass.

naive bayes, rocchio, knn are all examples of this.

(ii) iterative algorithms

support vector machines
id88 (example available as pdf on website:
http://cislmu.org)

the best performing learning algorithms usually require
iterative learning.

8 / 49

linear classi   ers: discussion

many common text classi   ers are linear classi   ers: naive
bayes, rocchio, id28, linear support vector
machines etc.
each method has a di   erent way of selecting the separating
hyperplane

huge di   erences in performance on test documents

can we get better performance with more powerful nonlinear
classi   ers?

not in general: a given amount of training data may su   ce
for estimating a linear boundary, but not for estimating a
more complex nonlinear boundary.

9 / 49

take-away today

support vector machines: state-of-the-art text classi   cation
methods (linear and nonlinear)

introduction to id166s

formalization

soft margin case for nonseparable problems

discussion: which classi   er should i use for my problem?

10 / 49

overview

1 recap

2 id166 intro

3 id166 details

4 classi   cation in the real world

11 / 49

outline

1 recap

2 id166 intro

3 id166 details

4 classi   cation in the real world

12 / 49

support vector machines

machine-learning research in the last two decades has
improved classi   er e   ectiveness.

new generation of state-of-the-art classi   ers: support vector
machines (id166s), boosted id90, regularized logistic
regression, maximum id178, neural networks, and random
forests

as we saw in iir: applications to ir problems, particularly
text classi   cation

13 / 49

what is a support vector machine        rst take

vector space classi   cation (similar to rocchio, knn, linear
classi   ers)

di   erence from previous methods: large margin classi   er

we aim to    nd a separating hyperplane (decision boundary)
that is maximally far from any point in the training data

in case of non-linear-separability: we may have to discount
some points as outliers or noise.

14 / 49

which hyperplane?

15 / 49

(linear) support vector machines

binary classi   cation
problem

decision boundary is
linear separator.

criterion: being maximally
far away from any data
point     determines
classi   er margin

vectors on margin lines
are called support vectors

set of support vectors are
a complete speci   cation
of classi   er

maximum
margin
decision
hyperplane

support vectors

margin is
maximized

16 / 49

b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
why maximize the margin?

points near the decision
surface are uncertain
classi   cation decisions.
a classi   er with a large
margin makes no low
certainty classi   cation
decisions (on the
training set).
gives classi   cation
safety margin with
respect to errors and
random variation

maximum
margin
decision
hyperplane

support vectors

margin is
maximized

17 / 49

b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
why maximize the margin?

id166 classi   cation = large
margin around decision
boundary

we can think of the margin
as a    fat separator        a
fatter version of our regular
decision hyperplane.

unique solution

increased ability to correctly
generalize to test data

18 / 49

separating hyperplane: recap

hyperplane
an n-dimensional generalization of a plane (point in 1-d space,
line in 2-d space, ordinary plane in 3-d space).

decision hyperplane
can be de   ned by:

intercept term b (we were calling this    before)
normal vector ~w (weight vector) which is perpendicular to the
hyperplane

all points ~x on the hyperplane satisfy:

~w t~x + b = 0

19 / 49

notation: di   erent conventions for linear separator

~w t~x + b = 0

used in id166 literature

~w t~x = 0

often used in id88 literature, folds threshold into vector
by adding a constant dimension (set to 1 or -1 for all vectors)

pm

i =1 wi di =   

   spelled out    version we used in the last chapter for linear
separators

20 / 49

exercise

3

2

1

0

0

1

2

3

draw the maximum margin

separator. which vectors are the support vectors? coordinates of
dots: (3,3), (-1,1). coordinates of triangle: (3,0)

21 / 49

b
b
u
t
outline

1 recap

2 id166 intro

3 id166 details

4 classi   cation in the real world

22 / 49

formalization of id166s

training set
consider a binary classi   cation problem:

~xi are the input vectors
yi are the labels

for id166s, the two classes are yi = +1 and yi =    1.
the linear classi   er is then:

f (~x) = sign(~w t~x + b)

a value of    1 indicates one class, and a value of +1 the other
class.

23 / 49

functional margin of a point

id166 makes its decision based on the score ~w t~x + b. clearly, the
larger |~w t~x + b| is, the more con   dence we can have that the
decision is correct.

functional margin

the functional margin of the vector ~xi w.r.t the hyperplane
h~w , bi is: yi (~w t~xi + b)
the functional margin of a data set w.r.t a decision surface is
twice the functional margin of any of the points in the data
set with minimal functional margin

factor 2 comes from measuring across the whole width of the
margin.

problem: we can increase functional margin by scaling ~w and b.
    we need to place some constraint on the size of ~w .

24 / 49

geometric margin

geometric margin of the classi   er: maximum width of the band
that can be drawn separating the support vectors of the two
classes. to compute the geometric margin, we need to compute
the distance of a vector ~x from the hyperplane:

r = y

~w t~x + b

|~w|

(why? we will see that this is so graphically in a few moments)
distance is of course invariant to scaling: if we replace ~w by 5~w
and b by 5b, then the distance is the same because it is normalized
by the length of ~w .

25 / 49

optimization problem solved by id166s

assume canonical    functional margin    distance assume that every
data point has at least distance 1 from the hyperplane, then:

yi (~w t~xi + b)     1

since each example   s distance from the hyperplane is
ri = yi (~w t~xi + b)/|~w|, the margin is    = 2/|~w|. we want to
maximize this margin. that is, we want to    nd ~w and b such that:

for all (~xi , yi )     d, yi (~w t~xi + b)     1
   = 2/|~w| is maximized

26 / 49

support vectors in red

maximum
margin
decision
hyperplane

0.5x + 0.5y     2 = 1
~w t~x + b = 1
distance of
margin is
0.5x + 0.5y     2 = 0
support vector
~w t~x + b = 0
maximized
from separator
0.5x + 0.5y     2 =    1
~w t~x + b =    1

projection of ~x onto ~w

weight vector ~w
support vector ~x

27 / 49

b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
b
b
b
b
b
b
b
b
b
u
t
u
t
u
t
u
t
u
t
u
t
u
t
~w t ~w     + b = 0
b =    ~w t ~w    
~w t ~w    
|~w|

b
|~w|

=    

distance of support vector from separator =

(length of projection of ~x onto ~w ) minus (length of ~w    )

~w t~x

|~w|    

~w t ~w    
|~w|
~w t~x
b
|~w|
|~w|
~w t~x + b

+

=

=

|~w|

28 / 49

distance of support vector from separator =
(length of projection of ~x = (1 5)t onto ~w ) minus (length of ~w    )

(0.5    1 + 0.5    5)/(1/   2)     (0.5    2 + 0.5    2)/(1/   2)

~w t~x

|~w|    

~w t ~w    
|~w|

3/(1/   2)     2/(1/   2)

~w t~x
|~w|

+

b
|~w|

3/(1/   2) + (   2)/(1/   2)

3     2
1/   2
   2

29 / 49

optimization problem solved by id166s (2)

maximizing 2/|~w| is the same as minimizing |~w|/2. this gives the
   nal standard formulation of an id166 as a minimization problem:

optimization problem solved by id166s
find ~w and b such that:

1

2 ~w t ~w is minimized (because |~w| =    ~w t ~w ), and
for all {(~xi , yi )}, yi (~w t~xi + b)     1

we are now optimizing a quadratic function subject to linear
constraints. quadratic optimization problems are standard
mathematical optimization problems, and many algorithms exist
for solving them (e.g. quadratic programming libraries).

30 / 49

recap

we start with a training set.

the data set de   nes the maximum-margin separating
hyperplane (if it is separable).

we use quadratic optimization to    nd this plane.

given a new point ~x to classify, the classi   cation function
f (~x) computes the functional margin of the point (=
normalized distance).

the sign of this function determines the class to assign to the
point.

if the point is within the margin of the classi   er, the classi   er
can return    don   t know    rather than one of the two classes.

the value of f (~x) may also be transformed into a id203
of classi   cation

31 / 49

exercise

3

2

1

0

0

1

2

3

which vectors are the support

vectors? draw the maximum margin separator. what values of w1,
w2 and b (for w1x + w2y + b = 0) describe this separator? recall
that we must have w1x + w2y + b     {1,   1} for the support
vectors.

32 / 49

b
b
u
t
walkthrough example

working geometrically:

the maximum margin weight vector
will be parallel to the shortest line
connecting points of the two classes,
that is, the line between (1, 1) and
(2, 3), giving a weight vector of (1, 2).

the optimal decision surface is
orthogonal to that line and intersects
it at the halfway point. therefore, it
passes through (1.5, 2).

the id166 decision boundary is:

3

2

1

0

0

1

2

3

b   b = (1  x+2  y )   (1  1.5+2  2)     0 =

2
5

x+

4
5

y   

11
5

33 / 49

b
b
u
t
walkthrough example

working algebraically:

with the constraint
sign(yi (~w t~xi + b))     1, we seek to
minimize |~w|.
we know that the solution is
~w = (a, 2a) for some a. so:
a + 2a + b =    1, 2a + 6a + b = 1
hence, a = 2/5 and b =    11/5. so
the optimal hyperplane is given by
~w = (2/5, 4/5) and b =    11/5.
the margin    is 2/|~w| =
2/p4/25 + 16/25 = 2/(2   5/5) =
   5 = p(1     2)2 + (1     3)2.

3

2

1

0

0

1

2

3

34 / 49

b
b
u
t
soft margin classi   cation

what happens if data is not linearly separable?

standard approach: allow the fat decision margin to make a
few mistakes

some points, outliers, noisy examples are inside or on the
wrong side of the margin

pay cost for each misclassi   ed example, depending on how far
it is from meeting the margin requirement

slack variable   i : a non-zero value for   i allows ~xi to not meet the
margin requirement at a cost proportional to the value of   i .
optimization problem: trading o    how fat it can make the margin
vs. how many points have to be moved around to allow this
margin. the sum of the   i gives an upper bound on the number of
training errors. soft-margin id166s minimize training error traded
o    against margin.

35 / 49

using id166 for one-of classi   cation

recall how to use binary linear classi   ers (k classes) for
one-of: train and run k classi   ers and then select the class
with the highest con   dence
another strategy used with id166s: build k(k     1)/2
one-versus-one classi   ers, and choose the class that is selected
by the most classi   ers. while this involves building a very
large number of classi   ers, the time for training classi   ers may
actually decrease, since the training data set for each classi   er
is much smaller.

yet another possibility: id170. generalization
of classi   cation where the classes are not just a set of
independent, categorical labels, but may be arbitrary
structured objects with relationships de   ned between them

36 / 49

outline

1 recap

2 id166 intro

3 id166 details

4 classi   cation in the real world

37 / 49

text classi   cation

many commercial applications

there are many applications of text classi   cation for corporate
intranets, government departments, and internet publishers.

often greater performance gains from exploiting
domain-speci   c text features than from changing from one
machine learning method to another.

understanding the data is one of the keys to successful
categorization, yet this is an area in which many
categorization tool vendors are weak.

38 / 49

choosing what kind of classi   er to use

when building a text classi   er,    rst question: how much training
data is there currently available?

practical challenge: creating or obtaining enough training data
hundreds or thousands of examples from each class are required to
produce a high performance classi   er and many real world contexts
involve large sets of categories.

none?

very little?

quite a lot?

a huge amount, growing every day?

39 / 49

if you have no labeled training data

use hand-written rules!

example
if (wheat or grain) and not (whole or bread) then
c = grain

in practice, rules get a lot bigger than this, and can be phrased
using more sophisticated query languages than just boolean
expressions, including the use of numeric scores. with careful
crafting, the accuracy of such rules can become very high (high
90% precision, high 80% recall). nevertheless the amount of work
to create such well-tuned rules is very large. a reasonable estimate
is 2 days per class, and extra time has to go into maintenance of
rules, as the content of documents in classes drifts over time.

40 / 49

a verity topic (a complex classi   cation rule)

41 / 49

westlaw: example queries

information need: information on the legal theories involved in
preventing the disclosure of trade secrets by employees formerly
employed by a competing company query:    trade secret    /s
disclos! /s prevent /s employe!

information need: requirements

for disabled people to be able to access a workplace query: disab!
/p access! /s work-site work-place (employment /3 place)

information need: cases about a host   s responsibility for drunk
guests query: host! /p (responsib! liab!) /p (intoxicat! drunk!)
/p guest

42 / 49

if you have fairly little data and you are going to train a
supervised classi   er

work out how to get more labeled data as quickly as you can.

best way: insert yourself into a process where humans will be
willing to label data for you as part of their natural tasks.

example
often humans will sort or route email for their own purposes, and
these actions give information about classes.

active learning
a system is built which decides which documents a human should
label. usually these are the ones on which a classi   er is uncertain
of the correct classi   cation.

43 / 49

if you have labeled data

good amount of labeled data, but not huge
use everything that we have presented about text classi   cation.
consider hybrid approach (overlay boolean classi   er)

huge amount of labeled data
choice of classi   er probably has little e   ect on your results.
choose classi   er based on the scalability of training or runtime
e   ciency. rule of thumb: each doubling of the training data size
produces a linear increase in classi   er performance, but with very
large amounts of data, the improvement becomes sub-linear.

44 / 49

large and di   cult category taxonomies

if you have a small number of well-separated categories, then many
classi   cation algorithms are likely to work well. but often: very
large number of very similar categories.

example
web directories (e.g. the yahoo! directory consists of over
200,000 categories or the open directory project), library
classi   cation schemes (dewey decimal or library of congress), the
classi   cation schemes used in legal or medical applications.

accurate classi   cation over large sets of closely related classes is
inherently di   cult.     no general high-accuracy solution.

45 / 49

recap

is there a learning method that is optimal for all text
classi   cation problems?

no, because there is a tradeo    between bias and variance.
factors to take into account:

how much training data is available?
how simple/complex is the problem? (linear vs. nonlinear
decision boundary)
how noisy is the problem?
how stable is the problem over time?

for an unstable problem, it   s better to use a simple and robust
classi   er.

46 / 49

exercise

you are tasked with building a system that monitors the sentiment
expressed by tweeters about a company. functionality: the user
enters a set of #hashtags, @usernames and keyword queries that
are related to the company of interest. the system then computes
the proportion of positive and negative sentiment in the messages
containing these #hashtags, @usernames and queries. a key part
of this system is a classi   er that takes a tweet and classi   es it as
having positive or negative polarity. how would you build this
classi   er? you can use a rule-based or a statistical or a hybrid
approach.

47 / 49

take-away today

support vector machines: state-of-the-art text classi   cation
methods (linear and nonlinear)

introduction to id166s

formalization

soft margin case for nonseparable problems

discussion: which classi   er should i use for my problem?

48 / 49

resources

chapter 14 of iir (basic vector space classi   cation)

chapter 15 of iir (id166s)

discussion of    how to select the right classi   er for my
problem    in russell and norvig

resources at http://cislmu.org

49 / 49

