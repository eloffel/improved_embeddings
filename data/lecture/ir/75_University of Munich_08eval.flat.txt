introduction to information retrieval

http://informationretrieval.org

iir 8: evaluation & result summaries

hinrich sch  utze

center for information and language processing, university of munich

2013-05-07

1 / 60

overview

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

2 / 60

outline

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

3 / 60

pivot id172

source:
lillian lee

5 / 60

selecting k top scoring documents in o(n log k)

goal: keep the k top documents seen so far

use a binary min heap
to process a new document d     with score s    :
get current minimum hm of heap (in o(1))
if s         hm skip to next document
if s     > hm heap-delete-root (in o(log k))
heap-add d    /s     (in o(1))
reheapify (in o(log k))

6 / 60

heuristics for    nding the top k even faster

document-at-a-time processing

we complete computation of the query-document similarity
score of document di before starting to compute the
query-document similarity score of di +1.
requires a consistent ordering of documents in the postings
lists

term-at-a-time processing

we complete processing the postings list of query term ti
before starting to process the postings list of ti +1.
requires an accumulator for each document    still in the
running   

the most e   ective heuristics switch back and forth between
term-at-a-time and document-at-a-time processing.

7 / 60

tiered index

tier 1

tier 2

tier 3

auto

best

car

doc2

doc1

doc3

insurance

doc2

doc3

auto

best

car

insurance

auto

best

car

insurance

doc1

doc3

doc1

doc2

8 / 60

take-away today

introduction to evaluation: measures of an ir system

evaluation of unranked and ranked retrieval

evaluation benchmarks

result summaries

9 / 60

outline

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

10 / 60

measures for a search engine

how fast does it index

e.g., number of bytes per hour

how fast does it search

e.g., latency as a function of queries per second

what is the cost per query?

in dollars

11 / 60

measures for a search engine

all of the preceding criteria are measurable: we can quantify
speed / size / money

however, the key measure for a search engine is user
happiness.

what is user happiness?
factors include:

speed of response
size of index
uncluttered ui
most important: relevance
(actually, maybe even more important: it   s free)

note that none of these is su   cient: blindingly fast, but
useless answers won   t make a user happy.

how can we quantify user happiness?

12 / 60

who is the user?

who is the user we are trying to make happy?

web search engine: searcher. success: searcher    nds what
she was looking for. measure: rate of return to this search
engine

web search engine: advertiser. success: searcher clicks on
ad. measure: clickthrough rate

ecommerce: buyer. success: buyer buys something.
measures: time to purchase, fraction of    conversions    of
searchers to buyers

ecommerce: seller. success: seller sells something. measure:
pro   t per item sold

enterprise: ceo. success: employees are more productive
(because of e   ective search). measure: pro   t of the company

13 / 60

most common de   nition of user happiness: relevance

user happiness is equated with the relevance of search results
to the query.

but how do you measure relevance?
standard methodology in information retrieval consists of
three elements.

a benchmark document collection
a benchmark suite of queries
an assessment of the relevance of each query-document pair

14 / 60

relevance: query vs. information need

relevance to what?

first take: relevance to the query

   relevance to the query    is very problematic.

information need i :    i am looking for information on whether
drinking red wine is more e   ective at reducing your risk of
heart attacks than white wine.   

this is an information need, not a query.

query q: [red wine white wine heart attack]
consider document d    : at the heart of his speech was an
attack on the wine industry lobby for downplaying the role of
red and white wine in drunk driving.

d     is an excellent match for query q . . .
d     is not relevant to the information need i .

15 / 60

relevance: query vs. information need

user happiness can only be measured by relevance to an
information need, not by relevance to queries.

our terminology is sloppy in these slides and in iir: we talk
about query-document relevance judgments even though we
mean information-need-document relevance judgments.

16 / 60

outline

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

17 / 60

precision and recall

precision (p) is the fraction of retrieved documents that are
relevant

precision =

#(relevant items retrieved)

#(retrieved items)

= p(relevant|retrieved)

recall (r) is the fraction of relevant documents that are
retrieved

recall =

#(relevant items retrieved)

#(relevant items)

= p(retrieved|relevant)

18 / 60

precision and recall

retrieved
not retrieved

relevant
true positives (tp)
false negatives (fn)

nonrelevant
false positives (fp)
true negatives (tn)

p = tp/(tp + fp)

r = tp/(tp + fn)

19 / 60

precision/recall tradeo   

you can increase recall by returning more docs.

recall is a non-decreasing function of the number of docs
retrieved.

a system that returns all docs has 100% recall!

the converse is also true (usually): it   s easy to get high
precision for very low recall.

suppose the document with the largest score is relevant. how
can we maximize precision?

20 / 60

a combined measure: f

f allows us to trade o    precision against recall.

f =

1

   1

p + (1       ) 1

r

=

(  2 + 1)pr

  2p + r

where

  2 =

1       

  

       [0, 1] and thus   2     [0,    ]
most frequently used: balanced f with    = 1 or    = 0.5

this is the harmonic mean of p and r: 1

f = 1

2 ( 1

p + 1
r )

what value range of    weights recall higher than precision?

21 / 60

example for precision, recall, f1

retrieved
not retrieved

relevant
20
60
80

not relevant
40
1,000,000
1,000,040

60
1,000,060
1,000,120

p = 20/(20 + 40) = 1/3

r = 20/(20 + 60) = 1/4
f1 = 2 1
+ 1
1
4

= 2/7

1
1
3

22 / 60

accuracy

why do we use complex measures like precision, recall, and f ?

why not something simple like accuracy?

accuracy is the fraction of decisions (relevant/nonrelevant)
that are correct.

in terms of the contingency table above,
accuracy = (tp + tn)/(tp + fp + fn + tn).

23 / 60

exercise

compute precision, recall and f1 for this result set:

retrieved
not retrieved

relevant
18
82

not relevant
2
1,000,000,000

the snoogle search engine below always returns 0 results (   0
matching results found   ), regardless of the query. why does
snoogle demonstrate that accuracy is not a useful measure in
ir?

24 / 60

why accuracy is a useless measure in ir

simple trick to maximize accuracy in ir: always say no and
return nothing

you then get 99.99% accuracy on most queries.

searchers on the web (and in ir in general) want to    nd
something and have a certain tolerance for junk.

it   s better to return some bad hits as long as you return
something.

    we use precision, recall, and f for evaluation, not accuracy.

25 / 60

f: why harmonic mean?

why don   t we use a di   erent mean of p and r as a measure?

e.g., the arithmetic mean

the simple (arithmetic) mean is close to 50% for snoogle
search engine     which is too high.

desideratum: punish really bad performance on either
precision or recall.

taking the minimum achieves this.

but minimum is not smooth and hard to weight.

f (harmonic mean) is a kind of smooth minimum.

26 / 60

f1 and other averages

100 

80 

60 

40 

20 

0 

0 

minimum 

maximum 

arithmetic 

geometric 

harmonic 

20 

40 

60 

80 

100 

precision (recall fixed at 70%) 

we can view the harmonic mean as a kind of soft minimum

27 / 60

di   culties in using precision, recall and f

we need relevance judgments for information-need-document
pairs     but they are expensive to produce.

for alternatives to using precision/recall and having to
produce relevance judgments     see end of this lecture.

28 / 60

outline

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

29 / 60

precision-recall curve

precision/recall/f are measures for unranked sets.

we can easily turn set measures into measures of ranked lists.

just compute the set measure for each    pre   x   : the top 1,
top 2, top 3, top 4 etc results

doing this for precision and recall gives you a precision-recall
curve.

30 / 60

a precision-recall curve

i

i

n
o
s
c
e
r
p

1.0

0.8

0.6

0.4

0.2

0.0

0.0

0.2

0.4

0.6

0.8

1.0

recall

each point corresponds to a result for the top k ranked hits
(k = 1, 2, 3, 4, . . .).

interpolation (in red): take maximum of all future points

rationale for interpolation: the user is willing to look at more
stu    if both precision and recall get better.

questions?

31 / 60

11-point interpolated average precision

recall

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

interpolated
precision
1.00
0.67
0.63
0.55
0.45
0.41
0.36
0.29
0.13
0.10
0.08

11-point
0.425

average:

   

how can precision
at 0.0 be > 0?

32 / 60

averaged 11-point precision/recall graph

i

i

n
o
s
c
e
r
p

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

recall

compute interpolated precision at recall levels 0.0, 0.1, 0.2,
. . .

do this for each of the queries in the evaluation benchmark

average over queries

this measure measures performance at all recall levels.

the curve is typical of performance levels at trec.

note that performance is not very good!

33 / 60

roc curve

)
l
l

a
c
e
r
 
=
 
(
 
y
t
i
v
i
t
i
s
n
e
s

1.0

0.8

0.6

0.4

0.2

0.0

0

0.2

0.4

0.6

0.8

1

1     specificity

similar to precision-recall graph

but we are only interested in the small area in the lower left
corner.

precision-recall graph    blows up    this area.

34 / 60

variance of measures like precision/recall

for a test collection, it is usual that a system does badly on
some information needs (e.g., p = 0.2 at r = 0.1) and really
well on others (e.g., p = 0.95 at r = 0.1).

indeed, it is usually the case that the variance of the same
system across queries is much greater than the variance of
di   erent systems on the same query.

that is, there are easy information needs and hard ones.

35 / 60

outline

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

36 / 60

what we need for a benchmark

a collection of documents

documents should be representative of the documents we
expect to see in reality.

a collection of information needs (often incorrectly called
queries)

information needs should be representative of the information
needs we expect to see in reality.

human relevance assessments

we need to hire/pay    judges    or assessors to do this.
expensive, time-consuming
judges should be representative of the users we expect to see
in reality.

37 / 60

first standard relevance benchmark: cran   eld

pioneering:    rst testbed allowing precise quantitative
measures of information retrieval e   ectiveness

late 1950s, uk

1398 abstracts of aerodynamics journal articles, a set of 225
queries, exhaustive relevance judgments of all
query-document-pairs

too small, too untypical for serious ir evaluation today

38 / 60

second-generation relevance benchmark: trec

trec = text retrieval conference (trec)

organized by the u.s. national institute of standards and
technology (nist)

trec is actually a set of several di   erent relevance
benchmarks.

best known: trec ad hoc, used for    rst 8 trec
evaluations between 1992 and 1999

1.89 million documents, mainly newswire articles, 450
information needs

no exhaustive relevance judgments     too expensive

rather, nist assessors    relevance judgments are available
only for the documents that were among the top k returned
for some system which was entered in the trec evaluation
for which the information need was developed.

39 / 60

example of more recent benchmark: clueweb09

1 billion web pages

25 terabytes (compressed: 5 terabyte)

collected january/february 2009

10 languages

unique urls: 4,780,950,903 (325 gb uncompressed, 105 gb
compressed)

total outlinks: 7,944,351,835 (71 gb uncompressed, 24 gb
compressed)

40 / 60

validity of relevance assessments

relevance assessments are only usable if they are consistent.

if they are not consistent, then there is no    truth    and
experiments are not repeatable.

how can we measure this consistency or agreement among
judges?

    kappa measure

41 / 60

kappa measure

kappa is measure of how much judges agree or disagree.

designed for categorical judgments

corrects for chance agreement

p(a) = proportion of time judges agree

p(e ) = what agreement would we get by chance

   =

p(a)     p(e )

1     p(e )

   =? for (i) chance agreement (ii) total agreement

42 / 60

kappa measure (2)

values of    in the interval [2/3, 1.0] are seen as acceptable.

with smaller values: need to redesign relevance assessment
methodology used etc.

43 / 60

calculating the kappa statistic

judge 1
yes
relevance no

total

judge 2 relevance
yes
no total
320
20
300
70
10
80
400
310
90

observed proportion of

the times the judges agreed
p(a) = (300 + 70)/400 = 370/400 = 0.925
pooled marginals
p(nonrelevant) = (80 + 90)/(400 + 400) = 170/800 = 0.2125
p(relevant) = (320 + 310)/(400 + 400) = 630/800 = 0.7878
id203 that the two judges agreed by chance p(e ) =
p(nonrelevant)2 + p(relevant)2 = 0.21252 + 0.78782 = 0.665
kappa statistic    = (p(a)     p(e ))/(1     p(e )) =
(0.925     0.665)/(1     0.665) = 0.776 (still in acceptable range)

44 / 60

interjudge agreement at trec

information
need
51
62
67
95
127

number of
docs judged
211
400
400
400
400

disagreements

6
157
68
110
106

45 / 60

impact of interjudge disagreement

judges disagree a lot. does that mean that the results of
information retrieval experiments are meaningless?

no.

large impact on absolute performance numbers

virtually no impact on ranking of systems

supposes we want to know if algorithm a is better than
algorithm b

an information retrieval experiment will give us a reliable
answer to this question . . .

. . . even if there is a lot of disagreement between judges.

46 / 60

evaluation at large search engines

recall is di   cult to measure on the web

search engines often use precision at top k, e.g., k = 10 . . .

. . . or use measures that reward you more for getting rank 1
right than for getting rank 10 right.
search engines also use non-relevance-based measures.

example 1: clickthrough on    rst result
not very reliable if you look at a single clickthrough (you may
realize after clicking that the summary was misleading and the
document is nonrelevant) . . .
. . . but pretty reliable in the aggregate.
example 2: ongoing studies of user behavior in the lab     recall
last lecture
example 3: a/b testing

47 / 60

a/b testing

purpose: test a single innovation

prerequisite: you have a large search engine up and running.

have most users use old system

divert a small proportion of tra   c (e.g., 1%) to the new
system that includes the innovation

evaluate with an    automatic    measure like clickthrough on
   rst result

now we can directly see if the innovation does improve user
happiness.

probably the evaluation methodology that large search
engines trust most

48 / 60

critique of pure relevance

we   ve de   ned relevance for an isolated query-document pair.

alternative de   nition: marginal relevance

the marginal relevance of the document dk at position k in
the result list is the additional information it contributes over
and above the information that was contained in documents
d1 . . . dk   1.
exercise

why is marginal relevance a more realistic measure of user
happiness?
give an example where a non-marginal measure like precision
or recall is a misleading measure of user happiness, but
marginal relevance is a good measure.
in a practical application, what is the di   culty of using
marginal measures instead of non-marginal measures?

49 / 60

outline

1 recap

2

introduction

3 unranked evaluation

4 ranked evaluation

5 benchmarks

6 result summaries

50 / 60

how do we present results to the user?

most often: as a list     aka    10 blue links   

how should each document in the list be described?

this description is crucial.

the user often can identify good hits (= relevant hits) based
on the description.

no need to actually view any document

51 / 60

doc description in result list

most commonly: doc title, url, some metadata . . .

. . . and a summary

how do we    compute    the summary?

52 / 60

summaries

two basic kinds: (i) static (ii) dynamic

a static summary of a document is always the same,
regardless of the query that was issued by the user.

dynamic summaries are query-dependent. they attempt to
explain why the document was retrieved for the query at hand.

53 / 60

static summaries

in typical systems, the static summary is a subset of the
document.

simplest heuristic: the    rst 50 or so words of the document
more sophisticated: extract from each document a set of
   key    sentences

simple nlp heuristics to score each sentence
summary is made up of top-scoring sentences.
machine learning approach: see iir 13

most sophisticated: complex nlp to synthesize/generate a
summary

for most ir applications: not quite ready for prime time yet

54 / 60

dynamic summaries

present one or more    windows    or snippets within the
document that contain several of the query terms.

prefer snippets in which query terms occurred as a phrase

prefer snippets in which query terms occurred jointly in a
small window

the summary that is computed this way gives the entire
content of the window     all terms, not just the query terms.

55 / 60

google dynamic summaries for [vegetarian diet running]

good example
that snippet
selection is
non-trivial.

criteria:
occurrence of
keywords, density
of keywords,
coherence of
snippet, number
of di   erent
snippets in
summary, good
cutting points etc

56 / 60

generating dynamic summaries

where do we get these other terms in the snippet from?

we cannot construct a dynamic summary from the positional
inverted index     at least not e   ciently.

we need to cache documents.

the positional index tells us: query term occurs at position
4378 in the document.

byte o   set or word o   set?

note that the cached copy can be outdated

don   t cache very long documents     just cache a short pre   x

57 / 60

dynamic summaries

real estate on the search result page is limited     snippets
must be short . . .

. . . but snippets must be long enough to be meaningful.

snippets should communicate whether and how the document
answers the query.

ideally: linguistically well-formed snippets

ideally: the snippet should answer the query, so we don   t have
to look at the document.
dynamic summaries are a big part of user happiness because
. . .

. . . we can quickly scan them to    nd the relevant document we
then click on.
. . . in many cases, we don   t have to click at all and save time.

58 / 60

take-away today

introduction to evaluation: measures of an ir system

evaluation of unranked and ranked retrieval

evaluation benchmarks

result summaries

59 / 60

resources

chapter 8 of iir
resources at http://cislmu.org

the trec home page     trec had a huge impact on
information retrieval evaluation.
originator of f -measure: keith van rijsbergen
more on a/b testing
too much a/b testing at google?
tombros & sanderson 1998: one of the    rst papers on
dynamic summaries
google vp of engineering on search quality evaluation at
google

60 / 60

