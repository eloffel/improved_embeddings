introduction to information retrieval

http://informationretrieval.org

iir 13: text classi   cation & naive bayes

hinrich sch  utze

center for information and language processing, university of munich

2014-05-15

1 / 58

overview

1 recap

2 text classi   cation

3 naive bayes

4 nb theory

5 evaluation of tc

2 / 58

outline

1 recap

2 text classi   cation

3 naive bayes

4 nb theory

5 evaluation of tc

3 / 58

relevance feedback: basic idea

the user issues a (short, simple) query.

the search engine returns a set of documents.

user marks some docs as relevant, some as nonrelevant.

search engine computes a new representation of the
information need     should be better than the initial query.

search engine runs new query and returns new results.

new results have (hopefully) better recall.

4 / 58

rocchio illustrated

~  r     ~  nr
x

x
x

x
x

x

~qopt

~  r

~  nr

5 / 58

types of id183

manual thesaurus (maintained by editors, e.g., pubmed)

automatically derived thesaurus (e.g., based on co-occurrence
statistics)

query-equivalence based on query log mining (common on the
web as in the    palm    example)

6 / 58

id183 at search engines

main source of id183 at search engines: query logs
example 1: after issuing the query [herbs], users frequently
search for [herbal remedies].

       herbal remedies    is potential expansion of    herb   .

example 2: users searching for [   ower pix] frequently click on
the url photobucket.com/   ower. users searching for [   ower
clipart] frequently click on the same url.

          ower clipart    and       ower pix    are potential expansions of
each other.

7 / 58

take-away today

text classi   cation: de   nition & relevance to information
retrieval

naive bayes: simple baseline text classi   er

theory: derivation of naive bayes classi   cation rule & analysis

evaluation of text classi   cation: how do we know it worked /
didn   t work?

8 / 58

outline

1 recap

2 text classi   cation

3 naive bayes

4 nb theory

5 evaluation of tc

9 / 58

a text classi   cation task: email spam    ltering

from:              <takworlld@hotmail.com>
subject: real estate is the only way... gem oalvgkay
anyone can buy real estate with no money down
stop paying rent today !
there is no need to spend hundreds or even thousands for similar courses
i am 22 years old and i have already purchased 6 properties using the
methods outlined in this truly incredible ebook.
change your life now !
=================================================
click below to order:
http://www.wholesaledaily.com/sales/nmd.htm
=================================================

how would you write a program that would automatically detect
and delete this type of message?

10 / 58

formal de   nition of tc: training

given:

a document space x

documents are represented in this space     typically some type
of high-dimensional space.

a    xed set of classes c = {c1, c2, . . . , cj }

the classes are human-de   ned for the needs of an application
(e.g., spam vs. nonspam).

a training set d of labeled documents. each labeled
document hd, ci     x    c

using a learning method or learning algorithm, we then wish to
learn a classi   er    that maps documents to classes:

   : x     c

11 / 58

formal de   nition of tc: application/testing

given: a description d     x of a document determine:   (d)     c,

that is, the class that is most appropriate for d

12 / 58

topic classi   cation

regions

industries

subject areas

classes:

training
set:

uk

china

poultry

co   ee

elections

sports

congestion

olympics

feed

roasting

recount

diamond

london

beijing

chicken

beans

votes

baseball

  (d    ) =china

d    

first

private

chinese
airline

test
set:

parliament

tourism

big ben

great wall

pate

ducks

arabica

robusta

seat

forward

run-off

soccer

windsor

mao

bird flu

kenya

tv ads

team

the queen

communist

turkey

harvest

campaign

captain

13 / 58

exercise

find examples of uses of text classi   cation in information
retrieval

14 / 58

examples of how search engines use classi   cation

language identi   cation (classes: english vs. french etc.)

the automatic detection of spam pages (spam vs. nonspam)

sentiment detection: is a movie or product review positive or
negative (positive vs. negative)

topic-speci   c or vertical search     restrict search to a
   vertical    like    related to health    (relevant to vertical vs. not)

15 / 58

classi   cation methods: 1. manual

manual classi   cation was used by yahoo in the beginning of
the web. also: odp, pubmed

very accurate if job is done by experts

consistent when the problem size and team is small

scaling manual classi   cation is di   cult and expensive.

    we need automatic methods for classi   cation.

16 / 58

classi   cation methods: 2. rule-based

e.g., google alerts is rule-based classi   cation.

there are ide-type development enviroments for writing very
complex rules e   ciently. (e.g., verity)

often: boolean combinations (as in google alerts)

accuracy is very high if a rule has been carefully re   ned over
time by a subject expert.

building and maintaining rule-based classi   cation systems is
cumbersome and expensive.

17 / 58

a verity topic (a complex classi   cation rule)

18 / 58

classi   cation methods: 3. statistical/probabilistic

this was our de   nition of the classi   cation problem     text
classi   cation as a learning problem

(i) supervised learning of a the classi   cation function    and
(ii) application of    to classifying new documents

we will look at two methods for doing this: naive bayes and
id166s

no free lunch: requires hand-classi   ed training data

but this manual classi   cation can be done by non-experts.

19 / 58

outline

1 recap

2 text classi   cation

3 naive bayes

4 nb theory

5 evaluation of tc

20 / 58

the naive bayes classi   er

the naive bayes classi   er is a probabilistic classi   er.

we compute the id203 of a document d being in a class
c as follows:

p(c|d)     p(c) y
1   k   nd

p(tk |c)

nd is the length of the document. (number of tokens)
p(tk |c) is the id155 of term tk occurring in a
document of class c
p(tk |c) as a measure of how much evidence tk contributes
that c is the correct class.

p(c) is the prior id203 of c.

if a document   s terms do not provide clear evidence for one
class vs. another, we choose the c with highest p(c).

21 / 58

maximum a posteriori class

our goal in naive bayes classi   cation is to    nd the    best   
class.

the best class is the most likely or maximum a posteriori
(map) class c map:

c map = arg max

c   c

  p(c|d) = arg max

c   c

  p(c) y
1   k   nd

  p(tk |c)

22 / 58

taking the log

multiplying lots of small probabilities can result in    oating
point under   ow.

since log(xy ) = log(x) + log(y ), we can sum log probabilities
instead of multiplying probabilities.

since log is a monotonic function, the class with the highest
score does not change.

so what we usually compute in practice is:

c map = arg max

c   c

[log   p(c) + x
1   k   nd

log   p(tk |c)]

23 / 58

naive bayes classi   er

classi   cation rule:

c map = arg max

c   c

[log   p(c) + x
1   k   nd

log   p(tk |c)]

simple interpretation:

each conditional parameter log   p(tk |c) is a weight that
indicates how good an indicator tk is for c.
the prior log   p(c) is a weight that indicates the relative
frequency of c.
the sum of log prior and term weights is then a measure of
how much evidence there is for the document being in the
class.
we select the class with the most evidence.

24 / 58

parameter estimation take 1: maximum likelihood

estimate parameters   p(c) and   p(tk |c) from train data: how?
prior:

  p(c) =

nc
n

nc : number of docs in class c; n: total number of docs
conditional probabilities:

  p(t|c) =

tct

pt       v tct    

tct is the number of tokens of t in training documents from
class c (includes multiple occurrences)

we   ve made a naive bayes independence assumption here:
  p(tk |c) =   p(tk |c), independent of position

25 / 58

the problem with maximum likelihood estimates: zeros

c =china

x1=beijing

x2=and

x3=taipei

x4=join

x5=wto

p(china|d)     p(china)    p(beijing|china)    p(and|china)

   p(taipei|china)    p(join|china)    p(wto|china)

if wto never occurs in class china in the train set:

  p(wto|china) =

tchina,wto
pt       v tchina,t    

=

0

pt       v tchina,t    

= 0

26 / 58

the problem with maximum likelihood estimates: zeros
(cont)

if there are no occurrences of wto in documents in class
china, we get a zero estimate:

  p(wto|china) =

tchina,wto
pt       v tchina,t    

= 0

    we will get p(china|d) = 0 for any document that
contains wto!

27 / 58

to avoid zeros: add-one smoothing

before:

  p(t|c) =

tct

pt       v tct    

now: add one to each count to avoid zeros:

  p(t|c) =

tct + 1

pt       v (tct     + 1)

=

tct + 1

(pt       v tct    ) + b

b is the number of bins     in this case the number of di   erent
words or the size of the vocabulary |v | = m

28 / 58

naive bayes: summary

estimate parameters from the training corpus using add-one
smoothing

for a new document, for each class, compute sum of (i) log of
prior and (ii) logs of conditional probabilities of the terms

assign the document to the class with the largest score

29 / 58

naive bayes: training

trainmultinomialnb(c, d)

1 v     extractvocabulary(d)
2 n     countdocs(d)
3 for each c     c
4 do nc     countdocsinclass(d, c)
5
6
7
8
9
10
11 return v , prior , condprob

prior [c]     nc /n
textc     concatenatetextofalldocsinclass(d, c)
for each t     v
do tct     counttokensofterm(textc , t)
for each t     v
do condprob[t][c]     tct +1

pt     (tct     +1)

30 / 58

naive bayes: testing

applymultinomialnb(c, v , prior , condprob, d)
1 w     extracttokensfromdoc(v , d)
2 for each c     c
3 do score[c]     log prior [c]
4
5
6 return arg maxc   c score[c]

for each t     w
do score[c]+ = log condprob[t][c]

31 / 58

exercise: estimate parameters, classify test set

training set

test set

docid words in document
1
2
3
4
5

chinese beijing chinese
chinese chinese shanghai
chinese macao
tokyo japan chinese
chinese chinese chinese tokyo japan

in c = china?
yes
yes
yes
no
?

  p(c) =

  p(t|c) =

tct + 1

pt       v (tct     + 1)

nc
n

=

tct + 1

(pt       v tct    ) + b

(b is the number of bins     in this case the number of di   erent words or the
size of the vocabulary |v | = m)

c map = arg max

c   c

[   p(c)    y
1   k   nd

  p(tk |c)]

32 / 58

33 / 58

example: parameter estimates

priors:   p(c) = 3/4 and   p(c) = 1/4 conditional probabilities:

  p(chinese|c) = (5 + 1)/(8 + 6) = 6/14 = 3/7

  p(tokyo|c) =   p(japan|c) = (0 + 1)/(8 + 6) = 1/14
  p(chinese|c) = (1 + 1)/(3 + 6) = 2/9
  p(tokyo|c) =   p(japan|c) = (1 + 1)/(3 + 6) = 2/9

the denominators are (8 + 6) and (3 + 6) because the lengths of
textc and textc are 8 and 3, respectively, and because the constant
b is 6 as the vocabulary consists of six terms.

34 / 58

example: classi   cation

  p(c|d5)     3/4    (3/7)3    1/14    1/14     0.0003
  p(c|d5)     1/4    (2/9)3    2/9    2/9     0.0001

thus, the classi   er assigns the test document to c = china. the
reason for this classi   cation decision is that the three occurrences
of the positive indicator chinese in d5 outweigh the occurrences
of the two negative indicators japan and tokyo.

35 / 58

time complexity of naive bayes

time complexity

mode
training   (|d|lave + |c||v |)
testing   (la + |c|m a) =   (|c|m a)

lave: average length of a training doc, la: length of the test
doc, m a: number of distinct terms in the test doc, d:
training set, v : vocabulary, c: set of classes
  (|d|lave) is the time it takes to compute all counts.
  (|c||v |) is the time it takes to compute the parameters
from the counts.
generally: |c||v | < |d|lave
test time is also linear (in the length of the test document).

thus: naive bayes is linear in the size of the training set
(training) and the test document (testing). this is optimal.

36 / 58

outline

1 recap

2 text classi   cation

3 naive bayes

4 nb theory

5 evaluation of tc

37 / 58

naive bayes: analysis

now we want to gain a better understanding of the properties
of naive bayes.

we will formally derive the classi   cation rule . . .

. . . and make our assumptions explicit.

38 / 58

derivation of naive bayes rule

we want to    nd the class that is most likely given the document:

c map = arg max

c   c

p(c|d)

apply bayes rule p(a|b) = p(b|a)p(a)

p(b)

:

c map = arg max

c   c

p(d|c)p(c)

p(d)

drop denominator since p(d) is the same for all classes:

c map = arg max

c   c

p(d|c)p(c)

39 / 58

too many parameters / sparseness

c map = arg max

c   c

p(d|c)p(c)

= arg max

c   c

p(ht1, . . . , tk , . . . , tnd i|c)p(c)

there are too many parameters p(ht1, . . . , tk , . . . , tnd i|c), one
for each unique combination of a class and a sequence of
words.

we would need a very, very large number of training examples
to estimate that many parameters.

this is the problem of data sparseness.

40 / 58

naive bayes conditional independence assumption

to reduce the number of parameters to a manageable size, we
make the naive bayes conditional independence assumption:

p(d|c) = p(ht1, . . . , tnd i|c) = y
1   k   nd

p(xk = tk |c)

we assume that the id203 of observing the conjunction of
attributes is equal to the product of the individual probabilities
p(xk = tk |c). recall from earlier the estimates for these
conditional probabilities:   p(t|c) =

tct +1

(pt       v tct     )+b

41 / 58

generative model

c =china

x1=beijing

x2=and

x3=taipei

x4=join

x5=wto

p(c|d)     p(c) q1   k   nd

p(tk |c)

generate a class with id203 p(c)

generate each of the words (in their respective positions),
conditional on the class, but independent of each other, with
id203 p(tk |c)
to classify docs, we    reengineer    this process and    nd the
class that is most likely to have generated the doc.

42 / 58

second independence assumption

  p(xk1 = t|c) =   p(xk2 = t|c)
for example, for a document in the class uk, the id203
of generating queen in the    rst position of the document is
the same as generating it in the last position.

the two independence assumptions amount to the bag of
words model.

43 / 58

a di   erent naive bayes model: bernoulli model

c =china

u alaska=0

u beijing=1

u india=0

u join=1

u taipei=1

u wto=1

44 / 58

violation of naive bayes independence assumptions

conditional independence:

p(ht1, . . . , tnd i|c) = y
1   k   nd

p(xk = tk |c)

positional independence:
  p(xk1 = t|c) =   p(xk2 = t|c)
the independence assumptions do not really hold of
documents written in natural language.
exercise

examples for why conditional independence assumption is not
really true?
examples for why positional independence assumption is not
really true?

how can naive bayes work if it makes such inappropriate
assumptions?

45 / 58

why does naive bayes work?

naive bayes can work well even though conditional
independence assumptions are badly violated.

example:

c1
0.6
0.00099
0.99

c2
0.4
0.00001
0.01

true id203 p(c|d)
  p(c) q1   k   nd
  p(tk |c)
nb estimate   p(c|d)
double counting of evidence causes underestimation (0.01)
and overestimation (0.99).

c1

class selected
c1

classi   cation is about predicting the correct class and not
about accurately estimating probabilities.

naive bayes is terrible for correct estimation . . .

. . . but if often performs well at accurate prediction (choosing
the correct class).

46 / 58

naive bayes is not so naive

naive bayes has won some bakeo   s (e.g., kdd-cup 97)

more robust to nonrelevant features than some more complex
learning methods

more robust to concept drift (changing of de   nition of class
over time) than some more complex learning methods

better than methods like id90 when we have many
equally important features

a good dependable baseline for text classi   cation (but not the
best)

optimal if independence assumptions hold (never true for
text, but true for some domains)

very fast

low storage requirements

47 / 58

outline

1 recap

2 text classi   cation

3 naive bayes

4 nb theory

5 evaluation of tc

48 / 58

evaluation on reuters

regions

industries

subject areas

classes:

training
set:

uk

china

poultry

co   ee

elections

sports

congestion

olympics

feed

roasting

recount

diamond

london

beijing

chicken

beans

votes

baseball

  (d    ) =china

d    

first

private

chinese
airline

test
set:

parliament

tourism

big ben

great wall

pate

ducks

arabica

robusta

seat

forward

run-off

soccer

windsor

mao

bird flu

kenya

tv ads

team

the queen

communist

turkey

harvest

campaign

captain

49 / 58

example: the reuters collection

symbol
n
l
m

statistic
documents
avg. # word tokens per document
word types

value
800,000
200
400,000

type of class
region
industry
subject area

number
366
870
126

examples
uk, china
poultry, co   ee
elections, sports

50 / 58

a reuters document

51 / 58

evaluating classi   cation

evaluation must be done on test data that are independent of
the training data, i.e., training and test sets are disjoint.

it   s easy to get good performance on a test set that was
available to the learner during training (e.g., just memorize
the test set).
measures: precision, recall, f1, classi   cation accuracy

52 / 58

precision p and recall r

predicted to be in the class
predicted to not be in the class

in the class
true positives (tp)
false negatives (fn)

not in the class
false positives (fp)
true negatives (tn)

tp, fp, fn, tn are counts of documents. the sum of these four

counts is the total number of documents.

precision:p = tp/(tp + fp)

recall:r = tp/(tp + fn)

53 / 58

a combined measure: f

f1 allows us to trade o    precision against recall.

f1 =

1

1
2

1
p + 1
2

1
r

=

2pr
p + r

this is the harmonic mean of p and r: 1

f = 1

2 ( 1

p + 1
r )

54 / 58

averaging: micro vs. macro

we now have an evaluation measure (f1) for one class.
but we also want a single number that measures the
aggregate performance over all classes in the collection.
macroaveraging

compute f1 for each of the c classes
average these c numbers

microaveraging

compute tp, fp, fn for each of the c classes
sum these c numbers (e.g., all tp to get aggregate tp)
compute f1 for aggregate tp, fp, fn

55 / 58

f1 scores for naive bayes vs. other methods

micro-avg-l (90 classes)
macro-avg (90 classes)

nb rocchio
85
80
47
59

knn
86
60

(a)

(b)

earn
acq
money-fx
grain
crude
trade
interest
ship
wheat
corn
micro-avg (top 10)
micro-avg-d (118 classes)

97
92
78
82
86
77
74
79
77
78
82
n/a
naive bayes does pretty well, but some methods beat it
consistently (e.g., id166).

nb rocchio
93
96
65
88
47
57
79
68
70
80
65
64
63
65
49
85
70
69
48
65
65
82
75
62

knn trees
98
90
66
85
85
73
67
74
93
92
88
n/a

id166
89
60

id166
98
94
75
95
89
76
78
86
92
90
92
87

56 / 58

take-away today

text classi   cation: de   nition & relevance to information
retrieval

naive bayes: simple baseline text classi   er

theory: derivation of naive bayes classi   cation rule & analysis

evaluation of text classi   cation: how do we know it worked /
didn   t work?

57 / 58

resources

chapter 13 of iir
resources at http://cislmu.org

weka: a data mining software package that includes an
implementation of naive bayes
reuters-21578     text classi   cation evaluation set
vulgarity classi   er fail

58 / 58

