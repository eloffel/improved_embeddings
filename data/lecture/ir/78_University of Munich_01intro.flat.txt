introduction to information retrieval

http://informationretrieval.org

iir 1: boolean retrieval

hinrich sch  utze

center for information and language processing, university of munich

2014-04-09

1 / 60

take-away

boolean retrieval: design and data structures of a simple
information retrieval system

what topics will be covered in this class?

2 / 60

outline

1

2

introduction

inverted index

3 processing boolean queries

4 query optimization

5 course overview

3 / 60

de   nition of information retrieval

information retrieval (ir) is    nding material (usually documents) of
an unstructured nature (usually text) that satis   es an information
need from within large collections (usually stored on computers).

4 / 60

boolean retrieval

the boolean model is arguably the simplest model to base an
information retrieval system on.

queries are boolean expressions, e.g., caesar and brutus

the seach engine returns all documents that satisfy the
boolean expression.

does google use the boolean model?

7 / 60

does google use the boolean model?

on google, the default interpretation of a query [w1 w2
. . . wn] is w1 and w2 and . . . and wn
cases where you get hits that do not contain one of the wi :

anchor text
page contains variant of wi (morphology, id147,
synonym)
long queries (n large)
boolean expression generates very few hits

simple boolean vs. ranking of result set

simple boolean retrieval returns matching documents in no
particular order.
google (and most well designed boolean engines) rank the
result set     they rank good hits (according to some estimator
of relevance) higher than bad hits.

8 / 60

outline

1

2

introduction

inverted index

3 processing boolean queries

4 query optimization

5 course overview

9 / 60

unstructured data in 1650: shakespeare

10 / 60

unstructured data in 1650

which plays of shakespeare contain the words brutus and
caesar, but not calpurnia?

one could grep all of shakespeare   s plays for brutus and
caesar, then strip out lines containing calpurnia.
why is grep not the solution?
slow (for large collections)
grep is line-oriented, ir is document-oriented
   not calpurnia    is non-trivial
other operations (e.g.,    nd the word romans near
countryman) not feasible

11 / 60

term-document incidence matrix

anthony

and

cleopatra

julius
caesar tempest

the

haid113t othello macbeth

. . .

1
1
1
1
0
0
0

1
1
1
0
1
1
1

anthony
brutus
caesar
calpurnia
cleopatra
mercy
worser
. . .
entry is 1 if term occurs. example: calpurnia occurs in julius
caesar. entry is 0 if term doesn   t occur. example: calpurnia
doesn   t occur in the tempest.

0
0
1
0
0
1
1

0
0
0
0
0
1
1

0
1
1
0
0
1
1

1
0
1
0
0
1
0

12 / 60

incidence vectors

so we have a 0/1 vector for each term.
to answer the query brutus and caesar and not
calpurnia:

take the vectors for brutus, caesar, and calpurnia
complement the vector of calpurnia
do a (bitwise) and on the three vectors
110100 and 110111 and 101111 = 100100

13 / 60

0/1 vectors and result of bitwise operations

anthony

and

cleopatra

julius
caesar tempest

the

haid113t othello macbeth

. . .

anthony
brutus
caesar
calpurnia
cleopatra
mercy
worser
. . .
result:

1
1
1
0
1
1
1

1

1
1
1
1
0
0
0

0

0
0
0
0
0
1
1

0

0
1
1
0
0
1
1

1

0
0
1
0
0
1
1

0

1
0
1
0
0
1
0

0

14 / 60

answers to query

anthony and cleopatra, act iii, scene ii
agrippa [aside to domitius enobarbus]: why, enobarbus,

haid113t, act iii, scene ii

lord polonius:

when antony found julius caesar dead,
he cried almost to roaring; and he wept
when at philippi he found brutus slain.

i did enact julius caesar: i was killed i    the
capitol; brutus killed me.

15 / 60

bigger collections

consider n = 106 documents, each with about 1000 tokens
    total of 109 tokens
on average 6 bytes per token, including spaces and
punctuation     size of document collection is about 6    109 =
6 gb

assume there are m = 500,000 distinct terms in the collection

(notice that we are making a term/token distinction.)

16 / 60

can   t build the incidence matrix

m = 500,000    106 = half a trillion 0s and 1s.
but the matrix has no more than one billion 1s.

matrix is extremely sparse.

what is a better representations?

we only record the 1s.

17 / 60

inverted index

for each term t, we store a list of all documents that contain t.

brutus

       1

caesar

       1

2

2

4

4

11

31

45

173

174

5

6

16

57

132

. . .

calpurnia        2

31

54

101

...

{z

}
|
dictionary

|

{z

postings

}

18 / 60

inverted index construction

1 collect the documents to be indexed:

friends, romans, countrymen. so let it be with caesar . . .

2 tokenize the text, turning each document into a list of tokens:

friends romans

countrymen so . . .

3 do linguistic preprocessing, producing a list of normalized

tokens, which are the indexing terms:

friend roman

countryman so . . .

4

index the documents that each term occurs in by creating an
inverted index, consisting of a dictionary and postings.

19 / 60

id121 and preprocessing

doc 1. i did enact julius caesar: i
was killed i    the capitol; brutus killed
me.
doc 2. so let it be with caesar. the
noble brutus hath told you caesar
was ambitious:

=   

doc 1. i did enact julius caesar i was
killed i    the capitol brutus killed me
doc 2. so let it be with caesar the
noble brutus hath told you caesar was
ambitious

20 / 60

generate postings

doc 1. i did enact julius caesar i was
killed i    the capitol brutus killed me
doc 2. so let it be with caesar the
noble brutus hath told you caesar was
ambitious

=   

term docid
1
i
1
did
1
enact
1
julius
caesar
1
1
i
1
was
1
killed
1
i   
1
the
1
capitol
1
brutus
1
killed
me
1
2
so
2
let
2
it
2
be
2
with
2
caesar
2
the
noble
2
2
brutus
2
hath
2
told
2
you
2
caesar
2
was
ambitious
2

21 / 60

sort postings

term docid
1
i
1
did
1
enact
1
julius
caesar
1
1
i
1
was
1
killed
1
i   
1
the
1
capitol
1
brutus
1
killed
me
1
2
so
2
let
2
it
2
be
2
with
2
caesar
2
the
noble
2
2
brutus
2
hath
2
told
2
you
2
caesar
2
was
ambitious
2

=   

term docid
2
ambitious
2
be
1
brutus
2
brutus
capitol
1
1
caesar
2
caesar
2
caesar
1
did
1
enact
1
hath
1
i
1
i
i   
1
2
it
1
julius
1
killed
1
killed
2
let
1
me
2
noble
so
2
1
the
2
the
2
told
2
you
1
was
2
was
with
2

22 / 60

create postings lists, determine document frequency

term docid
2
ambitious
2
be
1
brutus
2
brutus
capitol
1
1
caesar
2
caesar
2
caesar
1
did
1
enact
1
hath
1
i
1
i
i   
1
2
it
1
julius
1
killed
1
killed
2
let
1
me
2
noble
so
2
1
the
2
the
2
told
2
you
1
was
2
was
with
2

=   

term doc. freq.     postings lists

2
1
2

1
1

ambitious
be
1
brutus
capitol
caesar
did
1
enact
hath
1
i
i   
1
it
1
julius
killed
1
let
me
1
noble
1
so
the
2
1
told
1
you
2
was
with
1

1
1

1

1

    2
    2
    1     2
    1
    1     2
    1
    1
    2
    1
    1
    2
    1
    1
    2
    1
    2
    2
    1     2
    2
    2
    1     2
    2

23 / 60

split the result into dictionary and postings    le

brutus

       1

caesar

       1

2

2

4

4

11

31

45

173

174

5

6

16

57

132

. . .

calpurnia        2

31

54

101

...

{z

}
|
dictionary

|

{z

postings    le

}

24 / 60

later in this course

index construction: how can we create inverted indexes for
large collections?

how much space do we need for dictionary and index?

index compression: how can we e   ciently store and process
indexes for large collections?

ranked retrieval: what does the inverted index look like when
we want the    best    answer?

25 / 60

outline

1

2

introduction

inverted index

3 processing boolean queries

4 query optimization

5 course overview

26 / 60

simple conjunctive query (two terms)

consider the query: brutus and calpurnia
to    nd all matching documents using inverted index:

1 locate brutus in the dictionary
2 retrieve its postings list from the postings    le
3 locate calpurnia in the dictionary
4 retrieve its postings list from the postings    le
5
6 return intersection to user

intersect the two postings lists

27 / 60

intersecting two postings lists

brutus

       1     2     4     11     31     45     173     174

calpurnia        2     31     54     101

intersection =    2     31

this is linear in the length of the postings lists.

note: this only works if postings lists are sorted.

28 / 60

intersecting two postings lists

then add(answer , docid(p1))

intersect(p1, p2)
1 answer     h i
2 while p1 6= nil and p2 6= nil
3 do if docid(p1) = docid(p2)
4
5
6
7
8
9
10 return answer

p1     next(p1)
p2     next(p2)

then p1     next(p1)
else p2     next(p2)

else if docid(p1) < docid(p2)

29 / 60

query processing: exercise

france        1     2     3     4     5     7     8     9     11     12     13     14     15

paris

lear

       2     6     10     12     14

       12     15

compute hit list for ((paris and not france) or lear)

30 / 60

boolean retrieval model: assessment

the boolean retrieval model can answer any query that is a
boolean expression.

boolean queries are queries that use and, or and not to join
query terms.
views each document as a set of terms.
is precise: document matches condition or not.

primary commercial retrieval tool for 3 decades
many professional searchers (e.g., lawyers) still like boolean
queries.

you know exactly what you are getting.

many search systems you use are also boolean: spotlight,
email, intranet etc.

31 / 60

commercially successful boolean retrieval: westlaw

largest commercial legal search service in terms of the
number of paying subscribers

over half a million subscribers performing millions of searches
a day over tens of terabytes of text data

the service was started in 1975.

in 2005, boolean search (called    terms and connectors    by
westlaw) was still the default, and used by a large percentage
of users . . .

. . . although ranked retrieval has been available since 1992.

32 / 60

westlaw: example queries

information need: information on the legal theories involved in
preventing the disclosure of trade secrets by employees formerly
employed by a competing company query:    trade secret    /s
disclos! /s prevent /s employe!

information need: requirements

for disabled people to be able to access a workplace query: disab!
/p access! /s work-site work-place (employment /3 place)

information need: cases about a host   s responsibility for drunk
guests query: host! /p (responsib! liab!) /p (intoxicat! drunk!)
/p guest

33 / 60

westlaw: comments

proximity operators: /3 = within 3 words, /s = within a
sentence, /p = within a paragraph

space is disjunction, not conjunction! (this was the default in
search pre-google.)

long, precise queries: incrementally developed, not like web
search

why professional searchers often like boolean search:
precision, transparency, control

when are boolean queries the best way of searching? depends
on: information need, searcher, document collection, . . .

34 / 60

outline

1

2

introduction

inverted index

3 processing boolean queries

4 query optimization

5 course overview

35 / 60

query optimization

consider a query that is an and of n terms, n > 2

for each of the terms, get its postings list, then and them
together

example query: brutus and calpurnia and caesar

what is the best order for processing this query?

36 / 60

query optimization

example query: brutus and calpurnia and caesar

simple and e   ective optimization: process in order of
increasing frequency

start with the shortest postings list, then keep cutting further

in this example,    rst caesar, then calpurnia, then
brutus

brutus

       1     2     4     11     31     45     173     174

calpurnia        2     31     54     101

caesar

       5     31

37 / 60

optimized intersection algorithm for conjunctive queries

intersect(ht1, . . . , tni)
1 terms     sortbyincreasingfrequency(ht1, . . . , tni)
2 result     postings(   rst(terms))
3 terms     rest(terms)
4 while terms 6= nil and result 6= nil
5 do result     intersect(result , postings(   rst(terms)))
6
7 return result

terms     rest(terms)

38 / 60

more general optimization

example query: (madding or crowd) and (ignoble or
strife)

get frequencies for all terms

estimate the size of each or by the sum of its frequencies
(conservative)

process in increasing order of or sizes

39 / 60

outline

1

2

introduction

inverted index

3 processing boolean queries

4 query optimization

5 course overview

40 / 60

course overview

we are done with chapter 1 of iir (iir 01).

plan for the rest of the semester: 18   20 of the 21 chapters of
iir

in what follows: teasers for most chapters     to give you a
sense of what will be covered.

41 / 60

iir 02: the term vocabulary and postings lists

phrase queries:    stanford university   

proximity queries: gates near microsoft

we need an index that captures position information for
phrase queries and proximity queries.

42 / 60

iir 03: dictionaries and tolerant retrieval

bo

or

rd

   

aboard

   

about

   

boardroom

   

border

   

border

   

lord

   

morbid

   

sordid

   

aboard

   

ardent

   

boardroom

   

border

43 / 60

iir 04: index construction

splits

assign

master

assign

postings

parser

a-f g-p q-z

inve rter

a-f

parser

a-f g-p q-z

inve rter

g-p

parser

a-f

g-p q-z

inve rter

q-z

map
phase

segment
files

reduce
phase

44 / 60

iir 05: index compression

f
c
 
0
1
g
o

l

7

6

5

4

3

2

1

0

0

1

2

3

4

5

6

7

log10 rank

zipf   s law

45 / 60

iir 06: scoring, term weighting and the vector space
model

ranking search results

boolean queries only give inclusion or exclusion of documents.
for ranked retrieval, we measure the proximity between the query and
each document.
one formalism for doing this: the vector space model

key challenge in ranked retrieval: evidence accumulation for a term in
a document

1 vs. 0 occurence of a query term in the document
3 vs. 2 occurences of a query term in the document
usually: more is better
but by how much?
need a scoring function that translates frequency into score or weight

46 / 60

iir 07: scoring in a complete search system

parsing 
linguistics

user query

documents

free text query parser

results 

page

document 
cache

indexers

spell correction scoring and ranking

metadata in 

zone and 
field indexes

inexact 
top k 
retrieval

tiered inverted 
positional index

k-gram

indexes

scoring 

parameters

mlr

training 

set

47 / 60

iir 08: evaluation and dynamic summaries

48 / 60

iir 09: relevance feedback & id183

49 / 60

iir 12: language models

q1

p(w |q1) w

w
stop 0.2
0.2
the
0.1
a
frog
0.01

p(w |q1)
0.01
0.03
0.02
0.04
. . .

this

toad
said
likes
that
. . .

is a one-state probabilistic    nite-state automaton     a unigram
language model     and the state emission distribution for its one
state q1. stop is not a word, but a special symbol indicating that

the automaton stops. frog said that toad likes frog stop

p(string) = 0.01   0.03   0.04   0.01   0.02   0.01   0.2

= 0.0000000000048

50 / 60

iir 13: text classi   cation & naive bayes

text classi   cation = assigning documents automatically to
prede   ned classes
examples:

language (english vs. french)
adult content
region

51 / 60

iir 14: vector classi   cation

x

x

x

x

x

x

x

x

   

x

x

x

52 / 60

iir 15: support vector machines

53 / 60

iir 16: flat id91

54 / 60

iir 17: hierarchical id91

http://news.google.com

55 / 60

iir 18: id45

56 / 60

iir 19: the web and its challenges

unusual and diverse documents

unusual and diverse users and information needs

beyond terms and text: exploit link analysis, user data

how do web search engines work?

how can we make them better?

57 / 60

iir 21: link analysis / id95

58 / 60

take-away

boolean retrieval: design and data structures of a simple
information retrieval system

what topics will be covered in this class?

59 / 60

resources

chapter 1 of iir
http://cislmu.org

course schedule
information retrieval links
shakespeare search engine

60 / 60

