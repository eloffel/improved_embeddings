cs6200	
   

informa.on	
   retrieval	
   

david	
   smith	
   

college	
   of	
   computer	
   and	
   informa.on	
   science	
   

northeastern	
   university	
   

evalua.on	
   

ir	
   systems	
   oaen	
   components	
   of	
   larger	
   search	
   engines	
   or	
   
other	
   user	
   applica.on	
   

      
       might	
   evaluate	
   several	
   aspects	
   
       assistant	
   in	
   formula.ng	
   queries	
   
       speed	
   of	
   retrieval	
   
       resources	
   required	
   
       presenta.on	
   of	
   documents	
   
       ability	
   to	
      nd	
   relevant	
   documents	
   
       appeal	
   to	
   users	
   (market	
   evalua.on)	
   
       evalua.on	
   generally	
   compara.ve	
   

       system	
   a	
   vs.	
   b	
   
       but	
   could	
   be	
   absolute:	
   response	
   .me	
   <	
   1s	
   
       possible	
   improvements	
   could	
   have	
   costs	
   

evalua.on	
   

       evalua.on	
   is	
   key	
   to	
   building	
   e   ec$ve	
   and	
   

e   cient	
   search	
   engines	
   
      measurement	
   usually	
   carried	
   out	
   in	
   controlled	
   
laboratory	
   experiments	
   
      online	
   tes.ng	
   can	
   also	
   be	
   done	
   

       e   ciency	
   measures	
   similar	
   to	
   database	
   

systems	
   
      e.g.,	
   indexing	
   .me,	
   query	
   throughput,	
   index	
   size	
   

       our	
   focus	
   is	
   on	
   e   ec$veness	
   metrics	
   

test	
   collec.ons	
   

       compare	
   retrieval	
   performance	
   using	
   a	
   test	
   collec$on	
   

       set	
   of	
   documents	
   
       set	
   of	
   queries	
   
       set	
   of	
   relevance	
   judgments	
   

       to	
   compare	
   the	
   performance	
   of	
   two	
   techniques	
   

       each	
   technique	
   used	
   to	
   evaluate	
   test	
   queries	
   
       results	
   (set	
   or	
   ranked	
   list)	
   compared	
   using	
   some	
   

performance	
   measure	
   

       usually	
   use	
   mul.ple	
   measures	
   to	
   get	
   di   erent	
   views	
   of	
   
       usually	
   test	
   with	
   mul.ple	
   collec.ons	
   

performance	
   

       performance	
   can	
   vary	
   widely	
   with	
   collec.on	
   

test	
   collec.ons	
   

       examples	
   of	
   historically	
   important	
   collec.ons	
   

test	
   collec.ons	
   

trec	
   topic	
   example	
   

relevance	
   

       di   cult	
   to	
   de   ne	
   
       relevant	
   document:	
   judged	
      useful   	
   in	
   the	
   context	
   of	
   a	
   

query	
   
       by	
   whom?	
   
       humans	
   not	
   very	
   consistent	
   
       judgments	
   depend	
   on	
   more	
   than	
   doc+query	
   

documents	
   

       with	
   real	
   collec.ons,	
   never	
   know	
   full	
   set	
   of	
   relevant	
   
       retrieval	
   model	
   incorporates	
   some	
   no.on	
   of	
   relevance	
   
      

individual	
   humans	
   disagree	
   but	
   (hopefully)	
   converge	
   
on	
   average,	
   in	
   the	
   limit	
   

relevance	
   judgments	
   

       obtaining	
   relevance	
   judgments	
   is	
   an	
   expensive,	
   
       sources	
   of	
   varia.on	
   similar	
   to	
   psychometrics,	
   

.me-     consuming	
   process	
   

opinion	
   polling,	
   etc.	
   
       who	
   does	
   it?	
   
       what	
   are	
   the	
   instruc.ons?	
   
       what	
   is	
   the	
   level	
   of	
   agreement?	
   

       trec	
   judgments	
   
       depend	
   on	
   task	
   being	
   evaluated	
   
       generally	
   binary	
   
       agreement	
   good	
   because	
   of	
      narra.ve   	
   

trec	
   
       text	
   retrieval	
   conference	
   
       established	
   in	
   1992	
   to	
   evaluate	
   large-     scale	
   ir	
   	
   
       retrieving	
   documents	
   from	
   a	
   gigabyte	
   collec.on	
   
       run	
   by	
   nist   s	
   informa.on	
   access	
   division	
   
       ini.ally	
   sponsored	
   by	
   darpa	
   as	
   part	
   of	
   tipster	
   
       now	
   supported	
   by	
   many,	
   including	
   darpa,	
   arda,	
   and	
   

program	
   

       probably	
   most	
   well	
   known	
   ir	
   evalua.on	
   secng	
   
       proceedings	
   available	
   on-     line	
   (hdp://

nist	
   

trec.nist.gov)	
   	
   

pooling	
   

       exhaus.ve	
   judgments	
   for	
   all	
   documents	
   in	
   a	
   
       pooling	
   technique	
   is	
   used	
   in	
   trec	
   

collec.on	
   is	
   not	
   prac.cal	
   

       top	
   k	
   results	
   (for	
   trec,	
   k	
   varied	
   between	
   50	
   and	
   200)	
   

from	
   the	
   rankings	
   obtained	
   by	
   di   erent	
   search	
   
engines	
   (or	
   retrieval	
   algorithms)	
   are	
   merged	
   into	
   a	
   
pool	
   

       duplicates	
   are	
   removed	
   
       documents	
   are	
   presented	
   in	
   some	
   random	
   order	
   to	
   
       produces	
   a	
   large	
   number	
   of	
   relevance	
   judgments	
   

the	
   relevance	
   judges	
   

for	
   each	
   query,	
   although	
   s.ll	
   incomplete	
   

query	
   logs	
   

       used	
   for	
   both	
   tuning	
   and	
   evalua.ng	
   search	
   

engines	
   
       also	
   for	
   various	
   techniques	
   such	
   as	
   query	
   sugges.on	
   

       typical	
   contents	
   

       user	
   iden.   er	
   or	
   user	
   session	
   iden.   er	
   
       query	
   terms	
   -     	
   stored	
   exactly	
   as	
   user	
   entered	
   
       list	
   of	
   urls	
   of	
   results,	
   their	
   ranks	
   on	
   the	
   result	
   list,	
   

and	
   whether	
   they	
   were	
   clicked	
   on	
   

       timestamp(s)	
   -     	
   records	
   the	
   .me	
   of	
   user	
   events	
   such	
   

as	
   query	
   submission,	
   clicks	
   

query	
   logs	
   

       clicks	
   are	
   not	
   relevance	
   judgments	
   

      although	
   they	
   are	
   correlated	
   
      biased	
   by	
   a	
   number	
   of	
   factors	
   such	
   as	
   rank	
   on	
   
result	
   list	
   

       can	
   use	
   clickthough	
   data	
   to	
   predict	
   

preferences	
   between	
   pairs	
   of	
   documents	
   
      appropriate	
   for	
   tasks	
   with	
   mul.ple	
   levels	
   of	
   
relevance,	
   focused	
   on	
   user	
   relevance	
   
      various	
      policies   	
   used	
   to	
   generate	
   preferences	
   

example	
   click	
   policy	
   

       skip	
   above	
   and	
   skip	
   next	
   

      click	
   data	
   

      generated	
   preferences	
   

query	
   logs	
   

       click	
   data	
   can	
   also	
   be	
   aggregated	
   to	
   remove	
   

noise	
   

       click	
   distribu$on	
   informa.on	
   

      can	
   be	
   used	
   to	
   iden.fy	
   clicks	
   that	
   have	
   a	
   higher	
   
frequency	
   than	
   would	
   be	
   expected	
   
      high	
   correla.on	
   with	
   relevance	
   
      e.g.,	
   using	
   click	
   devia$on	
   to	
      lter	
   clicks	
   for	
   
preference-     genera.on	
   policies	
   

filtering	
   clicks	
   

       click	
   devia$on	
   cd(d,	
   p)	
   for	
   a	
   result	
   d	
   in	
   

posi.on	
   p:	
   

o(d,p):	
   observed	
   click	
   frequency	
   for	
   a	
   document	
   in	
   a	
   
rank	
   posi.on	
   p	
   over	
   all	
   instances	
   of	
   a	
   given	
   query	
   

e(p):	
   expected	
   click	
   frequency	
   at	
   rank	
   p	
   averaged	
   

across	
   all	
   queries	
   

e   ec.veness	
   measures	
   

a	
   is	
   set	
   of	
   relevant	
   documents,	
   	
   
b	
   is	
   set	
   of	
   retrieved	
   documents	
   

classi   ca.on	
   errors	
   

       false	
   posi$ve	
   (type	
   i	
   error)	
   

      a	
   non-     relevant	
   document	
   is	
   retrieved	
   

       false	
   nega$ve	
   (type	
   ii	
   error)	
   

      a	
   relevant	
   document	
   is	
   not	
   retrieved	
   
      1-     	
   recall	
   

       precision	
   is	
   used	
   when	
   id203	
   that	
   a	
   

posi.ve	
   result	
   is	
   correct	
   is	
   important	
   

f	
   measure	
   

       harmonic	
   mean	
   of	
   recall	
   and	
   precision	
   

propor.ons	
   

       generally	
   used	
   when	
   averaging	
   probabili.es	
   and	
   
       harmonic	
   mean	
   emphasizes	
   the	
   importance	
   of	
   small	
   
values,	
   whereas	
   the	
   arithme.c	
   mean	
   is	
   a   ected	
   more	
   
by	
   outliers	
   that	
   are	
   unusually	
   large	
   

       more	
   general	
   form	
   

         	
   is	
   a	
   parameter	
   that	
   determines	
   rela.ve	
   importance	
   

of	
   recall	
   and	
   precision	
   

ranking	
   e   ec.veness	
   

summarizing	
   a	
   ranking	
   

       calcula.ng	
   recall	
   and	
   precision	
   at	
      xed	
   rank	
   

posi.ons	
   

       calcula.ng	
   precision	
   at	
   standard	
   recall	
   levels,	
   

from	
   0.0	
   to	
   1.0	
   
      requires	
   interpola$on	
   

       averaging	
   the	
   precision	
   values	
   from	
   the	
   rank	
   

posi.ons	
   where	
   a	
   relevant	
   document	
   was	
   
retrieved	
   

average	
   precision	
   

averaging	
   across	
   queries	
   

averaging	
   
       mean	
   average	
   precision	
   (map)	
   

      summarize	
   rankings	
   from	
   mul.ple	
   queries	
   by	
   
averaging	
   average	
   precision	
   
      most	
   commonly	
   used	
   measure	
   in	
   research	
   papers	
   
      assumes	
   user	
   is	
   interested	
   in	
      nding	
   many	
   
relevant	
   documents	
   for	
   each	
   query	
   
      requires	
   many	
   relevance	
   judgments	
   in	
   text	
   
collec.on	
   
       recall-     precision	
   graphs	
   are	
   also	
   useful	
   

summaries	
   

map	
   

recall-     precision	
   graph	
   

interpola.on	
   

       to	
   average	
   graphs,	
   calculate	
   precision	
   at	
   

standard	
   recall	
   levels:	
   

      where	
   s	
   is	
   the	
   set	
   of	
   observed	
   (r,p)	
   points	
   

       de   nes	
   precision	
   at	
   any	
   recall	
   level	
   as	
   the	
   
maximum	
   precision	
   observed	
   in	
   any	
   recall-     
precision	
   point	
   at	
   a	
   higher	
   recall	
   level	
   
      produces	
   a	
   step	
   func.on	
   
      de   nes	
   precision	
   at	
   recall	
   0.0	
   

interpola.on	
   

average	
   precision	
   at	
   	
   
standard	
   recall	
   levels	
   

      	
   	
   recall-     precision	
   graph	
   ploded	
   by	
   simply	
   	
   
	
   	
   	
   	
   joining	
   the	
   average	
   precision	
   points	
   at	
   	
   
	
   	
   	
   	
   the	
   standard	
   recall	
   levels	
   

average	
   recall-     precision	
   graph	
   

graph	
   for	
   50	
   queries	
   

focusing	
   on	
   top	
   documents	
   

       users	
   tend	
   to	
   look	
   at	
   only	
   the	
   top	
   part	
   of	
   the	
   
ranked	
   result	
   list	
   to	
      nd	
   relevant	
   documents	
   

       some	
   search	
   tasks	
   have	
   only	
   one	
   relevant	
   

document	
   
      e.g.,	
   naviga.onal	
   search,	
   ques.on	
   answering	
   

       recall	
   not	
   appropriate	
   

      instead	
   need	
   to	
   measure	
   how	
   well	
   the	
   search	
   
engine	
   does	
   at	
   retrieving	
   relevant	
   documents	
   at	
   
very	
   high	
   ranks	
   

focusing	
   on	
   top	
   documents	
   

       precision	
   at	
   rank	
   r	
   
      r	
   typically	
   5,	
   10,	
   20	
   
      easy	
   to	
   compute,	
   average,	
   understand	
   
      not	
   sensi.ve	
   to	
   rank	
   posi.ons	
   less	
   than	
   r	
   

       reciprocal	
   rank	
   

      reciprocal	
   of	
   the	
   rank	
   at	
   which	
   the	
      rst	
   relevant	
   
document	
   is	
   retrieved	
   
      mean	
   reciprocal	
   rank	
   (mrr)	
   is	
   the	
   average	
   of	
   the	
   
reciprocal	
   ranks	
   over	
   a	
   set	
   of	
   queries	
   
      very	
   sensi.ve	
   to	
   rank	
   posi.on	
   

discounted	
   cumula.ve	
   gain	
   
       popular	
   measure	
   for	
   evalua.ng	
   web	
   search	
   

and	
   related	
   tasks	
   
       two	
   assump.ons:	
   

      highly	
   relevant	
   documents	
   are	
   more	
   useful	
   than	
   
marginally	
   relevant	
   document	
   
      the	
   lower	
   the	
   ranked	
   posi.on	
   of	
   a	
   relevant	
   
document,	
   the	
   less	
   useful	
   it	
   is	
   for	
   the	
   user,	
   since	
   it	
   
is	
   less	
   likely	
   to	
   be	
   examined	
   

discounted	
   cumula.ve	
   gain	
   
       uses	
   graded	
   relevance	
   as	
   a	
   measure	
   of	
   the	
   

usefulness,	
   or	
   gain,	
   from	
   examining	
   a	
   
document	
   

       gain	
   is	
   accumulated	
   star.ng	
   at	
   the	
   top	
   of	
   the	
   
ranking	
   and	
   may	
   be	
   reduced,	
   or	
   discounted,	
   at	
   
lower	
   ranks	
   

       typical	
   discount	
   is	
   1/log	
   (rank)	
   

      with	
   base	
   2,	
   the	
   discount	
   at	
   rank	
   4	
   is	
   1/2,	
   and	
   at	
   
rank	
   8	
   it	
   is	
   1/3	
   

discounted	
   cumula.ve	
   gain	
   

       dcg	
   is	
   the	
   total	
   gain	
   accumulated	
   at	
   a	
   

par.cular	
   rank	
   p:	
   

       alterna.ve	
   formula.on:	
   

      used	
   by	
   some	
   web	
   search	
   companies	
   
      emphasis	
   on	
   retrieving	
   highly	
   relevant	
   documents	
   

dcg	
   example	
   

       10	
   ranked	
   documents	
   judged	
   on	
   0-     3	
   relevance	
   

scale:	
   	
   
3,	
   2,	
   3,	
   0,	
   0,	
   1,	
   2,	
   2,	
   3,	
   0	
   

       discounted	
   gain:	
   	
   

3,	
   2/1,	
   3/1.59,	
   0,	
   0,	
   1/2.59,	
   2/2.81,	
   2/3,	
   3/3.17,	
   0	
   	
   
=	
   3,	
   2,	
   1.89,	
   0,	
   0,	
   0.39,	
   0.71,	
   0.67,	
   0.95,	
   0	
   

       dcg:	
   

3,	
   5,	
   6.89,	
   6.89,	
   6.89,	
   7.28,	
   7.99,	
   8.66,	
   9.61,	
   9.61	
   

	
   

normalized	
   dcg	
   

       dcg	
   numbers	
   are	
   averaged	
   across	
   a	
   set	
   of	
   

queries	
   at	
   speci   c	
   rank	
   values	
   
      e.g.,	
   dcg	
   at	
   rank	
   5	
   is	
   6.89	
   and	
   at	
   rank	
   10	
   is	
   9.61	
   

       dcg	
   values	
   are	
   oaen	
   normalized	
   by	
   

comparing	
   the	
   dcg	
   at	
   each	
   rank	
   with	
   the	
   dcg	
   
value	
   for	
   the	
   perfect	
   ranking	
   
      makes	
   averaging	
   easier	
   for	
   queries	
   with	
   di   erent	
   
numbers	
   of	
   relevant	
   documents	
   

ndcg	
   example	
   

       perfect	
   ranking:	
   

3,	
   3,	
   3,	
   2,	
   2,	
   2,	
   1,	
   0,	
   0,	
   0	
   

       ideal	
   dcg	
   values:	
   

3,	
   6,	
   7.89,	
   8.89,	
   9.75,	
   10.52,	
   10.88,	
   10.88,	
   10.88,	
   10	
   

       ndcg	
   values	
   (divide	
   actual	
   by	
   ideal):	
   

1,	
   0.83,	
   0.87,	
   0.76,	
   0.71,	
   0.69,	
   0.73,	
   0.8,	
   0.88,	
   0.88	
   
      ndcg	
      	
   1	
   at	
   any	
   rank	
   posi.on	
   

using	
   preferences	
   

       two	
   rankings	
   described	
   using	
   preferences	
   can	
   
be	
   compared	
   using	
   the	
   kendall	
   tau	
   coe   cient	
   
(  	
   ):	
   

      p	
   is	
   the	
   number	
   of	
   preferences	
   that	
   agree	
   and	
   q	
   is	
   
the	
   number	
   that	
   disagree	
   

       for	
   preferences	
   derived	
   from	
   binary	
   relevance	
   

judgments,	
   can	
   use	
   bpref	
   

bpref	
   

       for	
   a	
   query	
   with	
   r	
   relevant	
   documents,	
   only	
   

the	
      rst	
   r	
   non-     relevant	
   documents	
   are	
   
considered	
   

      dr	
   is	
   a	
   relevant	
   document,	
   and	
   ndr	
   gives	
   the	
   
number	
   of	
   non-     relevant	
   documents	
   

       alterna.ve	
   de   ni.on	
   

e   ciency	
   metrics	
   

hypothesis	
   tes.ng	
   

       one-     sample	
   

       two-     sample	
   

          is	
   the	
   system   s	
   response	
   .me	
   under	
   1	
   sec.?   	
   

          does	
   the	
   system	
   perform	
   equally	
   well	
   on	
   these	
   
two	
   types	
   of	
   queries?   	
   

       paired-     sample	
   

          is	
   this	
   system	
   a	
   beder	
   than	
   system	
   b?   	
   

terminological	
   prelude	
   

       popula.ons	
   

       popula.on	
   distribu.ons	
   
          performance	
   on	
   all	
   queries   .	
   how	
   big?	
   

       samples	
   

       sta.s.cs	
   

       sampling	
   distribu.ons	
   
          performance	
   on	
   this	
   test	
   set   	
   

       func.ons	
   of	
   data	
   
          what	
   is	
   the	
   map?	
   p@10?	
   ndcg?   	
   

       models	
   

       parameters	
   

signi   cance	
   tests	
   

       given	
   the	
   results	
   from	
   a	
   number	
   of	
   queries,	
   

how	
   can	
   we	
   conclude	
   that	
   ranking	
   algorithm	
   a	
   
is	
   beder	
   than	
   algorithm	
   b?	
   
       a	
   signi   cance	
   test	
   enables	
   us	
   to	
   reject	
   the	
   null	
   

hypothesis	
   (no	
   di   erence)	
   in	
   favor	
   of	
   the	
   
alterna$ve	
   hypothesis	
   (b	
   is	
   beder	
   than	
   a)	
   
      the	
   power	
   of	
   a	
   test	
   is	
   the	
   id203	
   that	
   the	
   test	
   
will	
   reject	
   the	
   null	
   hypothesis	
   correctly	
   
      increasing	
   the	
   number	
   of	
   queries	
   in	
   the	
   
experiment	
   also	
   increases	
   power	
   of	
   test	
   

signi   cance	
   tests	
   

one-     sided	
   test	
   

       distribu.on	
   for	
   the	
   possible	
   values	
   of	
   a	
   test	
   

sta.s.c	
   assuming	
   the	
   null	
   hypothesis	
   

       shaded	
   area	
   is	
   region	
   of	
   rejec$on	
   

example	
   experimental	
   results	
   

t-     test	
   

       assump.on	
   is	
   that	
   the	
   di   erence	
   between	
   the	
   
e   ec.veness	
   values	
   is	
   a	
   sample	
   from	
   a	
   normal	
   
distribu.on	
   

       null	
   hypothesis	
   is	
   that	
   the	
   mean	
   of	
   the	
   

distribu.on	
   of	
   di   erences	
   is	
   zero	
   

       test	
   sta.s.c	
   

      for	
   the	
   example,	
   

wilcoxon	
   signed-     ranks	
   test	
   
       nonparametric	
   test	
   based	
   on	
   di   erences	
   

between	
   e   ec.veness	
   scores	
   

       test	
   sta.s.c	
   

      to	
   compute	
   the	
   signed-     ranks,	
   the	
   di   erences	
   are	
   
ordered	
   by	
   their	
   absolute	
   values	
   (increasing),	
   and	
   
then	
   assigned	
   rank	
   values	
   
      rank	
   values	
   are	
   then	
   given	
   the	
   sign	
   of	
   the	
   original	
   
di   erence	
   

wilcoxon	
   example	
   

       9	
   non-     zero	
   di   erences	
   are	
   (in	
   rank	
   order	
   of	
   

absolute	
   value):	
   
	
   2,	
   9,	
   10,	
   24,	
   25,	
   25,	
   41,	
   60,	
   70	
   

       signed-     ranks:	
   

-     1,	
   +2,	
   +3,	
   -     4,	
   +5.5,	
   +5.5,	
   +7,	
   +8,	
   +9	
   

       w	
   =	
   35,	
   p-     value	
   =	
   0.025	
   

nota.on	
   

       p	
   is	
   a	
   popula.on	
   
       s	
   =	
   [s1,	
   s2,	
   ...,	
   sn]	
   is	
   a	
   sample	
   from	
   p	
   
       let	
   x	
   =	
   [x1,	
   x2,	
   ...,	
   xn]	
   be	
   some	
   numerical	
   
measurement	
   on	
   the	
   si	
   
      distributed	
   over	
   p	
   according	
   to	
   unknown	
   f	
   

       we	
   may	
   use	
   y,	
   z	
   for	
   other	
   measurements.	
   
	
   

plug-     in	
   principle	
   

       we	
   don   t	
   have	
   (and	
   can   t	
   get)	
   p	
   

      we	
   don   t	
   know	
   f,	
   the	
   true	
   distribu.on	
   over	
   x	
   

       we	
   do	
   have	
   s	
   (the	
   sample)	
   

      we	
   do	
   know	
   	
   	
   ,	
   the	
   sample	
   distribu.on	
   over	
   x	
   

f  

       es.ma.ng	
   a	
   sta.s.c:	
   	
   use	
   	
   	
   	
   for	
   f	
   

f  

the	
   bootstrap	
   

       simulates	
   the	
   sampling	
   distribu.on	
   
       proposed	
   by	
   efron	
   in	
   1979	
   

      an.cipated	
   by	
   permuta.on	
   tests,	
   jackknife,	
   cross-     
valida.on	
   
       from	
   original	
   sample	
   of	
   size	
   n,	
   draw	
   b	
   samples	
   

of	
   size	
   n	
   with	
   replacement	
   and	
   calculate	
   the	
   
sta.s.c	
   on	
   each	
   

bootstrap	
   world	
   

unknown distribution f 

observed random sample x 

statistic of interest 

)x(s   =  

empirical distribution 

f  

bootstrap random sample x* 

bootstrap replication 

*)x(s*   =  

statistics about the estimate (e.g., standard error) 

bootstrap	
   sample	
   

       x	
   =	
   [3.0,	
   2.8,	
   3.7,	
   3.4,	
   3.5]	
   
       x*	
   could	
   be:	
   

      [2.8,	
   3.4,	
   3.7,	
   3.4,	
   3.5]	
   
      [3.5,	
   3.0,	
   3.4,	
   2.8,	
   3.7]	
   
      [3.5,	
   3.5,	
   3.4,	
   3.0,	
   2.8]	
   
      ...	
   

draw	
   n	
   elements	
   with	
   replacement.	
   

re   ec.on	
   

       imagine	
   doing	
   this	
   with	
   a	
   pencil	
   and	
   paper.	
   
       the	
   bootstrap	
   was	
   born	
   in	
   1979.	
   
       typically,	
   sampling	
   is	
   costly	
   and	
   computa.on	
   

is	
   cheap.	
   

       in	
   (empirical)	
   cs,	
   sampling	
   isn   t	
   even	
   

necessarily	
   all	
   that	
   costly.	
   

paired-     sample	
   permuta.on	
   test	
   
          is	
   ranking	
   algorithm	
   f	
   beder	
   than	
   g?   	
   
       match	
   the	
   ranked	
   list	
   for	
   each	
   query	
   
       swap	
   the	
   lists	
   in	
   each	
   pair	
   with	
   0.5	
   id203	
   
       replicate	
   this	
   process	
   b=100s	
   of	
   .mes	
   
       calculate	
   the	
   mean	
   di   erence	
   in	
   metric	
   (map,	
   

ndgc,	
   p@10,	
   etc.)	
   for	
   each	
   replicate	
   

       p-     value:	
   #{  *(b)	
   >=	
     }/b	
   

a	
   two-     sample	
   bootstrap	
   test	
   

          is	
   this	
   system	
   beder	
   at	
   long	
   than	
   short	
   
queries?   	
   
       h0:	
   equality	
   of	
   two	
   distribu.ons	
   f	
   and	
   g,	
   
samples	
   z	
   and	
   y	
   or	
   size	
   n	
   and	
   m	
   
       test	
   sta.s.c	
     	
   =	
   z	
      	
   y	
   
       pool	
   samples	
   into	
   x;	
   draw	
   b	
   samples	
   with	
   

replacement;	
   call	
      rst	
   n	
   in	
   resample	
   
observa.ons	
   from	
   f	
   

       p-     value:	
   #{  *(b)	
   >=	
     }/b	
   

sign	
   test	
   

       ignores	
   magnitude	
   of	
   di   erences	
   
       null	
   hypothesis	
   for	
   this	
   test	
   is	
   that	
   

      p(b	
   >	
   a)	
   =	
   p(a	
   >	
   b)	
   =	
     	
   
      number	
   of	
   pairs	
   where	
   b	
   is	
      beder   	
   than	
   a	
   would	
   
be	
   the	
   same	
   as	
   the	
   number	
   of	
   pairs	
   where	
   a	
   is	
   
   beder   	
   than	
   b	
   
       test	
   sta.s.c	
   is	
   number	
   of	
   pairs	
   where	
   b>a	
   
       for	
   example	
   data,	
   	
   

      test	
   sta.s.c	
   is	
   7,	
   p-     value	
   =	
   0.17	
   
      cannot	
   reject	
   null	
   hypothesis	
   

secng	
   parameter	
   values	
   

       retrieval	
   models	
   oaen	
   contain	
   parameters	
   
that	
   must	
   be	
   tuned	
   to	
   get	
   best	
   performance	
   
for	
   speci   c	
   types	
   of	
   data	
   and	
   queries	
   

       for	
   experiments:	
   

      use	
   training	
   and	
   test	
   data	
   sets	
   
      if	
   less	
   data	
   available,	
   use	
   cross-     valida$on	
   by	
   
par..oning	
   the	
   data	
   into	
   k	
   subsets	
   
      using	
   training	
   and	
   test	
   data	
   avoids	
   over   zng	
      	
   
when	
   parameter	
   values	
   do	
   not	
   generalize	
   well	
   to	
   
other	
   data	
   

finding	
   parameter	
   values	
   

       many	
   techniques	
   used	
   to	
      nd	
   op.mal	
   
parameter	
   values	
   given	
   training	
   data	
   
      standard	
   problem	
   in	
   machine	
   learning	
   

       in	
   ir,	
   oaen	
   explore	
   the	
   space	
   of	
   possible	
   

parameter	
   values	
   by	
   brute	
   force	
   
      requires	
   large	
   number	
   of	
   retrieval	
   runs	
   with	
   small	
   
varia.ons	
   in	
   parameter	
   values	
   (parameter	
   sweep)	
   

       learning	
   to	
   rank	
   techniques	
   are	
   e   cient	
   

procedures	
   for	
      nding	
   good	
   parameter	
   values	
   
with	
   large	
   numbers	
   of	
   parameters	
   

online	
   tes.ng	
   

       test	
   (or	
   even	
   train)	
   using	
   live	
   tra   c	
   on	
   a	
   

search	
   engine	
   

       bene   ts:	
   

       drawbacks:	
   

      real	
   users,	
   less	
   biased,	
   large	
   amounts	
   of	
   test	
   data	
   

      noisy	
   data,	
   can	
   degrade	
   user	
   experience	
   

       oaen	
   done	
   on	
   small	
   propor.on	
   (1-     5%)	
   of	
   live	
   

tra   c	
   

summary	
   

       no	
   single	
   measure	
   is	
   the	
   correct	
   one	
   for	
   any	
   

applica.on	
   
      choose	
   measures	
   appropriate	
   for	
   task	
   
      use	
   a	
   combina.on	
   
      shows	
   di   erent	
   aspects	
   of	
   the	
   system	
   
e   ec.veness	
   

       use	
   signi   cance	
   tests	
   
      t-     test,	
   permuta.on	
   test	
   

       analyze	
   performance	
   of	
   individual	
   queries	
   

query	
   summary	
   

