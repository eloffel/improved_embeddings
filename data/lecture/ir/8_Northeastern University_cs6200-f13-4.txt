cs6200	
   

informa.on	
   retrieval	
   

david	
   smith	
   

college	
   of	
   computer	
   and	
   informa.on	
   science	
   

northeastern	
   university	
   

indexing	
   process	
   

processing	
   text	
   

       conver.ng	
   documents	
   to	
   index	
   terms	
   
       why?	
   

      matching	
   the	
   exact	
   string	
   of	
   characters	
   typed	
   by	
   
the	
   user	
   is	
   too	
   restric.ve	
   
       i.e.,	
   it	
   doesn   t	
   work	
   very	
   well	
   in	
   terms	
   of	
   e   ec.veness	
   

      not	
   all	
   words	
   are	
   of	
   equal	
   value	
   in	
   a	
   search	
   
      some.mes	
   not	
   clear	
   where	
   words	
   begin	
   and	
   end	
   

       not	
   even	
   clear	
   what	
   a	
   word	
   is	
   in	
   some	
   languages	
   

       e.g.,	
   chinese,	
   korean	
   

text	
   sta.s.cs	
   

       huge	
   variety	
   of	
   words	
   used	
   in	
   text	
   but	
   
       many	
   sta.s.cal	
   characteris.cs	
   of	
   word	
   

occurrences	
   are	
   predictable	
   
      e.g.,	
   distribu.on	
   of	
   word	
   counts	
   

       retrieval	
   models	
   and	
   ranking	
   algorithms	
   
depend	
   heavily	
   on	
   sta.s.cal	
   proper.es	
   of	
   
words	
   
      e.g.,	
   important	
   words	
   occur	
   open	
   in	
   documents	
   
but	
   are	
   not	
   high	
   frequency	
   in	
   collec.on	
   

	
   

zipf   s	
   law	
   

       distribu.on	
   of	
   word	
   frequencies	
   is	
   very	
   skewed	
   

       a	
   few	
   words	
   occur	
   very	
   open,	
   many	
   words	
   hardly	
   ever	
   

occur	
   

       e.g.,	
   two	
   most	
   common	
   words	
   (   the   ,	
      of   )	
   make	
   up	
   
about	
   10%	
   of	
   all	
   word	
   occurrences	
   in	
   text	
   documents	
   

       zipf   s	
      law   	
   (more	
   generally,	
   a	
      power	
   law   ):	
   

       observa.on	
   that	
   rank	
   (r)	
   of	
   a	
   word	
   .mes	
   its	
   frequency	
   

(f)	
   is	
   approximately	
   a	
   constant	
   (k)	
   
       assuming	
   words	
   are	
   ranked	
   in	
   order	
   of	
   decreasing	
   frequency	
   
       i.e.,	
   	
   r.f	
      	
   k	
   or	
   	
   r.pr	
      	
   c,	
   where	
   pr	
   is	
   id203	
   of	
   word	
   
occurrence	
   and	
   c	
       0.1	
   for	
   english	
   

zipf   s	
   law	
   

news	
   collec.on	
   (ap89)	
   sta.s.cs	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   84,678	
   
total	
   documents	
   	
   
	
   	
   	
   	
   	
   	
   39,749,179	
   
total	
   word	
   occurrences	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   198,763	
   
vocabulary	
   size	
    	
   
words	
   occurring	
   >	
   1000	
   .mes	
   	
   	
   	
   	
   	
   	
   	
   	
   4,169	
   
words	
   occurring	
   once	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   70,064	
   

	
   	
   	
   	
   	
   	
   	
   freq.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   r	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   pr(%)	
   

word	
   
	
   r.pr	
   
assistant	
   	
   	
   	
   	
   5,095	
   	
   	
   	
   	
   	
   	
   	
   1,021	
   	
   	
   	
   	
   	
   	
   	
   	
   .013	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
    	
   0.13	
   
sewers	
    	
   	
   	
   	
   	
   	
   	
   	
   	
   100	
   	
   	
   	
   	
   	
   17,110	
   	
   	
   	
   2.56	
     	
   10   4	
   	
   	
   	
   	
   	
   0.04	
   
toothbrush	
   	
   	
   	
   	
   	
   10	
   	
   	
   	
   	
   	
   51,555	
   	
   	
   	
   2.56	
     	
   10   5	
   	
   	
   	
   	
   	
   0.01	
   
hazmat	
    	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   1	
   	
   	
   	
   166,945	
   	
   	
   	
   2.56	
     	
   10   6	
   	
   	
   	
   	
   	
   0.04	
   

top	
   50	
   words	
   from	
   ap89	
   

zipf   s	
   law	
   for	
   ap89	
   

       log-     log	
   plot:	
   note	
   problems	
   at	
   high	
   and	
   low	
   frequencies	
   

zipf   s	
   law	
   

       what	
   is	
   the	
   propor.on	
   of	
   words	
   with	
   a	
   given	
   

frequency?	
   
      word	
   that	
   occurs	
   n	
   .mes	
   has	
   rank	
   rn	
   =	
   k/n	
   
      number	
   of	
   words	
   with	
   frequency	
   n	
   is	
   

       rn	
      	
   rn+1	
   	
   =	
   	
   k/n	
      	
   k/(n	
   +	
   1)	
   	
   =	
   	
   k/n(n	
   +	
   1)	
   

      propor.on	
   found	
   by	
   dividing	
   by	
   total	
   number	
   of	
   
words	
   =	
   highest	
   rank	
   =	
   k	
   
      so,	
   propor.on	
   with	
   frequency	
   n	
   is	
   1/n(n+1)	
   

zipf   s	
   law	
   	
   

       example	
   word	
   	
   
	
   	
   	
   	
   frequency	
   ranking	
   

	
   
       to	
   compute	
   number	
   of	
   words	
   with	
   frequency	
   5,099	
   	
   

      rank	
   of	
      chemical   	
   minus	
   the	
   rank	
   of	
      summit   	
   
      1006	
      	
   1002	
   =	
   4	
   

example	
   

       propor.ons	
   of	
   words	
   occurring	
   n	
   .mes	
   in	
   

336,310	
   trec	
   documents	
   
       vocabulary	
   size	
   is	
   508,209	
   

vocabulary	
   growth	
   

       as	
   corpus	
   grows,	
   so	
   does	
   vocabulary	
   size	
   

      fewer	
   new	
   words	
   when	
   corpus	
   is	
   already	
   large	
   
	
   

       observed	
   rela.onship	
   (heaps   	
   law):	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   v	
   =	
   k.n  	
   
	
   	
   	
   	
   	
   	
   	
   where	
   v	
   is	
   vocabulary	
   size	
   (number	
   of	
   unique	
   words),	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   

	
   

	
   n	
   is	
   the	
   number	
   of	
   	
   words	
   in	
   corpus,	
   
	
   k,	
     	
   are	
   parameters	
   that	
   vary	
   for	
   each	
   corpus	
   	
   	
   
	
   (typical	
   values	
   given	
   are	
   10	
      	
   k	
      	
   100	
   and	
     	
      	
   0.5)	
   	
   	
   	
   	
   

ap89	
   example	
   

heaps   	
   law	
   predic.ons	
   

       predic.ons	
   for	
   trec	
   collec.ons	
   are	
   accurate	
   

for	
   large	
   numbers	
   of	
   words	
   
      e.g.,	
      rst	
   10,879,522	
   words	
   of	
   the	
   ap89	
   collec.on	
   
scanned	
   
      predic.on	
   is	
   100,151	
   unique	
   words	
   
      actual	
   number	
   is	
   100,024	
   

       predic.ons	
   for	
   small	
   numbers	
   of	
   words	
   (i.e.	
   	
   	
   	
   

<	
   1000)	
   are	
   much	
   worse	
   

gov2	
   (web)	
   example	
   

web	
   example	
   

       heaps   	
   law	
   works	
   with	
   very	
   large	
   corpora	
   

      new	
   words	
   occurring	
   even	
   aper	
   seeing	
   30	
   million!	
   
      parameter	
   values	
   di   erent	
   than	
   typical	
   trec	
   
values	
   

       new	
   words	
   come	
   from	
   a	
   variety	
   of	
   sources	
   

       spelling	
   errors,	
   invented	
   words	
   (e.g.	
   product,	
   company	
   
names),	
   code,	
   other	
   languages,	
   email	
   addresses,	
   etc.	
   
       search	
   engines	
   must	
   deal	
   with	
   these	
   large	
   and	
   

growing	
   vocabularies	
   

es.ma.ng	
   result	
   set	
   size	
   

       how	
   many	
   pages	
   contain	
   all	
   of	
   the	
   query	
   terms?	
   
       for	
   the	
   query	
      a	
   b	
   c   :	
   

	
   	
   fabc	
   =	
   n	
        	
   fa/n	
        	
   fb/n	
        	
   fc/n	
   =	
   (fa	
        	
   fb	
        	
   fc)/n2	
   
       assuming	
   that	
   terms	
   occur	
   independently	
   
       fabc	
   is	
   the	
   es.mated	
   size	
   of	
   the	
   result	
   set	
   	
   
       fa,	
   fb,	
   fc	
   are	
   the	
   number	
   of	
   documents	
   that	
   terms	
   a,	
   b,	
   and	
   c	
   
occur	
   in	
   
       n	
   is	
   the	
   number	
   of	
   documents	
   in	
   the	
   collec.on	
   

gov2	
   example	
   

collec.on	
   size	
   (n)	
   is	
   25,205,179	
   

result	
   set	
   size	
   es.ma.on	
   
       poor	
   es.mates	
   because	
   words	
   are	
   not	
   

independent	
   

       berer	
   es.mates	
   possible	
   if	
   co-     occurrence	
   

informa.on	
   available	
   
	
   p(a	
      	
   b	
      	
   c)	
   =	
   p(a	
      	
   b)	
        	
   p(c|(a	
      	
   b))	
   
	
   ftropical      sh   aquarium	
   =	
   ftropical   aquarium	
        	
   f   sh   aquarium/faquarium	
   	
   
	
   	
   =	
   1921	
        	
   9722/26480	
   =	
   705	
   
ftropical      sh   breeding	
   =	
   ftropical   breeding	
        	
   f   sh   breeeding/fbreeding	
   	
   
	
   	
   =	
   5510	
        	
   36427/81885	
   =	
   2451	
   

result	
   set	
   es.ma.on	
   

       even	
   berer	
   es.mates	
   using	
   ini.al	
   result	
   set	
   

      es.mate	
   is	
   simply	
   c/s	
   

       where	
   s	
   is	
   the	
   propor.on	
   of	
   the	
   total	
   documents	
   that	
   
have	
   been	
   ranked,	
   and	
   c	
   is	
   the	
   number	
   of	
   documents	
   
found	
   that	
   contain	
   all	
   the	
   query	
   words	
   
      e.g.,	
      tropical	
      sh	
   aquarium   	
   in	
   gov2	
   

       aper	
   processing	
   3,000	
   out	
   of	
   the	
   26,480	
   documents	
   
that	
   contain	
      aquarium   ,	
   c	
   =	
   258	
   

	
   	
   	
   ftropical      sh   aquarium	
   =	
   258/(3000  26480)	
   =	
   2,277	
   
       aper	
   processing	
   20%	
   of	
   the	
   documents,	
   	
   
	
   	
   ftropical      sh   aquarium	
   =	
   1,778	
   	
   	
   (1,529	
   is	
   real	
   value)	
   

es.ma.ng	
   collec.on	
   size	
   
       important	
   issue	
   for	
   web	
   search	
   engines	
   
       simple	
   technique:	
   use	
   independence	
   model	
   
      given	
   two	
   words	
   a	
   and	
   b	
   that	
   are	
   independent	
   
	
   	
   	
   	
   	
   	
   fab/n	
   =	
   fa/n	
        	
   fb/n	
   
	
   	
   	
   	
   	
   	
   	
   n	
   =	
   (fa	
        	
   fb)/fab	
   
	
   
      e.g.,	
   for	
   gov2	
   

	
   	
   flincoln	
   =	
   771,326	
   	
   ftropical	
   =	
   120,990	
   	
   flincoln	
      	
   tropical	
   =	
   3,018	
   
	
   n	
   =	
   (120990	
        	
   771326)/3018	
   =	
   30,922,045	
   
	
   	
   	
   	
   	
   (actual	
   number	
   is	
   25,205,179)	
   

tokenizing	
   

       forming	
   words	
   from	
   sequence	
   of	
   characters	
   
       surprisingly	
   complex	
   in	
   english,	
   can	
   be	
   harder	
   

in	
   other	
   languages	
   
       early	
   ir	
   systems:	
   

      any	
   sequence	
   of	
   alphanumeric	
   characters	
   of	
   
length	
   3	
   or	
   more	
   	
   
      terminated	
   by	
   a	
   space	
   or	
   other	
   special	
   character	
   
      upper-     case	
   changed	
   to	
   lower-     case	
   

tokenizing	
   

       example:	
   

         bigcorp's	
   2007	
   bi-     annual	
   report	
   showed	
   pro   ts	
   
rose	
   10%.   	
   becomes	
   
         bigcorp	
   2007	
   annual	
   report	
   showed	
   pro   ts	
   rose   	
   

       too	
   simple	
   for	
   search	
   applica.ons	
   or	
   even	
   

large-     scale	
   experiments	
   

       why?	
   too	
   much	
   informa.on	
   lost	
   

      small	
   decisions	
   in	
   tokenizing	
   can	
   have	
   major	
   
impact	
   on	
   e   ec.veness	
   of	
   some	
   queries	
   

tokenizing	
   problems	
   

usually	
   in	
   combina.ons	
   

       small	
   words	
   can	
   be	
   important	
   in	
   some	
   queries,	
   
       	
   xp,	
   ma,	
   pm,	
   ben	
   e	
   king,	
   el	
   paso,	
   master	
   p,	
   gm,	
   j	
   lo,	
   world	
   
war	
   ii	
   
       both	
   hyphenated	
   and	
   non-     hyphenated	
   forms	
   of	
   

       e-     bay,	
   wal-     mart,	
   ac.ve-     x,	
   cd-     rom,	
   t-     shirts	
   	
   

many	
   words	
   are	
   common	
   	
   
       some.mes	
   hyphen	
   is	
   not	
   needed	
   	
   
       at	
   other	
   .mes,	
   hyphens	
   should	
   be	
   considered	
   either	
   
as	
   part	
   of	
   the	
   word	
   or	
   a	
   word	
   separator	
   
       winston-     salem,	
   mazda	
   rx-     7,	
   e-     cards,	
   pre-     diabetes,	
   t-     mobile,	
   
spanish-     speaking	
   

tokenizing	
   problems	
   

       special	
   characters	
   are	
   an	
   important	
   part	
   of	
   tags,	
   
       capitalized	
   words	
   can	
   have	
   di   erent	
   meaning	
   

urls,	
   code	
   in	
   documents	
   

from	
   lower	
   case	
   words	
   
       bush,	
   	
   apple	
   

       apostrophes	
   can	
   be	
   a	
   part	
   of	
   a	
   word,	
   a	
   part	
   of	
   a	
   
possessive,	
   or	
   just	
   a	
   mistake	
   
       rosie	
   o'donnell,	
   can't,	
   don't,	
   80's,	
   1890's,	
   men's	
   straw	
   

hats,	
   master's	
   degree,	
   england's	
   ten	
   largest	
   ci.es,	
   
shriner's	
   

tokenizing	
   problems	
   

       numbers	
   can	
   be	
   important,	
   including	
   decimals	
   	
   

      nokia	
   3250,	
   top	
   10	
   courses,	
   united	
   93,	
   quick.me	
   
6.5	
   pro,	
   92.3	
   the	
   beat,	
   288358	
   	
   

       periods	
   can	
   occur	
   in	
   numbers,	
   abbrevia.ons,	
   
urls,	
   ends	
   of	
   sentences,	
   and	
   other	
   situa.ons	
   
      i.b.m.,	
   ph.d.,	
   cs.umass.edu,	
   f.e.a.r.	
   

       note:	
   tokenizing	
   steps	
   for	
   queries	
   must	
   be	
   

iden.cal	
   to	
   steps	
   for	
   documents	
   

tokenizing	
   process	
   

parts	
   of	
   document	
   to	
   tokenize	
   

       first	
   step	
   is	
   to	
   use	
   parser	
   to	
   iden.fy	
   appropriate	
   
       defer	
   complex	
   decisions	
   to	
   other	
   components	
   
       word	
   is	
   any	
   sequence	
   of	
   alphanumeric	
   characters,	
   

terminated	
   by	
   a	
   space	
   or	
   special	
   character,	
   with	
   
everything	
   converted	
   to	
   lower-     case	
   

       everything	
   indexed	
   
       example:	
   92.3	
      	
   92	
   3	
   but	
   search	
      nds	
   documents	
   
       incorporate	
   some	
   rules	
   to	
   reduce	
   dependence	
   on	
   

with	
   92	
   and	
   3	
   adjacent	
   

query	
   transforma.on	
   components	
   

tokenizing	
   process	
   

       not	
   that	
   di   erent	
   than	
   simple	
   tokenizing	
   

process	
   used	
   in	
   past	
   

       examples	
   of	
   rules	
   used	
   with	
   trec	
   

      apostrophes	
   in	
   words	
   ignored	
   

       o   connor	
      	
   oconnor	
   	
   bob   s	
      	
   bobs	
   
      periods	
   in	
   abbrevia.ons	
   ignored	
   

       i.b.m.	
      	
   ibm	
   	
   ph.d.	
      	
   ph	
   d	
   

	
   

stopping	
   

       func.on	
   words	
   (determiners,	
   preposi.ons)	
   

have	
   lirle	
   meaning	
   on	
   their	
   own	
   

       high	
   occurrence	
   frequencies	
   
       treated	
   as	
   stopwords	
   (i.e.	
   removed)	
   	
   

      reduce	
   index	
   space,	
   improve	
   response	
   .me,	
   
improve	
   e   ec.veness	
   

       can	
   be	
   important	
   in	
   combina.ons	
   

      e.g.,	
      to	
   be	
   or	
   not	
   to	
   be   	
   

stopping	
   

       stopword	
   list	
   can	
   be	
   created	
   from	
   high-     

frequency	
   words	
   or	
   based	
   on	
   a	
   standard	
   list	
   

       lists	
   are	
   customized	
   for	
   applica.ons,	
   domains,	
   

and	
   even	
   parts	
   of	
   documents	
   
      e.g.,	
      click   	
   is	
   a	
   good	
   stopword	
   for	
   anchor	
   text	
   

       best	
   policy	
   is	
   to	
   index	
   all	
   words	
   in	
   documents,	
   
make	
   decisions	
   about	
   which	
   words	
   to	
   use	
   at	
   
query	
   .me	
   

id30	
   

       many	
   morphological	
   varia.ons	
   of	
   words	
   

       in   ecvonal	
   (plurals,	
   tenses)	
   
       derivavonal	
   (making	
   verbs	
   nouns	
   etc.)	
   

similar	
   meanings	
   (but	
   cf.	
      building   )	
   

       in	
   most	
   cases,	
   these	
   have	
   the	
   same	
   or	
   very	
   
       stemmers	
   arempt	
   to	
   reduce	
   morphological	
   
varia.ons	
   of	
   words	
   to	
   a	
   common	
   stem	
   
       morphology:	
   many-     many;	
   id30:	
   many-     one	
   
       usually	
   involves	
   removing	
   su   xes	
   

       can	
   be	
   done	
   at	
   indexing	
   .me	
   or	
   as	
   part	
   of	
   query	
   

processing	
   (like	
   stopwords)	
   

id30	
   

       generally	
   a	
   small	
   but	
   signi   cant	
   e   ec.veness	
   

improvement	
   
      can	
   be	
   crucial	
   for	
   some	
   languages	
   
      e.g.,	
   5-     10%	
   improvement	
   for	
   english,	
   up	
   to	
   50%	
   in	
   
arabic	
   
	
   

words	
   with	
   the	
   arabic	
   root	
   ktb	
   

id30	
   

       two	
   basic	
   types	
   

      dic.onary-     based:	
   uses	
   lists	
   of	
   related	
   words	
   
      algorithmic:	
   uses	
   program	
   to	
   determine	
   related	
   
words	
   

       algorithmic	
   stemmers	
   

      su   x-     s:	
   remove	
      s   	
   endings	
   assuming	
   plural	
   

       e.g.,	
   cats	
      	
   cat,	
   lakes	
      	
   lake,	
   wiis	
      	
   wii	
   
       many	
   false	
   negavves:	
   supplies	
      	
   supplie	
   
       some	
   false	
   posivves:	
   ups	
      	
   up	
   

porter	
   stemmer	
   

       algorithmic	
   stemmer	
   used	
   in	
   ir	
   experiments	
   

since	
   the	
   70s	
   

       consists	
   of	
   a	
   series	
   of	
   rules	
   designed	
   to	
   the	
   

longest	
   possible	
   su   x	
   at	
   each	
   step	
   

       e   ec.ve	
   in	
   trec	
   
       produces	
   stems	
   not	
   words	
   
       makes	
   a	
   number	
   of	
   errors	
   and	
   di   cult	
   to	
   

modify	
   

porter	
   stemmer	
   

       example	
   step	
   (1	
   of	
   5)	
   

porter	
   stemmer	
   

       porter2	
   stemmer	
   addresses	
   some	
   of	
   these	
   issues	
   
       approach	
   has	
   been	
   used	
   with	
   other	
   languages	
   

krovetz	
   stemmer	
   

       hybrid	
   algorithmic-     dic.onary	
   

       word	
   checked	
   in	
   dic.onary	
   

       if	
   present,	
   either	
   lep	
   alone	
   or	
   replaced	
   with	
      excep.on   	
   
       if	
   not	
   present,	
   word	
   is	
   checked	
   for	
   su   xes	
   that	
   could	
   be	
   
removed	
   
       aper	
   removal,	
   dic.onary	
   is	
   checked	
   again	
   

       produces	
   words	
   not	
   stems	
   
       comparable	
   e   ec.veness	
   
       lower	
   false	
   posi.ve	
   rate,	
   somewhat	
   higher	
   false	
   

nega.ve	
   

stemmer	
   comparison	
   

phrases	
   

       many	
   queries	
   are	
   2-     3	
   word	
   phrases	
   
       phrases	
   are	
   

       more	
   precise	
   than	
   single	
   words	
   

       e.g.,	
   documents	
   containing	
      black	
   sea   	
   vs.	
   two	
   words	
   
   black   	
   and	
      sea   	
   
       less	
   ambiguous	
   

       e.g.,	
      big	
   apple   	
   vs.	
      apple   	
   
       can	
   be	
   di   cult	
   for	
   ranking	
   

       e.g.,	
   given	
   query	
         shing	
   supplies   ,	
   how	
   do	
   we	
   score	
   
documents	
   with	
   

       exact	
   phrase	
   many	
   .mes,	
   exact	
   phrase	
   just	
   once,	
   individual	
   
words	
   in	
   same	
   sentence,	
   same	
   paragraph,	
   whole	
   document,	
   
varia.ons	
   on	
   words?	
   

phrases	
   

       text	
   processing	
   issue	
      	
   how	
   are	
   phrases	
   

recognized?	
   

       three	
   possible	
   approaches:	
   

      iden.fy	
   syntac.c	
   phrases	
   using	
   a	
   part-     of-     speech	
   
(pos)	
   tagger	
   
      use	
   word	
   n-     grams	
   
      store	
   word	
   posi.ons	
   in	
   indexes	
   and	
   use	
   proximity	
   
operators	
   in	
   queries	
   

pos	
   tagging	
   

       pos	
   taggers	
   use	
   sta.s.cal	
   models	
   of	
   text	
   to	
   

predict	
   syntac.c	
   tags	
   of	
   words	
   
      example	
   tags:	
   	
   

       nn	
   (singular	
   noun),	
   nns	
   (plural	
   noun),	
   vb	
   (verb),	
   vbd	
   
(verb,	
   past	
   tense),	
   vbn	
   (verb,	
   past	
   par.ciple),	
   in	
   
(preposi.on),	
   jj	
   (adjec.ve),	
   cc	
   (conjunc.on,	
   e.g.,	
   
   and   ,	
      or   ),	
   prp	
   (pronoun),	
   and	
   md	
   (modal	
   auxiliary,	
   
e.g.,	
      can   ,	
      will   ).	
   
       phrases	
   can	
   then	
   be	
   de   ned	
   as	
   simple	
   noun	
   

groups,	
   for	
   example	
   

pos	
   tagging	
   example	
   

example	
   noun	
   phrases	
   

word	
   n-     grams	
   

       pos	
   tagging	
   can	
   be	
   slow	
   for	
   large	
   collec.ons	
   
       simpler	
   de   ni.on	
      	
   phrase	
   is	
   any	
   sequence	
   of	
   n	
   
words	
      	
   known	
   as	
   n-     grams	
   
       bigram:	
   2	
   word	
   sequence,	
   trigram:	
   3	
   word	
   sequence,	
   
       n-     grams	
   also	
   used	
   at	
   character	
   level	
   for	
   applica.ons	
   

unigram:	
   single	
   words	
   

such	
   as	
   ocr	
   

       n-     grams	
   typically	
   formed	
   from	
   overlapping	
   

sequences	
   of	
   words	
   
       i.e.	
   move	
   n-     word	
      window   	
   one	
   word	
   at	
   a	
   .me	
   in	
   

document	
   

n-     grams	
   

       frequent	
   n-     grams	
   are	
   more	
   likely	
   to	
   be	
   

meaningful	
   phrases	
   

       n-     grams	
   form	
   a	
   zipf	
   distribu.on	
   

      berer	
      t	
   than	
   words	
   alone	
   

       could	
   index	
   all	
   n-     grams	
   up	
   to	
   speci   ed	
   length	
   

      much	
   faster	
   than	
   pos	
   tagging	
   
      uses	
   a	
   lot	
   of	
   storage	
   

       e.g.,	
   document	
   containing	
   1,000	
   words	
   would	
   contain	
   
3,990	
   instances	
   of	
   word	
   n-     grams	
   of	
   length	
   2	
      	
   n	
      	
   5	
   

google	
   n-     grams	
   
       web	
   search	
   engines	
   index	
   n-     grams	
   
       google	
   sample	
   (frequency	
   >	
   40):	
   

       most	
   frequent	
   trigram	
   in	
   english	
   is	
      all	
   rights	
   

reserved   	
   
       in	
   chinese,	
      limited	
   liability	
   corpora.on   	
   

document	
   structure	
   and	
   markup	
   
       some	
   parts	
   of	
   documents	
   are	
   more	
   important	
   

than	
   others	
   

       document	
   parser	
   recognizes	
   structure	
   using	
   

markup,	
   such	
   as	
   html	
   tags	
   
      headers,	
   anchor	
   text,	
   bolded	
   text	
   all	
   likely	
   to	
   be	
   
important	
   
      metadata	
   can	
   also	
   be	
   important	
   
      links	
   used	
   for	
   link	
   analysis	
   

example	
   web	
   page	
   

example	
   web	
   page	
   

link	
   analysis	
   

       links	
   are	
   a	
   key	
   component	
   of	
   the	
   web	
   
       important	
   for	
   naviga.on,	
   but	
   also	
   for	
   search	
   

      e.g.,	
   <a	
   href="hrp://example.com"	
   >example	
   
website</a>	
   
         example	
   website   	
   is	
   the	
   anchor	
   text	
   
         hrp://example.com   	
   is	
   the	
   des.na.on	
   link	
   
      both	
   are	
   used	
   by	
   search	
   engines	
   

exercise:	
   link	
   analysis	
   

       	
   assump.on	
   1:	
   a	
   link	
   on	
   the	
   web	
   is	
   a	
   quality	
   
signal	
      	
   the	
   author	
   of	
   the	
   link	
   thinks	
   that	
   the	
   
linked-     to	
   page	
   is	
   high-     quality.	
   

       assump.on	
   2:	
   the	
   anchor	
   text	
   describes	
   the	
   

content	
   of	
   the	
   linked-     to	
   page.	
   

       is	
   assump.on	
   1	
   true	
   in	
   general?	
   
       is	
   assump.on	
   2	
   true	
   in	
   general?	
   

anchor	
   text	
   

       used	
   as	
   a	
   descrip.on	
   of	
   the	
   content	
   of	
   the	
   

similar	
   to	
   query	
   text	
   

desvnavon	
   page	
   
      i.e.,	
   collec.on	
   of	
   anchor	
   text	
   in	
   all	
   links	
   poin.ng	
   to	
   
a	
   page	
   used	
   as	
   an	
   addi.onal	
   text	
      eld	
   
       anchor	
   text	
   tends	
   to	
   be	
   short,	
   descrip.ve,	
   and	
   
       retrieval	
   experiments	
   have	
   shown	
   that	
   anchor	
   
text	
   has	
   signi   cant	
   impact	
   on	
   e   ec.veness	
   for	
   
some	
   types	
   of	
   queries	
   
      i.e.,	
   more	
   than	
   id95	
   
	
   

id95	
   

       billions	
   of	
   web	
   pages,	
   some	
   more	
   informa.ve	
   

than	
   others	
   

       links	
   can	
   be	
   viewed	
   as	
   informa.on	
   about	
   the	
   

popularity	
   (authority?)	
   of	
   a	
   web	
   page	
   
      can	
   be	
   used	
   by	
   ranking	
   algorithm	
   

       inlink	
   count	
   could	
   be	
   used	
   as	
   simple	
   measure	
   
       link	
   analysis	
   algorithms	
   like	
   id95	
   provide	
   

more	
   reliable	
   ra.ngs	
   
      less	
   suscep.ble	
   to	
   link	
   spam	
   

random	
   surfer	
   model	
   

       browse	
   the	
   web	
   using	
   the	
   following	
   algorithm:	
   

       choose	
   a	
   random	
   number	
   r	
   between	
   0	
   and	
   1	
   
       if	
   r	
   <	
     :	
   

       	
   go	
   to	
   a	
   random	
   page	
   

       if	
   r	
      	
     :	
   

       start	
   again	
   

       click	
   a	
   link	
   at	
   random	
   on	
   the	
   current	
   page	
   

       id95	
   of	
   a	
   page	
   is	
   the	
   id203	
   that	
   the	
   
   random	
   surfer   	
   will	
   be	
   looking	
   at	
   that	
   page	
   
       links	
   from	
   popular	
   pages	
   will	
   increase	
   id95	
   of	
   

pages	
   they	
   point	
   to	
   

dangling	
   links	
   

       random	
   jump	
   prevents	
   ge(cid:128)ng	
   stuck	
   on	
   

pages	
   that	
   
      do	
   not	
   have	
   links	
   
      contains	
   only	
   links	
   that	
   no	
   longer	
   point	
   to	
   
other	
   pages	
   
      have	
   links	
   forming	
   a	
   loop	
   

       links	
   that	
   point	
   to	
   the	
      rst	
   two	
   types	
   of	
   
pages	
   are	
   called	
   dangling	
   links	
   
      may	
   also	
   be	
   links	
   to	
   pages	
   that	
   have	
   not	
   yet	
   
been	
   crawled	
   

id95	
   

       id95	
   (pr)	
   of	
   page	
   c	
   =	
   pr(a)/2	
   +	
   pr(b)/1	
   
       more	
   generally,	
   	
   

	
   

	
   

       where	
   bu	
   is	
   the	
   set	
   of	
   pages	
   that	
   point	
   to	
   u,	
   and	
   lv	
   is	
   
the	
   number	
   of	
   outgoing	
   links	
   from	
   page	
   v	
   (not	
   
coun.ng	
   duplicate	
   links)	
   

id95	
   

       don   t	
   know	
   id95	
   values	
   at	
   start	
   
       assume	
   equal	
   values	
   (1/3	
   in	
   this	
   case),	
   then	
   
iterate:	
   
          rst	
   itera.on:	
   pr(c)	
   =	
   0.33/2	
   +	
   0.33	
   =	
   0.5,	
   pr(a)	
   =	
   
       second:	
   pr(c)	
   =	
   0.33/2	
   +	
   0.17	
   =	
   0.33,	
   pr(a)	
   =	
   0.5,	
   
       third:	
   pr(c)	
   =	
   0.42,	
   pr(a)	
   =	
   0.33,	
   pr(b)	
   =	
   0.25	
   

0.33,	
   and	
   pr(b)	
   =	
   0.17	
   

pr(b)	
   =	
   0.17	
   

       converges	
   to	
   pr(c)	
   =	
   0.4,	
   pr(a)	
   =	
   0.4,	
   and	
   pr(b)	
   =	
   

0.2	
   

id95	
   

       taking	
   random	
   page	
   jump	
   into	
   account,	
   1/3	
   

chance	
   of	
   going	
   to	
   any	
   page	
   when	
   r	
   <	
     	
   
       pr(c)	
   =	
     /3	
   +	
   (1	
      	
     )	
        	
   (pr(a)/2	
   +	
   pr(b)/1)	
   
       more	
   generally,	
   

	
   

      where	
   n	
   is	
   the	
   number	
   of	
   pages,	
     	
   typically	
   0.15	
   

a	
   id95	
   implementa.on	
   

       preliminaries:	
   

       1)	
   extract	
   links	
   from	
   the	
   source	
   text.	
   you'll	
   also	
   want	
   to	
   extract	
   the	
   

url	
   from	
   each	
   document	
   in	
   a	
   separate	
      le.	
   	
   now	
   you	
   have	
   all	
   the	
   links	
   
(source-     des.na.on	
   pairs)	
   and	
   all	
   the	
   source	
   documents	
   
       2)	
   remove	
   all	
   links	
   from	
   the	
   list	
   that	
   do	
   not	
   connect	
   two	
   documents	
   in	
   

the	
   corpus.	
   	
   the	
   easiest	
   way	
   to	
   do	
   this	
   is	
   to	
   sort	
   all	
   links	
   by	
   
des.na.on,	
   then	
   compare	
   that	
   against	
   the	
   corpus	
   urls	
   list	
   (also	
   
sorted)	
   

       3)	
   create	
   a	
   new	
      le	
   i	
   that	
   contains	
   a	
   (url,	
   id95)	
   pair	
   for	
   each	
   url	
   
in	
   the	
   corpus.	
   	
   the	
   ini.al	
   id95	
   value	
   is	
   1/#d	
   (#d	
   =	
   number	
   of	
   urls)	
   

       at	
   this	
   point	
   there	
   are	
   two	
   interes.ng	
      les:	
   

       	
   	
   [l]	
   links	
   (trimmed	
   to	
   contain	
   only	
   corpus	
   links,	
   sorted	
   by	
   source	
   url)	
   
       	
   	
   [i]	
   url/id95	
   pairs,	
   ini.alized	
   to	
   a	
   constant	
   

a	
   id95	
   implementa.on	
   
       preliminaries	
   -     	
   link	
   extrac.on	
   from	
   .corpus	
      le	
   using	
   	
   galago	
   

	
   documentsplit	
   -     >	
   indexreadersplitparser	
   -     >	
   tagtokenizer	
   
	
   split	
   =	
   new	
   documentsplit	
   (	
      lename,	
      letype,	
   new	
   byte[0],	
   new	
   byte[0]	
   )	
   	
   
	
   index	
   =	
   new	
   indexreadersplitparser	
   (	
   split	
   )	
   
	
   tokenizer	
   =	
   new.tagtokenizer	
   (	
   )	
   	
   
	
   tokenizer.setprocessor	
   (	
   nullprocessor	
   (	
   document.class	
   )	
   )	
   
	
   doc	
   =	
   index.nextdocument	
   (	
   )	
   	
   
	
   tokenizer.process	
   (	
   doc	
   )	
   

       doc.iden.   er	
   contains	
   the	
      le   s	
   name	
   
       doc.tags	
   now	
   contains	
   all	
   tags	
   
       links	
   can	
   be	
   extracted	
   by	
      nding	
   all	
   tags	
   with	
   name	
      a   	
   
       links	
   should	
   be	
   processed	
   so	
   that	
   they	
   can	
   be	
   compared	
   with	
   some	
   

   le	
   name	
   in	
   the	
   corpus	
   

a	
   id95	
   implementa.on	
   

itera.on:	
   	
   
       steps:	
   

1.    make	
   a	
   new	
   output	
      le,	
   r.	
   
2.   
3.   
4.   
5.   

read	
   l	
   and	
   i	
   in	
   parallel	
   (since	
   they're	
   all	
   sorted	
   by	
   url).	
   
for	
   each	
   unique	
   source	
   url,	
   determine	
   whether	
   it	
   has	
   any	
   outgoing	
   
links:	
   
if	
   not,	
   add	
   its	
   current	
   id95	
   value	
   to	
   the	
   sum:	
   t	
   (terminals).	
   
if	
   it	
   does	
   have	
   outgoing	
   links,	
   write	
   (source_url,	
   dest_url,	
   ip/|q|),	
   
where	
   ip	
   is	
   the	
   current	
   id95	
   value,	
   |q|	
   is	
   the	
   number	
   of	
   
outgoing	
   links,	
   and	
   dest_url	
   is	
   a	
   link	
   des.na.on.	
   	
   	
   
do	
   this	
   for	
   all	
   outgoing	
   links.	
   	
   write	
   this	
   to	
   r.	
   
sort	
   r	
   by	
   des.na.on	
   url.	
   
scan	
   r	
   and	
   i	
   at	
   the	
   same	
   .me.	
   	
   the	
   new	
   value	
   of	
   rp	
   is:	
   	
   	
   	
   
(1	
   -     	
   lambda)	
   /	
   #d	
   (a	
   frac.on	
   of	
   the	
   sum	
   of	
   all	
   pages)	
   
plus:	
   lambda	
   *	
   sum(t)	
   /	
   #d	
   (the	
   total	
   e   ect	
   from	
   terminal	
   pages),	
   
plus:	
   lambda	
   *	
   all	
   incoming	
   mass	
   from	
   step	
   5.	
   ()	
   
check	
   for	
   convergence	
   

8.   
9.    write	
   new	
   rp	
   values	
   to	
   a	
   new	
   i	
      le.	
   

6.   
7.   

a	
   id95	
   implementa.on	
   

       convergence	
   check	
   

       stopping	
   criteria	
   for	
   this	
   types	
   of	
   pr	
   algorithm	
   typically	
   is	
   of	
   the	
   form	
   
||new	
   -     	
   old||	
   <	
   tau	
   where	
   new	
   and	
   old	
   are	
   the	
   new	
   and	
   old	
   id95	
   
vectors,	
   respec.vely.	
   	
   

       tau	
   is	
   set	
   depending	
   on	
   how	
   much	
   precision	
   you	
   need.	
   reasonable	
   

values	
   include	
   0.1	
   or	
   0.01.	
   if	
   you	
   want	
   	
   really	
   fast,	
   but	
   inaccurate	
   
convergence,	
   then	
   you	
   can	
   use	
   something	
   like	
   tau=1.	
   	
   

       the	
   se(cid:128)ng	
   of	
   tau	
   also	
   depends	
   on	
   n	
   (=	
   number	
   of	
   documents	
   in	
   the	
   

collec.on),	
   since	
   ||new-     old||	
   (for	
   a	
      xed	
   numerical	
   precision)	
   
increases	
   as	
   n	
   increases,	
   so	
   you	
   can	
   alterna.vely	
   formulate	
   your	
   
convergence	
   criteria	
   as	
   ||new	
      	
   old||	
   /	
   n	
   <	
   tau.	
   	
   

       either	
   the	
   l1	
   or	
   l2	
   norm	
   can	
   be	
   used.	
   

	
   
	
   

link	
   quality	
   

       link	
   quality	
   is	
   a   ected	
   by	
   spam	
   and	
   other	
   

factors	
   
      e.g.,	
   link	
   farms	
   to	
   increase	
   id95	
   
      trackback	
   links	
   in	
   blogs	
   can	
   create	
   loops	
   
      links	
   from	
   comments	
   sec.on	
   of	
   popular	
   blogs	
   

       blog	
   services	
   modify	
   comment	
   links	
   to	
   contain	
   
rel=nofollow	
   arribute	
   
       e.g.,	
      come	
   visit	
   my	
   <a	
   rel=nofollow	
   href="hrp://
www.page.com">web	
   page</a>.   	
   

trackback	
   links	
   

