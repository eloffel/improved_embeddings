slides adapted from vlad morariu

cmsc 422 introduction to machine learning

lecture 20 deep learning i

furong huang / furongh@cs.umd.edu

history of neural networks

2

standard id161 pipeline

o

n
o

i
t
c
a
r
t
x
e

 

e
r
u

t

a
e

f

i
t

a
c
i
f
i
s
s
a
c

l

   cat    or    background   

n

features

predicted labels

   cat   

features: hog, sift, lbp,    

supervision

classifiers: id166, rf, knn,    

features are hand-crafted, not trained

eventually limited by feature quality

training

cat image credit: https://raw.githubusercontent.com/bvlc/caffe/master/examples/images/cat.jpg

3

deep learning

training

supervision

features

classifier

image credit: lecun, y., bottou, l., bengio, y., haffner, p.    gradient-based learning applied to 
document recognition.    proceedings of the ieee, 1998.

deep learning

multiple layer neural networks

learn features and classifiers directly (   end-to-end    
training)

4

id103

switchboard: telephone speech corpus for research and development

slide credit: bohyung han

5

image classification performance

figure from: k. he, x. zhang, s. ren, j. sun.     deep residual learning for image  recognition   . arxiv 2015. (slides)

6

slide credit: bohyung han

image classification top-5 errors (%)

biological inspiration

image source: http://cs231n.github.io/neural-networks-1/

7

artificial neuron

image source: http://cs231n.github.io/neural-networks-1/

activation function is usually non-linear

step, tanh, sigmoid

the actual biological system is much more complicated

8

mcculloch-pitts model

read: https://en.wikipedia.org/wiki/artificial_neuron

    threshold logic unit (tlu)

    warren mcculloch and walter pitts, 1943

    binary inputs/outputs and threshold activation function

    can represent and/or/not functions which can be composed for complex functions

9

rosenblatt   s id88

    id88

read: https://en.wikipedia.org/wiki/id88

    proposed by frank rosenblatt in 1957

    based on mcculloch-pitts model

    real inputs/outputs, threshold activation function

1 0

id88 learning

    given a training dataset of input features and 

labels

   

initialize weights  randomly                                     

    for each example in training set
    classify example using current weights

    update weights 

   

if data is linearly separable, convergence is 
guaranteed in a bounded number of iterations
https://en.wikipedia.org/wiki/id88

1 1

multiple output variables

weights to predict multiple outputs can be learned 
independently

b1
w11

w12
w13

w14
b2
w21

w22
w23

y1

y2

x1

x2

x3

x4

w24

b3

w31

y3

w34

w32
w33

inputs

outputs

1 2

id88 success

   

implemented as custom-built hardware, the    mark i 
id88   
   
    weights: potentiometers
    weight updates: electric motors

input: photocells

    demonstrated ability to classify 20x20 images
    generated lots of ai excitement

   

in 1958, the new york times reported the 
id88 to be
   

"the embryo of an electronic computer that [the navy] expects 
will be able to walk, talk, see, write, reproduce itself and be 
conscious of its existence."

https://en.wikipedia.org/wiki/id88

1 3

linear classifier

slide credit: bohyung han

1 4

nonlinear classifier

slide credit: bohyung han

1 5

nonlinear classifier     xor problem

1 6

id88 limitations

   

   

if there are multiple separating hyperplanes, 
learning will converge to one of them (not the 
optimal one)
if training set is not linearly separable, training 
will fail completely

    marvin minsky and seymour papert, 

   id88s   , 1969
   

proved that it was impossible to learn an xor function 
with a single layer id88 network

    led to the    ai winter    of the 1970   s

https://en.wikipedia.org/wiki/id88

1 7

multi-layer id88 (mlp)

image source: http://cs231n.github.io/neural-networks-1/

    activation function need not be a threshold
    multiple layers can represent xor function
    but id88 algorithm cannot be used to 

update weights
    why? hidden layers are not observed!

https://en.wikipedia.org/wiki/multilayer_id88
1 8

multi-layer: id26

            

        

        

    

            

        

    

           

sigmoid

neuron     

        
            

=

        
                

                
            

    

    

           

    

sigmoid

neuron     

        
                

=    

    

        
            

            
                

=    

            

    

        
            

=    

            

    

        
                

                
            

        
                

=    
    

        
    
                

    
                
    
            

    
            
                

    
                
    
            

    
            
                

=    
    

            

   
    

        
    
                

    
                
    
            

slide credit: bohyung han

1 9

stochastic id119 (sgd)

update weights for each sample

     =

1
2

                         2

              + 1 =                       

            
            

+ fast, online
    sensitive to noise

minibatch sgd: update weights for a small set of 
samples

     =

1
2

   
           

                         2

              + 1 =                       

            
            

+ fast, online
+ robust to noise

slide credit: bohyung han
2 0

momentum

remember the previous direction

+ converge faster
+ avoid oscillation

              =                       1         

        
            

(    )

          + 1 =           +     (    )

slide credit: bohyung han

2 1

weight decay

penalize the size of the weights

     =      +

1
2

   

2
        

    

              + 1 =                       

        
            

=                       

        
            

                

+ improve generalization a lot!

slide credit: bohyung han
2 2

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

