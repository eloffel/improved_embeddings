decision	
   trees	
   

lecture	
   11	
   

david	
   sontag	
   

new	
   york	
   university	
   

slides adapted from luke zettlemoyer, carlos guestrin, and 

andrew moore  

hypotheses: id90  f  : x ! y 

       each internal node 
tests an attribute xi 

       one branch for 
each possible 
attribute value xi=v 
       each leaf assigns a 

class y  

       to classify input x: 
traverse the tree 
from root to leaf, 
output the labeled y  

cylinders	
   

3	
   

4	
   

maker	
   

good 

6	
   

5	
   
bad 

bad 

8	
   

horsepower	
   

america	
   

asia	
    europe	
   

low	
    med	
    high	
   

bad 

good 

good 

bad 

good 

bad 

human	
   interpretable!	
   

hypothesis space 

       how many possible 

hypotheses? 

       what functions can be 

represented? 

cylinders	
   

3	
   

4	
   

maker	
   

good 

6	
   

5	
   
bad 

bad 

8	
   

horsepower	
   

america	
   
bad 

asia	
    europe	
   
good 

good 

low	
    med	
    high	
   
bad 

good 

bad 

expressiveness
what	
   funcgons	
   can	
   be	
   represented?	
   

discrete-input, discrete-output case:

    id90 can express any function of the input attributes.
    e.g., for boolean functions, truth table row   path to leaf:

       decision	
   trees	
   can	
   represent	
   

any	
   funcgon	
   of	
   the	
   input	
   
aiributes!	
   

a
f
f
t
t

b
f
t
f
t

a xor b

f
t
t
f

b

f

 t
t

f
f

a

 t

f
t

b

 t
f

       for	
   boolean	
   funcgons,	
   path	
   
to	
   leaf	
   gives	
   truth	
   table	
   row	
   
       could	
   require	
   exponengally	
   

many	
   nodes	
   

continuous-input, continuous-output case:

(figure	
   from	
   stuart	
   russell)	
   

    can approximate any function arbitrarily closely

trivially, there is a consistent decision tree for any training set
w/ one path to leaf for each example (unless f nondeterministic in x)
but it probably won   t generalize to new examples

cylinders	
   

3	
   

4	
   

need some kind of id173 to ensure more compact id90

horsepower	
   

maker	
   

good 

6	
   

5	
   
bad 

bad 

8	
   

america	
   
bad 

asia	
    europe	
   
good 

good 

low	
    med	
    high	
   
bad 

good 

bad 

cs194-10 fall 2011 lecture 8

cyl=3     (cyl=4     (maker=asia     maker=europe))         

learning	
   simplest	
   decision	
   tree	
   is	
   np-     hard	
   

       learning	
   the	
   simplest	
   (smallest)	
   decision	
   tree	
   is	
   
an	
   np-     complete	
   problem	
   [hya   l	
   &	
   rivest	
      76]	
   	
   

       resort	
   to	
   a	
   greedy	
   heurisgc:	
   
      start	
   from	
   empty	
   decision	
   tree	
   
      split	
   on	
   next	
   best	
   a1ribute	
   (feature)	
   
      recurse	
   

key	
   idea:	
   greedily	
   learn	
   trees	
   using	
   

recursion	
   

take the 
original 
dataset.. 

and partition it 
according 
to the value of 
the attribute we 
split on 

records 
in which 
cylinders 

= 4  

records 
in which 
cylinders 

= 5 

records 
in which 
cylinders 

= 6  

records 
in which 
cylinders 

= 8 

recursive	
   step	
   

build tree from 
these records.. 

build tree from 
these records.. 

build tree from 
these records.. 

build tree from 
these records.. 

records in 

which cylinders 

= 4  

records in 

which cylinders 

= 5 

records in 

which cylinders 

= 6  

records in 

which cylinders 

= 8 

second	
   level	
   of	
   tree	
   

recursively build a tree from the seven 
records in which there are four cylinders 
and the maker was based in asia 

(similar recursion in 
the other cases) 

a full tree 

spli^ng:	
   choosing	
   a	
   good	
   airibute	
   

would we prefer to split on x1 or x2?   

x1  

f 

t 

x2  

f 

t 

y=t : 4 
y=f : 0  

y=t : 1 
y=f : 3  

y=t : 3 
y=f : 1  

y=t : 2 
y=f : 2  

idea: use counts at leaves to define 
id203 distributions, so we can 
measure uncertainty! 

x1  x2 
t 
t 
t 
f 
t 
t 
f 
t 
t 
f 
f 
f 
t 
f 
f 
f 

y 
t 
t 
t 
t 
t 
f 
f 
f 

measuring	
   uncertainty	
   

       good	
   split	
   if	
   we	
   are	
   more	
   certain	
   about	
   

classi   cagon	
   a_er	
   split	
   
      determinisgc	
   good	
   (all	
   true	
   or	
   all	
   false)	
   
      uniform	
   distribugon	
   bad	
   
      what	
   about	
   distribugons	
   in	
   between?	
   

p(y=a) = 1/2  p(y=b) = 1/4  p(y=c) = 1/8  p(y=d) = 1/8 

p(y=a) = 1/4  p(y=b) = 1/4  p(y=c) = 1/4  p(y=d) = 1/4 

id178	
   

id178	
   h(y)	
   of	
   a	
   random	
   variable	
   y 

more uncertainty, more id178! 

id205 interpretation: 
 h(y) is the expected number of bits 
needed  to encode a randomly 
drawn value of y  (under most 
efficient code)  

	
   

y
p
o
r
t
n
e

id178	
   of	
   a	
   coin	
      ip	
   

id203	
   of	
   heads	
   

high,	
   low	
   id178	
   

          high	
   id178   	
   	
   

      y	
   is	
   from	
   a	
   uniform	
   like	
   distribugon	
   
      flat	
   histogram	
   
      values	
   sampled	
   from	
   it	
   are	
   less	
   predictable	
   

          low	
   id178   	
   	
   

      y	
   is	
   from	
   a	
   varied	
   (peaks	
   and	
   valleys)	
   

distribugon	
   

      histogram	
   has	
   many	
   lows	
   and	
   highs	
   
      values	
   sampled	
   from	
   it	
   are	
   more	
   predictable	
   

(slide from vibhav gogate) 

id178	
   example	
   

id178	
   of	
   a	
   coin	
      ip	
   

	
   

y
p
o
r
t
n
e

p(y=t) = 5/6 
p(y=f) = 1/6 

h(y) = - 5/6 log2 5/6 - 1/6 log2 1/6 
         = 0.65 

id203	
   of	
   heads	
   

x1  x2  y 
t 
t 
t 
t 
t 
t 
t 
t 
t 
f 
f 
f 

t 
f 
t 
f 
t 
f 

condigonal	
   id178	
   

condigonal	
   id178	
   h( y |x)	
   of	
   a	
   random	
   variable	
   y	
   condigoned	
   on	
   a	
   

random	
   variable	
   x 

example: 

p(x1=t) = 4/6 
p(x1=f) = 2/6 

x1  

f 

t 

y=t : 4 
y=f : 0  

y=t : 1 
y=f : 1  

h(y|x1) = - 4/6 (1 log2 1 + 0 log2 0) 
                    - 2/6 (1/2 log2 1/2 + 1/2 log2 1/2) 
             = 2/6 

x1  x2  y 
t 
t 
t 
t 
t 
t 
t 
t 
t 
f 
f 
f 

t 
f 
t 
f 
t 
f 

informagon	
   gain	
   

       decrease	
   in	
   id178	
   (uncertainty)	
   a_er	
   spli^ng	
   

in our running example: 

ig(x1) = h(y)     h(y|x1) 
           =  0.65     0.33  

ig(x1) > 0 ! we prefer the split! 

x1  x2  y 
t 
t 
t 
t 
t 
t 
t 
t 
t 
f 
f 
f 

t 
f 
t 
f 
t 
f 

learning	
   decision	
   trees	
   

       start	
   from	
   empty	
   decision	
   tree	
   
       split	
   on	
   next	
   best	
   a1ribute	
   (feature)	
   

      use,	
   for	
   example,	
   informagon	
   gain	
   to	
   select	
   

airibute:	
   

       recurse	
   

when	
   to	
   stop?	
   

first split looks good! but, when do we stop? 

base case 

one 

don   t split a 
node if all 
matching 

records have 

the same 

output value 

base case 

two 

don   t split a 
node if data 
points are 
identical on 
remaining 
attributes 

base	
   cases:	
   an	
   idea	
   

       base	
   case	
   one:	
   if	
   all	
   records	
   in	
   current	
   data	
   

subset	
   have	
   the	
   same	
   output	
   then	
   don   t	
   recurse	
   

       base	
   case	
   two:	
   if	
   all	
   records	
   have	
   exactly	
   the	
   
same	
   set	
   of	
   input	
   aiributes	
   then	
   don   t	
   recurse	
   

proposed base case 3: 
if all attributes have small 
information gain then don   t 

recurse 

      this is not a good idea 

the	
   problem	
   with	
   proposed	
   case	
   3	
   

y = a xor b 

the information gains: 

if	
   we	
   omit	
   proposed	
   case	
   3:	
   

y = a xor b 

the resulting decision tree: 

instead, perform 
pruning after building a 
tree 

decision	
   trees	
   will	
   over   t	
   

decision	
   trees	
   will	
   over   t	
   

       standard	
   decision	
   trees	
   have	
   no	
   learning	
   bias	
   

      training	
   set	
   error	
   is	
   always	
   zero!	
   

       (if	
   there	
   is	
   no	
   label	
   noise)	
   

      lots	
   of	
   variance	
   
      must	
   introduce	
   some	
   bias	
   towards	
   simpler	
   trees	
   

       many	
   strategies	
   for	
   picking	
   simpler	
   trees	
   

      fixed	
   depth	
   
      minimum	
   number	
   of	
   samples	
   per	
   leaf	
   	
   

       random	
   forests	
   

real-     valued	
   inputs	
   

what	
   should	
   we	
   do	
   if	
   some	
   of	
   the	
   inputs	
   are	
   real-     valued?	
   

infinite 
number of 
possible split 
values!!! 

   one	
   branch	
   for	
   each	
   numeric	
   value   	
   

idea:	
   

hopeless: hypothesis with such a high 
branching factor will shatter any dataset 
and overfit 

threshold	
   splits	
   

       binary	
   tree:	
   split	
   on	
   
airibute	
   x	
   at	
   value	
   t	
   
      one	
   branch:	
   x	
   <	
   t	
   
      other	
   branch:	
   x	
      	
   t	
   

       requires small change 
       allow repeated splits on same 

variable along a path 

year	
   

<78	
   

   78	
   

bad 
year	
   

good 

<70	
   

   70	
   

bad 

good 

the	
   set	
   of	
   possible	
   thresholds	
   

       binary	
   tree,	
   split	
   on	
   airibute	
   x	
   

       one	
   branch:	
   x	
   <	
   t	
   
       other	
   branch:	
   x	
      	
   t	
   

       search	
   through	
   possible	
   values	
   of	
   t	
   

optimal splits for continuous attributes

in   nitely many possible split points c to de   ne node test xj > c ?

       seems	
   hard!!!	
   

       but	
   only	
   a	
      nite	
   number	
   of	
   t   s	
   are	
   important:	
   

optimal splits for continuous attributes
no! moving split point along the empty space between two observed values
has no e ect on information gain or empirical loss; so just use midpoint

in   nitely many possible split points c to de   ne node test xj > c ?

xj

no! moving split point along the empty space between two observed values
has no e ect on information gain or empirical loss; so just use midpoint

c2
c1
t1 t2
       sort	
   data	
   according	
   to	
   x	
   into	
   {x1,   ,xm}	
   
c2
       consider	
   split	
   points	
   of	
   the	
   form	
   xi	
   +	
   (xi+1	
      	
   xi)/2	
   
xj
       morever,	
   only	
   splits	
   between	
   examples	
   of	
   di   erent	
   classes	
   maier!	
   

moreover, only splits between examples from di erent classes
can be optimal for information gain or empirical loss reduction

moreover, only splits between examples from di erent classes
can be optimal for information gain or empirical loss reduction

xj

c1

c1

c2

xj

c1
t1

c2
t2

(figures	
   from	
   stuart	
   russell)	
   

picking	
   the	
   best	
   threshold	
   

       suppose	
   x	
   is	
   real	
   valued	
   with	
   threshold	
   t	
   
       want	
   ig(y	
   |	
   x:t),	
   the	
   informagon	
   gain	
   for	
   y	
   when	
   

tesgng	
   if	
   x	
   is	
   greater	
   than	
   or	
   less	
   than	
   t	
   

       de   ne:	
   	
   

       h(y|x:t)	
   =	
   	
   p(x	
   <	
   t)	
   h(y|x	
   <	
   t)	
   +	
   p(x	
   >=	
   t)	
   h(y|x	
   >=	
   t)	
   
       ig(y|x:t)	
   =	
   h(y)	
   -     	
   h(y|x:t)	
   
       ig*(y|x)	
   =	
   maxt	
   ig(y|x:t)	
   

       use:	
   ig*(y|x)	
   for	
   congnuous	
   variables	
   

what	
   you	
   need	
   to	
   know	
   about	
   decision	
   trees	
   

       decision	
   trees	
   are	
   one	
   of	
   the	
   most	
   popular	
   ml	
   tools	
   

       easy	
   to	
   understand,	
   implement,	
   and	
   use	
   
       computagonally	
   cheap	
   (to	
   solve	
   heurisgcally)	
   
informagon	
   gain	
   to	
   select	
   aiributes	
   ( ,	
   c4.5,   )	
   

      
       presented	
   for	
   classi   cagon,	
   can	
   be	
   used	
   for	
   regression	
   and	
   

density	
   esgmagon	
   too	
   

       decision	
   trees	
   will	
   over   t!!!	
   

       must	
   use	
   tricks	
   to	
      nd	
      simple	
   trees   ,	
   e.g.,	
   

       fixed	
   depth/early	
   stopping	
   
       pruning	
   

       or,	
   use	
   ensembles	
   of	
   di   erent	
   trees	
   (random	
   forests)	
   

ensemble	
   learning	
   

slides adapted from navneet goyal, tan, steinbach, 

kumar, vibhav gogate 

ensemble	
   methods	
   

machine learning competition with a $1 million prize 

bias/variance	
   tradeo   	
   

hastie, tibshirani, friedman    elements of statistical learning    2001	


reduce	
   variance	
   without	
   increasing	
   bias	
   

       averaging	
   reduces	
   variance:	
   

(when predictions 
 are independent) 

average models to reduce model variance 
one problem:  

only one training set 
where do multiple models come from? 

id112:	
   bootstrap	
   aggregagon	
   

       leo	
   breiman	
   (1994)	
   
       take	
   repeated	
   bootstrap	
   samples	
   from	
   training	
   set	
   d	
   
       bootstrap	
   sampling:	
   given	
   set	
   d	
   containing	
   n	
   training	
   
examples,	
   create	
   d   	
   by	
   drawing	
   n	
   examples	
   at	
   random	
   
with	
   replacement	
   from	
   d.	
   

       id112:	
   

       create	
   k	
   bootstrap	
   samples	
   d1	
      	
   dk.	
   
       train	
   disgnct	
   classi   er	
   on	
   each	
   di.	
   
       classify	
   new	
   instance	
   by	
   majority	
   vote	
   /	
   average.	
   

general	
   idea	
   

example	
   of	
   id112	
   

       sampling	
   with	
   replacement	
   

data id 

training data 

       build	
   classi   er	
   on	
   each	
   bootstrap	
   sample	
   
       each	
   data	
   point	
   has	
   id203	
   (1	
      	
   1/n)n	
   of	
   being	
   

selected	
   as	
   test	
   data	
   

       training	
   data	
   =	
   1-     	
   (1	
      	
   1/n)n	
   of	
   the	
   original	
   data	
   

51 

decision tree learning algorithm; very similar to   

52 

shades of blue/red indicate strength of vote for particular classification 

random	
   forests	
   
       ensemble	
   method	
   speci   cally	
   designed	
   for	
   decision	
   

tree	
   classi   ers	
   

       introduce	
   two	
   sources	
   of	
   randomness:	
      id112   	
   

and	
      random	
   input	
   vectors   	
   
       id112	
   method:	
   each	
   tree	
   is	
   grown	
   using	
   a	
   bootstrap	
   

sample	
   of	
   training	
   data	
   

       random	
   vector	
   method:	
   at	
   each	
   node,	
   best	
   split	
   is	
   chosen	
   

from	
   a	
   random	
   sample	
   of	
   m	
   aiributes	
   instead	
   of	
   all	
   
aiributes	
   

random	
   forests	
   

random	
   forests	
   algorithm	
   

