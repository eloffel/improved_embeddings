unsupervised	learning	(part	1)	

lecture	19	

david	sontag	

new	york	university	

slides	adapted	from	carlos	guestrin,	dan	klein,	luke	ze@lemoyer,	

dan	weld,	vibhav	gogate,	and	andrew	moore	

a id110 is speci   ed by a directed acyclic graph
g = (v , e ) with:

bayesian	networks	enable	use	of	
1 one node i 2 v for each random variable xi
2 one id155 distribution (cpd) per node, p(xi | xpa(i)),
specifying the variable   s id203 conditioned on its parents    values

domain	knowledge	

corresponds 1-1 with a particular factorization of the joint
distribution:

p(x1, . . . xn) =yi2v

p(xi | xpa(i))

will	my	car	start	this	morning?	

powerful framework for designing algorithms to perform id203
computations

david sontag (nyu)

id114

lecture 1, january 31, 2013

30 / 44

heckerman	et	al.,	decision-theoremc	troubleshoomng,	1995	

a id110 is speci   ed by a directed acyclic graph
g = (v , e ) with:

bayesian	networks	enable	use	of	
1 one node i 2 v for each random variable xi
2 one id155 distribution (cpd) per node, p(xi | xpa(i)),
specifying the variable   s id203 conditioned on its parents    values

domain	knowledge	

corresponds 1-1 with a particular factorization of the joint
distribution:

p(x1, . . . xn) =yi2v

p(xi | xpa(i))

what	is	the	di   erenmal	diagnosis?	

powerful framework for designing algorithms to perform id203
computations

david sontag (nyu)

id114

lecture 1, january 31, 2013

30 / 44

beinlich	et	al.,	the	alarm	monitoring	system,	1989	

bayesian	networks	are	genera*ve	models	

       can	sample	from	the	joint	distribumon,	top-down	
       suppose	y	can	be	   spam   	or	   not	spam   ,	and	xi	is	a	binary	
       let   s	try	generamng	a	few	emails!	

indicator	of	whether	word	i	is	present	in	the	e-mail	

label

y

x1

x2

x3

. . .

xn

features

   

       ozen	helps	to	think	about	bayesian	networks	as	a	generamve	

model	when	construcmng	the	structure	and	thinking	about	
the	model	assumpmons	

id136	in	bayesian	networks	

       compumng	marginal	probabilimes	in	tree	structured	bayesian	

networks	is	easy	
       the	algorithm	called	   belief	propagamon   	generalizes	what	we	showed	for	

hidden	markov	models	to	arbitrary	trees	

x1	

x2	

x3	

x4	

x5	

x6	

y1	

y2	

y3	

y4	

y5	

y6	

       wait   	this	isn   t	a	tree!	what	can	we	do?	

label

y

x1

x2

x3

. . .

xn

features

   

id136	in	bayesian	networks	

      

in	some	cases	(such	as	this)	we	can	transform	this	into	what	is	
called	a	   juncmon	tree   ,	and	then	run	belief	propagamon	

approximate	id136	

       there	is	also	a	wealth	of	approximate	id136	algorithms	that	can	

be	applied	to	bayesian	networks	such	as	these	

       markov	chain	monte	carlo	algorithms	repeatedly	sample	

assignments	for	esmmamng	marginals	

       varia4onal	id136	algorithms	(determinismc)	   nd	a	simpler	

distribumon	which	is	   close   	to	the	original,	then	compute	marginals	
using	the	simpler	distribumon	

maximum	likelihood	esmmamon	in	

ml estimation in id110s
ml estimation in id110s
ml estimation in id110s

bayesian	networks	

suppose that we know the id110 structure g
suppose that we know the id110 structure g
suppose that we know the id110 structure g
let    xi|xpa(i) be the parameter giving the value of the cpd p(xi | xpa(i))
let    xi|xpa(i) be the parameter giving the value of the cpd p(xi | xpa(i))
let    xi|xpa(i) be the parameter giving the value of the cpd p(xi | xpa(i))
id113 corresponds to solving:
id113 corresponds to solving:
id113 corresponds to solving:

max
max
max
   
   
   

1
1
1
m
m
m

mxm=1
mxm=1
mxm=1

log p(xm;    )
log p(xm;    )
log p(xm;    )

subject to the non-negativity and id172 constraints
subject to the non-negativity and id172 constraints
subject to the non-negativity and id172 constraints
this is equal to:
this is equal to:
this is equal to:

max
max
max
   
   
   

1
1
1
m
m
m

mxm=1
mxm=1
mxm=1

log p(xm;    ) = max
log p(xm;    ) = max
log p(xm;    ) = max
   
   
   

1
1
1
m
m
m

nxi=1
mxm=1
mxm=1
nxi=1
mxm=1
nxi=1
mxm=1
nxi=1
mxm=1
nxi=1
mxm=1
nxi=1

1
1
1
m
m
m

log p(x m
log p(x m
log p(x m
i
i
i

log p(x m
log p(x m
log p(x m
i
i
i

| xm
pa(i);    )
| xm
pa(i);    )
| xm
pa(i);    )

| xm
pa(i);    )
| xm
pa(i);    )
| xm
pa(i);    )

= max
= max
= max
   
   
   

the optimization problem decomposes into an independent optimization
the optimization problem decomposes into an independent optimization
the optimization problem decomposes into an independent optimization
problem for each cpd! has a simple closed-form solution.
problem for each cpd! has a simple closed-form solution.
problem for each cpd! has a simple closed-form solution.
david sontag (nyu)
david sontag (nyu)
david sontag (nyu)

lecture 10, april 11, 2013
lecture 10, april 11, 2013
lecture 10, april 11, 2013

id114
id114
id114

17 / 22
17 / 22
17 / 22

returning	to	id91   	

       clusters	may	overlap	
       some	clusters	may	be	
   wider   	than	others	
       can	we	model	this	

explicitly?	

       with	what	id203	is	
a	point	from	a	cluster?	

probabilismc	id91	

       try	a	probabilismc	model!	

       allows	overlaps,	clusters	of	di   erent	
       can	tell	a	genera*ve	story	for	

size,	etc.	

data	
      p(y)p(x|y)	

       challenge:	we	need	to	esmmate	

model	parameters	without	
labeled	ys		

y	

??	

??	

??	

??	

??	

   	

x1	

x2	

0.1	 2.1	

0.5	

-1.1	

0.0	 3.0	

-0.1	

-2.0	

0.2	 1.5	

   	

   	

gaussian	mixture	models	

       p(y):	there	are	k	components	
       p(x|y):	each	component	generates	data	from	a	mul>variate	gaussian	

with	mean	  i	and	covariance	matrix	  i	

each	data	point	assumed	to	have	been	sampled	from	a	genera4ve	process:		
1.    choose	component	i	with	id203	p(y=i)					[mul*nomial]	
2.    generate	datapoint	~	n(mi,	  i	)	

p(x = x j |y = i) =
    
(2  )m /2 ||   i ||1/2 exp    
        

1

1
2 x j       i
(

)t

   1 x j       i
(
  i

)

    
        

    

by	   :ng	this	model	(unsupervised	
learning),	we	can	learn	new	insights	
about	the	data	

  1	

  2	

  3	

mulmvariate	gaussians	

p(x = x j |y = i) =
																p(x=xj)= 

1

#
(2  )m/2 ||   i ||1/2 exp    
$%

1
2 x j      i
(

)t

   1 x j      i
(
  i

&
)
'(

  	   	ideid4y	matrix	

mulmvariate	gaussians	

p(x = x j |y = i) =
																p(x=xj)= 

1

#
(2  )m/2 ||   i ||1/2 exp    
$%

1
2 x j      i
(

)t

   1 x j      i
(
  i

&
)
'(

  	=	diagonal	matrix	
xi	are	independent	ala	gaussian	nb	

mulmvariate	gaussians	

p(x = x j |y = i) =
																p(x=xj)= 

1

#
(2  )m/2 ||   i ||1/2 exp    
$%

1
2 x j      i
(

)t

   1 x j      i
(
  i

&
)
'(

  	=	arbitrary	(semide   nite)	matrix:		
	-	speci   es	rotamon	(change	of	basis)	
	-	eigenvalues	specify	relamve	elongamon	

mulmvariate	gaussians	

eigenvalue,	  ,	of	  	

covariance	matrix,	  ,	=	
degree	to	which	xi	vary	
together	

p(x = x j |y = i) =
																p(x=xj)= 

1

#
(2  )m/2 ||   i ||1/2 exp    
$%

1
2 x j      i
(

)t

   1 x j      i
(
  i

&
)
'(

modelling	erupmon	of	geysers	

old	faithful	data	set	

	

n
o
m
p
u
r
e
o
t
	
e
m
t

i

	

duramon	of	last	erupmon	

modelling	erupmon	of	geysers	

old	faithful	data	set	

single	gaussian	

mixture	of	two	gaussians	

marginal	distribumon	for	mixtures	of	

gaussians	

component	

mixing	coe   cient	

k=3	

marginal	distribumon	for	mixtures	of	

gaussians	

learning	mixtures	of	gaussians	

original	data	(hypothesized)	

observed	data	(y	missing)	

inferred	y   s	(learned	model)	

shown	is	the	posterior	id203	that	a	point	was	generated	
from	ith	gaussian:	

pr(y = i | x)

ml	esmmamon	in	supervised	setng	

       univariate	gaussian	

       mixture	of	mul4variate	gaussians	

ml	esmmate	for	each	of	the	mulmvariate	gaussians	is	given	by:	

k	

  ml =

1
n

n
   
j=1

xn

k	

  ml =

1
n

n
   
j=1

(

x j      ml

k	

) x j      ml
(

k	

)t

just	sums	over	x	generated	from	the	k   th	gaussian	

what	about	with	unobserved	data?	
       maximize	marginal	likelihood:	

      argmax  	   j	p(xj)	=	argmax	   j	   k=1	p(yj=k,	xj)	

k	

       almost	always	a	hard	problem!	
      usually	no	closed	form	solumon	
      even	when	lgp(x,y)	is	convex,	lgp(x)	generally	isn   t   	
      many	local	opmma	

expectamon	
maximizamon	

1977:	dempster,	laird,	&	rubin	

the	em	algorithm	

       a	clever	method	for	maximizing	marginal	

likelihood:	
       argmax  	   j	p(xj)	=	argmax  	   j	   k=1
      based	on	coordinate	descent.	easy	to	implement	

k	p(yj=k,	xj)	

(eg,	no	line	search,	learning	rates,	etc.)	

       alternate	between	two	steps:	

      compute	an	expectamon	
      compute	a	maximizamon	

       not	magic:	s4ll	op4mizing	a	non-convex	

func4on	with	lots	of	local	op4ma	
       the	computamons	are	just	easier	(ozen,	signi   cantly	so)	

em:	two	easy	steps	
k	p(yj=k,	xj	;	  )	=	   j	lg	   k=1

objec>ve:	argmax  	lg   j	   k=1

k	p(yj=k,	xj;	  )		

data:	{xj	|	j=1	..	n}		
       e-step:	compute	expectamons	to	      ll	in   	missing	y	values	

according	to	current	parameters,	  		
       for	all	examples	j	and	values	k	for	yj,	compute:	p(yj=k	|	xj;	  )		

       m-step:	re-esmmate	the	parameters	with	   weighted   	id113	

esmmates	
       set	  new	=	argmax  	   j	   k

	p(yj=k	|	xj	;  old)	log	p(yj=k,	xj	;	  )	

par>cularly	useful	when	the	e	and	m	steps	have	closed	form	solu>ons	

gaussian	mixture	example:	start	

azer	   rst	iteramon	

azer	2nd	iteramon	

azer	3rd	iteramon	

azer	4th	iteramon	

azer	5th	iteramon	

azer	6th	iteramon	

azer	20th	iteramon	

em	for	gmms:	only	learning	means	(1d)	
iterate:		on	the	t   th	iteramon	let	our	esmmates	be	

e-step	

  t	=	{	  1

(t),	  2

(t)	   	  k

(t)	}	

	compute	   expected   	classes	of	all	datapoints	

p yj = k x j,  1...  k
(

    
)     exp    
    
    

1
2  2 (x j       k)2

    
    p yj = k
(
    

)

m-step	

	compute	most	likely	new	  s	given	class	expectamons	

    

  k  =  

m
p yj = k x j
(
)
   
j =1
m
p yj = k x j
(
   
j =1

x j

)

    

what	if	we	do	hard	assignments?	

iterate:		on	the	t   th	iteramon	let	our	esmmates	be	

e-step	

  t	=	{	  1

(t),	  2

(t)	   	  k

(t)	}	

p yj = k xj,  1...  k
(

	compute	   expected   	classes	of	all	datapoints	
    
    p yj = k
(
)
    
  	represents	hard	
assignment	to	   most	
likely   	or	nearest	
cluster	

	compute	most	likely	new	  s	given	class	expectamons	

1
2  2 (xj      k)2

    
)     exp    
    
    

m-step	

    

  k  =  

x j

m
p yj = k x j
)
(
   
j =1
m
p yj = k x j
(
   
j =1

)

  k  =  

m       yj = k,x j
) x j
(
j =1
m
   yj = k,x j
)
(
   
j =1

	equivalent	to	id116	id91	algorithm!!!	

    

    

e.m.	for	general	gmms	
iterate:		on	the	t   th	iteramon	let	our	esmmates	be	
(t),	p2

(t)	   	  k

  t	=	{	  1

(t),	  2

(t),	  1

(t)	   	  k

(t),	  2

(t),	p1

(t)	   	pk

(t)	}	

(t)	is	shorthand	for	
pk
esmmate	of	p(y=k)	on	
t   th	iteramon	

e-step	

	compute	   expected   	classes	of	all	datapoints	for	each	class	

p yj = k x j;  t
(

)     pk

(t)p x j;  k

(

(t),  k

(t)

)

evaluate	id203	of	a	
mul*variate	a	gaussian	at	xj	

m-step			

		compute	weighted	id113	for	  	given	expected	classes	above	

    

t +1

) =

(

  k

 x j
p yj = k x j;  t
)
(
p yj = k x j;  t
)
(

   
j
   
j

    

   
j

=

(t +1)

pk
    

p yj = k x j;  t
(

)

m

   
j

t +1

) =

(

  k

p yj = k x j;  t
(

(

t +1
)

  x j       k
)
[
p yj = k x j;  t
(

t +1
)

(

]t

] x j       k
[
 
)

   
j

m	=	#training	examples	

the	general	learning	problem	with	missing	data	

       marginal	likelihood:	x	is	observed,	
	

	

	
	

	
				z	(e.g.	the	class	labels	y)	is	missing:	

	

	

	

	

	
	

       objecmve:	find	argmax  	l(  :data)	
       assuming	hidden	variables	are	missing	completely	at	random	
(otherwise,	we	should	explicitly	model	why	the	values	are	missing)	

propermes	of	em	

       one	can	prove	that:	

      em	converges	to	a	local	maxima	
      each	iteramon	improves	the	log-likelihood	

       how?	(same	as	id116)	

      likelihood	objecmve	instead	of	id116	objecmve	
      m-step	can	never	decrease	likelihood	

derivation of em algorithm

em	pictorially	

l(  n+1)
l(  n+1|  n)
l(  n) = l(  n|  n)

l(  )
l(  |  n)

likelihood	
objecmve	

lower	bound	
at	iter	n	

l(  )

l(  |  n)

  n

  n+1

  

(figure from tutorial by sean borman)

figure 2: graphical interpretation of a single iteration of the em algorithm:
the function l(  |  n) is bounded above by the likelihood function l(  ). the
functions are equal at    =   n. the em algorithm chooses   n+1 as the value of   
11 / 13
for which l(  |  n) is a maximum. since l(  )     l(  |  n) increasing l(  |  n) ensures
that the value of the likelihood function l(  ) is increased at each step.

lecture 11, april 12, 2012

david sontag (nyu)

id114

we have now a function, l(  |  n) which is bounded above by the likelihood

what	you	should	know	

       mixture	of	gaussians	
       em	for	mixture	of	gaussians:	

       how	to	learn	maximum	likelihood	parameters	in	the	case	of	unlabeled	data	
       relamon	to	id116		

       two	step	algorithm,	just	like	id116	
       hard	/	soz	id91	
       probabilismc	model	

       remember,	em	can	get	stuck	in	local	minima,	

       and	empirically	it	does	

		

