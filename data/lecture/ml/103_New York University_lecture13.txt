hierarchical)&)spectral)id91)

lecture)13)

david&sontag&

new&york&university&

slides adapted from luke zettlemoyer, vibhav gogate, 

carlos guestrin, andrew moore, dan klein 

agglomerative id91 

       agglomerative id91: 

       first merge very similar instances 
       incrementally build larger clusters out 

of smaller clusters 

       algorithm: 

       maintain a set of clusters 
       initially, each instance in its own 
cluster 
       repeat: 

       pick the two closest clusters 
       merge them into a new cluster 
       stop when there   s only one cluster left 

       produces not one id91, but a 
family of id91s represented 
by a dendrogram 

agglomerative id91 
       how should we define (cid:1)closest(cid:2) for clusters 

with multiple elements? 

agglomerative id91 
       how should we define (cid:1)closest(cid:2) for clusters 

with multiple elements? 

       many options: 
       closest pair 
 
       farthest pair   
 
       average of all pairs 

 (single-link id91) 

 (complete-link id91) 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

       different choices create                        

different id91 behaviors 

agglomerative id91 
       how should we define (cid:1)closest(cid:2) for clusters 

with multiple elements? 

closest pair  
(single-link id91) 

single link example

farthest pair 
(complete-link id91) 

complete link example

1

3

2

4

5

7

6

8

1

3

2

4

5

7

6

8

[pictures from thorsten joachims] 

id91&behavior&

average 

farthest 

nearest 

mouse tumor data from [hastie et al.] 

agglomera<ve&id91&

when&can&this&be&expected&to&work?&

closest pair  
(single-link id91) 

single link example

strong separation property: 
all points are more similar to points in 
their own cluster than to any points in 
any other cluster 

1

3

2

4

5

7

6

8

then, the true id91 corresponds to 
some pruning of the tree obtained by 
single-link id91! 

slightly weaker (stability) conditions are 
solved by average-link id91 

(balcan et al., 2008) 

spectral)id91)

slides adapted from james hays, alan fern, and tommi jaakkola 

3.5

3

2.5

2

1.5

1

0.5

3.5

3

2.5

2

1.5

1

0.5

3.5

3

2.5

2

spectral)id91)

1.5

1

3.5

3

2.5

2

1.5

1

0.5

3.5

4

4.5

5

0

0

0.5

1

1.5

0

0

2

0.5

2.5

1

3

1.5

3.5

2

4

2.5

4.5

3

5

3.5

4

4.5

5

rows of y (jittered, randomly subsampled) for twocircles

0.4

0.6

0.8

1

lineandballs, 3 clusters (meila and shi algorithm)

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

5

id116 

two circles, 2 clusters (k   means)

5

squiggles, 4 clusters

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0.5

1

1.5

0

0

2

0.5

2.5

1

3

1.5

3.5

2

4

2.5

4.5

3

5

3.5

4

4.5

5

nips, 8 clusters (kannan et al. algorithm)

5

threecircles   joined, 3 clusters

[shi & malik    00; ng, jordan, weiss nips    01] 

4.5

4.5

4

3.5

4

3.5

0.5

0

0

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1

0.8

0.6

0.4

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

0

0

0.5

1

1.5

spectral id91 

twocircles, 2 clusters

threecircles   joined, 2 clusters

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

0

0

0.5

1

1.5

rows of y (jittered, randomly subsampled) for twocircles

two circles, 2 clusters (k   means)

5

4.5

4

3.5

3

2.5

2

1.5

1

3

2.5

2

1.5

1

0.5

spectral)id91)

0.5

1.5

2.5

3.5

4.5

0.5

1.5

2.5

0

1

2

3

4

5

0

0

1

2

0

squiggles, 4 clusters
lineandballs, 3 clusters

twocircles, 2 clusters
fourclouds, 2 clusters

3

3.5

4

4.5

5

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

nips, 8 clusters

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

squiggles, 4 clusters

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

5
5

4.5
4.5

4
4

3.5
3.5

3
3

2.5
2.5

2
2

1.5
1.5

1
1

0.5
0.5

0
0
0
0

5
5

4.5
4.5

4
4

3.5
3.5

3
3

2.5
2.5

2
2

1.5
1.5

1
1

0.5
0.5

0
0
0
0

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

3.5
3.5

4

4

4.5
4.5

5

5

threecircles   joined, 3 clusters

twocircles, 2 clusters

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

3.5
3.5

4

4

4.5
4.5

5

5

0.5
0.5

1

1

1.5
1.5

2

2

2.5

2.5

3

3

3.5

3.5

4

4

4.5

4.5

5

5

rows of y (jittered, randomly subsampled) for twocircles

threecircles   joined, 2 clusters

0.5
   0.6

1
   0.4

1.5
   0.2

2

0

2.5

0.2

3

3.5
0.4

4
0.6

4.5
0.8

5

1

lineandballs, 3 clusters (meila and shi algorithm)

two circles, 2 clusters (k   means)

0.5

5

5

4.5
4.5

4

4

3.5
3.5

3

3

2.5
2.5

2

2

1.5
1.5

1

1

0.5
0.5

0

0
0

0

5

1

4.5

0.8
4
0.6

3.5

0.4
3

0.2

2.5

2

0

1.5
   0.2

1
   0.4
0.5
   0.6
0

0

5

5

4.5
4.5

4

4

3.5
3.5

3

[figures from ng, jordan, weiss nips    01] 

threecircles   joined, 3 clusters

threecircles   joined, 3 clusters (connected components)
rows of y (jittered, randomly subsampled) for twocircles

5
1

5

4.5

4

3.5

3

4.5
0.8

4
0.6
3.5
0.4

3

2.5

2

1.5

1

0.5

0

0

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

5

4.5

4

3.5

0.5

1

1.5

2

2.5

3

3.5

4

threecircles   joined, 2 clusters

0.5

1

1.5

2

2.5

3

3.5

4

two circles, 2 clusters (k   means)

0.5

1

1.5

2

2.5

3

3.5

4

nips, 8 clusters (kannan et al. algorithm)

spectral)id91)

))group)points)based)on)links)in)a)graph)

a 

b 

[slide from james hays] 

!"#(cid:1)$"(cid:1)%&'($'(cid:1)$)'(cid:1)*&(+)(cid:1),

    -$(cid:1)./(cid:1)0"11"2(cid:1)$"(cid:1)3/'(cid:1)((cid:1)*(3//.(2(cid:1)4'&2'5(cid:1)$"(cid:1)

0"1+3$'(cid:1)/.1.5(&.$6(cid:1)7'$#''2(cid:1)"78'0$/

(cid:3)(cid:1)

(cid:2)

(cid:1)

    92'(cid:1)0"35:(cid:1)0&'($'(cid:1)

    ;(cid:1)<3556(cid:1)0"22'0$':(cid:1)=&(+)
    4(cid:2)2'(&'/$(cid:1)2'.=)7"&(cid:1)=&(+)(cid:1)>'(0)(cid:1)2":'(cid:1)./(cid:1)"256(cid:1)

0"22'0$':(cid:1)$"(cid:1).$/(cid:1)4(cid:2)2'(&'/$(cid:1)2'.=)7"&/?

[slide from alan fern] 

a 

b 

can)we)use)minimum)cut)for)

id91?)

889

methods, the segmentation criteria used in most of them are
based on local properties of the graph. because perceptual
grouping is about extracting the global impressions of a
scene, as we saw earlier, this partitioning criterion often

in this paper, we propose a new graph-theoretic criterion
for measuring the goodness of an image partition  the
normalized cut. we introduce and justify this criterion in
this criterion can be
formulated as a generalized eigenvalue problem. the
eigenvectors can be used to construct good partitions of
the image and the process can be continued recursively as
desired (section 2.1). section 3 gives a detailed explanation
of the steps of our grouping algorithm. in section 4, we
show experimental results. the formulation and minimiza-
tion of the normalized cut criterion draws on a body of

fig. 1. a case where minimum cut gives a bad partition.

[shi & malik    00] 

between two groups. instead of looking at the value of total
edge weight connecting the two partitions, our measure
computes the cut cost as a fraction of the total edge

graph(cid:1)partitioning

graph(cid:1)terminologies

    degree(cid:1)of(cid:1)nodes

    volume(cid:1)of(cid:1)a(cid:1)set

graph(cid:1)cut

    consider(cid:1)a(cid:1)partition(cid:1)of(cid:1)the(cid:1)graph(cid:1)into(cid:1)two(cid:1)parts(cid:1)a(cid:1)

and(cid:1)b

    cut(a,(cid:1)b):(cid:1)sum(cid:1)of(cid:1)the(cid:1)weights(cid:1)of(cid:1)the(cid:1)set(cid:1)of(cid:1)edges(cid:1)that(cid:1)

connect(cid:1)the(cid:1)two(cid:1)groups

    an(cid:1)intuitive(cid:1)goal(cid:1)is(cid:1)find(cid:1)the(cid:1)partition(cid:1)that(cid:1)(cid:1)minimizes(cid:1)

the(cid:1)cut

normalized(cid:1)cut

    consider(cid:1)the(cid:1)connectivity(cid:1)between(cid:1)groups(cid:1)

relative(cid:1)to(cid:1)the(cid:1)volume(cid:1)of(cid:1)each(cid:1)group

ncut

(

ba
,

)

(cid:3)

)

cut
(
vol

ba
,
a
(
)

(cid:1)

)

cut
(
vol

ba
,
b
)
(

a

b

ncut

(

ba
,

)

(cid:3)

cut

(

ba
,

)

vol
(
vol

a
vol
)
(cid:1)
vol
a
)
(
(

b
(
b
)

)

minimized(cid:1)when(cid:1)vol(a)(cid:1)and(cid:1)vol(b)(cid:1)are(cid:1)equal.(cid:1)
thus(cid:1)encourage(cid:1)balanced(cid:1)cut

solving(cid:1)ncut

    how(cid:1)to(cid:1)minimize(cid:1)ncut?
wj
iw
;
 
matrix,
 
),(
similarity
 
 the
(cid:3)
ji
,
(cid:6)
iw
j
iid
,(
);
 the
diag.
),(
 
matrix,
 
 
(cid:3)
ai
ix
n
1)(,}1,1{ in
.
 vector 
(cid:4)(cid:5)(cid:3)

let 
be 
be dlet 
x
let 
a be 

w

(cid:2)

j

    with(cid:1)some(cid:1)simplifications,(cid:1)we(cid:1)can(cid:1)show:

min

x

ncut

x
)(

(cid:3)

min

y

ywdy
)
t

(
y

t

(cid:2)
dy
rayleigh(cid:1)quotient

subject(cid:1)to:

01 (cid:3)dyt

(y takes discrete values)

np(cid:2)hard!

solving(cid:1)ncut

by(cid:1)solving(cid:1)generalized(cid:1)eigenvalue(cid:1)system:

    relax(cid:1)the(cid:1)optimization(cid:1)problem(cid:1)into(cid:1)the(cid:1)continuous(cid:1)domain(cid:1)

(cid:2)(cid:1)(cid:3)(cid:34)(cid:18)(cid:29)(cid:14)(cid:39)(cid:15)(cid:18) subject(cid:1)to(cid:1)(cid:18)(cid:29)(cid:14)(cid:18)(cid:40)(cid:6)

    which(cid:1)gives:(cid:14)(cid:39)(cid:15)(cid:18)(cid:40)(cid:19)(cid:14)(cid:18)
    note(cid:1)that(cid:1)(cid:14)(cid:39)(cid:15)(cid:6)(cid:40)(cid:5),(cid:1)so(cid:1)the(cid:1)first(cid:1)eigenvector(cid:1)is(cid:1)(cid:18)(cid:20)(cid:40)(cid:6)
with(cid:1)eigenvalue(cid:1)(cid:5).

    the(cid:1)second(cid:1)smallest(cid:1)eigenvector(cid:1)is(cid:1)the(cid:1)real(cid:1)valued(cid:1)solution(cid:1)to(cid:1)

this(cid:1)problem!!

2(cid:2)way(cid:1)normalized(cid:1)cuts

1. compute(cid:1)the(cid:1)affinity(cid:1)matrix(cid:1)w,(cid:1)compute(cid:1)the(cid:1)

degree(cid:1)matrix(cid:1)(d),(cid:1)d(cid:1)is(cid:1)diagonal(cid:1)and(cid:1)

(cid:33)(cid:8)(cid:30)

2. solve(cid:1)

,(cid:1)where(cid:1)

is(cid:1)

called(cid:1)the(cid:1)laplacian matrix

3. use(cid:1)the(cid:1)eigenvector(cid:1)with(cid:1)the(cid:1)second(cid:1)smallest(cid:1)
eigen(cid:2)value(cid:1)to(cid:1)bipartition(cid:1)the(cid:1)graph(cid:1)into(cid:1)two(cid:1)
parts.

creating(cid:1)bi(cid:2)partition(cid:1)using(cid:1)2nd

eigenvector

   

   

sometimes(cid:1)there(cid:1)is(cid:1)not(cid:1)a(cid:1)clear(cid:1)threshold(cid:1)to(cid:1)split(cid:1)
based(cid:1)on(cid:1)the(cid:1)second(cid:1)vector(cid:1)since(cid:1)it(cid:1)(cid:1)takes(cid:1)
continuous(cid:1)values
how(cid:1)to(cid:1)choose(cid:1)the(cid:1)splitting(cid:1)point?(cid:1)
a) pick(cid:1)a(cid:1)constant(cid:1)value(cid:1)(0,(cid:1)or(cid:1)0.5).
b) pick(cid:1)the(cid:1)median(cid:1)value(cid:1)as(cid:1)splitting(cid:1)point.
c)

look(cid:1)for(cid:1)the(cid:1)splitting(cid:1)point(cid:1)that(cid:1)has(cid:1)the(cid:1)minimum(cid:1)ncut
value:
1. choose(cid:1)n possible(cid:1)splitting(cid:1)points.
2. compute(cid:1)ncut value.
3.

pick(cid:1)minimum.

spectral id91: example

6

5

4

3

2

1

0

   1

   2
   3

   2

   1

0

1

2

3

4

5

6

5

4

3

2

1

0

   1

   2
   4

   2

0

2

4

6

tommi jaakkola, mit csail

18

spectral id91: example cont   d

0.5

0.4

0.3

0.2

0.1

0

   0.1

   0.2

   0.3

   0.4

   0.5
0

5

10

15

20

25

30

35

40

components of the eigenvector corresponding to the second
largest eigenvalue

tommi jaakkola, mit csail

19

k(cid:2)way(cid:1)partition?

    recursive(cid:1)bi(cid:2)partitioning(cid:1)(hagen(cid:1)et(cid:1)al.,^91)

    recursively(cid:1)apply(cid:1)bi(cid:2)partitioning(cid:1)algorithm(cid:1)in(cid:1)a(cid:1)

hierarchical(cid:1)divisive(cid:1)manner.

    disadvantages:(cid:1)inefficient,(cid:1)unstable

    cluster(cid:1)multiple(cid:1)eigenvectors

    build(cid:1)a(cid:1)reduced(cid:1)space(cid:1)from(cid:1)multiple(cid:1)eigenvectors.
    commonly(cid:1)used(cid:1)in(cid:1)recent(cid:1)papers
    a(cid:1)preferable(cid:1)approach`(cid:1)its(cid:1)like(cid:1)doing(cid:1)dimension(cid:1)

reduction(cid:1)then(cid:1)k(cid:2)means

