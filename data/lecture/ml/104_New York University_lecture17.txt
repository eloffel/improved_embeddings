introduc)on	to	bayesian	methods	

(con)nued)	-	lecture	17	

david	sontag	

new	york	university	

slides adapted from luke zettlemoyer, carlos guestrin, dan klein, 

and vibhav gogate 

the	na  ve	bayes	classi   er	

       given:	

       prior	p(y)	
       n	condibonally	independent	
features	x	given	the	class	y	
       for	each	xi,	we	have	
likelihood	p(xi|y)	

y 

x1 

x2 

xn 

       decision	rule:	

if certain assumption holds, nb is optimal classifier! 
(they typically don   t) 

what has to be learned? 

1 
2 
3 
4 
5 
6 
7 
8 
9 
0 

0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 

1  0.01 
2  0.05 
3  0.05 
4  0.30 
5  0.80 
6  0.90 
7  0.05 
8  0.60 
9  0.50 
0  0.80 

1  0.05 
2  0.01 
3  0.90 
4  0.80 
5  0.90 
6  0.90 
7  0.25 
8  0.85 
9  0.60 
0  0.80 

arg max
arg max
w

arg max
arg max
w
w
w

= arg max
= arg max
w
       given	dataset	
w
w

       count(a=a,b=b)	  	number	of	examples	where	a=a	and	

id113	for	the	parameters	of	nb	

+
+

+
+

2   2
2   2

2   2
2   2

2   2
2   2

2   2
2   2

= arg max
= arg max
w

 [tj  pi wihi(xj)]2
ln    1
      2    n
 [tj  pi wihi(xj)]2
ln    1
      2    n
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
ln    1
      2    n
 [tj  pi wihi(xj)]2
ln    1
      2    n
nxj=1
nxj=1
 [tj  pi wihi(xj)]2
 [tj  pi wihi(xj)]2
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
nxj=1
nxj=1
[tj  xi
nxj=1
[tj  xi
nxj=1
[tj  xi
nxj=1
[tj  xi
py0 count(y = y )
py0 count(y = y )
py0 count(y = y )
py0 count(y = y )
px0 count(xi = x , y = y)
px0 count(xi = x , y = y)

count(xi = x, y = y)
count(xi = x, y = y)

= arg min
= arg min
= arg min
= arg min
w
w
w
w

count(y = y)
count(y = y)

count(y = y)
count(y = y)

wihi(xj)]2
wihi(xj)]2

wihi(xj)]2
wihi(xj)]2

p (y = y) =
p (y = y) =
       observabon	distribubon:		

p (y = y) =
p (y = y) =

b=b	

       prior:	

p (xi = x|y = y) =
p (xi = x|y = y) =

       id113	for	discrete	nb,	simply:	

what	about	if	there	is	missing	data?	
       one	of	the	key	strengths	of	bayesian	approaches	is	that	

missing data 

they	can	naturally	handle	missing	data	

       suppose don   t have value for some attribute xi 

       applicant   s credit history unknown 
       some medical test not performed on patient 
       how to compute p(x1=x1     xj=?     xd=xd | y) 

       easy with na  ve bayes 

      

ignore attribute in instance 
where its value is missing 

       compute likelihood based on observed attributes 
       no need to    fill in    or explicitly model missing values 
       based on conditional independence between attributes 

       ex: three coin tosses: event = {x1=h, x2=?, x3=t} 

       general case: xj has missing value 

[slide from victor lavrenko and nigel goddard] 

copyright    victor lavrenko, 2013 

copyright    victor lavrenko, 2013 

missing data (2) 

what	about	if	there	is	missing	data?	
       ex: three coin tosses: event = {x1=h, x2=?, x3=t} 

       event = head, unknown (either head or tail), tail  
       event = {h,h,t} + {h,t,t}  
       p(event) = p(h,h,t) + p(h,t,t) 

       general case: xj has missing value 

copyright    victor lavrenko, 2013 

[slide from victor lavrenko and nigel goddard] 

computational complexity 

naive bayes = linear classi   er

i   [1, n]
theorem: assume that                for all              . 
then, the naive bayes classi   er is de   ned by

xi   {0, 1}

and

wi = log pr[xi=1|+1]
b = log pr[+1]

x      sgn(w    x + b),
pr[xi=1| 1]   log pr[xi=0|+1]
where                                                  
pr[xi=0| 1]
pr[ 1] + n
i=1 log pr[xi=0|+1]
pr[xi=0| 1] .
proof: observe that for any              ,
i   [1, n]
pr[xi = 0 |  1]  xi+log
pr[xi = 0 | +1]

pr[xi = 1 | +1]
pr[xi = 1 |  1]   log

pr[xi | +1]
pr[xi |  1]

= log

pr[xi = 0 | +1]
pr[xi = 0 |  1] .

log

[slide from mehyrar mohri] 
mehryar mohri - introduction to machine learning

page

20

outline	of	lectures	

       review	of	id203	
       maximum	likelihood	esbmabon	

2	examples	of	bayesian	classi   ers:	
       na  ve	bayes	
       logis)c	regression	

 vibhav gogate, luke zettlemoyer, carlos guestrin, and dan weld] 

[next several slides adapted from: 

logisbc	regression	

!    learn p(y|x) directly! 

"   assume a particular functional form 
       linear classifier? on one side we say p(y=1|x)=1, and on 

the other p(y=1|x)=0 

       but, this is not differentiable (hard to learn)    doesn   t 

allow for label noise... 

p(y=1)=1 

p(y=1)=0 

logistic function (sigmoid): 

1

1 + e z

1
1

ased estimator (mvue) is sometimes used instead. it is

     2
ik =
     2
ik =

(    j  (y j = yk))  1    
(x j
i       ik)2 (y j = yk)
(    j  (y j = yk))  1    
(x j
i       ik)2 (y j = yk)

(15)
(15)
logisbc	regression	

j
j

       learn p(y|x) directly! 

3 id28
3 id28
id28 is an approach to learning functions of the form f : x     y, or
p(y|x) in the case where y is discrete-valued, and x =    x1 . . .xn    is any vector
id28 is an approach to learning functions of the form f : x     y , or
p(y|x) in the case where y is discrete-valued, and x =    x1 . . .xn    is any vector
containing discrete or continuous variables. in this section we will primarily con-
containing discrete or continuous variables. in this section we will primarily con-
sider the case where y is a boolean variable, in order to simplify notation. in the
sider the case where y is a boolean variable, in order to simplify notation. in the
   nal subsection we extend our treatment to the case where y takes on any    nite
   nal subsection we extend our treatment to the case where y takes on any    nite
number of discrete values.
number of discrete values.
id28 assumes a parametric form for the distribution p(y|x),
id28 assumes a parametric form for the distribution p(y|x),
then directly estimates its parameters from the training data. the parametric
then directly estimates its parameters from the training data. the parametric
model assumed by id28 in the case where y is boolean is:
z
model assumed by id28 in the case where y is boolean is:

       sigmoid applied to a linear 

       assume a particular 

function of the data: 

functional form 

p(y = 1|x) =
p(y = 1|x) =

1
1

1 + exp(w0 +   n
1 + exp(w0 +    n
exp(w0 +   n
exp(w0 +    n
1 + exp(w0 +   n
1 + exp(w0 +    n

i=1 wixi)
i=1 wixi)
i=1 wixi)
i=1 wixi)
i=1 wixi)
i=1 wixi)

p(y = 0|x) =
(17)
p(y = 0|x) =
(17)
notice that equation (17) follows directly from equation (16), because the sum of
notice that equation (17) follows directly from equation (16), because the sum of
these two probabilities must equal 1.
these two probabilities must equal 1.
one highly convenient property of this form for p(y|x) is that it leads to a
one highly convenient property of this form for p(y|x) is that it leads to a
simple linear expression for classi   cation. to classify any given x we generally

(16)
(16)

features can be 
discrete or 
continuous! 

logisbc	funcbon	in	n	dimensions	

sigmoid applied to a linear function of the data: 

features can be discrete or continuous! 

-
2
 
0
 
2
 
4
 
6
-
4
-
2
 
0
 
2
 
4
 
6
 
8
 
1
0
 
0
 
0
.
2
 
0
.
4
 
0
.
6
 
0
.
8
 
1
x
1
x
2
p(y = 1|x) =

5

0
x

0.4

0.2

0
   5

1

1

1

(17)

(16)

i=1 wixi)

i=1 wixi)

i=1 wixi)

i=1 wixi)

i=1 wixi)

i=1 wixi)

i=1 wixi)
i=1 wixi)

   nal subsection we extend our treatment to the case where y takes on any    nite
number of discrete values.

1 + exp(w0 +   n
then directly estimates its parameters from the training data. the parametric
model assumed by id28 in the case where y is boolean is:
1 + exp(w0 +    n
exp(w0 +   n

p(y = 1|x) =

id28 assumes a parametric form for the distribution p(y|x),
then directly estimates its parameters from the training data. the parametric
model assumed by id28 in the case where y is boolean is:

1 + exp(w0 +   n
exp(w0 +    n
notice that equation (17) follows directly from equation (16), because the sum of
1 + exp(w0 +    n
and
these two probabilities must equal 1.

p(y = 0|x) =
logisbc	regression:	decision	boundary		
i=1 wixi)

(17)
exp(w0 +   n

p(y = 1|x) =

p(y = 0|x) =

1 + exp(w0 +   n

exp(w0 +    n

1 + exp(w0 +    n

1 + exp(w0 +    n

p(y = 0|x) =

(16)
1 + exp(w0 +   n

substituting from equations (16) and (17), this becomes

notice that equation (17) follows directly from equation (16), because the sum of
these two probabilities must equal 1.

p(y = 1|x) =
notice that equation (17) follows directly from equation (16), because the sum of
one highly convenient property of this form for p(y|x) is that it leads to a
i=1 wixi)
these two probabilities must equal 1.
simple linear expression for classi   cation. to classify any given x we generally
one highly convenient property of this form for p(y|x) is that it leads to a
want to assign the value yk that maximizes p(y = yk|x). put another way, we
       prediction: output the y with 
notice that equation (17) follows directly from equation (16), because the sum of
i=1 wixi)
simple linear expression for classi   cation. to classify any given x we generally
assign the label y = 0 if the following condition holds:
 = 0 
these two probabilities must equal 1.
p(y = 0|x) =
highest p(y|x) 
i=1 wixi)
want to assign the value yk that maximizes p(y = yk|x). put another way, we
figure 1: form of the logistic function. in id28, p(y|x) is as-
one highly convenient property of this form for p(y|x) is that it leads to a
       for binary y, output y=0 if 
w0
p(y = 0|x)
assign the label y = 0 if the following condition holds:
simple linear expression for classi   cation. to classify any given x we generally
sumed to follow this form.
1 <
+
w.x
p(y = 1|x)
want to assign the value yk that maximizes p(y = yk|x). put another way, we
p(y = 0|x)
assign the label y = 0 if the following condition holds:
one highly convenient property of this form for p(y|x) is that it leads to a
p(y = 1|x)
simple linear expression for classi   cation. to classify any given x we generally
and taking the natural log of both sides we have a linear classi   cation rule that
p(y = 0|x)
want to assign the value yk that maximizes p(y = yk|x). put another way, we
substituting from equations (16) and (17), this becomes
   
assigns label y = 0 if x satis   es
wixi)
p(y = 1|x)
assign the label y = 0 if the following condition holds:
i=1
wixi)
wixi
(18)
n
   
i=1
interestingly, the parametric form of p(y|x) used by id28 is
precisely the form implied by the assumptions of a gaussian naive bayes classi-
   er. therefore, we can view id28 as a closely related alternative to
gnb, though the two can produce different results in many cases.

a linear classifier! 
substituting from equations (16) and (17), this becomes

1 < exp(w0 +
n
   
n
1 < exp(w0 +
p(y = 0|x)
   
0 < w0 +
i=1
p(y = 1|x)
i=1

substituting from equations (16) and (17), this becomes

and assigns y = 1 otherwise.

1 < exp(w0 +

1 < exp(w0 +

wixi)

   
i=1

wixi)

(17)

1 <

1 <

1 <

n

n

likelihood	vs.	condibonal	likelihood	

generabve	(na  ve	bayes)	maximizes	data	likelihood	

discriminabve	(logisbc	regr.)	maximizes	condi)onal	data	likelihood	

focuses	only	on	learning	p(y|x)	-	all	that	mazers	for	classi   cabon			

maximizing	condibonal	log	likelihood	

0 or 1! 

bad news: no closed-form solution to maximize l(w) 
good news: l(w) is concave function of w!  

 no local maxima 
 concave functions easy to optimize 

opbmizing	concave	funcbon	   	

gradient	ascent		

       condibonal	likelihood	for	logisbc	regression	is	concave	!		

gradient: 

update rule: 

learning rate,   >0 

maximize	condibonal	log	likelihood:	gradient	ascent	

yj ln
yj ln

i 

1

1

 w
 w

 w
   w

xj
xj

yj ln
yj ln

wixj
wixj

i )  
i )  

w2ixi)

= j      
= j      

 l(w)
   l(w)
 w
   wi

 l(w)
 l(w)
 w
 w

 
 
wixj
wixj
 w
 w

xj
 l(w)
xj
   l(w)
 w
   wi

p (y = j|x)
p (y = j|x)

wkhk(xi)   2
1 + ew0+pi wixi
1 + ew0+pi wixi

r 1 j=1
r 1   j=1
r 1 j=1
r 1 j=1
p (y = r|x) = 1  
(ti     ti)2 = i    ti   k
p (y = r|x) = 1  
p (y = r|x) = 1  
p (y = j|x)
p (y = r|x) = 1  
p (y = j|x)
error = i
ew0+pi wixi
ew0+pi wixi
= j
ew0+pi wixi
1
= j
ew0+pi wixi
=   j
1
= j
+ (1   yj) ln
1 + ew0+pi wixi
1 + ew0+pi wixi
+ (1   yj) ln
+ (1   yj) ln
1 + ew0+pi wixi
1 + ew0+pi wixi
+ (1   yj) ln
1 + ew0+pi wixi
1 + ew0+pi wixi
p (y = 1|x)     exp(w10 + i
w1ixi)
= j
i yj   p (y j = 1|xj, w)   
= j
i yj   p (y j = 1|xj, w)   
=   j
i yj   p (y j = 1|xj, w)   
= j
i yj   p (y j = 1|xj, w)   
p (y = 2|x)     exp(w20 + i
ln   1 + exp(w0 + i
i )      
ln   1 + exp(w0 + i
i )      
=   j      
ln   1 + exp(w0 +   i
= j        
ln   1 + exp(w0 + i
yj(w0 + i
yj(w0 + i
yj(w0 +   i
yj(w0 + i
 
   
 w
   w
i 
r 1 j=1
i )   
= j    yjxj
i )   
= j    yjxj
i exp(w0 +   i wixj
i exp(w0 +   i wixj
i )   
= j    yjxj
=   j
i exp(w0 +   i wixj
i )
i )
p (y = r|x) = 1  
p (y = j|x)
xj
yjxj
i )
1 + exp(w0 +   i wixj
1 + exp(w0 +   i wixj
i
i  
1 + exp(w0 +   i wixj
i   yj  
i )   
i   yj  
i )   
ew0+pi wixi
exp(w0 +   i wixj
exp(w0 +   i wixj
i   yj  
i )   
= j
= j
exp(w0 +   i wixj
= j
+ (1   yj) ln
1 + exp(w0 +   i wixj
1 + ew0+pi wixi
1 + ew0+pi wixi
1 + exp(w0 +   i wixj
1 + exp(w0 +   i wixj
= j
i yj   p (y j = 1|xj, w)   
yj(w0 + i

2 i
ln   1 + exp(w0 + i

ln p(w)      
   
wixj
   w
    ln p(w)

= j

   l(w)
   wi

= j        

1 + e ax

i )  

i )  
i )  

wixj
wixj

yj ln

i  
i  

   w

i )
i )

xj

w2
i

xj
xj

xj
xj

xj

i )

1

 

1

 l(w)
 l(w)
 w
 w

   l(w)
   wi

i )      
i )      

wixj

wixj

i )      

wixj

gradient	ascent	for	lr	

gradient ascent algorithm: (learning rate    > 0)  

do:	

	for	i=1	to	n:	(iterate	over	features)	

unbl	   change   	<	  	

loop over training examples 
(could also do stochastic gd) 

that   s	all	id113.		how	about	map?	

       one	common	approach	is	to	de   ne	priors	on	w	
       normal	distribubon,	zero	mean,	idenbty	covariance	
          pushes   	parameters	towards	zero	

       regulariza*on	

       helps	avoid	very	large	weights	and	over   bng	

       map	esbmate:	

= j        

   w

   w

i  

wixj

wixj

i )  

i )      

ln   1 + exp(w0 + i
= j    yjxj
yj(w0 + i
= j    yjxj
i )   
i exp(w0 +   i wixj
i   yj  
xj
= j
i  
1 + exp(w0 +   i wixj
map	as	regularizabon	
i   yj  
i )   
exp(w0 +   i wixj
= j
1 + exp(w0 +   i wixj
       adds	log	p(w)	to	objecbve:	

i )   
i exp(w0 +   i wixj
1 + exp(w0 +   i wixj
i )   
exp(w0 +   i wixj
1 + exp(w0 +   i wixj

ln p(w)      

1 + e ax

1 + e ax

w2
i

xj

xj

i )

i )

i )

 

1

1

2 i

ln p(w)      

 

2 i

w2
i

    ln p(w)

   wi

=   wi

    ln p(w)

       quadrabc	penalty:	drives	weights	towards	zero	
       adds	a	negabve	linear	term	to	the	gradients	

=   wi

   wi

quadratic penalty on weights, just like with id166s! 

3

na  ve	bayes				vs.			logis)c	regression	

learning:	h:x	! y	
	

	

	

	

genera)ve		
       assume	funcbonal	form	for		

       p(x|y)		assume	cond	indep		
       p(y)	
       est.	params	from	train	data	
       gaussian	nb	for	cont.	features	
       bayes	rule	to	calc.	p(y|x=	x):	

       p(y	|	x)	   	p(x	|	y)	p(y)	
indirect	computabon		
       can	generate	a	sample	of	the	data	
       can	easily	handle	missing	data	

      

					x	   	features	
					y	   	target	classes	

discrimina)ve	
       assume	funcbonal	form	for		
       p(y|x)			no	assump)ons	

       est	params	from	training	data	
       handles	discrete	&	cont	features	

       directly	calculate	p(y|x=x)	

       can   t	generate	data	sample	

	na  ve	bayes	vs.	logisbc	regression	

       generabve	vs.	discriminabve	classi   ers	
       	asymptobc	comparison		

[ng & jordan, 2002] 

(#	training	examples	#	in   nity)	
      	when	model	correct	

       nb,	linear	discriminant	analysis	(with	class	independent	
variances),	and	logisbc	regression	produce	idenbcal	
classi   ers	

      	when	model	incorrect	

       	lr	is	less	biased	   	does	not	assume	condibonal	
independence	
      therefore	lr	expected	to	outperform	nb	

na  ve	bayes	vs.	logisbc	regression	

       generabve	vs.	discriminabve	classi   ers	
       non-asymptobc	analysis	

      	convergence	rate	of	parameter	esbmates,		

[ng & jordan, 2002] 

			(n	=	#	of	ajributes	in	x)	
       size	of	training	data	to	get	close	to	in   nite	data	solubon	
       na  ve	bayes	needs	o(log	n)	samples	
       logisbc	regression	needs	o(n)	samples	

      na  ve	bayes	converges	more	quickly	to	its	(perhaps	

less	helpful)	asymptobc	esbmates	

na  ve bayes 
id28 

some	

experiments	
from	uci	data	

sets	

  carlos guestrin 2005-2009 

27 

error =   i
error =   i

logisbc	regression	for	discrete	

classi   cabon	
wkhk(xi)   2
wkhk(xi)   2

logisbc	regression	in	more	general	case,	where		
set	of	possible	y	is {y1,   ,yr}	
(ti     ti)2 =   i  ti     k
(ti     ti)2 =   i  ti     k
       de   ne	a	weight	vector	wi	for	each	yi, i=1,   ,r 
p (y = 1|x)     exp(w10 +   i
p (y = 1|x)     exp(w10 +   i
p (y = 2|x)     exp(w20 +   i
p (y = 2|x)     exp(w20 +   i

w2ixi)
w2ixi)

w1ixi)
w1ixi)

    

p(y=y1|x) 
biggest 

       also	called	   sol-max   	loss	

p(y=y3|x) 
biggest 

p(y=y2|x) 
biggest 

