learning	
   theory	
   and	
   decision	
   trees	
   

lecture	
   10	
   

david	
   sontag	
   

new	
   york	
   university	
   

slides adapted from carlos guestrin & luke zettlemoyer 

what	
   about	
   con:nuous	
   hypothesis	
   spaces?	
   

       con:nuous	
   hypothesis	
   space:	
   	
   

      |h|	
   =	
      	
   
      in   nite	
   variance???	
   

       only	
   care	
   about	
   the	
   maximum	
   number	
   of	
   

points	
   that	
   can	
   be	
   classi   ed	
   exactly!	
   

how	
   many	
   points	
   can	
   a	
   linear	
   boundary	
   classify	
   

exactly?	
   (1-     d)	
   

2 points: 

yes!! 

3 points: 

no    

etc (8 total) 

shalering	
   and	
   vapnik   chervonenkis	
   dimension	
   

a	
   set	
   of	
   points	
   is	
   sha$ered	
   by	
   a	
   hypothesis	
   
space	
   h	
   i   :	
   

      for	
   all	
   ways	
   of	
   spli+ng	
   the	
   examples	
   into	
   

posi:ve	
   and	
   nega:ve	
   subsets	
   

      there	
   exists	
   some	
   consistent	
   hypothesis	
   h	
   
the	
   vc	
   dimension	
   of	
   h	
   over	
   input	
   space	
   x	
   

      the	
   size	
   of	
   the	
   largest	
      nite	
   subset	
   of	
   x	
   

shalered	
   by	
   h	
   

how	
   many	
   points	
   can	
   a	
   linear	
   boundary	
   classify	
   

exactly?	
   (2-     d)	
   

5

3 points: 

yes!! 

4 points: 

no    

figure 1. three points in r2, shattered by oriented lines.

2.3. the vc dimension and the number of parameters

etc. 

the vc dimension thus gives concreteness to the notion of the capacity of a given set
of functions.
intuitively, one might be led to expect that learning machines with many
parameters would have high vc dimension, while learning machines with few parameters
would have low vc dimension. there is a striking counterexample to this, due to e. levin
and j.s. denker (vapnik, 1995): a learning machine with just one parameter, but with
in   nite vc dimension (a family of classi   ers is said to have in   nite vc dimension if it can
shatter l points, no matter how large l). de   ne the step function   (x), x     r : {  (x) =
1    x > 0;   (x) =    1    x     0}. consider the one-parameter family of functions, de   ned by
(4)
you choose some number l, and present me with the task of    nding l points that can be
shattered. i choose them to be:

f(x,   )       (sin(  x)), x,        r.

[figure from chris burges] 

xi = 10   i,

i = 1,       , l.

you specify any labels you like:

(5)

how	
   many	
   points	
   can	
   a	
   linear	
   boundary	
   classify	
   

exactly?	
   (d-     d)	
   

       a	
   linear	
   classi   er	
      j=1..dwjxj	
   +	
   b	
   	
   can	
   represent	
   all	
   

assignments	
   of	
   possible	
   labels	
   to	
   d+1	
   points	
   	
   
       but	
   not	
   d+2!	
   
       thus,	
   vc-     dimension	
   of	
   d-     dimensional	
   linear	
   classi   ers	
   is	
   
       bias	
   term	
   b	
   required	
   
       rule	
   of	
   thumb:	
   number	
   of	
   parameters	
   in	
   model	
   o_en	
   

d+1	
   

(but	
   not	
   always)	
   matches	
   max	
   number	
   of	
   points	
   	
   

       ques:on:	
   can	
   we	
   get	
   a	
   bound	
   for	
   error	
   as	
   a	
   func:on	
   of	
   

the	
   vc-     dimension?	
   

pac	
   bound	
   using	
   vc	
   dimension	
   

       vc	
   dimension:	
   number	
   of	
   training	
   points	
   that	
   can	
   be	
   
classi   ed	
   exactly	
   (shalered)	
   by	
   hypothesis	
   space	
   h!!!	
   
       measures	
   relevant	
   size	
   of	
   hypothesis	
   space	
   

       same	
   bias	
   /	
   variance	
   tradeo   	
   as	
   always	
   

       now,	
   just	
   a	
   func:on	
   of	
   vc(h)	
   

       note:	
   all	
   of	
   this	
   theory	
   is	
   for	
   binary	
   classi   ca:on	
   
       can	
   be	
   generalized	
   to	
   mul:-     class	
   and	
   also	
   regression	
   

cs683 scribe notes
anand bhaskar (ab394), ilya sukhar (is56)

4/28/08 (part 1)

1.1 rectangles
let   s try rectangles with horizontal and vertical edges. in order to show that the vc dimension is 4 (in this
case), we need to show two things:

1 vc-dimension
a set system (x, s) consists of a set x along with a collection of subsets of x. a subset containing a     x is
what	
   is	
   the	
   vc-     dimension	
   of	
   rectangle	
   
shattered by s if each subset of a can be expressed as the intersection of a with a subset in s.

1. there exist 4 points that can be shattered.

vc-dimension of a set system is the cardinality of the largest subset of a that can be shattered.

it   s clear that capturing just 1 point and all 4 points are both trivial. the    gure below shows how we
can capture 2 points and 3 points.

1.1 rectangles
let   s try rectangles with horizontal and vertical edges. in order to show that the vc dimension is 4 (in this
case), we need to show two things:

classi   ers?	
   

1. there exist 4 points that can be shattered.

       first,	
   show	
   that	
   there	
   are	
   4	
   points	
   that	
   can	
   be	
   

it   s clear that capturing just 1 point and all 4 points are both trivial. the    gure below shows how we
can capture 2 points and 3 points.

shalered:	
   

so, yes, there exists an arrangement of 4 points that can be shattered.

2. no set of 5 points can be shattered.

       then,	
   show	
   that	
   no	
   set	
   of	
   5	
   points	
   can	
   be	
   

suppose we have 5 points. a shattering must allow us to select all 5 points and allow us to select 4
points without the 5th.

so, yes, there exists an arrangement of 4 points that can be shattered.

2. no set of 5 points can be shattered.

shalered:	
   

suppose we have 5 points. a shattering must allow us to select all 5 points and allow us to select 4
points without the 5th.

our minimum enclosing rectangle that allows us to select all    ve points is de   ned by only four points
    one for each edge. so, it is clear that the    fth point must lie either on an edge or on the inside of
the rectangle. this prevents us from selecting four points without the    fth.

our minimum enclosing rectangle that allows us to select all    ve points is de   ned by only four points
    one for each edge. so, it is clear that the    fth point must lie either on an edge or on the inside of
the rectangle. this prevents us from selecting four points without the    fth.

[figures from anand bhaskar, ilya sukhar] 

generaliza:on	
   bounds	
   using	
   vc	
   dimension	
   

       linear	
   classi   ers:	
   	
   

      vc(h)	
   =	
   d+1,	
   for	
   d	
   features	
   plus	
   constant	
   term	
   b	
   

       classi   ers	
   using	
   gaussian	
   kernel	
   

      vc(h)	
   =	
   

    

29

euclidean 
distance, 
squared 

figure 11. gaussian rbf id166s of su   ciently small width can classify an arbitrarily large number of
training points correctly, and thus have in   nite vc dimension

[figure from chris burges] 

now we are left with a striking conundrum. even though their vc dimension is in   nite (if
the data is allowed to take all values in rdl), id166 rbfs can have excellent performance
(sch  olkopf et al, 1997). a similar story holds for polynomial id166s. how come?

[figure from mblondel.org] 

gap	
   tolerant	
   classi   ers	
   

30

       suppose	
   data	
   lies	
   in	
   rd	
   in	
   a	
   ball	
   of	
   diameter	
   d	
   
       consider	
   a	
   hypothesis	
   class	
   h	
   of	
   linear	
   classi   ers	
   that	
   can	
   only	
   

classify	
   point	
   sets	
   with	
   margin	
   at	
   least	
   m	
   
classi   ers corresponds to one of the sets of classi   ers in figure 4, with just three nested
subsets of functions, and with h1 = 1, h2 = 2, and h3 = 3.

       what	
   is	
   the	
   largest	
   set	
   of	
   points	
   that	
   h	
   can	
   shaler?	
   

  =0
y=0 

  =1
y=+1 

d = 2

m = 3/2

  =0
y=0 

  =   1
y=-1 

y=0 
  =0

cannot	
   shaler	
   these	
   points:	
   

< m

figure 12. a gap tolerant classi   er on data in r2.

vc dimension = min   d,

d2

m 2   

these ideas can be used to show how gap tolerant classi   ers implement structural risk
minimization. the extension of the above example to spaces of arbitrary dimension is
encapsulated in a (modi   ed) theorem of (vapnik, 1995):

[figure from chris burges] 

theorem 6 for data in rd, the vc dimension h of gap tolerant classi   ers of minimum

m = 2  = 2

1
||w||

id166	
   a@empts	
   to	
   
minimize	
   ||w||2,	
   which	
   
minimizes	
   vc-     dimension!!!	
   

gap	
   tolerant	
   classi   ers	
   

30

       suppose	
   data	
   lies	
   in	
   rd	
   in	
   a	
   ball	
   of	
   diameter	
   d	
   
       consider	
   a	
   hypothesis	
   class	
   h	
   of	
   linear	
   classi   ers	
   that	
   can	
   only	
   

classify	
   point	
   sets	
   with	
   margin	
   at	
   least	
   m	
   
classi   ers corresponds to one of the sets of classi   ers in figure 4, with just three nested
subsets of functions, and with h1 = 1, h2 = 2, and h3 = 3.

       what	
   is	
   the	
   largest	
   set	
   of	
   points	
   that	
   h	
   can	
   shaler?	
   

  =0
y=0 

  =1
y=+1 

d = 2

m = 3/2

  =0
y=0 

  =   1
y=-1 

y=0 
  =0

what	
   is	
   r=d/2	
   for	
   the	
   gaussian	
   kernel?	
   

r = max

x || (x)||

x p (x)     (x)
x pk(x, x)

= max

= max

= 1 !	
   

figure 12. a gap tolerant classi   er on data in r2.

vc dimension = min   d,

d2

m 2   

these ideas can be used to show how gap tolerant classi   ers implement structural risk
minimization. the extension of the above example to spaces of arbitrary dimension is
encapsulated in a (modi   ed) theorem of (vapnik, 1995):

[figure from chris burges] 

theorem 6 for data in rd, the vc dimension h of gap tolerant classi   ers of minimum

what	
   you	
   need	
   to	
   know	
   

       finite	
   hypothesis	
   space	
   

       derive	
   results	
   
       coun:ng	
   number	
   of	
   hypothesis	
   

       complexity	
   of	
   the	
   classi   er	
   depends	
   on	
   number	
   of	
   

points	
   that	
   can	
   be	
   classi   ed	
   exactly	
   
       finite	
   case	
      	
   number	
   of	
   hypotheses	
   considered	
   
       in   nite	
   case	
      	
   vc	
   dimension	
   
       vc	
   dimension	
   of	
   gap	
   tolerant	
   classi   ers	
   to	
   jus:fy	
   id166	
   

       bias-     variance	
   tradeo   	
   in	
   learning	
   theory	
   

decision	
   trees	
   

machine	
   learning	
   in	
   the	
   er	
   

triage information 
(blood pressure, heart 
rate, temperature,    ) 

md comments 
(free text) 

specialist consults 

30 min 

t=0 

repeated vital signs 
(continuous values) 
measured every 30 s 

lab results 
(continuous valued) 

physician 
documentation 

2 hrs 

disposition 

can	
   we	
   predict	
   infec:on?	
   

triage information 
(blood pressure, heart 
rate, temperature,    ) 

md comments 
(free text) 

physician 
documentation 

specialist consults 

many crucial decisions 
about a patient   s care are 
made here! 

lab results 
(continuous valued) 

repeated vital signs 
(continuous values) 
measured every 30 s 

can	
   we	
   predict	
   infec:on?	
   
       previous	
   automa:c	
   approaches	
   based	
   on	
   simple	
   criteria:	
   

       temperature	
   <	
   96.8	
     f	
   or	
   >	
   100.4	
     f	
   
       heart	
   rate	
   >	
   90	
   beats/min	
   
       respiratory	
   rate	
   >	
   20	
   breaths/min	
   

       too	
   simpli   ed   	
   e.g.,	
   heart	
   rate	
   depends	
   on	
   age!	
   

can	
   we	
   predict	
   infec:on?	
   

       these	
   are	
   the	
   alributes	
   we	
   have	
   for	
   each	
   pa:ent:	
   

       temperature	
   
       heart	
   rate	
   (hr)	
   
       respiratory	
   rate	
   (rr)	
   
       age	
   
       acuity	
   and	
   pain	
   level	
   
       diastolic	
   and	
   systolic	
   blood	
   pressure	
   (dbp,	
   sbp)	
   
       oxygen	
   satura:on	
   (sao2)	
   

       we	
   have	
   these	
   alributes	
   +	
   label	
   (infec:on)	
   for	
   200,000	
   
       let   s	
   learn	
   to	
   classify	
   infec:on	
   

pa:ents!	
   

predic:ng	
   infec:on	
   using	
   decision	
   trees	
   

