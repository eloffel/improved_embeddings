introduction to machine learning

david sontag

new york university

lecture 21, april 14, 2016

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

1 / 14

expectation maximization

algorithm is as follows:

1 write down the complete log-likelihood log p(x, z;   ) in such a way

that it is linear in z
initialize   0, e.g. at random or using a good    rst guess

2

3 repeat until convergence:

  t+1 = arg max

  

m(cid:88)m=1

ep(zm|xm;  t )[log p(xm, z;   )]

notice that log p(xm, z;   ) is a random function because z is unknown
by linearity of expectation, objective decomposes into expectation
terms and data terms
   e    step corresponds to computing the objective (i.e., the
expectations)
   m    step corresponds to maximizing the objective

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

2 / 14

derivation of em algorithm

(figure from tutorial by sean borman)

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

3 / 14

l(  )l(  |  n)  n  n+1l(  n)=l(  n|  n)l(  n+1|  n)l(  n+1)l(  )l(  |  n)  figure2:graphicalinterpretationofasingleiterationoftheemalgorithm:thefunctionl(  |  n)isboundedabovebythelikelihoodfunctionl(  ).thefunctionsareequalat  =  n.theemalgorithmchooses  n+1asthevalueof  forwhichl(  |  n)isamaximum.sincel(  )   l(  |  n)increasingl(  |  n)ensuresthatthevalueofthelikelihoodfunctionl(  )isincreasedateachstep.wehavenowafunction,l(  |  n)whichisboundedabovebythelikelihoodfunctionl(  ).additionally,observethat,l(  n|  n)=l(  n)+   (  n|  n)=l(  n)+!zp(z|x,  n)lnp(x|z,  n)p(z|  n)p(z|x,  n)p(x|  n)=l(  n)+!zp(z|x,  n)lnp(x,z|  n)p(x,z|  n)=l(  n)+!zp(z|x,  n)ln1=l(  n),(16)sofor  =  nthefunctionsl(  |  n)andl(  )areequal.ourobjectiveistochooseavaluesof  sothatl(  )ismaximized.wehaveshownthatthefunctionl(  |  n)isboundedabovebythelikelihoodfunctionl(  )andthatthevalueofthefunctionsl(  |  n)andl(  )areequalatthecurrentestimatefor  =  n.therefore,any  whichincreasesl(  |  n)willalsoincreasel(  ).inordertoachievethegreatestpossibleincreaseinthevalueofl(  ),theemalgorithmcallsforselecting  suchthatl(  |  n)ismaximized.wedenotethisupdatedvalueas  n+1.thisprocessisillustratedinfigure(2).7application to mixture models

this model is a type of (discrete) mixture model

called multinomial naive bayes (a word can appear multiple times)
document is generated from a single topic

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

4 / 14

i=1tond=1todwidprior distributionover topicstopic of doc dword  topic-worddistributions  zdem for mixture models

the complete likelihood is p(w, z;   ,   ) =(cid:81)d
n(cid:89)i=1

p(wd , zd ;   ,   ) =   zd

  zd ,wid

d=1 p(wd , zd ;   ,   ), where

trick #1: re-write this as

p(wd , zd ;   ,   ) =

  1[zd =k]
k

k(cid:89)k=1

n(cid:89)i=1

k(cid:89)k=1

  1[zd =k]
k,wid

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

5 / 14

i=1tond=1todwidprior distributionover topicstopic of doc dword  topic-worddistributions  zdem for mixture models

thus, the complete log-likelihood is:

log p(w, z;   ,   ) =

d(cid:88)d=1(cid:32) k(cid:88)k=1

1[zd = k] log   k +

1[zd = k] log   k,wid(cid:33)

n(cid:88)i=1

k(cid:88)k=1

in the    e    step, we take the expectation of the complete log-likelihood with
respect to p(z | w;   t,   t), applying linearity of expectation, i.e.

ep(z|w;  t ,  t )[log p(w, z;   ,   )] =

d(cid:88)d=1(cid:32) k(cid:88)k=1

p(zd = k | w;   t,   t) log   k +

p(zd = k | w;   t,   t) log   k,wid(cid:33)

n(cid:88)i=1

k(cid:88)k=1

in the    m    step, we maximize this with respect to    and   

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

6 / 14

to

d(cid:88)d=1(cid:32) k(cid:88)k=1
d(cid:88)d=1

log   k

k(cid:88)k=1

em for mixture models

just as with complete data, this maximization can be done in closed form

first, re-write expected complete log-likelihood from

p(zd = k | w;   t,   t) log   k +

p(zd = k | w;   t,   t) log   k,wid(cid:33)

n(cid:88)i=1

k(cid:88)k=1

p(zd = k | wd ;   t,   t)+

k(cid:88)k=1

w(cid:88)w =1

log   k,w

d(cid:88)d=1

ndw p(zd = k | wd ;   t,   t)

we then have that

  t+1

k = (cid:80)d
d=1 p(zd = k | wd ;   t,   t)
(cid:80)k
  k=1(cid:80)d

d=1 p(zd =   k | wd ;   t,   t)

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

7 / 14

id44 (lda)

topic models are powerful tools for exploring large data sets and for
making id136s about the content of documents

many applications in information retrieval, document summarization,
and classi   cation

lda is one of the simplest and most widely used topic models

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

8 / 14

!"#$%&'()*"+,#)+"/,9#)1+.&),3&'(1"65%51:5)2,'0("'1.&/,0,"'1-.&/,0,"'12,'3$14$3,5)%1&(2,#)16$332,)%1)+".()165)&65//1)"##&.165)7&(65//18""(65//1--complexity+of+id136+in+latent+dirichlet+alloca6on+david+sontag,+daniel+roy+(nyu,+cambridge)+w66+topic+models+are+powerful+tools+for+exploring+large+data+sets+and+for+making+id136s+about+the+content+of+documents+documents+topics+poli6cs+.0100+president+.0095+obama+.0090+washington+.0085+religion+.0060+almost+all+uses+of+topic+models+(e.g.,+for+unsupervised+learning,+informa6on+retrieval,+classi   ca6on)+require+probabilis)c+id136:+new+document+what+is+this+document+about?+words+w1,+   ,+wn+   distribu6on+of+topics+ t= p(w|z=t)    +religion+.0500+hindu+.0092+judiasm+.0080+ethics+.0075+buddhism+.0016+sports+.0105+baseball+.0100+soccer+.0055+basketball+.0050+football+.0045+   +   +weather+.50+   nance+.49+sports+.01+generative model for a document in lda

1 sample the document   s topic distribution    (aka topic vector)

       dirichlet(  1:t )

t=1 are    xed hyperparameters. thus    is a distribution

where the {  t}t
over t topics with mean   t =   t/(cid:80)t(cid:48)   t(cid:48)

2 for i = 1 to n, sample the topic zi of the i   th word

zi|         

3

... and then sample the actual word wi from the zi    th topic

wi|zi       zi

where {  t}t
words)

t=1 are the topics (a    xed collection of distributions on

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

9 / 14

generative model for a document in lda

1 sample the document   s topic distribution    (aka topic vector)

t=1 are hyperparameters.the dirichlet density, de   ned over

       dirichlet(  1:t )

t=1   t = 1}, is:

where the {  t}t
    = {(cid:126)       rt :    t   t     0,(cid:80)t

p(  1, . . . ,   t )    
for example, for t =3 (  3 = 1       1       2):

    t   1

t

t(cid:89)t=1

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

10 / 14

  1=  2=  3=  1  2logpr(  )  1  2logpr(  )  1=  2=  3=generative model for a document in lda

3

... and then sample the actual word wi from the zi    th topic

wi|zi       zi

where {  t}t
words)

t=1 are the topics (a    xed collection of distributions on

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

11 / 14

complexity+of+id136+in+latent+dirichlet+alloca6on+david+sontag,+daniel+roy+(nyu,+cambridge)+w66+topic+models+are+powerful+tools+for+exploring+large+data+sets+and+for+making+id136s+about+the+content+of+documents+documents+topics+poli6cs+.0100+president+.0095+obama+.0090+washington+.0085+religion+.0060+almost+all+uses+of+topic+models+(e.g.,+for+unsupervised+learning,+informa6on+retrieval,+classi   ca6on)+require+probabilis)c+id136:+new+document+what+is+this+document+about?+words+w1,+   ,+wn+   distribu6on+of+topics+ t= p(w|z=t)    +religion+.0500+hindu+.0092+judiasm+.0080+ethics+.0075+buddhism+.0016+sports+.0105+baseball+.0100+soccer+.0055+basketball+.0050+football+.0045+   +   +weather+.50+   nance+.49+sports+.01+example of using lda

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

12 / 14

(blei, introduction to probabilistic topic models, 2011)

gene     0.04dna      0.02genetic  0.01.,,life     0.02evolve   0.01organism 0.01.,,brain    0.04neuron   0.02nerve    0.01...data     0.02number   0.02computer 0.01.,,topicsdocumentstopic proportions andassignmentsfigure1:theintuitionsbehindlatentdirichletallocation.weassumethatsomenumberof   topics,   whicharedistributionsoverwords,existforthewholecollection(farleft).eachdocumentisassumedtobegeneratedasfollows.firstchooseadistributionoverthetopics(thehistogramatright);then,foreachword,chooseatopicassignment(thecoloredcoins)andchoosethewordfromthecorrespondingtopic.thetopicsandtopicassignmentsinthis   gureareillustrative   theyarenot   tfromrealdata.seefigure2fortopics   tfromdata.modelassumesthedocumentsarose.(theinterpretationofldaasaprobabilisticmodelis   eshedoutbelowinsection2.1.)weformallyde   neatopictobeadistributionovera   xedvocabulary.forexamplethegeneticstopichaswordsaboutgeneticswithhighid203andtheevolutionarybiologytopichaswordsaboutevolutionarybiologywithhighid203.weassumethatthesetopicsarespeci   edbeforeanydatahasbeengenerated.1nowforeachdocumentinthecollection,wegeneratethewordsinatwo-stageprocess.1.randomlychooseadistributionovertopics.2.foreachwordinthedocument(a)randomlychooseatopicfromthedistributionovertopicsinstep#1.(b)randomlychooseawordfromthecorrespondingdistributionoverthevocabulary.thisstatisticalmodelre   ectstheintuitionthatdocumentsexhibitmultipletopics.eachdocumentexhibitsthetopicswithdi   erentproportion(step#1);eachwordineachdocument1technically,themodelassumesthatthetopicsaregenerated   rst,beforethedocuments.3  dz1dznd  1  t   plate    notation for lda model

variables within a plate are replicated in a conditionally independent manner

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

13 / 14

  dirichlet hyperparametersi=1tond=1tod  dwidzidtopic distributionfor documenttopic of word i of doc dword  topic-worddistributionscomparison of mixture and admixture models

model on left is a mixture model

called multinomial naive bayes (a word can appear multiple times)
document is generated from a single topic

model on right (lda) is an admixture model

document is generated from a distribution over topics

david sontag (nyu)

introduction to machine learning

lecture 21, april 14, 2016

14 / 14

i=1tond=1todwidprior distributionover topicstopic of doc dword  topic-worddistributions  zd  dirichlet hyperparametersi=1tond=1tod  dwidzidtopic distributionfor documenttopic of word i of doc dword  topic-worddistributions