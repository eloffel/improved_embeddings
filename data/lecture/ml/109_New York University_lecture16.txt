introduc)on	to	bayesian	methods	

(con)nued)	-	lecture	16	

david	sontag	

new	york	university	

slides adapted from luke zettlemoyer, carlos guestrin, dan klein, 

and vibhav gogate 

outline	of	lectures	

       review	of	id203	

(a?er	midterm)	
maximum	likelihood	es)ma)on	
2	examples	of	bayesian	classi   ers:	
       na  ve	bayes	
       logisic	regression	

bayes    rule 

       two ways to factor a joint distribution over two variables: 

       dividing, we get: 

       why is this at all helpful? 

       let   s us build one conditional from its reverse 
       often one conditional is tricky but the other one is simple 
       foundation of many practical systems (e.g. asr, mt) 

      

in the running for most important ml equation! 

returning	to	thumbtack	example   	

       p(heads)	=	  ,		p(tails)	=	1-  	

    

       flips	are	i.i.d.:		

d={xi | i=1   n},  p(d |    ) =   ip(xi |    ) 

      independent	events	
      idenically	distributed	according	to	bernoulli	

distribuion	

       sequence	d	of	  h	heads	and	  t	tails			

called the    likelihood    of the data under the model 

maximum	likelihood	esimaion	

       data:	observed	set	d	of	  h	heads	and	  t	tails			
       hypothesis:	bernoulli	distribuion		
       learning:	   nding	   is	an	opimizaion	problem	

      what   s	the	objecive	funcion?	

       id113:	choose	  	to	maximize	id203	of	d	

brief article
january 11, 2012
january 11, 2012
the author

january 11, 2012

january 11, 2012

your	   rst	parameter	learning	algorithm	
    = arg max

ln p (d |  )
      = arg max

   

   

ln p (d |    )

   

      = arg max

ln     h

ln     h

      = arg max

ln p (d |    )
ln p (d |    )
ln     h
ln p (d |    )
       set	derivaive	to	zero,	and	solve!	
      = arg max
ln p (d |    )
ln     h
ln     h
d
d
   
[ln     h (1      ) t ] =
ln     h (1    )   t
ln p (d |    ) =
ln p (d |  ) =
d   
ln     h
d 
d
[ln     h (1      ) t ] =
ln p (d |    ) =
d
[ln     h (1      ) t ] =
d   
[ h ln     +  t ln(1      )]
d
[ln     h (1      ) t ] =
d   
[ h ln     +  t ln(1      )]
d
[ln     h (1      ) t ] =
[ h ln     +  t ln(1      )]
d   
d
d   
ln(1      ) =
=  h
d   
 h
 h
     
     

ln(1      ) =
ln(1      ) =

d
d   
= 0
= 0

d
d
d   
d 

d
d   

d
d   

ln     +  t
 t
 t
1      
1      

d
d   

[ h ln     +  t ln(1      )]

[ h ln     +  t ln(1      )]
 h
     

= 0

 t
1      

data 

l(   ;d) = ln p (d|   )

   

what	if	i	have	prior	beliefs?		

       billionaire	says:	wait,	i	know	that	the	thumbtack	
is	   close   	to	50-50.	what	can	you	do	for	me	now?	

       you	say:	i	can	learn	it	the	bayesian	way   	
       rather	than	esimaing	a	single	  ,	we	obtain	a	

distribuion	over	possible	values	of	  	

in the beginning 

after observations 

pr(   )

observe flips 
e.g.: {tails, tails} 

pr(    | d)

   

   

        2e 2n    2     p (mistake)

1      

        2e 2n    2     p (mistake)

bayesian	learning	

ln         ln 2   2n    2

prior 

       use	bayes   	rule!	
ln         ln 2   2n    2

data likelihood 

n    

ln(2/   )

2   2

3.8
id172 
= 190
0.02

posterior 

ln(2/   )

ln(2/0.05)

2   2

ln(2/0.05)

n    
n    
2     0.12    
       or equivalently: 
3.8
       for uniform priors, this reduces to 
p (   )     1
0.02

2     0.12    
id113! 
p (   )     1

= 190

p (    | d)     p (d |    )

n    

bayesian	learning	for	thumbtacks	

likelihood:	
       what	should	the	prior	be?	
       represent	expert	knowledge	
       simple	posterior	form	

       for	binary	variables,	commonly	used	prior	is	the	

beta	distribuion:	

ln         ln 2   2n    2

ln         ln 2   2n    2

beta	prior	distribuion	   	p(  )	

ln(2/   )

n    

ln(2/   )

2   2

n    

2   2

ln(2/0.05)

2     0.12    
p (   )     1

n    

= 190

3.8
0.02

3.8
0.02

ln(2/0.05)

2     0.12    
p (   )     1

= 190

       since	the	beta	distribuion	is	conjugate	to	the	bernoulli	distribuion,	the	

posterior	distribuion	has	a	paricularly	simple	form:	

p (    | d)     p (d |    )

p (    | d)     p (d |    )

p (    | d)         h (1      ) t       h 1(1      )   t  1

=       h + h 1 (1      )   t + t  1

p (    | d)         h (1    ) t       h 1(1    )   t  1 =     h +   h 1(1    ) t +   t+1 = beta( h+   h,  t +   t )

1

using	bayesian	id136	for	predicion	

       we	now	have	a	distribu)on	over	parameters	
       for	any	speci   c	f,	a	funcion	of	interest,	compute	the	

expected	value	of	f:	

integral	is	o?en	hard	to	compute	

      
       as	more	data	is	observed,	posterior	is	more	concentrated	
       map	(maximum	a	posteriori	approximaion):	use	most	likely	

parameter	to	approximate	the	expectaion	

outline	of	lectures	

       review	of	id203	
       maximum	likelihood	esimaion	

2	examples	of	bayesian	classi   ers:	
       na  ve	bayes	
       logisic	regression	

bayesian	classi   caion	

       problem	statement:	

      given	features	x1,x2,   ,xn	
      predict	a	label	y	

[next several slides adapted from: 

 vibhav gogate, jonathan huang, luke zettlemoyer, carlos 

guestrin, and dan weld] 

example	applicaion	

       digit	recogni)on	

x

y

5 

classifier 

       x1,   ,xn	   	{0,1}	(black	vs.	white	pixels)	
       y	   	{0,1,2,3,4,5,6,7,8,9}	

the	bayes	classi   er	

      

if	we	had	the	joint	distribuion	on	x1,   ,xn	and	y,	could	predict	
using:	

       (for	example:	what	is	the	id203	that	the	image	

represents	a	5	given	its	pixels?)	

       so	   	how	do	we	compute	that?	

the	bayes	classi   er	

       use	bayes	rule!	

likelihood 

prior 

id172 constant 

       why	did	this	help?		well,	we	think	that	we	might	be	able	to	

specify	how	features	are	   generated   	by	the	class	label	

the	bayes	classi   er	
       let   s	expand	this	for	our	digit	recogniion	task:	

       to	classify,	we   ll	simply	compute	these	probabiliies,	one	per	

class,	and	predict	based	on	which	one	is	largest	

model	parameters	

       how	many	parameters	are	required	to	specify	the	likelihood,	

p(x1,   ,xn|y)	?	
       (supposing	that	each	image	is	30x30	pixels)	

       the	problem	with	explicitly	modeling	p(x1,   ,xn|y)	is	that	

there	are	usually	way	too	many	parameters:	
       we   ll	run	out	of	space	
       we   ll	run	out	of	ime	
       and	we   ll	need	tons	of	training	data	(which	is	usually	not	

available)	

na  ve	bayes	

       na  ve	bayes	assumpion:	

       features	are	independent	given	class:	

       more	generally:	

       how	many	parameters	now?	
       suppose	x	is	composed	of	n	binary	features	

the	na  ve	bayes	classi   er	

       given:	

       prior	p(y)	
       n	condiionally	independent	features	
x1,	   ,	x1,	given	the	class	y	
       for	each	feature	i,	we	specify	p(xi|y)	

y 

x1 

x2 

xn 

       classi   caion	decision	rule:	

if certain assumption holds, nb is optimal classifier! 
(they typically don   t) 

a digit recognizer 

       input: pixel grids 

       output: a digit 0-9 

are	the	na  ve	bayes	assumpions	realisic	here?	

what has to be learned? 

1 
2 
3 
4 
5 
6 
7 
8 
9 
0 

0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 
0.1 

1  0.01 
2  0.05 
3  0.05 
4  0.30 
5  0.80 
6  0.90 
7  0.05 
8  0.60 
9  0.50 
0  0.80 

1  0.05 
2  0.01 
3  0.90 
4  0.80 
5  0.90 
6  0.90 
7  0.25 
8  0.85 
9  0.60 
0  0.80 

arg max
arg max
w

arg max
arg max
w
w
w

= arg max
= arg max
w
       given	dataset	
w
w

       count(a=a,b=b)	  	number	of	examples	where	a=a	and	

id113	for	the	parameters	of	nb	

+
+

+
+

2   2
2   2

2   2
2   2

2   2
2   2

2   2
2   2

= arg max
= arg max
w

 [tj  pi wihi(xj)]2
ln    1
      2    n
 [tj  pi wihi(xj)]2
ln    1
      2    n
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
ln    1
      2    n
 [tj  pi wihi(xj)]2
ln    1
      2    n
nxj=1
nxj=1
 [tj  pi wihi(xj)]2
 [tj  pi wihi(xj)]2
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
nxj=1
nxj=1
[tj  xi
nxj=1
[tj  xi
nxj=1
[tj  xi
nxj=1
[tj  xi
py0 count(y = y )
py0 count(y = y )
py0 count(y = y )
py0 count(y = y )
px0 count(xi = x , y = y)
px0 count(xi = x , y = y)

count(xi = x, y = y)
count(xi = x, y = y)

= arg min
= arg min
= arg min
= arg min
w
w
w
w

count(y = y)
count(y = y)

count(y = y)
count(y = y)

wihi(xj)]2
wihi(xj)]2

wihi(xj)]2
wihi(xj)]2

p (y = y) =
p (y = y) =
       observaion	distribuion:		

p (y = y) =
p (y = y) =

b=b	

       prior:	

p (xi = x|y = y) =
p (xi = x|y = y) =

       id113	for	discrete	nb,	simply:	

id113	for	the	parameters	of	nb	
       training	amounts	to,	for	each	of	the	classes,	averaging	all	of	

the	examples	together:	

arg max

w

w

w

w

+

+

2   2

+
+

b=b	

2   2

2   2
2   2

2   2
2   2

arg max

2   2
2   2

arg max
arg max
w
w

= arg max
       given	dataset	
w
= arg max

       count(a=a,b=b)	  	number	of	examples	where	a=a	and	

nxi=1
 [tj  pi wihi(xj)]2
ln    1
      2    n
 [tj  pi wihi(xj)]2
ln    1
      2    n
nxj=1
 [tj  pi wihi(xj)]2
nxj=1
      2    n
ln    1
nxj=1
 [tj  pi wihi(xj)]2
ln    1
      2    n
nxj=1
 [tj  pi wihi(xj)]2
 [tj  pi wihi(xj)]2
nxj=1
map	esimaion	for	nb	
 [tj  pi wihi(xj)]2
nxj=1
nxj=1
= arg max
= arg max
 [tj  pi wihi(xj)]2
w
nxj=1
nxj=1
[tj  xi
nxj=1
[tj  xi
nxj=1
[tj  xi
nxj=1
[tj  xi
py0 count(y = y )
py0 count(y = y )
py0 count(y = y )
py0 count(y = y )
px0 count(xi = x , y = y)
px0 count(xi = x , y = y)

wihi(xj)]2
       map	esimaion	for	discrete	nb,	simply:	
wihi(xj)]2
count(y = y)
count(y = y)

p (xi = x|y = y) =
p (xi = x|y = y) =
       called	   smoothing   .	corresponds	to	dirichlet	prior!	

p (y = y) =
p (y = y) =
       observaion	distribuion:		

count(xi = x, y = y)
count(xi = x, y = y)

= arg min
w
= arg min
w

count(y = y)
count(y = y)

= arg min
= arg min
w
w

p (y = y) =
p (y = y) =

wihi(xj)]2
wihi(xj)]2

+ |x_i|*a 

       prior:	

+ a 

