slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 8 practical issues: features, evaluation, debugging

furong huang / furongh@cs.umd.edu

what does this mean?
    id88 converges quickly when margin is 

large, slowly when it is small

    bound does not depend on number of training 

examples n, nor on number of features d

    proof guarantees that id88 converges, 

but not necessarily to the max margin 
separator

what you should know

    id88 concepts

   

training/prediction algorithms (standard, voting, 
averaged)

    convergence theorem and what practical guarantees it 

gives us

    how to draw/describe the decision boundary of a 

id88 classifier

    fundamental ml concepts

    determine whether a data set is linearly separable and 

define its margin

    error driven algorithms, online vs. batch algorithms

expressivity

   
   

    conjunctions:

    many functions are linear

!=#$   #&   #'
!=()*+(1  #$+1  #&+1  #'   3), w=[1,0,1,0,1]
!=89:;8(92=>(#$,#&,#')
    xor:!=#$   #?     #$     #?
    non trival dnf:!=#$   #?   #&   #b

    many functions are not

    at least m of n:

   

    but can be made linear

functions can be made linear

    data are not linearly separable in one 

dimension

    not separable if you insist on using a 

specific class of functions 

blown up feature space

data are separable in <!,!#> space

exclusive-or (xor)

practical issues

   

   garbage in, garbage out   

   

learning algorithms can   t compensate for 
useless training examples
    e.g., if all features are irrelevant

    feature design can have bigger impact on 

performance than tweaking the learning 
algorithm

practical issues

classifier
team a
team b
team c
team d

accuracy on test set
80.00
79.90
79.00
78.00

which classifier is the best?
    this result table alone cannot give us the 

answer

    solution: statistical hypothesis testing

practical issues

classifier
team a
team b
team c
team d

accuracy on test set
80.00
79.90
79.00
78.00

is the difference in accuracy between a and b 
statistically significant? 

what is the id203 that the observed difference 
in performance was due to chance? 

a confidence of 95% 

    does not mean
   there is a 95% chance than classifier a 
is better than classifier b    
    it means 
   if i run this experiment 100 times, i 
expect a to perform better than b 95 
times.    

practical issues: debugging!

    you   ve implemented a learning algorithm 
    you try it on some train/dev/test data
    but it doesn   t seem to learn

    what   s going on?

   

   

   

is the data too noisy?
is the learning problem too hard?
is the implementation of the learning algorithm 
buggy?

strategies for isolating
causes of errors

   

is the problem with generalization to test data?
    can learner fit the training data?
    yes: problem is in generalization to test data
    no: problem is in representation (need better features 

or better data)

    train/test mismatch?

    try reselecting train/test by shuffling training data and 

test together

    strategies for isolating

causes of errors

   

   

is algorithm implementation correct?
    measure loss rather than accuracy
    hand-craft a toy dataset

is representation adequate? 
    can you learn if you add a cheating feature that 

perfectly correlates with correct class?

    do you have enough data?

    try training on 80% of the training set, how much 

does it hurt performance?

practical issues: hyperparameter tuning 
with dev set vs. cross-validation

n-fold cross validation

improving input representations 

    feature pruning
    feature id172

    example id172

see ciml 5.3

practical issues: debugging!

    you probably have a bug

   

   

if the learning algorithm cannot overfit the 
training data
if the predictions are incorrect on a toy 2d 
dataset hand-crafted to be learnable 

practical issues: evaluation, beyond 
accuracy
    so far we   ve measured classification 

performance using accuracy

    but this is not a good metric when some errors 

matter mode than others
    given medical record, predict whether patient has 

cancer or not

    given a document collection and a query, find 

documents in collection that are relevant to query

a combined measure: f

    a combined measure that assesses the 

p/r tradeoff is f measure 

$1%+ 1   $ 1(= )*+1%(
1
!=
)*%+(
i.e., with)=1 (that is, $=+*)
    harmonic mean !=*,-,.-

   

    people usually use balanced f-1 measure 

formalizing errors

the learned 

classifier

set of all possible 
classifiers using a fixed 

representation

   

how far is the learned 
classifier f from the 
optimal classifier f*?

quality of the model 

family

aka hypothesis class

the bias/variance trade-off

    trade-off between

   
   

approximation error (bias)
estimation error (variance)

    example:

    consider the always positive classifier

    low variance as a function of a random draw of the 

    strongly biased toward predicting +1 no matter what the 

training set

input

recap: practical issues

   

learning algorithm is only one of many steps in 
designing a ml application

    many things can go wrong, but there are practical 

improving inputs

strategies for
   
    evaluating
    tuning
    debugging

    fundamental ml concepts:  estimation vs. 

approximation error

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

