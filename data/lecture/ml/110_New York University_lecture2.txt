introduction to learning 

lecture 2 

david sontag 

new york university 

slides adapted from luke zettlemoyer, vibhav gogate, 

pedro domingos, and carlos guestrin 

second example: regression 

dataset: 10 (x,y) points generated 
from a sin function, with noise  

t

1
y 
0

   1

0

x 

1

x

       regression: 
        f  : x ! y 
       x =  
   
       y = 
   

[bishop] 

degree-m polynomials 

how about letting f be a degree m polynomial? 

      which one is best? 

1

t

y 
0

   1

1

0

t

y 

   1

0

0

m = 0

x 

1

x

m = 3

x 

1

x

1

t

y 
0

   1

1

t

y 
0

   1

0

0

m = 1

x 

1

x

m = 9

x 

1

x

hypo. space: degree-n polynomials 

t

1

0

   1

0

m = 0

t

1

0

   1

m = 1

t

1

0

   1

m = 3

t

1

0

   1

m = 9

1

x

0

1

x

0

1

x

0

1

x

we measure error using a id168 

1

for regression, a common choice is 
squared loss: 

learning curve 

training
test

 

squared 

s
m
error 
r
e

0.5

the empirical loss of the function f applied 
to the training data is then: 

0

 

3

0
9
measure of model complexity 

6

m

hypo. space: degree-n polynomials 

t

1

0

   1

0

m = 0

t

1

0

   1

m = 1

t

1

0

   1

m = 3

t

1

0

   1

m = 9

1

x

0

1

x

0

1

x

0

1

x

we measure error using a id168 

1

for regression, a common choice is 
squared loss: 

squared 

s
m
error 
r
e

0.5

the empirical loss of the function f applied 
to the training data is then: 

0

 

 

learning curve 

training
test

example of 
overfitting 

3

0
9
measure of model complexity 

6

m

binary classification 

input: email 

      
       output: spam/ham 
       setup: 

       get a large collection of 

example emails, each 
labeled    spam    or    ham    

       note: someone has to hand 
       want to learn to predict 

label all this data! 

labels of new, future emails 
       features: the attributes used to 

make the ham / spam decision 
       words: free! 
       text patterns: $dd, caps 
       non-text: senderincontacts 
           

dear sir. 

first, i must solicit your confidence in this 
transaction, this is by virture of its nature 
as being utterly confidencial and top 
secret.     

to be removed from future 
mailings, simply reply to this 
message and put "remove" in the 
subject. 

99  million email addresses 
  for only $99 

ok, iknow this is blatantly ot but i'm 
beginning to go insane. had an old dell 
dimension xps sitting in the corner and 
decided to put it to use, i know it was 
working pre being stuck in the corner, but 
when i plugged it in, hit the power nothing 
happened. 

the id88 algorithm 

       1957: id88 algorithm invented by rosenblatt  

wikipedia:    a handsome bachelor, he drove a classic mga sports    for several years taught an 

interdisciplinary undergraduate honors course entitled "theory of brain mechanisms" that 
drew students equally from cornell's engineering and liberal arts colleges   this course was 
a melange of ideas .. experimental brain surgery on epileptic patients while conscious, 
experiments on .. the visual cortex of cats, ... analog and digital electronic circuits that 
modeled various details of neuronal behavior (i.e. the id88 itself, as a machine).    

       built on work of hebbs (1949); also developed by widrow-hoff (1960) 

       1960: id88 mark 1 computer     hardware implementation 

       1969: minksky & papert book shows id88s limited to linearly 

separable data, and rosenblatt dies in boating accident 

       1970   s: learning methods for two-layer neural networks 

[william cohen] 

linear classifiers 

       inputs are feature values 
       each feature has a weight 
       sum is the activation 

important note: changing notation! 

       if the activation is: 

       positive, output class 1 
       negative, output class 2 

w1 
w2 
w3 

f1 
f2 
f3 

  	


>0? 

example: spam 

       imagine 3 features (spam is    positive    class): 

1.    free (number of occurrences of    free   ) 
2.    money (occurrences of    money   ) 
3.    bias (intercept, always has value 1) 

   free money    

bias  :  1 
free  :  1 
money :  1 
... 

bias  : -3 
free  :  4 
money :  2 
... 

w.f(x)	
   >	
   0	
   !	
   spam!!!	
   

binary decision rule 

       in the space of feature vectors 

       examples are points 
       any weight vector is a hyperplane 
       one side corresponds to y=+1 
       other corresponds to y=-1 

 
y
e
n
o
m

bias  : -3 
free  :  4 
money :  2 
... 

-1 = ham 

2 

1 

0 

0 

+1 = spam 

1 

free 

w0 = ln

1      
   

+

  2
i0 +   2
i1
2   2
i

the id88 algorithm 
wi =

  i0 +   i1
       start with weight vector =  
       for each training instance (xi, yi): 
       classify with current weights 

   2
i

[y   j   p(y   j|xj, w)]f (xj)

(xi) 

i 

w = w +     j

t 

(xi) 

i 
i 

w = w + [y      y(x;w)]f (x)
       if correct (i.e., y=yi), no change! 
       if wrong: update 

w = w + y   f (x)
i 

i 

geometrical interpretation 

[slide by rong jin] 

what questions should we ask about a 

learning algorithm? 

       what is the id88 algorithm   s running 

time? 

       if a weight vector with small training error 

exists, will id88 find it? 

       how well does the resulting classifier 

generalize to unseen data? 

linearly separable 

called the functional margin 
with respect to the training set 

equivalently, for yt = +1, 

and for yt = -1, 

mistake bound for id88 
       assume the data set d is linearly separable 

with geometric margin   , i.e.,  

* 

       assume 
       theorem: the maximum number of 

mistakes made by the id88 algorithm 
is bounded by  

proof by induction 

. 

. 

(full proof given on board) 

[slide by rong jin] 

problems with the id88 algorithm 

      

if the data isn   t linearly separable, 
no guarantees of convergence or 
training accuracy 

       even if the training data is 

linearly separable, id88 
can overfit 

       averaged id88 is an 
algorithmic modification that 
helps with both issues 
       averages the weight vectors across all 

iterations 

ml methodology 

       data: labeled instances, e.g. emails marked spam/ham 

       training set 
       held out set (sometimes call validation set) 
       test set 
randomly allocate to these three, e.g. 60/20/20 

       features: attribute-value pairs which characterize each x 

       experimentation cycle 
       select a hypothesis f 
     (tune hyperparameters on held-out or validation set) 
       compute accuracy of test set 
       very important: never    peek    at the test set! 

       evaluation 

       accuracy: fraction of instances predicted correctly 

training 

data 

held-out 

data 

test 
data 

linear separators 

"    which of these linear separators is optimal?  

next week: support vector machines 

"    id166s (vapnik, 1990   s) choose the linear separator with the 

largest margin 

"    good according to intuition, theory, practice 

