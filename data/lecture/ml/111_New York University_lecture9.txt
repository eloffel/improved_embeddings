learning	
   theory	
   

lecture	
   9	
   

david	
   sontag	
   

new	
   york	
   university	
   

slides adapted from carlos guestrin & luke zettlemoyer 

roadmap	
   of	
   next	
   lectures	
   

1.    generaliza:on	
   of	
      nite	
   hypothesis	
   spaces	
   
2.    vc-     dimension	
   

       will	
   show	
   that	
   linear	
   classi   ers	
   need	
   to	
   see	
   approximately	
   d	
   training	
   points,	
   

where	
   d	
   is	
   the	
   dimension	
   of	
   the	
   feature	
   vectors	
   

      

explains	
   the	
   good	
   performance	
   we	
   obtained	
   using	
   id88!!!!	
   
(we	
   had	
   a	
   few	
   thousand	
   features)	
   

id88 algorithm 
on spam classification 

3.    margin	
   based	
   generaliza:on	
   
       applies	
   to	
   in   nite	
   dimensional	
   

feature	
   vectors	
   (e.g.,	
   gaussian	
   kernel)	
   

test error 
(percentage 
misclassified) 

number of training examples 

[figure from cynthia rudin] 

how	
   big	
   should	
   your	
   valida:on	
   set	
   be?	
   

      

in	
   ps1,	
   you	
   tried	
   many	
   con   gura:ons	
   of	
   your	
   algorithms	
   (avg	
   vs.	
   
regular	
   id88,	
   max	
   #	
   of	
   itera:ons)	
   and	
   chose	
   the	
   one	
   that	
   had	
   
smallest	
   valida:on	
   error	
   

       suppose	
   in	
   total	
   you	
   tested	
   |h|=40	
   di   erent	
   classi   ers	
   on	
   the	
   

valida:on	
   set	
   of	
   m	
   held-     out	
   e-     mails	
   

       the	
   best	
   classi   er	
   obtains	
   98%	
   accuracy	
   on	
   these	
   m	
   e-     mails!!!	
   
       but,	
   what	
   is	
   the	
   true	
   classi   ca:on	
   accuracy?	
   

       how	
   large	
   does	
   m	
   need	
   to	
   be	
   so	
   that	
   we	
   can	
   guarantee	
   that	
   the	
   

best	
   con   gura:on	
   (measured	
   on	
   validate)	
   is	
   truly	
   good?	
   

a	
   simple	
   se_ng   	
   	
   

h

hc    h 

consistent 
with data 

       classi   ca:on	
   

       m	
   data	
   points	
   
       finite	
   number	
   of	
   possible	
   hypothesis	
   (e.g.,	
   40	
   spam	
   classi   ers)	
   

       a	
   learner	
      nds	
   a	
   hypothesis	
   h	
   that	
   is	
   consistent	
   with	
   

training	
   data	
   
       gets	
   zero	
   error	
   in	
   training:	
   	
   errortrain(h)	
   =	
   0	
   
       i.e.,	
   assume	
   for	
   now	
   that	
   one	
   of	
   the	
   classi   ers	
   gets	
   100%	
   

accuracy	
   on	
   the	
   m	
   e-     mails	
   (we   ll	
   handle	
   the	
   98%	
   case	
   aderward)	
   

       what	
   is	
   the	
   id203	
   that	
   h	
   has	
   more	
   than	
     	
   true	
   error?	
   

       errortrue(h)	
      	
     	


intro	
   to	
   id203:	
   outcomes	
   

       an	
   outcome	
   space	
   speci   es	
   the	
   possible	
   outcomes	
   that	
   we	
   would	
   

like	
   to	
   reason	
   about,	
   e.g.	
   

   = { 

, 

} 

coin toss 

   = { 

, 

, 

, 

, 

, 

}  die toss 

       we	
   specify	
   a	
   id203	
   p(x)	
   for	
   each	
   outcome	
   x	
   such	
   that	
   

p(x)   0,

p(x) = 1

xx2   

e.g.,   p( 

p( 

) = .6 

) = .4 

intro	
   to	
   id203:	
   events	
   

       an	
   event	
   is	
   a	
   subset	
   of	
   the	
   outcome	
   space,	
   e.g.	
   

e = { 

o = { 

, 

, 

, 

, 

}  even die tosses 

}  odd die tosses 

       the	
   id203	
   of	
   an	
   event	
   is	
   given	
   by	
   the	
   sum	
   of	
   the	
   probabili:es	
   

of	
   the	
   outcomes	
   it	
   contains,	
   

p(e) =xx2e

p(x)

e.g.,   p(e) =  p(         ) + p(        ) + p(        ) 

= 1/2,  if fair die 

intro	
   to	
   id203:	
   union	
   bound	
   

       p(a	
   or	
   b	
   or	
   c	
   or	
   d	
   or	
      )	
   

    p(a) + p(b) + p(c) + p(d) +     

a

c

b

d

p(a [ b) = p(a) + p(b)   p(a \ b)

    p(a) + p(b)

q: when is this a tight bound? 

a: for disjoint events 
(i.e., non-overlapping circles) 

intro	
   to	
   id203:	
   independence	
   

       two	
   events	
   a	
   and	
   b	
   are	
   independent	
   if	
   

p(a \ b) = p(a)p(b)

a

b

are these events independent? 
no!  p(a \ b) = 0
p(a)p(b) =    1
6   2

intro	
   to	
   id203:	
   independence	
   

       two	
   events	
   a	
   and	
   b	
   are	
   independent	
   if	
   

p(a \ b) = p(a)p(b)

analogy:	
   outcome	
   space	
   de   nes	
   
all	
   possible	
   sequences	
   of	
   e-     mails	
   
in	
   training	
   set	
   

       suppose	
   our	
   outcome	
   space	
   had	
   two	
   di   erent	
   die:	
   
} 

   = { 

,     , 

, 

, 

2 die tosses 

62 = 36 outcomes 

and the id203 of each outcome is defined as 

p( 

) = a1 b1 

p( 

) = a1 b2 

    

a1	
   
.1	
   

b1	
   
.19	
   

a2	
   
.12	
   

b2	
   
.11	
   

a3	
   
.18	
   

b3	
   
.1	
   

a4	
   
.2	
   

b4	
   
.22	
   

a5	
   
.1	
   

b5	
   
.18	
   

a6	
   
.3	
   

b6	
   
.2	
   

ai = 1

bj = 1

6xi=1
6xj=1

intro	
   to	
   id203:	
   independence	
   

       two	
   events	
   a	
   and	
   b	
   are	
   independent	
   if	
   

p(a \ b) = p(a)p(b)

       are	
   these	
   events	
   independent?	
   

a 

b 

analogy:	
   asking	
   
about	
      rst	
   e-     mail	
   
in	
   training	
   set	
   

p(a) = p( 

) 

p(b) = p( 

) 
= b2

yes!  p(a \ b) = 0p( 
p(a)p(b) =    1
6   2
    p( 

) 

) p( 

) 

=

6xj=1

a1bj = a1

bj = a1

6xj=1

analogy:	
   asking	
   
about	
   second	
   e-     mail	
   
in	
   training	
   set	
   

discrete random variables

intro	
   to	
   id203:	
   discrete	
   random	
   variables	
   
often each outcome corresponds to a setting of various attributes
(e.g.,    age   ,    gender   ,    haspneumonia   ,    hasdiabetes   )
a random variable x is a mapping x :     ! d

d is some set (e.g., the integers)
induces a partition of all outcomes    

for some x 2 d, we say

p(x = x) = p({! 2     : x (!) = x})

   id203 that variable x assumes state x   
notation: val(x ) = set d of all values assumed by x
(will interchangeably call these the    values    or    states    of variable x )

p(x ) is a distribution: px2val(x ) p(x = x) = 1
   = { 

,     , 

id114

david sontag (nyu)

, 

, 

} 

lecture 1, january 31, 2013

2 die tosses 

20 / 44

for some x 2 d, we say

p(x = x) = p({! 2     : x (!) = x})

   id203 that variable x assumes state x   
intro	
   to	
   id203:	
   discrete	
   random	
   variables	
   
notation: val(x ) = set d of all values assumed by x
(will interchangeably call these the    values    or    states    of variable x )

p(x ) is a distribution: px2val(x ) p(x = x) = 1

       e.g.	
   x1	
   may	
   refer	
   to	
   the	
   value	
   of	
   the	
      rst	
   dice,	
   and	
   x2	
   to	
   the	
   value	
   of	
   the	
   

second	
   dice	
   
david sontag (nyu)

       we	
   call	
   two	
   random	
   variables	
   x	
   and	
   y	
   iden+cally	
   distributed	
   if	
   val(x)	
   =	
   

lecture 1, january 31, 2013

id114

20 / 44

val(y)	
   and	
   p(x=s)	
   =	
   p(y=s)	
   for	
   all	
   s	
   in	
   val(x)	
   

p( 

) = a1 b1 

p( 

) = a1 b2 

    

x1	
   and	
   x2	
   not	
   
iden:cally	
   
distributed 

a1	
   
.1	
   

b1	
   
.19	
   

a2	
   
.12	
   

b2	
   
.11	
   

   = { 

a3	
   
.18	
   

b3	
   
.1	
   

, 

a4	
   
.2	
   

b4	
   
.22	
   

, 

a5	
   
.1	
   

b5	
   
.18	
   

a6	
   
.3	
   

b6	
   
.2	
   

,     , 

} 

ai = 1

bj = 1

6xi=1
6xj=1

2 die tosses 

for some x 2 d, we say

p(x = x) = p({! 2     : x (!) = x})

   id203 that variable x assumes state x   
intro	
   to	
   id203:	
   discrete	
   random	
   variables	
   
notation: val(x ) = set d of all values assumed by x
(will interchangeably call these the    values    or    states    of variable x )

p(x ) is a distribution: px2val(x ) p(x = x) = 1

       e.g.	
   x1	
   may	
   refer	
   to	
   the	
   value	
   of	
   the	
      rst	
   dice,	
   and	
   x2	
   to	
   the	
   value	
   of	
   the	
   

second	
   dice	
   
david sontag (nyu)

       we	
   call	
   two	
   random	
   variables	
   x	
   and	
   y	
   iden+cally	
   distributed	
   if	
   val(x)	
   =	
   

lecture 1, january 31, 2013

id114

20 / 44

val(y)	
   and	
   p(x=s)	
   =	
   p(y=s)	
   for	
   all	
   s	
   in	
   val(x)	
   

p( 

) = a1 a1 

p( 

) = a1 a2 

    

a1	
   
.1	
   

a2	
   
.12	
   

a3	
   
.18	
   

x1	
   and	
   x2	
   
iden:cally	
   
distributed 

   = { 

, 

a4	
   
.2	
   

, 

a5	
   
.1	
   

a6	
   
.3	
   

ai = 1

6xi=1

,     , 

} 

2 die tosses 

intro	
   to	
   id203:	
   discrete	
   random	
   variables	
   

       x=x	
   is	
   simply	
   an	
   event,	
   so	
   can	
   apply	
   union	
   bound,	
   etc.	
   
       two	
   random	
   variables	
   x	
   and	
   y	
   are	
   independent	
   if:	
   

p(x = x, y = y) = p(x = x)p(y = y) 8x 2 val(x), y 2 val(y )

joint	
   id203.	
   formally,	
   given	
   by	
   the	
   event	
   x = x \ y = y
e[x] = xx2val(x)

       the	
   expectaeon	
   of	
   x	
   is	
   de   ned	
   as:	
   

      

if	
   x	
   is	
   binary	
   valued,	
   i.e.	
   x	
   is	
   either	
   0	
   or	
   1,	
   then:	
   

e[x] = p(x = 0)    0 + p(x = 1)    1

= p(x = 1)

       linearity	
   of	
   expecta:ons:	
   

eh nxi=1

xii =

nxi=1

e[xi]

p(x = x)x

a	
   simple	
   se_ng   	
   	
   

h

hc    h 

consistent 
with data 

       classi   ca:on	
   

       m	
   data	
   points	
   
       finite	
   number	
   of	
   possible	
   hypothesis	
   (e.g.,	
   40	
   spam	
   classi   ers)	
   

       a	
   learner	
      nds	
   a	
   hypothesis	
   h	
   that	
   is	
   consistent	
   with	
   

training	
   data	
   
       gets	
   zero	
   error	
   in	
   training:	
   	
   errortrain(h)	
   =	
   0	
   
       i.e.,	
   assume	
   for	
   now	
   that	
   one	
   of	
   the	
   classi   ers	
   gets	
   100%	
   

accuracy	
   on	
   the	
   m	
   e-     mails	
   (we   ll	
   handle	
   the	
   98%	
   case	
   aderward)	
   

       what	
   is	
   the	
   id203	
   h	
   correctly	
   classi   es	
   all	
   m	
   data	
   

points	
   given	
   that	
   h	
   has	
   more	
   than	
     	
   true	
   error?	
   
       errortrue(h)	
      	
     	


how	
   likely	
   is	
   a	
   single	
   hypothesis	
   to	
   get	
   

	
   m	
   data	
   points	
   right?	
   

the	
   id203	
   of	
   a	
   hypothesis	
   h	
   incorrectly	
   classifying:	
   

   h =

x(~x,y)

  p(~x, y)1[h(~x) 6= y]

zh
i

let	
   	
   	
   	
   	
   	
   	
   	
   be	
   a	
   random	
   variable	
   that	
   takes	
   two	
   values:	
   1	
   if	
   h	
   correctly	
   classi   es	
   ith	
   	
   
data	
   point,	
   and	
   0	
   otherwise	
   
the	
   zh	
   variables	
   are	
   independent	
   and	
   idenecally	
   distributed	
   (i.i.d.)	
   with	
   

      

      

      

  p(~x, y)1[h(~x) 6= y] =    h
       what	
   is	
   the	
   id203	
   that	
   h	
   classi   es	
   m	
   data	
   points	
   correctly?	
   

pr(zh

i = 0) = x(~x,y)

pr(h gets m iid data points right)  = (1      h)m     e    hm

are	
   we	
   done?	
   

pr(h gets m iid data points right | errortrue(h)       )     e-  m 

       says	
      with	
   id203	
   >	
   1-     e-       m,	
   if	
   h	
   gets	
   m	
   data	
   points	
   
correct,	
   then	
   it	
   is	
   close	
   to	
   perfect	
   (will	
   have	
   error	
      	
     )   	
   

       this	
   only	
   considers	
   one	
   hypothesis!	
   
       suppose	
   1	
   billion	
   classi   ers	
   were	
   tried,	
   and	
   each	
   was	
   a	
   

random	
   func:on	
   

       for	
   m	
   small	
   enough,	
   one	
   of	
   the	
   func:ons	
   will	
   classify	
   all	
   

points	
   correctly	
      	
   but	
   all	
   have	
   very	
   large	
   true	
   error	
   

how	
   likely	
   is	
   learner	
   to	
   pick	
   a	
   bad	
   hypothesis?	
   

pr(h gets m iid data points right | errortrue(h)       )     e-  m 

suppose	
   there	
   are	
   |hc|	
   hypotheses	
   consistent	
   with	
   the	
   training	
   data	
   

       how	
   likely	
   is	
   learner	
   to	
   pick	
   a	
   bad	
   one,	
   i.e.	
   with	
   true	
   error	
      	
     ?	
   
       we	
   need	
   a	
   bound	
   that	
   holds	
   for	
   all	
   of	
   them!	
   

p(errortrue(h1)        or	
   errortrue(h2)        or     or errortrue(h|hc|)       )  

       kp(errortrue(hk)       )  
       k(1-  )m  
    |h|(1-  )m  
    |h| e-m    

! union bound 
! bound on individual hks  
! |hc|     |h|   

! (1-  )     e-  	
   for	
   0        1  

extra	
   analysis	
   

generaliza:on	
   error	
   of	
      nite	
   hypothesis	
   spaces	
   

[haussler	
      88]	
   	
   

we just proved the following result: 

theorem:	
   hypothesis	
   space	
   h	
      nite,	
   dataset	
   d	
   

with	
   m	
   i.i.d.	
   samples,	
   0	
   <	
     	
   <	
   1	
   :	
   for	
   any	
   
learned	
   hypothesis	
   h	
   that	
   is	
   consistent	
   on	
   the	
   
training	
   data:	
   

   (u).   (v) =        
   (u).   (v) =            
   (u).   (v) =        

u2
u2
2
2
= (u.v)2

           .        
         .            
           .        

v2
v2
2
2

= (u1v1 + u2v2)2
= (u1v1 + u2v2)2
= (u1v1 + u2v2)2

           = u2
         = u2
           = u2
ln |h|e m           ln    

using	
   a	
   pac	
   bound	
   
   (u).   (v) = (u.v)d

= (u.v)2

p (errortrue(h)        )     |h|e m           
= (u.v)2
= (u.v)2

argument:	
   since	
   for	
   all	
   h	
   we	
   know	
   that	
   

   	
   with	
   id203	
   1-       	
   the	
   following	
   
holds   	
   (either	
   case	
   1	
   or	
   case	
   2)	
   

   (u).   (v) = (u.v)d
   (u).   (v) = (u.v)d
   (u).   (v) = (u.v)d

	
   typically,	
   2	
   use	
   cases:	
   
       1:	
   pick	
     	
   and	
     ,	
   compute	
   m	
   
       2:	
   pick	
   m	
   and	
     ,	
   compute	
     	


p (errortrue(h)        )     |h|e m           
ln |h|e m         ln    
ln |h|e m         ln    

ln |h|e m           ln    

p (errortrue(h)        )     |h|e m         
p (errortrue(h)        )     |h|e m         
p (errortrue(h)        )     |h|e m         
p(errortrue(h)      )     |h|e m   

    =   = .01,|h| = 40

need m   830

ln|h|   m        ln    

ln|h|   m        ln    
ln|h| + ln 1

 

says: we are willing to 
tolerate a    id203 of 
having        error  

ln|h|   m        ln    
case 1 
ln|h| + ln 1

 

m    

   

m    

   

case 2 

ln|h| + ln 1

 

m

       

log	
   dependence	
   on	
   |h|,	
   
ok	
   if	
   exponen:al	
   size	
   (but	
   
not	
   doubly)	
   

   has	
   stronger	
   
in   uence	
   than     

7
7

  	
   shrinks	
   at	
   rate	
   o(1/m)	
   
7

limita:ons	
   of	
   haussler	
      88	
   bound	
   

       there	
   may	
   be	
   no	
   consistent	
   hypothesis	
   h	
   (where	
   errortrain(h)=0)	
   

       size	
   of	
   hypothesis	
   space	
   

       what	
   if	
   |h|	
   is	
   really	
   big?	
   
       what	
   if	
   it	
   is	
   con:nuous?	
   

       first	
   goal:	
   can	
   we	
   get	
   a	
   bound	
   for	
   a	
   learner	
   with	
   errortrain(h)	
   in	
   the	
   

data	
   set?	
   	
   

ques:on:	
   what   s	
   the	
   expected	
   error	
   of	
   a	
   

hypothesis?	
   

       the	
   id203	
   of	
   a	
   hypothesis	
   incorrectly	
   classifying:	
   
      
       the	
   z	
   variables	
   are	
   independent	
   and	
   idenecally	
   distributed	
   (i.i.d.)	
   with	
   

let   s	
   now	
   let	
   	
   	
   	
   	
   	
   	
   be	
   a	
   random	
   variable	
   that	
   takes	
   two	
   values,	
   1	
   if	
   h	
   correctly	
   
classi   es	
   ith	
   	
   data	
   point,	
   and	
   0	
   otherwise	
   

  p(~x, y)1[h(~x) 6= y]

zh
i

x(~x,y)

pr(zh

i = 0) = x(~x,y)

  p(~x, y)1[h(~x) 6= y]

       es:ma:ng	
   the	
   true	
   error	
   id203	
   is	
   like	
   es:ma:ng	
   the	
   parameter	
   of	
   a	
   coin!	
   

       cherno   	
   bound:	
   for	
   m	
   i.i.d.	
   coin	
      ips,	
   x1,   ,xm,	
   where	
   xi	
      	
   {0,1}.	
   for	
   0<  <1:	
   

true	
   error	
   
id203	
   

observed	
   frac:on	
   of	
   

points	
   incorrectly	
   classi   ed	
   

p(xi = 1) =    

e[xi] =    

e[

1
m

mxi=1

xi] =

1
m

mxi=1

(by	
   linearity	
   of	
   expecta:on)	
   

generaliza:on	
   bound	
   for	
   |h|	
   hypothesis	
   

theorem:	
   hypothesis	
   space	
   h	
      nite,	
   dataset	
   d	
   

with	
   m	
   i.i.d.	
   samples,	
   0	
   <	
     	
   <	
   1	
   :	
   for	
   any	
   learned	
   
hypothesis	
   h:	
   

pr(errortrue(h)   errord(h) >    )     |h|e 2m   2

why? same reasoning as before. use the union 
bound over individual chernoff bounds  

pac	
   bound	
   and	
   bias-     variance	
   tradeo   	
   	
   

for all h, with id203 at least 1-  :	


errortrue(h)     errord(h) +s ln|h| + ln 1

2m

 

   bias    

   variance    

       for	
   large	
   |h|	
   

       low	
   bias	
   (assuming	
   we	
   can	
      nd	
   a	
   good	
   h)	
   
       high	
   variance	
   (because	
   bound	
   is	
   looser)	
   

       for	
   small	
   |h|	
   

       high	
   bias	
   (is	
   there	
   a	
   good	
   h?)	
   
       low	
   variance	
   (:ghter	
   bound)	
   

