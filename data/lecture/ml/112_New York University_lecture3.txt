support vector machines (id166s) 

lecture 3 

david sontag 

new york university 

slides adapted from luke zettlemoyer, vibhav gogate, 

and carlos guestrin 

geometry of linear separators 

(see blackboard) 

a plane can be specified as the set of all points given by: 

vector from origin to a point in the plane 

two non-parallel directions in the plane 

alternatively, it can be specified as: 

normal vector 
(we will call this w) 

only need to specify this dot product, 
a scalar (we will call this the offset, b) 

barber, section a.1.1-4 

linear separators 

!    if training data is linearly separable, id88 is 

guaranteed to find some linear separator 

!    which of these is optimal?  

support vector machine (id166) 

!    id166s (vapnik, 1990   s) choose the linear separator with the 

largest margin 

robust to 
outliers! 

       good according to intuition, theory, practice 
       id166 became famous when, using images as input, it gave 
accuracy comparable to neural-network with hand-designed 
features in a handwriting recognition task 

v. vapnik 

support vector machines: 3 key ideas 

1.   use optimization to find solution (i.e. a hyperplane) 

with few errors 

2.   seek large margin separator to improve 

generalization 

3.   use kernel trick to make large feature 

spaces computationally efficient 

finding a perfect classifier (when one exists) 

using id135 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

for every data point (xt, yt), enforce the 
constraint 

for yt = +1, 
and for yt = -1, 

equivalently, we want to satisfy all of the 
linear constraints 

this linear program can be efficiently 
solved using algorithms such as simplex, 
interior point, or ellipsoid 

finding a perfect classifier (when one exists) 

using id135 

weight space 

example of 2-dimensional 
id135  
(feasibility) problem: 

for id166s, each data point 
gives one inequality: 

what happens if the data set is not linearly separable? 

minimizing number of errors (0-1 loss) 

       try to find weights that violate as few 

constraints as possible? 

#(mistakes) 

       formalize this using the 0-1 loss: 

where 

       unfortunately, minimizing 0-1 loss is 

np-hard in the worst-case 
       non-starter. we need another 

approach. 

key idea #1: allow for slack 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

  1 

  3 

  2 

  4 

,    

  j   j 

-   j 

  j   0 

   slack variables    

we now have a linear program again, 
and can efficiently find its optimum 

for each data point: 
      if functional margin     1, don   t care 
      if functional margin < 1, pay linear penalty 

key idea #1: allow for slack 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

  1 

  3 

  2 

  4 

,    

  j   j 

-   j 

  j   0 

   slack variables    

what is the optimal value   j
of w* and b*? 

* as a function 

if  

if  

then   j =  0 

then   j =  

sometimes written as 

equivalent hinge loss formulation 

,    

  j   j 

-   j 

  j   0 

substituting 

into the objective, we get: 

the hinge loss is defined as  

this is empirical risk minimization, 
using the hinge loss 

hinge loss vs. 0/1 loss 

0-1 loss: 

hinge loss: 

1 

0 

1 

hinge loss upper bounds 0/1 loss! 

it is the tightest convex upper bound on the 0/1 loss  

key idea #2: seek large margin 

