bayesian	networks	

lecture	18	

david	sontag	

new	york	university	

outline	for	today	

       modeling	sequen&al	data	(e.g.,	=me	series,	

speech	processing)	using	hidden	markov	
models	(id48s)	

       bayesian	networks 		

      independence	proper=es	
      examples	
      learning	and	id136	

example	applica=on:	tracking	

observe	noisy	measurements	of	
missile	loca=on:	y1,	y2,	   	

where	is	the	missile	now?	where	will	it	be	in	10	seconds?	

radar	

probabilis=c	approach	

       our	measurements	of	the	missile	loca=on	were	

y1,	y2,	   ,	yn	
       let	xt	be	the	true	<missile	loca=on,	velocity>	at	
=me	t	

       to	keep	this	simple,	suppose	that	everything	is	

discrete,	i.e.	xt	takes	the	values	1,	   ,	k	

grid	the	space:	

probabilis=c	approach	

       first,	we	specify	the	condi&onal	distribu=on	

pr(xt	|	xt-1):	

from	basic	physics,	we	can	bound	
the	distance	that	the	missile	can	
have	traveled	

       then,	we	specify	pr(yt	|	xt=<(10,20),	200	mph	
toward	the	northeast>):	

with	id203	  ,	yt	=	xt	(ignoring	the	velocity).	otherwise,	yt	is	a	
uniformly	chosen	grid	loca=on	

hidden	markov	models	

1960   s	

       assume	that	the	joint	distribu=on	on	x1,	x2,	   ,	xn	and	y1,	y2,	

   ,	yn	factors	as	follows:	

pr(x1, . . . xn, y1, . . . , yn) = pr(x1) pr(y1 | x1)

nyt=2

pr(xt | xt 1) pr(yt | xt)

       to	   nd	out	where	the	missile	is	now,	we	do	marginal	

id136:	

pr(xn | y1, . . . , yn)

       to	   nd	the	most	likely	trajectory,	we	do	map	(maximum	a	

posteriori)	id136:	

arg max

x

pr(x1, . . . , xn | y1, . . . , yn)

id136	

       recall,	to	   nd	out	where	the	missile	is	now,	we	do	marginal	

id136:	

pr(xn | y1, . . . , yn)

       how	does	one	compute	this?	
       applying	rule	of	condi=onal	id203,	we	have:		

pr(xn | y1, . . . , yn) =

pr(xn, y1, . . . , yn)

pr(y1, . . . , yn)

=

pr(xn, y1, . . . , yn)
  xn=1 pr(  xn, y1, . . . , yn)

pk

       naively,	would	seem	to	require	kn-1	summa=ons,	
pr(x1, . . . , xn, y1, . . . , yn)

pr(xn, y1, . . . , yn) = xx1,...,xn 1

is	there	a	
more	e   cient	
algorithm?	

marginal	id136	in	id48s	

       use	dynamic	programming	

pr(a = a) =xb
pr(xn 1, y1, . . . , yn 1) pr(xn, yn | xn 1, y1, . . . , yn 1)

pr(xn 1, xn, y1, . . . , yn)

pr(  a =  a,  b =  b) = pr(  a =  a) pr(  b =  b |  a =  a)

pr(b = b, a = a)

condi=onal	independence	in	id48s	

pr(xn 1, y1, . . . , yn 1) pr(xn, yn | xn 1)

pr(a = a, b = b) = pr(a = a) pr(b = b | a = a)
condi=onal	independence	in	id48s	

pr(xn 1, y1, . . . , yn 1) pr(xn | xn 1) pr(yn | xn, xn 1)
pr(xn 1, y1, . . . , yn 1) pr(xn | xn 1) pr(yn | xn)

pr(xn, y1, . . . , yn) = xxn 1
= xxn 1
= xxn 1
= xxn 1
= xxn 1

       for	n=1,	ini=alize		
pr(x1, y1) = pr(x1) pr(y1 | x1)
       total	running	=me	is	o(nk)	   	linear	=me!	

easy	to	do	   ltering	

map	id136	in	id48s	

       map	id136	in	id48s	can	also	be	solved	in	linear	=me!	

arg max

x

pr(x1, . . . xn | y1, . . . , yn) = arg max

x

pr(x1, . . . xn, y1, . . . , yn)

= arg max

log pr(x1, . . . xn, y1, . . . , yn)

x

x

= arg max

logh pr(x1) pr(y1 | x1)i +

nxi=2
       formulate	as	a	shortest	paths	problem	
weight	for	edge	(xi-1,	xi)	is	 -	
weight	for	edge	(s,	x1)	is	
logh pr(x1) pr(y1 | x1)i
-	

logh pr(xi | xi 1) pr(yi | xi)i
logh pr(xi | xi 1) pr(yi | xi)i

path	from	s	to	t	gives	
the	map	assignment	

s	

   	

k	nodes	per	variable	

x1	

x2	

xn-1	

xn	

t	

weight	for	edge	(xn,	t)	is	0	

called	the	viterbi	algorithm	

applica=ons	of	id48s	

       speech	recogni=on	

       predict	phonemes	from	the	sounds	forming	words	(i.e.,	the	

actual	signals)	

       natural	language	processing	

       predict	parts	of	speech	(verb,	noun,	determiner,	etc.)	from	

the	words	in	a	sentence	
       computa=onal	biology	

       predict	intron/exon	regions	from	dna	
       predict	protein	structure	from	dna	(locally)	

       and	many	many	more!	

id48s	as	a	graphical	model	

       we	can	represent	a	hidden	markov	model	with	a	graph:	

x1	

x2	

x3	

x4	

x5	

x6	

y1	

y2	

y3	

y4	

y5	

y6	

shading	in	denotes	
observed	variables	(e.g.	what	
is	available	at	test	=me)	

pr(x1, . . . xn, y1, . . . , yn) = pr(x1) pr(y1 | x1)

nyt=2

pr(xt | xt 1) pr(yt | xt)

       there	is	a	1-1	mapping	between	the	graph	structure	and	the	factoriza=on	

of	the	joint	distribu=on	

na  ve	bayes	as	a	graphical	model	

       we	can	represent	a	na  ve	bayes	model	with	a	graph:	

label

y

x1

x2

x3

. . .

xn

shading	in	denotes	
observed	variables	(e.g.	what	
is	available	at	test	=me)	

features

   

pr(y, x1, . . . , xn) = pr(y)

nyi=1

pr(xi | y)

       there	is	a	1-1	mapping	between	the	graph	structure	and	the	factoriza=on	

of	the	joint	distribu=on	

id110s
reference: chapter 3

bayesian	networks	

       a	bayesian	network	is	speci   ed	by	a	directed	acyclic	graph	

a id110 is speci   ed by a directed acyclic graph
g = (v , e ) with:

g=(v,e)	with:	
       one	node	i	for	each	random	variable	xi	
       one	condi=onal	id203	distribu=on	(cpd)	per	node,	p(xi	|	xpa(i)),	
1 one node i 2 v for each random variable xi
specifying	the	variable   s	id203	condi=oned	on	its	parents   	values	
2 one id155 distribution (cpd) per node, p(xi | xpa(i)),
specifying the variable   s id203 conditioned on its parents    values
       corresponds	1-1	with	a	par=cular	factoriza=on	of	the	joint	

corresponds 1-1 with a particular factorization of the joint
distribution:

distribu=on:	

p(x1, . . . xn) =yi2v

p(xi | xpa(i))

       powerful	framework	for	designing	algorithms	to	perform	

powerful framework for designing algorithms to perform id203
computations

id203	computa=ons	

2011	turing	award	was	for	bayesian	networks	

example	

       consider	the	following	bayesian	network:	

example

consider the following id110:

d0
0.6

d1
0.4

dif   culty

intelligence

d0
0.6

d1
0.4

i0
0.7

i1
0.3

i0
0.7

i1
0.3

example	from	koller	&	
friedman,	probabilis&c	
graphical	models,	2009	

dif   culty

grade

intelligence

sat

i0,d0
i0,d1
i0,d0
i0,d1

g1
0.3
0.05
0.9
0.5

i0,d0
i0,d1
i0,d0
i0,d1

g2
0.4
g1
0.25
0.3
0.08
0.05
0.3
0.9
0.5

g3
0.3
g2
0.7
0.4
0.02
0.25
0.2
0.08
0.3

g3
0.3
0.7
0.02
0.2

grade

letter
letter

sat

s0
0.95
s1
0.2
0.05
0.8

i0
s0
i1
0.95
0.2

i0
i1

s1
0.05
0.8

l 0
l 0
0.1
0.1
0.4
0.4
0.99
0.99

l1
l1
0.9
0.9
0.6
0.6
0.01
0.01

g1
g2
g2

g1
g2
g2

what is its joint distribution?

       what	is	its	joint	distribu=on?	
p(x1, . . . xn) = yi2v

p(xi | xpa(i))

p(d, i, g , s, l) = p(d)p(i)p(g | i, d)p(s | i)p(l | g )

david sontag (nyu)

id114

lecture 1, january 31, 2013

31 / 44

example	

       consider	the	following	bayesian	network:	

d0
0.6

d1
0.4

i0
0.7

i1
0.3

dif   culty

intelligence

grade

sat

example	from	koller	&	
friedman,	probabilis&c	
graphical	models,	2009	

g1
0.3
0.05
0.9
0.5

g2
0.4
0.25
0.08
0.3

g3
0.3
0.7
0.02
0.2

i0,d0
i0,d1
i0,d0
i0,d1

letter

s0
0.95
0.2

s1
0.05
0.8

i0
i1

l1
0.9
0.6
0.01

       what	is	this	model	assuming?	

l 0
0.1
0.4
0.99

g1
g2
g2

sat 6? grade
sat ? grade | intelligence

example	

       consider	the	following	bayesian	network:	

d0
0.6

d1
0.4

i0
0.7

i1
0.3

dif   culty

intelligence

grade

sat

example	from	koller	&	
friedman,	probabilis&c	
graphical	models,	2009	

g1
0.3
0.05
0.9
0.5

g2
0.4
0.25
0.08
0.3

g3
0.3
0.7
0.02
0.2

i0,d0
i0,d1
i0,d0
i0,d1

s0
0.95
0.2

s1
0.05
0.8

i0
i1

letter

l 0
0.1
0.4
0.99

l1
0.9
0.6
0.01

g1
g2
g2

       compared	to	a	simple	log-linear	model	to	predict	intelligence:	
       captures	non-linearity	between	grade,	course	di   culty,	and	intelligence	
       modular.	training	data	can	come	from	di   erent	sources!	
       built	in	feature	selec&on:	lerer	of	recommenda=on	is	irrelevant	given	

grade	

a id110 is speci   ed by a directed acyclic graph
g = (v , e ) with:

bayesian	networks	enable	use	of	
1 one node i 2 v for each random variable xi
2 one id155 distribution (cpd) per node, p(xi | xpa(i)),
specifying the variable   s id203 conditioned on its parents    values

domain	knowledge	

corresponds 1-1 with a particular factorization of the joint
distribution:

p(x1, . . . xn) =yi2v

p(xi | xpa(i))

will	my	car	start	this	morning?	

powerful framework for designing algorithms to perform id203
computations

david sontag (nyu)

id114

lecture 1, january 31, 2013

30 / 44

heckerman	et	al.,	decision-theore=c	troubleshoo=ng,	1995	

a id110 is speci   ed by a directed acyclic graph
g = (v , e ) with:

bayesian	networks	enable	use	of	
1 one node i 2 v for each random variable xi
2 one id155 distribution (cpd) per node, p(xi | xpa(i)),
specifying the variable   s id203 conditioned on its parents    values

domain	knowledge	

corresponds 1-1 with a particular factorization of the joint
distribution:

p(x1, . . . xn) =yi2v

p(xi | xpa(i))

what	is	the	di   eren=al	diagnosis?	

powerful framework for designing algorithms to perform id203
computations

david sontag (nyu)

id114

lecture 1, january 31, 2013

30 / 44

beinlich	et	al.,	the	alarm	monitoring	system,	1989	

bayesian	networks	are	genera&ve	models	

       can	sample	from	the	joint	distribu=on,	top-down	
       suppose	y	can	be	   spam   	or	   not	spam   ,	and	xi	is	a	binary	
       let   s	try	genera=ng	a	few	emails!	

indicator	of	whether	word	i	is	present	in	the	e-mail	

label

y

x1

x2

x3

. . .

xn

features

   

       oven	helps	to	think	about	bayesian	networks	as	a	genera=ve	

model	when	construc=ng	the	structure	and	thinking	about	
the	model	assump=ons	

id136	in	bayesian	networks	

       compu=ng	marginal	probabili=es	in	tree	structured	bayesian	

networks	is	easy	
       the	algorithm	called	   belief	propaga=on   	generalizes	what	we	showed	for	

hidden	markov	models	to	arbitrary	trees	

x1	

x2	

x3	

x4	

x5	

x6	

y1	

y2	

y3	

y4	

y5	

y6	

       wait   	this	isn   t	a	tree!	what	can	we	do?	

label

y

x1

x2

x3

. . .

xn

features

   

id136	in	bayesian	networks	

      

in	some	cases	(such	as	this)	we	can	transform	this	into	what	is	
called	a	   junc=on	tree   ,	and	then	run	belief	propaga=on	

approximate	id136	

       there	is	also	a	wealth	of	approximate	id136	algorithms	that	can	

be	applied	to	bayesian	networks	such	as	these	

       markov	chain	monte	carlo	algorithms	repeatedly	sample	

assignments	for	es=ma=ng	marginals	

       varia=onal	id136	algorithms	(determinis=c)	   nd	a	simpler	

distribu=on	which	is	   close   	to	the	original,	then	compute	marginals	
using	the	simpler	distribu=on	

maximum	likelihood	es=ma=on	in	

ml estimation in id110s
ml estimation in id110s
ml estimation in id110s

bayesian	networks	

suppose that we know the id110 structure g
suppose that we know the id110 structure g
suppose that we know the id110 structure g
let    xi|xpa(i) be the parameter giving the value of the cpd p(xi | xpa(i))
let    xi|xpa(i) be the parameter giving the value of the cpd p(xi | xpa(i))
let    xi|xpa(i) be the parameter giving the value of the cpd p(xi | xpa(i))
id113 corresponds to solving:
id113 corresponds to solving:
id113 corresponds to solving:

max
max
max
   
   
   

1
1
1
m
m
m

mxm=1
mxm=1
mxm=1

log p(xm;    )
log p(xm;    )
log p(xm;    )

subject to the non-negativity and id172 constraints
subject to the non-negativity and id172 constraints
subject to the non-negativity and id172 constraints
this is equal to:
this is equal to:
this is equal to:

max
max
max
   
   
   

1
1
1
m
m
m

mxm=1
mxm=1
mxm=1

log p(xm;    ) = max
log p(xm;    ) = max
log p(xm;    ) = max
   
   
   

1
1
1
m
m
m

nxi=1
mxm=1
mxm=1
nxi=1
mxm=1
nxi=1
mxm=1
nxi=1
mxm=1
nxi=1
mxm=1
nxi=1

1
1
1
m
m
m

log p(x m
log p(x m
log p(x m
i
i
i

log p(x m
log p(x m
log p(x m
i
i
i

| xm
pa(i);    )
| xm
pa(i);    )
| xm
pa(i);    )

| xm
pa(i);    )
| xm
pa(i);    )
| xm
pa(i);    )

= max
= max
= max
   
   
   

the optimization problem decomposes into an independent optimization
the optimization problem decomposes into an independent optimization
the optimization problem decomposes into an independent optimization
problem for each cpd! has a simple closed-form solution.
problem for each cpd! has a simple closed-form solution.
problem for each cpd! has a simple closed-form solution.
david sontag (nyu)
david sontag (nyu)
david sontag (nyu)

lecture 10, april 11, 2013
lecture 10, april 11, 2013
lecture 10, april 11, 2013

id114
id114
id114

17 / 22
17 / 22
17 / 22

