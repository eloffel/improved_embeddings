support	
   vector	
   machines	
   (id166s)	
   

lecture	
   5	
   

david	
   sontag	
   

new	
   york	
   university	
   

so5	
   margin	
   id166	
   

	
   
1
+
	
   
=
	
   
b
	
   
+
	
   
x
.
w

	
   
0
	
   
=
	
   
b
	
   
+
	
   
x
.
w

	
   
1
-     
	
   
=
	
   
b
	
   
+
	
   
x
.
w

  3 

  1 

  2 

  4 

   slack	
   variables   	
   

slack	
   penalty	
   c > 0:	
   
       c=    !	
   minimizes	
   upper	
   bound	
   on	
   0-     1	
   loss	
   
       c   0 !	
   	
   points	
   with	
     i=0	
   have	
   big	
   margin	
   
      	
   select	
   using	
   cross-     valida=on	
   

support	
   vectors:	
   
data	
   points	
   for	
   which	
   the	
   constraints	
   are	
   binding	
   	
   

so5	
   margin	
   id166	
   

qp	
   form:	
   

more	
      natural   	
   form:	
   

equivalent	
   if	
   

regularizanon	
   

term	
   

empirical	
   loss	
   

subgradient	
   

(for	
   non-     di   erennable	
   funcnons)	
   

(sub)gradient	
   descent	
   of	
   id166	
   objecnve	
   

step	
   size:	
   

-     	
   

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direcnon,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   (1-       t  )	
   wt	
   +	
     t	
   yj	
   xj	
   

	
   wt+1	
   =	
   (1-       t  )	
   wt	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direcnon,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt-     	
   	
   yjxj)	
   

	
   wt+1	
   =	
   wt	
      	
     t  wt	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direcnon,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt-     	
   	
   yjxj)	
   

	
   wt+1	
   =	
   wt	
      	
     t  wt	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

convergence	
   choice	
   :	
   fixed	
   number	
   of	
   itera=ons	
   

	
   

	
   

	
   

	
   

	
   

	
   t=20*|data|	
   

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direcnon,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt-     	
   	
   yjxj)	
   

	
   wt+1	
   =	
   wt	
      	
     t  wt	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

stepsize	
   choice:	
   -     	
   ini=alize	
   with	
   1/  	
   	
   

	
   

	
   

	
   

	
   

	
   	
   	
   -     	
   decays	
   with	
   1/t	
   

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direc=on,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt-     	
   	
   yjxj)	
   

	
   wt+1	
   =	
   wt	
      	
     t  wt	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

direc=on	
   choice:	
    	
   stochas=c	
   approx	
   to	
   the	
   subgradient	
   

subgradient	
   calculanon	
   

objec=ve:	
   

 
2||w||2 +

max{0, 1   yiw    xi}

1

mxi

stochas=c	
   approx:	
   

 
2||w||2 + max{0, 1   yiw    xi}
for	
   a	
   randomly	
   chosen	
   data	
   point	
   i	
   

(in	
   the	
   assignment	
   the	
   choice	
   of	
   i	
   is	
   not	
   random	
   -     	
   easier	
   

to	
   debug	
   and	
   compare	
   between	
   students).	
   

subgradient	
   calculanon	
   

objec=ve:	
   

 
2||w||2 +

max{0, 1   yiw    xi}

stochas=c	
   approx:	
   

 
2||w||2 + max{0, 1   yiw    xi}

1

mxi

(sub)gradient:	
   

 ||w|| +

d
dw

max{0, 1   yiw    xi}

subgradient	
   calculanon	
   

objec=ve:	
   

 
2||w||2 +

max{0, 1   yiw    xi}

1

mxi

 
2||w||2 + max{0, 1   yiw    xi}
yiw    xi

0	
   

0	
   

1	
   

stochas=c	
   approx:	
   

(sub)gradient:	
   

 ||w|| +

d
dw

max{0, 1   yiw    xi}

yiw    xi
=1	
   

>1	
   

0

<1	
   

 yixi

subgradient	
   calculanon	
   

objec=ve:	
   

 
2||w||2 +

max{0, 1   yiw    xi}

stochas=c	
   approx:	
   

 
2||w||2 + max{0, 1   yiw    xi}
yiw    xi

0	
   

1

mxi

(sub)gradient:	
   

 ||w|| +

d
dw

0	
   

1	
   

max{0, 1   yiw    xi}

<1	
   

yiw    xi
=1	
   

>1	
   

 yixi

0

0

subgradient	
   calculanon	
   

objec=ve:	
   

 
2||w||2 +

max{0, 1   yiw    xi}

1

mxi

 
2||w||2 + max{0, 1   yiw    xi}

stochas=c	
   approx:	
   

(sub)gradient:	
   

if yiw    xi < 1
else

 w   yixi
 w + 0

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direc=on,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt-     	
   	
   yjxj)	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt	
   +	
   0)	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

direc=on	
   choice:	
    	
   stochas=c	
   approx	
   to	
   the	
   subgradient	
   

if yiw    xi < 1
else

 w   yixi
 w + 0

the	
   pegasos	
   algorithm	
   

general	
   framework	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   

while	
   not	
   converged	
   

	
   t	
   =	
   t+1	
   
	
   choose	
   a	
   stepsize,	
     t	
   
	
   choose	
   a	
   direcnon,	
   pt	
   
	
   go!	
   	
   
	
   test	
   for	
   convergence	
   

pegasos	
   algorithm	
   (from	
   homework)	
   
ini=alize:	
   w1	
   =	
   0,	
   t=0	
   
for	
   iter	
   =	
   1,2,   ,20	
   

	
   for	
   j=1,2,   ,|data|	
   
	
   
	
   
	
   
	
   
	
   
	
   

	
   t	
   =	
   t+1	
   
	
     t	
   =	
   1/(t  )	
   
	
   if	
   yj(wt	
   xj)	
   <	
   1	
   
	
   
	
   else	
   
	
   

	
   wt+1	
   =	
   wt	
      	
     t(  wt-     	
   	
   yjxj)	
   

	
   wt+1	
   =	
   wt	
      	
     t  wt	
   

output:	
   wt+1	
   

output:	
   wt+1	
   

go:	
   	
   update	
   wt+1	
   =	
   wt	
   -     	
     tpt	
   

why	
   is	
   this	
   algorithm	
   interesnng?	
   
       simple	
   to	
   implement,	
   state	
   of	
   the	
   art	
   results.	
   

       nonce	
   similarity	
   to	
   id88	
   algorithm!	
   

algorithmic	
   di   erences:	
   updates	
   if	
   insu   cient	
   
margin,	
   scales	
   weight	
   vector,	
   and	
   has	
   a	
   learning	
   rate.	
   

       since	
   based	
   on	
   stochas7c	
   gradient	
   descent,	
   its	
   

running	
   nme	
   guarantees	
   are	
   probabilisnc.	
   

       highlights	
   interesnng	
   tradeo   s	
   between	
   running	
   

nme	
   and	
   data.	
   

much	
   faster	
   than	
   previous	
   methods	
   

       3	
   datasets	
   (provided	
   by	
   joachims)	
   

       reuters	
   ccat	
   (800k	
   examples,	
   47k	
   features)	
   
       physics	
   arxiv	
   (62k	
   examples,	
   100k	
   features)	
   
       covertype	
   (581k	
   examples,	
   54	
   features)	
   

training	
   time	
   
(in	
   seconds):	
   

pegasos 

id166-perf 

id166-light 

reuters 

covertype 

astro-physics 

2 

6 

2 

77 

85 

5 

20,075 

25,514 

80 

approximate	
   algorithms	
   
error decomposition

prediction error

[shalev	
   schwartz,	
   
srebro	
      08]	
   

err(w)
err(w*)

err(w0)

note:	
   w0	
   is	
   rede   ned	
   in	
   this	
   
context	
   (see	
   below)	
      	
   
does	
   not	
   refer	
   to	
   ini=al	
   weight	
   
vector	
   

    approximation error:

    best error achievable by large-margin predictor
    error of population minimizer

w0 = argmin e[f(w)] = argmin   |w|2 + ex,y[loss(   w,x   ;y)]

    extra error due to replacing e[loss] with empirical loss

    estimation error:

w* = arg min fn(w)

    optimization error:

    extra error due to only optimizing to within finite precision

from	
   icml   08	
   presentanon	
   (available	
   here)	
   

approximate	
   algorithms	
   
error decomposition

prediction error

[shalev	
   schwartz,	
   
srebro	
      08]	
   

err(w)
err(w*)

err(w0)

pegasos	
   guarantees	
   

a5er	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   	
   	
   updates:	
   

t =   o    1
        

	
   

    approximation error:

    best error achievable by large-margin predictor
    error of population minimizer

w0 = argmin e[f(w)] = argmin   |w|2 + ex,y[loss(   w,x   ;y)]

    extra error due to replacing e[loss] with empirical loss

    estimation error:

w* = arg min fn(w)

    optimization error:

    extra error due to only optimizing to within finite precision

 
with	
   id203	
   1-     	
   

	
   

   
	
   err(wt)	
   <	
   err(w0)	
   +	
   	
   

approximate	
   algorithms	
   
error decomposition

prediction error

[shalev	
   schwartz,	
   
srebro	
      08]	
   

err(w)
err(w*)

err(w0)

pegasos	
   guarantees	
   

a5er	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   	
   	
   updates:	
   

t =   o    1
        

	
   

	
   

   
	
   err(wt)	
   <	
   err(w0)	
   +	
   	
   

    approximation error:

running	
   nme	
   does	
   not	
   depend	
   
on:	
   

    best error achievable by large-margin predictor
    error of population minimizer

w0 = argmin e[f(w)] = argmin   |w|2 + ex,y[loss(   w,x   ;y)]

	
   -     #	
   training	
   examples!	
   
    estimation error:

    extra error due to replacing e[loss] with empirical loss

    extra error due to only optimizing to within finite precision

 
with	
   id203	
   1-     	
   

w* = arg min fn(w)

it	
   does	
   depend	
   on:	
   
    optimization error:
	
   -     	
   dimensionality	
   d	
   (why?)	
   
 
	
   -     	
   approximanon	
   	
   	
   	
   	
   	
   and	
   	
   
	
   -     	
   di   culty	
   of	
   problem	
   
 

   

but	
   how	
   is	
   that	
   possible?	
   
the double-edged sword

prediction error

err(w)
err(w*)

err(w0)

[shalev	
   schwartz,	
   
srebro	
      08]	
   

data set size (n)

    when data set size increases:
    estimation error decreases
    can increase optimization error,
    but handling more data is expensive

as	
   the	
   dataset	
   grows,	
   our	
   approximanons	
   can	
   

i.e. optimize to within lesser accuracy     fewer iterations
be	
   worse	
   to	
   get	
   the	
   same	
   error!	
   
e.g. runtime of each iteration increases

    stochastic id119,

e.g. pegasos (primal efficient sub-gradient solver for id166s) 
    fixed runtime per iteration
    runtime to get fixed accuracy does not increase with n

[shalev-shwartz singer srebro, icml   07]

