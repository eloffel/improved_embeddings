introduction to machine learning

david sontag

new york university

lecture 22, april 19, 2016

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

1 / 15

id44 (lda)

topic models are powerful tools for exploring large data sets and for
making id136s about the content of documents

many applications in information retrieval, document summarization,
and classi   cation

lda is one of the simplest and most widely used topic models

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

2 / 15

!"#$%&'()*"+,#)+"/,9#)1+.&),3&'(1"65%51:5)2,'0("'1.&/,0,"'1-.&/,0,"'12,'3$14$3,5)%1&(2,#)16$332,)%1)+".()165)&65//1)"##&.165)7&(65//18""(65//1--complexity+of+id136+in+latent+dirichlet+alloca6on+david+sontag,+daniel+roy+(nyu,+cambridge)+w66+topic+models+are+powerful+tools+for+exploring+large+data+sets+and+for+making+id136s+about+the+content+of+documents+documents+topics+poli6cs+.0100+president+.0095+obama+.0090+washington+.0085+religion+.0060+almost+all+uses+of+topic+models+(e.g.,+for+unsupervised+learning,+informa6on+retrieval,+classi   ca6on)+require+probabilis)c+id136:+new+document+what+is+this+document+about?+words+w1,+   ,+wn+   distribu6on+of+topics+ t= p(w|z=t)    +religion+.0500+hindu+.0092+judiasm+.0080+ethics+.0075+buddhism+.0016+sports+.0105+baseball+.0100+soccer+.0055+basketball+.0050+football+.0045+   +   +weather+.50+   nance+.49+sports+.01+generative model for a document in lda

1 sample the document   s topic distribution    (aka topic vector)

       dirichlet(  1:t )

t=1 are    xed hyperparameters. thus    is a distribution

where the {  t}t
over t topics with mean   t =   t/(cid:80)t(cid:48)   t(cid:48)

2 for i = 1 to n, sample the topic zi of the i   th word

zi|         

3

... and then sample the actual word wi from the zi    th topic

wi|zi       zi

where {  t}t
words)

t=1 are the topics (a    xed collection of distributions on

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

3 / 15

comparison of mixture and admixture models

model on left is a mixture model

called multinomial naive bayes (a word can appear multiple times)
document is generated from a single topic

model on right (lda) is an admixture model

document is generated from a distribution over topics

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

4 / 15

i=1tond=1todwidprior distributionover topicstopic of doc dword  topic-worddistributions  zd  dirichlet hyperparametersi=1tond=1tod  dwidzidtopic distributionfor documenttopic of word i of doc dword  topic-worddistributionstwo steps

can typically separate out these two uses of topic models:

1 learn the model parameters (  ,   )
2 use model to make id136s about a single document

step 1 is when topic discovery happens. since the topic assignments
z are never observed, one can use em to do this

exact id136 is intractable: approximate id136 (typically gibbs
sampling) is used

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

5 / 15

polylingual topic models (mimno et al., emnlp    09)

goal: topic models that are aligned across languages
training data: corpora with multiple documents in each language

europarl corpus of parliamentary proceedings (11 western languages;
exact translations)
wikipedia articles (12 languages; not exact translations)

how to do this?

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

6 / 15

polylingual topic models (mimno et al., emnlp    09)

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

7 / 15

thecontentsofcollectionsinunfamiliarlanguagesandidentifytrendsintopicprevalence.2relatedworkbilingualtopicmodelsforparalleltextswithword-to-wordalignmentshavebeenstudiedpre-viouslyusingthehm-bitammodel(zhaoandxing,2007).tam,laneandschultz(tametal.,2007)alsoshowimprovementsinmachinetranslationusingbilingualtopicmodels.bothofthesetranslation-focusedtopicmodelsinferword-to-wordalignmentsaspartoftheirid136procedures,whichwouldbecomeexponentiallymorecomplexifadditionallanguageswereadded.wetakeasimplerapproachthatismoresuit-ablefortopicallysimilardocumenttuples(wheredocumentsarenotdirecttranslationsofonean-other)inmorethantwolanguages.arecentex-tendedabstract,developedconcurrentlybynietal.(nietal.,2009),discussesamultilingualtopicmodelsimilartotheonepresentedhere.how-ever,theyevaluatetheirmodelononlytwolan-guages(englishandchinese),anddonotusethemodeltodetectdifferencesbetweenlanguages.theyalsoprovidelittleanalysisofthediffer-encesbetweenpolylingualandsingle-languagetopicmodels.outsideofthe   eldoftopicmod-eling,kawabaetal.(kawabaetal.,2008)useawikipedia-basedmodeltoperformsentimentanalysisofblogposts.they   nd,forexample,thatenglishblogpostsaboutthenintendowiiof-tenrelatetoahack,whichcannotbementionedinjapanesepostsduetojapaneseintellectualprop-ertylaw.similarly,postsaboutwhalingoftenuse(positive)nationalistlanguageinjapaneseand(negative)environmentalistlanguageinenglish.3polylingualtopicmodelthepolylingualtopicmodel(pltm)isanexten-sionoflatentdirichletallocation(lda)(bleietal.,2003)formodelingpolylingualdocumenttu-ples.eachtupleisasetofdocumentsthatarelooselyequivalenttoeachother,butwrittenindif-ferentlanguages,e.g.,correspondingwikipediaarticlesinfrench,englishandgerman.pltmas-sumesthatthedocumentsinatuplesharethesametuple-speci   cdistributionovertopics.thisisun-likelda,inwhicheachdocumentisassumedtohaveitsowndocument-speci   cdistributionovertopics.additionally,pltmassumesthateach   topic   consistsofasetofdiscretedistributionsdn1tnl...w    wzz...  1  l  1  lfigure1:graphicalmodelforpltm.overwords   oneforeachlanguagel=1,...,l.inotherwords,ratherthanusingasinglesetoftopics ={ 1,..., t},asinlda,therearelsetsoflanguage-speci   ctopics, 1,..., l,eachofwhichisdrawnfromalanguage-speci   csym-metricdirichletwithconcentrationparameter l.3.1generativeprocessanewdocumenttuplew=(w1,...,wl)isgen-eratedby   rstdrawingatuple-speci   ctopicdis-tributionfromanasymmetricdirichletpriorwithconcentrationparameter   andbasemeasurem:      dir(   ,   m).(1)then,foreachlanguagel,alatenttopicassign-mentisdrawnforeachtokeninthatlanguage:zl   p(zl|   )=qn   zln.(2)finally,theobservedtokensarethemselvesdrawnusingthelanguage-speci   ctopicparameters:wl   p(wl|zl, l)=qn lwln|zln.(3)thegraphicalmodelisshownin   gure1.3.2id136givenacorpusoftrainingandtestdocumenttuples   wandw0,respectively   twopossibleid136tasksofinterestare:computingtheid203ofthetesttuplesgiventhetrainingtuplesandinferringlatenttopicassignmentsfortestdocuments.thesetaskscaneitherbeaccom-plishedbyaveragingoversamplesof 1,..., land   mfromp( 1,..., l,   m|w0, )orbyevaluatingapointestimate.wetakethelat-terapproach,andusethemapestimatefor   mandthepredictivedistributionsoverwordsfor 1,..., l.theid203ofheld-outdocu-menttuplesw0giventrainingtupleswisthenapproximatedbyp(w0| 1,..., l,   m).topicassignmentsforatestdocumenttuplew=(w1,...,wl)canbeinferredusinggibbs881learned topics

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

8 / 15

sampling.gibbssamplinginvolvessequentiallyresamplingeachzlnfromitsconditionalposterior:p(zln=t|w,z\l,n, 1,..., l,   m)/ lwln|t(nt)\l,n+   mtptnt 1+   ,(4)wherez\l,nisthecurrentsetoftopicassignmentsforallothertokensinthetuple,while(nt)\l,nisthenumberofoccurrencesoftopictinthetuple,excludingzln,thevariablebeingresampled.4resultsonparalleltextour   rstsetofexperimentsfocusesondocumenttuplesthatareknowntoconsistofdirecttransla-tions.inthiscase,wecanbecon   dentthatthetopicdistributionisgenuinelysharedacrossalllanguages.althoughdirecttranslationsinmulti-plelanguagesarerelativelyrare(incontrastwithcomparabledocuments),weusedirecttranslationstoexplorethecharacteristicsofthemodel.4.1datasettheeuroparlcorpusconsistsofparalleltextsinelevenwesterneuropeanlanguages:danish,ger-man,greek,english,spanish,finnish,french,italian,dutch,portugueseandswedish.thesetextsconsistofroughlyadecadeofproceedingsoftheeuropeanparliament.forourpurposesweusealignmentsatthespeechlevelratherthanthesentencelevel,asinmanytranslationtasksusingthiscorpus.wealsoremovethetwenty-   vemostfrequentwordtypesforef   ciencyreasons.theremainingcollectionconsistsofover121millionwords.detailsbylanguageareshownintable1.table1:averagedocumentlength,#documents,anduniquewordtypesper10,000tokensintheeuroparlcorpus.lang.avg.leng.#docstypes/10kda160.15365245121.4de178.68966497124.5el171.28946317124.2en176.4506952243.1es170.5366592959.5fi161.29360822336.2fr186.7426743054.8it187.4516603569.5nl176.1146695280.8pt183.4106571868.2sv154.60558011136.1modelsaretrainedusing1000iterationsofgibbssampling.eachlanguage-speci   ctopic   wordconcentrationparameter lissetto0.01.centralbank europ  iske ecb s l  n centralbanks zentralbank ezb bank europ  ischen investitionsbank darlehen                                                                                             bank central ecb banks european monetary banco central europeo bce bancos centrales keskuspankin ekp n euroopan keskuspankki eip banque centrale bce europ  enne banques mon  taire banca centrale bce europea banche prestiti bank centrale ecb europese banken leningen banco central europeu bce bancos empr  stimos centralbanken europeiska ecb centralbankens s l  n b  rn familie udnyttelse b  rns b  rnene seksuel kinder kindern familie ausbeutung familien eltern                                                                                                       children family child sexual families exploitation ni  os familia hijos sexual infantil menores lasten lapsia lapset perheen lapsen lapsiin enfants famille enfant parents exploitation familles bambini famiglia    gli minori sessuale sfruttamento kinderen kind gezin seksuele ouders familie crian  as fam  lia    lhos sexual crian  a infantil barn barnen familjen sexuellt familj utnyttjande m  l n   m  ls  tninger m  let m  ls  tning opn   ziel ziele erreichen zielen erreicht zielsetzungen                                                                                   objective objectives achieve aim ambitious set objetivo objetivos alcanzar conseguir lograr estos tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen objectif objectifs atteindre but cet ambitieux obiettivo obiettivi raggiungere degli scopo quello doelstellingen doel doelstelling bereiken bereikt doelen objectivo objectivos alcan  ar atingir ambicioso conseguir m  l m  let uppn   m  len m  ls  ttningar m  ls  ttning andre anden side ene andet   vrige anderen andere einen wie andererseits anderer                                                               other one hand others another there otros otras otro otra parte dem  s muiden toisaalta muita muut muihin muun autres autre part c  t   ailleurs m  me altri altre altro altra dall parte andere anderzijds anderen ander als kant outros outras outro lado outra noutros andra sidan    annat ena annan dadeelenesfifritnlptsv dadeelenesfifritnlptsv dadeelenesfifritnlptsv dadeelenesfifritnlptsv figure2:europarltopics(t=400)theconcentrationparameter   fortheprioroverdocument-speci   ctopicdistributionsisinitializedto0.01t,whilethebasemeasuremisinitializedtotheuniformdistribution.hyperparameters   marere-estimatedevery10gibbsiterations.4.2analysisoftrainedmodelsfigure2showsthemostprobablewordsinalllan-guagesforfourexampletopics,frompltmwith400topics.the   rsttopiccontainswordsrelatingtotheeuropeancentralbank.thistopicprovidesanillustrationofthevariationintechnicalter-minologycapturedbypltm,includingthewidearrayofacronymsusedbydifferentlanguages.thesecondtopic,concerningchildren,demon-stratesthevariabilityofeverydayterminology:al-thoughthefourromancelanguagesareclosely882learned topics

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

9 / 15

sampling.gibbssamplinginvolvessequentiallyresamplingeachzlnfromitsconditionalposterior:p(zln=t|w,z\l,n, 1,..., l,   m)/ lwln|t(nt)\l,n+   mtptnt 1+   ,(4)wherez\l,nisthecurrentsetoftopicassignmentsforallothertokensinthetuple,while(nt)\l,nisthenumberofoccurrencesoftopictinthetuple,excludingzln,thevariablebeingresampled.4resultsonparalleltextour   rstsetofexperimentsfocusesondocumenttuplesthatareknowntoconsistofdirecttransla-tions.inthiscase,wecanbecon   dentthatthetopicdistributionisgenuinelysharedacrossalllanguages.althoughdirecttranslationsinmulti-plelanguagesarerelativelyrare(incontrastwithcomparabledocuments),weusedirecttranslationstoexplorethecharacteristicsofthemodel.4.1datasettheeuroparlcorpusconsistsofparalleltextsinelevenwesterneuropeanlanguages:danish,ger-man,greek,english,spanish,finnish,french,italian,dutch,portugueseandswedish.thesetextsconsistofroughlyadecadeofproceedingsoftheeuropeanparliament.forourpurposesweusealignmentsatthespeechlevelratherthanthesentencelevel,asinmanytranslationtasksusingthiscorpus.wealsoremovethetwenty-   vemostfrequentwordtypesforef   ciencyreasons.theremainingcollectionconsistsofover121millionwords.detailsbylanguageareshownintable1.table1:averagedocumentlength,#documents,anduniquewordtypesper10,000tokensintheeuroparlcorpus.lang.avg.leng.#docstypes/10kda160.15365245121.4de178.68966497124.5el171.28946317124.2en176.4506952243.1es170.5366592959.5fi161.29360822336.2fr186.7426743054.8it187.4516603569.5nl176.1146695280.8pt183.4106571868.2sv154.60558011136.1modelsaretrainedusing1000iterationsofgibbssampling.eachlanguage-speci   ctopic   wordconcentrationparameter lissetto0.01.centralbank europ  iske ecb s l  n centralbanks zentralbank ezb bank europ  ischen investitionsbank darlehen                                                                                             bank central ecb banks european monetary banco central europeo bce bancos centrales keskuspankin ekp n euroopan keskuspankki eip banque centrale bce europ  enne banques mon  taire banca centrale bce europea banche prestiti bank centrale ecb europese banken leningen banco central europeu bce bancos empr  stimos centralbanken europeiska ecb centralbankens s l  n b  rn familie udnyttelse b  rns b  rnene seksuel kinder kindern familie ausbeutung familien eltern                                                                                                       children family child sexual families exploitation ni  os familia hijos sexual infantil menores lasten lapsia lapset perheen lapsen lapsiin enfants famille enfant parents exploitation familles bambini famiglia    gli minori sessuale sfruttamento kinderen kind gezin seksuele ouders familie crian  as fam  lia    lhos sexual crian  a infantil barn barnen familjen sexuellt familj utnyttjande m  l n   m  ls  tninger m  let m  ls  tning opn   ziel ziele erreichen zielen erreicht zielsetzungen                                                                                   objective objectives achieve aim ambitious set objetivo objetivos alcanzar conseguir lograr estos tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen objectif objectifs atteindre but cet ambitieux obiettivo obiettivi raggiungere degli scopo quello doelstellingen doel doelstelling bereiken bereikt doelen objectivo objectivos alcan  ar atingir ambicioso conseguir m  l m  let uppn   m  len m  ls  ttningar m  ls  ttning andre anden side ene andet   vrige anderen andere einen wie andererseits anderer                                                               other one hand others another there otros otras otro otra parte dem  s muiden toisaalta muita muut muihin muun autres autre part c  t   ailleurs m  me altri altre altro altra dall parte andere anderzijds anderen ander als kant outros outras outro lado outra noutros andra sidan    annat ena annan dadeelenesfifritnlptsv dadeelenesfifritnlptsv dadeelenesfifritnlptsv dadeelenesfifritnlptsv figure2:europarltopics(t=400)theconcentrationparameter   fortheprioroverdocument-speci   ctopicdistributionsisinitializedto0.01t,whilethebasemeasuremisinitializedtotheuniformdistribution.hyperparameters   marere-estimatedevery10gibbsiterations.4.2analysisoftrainedmodelsfigure2showsthemostprobablewordsinalllan-guagesforfourexampletopics,frompltmwith400topics.the   rsttopiccontainswordsrelatingtotheeuropeancentralbank.thistopicprovidesanillustrationofthevariationintechnicalter-minologycapturedbypltm,includingthewidearrayofacronymsusedbydifferentlanguages.thesecondtopic,concerningchildren,demon-stratesthevariabilityofeverydayterminology:al-thoughthefourromancelanguagesareclosely882discussion

how would you use this?

how could you extend this?

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

10 / 15

author-topic model (rosen-zvi et al., uai    04)

goal: topic models that take into consideration author interests
training data: corpora with label for who wrote each document

papers from nips conference from 1987 to 1999
twitter posts from us politicians

why do this?

how to do this?

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

11 / 15

author-topic model (rosen-zvi et al., uai    04)

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

12 / 15

zwd        txwndd    adaxzwd        atda(a) topic (lda)(b) author(c) author-topicndndfigure1:generativemodelsfordocuments.(a)latentdirichletallocation(lda;bleietal.,2003),atopicmodel.(b)anauthormodel.(c)theauthor-topicmodel.prior.themixtureweightscorrespondingtothecho-senauthorareusedtoselectatopicz,andawordisgeneratedaccordingtothedistribution  correspond-ingtothattopic,drawnfromasymmetricdirichlet(  )prior.theauthor-topicmodelsubsumesthetwomodelsde-scribedaboveasspecialcases:topicmodelslikeldacorrespondtothecasewhereeachdocumenthasoneuniqueauthor,andtheauthormodelcorrespondstothecasewhereeachauthorhasoneuniquetopic.byestimatingtheparameters  and  ,weobtaininforma-tionaboutwhichtopicsauthorstypicallywriteabout,aswellasarepresentationofthecontentofeachdocu-mentintermsofthesetopics.intheremainderofthepaper,wewilldescribeasimplealgorithmforestimat-ingtheseparameters,comparethesedi   erentmodels,andillustratehowtheresultsproducedbytheauthor-topicmodelcanbeusedtoanswerquestionsaboutwhichwhichauthorsworkonsimilartopics.3gibbssamplingalgorithmsavarietyofalgorithmshavebeenusedtoestimatetheparametersoftopicmodels,frombasicexpectation-maximization(em;hofmann,1999),toapproximateid136methodslikevariationalem(bleietal.,2003),expectationpropagation(minka&la   erty,2002),andgibbssampling(gri   ths&steyvers,2004).genericemalgorithmstendtofaceproblemswithlocalmaximainthesemodels(bleietal.,2003),suggestingamovetoapproximatemethodsinwhichsomeoftheparameters   suchas  and     canbein-tegratedoutratherthanexplicitlyestimated.inthispaper,wewillusegibbssampling,asitprovidesasim-plemethodforobtainingparameterestimatesunderdirichletpriorsandallowscombinationofestimatesfromseverallocalmaximaoftheposteriordistribu-tion.theldamodelhastwosetsofunknownparameters   theddocumentdistributions  ,andthettopicdistri-butions     aswellasthelatentvariablescorrespond-ingtotheassignmentsofindividualwordstotopicsz.byapplyinggibbssampling(seegilks,richardson,&spiegelhalter,1996),weconstructamarkovchainthatconvergestotheposteriordistributiononzandthenusetheresultstoinfer  and  (gri   ths&steyvers,2004).thetransitionbetweensuccessivestatesofthemarkovchainresultsfromrepeatedlydrawingzfromitsdistributionconditionedonallothervariables,sum-mingout  and  usingstandarddirichletintegrals:p(zi=j|wi=m,z   i,w   i)   cwtmj+  !m   cwtm   j+v  cdtdj+  !j   cdtdj   +t  (1)wherezi=jrepresentstheassignmentsoftheithwordinadocumenttotopicj,wi=mrepresentstheobservationthattheithwordisthemthwordinthelexicon,andz   irepresentsalltopicassignmentsnotincludingtheithword.furthermore,cwtmjisthenumberoftimeswordmisassignedtotopicj,notincludingthecurrentinstance,andcdtdjisthenum-beroftimestopicjhasoccurredindocumentd,notincludingthecurrentinstance.foranysamplefromthismarkovchain,beinganassignmentofeverywordtoatopic,wecanestimate  and  using  mj=cwtmj+  !m   cwtm   j+v  (2)  dj=cdtdj+  !j   cdtdj   +t  (3)most likely author for a topic

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

13 / 15

wordprob.wordprob.wordprob.wordprob.likelihood0.0539recognition0.0400reinforcement0.0411kernel0.0683mixture0.0509character0.0336policy0.0371support0.0377em0.0470characters0.0250action0.0332vector0.0257density0.0398tangent0.0241optimal0.0208kernels0.0217gaussian0.0349handwritten0.0169actions0.0208set0.0205estimation0.0314digits0.0159function0.0178id1660.0204log0.0263image0.0157reward0.0165space0.0188maximum0.0254distance0.0153sutton0.0164machines0.0168parameters0.0209digit0.0149agent0.0136regression0.0155estimate0.0204hand0.0126decision0.0118margin0.0151authorprob.authorprob.authorprob.authorprob.tresp_v0.0333simard_p0.0694singh_s0.1412smola_a0.1033singer_y0.0281martin_g0.0394barto_a0.0471scholkopf_b0.0730jebara_t0.0207lecun_y0.0359sutton_r0.0430burges_c0.0489ghahramani_z0.0196denker_j0.0278dayan_p0.0324vapnik_v0.0431ueda_n0.0170henderson_d0.0256parr_r0.0314chapelle_o0.0210jordan_m0.0150revow_m0.0229dietterich_t0.0231cristianini_n0.0185roweis_s0.0123platt_j0.0226tsitsiklis_j0.0194ratsch_g0.0172schuster_m0.0104keeler_j0.0192randlov_j0.0167laskov_p0.0169xu_l0.0098rashid_m0.0182bradtke_s0.0161tipping_m0.0153saul_l0.0094sackinger_e0.0132schwartz_a0.0142sollich_p0.0141wordprob.wordprob.wordprob.wordprob.speech0.0823bayesian0.0450model0.4963hinton0.0329recognition0.0497gaussian0.0364models0.1445visible0.0124id480.0234posterior0.0355modeling0.0218procedure0.0120speaker0.0226prior0.0345parameters0.0205dayan0.0114context0.0224distribution0.0259based0.0116university0.0114word0.0166parameters0.0199proposed0.0103single0.0111system0.0151evidence0.0127observed0.0100generative0.0109acoustic0.0134sampling0.0117similar0.0083cost0.0106phoneme0.0131covariance0.0117account0.0069weights0.0105continuous0.0129log0.0112parameter0.0068parameters0.0096authorprob.authorprob.authorprob.authorprob.waibel_a0.0936bishop_c0.0563omohundro_s0.0088hinton_g0.2202makhoul_j0.0238williams_c0.0497zemel_r0.0084zemel_r0.0545de-mori_r0.0225barber_d0.0368ghahramani_z0.0076dayan_p0.0340bourlard_h0.0216mackay_d0.0323jordan_m0.0075becker_s0.0266cole_r0.0200tipping_m0.0216sejnowski_t0.0071jordan_m0.0190rigoll_g0.0191rasmussen_c0.0215atkeson_c0.0070mozer_m0.0150hochberg_m0.0176opper_m0.0204bower_j0.0066williams_c0.0099franco_h0.0163attias_h0.0155bengio_y0.0062de-sa_v0.0087abrash_v0.0157sollich_p0.0143revow_m0.0059schraudolph_n0.0078movellan_j0.0149schottky_b0.0128williams_c0.0054schmidhuber_j0.0056topic 31topic 61topic 71topic 100topic 19topic 24topic 29topic 87figure2:anillustrationof8topicsfroma100-topicsolutionforthenipscollection.eachtopicisshownwiththe10wordsandauthorsthathavethehighestid203conditionedonthattopic.foreachtopic,thetop10mostlikelyauthorsarewell-knownauthorsintermsofnipspaperswrittenonthesetopics(e.g.,singh,barto,andsuttoninrein-forcementlearning).whilemost(orderof80to90%)ofthe100topicsinthemodelaresimilarlyspeci   cintermsofsemanticcontent,theremaining2topicswedisplayillustratesomeoftheothertypesof   top-ics   discoveredbythemodel.topic71issomewhatgeneric,coveringabroadsetoftermstypicaltonipspapers,withasomewhat   atterdistributionoverau-thorscomparedtoothertopics.topic100issomewhatorientedtowardsgeo   hinton   sgroupattheuniver-sityoftoronto,containingthewordsthatcommonlyappearedinnipspapersauthoredbymembersofthatresearchgroup,withanauthorlistlargelyconsistingofhintonplushispaststudentsandpostdocs.figure3showssimilartypesofresultsfor4selectedtopicsfromtheciteseerdataset,whereagaintop-icsonspeechrecognitionandbayesianlearningshowup.however,sinceciteseerismuchbroaderincon-tent(coveringcomputerscienceingeneral)comparedtonips,italsoincludesalargenumberoftopicsnotwordprob.wordprob.wordprob.wordprob.speech0.1134probabilistic0.0778user0.2541stars0.0164recognition0.0349bayesian0.0671interface0.1080observations0.0150word0.0295id2030.0532users0.0788solar0.0150speaker0.0227carlo0.0309interfaces0.0433magnetic0.0145acoustic0.0205monte0.0308graphical0.0392ray0.0144rate0.0134distribution0.0257interactive0.0354emission0.0134spoken0.0132id1360.0253interaction0.0261galaxies0.0124sound0.0127probabilities0.0253visual0.0203observed0.0108training0.0104conditional0.0229display0.0128subject0.0101music0.0102prior0.0219manipulation0.0099star0.0087authorprob.authorprob.authorprob.authorprob.waibel_a0.0156friedman_n0.0094shneiderman_b0.0060linsky_j0.0143gauvain_j0.0133heckerman_d0.0067rauterberg_m0.0031falcke_h0.0131lamel_l0.0128ghahramani_z0.0062lavana_h0.0024mursula_k0.0089woodland_p0.0124koller_d0.0062pentland_a0.0021butler_r0.0083ney_h0.0080jordan_m0.0059myers_b0.0021bjorkman_k0.0078hansen_j0.0078neal_r0.0055minas_m0.0021knapp_g0.0067renals_s0.0072raftery_a0.0054burnett_m0.0021kundu_m0.0063noth_e0.0071lukasiewicz_t0.0053winiwarter_w0.0020christensen-j0.0059boves_l0.0070halpern_j0.0052chang_s0.0019cranmer_s0.0055young_s0.0069muller_p0.0048korvemaker_b0.0019nagar_n0.0050topic 10topic 209topic 87topic 20figure3:anillustrationof4topicsfroma300-topicsolutionfortheciteseercollection.eachtopicisshownwiththe10wordsandauthorsthathavethehighestid203conditionedonthattopic.seeninnips,fromuserinterfacestosolarastrophysics(figure3).againtheauthorlistsarequitesensible   forexample,benshneidermanisawidely-knownse-nior   gureintheareaofuser-interfaces.forthenipsdataset,2000iterationsofthegibbssamplertook12hoursofwall-clocktimeonastan-dardpcworkstation(22secondsperiteration).cite-seertook111hoursfor700iterations(9.5minutesperiteration).thefulllistoftablescanbefoundathttp://www.datalab.uci.edu/author-topic,forboththe100-topicnipsmodelandthe300-topiccite-seermodel.inadditionthereisanonlinejavabrowserforinteractivelyexploringauthors,topics,anddocuments.theresultsaboveuseasinglesamplefromthegibbssampler.acrossdi   erentsampleseachsamplecancontainsomewhatdi   erenttopicsi.e.,somewhatdif-ferentsetsofmostprobablewordsandauthorsgiventhetopic,sinceaccordingtotheauthor-topicmodelthereisnotasinglesetofconditionalprobabilities,  and  ,butratheradistributionovertheseconditionalprobabilities.intheexperimentsinthesectionsbelow,weaverageovermultiplesamples(restrictedto10forcomputationalconvenience)inabayesianfashionforpredictivepurposes.4.2evaluatingpredictivepowerinadditiontothequalitativeevaluationoftopic-authorandtopic-wordresultsshownabove,wealsoevaluatedtheproposedauthor-topicmodelintermsofperplexity,i.e.,itsabilitytopredictwordsonnewunseendocuments.wedividedthed=1,740nipspapersintoatrainingsetof1,557paperswithatotalof2,057,729words,andatestsetof183papersofperplexity as a function of number of observed words

perplexity(wtest,d | wtrain,d , ad ) = exp(cid:104)    ln p(wtest,d|wtrain,d ,ad )

ntest,d

(cid:105)

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

14 / 15

5 topicsnd(train)01416642561024perplexity12001600200024002800320010 topicsnd(train)0141664256102450 topicsnd(train)01416642561024100 topicsnd(train)01416642561024perplexity120016002000240028003200200 topicsnd(train)01416642561024400 topicsnd(train)01416642561024400 topicsnd(train)01416642561024perplexity2000400060008000100001200014000topics (lda)author-topicsauthorfigure4:perplexityversusn(train)dfordi   erentnumbersoftopics,fortheauthor,author-topic,andtopic(lda)models.ure5(thickline).wealsoderivedtheperplexityofthetestdocumentsconditionedoneachoneoftheau-thorsfromthenipscollection,perplexity(wd|a)fora=1,...,k.thisresultsink=2,037di   erentper-plexityvalues.thenwerankedtheresultsandvariouspercentilesfromthisrankingarepresentedinfigure5.onecanseethatmakinguseoftheauthorshipinformationsigni   cantlyimprovesthepredictivelog-likelihood:themodelhasaccurateexpectationsaboutthecontentofdocumentsbyparticularauthors.asthenumberoftopicsincreasestherankingofthecor-rectauthorimproves,wherefor400topicstheaver-agedrankingofthecorrectauthoriswithinthe20highestrankedauthors(outof2,037possibleauthors).consequently,themodelprovidesausefulmethodforidentifyingpossibleauthorsfornoveldocuments.4.3illustrativeapplicationsofthemodeltheauthor-topicmodelcouldbeusedforavarietyofapplicationssuchasautomatedreviewerrecommenda-tions,i.e.,givenanabstractofapaperandalistoftheauthorsplustheirknownpastcollaborators,generatealistofotherhighlylikelyauthorsforthisabstractwhomightserveasgoodreviewers.suchataskre-quirescomputingthesimilaritybetweenauthors.toillustratehowthemodelcouldbeusedinthisrespect,wede   nedthedistancebetweenauthorsiandjasthesymmetrickldivergencebetweenthetopicsdistribu-tionconditionedoneachoftheauthors:skl(i,j)=t!t=1"  itlog  it  jt+  jtlog  jt  it#.(9)asearlier,wederivedtheaveragedsymmetrickldi-vergencebyaveragingoversamplesfromtheposteriortable1:symmetrickldivergenceforpairsofauthorsauthorsnt=400t=200t=100bartlettp(8)-2.521.580.90shawe-taylorj(8)bartoa(11)23.342.181.25singhs(17)amaris(9)33.442.481.57yangh(5)singhs(17)23.692.331.35suttonr(7)moorea(11)-4.252.891.87suttonr(7)median-5.524.013.33maximum-16.6114.9113.32note:nisnumberofcommonpapersinnipsdataset.distribution,p(  |dtrain).wesearchedforsimilarpairsofauthorsinthenipsdatasetusingthedistancemeasureabove.wesearchedonlyoverauthorswhowrotemorethan5papersinthefullnipsdataset   thereare125suchauthorsoutofthefullsetof2037.table1showsthe5pairsofauthorswiththehighestaveragedsklforthe400-topicmodel,aswellasthemedianandmin-imum.resultsforthe200and100-topicmodelsarealsoshownasarethenumberofpapersinthedatasetforeachauthor(inparentheses)andthenumberofco-authoredpapersinthedataset(2ndcolumn).allresultswereaveragedover10samplesfromthegibbssampler.againtheresultsarequiteintuitive.forexample,althoughauthorsbartlettandshawe-taylordidnothaveanyco-authoreddocumentsinthenipscollec-supervised topic models

the inferred    or z can be used as features in many prediction tasks.

performance can be improved by jointly training the representation
and the predictor.

hence, supervised lda:

david sontag (nyu)

introduction to machine learning

lecture 22, april 19, 2016

15 / 15

