l1	
   regulariza,on	
   &	
   intro	
   to	
   learning	
   theory	
   

lecture	
   8	
   

david	
   sontag	
   

new	
   york	
   university	
   

feature	
   selec9on	
   

se7ng:	
   lots	
   of	
   possible	
   features,	
   many	
   of	
   which	
   are	
   irrelevant	
   

example:	
   

when	
   studying	
   depression	
   in	
   teens,	
   a	
   researcher	
   distributes	
   a	
   
ques9onnaire	
   of	
   250	
   di   erent	
   ques9ons,	
   many	
   of	
   them	
   related	
   or	
   
irrelevant.	
   	
   

goal:	
   find	
   a	
   small	
   set	
   of	
   ques9ons	
   that	
   can	
   be	
   used	
   to	
   quickly	
   determine	
   
whether	
   or	
   not	
   a	
   teen	
   is	
   depressed.	
   

feature	
   selec9on	
   

se7ng:	
   lots	
   of	
   possible	
   features,	
   many	
   of	
   which	
   are	
   irrelevant	
   

example:	
   

when	
   studying	
   depression	
   in	
   teens,	
   a	
   researcher	
   distributes	
   a	
   
ques9onnaire	
   of	
   250	
   di   erent	
   ques9ons,	
   many	
   of	
   them	
   related	
   or	
   
irrelevant.	
   	
   

goal:	
   find	
   a	
   small	
   set	
   of	
   ques9ons	
   that	
   can	
   be	
   used	
   to	
   quickly	
   determine	
   
whether	
   or	
   not	
   a	
   teen	
   is	
   depressed.	
   

min

w

mathema9cally:	
   

`(w    x, y) +  (non-zero elements in w)

feature	
   selec9on	
   

se7ng:	
   lots	
   of	
   possible	
   features,	
   many	
   of	
   which	
   are	
   irrelevant	
   

example:	
   

when	
   studying	
   depression	
   in	
   teens,	
   a	
   researcher	
   distributes	
   a	
   
ques9onnaire	
   of	
   250	
   di   erent	
   ques9ons,	
   many	
   of	
   them	
   related	
   or	
   
irrelevant.	
   	
   

goal:	
   find	
   a	
   small	
   set	
   of	
   ques9ons	
   that	
   can	
   be	
   used	
   to	
   quickly	
   determine	
   
whether	
   or	
   not	
   a	
   teen	
   is	
   depressed.	
   

min

w

mathema9cally:	
   

`(w    x, y) +  (non-zero elements in w)

l1	
   regulariza9on	
   

       penalizing	
   the	
   l1	
   norm	
   of	
   the	
   weight	
   vector	
   
leads	
   to	
   sparse	
   (read:	
   many	
   0   s)	
   solu9ons	
   for	
   
w.	
   

min

w

`(w    x, y) +  |w|

       why?	
   	
   

l1	
   regulariza9on	
   

       penalizing	
   the	
   l1	
   norm	
   of	
   the	
   weight	
   vector	
   
leads	
   to	
   sparse	
   (read:	
   many	
   0   s)	
   solu9ons	
   for	
   
w.	
   

min

w

`(w    x, y) +  |w|

minimize	
   this:	
   

       why?	
   	
   

subject	
   to	
   
constant	
   l2	
   norm	
   

subject	
   to	
   
constant	
   l1	
   norm	
   

l1	
   regulariza9on	
   

       penalizing	
   the	
   l1	
   norm	
   of	
   the	
   weight	
   vector	
   
leads	
   to	
   sparse	
   (read:	
   many	
   0   s)	
   solu9ons	
   for	
   
w.	
   

min
(what	
   would	
   gradient	
   descent	
   do?)	
   

`(w    x, y) +  |w|

intui9on	
   #2	
      	
   w.w.g.d.d	
   	
   

w

       why?	
   	
   

d
dwi

 ||w||2 =    wi

d
dwi

 |w| =    

l1	
   regulariza9on	
   

       penalizing	
   the	
   l1	
   norm	
   of	
   the	
   weight	
   vector	
   
leads	
   to	
   sparse	
   (read:	
   many	
   0   s)	
   solu9ons	
   for	
   
w.	
   

min
(what	
   would	
   gradient	
   descent	
   do?)	
   

`(w    x, y) +  |w|

intui9on	
   #2	
      	
   w.w.g.d.d	
   	
   

w

       why?	
   	
   

d
dwi

 ||w||2 =    wi

d
dwi

 |w| =    

the	
   push	
   
towards	
   0	
   gets	
   
weaker	
   as	
   wi	
   
gets	
   smaller	
   

always	
   
pushes	
   
elements	
   of	
   
wi	
   towards	
   0	
   

example:	
   early	
   detec,on	
   of	
   type	
   2	
   diabetes	
   

       global	
   prevalence	
   will	
   go	
   from	
   171	
   million	
   in	
   

2000	
   to	
   366	
   million	
   in	
   2030	
   

       25%	
   of	
   people	
   in	
   the	
   us	
   with	
   diabetes	
   are	
   

undiagnosed	
   

       leads	
   to	
   complica9ons	
   of	
   cardiovascular,	
   
cerebrovascular,	
   renal,	
   and	
   vision	
   systems	
   

       early	
   lifestyle	
   changes	
   shown	
   to	
   prevent	
   or	
   delay	
   
the	
   onset	
   of	
   the	
   disease	
   be]er	
   than	
   me^ormin	
   

tradi9onal	
   

risk	
   

assessment	
   

       use	
   small	
   number	
   of	
   
risk	
   factors	
   (e.g.	
   ~20)	
   
       easy	
   to	
   ask/measure	
   

in	
   the	
   o   ce	
   

       simple	
   model:	
   

can	
   calculate	
   scores	
   
by	
   hand	
   

popula9on-     level	
   risk	
   stra9   ca9on	
   	
   
       key	
   idea:	
   use	
   automa9cally	
   collected	
   

administra9ve,	
   u9liza9on,	
   and	
   clinical	
   data	
   
       machine	
   learning	
   will	
      nd	
   surrogates	
   for	
   risk	
   

factors	
   that	
   would	
   otherwise	
   be	
   missing	
   

       enables	
   risk	
   stra9   ca9on	
   at	
   the	
   popula9on	
   level	
   

   	
   millions	
   of	
   pa9ents	
   

[n.	
   razavian,	
   s.	
   blecker,	
   a.m.	
   schmidt,	
   a.	
   smith-     mclallen,	
   s.	
   nigam,	
   d.	
   sontag.	
   
popula9on-     level	
   predic9on	
   of	
   type	
   2	
   diabetes	
   using	
   claims	
   data	
   and	
   analysis	
   of	
   
risk	
   factors.	
   big	
   data,	
   jan.	
   2016.]	
   

administra9ve	
   &	
   clinical	
   data	
   

eligibility	
   record:	
   
-     member	
   id	
   
-     age/gender	
-     id	
   of	
   subscriber	
   
-     company	
   code	
   

medica,ons:	
   

-     ndc	
   code	
   (drug	
   name)	
   	
   
-     days	
   of	
   supply	
   
-     quan9ty	
   
-     service	
   provider	
   id	
   
-     date	
   of	
      ll	
   

pa,ent:	
   

,me	
   

medical	
   claims:	
   
-     icd9	
   diagnosis	
   code	
   
-     cpt	
   code	
   (procedure)	
   
-     specialty	
   
-     loca9on	
   of	
   service	
   
-     date	
   of	
   service	
   

lab	
   tests:	
   

-     loinc	
   code	
   (urine	
   or	
   
blood	
   test	
   name)	
   
-     results	
   (actual	
   values)	
   
-     lab	
   id	
   
-     range	
   high/low-     date	
   

machine	
   learning	
   

task:	
   predict	
   the	
   id203	
   of	
   a	
   member	
   developing	
   diabetes	
   

training	
   data:	
   
member	
   informa,on	
   

representa,on	
   as	
   a	
   
set	
   of	
   feature	
   vectors	
   

training	
   labels:	
   
has	
   the	
   member	
   
developed	
   type	
   2	
   diabetes	
   
within	
   the	
   predic,on	
   
window?	
   

650,000	
   members   	
   
data	
   

op,mize	
   model	
   
parameters	
   
model:	
   l1	
   regularized	
   
logis5c	
   regression	
   

1)	
   training	
   step	
   
(model	
      7ng)	
   

model	
   parameters	
   

new	
   member   s	
   data	
   

representa,on	
   as	
   a	
   
feature	
   vector	
   

predict	
   the	
   label	
   for	
   
the	
   new	
   pa5ent	
   

2)	
   predic,on	
   step	
   

features	
   

32	
   service	
   places	
   
(urgent	
   care,	
   inpa9ent,	
   
outpa9ent,	
      )	
   

999	
   medica,on	
   groups	
   
(laxa9ves,	
   me^ormin,	
   an9-     
arthri9cs,	
      )	
   

457	
   procedure	
   
groups	
   

228	
   special,es	
   
(cardiology,	
   rheumatology,	
      )	
   

7000	
   laboratory	
   
indicators	
   

39	
   coverage	
   features	
   

22	
   risk	
   factors	
   derived	
   from	
   literature	
   
(age,	
   sex,	
   obesity,	
   fas9ng	
   glucose	
   level,	
   	
   
cardiovascular	
   disease,	
   hypertension,	
      )	
   

for	
   the	
   1000	
   most	
   frequent	
   lab	
   tests:	
   
      	
   	
   was	
   the	
   test	
   ever	
   administered?	
   
      	
   	
   was	
   the	
   result	
   ever	
   low?	
   
      	
   	
   was	
   the	
   result	
   ever	
   high?	
   
      	
   	
   was	
   the	
   result	
   ever	
   normal?	
   
      	
   	
   is	
   the	
   value	
   increasing?	
   
      	
   	
   is	
   the	
   value	
   decreasing?	
   
      	
   	
   is	
   the	
   value	
      uctua9ng?	
   

features	
   

32	
   service	
   places	
   
(urgent	
   care,	
   inpa9ent,	
   
outpa9ent,	
      )	
   

999	
   medica,on	
   groups	
   
(laxa9ves,	
   me^ormin,	
   an9-     
arthri9cs,	
      )	
   

457	
   procedure	
   
groups	
   

228	
   special,es	
   
(cardiology,	
   rheumatology,	
      )	
   

7000	
   laboratory	
   
indicators	
   

16,000	
   icd-     9	
   
diagnosis	
   codes	
   
(all	
   history)	
   

39	
   coverage	
   features	
   

22	
   risk	
   factors	
   derived	
   from	
   literature	
   
(age,	
   sex,	
   obesity,	
   fas9ng	
   glucose	
   level,	
   	
   
cardiovascular	
   disease,	
   hypertension,	
      )	
   

all	
   history	
   

24	
   month	
   
history	
   

6	
   month	
   
history	
   

total	
   features	
   per	
   pa,ent:	
   42,000	
   

what	
   are	
   the	
   discovered	
   risk	
   factors?	
   

feature	
   name 
impaired fasting glucose  (790.21) 
abnormal glucose nec  (790.29) 
hypertension  (401) 
obstructive sleep apnea  (327.23) 
obesity  (278) 
abnormal blood chemistry  (790.6) 
hyperlipidemia  (272.4) 
shortness of breath  (786.05) 
esophageal reflux  (530.81) 
acute bronchitis  (466.0) 
actinic keratosis  (702.0) 

posi,ve	
   weights	
   

addi,onal	
   risk	
   factors	
   iden_ied:	
   
impaired	
   oral	
   glucose	
   tolerance,	
   
chronic	
   liver	
   disease,	
   pituitary	
   
dwar   sm,	
   hypersomnia	
   with	
   
sleep	
   apnea,	
   joint	
   replaced	
   knee,	
   
liver	
   disorder,	
   iron	
   de   ciency	
   
anemia,	
   mitral	
   valve	
   disorder   	
   

diagnos9c	
   groups	
   

procedure	
   group	
   

lab	
   test	
    medica9on	
   group	
   

service	
   place	
   

what	
   are	
   the	
   discovered	
   risk	
   factors?	
   

feature	
   name 
hemoglobin a1c / hemoglobin.total - high 
glucose - high 
hemoglobin a1c / hemoglobin.total - request for test  
cholesterol.in hdl - low 
cholesterol.total / cholesterol.in hdl - high 
cholesterol.in vldl - request for test 
carbon dioxide - request for test 
glomerular filtration rate/1.73 sq. m. predicted 
black - request for test 

posi,ve	
   weights	
   

addi,onal	
   risk	
   factors	
   iden_ied:	
   
potassium	
   (low),	
   erythrocyte	
   
mean	
   corpuscular	
   hemoglobin	
   
concentra9on	
   (   uctua9ng),	
   
erythrocyte	
   distribu9on	
   width	
   
(high),	
   alanine	
   aminotransferase	
   
(high),	
   cholesterol.in	
   ldl	
   
(increasing),	
   crea9nine	
   
(decreasing),	
   albumin/globulin	
   
(increasing)   	
   

diagnos9c	
   groups	
   

procedure	
   group	
   

lab	
   test	
    medica9on	
   group	
   

service	
   place	
   

what	
   are	
   the	
   discovered	
   risk	
   factors?	
   

feature	
   name 
routine chest xray  
medication group: anti-arthritics  
service place: emergency room - hospital  
routine medical exam (v700) 
routine gynecological examination (v7231) 
routine child health exam (v202 ) 

very	
   posi,ve	
   

very	
   nega,ve	
   

~700	
   risk	
   factors	
   selected	
   for	
   model	
   

diagnos9c	
   groups	
   

procedure	
   group	
   

lab	
   test	
    medica9on	
   group	
   

service	
   place	
   

type	
   2	
   diabetes	
   predic9on	
   accuracy	
   

using	
   pa9ent	
   data	
   through	
   dec.	
   31,	
   2008,	
   who	
   will	
   be	
   newly	
   
diagnosed	
   with	
   type	
   2	
   diabetes	
   in	
   the	
   following	
   years?	
   

model 

auc 

2009-     2011	
   
(incident	
   
diabetes)	
   

2011-     2013	
   
(future	
   
diabe9cs)	
   

literature 
features only  0.75 
overall 
model  

0.8 

literature 
features only  0.72 
overall 
model  

0.76 

 top 10000 
top 1000 
area	
   under	
   the	
   roc	
   curve	
   (auc)	
   =	
   
predictions  
predictions  
randomly	
   choosing	
   two	
   members,	
   one	
   
who	
   did	
   get	
   diabetes	
   and	
   one	
   who	
   did	
   not,	
   
can	
   we	
   predict	
   which	
   is	
   which?	
   

sensitivity specificity  ppv  sensitivity specificity  ppv 

0.014 

0.996 

0.1 

0.114 

0.967 

0.08 

highest	
   risk	
   popula9on	
   	
   

0.033 

0.997 

0.24 

0.212 

0.969 

0.14 

0.013 

0.995 

0.04 

0.116 

2	
   years	
   lead	
   9me	
   for	
   this	
   
popula9on	
   

0.957 

0.03 

0.023 

0.995 

0.07 

0.179 

0.958 

0.05 

type	
   2	
   diabetes	
   predic9on	
   accuracy	
   

type	
   2	
   diabetes	
   predic9on	
   accuracy	
   

using	
   pa9ent	
   data	
   through	
   dec.	
   31,	
   2008,	
   who	
   will	
   be	
   newly	
   
diagnosed	
   with	
   type	
   2	
   diabetes	
   in	
   the	
   following	
   years?	
   

model 

auc 

2009-     2011	
   
(incident	
   
diabetes)	
   

2011-     2013	
   
(future	
   
diabe9cs)	
   

literature 
features only  0.75 
overall 
model  

0.8 

literature 
features only  0.72 
overall 
model  

0.76 

top 1000 
predictions  

 top 10000 
predictions  

sensitivity = tp/p 
   true positive rate    or 
sensitivity specificity  ppv  sensitivity specificity  ppv 
   recall    

0.014 

0.996 

0.1 

0.033 

0.997 

0.24 

0.013 

0.995 

0.04 

0.114 

0.967 

0.08 

specificity = tn/n 
   true negative rate    

0.212 

0.969 

0.14 

0.116 

ppv = tp/(tp+fp) 
0.03 
   positive predictive 
value    

0.957 

0.023 

0.995 

0.07 

0.179 

0.958 

0.05 

type	
   2	
   diabetes	
   predic9on	
   accuracy	
   

using	
   pa9ent	
   data	
   through	
   dec.	
   31,	
   2008,	
   who	
   will	
   be	
   newly	
   
diagnosed	
   with	
   type	
   2	
   diabetes	
   in	
   the	
   following	
   years?	
   

model 

auc 

top 1000 
predictions  

 top 10000 
predictions  

sensitivity specificity  ppv  sensitivity specificity  ppv 

2009-     2011	
   
(incident	
   
diabetes)	
   

2011-     2013	
   
(future	
   
diabe9cs)	
   

literature 
features only  0.75 
overall 
model  

0.8 

literature 
features only  0.72 
overall 
model  

0.76 

0.014 

0.996 

0.1 

0.114 

0.967 

0.08 

0.033 

0.997 

0.24 

0.212 

0.969 

0.14 

0.013 

0.995 

0.04 

0.116 

0.957 

0.03 

0.023 

0.995 

0.07 

0.179 

0.958 

0.05 

what   s	
   next   	
   
       we	
   gave	
   several	
   machine	
   learning	
   algorithms:	
   

       id88	
   
       linear	
   support	
   vector	
   machine	
   (id166)	
   
       id166	
   with	
   kernels,	
   e.g.	
   polynomial	
   or	
   gaussian	
   

       how	
   do	
   we	
   guarantee	
   that	
   the	
   learned	
   classi   er	
   will	
   perform	
   well	
   

on	
   test	
   data?	
   

       how	
   much	
   training	
   data	
   do	
   we	
   need?	
   

example:	
   id88	
   applied	
   to	
   spam	
   classi   ca9on	
   

      

in	
   your	
   homework	
   1,	
   you	
   trained	
   a	
   spam	
   classi   er	
   using	
   id88	
   
       the	
   training	
   error	
   was	
   always	
   zero	
   
       with	
   few	
   data	
   points,	
   there	
   is	
   a	
   big	
   gap	
   between	
   training	
   error	
   and	
   

test	
   error!	
   

test	
   error	
   -     	
   training	
   error	
   >	
   0.065	
   

test	
   error	
   

(percentage	
   misclassi   ed)	
   

test	
   error	
   -     	
   training	
   error	
   <	
   0.02	
   

number	
   of	
   training	
   examples	
   

how	
   much	
   training	
   data	
   do	
   you	
   need?	
   

       depends	
   on	
   what	
   hypothesis	
   class	
   the	
   learning	
   algorithm	
   considers	
   

       for	
   example,	
   consider	
   a	
   memoriza9on-     based	
   learning	
   algorithm	
   

input:	
   training	
   data	
   s	
   =	
   {	
   (xi,	
   yi)	
   }	
   

      
       output:	
   func9on	
   f(x)	
   which,	
   if	
   there	
   exists	
   (xi,	
   yi)	
   in	
   s	
   such	
   that	
   x=xi,	
   predicts	
   yi,	
   
       this	
   learning	
   algorithm	
   will	
   always	
   obtain	
   zero	
   training	
   error	
   
       but,	
   it	
   will	
   take	
   a	
   huge	
   amount	
   of	
   training	
   data	
   to	
   obtain	
   small	
   test	
   error	
   

and	
   otherwise	
   predicts	
   the	
   majority	
   label	
   

(i.e.,	
   its	
   generaliza9on	
   performance	
   is	
   horrible)	
   

       linear	
   classi   ers	
   are	
   powerful	
   precisely	
   because	
   of	
   their	
   simplicity	
   

       generaliza9on	
   is	
   easy	
   to	
   guarantee	
   

roadmap	
   of	
   next	
   lectures	
   

1.    generaliza9on	
   of	
      nite	
   hypothesis	
   spaces	
   
2.    vc-     dimension	
   

       will	
   show	
   that	
   linear	
   classi   ers	
   need	
   to	
   see	
   approximately	
   d	
   training	
   points,	
   

where	
   d	
   is	
   the	
   dimension	
   of	
   the	
   feature	
   vectors	
   

      

explains	
   the	
   good	
   performance	
   we	
   obtained	
   using	
   id88!!!!	
   
(we	
   had	
   a	
   few	
   thousand	
   features)	
   

3.    margin	
   based	
   generaliza9on	
   
       applies	
   to	
   in   nite	
   dimensional	
   

feature	
   vectors	
   (e.g.,	
   gaussian	
   kernel)	
   

test	
   error	
   
(percentage	
   
misclassi   ed)	
   

id88	
   algorithm	
   
on	
   spam	
   classi   ca9on	
   

number	
   of	
   training	
   examples	
   

[figure	
   from	
   cynthia	
   rudin]	
   

