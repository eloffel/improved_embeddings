support vector machines (id166s) 

lecture 6 

david sontag 

new york university 

slides adapted from luke zettlemoyer, vibhav gogate, 

and carlos guestrin 

pegasos vs. id88 

pegasos algorithm 
initialize: w1 = 0, t=0 
for iter = 1,2,   ,20 

 for j=1,2,   ,|data| 
 
 
 
 
 
 

 t = t+1 
   t = 1/(t  ) 
 if yj(wt xj) < 1 
 
 else 
 

 wt+1 = (1-  t  ) wt +   t yj xj 

 wt+1 = (1-  t  ) wt 

output: wt+1 

pegasos vs. id88 

id88 algorithm 
initialize: w1 = 0, t=0 
for iter = 1,2,   ,20 

 for j=1,2,   ,|data| 
 
 
 
 
 
 

 t = t+1 
   t = 1/(t  ) 
 if yj(wt xj) < 1 
0 
 
 else 
 

 wt+1 = (1-  t  ) wt +   t yj xj 

 wt+1 = (1-  t  ) wt 

output: wt+1 

much faster than previous methods 

       3 datasets (provided by joachims) 

       reuters ccat (800k examples, 47k features) 
       physics arxiv (62k examples, 100k features) 
       covertype (581k examples, 54 features) 

training time 
(in seconds): 

pegasos 

id166-perf 

id166-light 

reuters 

covertype 

astro-physics 

2 

6 

2 

77 

85 

5 

20,075 

25,514 

80 

running time guarantee 
error decomposition
prediction error

[shalev schwartz, 
srebro    08] 

err(w)
err(w*)

err(w0)

note: w0 is redefined in this 
context (see below)     
does not refer to initial weight 
vector 

    approximation error:

    best error achievable by large-margin predictor
    error of population minimizer

w0 = argmin e[f(w)] = argmin   |w|2 + ex,y[loss(   w,x   ;y)]

    extra error due to replacing e[loss] with empirical loss

    estimation error:

w* = arg min fn(w)

    optimization error:

    extra error due to only optimizing to within finite precision

running time guarantee 
error decomposition
prediction error

[shalev schwartz, 
srebro    08] 

err(w)
err(w*)

err(w0)

pegasos 

t =   o    1
        

after                  

  updates: 

    approximation error:

    best error achievable by large-margin predictor
    error of population minimizer

w0 = argmin e[f(w)] = argmin   |w|2 + ex,y[loss(   w,x   ;y)]

   
 err(wt) < err(w0) +  

    estimation error:

w* = arg min fn(w)

    optimization error:

    extra error due to replacing e[loss] with empirical loss

with id203 1- 
 

    extra error due to only optimizing to within finite precision

extending to multi-class classification 

w+ 

one versus all classification 

learn 3 classifiers: 
      - vs {o,+}, weights w- 
      + vs {o,-}, weights w+ 
      o vs {+,-}, weights wo 

w- 

wo 

predict label using: 

any problems? 

could we learn this (1-d) dataset? ! 

-1 

0 

1 

multi-class id166 

w+ 

w- 

wo 

simultaneously learn 3 sets 
of weights: 
      how do we guarantee the 
correct labels? 
      need new constraints! 

the    score    of the correct 
class must be better than the 
   score    of wrong classes: 

multi-class id166 

as for the id166, we introduce slack variables and maximize margin: 

to predict, we use: 

now can we learn it?  ! 

-1 

0 

1 

how to deal with imbalanced data? 

      

in many practical applications we may have 
imbalanced data sets 

       we may want errors to be equally distributed 

between the positive and negative classes 
       a slight modification to the id166 objective 

does the trick! 

class-specific weighting of the slack variables 

what if the data is not linearly 

separable? 

use features of features  
of features of features   . 

 (x) =

x(1)
. . .
x(n)

x(1)x(2)
x(1)x(3)

. . .
ex(1)
. . .

                                     

                                       

feature space can get really large really quickly! 

key idea #3: the kernel trick 
       high dimensional feature spaces at no extra cost! 
       after every update (of pegasos), the weight vector can 

       as a result, prediction can be performed with:  

   iyixi

be written in the form:  

w =xi
   iyi (xi))     (x)   
   iyi( (xi)     (x))   
   iyik(xi, x)   

  y   sign(w     (x))

= sign   (xi
= sign   xi
= sign   xi

where k(x, x0) =  (x)     (x0).

common kernels 

       polynomials of degree exactly d 

       polynomials of degree up to d 

       gaussian kernels 

       sigmoid 

       and many others: very active area of research! 

   w

 jyjxj

                                       
                 
                                       
                 
                                       
                 
                                       
                 
= w   j
   (x) =
= w   j
v2     = u1v1 + u2v2 = u.v
u2     .  v1
   (u).   (v) =  u1
polynomial kernel 
= w   j
= w   j
v2     = u1v1 + u2v2 = u.v
u2     .  v1
   (u).   (v) =  u1
         = u2
         .            
   (u).   (v) =            
v2     = u1v1 + u2v2 = u.v
u2     .  v1
   (u).   (v) =  u1
v2     = u1v1 + u2v2 = u.v
u2     .  v1
   (u).   (v) =  u1
         = u2
         .            
   (u).   (v) =            
= w   j
         = u2
         .            
   (u).   (v) =            
u2     .  v1
   (u).   (v) =  u1
v2     = u1v1 + u2v2 = u.v

. . .
ex(1)
ex(1)
   l
. . .
. . .
   w
. . .
 jyjxj
ex(1)
 jyjxj
v2
. . .
1
v1v2
v2
1
v2v1
v1v2
 jyjxj
v2
2
v2v1
v2
2
1 + 2u1v1u2v2 + u2

u2
1
u1u2
u2u1
v2
1
u2
v1v2
= (u1v1 + u2v2)2
2
v2v1
v2
2

   l
   w
u2
1
u1u2
u2u1
u2
2

x(1)x(2)
x(1)x(3)

u2
1
u1u2
u2u1
u2
2

= (u1v1 + u2v2)2
= (u.v)2
   (u).   (v) = (u.v)d

for any d (we will skip proof): 

   (u).   (v) = (u.v)d

   l
   l
   w
   w

 jyjxj

d=1 

d=2 

1v2

1v2

2v2
2

1 + 2u1v1u2v2 + u2
1v2

1 + 2u1v1u2v2 + u2

   (u).   (v) = (u.v)d

   (u).   (v) = (u.v)d

polynomials of degree exactly d 

quadratic kernel 

[tommi jaakkola] 

gaussian kernel 

level sets, i.e.                  for some r 

support vectors 

[cynthia rudin] 

[mblondel.org] 

kernel algebra 

q: how would you prove that the    gaussian kernel    is a valid kernel? 
a: expand the euclidean norm as follows: 

then, apply (e) from above 

the feature mapping is 

infinite dimensional! 

to see that this is a kernel, use the 
taylor series expansion of the 
exponential, together with repeated 
application of (a), (b), and (c): 

[justin domke] 

dual id166 interpretation: sparsity 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

final solution tends to 
be sparse 
        j=0 for most j 
      don   t need to store these 
points to compute w or make 
predictions  

support vectors: 
         j   0 

non-support vectors: 
        j=0 
      moving them will not 
change w 

overfitting? 

       huge feature space with kernels: should we worry about 

overfitting? 
       id166 objective seeks a solution with large margin 

       theory says that large margin leads to good generalization 

(we will see this in a couple of lectures) 
       but everything overfits sometimes!!! 
       can control by: 

       setting c  
       choosing a better kernel 
       varying parameters of the kernel (width of gaussian, etc.) 

