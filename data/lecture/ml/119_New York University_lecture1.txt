introduction to machine learning 

(csci-ua.0480-007) 

david sontag 

new york university 

slides adapted from luke zettlemoyer, pedro domingos, and 

carlos guestrin 

logistics 

       class webpage: 

       http://cs.nyu.edu/~dsontag/courses/ml16/ 
       sign up for piazza! 
       office hours: tbd 
       teaching assistant: 

kevin jiao <jjiao@stern.nyu.edu> 

       graders: 

       yijun xiao <ryjxiao@nyu.edu> 
       alexandre sablayrolles 

<alexandre.sablayrolles@gmail.com> 

evaluation 

       6-7 homeworks (50%) 

      both theory and programming 
      collaboration policy: 

       first try to solve the problems on your own  
       then, can discuss with other classmates 
       write-up solutions on your own 
       list names of anyone you talked to  

       midterm exam (25%) 
       project (20%) 
       course participation (5%) 

projects 

       be creative     think of new problems that you can 

tackle using machine learning 
       scope: ~40 hours/person 

       logistics: 

       2-3 students per group 
       begins mid-march. project proposal due week after 

midterm exam 

       will still be problem sets during this period! 

prerequisites 

required: 
       basic algorithms (cs 310) 

       id145, algorithmic analysis 
       can be taken concurrently 

strongly recommended: 
       id202 (math 140) 

       matrices, vectors, systems of linear equations 
       eigenvectors, matrix rank 
       singular value decomposition 

       multivariable calculus (math 123) 

       derivatives, integration, tangent planes 
       optimization, lagrange multipliers 

       good programming skills: python highly recommended 

source materials 

no textbook required. readings will come from 
freely available online material. 

if you really want a book for an additional reference, these 
are ok options: 
       c. bishop, pattern recognition and machine learning, 
springer, 2007 
       k. murphy, machine learning: a probabilistic 
perspective, mit press, 2012 
           may update this list throughout semester. i wouldn   t 
buy anything yet. 

what is machine learning ? 

(by examples) 

classification 

from data to discrete classes 

spam filtering  

data 

prediction 

spam 
vs. 

not spam 

face recognition 

example training images 

for each orientation 

10 

  2009 carlos guestrin 

weather prediction 

regression 

predicting a numeric value 

stock market 

weather prediction revisited 

temperature 

72   f 

ranking 

comparing items 

web search 

given image, find similar images 

http://www.tiltomo.com/ 

id185 

id126s 

id126s 

machine learning competition with a $1 million prize 

id91 

discovering structure in data 

id91 data: group similar things 

id91 images 

set of images 

[goldberger et al.] 

id91 web search results 

embedding 

visualizing data 

embedding images 

      

images have 
thousands or 
millions of pixels. 
       can we give each 

image a coordinate, 
such that similar 
images are near 
each other? 

26 

  2009 carlos guestrin 
[saul & roweis    03] 

embedding words 

[joseph turian] 

embedding words (zoom in) 

[joseph turian] 

id170 

from data to discrete classes 

id103 

natural language processing 

i need to hide a body 
noun, verb, preposition,     

growth of machine learning 
       machine learning is preferred approach to 

       id103, natural language processing 
       id161 
       medical outcomes analysis 
       robot control 
       computational biology 
       sensor networks 
           

       this trend is accelerating 

       big data 
       improved machine learning algorithms  
       faster computers 
       good open-source software 

course roadmap 

       first half of course: supervised learning 

      id166s, kernel methods 
      learning theory 
      id90, boosting, deep learning 

       second half of course: data science 
      unsupervised learning, em algorithm 
      id84 
      topic models 

supervised learning: find f 
       given: training set {(xi, yi)  | i = 1     n} 
       find: a good approximation to  f  : x ! y 
examples: what are x and y ? 
       spam detection 

       map email to {spam, not spam}  

       digit recognition 

       stock prediction 

       map pixels to {0,1,2,3,4,5,6,7,8,9}  

       map new, historic prices, etc. to     (the real numbers) 

   

a supervised learning problem 
dataset: 

       our goal is to find a 

function f  : x ! y 
       x = {0,1}4 
       y = {0,1} 

       question 1: how should 
we pick the hypothesis 
space, the set of possible 
functions f ? 

       question 2: how do we 

find the best f  in the 
hypothesis space? 

most general hypothesis space  
consider all possible boolean functions over four 
input features!  

dataset: 

      216 possible 
hypotheses 
      29 are consistent 
with our dataset 

      how do we 
choose the best 
one? 

a restricted hypothesis space  
consider all conjunctive boolean functions. 

dataset: 

      16 possible 
hypotheses 
      none are 
consistent with our 
dataset 

      how do we 
choose the best 
one? 

occam   s razor principle 

       william of occam: monk living in the 14th century 
       principle of parsimony: 

   one should not increase, beyond what is necessary, the number of 

entities required to explain anything    

       when many solutions are available for a given problem, we should 

select the simplest one 

       but what do we mean by simple? 
       we will use prior knowledge of the problem to solve to define what is 

a simple solution 

example of a prior: smoothness 

[samy bengio] 

key issues in machine learning 
       how do we choose a hypothesis space? 

       often we use prior knowledge to guide this choice 

       how can we gauge the accuracy of a hypothesis on unseen 

data? 
       occam   s razor: use the simplest hypothesis consistent with data! 

this will help us avoid overfitting. 

       learning theory will help us quantify our ability to generalize as 
a function of the amount of training data and the hypothesis space 

       how do we find the best hypothesis? 

       this is an algorithmic question, the main topic of computer 

science 

       how to model applications as machine learning problems? 

(engineering challenge) 

