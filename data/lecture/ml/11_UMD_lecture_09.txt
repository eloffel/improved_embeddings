slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning
lecture 9 imbalanced data and reductions

furong huang / furongh@cs.umd.edu

practical issues: evaluation, beyond 
accuracy
    so far we   ve measured classification 

performance using accuracy

    but this is not a good metric when some errors 

matter mode than others
    given medical record, predict whether patient has 

cancer or not

    given a document collection and a query, find 

documents in collection that are relevant to query

denominator: # of positive predictions

!"#$%&%'(= *+
*++-+
.#$/00= *+
*++-(

denominator: # of positive gold labels

    a combined measure that assesses the 

a combined measure: f

p/r tradeoff is f measure 

'= ()
()++)
,= ()
()++-
$1%+ 1   $ 1(= )!+1%(
1
!=
)!%+(
i.e., with"=1 (that is, %=*+)
    harmonic mean &=+,-,.-

   

    people usually use balanced f-1 measure 

formalizing errors

the learned 

classifier

set of all possible 
classifiers using a fixed 

representation

   

how far is the learned 
classifier f from the 
optimal classifier f*?

quality of the model 

family

aka hypothesis class

the bias/variance trade-off

    trade-off between

   
   

approximation error (bias)
estimation error (variance)

    example:

    consider the always positive classifier

    low variance as a function of a random draw of the 

    strongly biased toward predicting +1 no matter what the 

training set

input

recap: practical issues

   

learning algorithm is only one of many steps in 
designing a ml application

    many things can go wrong, but there are practical 

improving inputs

strategies for
   
    evaluating
    tuning
    debugging

    fundamental ml concepts:  estimation vs. 

approximation error

imbalanced data distributions

    sometimes training examples are drawn 

from an imbalanced distribution

    this results in an imbalanced training set

   needle in a haystack    problems

   
    e.g., find fraudulent transactions in credit card 

histories

    why is this a big problem for the ml 

algorithms we know?

    problem setting

recall: machine learning 
as function approximation

    set of possible instances !
    unknown target function ":!   %
    set of function hypotheses &=        :!   %}
    training examples {+,,., ,    +0,.0 } of unknown target 
function "
    hypothesis       & that best approximates target function "

    output

   

input

recall: id168

!(#,%(&)) where # is the truth and %& is the 
e.g.!#,%(&) =)0 +%#=%(&)
-.   012+30

system   s prediction

1

captures our notion of what is important to learn

recall: expected loss

    ! should make good predictions
    as measured by loss "
    on future examples that are also drawn from #
    $ , the expected loss of ! over # with respect to " should 
$   &',)~+"(-,!(.)) =   (',))#.,-"(-,!(.))

    formally

be small

   

given a good algorithm for solving the binary 
we define cost 
classification problem, how can i solve the   -
of misprediction
weighted binary classification problem?
as:

   > 1 for y = +1 

1 for y = -1

solution: train a binary classifier on 
an induced distribution

subsampling optimality

theorem: if the binary classifier achieves 
a binary error rate of   , then the error rate 
of the   -weighted classifier is      

let   s prove it.

(see also ciml 6.1)

strategies for inducing a new binary 
distribution

    undersample the negative class 

(dominant class)

    oversample the positive class 

(subordinate class)

strategies for inducing a new binary 
distribution

    undersample the negative class

    more computationally efficient

    oversample the positive class

    base binary classifier might do better with more 

training examples

    efficient implementations incorporate weight in 
algorithm, instead of explicitly duplicating data!

reductions

idea is to re-use simple and efficient 
algorithms for binary classification to 
perform more complex tasks

works great in practice: 

e.g., vowpal wabbit

learning with imbalanced data
is an example of reduction

subsampling optimality theorem:
if the binary classifier achieves a binary error rate of   , then 
the error rate of the   -weighted classifier is      

multiclass classification

    real world problems often have 

multiple classes (text, speech, image, 
biological sequences   )

    how can we perform multiclass 

classification?
    straightforward with id90 or knn
    can we use the id88 algorithm?

reductions for multiclass 
classification

how many classes 
can we handle in practice?

   

in most tasks, number of classes k < 
100

    for much larger k

    we need to frame the problem differently
   

e.g, machine translation or automatic speech 
recognition

what you should know

    how can we take the standard binary 

classifier and adapt it to handle 
problems with
   
    multiclass  classification problems

imbalanced data distributions

    algorithms & guarantees on error rate

    fundamental ml concept: reduction

reduction 1: ova

   

   one versus all    (aka    one versus rest   )
    train k-many binary classifiers
   

classifier k predicts whether an example belong 
to class k or not

    at test time, 

if only one classifier predicts positive, predict that class

   
    break ties randomly

time complexity

    suppose you have n training 

examples, in k classes. how long does 
it take to train an ova classifier
   

if the base binary classifier takes o(n) time to 
learn?
if the base binary classifier takes o(n^2) time to 
learn?

   

error bound

    theorem: suppose that the average error 

of the k binary classifiers is   , then the 
error rate of the ova multiclass classifier is 
at most (k-1)   

    to prove this: how do different errors affect 
the maximum ratio of the id203 of 
a multiclass error to the number of 
binary errors (   efficiency   )?

error bound proof

1. if we have a false negative on one of the 
binary classifiers (assuming all other 
classifiers correctly output negative) 
what is the id203 that we will make an 
incorrect multiclass prediction?

(k     1) / k

efficiency: [( k     1) / k] / 1 = (k     1 ) / k

error bound proof

2. if we have m false positives with the 
binary classifiers
what is the id203 that we will make 
an incorrect multiclass prediction?

if there is also a false negative: 1

efficiency =1 / (m + 1)

otherwise  m / ( m + 1)

efficiency = [m / (m + 1)] / m = 1 / ( m + 1)

error bound proof

3. what is the worst case scenario?

false negative case: efficiency is (k-1)/k

larger than false positive efficiencies 

there are k-many opportunities to get false 
negative, overall error bound is (k-1)   

reduction 2:  ava

all versus all (aka all pairs)

how many binary classifiers does this 
require?

time complexity

    suppose you have n training 

examples, in k classes. how long does 
it take to train an ava classifier
   

if the base binary classifier takes o(n) time to 
learn?
if the base binary classifier takes o(n^2) time to 
learn?

   

error bound

theorem: suppose that the average 
error of the k binary classifiers is   , then 
the error rate of the ava multiclass 
classifier is at most 2(k-1)   

question: does this mean that ava is 
always worse than ova?

extensions

divide and conquer

organize classes into binary tree structures

use confidence to weight predictions of 
binary classifiers

instead of using majority vote

topics

given an arbitrary method for binary 
classification, how can we learn to make 
multiclass predictions?

ova, ava

fundamental ml concept:  reductions

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

