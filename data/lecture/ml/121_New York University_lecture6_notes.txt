machine learning lecture 6 note

compiled by abhi ashutosh, daniel chen, and yijun xiao

february 16, 2016

1 pegasos algorithm

the pegasos algorithm looks very similar to the id88 algorithm. in fact,
just by changing a few lines of code in our id88 algorithms, we can get
the pegasos algorithm.

algorithm 1: id88 to pegasos

1 initialize w1 = 0, t = 0;
2 for iter = 1, 2, ..., 20 do
3

for j = 1, 2, ..., |data| do

4

5

6

7

8

9

10

11

t = t + 1;   t = 1
t   ;
if yj(wt

t xj) < 1 then

wt+1 = (1       t  )wt +   tyjxj;
wt+1 = (1       t  )wt;

else

end

end

end

side note: we can optimize both the pegasos and id88 algorithm by
using sparse vectors in the case of document classi   cation because most entries
in the feature vector x will be zeros.

as we discussed in the lecture, the original pegasos algorithm randomly
chooses one data point at each iteration instead of going through each data
point in order as shown in algorithm 1. pegasos algorithm is an application of
the stochastic sub-id119 method.

2 using pegasos to solve other id166 objec-

tives

2.1

imbalanced data set

sometimes it may be hard to classify an imbalanced data set where the classi-
   cation categories are not equally represented. in this case, we want to weigh

1

each data point di   erently by placing more weights on the data points in the
underrepresented categories. we can do this very easily by changing our opti-
mization problem to

(cid:88)

(cid:88)

(cid:107)w(cid:107)2

2 +

min

w

cn
2n+

  j +

cn
2n   

  j

j:yj =   1

j:yj =+1

where n+, n    are the number of positive data points and negative data points
respectively.   j   s are the slack variables.

an intuitive way to think about this is if we want to build a classi   er that
classi   es whether a point is blue or red. if in our data set we only have 1 data
point that   s labelled as red and 10 data points labelled as blue, then using the
modi   ed objective function is equivalent to duplicating the 1 red point 10 times
without explicitly creating more training data.

2.2 id21

suppose we want to build a personalized spam classi   er for professor david
sontag. however, professor david only has few of his emails labelled. professor
rob, on the other hand, has labelled all of the emails he has ever received as
spam or not spam and trained an accurate spam    lter on them. since professor
david and professor rob are both computer science professors and run a lab
together, we hope that they probably share similar standards for spams/non-
spams. in this case, a spam classi   er built for professor rob should work well
to a certain extent for professor david as well. what should the id166 objective
be? (class ideas: average the weight vectors of both professors; combine david
and rob   s data and put more weights on david   s data.)

one solution is to solve the following modi   ed optimization problem,

min
wd,bd

c
|dd|

max(0, 1     y(wt

d x + bd)) +

(cid:107)wd     wr(cid:107)2

1
2

(cid:88)

x,y   dd

the idea here is we assume that the weight for rob is going to be very close
to that for david. we then try to penalize the distance between the two. c
here can be interpreted as how con   dent we are that the weights for rob will
be similar to the weights for david. if we are very con   dent, a low c, then we
will really try to minimize the distance between the two weight vectors. if we
are not con   dent, large c, then we are more focused on david   s labelled data.

2.3 multiclass classi   cation

if we want to extend these ideas further to multi-class classi   cation, we have a
number of options. the simplest is called a one-vs-all classi   er in which we
learn n classi   ers, one for each of the n classes. we could run into issues if we
want to classify a point that fell in between our classi   ers as we would need to

2

decide in which class it belongs. we can predict the most probable class using
the formula

  y = argmax

k

wt

k x + bk

another solution is called multiclass id166. here, we put soft restrictions on

predicting correct labels for the training data using:

w(yj )t xj + b(yj )     w(y(cid:48))t xj + b(y(cid:48)) + 1       j,   y(cid:48) (cid:54)= yj,   j     0,   j

notice that we have one slack variable   j per data point and one set of weights
w(k), b(k) for each class k. we could derive a similar pegasos algorithm for a
multiclass classi   er.

3 kernel trick

what if the data is not linearly separable? we can create a mapping   (x)
that takes our feature vector x and converts it into a higher dimensional space.
creating a linear classi   cation in this higher dimension and projecting that onto
our original feature space will give us a squiggly line classi   er.

kernel tricks allow us to perform the aforementioned classi   cation with little
extra cost. for pegasos algorithm, we can do this by keeping track of just a
single variable per data point,   i, and calculating vector w when required.

(cid:88)

w =

  iyixi

i

let   s now derive the updating rule for such   i   s. notice in algorithm 1, the

update rule at each iteration is

wt+1 = (1       t  )wt + 1[yjwt

t xj < 1]      tyjxj

where 1[condition] is the indicator function. now instead of xj, yj, let us use
x(t), y(t) to denote the data point we randomly selected at iteration t. substitute
  t, we have

(cid:18)

(cid:19)

wt+1 =

1     1
t

wt +

1
  t

1[y(t)wt

t x(t) < 1]    y(t)x(t)

multiplying both sides with t, rearranging,

twt+1     (t     1)wt =

1[y(t)wt

t x(t) < 1]    y(t)x(t)

1
  

as the above equation holds for any t, we have the following t equations

                           

twt+1     (t     1)wt
= 1
(t     1)wt     (t     2)wt   1 = 1
  
      
= 1
  

w2

  

1[y(t)wt
1[y(t   1)wt

t x(t) < 1]    y(t)x(t)

t   1x(t   1) < 1]    y(t   1)x(t   1)
1 x(1) < 1]    y(1)x(1)

1[y(1)wt

3

summing over the above t equations and dividing both sides by t, we can

have

wt+1 =

t(cid:88)

k=1

1
  t

1[y(k)wt

k x(k) < 1]    y(k)x(k)

written in the form of summation over i:

wt+1 =

1[y(k)wt

k x(k) < 1]    1[(xi, yi) = (x(k), y(k))]

       yixi

       1

  t

(cid:88)

i

t(cid:88)

k=1

all the stu    in the huge parenthesis corresponds to   i we de   ned earlier.
counts the number of times data point i appears before iteration t and

k xi < 1. this implies a simple updating rule for   t  (t+1)

i

:

  t  (t+1)
satis   es yiwt

i

  t  (t+1)

i

=   (t     1)  (t)

i + 1[(xi, yi) = (x(t), y(t))]    1[yiwt

t xi < 1]

i.e. suppose we draw data point (xi, yi) at iteration t, we increment   t  i
t xi < 1. the algorithm is shown in algorithm 2. to simplify the

by 1 i    yiwt
notations, we denote   (t)

i =   (t     1)  (t)

i

.

algorithm 2: kernelized pegasos

randomly choose (x(t), y(t)) = (xj, yj) from training data
if yj

i xj < 1 then

i yixt

1

1 initialize   (1) = 0;
2 for t = 1, 2, ..., t do
3

(cid:80)

  (t   1)
  (t+1)
j

i   (t)
j + 1;

=   (t)

else

  (t+1)
j

=   (t)

j

;

end

4

5

6

7

8

9

end

after convergence, we can get back   i   s using   i = 1

  t   (t +1)

i

. in testing

time, predictions can be made with

  y = sign

      (cid:88)

      

  iyixt

i x

i

now suppose we want to use more complex features   (x) which can be ob-
tained by transforming the original features x to a higher dimensional space,
all we need to do is to substitute xt
i xj in both training and testing with
  (xi)t   (xj).

further notice that   (x) always appears in the form of dot products. which
indicates we do not necessarily need to explicitly compute it as long as we have

4

a formula to calculate the dot products. this is where kernels come into use.
instead of de   ning the function    to do the projection, we directly de   ne a
id81 k to calculate the dot product of the projected features.

k(xi, xj) =   (xi)t   (xj)

we can create di   erent id81s k(xi, xj) as long as those functions
are based on dot products. we can also create new valid id81s using
other valid id81s following certain rules. examples of popular kernel
functions include polynomial kernels, gaussian kernels, and many more.

references

shai shalev-shwartz, yoram singer, nathan srebro, andrew cotter. extended
version: pegasos: primal estimated sub-gradient solver for id166. mathemat-
ical programming, series b, 127(1):3-30, 2011

5

