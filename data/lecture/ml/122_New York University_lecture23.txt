dimensionality	reduc1on	

lecture	23	

david	sontag	

new	york	university	

slides adapted from carlos guestrin and luke zettlemoyer 

dimensionality	reduc9on	

       input	data	may	have	thousands	or	millions	of	

dimensions!	
      e.g.,	text	data	has	???,	images	have	???		

       dimensionality	reduc1on:	represent	data	with	

fewer	dimensions	
      easier	learning	   	fewer	parameters	
      visualiza9on	   	show	high	dimensional	data	in	2d	
      discover	   intrinsic	dimensionality   	of	data	

       high	dimensional	data	that	is	truly	lower	dimensional		
       noise	reduc9on	

!"#$%&"'%()$*+,-"'%
(cid:1) .&&+#/-"'%0(*1-1(21//)'3"#1-$456(4"$&('%(

1(4'7$)(*"#$%&"'%14(&/1,$

(cid:1) 831#/4$&0

n = 2 
k = 1 

slide from yi zhang 

n = 3 
k = 2 

9

example	(from	bishop)	

       suppose	we	have	a	dataset	of	digits	(   3   )	

perturbed	in	various	ways:	

       what	opera9ons	did	i	perform?	what	is	the	
       here	the	underlying	manifold	is	nonlinear	

data   s	intrinsic	dimensionality?	

=
=

m   j=1   z
m   j=1   z

q(z | xj) log    p (z | xj,  (t))p (xj |  (t))
q(z | xj) log    p (z | xj,  (t))p (xj |  (t))

q(z | xj)
q(z | xj)

lower	dimensional	projec9ons	

   
   
q(z | xj) log    q(z | xj)
p (z | xj,  (t))   
q(z | xj) log    q(z | xj)
p (z | xj,  (t))   

       obtain	new	feature	vector	by	transforming	the	original	

m   j=1   z
m   j=1   z

features	x1	   	xn	

q(z | xj) log p (xj |  (t))     
m   j=1   z
q(z | xj) log p (xj |  (t))     
m   j=1   z
0 +   i
0 +   i
z1 = w(1)
z1 = w(1)
    
0 +   i
zk = w(k)

w(1)
i xi
w(1)
i xi

w(k)

i xi

in	general	will	not	be	
inver9ble	   	cannot	go	
from	z	back	to	x	

       new	features	are	linear	combina9ons	of	old	ones	
       reduces	dimension	when	k<n	
       this	is	typically	done	in	an	unsupervised	sezng		

       just	x,	but	no	y	

example of this is if each data point represented a grayscale image, and each
took a value in {0, 1, . . . , 255} corresponding to the intensity value of

pixel j in image i.

now, having carried out the id172, how do we compute the    ma-
jor axis of variation    u   that is, the direction on which the data approxi-
mately lies? one way to pose this problem is as    nding the unit vector u so
that when the data is projected onto the direction corresponding to u, the
variance of the projected data is maximized. intuitively, the data starts o   
with some amount of variance/information in it. we would like to choose a
direction u so that if we were to approximate the data as lying in the direc-
tion/subspace corresponding to u, as much as possible of this variance is still
retained.

consider the following dataset, on which we have already carried out the

which	projec9on	is	be[er?	

we see that the projected data still has a fairly large variance, and the
points tend to be far from zero. in contrast, suppose had instead picked the
following direction:

id172 steps:

now, suppose we pick u to correspond the the direction shown in the
   gure below. the circles denote the projections of the original data onto this

here, the projections have a signi   cantly smaller variance, and are much

we see that the projected data still has a fairly large variance, and the
points tend to be far from zero. in contrast, suppose had instead picked the
following direction:

closer to the origin.

we would like to automatically select the direction u corresponding to
the    rst of the two    gures shown above. to formalize this, note that given a

from notes by andrew ng 

reminder:	vector	projec9ons	

       basic	de   ni9ons:	

      a.b	=	|a||b|cos	  	

       assume	|b|=1	(unit	vector)	

      a.b	=	|a|cos	  	
      so,	dot	product	is	length	of	

projec9on!	

using	a	new	basis	for	the	data	
       project	a	point	into	a	(lower	dimensional)	space:	

      point:	x	=	(x1,   ,xn)		
      select	a	basis	   	set	of	unit	(length	1)	basis	vectors	

(u1,   ,uk)	
       we	consider	orthonormal	basis:		

      uj   uj=1,	and	uj   ul=0	for	j   l	

      select	a	center	   	x,	de   nes	o   set	of	space		
      best	coordinates	in	lower	dimensional	space	
i	=	(xi-x)   uj	

de   ned	by	dot-products:	(z1,   ,zk),	zj

maximize	variance	of	projec9on	

unit vector u and a point x, the length of the projection of x onto u is given
by xt u. i.e., if x(i) is a point in our dataset (one of the crosses in the plot),
then its projection onto u (the corresponding circle in the    gure) is distance
xt u from the origin. hence, to maximize the variance of the projections, we
would like to choose a unit-length u so as to maximize:

let x(i) be the ith data point minus the mean. 

choose unit-length u to maximize: 

5

(x(i)t

1
m

m

!i=1

u)2 =

m

1
m

!i=1
= ut " 1

m

ut x(i)x(i)t

u

covariance 
matrix    

x(i)x(i)t# u.

m

!i=1

m$m

we easily recognize that the maximizing this subject to ||u||2 = 1 gives the
i=1 x(i)x(i)t , which is just the empirical
principal eigenvector of    = 1
covariance matrix of the data (assuming it has zero mean).1

let ||u||=1 and maximize. using the method of lagrange 
multipliers, can show that the solution is given by the principal 
eigenvector of the covariance matrix! (shown on board) 

to summarize, we have found that if we wish to    nd a 1-dimensional
subspace with with to approximate the data, we should choose u to be the
principal eigenvector of   . more generally, if we wish to project our data
into a k-dimensional subspace (k < n), we should choose u1, . . . , uk to be the
top k eigenvectors of   . the ui   s now form a new, orthogonal basis for the
data.2

basic	pca	algorithm	

       start	from	m	by	n	data	matrix	x	
       recenter:	subtract	mean	from	each	row	of	x	

[pearson	1901,	
	hotelling,	1933]	

      xc	   	x	   	x	

       compute	covariance	matrix:	

      	  	   	1/m	xc

t	xc	

       find	eigen	vectors	and	values	of	  		
       principal	components:	k	eigen	vectors	with	

highest	eigen	values	

pca	example	

data: 

projection: 

reconstruction: 

dimensionality	reduc9on	with	pca	
!"#$%&"'%()"*+,-$./0*"'%,/&"%1,234

in high-dimensional problem, data usually lies near a linear subspace, as 
noise introduces small variability
only keep data projections onto principal components with large eigenvalues
can ignore the components of lesser significance. 

var(zj) =

(zi

j)2

1
m

mxi=1
mxi=1

=

1
m

=  j

(xi    uj)2

percentage	of	total	variance	captured	
by	dimension	zj	for	j=1	to	10:	

l=1  l

 jpn

)

%

(
 
e
c
n
a
i
r
a
v

25

20

15

10

5

0

pc1 pc2 pc3 pc4 pc5 pc6 pc7 pc8 pc9 pc10

you might lose some information, but if the eigenvalues (cid:3)(cid:10)(cid:5)(cid:1)(cid:11)(cid:7)(cid:3)(cid:6)(cid:6)(cid:2)(cid:1)(cid:14)(cid:9)(cid:13)(cid:1)(cid:4)(cid:9)(cid:8)(cid:15)(cid:12)(cid:1)(cid:6)(cid:9)(cid:11)(cid:5)(cid:1)
slide from aarti singh 
23

much

eigenfaces	[turk,	pentland	   91]	

       input	images:	

!    principal components: 

eigenfaces	reconstruc9on	

       each	image	corresponds	to	adding	together	

(weighted	versions	of)	the	principal	
components:	

scaling	up	

       covariance	matrix	can	be	really	big!	

      	  	is	n	by	n	
      10000	features	can	be	common!		
         nding	eigenvectors	is	very	slow   	

       use	singular	value	decomposi9on	(svd)	

      finds	k	eigenvectors	
      great	implementa9ons	available,	e.g.,	matlab	svd	

       write	x	=	z	s	ut	

svd	

      x	   	data	matrix,	one	row	per	datapoint	
      s	   	singular	value	matrix,	diagonal	matrix	with	
entries	  i	
       rela9onship	between	singular	values	of	x	and	
eigenvalues	of	  	given	by	  i	=	  i

2/m	

      z	   	weight	matrix,	one	row	per	datapoint	
       z	9mes	s	gives	coordinate	of	xi	in	eigenspace		

      ut	   	singular	vector	matrix	

       in	our	sezng,	each	row	is	eigenvector	uj	

pca	using	svd	algorithm	

       start	from	m	by	n	data	matrix	x	
       recenter:	subtract	mean	from	each	row	of	x	

      xc	   	x	   	x	

       call	svd	algorithm	on	xc	   	ask	for	k	singular	
vectors	
       principal	components:	k	singular	vectors	with	

highest	singular	values	(rows	of	ut)	
      coe   cients:	project	each	point	onto	the	new	vectors	

!"#$%&'(%"&)*"+*"$),-.,/*+,'(0-,)*1-".%/,*#"-,*,++%2%,&(*-,1-,),&('(%"&3*'&/*
2'1(0-,*0&/,-45%&6*-,4'(%"&)*(7'(*6".,-&*(7,*/'('

8969** 86"3*1,-)"&'4%(5*'&/*%&(,44%6,&2,*'-,*7%//,&*'((-%$0(,)*(7'(*27'-'2(,-%:,**

70#'&*$,7'.%"-*%&)(,'/*"+*)0-.,5*;0,)(%"&)
<"1%2)*=)1"-()3*)2%,&2,3*&,>)3*,(29?*%&)(,'/*"+*/"20#,&()

non-linear	methods	

@+(,&*#'5*&"(*7'.,*175)%2'4*#,'&%&6

(cid:1)

a%&,'-

/)-%,-0"1&2.30.%$%#&4%"156-6&7/248
b'2("-*c&'45)%)
d&/,1,&/,&(*!"#1"&,&(*c&'45)%)*=d!c?

(cid:1) e"&4%&,'-

!"01",-"% *-9$%3"06
df@gch
a"2'4*a%&,'-*8#$,//%&6*=aa8?

slide from aarti singh 

12

triangles in (a) through
(c); open circles in (d)],
and isomap (   lled cir-
cles) on four data sets
(42). (a) face images
varying in pose and il-
lumination (fig. 1a).
(b) swiss roll data (fig.
3).
(c) hand images
varying in    nger exten-
sion and wrist rotation
(20). (d) handwritten
   2   s (fig. 1b). in all cas-
es, residual variance de-
creases as the dimen-
sionality d is increased.
the intrinsic dimen-
sionality of the data
es9mate	manifold	using	
can be estimated by
goal:	use	geodesic	
looking for the    elbow   
graph.	distance	between	
distance	between	points	
at which this curve ceases to decrease signi   cantly with added dimensions. arrows mark the true or
approximate dimensionality, when known. note the tendency of pca and mds to overestimate the
points	given	by	distance	of	
(with	respect	to	manifold)	
dimensionality, in contrast to isomap.
shortest	path	

isomap	

the natural appearance of linear interpolations
between distant points in the low-dimension-
al coordinate space confirms that isomap has
captured the data   s perceptually relevant
structure (fig. 4).

previous attempts to extend pca and
mds to nonlinear data sets fall
into two
broad classes, each of which suffers from
limitations overcome by our approach. local
linear techniques (21   23) are not designed to
represent the global structure of a data set
within a single coordinate system, as we do in
fig. 1. nonlinear techniques based on greedy
optimization procedures (24   30) attempt to
discover global structure, but lack the crucial
embed	onto	2d	plane	
algorithmic features that
isomap inherits
from pca and mds: a noniterative, polyno-
so	that	euclidean	distance	
mial time procedure with a guarantee of glob-
approximates	graph	
al optimality; for intrinsically euclidean man-
distance	

fig. 3. the    swiss roll    data set, illustrating how isomap exploits geodesic
paths for nonlinear id84. (a) for two arbitrary points
(circled) on a nonlinear manifold, their euclidean distance in the high-
dimensional
input space (length of dashed line) may not accurately
re   ect their intrinsic similarity, as measured by geodesic distance along
the low-dimensional manifold (length of solid curve). (b) the neighbor-
hood graph g constructed in step one of isomap (with k     7 and n    

1000 data points) allows an approximation (red segments) to the true
geodesic path to be computed ef   ciently in step two, as the shortest
path in g. (c) the two-dimensional embedding recovered by isomap in
step three, which best preserves the shortest path distances in the
neighborhood graph (overlaid). straight lines in the embedding (blue)
now represent simpler and cleaner approximations to the true geodesic
paths than do the corresponding graph paths (red).

[tenenbaum, silva, langford. science 2000] 

www.sciencemag.org science vol 290 22 december 2000

2321

isomap	

r e p o r t s

table 1. the isomap algorithm takes as input the distances dx(i, j ) between all pairs i, j from n data points
in the high-dimensional input space x, measured either in the standard euclidean metric (as in fig. 1a)
or in some domain-speci   c metric (as in fig. 1b). the algorithm outputs coordinate vectors yi in a
d-dimensional euclidean space y that (according to eq. 1) best represent the intrinsic geometry of the
data. the only free parameter (    or k ) appears in step 1.

step

1

construct neighborhood graph

2

compute shortest paths

3

construct d-dimensional embedding

de   ne the graph g over all data points by connecting

points i and j if [as measured by dx(i, j )] they are
closer than     (   -isomap), or if i is one of the k
nearest neighbors of j (k-isomap). set edge lengths
equal to dx(i, j).

initialize dg(i, j)     dx(i, j) if i, j are linked by an edge;
dg(i, j)         otherwise. then for each value of k    
1, 2, . . ., n in turn, replace all entries dg(i, j) by
min{dg(i, j), dg(i,k)     dg(k, j)}. the matrix of    nal
values dg     {dg(i, j)} will contain the shortest path
distances between all pairs of points in g (16, 19).
let    p be the p-th eigenvalue (in decreasing order) of

the matrix    (dg) (17), and v p
component of the p-th eigenvector. then set the
p-th component of the d-dimensional coordinate
vector yi equal to       pv p
i .

i be the i-th

ifolds, a guarantee of asymptotic conver-
gence to the true structure; and the ability to
discover manifolds of arbitrary dimensional-
ity, rather than requiring a fixed d initialized

local dimensionality varies across the data set. when
available, additional constraints such as the temporal
ordering of observations may also help to determine
neighbors. in earlier work (36) we explored a more
complex method (37), which required an order of
magnitude more data and did not support the theo-
retical performance guarantees we provide here for
   - and k-isomap.

16. this procedure, known as floyd   s algorithm, requires
o(n3) operations. more ef   cient algorithms exploit-
ing the sparse structure of the neighborhood graph
can be found in (38).

17. the operator     is de   ned by    (d)        hsh/2, where s

is the matrix of squared distances {sij     di j
the    centering matrix    {hij        ij     1/n} (13).

18. our proof works by showing that for a suf   ciently
high density (   ) of data points, we can always choose
a neighborhood size (    or k) large enough that the
graph will (with high id203) have a path not
much longer than the true geodesic, but small
enough to prevent edges that    short circuit    the true
geometry of the manifold. more precisely, given ar-
bitrarily small values of    1,    2, and    , we can guar-
antee that with id203 at least 1        , estimates
of the form

   1        1   dm   i, j        dg   i, j           1        2   dm   i, j   

will hold uniformly over all pairs of data points i, j. for
   -isomap, we require

           2/      r0   24   1,

           log   v/      d      2   /16   d      /   d      2   /8   d

where r0 is the minimal radius of curvature of the
manifold m as embedded in the input space x, s0 is
the minimal branch separation of m in x, v is the

tion, r     0.92). the input-space distances dx(i,j )
given to isomap were euclidean distances be-
tween the 4096-dimensional image vectors. (b)
isomap applied to n     1000 handwritten    2   s
from the mnist database (40). the two most
signi   cant dimensions in the isomap embedding,
shown here, articulate the major features of the
   2   : bottom loop (x axis) and top arch ( y axis).
input-space distances dx(i,j ) were measured by
tangent distance, a metric designed to capture the
invariances relevant in handwriting recognition
(41). here we used    -isomap (with         4.2) be-
cause we did not expect a constant dimensionality
to hold over the whole data set; consistent with
this, isomap    nds several tendrils projecting from
the higher dimensional mass of data and repre-
senting successive exaggerations of an extra
stroke or ornament in the digit.

isomap	

[tenenbaum, silva, langford. science 2000] 
22 december 2000 vol 290 science www.sciencemag.org

shortest paths in a graph with edges connect-
ing neighboring data points.

the complete isometric feature mapping,
or isomap, algorithm has three steps, which
are detailed in table 1. the first step deter-
mines which points are neighbors on the
manifold m, based on the distances dx (i, j)
between pairs of points i, j in the input space

edges of weight dx(i, j) between neighboring
points (fig. 3b).

in its second step, isomap estimates the
geodesic distances dm(i, j) between all pairs
of points on the manifold m by computing
their shortest path distances dg(i, j) in the
graph g. one simple algorithm (16) for find-
ing shortest paths is given in table 1.

geometry (fig. 3c). the coordinate vectors yi
for points in y are chosen to minimize the
cost function

e              dg              dy      l2

(1)
where dy denotes the matrix of euclidean
distances {dy(i, j)        yi     yj   } and    a    l2
the l2 matrix norm       i, j ai j
2 . the     operator

isomap	

fig. 1. (a) a canonical id84
problem from visual perception. the input consists
of a sequence of 4096-dimensional vectors, rep-
resenting the brightness values of 64 pixel by 64
images of a face rendered with different
poses and lighting directions. applied to n     698
raw images, isomap (k     6) learns a three-dimen-
sional embedding of the data   s intrinsic geometric
structure. a two-dimensional projection is shown,
with a sample of the original input images (red
circles) superimposed on all the data points (blue)
and horizontal sliders (under the images) repre-
senting the third dimension. each coordinate axis
of the embedding correlates highly with one de-
gree of freedom underlying the original data: left-
right pose (x axis, r     0.99), up-down pose ( y
axis, r     0.90), and lighting direction (slider posi-
tion, r     0.92). the input-space distances dx(i,j )
given to isomap were euclidean distances be-
tween the 4096-dimensional image vectors. (b)
isomap applied to n     1000 handwritten    2   s
from the mnist database (40). the two most
signi   cant dimensions in the isomap embedding,
shown here, articulate the major features of the
   2   : bottom loop (x axis) and top arch ( y axis).
input-space distances dx(i,j ) were measured by
tangent distance, a metric designed to capture the
invariances relevant in handwriting recognition
(41). here we used    -isomap (with         4.2) be-
cause we did not expect a constant dimensionality
to hold over the whole data set; consistent with
this, isomap    nds several tendrils projecting from
the higher dimensional mass of data and repre-
senting successive exaggerations of an extra
stroke or ornament in the digit.

[tenenbaum, silva, langford. science 2000] 

y is increased. for the swiss roll, where
classical methods fail, the residual variance
of isomap correctly bottoms out at d     2
(fig. 2b).

just as pca and mds are guaranteed,
to recover the true
given sufficient data,
structure of linear manifolds, isomap is guar-
anteed asymptotically to recover the true di-
mensionality and geometric structure of a
strictly larger class of nonlinear manifolds.
like the swiss roll,
these are manifolds

these guarantees of asymptotic conver-
gence rest on a proof that as the number of
data points increases,
the graph distances
dg(i, j) provide increasingly better approxi-
mations to the intrinsic geodesic distances
dm(i, j), becoming arbitrarily accurate in the
limit of infinite data (18, 19). how quickly
dg(i, j) converges to dm(i, j) depends on cer-
tain parameters of the manifold as it
lies
within the high-dimensional space (radius of
curvature and branch separation) and on the

isomap	

face	images	

swiss	roll	data	

pca	

isomap	

residual	
variance	

fig. 2. the residual
variance of pca (open
triangles), mds [open
triangles in (a) through
(c); open circles in (d)],
and isomap (   lled cir-
cles) on four data sets
(42). (a) face images
varying in pose and il-
lumination (fig. 1a).
(b) swiss roll data (fig.
3).
(c) hand images
varying in    nger exten-
sion and wrist rotation
(20). (d) handwritten
   2   s (fig. 1b). in all cas-
es, residual variance de-
creases as the dimen-
sionality d is increased.
the intrinsic dimen-
sionality of the data
can be estimated by
looking for the    elbow   
at which this curve ceases to decrease signi   cantly with added dimensions. arrows mark the true or
approximate dimensionality, when known. note the tendency of pca and mds to overestimate the

number	of	dimensions	

dimensional observations in terms of their
intrinsic nonlinear degrees of freedom. for a
set of synthetic face images, known to have
three degrees of freedom, isomap correctly
detects the dimensionality (fig. 2a) and sep-
arates out the true underlying factors (fig.
1a). the algorithm also recovers the known
low-dimensional structure of a set of noisy
real images, generated by a human hand vary-
ing in finger extension and wrist rotation
(fig. 2c) (20). given a more complex data
set of handwritten digits, which does not have
a clear manifold geometry, isomap still finds
globally meaningful coordinates (fig. 1b)
and nonlinear structure that pca or mds do
not detect (fig. 2d). for all three data sets,
the natural appearance of linear interpolations
between distant points in the low-dimension-
al coordinate space confirms that isomap has
captured the data   s perceptually relevant
structure (fig. 4).

previous attempts to extend pca and

mds to nonlinear data sets fall
broad classes, each of which suffers from
limitations overcome by our approach. local
linear techniques (21   23) are not designed to
represent the global structure of a data set
within a single coordinate system, as we do in
fig. 1. nonlinear techniques based on greedy
optimization procedures (24   30) attempt to
discover global structure, but lack the crucial
algorithmic features that
from pca and mds: a noniterative, polyno-
mial time procedure with a guarantee of glob-

what	you	need	to	know	

       dimensionality	reduc9on	

      why	and	when	it   s	important	
       principal	component	analysis	
      minimizing	reconstruc9on	error	
      rela9onship	to	covariance	matrix	and	eigenvectors	
      using	svd	

       non-linear	dimensionality	reduc9on	

