support vector machines (id166s) 

lecture 4 

david sontag 

new york university 

slides adapted from luke zettlemoyer, vibhav gogate, 

and carlos guestrin 

key idea #1: allow for slack 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

  1 

  3 

  2 

  4 

,    

  j   j 

-   j 

  j   0 

   slack variables    

solve for the optimal value   j
function of w and b: 

* as a 

then   j

*(w,b) =  0 

then   j

*(w,b) =  

if  

if  

* 

equivalent hinge loss formulation 

,    

  j   j 

-   j 

  j   0 

substituting 

into the objective, we get: 

now an unconstrained optimization problem. no longer a linear objective, 
but it is convex. 

key idea #2: seek large margin 

key idea #2: seek large margin 

       consider the constraints: 

       as the norm of the weight vector ||w|| and b get smaller, the 

optimization problem becomes infeasible: 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

as ||w|| (and |b|) 
get smaller 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

what is    (geometric margin) as a function of w? 

- 

we also know that: 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

  

x2 

x1 

so, 

(assuming there is a data point 
 on the w.x + b = +1 or -1 line) 

final result: can maximize      by minimizing ||w||2!!! 

(hard margin) support vector machines 

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

       example of a id76 problem 

       a quadratic program 
       polynomial-time algorithms to solve! 
       hyperplane defined by support vectors 
       could use them as a lower-dimension 
basis to write down line, although we 
haven   t seen how yet 

       more on these later 

support vectors: 
       data points on the 

canonical lines 

margin 2  	

non-support vectors: 
       everything else 
       moving them will 

not change w 

allowing for slack:    soft margin id166    

 
1
+
 
=
 
b
 
+
 
x
.
w

 
0
 
=
 
b
 
+
 
x
.
w

 
1
-
 
=
 
b
 
+
 
x
.
w

  1 

  3 

  2 

  4 

+ c   j   j 

-   j 

  j   0 

   slack variables    

slack penalty c > 0: 
       c=    ! have to separate the data! 
       c=0 !  ignores the data entirely! 
       select using cross-validation 

for each data point: 
      if margin     1, don   t care 
      if margin < 1, pay linear penalty 

equivalent formulation using hinge loss 

+ c   j   j 

-   j 

  j   0 

substituting 

into the objective, we get: 

recall, the hinge loss is 

this is called id173; 
used to prevent overfitting! 

this part is empirical risk minimization, 
using the hinge loss 

