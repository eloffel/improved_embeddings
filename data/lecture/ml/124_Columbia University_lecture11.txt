machine learning for

data science

support vector machines:

introduction

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. history of support vector machines (id166s)

2. basic idea

3. choice of the hyperplane: linearly separable case

4. why is it such a good idea? geometry of id166s

5. choice of the hyperplane: non-linearly separable case

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

history of id166s
    id166s: state-of-the-art classi   cation method.
    boser, guyon and vapnik 1992.
    powerful and widely used in both academia and industry:

1. handles high-dimensional data

2. handles non-linear problems

3. allows overlap in the classes

    a kernel method that depend only on the data through inner

products.

    come with theoretical guaranteed about their performance

(talk by ilia vovsha on march 5).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

basic idea
    find the optimal hyperplane for linearly separable examples.
    for non linearly separable data, transform the original data

using a id81.

    to allow for some overlap in the classes, use slack variables.
    the support vectors are the examples that are the closest to

the decision surface.

    support vectors are the most di   cult to classify.
    output a discrete answer     y = {   1, +1}.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

choice of the hyperplane

given: training data: (x1, y1), . . . , (xn, yn)/xi     rd and yi is dis-
crete yi     y = {   1, +1}.

task: learn a classi   cation function:
f : rd        y

f (x) = sign(

wixi)

d(cid:88)

i=0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

choice of the hyperplane

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

!"!"!"!"!"!"!"!"!"!"!"!"!"!"!"choice of the hyperplane

lots of possible solutions!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

+ + + + + - + + - - - - - - + choice of the hyperplane

lots of possible solutions!

idea of id166:    nd the optimal solution.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

+ + + + + - + + - - - - - - + choice of the hyperplane

the separable case:

    the hyperplane satis   es:

w.x + b = 0.

    w is the normal to the hyperplane.

||w|| is its norm.

    |b|/||w||

is the perpendicular dis-
tance from the hyperplane to the
origin.

    d+ is the shortest distance from the
hyperplane to the closest positive
example. d    is the shortest from
the hyperplane to the closest neg-
ative example.

    margin: d+ + d   

the id166 algorithm looks for separating hyperplane with the largest margin.

the examples on h1 and h2 are called the support vectors (svs) and
have w.x + b = 0 .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| choice of the hyperplane

the separable case:

    h1 and h2 are parallel.
    h1: w.xi + b = +1 with normal
w and perpendicular distance from
the origin |1     b|/||w||.

    h2: w.xi + b =    1 with normal
w and perpendicular distance from
the origin |     1     b|/||w||.

    d+ = d    = 1/||w||
    w.xi + b     +1 if yi = +1
w.xi + b        1 if yi =    1
    these can be combined:

yi(w.xi + b)     1    i = 1, . . . , n

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| de   nition

the maximum margin classi   er is th function that maximizes the
geometric margin 1/||w||, equivalent to minimizing ||w||2.

solve the constrained optimization problem:

argmin

w,b

||w||2

1
2

subject to:

yi(w.xi + b)     1    i = 1, . . . , n

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

why is it such a good idea?

geometry of id166s

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

choice of the hyperplane

the non separable case:
we allow errors but not too much!

argmin

w,b

||w||2 + c

1
2

(cid:88)

i

  i

subject to:

yi(w.xi + b)     1       i       i     0    i = 1, . . . , n
a large c corresponds to assigning a higher penalty to errors.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| + -!i/||w|| credit
    a user   s guide to support vector machines. benhur and we-

ston 2010.

    a tutorial on support vector machines for pattern recogni-
tion. burges, christopher data mining and knowledge dis-
covery 2, no. 2 (june 1998): 121-167.

    andre ng   s lecture notes

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

