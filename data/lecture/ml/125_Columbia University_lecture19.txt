machine learning for

data science

maximum a posteriori and
expectation maximization

faiza khan khattak

coms 4721     spring 2014

outline

1. id113 (id113) revisited

2. maximum a posteriori (map)

3. expectation maximization (em)

4. gaussian mixture models

5. em for gaussian mixture model

6. jensen   s inequaliity

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

1

maximum likelihood

    given: data set of size n drawn from a id203 distribution

with unknown parameters.

    goal: estimate the parameters, which increase the likelihood

of the given dataset.

    solution: id113 (id113)

    m l = arg max

  

l(  )

    m l = arg max

  

p (x|  )

    m l = arg max

  

log p (x|  )

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

2

maximum likelihood

    we can view it in terms of bayesian rule
p (  |x )

     = arg max

  

p (x|  )p (  )

p (x )

     = arg max

  

    p (  |x )     posterior,
    p (x|  )     likelihood,
    p (  )     prior,
    p (x )     evidence.

    a uniform prior is assumed in id113.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

3

maximum likelihood

    we can view it in terms of bayesian rule
p (  |x )

     = arg max

  

p (x|  )p (  )

p (x )

     = arg max

  

    p (  |x )     posterior,
    p (x|  )     likelihood,
    p (  )     prior,
    p (x )     evidence.

    a uniform prior is assumed in id113.
    question: what if we do not want a uniform prior?

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

4

maximum a posteriori

    answer: use the maximum a posteriori (map).
    idea: maximize the posterior instead of maximizing only like-

lihood.

    m ap = arg max

  

p (x|  )p (  )

      i
    the log trick can also be used:

    m ap = arg max

p (xi|  )p (  )

    map = arg max

  

[logp (x|  ) + logp (  )]

    m ap = arg max

  

[l(  ) + logp (  )]

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

5

id113 vs. map by example

    what would be a good way to write a spam    lter?
    we have dataset x and    is the parameter showing spam

emails.

    in general, only a small fraction of the emails is spam but

suppose the dataset we have consists of most spam emails.

    m l = arg max

    if we use id113 we will have
      i
    if we use map we will have
      i

    m ap = arg max

p (xi|  )

p (xi|  )p (  )

assuming i.i.d.

assuming i.i.d.

    which one will give a better estimate id113 or map?

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

6

calculating id113 and map

    analytical method
    a simple method.

    take the derivate w.r.t. parameters and put it equal to

zero.

    constraint: analytical method can not be used in the case

of missing variables.

    missing/hidden/latent variables: the variables that can-
not be observed from data but can be inferred using statistical
techniques.

    in the presence of missing data alternative methods can be

used including expectation maximization (em.)

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

7

expectation maximization

    em (dempster, laird and rubin 1977).
    e   cient iterative procedure to compute the id113 or map.
    em is used:

    in the presence of missing or hidden variables.

    when the likelihood or posterior is intractable and can be
made easier by assuming the existence of hidden variables.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

8

em diagram

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

9

em algorithm

input: x (observed variables), z (hidden variables),    (parameter(s))

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

10

em algorithm

input: x (observed variables), z (hidden variables),    (parameter(s))
1. initialization: initialize the parameters with a guess.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

11

em algorithm

input: x (observed variables), z (hidden variables),    (parameter(s))
1. initialization: initialize the parameters with a guess.

2. e-step: estimate the posterior of the hidden variables i.e.,
p (z|x,   old). calculate expected value of the complete data

q(  ,   old) =   z
or q(  ,   old) =   z

p (z|x,   old)logp (x, z|  old)

p (z|x,   old)logp (x, z|  old)

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

12

em algorithm

input: x (observed variables), z (hidden variables),    (parameter(s))
1. initialization: initialize the parameters with a guess.

2. e-step: estimate the posterior of the hidden variables i.e.,
p (z|x,   old). calculate expected value of the complete data

q(  ,   old) =   z
or q(  ,   old) =   z

p (z|x,   old)logp (x, z|  old)

p (z|x,   old)logp (x, z|  old)

3. m-step: calculated

  new = arg max   q(  ,   old)

(m le)

  new = arg max   [q(  ,   old) + logp (  )] (m ap )

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

13

em algorithm

input: x (observed variables), z (hidden variables),    (parameter(s))
1. initialization: initialize the parameters with a guess.

2. e-step: estimate the posterior of the hidden variables i.e.,
p (z|x,   old). calculate expected value of the complete data

q(  ,   old) =   z
or q(  ,   old) =   z

p (z|x,   old)logp (x, z|  old)

p (z|x,   old)logp (x, z|  old)

3. m-step: calculated

  new = arg max   q(  ,   old)

(m le)

  new = arg max   [q(  ,   old) + logp (  )] (m ap )

4. convergence: if ||  new       old|| <     stop, else   old       new go to 2

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

14

em toy example

    a distribution of balls consisting of big blue, big red and small

balls.

    complete information about data (ideal case)

number of big blue balls=z1, number of big red balls=z2,
number of small balls=z3.

    incomplete information about data (what we have)

number of big blue or red balls = x1 = z1 + z2,
number of small balls = x2 = z3

    goal: given incomplete data x1, x2    nd best parameters gov-

erning the distribution of balls.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

15

em toy example (cont   d)

    initialization: initialize the parameters.
    e-step: estimate the id203 of big blue, big red and small
balls given the observed variables number of big red or blue
balls and small balls parameters   . find the expected value of
the big blue balls, big red balls and small balls.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

16

em toy example (cont   d)

    initialization: initialize the parameters.
    e-step: estimate the id203 of big blue, big red and small
balls given the observed variables number of big red or blue
balls and small balls parameters   . find the expected value of
the big blue balls, big red balls and small balls.

    m-step: update the parameters governing the distribution of

all three types of balls using id113 or map.

    convergence: continue the e-step and m-step with latest pa-

rameters until convergence.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

17

pros and cons of em

    pros

1. simple to implement.

2. can handle missing data.

3. works well when the dimensionality of data is not too large.

4. monotonicity: every new    will not be less likely than its

previous guess.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

18

pros and cons of em

    pros

1. simple to implement.

2. can handle missing data.

3. works well when the dimensionality of data is not too large.

4. monotonicity: every new    will not be less likely than its

previous guess.

    cons

1. may get stuck on local maxima ignoring the global maxima.

2. may require many iterations.

3. can be slow for high dimensional data.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

19

gaussian mixture models

    a id203 p distribution may not be pure.
    p can be a weighted sum of several other distribution, p is

called a mixture distribution.

    gaussian mixture model (gmm) is the weighted sum of k

gaussians

p(x) =

  k n (x|  k,   k)

k   k=1

such that weights   k satisfy the conditions 0       k     1 and
   k   k = 1
    gmm parameters   : weights of gaussian   k, parameters of

gaussians   k,   k.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

20

gaussian mixture models

credit: pattern recognition and machine learning- bishop

three di   erent gaussian in blue and their sum in red.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

21

gaussian mixture models

    we can    nd the set of parameters denoted by    using id113.

     = arg max  p (x|  ) = arg max     i

p (xi|  )

which can be written as,

or

or

     = arg max  log [   i
k   k=1
k   k=1

     = arg max  log [   i
     = arg max     i

[log [

p (xi|  )]

  k n (x|  k,   k)]

  k n (x|  k,   k)]]

    problem: log of sum. di   cult to solve.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

22

gaussian mixture models

    solution: make it simple. assume existence of hidden variable
z     {0, 1} such that    k zk = 1
    variable z informs which gaussian is being used in generation

of a particular datapoint.

p (z) =

  zk
k ,

k   k=1
p (x) =   z [   k

p (x|z) =
k=1   zk

=    p (x) =

k   k=1

k   k=1n (x|  k,   k)zk

k n (x|  k,   k)zk]

log trick

log[  kn (x|  k,   k)]
k   k=1

log[  kn (x|  k,   k)]

     = arg max  

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

23

em for gmm

    initialization: initialize the    with a guess.
    e-step: find p (zk = 1|x,   old).

wnk = p (zk = 1|x,   old) =

wnk = p (zk = 1|x,   old) =

calculate expected value

p (zk = 1)p (x|zk = 1,   old)
   j p (zj = 1)p (x|zj = 1,   old)

  kn (xn|  k,   k)
   j   jn (xn|  j,   j)

log p (x|  ,   ,   ) =   n

log[   k

  k n (xn|  k,   k)]

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

24

em for gmm

    m-step:

  i =

nk
n

where

nk =   n

wnk

  new

k =    n wnkxn
new =    n wnk(xn       new

k
nk

nk

)(xn       new

k

)t

  k
    convergence:

if parameters or likelihood converges stop,

else update parameters and go to e-step.

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

25

jensen   s inequality

    for convex function f:
  ixi))       i

f(   i
f(      ixi))          i(f(xi))

  i(f(xi))

(discrete case)

(continuous case)

=    f(e(x))     e(f(x))

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

26

jensen   s inequality

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

27

jensen   s inequality

    for concave function f:
  ixi))       i

f(   i
f(      ixi))          i(f(xi))

  i(f(xi))

(discrete case)

(continuous case)

=    f(e(x))     e(f(x))

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

28

jensen   s inequality

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

29

credit and further reading

1.    em demysti   ed: an expectation-maximization tutorial    by

yihua chen and maya r. gupta.

2. bishop book
3.    a gentle tutorial of the em algorithm and its application to
parameter estimation for gaussian mixture and hidden markov
models    by je    a. bilmes

copyright c   faiza khan khattak: coms 4721     machine learning for data science.

30

