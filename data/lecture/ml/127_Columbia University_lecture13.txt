machine learning for

data science

support vector machines:

kernels

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review id166 primal and dual forms

2. non-linearity: example

3. from linear models to non-linear models

4. kernels

5. examples of kernels

6. validity of kernels

7. composition of kernels

8. conclusion

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

maximum margin

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

id166s primal and dual forms

separable case:

argmin

w,b

||w||2

1
2

subject to: yi(w.xi + b)     1    i = 1, . . . , n

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

argmax

  

  i  jyiyjxixj

s.t.

  i     0,    i = 1,          , n
n(cid:88)

  iyi = 0

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

id166s dual form
    solve the dual problem to    nd the      s!
    calculate w   s as follows:

w    =

     
i yixi

n(cid:88)

i=1

n(cid:88)

i=1

    how can we make a prediction given an example u (unknown)?

f (x) = sign(

     
i yixiu + b   )

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

soft margin

the non separable case:

argmin

w,b

||w||2 + c

1
2

n(cid:88)

i=1

  i

subject to: yi(w.xi + b)     1      i
n(cid:88)

n(cid:88)

n(cid:88)

argmax

  

  i     1
2

i=1

i=1

j=1

  i     0    i = 1, . . . , n

  i  jyiyjxixj

s.t.

0       i     c,    i = 1,          , n

n(cid:88)

i=1

  iyi = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

non-linear problems

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

!"!"!"!"!"#"#"#"#"#"#"!"#!$#"#$#non-linear problems

  (x) = (x2

1, x2)

f (x) = w.  (x) + b

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

!"!"!"!"!"#"#"#"#"#"#"!"#!$#"#$#!"!"!"!"!"#"#"#"#"#"!"#$!#$#""$#$"%$&$'$non-linear problems

  (x) = (x2
1,

   

2x1x2, x2
2)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

beyond the input space

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

!"#$%&'#()*&+*(%$,*&'#()*&!"#$%&'()"*+"&%,*'-,'.(*'/*%.0+*'1"%2*'-plug in    into the dual?

argmin

w,b

||w||2 + c

1
2

n(cid:88)

i=1

  i

subject to:

yi(w.xi + b)     1       i

  i     0    i = 1, . . . , n

argmax

  

s.t.

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

  i  jyiyjxixj

0       i     c,    i = 1,          , n

n(cid:88)

  iyi = 0

i=1

w    =

n(cid:88)

i=1

     
i yixi

replace all xi by   (xi)!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

problems with plugging   

exponential number of features in the feature space!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

problems with plugging   

exponential number of features in the feature space!

do we have to create and represent all these features?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

problems with plugging   

exponential number of features in the feature space!

do we have to create and represent all these features?

no need to do it explicitly. instead, use kernels!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

problems with plugging   

exponential number of features in the feature space!

do we have to create and represent all these features?

no need to do it explicitly. instead, use kernels!

k(x, x(cid:48)) =   (x).  (x(cid:48))

we can do so because the dual form relies on inner products!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

example

  (x) = (1,

   

2x1,

   

2x2, x2

1, x2
2,

   

2x1x2)

k(x, x(cid:48)) =   (x).  (x(cid:48))
k(x, x(cid:48)) = [x.x(cid:48) + 1]2
given two points xt = (x1, x2) and x(cid:48)t = (x(cid:48)
k(x, x(cid:48)) = [x.x(cid:48) + 1]2
k(x, x(cid:48)) = (x1x(cid:48)
1x2x(cid:48)
k(x, x(cid:48)) = x2
1x(cid:48)
which is the inner product   (x).  (x(cid:48)).

2 + 1)2
2 + 2x1x(cid:48)
2

1 + x2x(cid:48)
2x(cid:48)
2 + x2

2 + 2x1x(cid:48)

1

1, x(cid:48)
2)

1 + 2x2x(cid:48)

2

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

dual with kernel
n(cid:88)

n(cid:88)

n(cid:88)

argmax

  

  i     1
2

i=1

j=1

  i  jyiyjk(xi, xj)

i=1

s.t.

0       i     c,    i = 1,          , n

  iyi = 0

n(cid:88)
n(cid:88)

i=1

i=1

f (x) = sign(

     
i yik(xi, u) + b   )

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

examples of kernels

    linear: k(x, x(cid:48)) = x.x(cid:48)
    polynomial: k(x, x(cid:48)) = [x.x(cid:48) + 1]d
    radial basis function (rbf): exp(     [x     x(cid:48)]2)

kernels can compute inner products e   ciently.

note: in the polynomial kernel, d refers to the degree of
the polynomial, not the number of features.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

example of polynomial kernel

k(x, x(cid:48)) = [x.x(cid:48) + 1]2

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

example of rbf kernel

exp(     [x     x(cid:48)]2)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

validity of kernels

a kernel k(x, x(cid:48)) is a valid kernel i    for all example x1, x2, . . . , xn,
it produces a gram matrix:

1. symmetric: .

2. positive semi-de   nite:

gij = k(xi, xj)

g = gt

  t g       0

     

these are mercer conditions.
form.

it ensures convexity of the dual

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

composition of kernels

given two valid kernels k1 and k2,    > 0, 0            1, f a real-
valued function, a mapping   , k a positive semi-de   nite matrix,
then the following functions are valid kernels:
1. k(x, z) =   k1(x, z) + (1       )k2(x, z)
2. k(x, z) =   k1(x, z)

3. k(x, z) = k1(x, z)k2(x, z)

4. k(x, z) = f (x)f (z)

5. k(x, z) = k3(  (x),   (z))

6. k(x, z) = xt kz

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

conclusion

id166 with kernels:

    independent of the dimensionality of feature space.
    has one global optima.
    can represent any boolean function and reasonably any arbi-

trary smooth decision boundary.

    need to choose a kernel type and its parameters.
    setting the hyper-parameter is crucial but non-trivial.
    in practice, they are usually set using cross validation.
    rbf kernel is a reasonable    rst choice.
    there are very speci   c kernels depending on the applications

(e.g., tree kernels, graph kernels, etc.).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

credit
    a user   s guide to support vector machines. benhur and we-

ston 2010.

    a tutorial on support vector machines for pattern recogni-
tion. burges, christopher data mining and knowledge dis-
covery 2, no. 2 (june 1998): 121-167.

    statistical learning theory. vapnik, 1998.
    check out also    the practical guide to support vector clas-

si   cation    hsu and al. 2010, available online.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

