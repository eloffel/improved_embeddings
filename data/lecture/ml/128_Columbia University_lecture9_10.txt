machine learning for

data science

neural networks

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review: tree classi   ers (your questions)

2. review: id88

3. neural networks

4. the xor example

5. multiclass multilayer networks

6. derivations for the id26 algorithm

7. id26 algorithm

8. id26 example

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

neural networks

    algorithms that try to mimic how the brain functions.
    the    rst algorithm used was the id88 (resenblatt

1959).

    worked extremely well to recognize:

1. handwritten characters (lecun et a. 1989),
2. spoken words (lang et al. 1990),
3. faces (cottrel 1990)

    popular in the 90   s but then lost some of its popularity.
    now back with deep learning!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

id88

(left) credit: neuron cell. (right) machine learning. tom mitchell.

given n examples and d features.
for an example xi (the ith line in the matrix of examples)

f (xi) = sign(

wjxij)

d(cid:88)

j=0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

id88

id88 algorithm

input: a set of examples, (x1, y1),          , (xn, yn)
output: a id88 de   ned by (w0, w1,          , wd)

begin

2. initialize the weights wj to 0    j     {0,          , d}
3. repeat until convergence

4. for each example xi    i     {1,          , n}
5.
6.

if yif (xi)     0

#an error?

update all wj with wj := wj + yixi #adjust the weights

end

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

id88

    the wi determine the contribution of xi to the label.
       w0 is a quantity that (cid:80)n

i=1 wix1 needs to exceed for the per-

ceptron to output 1.

    can be used to represent many boolean functions: and, or,

nand, nor, not but not all of them (xor and xnor).

    neural networks use the ability of the id88s to represent
elementary functions and combine them in a network of layers
of elementary questions.

    but...wait: a cascade of linear functions is still linear!
    and we want networks that represent highly non-linear func-

tions!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

id88

    also, id88 used a threshold function, which is undi   er-
entiable and not suitable for id119 in case data is
not linearly separable.

    we want a function whose output is a linear function of the

inputs.

    sigmoid again! we use the sigmoid function:

g(z) =

ez

1 + ez

=

1

1 + e   z

g(z)     1 when z     +   

g(z)     0 when z           

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

id88 with sigmoid

credit: adapted from machine learning. tom mitchell.

given n examples and d features.
for an example xi (the ith line in the matrix of examples)

1

   (cid:80)d

f (xi) =

1 + e

j=0 wjxij

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

the xor example

let   s try to create a nn for the xor function using elementary
id88s.

x1 x2 x1 xor x2
0
0
1
1

0
1
0
1

0
1
1
0

(x1 or x2) and ( x1 nand x2)

0
1
1
0

1

g(z) =

1 + e   z
g(10) = 0.99995
g(   10) = 0.00004

let   s consider that: for z     10, g(z)    1. for z        10, g(z)     0.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

the xor example

x1 x2 x1 or x2
0
0
1
1

0
1
0
1

0
1
1
1

g(z)
g(w0 + w1x1 + w2x2) = g(   10)
g(10)
g(10)
g(10)

similarly, we obtain the id88s for the and and nand:

note: how the weights in the nand are the inverse weights of the and.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

!"#!#$!#$%#&#'(#!"#$%#$%#$!"#!#$!#$%#&#'()#!"#$%#$%#$!"#!#$!#$%#&#('()#"#$!%#$!%#$the xor example

xor as a combination of 3 basic id88s.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

!"#!#$!#$%#&#'(#!"#$%#$%#$!"#!#&#)*+#!&#$%#$%#$&#*)*+#&#$!%#$!%#$multi class case

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

!"#$%&'('!"#$%&')'!"#$%&'*'how many layers?

theorem (intuition): no more than 2 hidden layers can represent
any arbitrary region (assuming su   cient number of neurons or
units).

the number of neurons in each hidden layer are found by cross-
validation.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

id26 algorithm

    note: feedforward nn (as opposed to recurrent networks)

have no connections that loop.

    id26 stands for    backward propagation of errors   .
    learn the weights for a multilayer network.
    given a network with a    xed architecture (units and intercon-

nections).

    use id119 to minimize the squared error between

the network output value o and the ground truth y.

    we suppose multiple output k.
    challenge: search in all possible weight values for all units in

the network.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

id26 rules

adapted from tom mitchell, machine learning, chapter 4.

    we consider k outputs:
    for an example e de   ned by (x, y), the error on training exam-

ple e, summed over all output units in the network is:

(cid:88)
(yk     ok)2

ee(w) =

1
2

k

    remember, id119 iterates through all the training
examples one at a time, descending the gradient of the error
w.r.t. this example.

   wij =      

   ee(w)

   wij

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

id26 rules

adapted from tom mitchell, machine learning, chapter 4.

notations:
    xij: the ith input to unit j.
    wij: the weight associated with the ith input to unit j.
    zj =(cid:80) wijxj, weighted sum of inputs for unit j.
    oj: output computed by unit j.
    g is the sigmoid function.
    outputs: the set of units in the output layer.
    succ(j): the set of units whose immediate inputs include the

output of unit j.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

id26 rules

adapted from tom mitchell, machine learning, chapter 4.

   ee
   wij

=

   ee
   zj

   zj
   wij

=

   ee
   zj

xij

we consider two cases in calculating    ee
   zj
index e):

(let   s abandon the

    case 1: unit j is an output unit
    case 2: unit j is a hidden unit

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

id26 rules

adapted from tom mitchell, machine learning, chapter 4.

    case 1: unit j is an output unit

   e
   zj

=

   e
   oj

   oj
   zj

   e
   oj

=

   
   oj

1
2

(cid:88)
(yk     ok)2

k

   e
   oj

=

   
   oj

1
2

(yj     oj)2

   e
   oj

=

1
2

2 (yj     oj)

   (yj     oj)

   oj

=    (yj     oj)

   e
   oj

we have: oj = g(zj)

   oj
   zj

=

   g(zj)

   zj

= oj(1     oj)

   oj
   zj

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

id26 rules

adapted from tom mitchell, machine learning, chapter 4.

=    (yj     oj)oj(1     oj)

   e
   zj

   wij = +  (yj     oj)oj(1     oj)xij

we will note

  j =        e
   zj
    case 2: unit j is a hidden unit
= (cid:88)
= (cid:88)

k   succ{j}

   e
   zj

   e
   zj

k   succ{j}

   e
   zk

   zk
   zj

      k

   zk
   zj

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

id26 rules

adapted from tom mitchell, machine learning, chapter 4.

   e
   zj

= (cid:88)
= (cid:88)
= (cid:88)

   e
   zj

k   succ{j}

k   succ{j}

k   succ{j}

      k

   zk
   oj

   oj
   zj

      k wjk

   oj
   zj

   e
   zj

  j =        e
   zj

      k wjk oj (1     oj)

= oj (1     oj) (cid:88)

  k wjk

k   succ{j}

   wij =      j xij

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

id26 algorithm

id26 algorithm
input: a set of training examples (x, y), learning rate    (e.g.,    = 0.1), ni, nh
and no.
output: a neural network with one input layer, one hidden layer and one
output layer with ni, nh and no number of units respectively and all its weights.
    create feedforward network (ni, nh, no)
    initialize all weights to a small random number (e.g., [   0.5, 0.5])
    repeat

for each training example (x, y)

#propagate the input forward through the network:
1 input the example x to the network and compute the output oj from

every unit in the network.

#propagate the errors backward through the network:
2 for each network output unit k, calculate its error term   k.

3 for each network hidden unit h, calculate its error term   h.

  k = ok(1     ok)(yk     ok)

  h = oh(1     oh) (cid:88)

whk  k

k   succ(h)

4 update each weight wij     wij +     jxij

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

id26: example

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

credit
    machine learning 1997. t. mitchell.
    neural networks. 2004. satish kumar.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

