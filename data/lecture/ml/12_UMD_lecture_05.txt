slides adapted from prof. carpuat

cmsc 422 introduction to machine learning

lecture 5 id116 id91 (unsupervised learning)

furong huang / furongh@cs.umd.edu

question

    when applying a learning algorithm, some 

things are properties of the problem you 
are trying to solve, and some things are up 
to you to choose as the ml programmer. 

    which of the following are properties of the 

problem?
   the data generating distribution
   the train/dev/test split
   the learning model
   the id168

recap

    nearest neighbors (nn) algorithms for 

classification
   id92, epsilon ball nn
   take a geometric view of learning

    fundamental machine learning concepts

   decision boundary

v visualizes predictions over entire feature space
v characterizes complexity of learned model
v indicates overfitting/underfitting

exercise: when are dt vs knn
appropriate?
properties of 
classification problem

can id90 
handle them?

can id92 handle 
them?

binary features

numeric features
categorical features
robust to noisy training 
examples
fast classification is 
crucial
many irrelevant features

relevant features have 
very different scale

exercise: when are dt vs knn
appropriate?
properties of 
classification problem

can id90 
handle them?

can id92 handle 
them?

binary features

yes

yes

numeric features
categorical features
robust to noisy training 
examples
fast classification is 
crucial
many irrelevant features

relevant features have 
very different scale

yes
yes
no (for default algorithm) yes (when k > 1)

yes
yes

yes

yes

yes

no

no

no

today   s topics

    a new algorithm
   id116 id91

    fundamental machine learning 

concepts
   unsupervised vs. supervised learning
   decision boundary

id91

    goal: automatically partition examples 

into groups of similar examples

    why? it is useful for

   automatically organizing data
   understanding hidden structure in data
   preprocessing for further analysis

what can we cluster in practice?

    news articles or web pages by topic
    protein sequences by function, or 

genes according to expression profile

    users of social networks by interest
    customers according to purchase 

history

       

id91

   

input

   a set ! of " points in feature space
   a distance measure specifying distance #(%!,%")
between pairs (%!,%")
   a partition {	!#,!$,   ,!%} of !

    output

problem setting

supervised machine learning 
as function approximation

    set of possible instances !
    unknown target function ":!	   &
    set of function hypotheses '=    		   :!	   &}
    training examples {,-,/- ,    ,1,/1 } of unknown 
target function "
    hypothesis    	   '	that best approximates target function "

output

input

supervised 
vs. unsupervised learning

    id91 is an example of 

unsupervised learning

    we are not given examples of classes 

y
instead we have to discover classes in 
data

   

2 datasets with very different 
underlying structure!

the id116 algorithm

training data

k: number of 
clusters to 
discover

id116
1. ask user how many 
clusters they   d like. 
(e.g. k=5) 

id116
1. ask user how many 
clusters they   d like. 
(e.g. k=5) 

2. randomly guess k 

cluster center 
locations

id116
1. ask user how many 
clusters they   d like. 
(e.g. k=5) 

2. randomly guess k 

cluster center 
locations

3. each datapoint finds 
out which center it   s 
closest to. (thus 
each center    owns    
a set of datapoints)

id116
1. ask user how many 
clusters they   d like. 
(e.g. k=5) 

2. randomly guess k 

cluster center 
locations

3. each datapoint finds 
out which center it   s 
closest to.

4. each center finds 
the centroid of the 
points it owns

id116
1. ask user how many 
clusters they   d like. 
(e.g. k=5) 

2. randomly guess k 

cluster center 
locations

3. each datapoint finds 
out which center it   s 
closest to.

4. each center finds 
the centroid of the 
points it owns   

5.    and jumps there
6.    repeat until 

terminated!

id116 properties

    time complexity: o(knl) where

   k is the number of clusters
   n is number of examples
   l is the number of iterations

    k is a hyperparameter

   needs to be set in advance (or learned on dev set)

    different initializations yield different results!

   doesn   t necessarily converge to best partition

   

   global    view of data: revisits all examples at every 
iteration

id116 questions

are we sure it will terminate?
are we sure it will find an optimal 
id91?
how should we start it?
how could we automatically choose the 
number of centers?

   .we   ll deal with these questions over the next few slides

can id116 always win?

impact of initialization

impact of initialization

optimization view of id116

given a set of observations ("!,"",   ,"#) where each 
observation is a &-dimensional real vector
id116 id91 aims to partition the ' observations 
into k(   ") sets s= s!,s",   ,,$ so as to minimize 
argmin% 33 "   5& "=argmin' 3,&	var(,&)
	
where 5& is the mean of points in ,&. 

the within-cluster sum of squares.

formally, the objective is to find:

(
&,!

)   %+

(
&

trying to find good optima

idea 1: be careful about where you start
idea 2: do many runs of id116, each from a 
different random start configuration
many other ideas floating around.

common uses of id116
    often used as an exploratory data analysis tool
in one-dimension, a good way to quantize real-
   
valued variables into k non-uniform buckets

    used on acoustic data in speech understanding to 

convert waveforms into one of k categories 
(known as vector quantization)

    also used for choosing color palettes on old 

fashioned graphical display devices!

questions for you   

    are there clusters that cannot be 

discovered using id116?

    do you know any other id91 

algorithms?

single linkage hierarchical id91

1. say    every point is its 

own cluster   

single linkage hierarchical id91

1. say    every point is its 

own cluster   

2. find    most similar    pair 

of clusters

single linkage hierarchical id91

1. say    every point is its 

own cluster   

2. find    most similar    pair 

of clusters

3. merge it into a parent 

cluster

single linkage hierarchical id91

1. say    every point is its 

own cluster   

2. find    most similar    pair 

of clusters

3. merge it into a parent 

cluster
4. repeat

single linkage hierarchical id91

1. say    every point is its 

own cluster   

2. find    most similar    pair 

of clusters

3. merge it into a parent 

cluster
4. repeat

how do we define similarity 

single linkage hierarchical id91
between clusters?

    minimum distance between 

points in clusters 

    maximum distance between 

points in clusters

    average distance between 

points in clusters 

you   re left with a nice 

dendrogram, or taxonomy, or 
hierarchy of datapoints (not 

shown here)

1. say    every point is its 

own cluster   

2. find    most similar    pair 

of clusters

3. merge it into a parent 

cluster

4. repeat   until you   ve 

merged the whole 
dataset into one cluster

aside: curse of dimensionality

    challenges of working with high 

dimensional spaces
   hard to visualize
   computational cost
   many of our intuitions  about 2d or 3d spaces don   t 

hold
v high dimensional hyperspheres    look more like porcupines 

than balls    

approximately the same

v distances between two random points in high dimensions are 
(ciml section 3.5 + hw #3)

what you should know

    new algorithms
   id92 classification
   id116 id91

    fundamental ml concepts
   how to draw decision boundaries
   what decision boundaries tells us about the 

underlying classifiers

   the difference between supervised and 

unsupervised learning

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

