machine learning for

data science

mining frequent patterns

and association rules

ansaf salleb-aouissi

coms 4721     spring 2014

top ml algorithms

http://www.cs.umd.edu/~samir/498/10algorithms-08.pdf

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

outline

1. introduction
2. a two-step process
3. applications
4. de   nitions and examples
5. frequent patterns: apriori algorithm
6. example
7. representation of d
8. de   nitions cont   d
9. association rules algorithm

10. example
11. a probabilistic framework association rules
12. support-con   dence cons

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

introduction

    unsupervised task.

    r. agrawal, t. imielinski and a.n. swami mining association
rules between sets of items in large databases. proceedings
of sigmod 1993.

    cited by 14,183 papers according to google scholar.

    top 20 in citeseer   s list of most cited cs papers.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

applications

    market basket analysis: cross-selling (ex. amazon), product

placement, a   nity promotion, customer behavior analysis

    collaborative    ltering
    symptoms-diseases associations

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

a two-step process

given a transaction dataset d
1. mining frequent patterns in d

2. generation of strong association rules

example:

{bread, butter} is a frequent pattern (itemset)

bread     butter is a strong rule

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

de   nitions

    item: an object belonging to i = {x1, x2, ..., xm}.
    itemset: any subset of i.
    k-itemset: an itemset of cardinality k.
    we de   ne a total order (i, <) on the items.
    p(i) is a lattice with     =     and     = i.
    transaction: itemset identi   ed by a unique identi   er tid.
    t : the set of all transactions ids. tidset: a subset of t .
    transaction dataset: d = {(tid, xtid) /tid    t , xtid    i}

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

example

item name

a
b
c
d

cafe
milk
butter
bread

d
a b
a c
c d
b c d
a b c d

tid transaction
1
2
3
4
5

i=
t =
d=

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

example

item name

a
b
c
d

cafe
milk
butter
bread

d
a b
a c
c d
b c d
a b c d

tid transaction
1
2
3
4
5

i = {a, b, c, d}
t = {1, 2, 3, 4, 5}
d = {(1, ab), (2, ac), (3, cd), (4, bcd), (5, abcd)}

e.g., {b, c} is a 2-itemset, for writing simpli   cation we will give up
the braces and write bc. {3, 4, 5} is a tidset similarly let   s abandon
the braces here too and write 345.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

example

lattice of itemsets of size . . .

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

example

lattice of itemsets of size 2|i| = 16.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

de   nitions cont   d

    mapping t:

t : p(i)    p (t )
x        t(x) = {tid    t |    xtid, (tid, xtid)    d    x     xtid}

    mapping i:

i : p(t )    p (i)
y        i(y ) = {x    i|    (tid, xtid)    d , tid     y     x     xtid}

question: what are the properties of t and i?

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

properties of t and i

let x, x1, x2    i be 3 itemsets
let y, y1, y2    t be 3 tidsets

    x1     x2     . . .
    y1     y2     . . .
    y     t(x)     . . .
    t(x1     x2) = . . .

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

properties of t and i

let x, x1, x2    i be 3 itemsets
let y, y1, y2    t be 3 tidsets

    x1     x2     t(x2)     t(x1)
    y1     y2     i(y1)     i(y2)
    y     t(x)     x     i(y )
    t(x1     x2) = t(x1)     t(x2)

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

de   nitions cont   d

    frequency: f req(x) = |{(tid, xtid)    d /x     xtid}| = |t(x)|
    support: supp(x) = |t(x)|
|d|
    frequent itemset: x is frequent i    supp(x)     minsupp
    property (support downward closure) :
frequent then all its subsets also are frequent.

if an itemset is

    mining frequent itemsets:

f = { x    i| supp(x)    m insupp}

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

example

minsupp=40%

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

bfs and dfs

a : 1

a : 2

b : 1

chapitre 2. extraction des motifs fr  equents et r`egles d   association

d : 1

c : 4

null
4
3
index
   lms
drame
horreur
policier
com  edies
science    ction
et fantastiques
      
musicales
dramatiques

   

a

b

c

d

ab

ac

ad

bc

bd

cd

abc

abd

acd

bcd

abcd

breadth first search

depth first search

fig. 2.4     ordre de rymon

propri  et  e 2.1 (propri  et  e d   antimonotonicit  e). tout sous-ensemble d   un itemset fr  equent
est un itemset fr  equent.

preuve

soit x un itemset fr  equent. supposons qu   il existe un itemset peu fr  equent x         x.
x        x        x           x tel que x = x        x      
supp(x) = |t(x)|
<    (car x    n   est pas
|d|
|d|
fr  equent), ce qui aboutit `a une contradiction, car x est fr  equent par hypoth`ese.

= |t(x   )   t(x      )|

< |t(x   )|
|d|

= |t(x      x      )|

|d|

intuitivement, cela veut dire que si un itemset a un support inf  erieur au support
minimum aucun de ses sur ensembles n   aura un support sup  erieur ou   egal au support
minimum.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

la d  ecouverte de tous les itemsets fr  equents s   e   ectue par un parcours du treillis des
itemsets. plusieurs sortes de parcours sont possibles, dont les plus communs sont le par-
cours en largeur d   abord3 et le parcours en profondeur d   abord4.

16

dans le parcours en largeur, le treillis des itemsets est parcouru niveau par niveau,

apriori pseudo-algorithm

level-wise algorithm     lattice explored with a breath first search
approach (bfs). start at level 1 in the lattice: k = 1

    generate candidates of size k

ck = {(ck, supp(ck))|   x     ck, x    =    , support(x)     m insupp}

    scan the dataset to compute the support of each candidate

and keep the frequent ones

fk = {(lk, supp(lk))| supp(lk)     m insupp}

    go the the next level k = k + 1 and redo the process.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

apriori algorithm
input:transaction dataset d ; minimum support
output: frequent itemsets f
1. f1 = { frequent 1-itemsets}
2. for (k = 2;fk   1    =    ; k++)
ck =apriori-gen(fk   1)
3.
for each transaction t    d

4.

ct =sub-sets(ck, t) //ct = {c     ck, c     t}
for each candidate c     ct

supp(c)++

5.

6.

7.

8.

fk = {c     ck/supp(c)     m insupp}

9. return f =   k fk

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

apriori-gen

function apriori-gen(fk   1)
1. ck =    
2. for each itemset p    f k   1
3.

for each itemset q    f k   1

4.

5.

6.

7.

if (p[1] = q[1]     . . .     p[k     2] = q[k     2]     p[k     1] < q[k     1])

c = p        q

if has-non-frequent-subset(c,fk   1)=false
then add c to ck

8. return ck

question: why condition 4?

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

example

minsupp=2/5 (40%)

tid
1
2
3
4
5

d

transaction

a b
a c
c d
b c d
a b c d

itemset support

itemset support

f1

itemset

c1
a
b
c
d

itemset

c2
ab
ac
ad
bc
bd
cd

itemset

c3
abc
bcd

scan            of d

a
b
c
d

scan            of d

scan            of d

ab
ac
ad
bc
bd
cd

abc
bcd

c1

c2

3/5
3/5
4/5
3/5

2/5
2/5
1/5
2/5
2/5
3/5

1/5
2/5

   

   

   

a
b
c
d

ab
ac
bc
bd
cd

3/5
3/5
4/5
3/5

2/5
2/5
2/5
2/5
3/5

itemset support

c3

f3

itemset support

bcd

2/5

itemset support

itemset support

f2

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

example

minsupp=2/5 (40%)

tid
1
2
3
4
5

d

transaction

a b
a c
c d
b c d
a b c d

itemset support

itemset support

f1

itemset

c1
a
b
c
d

itemset

c2
ab
ac
ad
bc
bd
cd

itemset

c3
abc
bcd

scan            of d

a
b
c
d

scan            of d

scan            of d

ab
ac
ad
bc
bd
cd

abc
bcd

c1

c2

3/5
3/5
4/5
3/5

2/5
2/5
1/5
2/5
2/5
3/5

1/5
2/5

   

   

   

a
b
c
d

ab
ac
bc
bd
cd

3/5
3/5
4/5
3/5

2/5
2/5
2/5
2/5
3/5

itemset support

c3

f3

itemset support

bcd

2/5

itemset support

itemset support

f2

question: the more we increase minsup the...

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

apriori bottleneck

characteristics of real-life datasets:

1. billions of transactions,

2. tens of thousands of items,

3. tera-bytes of data.

this leads to:

1. multiple scans of the dataset residing in the disk (costly i/o

operations)

2. a huge number of candidates sets.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

representation of d

b

c

row-wise
1 a
2 a
3 c
4 b
5 a

d

c

b

d

column-wise
a
1
2
5

b
1
4
5

c
2
3
4
5

d

3
4
5

c

d

b

a

boolean
c

d
1 1 1 0 0
2 1 0 1 0
3 0 0 1 1
4 0 1 1 1
5 1 1 1 1

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

de   nitions cont   d

    given f and a minimum con   dence threshold minconf
    generate rules:

(l     c)     c

conf((l     c)     c) =

supp(l)

supp(l     c)     m inconf

    from a k     itemset (k > 1), one can generate 2k     1 rules.
property

let l be a large (frequent) itemset:

   c     l, c    =    , [(l    c)     c] is strong           c     c,   c    =    , [(l      c)       c] is strong

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

association rules algorithm
input: frequent itemsets f ; minconf
output: association rules r
1. r =    
2. for each k-itemset lk    f , k     2 do
3.

h1 = {1-itemsets frequents subsets of lk}
for each h1     h1 do
supp(lk   h1)

conf= supp(lk)
if conf     m inconf then

r : (lk     h1)     h1
r = r    r

else delete h1 of h1 //property

4.

5.

6.

7.

8.

9.

genrules(lk, h1)

10.
11. return r

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

association rules algorithm

function genrules(lk, hm)
1. if (k > m + 1) then

2.

3.

4.

5.

6.

7.

8.

9.

hm+1 =apriori-gen(hm)
for all hm+1     hm+1 do

supp(lk   hm+1)

conf= supp(lk)
if conf     m inconf then
r : (lk     hm+1)     hm+1
r = r    r
else delete hm+1 de hm+1 //property

genrules(lk, hm+1)

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

example

minconf=60%

ab

ac

con   dence

itemset rule# rule
2/3 = 66.66%
a     b
2/3 = 66.66%
b     a
2/3 = 66.66%
a     c
2/4 = 50.00%
c     a
2/3 = 66.66%
b     c
2/4 = 50.00%
c     b
2/3 = 66.66%
b     d
2/3 = 66.66%
d     b
3/4 = 75.00%
c     d
d     c 3/3 = 100.00%

1
2
3
4
5
6
7
8
9
10

cd

bd

bc

strong?

yes
yes
yes
no
yes
no
yes
yes
yes
yes

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

example

minconf=60%

itemset rule# rule
2/3 = 66.66%
cd     b
bd     c 2/2 = 100.00%
bc     d 2/2 = 100.00%

con   dence

11
12
13

bcd

itemset rule# rule

bcd

14
15
16

con   dence
d     bc 2/3 = 66.66%
c     bd 2/4 = 50.00%
b     cd 2/3 = 66.66%

strong?

yes
yes
yes

strong?

yes
no
yes

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

probabilistic interpretation

brin et al. 97

r : a        c

    r measures the distribution of a and c in the    nite space d.
    the sets a and c are 2 events
    p (a) and p (c) the probabilities that events a and c happen

resp. estimated by the the frequency of a and c resp. in d

supp(a     c) = supp(a     c) = p (a     c)

conf(a     c) = p (c|a) =

p (a     c)

p (a)

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

29

support-con   dence: cons

    example (brin et al. 97)

tea
tea

    columns

caf e
20
70
90

caf e     lines

25
75
100

5
5
10

tea     caf e

(supp = 20%, conf = 80%)

strong rule?

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

30

support-con   dence: cons

    example (brin et al. 97)

tea
tea

    columns

caf e
20
70
90

caf e     lines

25
75
100

5
5
10

tea     caf e

(supp = 20%, conf = 80%)

strong rule? yes but a misleading one!

support(caf e) = 90% is a bias that the con   dence cannot detect
because it ignores support(cafe).

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

31

other evaluation measures

    interest (piatetsky-shapiro 91) or lift (bayardo et al. 99)

interest(a     c) =

p (a     c)

p (a)    p (c)

=

supp(a     c)

supp(a)    supp(c)

interest is between 0 and +   :
1. if interest(r) = 1 then a and c are independent;
2. if interest(r) > 1 then a and c are positively dependent;
3. if interest(r) < 1 then a and c are negatively dependent.

interest(a     c) =

conf(a     c)

supp(c)

=

conf(c     a)

supp(a)

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

32

other evaluation measures

caf e
20
70
90

tea
tea

    columns

caf e     lines

25
75
100

5
5
10

interest(tea     caf e) =

p (tea     caf e)

p (tea)    p (caf e)

=

0.2

0.25     0.9

= 0.89 < 1

caf e

caf e
tea 0.89
tea 1.03 0.66

2

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

33

multi-dimensional rules

    one-dimensional rules:

buy(x,    bread      )        buy(x,    butter      )

    multi-dimensional rules:

buy(x,    p izza      )     age(x,    y oung      )        buy(x,    coke      )

    construct k-predicatesets instead of k-itemsets

    how about numerical features?

buy(x,    p izza      )     age(x,    18     22      )        buy(x,    coke      )

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

34

references

1994.

helsinki institute for information technology, 2003.

tween sets of items in large databases   . sigmod 1993.

    r. agrawal, t. imielinski and a.n. swami    mining association rules be-
    r. agrawal, r. srikant   fast algorithms for mining association rules     vldb
    b. goethals    survey on frequent pattern mining    technical report,
    s. brin et al.    beyond market baskets: generalizing association rules to
    r. agrawal et al.    mining association rules with item constraints   . kdd
    a. salleb et al.    quantminer: a genetic algorithm for mining quantitative
    u. m. fayyad, g. piatetsky-shapiro, and p. smyth.    from data mining to
knowledge discovery: an overview   . in advances in knowledge discovery
and data mining, 1996.

association rules   , ijcai 2007.

correlations   . sigmod 1997.

1997.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

35

