machine learning for

data science

id116, gmms and em

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review: id116

2. review id113

3. em id91

4. naive bayes classi   er with unobserved labels

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

unsupervised learning

data segmentation or id91
input:    examples    with unobserved labels.
x1, . . . , xn, xi     x     rn

output: f : x        {c1, . . . ck} (set of clusters).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

!"#!$#unsupervised learning

data segmentation or id91
input:    examples    with unobserved labels.
x1, . . . , xn, xi     x     rn

output: f : x        {c1, . . . ck} (set of clusters).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

!"#!$#id91: id116

    goal: assign each example (x1, . . . , xn) to one of the k clusters

{c1, . . . ck}.

      j is the mean of all examples in the jth cluster.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

id91: id116

    goal: assign each example (x1, . . . , xn) to one of the k clusters

{c1, . . . ck}.

      j is the mean of all examples in the jth cluster.
    minimize:

k(cid:88)

(cid:88)

j=1

xi   cj

j =

||xi       j||2

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

id91: id116

algorithm id116:

initialize randomly   1,            k.

repeat

assign each point xi to the cluster with the closest   j.
calculate the new mean for each cluster as follows:

  j =

1
|cj|

until convergence   .

(cid:88)

xi

xi   cj

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

id91: id116

algorithm id116:

initialize randomly   1,            k.

repeat

assign each point xi to the cluster with the closest   j.
calculate the new mean for each cluster as follows:

  j =

1
|cj|

until convergence   .

(cid:88)

xi

xi   cj

   convergence: means no change in the clusters or maximum
number of iterations reached.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

id116: applet

http://home.deib.polimi.it/matteucc/id91/tutorial_html/appletkm.html

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

id116 & em

algorithm id116:

initialize randomly   1,            k.

repeat

//assign each point xi to the cluster with the closest   j.
expectation:

freeze the   j, minimize j

until convergence.

note: freeze the points:

means keep the belonging of points

to clusters fixed.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

id116 & em

algorithm id116:

initialize randomly   1,            k.

repeat

//assign each point xi to the cluster with the closest   j.
expectation:

freeze the   j, minimize j

//calculate the new mean for each cluster as follows:
maximization: freeze the points, minimize j

  j =

1
|cj|

until convergence.

(cid:88)

xi

xi   cj

note: freeze the points:

means keep the belonging of points

to clusters fixed.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

em in general

given:

    x: set of observed data,
    z: set of unobserved latent data,
      : set of unknown parameters,
    l(  ; x, z) = p(x, z|  ), a likelihood function.

in general, em is an iterative procedure for    nding the id113.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

em in general

given:

    x: set of observed data,
    z: set of unobserved latent data,
      : set of unknown parameters,
    l(  ; x, z) = p(x, z|  ), a likelihood function.

in general, em is an iterative procedure for    nding the id113.

map the framework to id116.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

em in general

both    and z are unknown.

1. initialize the parameters    to some random values.

2. compute the best values of z given these parameter values.

3. use the just-computed values of z to find better estimates

for   .

4. go to 2. until convergence.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

digression: id113
given a set of potential distributions: {p(x|  ) |        rd}
given a dataset {x1,          , xn}
find the distribution that best explains or    t the data.

find the best parameter     .

maximum likelihood approach to    nd the best distribution:
maximum likelihood assumes that data is best explained by the
distribution under which it has the highest id203.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

digression: id113
given a set of potential distributions: {p(x|  ) |        rd}
given a dataset {x1,          , xn}
find the distribution that best explains or    t the data.

find the best parameter     .

maximum likelihood approach to    nd the best distribution:
maximum likelihood assumes that data is best explained by the
distribution under which it has the highest id203.

maximum likelihood estimator (id113) is:

    m le = argmax   p(x1,          , xn|  )

the parameter which maximizes the joint density of the data.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

digression: id113

we assume that the training examples (sequence of variables or
features) are independent, and identically distributed (i.i.d. us-
ing the machine learning jargon):
1. if each feature (random variable) has the same id203 dis-

tribution as the others and

2. if all features are mutually independent

    m le = argmax  

p(xi|  )

n(cid:89)

i=1

id113 equation:

n(cid:89)

i=1

     (

p(xi|  ) = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

digression: logarithm trick

we have:

log((cid:89)

fi) =(cid:88)

i

i

log(fi)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

digression: logarithm trick

we have:

log((cid:89)

fi) =(cid:88)

i

i

    m le = argmax  

log(fi)

n(cid:89)

i=1

p(xi|  )

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

digression: logarithm trick

we have:

log((cid:89)

i

fi) =(cid:88)

log(fi)

    m le = argmax  

p(xi|  )

    m le = argmax  

log p(xi|  )

i

n(cid:89)
n(cid:88)

i=1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

digression: logarithm trick

we have:

log((cid:89)

i

fi) =(cid:88)

log(fi)

    m le = argmax  

p(xi|  )

    m le = argmax  

      log p(xi|  ) =

n(cid:88)

i=1

log p(xi|  )

      p(xi|  )
p(xi|  )

= 0

i=1

i

n(cid:89)
n(cid:88)
n(cid:88)

i=1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

gaussian distributions

p(x|  ,   ) = n (x;   ,   ) =

density of the multivariate gaus-
sian (normal) distribution.

(cid:0)x       )t      1(x       )(cid:1)

1(cid:112)(2  )d|  |exp(   1

2

    ln n (x1,          , xn|  ,   )

= 0

    

n(cid:88)

i=1

1
n

    m le =

xi

n(cid:88)

i=1

    m le =

1
n

(xi         m le)(xi         m le)t

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

gmm assumption

assumes a linear combination of gaussians:

k(cid:88)

i=1

f (x) =

wi n (x;   i,   i),

where

k(cid:88)

i=1

wi = 1

we denote a mixture of gaussians by g.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

em for gmm

given:
    training set {x1,          , xn}.
    number of clusters k.

model the data using a mixture of gaussians:
    weights {w1,          , wk}.
    means and covariances:   1,          ,   k,   1,          ,   k.
k(cid:88)

k(cid:88)

wi n (x;   i,   i),

f (x) =

i=1

where

wi = 1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

gaussian distributions

p(x|  ,   ) = n (x;   ,   ) =

density of the multivariate gaus-
sian (normal) distribution.

(cid:0)x       )t      1(x       )(cid:1)

1(cid:112)(2  )d|  |exp(   1

2

    ln n (x1,          , xn|  ,   )

= 0

    

n(cid:88)

i=1

1
n

    m le =

xi

n(cid:88)

i=1

    m le =

1
n

(xi         m le)(xi         m le)t

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

gmm assumption

assumes a linear combination of gaussians:

k(cid:88)

i=1

f (x) =

wi n (x;   i,   i),

where

k(cid:88)

i=1

wi = 1

we denote a mixture of gaussians by g.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

em for gmm

given:
    training set {x1,          , xn}.
    number of clusters k.

model the data using a mixture of gaussians:
    weights {w1,          , wk}.
    means and covariances:   1,          ,   k,   1,          ,   k.
k(cid:88)

k(cid:88)

wi n (x;   i,   i),

f (x) =

i=1

where

wi = 1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

em for gmm

likelihood of the whole training set:

n(cid:89)
p(x1,          , xn|g) =
n(cid:89)
k(cid:88)

i=1

p(xi|g)

p(x1,          , xn|g) =

wjn (xi|  j,   j)

write the em for id91.

i=1

j=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

em for gmm

http://www.cs.cmu.edu/~alad/em/

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

nb with unobserved labels

from text classi   cation from labeled and unlabeled document using em.

nigam et al. 2000.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

29

nb with unobserved labels
    newsgroup postings 20 newsgroups, 1,000/group
    web page classi   cation student, faculty, course, project 4,199

web pages

    reuters newswire articles 12,902 articles 90 topics categories

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

30

nb with unobserved labels

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

31

nb with unobserved labels

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

32

credit and further reading

1. greg hamerly and charles elkan. learning the k in id116.

in in neural information processing systems, 2003.

2. david arthur and sergei vassilvitskii. id116 ++ : the ad-
vantages of careful seeding. in proceedings of the eighteenth
annual acm-siam symposium on discrete algorithms, volume
8, pages 10271035, 2007.

3. c.f. wu. on the convergence properties of the em algorithm.

the annals of statistics, 11:95103, 1983.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

33

