machine learning for

data science

support vector machines:

primal, dual forms and

soft-margin

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. maximum margin

2. choice of the hyperplane

3. id166 primal form

4. lagrange duality

5. id166 dual form

6. id166 with a soft-margin

7. a hint of kernels (more in the next lecture)

8. id166s in practice

9. demo

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

choice of the hyperplane

lots of possible solutions!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

+ + + + + - + + - - - - - - + choice of the hyperplane

lots of possible solutions!

idea of id166:    nd the    best    margin   .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

+ + + + + - + + - - - - - - + maximum margin: intuition

why is a fat margin the best?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

maximum margin: intuition

why is a fat margin the best?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

choice of the hyperplane
given: training data: (x1, y1), . . . , (xn, yn)/xi     rd and yi is dis-
crete yi     y = {   1, +1}.
task: learn a classi   cation function: f : rd        y

f (x) = sign(

wixi)

f (x) = sign(

wixi + b)

d(cid:88)
d(cid:88)

i=0

i=1

f (x) = sign(w.x + b)

(with    .    is the dot product)

note: b corresponds to the intercept   0 in the methods we have seen before.
we use w and b as these are the most commonly used for id166s. it will help in
case you read id166s literature.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

choice of the hyperplane

the separable case:

    the hyperplane satis   es:

w.x + b = 0.

    w is the normal to the hyperplane.

||w|| is its norm.

    |b|/||w||

is the perpendicular dis-
tance from the hyperplane to the
origin.

    d+ is the shortest distance from the
hyperplane to the closest positive
example. d    is the shortest from
the hyperplane to the closest neg-
ative example.

    margin: d+ + d   

the id166 algorithm looks for separating hyperplane with the largest margin.

the examples on h1 and h2 are called the support vectors (svs) and
have w.x + b = 0 .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| choice of the hyperplane

the separable case:

    h1 and h2 are parallel.
    h1: w.xi + b = +1 with normal
w and perpendicular distance from
the origin |1     b|/||w||.

    h2: w.xi + b =    1 with normal
w and perpendicular distance from
the origin |     1     b|/||w||.

    d+ = d    = 1/||w||
    w.xi + b     +1 if yi = +1
w.xi + b        1 if yi =    1
    these can be combined:

yi(w.xi + b)     1    i = 1, . . . , n

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| choice of the hyperplane
why is d+ = d    = 1||w||?

the distance from a point (x0, y0) to a line with equation ax + by + c = 0 is:

(cid:112)

|ax0 + by0 + c|

a2 + b2

distance(ax + by + c = 0, (x0, y0)) =

(cid:113)

if we reason in 2d without loss of gener-
ality, we have vector w = (w1, w2). we
have a = w1, b = w2, c = b.

w2

1 + w2

2 = ||w||

that is norm of vector w

distance d+ from any positive point x+
in h1 to the hyperplane h.
|w.x+ + b|

d+ =

||w||

x+ being on h1 veri   es the equation
w.x+ + b = 1.
hence: d+ = 1||w||

one could do a similar calculation for a point x    on h2 to get d   .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

id166s primal form

the maximum margin classi   er is the function that maximizes the
geometric margin 1/||w||, equivalent to minimizing ||w||2.

solve the constrained optimization problem:

argmin

w,b

||w||2

1
2

subject to:

yi(w.xi + b)     1    i = 1, . . . , n

inequality constraint

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

lagrange duality

solving constrained optimization problems:

                     

argmin

w

f (w)

s.t.

hi(w) = 0    i = 1, . . . , n

can be solved with lagrange multipliers.

lagrangian:

l(w,   ) = f (w) +

n(cid:88)

i=1

  ihi(w)

the   s are called lagrange multipliers.
   l
     i

   l
   wi

= 0

= 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

lagrange duality

argmin

w

f (w)

                                 

s.t.
s.t.

gi(w)     0    i = 1, . . . , k
hi(w) = 0    i = 1, . . . , l
l(cid:88)

k(cid:88)

  igi(w) +

i=1

i=1

generalized lagrangian:

l(w,   ,   ) = f (w) +

  ihi(w)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

lagrange duality

argmin

w

f (w)

                                 

s.t.
s.t.

generalized lagrangian:

gi(w)     0    i = 1, . . . , k
hi(w) = 0    i = 1, . . . , l
l(cid:88)
i=1
solution w    in the primal,       and       in the dual.
for a solution to exist (and hence the primal and id78 are
equivalent), the karush-kuhn-tucker kkt conditions must be
ful   lled.

l(w,   ,   ) = f (w) +

  igi(w) +

  ihi(w)

k(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

lagrange duality

karush-kuhn-tucker kkt conditions.

1.

   
   wi

l(w   ,      ,      ) = 0,    i = 1, . . . , n
l(w   ,      ,      ) = 0,    i = 1, . . . , l
i gi(w   ) = 0, ,    i = 1, . . . , k

   
2.
     i
3.      
4. gi(w   )     0,    i = 1, . . . , k
5.           0,    i = 1, . . . , k

for more details: t. rockarfeller (1970) convex analysis, princeton university press.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

id166s dual form

||w||2     n(cid:88)

1
2

i=1

l(w, b,   ) =

   l
   w

= w     n(cid:88)
=     n(cid:88)

i=1

   l
   b

i=1

  i[yi(w.xi + b)     1]

n(cid:88)

i=1

  iyixi

  iyi = 0

  iyixi = 0     w =

  iyi = 0     n(cid:88)

i=1

l(w, b,   ) =

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

  i  jyiyjxixj

by plugging in these 2 quantities back into l:

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

id166s dual form
n(cid:88)

n(cid:88)

n(cid:88)

argmax

  

n  i     1
2

i=1

j=1

  i  jyiyjxixj

i=1

s.t.

  i     0,    i = 1,          , n
n(cid:88)

  iyi = 0

i=1

solve the dual problem to    nd the      s!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

id166s dual form

few observations:

    total dependence on the dot product.
    the dual form depends only on the inputs.
    once we    nd the      s, we can    nd the optimal w   s.

n(cid:88)

i=1

w    =

     
i yixi

    we can    nd the optimal b, that is b    but reconsidering the

primal form.
teaser: calculate b    using w   .

    except for support vectors, all      s will be 0 (from kkt).
    how can we make a prediction given an example u (unknown)?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

id166s dual form

few observations:

    total dependence on the dot product.
    the dual form depends only on the inputs.
    once we    nd the      s, we can    nd the optimal w   s.

n(cid:88)

i=1

w    =

     
i yixi

    we can    nd the optimal b, that is b    but reconsidering the

primal form.
teaser: calculate b    using w   .

    except for support vectors, all      s will be 0 (from kkt).
    how can we make a prediction given an example u (unknown)?

n(cid:88)

i=1

     
i yixiu + b   

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

soft margin

the non separable case: we allow errors but not too much!

argmin

w,b

||w||2 + c

1
2

n(cid:88)

i=1

  i

subject to:

yi(w.xi + b)     1       i

  i     0    i = 1, . . . , n

a large c corresponds to assigning a higher penalty to errors.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

+ + + + + - + + - - - - - - + h2 h1 h d+ d- w -b/||w|| + -!i/||w|| soft margin: dual form

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

  i  jyiyjxixj

0       i     c,    i = 1,          , n

argmax

  

s.t.

n(cid:88)

i=1

  iyi = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

a hint of kernels

example:

f (x) = w.  (x) + b

  (x) = (x2
1,

   

2x1x2, x2
2)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

id166 in practice

    id172 is important.
    do model selection by searching the parameters.
    id166 with unbalanced datasets:
(cid:88)

||w||2 + c    (cid:88)
1
  i + c+
2
yk[w(cid:62)xk + b]     1       k,    k,

yi=   1

w,  

s.t.

argmin

yj=+1

  j

    rbf (gaussian kernel) is an e   ective and mostly used kernel.

c+ n+ = c    n   

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

id166 in practice

    free implementations: libid166, id166light.
    http://www.kernel-machines.org/
    demo:

http://www.ml.inf.ethz.ch/education/lectures_and_seminars/

annex_estat/classifier/jsupportvectorapplet.html

    more on kernels next time.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

credit
    a user   s guide to support vector machines. benhur and we-

ston 2010.

    a tutorial on support vector machines for pattern recogni-
tion. burges, christopher data mining and knowledge dis-
covery 2, no. 2 (june 1998): 121-167.

    andre ng   s lecture notes.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

