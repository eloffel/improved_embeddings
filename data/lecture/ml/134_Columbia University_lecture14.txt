machine learning for

data science

ensemble methods:

boosting, id112 and

id79s

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review

2. majority voting

3. boosting

4. id112

5. id79s

6. conclusion

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

majority voting

    a randomly chosen hyperplane has an expected error of 0.5.
    many random hyperplanes combined by majority vote will sill

be random.

    suppose we have m classi   ers, performing slightly better than

random, that is error = 0.5- .

    combine: make a decision bases on majority vote?
    what if we combined these m slightly-better-than-random

classi   ers? would majority vote be a good choice?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

condorcet   s jury theorem

marquis de condorcet appli-
cation of analysis to the prob-
ability of majority decisions.
1785.

assumptions:

1. each individual makes the right choice with a id203 p.

2. the votes are independent.

if p > 0.5, then adding more voters increases the id203 that the majority
decision is correct. if p < 0.5, then adding more voters makes things worse.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

ensemble methods

    an ensemble method combines the predictions of many in-

dividual classi   ers by majority voting.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

ensemble methods

    an ensemble method combines the predictions of many in-

dividual classi   ers by majority voting.

    such individual classi   ers, called weak learners, are required

to perform slightly better than random.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

ensemble methods

    an ensemble method combines the predictions of many in-

dividual classi   ers by majority voting.

    such individual classi   ers, called weak learners, are required

to perform slightly better than random.

how do we produce independent weak learners

using the same training data?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

ensemble methods

    an ensemble method combines the predictions of many in-

dividual classi   ers by majority voting.

    such individual classi   ers, called weak learners, are required

to perform slightly better than random.

how do we produce independent weak learners

using the same training data?

    use a strategy to obtain relatively independent weak learners!
    di   erent methods:

1. boosting
2. id112
3. id79s

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

boosting
    first ensemble method.
    one of the most powerful machine learning methods.
    popular algorithm: adaboost.m1 (freund and shapire 1997).
       best of-the-shelf classi   er in the world    breiman (cart   s

inventor), 1998.
    simple algorithm.
    weak learners can be trees, id88s, decision stumps, etc.
    idea:

train the weak learners on weighted training examples.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

boosting

top algorithms:

http://www.cs.umd.edu/~samir/498/10algorithms-08.pdf

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

boosting

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

boosting

      m is the contribution of each weak learner gm.
    the predictions from all of the gm, m     {1,          , m} are com-

bined with a weighted majority voting.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

boosting

the error rate on the training sample:

(cid:80)n
i=1 1{yi (cid:54)= g(xi)}

n

err :=

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

boosting

the error rate on the training sample:

(cid:80)n
i=1 1{yi (cid:54)= g(xi)}

n

err :=

the error rate on each weak learner:

(cid:80)n
i=1 wi 1{yi (cid:54)= gm(xi)}

(cid:80)n

i=1 wi

errm :=

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

boosting

the error rate on the training sample:

(cid:80)n
i=1 1{yi (cid:54)= g(xi)}

n

err :=

the error rate on each weak learner:

(cid:80)n
i=1 wi 1{yi (cid:54)= gm(xi)}

(cid:80)n

i=1 wi

errm :=

intuition:
- give large weights for hard examples.
- give small weights for easy examples.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

boosting

for each weak learner m, we associate an error errm.

1   errm
errm

  m = log(1   errm

errm

)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

adaboost

1. initialize the example weights wi = 1

n, i = 1,          , n.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

adaboost

1. initialize the example weights wi = 1
2. for m = 1 to m (number of weak learners)

n, i = 1,          , n.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

adaboost

1. initialize the example weights wi = 1
2. for m = 1 to m (number of weak learners)

n, i = 1,          , n.

(a) fit a classi   er gm(x) to training data using the weights wi.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

adaboost

1. initialize the example weights wi = 1
2. for m = 1 to m (number of weak learners)

n, i = 1,          , n.

(a) fit a classi   er gm(x) to training data using the weights wi.
(b) compute

(cid:80)n
i=1 wi 1{yi (cid:54)= gm(xi)}

(cid:80)n

i=1 wi

errm :=

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

adaboost

1. initialize the example weights wi = 1
2. for m = 1 to m (number of weak learners)

n, i = 1,          , n.

(a) fit a classi   er gm(x) to training data using the weights wi.
(b) compute

(cid:80)n
i=1 wi 1{yi (cid:54)= gm(xi)}

(cid:80)n

i=1 wi

errm :=

(c) compute

  m = log(

1     errm

errm

)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

adaboost

1. initialize the example weights wi = 1
2. for m = 1 to m (number of weak learners)

n, i = 1,          , n.

(a) fit a classi   er gm(x) to training data using the weights wi.
(b) compute

(cid:80)n
i=1 wi 1{yi (cid:54)= gm(xi)}

(cid:80)n

i=1 wi

errm :=

(c) compute

  m = log(
(d) wi     wi.exp[  m.1(yi (cid:54)= gm(xi))]

1     errm

)

errm

f or i = 1,          , n.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

adaboost

1. initialize the example weights wi = 1
2. for m = 1 to m (number of weak learners)

n, i = 1,          , n.

(a) fit a classi   er gm(x) to training data using the weights wi.
(b) compute

(cid:80)n
i=1 wi 1{yi (cid:54)= gm(xi)}

(cid:80)n

i=1 wi

errm :=

(c) compute

  m = log(
(d) wi     wi.exp[  m.1(yi (cid:54)= gm(xi))]
m(cid:88)
3. output

g(x) = sign[

  mgm(x)]

1     errm

)

errm

f or i = 1,          , n.

m=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

digression: decision stumps

this is an example of very weak classi   er

a simple 2-terminal node decision tree for binary classi   cation.

f (xi|j, t) =

where j     {1,          d}.

(cid:40) +1

   1

if xij > t
otherwise

example: a dataset with 10 features, 2,000 examples training
and 10,000 testing.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

adaboost performance

error rates:
- random: 50%.
- stump: 45.8%.
- large classi   cation tree: 24.7%.
- adaboost with stumps: 5.8% after 400 iterations!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

34010.boostingandadditivetrees01002003004000.00.10.20.30.40.5boosting iterationstest errorsingle stump244 node treefigure10.2.simulateddata(10.2):testerrorrateforboostingwithstumps,asafunctionofthenumberofiterations.alsoshownarethetesterrorrateforasinglestump,anda244-nodeclassi   cationtree.randomguessing.however,asboostingiterationsproceedtheerrorratesteadilydecreases,reaching5.8%after400iterations.thus,boostingthissimpleveryweakclassi   erreducesitspredictionerrorratebyalmostafactoroffour.italsooutperformsasinglelargeclassi   cationtree(errorrate24.7%).sinceitsintroduction,muchhasbeenwrittentoexplainthesuccessofadaboostinproducingaccurateclassi   ers.mostofthisworkhascenteredonusingclassi   cationtreesasthe   baselearner   g(x),whereimprovementsareoftenmostdramatic.infact,breiman(nipsworkshop,1996)referredtoadaboostwithtreesasthe   besto   -the-shelfclassi   erintheworld   (seealsobreiman(1998)).thisisespeciallythecasefordata-miningapplications,asdiscussedmorefullyinsection10.7laterinthischapter.10.1.1outlineofthischapterhereisanoutlineofthedevelopmentsinthischapter:   weshowthatadaboost   tsanadditivemodelinabaselearner,optimizinganovelexponentiallossfunction.thislossfunctionisadaboost performance

adaboost with decision stumps lead to a form of:

feature selection

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

34010.boostingandadditivetrees01002003004000.00.10.20.30.40.5boosting iterationstest errorsingle stump244 node treefigure10.2.simulateddata(10.2):testerrorrateforboostingwithstumps,asafunctionofthenumberofiterations.alsoshownarethetesterrorrateforasinglestump,anda244-nodeclassi   cationtree.randomguessing.however,asboostingiterationsproceedtheerrorratesteadilydecreases,reaching5.8%after400iterations.thus,boostingthissimpleveryweakclassi   erreducesitspredictionerrorratebyalmostafactoroffour.italsooutperformsasinglelargeclassi   cationtree(errorrate24.7%).sinceitsintroduction,muchhasbeenwrittentoexplainthesuccessofadaboostinproducingaccurateclassi   ers.mostofthisworkhascenteredonusingclassi   cationtreesasthe   baselearner   g(x),whereimprovementsareoftenmostdramatic.infact,breiman(nipsworkshop,1996)referredtoadaboostwithtreesasthe   besto   -the-shelfclassi   erintheworld   (seealsobreiman(1998)).thisisespeciallythecasefordata-miningapplications,asdiscussedmorefullyinsection10.7laterinthischapter.10.1.1outlineofthischapterhereisanoutlineofthedevelopmentsinthischapter:   weshowthatadaboost   tsanadditivemodelinabaselearner,optimizinganovelexponentiallossfunction.thislossfunctionisadaboost-decision stumps

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

35410.boostingandadditivetrees!$hpremovefreecapaveyourcapmaxgeorgecaptoteduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d020406080100relative importancefigure10.6.predictorvariableimportancespectrumforthespamdata.thevariablenamesarewrittenontheverticalaxis.adaboost

adaboost applet

http://cseweb.ucsd.edu/~yfreund/adaboost/

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

id112 & id64
    bootstrap is a re-sampling technique     sampling from the em-

pirical distribution.

    aims to improve the quality of estimators.
    id112 and boosting are based on boostrapping.
    both use re-sampling to generate weak learners for classi   ca-

tion.

    strategy: randomly distort data by re-sampling.
    train weak learners on re-sampled training sets.
    bootstrap aggregation     id112.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

id112

training
for b = 1,          , b
1. draw a bootstrap sample bb of size (cid:96) from training data.
2. train a classi   er fb on bb.

classi   cation: classify by majority vote among the b trees:

favg :=

1
b

fb(x)

b(cid:88)

b=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

29

id112

id112 works well for trees:

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

30

2848.modelid136andaveraging|x.1 < 0.39501010110original tree|x.1 < 0.55501001b = 1|x.2 < 0.205010101b = 2|x.2 < 0.28511010b = 3|x.3 < 0.985010111b = 4|x.4 < !1.3601101010b = 5|x.1 < 0.39511001b = 6|x.1 < 0.39501011b = 7|x.3 < 0.985010010b = 8|x.1 < 0.395010110b = 9|x.1 < 0.55510101b = 10|x.1 < 0.5550101b = 11figure8.9.id112treesonsimulateddataset.thetopleftpanelshowstheoriginaltree.eleventreesgrownonbootstrapsamplesareshown.foreachtree,thetopsplitisannotated.id79s

1. boosting typically outperforms id112. why?

2. id79s: modi   es id112 with trees to reduce corre-

lation between trees.

3. tree training optimizes each split over all dimensions.

4. but for id79s, choose a di   erent subset of di-

mensions at each split. number of dimensions chosen m.

5. optimal split is chosen within the subset.

6. the subset is chosen at random out of all dimensions 1, . . . , d.

7. recommended m =

d or smaller.

   

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

31

conclusion

1. summarize what we have seen so far.
2. caruana   s table

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

32

anempiricalcomparisonofsupervisedlearningalgorithmstable3.normalizedscoresofeachlearningalgorithmbyproblem(averagedovereightmetrics)modelcalcovtadultltr.p1ltr.p2medisslachsmgcalhouscodbactmeanbst-dtplt.938.857.959.976.700.869.933.855.974.915.878*.896*rfplt.876.930.897.941.810.907*.884.883.937.903*.847.892bag-dt   .878.944*.883.911.762.898*.856.898.948.856.926.887*bst-dtiso.922*.865.901*.969.692*.878.927.845.965.912*.861.885*rf   .876.946*.883.922.785.912*.871.891*.941.874.824.884bag-dtplt.873.931.877.920.752.885.863.884.944.865.912*.882rfiso.865.934.851.935.767*.920.877.876.933.897*.821.880bag-dtiso.867.933.840.915.749.897.856.884.940.859.907*.877id166plt.765.886.936.962.733.866.913*.816.897.900*.807.862ann   .764.884.913.901.791*.881.932*.859.923.667.882.854id166iso.758.882.899.954.693*.878.907.827.897.900*.778.852annplt.766.872.898.894.775.871.929*.846.919.665.871.846anniso.767.882.821.891.785*.895.926*.841.915.672.862.842bst-dt   .874.842.875.913.523.807.860.785.933.835.858.828knnplt.819.785.920.937.626.777.803.844.827.774.855.815knn   .807.780.912.936.598.800.801.853.827.748.852.810knniso.814.784.879.935.633.791.794.832.824.777.833.809bst-stmpplt.644.949.767.688.723.806.800.862.923.622.915*.791id166   .696.819.731.860.600.859.788.776.833.864.763.781bst-stmpiso.639.941.700.681.711.807.793.862.912.632.902*.780bst-stmp   .605.865.540.615.624.779.683.799.817.581.906*.710dtiso.671.869.729.760.424.777.622.815.832.415.884.709dt   .652.872.723.763.449.769.609.829.831.389.899*.708dtplt.661.863.734.756.416.779.607.822.826.407.890*.706lr   .625.886.195.448.777*.852.675.849.838.647.905*.700lriso.616.881.229.440.763*.834.659.827.833.636.889*.692lrplt.610.870.185.446.738.835.667.823.832.633.895.685nbiso.574.904.674.557.709.724.205.687.758.633.770.654nbplt.572.892.648.561.694.732.213.690.755.632.756.650nb   .552.843.534.556.011.714-.654.655.759.636.688.481thanboostingstumpsonlyonsevenproblems.occa-sionallyboostedstumpsperformverywell,butsome-timestheyperformverypoorlysotheiraverageper-formanceislow.onadult,whenboostingtrees,the   rstiterationofboostinghurtstheperformanceofalltreetypes,andneverrecoversinsubsequentrounds.whenthishappensevensingledecisiontreesoutper-formtheirboostedcounterparts.baggedtreesandrandomforests,however,consistentlyoutperformsin-gletreesonallproblems.id112andrandomforestsare   safer   thanboosting,evenonthemetricsforwhichboostingyieldsthebestoverallperformance.5.bootstrapanalysistheresultsdependonthechoiceofproblemsandmet-rics.whatimpactmightselectingotherproblems,orevaluatingperformanceonothermetrics,haveontheresults?forexample,neuralnetsperformwellonallmetricson10of11problems,butperformpoorlyoncod.ifwehadn   tincludedthecodproblem,neuralnetswouldmoveup1-2placesintherankings.tohelpevaluatetheimpactofthechoiceofproblemsandmetricsweperformedabootstrapanalysis.werandomlyselectabootstrapsample(samplingwithreplacement)fromtheoriginal11problems.forthissampleofproblemswethenrandomlyselectaboot-strapsampleof8metricsfromtheoriginal8metrics(againsamplingwithreplacement).forthisbootstrapsampleofproblemsandmetricswerankthetenalgo-rithmsbymeanperformanceacrossthesampledprob-lemsandmetrics(andthe5folds).thisbootstrapsamplingisrepeated1000times,yielding1000poten-tiallydi   erentrankingsofthelearningmethods.table4showsthefrequencythateachmethodranks1st,2nd,3rd,etc.the0.228entryforboostedtreesinthecolumnfor2ndplace,tellsusthatthereisa22.8%chancethatboosteddecisiontreeswouldhaveplaced2ndinthetableofresults(insteadof1st)hadweselectedotherproblemsand/ormetrics.thebootstrapanalysiscomplementsthet-testsinta-bles2and3.theresultssuggestthatifwehadse-lectedotherproblems/metrics,thereisa58%chancethatboosteddecisiontreeswouldstillhaveranked1stcredit
    an empirical comparison of supervised learning algorithms
icml

rich caruana caruana and alexandru niculescu-mizil
2006.

    isl book.
    peter orbanz    lecture notes.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

33

