machine learning for

data science

maximum likelihood and

gaussian models

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. bayes rule.

2. generative models.

3. maximum likelihood.

4. gaussian distributions.

5. gaussian id113.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

bayes rule

p(a|b) =

p(a     b)

p(b)

p(a     b) = p(a|b)     p(b)

two events are independent, if:

p(a     b) = p(a)     p(b)

p(c     a     b) = p(c|a     b)     p(a|b)     p(b)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

bayes rule

writing p(a     b) in two di   erent ways:

p(a     b) = p(b|a)     p(a)
p(a     b) = p(a|b)     p(b)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

bayes rule

writing p(a     b) in two di   erent ways:

p(a     b) = p(b|a)     p(a)
p(a     b) = p(a|b)     p(b)

p(a|b) =

p(b|a)     p(a)

p(b)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

bayes rule

writing p(a     b) in two di   erent ways:

p(a     b) = p(b|a)     p(a)
p(a     b) = p(a|b)     p(b)

p(a|b) =

p(b|a)     p(a)

p(b)

p(a|b) is called posterior (posterior distribution on a given b.)
p(a) is called prior.
p(b) is called evidence.
p(b|a) is called likelihood.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

bayes rule

other forms:

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

bayes rule

other forms:

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

p(a|b     x) =

p(b|a     x)     p(a     x)

p(b     x)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008

p(  a) = 0.992

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008
p(b|a) = 0.98

p(  a) = 0.992
p(  b|a) = 0.02

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008
p(b|a) = 0.98
p(b|  a) = 0.03

p(  a) = 0.992
p(  b|a) = 0.02
p(  b|  a) = 0.97

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

example of using bayes rule

p(a|b) =

p(b|a)     p(a)

p(b|a)     p(a) + p(b|  a)     p(  a)

a: patient has cancer.
b: patient has a positive lab test.

p(a) = 0.008
p(b|a) = 0.98
p(b|  a) = 0.03

p(  a) = 0.992
p(  b|a) = 0.02
p(  b|  a) = 0.97

p(a|b) =

0.98     0.008

0.98     0.008 + 00.3     0.992

= 0.21

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

why probabilities?

why are we bringing here a bayesian framework?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

why probabilities?

why are we bringing here a bayesian framework?

recall classi   cation framework:
given: training data: (x1, y1), . . . , (xn, yn)/xi     rd and yi     y.
task: learn a classi   cation function: f : rd        y

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

why probabilities?

why are we bringing here a bayesian framework?

recall classi   cation framework:
given: training data: (x1, y1), . . . , (xn, yn)/xi     rd and yi     y.
task: learn a classi   cation function: f : rd        y

learn a mapping from x to y.
we would like to    nd this mapping f (x) = y through p(y|x)!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

discriminative algorithms

    discriminative algorithms:

    idea: model p(y|x;   ), conditional distribution of y given x.
    e.g., we modeled in id28 p(y|x;   ) as g(  t x)

where g is the sigmoid function.

    in discriminative algorithms:    nd a decision boundary that

separates positive from negative example.

    to predict a new example, check on which side of the de-

cision boundary it falls.
    model p(y|x) directly!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

generative algorithms

    generative algorithms adopt a di   erent approach:

    idea: build a model for what positive examples look like.
build a di   erent model for what negative example look like.

    to predict a new example, match it with each of the models

and see which match is best.

    model p(x|y) and p(y)!
    use bayes rule to obtain p(y|x) = p(x|y)p(y)
    to make a prediction:

p(x)

.

argmaxyp(y|x) = argmaxy
argmaxyp(y|x)     argmaxyp(x|y)p(y)

p(x)

p(x|y)p(y)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

maximum likelihood
given a set of potential distributions: {p(x|  ) |        rd}
given a dataset {x1,          , xn}
find the distribution that best explains or    t the data.

find the best parameter     .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

maximum likelihood
given a set of potential distributions: {p(x|  ) |        rd}
given a dataset {x1,          , xn}
find the distribution that best explains or    t the data.

find the best parameter     .

maximum likelihood approach to    nd the best distribution:
maximum likelihood assumes that data is best explained by the
distribution under which it has the highest id203.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

maximum likelihood
given a set of potential distributions: {p(x|  ) |        rd}
given a dataset {x1,          , xn}
find the distribution that best explains or    t the data.

find the best parameter     .

maximum likelihood approach to    nd the best distribution:
maximum likelihood assumes that data is best explained by the
distribution under which it has the highest id203.

maximum likelihood estimator (id113) is:

    m le = argmax   p(x1,          , xn|  )

the parameter which maximizes the joint density of the data.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

maximum likelihood

we assume that the training examples (sequence of variables or
features) are independent, and identically distributed (i.i.d. us-
ing the machine learning jargon):
1. if each feature (random variable) has the same id203 dis-

tribution as the others and

2. if all features are mutually independent

    m le = argmax  

p(xi|  )

n(cid:89)

i=1

id113 equation:

n(cid:89)

i=1

     (

p(xi|  ) = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

logarithm trick
fi) =(cid:88)

log((cid:89)

we have:

log(fi)

i

i

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

logarithm trick
fi) =(cid:88)

log((cid:89)

we have:

i

i

    m le = argmax  

log(fi)

n(cid:89)

i=1

p(xi|  )

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

logarithm trick
fi) =(cid:88)

log((cid:89)

we have:

log(fi)

i

    m le = argmax  

p(xi|  )

    m le = argmax  

log p(xi|  )

i

n(cid:89)
n(cid:88)

i=1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

logarithm trick
fi) =(cid:88)

log((cid:89)

we have:

log(fi)

i

    m le = argmax  

p(xi|  )

    m le = argmax  

      log p(xi|  ) =

n(cid:88)

i=1

log p(xi|  )

      p(xi|  )
p(xi|  )

= 0

i=1

i

n(cid:89)
n(cid:88)
n(cid:88)

i=1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

logarithm trick
fi) =(cid:88)

log((cid:89)

we have:

log(fi)

i

    m le = argmax  

p(xi|  )

    m le = argmax  

      log p(xi|  ) =

n(cid:88)

i=1

log p(xi|  )

      p(xi|  )
p(xi|  )

= 0

i=1

i

n(cid:89)
n(cid:88)
n(cid:88)

i=1

i=1

the solution will depend on the model!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

gaussian distributions

density of the univariate gaussian
(normal) distribution.

p(x|  ,   ) =

1   
2    

exp(   (x       )2
2  2

)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

gaussian distributions

density of the multivariate gaus-
sian (normal) distribution.

p(x|  ,   ) =

1(cid:113)
(2  )d|  |exp(   1

2

(cid:68)

(x       ),      1(x       )

(cid:69)

)

  : the mean vector
  : covariance matrix     0 positive semi de   nite
|  |: determinant of   .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

example: gaussian id113
the set of possible distributions is gaussian densities on rd with
   xed   :

{p(x;   ,   ) |        rd}

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

29

example: gaussian id113
the set of possible distributions is gaussian densities on rd with
   xed   :

we solve the id113 equation:

{p(x;   ,   ) |        rd}
n(cid:88)

      log p(xi|  ,   ) = 0

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

30

example: gaussian id113
the set of possible distributions is gaussian densities on rd with
   xed   :

we solve the id113 equation:

{p(x;   ,   ) |        rd}
n(cid:88)

      log p(xi|  ,   ) = 0

n(cid:88)

i=1

      log

i=1

1(cid:113)
(2  )d|  |exp(   1

2

(cid:68)
(x       ),      1(x       )

(cid:69)

) = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

31

example: gaussian id113
(cid:69)
(cid:68)
n(cid:88)
(x       ),      1(x       )

1(cid:113)
(2  )d|  |) + log(exp(   1

      log(

2

i=1

)) = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

32

example: gaussian id113
(cid:69)
(cid:68)
n(cid:88)
(x       ),      1(x       )

      log(

)) = 0

1(cid:113)
(2  )d|  |) + log(exp(   1
n(cid:88)

(cid:68)

2

      (   1
2

i=1

(x       ),      1(x       )

(cid:69)

) = 0

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

33

example: gaussian id113
(cid:69)
(cid:68)
n(cid:88)
(x       ),      1(x       )

      log(

)) = 0

i=1

2

1(cid:113)
(2  )d|  |) + log(exp(   1
n(cid:88)

(cid:68)
(x       ),      1(x       )
      (   1
2
    n(cid:88)

     1(xi       ) = 0

i=1

i=1

(cid:69)

) = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

34

example: gaussian id113
(cid:69)
(cid:68)
n(cid:88)
(x       ),      1(x       )

      log(

)) = 0

i=1

2

1(cid:113)
(2  )d|  |) + log(exp(   1
n(cid:88)

(cid:68)
(x       ),      1(x       )
      (   1
2
    n(cid:88)

     1(xi       ) = 0

i=1

(cid:69)

) = 0

the id113 of the gaussian expectation for a    xed variance is:

n(cid:88)

i=1

1
n

xi

i=1

    n(cid:88)

i=1

(xi       ) = 0        =
n(cid:88)

    m le =

xi

1
n

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

35

example: gaussian id113
the set of possible distributions is gaussian densities on rd with
unknown covariance   :

{p(x;   ,   ) |        rd,           d}

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

36

example: gaussian id113
the set of possible distributions is gaussian densities on rd with
unknown covariance   :

{p(x;   ,   ) |        rd,           d}
n(cid:88)

      log p(xi|    m le,   ) = 0

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

37

example: gaussian id113
the set of possible distributions is gaussian densities on rd with
unknown covariance   :

{p(x;   ,   ) |        rd,           d}
n(cid:88)

      log p(xi|    m le,   ) = 0

i=1

the id113 of the gaussian covariance is:

    m le =

1
n

(xi         m le)(xi         m le)t

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

38

credit
    machine learning. tom mitchell 1997.
    esl book.
    peter orbanz    lecture notes with permission.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

39

