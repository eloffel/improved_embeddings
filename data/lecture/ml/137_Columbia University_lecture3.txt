machine learning for

data science

basic concepts ii

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. training and testing

2. over   tting and under   tting

3. structural risk minimization

4. avoiding over   tting

5. occam   s razor

6. k-fold cross validation

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

training and testing

question: how can we be con   dent about f predicting y given x?

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

!"#$%$%&'()*'+,'-.&/"$*01'+/2).'345'6%7/1)8''&)%2)"8''#&)8''4#1$.9'(*#*:(8';$<7/2)'=")2$*'#1/:%*'>'=")2$*'9)(?%/'training and testing

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

!"#$%&'()&'!"#$%&'()&'!"#$%&'()&'training and testing

high bias (under   tting)

just right!

high variance (over   tting)

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

!"#$%&'()&'!"#$%&'()&'!"#$%&'()&'training and testing

    we calculate etrain the in-sample error (training error or em-

pirical error/risk).

etrain(f ) =

n(cid:88)

i=1

(cid:96)oss(yi, f (xi))

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

training and testing

    we calculate etrain the in-sample error (training error or em-

pirical error/risk).

etrain(f ) =

n(cid:88)

i=1

(cid:96)oss(yi, f (xi))

    classi   cation error:

(cid:96)oss(yi, f (xi)) =

(cid:40) 1 if sign(yi) (cid:54)= sign(f (xi))

0 otherwise

    least square loss: (cid:96)oss(yi, f (xi)) = (yi     f (xi))2
    hinge loss: (cid:96)oss(yi, f (xi)) = max(0, 1     yif (xi))
    logistic loss: (cid:96)oss(yi, f (xi)) = log2(1 + e   yif (xi))
    exponential loss: (cid:96)oss(yi, f (xi)) = e   yif (xi)

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

training and testing

    we calculate etrain the in-sample error (training error or em-

pirical error/risk).

etrain(f ) =

n(cid:88)

(cid:96)oss(yi, f (xi))

i=1

    we aim to have etrain(f ) small, i.e., minimize etrain(f )
    we hope that etest(f ), the out-sample error (test/true error),

will be small too.

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

over   tting/under   tting

an intuitive example

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

structural risk minimization

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

!"#$%&   ()*+""("*,(-*******************************************.(/01#2%34*(5*36#*/($#1*************************************7%86*9999:#;3*#""("****9999:"<%)%)8*#""("*7%86*=%<;****************,(-*=%<;**,(-*><"%<)&#*********7%86*><"%<)&#*?)$#"@a)8****************b(($*/($#1;*****cd#"@a)8************structural risk minimization

question: how do we check that the model is under   tting?

question: how do we check that the model is over   tting?

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

!"#$%&   ()*+""("*,(-*******************************************.(/01#2%34*(5*36#*/($#1*************************************7%86*9999:#;3*#""("****9999:"<%)%)8*#""("*7%86*=%<;****************,(-*=%<;**,(-*><"%<)&#*********7%86*><"%<)&#*?)$#"@a)8****************b(($*/($#1;*****cd#"@a)8************avoid over   tting

introduce a bias! e.g., use simple models.

    reduce the number of features manually or do feature selection

(later).

    do a model selection (later).
    use id173 (keep the features but reduce their impor-

tance by setting small parameter values), (later).
    do a cross-validation to estimate the test error.

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

occam   s razor

occam   s razor:

prefer the shortest hypothesis that    ts the data.

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

id173: intuition

we want to minimize:

classi   cation term + c    id173 term

n(cid:88)

i=1

(cid:96)oss(yi, f (xi)) + c    r(f )

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

id173: intuition

f (x) =   0 +   1x ... (1)
f (x) =   0 +   1x +   2x2 ... (2)
f (x) =   0 +   1x +   2x2 +   3x3 +   4x4 ... (3)

hint: avoid high-degree polynomials. get rid of the fourth and
   fth terms in (3).

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

!"#$%&'()&'!"#$%&'()&'!"#$%&'()&'train, validation and test

train-validation-test framework:

1. training set is a set of examples used for learning a model

(e.g., a classi   cation model).

2. validation set is a set of examples that cannot be used for
learning the model but can help tune model parameters (e.g.,
selecting k in id92). validation helps control over   tting.

3. test set is used to assess the performance of the    nal model

and provide an estimation of the test error.
note: never use the test set in any way to further tune
the parameters or revise the model.

example: split the data randomly into 60% for training, 20 %
for validation and 20% for testing.

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

train validation test k-fold cross validation

a method for estimating test error using training data.

algorithm:
given a learning algorithm a and a dataset d
step 1: randomly partition d into k equal-size subsets d1, . . . , dk
step 2:
for j = 1 to k

train a on all di, i     1, . . . k and i (cid:54)= j, and get fj.
apply fj to dj and compute edj

step 3: average error over all folds.
(edj)

k(cid:88)

j=1

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

review

review the concepts and terminology:

instance, example, feature, label, supervised learning, unsu-
pervised learning, classi   cation, regression, id91, pre-
diction, training set, validation set, test set, k-fold cross
validation, classi   cation error, id168, over   tting, un-
der   tting, occam   s razor, id173.

coprtight c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

