machine learning for

data science

naive bayes classi   er

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. naive bayes classi   er.

2. setting

3. example

4. estimating probabilities

5. text classi   cation

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

naive bayes classi   er

    probabilistic model.
    highly practical method.
    application domains to natural language text documents.
    naive because of the strong independence assumption it makes

(not realistic).
    simple model.
    strong method can be comparable to id90 and neural

networks in some cases.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

naive bayes classi   er

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

anempiricalcomparisonofsupervisedlearningalgorithmstable3.normalizedscoresofeachlearningalgorithmbyproblem(averagedovereightmetrics)modelcalcovtadultltr.p1ltr.p2medisslachsmgcalhouscodbactmeanbst-dtplt.938.857.959.976.700.869.933.855.974.915.878*.896*rfplt.876.930.897.941.810.907*.884.883.937.903*.847.892bag-dt   .878.944*.883.911.762.898*.856.898.948.856.926.887*bst-dtiso.922*.865.901*.969.692*.878.927.845.965.912*.861.885*rf   .876.946*.883.922.785.912*.871.891*.941.874.824.884bag-dtplt.873.931.877.920.752.885.863.884.944.865.912*.882rfiso.865.934.851.935.767*.920.877.876.933.897*.821.880bag-dtiso.867.933.840.915.749.897.856.884.940.859.907*.877id166plt.765.886.936.962.733.866.913*.816.897.900*.807.862ann   .764.884.913.901.791*.881.932*.859.923.667.882.854id166iso.758.882.899.954.693*.878.907.827.897.900*.778.852annplt.766.872.898.894.775.871.929*.846.919.665.871.846anniso.767.882.821.891.785*.895.926*.841.915.672.862.842bst-dt   .874.842.875.913.523.807.860.785.933.835.858.828knnplt.819.785.920.937.626.777.803.844.827.774.855.815knn   .807.780.912.936.598.800.801.853.827.748.852.810knniso.814.784.879.935.633.791.794.832.824.777.833.809bst-stmpplt.644.949.767.688.723.806.800.862.923.622.915*.791id166   .696.819.731.860.600.859.788.776.833.864.763.781bst-stmpiso.639.941.700.681.711.807.793.862.912.632.902*.780bst-stmp   .605.865.540.615.624.779.683.799.817.581.906*.710dtiso.671.869.729.760.424.777.622.815.832.415.884.709dt   .652.872.723.763.449.769.609.829.831.389.899*.708dtplt.661.863.734.756.416.779.607.822.826.407.890*.706lr   .625.886.195.448.777*.852.675.849.838.647.905*.700lriso.616.881.229.440.763*.834.659.827.833.636.889*.692lrplt.610.870.185.446.738.835.667.823.832.633.895.685nbiso.574.904.674.557.709.724.205.687.758.633.770.654nbplt.572.892.648.561.694.732.213.690.755.632.756.650nb   .552.843.534.556.011.714-.654.655.759.636.688.481thanboostingstumpsonlyonsevenproblems.occa-sionallyboostedstumpsperformverywell,butsome-timestheyperformverypoorlysotheiraverageper-formanceislow.onadult,whenboostingtrees,the   rstiterationofboostinghurtstheperformanceofalltreetypes,andneverrecoversinsubsequentrounds.whenthishappensevensingledecisiontreesoutper-formtheirboostedcounterparts.baggedtreesandrandomforests,however,consistentlyoutperformsin-gletreesonallproblems.id112andrandomforestsare   safer   thanboosting,evenonthemetricsforwhichboostingyieldsthebestoverallperformance.5.bootstrapanalysistheresultsdependonthechoiceofproblemsandmet-rics.whatimpactmightselectingotherproblems,orevaluatingperformanceonothermetrics,haveontheresults?forexample,neuralnetsperformwellonallmetricson10of11problems,butperformpoorlyoncod.ifwehadn   tincludedthecodproblem,neuralnetswouldmoveup1-2placesintherankings.tohelpevaluatetheimpactofthechoiceofproblemsandmetricsweperformedabootstrapanalysis.werandomlyselectabootstrapsample(samplingwithreplacement)fromtheoriginal11problems.forthissampleofproblemswethenrandomlyselectaboot-strapsampleof8metricsfromtheoriginal8metrics(againsamplingwithreplacement).forthisbootstrapsampleofproblemsandmetricswerankthetenalgo-rithmsbymeanperformanceacrossthesampledprob-lemsandmetrics(andthe5folds).thisbootstrapsamplingisrepeated1000times,yielding1000poten-tiallydi   erentrankingsofthelearningmethods.table4showsthefrequencythateachmethodranks1st,2nd,3rd,etc.the0.228entryforboostedtreesinthecolumnfor2ndplace,tellsusthatthereisa22.8%chancethatboosteddecisiontreeswouldhaveplaced2ndinthetableofresults(insteadof1st)hadweselectedotherproblemsand/ormetrics.thebootstrapanalysiscomplementsthet-testsinta-bles2and3.theresultssuggestthatifwehadse-lectedotherproblems/metrics,thereisa58%chancethatboosteddecisiontreeswouldstillhaveranked1stsetting

    a training data (xi, yi), xi is a feature vector and yi is a discrete

label.

    d features, and n examples.
    example: consider document classi   cation, each example is a
documents, each feature represents the presence or absence of
a particular word in the document.

    we have a training set.
    a new example with feature values xnew = (a1, a2,          , ad).
    we want to predict the label ynew of the new example.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

setting

ynew = arg max

y   y

p(y|a1, a2,          , ad)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

setting

ynew = arg max

y   y

p(y|a1, a2,          , ad)

use bayes rule to obtain:

ynew = arg max

y   y

p(a1, a2,          , ad|y)     p(y)

p(a1, a2,          , ad)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

setting

ynew = arg max

y   y

p(y|a1, a2,          , ad)

use bayes rule to obtain:

ynew = arg max

y   y

ynew = arg max

y   y

p(a1, a2,          , ad|y)     p(y)

p(a1, a2,          , ad)

p(a1, a2,          , ad|y)     p(y)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

setting

ynew = arg max

y   y

p(y|a1, a2,          , ad)

use bayes rule to obtain:

ynew = arg max

y   y

ynew = arg max

y   y

p(a1, a2,          , ad|y)     p(y)

p(a1, a2,          , ad)

p(a1, a2,          , ad|y)     p(y)

can we estimate these two terms from the training data?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

setting

ynew = arg max

y   y

p(y|a1, a2,          , ad)

use bayes rule to obtain:

ynew = arg max

y   y

ynew = arg max

y   y

p(a1, a2,          , ad|y)     p(y)

p(a1, a2,          , ad)

p(a1, a2,          , ad|y)     p(y)

can we estimate these two terms from the training data?
1. p(y) can be easy to estimate: count the frequency with which

each label y occurs in the training data.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

setting

ynew = arg max

y   y

p(y|a1, a2,          , ad)

use bayes rule to obtain:

ynew = arg max

y   y

ynew = arg max

y   y

p(a1, a2,          , ad|y)     p(y)

p(a1, a2,          , ad)

p(a1, a2,          , ad|y)     p(y)

can we estimate these two terms from the training data?
1. p(y) can be easy to estimate: count the frequency with which

each label y.

2. p(a1, a2,          , ad|y) is not easy to estimate unless we have a very
very large sample. (we need to see every example many times
to get reliable estimates) #possible ex. * #possible y

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,          , ad is the product of the probabilities for the
individual features:

p(a1, a2,          , ad|y) =(cid:89)

p(aj|y)

j

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,          , ad is the product of the probabilities for the
individual features:

naive bayes classi   er:

p(a1, a2,          , ad|y) =(cid:89)
p(y)(cid:89)

ynew = arg max

j

y   y

p(aj|y)

p(aj|y)

j

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,          , ad is the product of the probabilities for the
individual features:

naive bayes classi   er:

p(a1, a2,          , ad|y) =(cid:89)
p(y)(cid:89)

ynew = arg max

j

y   y

p(aj|y)

p(aj|y)

j

can we estimate these two terms from the training data?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

naive bayes classi   er

makes a simplifying assumption that the feature values are condi-
tionally independent given the label.
given the label of the example, the id203 of observing the
conjunction a1, a2,          , ad is the product of the probabilities for the
individual features:

naive bayes classi   er:

p(a1, a2,          , ad|y) =(cid:89)
p(y)(cid:89)

ynew = arg max

j

y   y

p(aj|y)

p(aj|y)

j

can we estimate these two terms from the training data?

#possible distinct feature values * #possible y

yes!!!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

algorithm

learning: based on the frequency counts in the dataset:
1. estimate all p(y),    y     y.
2. estimate all p(aj|y)    y     y,    ai.

classi   cation: for a new example, use:

ynew = arg max

y   y

p(y)(cid:89)

j

p(aj|y)

note: no model per se or hyperplane, just count the frequencies
of various data combinations within the training examples.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

example

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

outlook temperaturehumiditywindyplaysunnyhothighfalsenosunnyhothightruenoovercasthothighfalseyesrainymildhighfalseyesrainycoolnormalfalseyesrainycoolnormaltruenoovercastcoolnormaltrueyessunnymildhighfalsenosunnycoolnormalfalseyesrainymildnormalfalseyessunnymildnormaltrueyesovercastmildhightrueyesovercasthotnormalfalseyesrainymildhightruenoexample

new example:

(outlook = sunny, temperature = cool, humidity = high, wind =strong).

can we predict the class of the new example?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

outlook temperaturehumiditywindyplaysunnyhothighfalsenosunnyhothightruenoovercasthothighfalseyesrainymildhighfalseyesrainycoolnormalfalseyesrainycoolnormaltruenoovercastcoolnormaltrueyessunnymildhighfalsenosunnycoolnormalfalseyesrainymildnormalfalseyessunnymildnormaltrueyesovercastmildhightrueyesovercasthotnormalfalseyesrainymildhightruenoexample

ynew = arg max
y   {yes,no}

p(y)     p(outlook = sunny|y)     p(temp = cool|y)   

p(humidity = high|y)     p(windy = strong|y)

p(play = yes) = 9/14 = 0.64

p(play = no) = 5/14 = 0.36

conditional probabilities:

p(wind = strong|play = yes) = 3/9 = 0.33
p(wind = strong|play = no) = 3/5 = 0.6

p(yes)     p(sunny|yes)     p(cool|yes)     p(high|yes)     p(strong|yes) = 0.0053

p(no)     p(sunny|no)     p(cool|no)     p(high|no)     p(strong|no) = 0.0206

ynew = no

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

estimating probabilities

m-estimate of the id203:

p(aj|y) =

nc + m     p
ny + m

intuition:
augment the sample size by m virtual examples, distributed ac-
cording to prior p (prior estimate of each value). if a feature has
k values, we can set p = 1
k.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

credit
    machine learning. tom mitchell 1997.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

