machine learning for

data science

id75, ridge

regression, and lasso

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. ordinary least squares

2. geometric interpretation

3. gauss-markov theorem

4. shortcomings of ols

5. ridge regression

6. lasso

7. summary

8. elastic net

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

recall: normal equation

we want to    nd (d + 1)      s that minimize r. we write r:

r(  ) =

r(  ) =

   r
     

1
2n
=    1
n

||(y     x  )||2

1
2n
(y     x  )t (y     x  )

xt (y     x  )

we solve:

the unique solution is:

xt (y     x  ) = 0

   = (xt x)   1xt y

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

ordinary least squares
    id75 with least squares is also referred to as

ordinary least squares (ols).

    we have seen both the analytical and iterative approach.
    in the analytical approach, calculate the formula:

   = (xt x)   1xt y

  y = x   = x(xt x)   1xt y

  y = hy

h is the hat matrix or projection matrix.
  y is the orthogonal expression of y in the space.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

geometric interpretation

credit: element of statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

x1x2ygauss-markov theorem

least-squares gives the    best linear unbiased estimator    of      s.

ols is the blue.

   the gauss-markov theorem implies that the least squares esti-
mator has the smallest mean squared error of all linear estimators
with no bias. however, there may well exist a biased estimator
with smaller mean squared error.    esl.

optional: for the most curious, there are many versions of
the complete proof of this theorem online.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

shortcoming of ols

1. ols assumes xt x is invertible (regular).

if xt x is non in-
vertible (singular), ols will get an unstable solution (example,
if two features are strongly correlated).

2. prediction accuracy: low bias but high variance     control

variance but introducing some bias.

3. interpretation shortcoming

    ols is not designed for high-dimensional datasets.
    same importance for all features, hence important features

get mixed with the not-so-important ones.

if xt x is (almost) singular, move to an estimator that has more
bias but less variance, (remember it   s a tradeo   !) use a shrinkage
or regularized or penalized method such as: ridge regression.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

structural risk minimization

recall:

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

!"#$%&   ()*+""("*,(-*******************************************.(/01#2%34*(5*36#*/($#1*************************************7%86*9999:#;3*#""("****9999:"<%)%)8*#""("*7%86*=%<;****************,(-*=%<;**,(-*><"%<)&#*********7%86*><"%<)&#*?)$#"@a)8****************b(($*/($#1;*****cd#"@a)8************ridge regression

ols often performs poorly for both prediction and interpretation.

to address prediction:

    ridge regression: hoerl and kennard 1988.
    also known as tikhonov id173.
    ridge regression modi   es ols to make it more robust in the

case where xt x is (almost) singular.

    it shrinks the regression coe   cients by imposing a penalty on

their size.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

ridge regression

find   ridge:

  ridge := argmin

  

          1

2n

n(cid:88)

i=1

(yi       0     d(cid:88)

j=1

  jxij)2 +   

         

d(cid:88)

j=1

  2
j

where        0 is a complexity parameter that controls the amount
of shrinkage.

   controls the tradeo    between bias and variance!
large        high shrinkage          s shrunk toward zeros.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

ridge regression

an equivalent way to write it:

  ridge := argmin

  

         

  jxij)2

          1

2n

n(cid:88)

i=1

(yi       0     d(cid:88)
d(cid:88)

j     t
  2

j=1

j=1

subject to

    imposes a size constraint on the coe   cients to alleviate the

problem of correlation between features.

    there is a one-to-one correspondence between the parameters

   and t.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

ridge regression

another way to write it:
1
2n

r(  ,   ) =

(y     x  )t (y     x  ) +     t   

  ridge = (xt x +   i)   1xt y

r(  ,   ) =

||(y     x  )||2 + ||    ||2

1
2n

where i is the d    d identity matrix.
xt x +   i is called the regularized term of xt x.

notice: we added a positive constant to the diagonal of xt x.

why would this help? (hint: determinant.)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

ridge regression

two important observations about ridge regression:

    uses a bound on the l2 norm of the coe   cients.

r(  ,   ) =

||(y     x  )||2 + ||    ||2

1
2n

    always keeps all predictors in the model.

ridge regression cannot produce a sparse model.

    alternative is lasso.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

what is sparsity?

    suppose the number of dimensions (features) d is large.
    we want some interpretability, so we want to inhibit a large

number of features and keep the essential ones.

    this is called in ml: feature selection.
    in id75, we want to    nd a model where many      s

are zeros!

    feature j is inhibited if its   j is inhibited.
    such a model is called a sparse model.
    however, we still want to predict well with fewer features!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

lasso

    method for sparse regression.
    lasso     least absolute shrinkage and selection operator
    tibshirani 1996. (co-author of esl book).
    aim is to do automatic variable selection.
    subtle di   erence from ridge regression: it uses the l1 norm.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

lasso

find   lasso:

  lasso := argmin

  

          1

2n

n(cid:88)

i=1

(yi       0     d(cid:88)

j=1

         

d(cid:88)

j=1

  j

  jxij)2 +   

uses a bound on the l1 norm of the coe   cients.
||(y     x  )||2 + ||    ||

r(  ,   ) =

1
2n

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

lasso vs. ridge penalty

in red: contours of ||y     x  ||2. in cyan, the constrained regions:

left: lasso |  1| + |  2|     t, right: ridge   2

1 +   2

2     t2

credit: element of statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

lasso   s limitations

    no analytical solution use numerical optimization.
    does not detect groups of variables (select one feature at ran-

dom in the group).

    is limited when d > n.
    if there is a high correlation between variables, ridge regression

has a better performance.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

summary: id75

    id75 methods: ols, ridge regression, lasso.

question: let us summarize the properties of each?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

elastic net

    zou and hastie (2005) introduced the elastic net penalty
    compromise between lasso and ridge.
    d>>n

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

elastic net

credit: hui zou and trevor hastie. id173 and variable selection via

the elastic net (pdf). jrssb (2005) 67(2) 301-320.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

elastic net

optional: investigate elastic net approach.

credit: hui zou and trevor hastie. id173 and variable selection via

the elastic net (pdf). jrssb (2005) 67(2) 301-320.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

credit

when mentioned, some of the    gures in this presentation are taken
from    an introduction to statistical learning, with applications in
r    (springer, 2013)    with permission from the authors: g. james,
d. witten, t. hastie and r. tibshirani. or from    the elements
of statistical learning    hastie, tibshirani and friedman.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

