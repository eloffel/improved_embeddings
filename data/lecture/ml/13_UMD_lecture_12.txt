slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 12 bias and fairness

furong huang / furongh@cs.umd.edu

some ml issues in the real world

midterm     grade distribution (histogram)

mean: 70.5, median: 70

midterm     makeup policy

   resubmit your answer to question(s)

   up to 8 points to make up
   i trust you, don   t seek help from others
   for t/f problems, give detailed justification/proof
   grading will be very strict, you will get 0 points if 

the justification is flawed

   detailed printout of justifications/proofs

   your name, your session id, your uid
   deadline: march 15 (thursday), 11:00 am before 

class.

requirements from my first lecture

   do the reading before class

   already familiar with high-level concepts and 

mathematical notation by the time you come to 
class. 

   you can get more out of class time by focusing on 

understanding the reasoning and clarifying what 
didn't make sense when you first read it

   understanding

   being able to precisely define and manipulate the 

mathematical concepts 

   being able to discuss the intuition behind algorithms 

with words is necessary but not sufficient.

why?

final exam     how to prepare

    we will assign more questions that require 

mathematical reasoning in the homework

    you will read the textbook before coming to

class

    you will ask questions during the lecture
    you could form study groups if need to 

review id202ic knowledge, logical 
reasoning techniques

midterm     regrading requests

   written only requests

   list the problems that should be regraded
   suggest the points you should be getting
   justify why

   submit a detailed printout

   your name, your session id, your uid
   deadline: march 27, 11:00 am before class

ranking

canonical example: web search

given all the documents on the web
for a user query, retrieve relevant 
documents, ranked from most relevant to 
least relevant

how can we reduce ranking to binary 
classification?

preference function

    given a query q and documents di and 

dj, the preference function outputs 
whether
   
    or dj should be preferred to di

di should be preferred to dj

    that   s a binary classification problem!

specifying the reduction from ranking 
to binary classification

    how to train classifier that predicts 

preferences?

    how to turn the predicted preferences 

into a ranking?

features 

associated with 

comparing 

document j and 
document j for 

query n

na  ve approach

works well for bipartite problems

   is this document relevant or not?   

not ideal for full ranking problems, 
because

binary preference problems are not all equally 
important
separates preference function and sorting

improving on na  ve approach

example of cost functions

resulting ranking algorithms

exercise: understand this offline

ranktest

a probabilistic version of the quicksort algorithm

only o(m log2m) calls to f in expectation

better error bound than na  ve algorithm

(see ciml for theorem)

what you should know

    what are reductions and why they are useful

   

implement, analyze and prove error bounds of 
algorithms for
    weighted binary classification
    multiclass classification (ova, ava)

    !   ranking

    understand algorithms for

bias and fairness

id27s could be biased

bolukbasi et al. nips 2016.

https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

recall: formal definition of binary 
classification (from ciml)

train/test mismatch

    when working with real world data,  training 

sample
   

reflects human biases
is influenced by practical concerns
   

e.g., what  kind of data is easy to obtain

   

    train/test distribution mismatch is frequent 

issue
    aka sample selection bias, id20

id20

    what does it mean for 2 distributions to be related?

    when 2 distributions are related how can we build 

models that effectively share information between them?

unsupervised adaptation

goal: learn a classifier f that achieves low 
expected loss under new distribution

given labeled training data from old distribution 

and unlabeled examples from new distribution

relation between test loss in new 
domain and old domain

how can we estimate the ratio between 
dnew and dold?

fixed base 
distribution

s = selection 

variable

we can estimate p(s=1|x) 
using a binary classifier!

clarifications

    note znew is the same for all the data points (i.e., 

   (#,%)). it is also a id172 to make sure 
   (#,())*+,-,% =1.
1*+,=2(#,()[)456+-,%78=0-]
therefore, znew is not a function of (#,%), in fact it is a 

   

constant. 

    similarly for zold.
    all examples are drawn from fixed base distribution, 

some are selected to go into dnew, some are selected to 
go into dold according to a selection variable s.

we will revisit id28! 

supervised adaptation

goal: learn a classifier f that achieves low 
expected loss under new distribution

given labeled training data from old distribution 

and labeled examples from new distribution

one solution: feature augmentation

map inputs to a new augmented representation

one solution: feature augmentation

    transform dold and dnew training 

examples

    train a classifier on new 

representations

    done!

one solution: feature augmentation

    adding instance weighting might be 

useful if n >> m

    most effective when distributions are 

   not too close but not too far   
   

in practice, always try    old only   ,    new only   , 
   union of old and new    as well!

bias is pervasive

    bias in the labeling
    sample selection bias
    bias in choice of labels
    bias in features or model structure
    bias in id168
    deployed systems create feedback loops

bias and how to deal with it

    train/test mismatch

    unsupervised adaptation

    supervised adaptation

acm code of ethics

   to minimize the possibility of indirectly harming others, 
computing professionals must minimize malfunctions by 
following generally accepted standards for system design 
and testing. furthermore, it is often necessary to assess 
the social consequences of systems to project the 
likelihood of any serious harm to others. if system features 
are misrepresented to users, coworkers, or supervisors, the 
individual computing professional is responsible for any 
resulting injury.   

https://www.acm.org/about-acm/acm-code-of-ethics-and-professional-conduct

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

