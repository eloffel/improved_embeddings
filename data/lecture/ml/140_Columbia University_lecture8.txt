machine learning for

data science

tree classi   ers
(id90)

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. tree classi   ers: de   nition & history

2. a toy example: play tennis?

3. splitting criteria in c4.5

4. numerical features

5. pruning strategies

6. alternative method: cart

7. practical considerations

8. tree classi   ers: pros & cons

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

tree classi   ers
    popular classi   cation methods.
    easy to understand, simple algorithmic approach.
    no assumption about linearity.
    history:

    cart (classi   cation and regression trees): friedman

1977.

      and c4.5 family: quilan 1979-1983.
    re   nements in mid 1990   s (e.g., pruning, numerical fea-

tures etc.).
    applications:

    botany (e.g., new flora of the british isles stace 1991).

    medical research (e.g., pima indian diabetes diagnosis, early

diagnosis of acute myocardial infarction).

    computational biology (e.g., interaction between genes)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

tree classi   ers

    the terminology tree is graphic.
    however, a decision tree is grown from the root downward.
the idea is to send the examples down the tree, using the
concept of information id178.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

tree classi   ers

    the terminology tree is graphic.
    however, a decision tree is grown from the root downward.
the idea is to send the examples down the tree, using the
concept of information id178.

    general steps to build a tree:

1. start with the root node that has all the examples.

2. greedy selection of the next best feature to build the

branches. the splitting criteria is node purity.

3. class majority will be assigned to the leaves.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

classi   cation

given: training data:

(x1, y1), . . . , (xn, yn)

where xi     rd and yi is discrete (categorical/qualitative), yi     y.
example y = {   1, +1}, y = {0, 1}.

task: learn a classi   cation function:

f : rd        y

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

classi   cation

given: training data:

(x1, y1), . . . , (xn, yn)

where xi     rd and yi is discrete (categorical/qualitative), yi     y.
example y = {   1, +1}, y = {0, 1}.

task: learn a classi   cation function:

f : rd        y

in the case of tree classi   ers:

1. no need for xi     rd, so no need to turn categorical features

into numerical features.

2. the model is a tree.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

toy example: play tennis   ?

   adapted from tom mitchell. machine learning 1997.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

outlook temperaturehumiditywindyplaysunnyhothighfalsenosunnyhothightruenoovercasthothighfalseyesrainymildhighfalseyesrainycoolnormalfalseyesrainycoolnormaltruenoovercastcoolnormaltrueyessunnymildhighfalsenosunnycoolnormalfalseyesrainymildnormalfalseyessunnymildnormaltrueyesovercastmildhightrueyesovercasthotnormalfalseyesrainymildhightruenotoy example: play tennis?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

outlook temperaturehumiditywindyplaysunnyhothighfalsenosunnyhothightruenoovercasthothighfalseyesrainymildhighfalseyesrainycoolnormalfalseyesrainycoolnormaltruenoovercastcoolnormaltrueyessunnymildhighfalsenosunnycoolnormalfalseyesrainymildnormalfalseyessunnymildnormaltrueyesovercastmildhightrueyesovercasthotnormalfalseyesrainymildhightruenooutlook temp  rature humiditywindyplaysunnycoolhightrue?toy example: play tennis?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

outlook temperaturehumiditywindyplaysunnyhothighfalsenosunnyhothightruenoovercasthothighfalseyesrainymildhighfalseyesrainycoolnormalfalseyesrainycoolnormaltruenoovercastcoolnormaltrueyessunnymildhighfalsenosunnycoolnormalfalseyesrainymildnormalfalseyessunnymildnormaltrueyesovercastmildhightrueyesovercasthotnormalfalseyesrainymildhightruenooutlook yes(9), no(5) sunny overcast rainy humidity high normal no(3) yes(2) yes(4) windy true false no(2) yes(3) splitting criteria in c4.5

1. the central choice is selecting the next attribute to split on.

2. we want some criteria that measures the homogeneity or im-

purity of examples in the nodes:

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

splitting criteria in c4.5

1. the central choice is selecting the next attribute to split on.

2. we want some criteria that measures the homogeneity or im-

purity of examples in the nodes:

(a) quantify the mix of classes at each node.

(b) maximum if equal number of examples from each class.

(c) minimum if the node is pure.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

splitting criteria in c4.5

1. the central choice is selecting the next attribute to split on.

2. we want some criteria that measures the homogeneity or im-

purity of examples in the nodes:

(a) quantify the mix of classes at each node.

(b) maximum if equal number of examples from each class.

(c) minimum if the node is pure.

3. a perfect measure commonly used in id205:

id178(s) = - p    log2 p        p(cid:9) log2 p(cid:9)

p    is the proportion of positive examples.
p(cid:9) is the proportion of negative examples.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

splitting criteria in c4.5

id178(s) = - p    log2 p        p(cid:9) log2 p(cid:9)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

id178(s)1.00.50.00.51.0p+splitting criteria in c4.5

id178(s) = - p    log2 p        p(cid:9) log2 p(cid:9)

in general, for c classes:

id178(s) =

c(cid:88)i=1    pi log2 pi

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

id178(s)1.00.50.00.51.0p+splitting criteria in c4.5

    now each node has some id178 that measures the homo-

geneity in the node.

    how to decide which attribute is best to split on based on

id178?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

splitting criteria in c4.5

    now each node has some id178 that measures the homo-

geneity in the node.

    how to decide which attribute is best to split on based on

id178?

    we use information gain that measures the expected reduc-
tion in id178 caused by partitioning the examples according
to the attributes:

gain(s, a) = id178(s)    

(cid:88)v   values(a)

|sv|
|s|

id178(sv)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

back to the example

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

outlook yes(9), no(5) yes(2) no(3) yes(4)         yes(3) no(2)   id178(2,3)  id178(4,0)  id178(3,2) sunny overcast rainy !"id178(9,5)= -9/14 log 9/14 - 5/14 log 5/14 = 0.94 gain(s, outlook) = 0.94     5/14*0.97     0     5/14* 0.97 = 0.247  temperature yes(9), no(5) yes(2) no(2) yes(4) no(2)         yes(3) no(1)   id178(2,2)  id178(4,2)  id178(3,1) hot mild cool !"id178(9,5)= -9/14 log 9/14 - 5/14 log 5/14 = 0.94 gain(s, temperature)= 0.94     4/14*1     6/14*0.918     4/14*0.811                       = 0.029   humidity yes(9), no(5) yes(3) no(4) yes(6) no(1) id178(3,4)   id178(6,1) high normal !"id178(9,5)= -9/14 log 9/14 - 5/14 log 5/14 = 0.94 gain(s, humidity) = 0.94     7/14*0.985     7/14* 0.592 = 0.152   windy yes(9), no(5) yes(6) no(2) yes(3) no(3) id178(9,5)= -9/14 log 9/14 - 5/14 log 5/14 = 0.94 id178(6,2) id178(3,3) false true !"gain(s, windy) = 0.94     8/14*0.811     6/14* 1 = 0.048  back to the example

at the    rst split starting from the root, we choose outlook that
has the max gain.

then, we re-start the same process at each of outlook children
nodes (if node not pure).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

featureinformation gainoutlook0.247temperature0.029humidity0.152windy0.048numerical features

   adapted from tom mitchell. machine learning 1997.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

outlook temperaturehumiditywindyplaysunny8585falsenosunny8090truenoovercast8386falseyesrainy7096falseyesrainy6880falseyesrainy6570truenoovercast6465trueyessunny7295falsenosunny6970falseyesrainy7580falseyessunny7570trueyesovercast7290trueyesovercast8175falseyesrainy7191truenoover   tting the data

accuracy = 1 - error

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

0.50.550.60.650.70.750.80.850.90102030405060708090100accuracysize of tree (number of nodes)on training dataon test datapruning strategies

to get suitable tree sizes and avoid over   tting:
    stop growing the tree earlier, before it reaches the point where
it perfectly classi   es the training examples. (di   cult to know
when to stop!).

    grow a complex tree then to prune it back (best strategy

found).

1. use a validation set / cross validation to evaluate the utility
of post-pruning (remove a subtree if the performance of the
new tree is no worse than the original tree).

2. rule post pruning.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

cart

    adopt same greedy, top-down algorithm.
    binary splits instead of multiway splits.
    uses gini index instead of information id178.

gini = 1     p2

        p 2
(cid:9)

    works with regression.

teaser: how does cart work for regression?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

9.2tree-basedmethods3090.00.20.40.60.81.00.00.10.20.30.40.5pid178gini indexmisclassification errorfigure9.3.nodeimpuritymeasuresfortwo-classclassi   cation,asafunctionoftheproportionpinclass2.cross-id178hasbeenscaledtopassthrough(0.5,0.5).impuritymeasureqm(t)de   nedin(9.15),butthisisnotsuitableforclassi   cation.inanodem,representingaregionrmwithnmobservations,let  pmk=1nm!xi   rmi(yi=k),theproportionofclasskobservationsinnodem.weclassifytheobser-vationsinnodemtoclassk(m)=argmaxk  pmk,themajorityclassinnodem.di   erentmeasuresqm(t)ofnodeimpurityincludethefollowing:misclassi   cationerror:1nm"i   rmi(yi!=k(m))=1     pmk(m).giniindex:"k"=k!  pmk  pmk!="kk=1  pmk(1     pmk).cross-id178ordeviance:   "kk=1  pmklog  pmk.(9.17)fortwoclasses,ifpistheproportioninthesecondclass,thesethreemea-suresare1   max(p,1   p),2p(1   p)and   plogp   (1   p)log(1   p),respectively.theyareshowninfigure9.3.allthreearesimilar,butcross-id178andtheginiindexaredi   erentiable,andhencemoreamenabletonumericaloptimization.comparing(9.13)and(9.15),weseethatweneedtoweightthenodeimpuritymeasuresbythenumbernmlandnmrofobservationsinthetwochildnodescreatedbysplittingnodem.inaddition,cross-id178andtheginiindexaremoresensitivetochangesinthenodeprobabilitiesthanthemisclassi   cationrate.forexample,inatwo-classproblemwith400observationsineachclass(denotethisby(400,400)),supposeonesplitcreatednodes(300,100)and(100,300),whilecart

credit: elements of statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

3069.additivemodels,trees,andrelatedmethods|t1t2t3t4r1r1r2r2r3r3r4r4r5r5x1x1x1x2x2x2x1   t1x2   t2x1   t3x2   t4figure9.2.partitionsandcart.toprightpanelshowsapartitionofatwo-dimensionalfeaturespacebyrecursivebinarysplitting,asusedincart,appliedtosomefakedata.topleftpanelshowsageneralpartitionthatcannotbeobtainedfromrecursivebinarysplitting.bottoid113ftpanelshowsthetreecor-respondingtothepartitioninthetoprightpanel,andaperspectiveplotofthepredictionsurfaceappearsinthebottomrightpanel.id74

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

!"#$%$&   (   )*%$&   !"#$%$&   !"#$%&   ()*)+$%,!&-./0($%&   ()*)+$%,.&-(   )*%$&   ./0($%1$2/*)+$%,.1-!"#$%1$2/*)+$%,!1-344#"/45+,!-.-,(/-0-+,!-.-,(-.-1!-.-1(/&"$4)()   6,!-0-+,!-.-1!/7$6()*)+)*5%,8$4/00-,!-0-+,!-.-1(/79$4):)4)*5,(-0-+,(-.-1!/34*#/0%;/<$0%&"$=)4*$=%;/<$0,2   -3   45   6%*)   -"7-3"#$%$&   -34   8$5%$"6#-%2*%-*4   -5"44   5%,2   -3   45   6%*)   -"7-3"#$%$&   -5*#   #-%2*%-9   4   -34   8$5%   8-*#-3"#$%$&   ,2   -3   45   6%*)   -"7-6   )*%$&   -5*#   #-%2*%-9   4   -34   8$5%   8-*#-6   )*%$&   ,2   -3   45   6%*)   -"7-34   8$5%$"6#-%2*%-*4   -5"44   5%practical considerations

1. consider performing id84 beforehand to

keep the most discriminative features.

2. use ensemble methods. e.g., id79, have a great

performance.*

3. balance your dataset before training to prevent the tree from

creating a tree biased toward the classes that are dominant.
    under-sampling: reduce the majority class
    over-sampling: synthetic data generation for the minority

class (e.g., smote, and adasyn).

*an empirical comparison of supervised learning algorithms rich by caruana

and alexandru niculescu-mizil. icml 2006.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

practical considerations

credit: he and garcia: learning from imbalanced data 2009

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

tree classi   ers: pros & cons

+ intuitive, interpretable (but...).

+ can be turned into rules.

+ well-suited for categorical data.

+ simple to build.

+ no need to scale the data.

- unstable (change in an example may lead to a di   erent tree).

- univariate (split one attribute at a time, does not combine
features).

- a choice at some node depends on the previous choices.

- need to balance the data.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

credit

    the elements of statistical learning. data mining, id136,
and prediction. 10th edition 2009. t. hastie, r. tibshirani,
j. friedman.

    machine learning 1997. tom mitchell.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

