machine learning for

data science

mining frequent patterns

and association rules

(continued)

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review
2. post-processing of association rules
3. implementations
4. performance
5. uniform notion of item
6. quantitative association rules
7. quantminer demo

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

post-processing of ar

    ar framework may lead to a large number of rules.

    how one can reduce the number of rules?

1. use many evaluation measures

2. increase minimum support

3. increase minimum con   dence

4. use rule templates (de   ne constraints on max rule length,
include in the rules speci   c items)

exclude some items,
(agrawal et al. 1995, salleb et al. 2007)

    question: can transitivity be used to prune the set of rules?

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

implementations

    fimi frequent itemset mining implementations repository
http://fimi.cs.helsinki.fi/ fimi   03 and fimi   04 workshop,
bayardo, goethals & zaki

    apriori
borgelt

http://www.borgelt.net/apriori.html developed by

    weka http://www.cs.waikato.ac.nz/ml/weka/ by witten &

frank

    armada data mining tool version 1.3.2 in matlab available

at mathworks, by malone

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

fp algorithms

according to the strategy to traverse the search space:

    breadth first search (ex: apriori, aprioritid, partition, dic)

    depth first search (ex: eclat, clique, depth project)

    hybrid (ex: apriorihybrid, hybrid, viper, kdci)

    pattern growth, i.e. no candidate generation (ex: fpgrowth,

hmine, co   )

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

performance

experiments are conducted on synthetic and real benchmarks
datasets.

    uci machine learning repository
http://archive.ics.uci.edu/ml/

    uci knowledge discovery in databases archive

http://kdd.ics.uci.edu/

    synthetic datasets (ex: ibm generator)

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

performance

(goethals 2004)

data set
t40i10d100k
mushroom
bms-webview-1
basket

#items #transactions min|t| max|t|
942
119
497

100 000
8 124
59 602
41 373

77
23
267
52

4
23
1
1

13 103

avg|t|
39
23
2
9

table 5: data set characteristics.

5 example data sets

for all experiments we performed in this thesis, we used four data sets with
di   erent characteristics. we have experimented using three real data sets,
of which two are publicly available, and one synthetic data set generated by
the program provided by the quest research group at ibm almaden [5]. the
mushroom data set contains characteristics of various species of mushrooms,
and was originally obtained from the uci repository of machine learning
databases [10]. the bms-webview-1 data set contains several months worth
of clickstream data from an e-commerce web site, and is made publicly avail-
able by blue martini software [23]. the basket data set contains transactions
from a belgian retail store, but can unfortunately not be made publicly avail-
able. table 5 shows the number of items and the number of transactions in

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

performance

1000

)
s
d
n
o
c
e
s
(
 
e
m

i
t

100

10

apriori
eclat
fpgrowth
hybrid

10000

)
s
d
n
o
c
e
s
(
 
e
m

i
t

1000

apriori
eclat
fpgrowth
hybrid

1

0

10

20

30

40

50

60

70

80

90

100

support

(a) basket

100

700

750

800

850

support

900

950

1000

(c) t40i10d100k

apriori
eclat
fpgrowth
hybrid

1000

)
s
d
n
o
c
e
s
(
 

e
m

i
t

100

10

apriori
eclat
fpgrowth
hybrid

40

45

50

55

60

support

(b) bms-webview-1

1
600

650

700

750

800

support

850

900

950

1000

(d) mushroom

)
s
d
n
o
c
e
s
(
 

e
m

i
t

100

10

1

figure 4: frequent itemset mining performance.

figure 4: frequent itemset mining performance.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

35

36

uniform notion of item
    apriori has been initially designed for boolean tables (trans-
actional datasets) thus id118 was su   cient to ex-
press: items, itemsets and rules.

milk     cereals

    for relational tables, one need to extend the notion of items

to literals:

item     (attribute, value)

an attribute could be:
1. categorical, for ex. (color, blue),

2. quantitative with a few numerical values, for ex. (#cars, 2),

3. quantitative with a

large domain values,

for

ex.

(age, [20, 40]).

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

example

d : people

id age married? #cars
1
2
3
4
5

no
yes
no
yes
yes

23
25
29
34
38

1
1
0
2
2

examples of frequent itemsets

itemset

(age, 20..29)
(age, 30..39)
(married?, yes)
(married?, no)

(#cars, 1)
(#cars, 2)

(age, 30..39),(married?, yes)

examples of rules

rule

(age, 30..39) et (married?, yes)        (#cars, 2)

(age, 20..29)        (#cars, 1)

support

3
2
3
2
2
2
2

support

con   dence

40%
60%

100%
66.6%

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

quantitative ar

question: mining quantitative ar is not a simple extension of
mining categorical ar. why?
    in   nite search space:

in boolean ar, the ariori property
allows to prune the search space e   ciently, but we do explore
the whole space of hypothesis (lattice of itemsets), which is
impossible for quantitative ar.

    the support-con   dence tradeo   : choosing intervals is

quite sensitive to support and con   dence.

- intervals too small, not enough support;
- intervals too large, not enough con   dence.

    what is the di   erence between supervised and unsupervised

discretization?

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

approaches to mine qars

    discretization-based approaches

    distribution-based approaches

    optimization-based approaches

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

approaches to mine qars

optimization-based approaches
    ruckert et al. 2004 use half-spaces to mine such rules like:

x1 > 20     0.5x3 + 2.3x6     100

cannot handle categorical attributes.

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

approaches to mine qars

function fitness(a     b)
tempfitness = gain(a     b)
if tempfitness     0 then

optimization-based approaches
    ruckert et al. 2004 use half-spaces to mine such rules like:

foreach interval i in a     b do
//favor small intervals
tempfitness * = (1-prop(i))2
if support(a     b) < m insupp then

//penalize low support rules
tempfitness - = nbtuples

a1, . . . , an, i.e., the set of instantiations v1, . . . , vn such that
a1 = v1     . . . an = vn is frequent and generate a rule tem-
plate for each such instantiation. this leads to as many rules
as the number of instantiations. notice that this step is similar
to apriori [agrawal et al., 1993].
    individual representation an individual is a set of items
of the form attributei     [li, ui], where attributei is the ith
numeric attribute in the rule template from the left to the right.
    initial population the initial population of individuals is
in the    rst individual, the intervals
generated as follows:
[li, ui] represent the whole domain of the i th numeric at-
tribute, and the following individuals encode intervals with
decreasing amplitudes until they reach a minimum support in
the dataset. once the amplitudes are    xed for an individual,
the bounds li and ui are chosen at random. this ensures to
start with enough diversity in the initial population that thus
model general and speci   c rules.

x1 > 20     0.5x3 + 2.3x6     100

gain(a     b) = supp(ab)     m inconf     supp(a)

    salleb et al. 2007: quantminer optimize the gain of rules

templates using a genetic algorithm.

cannot handle categorical attributes.

return tempfitness

domain of attributei

lili

uiui

l   i

u   i

crossover

lili

uiui

l   i

lili

u   i

u   i

l   i

ui

lili

uiui

mutation

lili

u      i
l      i

uiui

figure 1: crossover and mutation operators used in quant-
miner

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

    genetic operators mutation and crossover (figure 1) are
both used in order to transform a generation of individuals
into another one, improving its quality. the crossover oper-

algorithm 2: quantminer

input: a dataset composed of nbtuples, popsize,

gennb, cr, mr, minsupp, minconf

output: quantitative association rules r
select a set of attributes
let rt a set of rule templates de   ned on these attributes
compute the set of frequent itemsets on categorical
attributes in rt
r =    
foreach r     rt do

generate a random population pop of popsize
instantiated rules following the template r
i=1
while i     gennb do

form the next generation of population by
mutation and crossover w.r.t. mr and cr.
keep popsize rules in pop with the best fitness
values
i++

r = r     argmaxr   p op f itness(r)

22

return r

approaches to mine qars

optimization-based approaches: quantminer cont   d.
example uci iris dataset:

species=
value

       pw     [l1, u1] sw     [l2, u2]

pl     [l3, u3] sl     [l4, u4]     supp%

conf%

species=

species=

setosa        pw     [1, 6] sw     [31, 39]
versicolor        pw     [10, 15] sw     [22, 30]
virginica        pw     [18, 25] sw     [27, 33]

pl     [10, 19] sl     [46, 54]     23%
pl     [35, 47] sl     [55, 66]     21%
pl     [48, 60] sl     [58, 72]     20%

species=

70%

64%

60%

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

quantminer

http://quantminer.github.io/quantminer/

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

quantminer

uci iris dataset 

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

quantminer

1.  !
nulliparous_preterm_before_30_weeks !
!   cervical_length in [7, 30]!

confidence (a ! b) ~ 90%!
confidence (non a ! b) ~ 28%!

2. !
nulliparous_after_36_weeks ! cervical_length in [26, 45] 
and infection = no   

confidence (a ! b) ~ 70%!
confidence (non a ! b) ~ 50%!

copyright c   ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

