machine learning for

data science

naive bayes: text

classi   cation

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review naive bayes classi   er

2. estimating probabilities

3. learning to classify text

4. example

5. linearity of naive bayes classi   ers?

6. naive bayes with missing labels

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

setting

    a training data (xi, yi), xi is a feature vector and yi is a discrete

label. d features, and n examples.

    example: consider document classi   cation.
    a new example with feature values xnew = (a1, a2,          , ad).
    we want to predict the label ynew of the new example.

ynew = arg max

y   y

p(y|a1, a2,          , ad)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

naive bayes classi   er

use simplifying assumption:

naive bayes classi   er:

p(a1, a2,          , ad|y) =(cid:89)
p(y)(cid:89)

ynew = arg max

j

y   y

p(aj|y)

p(aj|y)

j

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

algorithm

learning: based on the frequency counts in the dataset:
1. estimate all p(y),    y     y.
2. estimate all p(aj|y)    y     y,    ai.

classi   cation: for a new example, use:

ynew = arg max

y   y

p(y)(cid:89)

j

p(aj|y)

note: no model per se or hyperplane, just count the frequencies
of various data combinations within the training examples.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

estimating probabilities

m-estimate of the id203:
p(aj|y) =

nc + m     p
ny + m

where:
ny: total number of examples for which the class is y.
nc: total number of examples for which the class is y and feature
xj = aj.
m: called equivalent sample size

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

estimating probabilities

m-estimate of the id203:
p(aj|y) =

nc + m     p
ny + m

where:
ny: total number of examples for which the class is y.
nc: total number of examples for which the class is y and feature
xj = aj.
m: called equivalent sample size

intuition:
augment the sample size by m virtual examples, distributed ac-
cording to prior p (prior estimate of each value).

if prior is unknown, assume uniform prior: if a feature has k values,
we can set p = 1
k.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

text classi   cation

    given a document (corpus), de   ne an attribute for each word

position in the document.

    the value of the attribute is the english word in that position.
    to reduce the number of probabilities that needs to be esti-
mated, besides nb independence assumption, we assume that:
the id203 of a given word wk occurrence is independent
of the word position within the text. that is:
p(x1 = wk|cj), p(x2 = wk|cj),         

estimated by:

p(wk|cj)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

text classi   cation

    m-estimate of the probabilities:

p(wk|cj) =

nk + 1

nj + |v ocabulary|

where:
nj: total #word positions in all training examples of class cj.
nk: number of times the word wk is found in among these nj
word positions.

    the following function learns the probabilities p (wk/cj) de-
scribing the id203 that a randomly drawn word from a
document with class cj is the english word wk.
it also learn
the class priors p (cj).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

text classi   cation

learn naive bayes texte(examples, c)

input: examples is a set of document with classes. c is the set of classes.

1. collect all words, punctuations and tokens occuring in the examples. let

the pertinent vocabulary be v .

2. calculate p (cj) and p (wk/cj).

    for each class cj in c

    docsj     the subset of documents from examples for which the

label=cj

    p (cj)     |docsj|
|examples|
    textj     a single document concatenation of all documents in docj
    nj     total number of distinct word positions in textj
    for each word wk in v

    nk     number of times word wk appears in textj
    p (wk/cj)     nk+1
nj+|v |

output: all p (cj) and p (wk/cj).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

text classi   cation

classify naive bayes text(doc)

return the estimated label for the document doc. ai denotes the word found
in the ith position within doc.

    positions     all word positions in doc that contain token found in v .
    return cdoc where:

p (cj) (cid:89)

i   positions

cdoc = arg max

cj   c

p (ai/cj)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

example

classi   cation of radio and tv sentences.

tv:
1. tv programs are not interesting     tv is annoying.
2. kids like tv.
3. we receive tv by radio waves.

radio:
1. it i interesting to listen to the radio.
2. on the waves, kids programs are rare.
3. the kids listen to the radio; it is rare!
vocabulary: v={tv, program, interesting, kids, radio, wave, lis-
ten, rare}

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

example

p(ct v) = 3/6 = 0.5
nt v = 9

nradio = 11

p(cradio) = 3/6 = 0.5

w     v

tv
program
interesting
kids
radio
wave
listen
rare

nt v
9
9
9
9
9
9
9
9

p(w|ct v )

class    tv   
nw
4
1
1
1
1
1
0
0

(4+1)/(9+8)
(1+1)/(9+8)
(1+1)/(9+8)
(1+1)/(9+8)
(1+1)/(9+8)
(1+1)/(9+8)
(0+1)/(9+8)
(0+1)/(9+8)

class    radio   

nradio nw
0
1
1
2
2
1
2
2

11
11
11
11
11
11
11
11

p(w|cradio)
1/(11+8)
2/(11+8)
2/(11+8)
3/(11+8)
3/(11+8)
2/(11+8)
3/(11+8)
3/(11+8)

example: courtesy guillaume cleuziou

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

linearity of nb?

is naive bayes a linear separator?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

linearity of nb?

is naive bayes a linear separator?

yes!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

linearity of nb?

suppose we have a binary classi   cation case with binary features.

source: avi pfeer. naive bayes and autoclass.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

linearity of nb?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

linearity of nb?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

linearity of nb?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

conclusion

1. naive bayes is a linear classi   er.

2. incredibly simple and easy to implement.

3. works wonderful for text.

4. use it as a baseline to compare other learning algorithms.

5. nb is a simple id110 (graphical model).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

nb with missing labels

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

outlook temperaturehumiditywindyplaysunny8585falsenosunny8090truenoovercast8386falseyesrainy7096falseyesrainy6880false?rainy6570truenoovercast6465true?sunny7295falsenosunny6970false?rainy7580false?sunny7570true?overcast7290trueyesovercast8175falseyesrainy7191truenocredit
    machine learning. tom mitchell 1997.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

