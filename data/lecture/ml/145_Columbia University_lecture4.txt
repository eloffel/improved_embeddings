machine learning for

data science

id75, least

squares and gradient

descent

ansaf salleb-aouissi

coms 4721     spring 2014

outline

1. review

2. id75: history

3. id75 with least squares

4. matrix representation

5. normal equation method

6. iterative method: id119

7. pros and cons of both methods

8. practical considerations

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

1

review

review the concepts and terminology:

instance, example, feature, label, supervised learning, unsu-
pervised learning, classi   cation, regression, id91, pre-
diction, training set, validation set, test set, k-fold cross
validation, classi   cation error, id168, over   tting, un-
der   tting, occam   s razor, id173.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

2

id75: history

    a very popular technique.
    rooted in statistics.
    method of least squares used as early as

1795 by gauss.

    re-invented in 1805 by legendre.
    used in astronomy.
    still a very useful tool today.

carl friedrich gauss

credit: wikipedia

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

3

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

. . .

example x1    
example xi    
example xn    

. . .

x11 x12 . . . x1d
. . .
. . .
. . .
xi1
. . . xid
. . .
. . .
. . .
xn1 xn2 . . . xnd

. . .
xi2
. . .

. . .

y1     label
yi     label
yn     label

. . .

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

4

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

. . .

example x1    
example xi    
example xn    

. . .

x11 x12 . . . x1d
. . .
. . .
. . .
xi1
. . . xid
. . .
. . .
. . .
xn1 xn2 . . . xnd

. . .
xi2
. . .

. . .

y1     label
yi     label
yn     label

. . .

task: learn a regression function:

f : rd        r
f (x) = y

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

5

id75
given: training data: (x1, y1), . . . , (xn, yn) / xi     rd and yi     r

. . .

example x1    
example xi    
example xn    

. . .

x11 x12 . . . x1d
. . .
. . .
. . .
xi1
. . . xid
. . .
. . .
. . .
xn1 xn2 . . . xnd

. . .
xi2
. . .

. . .

y1     label
yi     label
yn     label

. . .

task: learn a regression function:

f : rd        r
f (x) = y

id75: a regression model
is said to be linear if
it is represented by a linear function (linear hyperplane in rd   1
dimensional space).

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

6

id75

id75 model:

d(cid:88)

j=1

f (x) =   0 +

  jxj with   j     r,

j     {1, . . . , d}

      s are called parameters or coe   cients or weights.
learning the linear model        learning the   (cid:48)s

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

7

id75

id75 model:

d(cid:88)

j=1

f (x) =   0 +

  jxj with   j     r,

j     {1, . . . , d}

      s are called parameters or coe   cients or weights.
learning the linear model        learning the   (cid:48)s

estimation with least squares:
use least square loss: (cid:96)oss(yi, f (xi)) = (yi     f (xi))2
we want to minimize the loss over all examples, that is minimize
the risk or cost function r:
1
2n

(yi     f (xi))2

n(cid:88)

r =

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

8

id75

d = 1, line in r2

d = 2, hyperplane is r3

credit: introduction to statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

9

050100150200250300510152025tvsales!"#"x1x2yid75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

10

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

we want to minimize:

r =

1
2n

n(cid:88)

i=1

(yi     f (xi))2

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

11

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

we want to minimize:

r =

r(  ) =

1
2n

1
2n

n(cid:88)
n(cid:88)

i=1

i=1

(yi     f (xi))2

(yi       0       1xi)2

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

12

id75

a simple case with one feature ( d = 1):

f (x) =   0 +   1x

we want to minimize:

r =

1
2n

(yi     f (xi))2

r(  ) =

1
2n

(yi       0       1xi)2

i=1
find   0 and   1 that minimize:

r(  ) =

1
2n

(yi       0       1xi)2

n(cid:88)
n(cid:88)

i=1

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

13

id75

(left) contours of   0 and   1. (right) bowl shape of the three-dimensional

plot of the r function. the red dots correspond to the least squares

estimates. credit: introduction to statistical learning.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

14

  0  1 2.15  2.2  2.3  2.5  3  3  3  3 567890.030.040.050.06rss  1  0id75

find   0 and   1 so that:

argmin  (

n(cid:88)

i=1

1
2n

(yi       0       1xi)2)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

15

id75

find   0 and   1 so that:

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

   r
     1

= 0

n(cid:88)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

16

id75

find   0 and   1 so that:

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

   r
     1

= 0

n(cid:88)

   r
     0

= 2    1
2n

(yi       0       1xi)       
     0

(yi       0       1xi)

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

17

id75

find   0 and   1 so that:

n(cid:88)

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

   r
     1

= 0

   r
     0

= 2    1
2n

   r
     0

=

n(cid:88)
(yi       0       1xi)       
     0
n(cid:88)

i=1

1
n

i=1

(yi       0       1xi)    (   1) = 0

(yi       0       1xi)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

18

id75

find   0 and   1 so that:

argmin  (

1
2n

(yi       0       1xi)2)

i=1
minimize: r(  0,   1), that is:    r
     0

= 0

n(cid:88)

   r
     0

= 2    1
2n

   r
     0

=

(yi       0       1xi)

   r
     1

= 0

n(cid:88)
(yi       0       1xi)       
     0
n(cid:88)

i=1

1
n

i=1

  0 =

1
n

(yi       0       1xi)    (   1) = 0
n(cid:88)

n(cid:88)

yi       1

1
n

xi

i=1

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

19

id75

   r
     1

= 2    1
2n

(yi       0       1xi)       
     1

(yi       0       1xi)

n(cid:88)

i=1

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

20

id75

   r
     1

= 2    1
2n

   r
     1

=

n(cid:88)
(yi       0       1xi)       
     1
n(cid:88)

i=1

1
n

i=1

(yi       0       1xi)    (   xi) = 0

(yi       0       1xi)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

21

id75

   r
     1

= 2    1
2n

   r
     1

=

(yi       0       1xi)

i=1

n(cid:88)
(yi       0       1xi)       
     1
n(cid:88)
n(cid:88)

xiyi     n(cid:88)

n(cid:88)

2 =

i=1

1
n

xi

i=1

i=1

i=1

  1

  0xi

(yi       0       1xi)    (   xi) = 0

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

22

id75

   r
     1

= 2    1
2n

   r
     1

=

(yi       0       1xi)

(yi       0       1xi)    (   xi) = 0

i=1

1
n

i=1

n(cid:88)
(yi       0       1xi)       
     1
n(cid:88)
n(cid:88)

xiyi     n(cid:88)
(cid:80)n
(cid:80)n
(cid:80)n
(cid:80) xi
(cid:80)n
(cid:80)n
i=1 yixi     1
i     1

i=1 yi
i=1 xi

i=1 x2

n(cid:88)

2 =

i=1

i=1

i=1

xi

n

n

  0xi

i=1 xi

  1

plugging   0 in   1:

  1 =

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

23

id75
d(cid:88)

with more than one feature:

f (x) =   0 +

  jxj

j=1

find the   j that minimize:

n(cid:88)

(yi       0     d(cid:88)

i=1

j=1

r =

1
2n

  jxij))2

let   s write it more elegantly with matrices!

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

24

matrix representation
let x be an n    (d + 1) matrix where each row starts with a 1
followed by a feature vector.
let y be the label vector of the training set.
let    be the vector of weights (that we want to estimate!).

                        
                        

x :=

                        

y1...
yi...
yn

y :=

...

...

1 x11          x1j
...
...
1 xi1          xij
...
...
1 xn1          xnj

...

...

         x1d
...
...
         xid
...
...
         xnd

                        
                        

                        

  0...
  j...
  d

   :=

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

25

normal equation
we want to    nd (d + 1)      s that minimize r. we write r:

r(  ) =

r(  ) =

   r
     

1
2n
=    1
n

||(y     x  )||2

1
2n
(y     x  )t (y     x  )

xt (y     x  )

we have that:

   2r
     

=    1
n

xt x

is positive de   nite which ensures that    is a minimum. we solve:

the unique solution is:

xt (y     x  ) = 0
   = (xt x)   1xt y

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

26

id119

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

27

!"#$%&'()'*$%&'('+,-./0$'12,3$'4.5'id119

id119 is an optimization method.

repeat until convergence:

update simultaneously all   j for (j = 0 and j = 1)

  0 :=   0       

  1 :=   1       

   

     0

   

     1

r(  0,   1)

r(  0,   1)

   is a learning rate.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

28

id119

in the linear case:

   r
     1
let   s generalize it!

n(cid:88)

i=1

=

1
n

(yi       0       1xi)    (   xi)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

29

id119

in the linear case:

   r
     1

=

1
n

n(cid:88)

i=1

(yi       0       1xi)    (   xi)

repeat until convergence:

update simultaneously all   j for (j = 0 and j = 1)

  0 :=   0       

  1 :=   1       

1
n

1
n

n(cid:88)
n(cid:88)

i=1

i=1

(  0 +   1xi     yi)(xi0)

(  0 +   1xi     yi)(xi1)

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

30

pros and cons

analytical approach: normal equation

+ no need to specify a convergence rate or iterate.
- works only if xtx is invertible
- very slow if d is large o(d3) to compute (xt x)   1
- treats all features equally.

iterative approach: id119

+ e   ective and e   cient even in high dimensions.

- iterative (sometimes need many iterations to converge).
- needs to choose the rate   .
- treats all features equally.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

31

practical considerations
1. scaling: bring your features to a similar scale. for the jth

feature, update each xij as follows:

xij       j

xij :=

max(xj)     min(xj)

xij :=

xij       i
stdev(xj)

2. learning rate: don   t use a rate that is too small or too large.
3. r should decrease after each iteration.
4. declare convergence if it start decreasing by less than  .
5. if xt x is not invertible?

(a) too many features as compared to the number of examples

(e.g., 50 examples and 500 features).

(b) features linearly dependent: e.g., weight in pounds/kilos.

6. how can we extend to polynomial regression?

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

32

credit

as mentioned, some of the    gures in this presentation are taken
from    an introduction to statistical learning, with applications in
r    (springer, 2013) with permission from the authors: g. james,
d. witten, t. hastie and r. tibshirani.

copyright c(cid:13)ansaf salleb-aouissi: coms 4721     machine learning for data science.

33

