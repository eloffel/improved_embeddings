cmsc 422 introduction to machine learning

lecture 2 id90

furong huang / furongh@cs.umd.edu

last week: introducing 
machine learning

what does    learning by example    mean?

classification tasks

learning requires examples + inductive bias
generalization vs. memorization

formalizing the learning problem

function approximation
learning as minimizing expected loss0poo

input

output

supervised learning

y='(!)

!   #
y   %
an item !
an item y
input space #
output space %
consider systems that apply a function '() to 
input items ! and return outputs *='(!).

drawn from an 

drawn from an 

system

supervised learning

input

system

output

y='(!)

!   #
y   %
an item !
an item y
input space #
output space %
with systems whose '(!) is learned from 

in (supervised) machine learning, we deal 

drawn from an 

drawn from an 

examples.

supervised learning

input

output

y='(!)

!   #
y   %
an item !
an item y
input space #
output space %
the function '(!) we want the system to 

we typically use machine learning when 

drawn from an 

drawn from an 

system

apply is unknown to us, and we cannot 
   think    about it.

supervised learning

input

!   #
an item !
instance space #

drawn from an 

target function

y=+(!)
y=((!)

learned model

output

y   %
an item &
label space %

drawn from a 

labeled

training data

supervised learning: training

!train
("#,%1)
("(,%2)
   
("+,%,)
given the training examples in !train
the learner returns a model -(")

learning 
algorithm

-(")

learned
model

supervised learning: testing

labeled
test data

!test
("#   ,&1   )
(")   ,&2   )
   
(",   ,&-   )

supervised learning: testing

labeled
test data

!test
("#   ,&1   )
(")   ,&2   )
   
(",   ,&-   )

test

labels/test&1   &2      &-   

raw

test data.test"#   ")      ",   

supervised learning: testing

raw

test data!test"#   "%      "'   

((")

learned
model

predicted

labels

(	(! test)
(("#   )
(("%   )   (("'   )

test

labels+test,1   ,2      ,/   

    apply the model to the raw test data
    evaluate by comparing predicted labels 

c a n   w e   u s e   t h e   t e s t   d a t a   o t h e r w i s e ?

against the test labels

learning formally

    given: examples (",$(")) of unknown function $
    find: a good approximation of $
    " provides some representation of the input 
    the target function $() (label)
"    0,1 ),"      )
$"    {   1,+1}
$"     1,2,3,   ,2   1 multi-class classification
$"       

feature extraction: the process of mapping a domain element 
into a representation.

binary classification

regression

  

  

  

  

  

learning formally - continued

    hypothesis space

   set of possible instances x={$   &}
   set of possible outputs y={y   *}
from the input to output space +=    		   :$   y,$   &,y   *}

the set of function hypotheses that provide some mapping 

  

   size of hypothesis space

machine learning 
as function approximation

    problem setting

   set of possible instancesx={$   &}
   unknown target function (:y={y   ,}
   set of function hypotheses -=    		   :$   y,$   &,y   ,}
   training examples {$2,32 ,    $5,35 } of unknown target 
function (
   hypothesis    	   -	that best approximates target function (

    output

   

input

formalizing induction:
id168

!(#,%(&)) where #	is the truth and %&	is the 
e.g.	!#,%(&)	 =	*0					,%	#=%(&)
1							./   123,41

system   s prediction

captures our notion of what is important to learn

formalizing induction:
data generating distribution
where does the data come from?

    data generating distribution

a id203 distribution ! over (#,%) pairs
    we don   t know what ! is!

we only get a random sample from it: our training data

formalizing induction:
expected loss

    ! should make good predictions
    as measured by loss "
    on future examples that are also drawn from #
$	, the expected loss of !	over # with respect to " should be 
$   '(,*~,"(.,!(/)) =	   
#/,."(.,!(/))
((,*)

    formally

small

   

formalizing induction:
training error

    we can   t compute expected loss because we don   t know what !
    we only have a sample of !
training examples {#$,&$ ,    #(,&( }

is

   

    all we can compute is the training error

*     	.101(&3,4(#3))
(
36$

formalizing induction

   
   

    given

a id168 !
a sample from some unknown data distribution "
expected error over " with respect to !.
#$,&~(!(*,+(,)) =		0",,*!(*,+(,))

    our task is to compute a function f that has low 

($,&)

today: id90

    what is a decision tree?

    how to learn a decision tree from data?

    what is the inductive bias?

    generalization?

an example training set

a decision tree
to decide whether to play tennis

id90

    representation

   each internal node tests a feature
   each branch corresponds to a feature value
   each leaf node assigns a classification

v or a id203 distribution over classifications

    id90 represent functions that map 

examples in x to classes in y

exercise

how would you represent the following 
boolean functions with id90?

!   #!   #

take home exercise: !   #    (&     ()

today: id90

    what is a decision tree?

    how to learn a decision tree from data?

    what is the inductive bias?

    generalization?

function approximation
with id90
    problem setting

   set of possible instances !
v each instance "	   ! is a feature vector "=["',   ,"*]
   unknown target function ,:!	   /
v/ is discrete valued 
   set of function hypotheses 0=    		   :!	   /}
v each hypothesis     is a decision tree
   training examples {4',5' ,    46,56 } of unknown target 
function ,
   hypothesis    	   0	that best approximates target function ,

    output

input

   

id90 learning

    finding the hypothesis    	   $
   $	 is too large for exhaustive search!

   that minimizes training error
   or maximizes training accuracy

    how? 

   we will use a heuristic search algorithm which

v picks questions to ask, in order
v such that classification accuracy is maximized

top-down induction
of id90

currentnode = root

dttrain (examples for currentnode, features at 
currentnode):

1. find f, the    best    decision feature for next 

node

node

2. for each value of f, create new descendant of 

3. sort training examples to leaf nodes
4. if training examples perfectly classified

stop
else
recursively apply dttrain over new leaf nodes 

how to select the    best    feature?

    a good feature is a feature that lets us 

make correct classification decision

    one way to do this:

   select features based on their classification 

accuracy

    let   s try it on the playtennis dataset

let   s build a decision tree 
using features o, t, h, w

partitioning examples according to 
humidity feature

partitioning examples:
h  = normal

partitioning examples:
h = normal and w = strong

id90

    can represent any boolean function
    can be viewed as a way to compactly represent a lot of 

data.  
the evaluation of the classifier is easy

   
    clearly, given data, there are many ways to represent it 

as a decision tree.
learning a good representation from data is the 
challenge.

   

will i play tennis today?

    features
   outlook:
   temperature:
   humidity:
   wind:

    labels

{sun, overcast, rain}
{hot, mild, cool}
{high, normal, low}
{strong, weak}

   binary classification task: y = {+, -}

will i play tennis today?

t
o
h
s
1
h
s
2
o
h
3
r m
4
c
r
5
r
c
6
o
c
7
s m
8
9
c
s
10 r m
11
s m
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

   outlook:

s(unny),
o(vercast),
r(ainy)

   temperature: h(ot), 
m(ild), 
c(ool)

   humidity:

   wind:

h(igh), 
n(ormal), 
l(ow)

s(trong), 
w(eak)

will i play tennis today?

   data is processed in batch 

(i.e. all the data available)

   recursively build a decision 

tree top down.

t
o
h
s
1
h
s
2
o
h
3
r m
4
c
r
5
r
c
6
o
c
7
s m
8
9
c
s
10 r m
11
s m
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

top-down induction
of id90

currentnode = root

dttrain (examples for currentnode, features at 
currentnode):

1. find f, the    best    decision feature for next 

node

node

2. for each value of f, create new descendant of 

3. sort training examples to leaf nodes
4. if training examples perfectly classified

stop
else
recursively apply dttrain over new leaf nodes 

picking the root attribute

    the goal is to have the resulting decision tree 

as small as possible (occam   s razor)
   however, finding the minimal decision tree 

consistent with the data is np-hard

    the recursive algorithm is a greedy heuristic 

search for a simple tree, but cannot guarantee 
optimality.

    the main decision in the algorithm is the 

selection of the next attribute to condition on.

picking the root attribute

training data with 2 boolean attributes (a,b)

( (a=0,b=0), -): 50 examples
( (a=0,b=1), -): 50 examples
( (a=1,b=0), -): 0 examples
( (a=1,b=1), +): 100 examples

what should be the first attribute we select?

splitting on a: we get purely labeled 
nodes

splitting on b: we don   t get purely 
labeled nodes

picking the root attribute

training data with 2 boolean attributes (a,b)

( (a=0,b=0), -): 50 examples
( (a=0,b=1), -): 50 examples
( (a=1,b=0), -): 3 examples
( (a=1,b=1), +): 100 examples

trees looks structurally similar; which attribute should we choose?

another feature selection criterion: 
id178
    used in the   algorithm [quinlan, 1963]

   pick feature with smallest id178 to split the 

examples at current iteration

    id178 measures impurity of a sample of 

examples

sample id178

high id178    
high level of uncertainty

low id178    
low level of uncertainty

information gain

    the information gain of an attribute ! is the 
"!#$%,! =($)*+,-%     / |%1|
|%|($)*+,-(%1)
1   156789(5)
   original set is %.	
	%1	 is the subset of % for which attribute ! has value <. 

expected reduction in id178 caused by 
partitioning on this attribute

  

  

  

the id178 of partitioning the data is calculated by weighing the 
id178 of each partition by its size relative to the original set.

v

partitions of low id178 (imbalanced splits) lead to high gain

take home exercise: go back and check which of the a, b splits is 
better. 

will i play tennis today?

t
o
h
s
1
h
s
2
o
h
3
r m
4
c
r
5
r
c
6
o
c
7
s m
8
9
c
s
10 r m
11
s m
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

   outlook:

s(unny),
o(vercast),
r(ainy)

   temperature: h(ot), 
m(ild), 
c(ool)

   humidity:

   wind:

h(igh), 
n(ormal), 
l(ow)

s(trong), 
w(eak)

will i play tennis today?

   current id178:

p = 9/14
n =5/14

   h(y)=

   0.94

-(9/14) log2(9/14)
-(5/14) log2(5/14)

t
o
h
s
1
h
s
2
o
h
3
r m
4
c
r
5
r
c
6
o
c
7
s m
8
9
c
s
10 r m
11
s m
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

will i play tennis today?

t
o
s
h
1
s
h
2
o
h
3
r m
4
r
c
5
r
c
6
o
c
7
s m
8
s
9
c
10 r m
s m
11
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

   outlook = sunny:

p=2/5

n =3/5  hs=0.971

   outlook =overcast:

p=4/4  n =0 

ho=0

   outlook = rainy:

   expected id178

p=3/5  n =2/5  hr=0.971

514   0.971+ 414   0
+ 514   0.971=+.,-.

   information gain:

0.940-0.694 = 0.246

will i play tennis today?

t
o
h
s
1
h
s
2
o
h
3
r m
4
c
r
5
r
c
6
o
c
7
s m
8
9
c
s
10 r m
11
s m
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
n
s
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

   humidity = sunny:

p=3/7

n =4/7  hh=0.985

   humidity =overcast:

   expected id178

p=6/7  n =1/7  ho=0.592

714   0.985+ 714   0.592
=-.../0

   information gain:

0.940-0.7785 = 0.1515

which feature to split on?

   information gain:

outlook: 0.246
humidity: 0.151
wind: 0.048
temperature: 0.029

   split on outlook

t
o
s
h
1
s
h
2
o
h
3
r m
4
r
c
5
r
c
6
o
c
7
s m
8
s
9
c
10 r m
s m
11
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

an illustrative example (iii)

t
o
s
h
1
s
h
2
o
h
3
r m
4
r
c
5
r
c
6
o
c
7
s m
8
s
9
c
10 r m
s m
11
12 o m
13 o
h
14 r m

h w play?
h w
h
s
h w
h w
n w
n
s
n
s
h w
n w
n w
s
n
h
s
n w
h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

continue until:
    every attribute is included in path, or,
    all examples in the leaf have same label

an illustrative example (iv)

an illustrative example (v)

2. find the feature with the most information gain:

inducedecisiontree(s)

1. does ! uniquely define a class?
if all "   ! have the same label y: return!;
3. add children to !	:
%=argmax,-.%/(!,2,)
for5 in values(2,):
!6= "   !2,=5
addchild(!	,!6)
inducedecisiontree(!6)

return s;

an illustrative example (vi)

hypothesis space in decision tree 
induction
    conduct a search of the space of decision 

trees which can represent all possible 
discrete functions. (pros and cons)
    goal: to find the best decision tree 
    finding a minimal decision tree consistent 

with a set of data is np-hard.

    performs a greedy heuristic search: hill 

climbing without backtracking.

    makes statistically based decisions using 

all data.

a decision tree to distinguish homes in 
new york from homes in san francisco

take a look at home:
http://www.r2d3.us/visual-intro-to-machine-learning-part-1/

vcheck out course webpage, canvas, piazza

vsubmit hw01

qdue thursday 10:30am

vdo the readings!

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

