csc321 lecture 16: learning long-term dependencies

roger grosse

roger grosse

csc321 lecture 16: learning long-term dependencies

1 / 1

overview

yesterday, we saw how to compute the id119 update for an
id56 using backprop through time.

the updates are mathematically correct, but unless we   re very careful,
id119 completely fails because the gradients explode or
vanish.

the problem is, it   s hard to learn dependencies over long time
windows.

today   s lecture is about what causes exploding and vanishing
gradients, and how to deal with them. or, equivalently, how to learn
long-term dependencies.

roger grosse

csc321 lecture 16: learning long-term dependencies

2 / 1

why gradients explode or vanish

recall the id56 for machine translation. it reads an entire english
sentence, and then has to output its french translation.

a typical sentence length is 20 words. this means there   s a gap of 20
time steps between when it sees information and when it needs it.

the derivatives need to travel over this entire pathway.

roger grosse

csc321 lecture 16: learning long-term dependencies

3 / 1

why gradients explode or vanish

recall: backprop through time

activations:

l = 1
y (t) = l    l

   y (t)
(cid:48)

r (t) = y (t)   

(r (t))

h(t) = r (t) v + z (t+1) w

z (t) = h(t)   

(cid:48)

(z (t))

parameters:

u =

v =

w =

(cid:88)
(cid:88)
(cid:88)

t

t

z (t) x (t)

r (t) h(t)

z (t+1) h(t)

roger grosse

csc321 lecture 16: learning long-term dependencies

4 / 1

t

why gradients explode or vanish

consider a univariate version of the encoder network:

backprop updates:

h(t) = z (t+1) w
z (t) = h(t)   (cid:48)(z (t))

applying this recursively:

h(1) = w t   1  (cid:48)(z (2))         (cid:48)(z (t ))

(cid:123)(cid:122)

(cid:124)

the jacobian    h(t )/   h(1)

with linear activations:

   h(t )/   h(1) = w t   1

exploding:

w = 1.1, t = 50        h(t )

   h(1) = 117.4

vanishing:

w = 0.9, t = 50        h(t )

   h(1) = 0.00515

h(t )

(cid:125)

roger grosse

csc321 lecture 16: learning long-term dependencies

5 / 1

why gradients explode or vanish

more generally, in the multivariate case, the jacobians multiply:

   h(t )
   h(1)

=

   h(t )
   h(t   1)

          h(2)
   h(1)

matrices can explode or vanish just like scalar values, though it   s
slightly harder to make precise.
contrast this with the forward pass:

the forward pass has nonlinear id180 which squash the
activations, preventing them from blowing up.
the backward pass is linear, so it   s hard to keep things stable. there   s
a thin line between exploding and vanishing.

roger grosse

csc321 lecture 16: learning long-term dependencies

6 / 1

why gradients explode or vanish

we just looked at exploding/vanishing gradients in terms of the
mechanics of backprop. now let   s think about it conceptually.
the jacobian    h(t )/   h(1) means, how much does h(t ) change when
you change h(1)?
each hidden layer computes some function of the previous hiddens
and the current input:

h(t) = f (h(t   1), x(t))

this function gets iterated:

h(4) = f (f (f (h(1), x(2)), x(3)), x(4)).

let   s study iterated functions as a way of understanding what id56s
are computing.

roger grosse

csc321 lecture 16: learning long-term dependencies

7 / 1

iterated functions

iterated functions are complicated. consider:

f (x) = 3.5 x (1     x)

roger grosse

csc321 lecture 16: learning long-term dependencies

8 / 1

iterated functions

an aside:

remember the mandelbrot set? that   s based on an iterated
quadratic map over the complex plane:

zn = z 2

n   1 + c

the set consists of the values of c for which the iterates stay bounded.

cc by-sa 3.0, https://commons.wikimedia.org/w/index.php?curid=321973

roger grosse

csc321 lecture 16: learning long-term dependencies

9 / 1

iterated functions

consider the following iterated function:

we can determine the behavior of repeated iterations visually:

xt+1 = x 2

t + 0.15.

the behavior of the system can be summarized with a phase plot:

roger grosse

csc321 lecture 16: learning long-term dependencies

10 / 1

iterated functions

some observations:

fixed points of f correspond to points where f crosses the line xt+1 = xt.
fixed points with f (cid:48)(xt) > 1 correspond to sources.
fixed points with f (cid:48)(xt) < 1 correspond to sinks.

roger grosse

csc321 lecture 16: learning long-term dependencies

11 / 1

why gradients explode or vanish

let   s imagine an id56   s behavior as a dynamical system, which has
various attractors:

within one of the colored regions, the gradients vanish because even
if you move a little, you still wind up at the same attractor.

if you   re on the boundary, the gradient blows up because moving
slightly moves you from one attractor to the other.

    geo   rey hinton, coursera

roger grosse

csc321 lecture 16: learning long-term dependencies

12 / 1

why gradients explode or vanish

consider an id56 with tanh activation function:

the function computed by the network:

roger grosse

csc321 lecture 16: learning long-term dependencies

13 / 1

why gradients explode or vanish

cli   s make it hard to estimate the true cost gradient. here are the
loss and cost functions with respect to the bias parameter for the
hidden units:

generally, the gradients will explode on some inputs and vanish on
others. in expectation, the cost may be fairly smooth.

roger grosse

csc321 lecture 16: learning long-term dependencies

14 / 1

keeping things stable

one simple solution: gradient clipping
clip the gradient g so that it has a norm of at most   :

if (cid:107)g(cid:107) >   :

g       g
(cid:107)g(cid:107)

the gradients are biased, but at least they don   t blow up.

roger grosse

csc321 lecture 16: learning long-term dependencies

15 / 1

    goodfellow et al., deep learning

keeping things stable

another trick: reverse the input sequence.

this way, there   s only one time step between the    rst word of the
input and the    rst word of the output.

the network can    rst learn short-term dependencies between early
words in the sentence, and then long-term dependencies between later
words.

roger grosse

csc321 lecture 16: learning long-term dependencies

16 / 1

keeping things stable

really, we   re better o    redesigning the architecture, since the
exploding/vanishing problem highlights a conceptual problem with
vanilla id56s.
the hidden units are a kind of memory. therefore, their default
behavior should be to keep their previous value.

i.e., the function at each time step should be close to the identity
function.
it   s hard to implement the identity function if the activation function is
nonlinear!

if the function is close to the identity, the gradient computations are
stable.

the jacobians    h(t+1)/   h(t) are close to the identity matrix, so we can
multiply them together and things don   t blow up.

roger grosse

csc321 lecture 16: learning long-term dependencies

17 / 1

keeping things stable

identity id56s

use the relu activation function
initialize all the weight matrices to the identity matrix

negative activations are clipped to zero, but for positive activations,
units simply retain their value in the absence of inputs.

this allows learning much longer-term dependencies than vanilla
id56s.

it was able to learn to classify mnist digits, input as sequence one
pixel at a time!

le et al., 2015. a simple way to initialize
recurrent networks of recti   ed linear units.

roger grosse

csc321 lecture 16: learning long-term dependencies

18 / 1

long-term short term memory

another architecture which makes it easy to remember information
over long time periods is called long-term short term memory
(lstm)

what   s with the name? the idea is that a network   s activations are its
short-term memory and its weights are its long-term memory.
the lstm architecture wants the short-term memory to last for a long
time period.

it   s composed of memory cells which have controllers saying when to
store or forget information.

roger grosse

csc321 lecture 16: learning long-term dependencies

19 / 1

long-term short term memory

replace each single unit in an id56 by a memory block -

ct+1 = ct    forget gate + new input    input gate

i = 0, f = 1     remember the previous
value
i = 1, f = 1     add to the previous value
i = 0, f = 0     erase the value
i = 1, f = 0     overwrite the value

setting i = 0, f = 1 gives the reasonable
   default    behavior of just remembering things.

roger grosse

csc321 lecture 16: learning long-term dependencies

20 / 1

long-term short term memory

in each step, we have a vector of memory cells c, a vector of hidden
units h, and vectors of input, output, and forget gates i, o, and f.
there   s a full set of connections from all the inputs and hiddens to
the input and all of the gates:

             it

ft
ot
gt

             =

               

  
  

             w

tanh

(cid:18) yt

(cid:19)

ht   1

ct = ft     ct   1 + it     gt
ht = ot     tanh(ct)

exercise: show that if ft+1 = 1, it+1 = 0, and ot = 0, the gradients
for the memory cell get passed through unmodi   ed, i.e.

ct = ct+1.

roger grosse

csc321 lecture 16: learning long-term dependencies

21 / 1

long-term short term memory

sound complicated? ml researchers thought so, so lstms were
hardly used for about a decade after they were proposed.

in 2013 and 2014, researchers used them to get impressive results on
challenging and important problems like id103 and
machine translation.

since then, they   ve been one of the most widely used id56
architectures.

there have been many attempts to simplify the architecture, but
nothing was conclusively shown to be simpler and better.

you never have to think about the complexity, since frameworks like
tensorflow provide nice black box implementations.

roger grosse

csc321 lecture 16: learning long-term dependencies

22 / 1

long-term short term memory

visualizations:

http://karpathy.github.io/2015/05/21/id56-effectiveness/

roger grosse

csc321 lecture 16: learning long-term dependencies

23 / 1

