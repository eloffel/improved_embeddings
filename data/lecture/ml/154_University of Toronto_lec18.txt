csc321 lecture 18: learning probabilistic models

roger grosse

roger grosse

csc321 lecture 18: learning probabilistic models

1 / 25

overview

so far in this course: mainly supervised learning
id38 was our one unsupervised task; we broke it down
into a series of prediction tasks

this was an example of distribution estimation: we   d like to learn a
distribution which looks as much as possible like the input data.

this lecture: basic concepts in probabilistic modeling

this will be review if you   ve taken 411.

following two lectures: more recent approaches to unsupervised
learning

roger grosse

csc321 lecture 18: learning probabilistic models

2 / 25

maximum likelihood

we already used maximum likelihood in this course for training
language models. let   s cover it in a bit more generality.
motivating example: estimating the parameter of a biased coin

you    ip a coin 100 times. it lands heads nh = 55 times and tails
nt = 45 times.
what is the id203 it will come up heads if we    ip again?
model:    ips are independent bernoulli random variables with
parameter   .

assume the observations are independent and identically distributed
(i.i.d.)

roger grosse

csc321 lecture 18: learning probabilistic models

3 / 25

maximum likelihood

the likelihood function is the id203 of the observed data, as a
function of   .

in our case, it   s the id203 of a particular sequence of h   s and t   s.

under the bernoulli model with i.i.d. observations,
l(  ) = p(d) =   nh (1       )nt

this takes very small values (in this case,
l(0.5) = 0.5100     7.9    10   31)
therefore, we usually work with log-likelihoods:

(cid:96)(  ) = log l(  ) = nh log    + nt log(1       )

here, (cid:96)(0.5) = log 0.5100 = 100 log 0.5 =    69.31

roger grosse

csc321 lecture 18: learning probabilistic models

4 / 25

maximum likelihood

nh = 55, nt = 45

roger grosse

csc321 lecture 18: learning probabilistic models

5 / 25

maximum likelihood

good values of    should assign high id203 to the observed data.
this motivates the maximum likelihood criterion.

remember how we found the optimal solution to id75 by
setting derivatives to zero? we can do that again for the coin
example.

d(cid:96)
d  

=

=

d
d  
nh
  

(nh log    + nt log(1       ))
    nt
1       

setting this to zero gives the maximum likelihood estimate:

    ml =

nh

nh + nt

,

roger grosse

csc321 lecture 18: learning probabilistic models

6 / 25

maximum likelihood

this is equivalent to minimizing cross-id178. let ti = 1 for heads
and ti = 0 for tails.
lce =

   ti log        (1     ti ) log(1       )

(cid:88)

i

=    nh log        nt log(1       )
=    (cid:96)(  )

roger grosse

csc321 lecture 18: learning probabilistic models

7 / 25

maximum likelihood

recall the gaussian, or normal,
distribution:
n (x;   ,   ) =

    (x       )2

(cid:18)

exp

(cid:19)

1   
2    

2  2

the central limit theorem says
that sums of lots of independent
random variables are approximately
gaussian.

in machine learning, we use
gaussians a lot because they make
the calculations easy.

roger grosse

csc321 lecture 18: learning probabilistic models

8 / 25

maximum likelihood

suppose we want to model the distribution of temperatures in
toronto in march, and we   ve recorded the following observations:

-2.5

-9.9

-12.1

-8.9

-6.0

-4.8

2.4

assume they   re drawn from a gaussian distribution with known
standard deviation    = 5, and we want to    nd the mean   .
log-likelihood function:

(cid:96)(  ) = log

(cid:34)
(cid:34)

i=1

n(cid:89)
n(cid:88)
n(cid:88)

i=1

log

    1
2

(cid:124)

i=1

=

=

1   
2        

exp

1   
2        

exp

log 2       log   

(cid:123)(cid:122)

constant!

(cid:33)(cid:35)
(cid:33)(cid:35)

2  2

(cid:32)
    (x (i)       )2
(cid:32)
    (x (i)       )2
    (x (i)       )2

2  2

2  2

(cid:125)

roger grosse

csc321 lecture 18: learning probabilistic models

9 / 25

maximum likelihood

maximize the log-likelihood by setting the derivative to zero:

0 =

d(cid:96)
d  

=     1
2  2

(x (i)       )2

d
d  

n(cid:88)
n(cid:88)

i=1

i=1

x (i)       

=

1
  2

(cid:80)n

solving we get    = 1
n
this is just the mean of the observed values, or the empirical mean.

i=1 x (i)

roger grosse

csc321 lecture 18: learning probabilistic models

10 / 25

maximum likelihood

in general, we don   t know the true standard deviation   , but we can
solve for it as well.

set the partial derivatives to zero, just like in id75.

0 =

0 =

   (cid:96)
     

   (cid:96)
     

x (i)       

    1
2

log 2       log        1
2  2

(x (i)       )2

(cid:35)

    1
2

   
     

log 2          
     

log           
     

1
2  

(x (i)       )2

n(cid:88)
(cid:34) n(cid:88)

i=1

i=1

=     1
  2

=

=

=

   
     

n(cid:88)
n(cid:88)

i=1

i=1

0     1
  

(x (i)       )2

=     n
  

+

1
  3

(x (i)       )2

1
  3

+

n(cid:88)

i=1

x (i)

1
n

n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1
n(cid:88)

i=1

n

i=1

(x (i)       )2

    ml =

    ml =

roger grosse

csc321 lecture 18: learning probabilistic models

11 / 25

maximum likelihood

so far, maximum likelihood has told us to use empirical counts or
statistics:

bernoulli:    = nh
gaussian:    = 1
n

nh +nt

(cid:80) x (i),   2 = 1

(cid:80)(x (i)       )2

n

this doesn   t always happen; e.g. for the neural language model, there
was no closed form, and we needed to use id119.

but these simple examples are still very useful for thinking about
maximum likelihood.

roger grosse

csc321 lecture 18: learning probabilistic models

12 / 25

data sparsity

maximum likelihood has a pitfall: if you have too little data, it can
over   t.

e.g., what if you    ip the coin twice and get h both times?

  ml =

nh

nh + nt

=

2

2 + 0

= 1

because it never observed t, it assigns this outcome id203 0.
this problem is known as data sparsity.
if you observe a single t in the test set, the likelihood is       .

roger grosse

csc321 lecture 18: learning probabilistic models

13 / 25

(the rest of this lecture is optional)

roger grosse

csc321 lecture 18: learning probabilistic models

14 / 25

bayesian parameter estimation (optional)

in maximum likelihood, the observations are treated as random
variables, but the parameters are not.

the bayesian approach treats the parameters as random variables as
well.
to de   ne a bayesian model, we need to specify two distributions:

the prior distribution p(  ), which encodes our beliefs about the
parameters before we observe the data
the likelihood p(d |   ), same as in maximum likelihood

when we update our beliefs based on the observations, we compute
the posterior distribution using bayes    rule:

p(   |d) =

(cid:82) p(  (cid:48))p(d |   (cid:48)) d  (cid:48) .

p(  )p(d |   )

we rarely ever compute the denominator explicitly.

roger grosse

csc321 lecture 18: learning probabilistic models

15 / 25

bayesian parameter estimation (optional)

let   s revisit the coin example. we already know the likelihood:

l(  ) = p(d) =   nh (1       )nt

it remains to specify the prior p(  ).

we can choose an uninformative prior, which assumes as little as
possible. a reasonable choice is the uniform prior.
but our experience tells us 0.5 is more likely than 0.99. one
particularly useful prior that lets us specify this is the beta distribution:

p(  ; a, b) =

  (a + b)
  (a)  (b)

  a   1(1       )b   1.

this notation for proportionality lets us ignore the id172
constant:

p(  ; a, b)       a   1(1       )b   1.

roger grosse

csc321 lecture 18: learning probabilistic models

16 / 25

bayesian parameter estimation (optional)

beta distribution for various values of a, b:

some observations:

the expectation e[  ] = a/(a + b).
the distribution gets more peaked when a and b are large.
the uniform distribution is the special case where a = b = 1.

the main thing the beta distribution is used for is as a prior for the bernoulli
distribution.

roger grosse

csc321 lecture 18: learning probabilistic models

17 / 25

bayesian parameter estimation (optional)

computing the posterior distribution:
p(   |d)     p(  )p(d |   )

   (cid:104)
  a   1(1       )b   1(cid:105)(cid:104)

  nh (1       )nt

(cid:105)

=   a   1+nh (1       )b   1+nt .

this is just a beta distribution with parameters nh + a and nt + b.
the posterior expectation of    is:

e[   |d] =

nh + a

nh + nt + a + b

the parameters a and b of the prior can be thought of as
pseudo-counts.

the reason this works is that the prior and likelihood have the same
functional form. this phenomenon is known as conjugacy, and it   s very
useful.

roger grosse

csc321 lecture 18: learning probabilistic models

18 / 25

bayesian parameter estimation (optional)

bayesian id136 for the coin    ip example:

small data setting
nh = 2, nt = 0

large data setting
nh = 55, nt = 45

when you have enough observations, the data overwhelm the prior.

roger grosse

csc321 lecture 18: learning probabilistic models

19 / 25

bayesian parameter estimation (optional)

what do we actually do with the posterior?
the posterior predictive distribution is the distribution over future
observables given the past observations. we compute this by
marginalizing out the parameter(s):

(cid:90)

p(d(cid:48) |d) =

p(   |d)p(d(cid:48) |   ) d  .

(1)

for the coin    ip example:

  pred = pr(x

(cid:48)

= h |d)
p(   |d)pr(x

(cid:48)

= h |   ) d  

(cid:90)
(cid:90)

=

beta(  ; nh + a, nt + b)       d  

=
= ebeta(  ;nh +a,nt +b)[  ]

=

nh + a

nh + nt + a + b

,

roger grosse

csc321 lecture 18: learning probabilistic models

(2)

20 / 25

bayesian parameter estimation (optional)

bayesian estimation of the mean temperature in toronto

assume observations are
i.i.d. gaussian with known
standard deviation    and
unknown mean   

broad gaussian prior over   ,
centered at 0

we can compute the posterior
and posterior predictive
distributions analytically (full
derivation in notes)

why is the posterior predictive
distribution more spread out than
the posterior distribution?

roger grosse

csc321 lecture 18: learning probabilistic models

21 / 25

bayesian parameter estimation (optional)

comparison of maximum likelihood and bayesian parameter estimation

the bayesian approach deals better with data sparsity
maximum likelihood is an optimization problem, while bayesian
parameter estimation is an integration problem

this means maximum likelihood is much easier in practice, since we
can just do id119
automatic di   erentiation packages make it really easy to compute
gradients
there aren   t any comparable black-box tools for bayesian parameter
estimation (although stan can do quite a lot)

roger grosse

csc321 lecture 18: learning probabilistic models

22 / 25

maximum a-posteriori estimation (optional)

maximum a-posteriori (map) estimation:    nd the most likely
parameter settings under the posterior

this converts the bayesian parameter estimation problem into a
maximization problem

    map = arg max
  

= arg max

  

= arg max

  

= arg max

  

p(   |d)
p(  ,d)
p(  ) p(d |   )
log p(  ) + log p(d |   )

roger grosse

csc321 lecture 18: learning probabilistic models

23 / 25

maximum a-posteriori estimation (optional)

joint id203 in the coin    ip example:
log p(  ,d) = log p(  ) + log p(d |   )

= const + (a     1) log    + (b     1) log(1       ) + nh log    + nt log(1       )
= const + (nh + a     1) log    + (nt + b     1) log(1       )

maximize by    nding a critical point

0 =

d
d  

log p(  ,d) =

nh + a     1

  

    nt + b     1

1       

solving for   ,

    map =

nh + a     1

nh + nt + a + b     2

roger grosse

csc321 lecture 18: learning probabilistic models

24 / 25

maximum a-posteriori estimation (optional)

comparison of estimates in the coin    ip example:

formula

nh = 2, nt = 0

nh = 55, nt = 45

    ml

  pred
    map

nh

nh +nt
nh +a
nh +a   1

nh +nt +a+b
nh +nt +a+b   2

1

4

6     0.67
3
4 = 0.75

55
100 = 0.55
104     0.548
57
102     0.549

56

    map assigns nonzero probabilities as long as a, b > 1.

roger grosse

csc321 lecture 18: learning probabilistic models

25 / 25

maximum a-posteriori estimation (optional)

comparison of predictions in the toronto temperatures example

1 observation

7 observations

roger grosse

csc321 lecture 18: learning probabilistic models

26 / 25

