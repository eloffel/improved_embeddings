csc321 lecture 23: go

roger grosse

roger grosse

csc321 lecture 23: go

1 / 21

final exam

friday, april 20, 9am-noon

last names a   y: clara benson building (bn) 2n
last names z: clara benson building (bn) 2s

covers all lectures, tutorials, homeworks, and programming
assignments

1/3 from the    rst half, 2/3 from the second half
if there   s a question on lectures 22 or 23, it will be easy

emphasis on concepts covered in multiple of the above

similar in format and di   culty to the midterm, but about 3x longer

practice exams are posted

roger grosse

csc321 lecture 23: go

2 / 21

overview

most of the problem domains we   ve discussed so far were natural
application areas for deep learning (e.g. vision, language)

we know they can be done on a neural architecture (i.e. the human
brain)
the predictions are inherently ambiguous, so we need to    nd statistical
structure

board games are a classic ai domain which relied heavily on
sophisticated search techniques with a little bit of machine learning
full observations, deterministic environment     why would we need
uncertainty?

this lecture is about alphago, deepmind   s go playing system which
took the world by storm in 2016 by defeating the human go
champion lee sedol

combines ideas from our last two lectures (policy gradient and value
function learning)

roger grosse

csc321 lecture 23: go

3 / 21

overview

some milestones in computer game playing:

1949     claude shannon proposes the idea of game tree search,
explaining how games could be solved algorithmically in principle

1951     alan turing writes a chess program that he executes by hand

1956     arthur samuel writes a program that plays checkers better
than he does

1968     an algorithm defeats human novices at go

...silence...

1992     td-gammon plays backgammon competitively with the best
human players

1996     chinook wins the us national checkers championship

1997     deepblue defeats world chess champion garry kasparov

after chess, go was humanity   s last stand

roger grosse

csc321 lecture 23: go

4 / 21

go

played on a 19    19 board
two players, black and white, each place one stone per turn

capture opponent   s stones by surrounding them

roger grosse

csc321 lecture 23: go

5 / 21

go

goal is to control as much territory as possible:

roger grosse

csc321 lecture 23: go

6 / 21

go

what makes go so challenging:

hundreds of legal moves from any position, many of which are
plausible

games can last hundreds of moves

unlike chess, endgames are too complicated to solve exactly
(endgames had been a major strength of computer players for games
like chess)

heavily dependent on pattern recognition

roger grosse

csc321 lecture 23: go

7 / 21

game trees

each node corresponds to a legal state of the game.

the children of a node correspond to possible actions taken by a player.

leaf nodes are ones where we can compute the value since a win/draw
condition was met

https://www.cs.cmu.edu/~adamchik/15-121/lectures/game%20trees/game%20trees.html

roger grosse

csc321 lecture 23: go

8 / 21

game trees

to label the internal nodes, take the max over the children if it   s
player 1   s turn, min over the children if it   s player 2   s turn

https://www.cs.cmu.edu/~adamchik/15-121/lectures/game%20trees/game%20trees.html

roger grosse

csc321 lecture 23: go

9 / 21

game trees

as claude shannon pointed out in 1949, for games with    nite
numbers of states, you can solve them in principle by drawing out the
whole game tree.
ways to deal with the exponential blowup

search to some    xed depth, and then estimate the value using an
evaluation function
prioritize exploring the most promising actions for each player
(according to the evaluation function)

having a good evaluation function is key to good performance

traditionally, this was the main application of machine learning to
game playing
for programs like deep blue, the evaluation function would be a
learned linear function of carefully hand-designed features

roger grosse

csc321 lecture 23: go

10 / 21

id169

in 2006, computer go was revolutionized by a technique called monte
carlo tree search.

estimate the value of a position by simulating lots of rollouts,
i.e. games played randomly using a quick-and-dirty policy
keep track of number of wins and losses for each node in the tree
key question: how to select which parts of the tree to evaluate?

silver et al., 2016

roger grosse

csc321 lecture 23: go

11 / 21

id169

the selection step determines which part of the game tree to spend
computational resources on simulating.
this is an instance of the exploration-exploitation tradeo    from last
lecture

want to focus on good actions for the current player
but want to explore parts of the tree we   re still uncertain about

uniform con   dence bound (ucb) is a common heuristic; choose the
node which has the largest frequentist upper con   dence bound on its
value:

(cid:115)

  i +

2 log n

ni

  i = fraction of wins for action i, ni = number of times we   ve tried
action i, n = total times we   ve visited this node

roger grosse

csc321 lecture 23: go

12 / 21

id169

improvement of computer go since mcts (plot is within the amateur range)

roger grosse

csc321 lecture 23: go

13 / 21

now for deepmind   s computer go player, alphago...

roger grosse

csc321 lecture 23: go

14 / 21

predicting expert moves

can a computer play go without any search?
ilya sutskever   s argument: experts players can identify a set of good moves in half
a second

this is only enough time for information to propagate forward through the
visual system     not enough time for complex reasoning
therefore, it ought to be possible for a conv net to identify good moves

roger grosse

csc321 lecture 23: go

15 / 21

predicting expert moves

can a computer play go without any search?
ilya sutskever   s argument: experts players can identify a set of good moves in half
a second

this is only enough time for information to propagate forward through the
visual system     not enough time for complex reasoning
therefore, it ought to be possible for a conv net to identify good moves
input: a 19    19 ternary (black/white/empty) image     about half the size of
mnist!
prediction: a distribution over all (legal) next moves
training data: kgs go server, consisting of 160,000 games and 29 million
board/next-move pairs
architecture: fairly generic conv net
when playing for real, choose the highest-id203 move rather than sampling
from the distribution

roger grosse

csc321 lecture 23: go

15 / 21

predicting expert moves

can a computer play go without any search?
ilya sutskever   s argument: experts players can identify a set of good moves in half
a second

this is only enough time for information to propagate forward through the
visual system     not enough time for complex reasoning
therefore, it ought to be possible for a conv net to identify good moves
input: a 19    19 ternary (black/white/empty) image     about half the size of
mnist!
prediction: a distribution over all (legal) next moves
training data: kgs go server, consisting of 160,000 games and 29 million
board/next-move pairs
architecture: fairly generic conv net
when playing for real, choose the highest-id203 move rather than sampling
from the distribution
this network, which just predicted expert moves, could beat a fairly strong
program called gnugo 97% of the time.

this was amazing     basically all strong game players had been based on
some sort of search over the game tree

roger grosse

csc321 lecture 23: go

15 / 21

self-play and reinforce

the problem from training with expert data: there are only 160,000
games in the database. what if we over   t?
there is e   ecitvely in   nite data from self-play

have the network repeatedly play against itself as its opponent
for stability, it should also play against older versions of itself

start with the policy which samples from the predictive distribution
over expert moves

the network which computes the policy is called the policy network

reinforce algorithm: update the policy to maximize the expected
reward r at the end of the game (in this case, r = +1 for win,    1 for
loss)
if    denotes the parameters of the policy network, at is the action at
time t, and st is the state of the board, and z the rollout of the rest
of the game using the current policy

r = eat   p  (at | st )[e[r (z)| st, at]]

roger grosse

csc321 lecture 23: go

16 / 21

policy and value networks

we just saw the policy network.
but alphago also has another
network called a value network.

this network tries to predict, for a
given position, which player has the
advantage.

this is just a vanilla conv net
trained with least-squares
regression.

data comes from the board
positions and outcomes
encountered during self-play.

roger grosse

csc321 lecture 23: go

17 / 21

silver et al., 2016

policy and value networks

alphago combined the policy and value networks with monte carlo
tree search

policy network used to simulate rollouts

value network used to evaluate leaf positions

roger grosse

csc321 lecture 23: go

18 / 21

alphago timeline

summer 2014     start of the project (internship project for uoft
grad student chris maddison)
october 2015     alphago defeats european champion

first time a computer go player defeated a human professional without
handicap     previously believed to be a decade away

january 2016     publication of nature article    mastering the game
of go with deep neural networks and tree search   
march 2016     alphago defeats gradmaster lee sedol
october 2017     alphago zero far surpasses the original alphago
without training on any human data
decemter 2017     it beats the best chess programs too, for good
measure

roger grosse

csc321 lecture 23: go

19 / 21

alphago

most of the go world expected alphago to lose 5-0 (even after it had
beaten the european champion)

it won the match 4-1

some of its moves seemed bizarre to human experts, but turned out
to be really good

its one loss occurred when lee sedol played a move unlike anything in
the training data

roger grosse

csc321 lecture 23: go

20 / 21

alphago

further reading:

silver et al., 2016. mastering the game of go with deep neural
networks and tree search. nature http://www.nature.com/
nature/journal/v529/n7587/full/nature16961.html
scienti   c american: https://www.scientificamerican.com/
article/how-the-computer-beat-the-go-master/
talk by the deepmind ceo:
https://www.youtube.com/watch?v=aiwqsa_7ziq&list=
plqymg7htrazcgiymt8wvvixlwkkpnbofn&index=8

roger grosse

csc321 lecture 23: go

21 / 21

