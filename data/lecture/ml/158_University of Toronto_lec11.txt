csc321 lecture 11: convolutional networks

roger grosse

roger grosse

csc321 lecture 11: convolutional networks

1 / 35

overview

what makes vision hard?

vison needs to be robust to a lot of transformations or distortions:

change in pose/viewpoint
change in illumination
deformation
occlusion (some objects are hidden behind others)

many object categories can vary wildly in appearance (e.g. chairs)

geo    hinton:    imaging a medical database in which the age of the
patient sometimes hops to the input dimension which normally codes
for weight!   

roger grosse

csc321 lecture 11: convolutional networks

2 / 35

overview

recall we looked at some hidden layer features for classifying handwritten
digits:

this isn   t going to scale to full-sized images.

roger grosse

csc321 lecture 11: convolutional networks

3 / 35

overview

suppose we want to train a network that takes a 200    200 rgb image as
input.

1000 hidden units

densely connected

200

200

3

what is the problem with having this as the    rst layer?

too many parameters! input size = 200    200    3 = 120k.
parameters = 120k    1000 = 120 million.
what happens if the object in the image shifts a little?

roger grosse

csc321 lecture 11: convolutional networks

4 / 35

overview

the same sorts of features that are useful in analyzing one part of the
image will probably be useful for analyzing other parts as well.

e.g., edges, corners, contours, object parts

we want a neural net architecture that lets us learn a set of feature
detectors that are applied at all image locations.

roger grosse

csc321 lecture 11: convolutional networks

5 / 35

overview

so far, we   ve seen two types of layers:

fully connected layers

embedding layers (i.e. lookup tables)

di   erent layers could be stacked together to build powerful models.
let   s add another layer type: the convolution layer.

roger grosse

csc321 lecture 11: convolutional networks

6 / 35

convolution layers

fully connected layers:

each hidden unit looks at the entire image.

roger grosse

csc321 lecture 11: convolutional networks

7 / 35

convolution layers

locally connected layers:

each column of hidden units looks at a small region of the image.

roger grosse

csc321 lecture 11: convolutional networks

8 / 35

convolution layers

convolution layers:

tied weights

each column of hidden units looks at a small region of the image, and the
weights are shared between all image locations.

roger grosse

csc321 lecture 11: convolutional networks

9 / 35

going deeply convolutional

convolution layers can be stacked:

tied weights

roger grosse

csc321 lecture 11: convolutional networks

10 / 35

convolution

we   ve already been vectorizing our computations by expressing them in
terms of matrix and vector operations.

now we   ll introduce a new high-level operation, convolution. here the
motivation isn   t computational e   ciency     we   ll see more e   cient ways
to do the computations later. rather, the motivation is to get some
understanding of what convolution layers can do.

roger grosse

csc321 lecture 11: convolutional networks

11 / 35

convolution

we   ve already been vectorizing our computations by expressing them in
terms of matrix and vector operations.

now we   ll introduce a new high-level operation, convolution. here the
motivation isn   t computational e   ciency     we   ll see more e   cient ways
to do the computations later. rather, the motivation is to get some
understanding of what convolution layers can do.

let   s look at the 1-d case    rst. if a and b are two arrays,

(cid:88)

  

(a     b)t =

a   bt      .

note: indexing conventions are inconsistent. we   ll explain them in each
case.

roger grosse

csc321 lecture 11: convolutional networks

11 / 35

convolution

method 1: translate-and-scale

roger grosse

csc321 lecture 11: convolutional networks

12 / 35

convolution

method 2:    ip-and-   lter

roger grosse

csc321 lecture 11: convolutional networks

13 / 35

convolution

convolution can also be viewed as id127:

                  

1
1 1
2 1 1
2 1
2

                  

      

       2

   1
1

(2,   1, 1)     (1, 1, 2) =

aside: this is how convolution is typically implemented. (more e   cient
than the fast fourier transform (fft) for modern conv nets on gpus!)

roger grosse

csc321 lecture 11: convolutional networks

14 / 35

convolution

some properties of convolution:

commutativity

linearity

a     b = b     a

a     (  1b +   2c) =   1a     b +   2a     c

roger grosse

csc321 lecture 11: convolutional networks

15 / 35

2-d convolution

2-d convolution is de   ned analogously to 1-d convolution.

if a and b are two 2-d arrays, then:

(cid:88)

(cid:88)

astbi   s,j   t.

(a     b)ij =

s

t

roger grosse

csc321 lecture 11: convolutional networks

16 / 35

2-d convolution

method 1: translate-and-scale

roger grosse

csc321 lecture 11: convolutional networks

17 / 35

2-d convolution

method 2: flip-and-filter

roger grosse

csc321 lecture 11: convolutional networks

18 / 35

2-d convolution

the thing we convolve by is called a kernel, or    lter.

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

19 / 35

 0101410102-d convolution

the thing we convolve by is called a kernel, or    lter.

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

19 / 35

 0101410102-d convolution

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

20 / 35

 0-10-18-10-102-d convolution

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

20 / 35

 0-10-18-10-102-d convolution

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

21 / 35

 0-10-14-10-102-d convolution

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

21 / 35

 0-10-14-10-102-d convolution

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

22 / 35

 10-120-210-12-d convolution

what does this convolution kernel do?

roger grosse

csc321 lecture 11: convolutional networks

22 / 35

 10-120-210-1convolutional networks

let   s    nally turn to convolutional networks. these have two kinds of
layers: detection layers (or convolution layers), and pooling layers.

the convolution layer has a set of    lters. its output is a set of feature
maps, each one obtained by convolving the image with a    lter.

roger grosse

csc321 lecture 11: convolutional networks

23 / 35

convolution826m.d.zeilerandr.fergus(a)(b)(c)(d)fig.5.(a):1stlayerfeatureswithoutfeaturescaleclipping.notethatonefeaturedom-inates.(b):1stlayerfeaturesfromkrizhevskyetal.[18].(c):our1stlayerfeatures.thesmallerstride(2vs4)and   ltersize(7x7vs11x11)resultsinmoredistinctivefeaturesandfewer   dead   features.(d):visualizationsof2ndlayerfeaturesfromkrizhevskyetal.[18].(e):visualizationsofour2ndlayerfeatures.thesearecleaner,withnoaliasingartifactsthatarevisiblein(d).1&2).thismodel,showninfig.3,signi   cantlyoutperformsthearchitectureofkrizhevskyetal.[18],beatingtheirsinglemodelresultby1.7%(testtop-5).whenwecombinemultiplemodels,weobtainatesterrorof14.8%,animprove-mentof1.6%.thisresultisclosetothatproducedbythedata-augmentationapproachesofhoward[15],whichcouldeasilybecombinedwithourarchitec-ture.however,ourmodelissomewayshortofthewinnerofthe2013id163classi   cationcompetition[28].table1.id1632012/2013classi   cationerrorrates.the   indicatesmodelsthatweretrainedonbothid1632011and2012trainingsets.valvaltesterror%top-1top-5top-5gunjietal.[12]--26.2decaf[7]--19.2krizhevskyetal.[18],1convnet40.718.2      krizhevskyetal.[18],5convnets38.116.416.4krizhevskyetal.   [18],1convnets39.016.6      krizhevskyetal.   [18],7convnets36.715.415.3ourreplicationofkrizhevskyetal.,1convnet40.518.1      1convnetasperfig.338.416.5      5convnetsasperfig.3   (a)36.715.315.31convnetasperfig.3butwithlayers3,4,5:512,1024,512maps   (b)37.516.016.16convnets,(a)&(b)combined36.014.714.8howard[15]--13.5clarifai[28]--11.7varyingid163modelsizes:intable2,we   rstexplorethearchitectureofkrizhevskyetal.[18]byadjustingthesizeoflayers,orremovingthementirely.ineachcase,themodelistrainedfromscratchwiththerevisedarchitecture.removingthefullyconnectedlayers(6,7)onlygivesaslightincreaseinerror(inconvolutional networks

let   s    nally turn to convolutional networks. these have two kinds of
layers: detection layers (or convolution layers), and pooling layers.

the convolution layer has a set of    lters. its output is a set of feature
maps, each one obtained by convolving the image with a    lter.

example    rst-layer    lters

(zeiler and fergus, 2013, visualizing and understanding

convolutional networks)

roger grosse

csc321 lecture 11: convolutional networks

23 / 35

convolution826m.d.zeilerandr.fergus(a)(b)(c)(d)fig.5.(a):1stlayerfeatureswithoutfeaturescaleclipping.notethatonefeaturedom-inates.(b):1stlayerfeaturesfromkrizhevskyetal.[18].(c):our1stlayerfeatures.thesmallerstride(2vs4)and   ltersize(7x7vs11x11)resultsinmoredistinctivefeaturesandfewer   dead   features.(d):visualizationsof2ndlayerfeaturesfromkrizhevskyetal.[18].(e):visualizationsofour2ndlayerfeatures.thesearecleaner,withnoaliasingartifactsthatarevisiblein(d).1&2).thismodel,showninfig.3,signi   cantlyoutperformsthearchitectureofkrizhevskyetal.[18],beatingtheirsinglemodelresultby1.7%(testtop-5).whenwecombinemultiplemodels,weobtainatesterrorof14.8%,animprove-mentof1.6%.thisresultisclosetothatproducedbythedata-augmentationapproachesofhoward[15],whichcouldeasilybecombinedwithourarchitec-ture.however,ourmodelissomewayshortofthewinnerofthe2013id163classi   cationcompetition[28].table1.id1632012/2013classi   cationerrorrates.the   indicatesmodelsthatweretrainedonbothid1632011and2012trainingsets.valvaltesterror%top-1top-5top-5gunjietal.[12]--26.2decaf[7]--19.2krizhevskyetal.[18],1convnet40.718.2      krizhevskyetal.[18],5convnets38.116.416.4krizhevskyetal.   [18],1convnets39.016.6      krizhevskyetal.   [18],7convnets36.715.415.3ourreplicationofkrizhevskyetal.,1convnet40.518.1      1convnetasperfig.338.416.5      5convnetsasperfig.3   (a)36.715.315.31convnetasperfig.3butwithlayers3,4,5:512,1024,512maps   (b)37.516.016.16convnets,(a)&(b)combined36.014.714.8howard[15]--13.5clarifai[28]--11.7varyingid163modelsizes:intable2,we   rstexplorethearchitectureofkrizhevskyetal.[18]byadjustingthesizeoflayers,orremovingthementirely.ineachcase,themodelistrainedfromscratchwiththerevisedarchitecture.removingthefullyconnectedlayers(6,7)onlygivesaslightincreaseinerror(inconvolutional networks

it   s common to apply a linear recti   cation nonlinearity: yi = max(zi , 0)

why might we do this?

roger grosse

csc321 lecture 11: convolutional networks

24 / 35

convolutionlinearrecti   cationconvolution layerconvolutional networks

it   s common to apply a linear recti   cation nonlinearity: yi = max(zi , 0)

why might we do this?

convolution is a linear operation.
therefore, we need a nonlinearity,
otherwise 2 convolution layers
would be no more powerful than 1.

two edges in opposite directions
shouldn   t cancel

makes the gradients sparse, which
helps optimization (recall the
backprop exercise from lecture 6)

roger grosse

csc321 lecture 11: convolutional networks

24 / 35

convolutionlinearrecti   cationconvolution layerpooling layers

the other type of layer in a pooling layer. these layers reduce the size of
the representation and build in invariance to small transformations.

most commonly, we use max-pooling, which computes the maximum value
of the units in a pooling group:

yi =

max

j in pooling group

zj

roger grosse

csc321 lecture 11: convolutional networks

25 / 35

z1z2z3z4z5z6y1z7y2y3convolutional networks

roger grosse

csc321 lecture 11: convolutional networks

26 / 35

convolutionlinearrecti   cationmaxpoolingconvolution...convolution layerpooling layerconvolutional networks

because of pooling, higher-layer    lters can cover a larger region of the input than
equal-sized    lters in the lower layers.

roger grosse

csc321 lecture 11: convolutional networks

27 / 35

convolutionlinearrecti   cationmaxpoolingconvolution...convolution layerpooling layerequivariance and invariance

we said the network   s responses should be robust to translations of the
input. but this can mean two di   erent things.

convolution layers are equivariant: if you translate the inputs, the
outputs are translated by the same amount.

we   d like the network   s predictions to be invariant: if you translate
the inputs, the prediction should not change.

pooling layers provide invariance to small translations.

roger grosse

csc321 lecture 11: convolutional networks

28 / 35

convolution layers

each layer consists of several feature maps, each of which is an array. for
the input layer, the feature maps are usually called channels.

if the input layer represents a grayscale image, it consists of one
channel. if it represents a color image, it consists of three channels.

each unit is connected to each unit within its receptive    eld in the
previous layer. this includes all of the previous layer   s feature maps.

roger grosse

csc321 lecture 11: convolutional networks

29 / 35

convolution layers

for simplicity, focus on 1-d signals (e.g. audio waveforms). suppose the
convolution layer   s input has j feature maps and its output has i feature
maps. let t index the locations. suppose the convolution kernels have
radius r, i.e. dimension k = 2r + 1.

each unit in a convolution layer receives inputs from all the units in its
receptive    eld in the previous layer:

yi,t =

in terms of convolution,

wi,j,   xj,t+   .

r(cid:88)

   =   r

j(cid:88)
(cid:88)

j=1

j

yi =

xj        ip(wi,j ).

roger grosse

csc321 lecture 11: convolutional networks

30 / 35

backprop updates (optional)

how do we train a conv net? with backprop, of course!

recall what we need to do. backprop is a message passing procedure,
where each layer knows how to pass messages backwards through the
computation graph. let   s determine the updates for convolution layers.
we assume we are given the loss derivatives yi,t with respect to the
output units.

we need to compute the cost derivatives with respect to the input
units and with respect to the weights.

the only new feature is: how do we do backprop with tied weights?

roger grosse

csc321 lecture 11: convolutional networks

31 / 35

backprop updates (optional)

consider the computation graph for the inputs:

each input unit in   uences all the output units that have it within their
receptive    elds. using the multivariate chain rule, we need to sum
together the derivative terms for all these edges.

roger grosse

csc321 lecture 11: convolutional networks

32 / 35

backprop updates (optional)

recall the formula for the convolution layer:

j(cid:88)

r(cid:88)

   =   r

j=1

yi,t =

wi,j,   xj,t+   .

we compute the derivatives, which requires summing over all the outputs
units which have the input unit in their receptive    eld:

(cid:88)
(cid:88)

  

xj,t =

=

yi,t     

   yi,t     
   xj,t

yi,t      wi,j,  

written in terms of convolution,

  

xj = yi     wi,j .

roger grosse

csc321 lecture 11: convolutional networks

33 / 35

backprop updates (optional)

consider the computation graph for the weights:

each of the weights a   ects all the output units for the corresponding input
and output feature maps.

roger grosse

csc321 lecture 11: convolutional networks

34 / 35

backprop updates (optional)

recall the formula for the convolution layer:

j(cid:88)

r(cid:88)

   =   r

j=1

yi,t =

wi,j,   xj,t+   .

we compute the derivatives, which requires summing over all spatial
locations:

(cid:88)
(cid:88)

t

wi,j,   =

=

yi,t

   yi,t
   wi,j,  

yi,t xj,t+  

t

roger grosse

csc321 lecture 11: convolutional networks

35 / 35

