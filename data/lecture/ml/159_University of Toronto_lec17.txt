csc321 lecture 17: resnets and attention

roger grosse

roger grosse

csc321 lecture 17: resnets and attention

1 / 24

overview

two topics for today:

topic 1: deep residual networks (resnets)

this is the state-of-the art approach to object recognition.
it applies the insights of avoiding exploding/vanishing gradients to
train really deep conv nets.

topic 2: attention

machine translation: it   s hard to summarize long sentences in a single
vector, so let   s let the decoder peek at the input.
vision: have a network glance at one part of an image at a time, so
that we can understand what information it   s using
we can use attention to build di   erentiable computers (e.g. neural
turing machines)

roger grosse

csc321 lecture 17: resnets and attention

2 / 24

deep residual networks

i promised you i   d explain the best id163 object recognizer from
2015, but that it required another idea.

year model
2010 hand-designed descriptors + id166
2011 compressed fisher vectors + id166
2012 alexnet
2013
2014 googlenet
2015

a variant of alexnet

deep residual nets

top-5 error

28.2%
25.8%
16.4%
11.7%
6.6%
4.5%

that idea is exploding and vanishing gradients, and dealing with them
by making it easy to pass information directly through a network.

roger grosse

csc321 lecture 17: resnets and attention

3 / 24

deep residual networks

recall: the jacobian    h(t )/   h(1) is the product of the individual
jacobians:

   h(t )
   h(1)

=

   h(t )
   h(t   1)

          h(2)
   h(1)

but this applies to multilayer id88s and conv nets as well! (let
t index the layers rather than time.)
then how come we didn   t have to worry about exploding/vanishing
gradients until we talked about id56s?

mlps and conv nets were at most 10s of layers deep.
id56s would be run over hundreds of time steps.
this means if we want to train a really deep conv net, we need to
worry about exploding/vanishing gradients!

roger grosse

csc321 lecture 17: resnets and attention

4 / 24

deep residual networks

remember homework 3? you derived backprop for this architecture:

z = w(1)x + b(1)
h =   (z)
y = x + w(2)h

this is called a residual block, and it   s actually
pretty useful.

each layer adds something (i.e. a residual) to
the previous value, rather than producing an
entirely new value.
note: the network for f can have multiple
layers, be convolutional, etc.

roger grosse

csc321 lecture 17: resnets and attention

5 / 24

deep residual networks

we can string together a bunch of residual
blocks.
what happens if we set the parameters such
that f(x((cid:96))) = 0 in every layer?

then it passes x(1) straight through unmodi   ed!
this means it   s easy for the network to
represent the identity function.

backprop:

x((cid:96)) = x((cid:96)+1) + x((cid:96)+1)    f
(cid:19)

(cid:18)

   x

= x((cid:96)+1)

i +

   f
   x

as long as the jacobian    f/   x is small, the
derivatives are stable.

roger grosse

csc321 lecture 17: resnets and attention

6 / 24

deep residual networks

deep residual networks (resnets) consist of many layers of residual
blocks.
for vision tasks, the f functions are usually 2- or 3-layer conv nets.
performance on cifar-10, a small object recognition dataset:

for a regular convnet (left), performance declines with depth, but for
a resnet (right), it keeps improving.

roger grosse

csc321 lecture 17: resnets and attention

7 / 24

deep residual networks

a 152-layer resnet achieved 4.49% top-5 error on image net. an
ensemble of them achieved 3.57%.

previous state-of-the-art: 6.6% (googlenet)

humans: 5.1%

they were able to train resnets with more than 1000 layers, but
classi   cation performance leveled o    by 150.

what are all these layers doing? we don   t have a clear answer, but
the idea that they   re computing increasingly abstract features is
starting to sound    shy...

roger grosse

csc321 lecture 17: resnets and attention

8 / 24

attention-based machine translation

next topic: attention-based models.

remember the encoder/decoder architecture for machine translation:

the network reads a sentence and stores all the information in its
hidden units.
some sentences can be really long. can we really store all the
information in a vector of hidden units?

let   s make things easier by letting the decoder refer to the input
sentence.

roger grosse

csc321 lecture 17: resnets and attention

9 / 24

attention-based machine translation

we   ll look at the translation model from the classic paper:

bahdanau et al., id4 by jointly
learning to align and translate. iclr, 2015.

basic idea: each output word comes from one word, or a handful of
words, from the input. maybe we can learn to attend to only the
relevant ones as we produce the output.

roger grosse

csc321 lecture 17: resnets and attention

10 / 24

attention-based machine translation

the model has both an encoder and a decoder. the encoder
computes an annotation of each word in the input.
it takes the form of a bidirectional id56. this just means we have an
id56 that runs forwards and an id56 that runs backwards, and we
concantenate their hidden vectors.

the idea: information earlier or later in the sentence can help
disambiguate a word, so we need both directions.
the id56 uses an lstm-like architecture called id149.

roger grosse

csc321 lecture 17: resnets and attention

11 / 24

attention-based machine translation

the decoder network is also an id56. like the encoder/decoder translation
model, it makes predictions one word at a time, and its predictions are fed
back in as inputs.
the di   erence is that it also receives a context vector c(t) at each time step,
which is computed by attending to the inputs.

roger grosse

csc321 lecture 17: resnets and attention

12 / 24

attention-based machine translation

the context vector is computed as a weighted average of the
encoder   s annotations.

(cid:88)

j

c(i) =

  ij h(j)

the attention weights are computed as a softmax, where the inputs
depend on the annotation and the decoder   s state:

(cid:80)

exp(eij )
  ij =
j(cid:48) exp(eij(cid:48))
eij = a(s(i   1), h(j))

note that the attention function depends on the annotation vector,
rather than the position in the sentence. this means it   s a form of
content-based addressing.

my language model tells me the next word should be an adjective.
find me an adjective in the input.

roger grosse

csc321 lecture 17: resnets and attention

13 / 24

attention-based machine translation

here   s a visualization of the attention maps at each time step.

nothing forces the model to go linearly through the input sentence,
but somehow it learns to do it.

it   s not perfectly linear     e.g., french adjectives can come after the
nouns.

roger grosse

csc321 lecture 17: resnets and attention

14 / 24

attention-based machine translation

the attention-based translation model does much better than the
encoder/decoder model on long sentences.

roger grosse

csc321 lecture 17: resnets and attention

15 / 24

attention-based id134

attention can also be used to understand images.
we humans can   t process a whole visual scene at once.

the fovea of the eye gives us high-acuity vision in only a tiny region of
our    eld of view.
instead, we must integrate information from a series of glimpses.

the next few slides are based on this paper from the uoft machine
learning group:

xu et al. show, attend, and tell: neural image caption
generation with visual attention. icml, 2015.

roger grosse

csc321 lecture 17: resnets and attention

16 / 24

attention-based id134

the id134 task: take an image as input, and produce a
sentence describing the image.
encoder: a classi   cation conv net (vggnet, similar to alexnet).
this computes a bunch of feature maps over the image.
decoder: an attention-based id56, analogous to the decoder in the
translation model

in each time step, the decoder computes an attention map over the
entire image, e   ectively deciding which regions to focus on.
it receives a context vector, which is the weighted average of the conv
net features.

roger grosse

csc321 lecture 17: resnets and attention

17 / 24

attention-based id134

this lets us understand where the network is looking as it generates a
sentence.

roger grosse

csc321 lecture 17: resnets and attention

18 / 24

attention-based id134

this can also help us understand the network   s mistakes.

roger grosse

csc321 lecture 17: resnets and attention

19 / 24

id63s (optional)

we said earlier that multilayer id88s are like di   erentiable circuits.

using an attention model, we can build di   erentiable computers.

we   ve seen hints that sparsity of memory accesses can be useful:

computers have a huge memory, but they only access a handful of locations
at a time. can we make neural nets more computer-like?

roger grosse

csc321 lecture 17: resnets and attention

20 / 24

id63s (optional)

recall turing machines:

you have an in   nite tape, and a head, which transitions between various
states, and reads and writes to the tape.

   if in state a and the current symbol is 0, write a 0, transition to state b,
and move right.   

these simple machines are universal     they   re capable of doing any
computation that ordinary computers can.

roger grosse

csc321 lecture 17: resnets and attention

21 / 24

id63s (optional)

id63s are an analogue of turing machines where all of the
computations are di   erentiable.

this means we can train the parameters by doing backprop through the
entire computation.

each memory location stores a
vector.

the read and write heads interact
with a weighted average of memory
locations, just as in the attention
models.

the controller is an id56 (in
particular, an lstm) which can
issue commands to the read/write
heads.

roger grosse

csc321 lecture 17: resnets and attention

22 / 24

id63s (optional)

repeat copy task: receives a sequence of binary vectors, and has to
output several repetitions of the sequence.
pattern of memory accesses for the read and write heads:

roger grosse

csc321 lecture 17: resnets and attention

23 / 24

id63s (optional)

priority sort: receives a sequence of (key, value) pairs, and has to
output the values in sorted order by key.

sequence of memory accesses:

roger grosse

csc321 lecture 17: resnets and attention

24 / 24

