slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 16 a probabilistic view of machine learning ii

furong huang / furongh@cs.umd.edu

today   s topics

bayes rule review

a probabilistic view of machine learning

joint distributions
bayes optimal classifier

statistical estimation

maximum likelihood estimates
derive relative frequency as the solution to a 
constrained optimization problem

the bayes optimal classifier

assume we know the data generating distribution !

we define the bayes optimal classifier as

theorem: of all possible classifiers, the bayes optimal classifier 
achieves the smallest zero/one loss

bayes error rate

defined as the error rate of the bayes optimal classifier
best error rate we can ever hope to achieve under zero/one loss

we define the bayes optimal classifier as

the bayes optimal classifier

assume we know the data generating distribution !
if we had access to !, 
we don   t have access to !

bayes error rate

theorem: of all possible classifiers, the bayes optimal classifier 
achieves the smallest zero/one loss

finding an optimal classifier would be trivial!

so let   s try to estimate it instead!
defined as the error rate of the bayes optimal classifier
best error rate we can ever hope to achieve under zero/one loss

what does    training    mean in 
probabilistic settings?

    training = estimating ! from a finite training set
    we typically assume that ! comes from a specific 

family of id203 distributions
e.g., bernouilli, gaussian, etc

   
    learning means inferring parameters of that 

distributions
e.g., mean and covariance of the gaussian

   

training assumption: training 
examples are iid

   

independently and identically 
distributed

i.e. as we draw a sequence of examples from !, 

the n-th draw is independent from the previous 
n-1 sample

   

    this assumption is usually false!
    but sufficiently close to true to be useful

how can we estimate the joint id203 
distribution from data?
what are the challenges?

what we know so far   

    bayes rule

    a probabilistic view of machine learning

   

if we know the data generating distribution, we can 
define the bayes optimal classifier

    under iid assumption

    how to estimate a id203 distribution from 

data?
    id113

id113

    find the parameters that maximize the 

id203 of the data

    example: how to model a biased coin?

(on board)

maximum likelihood estimates

given a data set d of iid flips, which 

each coin flip yields a boolean value 
for x

x ~ bernouilli: !" =$!(1   $)"#!
contains )" ones and )$ zeros
!%(*)=$&'(1   $)&(
)")"+)$
+$)*+=,-./,0%!%* =

id113

    exercise: how to model a k-sided die?

(on board)

let   s learn a classifier
by learning p(y|x)

goal: learn a classifier p(y|x) 

prediction: 

predict !"=$%&'$(!)*="+=()

given an example x

parameters for p(x,y) vs. p(y|x) 

y = wealth
x = <gender, hours_worked>

joint id203 
distribution p(x,y)

conditional 

id203 distribution 

p(y|x)

how many parameters
do we need to learn?

suppose !=<!!,!",   !#>
where !$ and ' are boolean random variables
(('|!!,!",   !#)?

q:  how many parameters do we need to estimate 

a: too many to estimate p(y|x) directly from data!

na  ve bayes assumption

na  ve bayes assumes

!"#,"%,   "'( =   +,#' !("+|()
i.e., that "+ and "0 are conditionally 
independent given y, for all 1   3

conditional independence

definition:

x is conditionally independent of y given z
if p(x|y,z) = p(x|z)

recall that x is independent of y if 
p(x|y)=p(x)

na  ve bayes classifier

!" = $%&'$(!)*="+=()
= $%&'$(!)(*="))+=( *=")
% )+"=(" *=")
= $%&'$(!)(*=")."#$

bayes rule
+ conditional independence assumption 

how many parameters do we need to 
learn? 
to describe p(y)?

to describe !"=<"%,"',   ")>+)

without conditional independence assumption?

with conditional independence assumption?

(suppose all random variables are 
boolean)

training a na  ve bayes classifier

na  ve bayes wrap-up

an easy to implement classifier, that 
performs well in practice 

subtleties

often the xi are not really conditionally independent
what if the maximum likelihood estimate for 
p(xi|y) is zero?

what is the decision boundary of a na  ve 
bayes classifier?

na  ve bayes properties

na  ve bayes is a linear classifier

see ciml for example of computation of log 
likelihood ratio

choice of id203 distribution is a form 
of inductive bias

generative stories

probabilistic models tell a fictional story explaining 
how our training data was created

example of a generative story for a multiclass 
classification task with continuous features

from the generative story to the 
likelihood function

what you should know

the na  ve bayes classifier

conditional independence assumption
how to train it?
how to make predictions?
how does it relate to other classifiers we know?

fundamental machine learning concepts

iid assumption
bayes optimal classifier
id113
generative story

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

