csc321 lecture 22: id24

roger grosse

roger grosse

csc321 lecture 22: id24

1 / 21

overview

second of 3 lectures on id23
last time: policy gradient (e.g. reinforce)

optimize a policy directly, don   t represent anything about the
environment

today: id24

learn an action-value function that predicts future returns

next time: alphago uses both a policy network and a value network

this lecture is review if you   ve taken 411

this lecture has more new content than i   d intended. if there is an
exam question about this lecture or next one, it won   t be a hard
question.

roger grosse

csc321 lecture 22: id24

2 / 21

overview

agent interacts with an environment, which we treat as a black box
your rl code accesses it only through an api since it   s external to
the agent

i.e., you   re not    allowed    to inspect the transition probabilities, reward
distributions, etc.

roger grosse

csc321 lecture 22: id24

3 / 21

recap: id100

the environment is represented as a markov decision process (mdp)
m.
markov assumption: all relevant information is encapsulated in the
current state
components of an mdp:

initial state distribution p(s0)
transition distribution p(st+1 | st, at)
reward function r (st, at)
policy     (at | st) parameterized by   
assume a fully observable environment, i.e. st can be observed directly

roger grosse

csc321 lecture 22: id24

4 / 21

finite and in   nite horizon

last time:    nite horizon mdps

fixed number of steps t per episode
maximize expected return r = ep(   )[r (   )]

now: more convenient to assume in   nite horizon

we can   t sum in   nitely many rewards, so we need to discount them:
$100 a year from now is worth less than $100 today
discounted return

gt = rt +   rt+1 +   2rt+2 +       

want to choose an action to maximize expected discounted return
the parameter    < 1 is called the discount factor

small    = myopic
large    = farsighted

roger grosse

csc321 lecture 22: id24

5 / 21

value function

value function v   (s) of a state s under policy   : the expected
discounted return if we start in s and follow   

v   (s) = e[gt | st = s]

(cid:34)    (cid:88)

= e

  i rt+i | st = s

(cid:35)

i=0

computing the value function is generally impractical, but we can try
to approximate (learn) it

the bene   t is credit assignment: see directly how an action a   ects
future returns rather than wait for rollouts

roger grosse

csc321 lecture 22: id24

6 / 21

value function

rewards: -1 per time step
undiscounted (   = 1)
actions: n, e, s, w
state: current location

roger grosse

csc321 lecture 22: id24

7 / 21

value function

roger grosse

csc321 lecture 22: id24

8 / 21

action-value function

can we use a value function to choose actions?

arg max

a

r (st, a) +   ep(st+1 | st ,at )[v   (st+1)]

roger grosse

csc321 lecture 22: id24

9 / 21

action-value function

can we use a value function to choose actions?

arg max

a

r (st, a) +   ep(st+1 | st ,at )[v   (st+1)]

problem: this requires taking the expectation with respect to the
environment   s dynamics, which we don   t have direct access to!
instead learn an action-value function, or q-function: expected
returns if you take action a and then follow your policy

q   (s, a) = e[gt | st = s, at = a]

(cid:88)

relationship:

v   (s) =

  (a| s)q   (s, a)

optimal action:

a

arg max

a

q   (s, a)

roger grosse

csc321 lecture 22: id24

9 / 21

bellman equation

the bellman equation is a recursive formula for the action-value
function:

q   (s, a) = r (s, a) +   ep(s(cid:48) | s,a)   (a(cid:48) | s(cid:48))[q   (s(cid:48), a(cid:48))]

there are various bellman equations, and most rl algorithms are
based on repeatedly applying one of them.

roger grosse

csc321 lecture 22: id24

10 / 21

optimal bellman equation

the optimal policy       is the one that maximizes the expected
discounted return, and the optimal action-value function q    is the
action-value function for      .
the optimal bellman equation gives a recursive formula for q   :
a(cid:48) q   (st+1, a(cid:48))| st = s, at = a

q   (s, a) = r (s, a) +   ep(s(cid:48) | s,a)

(cid:20)

max

(cid:21)

this system of equations characterizes the optimal action-value
function. so maybe we can approximate q    by trying to solve the
optimal bellman equation!

roger grosse

csc321 lecture 22: id24

11 / 21

id24

let q be an action-value function which hopefully approximates q   .
the bellman error is the update to our expected return when we
observe the next state s(cid:48).

(cid:124)

r (st, at) +    max

q(st+1, a)

    q(st, at)

inside e in rhs of bellman eqn

(cid:125)

the bellman equation says the bellman error is 0 in expectation
id24 is an algorithm that repeatedly adjusts q to minimize the
bellman error
each time we sample consecutive states and actions (st, at, st+1):
q(st+1, a)     q(st, at)
q(st, at)     q(st, at) +   

r (st, at) +    max

(cid:123)(cid:122)

a

bellman error

(cid:105)
(cid:125)

(cid:123)(cid:122)

a

(cid:104)
(cid:124)

roger grosse

csc321 lecture 22: id24

12 / 21

exploration-exploitation tradeo   

notice: id24 only learns about the states and actions it visits.

exploration-exploitation tradeo   : the agent should sometimes pick
suboptimal actions in order to visit new states and actions.
simple solution:  -greedy policy

with id203 1      , choose the optimal action according to q
with id203  , choose a random action

believe it or not,  -greedy is still used today!

roger grosse

csc321 lecture 22: id24

13 / 21

exploration-exploitation tradeo   

you can   t use an epsilon-greedy strategy with policy gradient because
it   s an on-policy algorithm: the agent can only learn about the policy
it   s actually following.

id24 is an o   -policy algorithm: the agent can learn q regardless
of whether it   s actually following the optimal policy

hence, id24 is typically done with an  -greedy policy, or some
other policy that encourages exploration.

roger grosse

csc321 lecture 22: id24

14 / 21

id24

roger grosse

csc321 lecture 22: id24

15 / 21

function approximation

so far, we   ve been assuming a tabular representation of q: one entry
for every state/action pair.

this is impractical to store for all but the simplest problems, and
doesn   t share structure between related states.
solution: approximate q using a parameterized function, e.g.

linear function approximation: q(s, a) = w(cid:62)  (s, a)
compute q with a neural net

update q using backprop:

t     r (st, at) +    max
          +   (t     q(s, a))

a

q(st+1, a)

   q
     

roger grosse

csc321 lecture 22: id24

16 / 21

function approximation

approximating q with a neural net is a decades-old idea, but
deepmind got it to work really well on atari games in 2013 (   deep
id24   )
they used a very small network by today   s standards

main technical innovation: store experience into a replay bu   er, and
perform id24 using stored experience

gains sample e   ciency by separating environment interaction from
optimization     don   t need new experience for every sgd update!

roger grosse

csc321 lecture 22: id24

17 / 21

atari

mnih et al., nature 2015. human-level control through deep
id23

network was given raw pixels as observations

same architecture shared between all games

assume fully observable environment, even though that   s not the case
after about a day of training on a particular game, often beat
   human-level    performance (number of points within 5 minutes of
play)

did very well on reactive games, poorly on ones that require planning
(e.g. montezuma   s revenge)

https://www.youtube.com/watch?v=v1eynij0rnk

https://www.youtube.com/watch?v=4mlzncshy1q

roger grosse

csc321 lecture 22: id24

18 / 21

wireheading

if rats have a lever that causes an electrode to stimulate certain
   reward centers    in their brain, they   ll keep pressing the lever at the
expense of sleep, food, etc.

rl algorithms show this    wireheading    behavior if the reward
function isn   t designed carefully

https://blog.openai.com/faulty-reward-functions/

roger grosse

csc321 lecture 22: id24

19 / 21

policy gradient vs. id24

policy gradient and id24 use two very di   erent choices of
representation: policies and value functions

advantage of both methods: don   t need to model the environment
pros/cons of policy gradient

pro: unbiased estimate of gradient of expected return
pro: can handle a large space of actions (since you only need to sample
one)
con: high variance updates (implies poor sample e   ciency)
con: doesn   t do credit assignment

pros/cons of id24

pro: lower variance updates, more sample e   cient
pro: does credit assignment
con: biased updates since q function is approximate (drinks its own
kool-aid)
con: hard to handle many actions (since you need to take the max)

roger grosse

csc321 lecture 22: id24

20 / 21

actor-critic (optional)

actor-critic methods combine the best of both worlds

fit both a policy network (the    actor   ) and a value network (the
   critic   )
repeatedly update the value network to estimate v   
unroll for only a few steps, then compute the reinforce policy
update using the expected returns estimated by the value network

the two networks adapt to each other, much like gan training

modern version: asynchronous advantage actor-critic (a3c)

roger grosse

csc321 lecture 22: id24

21 / 21

