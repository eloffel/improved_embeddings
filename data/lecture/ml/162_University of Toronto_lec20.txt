csc321 lecture 20: reversible and

autoregressive models

roger grosse

roger grosse

csc321 lecture 20: reversible and autoregressive models

1 / 23

overview

four modern approaches to generative modeling:

id3 (last lecture)

reversible architectures (today)

autoregressive models (today)

id5 (csc412)

all four approaches have di   erent pros and cons.

roger grosse

csc321 lecture 20: reversible and autoregressive models

2 / 23

overview

remember that the gan generator network represents a distribution
by sampling from a simple distribution pz (z) over code vectors z.

i   ll use pz here to emphasize that it   s a distribution on z.

a gan was an implicit generative model, since we could only
generate samples, not evaluate the log-likelihood.

can   t tell if it   s missing modes, memorizing the training data, etc.

reversible architectures are an elegant kind of generator network with
tractable log-likelihood.

roger grosse

csc321 lecture 20: reversible and autoregressive models

3 / 23

change of variables formula

let f denote a di   erentiable, bijective mapping from space z to
space x . (i.e., it must be 1-to-1 and cover all of x .)
since f de   nes a one-to-one correspondence between values z     z
and x     x , we can think of it as a change-of-variables transformation.
change-of-variables formula from id203 theory: if x = f (z),
then

(cid:12)(cid:12)(cid:12)(cid:12)det

(cid:18)    x

   z

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)   1

px (x) = pz (z)

intuition for the jacobian term:

roger grosse

csc321 lecture 20: reversible and autoregressive models

4 / 23

change of variables formula

suppose we have a generator network which computes the function f .
it   s tempting to apply the change-of-variables formula in order to
compute the density px (x).
i.e., compute z = f    1(x)

(cid:12)(cid:12)(cid:12)(cid:12)det

(cid:18)    x

   z

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)   1

px (x) = pz (z)

problems?

the mapping f needs to be invertible, with an easy-to-compute inverse.
it needs to be di   erentiable, so that the jaobian    x/   z is de   ned.
we need to be able to compute the (log) determinant.

the gan generator may be di   erentiable, but it doesn   t satisfy the
other two properties.

roger grosse

csc321 lecture 20: reversible and autoregressive models

5 / 23

reversible blocks

now let   s de   ne a reversible block which is invertible and has a
tractable determinant.

such blocks can be composed.

inversion: f    1 = f    1

determinants: (cid:12)(cid:12)    xk

(cid:12)(cid:12) =(cid:12)(cid:12)    xk

1

               f    1

k

(cid:12)(cid:12)      (cid:12)(cid:12)    x2

   x1

   xk   1

   z

(cid:12)(cid:12)(cid:12)(cid:12)    x1

   z

(cid:12)(cid:12)

roger grosse

csc321 lecture 20: reversible and autoregressive models

6 / 23

reversible blocks

recall the residual blocks:
y = x + f(x)

reversible blocks are a variant of
residual blocks. divide the units into
two groups, x1 and x2.

y1 = x1 + f(x2)
y2 = x2

inverting a reversible block:

x2 = y2
x1 = y1     f(x2)

roger grosse

csc321 lecture 20: reversible and autoregressive models

7 / 23

reversible blocks

composition of two reversible blocks, but with x1 and x2 swapped:

forward:

backward:

y1 = x1 + f(x2)
y2 = x2 + g(y1)

x2 = y2     g(y1)
x1 = y1     f(x2)

roger grosse

csc321 lecture 20: reversible and autoregressive models

8 / 23

volume preservation

it remains to compute the log determinant of the jacobian.

the jacobian of the reversible block:

y1 = x1 + f(x2)
y2 = x2

   y
   x

=

(cid:18) i

0

(cid:19)

   f
   x2
i

this is an upper triangular matrix. the determinant of an upper
triangular matrix is the product of the diagonal entries, or in this
case, 1.

since the determinant is 1, the mapping is said to be volume
preserving.

roger grosse

csc321 lecture 20: reversible and autoregressive models

9 / 23

nonlinear independent components estimation

we   ve just de   ned the reversible block.

easy to invert by subtracting rather than adding the residual function.
the determinant of the jacobian is 1.

nonlinear independent components estimation (nice) trains a
generator network which is a composition of lots of reversible blocks.
we can compute the likelihood function using the change-of-variables
formula:

px (x) = pz (z)

= pz (z)

we can train this model using maximum likelihood. i.e., given a
dataset {x(1), . . . , x(n)}, we maximize the likelihood

n(cid:89)

px (x(i)) =

pz (f    1(x(i)))

i=1

i=1

roger grosse

csc321 lecture 20: reversible and autoregressive models

10 / 23

(cid:18)    x

   z

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)   1

(cid:12)(cid:12)(cid:12)(cid:12)det
n(cid:89)

nonlinear independent components estimation

likelihood:

px (x) = pz (z) = pz (f    1(x))

remember, pz is a simple,    xed distribution (e.g. independent
gaussians)
intuition: train the network such that f    1 maps each data point to a
high-density region of the code vector space z.

without constraints on f , it could map everything to 0, and this
likelihood objective would make no sense.
but it can   t do this because it   s volume preserving.

roger grosse

csc321 lecture 20: reversible and autoregressive models

11 / 23

nonlinear independent components estimation

roger grosse

csc321 lecture 20: reversible and autoregressive models

12 / 23

dinh et al., 2016. density estimation using realnvp.

nonlinear independent components estimation

samples produced by realnvp, a model based on nice.

dinh et al., 2016. density estimation using realnvp.

roger grosse

csc321 lecture 20: reversible and autoregressive models

13 / 23

revnets (optional)

a side bene   t of reversible blocks: you don   t need to store the
activations in memory to do backprop, since you can reverse the
computation.

i.e., compute the activations as you need them, moving backwards
through the computation graph.

notice that reversible blocks look a lot like residual blocks.
we recently designed a reversible residual network (revnet)
architecture which is like a resnet, but with reversible blocks instead
of residual blocks.

matches state-of-the-art performance on id163, but without the
memory cost of activations!
gomez et al., nips 2017.    the revesible residual network: backrpop
without storing activations   .

roger grosse

csc321 lecture 20: reversible and autoregressive models

14 / 23

overview

four modern approaches to generative modeling:

id3 (last lecture)

reversible architectures (today)

autoregressive models (today)

id5 (csc412)

all four approaches have di   erent pros and cons.

roger grosse

csc321 lecture 20: reversible and autoregressive models

15 / 23

autoregressive models

we   ve already looked at autoregressive models in this course:

neural language models
id56 language models (and decoders)

we can push this further, and generate very long sequences.

problem: training an id56 to generate these sequences requires a for
loop over > 10,000 time steps.

roger grosse

csc321 lecture 20: reversible and autoregressive models

16 / 23

causal convolution

idea 1: causal convolution

for id56 language models, we used the training sequence as both the
inputs and the outputs to the id56.

we made sure the model was causal: each prediction depended only on
inputs earlier in the sequence.

we can do the same thing using a convolutional architecture.

no for loops! processing each input sequence just requires a series of
convolution operations.

roger grosse

csc321 lecture 20: reversible and autoregressive models

17 / 23

causal convolution

causal convolution for images:

roger grosse

csc321 lecture 20: reversible and autoregressive models

18 / 23

id98 vs. id56

we can turn a causal id98 into an id56 by adding recurrent
connections. is this a good idea?

the id56 has a memory, so it can use information from all past time
steps. the id98 has a limited context.
but training the id56 is very expensive since it requires a for loop over
time steps. the id98 only requires a series of convolutions.
generating from both models is very expensive, since it requires a for
loop. (whereas generating from a gan or a reversible model is very
fast.)

roger grosse

csc321 lecture 20: reversible and autoregressive models

19 / 23

pixelid98 and pixelid56

van den oord et al., icml 2016,    pixel recurrent neural networks   
this paper introduced two autoregressive models of images: the
pixelid56 and the pixelid98. both generated amazingly good
high-resolution images.
the output is a softmax over 256 possible pixel intensities.
completing an image using an pixelid98:

roger grosse

csc321 lecture 20: reversible and autoregressive models

20 / 23

pixelid98 and pixelid56

samples from a pixelid56 trained on id163:

roger grosse

csc321 lecture 20: reversible and autoregressive models

21 / 23

dilated convolution

idea 2: dilated convolution

the advantage of id56s over id98s is that their memory lets them
learn arbitrary long-distance dependencies.

but we can dramatically increase a id98   s receptive    eld using dilated
convolution.

you did this in programming assignment 2.

roger grosse

csc321 lecture 20: reversible and autoregressive models

22 / 23

wavenet

wavenet is an autoregressive model for raw audio based on causal
dilated convolutions.

van den oord et al., 2016.    wavenet: a generative model for raw
audio   .

audio needs to be sampled at at least 16k frames per second for good
quality. so the sequences are very long.

wavenet uses dilations of 1, 2, . . . , 512, so each unit at the end of
this block as a receptive    eld of length 1024, or 64 milliseconds.

it stacks several of these blocks, so the total context length is about
300 milliseconds.
https://deepmind.com/blog/wavenet-generative-model-raw-audio/

roger grosse

csc321 lecture 20: reversible and autoregressive models

23 / 23

