csc321 lecture 15: recurrent neural networks

roger grosse

roger grosse

csc321 lecture 15: recurrent neural networks

1 / 26

overview

sometimes we   re interested in predicting sequences

speech-to-text and text-to-speech
id134
machine translation

if the input is also a sequence, this setting is known as
sequence-to-sequence prediction.
we already saw one way of doing this: neural language models

but autoregressive models are memoryless, so they can   t learn
long-distance dependencies.
recurrent neural networks (id56s) are a kind of architecture which can
remember things over time.

roger grosse

csc321 lecture 15: recurrent neural networks

2 / 26

overview

recall that we made a markov assumption:

p(wi | w1, . . . , wi   1) = p(wi | wi   3, wi   2, wi   1).

this means the model is memoryless, i.e. it has no memory of anything
before the last few words. but sometimes long-distance context can be
important.

roger grosse

csc321 lecture 15: recurrent neural networks

3 / 26

overview

autoregressive models such as the neural language model are
memoryless, so they can only use information from their immediate
context (in this    gure, context length = 1):

if we add connections between the hidden units, it becomes a
recurrent neural network (id56). having a memory lets an id56 use
longer-term dependencies:

roger grosse

csc321 lecture 15: recurrent neural networks

4 / 26

recurrent neural nets

we can think of an id56 as a dynamical system with one set of
hidden units which feed into themselves. the network   s graph would
then have self-loops.
we can unroll the id56   s graph by explicitly representing the units at
all time steps. the weights and biases are shared between all time
steps

except there is typically a separate set of biases for the    rst time step.

roger grosse

csc321 lecture 15: recurrent neural networks

5 / 26

id56 examples

now let   s look at some simple examples of id56s.

this one sums its inputs:

roger grosse

csc321 lecture 15: recurrent neural networks

6 / 26

222w=1w=1-0.51.51.5w=1w=112.52.5w=1w=113.53.5w=1w=1t=1t=2t=3t=4w=1w=1w=1inputunitlinear hiddenunitlinearoutputunitw=1w=1w=1id56 examples

this one determines if the total values of the    rst or second input are larger:

roger grosse

csc321 lecture 15: recurrent neural networks

7 / 26

inputunit1linear hiddenunitlogisticoutputunitw=5w=1w=1inputunit2w= -1241.00-2t=100.50.923.5t=21-0.70.032.2t=3example: parity

assume we have a sequence of binary inputs. we   ll consider how to
determine the parity, i.e. whether the number of 1   s is even or odd.

we can compute parity incrementally by keeping track of the parity of the
input so far:

parity bits:
input:

0 1 1 0 1 1       
0 1 0 1 1 0 1 0 1 1

each parity bit is the xor of the input and the previous parity bit.

parity is a classic example of a problem that   s hard to solve with a shallow
feed-forward net, but easy to solve with an id56.

roger grosse

csc321 lecture 15: recurrent neural networks

8 / 26

example: parity

assume we have a sequence of binary inputs. we   ll consider how to
determine the parity, i.e. whether the number of 1   s is even or odd.

let   s    nd weights and biases for the
id56 on the right so that it computes
the parity. all hidden and output units
are binary threshold units.
strategy:

the output unit tracks the current
parity, which is the xor of the
current input and previous output.
the hidden units help us compute the
xor.

roger grosse

csc321 lecture 15: recurrent neural networks

9 / 26

example: parity

unrolling the parity id56:

roger grosse

csc321 lecture 15: recurrent neural networks

10 / 26

example: parity

the output unit should compute the xor of the current input and
previous output:

y (t   1)

0
0
1
1

x (t)
0
1
0
1

y (t)
0
1
1
0

roger grosse

csc321 lecture 15: recurrent neural networks

11 / 26

example: parity

let   s use hidden units to help us compute xor.

have one unit compute and, and the other one compute or.

then we can pick weights and biases just like we did for multilayer
id88s.

y (t   1)

0
0
1
1

x (t)
0
1
0
1

h(t)
1
0
0
0
1

h(t)
2
0
1
1
1

y (t)
0
1
1
0

roger grosse

csc321 lecture 15: recurrent neural networks

12 / 26

example: parity

let   s use hidden units to help us compute xor.

have one unit compute and, and the other one compute or.

then we can pick weights and biases just like we did for multilayer
id88s.

y (t   1)

0
0
1
1

x (t)
0
1
0
1

h(t)
1
0
0
0
1

h(t)
2
0
1
1
1

y (t)
0
1
1
0

roger grosse

csc321 lecture 15: recurrent neural networks

12 / 26

example: parity

we still need to determine the hidden biases for the    rst time step.

the network should behave as if the previous output was 0. this is
represented with the following table:

x (1)
0
1

h(1)
1
0
0

h(1)
2
0
1

roger grosse

csc321 lecture 15: recurrent neural networks

13 / 26

backprop through time

as you can guess, we don   t usually set id56 weights by hand.
instead, we learn them using backprop.

in particular, we do backprop on the unrolled network. this is known
as backprop through time.

roger grosse

csc321 lecture 15: recurrent neural networks

14 / 26

backprop through time

here   s the unrolled computation graph. notice the weight sharing.

roger grosse

csc321 lecture 15: recurrent neural networks

15 / 26

backprop through time

activations:

l = 1
y (t) = l    l

   y (t)
(cid:48)

r (t) = y (t)   

(r (t))

h(t) = r (t) v + z (t+1) w

z (t) = h(t)   

(cid:48)

(z (t))

parameters:

u =

v =

w =

(cid:88)
(cid:88)
(cid:88)

t

t

z (t) x (t)

r (t) h(t)

z (t+1) h(t)

roger grosse

csc321 lecture 15: recurrent neural networks

16 / 26

t

backprop through time

now you know how to compute the derivatives using backprop
through time.

the hard part is using the derivatives in optimization. they can
explode or vanish. addressing this issue will take all of the next
lecture.

roger grosse

csc321 lecture 15: recurrent neural networks

17 / 26

id38

one way to use id56s as a language model:

as with our language model, each word is represented as an indicator vector, the
model predicts a distribution, and we can train it with cross-id178 loss.

this model can learn long-distance dependencies.

roger grosse

csc321 lecture 15: recurrent neural networks

18 / 26

id38

when we generate from the model (i.e. compute samples from its
distribution over sentences), the outputs feed back in to the network as
inputs.

at training time, the inputs are the tokens from the training set (rather
than the network   s outputs). this is called teacher forcing.

roger grosse

csc321 lecture 15: recurrent neural networks

19 / 26

some remaining challenges:

vocabularies can be very large once you include people, places, etc.
it   s computationally di   cult to predict distributions over millions of
words.

how do we deal with words we haven   t seen before?

in some languages (e.g. german), it   s hard to de   ne what should be
considered a word.

roger grosse

csc321 lecture 15: recurrent neural networks

20 / 26

id38

another approach is to model text one character at a time!

this solves the problem of what to do about previously unseen words.
note that long-term memory is essential at the character level!

note: modeling language well at the character level requires multiplicative interactions,
which we   re not going to talk about.

roger grosse

csc321 lecture 15: recurrent neural networks

21 / 26

id38

from geo    hinton   s coursera course, an example of a paragraph
generated by an id56 language model one character at a time:

j. martens and i. sutskever, 2011. learning recurrent neural networks with hessian-free optimization.

http://machinelearning.wustl.edu/mlpapers/paper_files/icml2011martens_532.pdf

roger grosse

csc321 lecture 15: recurrent neural networks

22 / 26

 he was elected president during the revolutionary war and forgave opus paul at rome. the regime of his crew of england, is now arab women's icons in  and the demons that use something between the characters    sisters in lower coil trains were always operated on the line of the ephemerable street, respectively, the graphic or other facility for deformation of a given proportion of large segments at rtus). the b every chord was a "strongly cold internal palette pour even the white blade.     id4

we   d like to translate, e.g., english to french sentences, and we have pairs
of translated sentences to train on.

what   s wrong with the following setup?

roger grosse

csc321 lecture 15: recurrent neural networks

23 / 26

id4

we   d like to translate, e.g., english to french sentences, and we have pairs
of translated sentences to train on.

what   s wrong with the following setup?

the sentences might not be the same length, and the words might
not align perfectly.
you might need to resolve ambiguities using information from later in
the sentence.

roger grosse

csc321 lecture 15: recurrent neural networks

23 / 26

id4

encoder-decoder architecture: the network    rst reads and memorizes the
sentence. when it sees the end token, it starts outputting the translation.

the encoder and decoder are two di   erent networks with di   erent weights.

learning phrase representations using id56 encoder-decoder for id151, k. cho, b. van merrienboer,
c. gulcehre, d. bahdanau, f. bougares, h. schwenk, y. bengio. emnlp 2014.

sequence to sequence learning with neural networks, ilya sutskever, oriol vinyals and quoc le, nips 2014.

roger grosse

csc321 lecture 15: recurrent neural networks

24 / 26

what can id56s compute?

in 2014, google researchers built an encoder-decoder id56 that learns to
execute simple python programs, one character at a time!

a training input with characters scrambled

example training inputs

w. zaremba and i. sutskever,    learning to execute.    http://arxiv.org/abs/1410.4615

roger grosse

csc321 lecture 15: recurrent neural networks

25 / 26

learningtoexecute(maddison&tarlow,2014)learnedalanguagemodelonparsetrees,and(mouetal.,2014)predictedwhethertwoprogramsareequivalentornot.bothoftheseapproachesrequireparsetrees,whilewelearnfromaprogramcharac-terlevelsequence.predictingprogramoutputrequiresthatthemodeldealswithlongtermdependenciesthatarisefromvariableas-signment.thuswechosetouserecurrentneuralnet-workswithlongshorttermmemoryunits(hochreiter&schmidhuber,1997),althoughtherearemanyotherid56variantsthatperformwellontaskswithlongtermdepen-dencies(choetal.,2014;jaegeretal.,2007;koutn    ketal.,2014;martens,2010;bengioetal.,2013).initially,wefounditdif   culttotrainlstmstoaccuratelyevaluateprograms.thecompositionalnatureofcomputerprogramssuggeststhatthelstmwouldlearnfasterifwe   rsttaughtittheindividualoperatorsseparatelyandthentaughtthelstmhowtocombinethem.thisapproachcanbeimplementedwithcurriculuid113arning(bengioetal.,2009;kumaretal.,2010;lee&grauman,2011),whichprescribesgraduallyincreasingthe   dif   cultylevel   oftheexamplespresentedtothelstm,andispartiallymotivatedbyfactthathumansandanimalslearnmuchfasterwhentheirinstructionprovidesthemwithhardbutmanageableexercises.unfortunately,wefoundthenaivecurriculuid113arningstrategyofbengioetal.(2009)tobegenerallyineffectiveandoccasionallyharmful.oneofourkeycon-tributionsistheformulationofanewcurriculuid113arningstrategythatsubstantiallyimprovesthespeedandthequal-ityoftrainingineveryexperimentalsettingthatweconsid-ered.3.subclassofprogramswetrainid56sonclassofsimpleprogramsthatcanbeevaluatedino(n)timeandconstantmemory.thisre-strictionisdictatedbythecomputationalstructureoftheid56itself,atitcanonlydoasinglepassoverthepro-gramusingaverylimitedmemory.ourprogramsusethepythonsyntaxandarebasedonasmallnumberofoper-ationsandtheircomposition(nesting).weconsiderthefollowingoperations:addition,subtraction,multiplication,variableassignment,if-statement,andfor-loops,althoughweforbiddoubleloops.everyprogramendswithasingle   print   statementthatoutputsanumber.severalexampleprogramsareshowninfigure1.weselectourprogramsfromafamilyofdistributionspa-rameterizedbylengthandnesting.thelengthparameteristhenumberofdigitsinnumbersthatappearintheprograms(sothenumbersarechosenuniformlyfrom[1,10length]).forexample,theprogramsaregeneratedwithlength=4(andnesting=3)infigure1.input:j=8584forxinrange(8):j+=920b=(1500+j)print((b+7567))target:25011.input:i=8827c=(i-5347)print((c+8704)if2641<8500else5308)target:1218.figure1.exampleprogramsonwhichwetrainthelstm.theoutputofeachprogramisasinglenumber.a   dot   symbolindi-catestheendofanumberandhastobepredictedaswell.wearemorerestrictivewithmultiplicationandtherangesoffor-loop,asthesearemuchmoredif   culttohandle.weconstrainoneoftheoperandsofmultiplicationandtherangeoffor-loopstobechosenuniformlyfromthemuchsmallerrange[1,4  length].thischoiceisdictatedbythelimitationsofourarchitecture.ourmodelsareabletoper-formlinear-timecomputationwhilegenericintegermul-tiplicationrequiressuperlineartime.similarrestrictionsapplytofor-loops,sincenestedfor-loopscanimplementintegermultiplication.thenestingparameteristhenumberoftimesweareal-lowedtocombinetheoperationswitheachother.highervalueofnestingresultsinprogramswithadeeperparsetree.nestingmakesthetaskmuchharderforourlstms,becausetheydonothaveanaturalwayofdealingwithcompositionality,incontrasttotreeneuralnetworks.itissurprisingthattheyareabletodealwithnestedexpres-sionsatall.itisimportanttoemphasizethatthelstmreadstheinputonecharacteratatimeandproducestheoutputcharacterbycharacter.thecharactersareinitiallymeaninglessfromthemodel   sperspective;forinstance,themodeldoesnotknowthat   +   meansadditionorthat6isfollowedby7.indeed,scramblingtheinputcharacters(e.g.,replacing   a   with   q   ,   b   with   w   ,etc.,)wouldhavenoeffectonthemodel   sabilitytosolvethisproblem.wedemonstratethedif   cultyofthetaskbypresentinganinput-outputexamplewithscrambledcharactersinfigure2.learningtoexecuteinput:vqppknsqdvfljmncy2vxdddsepnimcbvubkomhrpliibtwztbljipcctarget:hkhpgfigure2.anexampleprogramwithscrambledcharacters.ithelpsillustratethedif   cultyfacedbyourneuralnetwork.3.1.memorizationtaskinadditiontoprogramevaluation,wealsoinvestigatethetaskofmemorizingarandomsequenceofnumbers.givenanexampleinput123456789,thelstmreadsitonechar-acteratatime,storesitinmemory,andthenoutputs123456789onecharacteratatime.wepresentandex-ploretwosimpleperformanceenhancingtechniques:inputreversing(fromsutskeveretal.(2014))andinputdoubling.theideaofinputreversingistoreversetheorderoftheinput(987654321)whilekeepingthedesiredoutputun-changed(123456789).itseemstobeaneutraloperationastheaveragedistancebetweeneachinputanditscorrespond-ingtargetdidnotbecomeshorter.however,inputreversingintroducesmanyshorttermdependenciesthatmakeiteas-ierforthelstmtostartmakingcorrectpredictions.thisstrategywas   rstintroducedforlstmsformachinetrans-lationbysutskeveretal.(2014).thesecondperformanceenhancingtechniqueisinputdou-bling,wherewepresenttheinputsequencetwice(sotheexampleinputbecomes123456789;123456789),whiletheoutputisunchanged(123456789).thismethodismean-inglessfromaprobabilisticperspectiveasid56sapprox-imatetheconditionaldistributionp(y|x),yethereweat-tempttolearnp(y|x,x).still,itgivesnoticeableper-formanceimprovements.byprocessingtheinputseveraltimesbeforeproducinganoutput,thelstmisgiventheopportunitytocorrectthemistakesitmadeintheearlierpasses.4.curriculuid113arningourprogramgenerationschemeisparametrizedbylengthandnesting.thesetwoparametersallowuscontrolthecomplexityoftheprogram.whenlengthandnestingarelargeenough,thelearningproblemnearlyintractable.thisindicatesthatinordertolearntoevaluateprogramsofagivenlength=aandnesting=b,itmayhelpto   rstlearntoevaluateprogramswithlength   aandnesting   b.wecomparethefollowingcurriculuid113arningstrategies:nocurriculuid113arning(baseline)thebaselineapproachdoesnotusecurriculuid113arning.thismeansthatwegenerateallthetrainingsampleswithlength=aandnesting=b.thisstrategyismost   sound   fromstatis-ticalperspective,asitisgenerallyrecommendedtomakethetrainingdistributionidenticaltotestdistribution.naivecurriculumstrategy(naive)webeginwithlength=1andnesting=1.oncelearningstopsmakingprogress,weincreaselengthby1.werepeatthisprocessuntilitslengthreachesa,inwhichcaseweincreasenestingbyoneandresetlengthto1.wecanalsochooseto   rstincreasenestingandthenlength.however,itdoesnotmakeanoticeabledifferenceinper-formance.weskipthisoptionintherestofpaper,andincreaselength   rstinallourexperiments.thisstrategyishasbeenexaminedinpreviousworkoncurriculuid113arn-ing(bengioetal.,2009).however,weshowthatoftenitgivesevenworseperformancethanbaseline.mixedstrategy(mix)togeneratearandomsample,we   rstpickarandoid113ngthfrom[1,a]andarandomnestingfrom[1,b]independentlyforeverysample.themixedstrategyusesabalancedmix-tureofeasyanddif   cultexamples,soatanytimeduringtraining,asizablefractionofthetrainingsampleswillhavetheappropriatedif   cultyforthelstm.combiningthemixedstrategywithnaivecurriculumstrategy(combined)thisstrategycombinesthemixstrategywiththenaivestrategy.inthisapproach,everytrainingcaseisobtainedeitherbythenaivestrategyorbythemixstrategy.asaresult,thecombinedstrategyalwaysexposesthenetworkatleasttosomedif   cultexamples,whichisthekeywayinwhichitdiffersfromthenaivecurriculumstrategy.weno-ticedthatitreliablyoutperformedtheotherstrategiesinourexperiments.weexplainwhyournewcurriculuid113arningstrategiesoutperformthenaivecurriculumstrategyinsec-tion7.weevaluatethesefourstrategiesontheprogramevaluationtask(section6.1)andonthememorizationtask(section6.2).5.id56withlstmcellsinthissectionwebrie   ydescribethedeeplstm(sec-tion5.1).allvectorsaren-dimensionalunlessexplicitlystatedotherwise.lethlt2rnbeahiddenstateinlayerlintimestept.lettn,m:rn!rmbeabiasedlin-earmapping(x!wx+bforsomewandb).welet beelement-wisemultiplicationandleth0tbethein-putattimestepk.weusetheactivationsatthetoplayerl(namelyhlt)topredictytwherelisthedepthofourlstm.what can id56s compute?

some example results:

take a look through the results (http://arxiv.org/pdf/1410.4615v2.pdf#page=10). it   s fun

to try to guess from the mistakes what algorithms it   s discovered.

roger grosse

csc321 lecture 15: recurrent neural networks

26 / 26

underreviewasaconferencepaperaticlr2015supplementarymaterialinput:length,nestingstack=emptystack()operations=addition,subtraction,multiplication,if-statement,for-loop,variableassignmentfori=1tonestingdooperation=arandomoperationfromoperationsvalues=listcode=listforparamsinoperation.paramsdoifnotemptystackanduniform(1)>0.5thenvalue,code=stack.pop()elsevalue=random.int(10length)code=tostring(value)endifvalues.append(value)code.append(code)endfornewvalue=operation.evaluate(values)newcode=operation.generatecode(codes)stack.push((newvalue,newcode))endforfinalvalue,finalcode=stack.pop()datasets=training,validation,testingidx=hash(finalcode)modulo3datasets[idx].add((finalvalue,finalcode))algorithm1:pseudocodeofthealgorithmusedtogeneratethedistributionoverthepythonpro-gram.programsproducedbythisalgorithmareguaranteedtoneverhavedeadcode.thetypeofthesample(train,test,orvalidation)isdeterminedbyitshashmodulo3.11additionalresultsonthememorizationproblemwepresentthealgorithmforgeneratingthetrainingcases,andpresentanextensivequalitativeevaluationofthesamplesandthekindsofpredictionsmadebythetrainedlstms.weemphasizethatthesepredictionsrelyonteacherforcing.thatis,evenifthelstmmadeanincorrectpredictioninthei-thoutputdigit,thelstmwillbeprovidedasinputthecorrecti-thoutputdigitforpredictingthei+1-thdigit.whileteacherforcinghasnoeffectwheneverthelstmmakesnoerrorsatall,asamplethatmakesanearlyerrorandgetstheremainderofthedigitscorrectlyneedstobeinterpretedwithcare.12qualitativeevaluationofthecurriculumstrategies12.1examplesofprogramevaluationprediction.length=4,nesting=1input:print(6652).target:6652.   baseline   prediction:6652.   naive   prediction:6652.   mix   prediction:6652.   combined   prediction:6652.input:10underreviewasaconferencepaperaticlr2015input:b=9930forxinrange(11):b-=4369g=b;print(((g-8043)+9955)).target:-36217.   baseline   prediction:-37515.   naive   prediction:-38609.   mix   prediction:-35893.   combined   prediction:-35055.input:d=5446forxinrange(8):d+=(2678if4803<2829else9848)print((dif5935<4845else3043)).target:3043.   baseline   prediction:3043.   naive   prediction:3043.   mix   prediction:3043.   combined   prediction:3043.input:print((((2578if7750<1768else8639)-2590)+342)).target:6391.   baseline   prediction:-555.   naive   prediction:6329.   mix   prediction:6461.   combined   prediction:6105.input:print((((841if2076<7326else1869)*10)if7827<317else7192)).target:7192.   baseline   prediction:7192.   naive   prediction:7192.   mix   prediction:7192.   combined   prediction:7192.input:d=8640;print((7135if6710>((d+7080)*14)else7200)).target:7200.   baseline   prediction:7200.   naive   prediction:7200.   mix   prediction:7200.   combined   prediction:7200.input:b=6968forxinrange(10):b-=(299if3389<9977else203)print((12*b)).15underreviewasaconferencepaperaticlr2015figure8:predictionaccuracyonthememorizationtaskforthefourcurriculumstrategies.theinputlengthrangesfrom5to65digits.everystrategyisevaluatedwiththefollowing4inputmodi   cationschemes:nomodi   cation;inputinversion;inputdoubling;andinputdoublingandinversion.thetrainingtimeislimitedto20epochs.print((5997-738)).target:5259.   baseline   prediction:5101.   naive   prediction:5101.   mix   prediction:5249.   combined   prediction:5229.input:print((16*3071)).target:49136.   baseline   prediction:49336.   naive   prediction:48676.   mix   prediction:57026.   combined   prediction:49626.input:c=2060;print((c-4387)).target:-2327.   baseline   prediction:-2320.   naive   prediction:-2201.   mix   prediction:-2377.   combined   prediction:-2317.input:print((2*5172)).11underreviewasaconferencepaperaticlr2015target:47736.   baseline   prediction:-0666.   naive   prediction:11262.   mix   prediction:48666.   combined   prediction:48766.input:j=(1*5057);print(((j+1215)+6931)).target:13203.   baseline   prediction:13015.   naive   prediction:12007.   mix   prediction:13379.   combined   prediction:13205.input:print(((1090-3305)+9466)).target:7251.   baseline   prediction:7111.   naive   prediction:7099.   mix   prediction:7595.   combined   prediction:7699.input:a=8331;print((a-(15*7082))).target:-97899.   baseline   prediction:-96991.   naive   prediction:-19959.   mix   prediction:-95551.   combined   prediction:-96397.12.4examplesofprogramevaluationprediction.length=6,nesting=1input:print((71647-548966)).target:-477319.   baseline   prediction:-472122.   naive   prediction:-477591.   mix   prediction:-479705.   combined   prediction:-475009.input:print(1508).target:1508.   baseline   prediction:1508.   naive   prediction:1508.   mix   prediction:1508.   combined   prediction:1508.input:16