slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 18 neural networks i

furong huang / furongh@cs.umd.edu

neural networks

today

what are neural networks?
how to make a prediction given an input?
why are neural networks powerful? 

next tuesday

how to train them?

a warm-up example

id31 for movie review

the movie was horrible 
the actors are excellent 
the movie was not horrible 
he is usually an excellent actor, but not in this movie 

+1
-1
-1

+1

(on board)

binary classification
via hyperplanes

at test time, we check on 
what side of the hyperplane 
examples fall

!"=$%&'()*++-)

function approximation
with id88
problem setting

set of possible instances !
each instance "   ! is a feature vector "=["&,   ,")]
unknown target function +:!   .
. is binary valued {-1; +1}
set of function hypotheses /=        :!   .}
each hypothesis     is a hyperplane in d-dimensional space
training examples {"&,3& ,    "4,34 } of unknown target 
function +
hypothesis       / that best approximates target function +

output

input

neural networks

we can think of neural networks as 
combination of multiple id88s

multilayer id88

why would we want to do that?

discover more complex decision boundaries
learn combinations of features

what does a 2-layer
id88 look like?

(illustration on board) 

key concepts:

input dimensionality
hidden units
hidden layer
output layer
id180 

id180

id180 are non-linear
functions

sign function as in the id88
hyperbolic tangent and other sigmoid functions that 
approximate sign but are differentiable

what happens if the hidden units use the 
identify function as an activation function? 

matrix of hidden layer 

parameters

vector of output layer 

parameters

what functions can we approximate with 
a 2 layer id88?
problem setting

set of possible instances !
each instance "   ! is a feature vector "=["&,   ,")]
unknown target function +:!   .
. is binary valued {-1; +1}
set of function hypotheses /=        :!   .}
training examples {"&,3& ,    "4,34 } of unknown target 
function +
hypothesis       / that best approximates target function +

output

input

two-layer networks are 
universal function approximators

theorem (th 9 in ciml):
let f be a continuous function on a bounded subset of d-
dimensional space. then there exists a two-layer neural 

network !" with a finite number of hidden units that 

approximates f arbitrarily well. namely, for all x in the 
domain of f, 

example: a neural network to solve the 
xor problem

  0(x1) = {-1, 1}
  2

x

  0(x2) = {1, 1}
o

  1(x3) = {-1, 1}

o

  1[1]

  1

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

  1[0]

o

  1(x2) = {1, -1}

x

  1(x1) = {-1, -1}
  1(x4) = {-1, -1}

  1[0]

  1[1]

1
1
-1

-1
-1
-1

example
   in new space, the examples are linearly separable!

  0(x1) = {-1, 1}
  0[1]

x

  0(x2) = {1, 1}
o

  0[0]

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

-1
-1
-1

  2[0] = y

1
1
-1

-1
-1
-1

  1[0]

  1[1]

  1(x3) = {-1, 1}

o

  1[1]

  1(x1) = {-1, -1} x
  1(x4) = {-1, -1}

  1[0]

o   1(x2) = {1, -1}

example

   the final net

  0[0]
  0[1]
1

  0[0]
  0[1]
1

1
1

-1

-1
-1

-1

tanh

  1[0]

-1

tanh

  2[0]

-1

tanh

  1[1]

1 -1

discussion

2-layer id88 lets us

discover more complex decision boundaries than 
id88
learn combinations of features that are useful for 
classification

key design question

how many hidden units?
more hidden units yield more complex functions
fewer hidden units requires fewer examples to train

neural networks

today

what are neural networks?

multilayer id88

how to make a prediction given an input?

simple matrix operations + non-linearities
why are neural networks powerful? 

universal function approximators!

next

how to train them?

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

