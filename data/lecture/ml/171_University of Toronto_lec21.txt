csc321 lecture 21: policy gradient

roger grosse

roger grosse

csc321 lecture 21: policy gradient

1 / 21

overview

most of this course was about supervised learning, plus a little
unsupervised learning.
final 3 lectures: id23

middle ground between supervised and unsupervised learning
an agent acts in an environment and receives a reward signal.

today: policy gradient (directly do sgd over a stochastic policy
using trial-and-error)

next lecture: id24 (learn a value function predicting returns
from a state)

final lecture: policies and value functions are way more powerful in
combination

roger grosse

csc321 lecture 21: policy gradient

2 / 21

id23

an agent interacts with an environment (e.g. game of breakout)
in each time step t,

the agent receives observations (e.g. pixels) which give it information
about the state st (e.g. positions of the ball and paddle)
the agent picks an action at (e.g. keystrokes) which a   ects the state
the agent periodically receives a reward r (st, at), which depends on
the state and action (e.g. points)
the agent wants to learn a policy     (at | st)

distribution over actions depending on the current state and
parameters   

roger grosse

csc321 lecture 21: policy gradient

3 / 21

id100

the environment is represented as a markov decision process m.
markov assumption: all relevant information is encapsulated in the
current state; i.e. the policy, reward, and transitions are all
independent of past states given the current state
components of an mdp:

initial state distribution p(s0)
policy     (at | st)
transition distribution p(st+1 | st, at)
reward function r (st, at)

assume a fully observable environment, i.e. st can be observed directly
rollout, or trajectory    = (s0, a0, s1, a1, . . . , st , at )
id203 of a rollout
p(   ) = p(s0)     (a0 | s0) p(s1 | s0, a0)       p(st | st   1, at   1)     (at | st )

roger grosse

csc321 lecture 21: policy gradient

4 / 21

id100

continuous control in simulation, e.g. teaching an ant to walk

state: positions, angles, and velocities of the joints
actions: apply forces to the joints
reward: distance from starting point
policy: output of an ordinary mlp, using the state as input
more environments: https://gym.openai.com/envs/#mujoco

roger grosse

csc321 lecture 21: policy gradient

5 / 21

id100

return for a rollout: r (   ) =(cid:80)t

t=0 r (st, at)

note: we   re considering a    nite horizon t , or number of time steps;
we   ll consider the in   nite horizon case later.

goal: maximize the expected return, r = ep(   )[r (   )]
the expectation is over both the environment   s dynamics and the
policy, but we only have control over the policy.
the stochastic policy is important, since it makes r a continuous
function of the policy parameters.

reward functions are often discontinuous, as are the dynamics
(e.g. collisions)

roger grosse

csc321 lecture 21: policy gradient

6 / 21

reinforce

reinforce is an elegant algorithm for maximizing the expected
return r = ep(   ) [r (   )].
intuition: trial and error

sample a rollout    . if you get a high reward, try to make it more likely.
if you get a low reward, try to make it less likely.

interestingly, this can be seen as stochastic gradient ascent on r.

roger grosse

csc321 lecture 21: policy gradient

7 / 21

reinforce

recall the derivative formula for log:

   
     

log p(   ) =

   
      p(   )
p(   )

=   

   
     

p(   ) = p(   )

   
     

log p(   )

gradient of the expected return:

   
     

ep(   ) [r (   )] =

=

=

   
     

(cid:88)
(cid:88)

  

(cid:88)

  

r (   )p(   )

r (   )

   
     

p(   )

(cid:20)

  

= ep(   )

r (   )p(   )

log p(   )

   
     

(cid:21)

r (   )

   
     

log p(   )

compute stochastic estimates of this expectation by sampling rollouts.

roger grosse

csc321 lecture 21: policy gradient

8 / 21

reinforce

for reference:

   
     

ep(   ) [r (   )] = ep(   )

(cid:20)

r (   )

   
     

(cid:21)

log p(   )

if you get a large reward, make the rollout more likely. if you get a
small reward, make it less likely.
unpacking the reinforce gradient:

(cid:35)

p(s0)

    (at | st )

p(st | st   1, at   1)

(cid:34)
t(cid:89)

t(cid:89)

t(cid:89)

t=0

t=1

    (at | st )

t=0

log     (at | st )

   
     

log p(   ) =

   
     

log

=

=

   
     

t(cid:88)

t=0

log

   
     

hence, it tries to make all the actions more likely or less likely,
depending on the reward. i.e., it doesn   t do credit assignment.

this is a topic for next lecture.

roger grosse

csc321 lecture 21: policy gradient

9 / 21

reinforce

repeat forever:

r (   )    (cid:80)t

sample a rollout    = (s0, a0, s1, a1, . . . , st , at )

for t = 0, . . . , t :

k=0 r (sk , ak )
          +   r (   )    

      log     (ak | sk )

observation: actions should only be reinforced based on future
rewards, since they can   t possibly in   uence past rewards.

you can show that this still gives unbiased gradient estimates.

repeat forever:

sample a rollout    = (s0, a0, s1, a1, . . . , st , at )
for t = 0, . . . , t :

rt (   )    (cid:80)t

k=t r (sk , ak )

          +   rt (   )    

      log     (ak | sk )

roger grosse

csc321 lecture 21: policy gradient

10 / 21

optimizing discontinuous objectives

edge case of rl: handwritten digit classi   cation, but maximizing
accuracy (or minimizing 0   1 loss)

id119 completely fails if the cost function is discontinuous:

original solution: use a surrogate id168, e.g.
logistic-cross-id178

rl formulation: in each episode, the agent is shown an image, guesses
a digit class, and receives a reward of 1 if it   s right or 0 if it   s wrong

we   d never actually do it this way, but it will give us an interesting
comparison with backprop

roger grosse

csc321 lecture 21: policy gradient

11 / 21

optimizing discontinuous objectives

rl formulation

one time step
state x: an image
action a: a digit class
reward r (x, a): 1 if correct, 0 if wrong
policy   (a| x): a distribution over categories

compute using an mlp with softmax outputs     this is a policy network

roger grosse

csc321 lecture 21: policy gradient

12 / 21

optimizing discontinuous objectives

let zk denote the logits, yk denote the softmax output, t the integer
target, and tk the target one-hot representation.
to apply reinforce, we sample a         (  | x) and apply:

          +   r (a, t)

log     (a| x)

=    +   r (a, t)

=    +   r (a, t)

log ya
(ak     yk )

   
     

zk

   
     
   
     

(cid:88)

k

compare with the id28 sgd update:

          +   
       +   

   
     

(cid:88)

log yt
(tk     yk )

k

   
     

zk

roger grosse

csc321 lecture 21: policy gradient

13 / 21

reward baselines

for reference:

          +   r (a, t)

log     (a| x)

   
     

clearly, we can add a constant o   set to the reward, and we get an
equivalent optimization problem.
behavior if r = 0 for wrong answers and r = 1 for correct answers

wrong: do nothing
correct: make the action more likely

if r = 10 for wrong answers and r = 11 for correct answers

wrong: make the action more likely
correct: make the action more likely (slightly stronger)

if r =    10 for wrong answers and r =    9 for correct answers

wrong: make the action less likely
correct: make the action less likely (slightly weaker)

roger grosse

csc321 lecture 21: policy gradient

14 / 21

reward baselines

problem: the reinforce update depends on arbitrary constant
factors added to the reward.
observation: we can subtract a baseline b from the reward without
biasing the gradient.
ep(   )

(r (   )     b)

    bep(   )

(cid:20)    

= ep(   )

log p(   )

log p(   )

(cid:20)

(cid:21)

r (   )

log p(   )

(cid:21)

   
     

   
     

(cid:20)
(cid:20)
(cid:20)
(cid:20)

(cid:88)
(cid:88)

  

  

     

log p(   )

p(   )

   
     

   
     

p(   )

= ep(   )

= ep(   )

= ep(   )

r (   )

r (   )

r (   )

   
     

   
     

   
     

log p(   )

log p(   )

log p(   )

    b

    b

    0

(cid:21)
(cid:21)
(cid:21)
(cid:21)

we   d like to pick a baseline such that good rewards are positive and
bad ones are negative.
e[r (   )] is a good choice of baseline, but we can   t always compute it
easily. there   s lots of research on trying to approximate it.

roger grosse

csc321 lecture 21: policy gradient

15 / 21

more tricks

we left out some more tricks that can make policy gradients work a
lot better.

evaluate each action using only future rewards, since it has no in   uence
on past rewards. it can be shown this gives unbiased gradients.
natural policy gradient corrects for the geometry of the space of
policies, preventing the policy from changing too quickly.
rather than use the actual return, evaluate actions based on estimates
of future returns. this is a class of methods known as actor-critic,
which we   ll touch upon next lecture.

trust region policy optimization (trpo) and proximal policy
optimization (ppo) are modern policy gradient algorithms which are
very e   ective for continuous control problems.

roger grosse

csc321 lecture 21: policy gradient

16 / 21

discussion

what   s so great about backprop and id119?

backprop does credit assignment     it tells you exactly which
activations and parameters should be adjusted upwards or downwards
to decrease the loss on some training example.
reinforce doesn   t do credit assignment. if a rollout happens to be
good, all the actions get reinforced, even if some of them were bad.
reinforcing all the actions as a group leads to random walk behavior.

roger grosse

csc321 lecture 21: policy gradient

17 / 21

discussion

why policy gradient?

can handle discontinuous cost functions
don   t need an explicit model of the environment, i.e. rewards and
dynamics are treated as black boxes

policy gradient is an example of model-free id23,
since the agent doesn   t try to    t a model of the environment
almost everyone thinks model-based approaches are needed for ai, but
nobody has a clue how to get it to work

roger grosse

csc321 lecture 21: policy gradient

18 / 21

evolution strategies (optional)

reinforce can handle discontinuous dynamics and reward
functions, but it requires a di   erentiable network since it computes
   

      log     (at | st)

evolution strategies (es) take the policy gradient idea a step further,
and avoid backprop entirely.
es can use deterministic policies. it randomizes over the choice of
policy rather than over the choice of actions.

i.e., sample a random policy from a distribution p  (  ) parameterized
by    and apply the policy gradient trick

(cid:20)

(cid:21)

   
     

e     p   [r (   (  ))] = e     p  

r (   (  ))

log p  (  )

   
     

the neural net architecture itself can be discontinuous.

roger grosse

csc321 lecture 21: policy gradient

19 / 21

evolution strategies (optional)

https://arxiv.org/pdf/1703.03864.pdf

roger grosse

csc321 lecture 21: policy gradient

20 / 21

evolution strategies (optional)

the ieee    oating point standard is nonlinear, since small enough
numbers get truncated to zero.

this acts as a discontinuous activation
function, which es is able to handle.

es was able to train a good mnist
classi   er using a    linear    activation
function.

https://blog.openai.com/
nonlinear-computation-in-linear-networks/

roger grosse

csc321 lecture 21: policy gradient

21 / 21

