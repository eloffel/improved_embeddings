10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

optimization   for   ml

+

linear   regression

optimization   readings:
lecture   notes   from   10-     600   (see   
piazza   note)

   convex   optimization      boyd   and   
vandenberghe (2009)      [see   chapter   
9.   this   advanced   reading   is   entirely   
optional.]

linear   regression   readings:
murphy   7.1       7.3
bishop   3.1
htf   3.1       3.4
mitchell   4.1-     4.3

matt   gorid113y
lecture   7
february   8,   2016

1

reminders

    homework 2:   naive bayes

    release:   wed,   feb.   1
    due:   mon,   feb.   13   at   5:30pm

    homework 3:   linear   /   id28

    release:   mon,   feb.   13
    due:   wed,   feb.   22   at   5:30pm

2

optimization   outline

    optimization   for   ml

    gradient   descent

    optimization:   closed   form   solutions

    differences   
    types   of   optimization   problems
    unconstrained   optimization
    convex,   concave,   nonconvex
    example:   1-     d   function
    example:   higher   dimensions
    gradient   and   hessian
    example:   2d   gradients
    algorithm
    details:   starting   point,   stopping   criterion,   line   search
stochastic   gradient   descent   (sgd)
    expectations   of   gradients
    algorithm
    mini-     batches
    details:   mini-     batches,   step   size,   stopping   criterion
    problematic   cases   for   sgd
convergence
    comparison   of   newton   s   method,   gradient   descent,   sgd
    asymptotic   convergence
    convergence   in   practice

   

   

3

optimization   for   ml

not   quite   the   same   setting   as   other   fields   
    function   we   are   optimizing   might   not   be   the   

true   goal   
(e.g.   likelihood   vs   generalization   error)

    precision   might   not   matter   

(e.g.   data   is   noisy,   so   optimal   up   to   1e-     16   might   
not   help)

    stopping   early   can   help   generalization   error

(i.e.      early   stopping      is   a   technique   for   
id173       discussed   more   next   time)

4

optimization   for   ml

whiteboard

    differences   
    types   of   optimization   problems
    unconstrained   optimization
    convex,   concave,   nonconvex

5

convexity

there   is   only   one   local   optimum   if   the   function   is   convex

slide   adapted   from   william   cohen

6

optimization:   closed   form   solutions
whiteboard

    example:   1-     d   function
    example:   higher   dimensions
    gradient   and   hessian

7

gradients

8

gradients

these   are   the   gradients that   
gradient   ascent   would   follow.

9

negative   gradients

these   are   the   negative gradients   that   

gradient   descent would   follow.

10

negative   gradient   paths

shown   are   the   paths   that   gradient   descent   
would   follow   if   it   were   making   infinitesimally   

small   steps.

11

gradient   descent

whiteboard

    example:   2d   gradients
    algorithm
    details:   starting   point,   stopping   criterion,   line   

search

12

gradient   ascent

to	
   find	
   argminx f(x):

    start	
   with	
   x0
    for	
   t=1   .

    xt+1	
   =	
   xt +	
     	
   f   (x t)
where	
     	
   is	
   small

slide   courtesy   of   william   cohen

13

gradient   descent

likelihood:   ascent

loss:   descent

slide   courtesy   of   william   cohen

14

pros   and   cons   of   gradient   descent

    simple   and   often   quite   effective   on   ml   tasks
    often   very   scalable   
    only   applies   to   smooth   functions   (differentiable)
    might   find   a   local   minimum,   rather   than   a   global   

one

slide   courtesy   of   william   cohen

15

gradient   descent

algorithm 1 id119
1: procedure gd(d,  (0))

     (0)
while not converged do

      +    j( )

   

return  

2:
3:
4:

5:

in   order   to   apply   gd   to   linear   
regression   all   we   need   is   the   
gradient of   the   objective   
function   (i.e.   vector   of   partial   
derivatives).   

  j( ) =     

d
d 1
d
d 2

d

d n

j( )
j( )
...
j( )

     

16

gradient   descent

algorithm 1 id119
1: procedure gd(d,  (0))

     (0)
while not converged do

      +    j( )

   

return  

2:
3:
4:

5:

there   are   many   possible   ways   to   detect   convergence.      
for   example,   we   could   check   whether   the   l2   norm   of   
the   gradient   is   below   some   small   tolerance.

||  j( )||2    

alternatively   we   could   check   that   the   reduction   in   the   
objective   function   from   one   iteration   to   the   next   is   small.

17

stochastic   gradient   descent   (sgd)
whiteboard

    expectations   of   gradients
    algorithm
    mini-     batches
    details:   mini-     batches,   step   size,   stopping   

criterion

    problematic   cases   for   sgd

18

stochastic   gradient   descent   (sgd)

algorithm 2 stochastic id119 (sgd)
1: procedure sgd(d,  (0))
     (0)
while not converged do

for i   shu e({1, 2, . . . , n}) do

      +    j (i)( )

   

return  

2:
3:
4:
5:

6:

we   need   a   per-     example   objective:
i=1 j (i)( )

let j( ) = n

where j (i)( ) = 1

2 ( t x(i)   y(i))2.

19

stochastic   gradient   descent   (sgd)

algorithm 2 stochastic id119 (sgd)
1: procedure sgd(d,  (0))
     (0)
while not converged do

for i   shu e({1, 2, . . . , n}) do

for k   {1, 2, . . . , k} do
j (i)( )

 k    k +   d

d k

   

7:

return  

2:
3:
4:
5:
6:

we   need   a   per-     example   objective:
i=1 j (i)( )

let j( ) = n

where j (i)( ) = 1

2 ( t x(i)   y(i))2.

20

convergence

whiteboard

    comparison   of   newton   s   method,   gradient   

descent,   sgd

    asymptotic   convergence
    convergence   in   practice

21

linear   regression   outline

   

   

   

   

linear   functions

regression   problems
    definition
   
    residuals
    notation   trick:   fold   in   the   intercept
linear   regression   as   function   approximation
    objective   function:   mean   squared   error
    hypothesis   space:   linear   functions
optimization   for   linear   regression
    normal   equations   (closed-     form   solution)

   
   

computational   complexity
stability

   

sgd   for   linear   regression

partial   derivatives

   
    update   rule

    gradient   descent   for   linear   regression
probabilistic   interpretation   of   linear   regression
    generative   vs.   discriminative
   
   
   
   

conditional   likelihood
background:   gaussian   distribution
case   #1:   1d   linear   regression
case   #2:   multiple   linear   regression

22

regression   problems

whiteboard
    definition
    linear   functions
    residuals
    notation   trick:   fold   in   the   intercept

23

linear   regression   as   function   

approximation

whiteboard

    objective   function:   mean   squared   error
    hypothesis   space:   linear   functions

24

optimization   for   linear   regression
whiteboard

    normal   equations   (closed-     form   solution)

    computational   complexity
    stability

    sgd   for   linear   regression

    partial   derivatives
    update   rule

    gradient   descent   for   linear   regression

25

probabilistic   interpretation   of   linear   

regression

whiteboard

    generative   vs.   discriminative
    conditional   likelihood
    background:   gaussian   distribution
    case   #1:   1d   linear   regression
    case   #2:   multiple   linear   regression

26

convergence   curves

    for   the   batch   method,   

the   training   mse   is   
initially   large   due   to   
uninformed   
initialization

    in   the   online   update,   
n   updates   for   every   
epoch   reduces   mse   to   
a   much   smaller   value.

     eric   xing   @   cmu,   2006-     2011

27

