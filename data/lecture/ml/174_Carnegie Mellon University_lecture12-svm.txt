10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

kernels   +   

support   vector   
machines   (id166s)

id166   readings:
murphy   14.5
bishop   7.1
htf   12   -      12.38
mitchell   -     -     

matt   gorid113y
lecture   12
february   27,   2016

1

reminders

    homework 4:   id88 /   kernels /   id166

    release:   wed,   feb.   22
    due:   fri,   mar.   03   at   11:59pm

    midterm exam (evening exam)
    tue,   mar.   07   at   7:00pm       9:30pm
    see piazza   for details about location

9   days
for   hw4

    grading

2

outline

    kernels

    kernel   id88
    kernel   as   a   dot   product
    gram   matrix
    examples:   polynomial,   rbf

    support   vector   machine   (id166)

    background:   constrained   

optimization,   linearly   separable,   
margin

    id166   primal   (linearly   separable   case)
    id166   primal   (non-     linearly   separable   

case)

    id166   dual

last   lecture

this   lecture

3

kernels

4

kernels:   motivation

most   real-     world   problems   exhibit   data   that   is   
not   linearly   separable.

example:   pixel   representation   for   facial   recognition:

q:   when   your   data   is   not   linearly   separable,   

how   can   you   still   use   a   linear   classifier?

a: preprocess   the   data   to   produce   nonlinear

features

5

kernels:   motivation
    motivation   #1:   inefficient   features

    non-     linearly   separable   data   requires   high   

dimensional   representation

    might   be   prohibitively   expensive   to   compute   or   

store

    motivation   #2:   memory-     based   methods

    k-     nearest   neighbors   (knn)   for   facial   recognition   

allows   a   distance   metric between   images   -     -      no   
need   to   worry   about   linearity   restriction   at   all

6

kernels

whiteboard

    kernel   id88
    kernel   as   a   dot   product
    gram   matrix
    examples:   rbf   kernel,   string   kernel

7

kernel   methods

    key   idea:   

1.

2.

rewrite the   algorithm   so   that   we   only   work   with   dot   products xtz
of   feature   vectors
replace the   dot   products   xtz with   a   kernel   function   k(x,   z)

    the   kernel   k(x,z)   can   be   any legal   definition   of   a   dot   product:   

k(x,   z)   =     (x) t  (z)   for   any   function     :   x       rd

so   we   only   compute   the        dot   product   implicitly

    this      kernel   trick    can   be   applied   to   many   algorithms:

    classification:   id88,   id166,      
    regression:   ridge   regression,      
    id91:   k-     means,      

8

kernel   methods

q:   these   are   just   non-     linear   features,   right?
a: yes,   but   

q:   can   t   we   just   compute   the   feature   

transformation      explicitly?

a: that   depends...

q:   so,   why   all   the   hype   about   the   kernel   trick?
a: because   the   explicit   features   might   either   
be   prohibitively   expensive   to   compute   or   
infinite   length   vectors

9

example 

example:   polynomial   kernel

for n=2, d=2, the kernel kx,z = x   z d corresponds to  
    1,    2           =(    12,    22, 2    1    2) 
  :r2   r3, x1,x2      x =(x12,x22, 2x1x2) 
  :r2   r3, x1,x2      x =(x12,x22, 2x1x2) 
example 
example 
  -space 
  x           = x12,x22, 2x1x2    (    12,    22, 2    1    2) 
  x           = x12,x22, 2x1x2    (    12,    22, 2    1    2) 
= x1    1+x2    2 2= x        2=k(x,z) 
for n=2, d=2, the kernel kx,z = x   z d corresponds to  
for n=2, d=2, the kernel kx,z = x   z d corresponds to  
= x1    1+x2    2 2= x        2=k(x,z) 
    1,    2           =(    12,    22, 2    1    2) 
    1,    2           =(    12,    22, 2    1    2) 
  -space 
  -space 

original space 

example 

example 

x2 

x 

x 

x 

x 

x 

x 

x 

x 
x 
original space 
original space 
x 
x 
original space 
original space 
x2 
x2 
x2 
x2 
x 
o 
x 
x 
x 
x 
o 
x 
x 
x 
x 
o 
x 
o 
x 
x 
x 
x 
o 
x 
x 
o 
x 
o  o 
x1 
o 
x 
o 
o 
o 
o 
o 
o 
o  o 
x 
x 
o 
o 
o 
o 
o 
x 
x 
x 
x 
x 
o  o 
o  o 
x 
z3 
x 
x 
o 
o 

o 
o 
x 
o 
x 
o 
o 
x 
x 
o 
o  o 
x 
x 
x 
x 

x 
o 
x 
x 
x 

o 
o 
x 
x 

x 
x 

o 

x 

x 

x 

x 

x 
x 
x 
x1 
x 
x 
x 
x1 

o 

x 

x  x 
x 
x 

x 
x 
x 
o 
o  o 
o 
o 
x 
o 
x 
x1 
x1 
o 
x 
x 
z3 
x 
z3 
x 
x 

o 
x 
x 

x 

x 

slide   from   nina   balcan

x 
x 

x 
x 

x 
x 

x 
x 
x 
x 
x 

x 
x 

x  x 
x 
x 

x 

x 

x 
x 
o 
x  x 
o  o 
x  x 
x 
x  x 
x 
x 
x 
x 
x 
x 
o 
x 
o 
o 
x 
x 
x 
o 
x 
x 
o  o 
o 
o 
x 
x 
o 
x 
x 
x 
x 
o 
o 
z1 
z1 
o 
o 
o  o 
x 
o  o 
o 
x 
x 
o 
x 
x 
o 
x 
o 
o 
o 
o 
o 
x 
x 
x 
x 
o 
x 
o 
x 
x 
o 
o 
x 
x 
x 
x 

x 
x 
x 

o 
x 
x 

o 
x 

o 
o 

x 

x 
x 
10

z1 

x 

x 
x 

z3 

  -space 
  -space 

example:   polynomial   kernel

avoid explicitly expanding the features 
feature space can grow really large and really quickly   . 

crucial to think of    as implicit, not explicit!!!! 
    polynomial kernel degreee     ,         ,     =                 =                      
       1    ,     1    2           ,     12    2              1 
=     +       1!
    +       1
    
    !        1!  
        =6,    =100, there are 1.6 billion terms 
                                                      ! 
        ,     =                 =                      

    total number of such feature is 

slide   from   nina   balcan

11

side   note:   the   feature   space   might   not   be   unique!

explicit   representation   #1:

kernel   examples

example 

note:  feature space might not be unique. 

  :r2   r3, x1,x2      x =(x12,x22, 2x1x2) 
  x           = x12,x22, 2x1x2    (    12,    22, 2    1    2) 
= x1    1+x2    2 2= x        2=k(x,z) 
  :r2   r4, x1,x2      x =(x12,x22,x1x2,x2x1) 
  x           =(x12,x22,x1x2,x2x1)   (z12,z22,z1z2,z2z1) 

= x        2=k(x,z) 

explicit   representation   #2:

these   two   different   feature   representations   correspond   to   the   same   

kernel   function!

slide   from   nina   balcan

12

kernel   examples

kernel   function
(implicit dot   product)

name

linear

polynomial   (v1)

polynomial (v2)

gaussian

hyperbolic
tangent   
(sigmoid)   
kernel

feature   space
(explicit   dot   product)

same   as   original   input   
space

all   polynomials of degree   
d

all   polynomials up   to   
degree   d

infinite   dimensional   space

(with id166,   this   is   
equivalent   to   a   2-     layer   
neural   network)

13

rbf   kernel   example

rbf   kernel:

14

rbf   kernel   example

rbf   kernel:

15

rbf   kernel   example

rbf   kernel:

16

rbf   kernel   example

rbf   kernel:

17

rbf   kernel   example

rbf   kernel:

18

rbf   kernel   example

rbf   kernel:

19

rbf   kernel   example

rbf   kernel:

20

rbf   kernel   example

rbf   kernel:

21

rbf   kernel   example

rbf   kernel:

22

rbf   kernel   example

rbf   kernel:

23

rbf   kernel   example

rbf   kernel:

24

rbf   kernel   example

rbf   kernel:

25

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

26

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

27

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

28

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

29

example:   string   kernel

setup:

    input   instances   x are   strings   of   characters   (e.g.   

x(3) =   [   s   ,      a   ,      t   ],   x(7) =   [   c   ,      a   ,      t   ]   

    want   indicator   features   for   the   presence   /   

absence   of   each   possible   substring   up   to   length   
k

questions:

1. what   is   the   best   runtime of   a   single   standard   

2. what   is   the   best   runtime of   a   single   kernel   

id88 update?

id88 update?

30

kernels:   discussion
kernels, discussion 

    if  all computations involving instances are in terms 

(cid:131)  computationally, only need to modify the algo by replacing 

of inner products then: 
(cid:131) conceptually, work in a very high diml space and the alg   s 
performance depends only on linear separability in that 
extended space. 

each x   z with a kx,z . 
    use cross-validation to choose the parameters, e.g.,       for 
gaussian kernel   kx,     =exp                2
2     2    

how to choose a kernel: 
    kernels often encode domain knowledge (e.g., string kernels) 

    learn a good kernel; e.g.,  [lanckriet-cristianini-bartlett-el ghaoui-

jordan   04] 

slide   from   nina   balcan

31

support   vector   machine   
(id166)

32

id166:   optimization   background

whiteboard

    constrained   optimization
    linear   programming
    quadratic   programming
    example:   2d   quadratic   function   with   linear   

constraints

33

quadratic   program

34

quadratic   program

35

quadratic   program

36

quadratic   program

37

quadratic   program

38

id166

whiteboard

    id166   primal   (linearly   separable   case)
    id166   primal   (non-     linearly   separable   case)

39

id166   qp

40

id166   qp

41

id166   qp

42

id166   qp

43

id166   qp

44

id166   qp

45

support vector machines (id166s) 

find 

 s.t.: 

input: s={(x1,    1),    ,(xm,    m)};  
argminw,    1,   ,               2+             
    
      for all i,                           1            
           0 
input: s={(x1,y1),    ,(xm,ym)};  
0     i   ci 
 yi  i=0
i

argmin  12  yiyj   i  jxi   xj      i
i

which is equivalent to: 

      for all i,  

find 

i

j

 

primal 
form 

can  be kernelized!!! 

lagrangian 
dual 

 s.t.: 

slide   from   nina   balcan

46

id166s (lagrangian dual) 

j

i

find 

argmin  12  yiyj   i  jxi   xj      i
i
           =   1 

input: s={(x1,y1),    ,(xm,ym)};  
0     i   ci 
 yi  i=0
i
    final classifier is: w=   iyixi
i
    the points xi for which   i   0 

      for all i,  

 s.t.: 

  

 

are called the    support vectors    

+ 

- 

- 
-  - 
- 
- 

- 
+ 

+ 

- 
+ 
- 

w 

           =1 
+ 

slide   from   nina   balcan

47

id166   takeaways

    maximizing   the   margin   of   a   linear   separator   

is   a   good   training   criteria

    support   vector   machines   (id166s)   learn   a   

max-     margin   linear   classifier

    the   id166   optimization   problem   can   be   

solved   with   black-     box   quadratic   
programming   (qp)   solvers

    learned   decision   boundary   is   defined   by   its   

support   vectors

48

