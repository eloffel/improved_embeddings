10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

neural   networks

and

id26

neural   net   readings:
murphy   -     -     
bishop   5
htf   11
mitchell   4

matt   gorid113y
lecture   20
april   3,   2017

1

reminders

    homework 6:   unsupervised learning

    release:   wed,   mar.   22
    due:   mon,   apr.   03   at   11:59pm

    homework 5 (part ii):   peer   review

    release:   wed,   mar.   29
    due:   wed,   apr.   05   at   11:59pm

expectation:   you   

should   spend   at   most   1   
hour   on   your   reviews

    peer   tutoring

2

neural   networks   outline

   

logistic   regression   (recap)
    data,   model,   learning,   prediction

    neural   networks

    a   recipe   for   machine   learning
    visual   notation   for   neural   networks
    example:   logistic   regression   output   surface
    2-     layer   neural   network
    3-     layer   neural   network
    neural   net   architectures

    objective   functions
    activation   functions

    id26

    basic   chain   rule   (of   calculus)
    chain   rule   for   arbitrary   computation   graph
    id26   algorithm
    module-     based   automatic   differentiation   

(autodiff)

last   lecture

this   lecture

3

decision   boundary   examples

4

example   #1:   diagonal   band

5

example   #2:   one   pocket

6

example   #3:   four   gaussians

7

example   #4:   two   pockets

8

example   #1:   diagonal   band

9

example   #1:   diagonal   band

10

example   #1:   diagonal   band

error   in   slides:      layers      
should   read      number   of   
hidden   units   

all   the   neural   networks   in   
this   section   used   1   hidden   
layer.

11

example   #1:   diagonal   band

12

example   #1:   diagonal   band

13

example   #1:   diagonal   band

14

example   #1:   diagonal   band

15

example   #2:   one   pocket

16

example   #2:   one   pocket

17

example   #2:   one   pocket

18

example   #2:   one   pocket

19

example   #2:   one   pocket

20

example   #2:   one   pocket

21

example   #2:   one   pocket

22

example   #2:   one   pocket

23

example   #3:   four   gaussians

24

example   #3:   four   gaussians

25

example   #3:   four   gaussians

26

example   #3:   four   gaussians

27

example   #3:   four   gaussians

28

example   #3:   four   gaussians

29

example   #3:   four   gaussians

36

example   #3:   four   gaussians

37

example   #3:   four   gaussians

38

example   #4:   two   pockets

39

example   #4:   two   pockets

40

example   #4:   two   pockets

41

example   #4:   two   pockets

42

example   #4:   two   pockets

43

example   #4:   two   pockets

44

example   #4:   two   pockets

45

example   #4:   two   pockets

46

example   #4:   two   pockets

47

architectures

54

neural   network   architectures
even   for   a   basic   neural   network,   there   are   
many   design   decisions   to   make:

1. #   of   hidden   layers   (depth)
2. #   of   units   per   hidden   layer   (width)
3. type   of   activation   function   (nonlinearity)
4. form   of   objective   function

55

activation   functions

neural   network   with   sigmoid   

activation   functions

(f) loss
j = 1

2 (y   y )2

output

hidden   layer

   

(e) output (sigmoid)
1+(cid:50)(cid:116)(cid:84)( b)

y =

1

(d) output (linear)

j=0  jzj

b = d

input

   

zj =

1

1+(cid:50)(cid:116)(cid:84)( aj ) ,  j

(c) hidden (sigmoid)

(b) hidden (linear)

i=0  jixi,  j

aj = m
(a) input
given xi,  i

56

activation   functions

neural   network   with   arbitrary   
nonlinear   activation   functions

output

hidden   layer

   

(f) loss
j = 1

2 (y   y )2

(e) output (nonlinear)

y =  (b)

(d) output (linear)

j=0  jzj

b = d

input

   

(c) hidden (nonlinear)
zj =  (aj),  j

(b) hidden (linear)

i=0  jixi,  j

aj = m
(a) input
given xi,  i

57

activation   functions

sigmoid   /   logistic   function

logistic(u)    

1
1+e   u

so   far,   we   ve   
assumed   that   the   
activation   function   
(nonlinearity)   is   
always   the   sigmoid   
function   

58

activation   functions

    a   new   change:   modifying   the   nonlinearity
    the   logistic   is   not   widely   used   in   modern   anns

alternate   1:   
tanh

like   logistic   function   but   
shifted   to   range   [-     1,   +1]

slide   from   william   cohen

ai   stats   2010

depth   4?

sigmoid   
vs.   
tanh

figure   from   glorot &   bentio (2010)

activation   functions

    a   new   change:   modifying   the   nonlinearity

    relu often   used   in   vision   tasks

alternate   2:   rectified   linear   unit

linear   with   a   cutoff   at   zero

(implementation:   clip   the   gradient   
when   you   pass   zero)

slide   from   william   cohen

activation   functions

    a   new   change:   modifying   the   nonlinearity

    relu often   used   in   vision   tasks

alternate   2:   rectified   linear   unit

soft   version:   log(exp(x)+1)

doesn   t   saturate   (at   one   end)
sparsifies outputs
helps   with   vanishing   gradient   

slide   from   william   cohen

objective   functions   for   nns

    regression:

    classification:

    use   the   same   objective   as   linear   regression
    quadratic   loss   (i.e.   mean   squared   error)

    use   the   same   objective   as   logistic   regression
    cross-     id178   (i.e.   negative   log   likelihood)
    this   requires   probabilities,   so   we   add   an   additional   

   softmax      layer   at   the   end   of   our   network

forward

quadratic j =

1
2

(y   y )2

cross id178 j = y  (cid:72)(cid:81)(cid:59)(y) + (1   y ) (cid:72)(cid:81)(cid:59)(1   y)

backward
dj
dy
dj
dy

= y   y 
= y  1
y

+ (1   y )

1
y   1

63

cross-     id178   vs.   quadratic   loss

figure   from   glorot &   bentio (2010)

background

a   recipe   for   

machine   learning

1.   given   training   data:

3.   define   goal:

2.   choose   each   of   these:

    decision   function

    loss   function

4.   train   with   sgd:
(take   small   steps   
opposite   the   gradient)

67

objective   functions

matching   quiz: suppose   you   are   given   a   neural   net   with   a   
single   output,   y,   and   one   hidden   layer.
1)   minimizing   sum   of   squared   
errors   
2)   minimizing   sum   of   squared   
errors   plus   squared euclidean   
norm   of   weights   
3)   minimizing cross-     id178   

   gives   

5)      id113   estimates   of   weights   assuming   
target follows   a   bernoulli   with   
parameter   given   by   the   output   value
6)      map   estimates   of weights
assuming   weight   priors   are   zero   mean   
gaussian
7)      estimates   with   a   large margin   on   
the   training   data
8)      id113   estimates   of   weights   assuming   
zero   mean   gaussian   noise   on   the   output   
value

4)   minimizing   hinge loss   

a.
b.
c.
d.

1=5,   2=7,   3=6,   4=8
1=5,   2=7,   3=8,   4=6
1=7,   2=5,   3=5,   4=7
1=7,   2=5,   3=6,   4=8

e.
f.

1=8,   2=6,   3=5,   4=7
1=8,   2=6,   3=8,   4=6

68

id26

69

background

a   recipe   for   

machine   learning

1.   given   training   data:

3.   define   goal:

2.   choose   each   of   these:

    decision   function

    loss   function

4.   train   with   sgd:
(take   small   steps   
opposite   the   gradient)

70

training

approaches   to   
differentiation

    question   1:

when   can   we   compute   the   gradients   of   the   
parameters   of   an   arbitrary   neural   network?

    question   2:

when   can   we   make   the   gradient   
computation   efficient?

71

training

approaches   to   
differentiation

1.

2.

3.

4.

   
   
   

   
   
   
   
   

   
   

   
   

   
   

   
   

finite   difference   method

pro:   great   for   testing   implementations   of   id26
con:   slow   for   high   dimensional   inputs   /   outputs
required:   ability   to   call   the   function   f(x)   on   any   input   x

symbolic   differentiation

note:   the   method   you   learned   in   high-     school
note:   used   by   mathematica   /   wolfram   alpha   /   maple
pro:   yields   easily   interpretable   derivatives
con:   leads   to   exponential   computation   time   if   not   carefully   implemented
required:   mathematical   expression   that   defines   f(x)

automatic   differentiation   -      reverse   mode

note:   called   id26 when   applied   to   neural   nets
pro:   computes   partial   derivatives   of   one   output   f(x)i with   respect   to   all   inputs   xj in   time   proportional   
to   computation   of   f(x)
con:   slow   for   high   dimensional   outputs   (e.g.   vector-     valued   functions)
required:   algorithm   for   computing   f(x)

automatic   differentiation   -      forward   mode

note:   easy   to   implement.   uses   dual   numbers.
pro:   computes   partial   derivatives   of   all   outputs   f(x)i with   respect   to   one   input   xj in   time   proportional   
to   computation   of   f(x)
con:   slow   for   high   dimensional   inputs   (e.g.   vector-     valued   x)
required:   algorithm   for   computing   f(x)

72

training

finite   difference   method

notes:
    suffers   from   issues   of   

floating   point   precision,   in   
practice

    typically   only   appropriate   
to   use   on   small   examples   
with   an   appropriately   
chosen   epsilon

73

training

symbolic   differentiation

calculus   quiz   #1:
suppose   x   =   2   and   z   =   3,   what   are   dy/dx   and   
dy/dz for   the   function   below?

74

training

symbolic   differentiation

calculus   quiz   #2:

   

   

   

75

training

chain   rule

whiteboard

    chain   rule   of   calculus

76

and h such that f = (g  h). if the inputs and outputs of g and h are vector-valued variables
then f is as well: h : rk ! rj and g : rj ! ri ) f : rk ! ri. given an input
vector x = {x1, x2, . . . , xk}, we compute the output y = {y1, y2, . . . , yi}, in terms of an
intermediate vector u = {u1, u2, . . . , uj}. that is, the computation y = f (x) = g(h(x))
can be described in a feed-forward manner: y = g(u) and u = h(x). then the chain rule
must sum over all the intermediate quantities.

rule allows us to differentiate a function f de   ned as the composition of two functions g
and h such that f = (g  h). if the inputs and outputs of g and h are vector-valued variables
then f is as well: h : rk ! rj and g : rj ! ri ) f : rk ! ri. given an input
vector x = {x1, x2, . . . , xk}, we compute the output y = {y1, y2, . . . , yi}, in terms of an
intermediate vector u = {u1, u2, . . . , uj}. that is, the computation y = f (x) = g(h(x))
can be described in a feed-forward manner: y = g(u) and u = h(x). then the chain rule
given:   
must sum over all the intermediate quantities.
chain   rule:
dyi
dyi
dxk
duj

chain   rule

dyi
8i, k
duj

training

duj
dxk

8i, k

duj
=
dxk

dyi
dxk

(2.3)

=

,

jxj=1

jxj=1

,

if the inputs and outputs of f, g, and h are all scalars, then we obtain the familiar form

if the inputs and outputs of f, g, and h are all scalars, then we obtain the familiar form

dy
dx

=

dy
du

du
dx

dy
dx

=

dy
du

du
dx

   

(2.4)

binary id28 binary id28 can be interpreted as a arithmetic
circuit. to compute the derivative of some id168 (below we use regression) with

binary id28 binary id28 can be interpreted as a arithmetic
circuit. to compute the derivative of some id168 (below we use regression) with

77

and h such that f = (g  h). if the inputs and outputs of g and h are vector-valued variables
then f is as well: h : rk ! rj and g : rj ! ri ) f : rk ! ri. given an input
vector x = {x1, x2, . . . , xk}, we compute the output y = {y1, y2, . . . , yi}, in terms of an
intermediate vector u = {u1, u2, . . . , uj}. that is, the computation y = f (x) = g(h(x))
can be described in a feed-forward manner: y = g(u) and u = h(x). then the chain rule
must sum over all the intermediate quantities.

rule allows us to differentiate a function f de   ned as the composition of two functions g
and h such that f = (g  h). if the inputs and outputs of g and h are vector-valued variables
then f is as well: h : rk ! rj and g : rj ! ri ) f : rk ! ri. given an input
vector x = {x1, x2, . . . , xk}, we compute the output y = {y1, y2, . . . , yi}, in terms of an
intermediate vector u = {u1, u2, . . . , uj}. that is, the computation y = f (x) = g(h(x))
can be described in a feed-forward manner: y = g(u) and u = h(x). then the chain rule
given:   
must sum over all the intermediate quantities.
chain   rule:
dyi
dyi
dxk
duj

chain   rule

dyi
8i, k
duj

training

duj
dxk

8i, k

duj
=
dxk

dyi
dxk

(2.3)

=

,

if the inputs and outputs of f, g, and h are all scalars, then we obtain the familiar form

if the inputs and outputs of f, g, and h are all scalars, then we obtain the familiar form

jxj=1

jxj=1

,

id26
is   just   repeated   
dy
du
dy
dy
application   of   the   
dx
dx
du
chain   rule   from   
dx
calculus   101.

=

=

dy
du

du
dx

   

(2.4)

binary id28 binary id28 can be interpreted as a arithmetic
circuit. to compute the derivative of some id168 (below we use regression) with

binary id28 binary id28 can be interpreted as a arithmetic
circuit. to compute the derivative of some id168 (below we use regression) with

78

training

id26

whiteboard

    example:   id26   for   calculus   quiz   #1

calculus   quiz   #1:
suppose   x   =   2   and   z   =   3,   what   are   dy/dx   
and   dy/dz   for   the   function   below?

79

training

id26

automatic   differentiation       reverse   mode   (aka.   id26)

forward   computation
1. write   an   algorithm for   evaluating   the   function   y   =   f(x).   the   

algorithm   defines   a   directed   acyclic   graph,   where   each   variable   is   a   
node   (i.e.   the      computation   graph   )
2. visit   each   node   in   topological   order.   
for   variable   ui with   inputs   v1,   ,   vn
a. compute   ui =   gi(v1,   ,   vn)
b. store   the   result   at   the   node

backward   computation
1.
2. visit   each   node   in   reverse   topological   order.   

initialize all   partial   derivatives   dy/duj to   0   and   dy/dy =   1.
for   variable   ui =   gi(v1,   ,   vn)
a. we   already   know   dy/dui
b.

increment   dy/dvj by   (dy/dui)(dui/dvj)
(choice   of   algorithm   ensures   computing   (dui/dvj)   is   easy)

return   partial   derivatives   dy/dui   for   all   variables

80

training

id26

simple example: the goal is to compute j = (cid:43)(cid:81)(cid:98)((cid:98)(cid:66)(cid:77)(x2) + 3x2)
on the forward pass and the derivative dj

dx on the backward pass.

forward

j = cos(u)

u = u1 + u2

u1 = sin(t)

u2 = 3t

t = x2

(cid:89)=  sin(u)
du
(cid:89)=
du1
du1
dt
du2
dt

backward
dj
du
dj
du1
dj
dt
dj
dt
dj
dx

dj
du
dj
du1
dj
du2
dj
dt

dt
dx

(cid:89)=

(cid:89)=

(cid:89)=

dj
du2

(cid:89)=

dj
du

du
du2

,

du
du2

= 1

,

,

,

du
du1
du1
dt
du2
dt

= 1

= (cid:43)(cid:81)(cid:98)(t)

= 3

,

dt
dx

= 2x

81

training

id26

simple example: the goal is to compute j = (cid:43)(cid:81)(cid:98)((cid:98)(cid:66)(cid:77)(x2) + 3x2)
on the forward pass and the derivative dj

dx on the backward pass.

forward

j = cos(u)

u = u1 + u2

u1 = sin(t)

u2 = 3t

t = x2

(cid:89)=  sin(u)
du
(cid:89)=
du1
du1
dt
du2
dt

backward
dj
du
dj
du1
dj
dt
dj
dt
dj
dx

dj
du
dj
du1
dj
du2
dj
dt

dt
dx

(cid:89)=

(cid:89)=

(cid:89)=

dj
du2

(cid:89)=

dj
du

du
du2

,

du
du2

= 1

,

,

,

du
du1
du1
dt
du2
dt

= 1

= (cid:43)(cid:81)(cid:98)(t)

= 3

,

dt
dx

= 2x

82

training

id26

whiteboard

    sgd   for   neural   network
    example:   id26   for   neural   network

83

training

case   1:
logistic   
regression

id26

output

input

  1

  2

  3

  m

   

forward
j = y  (cid:72)(cid:81)(cid:59) y + (1   y ) (cid:72)(cid:81)(cid:59)(1   y)

y =

a =

1

1 + (cid:50)(cid:116)(cid:84)( a)
d j=0

 jxj

=

backward
y 
dj
y
dy
dj
dj
dy
da

=

(1   y )
y   1
dy
=
da

,

+

dy
da

dj
da

da
d j

,

da
d j

dj
d j

dj
dxj

=

=

(cid:50)(cid:116)(cid:84)( a)

((cid:50)(cid:116)(cid:84)( a) + 1)2
= xj

dj
da

da
dxj

,

da
dxj

=  j

84

training

id26

(f) loss
j = 1

2 (y   y(d))2

output

hidden   layer

   

(e) output (sigmoid)
1+(cid:50)(cid:116)(cid:84)( b)

y =

1

(d) output (linear)

j=0  jzj

b = d

input

   

zj =

1

1+(cid:50)(cid:116)(cid:84)( aj ) ,  j

(c) hidden (sigmoid)

(b) hidden (linear)

i=0  jixi,  j

aj = m
(a) input
given xi,  i

85

training

id26

(f) loss
j = 1

2 (y   y )2

output

hidden   layer

   

(e) output (sigmoid)
1+(cid:50)(cid:116)(cid:84)( b)

y =

1

(d) output (linear)

j=0  jzj

b = d

input

   

zj =

1

1+(cid:50)(cid:116)(cid:84)( aj ) ,  j

(c) hidden (sigmoid)

(b) hidden (linear)

i=0  jixi,  j

aj = m
(a) input
given xi,  i

86

training

id26

case   2:
neural   
network

   

   

forward
j = y  (cid:72)(cid:81)(cid:59) y + (1   y ) (cid:72)(cid:81)(cid:59)(1   y)

y =

b =

1

1 + (cid:50)(cid:116)(cid:84)( b)
d j=0

 jzj

zj =

aj =

1

1 + (cid:50)(cid:116)(cid:84)( aj)
m i=0

 jixi

=

backward
y 
dj
y
dy
dj
dj
dy
db

=

+

dy
db

(1   y )
y   1
dy
=
db

,

dj
d j

dj
dzj
dj
daj

=

dj
db

db
d j

,

db
d j

=

=

dj
db
dj
dzj

,

db
dzj
dzj
daj

,

db
dzj
dzj
daj

=  j

=

dj
d ji

=

dj
daj

daj
d ji

,

daj
d ji

dj
dxi

=

dj
daj

daj
dxi

,

daj
dxi

=

(cid:50)(cid:116)(cid:84)( b)

((cid:50)(cid:116)(cid:84)( b) + 1)2
= zj

(cid:50)(cid:116)(cid:84)( aj)

((cid:50)(cid:116)(cid:84)( aj) + 1)2
= xi

 ji

d j=0

87

training

id26

id26   (auto.diff.   -      reverse   mode)

forward   computation
1. write   an   algorithm for   evaluating   the   function   y   =   f(x).   the   

algorithm   defines   a   directed   acyclic   graph,   where   each   variable   is   a   
node   (i.e.   the      computation   graph   )
2. visit   each   node   in   topological   order.   

a. compute   the   corresponding   variable   s   value
b. store   the   result   at   the   node

backward   computation
3.
4. visit   each   node   in   reverse   topological   order.   

initialize all   partial   derivatives   dy/duj to   0   and   dy/dy =   1.

for   variable   ui =   gi(v1,   ,   vn)
a. we   already   know   dy/dui
b.

increment   dy/dvj by   (dy/dui)(dui/dvj)
(choice   of   algorithm   ensures   computing   (dui/dvj)   is   easy)

return   partial   derivatives   dy/dui   for   all   variables

88

training

id26

case   2:
neural   
module   5
network

module   4

   

   

module   3

forward
j = y  (cid:72)(cid:81)(cid:59) y + (1   y ) (cid:72)(cid:81)(cid:59)(1   y)

y =

b =

1

1 + (cid:50)(cid:116)(cid:84)( b)
d j=0

 jzj

module   2

zj =

aj =

module   1

1

1 + (cid:50)(cid:116)(cid:84)( aj)
m i=0

 jixi

=

backward
y 
dj
y
dy
dj
dj
dy
db

=

+

dy
db

(1   y )
y   1
dy
=
db

,

dj
d j

dj
dzj
dj
daj

=

dj
db

db
d j

,

db
d j

=

=

dj
db
dj
dzj

,

db
dzj
dzj
daj

,

db
dzj
dzj
daj

=  j

=

dj
d ji

=

dj
daj

daj
d ji

,

daj
d ji

dj
dxi

=

dj
daj

daj
dxi

,

daj
dxi

=

(cid:50)(cid:116)(cid:84)( b)

((cid:50)(cid:116)(cid:84)( b) + 1)2
= zj

(cid:50)(cid:116)(cid:84)( aj)

((cid:50)(cid:116)(cid:84)( aj) + 1)2
= xi

 ji

d j=0

89

background

a   recipe   for   

gradients

machine   learning

1.   given   training   data:

3.   define   goal:

id26 can   compute   this   
gradient!   
and   it   s   a   special   case   of   a   more   
general   algorithm   called   reverse-     
mode   automatic   differentiation   that   
4.   train   with   sgd:
can   compute   the   gradient   of   any   
differentiable   function   efficiently!
(take   small   steps   
opposite   the   gradient)

2.   choose   each   of   these:

    decision   function

    loss   function

90

summary

1. neural   networks   

    provide   a   way   of   learning   features
    are   highly   nonlinear   prediction   functions
    (can   be)   a   highly   parallel   network   of   logistic   

regression   classifiers

    discover   useful   hidden   representations   of   the   

input

2. id26   

    provides   an   efficient   way   to   compute   gradients
    is   a   special   case   of   reverse-     mode   automatic   

differentiation

91

