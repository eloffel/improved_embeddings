10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

learning   theory

learning   theory   readings:
murphy   -     -     
bishop   -     -     
htf   -     -     
mitchell   7

matt   gorid113y
lecture   13
march   1,   2016

1

reminders

    homework 4:   id88 /   kernels /   id166

    release:   wed,   feb.   22
    due:   fri,   mar.   03   at   11:59pm

    midterm exam (evening exam)
    tue,   mar.   07   at   7:00pm       9:30pm
    see piazza   for details about location

9   days
for   hw4

2

outline

    generative   vs.   discriminative   modeling
    bayes   optimal   classifier

    loss   function,   expected   loss
    bayes   error
    bayes   optimal

    consistency   of   id113

    consistency
    almost   sure   convergence   of   id113

3

discriminative   and   
generative   classifiers

4

generative   vs.   discriminative

    generative   classifiers:
    example:   na  ve   bayes
    define   a   joint   model   of   the   observations   x and   the   

labels   y:

p(x, y)

    learning   maximizes   (joint)   likelihood
    use   bayes      rule   to   classify   based   on   the   posterior:

    discriminative   classifiers:

p(y|x) = p(x|y)p(y)/p(x)
    example:   logistic   regression
    directly   model   the   conditional:      
    learning   maximizes   conditional   likelihood

p(y|x)

5

generative   vs.   discriminative

whiteboard

    contrast:   to   model   p(x)   or   not   to   model   p(x)?

6

generative   vs.   discriminative
finite   sample   analysis   (ng   &   jordan,   2002)
[assume   that   we   are   learning   from   a   finite   
training   dataset]

if   model   assumptions   are   correct:   naive   bayes   is   a   more   
efficient   learner   (requires   fewer   samples)   than   logistic   
regression

if   model   assumptions   are   incorrect:   logistic   regression   has   
lower   asymtotic error,   and   does   better   than   na  ve   bayes

7

solid:   nb   dashed:   lr

slide   courtesy   of   william   cohen

8

solid:   nb   dashed:   lr

na  ve   bayes   makes   stronger   assumptions   about   the   data
but   needs   fewer   examples   to   estimate   the   parameters

   on   discriminative   vs generative   classifiers:      .      andrew   ng   
and   michael   jordan,   nips   2001.

slide   courtesy   of   william   cohen

9

generative   vs.   discriminative

learning   (parameter   estimation)

na  ve   bayes:   
parameters   are   decoupled         closed   form   solution   for   id113

logistic   regression:   
parameters   are   coupled         no   closed   form   solution       must   
use   iterative   optimization   techniques   instead

10

na  ve   bayes   vs.   logistic   reg.
learning   (map   estimation   of   parameters)

bernoulli   na  ve   bayes:   
parameters   are   probabilities         beta   prior   (usually)   pushes   
probabilities   away   from   zero   /   one   extremes

logistic   regression:   
parameters   are   not   probabilities         gaussian   prior   
encourages   parameters   to   be   close   to   zero   

(effectively   pushes   the   probabilities   away   from   zero   /   one   
extremes)

11

na  ve   bayes   vs.   logistic   reg.

features

na  ve   bayes:   
features   x are   assumed   to   be   conditionally   independent   
given   y.   (i.e.   na  ve   bayes   assumption)

logistic   regression:   
no   assumptions   are   made   about   the   form   of   the   features   x.      
they   can   be   dependent   and   correlated   in   any   fashion.   

12

learning   theory

14

outline

    generative   vs.   discriminative   modeling
    bayes   optimal   classifier

    loss   function,   expected   loss
    bayes   error
    bayes   optimal

    consistency   of   id113

    consistency
    almost   sure   convergence   of   id113

15

questions   for   today

1. given   a   id203   distribution,   how   do   we   

construct   the   optimal   classifier?
(bayes   optimal   classifier)

2. given   data,   what   guarantees   do   we   have   

about   the   id113   parameters?
(consistency   of   id113)

16

bayes   optimal   classifier

whiteboard:

    loss   function,   expected   loss
    bayes   error
    bayes   optimal

17

questions   for   today

1. given   a   id203   distribution,   how   do   we   

construct   the   optimal   classifier?
(bayes   optimal   classifier)

2. given   data,   what   guarantees   do   we   have   

about   the   id113   parameters?
(consistency   of   id113)

18

consistency   of   id113

whiteboard:

    consistency
    almost   sure   convergence   of   id113

19

midterm   exam   logistics

20

midterm   exam

    logistics

    evening   exam

tue,   mar.   07   at   7:00pm       9:30pm

    8-     9   sections
    format   of   questions:

    multiple   choice
    true   /   false   (with   justification)
    derivations
    short   answers
    interpreting   figures

    no   electronic   devices
    you   are   allowed   to   bring one   8     x   11   sheet   of   notes   

(front   and   back)

21

midterm   exam

    how   to   prepare

    attend   the   midterm   review   session:   

thu,   march   2   at   6:30pm   (ph   100)

    attend   the   midterm   review   lecture

mon,   march   6   (in-     class)

    review   prior   year   s   exam   and   solutions

(we   ll   post   them)

    review   this   year   s   homework   problems

22

midterm   exam

    advice   (for   during   the   exam)
    solve   the   easy   problems   first   

(e.g.   multiple   choice   before   derivations)
    if   a   problem   seems   extremely   complicated   you   re   likely   

missing   something

    don   t   leave   any   answer   blank!
    if   you   make   an   assumption,   write   it   down
    if   you   look   at   a   question   and   don   t   know   the   

answer:
    we   probably   haven   t   told   you   the   answer
    but   we   ve   told   you   enough   to   work   it   out
    imagine   arguing   for   some   answer   and   see   if   you   like   it

23

topics   for   midterm
    regression

    foundations
    id203
    id113,   map
    optimization

    classifiers

    knn
    na  ve   bayes
    logistic   regression
    id88
    id166

    linear   regression

    important   concepts

    kernels
    id173   and   

overfitting

    experimental   design

24

