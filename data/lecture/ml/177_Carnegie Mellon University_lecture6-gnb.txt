10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

gaussian   na  ve   bayes

na  ve   bayes   readings:
   generative   and   discriminative   
classifiers:   naive   bayes   and   logistic   
regression      
(mitchell,   2016)

optimization   readings:   
(next   lecture)
lecture   notes   from   10-     600   (see   
piazza   note)

murphy   3
bishop   -     -     
htf   -     -     
mitchell   6.1-     6.10

   convex   optimization      boyd   and   
vandenberghe (2009)      [see   chapter   
9.   this   advanced   reading   is   entirely   
optional.]

matt   gorid113y
lecture   6
february   6,   2016

1

reminders

    homework 2:   naive bayes

    release:   wed,   feb.   1
    due:   mon,   feb.   13   at   5:30pm

    homework 3:   linear   /   id28

    release:   mon,   feb.   13
    due:   wed,   feb.   22   at   5:30pm

2

na  ve   bayes   outline

    probabilistic   (generative)   view   of   

classification
    decision   rule   for   id203   model

    real-     world   dataset

    economist   vs.   onion   articles
    document         bag-     of-     words         binary   feature   

vector

    naive   bayes:   model

    generating   synthetic   "labeled   documents"
    definition   of   model
    naive   bayes   assumption
    counting   #   of   parameters   with   /   without   nb   

assumption

    na  ve   bayes:   learning   from   data

    data   likelihood
    id113   for   naive   bayes
    map   for   naive   bayes

    visualizing   gaussian   naive   bayes

last   lecture

this   lecture

3

naive   bayes:   model

whiteboard

    generating   synthetic   "labeled   documents"
    definition   of   model
    naive   bayes   assumption
    counting   #   of   parameters   with   /   without   nb   

assumption

4

what   s   wrong   with   the   
na  ve   bayes   assumption?
the   features   might   not   be   independent!!
    example   1:

    if   a   document   contains   the   word   
   donald   ,   it   s   extremely   likely   to   
contain   the   word      trump   
    these   are   not   independent!

    example   2:

    if   the   petal   width   is   very   high,   
the   petal   length   is   also   likely   to   
be   very   high

5

na  ve   bayes:   learning   from   data

whiteboard

    data   likelihood
    id113   for   naive   bayes
    map   for   naive   bayes

6

visualizing   na  ve   bayes

slides   in   this   section   from   william   cohen   (10-     601b,   spring   2016)

7

fisher   iris   dataset

fisher   (1936)   used   150   measurements   of   flowers   
from   3   different   species:   iris   setosa (0),   iris   
virginica (1),   iris   versicolor (2)   collected   by   
anderson   (1936)

species

0
0
0
1
1
1
1

sepal   
length
4.3
4.9
5.3
4.9
5.7
6.3
6.7

sepal   
width
3.0
3.6
3.7
2.4
2.8
3.3
3.0

petal   
length
1.1
1.4
1.5
3.3
4.1
4.7
5.0

petal   
width
0.1
0.1
0.2
1.0
1.3
1.6
1.7

full   dataset:   https://en.wikipedia.org/wiki/iris_flower_data_set

9

slide   from   william   cohen

slide   from   william   cohen

plot   the   difference   of   the   probabilities

z-     axis   is   the   difference   of   the   posterior   

probabilities:   p(y=1   |   x)       p(y=0   |   x)

slide   from   william   cohen

question:   what   does   the   boundary   between   positive   and   

negative   look   like   for   na  ve   bayes?

slide   from   william   cohen   (10-     601b,   spring   2016)

iris   data   (2   classes)

14

iris   data   (sigma not   shared)

15

iris   data   (sigma=1)

16

iris   data   (3   classes)

17

iris   data   (sigma not   shared)

18

iris   data   (sigma=1)

19

na  ve   bayes   has   a   linear decision   boundary   

(if   sigma   is   shared   across   classes)

slide   from   william   cohen   (10-     601b,   spring   2016)

figure   from   william   cohen   (10-     601b,   spring   2016)

figure   from   william   cohen   (10-     601b,   spring   2016)

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

beyond   the   scope   of   this   lecture
    multinomial na  ve   bayes   can   be   used   for   

integer features

    multi-     class   na  ve   bayes   can   be   used   if   your   

classification   problem   has   >   2   classes

23

summary

1. na  ve   bayes   provides   a   framework   for   

generative   modeling

2. choose   p(xm |   y)   appropriate   to   the   data

(e.g.   bernoulli   for   binary   features,   
gaussian   for   continuous   features)

3. train   by   id113 or   map
4. classify   by   maximizing   the   posterior

24

