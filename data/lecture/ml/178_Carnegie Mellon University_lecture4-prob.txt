10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

the   probabilistic   

approach   to   learning   

from   data

prob.   readings:
lecture   notes   from   10-     600   
(see   piazza   post   for   the   pointers)

murphy   2
bishop   2
htf   -     -     
mitchell   -     -     

matt   gorid113y
lecture   4
january   30,   2016

1

reminders
    website schedule updated
    background exercises (homework 1)

    released:   wed,   jan.   25
    due:   wed,   feb.   1   at   5:30pm

(the deadline was extended!)

    homework 2:   naive bayes

    released:   wed,   feb.   1
    due:   mon,   feb.   13   at   5:30pm

2

outline

    generating   data

    natural   (stochastic)   data
    synthetic   data
    why   synthetic   data?
    examples:   multinomial,   bernoulli,   gaussian

    data   likelihood

   

   

    independent   and   identically   distributed   (i.i.d.)
    example:   dice   rolls
learning   from   data   (frequentist)
    principle   of   maximum   likelihood   estimation   (id113)
    optimization   for   id113
    examples:   1d   and   2d   optimization
    example:   id113   of   multinomial
    aside:   method   of   langrange multipliers
learning   from   data   (bayesian)
    maximum   a   posteriori   (map)   estimation
    optimization   for   map
    example:   map   of   bernoulli   beta   

3

generating   data

whiteboard

    natural   (stochastic)   data
    synthetic   data
    why   synthetic   data?
    examples:   multinomial,   bernoulli,   gaussian

4

in-     class   exercise

1. with   your   neighbor,   write   a   function   which   

returns   samples   from   a   categorical
    assume   access   to   the   rand() function
    function   signature   should   be:
categorical_sample(phi)
where   phi   is   the   array   of   parameters

    make   your   implementation   as   efficient as   

possible!

2. what   is   the   expected   runtime of   your   

function?

5

data   likelihood

whiteboard

    independent   and   identically   distributed   (i.i.d.)
    example:   dice   rolls

6

learning   from   data   (frequentist)

whiteboard

    principle   of   maximum   likelihood   estimation   

(id113)

    optimization   for   id113
    examples:   1d   and   2d   optimization
    example:   id113   of   multinomial
    aside:   method   of   langrange multipliers

7

learning   from   data   (bayesian)

whiteboard

    maximum   a   posteriori   (map)   estimation
    optimization   for   map
    example:   map   of   bernoulli   beta   

8

takeaways

    one   view   of   what   ml   is   trying   to   accomplish   is   

function   approximation

    the   principle   of   maximum   likelihood   

estimation   provides   an   alternate   view   of   
learning

    synthetic   data   can   help   debug ml   algorithms
    id203   distributions   can   be   used   to   model

real   data   that   occurs   in   the   world
(don   t   worry   we   ll   make   our   distributions   more   
interesting   soon!)

9

the   remaining   slides   are   extra   

slides for   your   reference.

since   they   are   background   

material   they   were   not   

(explicitly)   covered   in   class.

10

outline   of   extra   slides

    random   variables,   id203   mass   function   (pmf),   id203   

density   function   (pdf),   cumulative   distribution   function   (cdf)

    id203   theory

    sample   space,   outcomes,   events
    kolmogorov   s   axioms   of   id203

    random   variables

    examples
    notation
    expectation   and   variance
    joint,   conditional,   marginal   probabilities
    independence
    bayes      rule

    common   id203   distributions

    beta,   dirichlet,   etc.

11

id203   theory

12

id203   theory:   definitions

example   1:   flipping   a   coin
sample   space

 

{heads, tails}

outcome

event

id203

     
e    
p (e)

example:   heads

example:   {heads}
p({heads}) = 0.5
p({tails}) = 0.5

13

id203   theory:   definitions
id203   provides   a   science   for   id136   
about   interesting   events
sample   space

the   set   of   all   possible   outcomes

 

possible result   of   an   experiment

any   subset   of   the   sample   space

outcome

event

id203

     
e    
p (e)
    each   outcome   is   unique
    only   one   outcome   can   occur   per   experiment
    an   outcome   can   be   in   multiple   events
    an   elementary   event   consists   of   exactly   one   outcome

the   non-     negative   number   assigned   
to   each   event   in   the   sample   space

14

id203   theory:   definitions

example   2:   rolling   a   6-     sided   die
{1,2,3,4,5,6}
sample   space

 

outcome

event

id203

     
e    
p (e)

example:   3

example:   {3}   
(the event      the   die came   up   3   )
p({3}) = 1/6
p({4}) = 1/6

15

id203   theory:   definitions

example   2:   rolling   a   6-     sided   die
{1,2,3,4,5,6}
sample   space

 

outcome

event

id203

     
e    
p (e)

example:   3

example:   {2,4,6}   
(the event      the   roll   was even   )
p({2,4,6}) = 0.5
p({1,3,5}) = 0.5

16

id203   theory:   definitions

example   3:   timing   how   long   it   takes   a   monkey   to   
reproduce   shakespeare
sample   space

[0,   +   )

 

outcome

event

id203

     
e    
p (e)

example:   1,433,600   hours

example:   [1,   6]   hours
p([1,6]) = 0.000000000001
p([1,433,600,   +   )) = 0.99

17

kolmogorov   s   axioms

1. p (e)   0, for all events e
2. p ( ) = 1
3. if e1, e2, . . . are disjoint, then

p (e1 or e2 or . . .) = p (e1) + p (e2) + . . .

18

kolmogorov   s   axioms

1. p (e)   0, for all events e
2. p ( ) = 1
3. if e1, e2, . . . are disjoint, then

p    i=1

ei  =

  i=1

p (ei)

all   of   

id203   can   

be   derived   
from   just   
these!

in   words:
1. each   event   has   non-     negative   id203.
2. the   id203   that   some event   will   occur   is   one.
3. the   id203   of   the   union   of   many   disjoint   sets   is   

the   sum   of   their   probabilities

19

id203   theory:   definitions

    the   complement of   an   event   e,   denoted   ~e,   

is   the   event   that   e does   not   occur.

 

e

~e

20

random   variables

21

random   variables:   definitions

random
variable

x
(capital
letters)

def 1:   variable whose   possible   values   
are   the   outcomes   of   a   random   
experiment

value   of   a   
random
variable

x
letters)

(lowercase

the   value   taken   by   a   random   variable

22

random   variables:   definitions

random
variable

x

def 1:   variable whose   possible   values   
are   the   outcomes   of   a   random   
experiment

discrete
random   
variable
continuous   
random
variable

x

x

random   variable   whose   values   come   
from   a   countable   set   (e.g.   the   natural   
numbers   or   {true,   false})
random   variable   whose   values   come   
from   an interval   or   collection   of   
intervals   (e.g.   the   real   numbers   or   the   
range   (3,   5))

23

random   variables:   definitions

random
variable

x

def 1:   variable whose   possible   values   
are   the   outcomes   of   a   random   
experiment

def 2:   a   measureable   function   from   
the   sample   space   to   the   real   numbers:

x :     e

random   variable   whose   values   come   
from   a   countable   set   (e.g.   the   natural   
numbers   or   {true,   false})
random   variable   whose   values   come   
from   an interval   or   collection   of   
intervals   (e.g.   the   real   numbers   or   the   
range   (3,   5))

24

discrete
random   
variable
continuous   
random
variable

x

x

random   variables:   definitions

discrete   
random
variable
id203   
mass   
function   
(pmf)

x

p(x)

random   variable   whose   values   come   
from   a   countable   set   (e.g.   the   natural   
numbers   or   {true,   false})
function   giving   the   id203 that   
discrete   r.v.   x   takes   value   x.

p(x) := p (x = x)

25

random   variables:   definitions

example   2:   rolling   a   6-     sided   die
{1,2,3,4,5,6}
sample   space

 

outcome

event

id203

     
e    
p (e)

example:   3

example:   {3}   
(the event      the   die came   up   3   )
p({3}) = 1/6
p({4}) = 1/6

26

random   variables:   definitions

example   2:   rolling   a   6-     sided   die
{1,2,3,4,5,6}
sample   space

 

outcome

event

id203

discrete   ran-     
dom variable
prob. mass   
function   
(pmf)

     
e    
p (e)

x

p(x)

example:   3

example:   {3}   
(the event      the   die came   up   3   )
p({3}) = 1/6
p({4}) = 1/6
example:   the   value   on   the   top   face
of   the   die.
p(3)   =   1/6
p(4)   =   1/6

27

random   variables:   definitions

example   2:   rolling   a   6-     sided   die
{1,2,3,4,5,6}
sample   space

 

outcome

event

id203

discrete   ran-     
dom variable
prob. mass   
function   
(pmf)

     
e    
p (e)

x

p(x)

example:   3

example:   {2,4,6}   
(the event      the   roll   was even   )
p({2,4,6}) = 0.5
p({1,3,5}) = 0.5
example:   1   if   the die   landed   on   an   
even   number   and   0   otherwise
p(1)   =   0.5
p(0)   =   0.5

28

random   variables:   definitions

discrete   
random
variable
id203   
mass   
function   
(pmf)

x

p(x)

random   variable   whose   values   come   
from   a   countable   set   (e.g.   the   natural   
numbers   or   {true,   false})
function   giving   the   id203 that   
discrete   r.v.   x   takes   value   x.

p(x) := p (x = x)

29

random   variables:   definitions

continuous   
random
variable

x

random   variable   whose   values   come   
from   an interval   or   collection   of   
intervals   (e.g.   the   real   numbers   or   the   
range   (3,   5))
function   the returns   a   nonnegative   
real indicating   the   relative   likelihood   
that   a   continuous   r.v.   x   takes   value   x

f (x)

id203   
density   
function   
(pdf)
    for   any   continuous   random   variable:   p(x = x) = 0
    non-     zero   probabilities   are   only   available   to   intervals:   

p (a   x   b) =  b

a

f (x)dx

30

random   variables:   definitions

example   3:   timing   how   long   it   takes   a   monkey   to   
reproduce   shakespeare
sample   space

[0,   +   )

 

outcome

event

id203

continuous   
random var.
prob.   density   
function

     
e    
p (e)

x

f (x)

example:   1,433,600   hours

example:   [1,   6]   hours
p([1,6]) = 0.000000000001
p([1,433,600,   +   )) = 0.99
example:   represents   time   to   
reproduce   (not an interval!)
example:   gamma   distribution

31

random   variables:   definitions
   region   -     valued   random   variables

sample   space
events

  
x

{1,2,3,4,5}
the   sub-     regions   1,   2,   3,   4,   or   5

x

discrete   ran-     
dom variable
prob.   mass fn. p(x=x) proportional to   size   of   sub-     region

represents   a   random   selection   of   a   
sub-     region

x=1

x=2

x=3

x=4

x=5

32

random   variables:   definitions
   region   -     valued   random   variables

sample   space
events

  
x

all points   in   the   region:   
the   sub-     regions   1,   2,   3,   4,   or   5

recall that   an   event   
is   any   subset   of   the   

x
sample   space.

discrete   ran-     
dom variable
prob.   mass fn. p(x=x) proportional to   size   of   sub-     region

represents   a   random   selection   of   a   
sub-     region

so   both   definitions   
of   the   sample   space   

here   are   valid.
x=1

x=3

x=2

x=4

x=5

33

random   variables:   definitions

string-     valued   random   variables

sample   space

event

discrete   ran-     
dom variable
id203

english:

korean:

  

x

x

all   korean   sentences   
(an   infinitely   large   set)
translation of   an   english   sentence   
into   korean   (i.e.   elementary   events)
represents   a   translation

p(x=x) given   by   a   model

machine   learning   requires   id203   and   statistics

p( x =                                                                    )
p( x =                                                                    )

                                           
                                        

p( x =                                                                    )
                                                     
   

34

random   variables:   definitions

cumulative
distribution   
function

f (x)

function that   returns   the   id203   
that   a   random   variable   x   is   less   than   or   
equal   to   x:

    for   discrete random   variables:

    for   continuous random   variables:

f (x) = p (x   x) =  x <x
f (x) = p (x   x) =  x

  

f (x) = p (x   x)
p (x = x ) =  x <x

p(x )

f (x )dx 

35

random   variables   and   events

question:   something   seems   wrong   
    we   defined   p(e) (the   capital      p   )   as   

a   function   mapping   events to   
probabilities

    so   why   do   we   write   p(x=x)?
    a   good   guess:   x=x is   an   event   

random
variable

def 2:   a   measureable   
function   from   the   
sample   space   to   the   
real   numbers:

x :     r

answer:   p(x=x) is   just   shorthand!
example   1:

these   sets   are   events!

example   2:   

p (x = x)   p ({      : x( ) = x})
p (x   7)   p ({      : x( )   7})

36

notational   shortcuts

a   convenient   shorthand:

p (a|b) =

p (a, b)

p (b)

  for all values of a and b:

p (a = a|b = b) =

p (a = a, b = b)

p (b = b)

37

notational   shortcuts

but   then   how   do   we   tell   p(e) apart   from   p(x) ?

event

random
variable

instead   of   writing:

p (a|b) =

p (a, b)

p (b)

we   should   write:

pa|b(a|b) =

pa,b(a, b)

pb(b)

   but   only   id203   theory   textbooks   go   to   such   lengths.

38

expectation   and   variance

the   expected   value   of   x is   e[x].   also   called   the   mean.

xp(x)

    discrete   random   variables:
suppose x can take any value in the set x .
e[x] =  x x
e[x] =  + 

    continuous   random   variables:

xf (x)dx

  

39

expectation   and   variance

the   variance of   x is   var(x).

    discrete   random   variables:

v ar(x) = e[(x   e[x])2]
v ar(x) =  x x
v ar(x) =  + 

(x     )2p(x)
    continuous   random   variables:

(x     )2f (x)dx

  

  =

e[x]

40

joint   id203
marginal   id203
conditional   id203
multiple   random   variables

41

joint   id203
joint id203

    key concept: two or more random variables may interact.
thus, the id203 of one taking on a certain value depends on
which value(s) the others are taking.
    we call this a joint ensemble and write

p(x, y) = prob(x = x and y = y)

z

p(x,y,z)

y

x

slide   from   sam   roweis (mlss,   2005)

42

marginal   probabilities

marginal probabilities

    we can    sum out    part of a joint distribution to get the marginal
distribution of a subset of variables:

p(x) ="y

p(x, y)

    this is like adding slices of the table together.

p(x,y)

z  

z

y

y

x

x

    another equivalent de   nition: p(x) =#y p(x|y)p(y).

slide   from   sam   roweis (mlss,   2005)

43

conditional   id203

id155

    if we know that some event has occurred, it changes our belief
about the id203 of other events.
    this is like taking a    slice    through the joint table.

p(x|y) = p(x, y)/p(y)

z

p(x,y|z)

y

x

slide   from   sam   roweis (mlss,   2005)

44

independence   and   

conditional   independence

independence & conditional independence

    two variables are independent i    their joint factors:

p(x, y) = p(x)p(y)

p(x,y)

p(x)

=

x

p(y)

    two variables are conditionally independent given a third one if for
all values of the conditioning variable, the resulting slice factors:

p(x, y|z) = p(x|z)p(y|z)

   z

slide   from   sam   roweis (mlss,   2005)

45

id113   and   map

46

id113

what   does   maximizing   likelihood   accomplish?
    there   is   only   a   finite   amount   of   id203   

mass   (i.e.   sum-     to-     one   constraint)

    id113   tries   to   allocate   as   much   id203   

mass   as   possible   to   the   things   we   have   
observed   

   at   the   expense of   the   things   we   have   not
observed

47

id113   vs.   map

suppose we have data d = {x(i)}n

i=1

 id113 = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

n i=1

n i=1
p((cid:116)(i)| )
 id113 = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
 
n i=1

 map = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

maximum   likelihood   

estimate   (id113)

p((cid:116)(i)| )

p((cid:116)(i)| )p( )

48

background:   id113

example:   id113   of   exponential   distribution

    pdf of exponential( ): f (x) =  e  x
    suppose xi   exponential( ) for 1   i   n.
    find id113 for data d = {x(i)}n
    first write down log-likelihood of sample.
    compute    rst derivative, set to zero, solve for  .
    compute second derivative and check that it is

concave down at  id113.

i=1

49

background:   id113

example:   id113   of   exponential   distribution

    first write down log-likelihood of sample.

 ( ) =

(cid:72)(cid:81)(cid:59)(  (cid:50)(cid:116)(cid:84)(  x(i)))

(cid:72)(cid:81)(cid:59) f (x(i))

=

n i=1
n i=1
n i=1
= n (cid:72)(cid:81)(cid:59)( )    

=

(cid:72)(cid:81)(cid:59)( ) +   x(i)
n i=1

x(i)

(1)

(2)

(3)

(4)

50

background:   id113

example:   id113   of   exponential   distribution
    compute    rst derivative, set to zero, solve for  .

x(i)

(1)

n i=1

d ( )

d 

x(i) = 0

d
d 

=

=

n
   

n (cid:72)(cid:81)(cid:59)( )    
n i=1
   id113 =
 n

n
i=1 x(i)

(2)

(3)

51

background:   id113

example:   id113   of   exponential   distribution

    pdf of exponential( ): f (x) =  e  x
    suppose xi   exponential( ) for 1   i   n.
    find id113 for data d = {x(i)}n
    first write down log-likelihood of sample.
    compute    rst derivative, set to zero, solve for  .
    compute second derivative and check that it is

concave down at  id113.

i=1

52

id113   vs.   map

suppose we have data d = {x(i)}n

i=1

 id113 = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 map = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

n i=1
p((cid:116)(i)| )
 id113 = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
 
n i=1
p((cid:116)(i)| )p( )

 map = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
 

n i=1
n i=1

 

prior

maximum   likelihood   

estimate   (id113)

p((cid:116)(i)| )

maximum   a   posteriori

(map)   estimate

p((cid:116)(i)| )p( )

53

common   id203   
distributions

54

common   id203   distributions

    for   discrete   random   variables:

    for   continuous   random   variables:

    bernoulli
    binomial
    multinomial
    categorical
    poisson

    exponential
    gamma
    beta
    dirichlet
    laplace
    gaussian   (1d)
    multivariate   gaussian

55

email

common   id203   distributions

1 distributions

beta   distribution

id203   density   function:

f(   | ,    ) =

1

b( ,    ) x  1(1   x)    1

2 sctm

4

3

   
 
(
f

1

a product of experts (poe) [1] model p(x|   1, . . . ,    c) =
components, and the summation in the denominator is over all possible feature types.

)
 

2

|

,

, where there are c

c=1    cx

qc
pv
v=1qc

    = 0.1,   = 0.9
    = 0.5,   = 0.5
c=1    cv
    = 1.0,   = 1.0
    = 5.0,   = 5.0
    = 10.0,   = 5.0

id44 generative process

the finite ibp model generative process

0

for each topic k     {1, . . . , k}:
 k     dir( )
0
for each document m     {1, . . . , m}
   m     dir(   )
for each word n     {1, . . . , nm}

zmn     mult(1,    m)

[draw distribution over words]
0.6

0.2

0.4

[draw distribution over topics]

 

[draw topic]

for each component c     {1, . . . , c}:
0.8

1

[columns]

   c     beta(  
for each topic k     {1, . . . , k}:

c , 1)

bkc     bernoulli(   c)

[draw id203 of component c]

[rows]

email

common   id203   distributions

1 distributions

dirichlet distribution
id203   density   function:

f(   | ,    ) =

1

b( ,    ) x  1(1   x)    1

2 sctm

4

3

   
 
(
f

1

a product of experts (poe) [1] model p(x|   1, . . . ,    c) =
components, and the summation in the denominator is over all possible feature types.

)
 

2

|

,

, where there are c

c=1    cx

qc
pv
v=1qc

    = 0.1,   = 0.9
    = 0.5,   = 0.5
c=1    cv
    = 1.0,   = 1.0
    = 5.0,   = 5.0
    = 10.0,   = 5.0

id44 generative process

the finite ibp model generative process

0

for each topic k     {1, . . . , k}:
 k     dir( )
0
for each document m     {1, . . . , m}
   m     dir(   )
for each word n     {1, . . . , nm}

zmn     mult(1,    m)

[draw distribution over words]
0.6

0.2

0.4

[draw distribution over topics]

 

[draw topic]

for each component c     {1, . . . , c}:
0.8

1

[columns]

   c     beta(  
for each topic k     {1, . . . , k}:

c , 1)

bkc     bernoulli(   c)

[draw id203 of component c]

[rows]

1 distributions

common   id203   distributions

f(   | ,    ) =

b( ,    ) x  1(1   x)    1

1

dirichlet distribution
id203   density   function:

dirichlet

p(      | ) =

1

b( )

    k 1
k

k   k=1

15

10

2 sctm

p
(
~ 

|

~   
)

k=1  ( k)
k=1  k)

where b( ) =    k
 ( k
qc
pv
v=1qc

p
(
~ 

2.5

~   
)

3

1.5
0

5

a product of experts (poe) [1] model p(x|   1, . . . ,    c) =
components, and the summation in the denominator is over all possible feature types.

c=1    cv

0
0

2

|

c=1    cx

, where there are c

0.25

id44 generative process

 

1

0.5

1

0.8

0.25

 

1

0.6

0.75

for each topic k     {1, . . . , k}:
 k     dir( )
for each document m     {1, . . . , m}
   m     dir(   )
for each word n     {1, . . . , nm}

zmn     mult(1,    m)

0.4

0.2

[draw distribution over words]
1
[draw distribution over topics]

  2

0

[draw topic]

the finite ibp model generative process
0.5

1

0.8

0.6

0.75

for each component c     {1, . . . , c}:

0.2

0.4

  2

1
0
c , 1)

   c     beta(  
for each topic k     {1, . . . , k}:

bkc     bernoulli(   c)

[draw id203 of component c]

[columns]

[rows]

