10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

machine   learning   in   
practice   +   k-     nearest   

neighbors

intro   readings:
mitchell   1
htf   1,   2
murphy   1
bishop   1

knn   readings:
mitchell   8.2
htf   13.3
murphy   -     -     -     
bishop   2.5.2

matt   gorid113y
lecture   2
january   23,   2016

1

reminders

    background test

    tue,   jan.   24   at   6:30pm
    **your test   location depends on your

registration status       see piazza   for details

    background exercises (homework 1)

    released:   tue,   jan.   24   after the test
    due:   mon,   jan.   30   at   5:30pm

2

machine   learning   &   ethics

what   ethical   responsibilities   do   we   
have   as   machine   learning   experts?

some   topics   that   we   

won   t   cover   are   probably   
deserve   an   entire   course

if   our   search   results   for   news   are   
optimized   for   ad   revenue,   might   
they   reflect   gender   /   racial   /   socio-     
economic   biases?

http://arstechnica.com/

http://bing.com/

should   restrictions   be   placed   on   
intelligent   agents   that   are   capable   of   
interacting   with   the   world?

how   do   autonomous   vehicles   make   
decisions   when   all   of   the   outcomes   
are   likely   to   be   negative?

http://vizdoom.cs.put.edu.pl/

3

outline

    defining   learning   problems

    artificial   intelligence   (ai)
    mitchell   s   definition   of   learning
    example   learning   problems
    data   annotation
    the   machine   learning   framework

    classification

    binary   classification
    2d   examples
    decision   rules   /   hypotheses
    k-     nearest   neighbors   (knn)
    knn   for   binary   classification
    distance   functions
    special   cases
    choosing   k

covered   

next   
lecture

4

this   section   is   based   on   chapter   1   of   (mitchell,   1997)
defining   learning   problems

5

artificial   intelligence

the   basic   goal   of   ai   is   to   develop   intelligent   
machines.   

this   consists   of   many   sub-     goals:
    perception
    reasoning
    control   /   motion   /   manipulation
    planning
    communication
    creativity
    learning

artificial   
intelligence

machine   
learning

6

amazon   go

https://www.amazon.com/b?node=16008589011

https://www.youtube.com/watch?v=nrmmk1myrxc

7

slide   from   roni   rosenfeld

artificial   intelligence   (ai):   

example   tasks:

    identify   objects   in   an   image
    translate   from   one   human   language   to   another
    recognize   speech
    assess   risk   (e.g.   in   loan   application)
    make   decisions   (e.g.   in   loan   application)
    assess   potential   (e.g.   in   admission   decisions)
    categorize   a   complex   situation   (e.g.   medical   diagnosis)
    predict   outcome   (e.g.   medical   prognosis,   stock   prices,   

inflation,   temperature)

    predict   events   (default   on   loans,   quitting   school,   war)
    plan   ahead   under   perfect   knowledge   (chess)
    plan   ahead   under   partial   knowledge   (poker,   bridge)

     roni   rosenfeld,   2016

8

well-     posed   learning   problems

three   components:

1. task,   t
2. performance   measure,   p
3. experience,   e

mitchell   s   definition   of   learning:
a   computer   program   learns if   its   performance   
at   tasks   in   t,   as   measured   by   p,   improves   with   
experience   e.

definition   from   (mitchell,   1997)

9

example   learning   problems

(historical   perspective)
1.   learning   to   recognize   spoken   words

now

then

      the sphinx system (e.g. 
lee 1989) learns speaker-
specific strategies for 
recognizing the primitive 
sounds (phonemes) and 
words from the observed 
speech signal   neural 
network methods   hidden 
markov models      

(mitchell, 1997)

source:   https://www.stonetemple.com/great-     knowledge-     box-     
showdown/#voicestudyresults

10

example   learning   problems

(historical   perspective)

2.   learning   to   drive   an   autonomous   vehicle

then

      the alvinn system 
(pomerleau 1989) has used 
its learned strategies to drive 
unassisted at 70 miles per 
hour for 90 miles on public 
highways among other 
cars      

(mitchell, 1997)

now

waymo.com

11

example   learning   problems

(historical   perspective)

2.   learning   to   drive   an   autonomous   vehicle

now

then

      the alvinn system 
(pomerleau 1989) has used 
its learned strategies to drive 
unassisted at 70 miles per 
hour for 90 miles on public 
highways among other 
cars      

(mitchell, 1997)

https://www.geek.com/wp-     

content/uploads/2016/03/uber.jpg

12

example   learning   problems

(historical   perspective)

3.   learning   to   beat   the   masters   at   board   games

now

then

      the world   s top computer 
program for backgammon, 
td-gammon (tesauro, 
1992, 1995), learned its 
strategy by playing over one 
million practice games 
against itself      

(mitchell, 1997)

13

example   learning   problems

3.   learning   to   beat   the   masters   at   chess

1. task,   t:

2. performance   measure,   p:   

3. experience,   e:   

14

example   learning   problems

4.   learning   to   respond   to   voice   commands   (siri)

1. task,   t:

2. performance   measure,   p:   

3. experience,   e:

15

capturing   the   knowledge   of   experts

1980

1990

2000

2010

solution   #1:   expert   systems
    over   20   years   ago,   we   
had   rule   based   systems

    ask   the   expert   to
1. obtain   a   phd   in   

2.

linguistics
introspect   about   the   
structure   of   their   native   
language

3. write   down   the   rules   

they   devise

give me directions to starbucks
if:    give me directions to x   
then: directions(here, nearest(x))

how do i get to starbucks?
if:    how do i get to x   
then: directions(here, nearest(x))

where is the nearest starbucks?
if:    where is the nearest x   
then: directions(here, nearest(x))

16

capturing   the   knowledge   of   experts

1980

1990

2000

2010

solution   #1:   expert   systems
    over   20   years   ago,   we   
had   rule   based   systems

    ask   the   expert   to
1. obtain   a   phd   in   

2.

linguistics
introspect   about   the   
structure   of   their   native   
language

3. write   down   the   rules   

they   devise

give me directions to starbucks
i need directions to starbucks
if:    give me directions to x   
if:    i need directions to x   
then: directions(here, nearest(x))
then: directions(here, nearest(x))

how do i get to starbucks?
starbucks directions
if:    how do i get to x   
if:    x directions   
then: directions(here, nearest(x))
then: directions(here, nearest(x))

where is the nearest starbucks?
is there a starbucks nearby?
if:    where is the nearest x   
if:    is there an x nearby   
then: directions(here, nearest(x))
then: directions(here, nearest(x))

17

capturing   the   knowledge   of   experts

1980

1990

2000

2010

solution   #2:   annotate   data   and   learn
    experts:

    very   good   at   answering   questions   about   specific   

cases

    not   very   good   at   telling   how they   do   it

    1990s:   so   why   not   just   have   them   tell   you   what   

they   do   on   specific   cases   and   then   let   
machine   learning   tell   you   how   to   come   to   
the   same   decisions   that   they   did

18

capturing   the   knowledge   of   experts

1980

1990

2000

2010

solution   #2:   annotate   data   and   learn
1. collect   raw   sentences   {x1,    , xn}
2. experts   annotate   their   meaning   {y1,    , yn}

x1: how do i get to starbucks?
y1: directions(here,

nearest(starbucks))

x2: show me the closest starbucks
y2: map(nearest(starbucks))

x3: send a text to john that i   ll be late
y3: txtmsg(john, i   ll be late)

x4: set an alarm for seven in the 
morning
y4: setalarm(7:00am)

19

example   learning   problems

4.   learning   to   respond   to   voice   commands   (siri)

1. task,   t:   

predicting   action   from   speech

2. performance   measure,   p:   

percent   of   correct   actions   taken   in   user   pilot   
study

3. experience,   e:   

examples   of   (speech,   action)   pairs

20

slide   from   roni   rosenfeld

the   machine   learning   framework

    formulate   a   task   as   a   mapping   from   input   to   output

    task   examples   will   usually   be   pairs:   (input,   correct_output)

    formulate   performance   as   an   error   measure

    or   more   generally,   as   an   objective   function   (aka   loss   function)

    examples:

    medical   diagnosis

    mapping   input   to   one   of   several   classes/categories         classification

    predict   tomorrow   s   temperature

    mapping   input   to   a   number            regression

    chance   of   survival: from   patient   data   to   p(survive   >=   5   years)

    mapping   input   to   id203         density   estimation

    driving   recommendation

    mapping   input   into   a   plan         planning

     roni   rosenfeld,   2016

21

slide   from   roni   rosenfeld

choices   in   ml   formulation

often,   the   same   task   can   be   formulated   in   more   than   one   way:
    ex.   1:   loan   applications   

    creditworthiness/score   (regression)
    id203   of   default   (density   estimation)
    loan   decision   (classification)

    ex.   2:   chess

    nature   of   available   training   examples/experience:

    expert   advice   (painful   to   experts)
    games   against   experts   (less   painful   but   limited,   and   not   much   control)
    experts      games   (almost   unlimited,   but   only      found   data          no   control)
    games   against   self    (unlimited,   flexible,   but   can   you   learn   this   way?)

    choice   of   target   function:   board     move vs.   board     score

     roni   rosenfeld,   2016

22

slide   from   roni   rosenfeld

how   to   approach   a   machine   learning   problem

1. consider   your   goal         definition   of   task   t

   

e.g.   make   good   loan   decisions,   win   chess   competitions,      

2. consider   the   nature   of   available   (or   potential)   experience   e

    how   much   data   can   you   get?      what   would   it   cost   (in   money,   time   or   effort)?

3. choose   type   of   output   o   to   learn

   

(numerical?   category?   id203?   plan?)   

4.choose   the   performance   measure   p (error/loss   function)

5. choose   a   representation   for   the   input   x
6.choose   a   set   of   possible   solutions   h (hypothesis   space)

    set   of   functions   h:   x         o
    (often,   by   choosing   a   representation   for   them)

7. choose   or   design   a   learning   algorithm

   

for   using   examples   (e)   to   converge   on   a   member   of   h that   optimizes   p

     roni   rosenfeld,   2016

23

classification

24

fisher   iris   dataset

fisher   (1936)   used   150   measurements   of   flowers   
from   3   different   species:   iris   setosa (0),   iris   
virginica (1),   iris   versicolor (2)   collected   by   
anderson   (1936)

species

0
0
0
1
1
1
1

sepal   
length
4.3
4.9
5.3
4.9
5.7
6.3
6.7

sepal   
width
3.0
3.6
3.7
2.4
2.8
3.3
3.0

petal   
length
1.1
1.4
1.5
3.3
4.1
4.7
5.0

petal   
width
0.1
0.1
0.2
1.0
1.3
1.6
1.7

full   dataset:   https://en.wikipedia.org/wiki/iris_flower_data_set

26

fisher   iris   dataset

classification

whiteboard:

    binary   classification
    2d   examples
    decision   rules   /   hypotheses

28

k-     nearest   neighbors

29

k-     nearest   neighbors

whiteboard:

    knn   for   binary   classification
    distance   functions

30

takeaways

    learning   problems

    defining   a   learning   problem   is   tricky
    formalizing   exposes   the   many   possibilities

    k-     nearest   neighbors

    knn   is   an   extremely   simple   algorithm   for   

classification

31

