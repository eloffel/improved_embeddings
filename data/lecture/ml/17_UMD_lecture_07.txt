slides adapted from prof. carpuat

cmsc 422 introduction to machine learning

lecture 7 the id88

furong huang / furongh@cs.umd.edu

this week

q the perception: a new model/algorithm

q its variants: voted, averaged
q convergence proof

q fundamental machine learning concepts

q online vs. batch learning
q error-driven learning
q linear separability and margin of a dataset

q project 1 published today

recap: id88 for binary 
classification

    classifier = hyperplane 
that separates positive 
from negative examples

!"=$%&'()*++-)

    id88 training

    finds such a hyperplane
    online & error-driven

learning

find algorithm that gives us ! and " for a given data set 
d	(&,()
once we know ! and " we can predict the class of a new 
data point &* by evaluating

(+=-./0(!1&*+")

many algorithms possible

   

   

   

    we learned a particular way of finding these parameters   

via the id88 update rule

   

iterative online algorithm   visits all the data over 
epochs

id88 update: 
geometric interpretation

a training example %,' is misclassified, i.e.,
let   s say '=+1

()*+,"#$- %+/    '

!"#$

id88 update: 
geometric interpretation

update: %&'(   %"#$++,, i.e., %&'(   %"#$+,

!"#$

id88 update: 
geometric interpretation

update: (%&'   ("#$++,, i.e., (%&'   ("#$+,

!"#$

!%&'

recap: id88 updates

! = 1

recap: id88 updates

! = -1

today

    example of id88 + averaged 

id88 training

    id88 convergence proof

    fundamental machine learning 

concepts
   

linear separability and margin of a dataset

standard id88: predict based on 
final parameters

predict based on final + intermediate 
parameters
    the voted id88

    the averaged id88

    require keeping track of    survival time    of

weight vectors

averaged id88 decision rule

can be rewritten as

averaged id88: predict based 
on average of intermediate parameters

convergence of id88

    the id88 has converged if it can 

classify every training example 
correctly
   

i.e. if it has found a hyperplane that correctly 
separates positive and negative examples

    under which conditions does the 

id88 converge and how long 
does it take?

convergence of id88

margin of a data set d

distance between the 
hyperplane (w,b) and 
the nearest point in d

largest attainable 

margin on d

what does this mean?
    id88 converges quickly when margin is 

large, slowly when it is small

    bound does not depend on number of training 

examples n, nor on number of features d

    proof guarantees that id88 converges, 

but not necessarily to the max margin 
separator

what you should know

    id88 concepts

   

training/prediction algorithms (standard, voting, 
averaged)

    convergence theorem and what practical guarantees it 

gives us

    how to draw/describe the decision boundary of a 

id88 classifier

    fundamental ml concepts

    determine whether a data set is linearly separable and 

define its margin

    error driven algorithms, online vs. batch algorithms

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

