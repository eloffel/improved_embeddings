10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

id88

+

kernels

id88   readings:
murphy   8.5.4
bishop   4.1.7
htf   -     -     
mitchell   4.4.0

kernel   readings:
murphy   14.1       14.2.4
bishop   6.1       6.2
htf   -     -     
mitchell   -     -     

matt   gorid113y
lecture   11
february   22,   2016

1

reminders

    homework 3:   linear   /   id28

    release:   mon,   feb.   13
    due:   wed,   feb.   22   at   11:59pm

    course survey [5   pts]

    due fri,   feb   24   at   11:59pm

    homework 4:   id88 /   kernels /   id166

    release:   wed,   feb.   22
    due:   fri,   mar.   03   at   11:59pm

new   due   date

(9   days   for   hw4)

3

outline

    id88

    kernels

    online   learning
    id88   algorithm
    margin   definitions
    id88   mistake   bound

    kernel   id88
    kernel   as   a   dot   product
    gram   matrix
    examples:   polynomial,   rbf

    support   vector   machine   (id166)

last   lecture

this   lecture

4

id88

5

background:   hyperplanes

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

background:   hyperplanes

hyperplane (definition   1):   
h = {x : wt x = b}
hyperplane (definition   2):   
h = {x : wt x = 0
and x1 = 1}

x0

w

half-     spaces:   
h+ = {x : wt x > 0 and x1 = 1}
h  = {x : wt x < 0 and x1 = 1}

x0
x0

background:   hyperplanes

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

directly   modeling   the   
hyperplane would   use   a   
decision   function:

h((cid:116)) = sign( t (cid:116))

for:

y   { 1, +1}

online   learning

for   i =   1,   2,   3,      :
    receive an   unlabeled   instance   x(i)
    predict y      =   h(x(i))
    receive true   label   y(i)

check for   correctness   (y      ==   y(i))

goal:
    minimize the   number   of   mistakes

10

online   learning:   motivation

examples
1. email   classification   (distribution   of   both   

spam   and   regular   mail   changes   over   time,   
but   the   target   function   stays   fixed   -      last   
year's   spam   still   looks   like   spam).

2. recommendation   systems.   recommending   

movies,   etc.

3. predicting   whether   a   user   will   be   interested   

in   a   new   news   article   or   not.

4. ad   placement   in   a   new   market.

slide   from   nina   balcan

11

id88   algorithm

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.

d = {(cid:116)(i), y(i)}n

i=1 where (cid:116)   rk and y   {+1, 1}

prediction:   output   determined   by   hyperplane.

  y = h (x) = sign( t x)

sign(a) = 1,

if a   0

 1, otherwise

learning:   iterative   procedure:
    while   not   converged

receive next   example   (x(i),   y(i))

   
    predict y      =   h(x(i))
   
   

if positive   mistake:   add x(i) to   parameters
if negative   mistake:   subtract x(i) from   parameters

12

id88   algorithm

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.

d = {(cid:116)(i), y(i)}n

i=1 where (cid:116)   rk and y   {+1, 1}

prediction:   output   determined   by   hyperplane.

  y = h (x) = sign( t x)

learning:

sign(a) = 1,

if a   0

 1, otherwise

13

id88   algorithm:   example

example:    1,2    
1,0 +1,1 +
   1,0    
   1,   2    
1,   1 +

x
a   
x
a   
x
a   

algorithm:
     

set   t=1,   start   with   all-     zeroes   weight   vector       &.
      given   example       ,   predict   positive   iff    2          0.
    mistake   on   positive,   update       26&       2+    
    mistake   on   negative,   update       26&       2       

      on   a   mistake,   update   as   follows:   

slide   adapted   from   nina   balcan

-
-
-

+
+
+

    &=(0,0)
    +=    &       1,2 =(1,   2)
    ,=    ++ 1,1 =(2,   1)
    .=    ,       1,   2 =(3,1)

definition: the   margin of   example        w.r.t. a   linear   sep.     is   the   
distance   from       	
   to   the   plane       	
          =0 (or   the   negative if   on   wrong   side)

geometric   margin

margin   of   positive   example       &
    &

w

margin   of   negative   example       +

    +

slide   from   nina   balcan

geometric   margin

definition: the   margin of   example        w.r.t. a   linear   sep.     is   the   
distance   from       	
   to   the   plane              =0 (or   the   negative if   on   wrong   side)
definition: the   margin       : of   a   set   of   examples        wrt a   linear   
separator        is   the   smallest   margin   over   points              .

+

    :    :

-
-

-

+
+
+ +

-
-

w

+

+

-

-
-

-

+

slide   from   nina   balcan

geometric   margin

definition: the   margin of   example        w.r.t. a   linear   sep.     is   the   
distance   from       	
   to   the   plane              =0 (or   the   negative if   on   wrong   side)
definition: the   margin       : of   a   set   of   examples        wrt a   linear   
separator        is   the   smallest   margin   over   points              .
definition: the   margin        of   a   set   of   examples        is   the   maximum    :
over   all   linear   separators       .
         

+ +

w

-
-

-

+

+

-
-

-

-
-

+

slide   from   nina   balcan

-

analysis:   id88

id88   mistake   bound

guarantee: if data has margin   and all points inside a ball of
radius r, then id88 makes   (r/ )2 mistakes.
(normalized   margin:   multiplying all   points   by   100,   or   dividing   all   points   by   100,   
doesn   t   change   the   number   of   mistakes;   algo is   invariant   to   scaling.)

+

g   

g   
-
-
-

-

-

  
+

+

+
+
+
+ +

r

-
-

18

-
-

slide   adapted   from   nina   balcan

analysis:   id88

id88   mistake   bound

theorem 0.1 (block (1962), noviko    (1962)).
given dataset: d = {((cid:116)(i), y(i))}n
i=1.
suppose:
1. finite size inputs: ||x(i)||   r
2. linearly separable data:     s.t. ||  || = 1 and
y(i)(      (cid:116)(i))    , i
then: the number of mistakes made by the id88
algorithm on this dataset is

+
+

+

+

k   (r/ )2

+

  
+

+

r

-

-

g   

-

-

g   

+

-

-

-

-

-

19

figure   from   nina   balcan

analysis:   id88

proof   of   id88   mistake   bound:

we   will   show   that   there   exist   constants   a   and   b   s.t.

ak   || (k+1)||   b k

ak   || (k+1)||   b k

ak   || (k+1)||   b k

ak   || (k+1)||   b k

ak   || (k+1)||   b k

20

analysis:   id88

theorem 0.1 (block (1962), noviko    (1962)).
given dataset: d = {((cid:116)(i), y(i))}n
i=1.
suppose:
1. finite size inputs: ||x(i)||   r
2. linearly separable data:     s.t. ||  || = 1 and
y(i)(      (cid:116)(i))    , i
then: the number of mistakes made by the id88
algorithm on this dataset is

+

g   

-

-

-

g   

-

-

+

+

+
+

+

-

-

  
+

+

r

-

-

k   (r/ )2

algorithm 1 id88 learning algorithm (online)
1: procedure p(cid:266)(cid:279)(cid:264)(cid:266)(cid:277)(cid:281)(cid:279)(cid:276)(cid:275)(d = {((cid:116)(1), y(1)), ((cid:116)(2), y(2)), . . .})

2:
3:
4:
5:
6:
7:

    0, k = 1
for i   {1, 2, . . .} do

if y(i)( (k)    (cid:116)(i))   0 then
 (k+1)    (k) + y(i)(cid:116)(i)
k   k + 1

return  

  initialize parameters
  for each example
  if mistake
  update parameters

21

analysis:   id88

whiteboard:   
proof   of   id88   mistake   bound

22

analysis:   id88

proof   of   id88   mistake   bound:
part   1:   for   some   a,    ak   || (k+1)||   b k
 (k+1)       = ( (k) + y(i)(cid:116)(i))  

by id88 algorithm update
=  (k)       + y(i)(      (cid:116)(i))
   (k)       +  
by assumption
   (k+1)         k 
by induction on k since  (1) = 0
  || (k+1)||   k 
since ||(cid:114)||   ||(cid:109)||   (cid:114)    (cid:109) and ||  || = 1

cauchy-     schwartz   inequality

23

analysis:   id88

proof   of   id88   mistake   bound:
part   2:   for   some   b,   ak   || (k+1)||   b k
|| (k+1)||2 = || (k) + y(i)(cid:116)(i)||2

by id88 algorithm update
= || (k)||2 + (y(i))2||(cid:116)(i)||2 + 2y(i)( (k)    (cid:116)(i))
  || (k)||2 + (y(i))2||(cid:116)(i)||2
since kth mistake   y(i)( (k)    (cid:116)(i))   0
= || (k)||2 + r2
since (y(i))2||(cid:116)(i)||2 = ||(cid:116)(i)||2 = r2 by assumption and (y(i))2 = 1
  || (k+1)||2   kr2
by induction on k since ( (1))2 = 0
 kr
  || (k+1)||  

24

analysis:   id88

proof   of   id88   mistake   bound:
part   3:   combining   the   bounds   finishes   the   proof.

 kr

k    || (k+1)||  

 k   (r/ )2

the   total   number   of   mistakes   

must   be   less   than   this

25

(batch)   id88   algorithm
learning for   id88   also   works   if   we   have   a   fixed   training   
dataset,   d.   we   call   this   the      batch      setting   in   contrast   to   the      online      
setting   that   we   ve   discussed   so   far.

algorithm 1 id88 learning algorithm (batch)
1: procedure p(cid:266)(cid:279)(cid:264)(cid:266)(cid:277)(cid:281)(cid:279)(cid:276)(cid:275)(d = {((cid:116)(1), y(1)), . . . , ((cid:116)(n ), y(n ))})
  initialize parameters

2:
3:
4:
5:
6:
7:
8:

    0
while not converged do
  y   sign( t (cid:116)(i))
if   y  = y(i) then

for i   {1, 2, . . . , n} do

      + y(i)(cid:116)(i)

return  

  for each example
  predict
  if mistake
  update parameters

26

(batch)   id88   algorithm
learning for   id88   also   works   if   we   have   a   fixed   training   
dataset,   d.   we   call   this   the      batch      setting   in   contrast   to   the      online      
setting   that   we   ve   discussed   so   far.

discussion:
the   batch   id88   algorithm   can   be   derived   by   applying   
stochastic   gradient   descent   (sgd)   to   minimize   a   so-     called   hinge   loss   
on   a   linear   separator

27

extensions   of   id88

    kernel   id88

    choose   a   kernel   k(x   ,   x)
    apply   the   kernel   trick   to   id88
    resulting   algorithm   is   still   very   simple

    structured   id88

    basic   idea   can   also   be   applied   when   y ranges   

over   an   exponentially   large   set

    mistake   bound   does   not depend   on   the   size   of   

that   set

28

matching   game

goal:   match   the   algorithm   to   its   update   rule

1.   sgd   for   logistic   regression

h (x) = p(y|x)

2.   least   mean   squares

h (x) =  t x

3.   id88

h (x) = sign( t x)

4.

5.

6.

 k    k + (h (x(i))   y(i))

 k    k +

1

1 + exp  (h (x(i))   y(i))

 k    k +  (h (x(i))   y(i))x(i)

k

a.   1=5,   2=4,   3=6
b.   1=5,   2=6,   3=4
c.   1=6,   2=4,   3=4
d.   1=5,   2=6,   3=6
e.   1=6,   2=6,   3=6

29

summary:   id88

    id88   is   a   linear   classifier
    simple learning   algorithm:   when   a   mistake   

is   made,   add   /   subtract   the   features

    for   linearly   separable   and   inseparable   data,   

we   can   bound   the   number   of   mistakes   
(geometric   argument)

    extensions   support   nonlinear   separators   and   

structured   prediction

30

kernels

31

kernels:   motivation

most   real-     world   problems   exhibit   data   that   is   
not   linearly   separable.

example:   pixel   representation   for   facial   recognition:

q:   when   your   data   is   not   linearly   separable,   

how   can   you   still   use   a   linear   classifier?

a: preprocess   the   data   to   produce   nonlinear

features

32

kernels:   motivation
    motivation   #1:   inefficient   features

    non-     linearly   separable   data   requires   high   

dimensional   representation

    might   be   prohibitively   expensive   to   compute   or   

store

    motivation   #2:   memory-     based   methods

    k-     nearest   neighbors   (knn)   for   facial   recognition   

allows   a   distance   metric between   images   -     -      no   
need   to   worry   about   linearity   restriction   at   all

33

kernels

whiteboard

    kernel   id88
    kernel   as   a   dot   product
    gram   matrix
    examples:   rbf   kernel,   string   kernel

34

kernel   methods

    key   idea:   

1.

2.

rewrite the   algorithm   so   that   we   only   work   with   dot   products xtz
of   feature   vectors
replace the   dot   products   xtz with   a   kernel   function   k(x,   z)

    the   kernel   k(x,z)   can   be   any legal   definition   of   a   dot   product:   

k(x,   z)   =     (x) t  (z)   for   any   function     :   x       rd

so   we   only   compute   the        dot   product   implicitly

    this      kernel   trick    can   be   applied   to   many   algorithms:

    classification:   id88,   id166,      
    regression:   ridge   regression,      
    id91:   k-     means,      

35

kernel   methods

q:   these   are   just   non-     linear   features,   right?
a: yes,   but   

q:   can   t   we   just   compute   the   feature   

transformation      explicitly?

a: that   depends...

q:   so,   why   all   the   hype   about   the   kernel   trick?
a: because   the   explicit   features   might   either   
be   prohibitively   expensive   to   compute   or   
infinite   length   vectors

36

example 

example:   polynomial   kernel

for n=2, d=2, the kernel kx,z = x   z d corresponds to  
    1,    2           =(    12,    22, 2    1    2) 
  :r2   r3, x1,x2      x =(x12,x22, 2x1x2) 
  :r2   r3, x1,x2      x =(x12,x22, 2x1x2) 
example 
example 
  -space 
  x           = x12,x22, 2x1x2    (    12,    22, 2    1    2) 
  x           = x12,x22, 2x1x2    (    12,    22, 2    1    2) 
= x1    1+x2    2 2= x        2=k(x,z) 
for n=2, d=2, the kernel kx,z = x   z d corresponds to  
for n=2, d=2, the kernel kx,z = x   z d corresponds to  
= x1    1+x2    2 2= x        2=k(x,z) 
    1,    2           =(    12,    22, 2    1    2) 
    1,    2           =(    12,    22, 2    1    2) 
  -space 
  -space 

original space 

example 

example 

x2 

x 

x 

x 

x 

x 

x 

x 

x 
x 
original space 
original space 
x 
x 
original space 
original space 
x2 
x2 
x2 
x2 
x 
o 
x 
x 
x 
x 
o 
x 
x 
x 
x 
o 
x 
o 
x 
x 
x 
x 
o 
x 
x 
o 
x 
o  o 
x1 
o 
x 
o 
o 
o 
o 
o 
o 
o  o 
x 
x 
o 
o 
o 
o 
o 
x 
x 
x 
x 
x 
o  o 
o  o 
x 
z3 
x 
x 
o 
o 

o 
o 
x 
o 
x 
o 
o 
x 
x 
o 
o  o 
x 
x 
x 
x 

x 
o 
x 
x 
x 

o 
o 
x 
x 

x 
x 

o 

x 

x 

x 

x 

x 
x 
x 
x1 
x 
x 
x 
x1 

o 

x 

x  x 
x 
x 

x 
x 
x 
o 
o  o 
o 
o 
x 
o 
x 
x1 
x1 
o 
x 
x 
z3 
x 
z3 
x 
x 

o 
x 
x 

x 

x 

slide   from   nina   balcan

x 
x 

x 
x 

x 
x 

x 
x 

x 
x 

x 
x 
x 
x 
x 

x 
x 

x  x 
x 
x 

x 

x 

x 
x 
o 
x  x 
o  o 
x  x 
x 
x  x 
x 
x 
x 
x 
x 
x 
o 
x 
o 
o 
x 
x 
x 
o 
x 
x 
o  o 
o 
o 
x 
x 
o 
x 
x 
x 
x 
o 
o 
z1 
z1 
o 
o 
o  o 
x 
o  o 
o 
x 
x 
o 
x 
x 
o 
x 
o 
o 
o 
o 
o 
x 
x 
x 
x 
o 
x 
o 
x 
x 
o 
o 
x 
x 
x 
x 
x 
x 

x 
x 
x 

o 
x 
x 

z3 
z3 

o 
x 

o 
o 

x 
x 

x 

x 
x 
37

z1 

x 
x 

x 

x 
x 

x 
x 

  -space 
  -space 

example:   polynomial   kernel

avoid explicitly expanding the features 
feature space can grow really large and really quickly   . 

crucial to think of    as implicit, not explicit!!!! 
    polynomial kernel degreee     ,         ,     =                 =                      
       1    ,     1    2           ,     12    2              1 
=     +       1!
    +       1
    
    !        1!  
        =6,    =100, there are 1.6 billion terms 
                                                      ! 
        ,     =                 =                      

    total number of such feature is 

slide   from   nina   balcan

38

side   note:   the   feature   space   might   not   be   unique!

explicit   representation   #1:

kernel   examples

example 

note:  feature space might not be unique. 

  :r2   r3, x1,x2      x =(x12,x22, 2x1x2) 
  x           = x12,x22, 2x1x2    (    12,    22, 2    1    2) 
= x1    1+x2    2 2= x        2=k(x,z) 
  :r2   r4, x1,x2      x =(x12,x22,x1x2,x2x1) 
  x           =(x12,x22,x1x2,x2x1)   (z12,z22,z1z2,z2z1) 

= x        2=k(x,z) 

explicit   representation   #2:

these   two   different   feature   representations   correspond   to   the   same   

kernel   function!

slide   from   nina   balcan

39

kernel   examples

kernel   function
(implicit dot   product)

name

linear

polynomial   (v1)

polynomial (v2)

gaussian

hyperbolic
tangent   
(sigmoid)   
kernel

feature   space
(explicit   dot   product)

same   as   original   input   
space

all   polynomials of degree   
d

all   polynomials up   to   
degree   d

infinite   dimensional   space

(with id166,   this   is   
equivalent   to   a   2-     layer   
neural   network)

40

rbf   kernel   example

rbf   kernel:

41

rbf   kernel   example

rbf   kernel:

42

rbf   kernel   example

rbf   kernel:

43

rbf   kernel   example

rbf   kernel:

44

rbf   kernel   example

rbf   kernel:

45

rbf   kernel   example

rbf   kernel:

46

rbf   kernel   example

rbf   kernel:

47

rbf   kernel   example

rbf   kernel:

48

rbf   kernel   example

rbf   kernel:

49

rbf   kernel   example

rbf   kernel:

50

rbf   kernel   example

rbf   kernel:

51

rbf   kernel   example

rbf   kernel:

52

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

53

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

54

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

55

rbf   kernel   example

knn   vs.   id166

rbf   kernel:

56

kernels:   discussion
kernels, discussion 

    if  all computations involving instances are in terms 

(cid:131)  computationally, only need to modify the algo by replacing 

of inner products then: 
(cid:131) conceptually, work in a very high diml space and the alg   s 
performance depends only on linear separability in that 
extended space. 

each x   z with a kx,z . 
    use cross-validation to choose the parameters, e.g.,       for 
gaussian kernel   kx,     =exp                2
2     2    

how to choose a kernel: 
    kernels often encode domain knowledge (e.g., string kernels) 

    learn a good kernel; e.g.,  [lanckriet-cristianini-bartlett-el ghaoui-

jordan   04] 

slide   from   nina   balcan

58

