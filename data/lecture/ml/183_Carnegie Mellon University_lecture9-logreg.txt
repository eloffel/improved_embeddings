10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

logistic   regression,      
nonlinear   features,      

id173

logistic   regression   readings:
murphy   8.1-     8.3,   8.6
bishop   4.3.2,   4.3.4
htf   4.1,   4.4
mitchell      

   generative      logistic   regression      
(mitchell,   2016)
   maximum      gradient   training   
(elkan,   2014)

matt   gorid113y
lecture   9
february   15,   2016

1

reminders

    homework 3:   linear   /   id28

    release:   mon,   feb.   13
    due:   wed,   feb.   22   at   11:59pm
note   the   
change   in   

time.

2

outline

    motivation:

    choosing   the   right   classifier
    example:   image   classification

    logistic   regression

    background:   hyperplanes
    data,   model,   learning,   prediction
    log-     odds
    bernoulli   interpretation
    maximum   conditional   likelihood   estimation
    gradient   descent   for   logistic   regression

    stochastic   gradient   descent   (sgd)
    computing   the   gradient
    details   (learning   rate,   finite   differences)

    nonlinear   features

3

motivation:   
logistic   regression

4

classifiers

which   classification   method   should   we   use?

1.

2.

3.

4.

during   training
during   classification

on   the   training   data
on   the   (unseen)   test   data
on   the   (held-     out)   validation   data

the   one   that   gives   the   best   predictions   
   
   
   
the   one   that   is   computationally   efficient   
   
   
the   most   interpretable   one   
   
   
the   one   that   is   easiest   to   implement   
   
   

in   terms   of   its   parameters
as   a   model

for   learning
for   classification

5

classifiers

which   classification   method   should   we   use?

na  ve   bayes   defined   a   generative   model   p(x, y)
of   the   features   x and   the   class   y.   

why   should   we   define   a   model   of   p(x, y) at   all?   

why   not   directly   model   p(y | x)?

6

example:   image   classification

    id163 lsvrc-     2010   contest:   

    dataset:   1.2   million   labeled   images,   1000   classes
    task:   given   a   new   image,   label   it   with   the   correct   class
    multiclass classification   problem

    examples   from   http://image-     net.org/

7

8

9

10

example:   image   classification

id98   for   image   classification
(krizhevsky,   sutskever &   hinton,   2011)
17.5%   error   on   id163 lsvrc-     2010   contest

input   
image   
(pixels)

   

five   convolutional   layers   
(w/max-     pooling)

    three   fully   connected   layers

1000-     way   
softmax

11
figure 2: an illustration of the architecture of our id98, explicitly showing the delineation of responsibilities
between the two gpus. one gpu runs the layer-parts at the top of the    gure while the other runs the layer-parts

example:   image   classification

id98   for   image   classification
(krizhevsky,   sutskever &   hinton,   2011)
17.5%   error   on   id163 lsvrc-     2010   contest

input   
image   
(pixels)

   

five   convolutional   layers   
(w/max-     pooling)

    three   fully   connected   layers

1000-     way   
softmax

the   rest   is   just
some   fancy   

feature   extraction   
(discussed   later   in   

the   course)

this      softmax      
layer   is   logistic   

regression!

12
figure 2: an illustration of the architecture of our id98, explicitly showing the delineation of responsibilities
between the two gpus. one gpu runs the layer-parts at the top of the    gure while the other runs the layer-parts

logistic   regression

13

logistic   regression

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.

we   are   back   to   
classification.

despite   the   name   
logistic   regression.

14

background:   hyperplanes

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

background:   hyperplanes

hyperplane (definition   1):   
h = {x : wt x = b}
hyperplane (definition   2):   
h = {x : wt x = 0
and x1 = 1}

x0

w

half-     spaces:   
h+ = {x : wt x > 0 and x1 = 1}
h  = {x : wt x < 0 and x1 = 1}

x0
x0

background:   hyperplanes

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

directly   modeling   the   
hyperplane would   use   a   
decision   function:

h((cid:116)) = sign( t (cid:116))

for:

y   { 1, +1}

using   gradient   ascent   for   linear   

classifiers

key   idea   behind   today   s   lecture:

1. define   a   linear   classifier   (logistic   regression)
2. define   an   objective   function   (likelihood)
3. optimize   it   with   gradient   descent   to   learn   

parameters

4. predict   the   class   with   highest   id203   under   

the   model

18

using   gradient   ascent   for   linear   

classifiers

this   decision   function   isn   t   
differentiable:

use   a   differentiable   
function   instead:

h((cid:116)) = sign( t (cid:116))

p (y = 1|(cid:116)) =

1

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))

sign(x)

logistic(u)    

1
1+e   u

19

using   gradient   ascent   for   linear   

classifiers

this   decision   function   isn   t   
differentiable:

use   a   differentiable   
function   instead:

h((cid:116)) = sign( t (cid:116))

p (y = 1|(cid:116)) =

1

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))

sign(x)

logistic(u)    

1
1+e   u

20

logistic   regression

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.

model:   logistic   function   applied   to   dot   product   of   
parameters   with   input   vector.
p (y = 1|(cid:116)) =

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))
learning:   finds   the   parameters   that   minimize   some   
objective   function.    = argmin

j( )

1

 

prediction:   output   is   the   most   probable   class.

  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
y {0,1}

p (y|(cid:116))

21

whiteboard

    decision   boundary
    bernoulli   interpretation

22

logistic   regression

23

logistic   regression

24

logistic   regression

25

learning   logistic   regression

26

maximum   conditional
likelihood   estimation

learning:   finds   the   parameters   that   minimize   some   
objective   function.

   = argmin

j( )

we   minimize   the   negative log   conditional   likelihood:

 

j( ) =   (cid:72)(cid:81)(cid:59)

p (y(i)|(cid:116)(i))

n i=1

why?

1. we   can   t   maximize   likelihood   (as   in   na  ve   bayes)   

2.

because   we   don   t   have   a   joint   model   p(x,y)
it   worked   well   for   linear   regression   (least   squares   is   
mcle)

27

maximum   conditional
likelihood   estimation

learning:   four   approaches   to   solving   

   = argmin

j( )

 

approach   1:   gradient   descent
(take   larger       more   certain       steps   opposite   the   gradient)

approach   2:   stochastic   gradient   descent   (sgd)
(take   many   small   steps   opposite   the   gradient)

approach   3:   newton   s   method
(use   second   derivatives   to   better   follow   curvature)

approach   4:   closed   form???
(set   derivatives   equal   to   zero   and   solve   for   parameters)

28

maximum   conditional
likelihood   estimation

learning:   four   approaches   to   solving   

   = argmin

j( )

 

approach   1:   gradient   descent
(take   larger       more   certain       steps   opposite   the   gradient)

approach   2:   stochastic   gradient   descent   (sgd)
(take   many   small   steps   opposite   the   gradient)

approach   3:   newton   s   method
(use   second   derivatives   to   better   follow   curvature)

approach   4:   closed   form???
(set   derivatives   equal   to   zero   and   solve   for   parameters)

logistic   regression   does   not   
have   a   closed   form   solution   
for   id113   parameters.

29

gradient   descent

algorithm 1 id119
1: procedure gd(d,  (0))

     (0)
while not converged do

      +    j( )

   

return  

2:
3:
4:

5:

in   order   to   apply   gd   to   logistic   
regression   all   we   need   is   the   
gradient of   the   objective   
function   (i.e.   vector   of   partial   
derivatives).   

  j( ) =     

d
d 1
d
d 2

d

d n

j( )
j( )
...
j( )

     

30

stochastic   gradient   descent   (sgd)

   

we   can   also   apply   sgd   to   solve   the   mcle   
problem   for   logistic   regression.
we   need   a   per-     example   objective:

let j( ) = n
where j (i)( ) =   (cid:72)(cid:81)(cid:59) p (yi|(cid:116)i).

i=1 j (i)( )

31

gradient   for   logistic   
regression

33

whiteboard

    partial   derivative   for   logistic   regression
    gradient   for   logistic   regression

34

details:   picking   learning   rate
    use   grid-     search   in   log-     space   over   small   

values   on   a   tuning   set:
    e.g.,   0.01,   0.001,      

    sometimes,   decrease   after   each   pass:

    e.g factor   of   1/(1   +   dt),   t=epoch
    sometimes   1/t2

    fancier   techniques   i   won   t   talk   about:

    adaptive   gradient:   scale   gradient   differently   for   

each   dimension   (adagrad,   adam,      .)

slide   courtesy   of   william   cohen

39

sgd   for   logistic   regression

we   can   also   apply   sgd   to   solve   the   mcle   
problem   for   logistic   regression.
we   need   a   per-     example   objective:

let j( ) = n
where j (i)( ) =   (cid:72)(cid:81)(cid:59) p (yi|(cid:116)i).

i=1 j (i)( )

41

takeaways

1. discriminative   classifiers   directly   model   the   

conditional,   p(y|x)

2. logistic   regression   is   a   simple   linear   
classifier,   that   retains   a   probabilistic   
semantics

3. parameters   in   lr   are   learned   by   iterative   

optimization   (e.g.   sgd)

42

non-     linear   features

43

nonlinear   features

whiteboard

    example   functions
    nonlinear   features   for   linear   regression
    nonlinear   features   for   logistic   regression
    nonlinear   features   for   knn
    nonlinear   features   for   na  ve   bayes

44

example:   linear   regression   

nonlinear   features

polynomial   basis   vectors   on   a   small   dataset

    from   bishop   ch   1

slide   courtesy   of   william   cohen

0th order   polynomial

slide   courtesy   of   william   cohen

n=10

1st order   polynomial

slide   courtesy   of   william   cohen

3rd order   polynomial

slide   courtesy   of   william   cohen

9th order   polynomial

slide   courtesy   of   william   cohen

over-  fitting

root-  mean-  square   (rms)   error:

slide   courtesy   of   william   cohen

polynomial   coefficients         

slide   courtesy   of   william   cohen

overfitting

definition:   the   problem   of   overfitting is   when   
the   model   captures   the   noise   in   the   training   data   
instead   of   the   underlying   structure   

overfitting   can   occur   in   all   the   models   we   ve   seen   
so   far:   

    knn   (e.g.   when   k   is   small)
    na  ve   bayes   (e.g.   without   a   prior)
    linear   regression   (e.g.   with   basis   function)
    logistic   regression   (e.g.   with   many   rare   features)

55

9th order   polynomial

(small   #   of   examples)

slide   courtesy   of   william   cohen

9th order   polynomial

(large   #   of   examples)

slide   courtesy   of   william   cohen

