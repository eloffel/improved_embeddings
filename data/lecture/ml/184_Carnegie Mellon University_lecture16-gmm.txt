10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

k-     means

+

gmms

id91   readings:
murphy   25.5
bishop   12.1,   12.3
htf   14.3.0
mitchell   -     -     

em,   gmm   readings:
murphy   11.4.1,   11.4.2,   11.4.4
bishop   9
htf   8.5   -      8.5.3
mitchell   6.12   -      6.12.2

matt   gorid113y
lecture   16
march   20,   2017

1

reminders

    homework 5:   readings /   application of   ml

    release:   wed,   mar.   08
    due:   wed,   mar.   22   at   11:59pm

2

peer   tutoring

tutor

tutee

3

k-     means

5

k-     means   outline

    id91:   motivation   /   applications
    optimization   background

    coordinate   descent
    block   coordinate   descent

    id91

    inputs   and   outputs
    objective-     based   id91

    k-     means

    k-     means   objective
    computational   complexity
    k-     means   algorithm   /   lloyd   s   method

    k-     means   initialization

    random
    farthest   point
    k-     means++

last   lecture

this   lecture

6

id91,   informal   goals

goal:   automatically   partition   unlabeled data   into   groups   of   similar   
datapoints.

question:   when   and   why   would   we   want   to   do   this?

useful   for:

      automatically   organizing   data.
      understanding   hidden   structure   in   data.

      preprocessing   for   further   analysis.

      representing   high-     dimensional   data   in   a   low-     dimensional   space   (e.g.,   
for   visualization   purposes).

slide   courtesy   of   nina   balcan

applications (id91   comes   up   everywhere   )

    cluster   news   articles   or   web   pages   or   search   results   by   topic.

    cluster   protein   sequences   by   function   or   genes   according   to   expression   

profile.

    cluster   users   of   social   networks   by   interest   (community   detection).

facebook network

twitter network

slide   courtesy   of   nina   balcan

applications   (id91   comes   up   everywhere   )

    cluster   customers   according   to   purchase   history.

    cluster   galaxies   or   nearby   stars (e.g.   sloan   digital   sky   survey)

    and   many   many more   applications   .

slide   courtesy   of   nina   balcan

optimization   background

whiteboard:

    coordinate   descent
    block   coordinate   descent

10

id91

question:   which   of   these   partitions   is      better   ?

11

id91

whiteboard:

    inputs   and   outputs
    objective-     based   id91

12

k-     means

whiteboard:

    k-     means   objective
    computational   complexity
    k-     means   algorithm   /   lloyd   s   method

13

k-     means   initialization

whiteboard:
    random
    furthest   traversal
    k-     means++

14

lloyd   s   method:   random   initialization

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

example:   given   a   set   of   datapoints

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

select   initial   centers   at   random

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

assign   each   point   to   its   nearest   center

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

recompute optimal   centers   given   a   fixed   id91

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

assign   each   point   to   its   nearest   center

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

recompute optimal   centers   given   a   fixed   id91

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

assign   each   point   to   its   nearest   center

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

recompute optimal   centers   given   a   fixed   id91

get   a   good      quality   solution   in   this   example.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

it   always   converges,   but   it   may   converge   at   a   local   optimum   that   is   
different   from   the   global   optimum,   and   in   fact   could   be   arbitrarily   
worse   in   terms   of   its   score.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

local   optimum:   every   point   is   assigned   to   its   nearest   center   and   
every   center   is   the   mean   value   of   its   points.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

.it   is   arbitrarily   worse   than   optimum   solution   .

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

this   bad   performance,   can   happen   
even   with   well   separated   gaussian   
clusters.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

this   bad   performance,   can   
happen   even   with   well   
separated   gaussian   clusters.

some   gaussian   are   
combined   ..

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

   

if   we   do   random   initialization,   as   k increases,   it   becomes   more   likely   
we   won   t   have   perfectly   picked   one   center   per   gaussian   in   our   
initialization   (so   lloyd   s   method   will   output   a   bad   solution).

    for   k   equal-     sized   gaussians,   pr[each   initial   center   is   in   a   

different   gaussian]      "!"$    %&$

    becomes   unlikely   as   k   gets   large.   

slide   courtesy   of   nina   balcan

another   initialization   idea:   furthest   point   

heuristic

choose            arbitrarily   (or   at   random).
    for   j=2,   ,k
    pick            among   datapoints           ,        ,   ,         that   is   farthest   
from   previously   chosen           ,        ,   ,        0    

fixes   the   gaussian   problem.   but   it   can   be   thrown   off   by   
outliers   .

slide   courtesy   of   nina   balcan

furthest   point   heuristic   does   well   on   previous   

example

slide   courtesy   of   nina   balcan

furthest   point   initialization   heuristic   sensitive   

to   outliers

assume   k=3

(-     2,0)

(0,1)

(0,-     1)

(3,0)

slide   courtesy   of   nina   balcan

furthest   point   initialization   heuristic   sensitive   

to   outliers

assume   k=3

(-     2,0)

(0,1)

(0,-     1)

(3,0)

slide   courtesy   of   nina   balcan

   
   

interpolate   between   random   and   furthest   point   initialization

k-     means++   initialization:   d6 sampling   [av07]
let   d(x) be   the   distance   between   a   point        and   its   nearest   center.   
chose   the   next   center   proportional   to   d6(    ).
    choose            at   random.
    for   j=2,   ,k
    pick            among           ,        ,   ,         according   to   the   distribution
        (        =        )                   ?@    	
                       ?     

d6(        )

theorem:   k-     means++   always   attains   an   o(log   k)   approximation   to   optimal   
k-     means   solution   in   expectation.

running   lloyd   s can   only   further   improve   the   cost.

slide   courtesy   of   nina   balcan

   

   

interpolate   between   random   and   furthest   point   initialization

k-     means++   idea:   d6 sampling
let   d(x) be   the   distance   between   a   point        and   its   nearest   center.
chose   the   next   center   proportional   to   dd     =                ?@    	
                       ?     
        =0,   random   sampling
        =   ,   furthest   point    (side   note:   it   actually   works   well   for   k-     center)
        =2,   k-     means++
side   note:       =1,   works   well   for   k-     median   

.

slide   adapted   from   nina   balcan

k-     means   ++   fix

(-     2,0)

(0,1)

(0,-     1)

(3,0)

slide   courtesy   of   nina   balcan

k-     means++/ lloyd   s running   time

    k-     means   ++   initialization:   o(nd)   and   one   pass   over   data   to   select   

next   center.   so   o(nkd)   time   in   total.

    lloyd   s   method

repeat until   there   is   no   change   in   the   cost.

for   each   j:      ck   {            whose   closest   center   is           }
for   each   j:              mean   of   ck

   

   

each   round   takes   time   
o(nkd).

    exponential   #   of   rounds   in   the   worst   case   [av07].

    expected   polynomial   time   in   the   smoothed   analysis   (non   worst-     case)   

model!

slide   courtesy   of   nina   balcan

k-     means++/ lloyd   s   summary

    running   lloyd   s   can   only   further   improve   the   cost.

    exponential   #   of   rounds   in   the   worst   case   [av07].

    expected   polynomial   time   in   the   smoothed   analysis   model!

    does   well   in   practice.

    k-     means++   always   attains   an   o(log   k)   approximation   to   optimal   

k-     means   solution   in   expectation.

slide   courtesy   of   nina   balcan

what   value   of   k?

    heuristic:   find   large   gap   between   (k   -      1)-     means   cost   and   

k-     means   cost.

    hold-     out   validation/cross-     validation   on   auxiliary   task   (e.g.,   

supervised   learning   task).

    try   hierarchical   id91.

slide   courtesy   of   nina   balcan

em   and   gmms

43

expectation-     maximization   outline

    background

    multivariate   gaussian   distribution
    marginal   probabilities
    building   up   to   gmms

    distinction   #1:   model   vs.   objective   function
    gaussian   na  ve   bayes   (gnb)
    gaussian   discriminant   analysis
    gaussian   mixture   model   (gmm)
expectation-     maximization
    distinction   #2:   data   likelihood   vs.   marginal   data   likelihood
    distinction   #3:   latent   variables   vs.   parameters
    objective   functions   for   em
    hard   (viterbi)   em   algorithm

    example:   k-     means   as   hard   em
    soft   (standard)   em   algorithm
    example:   soft   em   for   gmm

properties   of   em
    nonconvexity /   local   optimization
    example:   grammar   induction
    variants   of   em

   

   

44

background

whiteboard

    multivariate   gaussian   distribution
    marginal   probabilities

46

gaussian   mixture   model   
(gmm)

47

building   up   to   gmms

whiteboard

    distinction   #1:   model   vs.   objective   function
    gaussian   na  ve   bayes   (gnb)
    gaussian   discriminant   analysis
    gaussian   mixture   model   (gmm)

48

gaussian   discriminant   

data:   

d = {((cid:116)(i), (cid:120)(i))}n

generative   story:

model:

joint:

analysis
i=1 where (cid:116)(i)   rm and z(i)   {1, . . . , k}
z   categorical( )
(cid:116)   gaussian(  z,  z)
p((cid:116), z;  ,   ,  ) = p((cid:116)|z;   ,  )p(z;  )

log-     likelihood:

 ( ,   ,  ) = (cid:72)(cid:81)(cid:59)

=

n i=1

p((cid:116)(i), z(i);  ,   ,  )

n i=1
(cid:72)(cid:81)(cid:59) p((cid:116)(i)|z(i);   ,  ) + (cid:72)(cid:81)(cid:59) p(z(i);  )

49

gaussian   discriminant   

data:   

d = {((cid:116)(i), (cid:120)(i))}n

analysis
i=1 where (cid:116)(i)   rm and z(i)   {1, . . . , k}

log-     likelihood:

 ( ,   ,  ) =

n i=1

(cid:72)(cid:81)(cid:59) p((cid:116)(i)|z(i);   ,  ) + (cid:72)(cid:81)(cid:59) p(z(i);  )

maximum   likelihood   estimates:
take   the   derivative   of   the   lagrangian,   set   it   equal   to   zero   
and   solve.

implementation:   

just   counting   

1
n

 k =

n i=1
i(z(i) = k), k
  k =  n
i=1 i(z(i) = k)(cid:116)(i)
 n
i=1 i(z(i) = k)
 k =  n
i=1 i(z(i) = k)((cid:116)(i)     k)((cid:116)(i)     k)t

i=1 i(z(i) = k)

, k

 n

, k

50

assume we are given data, d, consisting of n fully unsupervised ex-
amples in m dimensions:

gaussian   mixture-     model

data:   

d = {(cid:116)(i)}n

generative   story:

model:

joint:

marginal:

i=1 where (cid:116)(i)   rm
z   categorical( )
(cid:116)   gaussian(  z,  z)
p((cid:116), z;  ,   ,  ) = p((cid:116)|z;   ,  )p(z;  )
p((cid:116);  ,   ,  ) =
p((cid:116)|z;   ,  )p(z;  )

k z=1

(marginal)   log-     likelihood:

 ( ,   ,  ) = (cid:72)(cid:81)(cid:59)

=

n i=1

n i=1

(cid:72)(cid:81)(cid:59)

p((cid:116)(i);  ,   ,  )

k z=1

p((cid:116)(i)|z;   ,  )p(z;  )

51

assume we are given data, d, consisting of n fully unsupervised ex-
amples in m dimensions:

mixture-     model

data:   

d = {(cid:116)(i)}n

generative   story:

i=1 where (cid:116)(i)   rm
z   categorical( )
z   multinomial( )
(cid:116)   p (  |z)
(cid:116)   gaussian(  z,  z)

model:

joint:

marginal:

p , ((cid:116), z) = p ((cid:116)|z)p (z)
p ((cid:116)|z)p (z)
p , ((cid:116)) =

k z=1

(marginal)   log-     likelihood:

 ( ) = (cid:72)(cid:81)(cid:59)

=

n i=1

n i=1

(cid:72)(cid:81)(cid:59)

p , ((cid:116)(i))

k z=1

p ((cid:116)(i)|z)p (z)

52

assume we are given data, d, consisting of n fully unsupervised ex-
amples in m dimensions:

mixture-     model

data:   

d = {(cid:116)(i)}n

generative   story:

model:

joint:

marginal:

(marginal)   log-     likelihood:

i=1 where (cid:116)(i)   rm
z   categorical( )
z   multinomial( )
(cid:116)   p (  |z)
(cid:116)   gaussian(  z,  z)this   could   be   any   
arbitrary   distribution   
parameterized   by     .

p , ((cid:116), z) = p ((cid:116)|z)p (z)
today   we   re   thinking   
p ((cid:116)|z)p (z)
p , ((cid:116)) =
about   the   case   where   it   
is   a   multivariate   
gaussian.

k z=1

 ( ) = (cid:72)(cid:81)(cid:59)

=

n i=1

n i=1

(cid:72)(cid:81)(cid:59)

p , ((cid:116)(i))

k z=1

p ((cid:116)(i)|z)p (z)

53

learning   a   mixture   model

supervised   learning:   the   
parameters   decouple!
d = {((cid:116)(i), (cid:120)(i))}n

i=1

z

unsupervised   learning:   parameters   
are   coupled   by   marginalization.

d = {(cid:116)(i)}n

i=1

z

x1

x2

   

xm

x1

x2

   

xm

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 , 

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

n i=1
n i=1
n i=1

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))p (z(i))

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 , 

n i=1

(cid:72)(cid:81)(cid:59)

k z=1

p ((cid:116)(i)|z)p (z)

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))

(cid:72)(cid:81)(cid:59) p (z(i))

54

learning   a   mixture   model

unsupervised   learning:   parameters   
are   coupled   by   marginalization.

d = {(cid:116)(i)}n

i=1

z

x1

x2

   

xm

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 , 

n i=1

(cid:72)(cid:81)(cid:59)

k z=1

p ((cid:116)(i)|z)p (z)

supervised   learning:   the   
parameters   decouple!
d = {((cid:116)(i), (cid:120)(i))}n
training   certainly   isn   t   as   simple   
as   the   supervised   case.

i=1

z

x2

in   many   cases,   we   could   still   use   
some   black-     box   optimization   
x1
method   (e.g.   newton-     raphson)   
to   solve   this   coupled
optimization   problem.

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))p (z(i))

xm

   

 , 

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

this   lecture   is   about   a   more   
problem-     specific   method:   em.

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))

 

n i=1
n i=1
n i=1

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

(cid:72)(cid:81)(cid:59) p (z(i))

55

example:   k-     means   vs   gmm

56

example:   k-     means

57

example:   k-     means

58

example:   k-     means

59

example:   k-     means

60

example:   k-     means

61

example:   k-     means

62

example:   k-     means

63

example:   k-     means

64

example:   gmm

65

example:   gmm

66

example:   gmm

67

example:   gmm

68

example:   gmm

69

example:   gmm

70

example:   gmm

71

example:   gmm

72

example:   gmm

73

example:   gmm

74

example:   gmm

75

example:   gmm

76

example:   gmm

77

example:   gmm

78

example:   gmm

79

example:   gmm

80

example:   gmm

81

example:   gmm

82

example:   gmm

83

example:   gmm

84

example:   gmm

85

example:   gmm

86

k-     means   vs.   gmm

convergence:   
k-     means tends   to   converge much   faster   than   a   gmm

speed:   
each   iteration   of   k-     means is   computationally   less   intensive than   
each   iteration   of   a   gmm

initialization:   
to   initialize a   gmm,   we   typically   first   run   k-     means and   use   the   
resulting   cluster   centers   as   the   means   of   the   gaussian   components

output:   
a   gmm   yields   a   id203   distribution   over   the   cluster   assignment   
for   each   point;   whereas   k-     means gives   a   single   hard   assignment

87

