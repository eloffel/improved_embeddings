10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

linear   regression

linear   regression   readings:
murphy   7.1       7.3
bishop   3.1
htf   3.1       3.4
mitchell   4.1-     4.3

logistic   regression   readings:
murphy   8.1-     8.3,   8.6
bishop   4.3.2,   4.3.4
htf   4.1,   4.4
mitchell      

   generative      logistic   regression      
(mitchell,   2016)
   maximum      gradient   training   
(elkan,   2014)

matt   gorid113y
lecture   8
february   13,   2016

1

reminders

    homework 2:   naive bayes

    release:   wed,   feb.   1
    due:   mon,   feb.   13   at   5:30pm

    homework 3:   linear   /   id28

    release:   mon,   feb.   13
    due:   wed,   feb.   22   at   5:30pm

2

linear   regression   outline

   

   

   

   

linear   functions

regression   problems
    definition
   
    residuals
    notation   trick:   fold   in   the   intercept
linear   regression   as   function   approximation
    objective   function:   mean   squared   error
    hypothesis   space:   linear   functions
optimization   for   linear   regression
    normal   equations   (closed-     form   solution)

   
   

computational   complexity
stability

   

sgd   for   linear   regression

partial   derivatives

   
    update   rule

    gradient   descent   for   linear   regression
probabilistic   interpretation   of   linear   regression
    generative   vs.   discriminative
   
   
   
   

conditional   likelihood
background:   gaussian   distribution
case   #1:   1d   linear   regression
case   #2:   multiple   linear   regression

last   lecture

this   lecture

3

regression   problems

whiteboard
    definition
    linear   functions
    residuals
    notation   trick:   fold   in   the   intercept

4

linear   regression   as   function   

approximation

whiteboard

    objective   function:   mean   squared   error
    hypothesis   space:   linear   functions

5

optimization   for   linear   regression
whiteboard

    normal   equations   (closed-     form   solution)

    computational   complexity
    stability

    sgd   for   linear   regression

    partial   derivatives
    update   rule

    gradient   descent   for   linear   regression

6

probabilistic   interpretation   of   linear   

regression

whiteboard

    generative   vs.   discriminative
    conditional   likelihood
    background:   gaussian   distribution
    case   #1:   1d   linear   regression
    case   #2:   multiple   linear   regression

7

convergence   curves

    for   the   batch   method,   

the   training   mse   is   
initially   large   due   to   
uninformed   
initialization

    in   the   online   update,   
n   updates   for   every   
epoch   reduces   mse   to   
a   much   smaller   value.

     eric   xing   @   cmu,   2006-     2011

8

