10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

experimental   design

+

k-     nearest   neighbors

knn   readings:
mitchell   8.2
htf   13.3
murphy   -     -     -     
bishop   2.5.2

prob.   readings:   (next   lecture)
lecture   notes   from   10-     600   
(see   piazza   post   for   the   pointers)

murphy   2
bishop   2
htf   -     -     
mitchell   -     -     

matt   gorid113y
lecture   3
january   25,   2016

1

reminders

    background exercises (homework 1)

    released:   wed,   jan.   25
    due:   mon,   jan.   30   at   5:30pm

    website updates

    office   hours google   calendar   on    people   
    readings on    schedule   

    meet ais:   sarah,   daniel,   brynn

2

outline
    k-     nearest   neighbors   (knn)

    special   cases
    choosing   k
    case   study:   knn   on   fisher   iris   data
    case   study:   knn   on   2d   gaussian   data

    experimental   design

    train   error   vs.   test   error
    train   /   validation   /   test   splits
    cross-     validation

    function   approximation   view   of   ml

3

k-     nearest   neighbors

4

k-     nearest   neighbors

whiteboard:

    special   cases
    choosing   k

5

knn   on   fisher   iris   data

6

fisher   iris   dataset

fisher   (1936)   used   150   measurements   of   flowers   
from   3   different   species:   iris   setosa (0),   iris   
virginica (1),   iris   versicolor (2)   collected   by   
anderson   (1936)

species

0
0
0
1
1
1
1

sepal   
length
4.3
4.9
5.3
4.9
5.7
6.3
6.7

sepal   
width
3.0
3.6
3.7
2.4
2.8
3.3
3.0

petal   
length
1.1
1.4
1.5
3.3
4.1
4.7
5.0

petal   
width
0.1
0.1
0.2
1.0
1.3
1.6
1.7

full   dataset:   https://en.wikipedia.org/wiki/iris_flower_data_set

7

knn   on   fisher   iris   data

8

knn   on   fisher   iris   data

special   case:   nearest   neighbor

9

knn   on   fisher   iris   data

special   case:   majority   vote

10

knn   on   fisher   iris   data

11

knn   on   fisher   iris   data

special   case:   nearest   neighbor

12

knn   on   fisher   iris   data

13

knn   on   fisher   iris   data

14

knn   on   fisher   iris   data

15

knn   on   fisher   iris   data

16

knn   on   fisher   iris   data

17

knn   on   fisher   iris   data

18

knn   on   fisher   iris   data

19

knn   on   fisher   iris   data

20

knn   on   fisher   iris   data

21

knn   on   fisher   iris   data

22

knn   on   fisher   iris   data

23

knn   on   fisher   iris   data

24

knn   on   fisher   iris   data

25

knn   on   fisher   iris   data

26

knn   on   fisher   iris   data

27

knn   on   fisher   iris   data

28

knn   on   fisher   iris   data

29

knn   on   fisher   iris   data

30

knn   on   fisher   iris   data

31

knn   on   fisher   iris   data

special   case:   majority   vote

32

knn   on   gaussian   data

33

knn   on   gaussian   data

34

knn   on   gaussian   data

35

knn   on   gaussian   data

36

knn   on   gaussian   data

37

knn   on   gaussian   data

38

knn   on   gaussian   data

39

knn   on   gaussian   data

40

knn   on   gaussian   data

41

knn   on   gaussian   data

42

knn   on   gaussian   data

43

knn   on   gaussian   data

44

knn   on   gaussian   data

45

knn   on   gaussian   data

46

knn   on   gaussian   data

47

knn   on   gaussian   data

48

knn   on   gaussian   data

49

knn   on   gaussian   data

50

knn   on   gaussian   data

51

knn   on   gaussian   data

52

knn   on   gaussian   data

53

knn   on   gaussian   data

54

knn   on   gaussian   data

55

knn   on   gaussian   data

56

knn   on   gaussian   data

57

knn   on   gaussian   data

58

choosing   the   number   of   
neighbors

59

(name   changed   from   k-     fold   cross-     validation   to   avoid   confusion   with   knn)

f-     fold   cross-     validation

key   idea:   rather   than   just   a   single      validation      set,   use   
many!   (error   is   more   stable.   slower   computation.)

d = y(1)
y(2)

x(1)
x(2)

y(n)

x(n)

fold   1

fold   2

fold   3

fold   4

2.

divide   data   into   folds   (e.g.   4)
train   on   folds   {1,2,3}   and   
1.
predict   on   {4}
train   on   folds   {1,2,4}   and   
predict   on   {3}
train   on   folds   {1,3,4}   and   
predict   on   {2}
train   on   folds   {2,3,4}   and   
predict   on   {1}

4.

3.

concatenate   all   the   
predictions   and   evaluate   
error

60

math   as   code

how   to   implement?

ymax =   argmax f(y)

y       y   

it   depends   on   how   large   the   set   y   is!
if   it   s   a   small   enumerable   set   y   =   {1,2,   ,77},   
then:

ymax = -inf
for y in {1,2,   77}:
if f(y) > ymax:
ymax = y

return ymax

61

math   as   code

how   to   implement?

vmax =   max f(y)

y       y   

it   depends   on   how   large   the   set   y   is!
if   it   s   a   small   enumerable   set   y   =   {1,2,   ,77},   
then:

vmax = -inf
for y in {1,2,   77}:
if f(y) > vmax:
vmax = f(y)

return vmax

62

function   approximation   view   of   ml
whiteboard

63

beyond   the   scope   of   this   lecture
    k-     nearest   neighbors   (knn)   for   regression
    distance-     weighted knn
    cover   &   hart   (1967)   bayes   error   rate   bound
    knn   for   facial   recognition   (see   eigenfaces

in   pca   lecture)

64

takeaways

    k-     nearest   neighbors

    requires   careful   choice   of   k   (#   of   neighbors)
    experimental   design   can   be   just   as   important   as   the   

learning   algorithm   itself

    function   approximation   view

    assumption:   inputs   are   sampled   from   some   

unknown   distributions

    assumption:   outputs   come   from   a   fixed   unknown   

function   (e.g.   human   annotator)

    goal:   learn   a   hypothesis   which   closely   

approximates   that   function

65

