10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

id91
(k-     means)

id91   readings:
murphy   25.5
bishop   12.1,   12.3
htf   14.3.0
mitchell   -     -     

matt   gorid113y
lecture   15
march   8,   2017

1

reminders

    homework 5:   readings /   application of   ml

    release:   wed,   mar.   08
    due:   wed,   mar.   22   at   11:59pm

2

outline
    id91:   motivation   /   applications
    optimization   background

    coordinate   descent
    block   coordinate   descent

    id91

    inputs   and   outputs
    objective-     based   id91

    k-     means

    k-     means   objective
    computational   complexity
    k-     means   algorithm   /   lloyd   s   method

    k-     means   initialization

    random
    farthest   point
    k-     means++

3

id91,   informal   goals

goal:   automatically   partition   unlabeled data   into   groups   of   similar   
datapoints.

question:   when   and   why   would   we   want   to   do   this?

useful   for:

      automatically   organizing   data.
      understanding   hidden   structure   in   data.

      preprocessing   for   further   analysis.

      representing   high-     dimensional   data   in   a   low-     dimensional   space   (e.g.,   
for   visualization   purposes).

slide   courtesy   of   nina   balcan

applications (id91   comes   up   everywhere   )

    cluster   news   articles   or   web   pages   or   search   results   by   topic.

    cluster   protein   sequences   by   function   or   genes   according   to   expression   

profile.

    cluster   users   of   social   networks   by   interest   (community   detection).

facebook network

twitter network

slide   courtesy   of   nina   balcan

applications   (id91   comes   up   everywhere   )

    cluster   customers   according   to   purchase   history.

    cluster   galaxies   or   nearby   stars (e.g.   sloan   digital   sky   survey)

    and   many   many more   applications   .

slide   courtesy   of   nina   balcan

optimization   background

whiteboard:

    coordinate   descent
    block   coordinate   descent

7

id91

whiteboard:

    inputs   and   outputs
    objective-     based   id91

8

k-     means

whiteboard:

    k-     means   objective
    computational   complexity
    k-     means   algorithm   /   lloyd   s   method

9

k-     means   initialization

whiteboard:
    random
    furthest   traversal
    k-     means++

10

lloyd   s   method:   random   initialization

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

example:   given   a   set   of   datapoints

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

select   initial   centers   at   random

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

assign   each   point   to   its   nearest   center

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

recompute optimal   centers   given   a   fixed   id91

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

assign   each   point   to   its   nearest   center

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

recompute optimal   centers   given   a   fixed   id91

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

assign   each   point   to   its   nearest   center

slide   courtesy   of   nina   balcan

lloyd   s   method:   random   initialization

recompute optimal   centers   given   a   fixed   id91

get   a   good      quality   solution   in   this   example.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

it   always   converges,   but   it   may   converge   at   a   local   optimum   that   is   
different   from   the   global   optimum,   and   in   fact   could   be   arbitrarily   
worse   in   terms   of   its   score.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

local   optimum:   every   point   is   assigned   to   its   nearest   center   and   
every   center   is   the   mean   value   of   its   points.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

.it   is   arbitrarily   worse   than   optimum   solution   .

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

this   bad   performance,   can   happen   
even   with   well   separated   gaussian   
clusters.

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

this   bad   performance,   can   
happen   even   with   well   
separated   gaussian   clusters.

some   gaussian   are   
combined   ..

slide   courtesy   of   nina   balcan

lloyd   s   method:   performance

   

if   we   do   random   initialization,   as   k increases,   it   becomes   more   likely   
we   won   t   have   perfectly   picked   one   center   per   gaussian   in   our   
initialization   (so   lloyd   s   method   will   output   a   bad   solution).

    for   k   equal-     sized   gaussians,   pr[each   initial   center   is   in   a   

different   gaussian]      "!"$    %&$

    becomes   unlikely   as   k   gets   large.   

slide   courtesy   of   nina   balcan

another   initialization   idea:   furthest   point   

heuristic

choose            arbitrarily   (or   at   random).
    for   j=2,   ,k
    pick            among   datapoints           ,        ,   ,         that   is   farthest   
from   previously   chosen           ,        ,   ,        0    

fixes   the   gaussian   problem.   but   it   can   be   thrown   off   by   
outliers   .

slide   courtesy   of   nina   balcan

furthest   point   heuristic   does   well   on   previous   

example

slide   courtesy   of   nina   balcan

furthest   point   initialization   heuristic   sensitive   

to   outliers

assume   k=3

(-     2,0)

(0,1)

(0,-     1)

(3,0)

slide   courtesy   of   nina   balcan

furthest   point   initialization   heuristic   sensitive   

to   outliers

assume   k=3

(-     2,0)

(0,1)

(0,-     1)

(3,0)

slide   courtesy   of   nina   balcan

   
   

interpolate   between   random   and   furthest   point   initialization

k-     means++   initialization:   d6 sampling   [av07]
let   d(x) be   the   distance   between   a   point        and   its   nearest   center.   
chose   the   next   center   proportional   to   d6(    ).
    choose            at   random.
    for   j=2,   ,k
    pick            among           ,        ,   ,         according   to   the   distribution
        (        =        )                   ?@    	
                       ?     

d6(        )

theorem:   k-     means++   always   attains   an   o(log   k)   approximation   to   optimal   
k-     means   solution   in   expectation.

running   lloyd   s can   only   further   improve   the   cost.

slide   courtesy   of   nina   balcan

   
   

interpolate   between   random   and   furthest   point   initialization

k-     means++   idea:   d6 sampling
let   d(x) be   the   distance   between   a   point        and   its   nearest   center.   
chose   the   next   center   proportional   to   dd(    ).
        =0,   random   sampling
        =   ,   furthest   point    (side   note:   it   actually   works   well   for   k-     center)
        =2,   k-     means++
side   note:       =1,   works   well   for   k-     median   

slide   courtesy   of   nina   balcan

k-     means   ++   fix

(-     2,0)

(0,1)

(0,-     1)

(3,0)

slide   courtesy   of   nina   balcan

k-     means++/ lloyd   s running   time

    k-     means   ++   initialization:   o(nd)   and   one   pass   over   data   to   select   

next   center.   so   o(nkd)   time   in   total.

    lloyd   s   method

repeat until   there   is   no   change   in   the   cost.

for   each   j:      cj   {            whose   closest   center   is           }
for   each   j:              mean   of   cj

   

   

each   round   takes   time   
o(nkd).

    exponential   #   of   rounds   in   the   worst   case   [av07].

    expected   polynomial   time   in   the   smoothed   analysis   (non   worst-     case)   

model!

slide   courtesy   of   nina   balcan

k-     means++/ lloyd   s   summary

    running   lloyd   s   can   only   further   improve   the   cost.

    exponential   #   of   rounds   in   the   worst   case   [av07].

    expected   polynomial   time   in   the   smoothed   analysis   model!

    does   well   in   practice.

    k-     means++   always   attains   an   o(log   k)   approximation   to   optimal   k-     

means   solution   in   expectation.

slide   courtesy   of   nina   balcan

what   value   of   k???

    heuristic:   find   large   gap   between   k   -     1-     means   cost   and   k-     

means   cost.

    hold-     out   validation/cross-     validation   on   auxiliary   task   (e.g.,   

supervised   learning   task).

    try   hierarchical   id91.

slide   courtesy   of   nina   balcan

