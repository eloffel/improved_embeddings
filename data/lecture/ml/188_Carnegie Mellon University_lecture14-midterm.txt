10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

midterm   exam   review

matt   gorid113y
lecture   14
march   6,   2017

1

reminders

    midterm exam (evening exam)
    tue,   mar.   07   at   7:00pm       9:30pm
    see piazza   for details about location

2

outline

    midterm   exam   logistics
    sample   questions
    classification   and   regression:   

the   big   picture

    q&a

3

midterm   exam   logistics

4

midterm   exam

    logistics

    evening   exam

tue,   mar.   07   at   7:00pm       9:30pm

    8-     9   sections
    format   of   questions:

    multiple   choice
    true   /   false   (with   justification)
    derivations
    short   answers
    interpreting   figures

    no   electronic   devices
    you   are   allowed   to   bring one   8     x   11   sheet   of   notes   

(front   and   back)

5

midterm   exam

    how   to   prepare

    attend   the   midterm   review   session:   

thu,   march   2   at   6:30pm   (ph   100)

    attend   the   midterm   review   lecture

mon,   march   6   (in-     class)

    review   prior   year   s   exam   and   solutions

(we   ll   post   them)

    review   this   year   s   homework   problems

6

midterm   exam

    advice   (for   during   the   exam)
    solve   the   easy   problems   first   

(e.g.   multiple   choice   before   derivations)
    if   a   problem   seems   extremely   complicated   you   re   likely   

missing   something

    don   t   leave   any   answer   blank!
    if   you   make   an   assumption,   write   it   down
    if   you   look   at   a   question   and   don   t   know   the   

answer:
    we   probably   haven   t   told   you   the   answer
    but   we   ve   told   you   enough   to   work   it   out
    imagine   arguing   for   some   answer   and   see   if   you   like   it

7

topics   for   midterm
    regression

    foundations
    id203
    id113,   map
    optimization

    classifiers

    knn
    na  ve   bayes
    logistic   regression
    id88
    id166

    linear   regression

    important   concepts

    kernels
    id173   and   

overfitting

    experimental   design

8

sample   questions

9

sample   questions

1.4 id203
1.4 id203
assume we have a sample space    . answer each question with t or f. no justi   cation
assume we have a sample space    . answer each question with t or f. no justi   cation
is required.
is required.
1.4 id203
(a) [1 pts.] t or f: if events a, b, and c are disjoint then they are independent.
(a) [1 pts.] t or f: if events a, b, and c are disjoint then they are independent.
assume we have a sample space    . answer each question with t or f. no justi   cation
is required.

(a) [1 pts.] t or f: if events a, b, and c are disjoint then they are independent.
(b) [1 pts.] t or f: p (a|b) /
(b) [1 pts.] t or f: p (a|b) /

. (the sign    /    means    is proportional to   )
. (the sign    /    means    is proportional to   )

p (a)p (b|a)
p (a)p (b|a)

p (a|b)
p (a|b)

(b) [1 pts.] t or f: p (a|b) /
(c) [1 pts.] t or f: p (a [ b)     p (a).
(c) [1 pts.] t or f: p (a [ b)     p (a).

p (a|b)

p (a)p (b|a)

. (the sign    /    means    is proportional to   )

(c) [1 pts.] t or f: p (a [ b)     p (a).
(d) [1 pts.] t or f: p (a \ b)   p (a).
(d) [1 pts.] t or f: p (a \ b)   p (a).

(d) [1 pts.] t or f: p (a \ b)   p (a).

10

10-701 machine learning

10-701 machine learning

4 id92 [12 pts]

sample   questions

midterm exam - page 7 of 17

midterm exam - page 8 of 17

11/02/2016

11/02/2016

in this problem, you will be tested on your knowledge of k-nearest neighbors (id92), where
k indicates the number of nearest neighbors.

now we will apply k-nearest neighbors using euclidean distance to a binary classi   -
cation task. we assign the class of the test point to be the class of the majority of the
k nearest neighbors. a point can be its own neighbor.
1. [3 pts] for id92 in general, are there any cons of using very large k values? select

one. brie   y justify your answer.

(a) yes

(b) no

2. [3 pts] for id92 in general, are there any cons of using very small k values? select

one. brie   y justify your answer.

(a) yes

(b) no

figure 5

3. [2 pts] what value of k minimizes leave-one-out cross-validation error for the dataset

shown in figure 5? what is the resulting error?

11

10-601: machine learning
(b) [2 pts.] derive the following formula for the log likelihood:
10-601: machine learning

page 3 of 16
page 3 of 16

sample   questions

2/29/2016
2/29/2016

xi! log(   ) + n  

`(   ; x1, . . . , xn) =  nxi=1

1.2 id113 (id113)
1.2 id113 (id113)
assume we have a random sample that is bernoulli distributed x1, . . . , xn     bernoulli(   ).
assume we have a random sample that is bernoulli distributed x1, . . . , xn     bernoulli(   ).
we are going to derive the id113 for    . recall that a bernoulli random variable x takes
we are going to derive the id113 for    . recall that a bernoulli random variable x takes
values in {0, 1} and has id203 mass function given by
values in {0, 1} and has id203 mass function given by
p (x;    ) =    x(1      )1 x.
p (x;    ) =    x(1      )1 x.

xi! log(1      ).

nxi=1

(a) [2 pts.] derive the likelihood, l(   ; x1, . . . , xn).
(a) [2 pts.] derive the likelihood, l(   ; x1, . . . , xn).

(c) extra credit: [2 pts.] derive the following formula for the id113:       =

1
n

i=1 xi).

(pn

(b) [2 pts.] derive the following formula for the log likelihood:
(b) [2 pts.] derive the following formula for the log likelihood:

`(   ; x1, . . . , xn) =  nxi=1
`(   ; x1, . . . , xn) =  nxi=1

xi! log(   ) + n  
xi! log(   ) + n  

nxi=1
nxi=1

xi! log(1      ).
xi! log(1      ).

12

sample   questions

10-601: machine learning

page 4 of 16

2/29/2016

1.3 map vs id113

answer each question with t or f and provide a one sentence explanation of your
answer:

(a) [2 pts.] t or f: in the limit, as n (the number of samples) increases, the map and

id113 estimates become the same.

(b) [2 pts.] t or f: naive bayes can only be used with map estimates, and not id113

13

estimates.

page 2 of 16
page 2 of 16

sample   questions

1.1 naive bayes
10-601: machine learning
2/29/2016
you are given a data set of 10,000 students with their sex, height, and hair color. you are
2/29/2016
10-601: machine learning
trying to build a classi   er to predict the sex of a student, so you randomly split the data
into a training set and a testing set. here are the speci   cations of the data set:
1 naive bayes, id203, and id113 [20 pts. + 2 extra credit]
1 naive bayes, id203, and id113 [20 pts. + 2 extra credit]
    sex 2 {male,female}
1.1 naive bayes
    height 2 [0,300] centimeters
1.1 naive bayes
you are given a data set of 10,000 students with their sex, height, and hair color. you are
    hair 2 {brown, black, blond, red, green}
you are given a data set of 10,000 students with their sex, height, and hair color. you are
trying to build a classi   er to predict the sex of a student, so you randomly split the data
trying to build a classi   er to predict the sex of a student, so you randomly split the data
into a training set and a testing set. here are the speci   cations of the data set:
    3240 men in the data set
into a training set and a testing set. here are the speci   cations of the data set:
    sex 2 {male,female}
    6760 women in the data set
    sex 2 {male,female}
    height 2 [0,300] centimeters
    height 2 [0,300] centimeters
    hair 2 {brown, black, blond, red, green}
    hair 2 {brown, black, blond, red, green}
    3240 men in the data set
    3240 men in the data set
    6760 women in the data set
since it cannot handle continuous valued variables.
    6760 women in the data set

under the assumptions necessary for naive bayes (not the distributional assumptions you
might naturally or intuitively make about the dataset) answer each question with t or f
and provide a one sentence explanation of your answer:

(a) [2 pts.] t or f: as height is a continuous valued variable, naive bayes is not appropriate

under the assumptions necessary for naive bayes (not the distributional assumptions you
under the assumptions necessary for naive bayes (not the distributional assumptions you
might naturally or intuitively make about the dataset) answer each question with t or f
might naturally or intuitively make about the dataset) answer each question with t or f
and provide a one sentence explanation of your answer:
and provide a one sentence explanation of your answer:
(a) [2 pts.] t or f: as height is a continuous valued variable, naive bayes is not appropriate
(b) [2 pts.] t or f: since there is not a similar number of men and women in the dataset,
(a) [2 pts.] t or f: as height is a continuous valued variable, naive bayes is not appropriate

since it cannot handle continuous valued variables.
naive bayes will have high test error.
since it cannot handle continuous valued variables.

(b) [2 pts.] t or f: since there is not a similar number of men and women in the dataset,
(c) [2 pts.] t or f: p (height|sex, hair) = p (height|sex).
(b) [2 pts.] t or f: since there is not a similar number of men and women in the dataset,

naive bayes will have high test error.
naive bayes will have high test error.

(d) [2 pts.] t or f: p (height, hair|sex) = p (height|sex)p (hair|sex).
(c) [2 pts.] t or f: p (height|sex, hair) = p (height|sex).
(c) [2 pts.] t or f: p (height|sex, hair) = p (height|sex).

14

arg min

page 7 of 16

real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:

given that we have an input x and we want to estimate an output y, in id75
we assume the relationship between them is of the form y = wx + b +    , where w and b are
real-valued parameters we estimate and     represents the noise in the data. when the noise
10-601: machine learning
2/29/2016
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:
3 linear and id28 [20 pts. + 2 extra credit]

sample   questions
nxi=1
(yi   (wxi + b))2.
nxi=1

consider the dataset s plotted in fig. 1 along with its associated regression line. for
3.1 id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
given that we have an input x and we want to estimate an output y, in id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
we assume the relationship between them is of the form y = wx + b +    , where w and b are
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
your answers in the table below.
the parameters w and b is equivalent to minimizing the squared error:

(yi   (wxi + b))2.

dataset

arg min

(b)

(d)

(a)

(c)

(e)

w

w

regression line
dataset

regression line
arg min

w

(c)

(b)

(d)
(a)
(yi   (wxi + b))2.

(e)

nxi=1

dataset

10-601: machine learning

page 8 of 16

consider the dataset s plotted in fig. 1 along with its associated regression line. for
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

dataset

(a)

(b)

(c)

(d)

(e)

regression line

figure 1: an observed data set and its associated regression line.

figure 1: an observed data set and its associated regression line.

(a) adding one outlier to the
original data set.

(b) adding two outliers to the original data
set.

figure 1: an observed data set and its associated regression line.

figure 2: new regression lines for altered data sets snew.

figure 2: new regression lines for altered data sets snew.

15

arg min

page 7 of 16

real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:

given that we have an input x and we want to estimate an output y, in id75
we assume the relationship between them is of the form y = wx + b +    , where w and b are
real-valued parameters we estimate and     represents the noise in the data. when the noise
10-601: machine learning
2/29/2016
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:
3 linear and id28 [20 pts. + 2 extra credit]

sample   questions
nxi=1
(yi   (wxi + b))2.
nxi=1

consider the dataset s plotted in fig. 1 along with its associated regression line. for
3.1 id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
given that we have an input x and we want to estimate an output y, in id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
we assume the relationship between them is of the form y = wx + b +    , where w and b are
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
your answers in the table below.
the parameters w and b is equivalent to minimizing the squared error:

(yi   (wxi + b))2.

dataset

arg min

(b)

(d)

(a)

(c)

(e)

w

w

regression line
dataset

regression line
arg min

w

(c)

(b)

(d)
(a)
(yi   (wxi + b))2.

(e)

nxi=1

dataset

(a) adding one outlier to the
original data set.

(b) adding two outliers to the original data
set.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

dataset

(a)

(b)

(c)

(d)

(e)

regression line

figure 1: an observed data set and its associated regression line.

figure 1: an observed data set and its associated regression line.

(c) adding three outliers to the original data
set. two on one side and one on the other
side.

figure 1: an observed data set and its associated regression line.

figure 2: new regression lines for altered data sets snew.

figure 2: new regression lines for altered data sets snew.

16

arg min

page 7 of 16

real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:

given that we have an input x and we want to estimate an output y, in id75
we assume the relationship between them is of the form y = wx + b +    , where w and b are
real-valued parameters we estimate and     represents the noise in the data. when the noise
10-601: machine learning
2/29/2016
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:
3 linear and id28 [20 pts. + 2 extra credit]

sample   questions
nxi=1
(yi   (wxi + b))2.
nxi=1

consider the dataset s plotted in fig. 1 along with its associated regression line. for
3.1 id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
given that we have an input x and we want to estimate an output y, in id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
we assume the relationship between them is of the form y = wx + b +    , where w and b are
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
your answers in the table below.
the parameters w and b is equivalent to minimizing the squared error:

(yi   (wxi + b))2.

dataset

arg min

(b)

(d)

(a)

(c)

(e)

w

w

(a) adding one outlier to the
original data set.
(c)

(e)

regression line
dataset

regression line
arg min

w

(b)

(d)
(a)
(yi   (wxi + b))2.

nxi=1

dataset

(b) adding two outliers to the original data
set.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

dataset

(a)

(b)

(c)

(d)

(e)

regression line

figure 1: an observed data set and its associated regression line.

figure 1: an observed data set and its associated regression line.

(c) adding three outliers to the original data
set. two on one side and one on the other
side.

(d) duplicating the original data set.

figure 1: an observed data set and its associated regression line.

figure 2: new regression lines for altered data sets snew.

figure 2: new regression lines for altered data sets snew.

17

arg min

page 7 of 16

real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:

given that we have an input x and we want to estimate an output y, in id75
we assume the relationship between them is of the form y = wx + b +    , where w and b are
real-valued parameters we estimate and     represents the noise in the data. when the noise
10-601: machine learning
2/29/2016
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
the parameters w and b is equivalent to minimizing the squared error:
3 linear and id28 [20 pts. + 2 extra credit]

sample   questions
nxi=1
(yi   (wxi + b))2.
nxi=1

consider the dataset s plotted in fig. 1 along with its associated regression line. for
3.1 id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
given that we have an input x and we want to estimate an output y, in id75
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
we assume the relationship between them is of the form y = wx + b +    , where w and b are
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
real-valued parameters we estimate and     represents the noise in the data. when the noise
is gaussian, maximizing the likelihood of a dataset s = {(x1, y1), . . . , (xn, yn)} to estimate
your answers in the table below.
the parameters w and b is equivalent to minimizing the squared error:

(yi   (wxi + b))2.

dataset

arg min

(b)

(d)

(a)

(c)

(e)

w

w

regression line
dataset

regression line
arg min

w

(c)

(b)

(d)
(a)
(yi   (wxi + b))2.

(e)

nxi=1

dataset

(c) adding three outliers to the original data
set. two on one side and one on the other
side.

(d) duplicating the original data set.

consider the dataset s plotted in fig. 1 along with its associated regression line. for
each of the altered data sets snew plotted in fig. 3, indicate which regression line (relative
to the original one) in fig. 2 corresponds to the regression line for the new data set. write
your answers in the table below.

dataset

(a)

(b)

(c)

(d)

(e)

regression line

figure 1: an observed data set and its associated regression line.

figure 1: an observed data set and its associated regression line.

(e) duplicating the original data set and
adding four points that lie on the trajectory
of the original regression line.

figure 3: new data set snew.

figure 1: an observed data set and its associated regression line.

figure 2: new regression lines for altered data sets snew.

figure 2: new regression lines for altered data sets snew.

18

training set, assuming a parametric model of the form

1

3.2 id28
given a training set {(xi, yi), i = 1, . . . , n} where xi 2 rd is a feature vector and yi 2 {0, 1}
the conditional log likelihood of the training set is
10-601: machine learning
2/29/2016
2/29/2016
10-601: machine learning
is a binary label, we want to    nd the parameters   w that maximize the likelihood for the
training set, assuming a parametric model of the form

sample   questions

p(y = 1|x; w) =

1 + exp( wt x)

page 9 of 16
page 9 of 16

.

`(w) =

yi log p(yi,|xi; w) + (1   yi) log(1   p(yi,|xi; w)),

3.2 id28
3.2 id28
given a training set {(xi, yi), i = 1, . . . , n} where xi 2 rd is a feature vector and yi 2 {0, 1}
given a training set {(xi, yi), i = 1, . . . , n} where xi 2 rd is a feature vector and yi 2 {0, 1}
and the gradient is
is a binary label, we want to    nd the parameters   w that maximize the likelihood for the
the conditional log likelihood of the training set is
is a binary label, we want to    nd the parameters   w that maximize the likelihood for the
training set, assuming a parametric model of the form
training set, assuming a parametric model of the form

p(y = 1|x; w) =

1 + exp( wt x)

1

.

nxi=1

`(w) =

nxi=1

r`(w) =
yi log p(yi,|xi; w) + (1   yi) log(1   p(yi,|xi; w)),
p(y = 1|x; w) =
p(y = 1|x; w) =

(yi   p(yi|xi; w))xi.
1 + exp( wt x)
1 + exp( wt x)

1
1

.
.

(a) [5 pts.]
and the gradient is
the conditional log likelihood of the training set is
the conditional log likelihood of the training set is

conditional log likelihood? how would you compute   w in practice?

is it possible to get a closed form for the parameters   w that maximize the

nxi=1

`(w) =
`(w) =

nxi=1
nxi=1

r`(w) =

yi log p(yi,|xi; w) + (1   yi) log(1   p(yi,|xi; w)),
yi log p(yi,|xi; w) + (1   yi) log(1   p(yi,|xi; w)),

(yi   p(yi|xi; w))xi.

nxi=1

(a) [5 pts.]
and the gradient is
and the gradient is
(b) [5 pts.] what is the form of the classi   er output by id28?

conditional log likelihood? how would you compute   w in practice?

is it possible to get a closed form for the parameters   w that maximize the

r`(w) =
r`(w) =

nxi=1
nxi=1

(yi   p(yi|xi; w))xi.
(yi   p(yi|xi; w))xi.

(a) [5 pts.]
(a) [5 pts.]
(b) [5 pts.] what is the form of the classi   er output by id28?

conditional log likelihood? how would you compute   w in practice?
conditional log likelihood? how would you compute   w in practice?

is it possible to get a closed form for the parameters   w that maximize the
is it possible to get a closed form for the parameters   w that maximize the

(c) [2 pts.] extra credit: consider the case with binary features, i.e, x 2 {0, 1}d     rd,
where feature x1 is rare and happens to appear in the training set with only label 1.
(b) [5 pts.] what is the form of the classi   er output by id28?
(b) [5 pts.] what is the form of the classi   er output by id28?
what is   w1? is the gradient ever zero for any    nite w? why is it important to include
a id173 term to control the norm of   w?

(c) [2 pts.] extra credit: consider the case with binary features, i.e, x 2 {0, 1}d     rd,
where feature x1 is rare and happens to appear in the training set with only label 1.
what is   w1? is the gradient ever zero for any    nite w? why is it important to include
a id173 term to control the norm of   w?

19

10-601b: machine learning
10-601b: machine learning

page 5 of ??
page 5 of ??

10/10/2016
10/10/2016

samples   questions

2 to err is machine-like [20 pts]
2 to err is machine-like [20 pts]
2.1 train and test errors
2.1 train and test errors
in this problem, we will see how you can debug a classi   er by looking at its train and test errors.
in this problem, we will see how you can debug a classi   er by looking at its train and test errors.
consider a classi   er trained till convergence on some training data dtrain, and tested on a separate
consider a classi   er trained till convergence on some training data dtrain, and tested on a separate
test set dtest. you look at the test error, and    nd that it is very high. you then compute the training
test set dtest. you look at the test error, and    nd that it is very high. you then compute the training
error and    nd that it is close to 0.
error and    nd that it is close to 0.

1. [4 pts] which of the following is expected to help? select all that apply.
1. [4 pts] which of the following is expected to help? select all that apply.

(a) increase the training data size.
(a) increase the training data size.
(b) decrease the training data size.
(b) decrease the training data size.
(c) increase model complexity (for example, if your classi   er is an id166, use a more
(c) increase model complexity (for example, if your classi   er is an id166, use a more

complex kernel. or if it is a decision tree, increase the depth).
complex kernel. or if it is a decision tree, increase the depth).

(d) decrease model complexity.
(d) decrease model complexity.
(e) train on a combination of dtrain and dtest and test on dtest
(e) train on a combination of dtrain and dtest and test on dtest
(f) conclude that machine learning does not work.
(f) conclude that machine learning does not work.

2. [5 pts] explain your choices.
2. [5 pts] explain your choices.

20

10-601b: machine learning

page 5 of ??

10/10/2016

2 to err is machine-like [20 pts]

samples   questions

2.1 train and test errors
3. [2 pts] what is this scenario called?
in this problem, we will see how you can debug a classi   er by looking at its train and test errors.
consider a classi   er trained till convergence on some training data dtrain, and tested on a separate
test set dtest. you look at the test error, and    nd that it is very high. you then compute the training
error and    nd that it is close to 0.

4. [1 pts] say you plot the train and test errors as a function of the model complexity. which
1. [4 pts] which of the following is expected to help? select all that apply.
of the following two plots is your plot expected to look like?
10/10/2016

10-601b: machine learning

page 6 of ??

(a) increase the training data size.
(b) decrease the training data size.
(c) increase model complexity (for example, if your classi   er is an id166, use a more

complex kernel. or if it is a decision tree, increase the depth).

(d) decrease model complexity.
(e) train on a combination of dtrain and dtest and test on dtest
(f) conclude that machine learning does not work.

2. [5 pts] explain your choices.

(a)

(b)

21

10-601: machine learning

sample   questions

page 10 of 16

2/29/2016

4 id166, id88 and kernels [20 pts. + 4 extra credit]

4.1 true or false

answer each of the following questions with t or f and provide a one line justi   cation.
(a) [2 pts.] consider two datasets d(1) and d(2) where d(1) = {(x(1)

n , y(1)
n )}
and d(2) = {(x(2)
i 2 rd2. suppose d1 > d2
and n > m. then the maximum number of mistakes a id88 algorithm will make
is higher on dataset d(1) than on dataset d(2).

m )} such that x(1)

i 2 rd1, x(2)

1 ), ..., (x(1)

1 ), ..., (x(2)

1 , y(1)

1 , y(2)

m , y(2)

(b) [2 pts.] suppose  (x) is an arbitrary feature mapping from input x 2 x to  (x) 2 rn

and let k(x, z) =  (x)     (z). then k(x, z) will always be a valid id81.

(c) [2 pts.] given the same training data, in which the points are linearly separable, the
margin of the decision boundary produced by id166 will always be greater than or equal
to the margin of the decision boundary produced by id88.

24

sample   questions

10-601: machine learning
10-601: machine learning

page 11 of 16
page 11 of 16

2/29/2016
2/29/2016

4.3 analysis
4.3 analysis

(a) [4 pts.] in one or two sentences, describe the bene   t of using the kernel trick.
(a) [4 pts.] in one or two sentences, describe the bene   t of using the kernel trick.

(b) [4 pt.] the concept of margin is essential in both id166 and id88. describe why a
(b) [4 pt.] the concept of margin is essential in both id166 and id88. describe why a

large margin separator is desirable for classi   cation.
large margin separator is desirable for classi   cation.

(c) [4 pts.] extra credit: consider the dataset in fig. 4. under the id166 formulation in
(c) [4 pts.] extra credit: consider the dataset in fig. 4. under the id166 formulation in

section 4.2(a),
section 4.2(a),

(1) draw the decision boundary on the graph.
(1) draw the decision boundary on the graph.

(2) what is the size of the margin?
(2) what is the size of the margin?

(3) circle all the support vectors on the graph.
(3) circle all the support vectors on the graph.

25

(b) [4 pt.] the concept of margin is essential in both id166 and id88. describe why a

large margin separator is desirable for classi   cation.

sample   questions

(c) [4 pts.] extra credit: consider the dataset in fig. 4. under the id166 formulation in

section 4.2(a),

(1) draw the decision boundary on the graph.

(2) what is the size of the margin?

(3) circle all the support vectors on the graph.

figure 4: id166 toy dataset

26

10-601b: machine learning

sample   questions

page 8 of ??

10/10/2016

3. [extra credit: 3 pts.] one formulation of soft-margin id166 optimization problem is:

10-601b: machine learning

page 8 of ??

10/10/2016

3. [extra credit: 3 pts.] one formulation of soft-margin id166 optimization problem is:

min
w

1
2kwk2

2 + c

   i

nxi=1
nxi=1

   i

min
w

s.t. yi(w>xi)   1      i 8i = 1, ..., n

2 + c

1
2kwk2
   i   0 8i = 1, ..., n
s.t. yi(w>xi)   1      i 8i = 1, ..., n
c   0
   i   0 8i = 1, ..., n
c   0

where (xi, yi) are training samples and w de   nes a linear decision boundary.
where (xi, yi) are training samples and w de   nes a linear decision boundary.
derive a formula for    i when the objective function achieves its minimum (no steps neces-
derive a formula for    i when the objective function achieves its minimum (no steps neces-
sary). note it is a function of yiw>xi. sketch a plot of    i with yiw>xi on the x-axis and
sary). note it is a function of yiw>xi. sketch a plot of    i with yiw>xi on the x-axis and
value of    i on the y-axis. what is the name of this function?
value of    i on the y-axis. what is the name of this function?

figure 2: plot here

28

the   big   picture
classification   and   
regression

30

classification   and   regression:   

the   big   picture

whiteboard

    decision   rules   /   models   (probabilistic   
generative,   probabilistic   discriminative,   
id88,   id166,   regression)

    objective   functions   (likelihood,   conditional   
likelihood,   hinge   loss,   mean   squared   error)

    id173 (l1,   l2,   priors   for   map)
    update   rules   (sgd,   id88)
    nonlinear   features   (preprocessing,   kernel   trick)

31

q&a

32

