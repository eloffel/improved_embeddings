10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

pca

+

neural   networks

pca   readings:
murphy   12
bishop   12
htf   14.5
mitchell   -     -     

neural   net   readings:
murphy   -     -     
bishop   5
htf   11
mitchell   4

matt   gorid113y
lecture   18
march   27,   2017

1

reminders

    homework 6:   unsupervised learning

    release:   wed,   mar.   22
    due:   wed,   mar.   22   at   11:59pm

2

dimensionality   reduction

3

pca   outline

    dimensionality   reduction

    high-     dimensional   data
    learning   (low   dimensional)   representations

    principal   component   analysis   (pca)

    examples:   2d   and   3d
    data   for   pca
    pca   definition
    objective   functions   for   pca
    pca,   eigenvectors,   and   eigenvalues
    algorithms   for   finding   eigenvectors   /   

eigenvalues
    pca   examples

    face   recognition
    image   compression

last   lecture

this   lecture

4

big   &   high-     dimensional   data

    high-     dimensions   =   lot   of   features

document   classification
features   per   document   =   
thousands   of   words/unigrams
millions   of   bigrams,      contextual   
information

surveys   -      netflix

480189   users   x   17770   movies

slide   from   nina   balcan

big   &   high-     dimensional   data

    high-     dimensions   =   lot   of   features

meg   brain   imaging

120   locations   x   500   time   points   
x   20   objects

or   any   high-     dimensional   image   data

slide   from   nina   balcan

    big   &   high-     dimensional   data.

    useful   to   learn   lower   dimensional   

representations   of   the   data.

slide   from   nina   balcan

learning   representations

pca,   kernel   pca,   ica:   powerful   unsupervised   learning   techniques   
for   extracting   hidden   (potentially   lower   dimensional)   structure   
from   high   dimensional   datasets.

useful   for:
    visualization   
    more   efficient   use   of   resources   
(e.g.,   time,   memory,   communication)

    statistical:   fewer   dimensions         better   generalization
    noise   removal   (improving   data   quality)
    further   processing   by   machine   learning   algorithms

slide   from   nina   balcan

principal   component   
analysis   (pca)

11

pca   outline

    dimensionality   reduction

    high-     dimensional   data
    learning   (low   dimensional)   representations

    principal   component   analysis   (pca)

    examples:   2d   and   3d
    data   for   pca
    pca   definition
    objective   functions   for   pca
    pca,   eigenvectors,   and   eigenvalues
    algorithms   for   finding   eigenvectors   /   eigenvalues

    pca   examples

    face   recognition
    image   compression

12

principal   component   analysis   (pca)

in   case   where   data      lies   on   or   near   a   low   d-     dimensional   linear   subspace,   
axes   of   this   subspace   are   an   effective   representation   of   the   data.

identifying   the   axes   is   known   as   principal   components   analysis,   and   can   be   
obtained   by   using   classic   matrix   computation   tools   (eigen   or   singular   value   
decomposition).

slide   from   nina   balcan

2d   gaussian   dataset

slide   from   barnabas   poczos

1st pca   axis

slide   from   barnabas   poczos

2nd pca   axis

slide   from   barnabas   poczos

principal   component   analysis   (pca)
whiteboard

    data   for   pca
    pca   definition
    objective   functions   for   pca

17

maximizing   the   variance
    consider   the   two   projections   below
    which   maximizes   the   variance?

option   a

we see that the projected data still has a fairly large variance, and the
points tend to be far from zero. in contrast, suppose had instead picked the
following direction:

option   b

4

we see that the projected data still has a fairly large variance, and the
points tend to be far from zero. in contrast, suppose had instead picked the
following direction:

figures   from   andrew   ng   (cs229   lecture   notes)

closer to the origin.

here, the projections have a signi   cantly smaller variance, and are much
20
we would like to automatically select the direction u corresponding to
the    rst of the two    gures shown above. to formalize this, note that given a

principal   component   analysis   (pca)
whiteboard

    pca,   eigenvectors,   and   eigenvalues
    algorithms   for   finding   eigenvectors   /   

eigenvalues

21

principal   component   analysis   (pca)

x	
   x#v=  v	
   ,   so   v   (the   first   pc)   is   the   eigenvector   of   
sample   correlation/covariance   matrix       	
       (
sample   variance   of   projection   v(    	
       (v=    v(v=    
thus,   the   eigenvalue       	
   denotes   the   amount   of   variability   
eigenvalues       *       ,       -      
    the   1st pc       * is   the   the eigenvector   of   the   sample   covariance   matrix       	
       (
    the   2nd   pc       , is   the   the eigenvector   of   the   sample   covariance   matrix   
    	
       ( associated   with   the   second   largest   eigenvalue   

captured   along   that   dimension (aka   amount   of   energy   along   that   
dimension).

associated   with   the   largest   eigenvalue   

    and   so   on      

slide   from   nina   balcan

how   many   pcs?

   

for   m original   dimensions,   sample   covariance   matrix   is   mxm,   and   has   
up   to   m eigenvectors.   so   m pcs.

    where   does   dimensionality   reduction   come   from?

can   ignore   the   components   of   lesser   significance.   

)

%

(
   

e
c
n
a

i
r
a
v

25

20

15

10

5

0

pc1 pc2 pc3 pc4 pc5 pc6 pc7 pc8 pc9 pc10

   

you   do   lose   some   information,   but   if   the   eigenvalues   are   small,   you   don   t   lose   
much
    m dimensions   in   original   data   
   
   
   

calculate m eigenvectors   and   eigenvalues
choose   only   the   first   d eigenvectors,   based   on   their   eigenvalues
final   data   set   has   only   d dimensions

     eric   xing   @   cmu,   2006-     2011

23

slides   from   barnabas   poczos

original   sources   include:   

    karl   booksh research   group
   
    ron   parr

tom   mitchell

pca   examples

24

face   recognition

slide   from   barnabas   poczos

challenge:   facial   recognition
    want   to   identify   specific   person,   based   on   facial   image
    robust   to   glasses,   lighting,   

      can   t   just   use   the   given   256   x   256   pixels

slide   from   barnabas   poczos

applying   pca:   eigenfaces

method: build   one   pca   database   for   the   whole   dataset   and   

then   classify   based   on   the   weights.

    example   data   set:      images   of   faces   

    famous   eigenface approach

[turk   &   pentland],   [sirovich &   kirby]

    each   face   x is      

    256         256   values   (luminance   at   location)
    x in        256     256            (view   as   64k   dim   vector)

r
e
a
l
   
v
a
u
e
s

2
5
6
   
x
   
2
5
6
   

l

x1,      ,   xm

x   =

m   faces

slide   from   barnabas   poczos

principle   components

slide   from   barnabas   poczos

reconstructing   

          faster   if   train   with   

    only   people   w/out   glasses
    same   lighting   conditions

slide   from   barnabas   poczos

shortcomings

    requires   carefully   controlled   data:

    all   faces   centered   in   frame
    same   size
    some   sensitivity   to   angle

    alternative:

       learn      one   set   of   pca   vectors   for   each   angle
    use   the   one   with   lowest   error

    method   is   completely   knowledge   free

    (sometimes   this   is   good!)
    doesn   t   know   that   faces   are   wrapped   around   3d   objects   

(heads)

    makes   no   effort   to   preserve   class   distinctions

slide   from   barnabas   poczos

image   compression

slide   from   barnabas   poczos

original   image

    divide   the   original   372x492 image   into   patches:

    each   patch   is   an   instance   that   contains   12x12   pixels   on   a   grid

    view   each   as   a   144-     d   vector

slide   from   barnabas   poczos

l2 error   and   pca   dim

slide   from   barnabas   poczos

pca   compression:   144d         60d

slide   from   barnabas   poczos

pca   compression:   144d         16d

slide   from   barnabas   poczos

16   most   important   eigenvectors

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2
4
6
8
10
12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

2 4 6 8 10 12

slide   from   barnabas   poczos

pca   compression:   144d         6d

slide   from   barnabas   poczos

6   most   important   eigenvectors

2
4
6
8
10
12

2
4
6
8
10
12

2

4

6

8

10 12

2

4

6

8

10 12

2
4
6
8
10
12

2
4
6
8
10
12

2

4

6

8

10 12

2

4

6

8

10 12

2
4
6
8
10
12

2
4
6
8
10
12

2

4

6

8

10 12

2

4

6

8

10 12

slide   from   barnabas   poczos

pca   compression:   144d         3d

slide   from   barnabas   poczos

3   most   important   eigenvectors

2

4

6

8

10

12

2

4

6

8

10

12

2

4

6

8

10

12

2

4

6

8

10

12

2

4

6

8

10

12

2

4

6

8

10

12

slide   from   barnabas   poczos

pca   compression:   144d         1d

slide   from   barnabas   poczos

60   most   important   eigenvectors

looks   like   the   discrete   cosine   bases   of   jpg!...

slide   from   barnabas   poczos

2d   discrete   cosine   basis

http://en.wikipedia.org/wiki/discrete_cosine_transform

slide   from   barnabas   poczos

neural   networks   outline

    logistic   regression   (recap)

    data,   model,   learning,   prediction

    neural   networks

    a   recipe   for   machine   learning
    visual   notation   for   neural   networks
    example:   logistic   regression   output   surface
    1-     layer   neural   network
    2-     layer   neural   network
    neural   net   architectures

    objective   functions
    activation   functions

    id26

    basic   chain   rule   (of   calculus)
    chain   rule   for   arbitrary   computation   graph
    id26   algorithm
    module-     based   automatic   differentiation   (autodiff)

44

recall:   logistic   regression

45

using   gradient   ascent   for   linear   

classifiers

key   idea   behind   today   s   lecture:

1. define   a   linear   classifier   (logistic   regression)
2. define   an   objective   function   (likelihood)
3. optimize   it   with   gradient   descent   to   learn   

parameters

4. predict   the   class   with   highest   id203   under   

the   model

46

using   gradient   ascent   for   linear   

classifiers

this   decision   function   isn   t   
differentiable:

use   a   differentiable   
function   instead:

h((cid:116)) = sign( t (cid:116))

p (y = 1|(cid:116)) =

1

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))

sign(x)

logistic(u)    

1
1+e   u

47

using   gradient   ascent   for   linear   

classifiers

this   decision   function   isn   t   
differentiable:

use   a   differentiable   
function   instead:

h((cid:116)) = sign( t (cid:116))

p (y = 1|(cid:116)) =

1

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))

sign(x)

logistic(u)    

1
1+e   u

48

logistic   regression

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.
i=1 where (cid:116)   rk and y   {0, 1}

d = {(cid:116)(i), y(i)}n

model:   logistic   function   applied   to   dot   product   of   
parameters   with   input   vector.
p (y = 1|(cid:116)) =

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))
learning:   finds   the   parameters   that   minimize   some   
objective   function.    = argmin

j( )

1

 

prediction:   output   is   the   most   probable   class.

  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
y {0,1}

p (y|(cid:116))

49

neural   networks

50

background

a   recipe   for   

machine   learning

1.   given   training   data:

face

face

not   a   face

2.   choose   each   of   these:

    decision   function

    loss   function

examples:   linear   regression,   
logistic   regression,   neural   network

examples:   mean-     squared   error,   
cross   id178

56

background

a   recipe   for   

machine   learning

1.   given   training   data:

3.   define   goal:

2.   choose   each   of   these:

    decision   function

    loss   function

4.   train   with   sgd:
(take   small   steps   
opposite   the   gradient)

57

background

a   recipe   for   

gradients

machine   learning

1.   given   training   data:

3.   define   goal:

id26 can   compute   this   
gradient!   
and   it   s   a   special   case   of   a   more   
general   algorithm   called   reverse-     
mode   automatic   differentiation   that   
4.   train   with   sgd:
can   compute   the   gradient   of   any   
differentiable   function   efficiently!
(take   small   steps   
opposite   the   gradient)

2.   choose   each   of   these:

    decision   function

    loss   function

58

background
goals   for   today   s   lecture

machine   learning

a   recipe   for   

1.   given   training   data:

1. explore   a   new   class   of   decision   functions   

3.   define   goal:

(neural   networks)

2. consider   variants   of   this   recipe   for   training

2.   choose   each   of   these:

    decision   function

    loss   function

4.   train   with   sgd:
(take   small   steps   
opposite   the   gradient)

59

decision   
functions

output

linear   regression

y = h (x) =  ( t x)
where  (a) = a

  1

  2

  3

input

  m

   

60

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

  1

  2

  3

input

  m

   

61

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

not   a   face

  1

  2

  3

input

face

face

  m

   

62

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1
in-     class   example

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

0

  1

  2

  3

input

1

y

1

  m

x1

   

x2

63

