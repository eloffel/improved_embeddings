cmsc 422 introduction to machine learning

lecture 21 deep learning ii

furong huang / furongh@cs.umd.edu

id28

    goal: model the id203 of a random 

variable y being 0 or 1 given experimental 
data. 

    consider a generalized linear model 

function parameterized by     ,

             =

1

1 +                   

    attempt to model the id203 that y is 0 

or 1 with function

pr          ;      =                   1                  1       

id28

    the likelihood assuming all the samples are 

independent
              

= pr          ;      =    

pr                  ;      =    

                

         1                     

1           

    maximum likelihood

    

    

max

    

    (    |    )     max

    

   

                

         1                     

1           

    

neural network with softmax classifier

    softmax classifier: multinomial logistic 

regression, the number of classes 
more than 2.

    score: instead of linear function as the 
exponent, we use a nonlinear function 
(e.g., a neural network      =     (        ;     )) 

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

softmax classifier (multinomial 
id28)

slide credit: fei-fei li & justin johnson & serena yeung

multi-layer: id26

            

        

        

    

            

        

    

           

sigmoid

neuron     

        
            

=

        
                

                
            

    

    

           

    

sigmoid

neuron     

        
                

=    

    

        
            

            
                

=    

            

    

        
            

=    

            

    

        
                

                
            

        
                

=    
    

        
    
                

    
                
    
            

    
            
                

    
                
    
            

    
            
                

=    
    

            

   
    

        
    
                

    
                
    
            

slide credit: bohyung han

2 2

back propagation revisited

    on board (gradient w.r.t elements): 

please take notes!

    vector format of the id26 

on slides

neural network: definition

neural network: forward pass

neural network: back prop i

neural network: back prop ii

revival in the 1980   s

    id26 discovered in 1970   s but popularized 

in 1986

    david e. rumelhart, geoffrey e. hinton, ronald j. 

williams.    learning representations by back-
propagating errors.    in nature, 1986.

    mlp is a universal approximator
    can approximate any non-linear function in theory, 

given enough neurons, data

    kurt hornik, maxwell stinchcombe, halbert white. 

   multilayer feedforward networks are universal 
approximators.    neural networks, 1989

    generated lots of excitement and applications

http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/
2 8

neural networks applied to vision

lenet     vision application

lecun, y; boser, b; denker, j; henderson, d; howard, r; 
hubbard, w; jackel, l,    id26 applied to handwritten 
zip code recognition,    in neural computation, 1989
usps digit recognition, later check reading
convolution, pooling (   weight sharing   ), fully connected layers

image credit: lecun, y., bottou, l., bengio, y., haffner, p.    gradient-based learning applied to 
document recognition.    proceedings of the ieee, 1998.

2 9

unsupervised neural networks

autoencoders
   

encode then decode the same 
input
no supervision needed

output x   

hidden layer

input x

h. bourlard and y. kamp. 1988. auto-association by multilayer id88s and singular value decomposition.
biol. cybern. 59, 4-5 (september 1988), 291-294.
(restricted) boltzman
machines (rbms)
   

hidden layer

stochastic networks that can 
learn representations 
restricted version: neurons 
must form bipartite graph

input x

ackley, david h; hinton geoffrey e; sejnowski, terrence j, "a learning algorithm for id82s", cognitive 
science, elsevier, 1985.
smolensky, paul. "chapter 6: information processing in dynamical systems: foundations of harmony theory.    in 
parallel distributed processing: explorations in the microstructure of cognition, volume 1: foundations, 1986.

3 0

   

   

recurrent neural networks

networks with loops
   

the output of a layer is used as input 
for the same (or lower) layer
can model dynamics (e.g. in space or 
time)

loops are unrolled
   

now a standard feed-forward network 
with many layers
suffers from vanishing gradient 
problem
in theory, can learn long term memory, 
in practice not (bengio et al, 1994)

   

   

   

image credit: chritopher olah   s blog http://colah.github.io/posts/2015-08-understanding-lstms/
sepp hochreiter (1991), untersuchungen zu dynamischen neuronalen netzen, diploma thesis. institut 
f. informatik, technische univ. munich. advisor: j. schmidhuber.
y. bengio, p. simard, p. frasconi. learning long-term dependencies with id119 is 
difficult. in tnn 1994.

3 1

long short term memory (lstm)

image credit: christopher colah   s blog,  http://colah.github.io/posts/2015-08-understanding-
lstms/

    a type of id56 explicitly designed not to have the vanishing or exploding 

gradient problem

    models long-term dependencies

    memory is propagated and accessed by gates

    used for id103, id38    

hochreiter, sepp; and schmidhuber, j  rgen.    long short-term memory.    neural computation, 1997.

3 2

issues in deep neural networks

large amount of training time

there are sometimes a lot of training data
many iterations (epochs) are typically required for optimization
computing gradients in each iteration takes too much time

overfitting

learned function fits training data well, but performs poorly on new data 
(high capacity model, not enough training data)

vanishing gradient problem

    

       

sigmoid

        
                

=    
    

        
    
                

    
                
    
            

    
            
                

    
            
                

    
                
        
            
    

=    
    

            

    
                
    
            

        
    
                

gradients in the lower layers are typically extremely small
optimizing multi-layer neural networks takes huge amount of time

slide credit: adapted from bohyung han
3 3

new    winter    and revival in early 2000   s

new    winter    in the early 2000   s due to

problems with training nns

   
    support vector machines (id166s), id79s 

(rf)     easy to train, nice theory

revival again by 2011-2012
    name change (   neural networks    ->    deep learning   )

   

   

   

+ algorithmic developments
    unsupervised layer-wise pre-training
    relu, dropout, layer normalizatoin
+ big data + gpu computing = 
large outperformance on many datasets (vision: 
ilsvrc   12)

http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/

3 4

big data

id163 large scale visual recognition challenge

    1000 categories w/ 1000 images per category

    1.2 million training images, 50,000 validation, 150,000 testing

o. russakovsky, j. deng, h. su, j. krause, s. satheesh, s. ma, z. huang, a. karpathy, a. khosla,
m. bernstein, a. c. berg and l. fei-fei. id163 large scale visual recognition challenge. ijcv, 2015.

3 5

alexnet architecture

60 million parameters!
various tricks

figure credit: krizhevsky et al, nips 2012.

   

   

   

   

   

   

relu nonlinearity
overlapping pooling
local response id172
dropout     set hidden neuron output to 0 with id203 .5
data augmentation
training on gpus

alex krizhevsky, ilya sutskeyer, geoffrey e. hinton. id163 classification with deep convolutional neural networks. nips, 2012.

3 6

gpu computing

    big data and big models require lots of 

computational power

    gpus

    thousands of cores for parallel operations

    multiple gpus

    still took about 5-6 days to train alexnet on two 

nvidia gtx 580 3gb gpus (much faster 
today)

3 7

image classification performance

figure from: k. he, x. zhang, s. ren, j. sun.     deep residual learning for image  recognition   . arxiv 2015. (slides)

3 8

slide credit: bohyung han

image classification top-5 errors (%)

questions?

references (& great tutorials):

http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-1/

http://cs231n.github.io/neural-networks-1/

http://colah.github.io/posts/2015-08-understanding-lstms/

3 9

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

