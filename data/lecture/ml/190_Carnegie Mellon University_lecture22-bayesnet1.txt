10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

bayesian   networks

(part   i)

graphical   model   readings:
murphy   10       10.2.1
bishop   8.1,   8.2.2
htf   -     -     
mitchell   6.11

matt   gorid113y
lecture   22
april   10,   2017

1

reminders

    peer   tutoring
    homework 7:   deep   learning

    release:   wed,   apr.   05   
    part i   due wed,   apr.   12
    part ii   due mon,   apr.   17

start   early

2

convolutional   neural   nets

3

deep   learning   outline

    background:   computer   vision

    image   classification
    ilsvrc   2010   -      2016
    traditional   feature   extraction   methods
    convolution   as   feature   extraction
convolutional   neural   networks   (id98s)
    learning   feature   abstractions
    common   id98   layers:
    convolutional   layer
    max-     pooling   layer
   
    softmax layer
    relu layer

fully-     connected   layer   (w/tensor   input)

    background:   subgradient
    architecture:   lenet
    architecture:   alexnet
training   a   id98
    sgd   for   id98s
    id26   for   id98s

   

   

4

convolutional   neural   network   (id98)
   

typical   layers   include:
    convolutional   layer
    max-     pooling   layer
    fully-     connected   (linear)   layer
    relu layer   (or   some   other   nonlinear   activation   function)
    softmax
these   can   be   arranged   into   arbitrarily   deep   topologies

   

architecture   #1:   lenet-     5

5

convolutional   layer
id98   key   idea:   

treat   convolution   matrix   as   
parameters   and   learn   them!

input image

0
1
0
0
1
0
0

0
1
0
1
0
0
0

0
1
1
0
0
0
0

0
1
0
0
0
0
0

0
0
0
0
0
0
0

0
0
0
0
0
0
0

0
1
1
1
1
1
0

learned

convolution
  11   12   13
  21   22   23
  31   32   33

convolved   image

.4 .5 .5 .5 .4
.4 .2
.3 .6 .3
.1
.5 .4 .4 .2
0
.1
.5 .6 .2
.4 .3
.1
0
0

6

downsampling by   averaging
    downsampling by   averaging   used   to   be a   common   approach
    this   is   a   special   case   of   convolution   where   the   weights   are   fixed   to   a   

uniform   distribution

    the   example   below   uses   a   stride   of   2

input image

1
0
0
1
0
0

1
0
1
0
0
0

1
1
0
0
0
0

1
0
0
0
0
0

0
0
0
0
0
0

1
1
1
1
1
0

convolution

1/4 1/4
1/4 1/4

convolved   image

3/4 3/4 1/4
3/4 1/4 0
1/4 0
0

7

max-     pooling

    max-     pooling   is   another   (common)   form   of   downsampling
   

instead   of   averaging,   we   take   the   max   value   within   the   same   range   as   
the   equivalently-     sized   convolution

    the   example   below   uses   a   stride   of   2

input image

1
0
0
1
0
0

1
0
1
0
0
0

1
1
0
0
0
0

1
0
0
0
0
0

0
0
0
0
0
0

1
1
1
1
1
0

max-     
pooling

xi,j

xi,j+1

xi+1,j xi+1,j+1

max-     pooled   

image

1
1
1

1
1
0

1
0
0

8

multi-     class   output

output

   

hidden   layer

   

input

   

10

multi-     class   output

softmax layer:

(cid:50)(cid:116)(cid:84)(bk)
l=1 (cid:50)(cid:116)(cid:84)(bl)

yk =

output

 k

   

hidden   layer

   

input

   

k=1 y k (cid:72)(cid:81)(cid:59)(yk)

(f) loss

j = k
 k
bk = d

(e) output (softmax)
yk = (cid:50)(cid:116)(cid:84)(bk)

l=1 (cid:50)(cid:116)(cid:84)(bl)

(d) output (linear)

j=0  kjzj  k

(c) hidden (nonlinear)
zj =  (aj),  j

(b) hidden (linear)

i=0  jixi,  j

aj = m
(a) input
given xi,  i

11

training   a   id98

whiteboard

    sgd   for   id98s
    id26   for   id98s

12

common   id98   layers

whiteboard

    relu layer
    background:   subgradient
    fully-     connected   layer   (w/tensor   input)
    softmax layer
    convolutional   layer
    max-     pooling   layer

13

convolutional   layer

14

convolutional   layer

15

max-     pooling   layer

16

max-     pooling   layer

17

convolutional   neural   network   (id98)
   

typical   layers   include:
    convolutional   layer
    max-     pooling   layer
    fully-     connected   (linear)   layer
    relu layer   (or   some   other   nonlinear   activation   function)
    softmax
these   can   be   arranged   into   arbitrarily   deep   topologies

   

architecture   #1:   lenet-     5

18

architecture   #2:   alexnet

id98   for   image   classification
(krizhevsky,   sutskever &   hinton,   2012)
15.3%   error   on   id163 lsvrc-     2012   contest

input   
image   
(pixels)

   

five   convolutional   layers   
(w/max-     pooling)

    three   fully   connected   layers

1000-     way   
softmax

19
figure 2: an illustration of the architecture of our id98, explicitly showing the delineation of responsibilities
between the two gpus. one gpu runs the layer-parts at the top of the    gure while the other runs the layer-parts

id98s   for   image   recognition

(slide from kaiming he   s recent presentation)

slide   from   kaiming he

fei-fei li & andrej karpathy & justin johnson
fei-fei li & andrej karpathy & justin johnson

lecture 7 -
lecture 7 -

78

27 jan 2016
27 jan 2016

20

mini-     batch   sgd

    gradient   descent:   

compute   true   gradient   exactly   from   all   n   
examples

    mini-     batch   sgd:   

approximate   true   gradient   by   the   average   
gradient   of   k   randomly   chosen   examples

    stochastic   gradient   descent   (sgd):

approximate   true   gradient   by   the   gradient   
of   one   randomly   chosen   example

21

mini-     batch   sgd

three   variants   of   first-     order   optimization:

22

id98   visualizations

23

3d   visualization   of   id98
http://scs.ryerson.ca/~aharley/vis/conv/

convolution   of   a   color   image
    color   images   consist   of   3   floats   per   pixel   for   

rgb   (red,   green   blue)   color   values

a closer look at spatial dimensions:

    convolution   must   also   be   3-     dimensional

32x32x3 image
5x5x3 filter

32

activation map

28

convolve (slide) over all 
spatial locations

32

3

28

1

figure   from   fei-     fei li   &   andrej   karpathy &   justin   johnson   (cs231n)   
fei-fei li & andrej karpathy & justin johnson
fei-fei li & andrej karpathy & justin johnson

lecture 7 -
lecture 7 -

23

25
27 jan 2016
27 jan 2016

animation   of   3d   convolution

http://cs231n.github.io/convolutional-     networks/

figure   from   fei-     fei li   &   andrej   karpathy &   justin   johnson   (cs231n)   

26

mnist   digit   recognition   with   id98s   

(in   your   browser)

https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html

figure   from   andrej   karpathy

27

id98   summary

id98s

    are   used   for   all   aspects   of   computer   vision,   and   

have   won   numerous   pattern   recognition   
competitions

    able   learn   interpretable   features   at   different   levels   

of   abstraction

    typically,   consist   of   convolution layers,   pooling
layers,   nonlinearities,   and   fully   connected   layers

other   resources:

    readings   on   course   website
    andrej   karpathy,   cs231n   notes

http://cs231n.github.io/convolutional-     networks/

28

bayesian   networks

29

bayes   nets   outline

    motivation

   

   

   

   

   

structured   prediction

familiar   models   as   bayes   nets

   
background
   
conditional   independence
   
chain   rule   of   id203
directed   graphical   models
    writing   joint   distributions
    definition:   bayesian   network
    qualitative   specification
    quantitative   specification
   
conditional   independence   in   bayes   nets
   
    d-     separation
    markov   blanket
learning
   
   
id136
   
    gibbs   sampling

fully   observed   bayes   net
(partially   observed   bayes   net)

three   case   studies

sampling   directly   from   the   joint   distribution

31

motivation:   structured   
prediction

32

structured   prediction

    most   of   the   models   we   ve   seen   so   far   were   

for   classification
    given   observations:
    predict   a   (binary)   label:

x = (x1, x2,    , xk) 
y

    many   real-     world   problems   require   

structured   prediction
    given   observations:   
    predict   a   structure:

x = (x1, x2,    , xk) 
y = (y1, y2,    , yj) 

    some   classification problems   benefit   from   

latent structure

33

structured   prediction   examples

    examples   of   structured   prediction

    part-     of-     speech   (pos)   tagging
    handwriting   recognition
    speech   recognition
    word   alignment
    congressional   voting

    examples   of   latent   structure

    object   recognition

34

dataset   for   supervised   

part-     of-     speech   (pos)   tagging
d = {x(n), y(n)}n

n=1

data:

sample   1:

sample   2:

sample   3:

sample   4:

n

time

n

time

n

flies

p

with

v

flies

n

flies

v

fly

n

time

p

like

v

like

p

with

n

you

d

an

d

an

n

n

arrow

n

arrow

n

their

wings

v

will

v

see

y(1)
x(1)

y(2)
x(2)

y(3)
x(3)

y(4)
x(4)

35

dataset   for   supervised   
handwriting   recognition

data:

d = {x(n), y(n)}n

n=1

sample   1:

u

n

e

x

p

e

c

t

e

d

sample   2:

v

o

l

c

a

n

i

c

sample   2:

e

m

b

r

a

c

e

s

total of 1,637,267 words. it is comprised of 49,115 unique
words, and each word in the corpus is labeled according to
its part of speech; there are a total of 43 different part-of-
speech labels. we use four types of features:

total of 1,637,267 words. it is comprised of 49,115 unique
total of 1,637,267 words. it is comprised of 49,115 unique
words, and each word in the corpus is labeled according to
words, and each word in the corpus is labeled according to
its part of speech; there are a total of 43 different part-of-
its part of speech; there are a total of 43 different part-of-
speech labels. we use four types of features:
speech labels. we use four types of features:

1.
1.
2.
2.

1.
first-order word-presence features.
first-order word-presence features.
first-order word-presence features.
2.
four-character prefix presence features.
four-character prefix presence features.
four-character prefix presence features.

handwriting recognition: error rates obtained
handwriting recognition: error rates obtained

y(1)

x(1)

y(2)

x(2)

y(3)

x(3)

fig. 5. handwriting recognition: example words from the dataset used.

fig. 5. handwriting recognition: example words from the dataset used.
fig. 5. handwriting recognition: example words from the dataset used.

figures   from   (chatzis &   demiris,   2013)

36

dataset   for   supervised   

phoneme   (speech)   recognition
d = {x(n), y(n)}n

n=1

data:

sample   1:

1704

h#

dh

ih

s

w

uh

z

iy

z

iy

y(1)

ieee transactions on signal processing, vol. 61, no. 7, april 1, 2013

sample   2:

ieee transactions on signal processing, vol. 61, no. 7, april 1, 2013

ah

s

s

h#

f

ao

r

x(1)

y(2)

fig. 5. extrinsic (top) and intrinsic (bottom) spectral representations for the utterance    this was easy for us.    note that a nonlinear mel-scale frequency warping
was used.

x(2)

are the input unlabeled data and

where
is the
new parametrization of the function we need to estimate. to
proceed, we plug the functional form of (9) into the optimization
problem of (8). taking the gradient with respect to the parameter
vector
and setting it to zero sets up the following generalized
eigenvalue problem:

figures   from   (jansen   &   niyogi,   2013)

(10)

novel utterance into this intrinsic representation requires only
the computation of equation (9) across the utterance.

fig. 5 shows an example extrinsic

spectrograms for the timit utterance    this was easy
for us    (timit sentence sx3). here, we constructed the dataset
with 200 examples of each of the 48 phonetic categories spec-
i   ed in [26].2 each example was extrinsically represented by
a 40-dimensional, homomorphically smoothed, auditory (log)
spectrum (40 mel scale bands, from 0   8 khz) computed from

37

application:

word   alignment   /   phrase   extraction
    variables   (boolean):

    for   each   (chinese   phrase,   

english   phrase)   pair,   
are   they   linked?

    interactions:

    word   fertilities
    few      jumps      (discontinuities)
    syntactic   reorderings
       itg   contraint      on   alignment
    phrases   are   disjoint   (?)

(burkett   &   klein,   2012)

38

application:

congressional   voting

    variables:

    interactions:

    text   of   all   speeches   of   a   

representative   

    local   contexts   of   

references   between   two   
representatives

    words   used   by   

representative   and   their   
vote

    pairs   of   representatives   
and   their   local   context

(stoyanov &   eisner,   2012)

39

structured   prediction   examples

    examples   of   structured   prediction

    part-     of-     speech   (pos)   tagging
    handwriting   recognition
    speech   recognition
    word   alignment
    congressional   voting

    examples   of   latent   structure

    object   recognition

40

case   study:   object   recognition

data   consists   of   images   x and   labels   y.

x(1)

y(1)

x(3)

y(3)

rhinoceros

llama

x(2)

y(2)

x(4)

y(4)

41

pigeon

leopard

case   study:   object   recognition

data   consists   of   images   x and   labels   y.
    preprocess   data   into   

   patches   

    posit   a   latent   labeling   z
describing   the   object   s   
parts   (e.g.   head,   leg,   
tail,   torso,   grass)

    define   graphical   
model   with   these   
latent   variables   in   
mind
z is   not   observed   at   
train   or   test   time

   

leopard

42

case   study:   object   recognition

data   consists   of   images   x and   labels   y.
    preprocess   data   into   

   patches   

    posit   a   latent   labeling   z
describing   the   object   s   
parts   (e.g.   head,   leg,   
tail,   torso,   grass)

    define   graphical   
model   with   these   
latent   variables   in   
mind
z is   not   observed   at   
train   or   test   time

   

z7

x7

z2

x2

leopard

y

43

case   study:   object   recognition

data   consists   of   images   x and   labels   y.
    preprocess   data   into   

   patches   

    posit   a   latent   labeling   z
describing   the   object   s   
parts   (e.g.   head,   leg,   
tail,   torso,   grass)

    define   graphical   
model   with   these   
latent   variables   in   
mind
z is   not   observed   at   
train   or   test   time

   

z7

  1

x7

  4

  2

  4

  4

z2

  3

x2

leopard

y

44

structured   prediction

preview   of   challenges   to   come   
    consider   the   task   of   finding   the   most   probable   

assignment to   the   output   

  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

classification
p(y|(cid:116))

y

where y   {+1, 1}

(cid:118)

structured   prediction
  (cid:118) = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
p((cid:118)|(cid:116))
where (cid:118)   y
and |y| is very large

45

machine   learning

the data   inspires   
the   structures   
we   want   to   

predict

id136 finds   
{best   structure,   marginals,   
partition   function} for   a   
new   observation

(id136 is   usually   
called   as   a   subroutine   

in   learning)

domain   
knowledge

mathematical   

modeling

ml

combinatorial   
optimization

optimization

our   model

defines   a   score   
for   each   structure

it   also   tells   us   
what   to   optimize

learning tunes   the   
parameters   of   the   

model

46

3 a l i c e s a w b o b o n a h i l l w i t h a t e l e s c o p e
machine   learning

t e l e s c o p e

a l i c e

w i t h

b o b

s a w

h i l l

o n

a

a

t i m e     i e s

4

l i k e a n a r r o w
data

l i k e

a n

a r r o w

t i m e

    i e s

l i k e

a n

a r r o w

t i m e

    i e s

l i k e

a n

a r r o w

t i m e

    i e s

id136

l i k e

a n

    i e s

a r r o w

t i m e

model

x1

x2

x4

x3

x5

objective

learning

2

(id136 is   usually   
called   as   a   subroutine   

in   learning)

47

background

48

background

whiteboard

    chain   rule   of   id203
    conditional   independence

49

background:   chain   rule

of   id203

for random variables a and b:

p (a, b) = p (a|b)p (b)

for random variables x1, x2, x3, x4:

p (x1, x2, x3, x4) =p (x1|x2, x3, x4)

p (x2|x3, x4)
p (x3|x4)
p (x4)

50

background:

conditional   independence

random variables a and b are conditionally
independent given c if:

p (a, b|c) = p (a|c)p (b|c)

or equivalently:

p (a|b, c) = p (a|c)

we write this as:

(1)

(2)

a|(cid:52)b|c

later   we   will   also   
write:   i<a, {c}, b>

(3)

51

bayesian   networks
directed   graphical   models

52

example:   tornado   alarms

1.

imagine   that   
you   work   at   the   
911   call   center   
in   dallas

2. you   receive   six   
calls   informing   
you   that   the   
emergency   
weather   sirens   
are   going   off
3. what   do   you   

conclude?

figure   from   https://www.nytimes.com/2017/04/08/us/dallas-     emergency-     sirens-     hacking.html

53

example:   tornado   alarms

1.

imagine   that   
you   work   at   the   
911   call   center   
in   dallas

2. you   receive   six   
calls   informing   
you   that   the   
emergency   
weather   sirens   
are   going   off
3. what   do   you   

conclude?

figure   from   https://www.nytimes.com/2017/04/08/us/dallas-     emergency-     sirens-     hacking.html

54

directed   graphical   models   

(bayes   nets)

whiteboard

    example:   tornado   alarms
    writing   joint   distributions

    idea   #1:   giant   table
    idea   #2:   rewrite   using   chain   rule
    idea   #3:   assume   full   independence
    idea   #4:   drop   variables   from   rhs   of   conditionals

    definition:   bayesian   network
    observed   variables   in   graphical   models

55

bayesian   network

x2

x1

x4

x3

x5

p(x1, x2, x3, x4, x5) =
p(x5|x3)p(x4|x2, x3)
p(x3)p(x2|x1)p(x1)

56

bayesian   network

definition:

p(x1   xn) =

n
   
i=1

x2

x1

x4

x3

x5

p(xi | parents(xi))

    a   bayesian   network   is   a   directed   graphical   model
   
    these   two   parts   full   specify   the   distribution:

it   consists   of   a   graph   g and   the   conditional   probabilities   p

    qualitative   specification:   g
    quantitative   specification:   p

57

