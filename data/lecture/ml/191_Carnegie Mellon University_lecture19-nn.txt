10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

neural   networks

and

id26

neural   net   readings:
murphy   -     -     
bishop   5
htf   11
mitchell   4

matt   gorid113y
lecture   19
march   29,   2017

1

reminders

    homework 6:   unsupervised learning

    release:   wed,   mar.   22
    due:   mon,   apr.   03   at   11:59pm

    homework 5 (part ii):   peer   review

    release:   wed,   mar.   29
    due:   wed,   apr.   05   at   11:59pm

expectation:   you   

should   spend   at   most   1   
hour   on   your   reviews

    peer   tutoring

2

neural   networks   outline

    logistic   regression   (recap)

    data,   model,   learning,   prediction

    neural   networks

    a   recipe   for   machine   learning
    visual   notation   for   neural   networks
    example:   logistic   regression   output   surface
    2-     layer   neural   network
    3-     layer   neural   network
    neural   net   architectures

    objective   functions
    activation   functions

    id26

    basic   chain   rule   (of   calculus)
    chain   rule   for   arbitrary   computation   graph
    id26   algorithm
    module-     based   automatic   differentiation   (autodiff)

3

recall:   logistic   regression

4

using   gradient   ascent   for   linear   

classifiers

key   idea   behind   today   s   lecture:

1. define   a   linear   classifier   (logistic   regression)
2. define   an   objective   function   (likelihood)
3. optimize   it   with   gradient   descent   to   learn   

parameters

4. predict   the   class   with   highest   id203   under   

the   model

5

using   gradient   ascent   for   linear   

classifiers

this   decision   function   isn   t   
differentiable:

use   a   differentiable   
function   instead:

h((cid:116)) = sign( t (cid:116))

p (y = 1|(cid:116)) =

1

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))

sign(x)

logistic(u)    

1
1+e   u

6

using   gradient   ascent   for   linear   

classifiers

this   decision   function   isn   t   
differentiable:

use   a   differentiable   
function   instead:

h((cid:116)) = sign( t (cid:116))

p (y = 1|(cid:116)) =

1

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))

sign(x)

logistic(u)    

1
1+e   u

7

logistic   regression

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.
i=1 where (cid:116)   rk and y   {0, 1}

d = {(cid:116)(i), y(i)}n

model:   logistic   function   applied   to   dot   product   of   
parameters   with   input   vector.
p (y = 1|(cid:116)) =

1 + (cid:50)(cid:116)(cid:84)(  t (cid:116))
learning:   finds   the   parameters   that   minimize   some   
objective   function.    = argmin

j( )

1

 

prediction:   output   is   the   most   probable   class.

  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
y {0,1}

p (y|(cid:116))

8

neural   networks

9

background

a   recipe   for   

machine   learning

1.   given   training   data:

face

face

not   a   face

2.   choose   each   of   these:

    decision   function

    loss   function

examples:   linear   regression,   
logistic   regression,   neural   network

examples:   mean-     squared   error,   
cross   id178

10

background

a   recipe   for   

machine   learning

1.   given   training   data:

3.   define   goal:

2.   choose   each   of   these:

    decision   function

    loss   function

4.   train   with   sgd:
(take   small   steps   
opposite   the   gradient)

11

background

a   recipe   for   

gradients

machine   learning

1.   given   training   data:

3.   define   goal:

id26 can   compute   this   
gradient!   
and   it   s   a   special   case   of   a   more   
general   algorithm   called   reverse-     
mode   automatic   differentiation   that   
4.   train   with   sgd:
can   compute   the   gradient   of   any   
differentiable   function   efficiently!
(take   small   steps   
opposite   the   gradient)

2.   choose   each   of   these:

    decision   function

    loss   function

12

background
goals   for   today   s   lecture

machine   learning

a   recipe   for   

1.   given   training   data:

1. explore   a   new   class   of   decision   functions   

3.   define   goal:

(neural   networks)

2. consider   variants   of   this   recipe   for   training

2.   choose   each   of   these:

    decision   function

    loss   function

4.   train   with   sgd:
(take   small   steps   
opposite   the   gradient)

13

decision   
functions

output

linear   regression

y = h (x) =  ( t x)
where  (a) = a

  1

  2

  3

input

  m

   

14

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

  1

  2

  3

input

  m

   

15

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

not   a   face

  1

  2

  3

input

face

face

  m

   

16

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1
in-     class   example

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

0

  1

  2

  3

input

1

y

1

  m

x1

   

x2

17

decision   
functions

output

id88

y = h (x) =  ( t x)
1

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

  1

  2

  3

input

  m

   

18

from   biological to   artificial

the   motivation   for   artificial   neural   networks   comes   from   biology   

biological      model   
    neuron:   an   excitable   cell
   

synapse:   connection   between   
neurons

    a   neuron sends   an   

electrochemical   pulse   along   its   
synapses when   a   sufficient   voltage   
change   occurs

    biological   neural   network:   
collection   of   neurons   along   some   
pathway   through   the   brain

biological      computation   
    neuron   switching   time   :    ~   0.001   sec
    number   of   neurons:   
   
   

~   1010
connections   per   neuron:    ~   104-     5
scene   recognition   time:    ~   0.1   sec

artificial   model
    neuron:   node   in   a   directed   acyclic   

graph   (dag)

    weight:   multiplier   on   each   edge
    activation   function:   nonlinear   

thresholding   function,   which   allows   a   
neuron   to      fire      when   the   input   value   
is   sufficiently   high   

    artificial   neural   network:   collection   
of   neurons   into   a   dag,   which   define   
some   differentiable   function

units

artificial   computation
    many   neuron-     like   threshold   switching   

    many   weighted   interconnections   

among   units

    highly   parallel,   distributed   processes   

slide   adapted   from   eric   xing

19

decision   
functions

output

logistic   regression

y = h (x) =  ( t x)
1

where  (a) =

1 + (cid:50)(cid:116)(cid:84)( a)

  1

  2

  3

input

  m

   

20

neural   networks

whiteboard

    example:   neural   network   w/1   hidden   layer
    example:   neural   network   w/2   hidden   layers
    example:   feed   forward   neural   network

21

neural   network   model

inputs

age

34

gender

stage

2

4

.1
.3

.6
.2

.7

.2

.4

.2

.5

.8

s   

s   

s   

independent 
variables

weights

hidden 
layer

weights

output

0.6

   id203 of 
beingalive   

dependent 
variable
prediction

     eric   xing   @   cmu,   2006-     2011

22

   combined   logistic   models   

inputs

age

34

gender

stage

2

4

.1

.6

.7

.5

.8

s   

independent 
variables

weights

hidden 
layer

weights

output

0.6

   id203 of 
beingalive   

dependent 
variable
prediction

     eric   xing   @   cmu,   2006-     2011

23

inputs

age

34

gender

stage

2

4

.3

.2

.2

.5

.8

s   

independent 
variables

weights

hidden 
layer

weights

output

0.6

   id203 of 
beingalive   

dependent 
variable
prediction

     eric   xing   @   cmu,   2006-     2011

24

inputs

age

34

gender

stage

1

4

.1
.3

.6
.2

.7

.2

.5

.8

s   

independent 
variables

weights

hidden 
layer

weights

output

0.6

   id203 of 
beingalive   

dependent 
variable
prediction

     eric   xing   @   cmu,   2006-     2011

25

not   really,   

no   target   for   hidden   units...

age

34

gender

stage

2

4

.1
.3

.6
.2

.7

.2

.4

.2

.5

.8

s   

s   

s   

independent 
variables

weights

hidden 
layer

weights

0.6

   id203 of 
beingalive   

dependent 
variable
prediction

     eric   xing   @   cmu,   2006-     2011

26

neural   network

decision   
functions

output

hidden   layer

   

input

   

28

decision   
functions

neural   network

(f) loss
j = 1

2 (y   y(d))2

output

hidden   layer

   

(e) output (sigmoid)
1+(cid:50)(cid:116)(cid:84)( b)

y =

1

(d) output (linear)

j=0  jzj

b = d

input

   

zj =

1

1+(cid:50)(cid:116)(cid:84)( aj ) ,  j

(c) hidden (sigmoid)

(b) hidden (linear)

i=0  jixi,  j

aj = m
(a) input
given xi,  i

29

neural   networks:   
representational   power

30

building   a   neural   net

q:   how   many   hidden   units,   d,   should   we   use?

output

features

   

31

building   a   neural   net

q:   how   many   hidden   units,   d,   should   we   use?

output

hidden   layer

   

1

1

1

input

   

d   =   m

32

building   a   neural   net

q:   how   many   hidden   units,   d,   should   we   use?

output

hidden   layer

input

   

   

d   =   m

33

building   a   neural   net

q:   how   many   hidden   units,   d,   should   we   use?

output

hidden   layer

input

   

   

d   =   m

34

building   a   neural   net

q:   how   many   hidden   units,   d,   should   we   use?

output

hidden   layer

   

input

   

what   method(s)   is   
this   setting   similar   to?

d   <   m

35

building   a   neural   net

q:   how   many   hidden   units,   d,   should   we   use?

output

hidden   layer

   

input

   

d   >   m

what   method(s)   is   
this   setting   similar   to?

36

decision   
functions

deeper   networks

q:   how   many   layers   should   we   use?

output

hidden   layer   1

   

input

   

37

decision   
functions

deeper   networks

q:   how   many   layers   should   we   use?

output

hidden   layer   2

hidden   layer   1

   

   

input

   

38

decision   
functions

deeper   networks

q:   how   many   layers   should   we   use?

output

hidden   layer   3

hidden   layer   2

hidden   layer   1

   

   

   

input

   

39

decision   
functions

deeper   networks

q:   how   many   layers   should   we   use?
    theoretical   answer:

    a   neural   network   with   1   hidden   layer   is   a   universal   function   

approximator

    cybenko (1989):   for   any   continuous   function   g(x),   there   

exists   a   1-     hidden-     layer   neural   net   h  (x)   
s.t. |   h  (x)       g(x)   |   <      for   all   x,   assuming   sigmoid   activation   
functions

output

    empirical   answer:

    before   2006:      deep   networks   (e.g.   3   or   more   hidden   layers)   

   

are   too   hard   to   train   

hidden   layer   1

    after   2006:      deep   networks   are   easier   to   train   than   shallow   

networks   (e.g.   2   or   fewer   layers)   for   many   problems   

big   caveat:   you   need   to   know   and   use   the   right   tricks.

input

   

40

decision   boundary

    0   hidden   layers:   linear   classifier

    hyperplanes

y

1x

2x

example from to eric postma via jason eisner

41

decision   boundary

    1   hidden   layer

    boundary   of   convex   region   (open   or   closed)   
y

1x

2x

example from to eric postma via jason eisner

42

decision   boundary

y

    2   hidden   layers

    combinations   of   convex   regions   

1x

2x

example from to eric postma via jason eisner

43

decision   
functions

different   levels   of   

abstraction

    we   don   t   know   

the      right      
levels   of   
abstraction

    so   let   the   model   

figure   it   out!

example   from   honglak lee   (nips   2010)

44

decision   
functions

different   levels   of   

abstraction

face   recognition:
    deep   network   

can   build   up   
increasingly   
higher   levels   of   
abstraction
    lines,   parts,   

regions

example   from   honglak lee   (nips   2010)

45

decision   
functions

different   levels   of   

abstraction

output

hidden   layer   3

hidden   layer   2

hidden   layer   1

   

   

   

input

   

example   from   honglak lee   (nips   2010)

46

