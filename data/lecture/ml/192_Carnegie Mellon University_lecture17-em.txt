10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

em
+

pca

em,   gmm   readings:
murphy   11.4.1,   11.4.2,   11.4.4
bishop   9
htf   8.5   -      8.5.3
mitchell   6.12   -      6.12.2

pca   readings:
murphy   12
bishop   12
htf   14.5
mitchell   -     -     

matt   gorid113y
lecture   17
march   22,   2017

1

reminders

    homework 5:   readings /   application of   ml

    release:   wed,   mar.   08
    due:   wed,   mar.   22   at   11:59pm

2

em   and   gmms

3

expectation-     maximization   outline

    background

    multivariate   gaussian   distribution
    marginal   probabilities
    building   up   to   gmms

marginal   data   likelihood

    distinction   #1:   model   vs.   objective   function
    gaussian   na  ve   bayes   (gnb)
    gaussian   discriminant   analysis
    gaussian   mixture   model   (gmm)
expectation-     maximization
    distinction   #2:   complete   data   likelihood   vs.   
    distinction   #3:   latent   variables   vs.   parameters
    objective   functions   for   em
    em   algorithm
    em   for   gmm   vs.   k-     means
properties   of   em
    nonconvexity /   local   optimization
    example:   grammar   induction
    variants   of   em

   

   

last   lecture

this   lecture

4

background

whiteboard

    multivariate   gaussian   distribution
    marginal   probabilities

6

gaussian   mixture   model   
(gmm)

7

building   up   to   gmms

whiteboard

    distinction   #1:   model   vs.   objective   function
    gaussian   na  ve   bayes   (gnb)
    gaussian   discriminant   analysis
    gaussian   mixture   model   (gmm)

8

gaussian   discriminant   

data:   

d = {((cid:116)(i), (cid:120)(i))}n

generative   story:

model:

joint:

analysis
i=1 where (cid:116)(i)   rm and z(i)   {1, . . . , k}
z   categorical( )
(cid:116)   gaussian(  z,  z)
p((cid:116), z;  ,   ,  ) = p((cid:116)|z;   ,  )p(z;  )

log-     likelihood:

 ( ,   ,  ) = (cid:72)(cid:81)(cid:59)

=

n i=1

p((cid:116)(i), z(i);  ,   ,  )

n i=1
(cid:72)(cid:81)(cid:59) p((cid:116)(i)|z(i);   ,  ) + (cid:72)(cid:81)(cid:59) p(z(i);  )

9

gaussian   discriminant   

data:   

d = {((cid:116)(i), (cid:120)(i))}n

analysis
i=1 where (cid:116)(i)   rm and z(i)   {1, . . . , k}

log-     likelihood:

 ( ,   ,  ) =

n i=1

(cid:72)(cid:81)(cid:59) p((cid:116)(i)|z(i);   ,  ) + (cid:72)(cid:81)(cid:59) p(z(i);  )

maximum   likelihood   estimates:
take   the   derivative   of   the   lagrangian,   set   it   equal   to   zero   
and   solve.

implementation:   

just   counting   

1
n

 k =

n i=1
i(z(i) = k), k
  k =  n
i=1 i(z(i) = k)(cid:116)(i)
 n
i=1 i(z(i) = k)
 k =  n
i=1 i(z(i) = k)((cid:116)(i)     k)((cid:116)(i)     k)t

i=1 i(z(i) = k)

, k

 n

, k

10

assume we are given data, d, consisting of n fully unsupervised ex-
amples in m dimensions:

gaussian   mixture-     model

data:   

d = {(cid:116)(i)}n

generative   story:

model:

joint:

marginal:

i=1 where (cid:116)(i)   rm
z   categorical( )
(cid:116)   gaussian(  z,  z)
p((cid:116), z;  ,   ,  ) = p((cid:116)|z;   ,  )p(z;  )
p((cid:116);  ,   ,  ) =
p((cid:116)|z;   ,  )p(z;  )

k z=1

(marginal)   log-     likelihood:

 ( ,   ,  ) = (cid:72)(cid:81)(cid:59)

=

n i=1

n i=1

(cid:72)(cid:81)(cid:59)

p((cid:116)(i);  ,   ,  )

k z=1

p((cid:116)(i)|z;   ,  )p(z;  )

11

assume we are given data, d, consisting of n fully unsupervised ex-
amples in m dimensions:

mixture-     model

data:   

d = {(cid:116)(i)}n

generative   story:

i=1 where (cid:116)(i)   rm
z   categorical( )
z   multinomial( )
(cid:116)   p (  |z)
(cid:116)   gaussian(  z,  z)

model:

joint:

marginal:

p , ((cid:116), z) = p ((cid:116)|z)p (z)
p ((cid:116)|z)p (z)
p , ((cid:116)) =

k z=1

(marginal)   log-     likelihood:

 ( ) = (cid:72)(cid:81)(cid:59)

=

n i=1

n i=1

(cid:72)(cid:81)(cid:59)

p , ((cid:116)(i))

k z=1

p ((cid:116)(i)|z)p (z)

12

assume we are given data, d, consisting of n fully unsupervised ex-
amples in m dimensions:

mixture-     model

data:   

d = {(cid:116)(i)}n

generative   story:

model:

joint:

marginal:

(marginal)   log-     likelihood:

i=1 where (cid:116)(i)   rm
z   categorical( )
z   multinomial( )
(cid:116)   p (  |z)
(cid:116)   gaussian(  z,  z)this   could   be   any   
arbitrary   distribution   
parameterized   by     .

p , ((cid:116), z) = p ((cid:116)|z)p (z)
today   we   re   thinking   
p ((cid:116)|z)p (z)
p , ((cid:116)) =
about   the   case   where   it   
is   a   multivariate   
gaussian.

k z=1

 ( ) = (cid:72)(cid:81)(cid:59)

=

n i=1

n i=1

(cid:72)(cid:81)(cid:59)

p , ((cid:116)(i))

k z=1

p ((cid:116)(i)|z)p (z)

13

learning   a   mixture   model

supervised   learning:   the   
parameters   decouple!
d = {((cid:116)(i), (cid:120)(i))}n

i=1

z

unsupervised   learning:   parameters   
are   coupled   by   marginalization.

d = {(cid:116)(i)}n

i=1

z

x1

x2

   

xm

x1

x2

   

xm

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 , 

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

n i=1
n i=1
n i=1

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))p (z(i))

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 , 

n i=1

(cid:72)(cid:81)(cid:59)

k z=1

p ((cid:116)(i)|z)p (z)

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))

(cid:72)(cid:81)(cid:59) p (z(i))

14

learning   a   mixture   model

unsupervised   learning:   parameters   
are   coupled   by   marginalization.

d = {(cid:116)(i)}n

i=1

z

x1

x2

   

xm

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 , 

n i=1

(cid:72)(cid:81)(cid:59)

k z=1

p ((cid:116)(i)|z)p (z)

supervised   learning:   the   
parameters   decouple!
d = {((cid:116)(i), (cid:120)(i))}n
training   certainly   isn   t   as   simple   
as   the   supervised   case.

i=1

z

x2

in   many   cases,   we   could   still   use   
some   black-     box   optimization   
x1
method   (e.g.   newton-     raphson)   
to   solve   this   coupled
optimization   problem.

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))p (z(i))

xm

   

 , 

  ,    = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

this   lecture   is   about   a   more   
problem-     specific   method:   em.

(cid:72)(cid:81)(cid:59) p ((cid:116)(i)|z(i))

 

n i=1
n i=1
n i=1

   = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

(cid:72)(cid:81)(cid:59) p (z(i))

15

expectation   maximization

16

expectation-     maximization

whiteboard

    distinction   #2:   complete   data   likelihood   vs.   

marginal   data   likelihood

    distinction   #3:   latent   variables   vs.   parameters
    objective   functions   for   em
    em   algorithm
    em   for   gmm   vs.   k-     means

17

example:   k-     means   vs   gmm

24

example:   k-     means

25

example:   k-     means

26

example:   k-     means

27

example:   k-     means

28

example:   k-     means

29

example:   k-     means

30

example:   k-     means

31

example:   k-     means

32

example:   gmm

33

example:   gmm

34

example:   gmm

35

example:   gmm

36

example:   gmm

37

example:   gmm

38

example:   gmm

39

example:   gmm

40

example:   gmm

41

example:   gmm

42

example:   gmm

43

example:   gmm

44

example:   gmm

45

example:   gmm

46

example:   gmm

47

example:   gmm

48

example:   gmm

49

example:   gmm

50

example:   gmm

51

example:   gmm

52

example:   gmm

53

example:   gmm

54

k-     means   vs.   gmm

convergence:   
k-     means tends   to   converge much   faster   than   em   for   gmm

speed:   
each   iteration   of   k-     means is   computationally   less   intensive than   
each   iteration   of   em   for   gmm

initialization:   
to   initialize em   for   gmm,   we   typically   first   run   k-     means and   use   the   
resulting   cluster   centers   as   the   means   of   the   gaussian   components

output:   
a   gmm   gives   a   id203   distribution   over   the   cluster   assignment   
for   each   point;   whereas   k-     means gives   a   single   hard   assignment

55

properties   of   em

56

properties   of   em

    em   is   trying to   optimize   a   

nonconvex function

    but   em   is   a   local

optimization   algorithm

    typical   solution:   random   

restarts
    just   like   k-     means,   we   run   the   

algorithm   many   times

    each   time   initialize   

parameters   randomly

    pick   the   parameters   that   give   

highest   likelihood

57

example:   grammar   induction
question:   can   maximizing   (unsupervised)   marginal   
likelihood   produce   useful   results?

answer: let   s   look   at   an   example   
    babies learn   the   syntax   of   their   native   language   (e.g.   

english)   just   by   hearing many   sentences

    can   a   computer similarly   learn   syntax   of   a   human   

language just   by   looking   at   lots   of   example   
sentences?
    this   is   the   problem   of   grammar   induction!
    it   s   an   unsupervised   learning   problem
    we   try   to   recover   the   syntactic   structure   for   each   

sentence   without   any   supervision

58

alice
alice
alice
alice

saw bob
saw bob
saw bob
saw bob

on
on
on
on

a
a
a
a

hill with
hill with
hill with
hill with

a
a
a
a

telescope
telescope
telescope
telescope

example:   grammar   induction

time    ies like an arrow
time    ies like an arrow
time    ies like an arrow
time    ies like an arrow

4
4
4
4

time
time
time
time

   ies
   ies
   ies
   ies

like
like
like
like

an
an
an
an

arrow
arrow
arrow
arrow

time
time
time
time

   ies
   ies
   ies
   ies

like
like
like
like

an
an
an
an

arrow
arrow
arrow
arrow

time
time
time
time

   ies
   ies
   ies
   ies

an
an
an
an

arrow
arrow
arrow
arrow

like
like
like
like

   

time
time
time
time

   ies
   ies
   ies
   ies

like
like
like
like

an
an
an
an

arrow
arrow
arrow
arrow

59

no   semantic   
interpretation

example:   grammar   induction

training   data:   sentences   only,   without   parses   

sample   1:

time

flies

like

an

arrow

sample   2:

real

flies

like

soup

sample   3:

flies

fly

with

their

wings

sample   4:

with

time

you

will

see

x(1)

x(2)

x(3)

x(4)

test   data:   sentences   with   parses,   so   we   can   evaluate   accuracy   

60

example:   grammar   induction

dependency model with valence (klein & manning, 2004)

q: does   likelihood   
correlate   with   
accuracy   on   a   task   
we   care   about?

a:   yes,   but   there   is   
still   a   wide   range   
of   accuracies   for   a   
particular   
likelihood   value

)

%

(
 
y
c
a
r
u
c
c
a

 
t
n
e
m
h
c
a
t
t

a

60

50

40

30

20

10

-20.2

figure   from   gimpel &   smith   (naacl   2012)   -      slides

pearson   s r = 0.63
(strong correlation)

-19.8

-20
-19.2
log-likelihood (per sentence)

-19.6

-19.4

lti

-19

4

61

variants   of   em

    generalized   em:   replace   the   m-     step   by   a   single   

gradient-     step   that   improves   the   likelihood

    monte   carlo   em:   approximate   the   e-     step   by   

sampling

    sparse   em:   keep   an      active   list      of   points   

(updated   occasionally)   from   which   we   estimate   
the   expected   counts   in   the   e-     step

    incremental   em   /   stepwise   em:   if   standard   em   
is   described   as   a   batch algorithm,   these   are   the   
online equivalent

    etc.

63

a   report   card   for   em

    some   good   things   about   em:

    no   learning   rate   (step-     size)   parameter
    automatically   enforces   parameter   constraints
    very   fast   for   low   dimensions
    each   iteration   guaranteed   to   improve   likelihood

    some   bad   things   about   em:
    can   get   stuck   in   local   minima
    can   be   slower   than   conjugate   gradient   (especially   

near   convergence)

    requires   expensive   id136   step
    is   a   maximum   likelihood/map   method

eric   xing

     eric   xing   @   cmu,   2006-     2011

64

dimensionality   reduction

65

pca   outline

    dimensionality   reduction

    high-     dimensional   data
    learning   (low   dimensional)   representations

    principal   component   analysis   (pca)

    examples:   2d   and   3d
    data   for   pca
    pca   definition
    objective   functions   for   pca
    pca,   eigenvectors,   and   eigenvalues
    algorithms   for   finding   eigenvectors   /   eigenvalues

    pca   examples

    face   recognition
    image   compression

66

big   &   high-     dimensional   data

    high-     dimensions   =   lot   of   features

document   classification
features   per   document   =   
thousands   of   words/unigrams
millions   of   bigrams,      contextual   
information

surveys   -      netflix

480189   users   x   17770   movies

slide   from   nina   balcan

big   &   high-     dimensional   data

    high-     dimensions   =   lot   of   features

meg   brain   imaging

120   locations   x   500   time   points   
x   20   objects

or   any   high-     dimensional   image   data

slide   from   nina   balcan

    big   &   high-     dimensional   data.

    useful   to   learn   lower   dimensional   

representations   of   the   data.

slide   from   nina   balcan

learning   representations

pca,   kernel   pca,   ica:   powerful   unsupervised   learning   techniques   
for   extracting   hidden   (potentially   lower   dimensional)   structure   
from   high   dimensional   datasets.

useful   for:
    visualization   
    more   efficient   use   of   resources   
(e.g.,   time,   memory,   communication)

    statistical:   fewer   dimensions         better   generalization
    noise   removal   (improving   data   quality)
    further   processing   by   machine   learning   algorithms

slide   from   nina   balcan

principal   component   
analysis   (pca)

73

pca   outline

    dimensionality   reduction

    high-     dimensional   data
    learning   (low   dimensional)   representations

    principal   component   analysis   (pca)

    examples:   2d   and   3d
    data   for   pca
    pca   definition
    objective   functions   for   pca
    pca,   eigenvectors,   and   eigenvalues
    algorithms   for   finding   eigenvectors   /   eigenvalues

    pca   examples

    face   recognition
    image   compression

74

principal   component   analysis   (pca)

in   case   where   data      lies   on   or   near   a   low   d-     dimensional   linear   subspace,   
axes   of   this   subspace   are   an   effective   representation   of   the   data.

identifying   the   axes   is   known   as   principal   components   analysis,   and   can   be   
obtained   by   using   classic   matrix   computation   tools   (eigen   or   singular   value   
decomposition).

slide   from   nina   balcan

2d   gaussian   dataset

slide   from   barnabas   poczos

1st pca   axis

slide   from   barnabas   poczos

2nd pca   axis

slide   from   barnabas   poczos

principal   component   analysis   (pca)
whiteboard

    data   for   pca
    pca   definition

79

