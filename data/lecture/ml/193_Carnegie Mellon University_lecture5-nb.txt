10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

id113/map

+

na  ve   bayes

id113   /   map   readings:
   estimating   probabilities      
(mitchell,   2016)

na  ve   bayes   readings:
   generative   and   discriminative   
classifiers:   naive   bayes   and   logistic   
regression      
(mitchell,   2016)

murphy   3
bishop   -     -     
htf   -     -     
mitchell   6.1-     6.10

matt   gorid113y
lecture   5
february   1,   2016

1

reminders

    background exercises (homework 1)

    release:   wed,   jan.   25
    due:   wed,   feb.   1   at   5:30pm
    only   hw1:   collaboration questions not

required

    homework 2:   naive bayes

    release:   wed,   feb.   1
    due:   mon,   feb.   13   at   5:30pm

2

id113   /   map   outline

    generating   data

    natural   (stochastic)   data
    synthetic   data
    why   synthetic   data?
    examples:   multinomial,   bernoulli,   gaussian

    data   likelihood

   

   

    independent   and   identically   distributed   (i.i.d.)
    example:   dice   rolls
learning   from   data   (frequentist)
    principle   of   maximum   likelihood   estimation   (id113)
    optimization   for   id113
    examples:   1d   and   2d   optimization
    example:   id113   of   multinomial
    aside:   method   of   lagrange   multipliers
learning   from   data   (bayesian)
    maximum   a   posteriori   (map)   estimation
    optimization   for   map
    example:   map   of   bernoulli   beta   

last   lecture

this   lecture

3

learning   from   data   (frequentist)

whiteboard

    aside:   method   of   langrange multipliers
    example:   id113   of   multinomial

4

learning   from   data   (bayesian)

whiteboard

    maximum   a   posteriori   (map)   estimation
    optimization   for   map
    example:   map   of   bernoulli   beta   

5

takeaways

    one   view   of   what   ml   is   trying   to   accomplish   is   

function   approximation

    the   principle   of   maximum   likelihood   

estimation   provides   an   alternate   view   of   
learning

    synthetic   data   can   help   debug ml   algorithms
    id203   distributions   can   be   used   to   model

real   data   that   occurs   in   the   world
(don   t   worry   we   ll   make   our   distributions   more   
interesting   soon!)

6

na  ve   bayes   outline

    probabilistic   (generative)   view   of   

classification
    decision   rule   for   id203   model

    real-     world   dataset

    economist   vs.   onion   articles
    document         bag-     of-     words         binary   feature   

vector

    naive   bayes:   model

    generating   synthetic   "labeled   documents"
    definition   of   model
    naive   bayes   assumption
    counting   #   of   parameters   with   /   without   nb   

assumption

    na  ve   bayes:   learning   from   data

    data   likelihood
    id113   for   naive   bayes
    map   for   naive   bayes

    visualizing   gaussian   naive   bayes

7

today   s   goal

to   define   a   generative   model   

of   emails   of   two   different   

classes   

(e.g.   spam   vs.   not   spam)

8

spam   news

the   economist

the   onion

9

real-     world   dataset

whiteboard

    economist   vs.   onion   articles
    document         bag-     of-     words         binary   feature   

vector

10

naive   bayes:   model

whiteboard

    generating   synthetic   "labeled   documents"
    definition   of   model
    naive   bayes   assumption
    counting   #   of   parameters   with   /   without   nb   

assumption

11

model   1:   bernoulli   na  ve   bayes

flip   weighted   coin

if   heads,   flip   
each   red   coin

   

each   red   coin   
corresponds   to   

an   xm

y

0

1

1

0

0

1

x1
1

0

1

0

1

1

x2
0

1

1

0

0

0

x3     xm
1     1

0     1

1     1

1     1

1     0

1     0

if   tails,   flip   
each   blue   coin

   

we   can   generate data   in   
this   fashion.   though   in   
practice   we   never   would   
since   our   data   is   given.   

instead,   this   provides   an   
explanation   of   how the   

data   was   generated   
(albeit   a   terrible   one).

12

naive   bayes:   model

whiteboard

    generating   synthetic   "labeled   documents"
    definition   of   model
    naive   bayes   assumption
    counting   #   of   parameters   with   /   without   nb   

assumption

13

what   s   wrong   with   the   
na  ve   bayes   assumption?
the   features   might   not   be   independent!!
    example   1:

    if   a   document   contains   the   word   
   donald   ,   it   s   extremely   likely   to   
contain   the   word      trump   
    these   are   not   independent!

    example   2:

    if   the   petal   width   is   very   high,   
the   petal   length   is   also   likely   to   
be   very   high

14

na  ve   bayes:   learning   from   data

whiteboard

    data   likelihood
    id113   for   naive   bayes
    map   for   naive   bayes

15

visualizing   na  ve   bayes

slides   in   this   section   from   william   cohen   (10-     601b,   spring   2016)

16

fisher   iris   dataset

fisher   (1936)   used   150   measurements   of   flowers   
from   3   different   species:   iris   setosa (0),   iris   
virginica (1),   iris   versicolor (2)   collected   by   
anderson   (1936)

species

0
0
0
1
1
1
1

sepal   
length
4.3
4.9
5.3
4.9
5.7
6.3
6.7

sepal   
width
3.0
3.6
3.7
2.4
2.8
3.3
3.0

petal   
length
1.1
1.4
1.5
3.3
4.1
4.7
5.0

petal   
width
0.1
0.1
0.2
1.0
1.3
1.6
1.7

full   dataset:   https://en.wikipedia.org/wiki/iris_flower_data_set

18

slide   from   william   cohen

slide   from   william   cohen

plot   the   difference   of   the   probabilities

slide   from   william   cohen

na  ve   bayes   has   a   linear decision   

boundary

slide   from   william   cohen   (10-     601b,   spring   2016)

figure   from   william   cohen   (10-     601b,   spring   2016)

figure   from   william   cohen   (10-     601b,   spring   2016)

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

beyond   the   scope   of   this   lecture
    multinomial na  ve   bayes   can   be   used   for   

integer features

    multi-     class   na  ve   bayes   can   be   used   if   your   

classification   problem   has   >   2   classes

25

summary

1. na  ve   bayes   provides   a   framework   for   

generative   modeling

2. choose   p(xm |   y)   appropriate   to   the   data

(e.g.   bernoulli   for   binary   features,   
gaussian   for   continuous   features)

3. train   by   id113 or   map
4. classify   by   maximizing   the   posterior

26

extra   slides

27

generic

na  ve   bayes   model

support: depends   on   the   choice   of   event   model,   p(xk|y)
model:   product   of   prior and   the   event   model

p ((cid:115), y ) = p (y )

k k=1

p (xk|y )

training:   find   the   class-     conditional   id113   parameters

for   p(y),   we   find   the   id113   using   all   the   data.   for   each   
p(xk|y) we   condition   on   the   data   with   the   corresponding   
class.
classification:   find   the   class   that   maximizes   the   posterior

  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

y

p(y|(cid:116))

28

generic

na  ve   bayes   model

(posterior)

classification:

  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

y

= (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

y

= (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

y

p(y|(cid:116))
p((cid:116)|y)p(y)
p((cid:116)|y)p(y)

p(x)

(by bayes    rule)

29

model   1:   bernoulli   na  ve   bayes

support:   binary   vectors   of   length   k

(cid:116)   {0, 1}k
generative   story:

y   bernoulli( )
xk   bernoulli( k,y )  k   {1, . . . , k}

model: p , (x, y) = p , (x1, . . . , xk, y)

= p (y)

k k=1

p k (xk|y)

= ( )y(1    )(1 y)

k k=1

( k,y)xk (1    k,y)(1 xk)

30

model   1:   bernoulli   na  ve   bayes

support:   binary   vectors   of   length   k

(cid:116)   {0, 1}k
generative   story:

p , (x, y) = p , (x1, . . . , xk, y)

k k=1

= p (y)

y   bernoulli( )
xk   bernoulli( k,y )  k   {1, . . . , k}
k k=1

= ( )y(1    )(1 y)
= p (y)

model: p , (x, y) = p , (x1, . . . , xk, y)

p k (xk|y)

p k (xk|y)

k k=1
  y = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
= ( )y(1    )(1 y)

y

k k=1
p(y|(cid:116))

same   as   generic   
( k,y)xk (1    k,y)(1 xk)

na  ve   bayes

( k,y)xk (1    k,y)(1 xk)

31

classification:   find   the   class   that   maximizes   the   posterior

model   1:   bernoulli   na  ve   bayes
training:   find   the   class-     conditional   id113   parameters

for   p(y),   we   find   the   id113   using   all   the   data.   for   each   
p(xk|y) we   condition   on   the   data   with   the   corresponding   
class.

  =  n
 k,0 =  n
 k,1 =  n

i=1 i(y(i) = 1)

n

k = 1)

k = 1)

i=1 i(y(i) = 0   x(i)
 n
i=1 i(y(i) = 0)
i=1 i(y(i) = 1   x(i)
 n
i=1 i(y(i) = 1)

 k   {1, . . . , k}

32

model   1:   bernoulli   na  ve   bayes
training:   find   the   class-     conditional   id113   parameters

for   p(y),   we   find   the   id113   using   all   the   data.   for   each   
p(xk|y) we   condition   on   the   data   with   the   corresponding   
class.

data:

  =  n
 k,0 =  n
 k,1 =  n

i=1 i(y(i) = 1)

n

i=1 i(y(i) = 0   x(i)
 n
i=1 i(y(i) = 0)
i=1 i(y(i) = 1   x(i)
 n
i=1 i(y(i) = 1)

 k   {1, . . . , k}

k = 1)

k = 1)

y

0

1

1

0

0

1

x1
1

0

1

0

1

1

x2
0

1

1

0

0

0

x3     xk
1     1

0     1

1     1

1     1

1     0

1     0

33

model   2:   multinomial   na  ve   bayes

support:

option   1:   integer   vector   (word   ids)

(cid:116) = [x1, x2, . . . , xm ] where xm   {1, . . . , k} a word id.
generative   story:

for i   {1, . . . , n}:

y(i)   bernoulli( )
for j   {1, . . . , mi}:

x(i)
j   multinomial( y(i), 1)

model:

p , (x, y) = p (y)

k k=1

p k (xk|y)

= ( )y(1    )(1 y)

 y,xj

mi j=1

34

model   3:   gaussian   na  ve   bayes

support:   

(cid:116)   rk

model:   product   of   prior and   the   event   model

p(x, y) = p(x1, . . . , xk, y)

= p(y)

k k=1

p(xk|y)

gaussian naive bayes assumes that p(xk|y) is given by
a normal distribution.

35

model   4:   multiclass   na  ve   bayes
model:
the only change is that we permit y to range over c
classes.

p(x, y) = p(x1, . . . , xk, y)

= p(y)

p(xk|y)

k k=1

now, y   multinomial( , 1) and we have a sepa-
rate conditional distribution p(xk|y) for each of the c
classes.

36

smoothing

1. add-     1   smoothing
2. add-        smoothing
3. map   estimation   (beta   prior)

37

id113

what   does   maximizing   likelihood   accomplish?
    there   is   only   a   finite   amount   of   id203   

mass   (i.e.   sum-     to-     one   constraint)

    id113   tries   to   allocate   as   much   id203   

mass   as   possible   to   the   things   we   have   
observed   

   at   the   expense of   the   things   we   have   not
observed

38

id113

for   na  ve   bayes,   suppose   we   never   observe   
the   word      serious      in   an   onion   article.
in   this   case,   what   is   the   id113   of   p(xk |   y)?

 k,0 =  n

k = 1)

i=1 i(y(i) = 0   x(i)
 n
i=1 i(y(i) = 0)

now   suppose   we   observe   the   word      serious      
at   test   time.   what   is   the   posterior   id203   
that   the   article   was   an   onion   article?
p(x|y)p(y)

p(y|x) =

p(x)

39

1.   add-     1   smoothing

the simplest setting for smoothing simply adds a single
pseudo-observation to the data. this converts the true
observations d into a new dataset d  from we derive
the id113s.

i=1

d = {((cid:116)(i), y(i))}n
d  = d   {((cid:121), 0), ((cid:121), 1), ((cid:82), 0), ((cid:82), 1)}

(1)
(2)
where (cid:121) is the vector of all zeros and (cid:82) is the vector of
all ones.
this has the e   ect of pretending that we observed
each feature xk with each class y.

40

1.   add-     1   smoothing

what if we write the id113s in terms of the original
dataset d?

i=1 i(y(i) = 1)

  =  n
1 + n
1 + n

 k,0 =

 k,1 =

n
i=1 i(y(i) = 0   x(i)
2 + n
i=1 i(y(i) = 0)
i=1 i(y(i) = 1   x(i)
2 + n
i=1 i(y(i) = 1)

k = 1)

k = 1)

 k   {1, . . . , k}

41

2.   add-        smoothing

for   the   categorical   distribution

suppose we have a dataset obtained by repeatedly
rolling a k-sided (weighted) die. given data d =
i=1 where x(i)   {1, . . . , k}, we have the fol-
{x(i)}n
lowing id113:

 k =  n

i=1 i(x(i) = k)

n

with add-  smoothing, we add pseudo-observations as
before to obtain a smoothed estimate:

 k =

  + n

i=1 i(x(i) = k)
k  + n

42

id113   vs.   map

suppose we have data d = {x(i)}n

i=1

 id113 = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 map = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)

 

n i=1
p((cid:116)(i)| )
 id113 = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
 
n i=1
p((cid:116)(i)| )p( )

 map = (cid:28)(cid:96)(cid:59)(cid:75)(cid:28)(cid:116)
 

n i=1
n i=1

 

prior

maximum   likelihood   

estimate   (id113)

p((cid:116)(i)| )

maximum   a   posteriori

(map)   estimate

p((cid:116)(i)| )p( )

43

3.   map   estimation   (beta   prior)

generative   story:
the   parameters   are   
drawn   once   for   the   
entire   dataset.

for k   {1, . . . , k}:

for y   {0, 1}:

 k,y   beta( ,  )

for i   {1, . . . , n}:

y(i)   bernoulli( )
for k   {1, . . . , k}:

x(i)
k   bernoulli( k,y(i))

training:   find   the   class-     conditional   
map   parameters

  =  n

i=1 i(y(i) = 1)

n

i=1 i(y(i) = 0   x(i)

k = 1)
i=1 i(y(i) = 0)

i=1 i(y(i) = 1   x(i)

k = 1)
i=1 i(y(i) = 1)

 k,0 =

 k,1 =

(    1) + n
(    1) + (    1) + n
(    1) + n
(    1) + (    1) + n

 k   {1, . . . , k}

44

