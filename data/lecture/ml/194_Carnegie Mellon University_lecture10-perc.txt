10-     601   introduction   to   machine   learning

machine   learning   department
school   of   computer   science
carnegie   mellon   university

id173

+

id88

id88   readings:
murphy   8.5.4
bishop   4.1.7
htf   -     -     
mitchell   4.4.0

matt   gorid113y
lecture   10
february   20,   2016

1

reminders

    homework 3:   linear   /   id28

    release:   mon,   feb.   13
    due:   wed,   feb.   22   at   11:59pm

    homework 4:   id88 /   kernels /   id166

    release:   wed,   feb.   22
    due:   wed,   mar.   01   at   11:59pm

1 week   
for   hw4

    midterm exam (evening exam)
    tue,   mar.   07   at   7:00pm       9:30pm
    see piazza   for details about location

2

outline

    id173

    motivation:   overfitting
    l2,   l1,   l0   id173
    relation   between   id173   and   map   

estimation
    id88

    online   learning
    margin   definitions
    id88   algorithm
    id88   mistake   bound

    generative   vs.   discriminative   classifiers

3

id173

11

overfitting

definition:   the   problem   of   overfitting is   when   
the   model   captures   the   noise   in   the   training   data   
instead   of   the   underlying   structure   

overfitting   can   occur   in   all   the   models   we   ve   seen   
so   far:   

    knn   (e.g.   when   k   is   small)
    na  ve   bayes   (e.g.   without   a   prior)
    linear   regression   (e.g.   with   basis   function)
    logistic   regression   (e.g.   with   many   rare   features)

12

motivation:   id173

example:   stock   prices
    suppose   we   wish   to   predict   google   s   stock   price   at   

time   t+1   

    what   features   should   we   use?

(putting   all   computational   concerns   aside)
    stock   prices   of   all   other   stocks   at   times   t,   t-     1,   t-     2,      ,   t   -      k
    mentions   of   google   with   positive   /   negative   sentiment   

words   in   all   newspapers   and   social   media   outlets

    do   we   believe   that   all of   these   features   are   going   to   

be   useful?

13

motivation:   id173

    occam   s   razor:   prefer   the   simplest   

hypothesis

    what   does   it   mean   for   a   hypothesis   (or   

model)   to   be   simple?
1.
2. small   number   of      important      features   

small   number   of   features   (model   selection)

(shrinkage)

14

id173

whiteboard

    l2,   l1,   l0   id173
    example:   linear   regression
    probabilistic   interpretation   of   id173

15

id173

don   t   regularize   the   bias   (intercept)   parameter!
   

usually   denoted   by       " -     -      that   is,   the   parameter   for   which   
we   fixed       "=1

in   our   models   so   far,   the   bias   /   intercept   parameter   is   

    regularizers always   avoid   penalizing   this   bias   /   intercept   

parameter

    why?   because   otherwise   the   learning   algorithms   wouldn   t   

be   invariant   to   a   shift   in   the   y-     values

whitening   data
   

it   s   common   to   whiten each   feature   by   subtracting   its   
mean   and   dividing   by   its   variance

    for   id173,   this   helps   all   the   features   be   penalized   
in   the   same   units   
(e.g.   convert   both   centimeters   and   kilometers   to   z-     scores)

16

id173:   

+

slide   courtesy   of   william   cohen

polynomial   coefficients         

none

exp(18)

huge

slide   courtesy   of   william   cohen

over   id173:   

slide   courtesy   of   william   cohen

id173   exercise

in-     class   exercise
1. plot   train   error   vs.   #   features   (cartoon)
2. plot   test   error   vs.   #   features   (cartoon)

r
o
r
r
e

#   features

20

example:   logistic   regression

training   

data

21

example:   logistic   regression

test
data

22

example:   logistic   regression

r
o
r
r
e

1/lambda

23

example:   logistic   regression

24

example:   logistic   regression

25

example:   logistic   regression

26

example:   logistic   regression

27

example:   logistic   regression

28

example:   logistic   regression

29

example:   logistic   regression

30

example:   logistic   regression

31

example:   logistic   regression

32

example:   logistic   regression

33

example:   logistic   regression

34

example:   logistic   regression

35

example:   logistic   regression

36

example:   logistic   regression

r
o
r
r
e

1/lambda

37

takeaways

1. nonlinear   basis   functions   allow   linear   

models (e.g.   linear   regression,   logistic   
regression)   to   capture   nonlinear aspects   of   
the   original   input

2. nonlinear   features   are   require   no   changes   

to   the   model   (i.e.   just   preprocessing)

3. id173 helps   to   avoid   overfitting
4. id173 and   map   estimation are   

equivalent   for   appropriately   chosen   priors

46

the   id88   algorithm

47

background:   hyperplanes

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

background:   hyperplanes

hyperplane (definition   1):   
h = {x : wt x = b}
hyperplane (definition   2):   
h = {x : wt x = 0
and x1 = 1}

x0

w

half-     spaces:   
h+ = {x : wt x > 0 and x1 = 1}
h  = {x : wt x < 0 and x1 = 1}

x0
x0

background:   hyperplanes

why   don   t   we   drop   the   
generative   model   and   

try   to   learn   this   

hyperplane directly?

directly   modeling   the   
hyperplane would   use   a   
decision   function:

h((cid:116)) = sign( t (cid:116))

for:

y   { 1, +1}

online   learning

for   i =   1,   2,   3,      :
    receive an   unlabeled   instance   x(i)
    predict y      =   h(x(i))
    receive true   label   y(i)

check for   correctness   (y      ==   y(i))

goal:
    minimize the   number   of   mistakes

52

online   learning:   motivation

examples
1. email   classification   (distribution   of   both   

spam   and   regular   mail   changes   over   time,   
but   the   target   function   stays   fixed   -      last   
year's   spam   still   looks   like   spam).

2. recommendation   systems.   recommending   

movies,   etc.

3. predicting   whether   a   user   will   be   interested   

in   a   new   news   article   or   not.

4. ad   placement   in   a   new   market.

slide   from   nina   balcan

53

id88   algorithm

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.

d = {(cid:116)(i), y(i)}n

i=1 where (cid:116)   rk and y   {+1, 1}

prediction:   output   determined   by   hyperplane.

  y = h (x) = sign( t x)

sign(a) = 1,

if a   0

 1, otherwise

learning:   iterative   procedure:
    while   not   converged

receive next   example   (x(i),   y(i))

   
    predict y      =   h(x(i))
   
   

if positive   mistake:   add x(i) to   parameters
if negative   mistake:   subtract x(i) from   parameters

54

id88   algorithm

data:   inputs   are   continuous   vectors   of   length   k.   outputs   
are   discrete.

d = {(cid:116)(i), y(i)}n

i=1 where (cid:116)   rk and y   {+1, 1}

prediction:   output   determined   by   hyperplane.

  y = h (x) = sign( t x)

learning:

sign(a) = 1,

if a   0

 1, otherwise

55

id88   algorithm:   example

example:    1,2    
1,0 +1,1 +
   1,0    
   1,   2    
1,   1 +

x
a   
x
a   
x
a   

algorithm:
     

set   t=1,   start   with   all-     zeroes   weight   vector       ).
      given   example       ,   predict   positive   iff    3          0.
    mistake   on   positive,   update       37)       3+    
    mistake   on   negative,   update       37)       3       

      on   a   mistake,   update   as   follows:   

slide   adapted   from   nina   balcan

-
-
-

+
+
+

    )=(0,0)
    -=    )       1,2 =(1,   2)
    .=    -+ 1,1 =(2,   1)
    0=    .       1,   2 =(3,1)

definition: the   margin of   example        w.r.t. a   linear   sep.     is   the   
distance   from       	
   to   the   plane              =0 (or   the   negative if   on   wrong   side)

geometric   margin

margin   of   positive   example       )
    )

w

margin   of   negative   example       -

    -

slide   from   nina   balcan

geometric   margin

definition: the   margin of   example        w.r.t. a   linear   sep.     is   the   
distance   from       	
   to   the   plane              =0 (or   the   negative if   on   wrong   side)
definition: the   margin       ; of   a   set   of   examples        wrt a   linear   
separator        is   the   smallest   margin   over   points              .

+

    ;    ;

-
-

-

+
+
+ +

-
-

w

+

+

-

-
-

-

+

slide   from   nina   balcan

geometric   margin

definition: the   margin of   example        w.r.t. a   linear   sep.     is   the   
distance   from       	
   to   the   plane              =0 (or   the   negative if   on   wrong   side)
definition: the   margin       ; of   a   set   of   examples        wrt a   linear   
separator        is   the   smallest   margin   over   points              .
definition: the   margin        of   a   set   of   examples        is   the   maximum    ;
over   all   linear   separators       .
         

+ +

w

-
-

-

+

+

-
-

-

-
-

+

slide   from   nina   balcan

-

analysis:   id88

id88   mistake   bound

guarantee: if data has margin   and all points inside a ball of
radius r, then id88 makes   (r/ )2 mistakes.
(normalized   margin:   multiplying all   points   by   100,   or   dividing   all   points   by   100,   
doesn   t   change   the   number   of   mistakes;   algo is   invariant   to   scaling.)

+

g   

g   
-
-
-

-

-

+
+

+

r

  
+

+

+

+

-

-

60

-

-

slide   adapted   from   nina   balcan

analysis:   id88

id88   mistake   bound

theorem 0.1 (block (1962), noviko    (1962)).
given dataset: d = {((cid:116)(i), y(i))}n
i=1.
suppose:
1. finite size inputs: ||x(i)||   r
2. linearly separable data:     s.t. ||  || = 1 and
y(i)(      (cid:116)(i))    , i
then: the number of mistakes made by the id88
algorithm on this dataset is

+
+

+

+

k   (r/ )2

+

  
+

+

r

-

-

g   

-

-

g   

+

-

-

-

-

-

61

figure   from   nina   balcan

analysis:   id88

proof   of   id88   mistake   bound:

we   will   show   that   there   exist   constants   a   and   b   s.t.

ak   || (k+1)||   b k

ak   || (k+1)||   b k

ak   || (k+1)||   b k

ak   || (k+1)||   b k

ak   || (k+1)||   b k

62

analysis:   id88

theorem 0.1 (block (1962), noviko    (1962)).
given dataset: d = {((cid:116)(i), y(i))}n
i=1.
suppose:
1. finite size inputs: ||x(i)||   r
2. linearly separable data:     s.t. ||  || = 1 and
y(i)(      (cid:116)(i))    , i
then: the number of mistakes made by the id88
algorithm on this dataset is

+

g   

-

-

-

g   

-

-

+

+

+
+

+

-

-

  
+

+

r

-

-

k   (r/ )2

algorithm 1 id88 learning algorithm (online)
1: procedure p(cid:266)(cid:279)(cid:264)(cid:266)(cid:277)(cid:281)(cid:279)(cid:276)(cid:275)(d = {((cid:116)(1), y(1)), ((cid:116)(2), y(2)), . . .})

2:
3:
4:
5:
6:
7:

    0, k = 1
for i   {1, 2, . . .} do

if y(i)( (k)    (cid:116)(i))   0 then
 (k+1)    (k) + y(i)(cid:116)(i)
k   k + 1

return  

  initialize parameters
  for each example
  if mistake
  update parameters

63

analysis:   id88

whiteboard:   
proof   of   id88   mistake   bound

64

analysis:   id88

proof   of   id88   mistake   bound:
part   1:   for   some   a,    ak   || (k+1)||   b k
 (k+1)       = ( (k) + y(i)(cid:116)(i))  

by id88 algorithm update
=  (k)       + y(i)(      (cid:116)(i))
   (k)       +  
by assumption
   (k+1)         k 
by induction on k since  (1) = 0
  || (k+1)||   k 
since ||(cid:114)||   ||(cid:109)||   (cid:114)    (cid:109) and ||  || = 1

cauchy-     schwartz   inequality

65

analysis:   id88

proof   of   id88   mistake   bound:
part   2:   for   some   b,   ak   || (k+1)||   b k
|| (k+1)||2 = || (k) + y(i)(cid:116)(i)||2

by id88 algorithm update
= || (k)||2 + (y(i))2||(cid:116)(i)||2 + 2y(i)( (k)    (cid:116)(i))
  || (k)||2 + (y(i))2||(cid:116)(i)||2
since kth mistake   y(i)( (k)    (cid:116)(i))   0
= || (k)||2 + r2
since (y(i))2||(cid:116)(i)||2 = ||(cid:116)(i)||2 = r2 by assumption and (y(i))2 = 1
  || (k+1)||2   kr2
by induction on k since ( (1))2 = 0
 kr
  || (k+1)||  

66

analysis:   id88

proof   of   id88   mistake   bound:
part   3:   combining   the   bounds   finishes   the   proof.

 kr

k    || (k+1)||  

 k   (r/ )2

the   total   number   of   mistakes   

must   be   less   than   this

67

(batch)   id88   algorithm
learning for   id88   also   works   if   we   have   a   fixed   training   
dataset,   d.   we   call   this   the      batch      setting   in   contrast   to   the      online      
setting   that   we   ve   discussed   so   far.

algorithm 1 id88 learning algorithm (batch)
1: procedure p(cid:266)(cid:279)(cid:264)(cid:266)(cid:277)(cid:281)(cid:279)(cid:276)(cid:275)(d = {((cid:116)(1), y(1)), . . . , ((cid:116)(n ), y(n ))})
  initialize parameters

2:
3:
4:
5:
6:
7:
8:

    0
while not converged do
  y   sign( t (cid:116)(i))
if   y  = y(i) then

for i   {1, 2, . . . , n} do

      + y(i)(cid:116)(i)

return  

  for each example
  predict
  if mistake
  update parameters

68

(batch)   id88   algorithm
learning for   id88   also   works   if   we   have   a   fixed   training   
dataset,   d.   we   call   this   the      batch      setting   in   contrast   to   the      online      
setting   that   we   ve   discussed   so   far.

discussion:
the   batch   id88   algorithm   can   be   derived   in   two   ways.

1. by   extending   the   online   id88   algorithm   to   the   batch   

setting   (as   mentioned   above)

2. by   applying   stochastic   gradient   descent   (sgd)   to   minimize   a   

so-     called   hinge   loss   on   a   linear   separator

69

extensions   of   id88

    kernel   id88

    choose   a   kernel   k(x   ,   x)
    apply   the   kernel   trick   to   id88
    resulting   algorithm   is   still   very   simple

    structured   id88

    basic   idea   can   also   be   applied   when   y ranges   

over   an   exponentially   large   set

    mistake   bound   does   not depend   on   the   size   of   

that   set

70

matching   game

goal:   match   the   algorithm   to   its   update   rule

1.   sgd   for   logistic   regression

h (x) = p(y|x)

2.   least   mean   squares

h (x) =  t x

3.   id88

h (x) = sign( t x)

4.

5.

6.

 k    k + (h (x(i))   y(i))

 k    k +

1

1 + exp  (h (x(i))   y(i))

 k    k +  (h (x(i))   y(i))x(i)

k

a.   1=5,   2=4,   3=6
b.   1=5,   2=6,   3=4
c.   1=6,   2=4,   3=4
d.   1=5,   2=6,   3=6
e.   1=6,   2=6,   3=6

71

summary:   id88

    id88   is   a   linear   classifier
    simple learning   algorithm:   when   a   mistake   

is   made,   add   /   subtract   the   features

    for   linearly   separable   and   inseparable   data,   

we   can   bound   the   number   of   mistakes   
(geometric   argument)

    extensions   support   nonlinear   separators   and   

structured   prediction

72

discriminative   and   
generative   classifiers

73

generative   vs.   discriminative

    generative   classifiers:
    example:   na  ve   bayes
    define   a   joint   model   of   the   observations   x and   the   

labels   y:

p(x, y)

    learning   maximizes   (joint)   likelihood
    use   bayes      rule   to   classify   based   on   the   posterior:

    discriminative   classifiers:

p(y|x) = p(x|y)p(y)/p(x)
    example:   logistic   regression
    directly   model   the   conditional:      
    learning   maximizes   conditional   likelihood

p(y|x)

74

generative   vs.   discriminative

whiteboard

    contrast:   to   model   p(x)   or   not   to   model   p(x)?

75

generative   vs.   discriminative
finite   sample   analysis   (ng   &   jordan,   2002)
[assume   that   we   are   learning   from   a   finite   
training   dataset]

if   model   assumptions   are   correct:   naive   bayes   is   a   more   
efficient   learner   (requires   fewer   samples)   than   logistic   
regression

if   model   assumptions   are   incorrect:   logistic   regression   has   
lower   asymtotic error,   and   does   better   than   na  ve   bayes

76

solid:   nb   dashed:   lr

slide   courtesy   of   william   cohen

77

solid:   nb   dashed:   lr

na  ve   bayes   makes   stronger   assumptions   about   the   data
but   needs   fewer   examples   to   estimate   the   parameters

   on   discriminative   vs generative   classifiers:      .      andrew   ng   
and   michael   jordan,   nips   2001.

slide   courtesy   of   william   cohen

78

generative   vs.   discriminative

learning   (parameter   estimation)

na  ve   bayes:   
parameters   are   decoupled         closed   form   solution   for   id113

logistic   regression:   
parameters   are   coupled         no   closed   form   solution       must   
use   iterative   optimization   techniques   instead

79

na  ve   bayes   vs.   logistic   reg.
learning   (map   estimation   of   parameters)

bernoulli   na  ve   bayes:   
parameters   are   probabilities         beta   prior   (usually)   pushes   
probabilities   away   from   zero   /   one   extremes

logistic   regression:   
parameters   are   not   probabilities         gaussian   prior   
encourages   parameters   to   be   close   to   zero   

(effectively   pushes   the   probabilities   away   from   zero   /   one   
extremes)

80

na  ve   bayes   vs.   logistic   reg.

features

na  ve   bayes:   
features   x are   assumed   to   be   conditionally   independent   
given   y.   (i.e.   na  ve   bayes   assumption)

logistic   regression:   
no   assumptions   are   made   about   the   form   of   the   features   x.      
they   can   be   dependent   and   correlated   in   any   fashion.   

81

