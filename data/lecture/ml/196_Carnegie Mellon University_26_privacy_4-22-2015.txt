machine learning and 
differential privacy 

maria-florina balcan 

04/22/2015 

learning and privacy 
    to do machine learning, we need data. 

    what if the data contains sensitive information?  

    medical data, web search query data, salary data, student grade data. 

    even if the (person running the) learning algo can 
be trusted, perhaps the output of the algorithm 
reveals sensitive info. 

    e.g., using search logs of friends to recommend 

query completions: 

why are _ 
why are my feet so itchy? 

learning and privacy 
    to do machine learning, we need data. 

    what if the data contains sensitive information?  

    even if the (person running the) learning algo can 
be trusted, perhaps the output of the algorithm 
reveals sensitive info. 

    e.g., id166 or id88 on medical data: 

- suppose feature      is has-green-hair and the learned      

has              0. 

- if there is only one person in town with green hair, you 

know they were in the study. 

learning and privacy 
    to do machine learning, we need data. 

    what if the data contains sensitive information?  

    even if the (person running the) learning algo can 
be trusted, perhaps the output of the algorithm 
reveals sensitive info. 

    an approach to address these problems: 

differential privacy 

   the algorithmic foundations of differential privacy   . cynthia dwork, 
aaron roth. foundations and trends in theoretical computer science, 
now publishers. 2014. 

differential privacy 

e.g., want to release average while preserving privacy. 

high level idea: 
    what we want is a protocol that has a id203 

distribution over outputs: 

such that if person i changed their input from xi to any 

other allowed xi   , the relative probabilities of any output 
do not change by much. 

differential privacy 

high level idea: 
    what we want is a protocol that has a id203 

distribution over outputs: 

such that if person i changed their input from xi to any 

other allowed xi   , the relative probabilities of any output 
do not change by much. 

    this would effectively allow that person to pretend their 

input was any other value they wanted. 

bayes rule: pr                                  
                            

pr         

=

pr                                  
   
pr                                  

   

pr         
pr         

     

(posterior     prior) 

differential privacy: definition 
it   s a property of a protocol a which you run on 
some dataset x producing some output a(x).  

    a is   -differentially private if for any two neighbor 
datasets s, s    (differ in just one element xi ! xi   ), 

xi 

x   i 

   for all outcomes v,  
             e-      pr(a(s)=v)/pr(a(s   )=v)    e   

 

   1-   

id203 over 
randomness in a 

   1+   

differential privacy: definition 
it   s a property of a protocol a which you run on 
some dataset x producing some output a(x).  

    a is   -differentially private if for any two neighbor 
datasets s, s    (differ in just one element xi ! xi   ), 

 view as model of plausible deniability 

 

if your real input is          and you   d like to pretend was            , somebody 
looking at the output of a can   t tell, since for any outcome v, it was 
nearly just as likely to come from s as it was to come from s   . 

   for all outcomes v,  
             e-      pr(a(s)=v)/pr(a(s   )=v)    e   

 

   1-   

id203 over 
randomness in a 

   1+   

differential privacy: methods 
it   s a property of a protocol a which you run on 
some dataset x producing some output a(x).  

    can we achieve it? 

    sure, just have a(x) always output 0.  

    this is perfectly private, but also completely 

useless. 

    can we achieve it while still providing useful 

information? 

laplace mechanism 

say have n inputs in range [0,b].  want to release 

average while preserving privacy. 

    changing one input can affect average by     b/n. 

    idea: take answer and add noise from laplace 

distrib                       |    |        /     

    changing one input 

changes prob of any 
given answer by 
            . 

b/n 

x 

value with real me 

value with fake me 

laplace mechanism 

say have n inputs in range [0,b].  want to release 

average while preserving privacy. 

    changing one input can affect average by     b/n. 

    idea: : compute the true answer and add noise from 

laplace distrib                       |    |        /     

    amount of noise added will be           /(        ). 

    to get an overall error of        , you need a sample size      =

    
        

. 

    if you want to ask      queries, the privacy loss adds, so to 

have     -differential privacy overall, you need      =

        
        

. 

laplace mechanism 

good features: 

    can run algorithms that just need to use 

approximate statistics (since just adding small 
amounts of noise to them). 

    e.g.,    approximately how much would this split in my 

decision tree reduce id178?    

more generally 

    anything learnable via    statistical queries    is learnable 

differentially privately. 
practical privacy: the sulq framework. blum, dwork, mcsherry,nissim. pods 2005.  

    statistical query model [kearns93] : 

s 

q(x,l) 
prd[q(x,f(x))=1]       . 

    what is the error rate of 

my current rule? 

    what is the correlation of 

x1 with f when x2=0?     

    many algorithms (including  , id88, id166, pca) can 

be re-written to interface via such statistical estimates.  

laplace mechanism 

problems: 

    if you ask many questions, need large dataset to be 

able to can give accurate and private answers to all of 
them. (privacy losses accumulate over questions asked). 

    also, differential privacy may not be appropriate if 

multiple examples correspond to same individual (e.g., 
search queries, restaurant reviews). 

more generally 

problems: 

    the more interconnected our data is (a and b are 

friends because of person c) the trickier it 
becomes to reason about privacy. 

    lots of current work on definitions and algorithms. 

