    support vector machines (id166s).  

    semi-supervised learning.  

   

semi-supervised id166s.  

maria-florina balcan 

03/25/2015 

support vector machines (id166s).  

one of the most theoretically well motivated 
and practically most e   ective classi   cation 
algorithms in machine learning. 

directly  motivated by margins and kernels! 

geometric margin 
wlog  homogeneous linear separators [w0 = 0]. 

definition: the margin of example      w.r.t. a linear sep.      is the 
distance from      to the plane               = 0. 

margin of example     1 

    1 

w 

if       = 1,  margin of x 
w.r.t. w is |             |. 

margin of example     2 

    2 

geometric margin 

definition: the margin of example      w.r.t. a linear sep.      is the 
distance from      to the plane               = 0. 

definition: the margin          of a set of examples      wrt a linear 
separator      is the smallest margin over points              . 

definition: the margin      of a set of examples      is the maximum 
         over all linear separators     . 

- 

     

     
- 
- 
- 

- 

- 

- 

+ 

+ 

w 

+ 

+ 

- 
- 

+ 

margin important theme in ml 
both sample complexity and algorithmic implications. 

sample/mistake bound complexity: 

    if large margin, # mistakes peceptron makes 

is small (independent on the dim of the space)! 

    if large margin      and if alg. produces a large 

margin classifier, then amount of data needed 
depends only on r/     [bartlett & shawe-taylor    99].  

algorithmic implications 

suggests searching for a large margin 
classifier    id166s 

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 
- 
- 

+ 

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

first, assume we know a lower bound on the margin      

input:     , s={(x1,     1),    ,(xm,     m)};  

find: some w where: 
  

2

= 1  

    w
     for all i,                                    

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 

- 
- 

+ 

output: w, a separator of margin      over s 

realizable case, where the data is linearly separable by margin      

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

e.g., search for the best possible       

input: s={(x1,     1),    ,(xm,     m)};  

find: some w and maximum      where: 
  

2

= 1  

    w
     for all i,                                    

output: maximum margin separator over s 

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 

- 
- 

+ 

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

input: s={(x1,     1),    ,(xm,     m)};  

maximize      under the constraint: 
  

= 1  

    w
     for all i,                                    

2

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 

- 
- 

+ 

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

input: s={(x1,     1),    ,(xm,     m)};  

maximize      under the constraint: 
  

= 1  

    w
     for all i,                                    

2

this is a 
constrained 
optimization 
problem. 

objective 
function 

constraints 

    famous example of constrained optimization: id135, 

where objective fn is linear, constraints are linear (in)equalities 

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

input: s={(x1,     1),    ,(xm,     m)};  

maximize      under the constraint: 
  

= 1  

    w
     for all i,                                    

2

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 
    1 

- 
- 

+ 

this constraint is non-linear. 

in fact, it   s even non-convex 

    1 +     2

2

 

    2 

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

input: s={(x1,     1),    ,(xm,     m)};  

maximize      under the constraint: 
  

= 1  

    w
     for all i,                                    

2

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 

- 
- 

+ 

         =      /    , then max      is equiv. to  minimizing ||       ||2 (since ||       ||2 = 1/    2).  
so, dividing both sides by      and writing in terms of w    we get: 

input: s={(x1,     1),    ,(xm,     m)};  

2

 under the constraint: 

minimize         
  

    for all i,                                  1 

                 =    1 

w    

- 

+ + 

+ + 
- 
- - 
- 
- 
- 

- 
- 

+ 

                 = 1 

support vector machines (id166s) 

directly optimize for the maximum margin separator: id166s 

input: s={(x1,     1),    ,(xm,     m)};  

argminw      
  

2

 s.t.: 

    for all i,                               1 

this is a 
constrained 
optimization 
problem. 

    the objective is convex (quadratic) 
    all constraints are linear 
    can solve efficiently (in poly time) using standard quadratic 

programing (qp) software 

support vector machines (id166s) 

question: what if data isn   t perfectly linearly separable? 

issue 1: now have two objectives  
    maximize margin 
    minimize # of misclassifications. 

ans 1: let   s optimize their sum: minimize 

2

    

+     (# misclassifications)  

where      is some tradeoff constant.  

              =    1 

+ 

w 

+ 

              = 1 

- 
+ + 

- 
+ 
- 

- 

- 
- - 
- 
- 

issue 2: this is computationally hard (np-hard). 
[even if didn   t care about margin and minimized # mistakes] 

np-hard [guruswami-raghavendra   06]   

support vector machines (id166s) 

question: what if data isn   t perfectly linearly separable? 

issue 1: now have two objectives  
    maximize margin 
    minimize # of misclassifications. 

ans 1: let   s optimize their sum: minimize 

2

    

+     (# misclassifications)  

where      is some tradeoff constant.  

              =    1 

+ 

w 

+ 

              = 1 

- 
+ + 

- 
+ 
- 

- 

- 
- - 
- 
- 

issue 2: this is computationally hard (np-hard). 
[even if didn   t care about margin and minimized # mistakes] 

np-hard [guruswami-raghavendra   06]   

support vector machines (id166s) 

question: what if data isn   t perfectly linearly separable? 

replace    # mistakes    with upper bound called    hinge loss    

                 =    1 

w    

input: s={(x1,     1),    ,(xm,     m)};  

2

 under the constraint: 

minimize         
  

    for all i,                                  1 

input: s={(x1,     1),    ,(xm,     m)};  

              =    1 

find 

2

+                
argminw,    1,   ,              
      for all i,                               1              

    

 s.t.: 

             0 

         are    slack variables    

- 

+ + 

+ + 
- 
- - 
- 
- 
- 

- 
- 
- 
+ + 

+ 

- 

- 
- - 
- 
- 

- 
+ 
- 

+ 

                 = 1 

w 

+ 

              = 1 

support vector machines (id166s) 

question: what if data isn   t perfectly linearly separable? 
replace    # mistakes    with upper bound called    hinge loss    

input: s={(x1,     1),    ,(xm,     m)};  

find 

2

+                
argminw,    1,   ,              
      for all i,                               1              

    

 s.t.: 

             0 

         are    slack variables    

c controls the relative weighting between the 
twin goals of making the      
 small (margin is 
large) and ensuring that most examples have 
functional margin     1. 
 

2

              =    1 

- 
+ + 

+ 

- 
+ 
- 

w 

+ 

              = 1 

- 

- 
- - 
- 
- 

         ,     ,      = max (0,1                       ) 

support vector machines (id166s) 

question: what if data isn   t perfectly linearly separable? 
replace    # mistakes    with upper bound called    hinge loss    

              =    1 

input: s={(x1,     1),    ,(xm,     m)};  

find 

2

+                
argminw,    1,   ,              
      for all i,                               1              

    

 s.t.: 

             0 

replace the number of mistakes with 
the hinge loss 

2

    

+     (# misclassifications)  

- 

- 
- - 
- 
- 

- 
+ + 

+ 

- 
+ 
- 

w 

+ 

              = 1 

         ,     ,      = max (0,1                       ) 

support vector machines (id166s) 

question: what if data isn   t perfectly linearly separable? 
replace    # mistakes    with upper bound called    hinge loss    

              =    1 

input: s={(x1,     1),    ,(xm,     m)};  

find 

2

+                
argminw,    1,   ,              
      for all i,                               1              

    

 s.t.: 

             0 

- 

- 
- - 
- 
- 

- 
+ + 

+ 

- 
+ 
- 

w 

+ 

              = 1 

total amount have to move the points to get them 
on the correct side of the lines               = +1/   1, 
where the distance between the lines               = 0 and 
              = 1 counts as    1 unit   .  

         ,     ,      = max (0,1                       ) 

what if the data is far from being 
linearly separable? 

example: 

vs 

no good linear 
separator in pixel 
representation. 
 

id166 philosophy:    use a kernel     

support vector machines (id166s) 

input: s={(x1,     1),    ,(xm,     m)};  

find 

2

+                
argminw,    1,   ,              
      for all i,                               1              

    

 s.t.: 

             0 

which is equivalent to: 

input: s={(x1, y1),    ,(xm, ym)};  

find 

    yiyj   i  jxi     xj         i

i

primal 
form 

lagrangian 
dual 

 s.t.: 

1
2

argmin  
      for all i,  

i

j

0       i     ci 

  yi  i = 0
i

 

id166s (lagrangian dual) 

input: s={(x1, y1),    ,(xm, ym)};  

find 

    yiyj   i  jxi     xj         i

 s.t.: 

i

1
2

argmin  
      for all i,  

i

j

0       i     ci 

  yi  i = 0
i

 

              =    1 

+ 

    final classifier is: w =     iyixi

i

  

    the points xi for which   i     0 

are called the    support vectors    

- 

- 
-  - 
- 
- 

- 
+ 

+ 

- 
+ 
- 

w 

+ 

              = 1 

kernelizing the dual id166s  

input: s={(x1, y1),    ,(xm, ym)};  

find 

    yiyj   i  jxi     xj         i

i

1
2

argmin  
      for all i,  

i

j

0       i     ci 

  yi  i = 0
i

 

 s.t.: 

replace xi     xj 
with  k xi, xj . 

    final classifier is: w =     iyixi

i

  

    the points xi for which   i     0 are called the    support vectors    

    with a kernel, classify x using     iyik(x, xi)
 

i

support vector machines (id166s).  

one of the most theoretically well motivated 
and practically most e   ective classi   cation 
algorithms in machine learning. 

directly  motivated by margins and kernels! 

what you should know 

    the importance of margins in machine learning. 

    the primal form of the id166 optimization problem 

    the dual form of the id166 optimization problem.  

   

kernelizing id166. 

    think about how it   s related to regularized logistic 

regression. 

modern (partially) supervised 
machine learning 

    using unlabeled data and 
interaction for learning 

classic paradigm insufficient nowadays 

modern applications: massive  amounts  of raw data. 

only a tiny fraction can be annotated by human experts. 

protein sequences 

billions of webpages 

images 

semi-supervised learning 

raw data 

face   

not face 

labeled data 

expert  
labeler 

classifier 

active learning 

raw data 

face   

not face   

o 

o 

o 

expert  
labeler 

classifier 

semi-supervised learning 

prominent paradigm in past 15 years in machine learning. 

 

    most applications have lots of unlabeled data, but 

labeled data is rare or expensive: 

    web page, document classification 
    id161 
    computational biology,  
       . 

semi-supervised learning 

data source 

learning 
algorithm 

unlabeled 
examples 

expert / oracle 

unlabeled 
examples 

                 labeled examples   

algorithm outputs a classifier  

 sl={(x1, y1),    ,(xml, yml)}  
xi drawn i.i.d from d, yi = c   (xi) 

 su={x1,    ,xmu} drawn i.i.d from d 

 goal:  h has small error over d. 

 errd h = pr
x~ d

(h x     c   (x)) 

semi-supervised learning: no querying. just have lots of 
additional unlabeled data. 

a bit puzzling since unclear what unlabeled data can 
do for you   . 

key insight 

unlabeled data useful if we have beliefs not only about 
the form of the target, but also about its relationship 

with the underlying distribution. 

combining labeled and unlabeled data 

    several methods have been developed to try to use 

unlabeled data to improve performance, e.g.: 
    transductive id166 [joachims    99] 
    co-training [blum & mitchell    98] 
    graph-based methods [b&c01], [zgl03] 

 

 

test of 

time awards 

at icml! 

workshops  [icml    03, icml    05,    ] 

books: 

    semi-supervised learning, mit 2006 

o. chapelle, b. scholkopf and a. zien (eds)  

    introduction to semi-supervised learning, 

morgan & claypool, 2009  zhu  & goldberg 

example of    typical    assumption: margins 

    the separator goes through low density regions of 

the space/large margin. 
    assume we are looking for linear separator 
    belief: should exist one with large separation 

+ 

+ 

_ 

_ 

+ 

+ 

_ 

_ 

+ 

+ 

_ 

_ 

id166 

labeled data only 

transductive id166 

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. [joachims    99] 

0 

0 

0 

0 

0 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

0 

0 

0 

argminw  w
  

2

 s.t.: 

    yi w     xi     1, for all i     {1,     , ml}  

0 

0 

0 

                 =    1 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
w    
0 
+ 
+ 
+ + 

0 
0 

0 
0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
0 

0 

0 

0 
0 

0 

+ 

0 

0 

                 = 1 

0 

- 
- 

0 

0 

0 

- 
- 

0 
0 

0 

0 

0 

0 

0 

0 

0 

- 

0 

0 

0 

0 

0 

0 

0 

0 
0 

- 

0 

0 

- 

0 

0 

0 

0 

   

   

 yu w     xu     1, for all u     {1,     , mu}  

 yu      {   1, 1} for all u     {1,     , mu}   

find a labeling of the unlabeled sample and      s.t.      separates both 
labeled and unlabeled data with maximum margin. 

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. [joachims    99] 

0 

0 

0 

0 

0 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

0 

0 

0 

2

argminw  w

+                

 
 +                 

 

- 
    yi w     xi     1-        , for all i     {1,     , ml}  

    

0 

0 

    

0 

0 

                 =    1 

0 

0 

0 

0 

- 

0 

0 

0 

0 

0 

- 
- 

0 
0 

0 

0 

0 

0 
0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
w    
0 
+ 
+ 
+ + 

0 
0 

0 
0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
0 

0 

0 

0 
0 

0 

+ 

0 

0 

                 = 1 

0 

0 

- 

0 

0 

0 

0 

0 

0 

- 
- 

0 

0 

0 

   

   

 yi  w     xu     1              
 yu      {   1, 1} for all u     {1,     , mu}   

  , for all u     {1,     , mu}  

find a labeling of the unlabeled sample and      s.t.      separates both 
labeled and unlabeled data with maximum margin. 

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

argminw  w

2

+                

    

 
 +                 

    

 

    yi w     xi     1-        , for all i     {1,     , ml}  

   

   

 yi  w     xu     1              
 yu      {   1, 1} for all u     {1,     , mu}   

  , for all u     {1,     , mu}  

np-hard   .. convex only after you guessed the labels    too many 
possible guesses    

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. 

 heuristic (joachims) high level idea: 

    first maximize margin over the labeled points 

    use this to give initial labels to unlabeled points 

based on this separator. 

    try flipping labels of unlabeled points to see if doing 

so can increase margin 

keep going until no more improvements. finds a locally-optimal solution. 

experiments [joachims99] 

transductive support vector machines 

helpful distribution 

non-helpful distributions 

+ 

highly compatible 

+ 

_ 

_ 

1/  2 clusters, 
all partitions  
separable by 
large margin 

semi-supervised learning 

prominent paradigm in past 15 years in machine learning. 

 

key insight 

unlabeled data useful if we have beliefs not only about 
the form of the target, but also about its relationship 

with the underlying distribution. 

prominent techniques 

 

 

    transductive id166 [joachims    99] 
    co-training [blum & mitchell    98] 
    graph-based methods [b&c01], [zgl03] 

 

