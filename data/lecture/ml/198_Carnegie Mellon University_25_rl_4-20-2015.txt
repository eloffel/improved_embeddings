id23 

maria-florina balcan 

carnegie mellon university 

 

april 20, 2015 

today: 
 
learning of control policies 
   
    id100 
    temporal difference learning 
    id24 

readings: 

    mitchell, chapter 13 

    kaelbling, et al., reinforcement 
learning: a survey 

 

slides courtesy: tom mitchell 

tom mitchell, april 2011 

overview 

    different from ml pbs so far: decisions we make will be 

about actions to take, such as a robot deciding which way 
to move next, which will influence what we see next.   

 
    our decisions influence the next example we see.  
 
    goal will be not just to predict (say, whether there is a 

door in front of us or not) but to decide what to do.  

 
    model: id100.  

tom mitchell, april 2011 

id23 
main impact of our actions will not come right away but instead that will 
only come later.   

[sutton and barto 1981; samuel 1957; ...] 

tom mitchell, april 2011 

...]r  r   e[r(s)v2t21tt*                  id23: backgammon 

learning task:  

    chose move at arbitrary board states 

 

training signal:  

final win or loss at the end of the game 

   

 

training: 

    played 300,000 games against itself 

 

algorithm: 

id23 + neural network 

   

 

result: 

    world-class backgammon player 

[tessauro, 1995] 

tom mitchell, april 2011 

outline 

    learning control strategies 

    credit assignment and delayed reward 

    discounted rewards 

 

    id100 

    solving a known mdp 

 

    online learning of control strategies 

    when next-state function is known: value function v*(s) 
    when next-state function unknown: learning q*(s,a) 

 

    role in modeling reward learning in animals 

 

tom mitchell, april 2011 

agent lives in some 
environment; in some 
state:  
    robot: where robot 

is, what direction it is 
pointing, etc.   

    backgammon, state 
of the board (where 
all pieces are). 

goal: maximize long 
term discounted reward. 
i.e.: want a lot of reward, 
prefer getting it earlier to 
getting it later. 

tom mitchell, april 2011 

markov decision process = id23 setting 

    set of states s 

    set of actions a 
    at each time, agent observes state st     s, then chooses action at     a 
    then receives reward rt , and state changes to st+1 
    markov assumption: p(st+1 | st, at, st-1, at-1, ...) = p(st+1 | st, at) 
    also assume reward markov:   p(rt | st, at, st-1, at-1,...) = p(rt | st, at) 
 

e.g., if tell robot to move forward one meter, maybe it ends up moving forward 1.5 meters by 
mistake, so where the robot is at time t+1 can be a probabilistic function of where it was at 
time t and the action taken, but shouldn   t depend on how we got to that state. 

 
    the task: learn a policy    : s     a for choosing actions that maximizes 

for every possible starting state s0 

tom mitchell, april 2011 

id23 task for autonomous agent 

execute actions in environment, observe results, and 
    learn control policy    : s   a that maximizes                

from every state s     s 

 

example: robot grid world, deterministic reward r(s,a) 

 

    actions: move up, down, left, and right  
[except when you are in the top-right you stay there, and say any action that 
bumps you into a wall leaves you were you were]]  
 
   

reward fns r(s,a) is deterministic with reward 100 
for entering the top-right and 0 everywhere else. 

tom mitchell, april 2011 

id23 task for autonomous agent 

execute actions in environment, observe results, and 
    learn control policy    : s   a that maximizes                

from every state s     s 

 

yikes!! 
    function to be learned is    : s   a  

    but training examples are not of the form <s, a> 

    they are instead of the form < <s,a>, r > 

 

tom mitchell, april 2011 

value function for each policy 

    given a policy     : s     a, define  

 

 

 

assuming action sequence chosen 
according to    , starting at state s 

expected discounted reward we will get starting from state s if we follow policy   .  

    goal: find the optimal policy    * where 

 

 

policy whose value function is 
the maximum out of all policies 
simultaneously for all states 

    for any mdp, such a policy exists! 
    we   ll abbreviate v    *(s) as v*(s) 
    note if we have v*(s) and p(st+1|st,a), we can compute 

   *(s)     

tom mitchell, april 2011 

value function     what are the v   (s) values? 

tom mitchell, april 2011 

value function     what are the v   (s) values? 

tom mitchell, april 2011 

value function     what are the v*(s) values? 

tom mitchell, april 2011 

immediate rewards r(s,a) 

state values v*(s) 

tom mitchell, april 2011 

recursive definition for v*(s) 

assuming actions are 
chosen according to the 
optimal policy,    * 

value        (    1) of performing optimal policy from     1, is expected reward of the first action     1 
taken plus      times the expected value, over states     2 reached by performing action     1 
from     1, of the value        (    2) of performing the optimal policy from then on.  

optimal value of any state s is the expected reward of performing        (    ) from s plus 
     times the expected value, over states s    reached by performing that action from state s, 
of the optimal value of s   . 

tom mitchell, april 2011 

value iteration for learning v* : assumes p(st+1|st, a) known 

initialize v(s) to 0 

[optimal value can get in zero steps] 

for t=1, 2,     [loop until policy good enough] 

   loop for s in s 

loop for a in a 

inductively,  if v is optimal discounted  reward can get in t-1 steps, 
q(s,a) is value of performing action a from state s and then being 
optimal from then on for the next t-1 steps.  

       

  

   end loop 

end loop 

optimal expected discounted reward can 
get by taking an action and then being 
optimal for t-1 steps= optimal expected 
discounted reward can get in t steps. 

v(s) converges to v*(s) 

id145 

tom mitchell, april 2011 

value iteration for learning v* : assumes p(st+1|st, a) known 

initialize v(s) to 0 

[optimal value can get in zero steps] 

for t=1, 2,     [loop until policy good enough] 

   loop for s in s 

loop for a in a 

each round we are computing the value of performing the optimal t-step policy starting 
from t=0, then t=1, t=2, etc, and since          goes to 0, once t is large enough this will be 
close to the optimal value         for the infinite-horizon case. 

       

  

   end loop 

end loop 

v(s) converges to v*(s) 

id145 

tom mitchell, april 2011 

value iteration for learning v* : assumes p(st+1|st, a) known 

initialize v(s) to 0 

[optimal value can get in zero steps] 

for t=1, 2,     [loop until policy good enough] 

   loop for s in s 

loop for a in a 

       

  

   end loop 

end loop 

    round t=0 we have v(s)=0 for all s.  
    after round t=1, a top-row of 0, 100, 0 and a 

   

   

bottom-row of 0, 0, 100.  
 after the next round (t=2), a top row of 90, 100, 
0 and a bottom row of 0, 90, 100.  
 after the next round (t=3) we will have a top-row 
of 90, 100, 0 and a bottom row of 81, 90, 100, 
and it will then stay there forever 

tom mitchell, april 2011 

value iteration 

so far, in our dp, each round we cycled through each state exactly once. 

interestingly, value iteration works even if we randomly traverse the 

environment instead of looping through each state and action 
methodically  

    but we must still visit each state infinitely often on an infinite run 

    for details: [bertsekas 1989] 

   

implications: online learning as agent randomly roams 

 

if for our dp,  max (over states) difference between two successive 

value function estimates is less than    , then the value of the greedy 
policy differs from the optimal policy by no more than  

tom mitchell, april 2011 

so far: learning optimal policy when we 

know p(st | st-1, at-1) 

 

what if we don   t? 

tom mitchell, april 2011 

define new function, closely related to v* 

id24 

v*(s) is the expected discounted reward of following the optimal policy from time 0 onward.  

q(s,a) is the expected discounted reward of first doing action a and then following the optimal 
policy from the next step onward.  

if agent knows q(s,a), it can choose optimal action 
without knowing p(st+1|st,a)  ! 

just chose the action that maximizes the q value 

and, it can learn q without knowing p(st+1|st,a) 

using something very much like the id145 algorithm we used to compute v*. 

tom mitchell, april 2011 

immediate rewards r(s,a) 

state values v*(s) 

state-action values q*(s,a) 

bellman equation.   

consider first the case where 
p(s   | s,a) is deterministic 

tom mitchell, april 2011 

[simplicity assume the transitions and rewards are deterministic. ] 

optimal value of a state s is the 
maximum, over actions a    of q(s,a   ).  

given current approx        to q, if we are 
in state s and perform action a and get 
to state s   , update our estimate      (    ,     ) 
to the reward r we got plus gamma 
times the maximum over a    of      (       ,        )  

tom mitchell, april 2011 

tom mitchell, april 2011 

tom mitchell, april 2011 

use general fact: 

tom mitchell, april 2011 

rather than replacing the old estimate with the new estimate, you want to compute a weighted average of 
them: (1           ) times your old estimate plus        times your new estimate.   this way you average out the 
probabilistic fluctuations, and one can show that this still converges. 

tom mitchell, april 2011 

mdp   s and rl: what you should know 

    learning to choose optimal actions a 

    from delayed reward 

    by learning evaluation functions like v(s), q(s,a) 

 

key ideas: 

   

   

if next state function st x at     st+1 is known 
    can use id145 to learn v(s) 
    once learned, choose action at that maximizes v(st+1) 
if next state function st x at     st+1 unknown 
    learn q(st,at) = e[v(st+1)] 
    to learn, sample st x at     st+1 in actual world 
    once learned, choose action at that maximizes q(st,at) 

tom mitchell, april 2011 

