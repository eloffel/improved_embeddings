sample complexity for function 
approximation. model selection. 

maria-florina (nina) balcan 

february 16th, 2015 

structural risk minimization 

sample complex. 

   

two core aspects of machine learning 

algorithm design. how to optimize? 

computation 

automatically generate rules that do well on observed data. 

    e.g.: id28, id166, adaboost, etc. 

confidence bounds, generalization 

(labeled) data 

confidence for rule effectiveness on future data. 

pac/slt models for supervised classification 

data 
source 

distribution d on x 

expert / oracle 

learning 
algorithm 

   labeled examples   

(x1,c*(x1)),   , (xm,c*(xm)) 

alg.outputs 

c* : x ! y 

h : x ! y 

x1 > 5 

x6 > 2 

+1 

+1 

-1 

+ 
+ 
+ 

+ 

- 

- 
- 

- 
- 
- 

pac/slt models for supervised learning 

    x     feature/instance space; distribution d over x 
           e.g., x = rd or x = {0,1}d 
    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi i.i.d. from d 
    labeled examples - drawn i.i.d. from d and labeled by target c* 
     labels 2 {-1,1}  - binary classification 

     algo does optimization over s, find hypothesis    . 

     goal:  h has small error over d. 

                      = pr
    ~     

(                    (    )) 

    fix hypothesis space h  [whose complexity is not too large] 

    realizable:                 .  

    agnostic:            close to    h.  

h 

+
+
  
  

c* 
-
-
-
  
-
  
  
instance space x 
  

+
  +
  

 sample complexity for supervised learning 

consistent learner 

   

input: s: (x1,c*(x1)),   , (xm,c*(xm)) 

 

realizable case 
 
 

     output: find h in h consistent with s (if one exits).  

prob. over different 
samples of m 
training examples 

linear in 1/     

sample complexity: infinite hypothesis spaces  

realizable case 

 
 

 

 

 
 
 
 
  
 

e.g., h= linear separators in rd 

vcdim(h)= d+1 

+ 

+ 
+ 
+ 

- 

- 
- 

- 
- 
- 

sample complexity linear in d 

so, if double the number of features, then i only need 
roughly twice the number of samples to do well. 

what if c        h? 

sample complexity:  uniform convergence 

agnostic case 

empirical risk minimization (erm) 

   

input: s: (x1,c*(x1)),   , (xm,c*(xm)) 

     output: find h in h with smallest errs(h) 

 

 

 
 
 
 

 
 
 
 
  
 

1/    2 dependence [as opposed 
to1/     for realizable] 

hoeffding bounds 

 consider coin of bias p flipped m times.   

let n be the observed # heads.  clearly e

n
m

= p. 

[n = x1 + x2 +       + xm, xi = 1 with prob. p, 0 with prob 1-p.] 

hoeffding inequality 
 let          [0,1]. 
    
    

      

                              2        2

 

exponentially decreasing tails 

              

     

     +      

tail inequality: bound id203 mass in tail of distribution (how 
concentrated is a random variable around its expectation). 

sample complexity:  finite hypothesis spaces 

agnostic case 

 
 
 
 

 

 

hoeffding & union bound. 

proof: 
    fix h; by hoeffding, prob. that  errs h     errd h        is at most 2e   2m  2
    by union bound over all             , the prob. that    h s.t.  errs h     errd h        is 

 
 
 
 
  
.  set to    . solve. 
 

at most 2|h|e   2m  2

 

fact:  
w.h.p.     1         ,errd          errd h    + 2  , 
h  is erm output, h    is hyp. of smallest 
true error rate.  

errd h     

errs h     

         

errs       

errd       

         

sample complexity:  finite hypothesis spaces 

1) how many examples suffice to get uc whp (so success for erm).  

 
 

agnostic case 

1/    2 dependence [as opposed to 1/     
for realizable], but get for 
something stronger. 

 

 

 
 
 
 
  
 

2) statistical learning theory style: 

with prob. at least 1         , for all h     h: 

1
    

  as opposed to  1
    

  for 

realizable 

errd h     errs h +

1
2m

ln (2 h ) + ln

1
    

. 

sample complexity:  infinite hypothesis spaces 

1) how many examples suffice to get uc whp (so success for erm).  

 
 

agnostic case 

 

 

 
 
 
 
  
 

2) statistical learning theory style: 

with prob. at least 1         , for all h     h: 

errd h     errs h + o

1
2m

vcdim h ln

em

vcdim h

+ ln

1
  

. 

vcdimension generalization bounds 

e.g., 

errd h     errs h + o

1
2m

vcdim h ln

em

vcdim h

+ ln

1
  

. 

vc bounds: distribution independent bounds  

    generic: hold for any concept class and any distribution. 

[nearly tight in the wc over choice of d] 

    might be very loose specific distr. that are more 

benign than the worst case   . 

    hold only for binary classification;  we want bounds for 
fns approximation  in general (e.g., multiclass classification and 
regression). 

rademacher complexity bounds 

[koltchinskii&panchenko 2002] 

    distribution/data dependent. tighter for nice distributions. 

    apply to general classes of real valued functions & can be used to 

recover the vcbounds for supervised classification. 

    prominent technique for generalization bounds in last decade. 

see    introduction to statistical learning theory    
 o. bousquet, s. boucheron, and g. lugosi. 

rademacher complexity 

problem setup 

    a space z and a distr. d|z 
   

 f be a class of functions from z to [0,1] 

   

 s  =   {z1,     , zm} be i.i.d. from  d|z 

 want a high prob. uniform convergence bound, all f     f satisfy: 

ed f z     es f z + term(complexity of f, niceness of d/s) 

e.g.,  z = x    y, y = {   1,1}, 

general discrete y 
  

 h = {h:  x     y} hyp. space (e.g., lin. sep) 

what measure of complexity? 

f = l(h) = {lh:  x     y}, where lh      = x, y = 1 h x           

 then e    ~     lh z = errd(h) and es lh z = errs(h).  

[loss fnc induced by h 
and 0/1 loss] 
  

errd h     errs h + term(complexity of h, niceness of d/s) 

rademacher complexity 

space z and a distr. d|z; f be a class of functions from z to [0,1] 
let s  =   {z1,     , zm} be i.i.d from  d|z.  

the empirical rademacher complexity of f is:  

r  m(f)   =   e  1,   ,  m sup
f   f

 

1
m

    if zi
i

   

where          are i.i.d. rademacher variables chosen uniformly from {   1,1}.  

the rademacher complexity of f is:  

rm f = es[r  m(f)] 

 sup measures for any given set s and rademacher vector     , 
the max correlation between f zi  and          for all f     f 

so,  taking the expectation over      this measures the ability of 
class f to fit random noise. 

rademacher complexity 

space z and a distr. d|z; f be a class of functions from z to [0,1] 
let s  =   {z1,     , zm} be i.i.d from  d|z.  

the empirical rademacher complexity of f is:  

r  m(f)   =   e  1,   ,  m sup
f   f

 

1
m

    if zi
i

   

where          are i.i.d. rademacher variables chosen uniformly from {   1,1}.  

the rademacher complexity of f is:  

rm f = es[r  m(f)] 

 theorem: 

 whp all f     f satisfy: 

useful if it decays with m. 

ed f z     es f z + 2rm f +

ln 2/  

2m

 

ed f z     es f z + 2 r m f + 3

ln 1/  

m

 

rademacher complexity 

space z and a distr. d|z; f be a class of functions from z to [0,1] 
let s  =   {z1,     , zm} be i.i.d from  d|z.  

the empirical rademacher complexity of f is:  

r  m(f)   =   e  1,   ,  m sup
f   f

 

1
m

    if zi
i

   

where          are i.i.d. rademacher variables chosen uniformly from {   1,1}.  

the rademacher complexity of f is:  
e.g.,: 
1) f={f}, then  r m(f)   = 0 
[linearity of expectation: each   if(zi) individually has expectation 0.] 

rm f = es[r  m(f)] 

2) f={all 0/1 fnc}, then   r m(f)   = 1/2 

[to maximize set f(zi) = 1 when   i = 1 and f(zi) = 0 when   i =    1. then quantity 
inside expectation is #1                , which is m/2 by linearity of expectation.] 

rademacher complexity 

space z and a distr. d|z; f be a class of functions from z to [0,1] 
let s  =   {z1,     , zm} be i.i.d from  d|z.  

the empirical rademacher complexity of f is:  

r  m(f)   =   e  1,   ,  m sup
f   f

 

1
m

    if zi
o

   

where          are i.i.d. rademacher variables chosen uniformly from {   1,1}.  

the rademacher complexity of f is:  
e.g.,: 
1) f={f}, then  r m(f)   = 0 
2) f={all 0/1 fnc}, then   r m(f)   = 1/2 

rm f = es[r  m(f)] 

3) f=l(h), h=binary classifiers then:  rs f    

h finite: 

rs f    

ln 2|h[s]|

m

ln 2|h|

m

 

 

rademacher complexity bounds 
space z and a distr. d|z; f be a class of functions from z to [0,1] 
let s  =   {z1,     , zm} be i.i.d from  d|z.  

the empirical rademacher complexity of f is:  

r  m(f)   =   e  1,   ,  m sup
f   f

 

1
m

    if zi
o

   

where          are i.i.d. rademacher variables chosen uniformly from {   1,1}.  

the rademacher complexity of f is:  

rm f = es[r  m(f)] 

 theorem: 

 whp all f     f satisfy: 

data dependent bound! 

ed f z     es f z + 2rm f +

ln 2/  

2m

 

bound expectation of each f in 
terms of its empirical average 
& the rc of f 

ed f z     es f z + 2 r m f + 3

ln 1/  

m

 

proof uses symmetrization and ghost sample tricks! (same as for vc bound) 

rademacher complex: binary classification 
fact: 

 h = {h:  x     y} hyp. space (e.g., lin. sep)  f= l(h), d=vcdim(h): 

rs f    

ln 2|h[s]|

m

 

so, by sauer   s lemma, rs f    

2dln

em
d

m

 

theorem: for any h, any distr. d, w.h.p.     1          all h     h satisfy: 

errd h     errs h + rm h + 3

ln 2/  

2m

. 

errd h     errs h +

2dln

em
d

m

+  3

ln 2/  

2m

 

generalization bound 

many more uses!!! margin bounds for id166, boosting, 
regression bounds, etc. 

can we use our bounds for 

model selection? 

true error, training error, overfitting 

model selection: trade-off between decreasing training error and 
keeping h simple. 

errd h     errs h + rm h +    

error 

train error 

generalization 

error 

complexity 

structural risk minimization (srm) 

    1         2         3                              

error 
rate 

overfitting 

empirical error 

hypothesis complexity 

what happens if we increase m? 

black curve will stay close to the red curve for 
longer, everything shifts to the right    

structural risk minimization (srm) 

    1         2         3                              

error 
rate 

overfitting 

empirical error 

hypothesis complexity 

structural risk minimization (srm) 

        1         2         3                              

   

   

 h 

k = argminh   hk{errs h } 
as k increases, errs h 

k  goes down but complex. term goes up. 

       = argmink   1{errs h 

k + complexity(hk)} 

output      =     

       

claim: w.h.p., errd h      mink   minh      hk    errd h    + 2complexity hk   
proof: 
    we chose h  s.t. errs h  + complexity hk      errs h    + complexity(hk   ). 
    whp, errd h      errs h  + complexity hk  . 
    whp, errs h        errd h    + complexity hk    . 

 

techniques to handle overfitting 

    structural risk minimization (srm). 

minimize gener. bound: 

      = argmink   1{errs h 

    1         2                              
k + complexity(hk)} 

    often computationally hard   . 
    nice case where it is possible: m. kearns, y. mansour, icml   98,    a fast, bottom-up 

decision tree pruning algorithm with near-optimal generalization     

    id173: 

general family closely related to srm 

    e.g., id166, regularized id28, etc. 
    minimizes expressions of the form: errs h +    h

2

 

some norm when h 
is a vector space; 
e.g., l2 norm 

    cross validation:  

picked through cross validation 

    hold out part of the training data and use it as a proxy for the 

generalization error 

what you should know 

    notion of sample complexity. 

    understand reasoning behind the simple sample 
complexity bound for finite h [exam question!]. 

    shattering, vc dimension as measure of complexity, 
sauer   s lemma, form of the vc bounds (upper and lower 
bounds). 

    rademacher complexity. 

    model selection, structural risk minimization. 

l2 vs. l1 id173 

gaussian p(w) 

    l2 id173 

laplace p(w) 

    l1 id173 

w2 

w1 

constant 
p(data|w) 

w2 

constant p(w) 

w1 

