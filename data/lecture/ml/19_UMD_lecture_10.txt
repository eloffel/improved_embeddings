slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 10 multiclass classification and reductions

furong huang / furongh@cs.umd.edu

topics

given an arbitrary method for binary 
classification, how can we learn to make 
multiclass predictions?

fundamental ml concept:  reductions

one example of reduction:
learning with imbalanced data

subsampling optimality theorem:
if the binary classifier achieves a binary error rate of   , then 
the error rate of the   -weighted classifier is      

another example of reduction:
multiclass classification

reduction 1: ova

   

   one versus all    (aka    one versus rest   )
    train k-many binary classifiers
   

classifier k predicts whether an example belong 
to class k or not

    at test time, 

if only one classifier predicts positive, predict that class

   
    break ties randomly

time complexity

    suppose you have n training 

examples, in k classes. how long does 
it take to train an ova classifier
   

if the base binary classifier takes o(n) time to 
learn?
if the base binary classifier takes o(n^2) time to 
learn?

   

error bound

    theorem: suppose that the average error 

of the k binary classifiers is   , then the 
error rate of the ova multiclass classifier is 
at most (k-1)   

    to prove this: how do different errors affect 
the maximum ratio of the id203 of 
a multiclass error to the number of 
binary errors (   efficiency   )?

error bound proof

1. if we have a false negative on one of the 
binary classifiers (assuming all other 
classifiers correctly output negative) 
what is the id203 that we will make an 
incorrect multiclass prediction?

(k     1) / k

efficiency: [( k     1) / k] / 1 = (k     1 ) / k

error bound proof

2. if we have m false positives with the 
binary classifiers
what is the id203 that we will make 
an incorrect multiclass prediction?

if there is also a false negative: 1

efficiency =1 / (m + 1)

otherwise  m / ( m + 1)

efficiency = [m / (m + 1)] / m = 1 / ( m + 1)

error bound proof

3. what is the worst case scenario?

false negative case: efficiency is (k-1)/k

larger than false positive efficiencies 

there are k-many opportunities to get false 
negative, overall error bound is (k-1)   

reduction 2:  ava

all versus all (aka all pairs)

how many binary classifiers does this 
require?

time complexity

    suppose you have n training 

examples, in k classes. how long does 
it take to train an ava classifier
   

if the base binary classifier takes o(n) time to 
learn?
if the base binary classifier takes o(n^2) time to 
learn?

   

error bound

theorem: suppose that the average 
error of the k binary classifiers is   , then 
the error rate of the ava multiclass 
classifier is at most 2(k-1)   

question: does this mean that ava is 
always worse than ova?

extensions

    divide and conquer

    organize classes into binary tree structures

    use confidence to weight predictions of 

binary classifiers
   

instead of using majority vote

topics

    given an arbitrary method for binary 

classification, how can we learn to make 
multiclass predictions?
    ova, ava

    fundamental ml concept:  reductions

ranking

canonical example: web search

given all the documents on the web
for a user query, retrieve relevant 
documents, ranked from most relevant to 
least relevant

how can we reduce ranking to binary 
classification?

preference function

    given a query q and documents di and 

dj, the preference function outputs 
whether
   
    or dj should be preferred to di

di should be preferred to dj

    that   s a binary classification problem!

specifying the reduction from ranking 
to binary classification

    how to train classifier that predicts 

preferences?

    how to turn the predicted preferences 

into a ranking?

features 

associated with 

comparing 

document j and 
document j for 

query n

na  ve approach

works well for bipartite problems

   is this document relevant or not?   

not ideal for full ranking problems, 
because

binary preference problems are not all equally 
important
separates preference function and sorting

improving on na  ve approach

example of cost functions

resulting ranking algorithms

ranktest

a probabilistic version of the quicksort algorithm

only o(mlog2m) calls to f in expectation

better error bound than na  ve algorithm

(see ciml for theorem)

what you should know

    what are reductions and why they are useful

   

implement, analyze and prove error bounds of 
algorithms for
    weighted binary classification
    multiclass classification (ova, ava)

    !   ranking

    understand algorithms for

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

