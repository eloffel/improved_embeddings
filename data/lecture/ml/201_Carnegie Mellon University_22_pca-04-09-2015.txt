pca, kernel pca, ica 

learning representations. 
id84.  

maria-florina balcan 

04/08/2015 

big & high-dimensional data 

 

  thousands of words/unigrams 
  millions of bigrams,  contextual  
  information 

  features per document =  
 
 
 

    high-dimensions = lot of features 
 
  document classification 
 
 
 
 
 
  surveys - netflix 
 
 
 

 480189 users x 17770 movies 

 

 

big & high-dimensional data 

    high-dimensions = lot of features 
 
  meg brain imaging 
 
 
 

 120 locations x 500 time points  
  x 20 objects 

 
 

  or any high-dimensional image data 
 

    big & high-dimensional data. 

    useful to learn lower dimensional 

representations of the data. 

learning representations 

pca, kernel pca, ica: powerful unsupervised learning 
techniques for extracting hidden (potentially lower 
dimensional) structure from high dimensional datasets. 

useful for: 

    visualization  

    more efficient use of resources  
   (e.g., time, memory, communication) 

    statistical: fewer dimensions     better generalization 

    noise removal (improving data quality) 

    further processing by machine learning algorithms 

principal component analysis (pca) 

what is pca: unsupervised technique for extracting 
variance structure from high dimensional datasets. 

    pca  is an orthogonal projection or transformation  of the data 
into a (possibly lower dimensional) subspace so that the variance 
of the projected data is maximized. 

principal component analysis (pca) 

intrinsically lower dimensional than the 
dimension of the ambient space. 

if we rotate data, again only one 

coordinate is more important. 

only one relevant feature 

both features are relevant  

question: can we transform the features so that we only need to 
preserve one latent feature?  

principal component analysis (pca) 

in case where data  lies on or near a low d-dimensional linear 
subspace, axes of this subspace are an effective representation 
of the data. 

identifying the axes is known as principal components analysis, and 
can be obtained by using classic matrix computation tools (eigen or 
singular value decomposition). 

principal component analysis (pca) 

principal components (pc) are orthogonal directions 
that capture most of the variance in the data. 

    first pc     direction of greatest variability in data. 

 

    projection of data points along first pc 

discriminates data most along any one direction 
(pts are the most spread out when we project the data on 
that direction compared to any other directions). 

quick reminder: 

||v||=1, point xi (d-dimensional vector) 

projection of xi onto v is  v        xi 

 

xi 

v 

v        xi 

principal component analysis (pca) 

principal components (pc) are orthogonal directions 
that capture most of the variance in the data. 

   

1st pc     direction of greatest variability in data. 

xi 

xi     v        xi 

v        xi 

    2nd pc     next orthogonal (uncorrelated) direction 

of greatest variability 

(remove all variability in first direction, then find next direction of 
greatest variability) 

    and so on     

principal component analysis (pca) 

let v1, v2,    , vd denote the d principal components. 

vi       vj   = 0, i      j 

and vi       vi   = 1,

 i =  j 

assume data is centered (we extracted the sample mean). 

let x = [x1, x2,     , xn] (columns are the datapoints) 

find vector that maximizes sample variance of projected data 

wrap constraints into the  

objective function 

principal component analysis (pca) 

x xt v =   v , so v (the first pc) is the eigenvector 
of sample correlation/covariance matrix               

sample variance of projection v                 v =     v    v =      

thus, the eigenvalue      denotes the amount of variability 
captured along that dimension (aka amount of energy along that 
dimension). 

eigenvalues     1         2         3         

    the 1st pc     1 is the the eigenvector of the sample covariance matrix 

              associated with the largest eigenvalue  

    the 2nd pc     2 is the the eigenvector of the sample covariance 

matrix               associated with the second largest eigenvalue  

    and so on     

principal component analysis (pca) 
    so, the new axes are the eigenvectors of the matrix of sample 

correlations               of the data. 

 
    transformed features are uncorrelated. 

x2 

    geometrically: centering followed by rotation. 

x1 

    linear transformation 

key computation: eigendecomposition of              (closely related 
to svd of     ). 

two interpretations 

so far: maximum variance subspace. pca finds vectors v such that 
projections on to the  vectors capture maximum variance in the data 

alternative viewpoint:  minimum reconstruction error. pca 
finds vectors v such that projection on to the vectors yields 
minimum mse reconstruction  

xi 

v 

v        xi 

two interpretations 

e.g., for the first component. 

maximum variance direction:  1st pc a vector v such that projection 
on to this vector capture maximum variance in the data (out of all 
possible one dimensional projections) 

minimum reconstruction error: 1st pc a vector v such that 
projection on to this vector yields minimum mse reconstruction  

xi 

v 

v        xi 

why? pythagorean theorem 

e.g., for the first component. 

maximum variance direction:  1st pc a vector v such that projection 
on to this vector capture maximum variance in the data (out of all 
possible one dimensional projections) 

minimum reconstruction error: 1st pc a vector v such that 
projection on to this vector yields minimum mse reconstruction  

blue2 + green2 = black2 

black2 is fixed (it   s just the data) 

so, maximizing blue2 is 
equivalent to minimizing green2  

xi 

v 

v        xi 

id84 using pca 

the eigenvalue      denotes the amount of variability captured along 
that dimension (aka amount of energy along that dimension). 

zero eigenvalues indicate no variability along those directions => 
data lies exactly on a linear subspace 

only keep data projections onto principal components with 
non-zero eigenvalues, say v1,     , vk, where k=rank(             ) 

 

 

original representation 

 

data point 

1,     ,         

    ) 

         = (        
 

 

transformed representation 

 

projection 

(    1             ,     ,                      ) 

 

d-dimensional vector 

d-dimensional vector 

xi 

v 

vtxi 

id84 using pca 

in high-dimensional problems, data sometimes lies near a linear 
subspace, as noise introduces small variability 

 

only keep data projections onto principal components with large 
eigenvalues  

can ignore the components of smaller significance.  

 

might lose some info, but if eigenvalues are small, do not lose much 

 

 

0510152025pc1pc2pc3pc4pc5pc6pc7pc8pc9pc10variance (%) 
can represent a face image using just 15 numbers!  

    pca provably useful before doing id116 id91 and also 

empirically useful. e.g., 

pca discussion 

strengths 

eigenvector method 

no tuning of the parameters 

no local optima 

weaknesses 

limited to second order statistics 

limited to linear projections 

21 

kernel pca (kernel principal 

component analysis) 

useful when data  lies on or near a low d-
dimensional linear subspace of the     -
space associated with a kernel 

properties of pca 

    given a set of      centered observations 

                     ,  1st pc is the direction that 
maximizes the variance 

        1 =                               =1

         =     1,     2,     ,           
1
    
1
    

=                               =1

                 

    

2

 

                       

    covariance matrix      =

1
    

            

        1 can be found by solving the 

eigenvalue problem: 

            1 =         1(of maximum     ) 

properties of pca 

    given a set of      centered observations 

                     ,  1st pc is the direction that 
maximizes the variance 

        1 =                               =1

         =     1,     2,     ,           
1
    
1
    

=                               =1

                 

    

2

 

                       

    covariance matrix      =
    covariance matrix      =
the (i,j) entry of             is the correlation of the i-th coordinate 
ofexamples with jth coordinate of examples 

           is a dxd matrix 
            

1
1
    
    

    to use kernels, need to use the inner-product matrix             . 

alternative expression for pca 

 

    the principal component lies in the span of the data 

    1 =                   

=          

    

why? 1st pc is direction of largest variance, and for any direction outside of 
the span of the data, only get more variance if we project that direction into 
the span.  

    plug this in we have 

        1 =

1
    

                    =                

    now, left-multiply the lhs and rhs by         . 

1
    

                           =                     

 
 

only depends on 
the inner product     

matrix  

kernel pca  

    key idea: replace inner product matrix by kernel matrix 

pca: 1
    

                           =                     

let      =              ,          
in the     -space. 

        

 be the matrix of all dot-products 

kernel pca: replace                    with     . 
             =             , or equivalently,  1
    

1
    

         =           

    key computation: form an      by       kernel matrix     , and 

then perform eigen-decomposition on     .  

kernel pca example 

    gaussian rbf kernel exp    

               2
2    2

 over 2 dimensional space 

    eigenvector evaluated at a test point      is a function 

                 =            <              

    

,           > =

              (        ,     )

    

 

27 

what you should know 

    principal component analysis  (pca) 

    what pca is, what is useful for. 

    both the maximum variance subspace and the 

minimum reconstruction error viewpoint. 

    kernel pca 

additional material on computing the principal 
components and ica 

power method for computing pcs 

given matrix                        , compute the top eigenvector of                

initialize with random                        

repeat 

v      x xtv  
v      v /||v || 

claim 

for any      > 0, whp over choice of initial vector, after     
iterations, we have                                1              1. 

1
    

log

    
    

 

then can subtract the       component off of each example and 
repeat to get the next. 

eigendecomposition 

any symmetric matrix      =              is guaranteed to have an 
eigendecomposition with real eigenvalues:      =                 . 

= 

    1 

    2 

0 

0 
    3 

    

    
=                           

    

  

a 

(dxd) 

v 

(dxd) 

   

(dxd) 

         
(dxd) 

matrix    is diagonal with eigenvalues     1         2         on the 
diagonal.  matrix v has the eigenvectors as the columns. 

singular value decomposition (svd) 

eigendecomp of              is closely related to svd of     . 

given a matrix                        , the svd is a decomposition:          =                  

    1 
0 

    2 

0 
    

     

(            ) 

         
(            ) 

    
=                           

    

  

= 

         
(            ) 

     

(            ) 

         is a diagonal matrix with the singular values     1,     ,          of     . 

    columns of     ,      are orthogonal, unit length. 
    so,              =                                  =         2         = eigendecomposition of             . 

so,          =         

2 and can read off the solution from the svd. 

singular value decomposition (svd) 

eigendecomp of              is closely related to svd of     . 

given a matrix                        , the svd is a decomposition:          =                  

    1 
0 

    2 

0 
    

     

(            ) 

         
(            ) 

    
=                           

    

  

= 

         
(            ) 

     

(            ) 

    in fact, can view the rows of          as the coordinates of 

each example along the axes given by the      eigenvectors. 

so,          =         

2 and can read off the solution from the svd. 

independent component analysis (ica) 

find a linear transformation 

     =               

for which coefficients      =     1,     2,     ,         
statistically independent 

     are 

         1,     2,     ,          =     1     1     2     2                        

algorithmically, we need to identify matrix v and coefficients s, 
s.t. under the condition      =                   the mutual information 
between     1,     2,     ,          is minimized: 

    

         1,     2,     ,          =                               1,     2,     ,         

 

    =1

pca finds directions of maximum variation,  
ica would find directions most    aligned    with data. 

