boosting approach to ml 

id88, margins, kernels 

maria-florina balcan 

03/18/2015 

recap from last time: boosting 

    general method for improving the accuracy of any given 

learning algorithm. 

    works by creating a series of challenge datasets s.t. even 

modest performance on these can be used to produce an 
overall high-accuracy predictor. 

    adaboost one of the top 10 ml algorithms. 

    works amazingly well in practice. 

    backed up by solid foundations.  

adaboost (adaptive boosting) 

input: s={(x1,     1),    ,(xm,     m)};  

xi         ,                   = {   1,1} 

weak learning algo a  (e.g., na  ve bayes, decision stumps) 
    for t=1,2,     ,t 
 

    construct dt on {x1,    , xm} 

    run a on dt producing ht:          {   1,1} 

output hfinal      = sign  

    =1

                    

 

 d1 uniform on {x1,    , xm} 

   
    given dt and ht set 

[i.e., d1      =

1
    

] 

ht 

+ 

+ 

+ 

+ 

+ 
+ 

+ 

+ 

- 
- 

- 
- 

- 
- 
- 
- 

        +1      =
         +1      =
 

             
        
             
        

 e                if          =                    

 e             if                                 

         =

1
2

ln

1             

        

> 0 

        +1      =

             
        

 e                                     

   

dt+1 puts half of weight on examples 
xi  where  ht  is  incorrect  &  half  on 
examples where ht is correct  

nice features of adaboost 

    very general: a meta-procedure, it can use any weak 

learning algorithm!!!  

(e.g., na  ve bayes, decision stumps) 

    very fast (single pass through data each round) & simple 

to code, no parameters to tune. 

    grounded in rich theory. 

analyzing training error 

theorem          = 1/2              (error of         over         ) 

2
                                              exp     2            

 

    

so, if         ,                   > 0, then                                               exp     2     2      

the training error drops exponentially in t!!! 

to get                                                   , need only      =     

1
    2 log

1
    

 rounds  

adaboost is adaptive 

    does not need to know      or t a priori 
    can exploit                      

generalization guarantees 

theorem 

2
                                              exp     2            

 

where          = 1/2              

    

how about generalization guarantees? 
  original analysis [freund&schapire   97]  

    h space of weak hypotheses; d=vcdim(h) 

                          is a weighted vote, so the hypothesis class is:  

g={all fns of the form sign( 

    
    =1

               (    )) 

} 

theorem [freund&schapire   97]  
                 ,                                             +      

        
    

    t= # of rounds 

key reason: vcd              =                 plus typical vc bounds. 

generalization guarantees 
theorem [freund&schapire   97]  

                 ,                                             +      

        
    

    

where d=vcdim(h) 

error 

train error 

generalization 

error 

complexity 

t= # of rounds 

generalization guarantees 

    experiments showed that the test error of the generated 

classifier usually does not increase as its size becomes 
very large. 

    experiments showed that continuing to add new weak 

learners after correct classification of the training set had 
been achieved could further improve test set performance!!! 

generalization guarantees 

    experiments showed that the test error of the generated 

classifier usually does not increase as its size becomes 
very large. 

    experiments showed that continuing to add new weak 

learners after correct classification of the training set had 
been achieved could further improve test set performance!!! 

    these results seem to contradict fs   97 bound and occam   s 

razor (in order achieve good test error the classifier should be as 
simple as possible)! 

                 ,                                             +      

        
    

    

how can we explain the experiments? 

r. schapire, y. freund, p. bartlett, w. s. lee. present in 
   boosting the margin: a new explanation for the effectiveness 
of voting methods    a nice theoretical explanation. 

key idea: 

training error does not tell the whole story.  

we need also to consider the classification confidence!! 

boosting didn   t seem 

to overfit   (!) 

   because it turned out to be 
increasing the margin of the 

classifier 

test error of base classifier 

(weak learner) 

test 
error 

train 
error 

error curve, margin distr. graph - plots from [sfbl98] 

classification margin 
    h space of weak hypotheses. the convex hull of h: 

    
               =      =  
    =1

               

    
,              0,  
    =1

         = 1,                 

 

    
    let                        ,      =  
    =1

               ,

    
             0,   
    =1

         = 1

.  

the majority vote rule          given by      (given by          =                 (          ))  
predicts wrongly on example (    ,     ) iff                   0. 

definition: margin of          (or of     ) on example (    ,     ) to be         (    ).  

    

    

              =                             =                            =                          

 

    =1

    =1

    :    =            

    :                   

the margin is positive iff      =               .  
see                 = |          | as the strength or the confidence of the vote. 

-1 

low confidence 

high confidence, 
incorrect 

1 

high confidence, 
correct 

boosting and margins 

theorem:vcdim(    ) =     , then with prob.     1         ,                     (    ),         > 0, 

pr
    

                  0     pr
    

                       +     

1
    

 

d ln2    
    
    2 + ln

1
    

  

note:  bound does not depend on  t (the # of rounds of boosting), 
depends only on the complex. of the weak hyp space and the margin! 

boosting and margins 

theorem:vcdim(    ) =     , then with prob.     1         ,                     (    ),         > 0, 

pr
    

                  0     pr
    

                       +     

1
    

 

d ln2    
    
    2 + ln

1
    

  

    if all training examples have large margins, then we can 

approximate the final classifier by a much smaller classifier. 

    can use this to prove that better margin     smaller test error, 

regardless of the number of weak classifiers. 

    can also prove that boosting tends to increase the margin of 

training examples by concentrating on those of smallest margin. 

    although final classifier is getting larger, 
margins are likely to be increasing, so the 
final classifier is actually getting closer to a 
simpler classifier, driving down test error. 

boosting and margins 

theorem:vcdim(    ) =     , then with prob.     1         ,                     (    ),         > 0, 

pr
    

                  0     pr
    

                       +     

1
    

 

d ln2    
    
    2 + ln

1
    

  

note:  bound does not depend on  t (the # of rounds of boosting), 
depends only on the complex. of the weak hyp space and the margin! 

boosting, adaboost summary 

    shift in mindset: goal is now just to find classifiers a 

bit better than random guessing. 

    backed up by solid foundations. 

    adaboost work and its variations well in practice with 

many kinds of data (one of the top 10 ml algos). 

    more about classic applications in recitation. 

    relevant for big data age: quickly focuses on    core difficulties   , so 

well-suited to distributed settings, where data must be 
communicated efficiently [balcan-blum-fine-mansour colt   12]. 

interestingly, the usefulness of margin 
recognized in machine learning since late 50   s. 

id88 [rosenblatt   57] analyzed via geometric  
(aka     2,     2) margin. 

original guarantee in the online learning scenario. 

the id88 algorithm 

    online learning model 

    margin analysis 

    kernels 

the online learning model 

    example arrive sequentially. 

    we need to make a prediction. 

afterwards observe the outcome. 

for i=1, 2,    , : 

phase i: 

online algorithm 

example          

prediction    (        ) 

observe c   (        ) 

mistake bound model 

    analysis wise, make no distributional assumptions. 

    goal: minimize the number of mistakes. 

the online learning model. motivation 

- email classification (distribution of both spam and regular 
mail changes over time, but the target function stays fixed - 
last year's spam still looks like spam). 

- id126s. recommending movies, etc. 

- predicting whether a user will be interested in a new news 
article or not. 

- add placement in a new market. 

linear separators 

    instance space x = rd 
    hypothesis class of linear decision 

surfaces in rd. 

   

 h x = w      x  + w0, if               0, then 
label x as +, otherwise label it as - 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

o 

o 

o 

o 

o 

o 
w 
o 

o 

claim: wlog w0 = 0. 
proof: can simulate a non-zero threshold with a dummy input 
feature     0 that is always set up to 1. 

         =     1,     ,                    =     1,     ,         , 1  

    w      x  + w0     0 iff      1,     ,         , w0

              0 

where w =     1,     ,           

linear separators: id88 algorithm 

    set t=1, start with the all zero vector     1. 
    given example     , predict positive iff                       0 
    on a mistake, update as follows: 

    mistake on positive, then update         +1              +      

    mistake on negative, then update         +1                       

note:            is weighted sum of incorrectly classified examples 

         =         1        1 +     +                          
                  =         1        1          +     +                                   

important when we talk about kernels. 

id88 algorithm: example 

example:     1,2     
1,0 + 
1,1 + 
   1,0     
   1,    2     
1,    1 + 

x 

    

x 

    

x 

    

- 

- 
- 

+ 
+ 
+ 

algorithm: 

    1 = (0,0) 

    set t=1, start with all-zeroes weight vector     1. 
    given example     , predict positive iff                       0. 

    on a mistake, update as follows:  

    mistake on positive, update         +1              +      
    mistake on negative, update         +1                       

    2 =     1        1,2 = (1,    2) 

    3 =     2 + 1,1 = (2,    1) 

    4 =     3        1,    2 = (3,1) 

geometric margin 

definition: the margin of example      w.r.t. a linear sep.      is 
the distance from      to the plane               = 0  (or the negative if on wrong side) 

margin of positive example     1 

    1 

w 

margin of negative example     2 

    2 

geometric margin 

definition: the margin of example      w.r.t. a linear sep.      is 
the distance from      to the plane               = 0  (or the negative if on wrong side) 

definition: the margin          of a set of examples      wrt a 
linear separator      is the smallest margin over points              . 

+ 

         

         
- 
- 
- 

+ 

+ 

+ 

+ 

w 

+ 

+ 

- 
- 

- 

- 

- 

- 

+ 

geometric margin 

definition: the margin of example      w.r.t. a linear sep.      is 
the distance from      to the plane               = 0 (or the negative if on wrong side) 

definition: the margin          of a set of examples      wrt a 
linear separator      is the smallest margin over points              . 

definition: the margin      of a set of examples      is the 
maximum          over all linear separators     . 

- 

     

     
- 
- 
- 

- 

- 

- 

+ 

+ 

w 

+ 

+ 

- 
- 

+ 

id88: mistake bound 

theorem: if data has margin      and all points inside a ball of 
radius     , then id88 makes         /     2 mistakes. 

(normalized margin: multiplying all points by 100, or dividing all points by 100, 
doesn   t change the number of mistakes; algo is invariant to scaling.) 

+ 

+ 
+ 

+ 

w* 
w* 

+ 

+ 

+ 

r 

    

+ 

    

- 

- 

- 
- 
- 
- 

- 

- 
- 

id88 algorithm: analysis 

theorem: if data has margin      and all 
points inside a ball of radius     , then 
id88 makes         /     2 mistakes. 

update rule:  
    mistake on positive:         +1              +      
    mistake on negative:         +1                       

proof: 
idea: analyze                      and               , where         is the max-margin sep,               = 1.  
claim 1:         +1                                      +     . 

(because                                    ) 

claim 2:          +1

2             

2 +     2. 

(by pythagorean theorem) 
        +1 

after      mistakes: 

        +1                          (by claim 1) 

        +1               (by claim 2) 

        +1                            +1     (since         is unit length) 

so,                       , so         

2

. 

    
    

     

         

id88 extensions 

    can use it to find a consistent separator (by cycling 

through the data). 

    one can convert the mistake bound guarantee into a 

distributional guarantee too (for the case where the         s 
come from a fixed distribution). 

    can be adapted to the case where there is no perfect 

separator as long as the so called hinge loss (i.e., the total 
distance needed to move the points to classify them correctly large 
margin) is small.  

    can be kernelized to handle non-linear decision boundaries! 

id88 discussion 

    simple online algorithm for learning linear separators with 

a nice guarantee that depends only on the geometric 
(aka     2,     2) margin. 

    it can be kernelized to handle non-linear decision 

boundaries --- see next class! 

    simple, but very useful in applications like branch 

prediction; it also has interesting extensions to 
id170. 

