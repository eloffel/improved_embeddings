machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

february 25, 2015 

today: 

       id114 
       bayes nets:   
      
id136 
       learning 
       em 

readings: 
       bishop chapter 8 
       mitchell chapter 6 
 

midterm 
       in class on monday, march 2 
       closed book 
       you may bring a 8.5x11    cheat sheet    of notes 

       covers all material through today 

       be sure to come on time.  we   ll start at 11am 

sharp 

 

id110s definition 

a bayes network represents the joint id203 distribution 

over a collection of random variables 

 
a bayes network is a directed acyclic graph and a set of 

id155 distributions (cpd   s) 

       each node denotes a random variable 
       edges denote dependencies 
       for each node xi its cpd defines p(xi | pa(xi))
       the joint distribution over all variables is defined to be 

pa(x) = immediate parents of x in the graph 

what you should know 
       bayes nets are convenient representation for encoding 

dependencies / conditional independence 

       bn = graph plus parameters of cpd   s 

       defines joint distribution over variables 
       can calculate everything else from that 
       though id136 may be intractable 

       reading conditional independence relations from the 

graph 
       each node is cond indep of non-descendents, given only its 

parents 

       x and y are conditionally independent given z if z d-separates 

every path connecting x to y 

       marginal independence : special case where z={} 

 

id136 in bayes nets 
      
       for certain cases, tractable 

in general, intractable (np-complete) 

       assigning id203 to fully observed set of variables 
       or if just one variable unobserved 
       or for singly connected graphs (ie., no undirected loops) 

       belief propagation 

       sometimes use monte carlo methods 

       generate many samples according to the bayes net 

distribution, then count up the results 

       variational methods for tractable approximate 

solutions 

example 
       bird flu and allegies both cause sinus problems 
       sinus problems cause headaches and runny nose 

prob. of joint assignment: easy  

       suppose we are interested in joint 
 assignment <f=f,a=a,s=s,h=h,n=n> 
 
what is p(f,a,s,h,n)? 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

prob. of marginals: not so easy  

       how do we calculate p(n=n) ? 
 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

generating a sample from  
joint distribution: easy  

how can we generate random samples 
drawn according to p(f,a,s,h,n)? 
 
hint: random sample of f according to p(f=1) =   f=1 : 
       draw a value of r uniformly from [0,1] 
      
 
 

if r<    then output f=1, else f=0 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

generating a sample from  
joint distribution: easy  

how can we generate random samples 
drawn according to p(f,a,s,h,n)? 
 
hint: random sample of f according to p(f=1) =   f=1 : 
       draw a value of r uniformly from [0,1] 
      
 
solution: 
       draw a random value f for f, using its cpd 
      
 

then draw values for a, for s|a,f, for h|s, for n|s 

if r<    then output f=1, else f=0 

generating a sample from  
joint distribution: easy  

 
note we can estimate marginals 
like p(n=n) by generating many samples 
from joint distribution, then count the fraction of samples 

for which n=n 

 
similarly, for anything else we care about  

 p(f=1|h=1, n=0) 

 
      weak but general method for estimating any 

id203 term    

id136 in bayes nets 

in general, intractable (np-complete) 

      
       for certain cases, tractable 

       assigning id203 to fully observed set of variables 
       or if just one variable unobserved 
       or for singly connected graphs (ie., no undirected loops) 

       variable elimination 
       belief propagation 

       often use monte carlo methods 

       e.g., generate many samples according to the bayes net 

distribution, then count up the results 

       id150 

       variational methods for tractable approximate solutions 
 
see id114 course 10-708 

learning of bayes nets 

       four categories of learning problems 
       graph structure may be known/unknown 
       variable values may be fully observed / partly unobserved 

       easy case: learn parameters for graph structure is 

known, and data is fully observed 

 
      

interesting case: graph known, data partly known 

       gruesome case: graph structure unknown, data partly 

unobserved 

learning cpts from fully observed data 

       example: consider learning 

the parameter 

flu 

allergy 

sinus 

       max likelihood estimate is 

headache 

nose 

kth training 
example 

  (x) = 1 if x=true,  
       =  0 if x=false 

       remember why? 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

id113 estimate of         from fully observed data 

       maximum likelihood estimate 

       our case: 

flu 

allergy 

sinus 

headache 

nose 

estimate     from partly observed data 

       what if fahn observed, but not s? 
       can   t calculate id113 

flu 

allergy 

sinus 

headache 

nose 

       let x be all observed variable values (over all examples) 
       let z be all unobserved variable values   
       can   t calculate id113: 
 
        what to do? 

estimate     from partly observed data 

       what if fahn observed, but not s? 
       can   t calculate id113 

flu 

allergy 

sinus 

headache 

nose 

       let x be all observed variable values (over all examples) 
       let z be all unobserved variable values   
       can   t calculate id113: 
 
        em seeks* to estimate: 

* em guaranteed to find local maximum 

        em seeks estimate: 

flu 

allergy 

sinus 

headache 

nose 

        here, observed x={f,a,h,n}, unobserved z={s} 

em algorithm - informally 

em is a general procedure for learning from partly observed data 
given  observed variables x, unobserved z  (x={f,a,h,n}, z={s}) 
 

begin with arbitrary choice for parameters    

iterate until convergence: 
       e step: estimate the values of unobserved z, using      
       m step: use observed values plus e-step estimates to  
                derive a better   

guaranteed to find local maximum. 
each iteration increases   

em algorithm - precisely 

em is a general procedure for learning from partly observed data 
given  observed variables x, unobserved z  (x={f,a,h,n}, z={s}) 
define 

iterate until convergence: 
       e step: use x and current    to calculate p(z|x,  ) 
       m step: replace current    by  

guaranteed to find local maximum. 
each iteration increases   

e step: use x,   , to calculate p(z|x,  ) 
observed x={f,a,h,n}, 
unobserved z={s} 

flu 

allergy 

sinus 

headache 

nose 

       how?  bayes net id136 problem. 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

e step: use x,   , to calculate p(z|x,  ) 
observed x={f,a,h,n}, 
unobserved z={s} 

flu 

allergy 

sinus 

headache 

nose 

       how?  bayes net id136 problem. 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

em and estimating   

observed x = {f,a,h,n}, unobserved z={s} 

flu 

allergy 

sinus 

headache 

nose 

e step:  calculate p(zk|xk;   ) for each training example, k  

m step: update all relevant parameters.  for example: 

recall id113 was: 

em and estimating   

flu 

allergy 

sinus 

more generally,  
given observed set x, unobserved set z of boolean values 

headache 

nose 

e step:  calculate for each training example, k  

 the expected value of each unobserved variable   

m step: 

calculate estimates similar to id113, but 
replacing each count by its expected count 

using unlabeled data to help train  

na  ve bayes classifier 

learn p(y|x) 

y

x1 

x2 

x3 

x4 

y 
1 
0 
0 
? 
? 

x1  x2  x3  x4 
0 
0 
0 
0 
0 

0 
1 
0 
1 
1 

1 
0 
1 
1 
0 

1 
0 
0 
0 
1 

e step:  calculate for each training example, k  

 the expected value of each unobserved variable   

em and estimating   

given observed set x, unobserved set y of boolean values 

e step:  calculate for each training example, k  

 the expected value of each unobserved variable y 

m step:  calculate estimates similar to id113, but 

replacing each count by its expected count 

let   s use y(k) to indicate value of y on kth example 

em and estimating   

given observed set x, unobserved set y of boolean values 

e step:  calculate for each training example, k  

 the expected value of each unobserved variable y 

m step:  calculate estimates similar to id113, but 

replacing each count by its expected count 

id113 would be: 

from [nigam et al., 2000] 

experimental evaluation 

       newsgroup postings  

       20 newsgroups, 1000/group 

       web page classification  

       student, faculty, course, project 
       4199 web pages 

       reuters newswire articles  

       12,902 articles 
       90 topics categories 

20 newsgroups 

word w ranked by 
p(w|y=course) /
p(w|y     course) 

using one labeled 
example per class 

20 newsgroups 

bayes nets     what you should know 

       representation 

distributions 

assumptions 

       bayes nets represent joint distribution as a dag + conditional 

       d-separation lets us decode conditional independence 

      

id136 
       np-hard in general 
       for some graphs, some queries, exact id136 is tractable 
       approximate methods too, e.g., monte carlo methods,     

       learning 

       easy for known graph, fully observed data (id113   s, map est.) 
       em for partly observed data, known graph 

