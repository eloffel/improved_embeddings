machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

january 21, 2015 

today: 
       bayes rule 
       estimating parameters 

       id113 
       map 

some of these slides are derived 
from william cohen, andrew 
moore, aarti singh, eric xing, 
carlos guestrin.   - thanks! 

readings: 
 
id203 review 
       bishop ch. 1 thru 1.2.3 
       bishop, ch. 2 thru 2.2 
       andrew moore   s online 

tutorial 

announcements 
       class is using piazza for questions/discussions 

about homeworks, etc. 
       see class website for piazza address 
       http://www.cs.cmu.edu/~ninamf/courses/601sp15/ 

       recitations thursdays 7-8pm, wean 5409 

       videos for future recitations (class website) 

       hw1 was accepted to sunday 5pm for full credit 
       hw2 out today on class website, due in 1 week 
       hw3 will involve programming (in octave ) 

p(a|b) = 

p(b|a) * p(a) 

p(b) 

bayes    rule 

we call p(a) the    prior    
 
and p(a|b) the    posterior    

bayes, thomas (1763) an essay 
towards solving a problem in the doctrine 
of chances. philosophical transactions of 
the royal society of london, 53:370-418 

   by no means merely a curious speculation in the doctrine of chances, 
but necessary to be solved in order to a sure foundation for all our 
reasonings concerning past facts, and what is likely to be hereafter   . 
necessary to be considered by any that would give a clear account of the 
strength of analogical or inductive reasoning    

other forms of bayes rule 

p(a|b) = 

p(b|a) * p(a) 

p(b) 

bap
(
)|

=

apabp
(
)
(
)
(
|~(

|
+

apabpapabp
(
)

(~)

)

)

|

xbap
(

   

|

)

=

xapxabp
(

|

   

(
   
xbp
(
)

)
   

)

applying bayes rule 

p(a |b) =

p(b | a)p(a)

p(b | a)p(a) + p(b |~ a)p(~ a)

a = you have the flu,   b = you just coughed 
 
assume: 
p(a) = 0.05 
p(b|a) = 0.80 
p(b| ~a) = 0.20 
 
what is p(flu | cough) = p(a|b)? 

    

what does all this have to do with 

function approximation? 

instead of  f: x      y, 
learn          p(y | x) 

the joint distribution 

recipe for making a joint 

distribution of m variables: 

a 
0 

0 

0 

0 

1 

1 

1 

1 

example: boolean 
variables a, b, c 
b 
prob 
0 
0.30 

c 
0 

0 

1 

1 

0 

0 

1 

1 

1 

0 

1 

0 

1 

0 

1 

0.05 

0.10 

0.05 

0.05 

0.10 

0.25 

0.10 

a 

0.05 
 
0.25 
 
b 
 

0.10 

0.10 

0.10 

0.05 

c 

0.05 

0.30 

[a. moore]  

the joint distribution 

recipe for making a joint 

distribution of m variables: 

 
1.    make a truth table listing all 
combinations of values (m 
boolean variables       2m rows). 

 

a 
0 

0 

0 

0 

1 

1 

1 

1 

example: boolean 
variables a, b, c 
b 
prob 
0 
0.30 

c 
0 

0 

1 

1 

0 

0 

1 

1 

1 

0 

1 

0 

1 

0 

1 

0.05 

0.10 

0.05 

0.05 

0.10 

0.25 

0.10 

a 

0.05 
 
0.25 
 
b 
 

0.10 

0.10 

0.10 

0.05 

c 

0.05 

0.30 

[a. moore]  

the joint distribution 

recipe for making a joint 

distribution of m variables: 

 
1.    make a truth table listing all 
combinations of values (m 
boolean variables       2m rows). 

2.    for each combination of 

values, say how probable it is. 

 

a 
0 

0 

0 

0 

1 

1 

1 

1 

example: boolean 
variables a, b, c 
b 
prob 
0 
0.30 

c 
0 

0 

1 

1 

0 

0 

1 

1 

1 

0 

1 

0 

1 

0 

1 

0.05 

0.10 

0.05 

0.05 

0.10 

0.25 

0.10 

a 

0.05 
 
0.25 
 
b 
 

0.10 

0.10 

0.10 

0.05 

c 

0.05 

0.30 

[a. moore]  

the joint distribution 

recipe for making a joint 

distribution of m variables: 

 
1.    make a truth table listing all 
combinations of values (m 
boolean variables       2m rows). 

2.    for each combination of 

values, say how probable it is. 

3.    if you subscribe to the axioms 

of id203, those 
probabilities must sum to 1. 

example: boolean 
variables a, b, c 
b 
prob 
0 
0.30 

c 
0 

0 

1 

1 

0 

0 

1 

1 

1 

0 

1 

0 

1 

0 

1 

0.05 

0.10 

0.05 

0.05 

0.10 

0.25 

0.10 

a 
0 

0 

0 

0 

1 

1 

1 

1 

a 

0.05 
 
0.25 
 
b 
 

0.10 

0.10 

0.10 

0.05 

c 

0.05 

0.30 

[a. moore]  

using the 
joint 
distribution 

one you have the jd 
you can ask for the 
id203 of any logical 
expression involving 
these variables 

ep
(

)

   =

p
row(
e
 
rows
matching
 

)

[a. moore]  

using the 
joint 

p(poor male) = 0.4654 

ep
(

)

   =

p
row(
e
 
rows
matching
 

)

[a. moore]  

using the 
joint 

p(poor) = 0.7604 

ep
(

)

   =

p
row(
e
 
rows
matching
 

)

[a. moore]  

id136 
with the 
joint 

eep
(

|

1

)

2

=

)

2

ep
(
   
1
ep
(

2

e
)

=

p
row(
   
e
e
and
 
 
matching
rows
 
 1
p
row(
   
e
matching
 
rows
 

)

 2

2

p(male | poor) = 0.4654 / 0.7604 = 0.612   

)

[a. moore]  

learning and 
the joint 
distribution 

suppose we want to learn the function f: <g, h>       w 
 
equivalently, p(w | g, h) 
 
solution: learn joint distribution from data, calculate p(w | g, h) 
 
e.g., p(w=rich | g = female, h = 40.5- ) = 

[a. moore]  

sounds like the solution to 

learning f: x      y, 

or p(y | x). 

 
 

are we done? 

sounds like the solution to 

learning f: x      y, 

or p(y | x). 

 
 

main problem: learning p(y|x)  

can require more data than we have 

consider learning joint dist. with 100 attributes 
# of rows in this table?  
# of people on earth? 
fraction of rows with 0 training examples? 

what to do? 
1.    be smart about how we estimate 

probabilities from sparse data 
       maximum likelihood estimates 
       maximum a posteriori estimates 

2.    be smart about how to represent joint 

distributions 
       bayes networks, id114 

1. be smart about how we    
    estimate probabilities  

estimating id203 of heads 

x=1  x=0 

x=1  x=0 

estimating    = p(x=1) 
test a:  
  100 flips: 51 heads (x=1), 49 tails (x=0) 
 
 
test b:  
  3 flips:  2 heads (x=1), 1 tails (x=0) 

estimating    = p(x=1) 
case c: (online learning) 
       keep flipping, want single learning algorithm 
that gives reasonable estimate after each flip 

x=1  x=0 

principles for estimating probabilities 

principle 1 (maximum likelihood): 
       choose parameters    that maximize p(data |   ) 
       e.g.,  
 
 
principle 2 (maximum a posteriori prob.): 
       choose parameters    that maximize p(   | data) 
       e.g. 
 
 

x=1  x=0 

id113 
p(x=1) =           p(x=0) = (1-  ) 
 
data d:  
 
 
 
 
flips produce data d with        heads,        tails 
      

flips are independent, identically distributed 1   s and 0   s 
(bernoulli) 
      and        are counts that sum these outcomes (binomial) 

      

maximum likelihood estimate for    

[c. guestrin]  

hint: 

summary:  
maximum likelihood estimate 

x=1  x=0 
p(x=1) =    
p(x=0) = 1-   
(bernoulli) 

 

principles for estimating probabilities 
 
principle 1 (maximum likelihood): 
       choose parameters    that maximize  

p(data |   ) 

 
principle 2 (maximum a posteriori prob.): 
       choose parameters    that maximize 
   p(   | data) = p(data |   ) p(  ) 
                              p(data) 
 

beta prior distribution     p(  ) 

beta prior distribution     p(  ) 

[c. guestrin]  

and map estimate is therefore  

and map estimate is therefore  

some terminology 
       likelihood function:  p(data |   ) 
       prior: p(  ) 
       posterior: p(   | data) 

       conjugate prior: p(  ) is the conjugate 

prior for likelihood function p(data |   ) if 
the forms of p(  ) and p(   | data) are the 
same. 

you should know 

       id203 basics 

       random variables, conditional probs,     
       bayes rule 
       joint id203 distributions 
       calculating probabilities from the joint distribution 

       estimating parameters from data 

       maximum likelihood estimates 
       maximum a posteriori estimates 
       distributions     binomial, beta, dirichlet,     
       conjugate priors 
 

extra slides 

independent events 
       definition: two events a and b are 
independent if   p(a ^ b)=p(a)*p(b) 
       intuition: knowing a tells us nothing 
about the value of b (and vice versa) 

picture    a independent of b    

expected values 
given a discrete random variable x, the expected value 

of x, written e[x] is 

 
 
 
example: 
 
 
 
 

x 
0 
1 
2 

p(x) 
0.3 
0.2 
0.5 

expected values 
given discrete random variable x, the expected value of 

x, written e[x] is 

 
 
 
 
we also can talk about the expected value of functions 

of x 

covariance 
given two discrete r.v.   s x and y, we define the 

covariance of x and y as 

 
 
e.g., x=gender, y=playsfootball 
or     x=gender, y=lefthanded 
 
 
remember: 

