bias, variance and error 

bias and variance 
given algorithm that outputs estimate     for     , we define: 

 the bias of the estimator:  

 the variance of estimator: 
 
e.g.,          estimator for id203     of heads, based on 

             n independent coin flips 

                     
 
what is its bias? 
 
variance? 

bias and variance 
given algorithm that outputs estimate     for     , we define: 

 the bias of the estimator:  

 the variance of estimator: 
 
which estimator has higher bias?   higher variance? 

bias     variance decomposition of error  

reading: bishop chapter 9.1, 9.2 

       consider simple regression problem f:x     y  

y = f(x) +   

noise n(0,  ) 

deterministic 

define the expected prediction error: 

  

expectation 

over 

training d 

learned 

estimate of f(x)  

sources of error 
 
 
what if we have perfect learner, infinite data? 

      our learned h(x) satisfies h(x)=f(x) 
      still have remaining, unavoidable error 
                                
                                   2 

sources of error 
       what if we have only n training examples? 
       what is our expected error 

      taken over random training sets of size n, 
drawn from distribution d=p(x,y)

 

sources of error 

l2 vs. l1 id173 

gaussian p(w) 
      l2 id173 

laplace p(w) 
      l1 id173 

w2 

w1 

constant p(data|w) 

w2 

constant p(w) 

w1 

summary 
       bias of parameter estimators 
       variance of parameter estimators 
       we can define analogous notions for estimators 

(learners) of functions 

       expected error in learned functions comes from 

       unavoidable error (invariant of training set size, due to noise) 
       bias (can be caused by incorrect modeling assumptions) 
       variance (decreases with training set size) 

       map estimates generally more biased than id113 

       but bias vanishes as training set size        

       id173 corresponds to producing map estimates 

       l2 / gaussian prior / leads to smaller weights 
       l1 / laplace prior / leads to fewer non-zero weights 

machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

february 18, 2015 

readings: 
 
       bishop chapter 8, through 8.2 
 

today: 

       id114 
       bayes nets: 
       representing 
distributions 
       conditional 
independencies   
       simple id136 
       simple learning 

id114 
       key idea:  

       conditional independence assumptions useful   
       but na  ve bayes is extreme! 
       id114 express sets of conditional 

independence assumptions via graph structure 

       graph structure plus associated parameters define 

joint id203 distribution over set of variables 

       two types of id114: 

       directed graphs (aka id110s) 
       undirected graphs (aka markov random fields) 

10-601 

id114     why care? 
       among most important ml developments of the decade 
  
       id114 allow combining: 

       prior knowledge in form of dependencies/independencies 
       prior knowledge in form of priors over parameters 
       observed training data 

       principled and ~general methods for 

       probabilistic id136 
       learning 

       useful in practice 

       diagnosis, help systems, text analysis, time series models, ... 

conditional independence   

definition: x is conditionally independent of y given z, if the 

id203 distribution governing x is independent of the value 
of y, given the value of z 

 
 
 
 
which we often write  
 
 
 
e.g.,  
 
 
 
 

marginal independence  

definition: x is marginally independent of y if 
 
 
 
 
 

equivalently, if  
 
 
 
 
 equivalently, if  
 
 
 
 
 

represent joint id203 distribution over variables 

describe network of dependencies 

bayes nets define joint id203 distribution 
in terms of this graph, plus parameters 

benefits of bayes nets: 
       represent the full joint distribution in fewer 
parameters, using prior knowledge about 
dependencies 

       algorithms for id136 and learning 

id110s definition 

a bayes network represents the joint id203 distribution 

over a collection of random variables 

 
a bayes network is a directed acyclic graph and a set of 

id155 distributions (cpd   s) 

       each node denotes a random variable 
       edges denote dependencies 
       for each node xi its cpd defines p(xi | pa(xi))
       the joint distribution over all variables is defined to be 

pa(x) = immediate parents of x in the graph 

id110 

stormclouds 

lightning 

rain 

thunder 

windsurf 

p(w|pa) 

nodes = random variables 
a id155 distribution (cpd) 
is associated with each node n, defining   
p(n | parents(n)) 
 
 
 
 
 
 
the joint distribution over all variables: 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

0 
0 
0.2 
0.9 

1.0 
1.0 
0.8 
0.1 

windsurf 

p(  w|pa) 

id110 

stormclouds 

lightning 

rain 

thunder 

windsurf 

what can we say about conditional 
independencies in a bayes net? 
one thing is this: 
each node is conditionally independent of 
its non-descendents, given only its 
immediate parents. 
  

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

some helpful terminology 

parents = pa(x) = immediate parents 
antecedents = parents, parents of parents, ... 
children = immediate children 
descendents = children, children of children, ... 

id110s 
       cpd for each node xi 
describes p(xi | pa(xi)) 

 
chain rule of id203 says that in general: 

 
but in a bayes net: 

stormclouds 

how many parameters? 

lightning 

rain 

thunder 

windsurf 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

to define joint distribution in general? 

to define joint distribution for this bayes net? 

stormclouds 

id136 in bayes nets 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

lightning 

rain 

thunder 

windsurf 

p(s=1, l=0, r=1, t=0, w=1)  = 

stormclouds 

learning a bayes net 

lightning 

rain 

thunder 

windsurf 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

consider learning when graph structure is given, and data = { <s,l,r,t,w> } 
what is the id113 solution?  map? 

