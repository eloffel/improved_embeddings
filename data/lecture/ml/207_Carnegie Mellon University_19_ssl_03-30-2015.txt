semi-supervised learning  

maria-florina balcan 

03/30/2015 

readings:  

    semi-supervised learning. encyclopedia of machine 

learning. jerry zhu, 2010 

    combining labeled and unlabeled data with co-

training. avrim blum, tom mitchell. colt  1998. 

fully supervised learning 

data 
source 

distribution d on x 

expert / oracle 

learning 
algorithm 

   labeled examples   

(x1,c*(x1)),   , (xm,c*(xm)) 

alg.outputs 

c* : x ! y 

h : x ! y 

x1 > 5 

x6 > 2 

+1 

+1 

-1 

+ 
+ 
+ 

+ 

- 

- 
- 

- 
- 
- 

fully supervised learning 

data 
source 

distribution d on x 

expert / oracle 

learning 
algorithm 

   labeled examples   

(x1,c*(x1)),   , (xm,c*(xm)) 

alg.outputs 

h : x ! y 

c* : x ! y 

 sl={(x1, y1),    ,(xml, yml)}  
xi drawn i.i.d from d, yi = c   (xi) 

 goal:  h has small error over d. 

 errd h = pr
x~ d

(h x     c   (x)) 

two core aspects of supervised learning 

algorithm design. how to optimize? 

computation 

automatically generate rules that do well on observed data. 

    e.g.: na  ve bayes, id28, id166, adaboost, etc. 

confidence bounds, generalization 

(labeled) data 

confidence for rule effectiveness on future data. 

    vc-dimension, rademacher complexity, margin based bounds, etc. 

classic paradigm insufficient nowadays 

modern applications: massive  amounts  of raw data. 

only a tiny fraction can be annotated by human experts. 

protein sequences 

billions of webpages 

images 

modern ml: new learning approaches 
modern applications: massive  amounts  of raw data. 

techniques that best utilize data, minimizing need for 
expert/human intervention. 

paradigms where there has been great progress. 

    semi-supervised learning, (inter)active learning. 

expert 

semi-supervised learning 

data source 

learning 
algorithm 

unlabeled 
examples 

expert / oracle 

unlabeled 
examples 

                 labeled examples   

algorithm outputs a classifier  

 sl={(x1, y1),    ,(xml, yml)}  
xi drawn i.i.d from d, yi = c   (xi) 

 su={x1,    ,xmu} drawn i.i.d from d 

 goal:  h has small error over d. 

 errd h = pr
x~ d

(h x     c   (x)) 

semi-supervised learning 

    major topic of research in ml. 

    several methods have been developed to try to use 

unlabeled data to improve performance, e.g.: 
    transductive id166 [joachims    99] 
    co-training [blum & mitchell    98] 
    graph-based methods [b&c01], [zgl03] 

 

 

test of time 

awards at icml! 

workshops  [icml    03, icml    05,    ] 
books: 

    semi-supervised learning, mit 2006 

o. chapelle, b. scholkopf and a. zien (eds)  

    introduction to semi-supervised learning, 

morgan & claypool, 2009  zhu  & goldberg 

semi-supervised learning 

    major topic of research in ml. 

    several methods have been developed to try to use 

unlabeled data to improve performance, e.g.: 
    transductive id166 [joachims    99] 
    co-training [blum & mitchell    98] 
    graph-based methods [b&c01], [zgl03] 

 

 

test of time 

awards at icml! 

both wide spread applications and solid foundational 
understanding!!! 

semi-supervised learning 

    major topic of research in ml. 

    several methods have been developed to try to use 

unlabeled data to improve performance, e.g.: 
    transductive id166 [joachims    99] 
    co-training [blum & mitchell    98] 
    graph-based methods [b&c01], [zgl03] 

 

 

test of time 

awards at icml! 

today: discuss these methods. 

very interesting, they all exploit unlabeled data in 
different, very interesting and creative ways. 

semi-supervised learning: no querying. just have 
lots of additional unlabeled data. 

a bit puzzling; unclear what unlabeled data can do 
for us   . it is missing the most important info. how 
can it help us in substantial ways? 

key insight 

unlabeled data useful if we have beliefs not only about 
the form of the target, but also about its relationship 

with the underlying distribution. 

semi-supervised id166 
[joachims    99] 

 margins based regularity 

target goes through low density regions (large margin). 

    assume we are looking for linear separator 
    belief: should exist one with large separation 

+ 

+ 

_ 

_ 

+ 

+ 

_ 

_ 

+ 

+ 

_ 

_ 

id166 

labeled data only 

transductive id166 

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. [joachims    99] 

0 

0 

0 

0 

0 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

0 

0 

0 

argminw  w
  

2

 s.t.: 

    yi w     xi     1, for all i     {1,     , ml}  

0 

0 

0 

                 =    1 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
w    
0 
+ 
+ 
+ + 

0 
0 

0 
0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
0 

0 

0 

0 
0 

0 

+ 

0 

0 

                 = 1 

0 

- 
- 

0 

0 

0 

- 
- 

0 
0 

0 

0 

0 

0 

0 

0 

0 

- 

0 

0 

0 

0 

0 

0 

0 

0 
0 

- 

0 

0 

- 

0 

0 

0 

0 

   

   

 yu w     xu     1, for all u     {1,     , mu}  

 yu      {   1, 1} for all u     {1,     , mu}   

find a labeling of the unlabeled sample and      s.t.      separates both 
labeled and unlabeled data with maximum margin. 

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. [joachims    99] 

0 

0 

0 

0 

0 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

0 

0 

0 

2

argminw  w

+                

 
 +                 

 

- 
    yi w     xi     1-        , for all i     {1,     , ml}  

    

0 

0 

    

0 

0 

                 =    1 

0 

0 

0 

0 

- 

0 

0 

0 

0 

0 

- 
- 

0 
0 

0 

0 

0 

0 
0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
w    
0 
+ 
+ 
+ + 

0 
0 

0 
0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 
0 

0 

0 

0 
0 

0 

+ 

0 

0 

                 = 1 

0 

0 

- 

0 

0 

0 

0 

0 

0 

- 
- 

0 

0 

0 

   

   

 yu w     xu     1              
 yu      {   1, 1} for all u     {1,     , mu}   

  , for all u     {1,     , mu}  

find a labeling of the unlabeled sample and      s.t.      separates both 
labeled and unlabeled data with maximum margin. 

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. 

0 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

argminw  w

2

+                

    

 
 +                 

    

 

    yi w     xi     1-        , for all i     {1,     , ml}  

   

   

 yu w     xu     1              
 yu      {   1, 1} for all u     {1,     , mu}   

  , for all u     {1,     , mu}  

np-hard   .. convex only after you guessed the labels    too many 
possible guesses    

transductive support vector machines 

optimize for the separator with large margin wrt labeled and 
unlabeled data. 

 heuristic (joachims) high level idea: 

    first maximize margin over the labeled points 

    use this to give initial labels to unlabeled points 

based on this separator. 

    try flipping labels of unlabeled points to see if doing 

so can increase margin 

keep going until no more improvements. finds a locally-optimal solution. 

experiments [joachims99] 

transductive support vector machines 

helpful distribution 

+ 

highly compatible 

+ 

_ 

_ 

non-helpful distributions 

margin not satisfied 

margin satisfied 

1/  2 clusters, 
all partitions  
separable by 
large margin 

co-training 
[blum & mitchell    98] 

different type of underlying regularity assumption:  

consistency or agreement between parts 

co-training: self-consistency 

agreement between two parts : co-training [blum-mitchell98]. 

-  examples contain two sufficient sets of features, x = h x1, x2 i  
-  belief: the parts are consistent, i.e. 9 c1, c2 s.t. c1(x1)=c2(x2)=c*(x) 

for example, if we want to classify web pages: 

x = h x1, x2 i 

as faculty member homepage or not 

prof. avrim blum 

my advisor 

prof. avrim blum 

my advisor 

x - link info & text info 

x1- text info 

x2- link info 

iterative co-training 

idea: use small labeled sample to learn initial rules. 

    e.g.,    my advisor    pointing to a page is a good indicator it is a 

faculty home page. 

    e.g.,    i am teaching    on a page is a good indicator it is a faculty 

 

home page. 

idea: use unlabeled data to  propagate learned information. 
 

my advisor 

iterative co-training 

idea: use small labeled sample to learn initial rules. 

    e.g.,    my advisor    pointing to a page is a good indicator it is a 

faculty home page. 

    e.g.,    i am teaching    on a page is a good indicator it is a faculty 

 

home page. 

idea: use unlabeled data to  propagate learned information. 
 
look for unlabeled examples where one rule is confident and 
the other is not. have it label the example for the other.  

hx1,x2i 
hx1,x2i 
hx1,x2i 

hx1,x2i 
hx1,x2i 
hx1,x2i 

training 2 classifiers, one on each type of info.  using 
each to help train the other. 

iterative co-training 

x1 

works by using unlabeled data to  

propagate learned information. 

 

h1 

+ 

+ 
+ 

x2 

h 

    have learning algos a1, a2 on each of the two views. 
    use labeled data to learn two initial hyp. h1, h2. 

 
 

repeat 

    look through unlabeled data to find examples 
where one of hi is  confident but other is not. 

    have the confident hi label it for algorithm a3-i. 

original application: webpage classification 

12 labeled examples, 1000 unlabeled 

(sample run) 

iterative co-training   

a simple example: learning intervals 

c2 

- 

+ 

c1 

- 

use unlabeled data to bootstrap 

  labeled examples 
  unlabeled examples 

1 

h2

1 
h1

use labeled data to learn h1

1 
1 and h2

1 

h2

2 

h2

2 
h1

2 
h1

expansion, examples: learning intervals 

consistency: zero id203 
mass in the regions 

c1 

c2 

non-expanding (non-helpful) 

expanding distribution 

distribution 

d+ 

c1 

s2 

c1 

d+ 

s1 

c2 

c2 

co-training: theoretical guarantees 

what properties do we need for co-training to work well? 
we need assumptions about: 

1.
2.

the underlying data distribution 
the learning algos on the two sides 

 [blum & mitchell, colt    98] 

view 1 

view 2 

1.  independence given the label 
2.  alg. for learning from random noise. 

[balcan, blum, yang, nips 2004] 

+ 
    1

    
    1

1. distributional expansion. 
2.  alg. for learning from positve data only. 

+ 
    2

    
    2

co-training [bm   98] 

say that    1 is a weakly-useful predictor if 

pr    1      = 1     1      = 1 > pr    1      = 1     1      = 0 +     . 

has higher id203 of saying positive on a true positive than it 
does on a true negative, by at least some gap      

say we have enough labeled data to produce such a starting point.  

theorem: if      is learnable from random classification noise, we can 
use a weakly-useful    1 plus unlabeled data to create a strong 
learner under independence given the label. 

co-training: benefits in principle 
[bb   05]: under independence given the label, any pair       1,    2    with high 
agreement over unlabeled data must be close to:  

   

    1,     2 ,          1,       2   ,                    ,                    , or                        ,                          

view 1 

view 2 

+ 
    1

    
    1

+ 
    2

    
    2

co-training: benefits in principle 
[bb   05]: under independence given the label, any pair       1,    2    with high 
agreement over unlabeled data must be close to:  

   

    1,     2 ,          1,       2   ,                    ,                    , or                        ,                          

e.g.,  

view 1 

view 2 

         

+ 
    1

    
    1

because of independence, we 
will see  lot disagreement   . 

+ 
    2

         

    
    2

co-training/multi-view ssl: direct 

optimization of agreement 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

2

ml

mu

argminh1,h2      l(hl xi , yi)

+ c   agreement(h1 xi , h2 xi )
 

l=1

i=1

i=1

each of them has small 

labeled error 

regularizer to encourage 

agreement over unlabeled dat 

e.g., 
p. bartlett, d. rosenberg, aistats 2007;   k. sridharan, s. kakade, colt 2008 

co-training/multi-view ssl: direct 

optimization of agreement 

 input: sl={(x1, y1),    ,(xml, yml)}  

 su={x1,    ,xmu} 

2

ml

mu

argminh1,h2      l(hl xi , yi)

+ c   agreement(h1 xi , h2 xi )
 

l=1

i=1

i=1

    l(h xi , yi) id168 

    e.g., square loss l h xi , yi = yi         xl
    e.g., 0/1 loss l h xi , yi = 1              (        ) 

2 

e.g., 
p. bartlett, d. rosenberg, aistats 2007;   k. sridharan, s. kakade, colt 2008 

original application: webpage classification 

12 labeled examples, 1000 unlabeled 

(sample run) 

many other applications 

e.g., [levin-viola-freund03] identifying objects in images.  

two different kinds of preprocessing. 

e.g., [collins&singer99] named-entity extraction. 

       i arrived in london yesterday    

central to nell!!! 

    

    

similarity based regularity  

[blum&chwala01], [zhughahramanilafferty03] 

graph-based methods 

    assume we are given a pairwise similarity fnc and that 

very similar examples probably have the same label. 

    if we have a lot of labeled data, this suggests a 

nearest-neighbor type of algorithm. 

    if you have a lot of unlabeled data, perhaps can use 

them as    stepping stones   . 

e.g., handwritten digits [zhu07]: 

graph-based methods 

idea: construct a graph with edges between very similar 
examples. 
unlabeled data can help    glue    the objects of the same 
class together. 

graph-based methods 

idea: construct a graph with edges between very similar 
examples. unlabeled data can help    glue    the objects of 
the same class together. 
 

 
person identification in webcam images: an application of semi-supervised 
learning. [balcan,blum,choi, lafferty, pantano, rwebangira, xiaojin zhu], icml 2005 workshop 
on learning with partially classified training data.  

graph-based methods 

often, transductive approach.  (given l + u, output predictions on 
u). are alllowed to output any labeling of              . 

main idea: 

    construct graph g with edges 

between very similar examples. 

    might have also glued together in g 

examples of different classes. 

    run a graph partitioning algorithm to 

separate the graph into pieces. 

several methods: 

    minimum/multiway cut [blum&chawla01] 
    minimum    soft-cut    [zhughahramanilafferty   03] 
    spectral partitioning 
        

what you should know 

    unlabeled data useful if we have beliefs not only about 
the form of the target, but also about its relationship 
with the underlying distribution. 

    different types of algorithms (based on different 

beliefs). 

 

    transductive id166 [joachims    99] 
    co-training [blum & mitchell    98] 
    graph-based methods [b&c01], [zgl03] 

 

additional material on 
graph based methods 

minimum/multiway cut[blum&chawla01] 
objective: solve for labels on unlabeled points that minimize total 
weight of edges whose endpoints have different labels. 

(i.e., the total weight of bad edges) 

    if just two labels, can be solved 

efficiently using max-flow min-cut 
algorithms 

- create super-source      connected by 

edges of weight     to all + labeled pts. 

- create super-sink      connected by 

edges of weight     to all     labeled pts. 

- find minimum-weight     -     cut 

minimum    soft cut    

[zhughahramanilafferty   03] 

objective solve for id203 vector over labels          on each 
unlabeled point     . 
(labeled points get coordinate vectors 
in direction of their known label) 

(0000000100) 

(0100000000) 

    minimize  

    =(    ,    )

                              

2

  

where                             is euclidean distance. 

    can be done efficiently by solving 

a set of linear equations. 

(1000000000) 

(0001000000) 

(0000000010) 

(0000000001) 

how to create the graph 

    empirically, the following works well: 

1. compute distance between i, j 

2. for each i, connect to its knn.  k very small but 

still connects the graph 

3. optionally put weights on (only) those edges 

 

4.  tune     

