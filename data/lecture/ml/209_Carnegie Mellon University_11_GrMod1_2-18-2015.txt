machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

february 18, 2015 

readings: 
 
       bishop chapter 8, through 8.2 
 

today: 

       id114 
       bayes nets: 
       representing 
distributions 
       conditional 
independencies   
       simple id136 
       simple learning 

id114 
       key idea:  

       conditional independence assumptions useful   
       but na  ve bayes is extreme! 
       id114 express sets of conditional 

independence assumptions via graph structure 

       graph structure plus associated parameters define 

joint id203 distribution over set of variables 

       two types of id114: 

       directed graphs (aka id110s) 
       undirected graphs (aka markov random fields) 

10-601 

id114     why care? 
       among most important ml developments of the decade 
  
       id114 allow combining: 

       prior knowledge in form of dependencies/independencies 
       prior knowledge in form of priors over parameters 
       observed training data 

       principled and ~general methods for 

       probabilistic id136 
       learning 

       useful in practice 

       diagnosis, help systems, text analysis, time series models, ... 

conditional independence   

definition: x is conditionally independent of y given z, if the 

id203 distribution governing x is independent of the value 
of y, given the value of z 

 
 
 
 
which we often write  
 
 
 
e.g.,  
 
 
 
 

marginal independence  

definition: x is marginally independent of y if 
 
 
 
 
 

equivalently, if  
 
 
 
 
 equivalently, if  
 
 
 
 
 

represent joint id203 distribution over variables 

describe network of dependencies 

bayes nets define joint id203 distribution 
in terms of this graph, plus parameters 

benefits of bayes nets: 
       represent the full joint distribution in fewer 
parameters, using prior knowledge about 
dependencies 

       algorithms for id136 and learning 

id110s definition 

a bayes network represents the joint id203 distribution 

over a collection of random variables 

 
a bayes network is a directed acyclic graph and a set of 

id155 distributions (cpd   s) 

       each node denotes a random variable 
       edges denote dependencies 
       for each node xi its cpd defines p(xi | pa(xi))
       the joint distribution over all variables is defined to be 

pa(x) = immediate parents of x in the graph 

id110 

stormclouds 

lightning 

rain 

thunder 

windsurf 

p(w|pa) 

nodes = random variables 
a id155 distribution (cpd) 
is associated with each node n, defining   
p(n | parents(n)) 
 
 
 
 
 
 
the joint distribution over all variables: 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

0 
0 
0.2 
0.9 

1.0 
1.0 
0.8 
0.1 

windsurf 

p(  w|pa) 

id110 

stormclouds 

lightning 

rain 

thunder 

windsurf 

what can we say about conditional 
independencies in a bayes net? 
one thing is this: 
each node is conditionally independent of 
its non-descendents, given only its 
immediate parents. 
  

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

some helpful terminology 

parents = pa(x) = immediate parents 
antecedents = parents, parents of parents, ... 
children = immediate children 
descendents = children, children of children, ... 

id110s 
       cpd for each node xi 
describes p(xi | pa(xi)) 

 
chain rule of id203 says that in general: 

 
but in a bayes net: 

stormclouds 

how many parameters? 

lightning 

rain 

thunder 

windsurf 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

to define joint distribution in general? 

to define joint distribution for this bayes net? 

stormclouds 

id136 in bayes nets 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

lightning 

rain 

thunder 

windsurf 

p(s=1, l=0, r=1, t=0, w=1)  = 

stormclouds 

learning a bayes net 

lightning 

rain 

thunder 

windsurf 

parents 
l, r  
l,   r 
  l, r 
  l,   r 

p(w|pa) 

p(  w|pa) 

1.0 
1.0 
0.8 
0.1 

0 
0 
0.2 
0.9 

windsurf 

consider learning when graph structure is given, and data = { <s,l,r,t,w> } 
what is the id113 solution?  map? 

algorithm for constructing bayes network 
       choose an ordering over variables, e.g., x1, x2, ... xn  
       for i=1 to n 

       add xi to the network 
       select parents pa(xi) as minimal subset of x1 ... xi-1 such that  
 

notice this choice of parents assures 
 

(by chain rule) 

(by 
construction) 

example 
       bird flu and allegies both cause nasal problems 
       nasal problems cause sneezes and headaches 

what is the bayes network for x1,   x4 with no 
assumed conditional independencies? 

what is the bayes network for na  ve bayes? 

what do we do if variables are mix of discrete 
and real valued? 

bayes network for a hidden markov model 

implies the future is conditionally independent of the past, 
given the present 

unobserved 
state: 

st-2 

observed 
output: 

ot-2 

st-1 

ot-1 

st 

ot 

st+1 

st+2 

ot+1 

ot+2 

what you should know 
       bayes nets are convenient representation for encoding 

dependencies / conditional independence 

       bn = graph plus parameters of cpd   s 

       defines joint distribution over variables 
       can calculate everything else from that 
       though id136 may be intractable 

       reading conditional independence relations from the 

graph 
       each node is cond indep of non-descendents, given only its 

parents 

          explaining away    

see bayes net applet: http://www.cs.cmu.edu/~javabayes/home/applet.html 
 

id136 in bayes nets 
      
       for certain cases, tractable 

in general, intractable (np-complete) 

       assigning id203 to fully observed set of variables 
       or if just one variable unobserved 
       or for singly connected graphs (ie., no undirected loops) 

       belief propagation 

       for multiply connected graphs 

       junction tree 

       sometimes use monte carlo methods 

       generate many samples according to the bayes net 

distribution, then count up the results 

       variational methods for tractable approximate 

solutions 

example 
       bird flu and allegies both cause sinus problems 
       sinus problems cause headaches and runny nose 

prob. of joint assignment: easy  

       suppose we are interested in joint 
 assignment <f=f,a=a,s=s,h=h,n=n> 
 
what is p(f,a,s,h,n)? 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

prob. of marginals: not so easy  

       how do we calculate p(n=n) ? 
 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

generating a sample from  
joint distribution: easy  

how can we generate random samples 
drawn according to p(f,a,s,h,n)? 
 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

generating a sample from  
joint distribution: easy  

 
note we can estimate marginals 
like p(n=n) by generating many samples 
from joint distribution, then count the fraction of samples 

for which n=n 

 
similarly, for anything else we care about  

 p(f=1|h=1, n=0) 

 
      weak but general method for estimating any 

id203 term    

let   s use p(a,b) as shorthand for p(a=a, b=b) 

prob. of marginals: not so easy  
but sometimes the structure of the network allows us to be 

clever       avoid exponential work 

 
eg., chain     

a 

b 

c 

d 

e 

id136 in bayes nets 
      
       for certain cases, tractable 

in general, intractable (np-complete) 

       assigning id203 to fully observed set of variables 
       or if just one variable unobserved 
       or for singly connected graphs (ie., no undirected loops) 

       variable elimination 
       belief propagation 

       for multiply connected graphs 

       junction tree 

       sometimes use monte carlo methods 

       generate many samples according to the bayes net 

distribution, then count up the results 

       variational methods for tractable approximate 

solutions 

