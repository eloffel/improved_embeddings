machine learning theory ii 

maria-florina (nina) balcan 

february 11th, 2015 

sample complex. 

   

two core aspects of machine learning 

algorithm design. how to optimize? 

computation 

automatically generate rules that do well on observed data. 

    e.g.: id28, id166, adaboost, etc. 

confidence bounds, generalization 

(labeled) data 

confidence for rule effectiveness on future data. 

today   s focus: sample complexity for supervised 
classification (function approximation) 
     statistical learning theory (vapnik) 
     pac (valiant) 

 

    recommended reading: mitchell: ch. 7  
    suggested exercises: 7.1, 7.2, 7.7 

    additional resources: my learning theory course!  

supervised learning 

    e.g., which emails are spam and which are important. 

not spam 

spam 

    e.g., classify images as man versus women. 

man 

women 

pac/slt models for supervised learning 

data 
source 

distribution d on x 

expert / oracle 

learning 
algorithm 

   labeled examples   

(x1,c*(x1)),   , (xm,c*(xm)) 

alg.outputs 

c* : x ! y 

h : x ! y 

x1 > 5 

x6 > 2 

+1 

+1 

-1 

+ 
+ 
+ 

+ 

- 

- 
- 

- 
- 
- 

pac/slt models for supervised learning 

    x     feature/instance space; distribution d over x 
           e.g., x = rd or x = {0,1}d 
    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi i.i.d. from d 
    labeled examples - drawn i.i.d. from d and labeled by target c* 
     labels 2 {-1,1}  - binary classification 

     algo does optimization over s, find hypothesis    . 

     goal:  h has small error over d. 

                      = pr
    ~     

(                    (    )) 

h 

+
+
  
  

c* 
-
-
-
  
-
  
  
instance space x 
  

+
  +
  

bias: fix hypothesis space h  [whose complexity is not too large] 

    realizable:                 .  
    agnostic:            close to    h.  

pac/slt models for supervised learning 

    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi i.i.d. from d 
      does optimization over s, find hypothesis             . 

    goal:  h has small error over d. 

true error: errd h = pr
x~ d

(h x     c   (x)) 

how often                     (    ) over future 
instances drawn at random from d  

    but, can only measure: 

training error: errs h =

1
m

  i h xi     c    xi

i

 

how often                     (    ) over training 
instances 

sample complexity: bound                       in terms of                       

 sample complexity for supervised learning 

consistent learner 

    input: s: (x1,c*(x1)),   , (xm,c*(xm)) 
     output: find h in h consistent with the sample (if one exits).  

bound only logarithmic in |h|, linear in 1/     

id203 over different samples of m 
training examples 

so, if c        h and can find consistent fns, then only need this many 
examples to get generalization error          with prob.     1          

 sample complexity for supervised learning 

consistent learner 

    input: s: (x1,c*(x1)),   , (xm,c*(xm)) 
     output: find h in h consistent with the sample (if one exits).  

example:  h is the class of conjunctions over x = 0,1 n. 

|h| = 3n 

e.g., h = x1 x3x5 or h = x1 x2x4    9 

then         

1
    

     ln 3 + ln

1
    

 suffice 

sample complexity:  finite hypothesis spaces 

realizable case 

1) pac: how many examples suffice to guarantee small error whp.  

 
 
 

 

 

 
 
 
 
  
 

2) statistical learning way: 

with id203 at least 1         , for all h     h s.t. errs h = 0 we have 

errd(h)    

1
m

ln  h + ln

1
    

. 

 supervised learning: pac model (valiant) 

    x - instance space, e.g., x = 0,1 n or x = rn 
    sl={(xi, yi)} - labeled examples drawn i.i.d. from some 
distr. d over x and labeled by some target concept c* 
     labels 2 {-1,1}  - binary classification 

     algorithm a pac-learns concept class h if for any  
target c* in h, any distrib. d over x, any    ,     > 0: 

 - a uses at most poly(n,1/   ,1/   ,size(c*)) examples and running 
time. 
 - with prob.     1         , a produces h in h of error at       . 

what if h is infinite? 

e.g., linear separators in rd 

+ 

+ 
+ 
+ 

- 

- 
- 

- 
- 
- 

e.g., thresholds on the real line 

e.g., intervals on the real line 

- 

+ 

w 

- 

+ 

- 

a  b 

sample complexity: infinite hypothesis spaces  
    h[m] - maximum number of ways to split m points using concepts 

in h; i.e.  

 

sauer   s lemma: h m = o mvcdim h  

effective number of hypotheses 

    h[s]     the set of splittings of dataset s using concepts from h. 

    h[m] - max number of ways to split m points using concepts in h 

h m = max
s =m

|h[s]| 

effective number of hypotheses 

    h[s]     the set of splittings of dataset s using concepts from h. 

    h[m] - max number of ways to split m points using concepts in h 

h m = max
s =m

|h[s]| 

h[m]     2m 
- 

e.g., h= thresholds on the real line 

- 

- 
- 
- 
+ 

- 

- 
- 
+ 
+ 

- 

- 
+ 
+ 
+ 

+ 

- 
+ 
+ 
+ 

+ 

w 

|h s | = 5 

in general, if |s|=m (all distinct), |h s | = m + 1     2m 

effective number of hypotheses 

    h[s]     the set of splittings of dataset s using concepts from h. 

    h[m] - max number of ways to split m points using concepts in h 
    h[m] - max number of ways to split m points using concepts in h 

h m = max
s =m

|h[s]| 

e.g., h= intervals on the real line 

h[m]     2m 
+ 

- 

- 

- 

- 

- 

- 

+ 

- 

in general, |s|=m (all distinct), h m =

- 

- 
m m+1 

2

+ 1 = o(m2)     2m 

there are m+1 possible options for the first part, m left for the second 
part, the order does not matter, so (m choose 2) + 1 (for empty interval). 

effective number of hypotheses 

    h[s]     the set of splittings of dataset s using concepts from h. 

    h[m] - max number of ways to split m points using concepts in h 

h m = max
s =m

|h[s]| 

h[m]     2m 

definition: h shatters s if |h s | = 2|    |. 

sample complexity: infinite hypothesis spaces  

    h[m] - max number of ways to split m points using concepts in h 

very very 
rough idea: 

s= {x1, x2,     , xm} i.i.d. from d 
b:     h     h with errs h = 0 but errd h       . 

    ,     , x    

    } another i.i.d.    ghost sample    from d 

s    ={x1
b   :     h     h with errs h = 0 but errs    h       . 

claim: to bound p(b), sufficient to bound p(b   ) 
. 
over s     s    only h[2m] effective hypotheses left    but, no randomness left. 
need randomness to bound the id203 of a bad event, another 
symmetrization trick   .  

sample complexity: infinite hypothesis spaces  

realizable case 

h[m] - max number of ways to split m points using concepts in h 

 
 
 

 

 

 
 
 
 
  
 

    not too easy to interpret sometimes hard to calculate 
exactly, but can get a good bound using    vc-dimension 

if h m = 2m, then m    

m
  

(    . )      

    vc-dimension is roughly the point at which h stops looking 

like it contains all functions, so hope for solving for m. 

sample complexity: infinite hypothesis spaces  

h[m] - max number of ways to split m points using concepts in h 

sauer   s lemma: h m = o mvcdim h  

shattering, vc-dimension 

definition: 

h shatters s if |h s | = 2|    |. 

a set of points s is shattered by h is there are hypotheses in h 
that split s in all of the 2|    | possible ways, all possible ways of 
classifying points in s are achievable using concepts in h. 

definition: 

vc-dimension (vapnik-chervonenkis dimension) 

the vc-dimension of a hypothesis space h is the cardinality of 
the largest set s that can be shattered by h. 

if arbitrarily large finite sets can be shattered by h, then 
vcdim(h) =     

shattering, vc-dimension 

definition: 

vc-dimension (vapnik-chervonenkis dimension) 

the vc-dimension of a hypothesis space h is the cardinality of 
the largest set s that can be shattered by h. 

if arbitrarily large finite sets can be shattered by h, then 
vcdim(h) =     

to show that vc-dimension is d: 

    there exists a set of d points that can be shattered 

    there is no set of d+1 points that can be shattered. 

fact: if h is finite, then vcdim (h)     log (|h|). 

shattering, vc-dimension 

if the vc-dimension is d, that means there exists a set of 
d points that can be shattered, but there is no set of d+1 
points that can be shattered. 

e.g., h= thresholds on the real line 

vcdim h = 1 

+ 

- 

- 

+ 

w 

e.g., h= intervals on the real line 

- 

+ 

- 

vcdim h = 2 

+ 

- 

+ 

shattering, vc-dimension 

if the vc-dimension is d, that means there exists a set of 
d points that can be shattered, but there is no set of d+1 
points that can be shattered. 

e.g., h= union of k intervals on the real line 

vcdim h = 2k 

- 

+ 

- 

+ 

- 

+ 

vcdim h     2k 

            a sample of size 2k shatters 

(treat each pair of points as a 
separate case of intervals)  

vcdim h < 2k + 1 

+ 

- 

+ 

- 

+ 

    

shattering, vc-dimension 

e.g., h= linear separators in r2 

vcdim h     3 

shattering, vc-dimension 

e.g., h= linear separators in r2 

vcdim h < 4 

case 1: one point inside the triangle formed by 
the others. cannot label inside point as positive 
and outside points as negative. 

case 2: all points on the boundary (convex hull).  
cannot label two diagonally as positive and other 
two as negative. 

fact: vcdim of linear separators in rd is d+1 

sauer   s lemma  

sauer   s lemma: 

let d = vcdim(h) 

    m     d, then h m = 2m 
    m>d, then h m = o m      

proof: induction on m and d. cool combinatorial argument! 

hint: try proving it for intervals    

sample complexity: infinite hypothesis spaces  

realizable case 

 
 
 

 

 

 
 
 
 
  
 

sauer   s lemma: h m = o mvcdim h  

sample complexity: infinite hypothesis spaces  

realizable case 

 
 
 

 

 

 
 
 
 
  
 

e.g., h= linear separators in rd 

sample complexity linear in d 

so, if double the number of features, then i only need 
roughly twice the number of samples to do well. 

nearly matching bounds 

theorem (lower bound):  
for any     , any algo     , any 0 <      < 1/8, any      < .01,      distr.       and 
target                  s.t. if      sees fewer than max
examples, then with prob.         ,      produces     with                      >     . 

                             1

log

32    

1
    

1
    

 

,

lower bound (simpler form) 

    theorem: for any      there exists      such that any algorithm 
. 

 examples to reach error      with prob    

needs   

                         

3
4

    

    proof: consider      =                     (    ) shattered points: 

prob 1     4     

prob  4    
       1

 each 

    consider target         that labels these points randomly. 
    unless i see roughly    of the rare points, have error           

    each example has only prob 4     of being one of the rare points, 

and need to see        1
2

 rare points, so need to see   

    
    

 total. 

what if c        h? 

uniform convergence 

   

 this basic result only bounds the chance that a bad hypothesis looks 
perfect on the data. what if there is no perfect h   h (agnostic case)? 

    what can we say if c        h? 
   

 can we say that whp all h   h satisfy |errd(h)     errs(h)|        ? 

    called    uniform convergence   . 
    motivates optimizing over s, even if we can   t find a 

perfect function. 

sample complexity:  finite hypothesis spaces 

realizable case 

 
 
 
 

 

 

 
 
agnostic case 
 
 
what if there is no perfect h?  
  
 

 
 
 
 

 

 

 
 
 
to prove bounds like this, need some good tail inequalities. 
 
 
  
 

hoeffding bounds 

consider coin of bias p flipped m times.   
 let n be the observed # heads.  let        [0,1]. 
hoeffding bounds: 
    pr[n/m > p +    ]     e-2m   2, and 
    pr[n/m < p -    ]     e-2m   2. 

exponentially decreasing tails 

    tail inequality: bound id203 mass in tail of 

distribution (how concentrated is a random variable 
around its expectation). 

sample complexity:  finite hypothesis spaces 

agnostic case 

 
 
 
 

 

 

   

   

proof: just apply hoeffding. 
    chance of failure at most 2|h|e-2|s|   2. 
    set to    . solve. 
so, whp, best on sample is    -best over d. 
    note: this is worse than previous bound (1/    has become 1/   2), 

 
 
 
 
  
 

because we are asking for something stronger. 

    can also get bounds    between    these two. 

what you should know 

    notion of sample complexity. 

    understand reasoning behind the simple sample 

complexity bound for finite h. 

    shattering, vc dimension as measure of complexity, 

sauer   s lemma, form of the vc bounds (upper and 
lower bounds). 

