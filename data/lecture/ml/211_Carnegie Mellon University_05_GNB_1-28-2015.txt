machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

january 28, 2015 

today: 
       na  ve bayes 

       discrete-valued xi   s 
       document classification 
       gaussian na  ve bayes 
      
       brain image classification 
 
 

real-valued xi   s 

readings: 
 
required: 
       mitchell:    na  ve bayes and 

id28    

     (available on class website) 
 
optional 
       bishop 1.2.4 
       bishop 4.2  

recently: 
       bayes classifiers to learn p(y|x) 
       id113 and map estimates for parameters of p 
       conditional independence 
       na  ve bayes       make bayesian learning practical 

next: 
       text classification 
       na  ve bayes and continuous variables xi: 

       gaussian na  ve bayes classifier 

       learn p(y|x) directly 

       id28, id173, gradient ascent 

       na  ve bayes or id28? 

       generative vs. discriminative classifiers 

na  ve bayes in a nutshell 
bayes rule: 

assuming conditional independence among xi   s: 
 
 
so, classification rule for xnew = < x1,    , xn > is: 
 

       d=1 iff drive or carpool to cmu 
       b=1 iff birthday is before july 1 
 

example: live in sq hill?  p(s|g,d,b) 
       s=1 iff live in squirrel hill 
       g=1 iff shop at sh giant eagle 
 
p(s=1) : 
p(d=1 | s=1) : 
p(d=1 | s=0) : 
p(g=1 | s=1) : 
p(g=1 | s=0) : 
p(b=1 | s=1) : 
p(b=1 | s=0) : 

p(s=0) : 
p(d=0 | s=1) : 
p(d=0 | s=0) : 
p(g=0 | s=1) : 
p(g=0 | s=0) : 
p(b=0 | s=1) : 
p(b=0 | s=0) : 

      
 
tom: d=1, g=0, b=0 
 
p(s=1|d=1,g=0,b=0) = 
 
                                         p(s=1) p(d=1|s=1) p(g=0|s=1) p(b=0|s=1)      
 
_____________________________________________________________________________ 
 
 
 
 [p(s=1) p(d=1|s=1) p(g=0|s=1) p(b=0|s=1) + p(s=0) p(d=1|s=0) p(g=0|s=0) p(b=0|s=0)] 
     
 

another way to view na  ve bayes (boolean y): 
decision rule: is this quantity greater or less than 1? 

 

na  ve bayes: classifying text documents 

       classify which emails are spam? 
       classify which emails promise an attachment? 

 
how shall we represent text documents for na  ve 

bayes? 

learning to classify documents: p(y|x) 

 

       y discrete valued.   

       e.g., spam or not 

       x = <x1, x2,     xn> = document 
 
 
       xi is a random variable describing    

learning to classify documents: p(y|x) 

       y discrete valued.   

       e.g., spam or not 

 

       x = <x1, x2,     xn> = document 
 
 
       xi is a random variable describing    
answer 1: xi is boolean, 1 if word i is in document, else 0 
                 e.g., xpleased = 1 
 
 
 
issues? 

learning to classify documents: p(y|x) 

       y discrete valued.   

       e.g., spam or not 

 

       x = <x1, x2,     xn> = document 
 
 
       xi is a random variable describing    
answer 2:  
       xi represents the ith word position in document 
       x1 =    i   ,  x2 =    am   , x3 =    pleased    
       and, let   s assume the xi are iid (indep, identically distributed) 

learning to classify document: p(y|x) 

the    bag of words    model 

       y discrete valued.  e.g., spam or not 
       x = <x1, x2,     xn> = document 
 
       xi are iid random variables.  each represents the word at its 

position i in the document 

       generating a document according to this distribution = 

rolling a 50,000 sided die, once for each word position in the 
document 

       the observed counts for each word follow a ??? distribution 
 

multinomial distribution 

multinomial bag of words 

aardvark  0 
 2 
about
all
 2 
 1 
africa
apple
 0 
 0 
anxious
... 
gas
... 
oil
    
zaire

 0 

 1 

 1 

map estimates for bag of words 
 
map estimate for multinomial 
 
 
 
 
what      s should we choose?  

na  ve bayes algorithm     discrete xi  
       train na  ve bayes (examples)   
 for each value yk
 
 
 

 estimate 
 for each value xij of each attribute xi
 

 estimate 

 
 
       classify (xnew)   
 

prob that word xij appears 

in position i, given y=yk 

 * additional assumption:  word probabilities are position independent 

for code and data, see 
www.cs.cmu.edu/~tom/mlbook.html  
click on    software and data    

what if we have continuous xi ? 
eg., image classification: xi is real-valued ith pixel 
 

what if we have continuous xi ? 
eg., image classification: xi is real-valued ith pixel 
 
na  ve bayes requires p(xi | y=yk), but xi is real (continuous) 
 
 
 
 
common approach: assume p(xi | y=yk) follows a normal 
 
 
 

(gaussian) distribution 

what if we have continuous xi ? 
eg., image classification: xi is real-valued ith pixel 
 
na  ve bayes requires p(xi | y=yk), but xi is real (continuous) 
 
 
 
 
common approach: assume p(xi | y=yk) follows a normal 
 
 
 

(gaussian) distribution 

gaussian 
distribution 
(also called    normal   ) 
 
p(x) is a id203 
density function, whose 
integral (not sum) is 1 

what if we have continuous xi ? 
gaussian na  ve bayes (gnb): assume 
 
 
 
 
sometimes assume variance 
      
is independent of y (i.e.,   i),  
       or independent of xi (i.e.,   k) 
       or both (i.e.,   ) 

gaussian na  ve bayes algorithm     continuous xi   
(but still discrete y) 

       train na  ve bayes (examples)   
 for each value yk
 
 

 estimate* 
 for each attribute xi estimate  
       class conditional mean        , variance        

 
       classify (xnew)   
 

 * probabilities must sum to 1, so need estimate only n-1 parameters... 

estimating parameters: y discrete, xi continuous  

maximum likelihood estimates: 

jth training 

example 

ith feature 

kth class 

  ()=1 if (yj=yk) 

else 0 

how many parameters must we estimate for gaussian 
na  ve bayes if y has k possible values, x=<x1,     xn>? 
 

gnb example: classify a person   s 
cognitive state, based on brain image 
       reading a sentence or viewing a picture? 
       reading the word describing a    tool    or    building   ?   
       answering the question, or getting confused? 

mean activations over all training examples for y=   bottle    

y is the mental state (reading    house    or    bottle   ) 
xi are the voxel activities,  
 
this is a plot of the      s defining p(xi | y=   bottle   ) 

fmri 

activation  
high 

average 

below 
average 

classification task: is person viewing a    tool    or    building   ? 

 

y
c
 
a
y
c
r
u
a
r
c
u
c
c
a
c
 
a
n
n
o
o
i
t
i
a
t
a
c
c
i
i
f
f
i
i
s
s
s
s
a
a
c
c

l

l

statistically 
significant 

p<0.05 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

p4 p8 p6 p11 p5 p7 p10 p9 p2 p12 p3 p1

participants

where is information encoded in the brain? 

accuracies of  

cubical 
27-voxel  
classifiers 
centered at 

each significant 

voxel 

[0.7-0.8] 

na  ve bayes: what you should know 
       designing classifiers based on bayes rule 
       conditional independence 

       what it is 
       why it   s important 

       which (and how many) parameters must be estimated under 

       na  ve bayes assumption and its consequences 
different generative models (different forms for p(x|y) ) 
       and why this matters 

       how to train na  ve bayes classifiers 
       id113 and map estimates  
       with discrete and/or continuous inputs xi 

questions to think about: 
       can you use na  ve bayes for a combination of 

discrete and real-valued xi?  

       how can we easily model just 2 of n attributes as 

dependent? 

       what does the decision surface of a na  ve bayes 

classifier look like? 

       how would you select a subset of xi   s? 

simple picture for gnb for p(y|x1) 

