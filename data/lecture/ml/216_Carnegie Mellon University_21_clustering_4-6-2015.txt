id91.  

unsupervised learning 

maria-florina balcan 

04/06/2015 

reading: 
    chapter 14.3: hastie, tibshirani, friedman. 

additional resources: 
    center based id91: a foundational perspective.  
     awasthi, balcan. handbook of id91 analysis. 2015.  

logistics 

    project: 

    midway review due today. 
    final report, may 8. 
    poster presentation, may 11. 
    communicate with your mentor ta! 

    exam #2 on april 29th. 

id91, informal goals 

goal: automatically partition unlabeled data into groups of 
similar datapoints. 

 question: when and why would we want to do this? 

 useful for: 

    automatically organizing data. 

    understanding hidden structure in data. 

    preprocessing for further analysis. 

    representing high-dimensional data in a low-dimensional space 
(e.g., for visualization purposes). 

applications (id91 comes up everywhere   ) 

    cluster news articles or web pages or search results by topic. 

 

 

    cluster protein sequences by function or genes according to expression 

profile. 

 

    cluster users of social networks by interest (community detection). 

facebook network 

twitter network 

applications (id91 comes up everywhere   ) 

    cluster customers according to purchase history. 

 

 

    cluster galaxies or nearby stars (e.g. sloan digital sky survey) 

 
    and many many more applications   . 

id91 

today: 

    objective based id91 
    hierarchical id91 
    mention overlapping clusters 

[march 4th: em-style algorithm for id91 for mixture of gaussians (specific 
probabilistic model).] 

objective based id91 

input: a set  s of n points, also a distance/dissimilarity 
measure specifying the distance d(x,y) between pairs (x,y). 

e.g., # keywords in common, id153, wavelets coef., etc. 

goal: output a partition of the data. 

    id116: find center pts         ,         ,     ,          to  
n  minj    1,   ,k d2(        ,         ) 
                minimize     i=1 

    k-median: find center pts         ,         ,     ,          to  
n  minj    1,   ,k d(        ,         ) 
                minimize     i=1 

y 
c1 

  x 

s  c3 

z 

c2 

    k-center: find partition to minimize the maxim radius 

euclidean id116 id91 

input: a set of n datapoints         ,         ,     ,          in rd 

target #clusters k 

output: k representatives         ,         ,     ,              rd 

objective: choose         ,         ,     ,              rd to minimize 

n  minj    1,   ,k

   i=1 

                     

2

 

euclidean id116 id91 

input: a set of n datapoints         ,         ,     ,          in rd  

target #clusters k 

output: k representatives         ,         ,     ,              rd 

objective: choose         ,         ,     ,              rd to minimize 

n  minj    1,   ,k

   i=1 

                     

2

 

natural assignment: each point assigned to its 
closest center, leads to a voronoi partition. 

euclidean id116 id91 

input: a set of n datapoints         ,         ,     ,          in rd  

target #clusters k 

output: k representatives         ,         ,     ,              rd 

objective: choose         ,         ,     ,              rd to minimize 

n  minj    1,   ,k

   i=1 

                     

2

 

computational complexity: 

np hard: even for k = 2 [dagupta   08] or 
d = 2 [mahajan-nimbhorkar-varadarajan09] 

there are a couple of easy cases    

an easy case for id116: k=1 

input: a set of n datapoints         ,         ,     ,          in rd 

output:          rd to minimize     i=1 

n 

                 

2

 

solution: 

the optimal choice is      =

1
n

n           

   i=1 

idea: bias/variance like decomposition 

n 

   i=1 

                 

2

=              

1
n

2

+

1
n

n 

   i=1 

                 

2

 

avg id116 cost wrt c 

avg id116 cost wrt    

so, the optimal choice for      is     . 

another easy case for id116: d=1 

input: a set of n datapoints         ,         ,     ,          in rd 

output:          rd to minimize     i=1 

n 

                 

2

 

extra-credit homework question 
hint: id145 in time o(n2k). 

common heuristic in practice: 

the lloyd   s method 

[least squares quantization in pcm, lloyd, ieee transactions on id205, 1982] 

input: a set of n datapoints         ,         ,     ,          in rd 

initialize centers         ,         ,     ,              rd and 
               clusters c1, c2,     , ck in any way. 

repeat until there is no further change in the cost. 

    for each j:  cj    {              whose closest center is         } 

    for each j:             mean of cj 

common heuristic in practice: 

the lloyd   s method 

[least squares quantization in pcm, lloyd, ieee transactions on id205, 1982] 

input: a set of n datapoints         ,         ,     ,          in rd 

initialize centers         ,         ,     ,              rd and 
               clusters c1, c2,     , ck in any way. 

repeat until there is no further change in the cost. 

    for each j:  cj    {              whose closest center is         } 

    for each j:             mean of cj 

holding         ,         ,     ,          fixed, 
pick optimal c1, c2,     , ck 

holding c1, c2,     , ck fixed, 
pick optimal         ,         ,     ,           

common heuristic: the lloyd   s method 

input: a set of n datapoints         ,         ,     ,          in rd 

initialize centers         ,         ,     ,              rd and 
               clusters c1, c2,     , ck in any way. 

repeat until there is no further change in the cost. 

    for each j:  cj    {              whose closest center is         } 

    for each j:             mean of cj 

note: it always converges. 

    the cost always drops and  
    there is only a finite #s of voronoi partitions 

(so a finite # of values the cost could take) 

initialization  for the lloyd   s method 

input: a set of n datapoints         ,         ,     ,          in rd 
initialize centers         ,         ,     ,              rd and 
               clusters c1, c2,     , ck in any way. 
repeat until there is no further change in the cost. 

    for each j:  cj    {              whose closest center is         } 

    for each j:             mean of cj 

    initialization is crucial (how fast it converges, quality of solution output) 

    discuss techniques  commonly used in practice  

    random centers from the datapoints (repeat a few times) 

    furthest traversal 

    id116 ++ (works well and has provable guarantees) 

lloyd   s method: random initialization 

lloyd   s method: random initialization 

example: given a set of datapoints 

lloyd   s method: random initialization 

select initial centers at random 

lloyd   s method: random initialization 

assign each point to its nearest center 

lloyd   s method: random initialization 

recompute optimal centers given a fixed id91 

lloyd   s method: random initialization 

assign each point to its nearest center 

lloyd   s method: random initialization 

recompute optimal centers given a fixed id91 

lloyd   s method: random initialization 

assign each point to its nearest center 

lloyd   s method: random initialization 

recompute optimal centers given a fixed id91 

get a good  quality solution in this example. 

lloyd   s method: performance 

it always converges, but it may converge at a local optimum 
that is different from the global optimum, and in fact could 
be arbitrarily worse in terms of its score. 

lloyd   s method: performance 

local optimum: every point is assigned to its nearest center 
and every center is the mean value of its points. 

lloyd   s method: performance 

.it is arbitrarily worse than optimum solution   . 

lloyd   s method: performance 

this bad performance, can happen 
even with well separated gaussian 
clusters. 

lloyd   s method: performance 

this bad performance, can 
happen even with well 
separated gaussian clusters. 

some gaussian are 
combined   .. 

lloyd   s method: performance 
    if we do random initialization, as k increases, it becomes 
more likely we won   t have perfectly picked one center per 
gaussian in our initialization (so lloyd   s method will output 
a bad solution). 

    for k equal-sized gaussians, pr[each initial center is in a 

different gaussian]    

    !
            

1

         

    becomes unlikely as k gets large.  

another initialization idea: furthest 

point heuristic 

choose          arbitrarily (or at random). 

    for j = 2,     , k 

    pick          among datapoints         ,         ,     ,          that is 

farthest from previously chosen         ,         ,     ,                 

fixes the gaussian problem. but it can be thrown 
off by outliers   . 

furthest point heuristic does well on 

previous example 

furthest point initialization heuristic 

sensitive to outliers 

assume k=3 

(-2,0) 

(0,1) 

(0,-1) 

(3,0) 

furthest point initialization heuristic 

sensitive to outliers 

assume k=3 

(-2,0) 

(0,1) 

(0,-1) 

(3,0) 

id116++ initialization: d2 sampling [av07] 

   

   

interpolate between random and furthest point initialization 

let d(x) be the distance between a point      and its nearest 
center. chose the next center proportional to d2(    ). 

    choose          at random. 

    for j = 2,     , k 

    pick          among         ,         ,     ,          according to the distribution 

        (         =         )                        <                              

    

 

d2(        ) 

theorem: id116++ always attains an o(log k) approximation to 
optimal id116 solution in expectation. 

running lloyd   s can only further improve the cost. 

id116++ idea: d2 sampling 

   

   

interpolate between random and furthest point initialization 

let d(x) be the distance between a point      and its nearest 
center. chose the next center proportional to d    (    ). 

         = 0, random sampling 

         =    , furthest point  (side note: it actually works well for k-center) 

         = 2, id116++  

side note:      = 1, works well for k-median  

id116 ++ fix 

(-2,0) 

(0,1) 

(0,-1) 

(3,0) 

id116++/ lloyd   s running time 

    id116 ++ initialization: o(nd) and one pass over data to 

select next center. so o(nkd) time in total. 

    lloyd   s method 

repeat until there is no change in the cost. 

   

for each j:  cj    {              whose closest center is         } 

   

for each j:             mean of cj 

each round takes 
time o(nkd). 

    exponential # of rounds in the worst case [av07]. 

    expected polynomial time in the smoothed analysis model! 

id116++/ lloyd   s summary 

    id116++ always attains an o(log k) approximation to optimal 

id116 solution in expectation. 

    running lloyd   s can only further improve the cost. 

    exponential # of rounds in the worst case [av07]. 

    expected polynomial time in the smoothed analysis model! 

    does well in practice. 

what value of k??? 

    heuristic: find large gap between k -1-means cost 

and id116 cost. 

    hold-out validation/cross-validation on auxiliary 

task (e.g., supervised learning task). 

    try hierarchical id91. 

hierarchical id91 

all topics 

sports 

fashion 

soccer 

tennis  

gucci 

lacoste  

    a hierarchy might be more natural. 

    different users might care about different levels of 

granularity or even prunings. 

hierarchical id91 

top-down (divisive) 

    partition data into 2-groups (e.g., 2-means) 

    recursively cluster each group. 

bottom-up (agglomerative) 

    start with every point in its own cluster. 

all topics 

    repeatedly merge the    closest    two clusters. 

    different defs of    closest    give different 

algorithms. 

sports 

fashion 

soccer 

tennis  

gucci 

lacoste  

bottom-up (agglomerative) 

have a distance measure on pairs of objects. 

all topics 

d(x,y)     distance between x and y 

e.g., # keywords in common, id153, etc 

sports 

fashion 

soccer 

tennis  

gucci 

lacoste  

     single linkage: 

dist a,      = min

x   a,x      b   

dist(x, x   ) 

     complete linkage: 

dist a, b = max

x   a,x      b   

dist(x, x   ) 

     average linkage: 

dist a, b = avg

x   a,x      b   

dist(x, x   ) 

     wards    method 

single linkage 

bottom-up (agglomerative) 
    start with every point in its own cluster. 
    repeatedly merge the    closest    two clusters. 

single linkage: dist a,      = min

x   a,x          

dist(x, x   ) 

dendogram 

5 

a b c d e f 

4 

a b c d e 

3 

a b c 

1 

a b 

2 

d e 

-3 
a 

-2 
b 

0 
c 

2.1 
d 

3.2 
e 

6 
f 

single linkage 

bottom-up (agglomerative) 
    start with every point in its own cluster. 
    repeatedly merge the    closest    two clusters. 

single linkage: dist a,      = min

x   a,x          

dist(x, x   ) 

one way to think of it: at any moment, we see connected components 
of the graph where connect any two pts of distance < r.  

watch as r grows (only n-1 relevant values because we only we merge 
at value of r corresponding to values of r in different clusters).  

1 

3 

-3 
a 

-2 
b 

4 

0 
c 

5 

2 

2.1 
d 

3.2 
e 

6 
f 

complete linkage 

bottom-up (agglomerative) 
    start with every point in its own cluster. 
    repeatedly merge the    closest    two clusters. 

complete linkage: dist a, b = max

x   a,x      b

dist(x, x   ) 

one way to think of it: keep max diameter as small as possible at 
any level. 

5 

a b c d e f 

3 

a b c 

4 

def 

1 

a b 

2 

d e 

-3 
a 

-2 
b 

0 
c 

2.1 
d 

3.2 
e 

6 
f 

complete linkage 

bottom-up (agglomerative) 
    start with every point in its own cluster. 
    repeatedly merge the    closest    two clusters. 

complete linkage: dist a, b = max

x   a,x      b

dist(x, x   ) 

one way to think of it: keep max diameter as small as possible. 

1 

3 

5 

2 

4 

-3 
a 

-2 
b 

0 
c 

2.1 
d 

3.2 
e 

6 
f 

ward   s method  

bottom-up (agglomerative) 
    start with every point in its own cluster. 
    repeatedly merge the    closest    two clusters. 

ward   s method: dist c, c    =

c     c   
c + c    mean c     mean c   

2 

merge the two clusters such that the increase in id116 cost is 
as small as possible. 

works well in practice. 

5 

1 

3 

2 

4 

-3 
a 

-2 
b 

0 
c 

2.1 
d 

3.2 
e 

6 
f 

running time 

    each algorithm starts with n clusters, and performs n-1 merges.  

    for each algorithm, computing                 (    ,        ) can be done in time 

    (                  ).  (e.g., examining                 (    ,        ) for all              ,                    ) 

    time to compute all pairwise distances and take smallest is     (    2). 
    overall time is     (    3). 

in fact, can run all these algorithms in time     (    2 log     ). 

see: christopher d. manning, prabhakar raghavan and hinrich sch  tze, introduction to 
information retrieval, cambridge university press. 2008. http://www-nlp.stanford.edu/ir-book/ 

hierarchical id91 experiments 

[blg, jmlr   15] 

ward   s method does the best among classic techniques. 

hierarchical id91 experiments 

[blg, jmlr   15] 

ward   s method does the best among classic techniques. 

what you should know 

    partitional id91. id116 and id116 ++ 

    lloyd   s method 

    initialization techniques (random, furthest 

traversal, id116++) 

    hierarchical id91. 

   

 single linkage, complete linkge, ward   s method 

additional slides 

smoothed analysis model 

    imagine a worst-case input. 

    but then add small gaussian perturbation to each data point. 

smoothed analysis model 

    imagine a worst-case input. 

    but then add small gaussian perturbation to each data point. 

    theorem [arthur-manthey-roglin 2009]:  

 

- e[number of rounds until lloyd   s converges] if add gaussian 

perturbation with variance     2 is polynomial in     , 1/    . 

- the actual bound is :     

    34    34    8

    6

 

    might still find local opt that is far from global opt. 

overlapping clusters: communities 

christos 

papadimitriou 

colleagues at 

berkeley 

databases 
systems 

tcs 

algorithmic game 

theory 

overlapping clusters: communities 

    social networks 

    professional networks 

    product purchasing networks, id191, 

biological networks, etc 

overlapping clusters: communities 

baby's favorite 

songs  

cds 

lullabies 

kids 

electronics 

