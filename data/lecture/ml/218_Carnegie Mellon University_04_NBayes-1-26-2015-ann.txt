machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

january 26, 2015 

today: 
       bayes classifiers 
       conditional independence 
       na  ve bayes 

readings: 
mitchell:  
      na  ve bayes and logistic 

regression    

     (available on class website) 

two principles for estimating parameters 
       maximum likelihood estimate (id113): choose    that 

maximizes id203 of observed data 

       maximum a posteriori (map) estimate: choose    that 
is most probable given prior id203 and the data 

maximum likelihood estimate 

x=1 

x=0 
p(x=1) =    
p(x=0) = 1-   
(bernoulli) 

 

maximum a posteriori (map) estimate 

x=1 

x=0 

let   s learn classifiers by learning p(y|x) 
consider y=wealth,  x=<gender, hoursworked> 
 
 
 
 
 
 
 
 
 

p(rich | g,hw)  p(poor | g,hw) 

hrsworked 

gender 

f 
f 
m 
m 

<40.5 
>40.5 
<40.5 
>40.5 

.09 
.21 
.23 
.38 

.91 
.79 
.77 
.62 

how many parameters must we estimate? 
suppose x =<x1,    xn>  
where xi and y are boolean rv   s 
 
to estimate p(y| x1, x2,     xn) 
 
 
 
 
if we have 30 boolean xi   s:  p(y | x1, x2,     x30) 
  
 
 
 
 
 

bayes rule 
 

which is shorthand for: 

equivalently: 

can we reduce params using bayes rule? 
suppose x =<x1,    xn>  
where xi and y are boolean rv   s 
 
how many parameters to define p(x1,    xn | y)? 
 
 
 
 
how many parameters to define p(y)? 

na  ve bayes 

na  ve bayes assumes 

   i.e., that xi and xj are conditionally 

independent given y, for all i   j 

conditional independence   
definition: x is conditionally independent of y  given z, if 
the id203 distribution governing x is independent 
of the value of y, given the value of z 

 
 
 
which we often write  
 
 
e.g., 
 
 

na  ve bayes uses assumption that the xi are conditionally 

independent, given y.   e.g., 

 
given this assumption, then: 
 
 
 

na  ve bayes uses assumption that the xi are conditionally 

independent, given y.   e.g., 

 
given this assumption, then: 
 
 
 
 
in general: 
 
 

na  ve bayes uses assumption that the xi are conditionally 

independent, given y.   e.g., 

 
given this assumption, then: 
 
 
 
 
in general: 
 
how many parameters to describe p(x1   xn|y)?  p(y)? 
       without conditional indep assumption? 
       with conditional indep assumption? 

na  ve bayes in a nutshell 
bayes rule: 

assuming conditional independence among xi   s: 
 
 
 
so, to pick most probable y for xnew = < x1,    , xn >  
 

na  ve bayes algorithm     discrete xi  
       train na  ve bayes (examples)   
 for each* value yk
 
 
 

 estimate 
 for each* value xij of each attribute xi
 

 estimate 

 
       classify (xnew)   
 

 * probabilities must sum to 1, so need estimate only n-1 of these... 

estimating parameters: y, xi discrete-valued  
maximum likelihood estimates (id113   s): 

number of items in 

dataset d for which y=yk 

example: live in sq hill?  p(s|g,d,b) 
       s=1 iff live in squirrel hill 
       g=1 iff shop at sh giant eagle 

       d=1 iff drive or carpool to cmu 
       b=1 iff birthday is before july 1 
 

what id203 parameters must we estimate? 

       d=1 iff drive or carpool to cmu 
       b=1 iff birthday is before july 1 
 

example: live in sq hill?  p(s|g,d,e) 
       s=1 iff live in squirrel hill 
       g=1 iff shop at sh giant eagle 
 
p(s=1) : 
p(d=1 | s=1) : 
p(d=1 | s=0) : 
p(g=1 | s=1) : 
p(g=1 | s=0) : 
p(b=1 | s=1) : 
p(b=1 | s=0) : 

p(s=0) : 
p(d=0 | s=1) : 
p(d=0 | s=0) : 
p(g=0 | s=1) : 
p(g=0 | s=0) : 
p(b=0 | s=1) : 
p(b=0 | s=0) : 

na  ve bayes: subtlety #1 

often the xi are not really conditionally independent 
 
       we use na  ve bayes in many cases anyway, and 

it often works pretty well 
       often the right classification, even when not the right 

id203 (see [domingos&pazzani, 1996]) 

 

       what is effect on estimated p(y|x)? 

       extreme case: what if we add two copies: xi  = xk   

extreme case: what if we add two copies: xi  = xk  

na  ve bayes: subtlety #2 

if unlucky, our id113 estimate for p(xi | y) might be zero.  

(for example, xi = birthdate.  xi = jan_25_1992) 

 
       why worry about just one parameter out of many? 

       what can be done to address this? 

estimating parameters 
       maximum likelihood estimate (id113): choose    that 

maximizes id203 of observed data 

       maximum a posteriori (map) estimate: choose    that 
is most probable given prior id203 and the data 

estimating parameters: y, xi discrete-valued  
maximum likelihood estimates: 

map estimates (beta, dirichlet priors): 
 

only difference: 

   imaginary    examples 

learning to classify text documents 
       classify which emails are spam? 
       classify which emails promise an attachment? 
       classify which web pages are student home pages? 

how shall we represent text documents for na  ve bayes? 

baseline: bag of words approach 

aardvark  0 
 2 
about
all
 2 
 1 
africa
apple
 0 
 0 
anxious
... 
gas
... 
oil
    
zaire

 0 

 1 

 1 

learning to classify document: p(y|x) 

the    bag of words    model 

       y discrete valued.  e.g., spam or not 
       x = <x1, x2,     xn> = document 
 
       xi is a random variable describing the word at position i in 

the document 

       possible values for xi : any word wk in english 
 
       document = bag of words: the vector of counts for all 

wk   s 
       like #heads, #tails, but we have many more than 2 values 
       assume word probabilities are position independent  

(i.i.d. rolls of a 50,000-sided die) 

na  ve bayes algorithm     discrete xi  
       train na  ve bayes (examples)   
 for each value yk
 
 
 

 estimate 
 for each value xj of each attribute xi
 

 estimate 

 
 
       classify (xnew)   
 

prob that word xj appears 
in position i, given y=yk 

 * additional assumption:  word probabilities are position independent 

map estimates for bag of words 
 
map estimate for multinomial 
 
 
 
 
what      s should we choose?  

for code and data, see 
www.cs.cmu.edu/~tom/mlbook.html  
click on    software and data    

what you should know: 
       training and using classifiers based on bayes rule 
       conditional independence 

       what it is 
       why it   s important 

       na  ve bayes 

       what it is 
       why we use it so much 
       training using id113, map estimates 
       discrete variables and continuous (gaussian) 

questions: 
 
       how can we extend na  ve bayes if just 2 of the xi   s 

are dependent? 

       what does the decision surface of a na  ve bayes 

classifier look like? 

       what error will the classifier achieve if na  ve bayes 
assumption is satisfied and we have infinite training 
data? 

       can you use na  ve bayes for a combination of 

discrete and real-valued xi?  

