coms 4721: machine learning for data science

lecture 19, 4/6/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

principal component

analysis

id84

we   re given data x1, . . . , xn, where x     rd. this data is often
high-dimensional, but the    information    doesn   t use the full d dimensions.

for example, we could represent the above images with three numbers since
they have three degrees of freedom. two for shifts and a third for rotation.

principal component analysis can be thought of as a way of automatically
mapping data xi into some new low-dimensional coordinate system.

(cid:73) it capture most of the information in the data in a few dimensions
(cid:73) extensions allow us to handle missing data, and    unwrap    the data.

principal component analysis

example: how can we approximate this data using a unit-length vector q?

q is a unit-length vector, qtq = 1.

red dot: the length, qtxi, to the axis after
projecting x onto the line de   ned by q.

the vector (qtxi)q takes q and stretches it
to the corresponding red dot.

so what   s a good q? how about minimizing the squared approximation error,

q = arg min
q

(cid:107)xi     qqtxi(cid:107)2

subject to qtq = 1

n(cid:88)

i=1

qqtxi = (qtxi)q : the approximation of xi by stretching q to the    red dot.   

qxi(qtxi)qpca : the first principal component

this is related to the problem of    nding the largest eigenvalue,

(cid:107)xi     qqtxi(cid:107)2

s.t. qtq = 1

n(cid:88)
n(cid:88)

i=1

i=1

q = arg min
q

= arg min

q

i xi     qt
xt

(cid:32) n(cid:88)
(cid:124)
(cid:123)(cid:122)

i=1

= xxt

(cid:33)
(cid:125)

xixt
i

q

we   ve de   ned x = [x1, . . . , xn]. since the    rst term doesn   t depend on q and
we have a negative sign in front of the second term, equivalently we solve

q = arg max

q

qt (xxt )q

subject to qtq = 1

this is the eigendecomposition problem:

(cid:73) q is the    rst eigenvector of xxt
(cid:73)    = qt (xxt )q is the    rst eigenvalue

pca: general

the general form of pca considers k eigenvectors,

q = arg min
q

= arg min

q

n(cid:88)
n(cid:88)

i=1

(cid:107)xi     k(cid:88)
(cid:124)
i xi     k(cid:88)

k=1

xt

i=1

k=1

approximates x

(xt

i qk)qk

(cid:123)(cid:122)

qt
k

(cid:107)2

(cid:125)
(cid:32) n(cid:88)
(cid:124)
(cid:123)(cid:122)

i=1

= xxt

xixt
i

s.t. qt

k qk(cid:48) =

(cid:33)
(cid:125)

qk

(cid:26) 1, k = k(cid:48)

0, k (cid:54)= k(cid:48)

the vectors in q = [q1, . . . , qk] give us a k dimensional subspace with
which to represent the data:

          ,

          qt

1 x
...
qt
kx

x     k(cid:88)

k=1

xproj =

(qt

k x)qk = qxproj

the eigenvectors of (xxt ) can be learned using built-in software.

eigenvalues, eigenvectors and the svd

an equivalent formulation of the
problem is to    nd (  , q) such that

(xxt )q =   q

since (xxt ) is a psd matrix,
there are r     min{d, n} pairs,
  1       2                  r > 0,
qt
qt
k qk(cid:48) = 0
k qk = 1,

why is (xxt ) psd? using the svd, x = usvt, we have that
  i = (s2)ii     0

(xxt ) = us2ut     q = u,

preprocessing: usually we    rst subtract off the mean of each dimension of x.

pca: example of projecting from r3 to r2

for this data, most information (structure in the data) can be captured in r2.

(left) the original data in r3. the hyperplane is de   ned by q1 and q2.
(right) the new coordinates for the data: xi     xproj

.

i =

(cid:21)

(cid:20) xt

i q1
xt
i q2

example: digits

data: 16  16 images of handwritten 3   s (as vectors in r256)

above: the    rst four eigenvectors q and their eigenvalues   .

above: reconstructing a 3 using the    rst m     1 eigenvectors plus the mean,
and approximation

x     mean +

(xtqk)qk

m   1(cid:88)

k=1

mean  1=3.4  105  2=2.8  105  3=2.4  105  4=1.6  105originalm=1m=10m=50m=250probabilistic pca

pca and the svd

we   ve discussed how any matrix x has a singular value decomposition,

x = usvt , utu = i, vtv = i

and s is a diagonal matrix with non-negative entries.

therefore,

xxt = us2ut     (xxt )u = us2

u is a matrix of eigenvectors, and s2 is a diagonal matrix of eigenvalues.

a modeling approach to pca

using the svd perspective of pca, we can also derive a probabilistic model
for the problem and use the em algorithm to learn it.

this model will have the advantages of:

(cid:73) handling the problem of missing data
(cid:73) allowing us to learn additional parameters such as noise
(cid:73) provide a framework that could be extended to more complex models
(cid:73) gives distributions used to characterize uncertainty in predictions
(cid:73) etc.

probabilistic pca

in effect, this is a new id105 model.

(cid:73) with the svd, we had x = usvt.
(cid:73) we now approximate x     wz, where

(cid:73) w is a d    k matrix. in different settings this is called a    factor loadings   
matrix, or a    dictionary.    it   s like the eigenvectors, but no orthonormality.
(cid:73) the ith column of z is called zi     rk. think of it as a low-dimensional

representation of xi.

the generative process of probabilistic pca is

xi     n(wzi,   2i),

zi     n(0, i).

in this case, we don   t know w or any of the zi.

the likelihood

maximum likelihood
our goal is to    nd the maximum likelihood solution of the matrix w under
the marginal distribution, i.e., with the zi vectors integrated out,

wml = arg max
w

ln p(x1, . . . , xn|w) = arg max

w

ln p(xi|w).

this is intractable because p(xi|w) = n(xi|0,   2i + wwt ),

n(xi|0,   2i + wwt ) =

(2  )

1

d

2 |  2i + wwt| 1

2

e    1

2 xt (  2i+wwt )   1x

we can set up an em algorithm that uses the vectors z1, . . . , zn.

n(cid:88)

i=1

em for probabilistic pca

setup
the marginal log likelihood can be expressed using em as

(cid:90)

ln

n(cid:88)

i=1

p(xi, zi|w) dzi =

+

q(zi) ln

q(zi) ln

p(xi, zi|w)

q(zi)

dzi

q(zi)

p(zi|xi, w)

dzi

    l

    kl

(cid:90)
(cid:90)

n(cid:88)
n(cid:88)

i=1

i=1

em algorithm: remember that em has two iterated steps
1. set q(zi) = p(zi|xi, w) for each i (making kl = 0) and calculate l
2. maximize l with respect to w
again, for this to work well we need that
(cid:73) we can calculate the posterior distribution p(zi|xi, w), and
(cid:73) maximizing l is easy, i.e., we update w using a simple equation

the algorithm

em for probabilistic pca
given: data x1:n, xi     rd and model xi     n(wzi,   2), zi     n(0, i), z     rk
output: point estimate of w and posterior distribution on each zi
e-step: set each q(zi) = p(zi|xi, w) = n(zi|  i,   i) where
  i = (i + wtw/  2)   1,   i =   iwtxi/  2

m-step: update w by maximizing the objective l from the e-step

(cid:35)(cid:34)

(cid:34) n(cid:88)

n(cid:88)
iterate e and m steps until increase in(cid:80)n

  2i +

xi  t
i

w =

i=1

i=1

(cid:35)   1

(  i  t

i +   i)

i=1 ln p(xi|w) is    small.   

comment:
(cid:73) the probabilistic framework gives a way to learn k and   2 as well.

example: image processing

for image problems such as denoising or inpainting (missing data)
(cid:73) extract overlapping patches (e.g., 8  8) and vectorize to construct x
(cid:73) model with a factor model such as probabilistic pca
(cid:73) approximate xi     w  i, where   i is the posterior mean of zi
(cid:73) reconstruct the image by replacing xi with w  i (and averaging)

data matrix, e.g., 64 x 262,144= 8 x 8 patchxexample: denoising

noisy image on left, denoised image on right. the noise variance parameter
  2 was learned for this example.

example: missing data

another somewhat extreme example:
(cid:73) image is 480  320  3 (rgb dimension)
(cid:73) throw away 80% at random
(cid:73) (left) missing data, (middle) reconstruction, (right) original image

kernel pca

kernel pca

we   ve seen how we can take an algorithm that uses dot products, xtx, and
generalize with a nonlinear kernel. this generalization can be made to pca.

recall: with pca we    nd the eigenvectors of the matrix(cid:80)n

i = xxt.

i=1 xixt

(cid:73) let   (x) be a feature mapping from rd to rd, where d (cid:29) d

(cid:73) we want to solve the eigendecomposition

(cid:34) n(cid:88)

(cid:35)

  (xi)  (xi)t

qk =   kqk

i=1

without having to work in the higher dimensional space.

(cid:73) that is, how can we do pca without explicitly using   (  ) and q?

kernel pca

notice that we can reorganize the operations of the eigendecomposition

n(cid:88)

  (xi)(cid:0)  (xi)tqk
(cid:123)(cid:122)

(cid:1) /  k
(cid:125)

= qk

(cid:124)
i=1 aki  (xi) for some vector ak     rn.

= aki

that is, the eigenvector qk =(cid:80)n

i=1

the trick is that instead of learning qk, we   ll learn ak.

plug this equation for qk back into the    rst equation:

n(cid:88)

n(cid:88)

i=1

j=1

(cid:124)

(cid:123)(cid:122)

(cid:125)

= k(xi,xj)

n(cid:88)

i=1

  (xi)

akj   (xi)t   (xj)

=   k

aki  (xi)

and multiply both sides by   (xl)t for each l     {1, . . . , n}.

kernel pca

when we multiply the following by   (xl)t for l = 1 . . . , n:

n(cid:88)

n(cid:88)

i=1

j=1

(cid:124)

(cid:123)(cid:122)

(cid:125)

= k(xi,xj)

n(cid:88)

i=1

  (xi)

akj   (xi)t   (xj)

=   k

aki  (xi)

we get a new set of linear equations

k2ak =   kkak        kak =   kak
where k is the n    n kernel matrix constructed on the data.

because k is guaranteed to be psd because it is a matrix of dot-products,
the lhs and rhs above share a solution for (  k, ak).

now perform    regular    pca, but on the kernel matrix k instead of the data
matrix xxt. we summarize the algorithm on the following slide.

kernel pca algorithm

kernel pca
given: data x1, . . . , xn, x     rd, and a id81 k(xi, xj).
construct: the kernel matrix on the data, e.g., kij = b exp
solve: the eigendecomposition

(cid:110)   (cid:107)xi   xj(cid:107)2

c

(cid:111)

.

for the    rst r (cid:28) n eigenvector/eigenvalue pairs (  1, a1), . . . , (  r, ar).

kak =   kak

output: a new coordinate system for xi by (implicitly) mapping   (xi) and
then projecting qt

k   (xi)

            1a1i

...
  rari

         

xi

projection      

where aki is the ith dimension of the kth eigenvector ak.

kernel pca and new data

q: how do we handle new data, x0? before, we could take the eigenvectors
qk and project xt

0 qk, but ak is different here.

a: recall the relationship of ak to qk in kernel pca is

n(cid:88)

qk =

aki  (xi).

we used the    kernel trick    to avoid working with or even de   ning   (xi).

i=1

as with regular pca, after mapping x0 we want to project onto eigenvectors

x0

projection      

plugging in for qk:   (x0)tqk =

n(cid:88)

i=1

            (x0)tq1

...

  (x0)tqr

         

aki  (x0)t   (xi) =

n(cid:88)

i=1

akik(x0, xi).

example results

an example of kernel pca using the gaussian kernel.

(left) original data, colored for reference (but may be classes)

(middle) new coordinates using kernel width c = 2
(right) new coordinates using kernel width c = 10

terminology: what we are doing is closely related to    spectral id91   
and can be considered an instance of    manifold learning.   

