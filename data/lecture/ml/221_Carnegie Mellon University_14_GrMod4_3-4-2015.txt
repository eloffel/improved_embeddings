machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

march 4, 2015 

today: 

       id114 
       bayes nets:   

       em 
       mixture of gaussian 
       learning bayes net 
structure (chow-liu) 

id91 

readings: 
       bishop chapter 8 
       mitchell chapter 6 
 

learning of bayes nets 

       four categories of learning problems 
       graph structure may be known/unknown 
       variable values may be fully observed / partly unobserved 

       easy case: learn parameters for graph structure is 

known, and data is fully observed 

 
      

interesting case: graph known, data partly known 

       gruesome case: graph structure unknown, data partly 

unobserved 

em algorithm - informally 

em is a general procedure for learning from partly observed data 
given  observed variables x, unobserved z  (x={f,a,h,n}, z={s}) 
 

begin with arbitrary choice for parameters    

iterate until convergence: 
       e step: estimate the values of unobserved z, using      
       m step: use observed values plus e-step estimates to  
                derive a better   

guaranteed to find local maximum. 
each iteration increases   

em algorithm - precisely 

em is a general procedure for learning from partly observed data 
given  observed variables x, unobserved z  (x={f,a,h,n}, z={s}) 
define 

iterate until convergence: 
       e step: use x and current    to calculate p(z|x,  ) 
       m step: replace current    by  

guaranteed to find local maximum. 
each iteration increases   

e step: use x,   , to calculate p(z|x,  ) 
observed x={f,a,h,n}, 
unobserved z={s} 

flu 

allergy 

sinus 

headache 

nose 

       how?  bayes net id136 problem. 

let   s use p(a,b) as shorthand for p(a=a, b=b) 

em and estimating   

observed x = {f,a,h,n}, unobserved z={s} 

flu 

allergy 

sinus 

headache 

nose 

e step:  calculate p(zk|xk;   ) for each training example, k  

m step: update all relevant parameters.  for example: 

recall id113 was: 

em and estimating   

flu 

allergy 

sinus 

more generally,  
given observed set x, unobserved set z of boolean values 

headache 

nose 

e step:  calculate for each training example, k  

 the expected value of each unobserved variable in  
 each training example 

   m step: 

calculate     similar to id113 estimates, but 
replacing each count by its expected count 

using unlabeled data to help train  

na  ve bayes classifier 

learn p(y|x) 

y

x1 

x2 

x3 

x4 

y 
1 
0 
0 
? 
? 

x1  x2  x3  x4 
0 
0 
0 
0 
0 

0 
1 
0 
1 
1 

1 
0 
1 
1 
0 

1 
0 
0 
0 
1 

em and estimating   

given observed set x, unobserved set y of boolean values 

e step:  calculate for each training example, k  

 the expected value of each unobserved variable y 

m step:  calculate estimates similar to id113, but 

replacing each count by its expected count 

id113 would be: 

experimental evaluation 

from [nigam et al., 2000] 

       newsgroup postings  

       20 newsgroups, 1000/group 

       web page classification  

       student, faculty, course, project 
       4199 web pages 

       reuters newswire articles  

       12,902 articles 
       90 topics categories 

20 newsgroups 

word w ranked by 
p(w|y=course) /
p(w|y     course) 

using one labeled 
example per class 

20 newsgroups 

usupervised id91 

  

just extreme case for em with 

zero labeled examples    

id91 

       given set of data points, group them 
       unsupervised learning 
       which patients are similar? (or which earthquakes, 

customers, faces, web pages,    ) 

mixture distributions 

model joint                     as mixture of multiple distributions. 
use discrete-valued random var z to indicate which 

distribution is being use for each random draw 

so 
 
 
mixture of gaussians: 
       assume each data point x=<x1,     xn> is generated by 

one of several gaussians, as follows: 

1.    randomly choose gaussian i, according to p(z=i) 
2.    randomly generate a data point <x1,x2 .. xn> according 

to n(  i,   i) 

mixture of gaussians 

em for mixture of gaussian id91 

let   s simplify to make this easier:    
1.   

assume x=<x1 ... xn>, and the xi are conditionally independent 
given z.   

 
 

2.   

assume only 2 clusters (values of z), and 

3.    assume    known,   1       k,   1i      ki unknown 

z

observed: x=<x1 ... xn> 
unobserved: z 
 

x1 

x2 

x3 

x4 

em 

z

given  observed variables x, unobserved z   
define 
where  

x1 

x2 

x3 

x4 

iterate until convergence: 
       e step: calculate p(z(n)|x(n),  ) for each example x(n). 
use this to construct  
       m step: replace current    by  

em     e step 

z

calculate p(z(n)|x(n),  ) for each observed example x(n) 
x(n)=<x1(n), x2(n),     xt(n)>.   

x1 

x2 

x3 

x4 

first consider update for   

em     m step  

z

  
    

      has no influence 

z=1 for nth 
example 

x1 

x2 

x3 

x4 

now consider update for   ji 

em     m step  

z

  
    

  ji    has no influence 

x1 

x2 

x3 

x4 

            

compare above to 
id113 if z were 
observable: 

em     putting it together 

z

given  observed variables x, unobserved z   
define 
where  

x1 

x2 

x3 

x4 

iterate until convergence: 
       e step: for each observed example x(n), calculate p(z(n)|x(n),  )   

 
        m step: update 

mixture of gaussians applet 

 
go to: http://www.socr.ucla.edu/htmls/socr_charts.html 
then go to go to    line charts           socr em mixture chart 
      
      
  

try it with 2 gaussian mixture components (   kernels   ) 
try it with 4  

what you should know about em 

       for learning from partly unobserved data 
       id113 of    =  
       em estimate:    = 

where x is observed part of data, z is unobserved 

       nice case is bayes net of boolean vars: 

       m step is like id113, with with unobserved values replaced by 

their expected values, given the other observed values 

       em for training bayes networks 
       can also develop map version of em 
       can also derive your own em algorithm for your own 

problem 
       write out expression for 
       e step: for each training example xk, calculate p(zk | xk,   ) 
       m step: chose new    to maximize                             

learning bayes net structure 

  
  

how can we learn bayes net graph structure? 

in general case, open problem 
       can require lots of data (else high risk of overfitting) 
       can use bayesian methods to constrain search 

one key result: 
       chow-liu algorithm: finds    best    tree-structured network   
       what   s best? 

       suppose p(x) is true distribution, t(x) is our tree-structured 

network, where x = <x1,     xn>  

       chow-liu minimizes id181: 

chow-liu algorithm 

key result:  to minimize kl(p || t), it suffices to find the tree 
network t that maximizes the sum of mutual informations 
over its edges 

 
mutual information for an edge between variable a and b:  
 
 
 
this works because for tree networks with nodes 
 
 

chow-liu algorithm 

1.    for each pair of vars a,b, use data to estimate p(a,b),  

p(a), p(b) 

 
2.    for each pair of vars a,b calculate mutual information 

3.    calculate the maximum spanning tree over the set of 

variables, using edge weights i(a,b) 
 (given n vars, this costs only o(n2) time) 

4.    add arrows to edges to form a directed-acyclic graph 

learn the cpd   s for this graph 

5.   
 

chow-liu algorithm example 

greedy algorithm to find max-spanning tree 

1/ 

1/ 

1/ 

1/ 

1/ 

1/ 

1/ 

1/ 

1/ 

1/ 

1/ 

[courtesy a. singh, c. guestrin] 

bayes nets     what you should know 

       representation 

distributions 

assumptions 

       bayes nets represent joint distribution as a dag + conditional 

       d-separation lets us decode conditional independence 

id136 
       np-hard in general 
       for some graphs, closed form id136 is feasible 
       approximate methods too, e.g., monte carlo methods,     

       learning 

       easy for known graph, fully observed data (id113   s, map est.) 
       em for partly observed data, known graph 
       learning graph structure: chow-liu for tree-structured networks 
       hardest when graph unknown, data incompletely observed 

      

 

