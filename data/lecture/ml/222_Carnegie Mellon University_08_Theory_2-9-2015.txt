machine learning theory

maria-florina (nina) balcan

february 9th, 2015

a2

  

goals of machine learning theory

develop & analyze models to understand: 

    what kinds of tasks we can hope to learn, and from what kind of data; 

what are key resources involved (e.g., data, running time) 

   

prove guarantees for practically successful algs (when will they 
succeed, how long will they take?)

    develop new algs that provably meet desired criteria  (within new 

learning paradigms)

interesting  tools & connections to other areas:
    algorithms, id203 & statistics, optimization, complexity theory, 

id205, game theory.

very vibrant field:
    conference on learning theory
    nips, icml

a2

  

today   s focus: sample complexity for supervised 
classification (function approximation)
    statistical learning theory (vapnik)
    pac (valiant)

    recommended reading: mitchell: ch. 7 
    suggested exercises: 7.1, 7.2, 7.7

    additional resources: my learning theory course! 

supervised classification

decide which emails are spam and which are important.

supervised classification

not spam

spam

goal: use emails seen so far to produce good prediction 
rule for future data.

4

example: supervised classification

represent each message by features. (e.g., keywords, spelling, etc.)

example

label

reasonable rules:

predict spam if unknown and (money or pills)

predict spam if 2money + 3pills    5 known > 0

+
+
+

+

-
-

-

-
-
-

linearly separable

5

two core aspects of machine learning

algorithm design. how to optimize?

computation

automatically generate rules that do well on observed data.

    e.g.: id28, id166, adaboost, etc.

confidence bounds, generalization

(labeled) data

confidence for rule effectiveness on future data.

    very well understood: occam   s bound, vc theory, etc.

    note: to talk about these we need a precise model.

pac/slt models for supervised learning

data 
source

distribution d on x

expert / oracle

learning 
algorithm

labeled examples  

(x1,c*(x1)),   , (xm,c*(xm))

alg.outputs

h : x ! y

x1 > 5

x6 > 2

+1

+1

-1

c* : x ! y

+
+
+

+

-
-

-

-
-
-

pac/slt models for supervised learning

data 
source

distribution d on x

learning 
algorithm

expert/oracle

labeled examples  
(x1,c*(x1)),   , (xm,c*(xm))

alg.outputs

h : x ! y

c* : x ! y

today: y={-1,1}

    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi independently 

and identically distributed (i.i.d.) from d; labeled by        

    does optimization over s, finds hypothesis h (e.g., a decision tree).
    goal:  h has small error over d.

pac/slt models for supervised learning

    x     feature or instance space; distribution d over x

e.g., x = rd or x = {0,1}d

    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi i.i.d. from d

    labeled examples - assumed to be drawn i.i.d. from some distr. 

d over x and labeled by some target concept c*
labels 2 {-1,1} - binary classification

   

    algo does optimization over s, find hypothesis    .

    goal:  h has small error over d.

                     = pr
    ~     

(                    (    ))

h

-

+ 

+ 

+ 

+ 

c*

-

-

need a bias: no free lunch.

-
instance space x

function approximation: the big picture

pac/slt models for supervised learning

    x     feature or instance space; distribution d over x

e.g., x = rd or x = {0,1}d

    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi i.i.d. from d

    labeled examples - assumed to be drawn i.i.d. from some distr. 

d over x and labeled by some target concept c*
labels 2 {-1,1} - binary classification

   

    algo does optimization over s, find hypothesis    .

    goal:  h has small error over d.

                     = pr
    ~     

(                    (    ))

h

-

+ 

+ 

+ 

+ 

c*

-

-

bias: fix hypotheses space h .

(whose complexity is not too large).

-
instance space x

realizable:                 . 
agnostic:            close to    h. 

pac/slt models for supervised learning

    algo sees training sample s: (x1,c*(x1)),   , (xm,c*(xm)), xi i.i.d. from d
    does optimization over s, find hypothesis             .

    goal:  h has small error over d.

true error:                      = pr
    ~     

(                    (    ))

how often                     (    ) over future 
instances drawn at random from d 

    but, can only measure:

training error:                      =

1
    

                                            

how often                     (    ) over training 
instances

sample complexity: bound                      in terms of                     

sample complexity for supervised learning

consistent learner

    input: s: (x1,c*(x1)),   , (xm,c*(xm))
    output: find h in h consistent with the sample (if one exits). 

contrapositive: if the target is in h, and we have an algo that 
can find consistent fns, then we only need this many 
examples to get generalization error          with prob.     1         

sample complexity for supervised learning

consistent learner

    input: s: (x1,c*(x1)),   , (xm,c*(xm))
    output: find h in h consistent with the sample (if one exits). 

bound inversely linear in     

bound only logarithmic in |h|

         is called error parameter

   

d might place low weight on certain parts of the space

         is called confidence parameter

   

there is a small chance the examples we get are not representative of 
the distribution

sample complexity for supervised learning

consistent learner

    input: s: (x1,c*(x1)),   , (xm,c*(xm))
    output: find h in h consistent with the sample (if one exits). 

example: h is the class of conjunctions over x = 0,1 n.

|h| = 3n

e.g., h = x1 x3x5 or h = x1 x2x4    9

then         

1
    

     ln 3 + ln

1
    

suffice

     = 10,      = 0.1,      = 0.01 then          156 suffice

sample complexity for supervised learning

consistent learner

    input: s: (x1,c*(x1)),   , (xm,c*(xm))
    output: find h in h consistent with the sample (if one exits). 

example: h is the class of conjunctions over x = 0,1 n.

side hwk question: show that any conjunction can be 
represented by a small decision tree; also by a linear 
separator.

sample complexity for supervised learning

proof

assume k bad hypotheses h1, h2,     , hk with errd hi       

1)  fix hi. prob. hi consistent with first training example is

    1       . 

prob. hi consistent with first m training examples is     1        m. 

2) prob. that at least one         consistent with first m training 
examples is

    k 1        m     h 1        m.

3) calculate value of m so that  h 1        m       

3) use the fact that 1     x     e   x, sufficient to set  h e     m       

sample complexity: finite hypothesis spaces

realizable case

id203 over different samples 
of m training examples

sample complexity: finite hypothesis spaces

realizable case

1) pac: how many examples suffice to guarantee small error whp. 

2) statistical learning way:

with id203 at least 1         , for all h     h s.t. errs h = 0 we have

errd(h)    

1
m

ln h + ln

1
    

.

supervised learning: pac model (valiant)

    x - instance space, e.g., x = 0,1 n or x = rn
    sl={(xi, yi)} - labeled examples drawn i.i.d. from some 
distr. d over x and labeled by some target concept c*
    labels 2 {-1,1} - binary classification

    algorithm a pac-learns concept class h if for any 
target c* in h, any distrib. d over x, any    ,     > 0:

- a uses at most poly(n,1/   ,1/   ,size(c*)) examples and running 
time.
- with probab. 1-   , a produces h in h of error at       .

uniform convergence

    this basic result only bounds the chance that a bad hypothesis looks 
perfect on the data. what if there is no perfect h   h (agnostic case)?

    what can we say if c        h?
    can we say that whp all h   h satisfy |errd(h)     errs(h)|        ?

    called    uniform convergence   .
    motivates optimizing over s, even if we can   t find a 

perfect function.

sample complexity: finite hypothesis spaces

realizable case

agnostic case

what if there is no perfect h? 

to prove bounds like this, need some good tail inequalities.

hoeffding bounds

consider coin of bias p flipped m times.  
let n be the observed # heads.  let        [0,1].
hoeffding bounds:
    pr[n/m > p +    ]     e-2m   2, and
    pr[n/m < p -    ]     e-2m   2.

exponentially decreasing tails

    tail inequality: bound id203 mass in tail of 

distribution (how concentrated is a random variable 
around its expectation).

sample complexity: finite hypothesis spaces

agnostic case

   

   

proof: just apply hoeffding.
    chance of failure at most 2|h|e-2|s|   2.
    set to    . solve.
so, whp, best on sample is    -best over d.
    note: this is worse than previous bound (1/    has become 1/   2), 

because we are asking for something stronger.

    can also get bounds    between    these two.

