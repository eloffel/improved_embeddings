kernels methods in machine learning 

   

id88. geometric margins. 

    support vector machines (id166s).  

maria-florina balcan 

03/23/2015 

quick recap about 

id88 and margins 

the online learning model 

    example arrive sequentially. 

    we need to make a prediction. 

afterwards observe the outcome. 

for i=1, 2,    , : 

phase i: 

online algorithm 

example          

prediction    (        ) 

observe c   (        ) 

mistake bound model 

    analysis wise, make no distributional assumptions. 

    goal: minimize the number of mistakes. 

id88 algorithm in online model 
wlog  homogeneous linear separators [w0 = 0]. 

    set t=1, start with the all zero vector     1. 

    given example     , predict + iff                       0 

    on a mistake, update as follows: 

    mistake on positive,         +1              +      
    mistake on negative,         +1                       

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x = rn 

o 

o 

o 

o 

o 

o 
w 

o 

o 

note 1:  wt is weighted sum of incorrectly classified examples 
         =         1        1 +     +                           so,                   =         1        1          +     +                                   

note 2:   number of mistakes ever made depends only on the 

geometric margin of examples seen. 

    no matter how long the sequence is or how high dimension n is! 

geometric margin 

definition: the margin of example      w.r.t. a linear sep.      is the 
distance from      to the plane               = 0. 

margin of example     1 

    1 

w 

if       = 1,  margin of x 
w.r.t. w is |             |. 

margin of example     2 

    2 

geometric margin 

definition: the margin of example      w.r.t. a linear sep.      is the 
distance from      to the plane               = 0. 

definition: the margin          of a set of examples      wrt a linear 
separator      is the smallest margin over points              . 

definition: the margin      of a set of examples      is the maximum 
         over all linear separators     . 

- 

     

     
- 
- 
- 

- 

- 

- 

+ 

+ 

w 

+ 

+ 

- 
- 

+ 

id88: mistake bound 

theorem: if data linearly separable by margin      and points inside 
a ball of radius     , then id88 makes         /     2 mistakes. 

    no matter how long the sequence is how high dimension n is! 

+ 

    

    
- 
- 
- 
- 

- 

margin: the amount of 
wiggle-room available for 
a solution. 

+ 

+ 

+ 
+ 

+ 

r 

w* 
+ 
+ 

- 
- 

- 
- 

(normalized margin: multiplying all points by 100, or dividing all points by 100, 
doesn   t change the number of mistakes; algo is invariant to scaling.) 

id88 extensions 

    can use it to find a consistent separator with a given set 
s linearly separable by margin      (by cycling through the data). 

    can convert the mistake bound guarantee into a distributional 

guarantee too (for the case where the         s come from a fixed 
distribution). 

    can be adapted to the case where there is no perfect 

separator as long as the so called hinge loss (i.e., the total 
distance needed to move the points to classify them correctly large 
margin) is small.  

    can be kernelized to handle non-linear decision boundaries! 

theorem: if data linearly separable by margin      and points inside 
a ball of radius     , then id88 makes         /     2 mistakes. 

implies that large margin classifiers have 
smaller complexity! 

complexity of large margin linear sep. 

    know that in rn we can shatter n+1 points with linear 

separators, but not n+2 points (vc-dim of linear sep is n+1). 

what if we require that the points be 
linearly separated by margin     ?  

x 

x 
x  x 
x 

x 

x 

x 

x 

x 

o 

o 

o 

o 

o 

o 
w 

o 

o 

2

can have at most      
 points inside ball of radius r 
    
that can be shattered at margin      (meaning that every 
labeling is achievable by a separator of margin     ). 

    so, large margin classifiers have smaller complexity! 
    nice implications for usual distributional learning setting. 

   

less classifiers to worry about that will look good over 
the sample, but bad over all   . 

    less prone to overfitting!!!! 

margin important theme in ml. 
both sample complexity and algorithmic implications. 

sample/mistake bound complexity: 

    if large margin, # mistakes peceptron makes 

is small (independent on the dim of the space)! 

    if large margin      and if alg. produces a large 

margin classifier, then amount of data needed 
depends only on r/     [bartlett & shawe-taylor    99].  

    suggests searching for a 
large margin classifier    

algorithmic implications: 

    id88, kernels, id166s    

- 

w 

     

     

+ + 
- 
+ 
+ 
- - 
- 
- 
- 
- 
- 

+ 

so  far,  talked  about  margins  in 
the  context  of  (nearly)  linearly 
separable datasets 

what if not linearly separable 

problem: data not linearly separable in the most natural 
feature representation. 

example: 

vs 

solutions: 

no good linear 
separator in pixel 
representation. 
 

   

   

   

   

    learn a more complex class of functions     

   

(e.g., id90, neural networks, boosting). 

    use a kernel     

(a neat solution that attracted a lot of attention) 

    use a deep network    

    combine  kernels  and deep networks    

overview of kernel methods 

what is a kernel? 
a kernel k is a legal def of dot-product: i.e. there exists an 
implicit mapping    s.t. k(    ,     ) =  (    )      (    )  

e.g., k(x,y) = (x    y + 1)d 

   : (n-dimensional space) ! nd-dimensional space 

why kernels matter? 

   

 many algorithms interact with data only via dot-products. 

    so, if replace x     z with k x, z  they act implicitly as if data  

was in the higher-dimensional   -space. 

    if data is linearly separable by large margin in the   -space, 

then good sample complexity.  

[or other regularity properties for controlling the capacity.] 

kernels 

definition 

k(   ,   ) is a  kernel if it can be viewed as a legal definition of 
inner product: 
          : x     rn  s.t. k x, z =    x       (z) 

    range of    is called the   -space. 

    n can be very large.  

    but think of    as implicit, not explicit!!!! 

example 

for n=2, d=2, the kernel k x, z = x     z d corresponds to  

    1,     2             = (    1

2,     2

2, 2    1    2) 

original space 

  -space 

x 

x 

x2 

x 

x 

x 

x 

x 

x 

o 

o 

o 

o 

o 

x 

x1 

o 

x 

x 

x 

o 

o 

x 

x 

x 

x 

x 

x 

x  x 

x 

x 

x 

o 

x 

o 

o 

o 

o 

o 

o 

o 

o 

x 

x 

x 

z1 

x 

x 

x 

x 

z3 

x 

x 

x 

x 

x 

  : r2     r3,  x1, x2        x = (x1

example 
2, 2x1x2) 

2, x2

   x             = x1

2, x2

2, 2x1x2     (    1

2,     2

2, 2    1    2) 

= x1    1 + x2    2

2 = x          2 = k(x, z) 

original space 

x 

x 

x2 

x 

x 

x 

x 

x 

x 

o 

o 

o 

o 

o 

x 

x1 

o 

x 

x 

x 

o 

o 

x 

x 

x 

x 

x 

x 

  -space 

x  x 

x 

x 

x 

o 

x 

o 

o 

o 

o 

o 

o 

o 

o 

x 

x 

x 

z1 

x 

x 

x 

x 

z3 

x 

x 

x 

x 

x 

kernels 

definition 

k(   ,   ) is a  kernel if it can be viewed as a legal definition of 
inner product: 
          : x     rn  s.t. k x, z =    x       (z) 

    range of    is called the   -space. 

    n can be very large.  

    but think of    as implicit, not explicit!!!! 

example 

note:  feature space might not be unique. 

  : r2     r3,  x1, x2        x = (x1

2, x2

2, 2x1x2) 

   x             = x1

2, x2

2, 2x1x2     (    1

2,     2

2, 2    1    2) 

= x1    1 + x2    2

2 = x          2 = k(x, z) 

  : r2     r4,  x1, x2        x = (x1

2, x2

2, x1x2, x2x1) 

   x             = (x1

2, x2

2, x1x2, x2x1)     (z1

2, z2

2, z1z2, z2z1) 

= x          2 = k(x, z) 

avoid explicitly expanding the features 

feature space can grow really large and really quickly   . 

crucial to think of    as implicit, not explicit!!!! 

    polynomial kernel degreee     ,          ,      =                  =                          

    ,     1     2             ,     1

        1
    total number of such feature is 

2     2                1 

     +          1

    

=

     +          1 !
 
    !          1 !

         = 6,      = 100, there are 1.6 billion terms 

                                                        ! 

         ,      =                  =                          

kernelizing a learning algorithm 

    if  all computations involving instances are in terms of 

inner products then: 

    conceptually, work in a very high diml space and the alg   s 

performance depends only on linear separability in that 
extended space. 

     computationally, only need to modify the algo by replacing 

each x     z with a k x, z . 

     examples of kernalizable algos: 

   

   

   

 classification: id88, id166. 

 regression: linear, ridge regression. 

 id91: id116. 

kernelizing  the id88 algorithm 

    set t=1, start with the all zero vector     1. 

    given example     , predict + iff                       0 

    on a mistake, update as follows: 

    mistake on positive,         +1              +      
    mistake on negative,         +1                       

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

o 

o 

o 

o 

o 

o 
w 

o 

o 

easy to kernelize since          is weighted sum of incorrectly 
classified examples           =         1        1 +     +                          

replace 

                  =         1        1          +     +                                   
        1     (        1,     ) +     +                 (            ,     ) 

with 

note: need to store all the mistakes so far. 

kernelizing  the id88 algorithm 
    given     , predict + iff 

    (               1)         (    ) 

  -space 

        1     (        1,     ) +     +                1    (               1,     )     0 

    on the      th  mistake, update as follows: 

    mistake on positive, set                  1; store              
    mistake on negative,                     1; store              

x 
x 
x  x 
x 

x 

x 

x 

x 

x 

o 

o 

o 

o 

o 

o 
w 

o 

o 

id88           =         1        1 +     +                          

                  =         1        1          +     +                                   

             1     (        1,     ) +     +                 (            ,     ) 

exact same behavior/prediction rule as if mapped data in the 
    -space and ran id88 there! 

do this implicitly, so computational savings!!!!! 

generalize well if good margin 
    if data is linearly separable by margin in the     -space, 

then small mistake bound. 

    if margin      in     -space, then id88 makes      
    

2

  mistakes. 

+ 

+ 
+ 

+ 

  -space 

w* 

+ 

+ 

+ 

r 

    

+ 

    

- 

- 

- 
- 
- 
- 

- 

- 
- 

kernels: more examples 

    linear: k x, z = x          

    polynomial: k x,      = x          d or k x,      = 1 + x          d 

    gaussian: k x,      = exp    

2

           

2     2    

    laplace kernel: k x,      = exp    

||           ||

2     2    

    kernel for non-vectorial data, e.g., measuring similarity 

between sequences. 

properties of kernels 

theorem (mercer) 

k is a kernel if and only if: 

    k is symmetric 

    for any set of training points     1,     2,     ,          and for 

any     1,     2,     ,                  , we have: 

                                ,              0
 

    ,    

                     0 

i.e.,      = (             ,          )    ,    =1,   ,      is positive semi-definite. 

kernel methods 

    offer great modularity. 

    no need to change the underlying learning 

algorithm to accommodate a particular choice 
of id81. 

    also, we can substitute a different algorithm 

while maintaining the same kernel. 

kernel, closure properties 

easily create new kernels using basic ones! 

fact: 

if  k1    ,     and  k2    ,     are kernels  c1     0,     2     0, 

then k x, z = c1k1 x, z + c2k2 x, z  is a kernel. 

key idea: concatenate the      spaces. 

   x = ( c1   1 x , c2   2(x))  

   x       (z) = c1   1 x       1 z + c2   2 x       2 z  

    1(    ,     ) 

    2(    ,     ) 

kernel, closure properties 

easily create new kernels using basic ones! 

fact: 

if  k1    ,     and  k2    ,     are kernels, 

then k x, z = k1 x, z k2 x, z  is a kernel. 

key idea:     x =   1,i x    2,j x

        1,   ,     ,       {1,   ,    }

  

   x       (z) =     1,i x    2,j x   1,i z    2,j z

 

    ,    

=     1,i x    1,     z     2,     x    2,j z

 
 

    

    

=     1,i x    1,     z k2 x, z

    

= k1 x, z  k2 x, z  

kernels, discussion 

    if  all computations involving instances are in terms 

of inner products then: 

    conceptually, work in a very high diml space and the alg   s 

performance depends only on linear separability in that 
extended space. 

     computationally, only need to modify the algo by replacing 

each x     z with a k x, z . 

     lots of machine learning algorithms are kernalizable: 

   

   

   

 classification: id88, id166. 

 regression: id75. 

 id91: id116. 

kernels, discussion 

    if  all computations involving instances are in terms 

of inner products then: 

    conceptually, work in a very high diml space and the alg   s 

performance depends only on linear separability in that 
extended space. 

     computationally, only need to modify the algo by replacing 

each x     z with a k x, z . 

how to choose a kernel: 

    kernels often encode domain knowledge (e.g., string kernels) 

    use cross-validation to choose the parameters, e.g.,       for 

gaussian kernel   k x,      = exp    

2

           

2     2    

    learn a good kernel; e.g.,  [lanckriet-cristianini-bartlett-el ghaoui-

jordan   04] 

