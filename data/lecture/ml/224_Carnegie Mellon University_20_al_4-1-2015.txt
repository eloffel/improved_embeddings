active learning  

maria-florina balcan 

04/01/2015 

logistics 

    hwk #6 due on friday. 

    midway project review due on monday. 

make sure to talk to your  mentor ta! 

classic fully supervised learning 
paradigm insufficient nowadays 

modern applications: massive  amounts  of raw data. 

only a tiny fraction can be annotated by human experts. 

protein sequences 

billions of webpages 

images 

modern ml: new learning approaches 
modern applications: massive  amounts  of raw data. 

techniques that best utilize data, minimizing need for 
expert/human intervention. 

paradigms where there has been great progress. 

    semi-supervised learning, (inter)active learning. 

expert 

semi-supervised learning 

data source 

learning 
algorithm 

unlabeled 
examples 

expert / oracle 

unlabeled 
examples 

                 labeled examples   

algorithm outputs a classifier  

 sl={(x1, y1),    ,(xml, yml)}  
xi drawn i.i.d from d, yi = c   (xi) 

 su={x1,    ,xmu} drawn i.i.d from d 

 goal:  h has small error over d. 

 errd h = pr
x~ d

(h x     c   (x)) 

semi-supervised learning 

key insight/underlying fundamental principle 

unlabeled data useful if we have a bias/belief not only about 
the form of the target, but also about its relationship with 
the underlying data distribution. 

e.g.,    large margin separator    

[joachims    99] 

 

+ 
+ 

_ 

_ 

similarity based 

(   small cut   ) 
[b&c01], [zgl03] 

 

+ 

   self-consistent rules    [blum & mitchell    98] 

x = h x1, x2 i 

 

h1(x1)=h2(x2) 

prof. avrim 

my advisor 

x1- text info 

x2- link info 

    unlabeled data can help reduce search space or re-order the fns 

in the search space according to our belief, biasing the search 
towards fns satisfying the belief (which becomes concrete once 
we see unlabeled data). 

a general discriminative model for ssl 

[balcanblum, colt 2005; jacm 2010] 

as in pac/slt, discuss algorithmic and sample complexity issues. 

analyze fundamental sample complexity aspects: 

    how much unlabeled data is needed. 

    depends both on complexity of h and of compatibility notion. 

    ability of unlabeled data to reduce #of labeled examples. 

    compatibility of the target, helpfulness of the distrib. 

    survey on    semi-supervised learning    (jerry zhu, 2010) 

explains the ssl techniques from this point of view. 

   

 note: the mixture method that tom talked about on feb 25th can 
be explained from this point of view too. see the zhu survey. 

active learning  

additional resources: 
    two faces of active learning. sanjoy dasgupta. 2011.  
    active learning. bur settles. 2012. 
    active learning. balcan-urner. encyclopedia of algorithms. 2015 

batch active learning 

data source 

learning 
algorithm 

unlabeled 
examples 

underlying data 
distr. d. 

expert  

request for the label of an example 

          a label for that example 

request for the label of an example 

          a label for that example 

.
 
.
 
.
 
 

algorithm outputs a classifier w.r.t d 

    learner can choose specific examples to be labeled.  
    goal:  use fewer labeled examples 

[pick informative examples to be labeled]. 

 

 

selective sampling active learning 

data source 

learning 
algorithm 

unlabeled 
unlabeled 
unlabeled 
example     3 
example     1 
example     2 

underlying data 
distr. d. 

expert  

a label     1for example     1 
a label     3for example     3 

request label 
request label 

request for 
label or let it 

go? 

let it 

go 

algorithm outputs a classifier w.r.t d 

    selective sampling al (online al): stream of unlabeled examples, 

when each arrives make a decision to ask for label or not. 

 
    goal:  use fewer labeled examples 

[pick informative examples to be labeled]. 

 

what makes a good active learning 

algorithm? 

    guaranteed to output a relatively good classifier 

for most learning problems. 

    doesn   t make too many label requests. 
 

    hopefully  a lot less than passive learning and ssl. 

    need to choose the label requests carefully, to get 

informative labels. 

can adaptive querying really do better than 

passive/random sampling? 

    yes! (sometimes) 

 

    we often need far fewer labels for active 

learning than for passive. 

    this is predicted by theory and has been 

observed in practice. 

can adaptive querying help? [cal92, dasgupta04] 

    threshold fns on the real line: 

 

hw(x) = 1(x    w),  c = {hw: w 2 r} 

- 

+ 

active algorithm 

w 

 get n unlabeled examples 

   
    how can we recover the correct labels with     n queries? 
      do binary search!  

just need o(log n) labels! 

+ 

- 

- 

   

 output a classifier consistent with the n inferred labels. 

    n = o(1/  )  we are guaranteed to get a classifier of error       .  

passive supervised:   (1/  ) labels to find an    -accurate threshold. 

active: only o(log 1/  ) labels. 

exponential improvement. 

common technique in practice 

uncertainty sampling in id166s common and quite useful 
in practice. 

e.g., [tong & koller, icml 2000; jain, vijayanarasimhan & grauman, nips 2010; 
schohon cohn, icml 2000] 

active id166 algorithm 

    at any time during the alg., we have a    current guess    
wt of the separator: the max-margin separator of all 
labeled points so far. 

    request the label of the example closest to the current 

separator. 

common technique in practice 

active id166 seems to be quite useful in practice. 

[tong & koller, icml 2000; jain, vijayanarasimhan & grauman, nips 2010] 

algorithm (batch version) 

input su={x1,    ,xmu} drawn i.i.d from the underlying source d 
start: query for the labels of a few random         s. 

for      =     ,    ., 

    find          the max-margin 
separator of all labeled 
points so far. 

    request the label of the 

example closest to the current 
separator: minimizing                        . 
(highest uncertainty) 

common technique in practice 

active id166 seems to be quite useful in practice. 

e.g., jain, vijayanarasimhan & grauman, nips 2010 

newsgroups dataset (20.000  documents from 20 categories) 

common technique in practice 

active id166 seems to be quite useful in practice. 

e.g., jain, vijayanarasimhan & grauman, nips 2010 

cifar-10 image dataset (60.000  images from 10 categories) 

active id166/uncertainty sampling 

    works sometimes   . 

    however, we need to be very very very careful!!! 

    myopic, greedy technique can suffer from sampling bias. 

    a bias created because of the querying strategy; as time 
goes on the sample is less and less representative of the true 
data source. 

[dasgupta10] 

active id166/uncertainty sampling 

    works sometimes   . 

    however, we need to be very very careful!!! 

active id166/uncertainty sampling 

    works sometimes   . 
    however, we need to be very very careful!!! 

    myopic, greedy technique can suffer from sampling bias. 

   

bias created because of the querying strategy; as time goes on 
the sample is less and less representative of the true source. 

    observed in practice too!!!! 

    main tension: want to choose informative points, but also 

want to guarantee that the classifier we output does  well on 
true random examples from the underlying distribution. 

safe active learning schemes 

disagreement based active learning 

hypothesis space search 

[cal92] 

[bbl06]  

 [hanneke   07, dhm   07, wang   09 , fridman   09, kolt10, bhw   08, bhlz   10, h   10, ailon   12,    ] 

version spaces 

    x     feature/instance space; distr. d over x;         target fnc 
    fix hypothesis space h. 

assume realizable case: c        h.  

definition (mitchell   82) 
 given a set of labeled examples (x1, y1),    ,(xml, yml),  yi = c   (xi) 
version space of h: part of h consistent with labels so far. 
i.e., h      vs(h) iff h xi = c    xi     i     {1,     , ml}. 

version spaces 

    x     feature/instance space; distr. d over x;         target fnc 
    fix hypothesis space h. 

definition (mitchell   82) 
 given a set of labeled examples (x1, y1),    ,(xml, yml),  yi = c   (xi) 
version space of h: part of h consistent with labels so far. 

assume realizable case: c        h.  

current version space 

e.g.,: data lies on 
circle in r2, h = 
homogeneous 
linear seps. 

+ 

+ 

region of disagreement 
in data space 

version spaces. region of disagreement 

definition (cal   92) 

version space: part of h consistent with labels so far. 

region of disagreement = part of data space about which there 
is still some uncertainty (i.e. disagreement within version space) 
x     x, x     dis(vs h ) iff    h1, h2     vs(h), h1 x     h2(x) 

e.g.,: data lies on 
circle in r2, h = 
homogeneous 
linear seps. 

+ 

current version space 

+ 

region of disagreement 
in data space 

disagreement based active learning [cal92] 

current version space 

algorithm:  

region of 
uncertainy 

pick a few points at random from the current 
region of uncertainty and query their labels. 

stop when region of uncertainty is small. 

note: it is active since we do not waste labels by querying 
in regions of space we are certain about the labels. 

disagreement based active learning [cal92] 

current version space 

region of 
uncertainy 

algorithm:  

query for the labels of a few random         s. 

let  h1  be the current version space. 

for      =     ,    ., 

pick a few points at random from the current region of 
disagreement dis(ht) and query their labels.  
let ht+1 be the new version space. 

region of uncertainty [cal92] 

     current version space: part of c consistent with labels so far. 
        region of uncertainty    = part of data space about which 
there is still some uncertainty (i.e. disagreement within version 
space) 

current version space 

+ 

+ 

 region of uncertainty 

in data space 

region of uncertainty [cal92] 

     current version space: part of c consistent with labels so far. 
        region of uncertainty    = part of data space about which 
there is still some uncertainty (i.e. disagreement within version 
space) 

new version space 

+ 

+ 

new region of 
disagreement in 
data space 

how about the agnostic case 
where the target might not 
belong the h? 

a2 agnostic active learner [bbl   06] 

current version space 

region of 
disagreement 

careful use of generalization bounds; 
avoid the sampling bias!!!! 

algorithm:  

let  h1 = h. 

for      =     ,    ., 

    pick a few points at random from the current region 

of disagreement dis(ht) and query their labels.  

    throw out hypothesis if you are statistically 

confident they are suboptimal.  

when active learning helps. agnostic case  
a2 the first algorithm which is robust to noise. 

[balcan, beygelzimer, langford, icml   06]  

[balcan, beygelzimer, langford, jcss   08]  

   region of disagreement    style:  pick a few points at random from the 
current region of disagreement, query their labels, throw out 
hypothesis if you are statistically confident they are suboptimal.  

guarantees for a2 [bbl   06,   08]: 
     it is safe (never worse than passive learning) & exponential improvements. 

     c     thresholds, low noise, exponential improvement. 

     c - homogeneous linear separators in rd, 

         d - uniform,  low  noise, only d2 log (1/   ) labels. 

c* 

a lot of subsequent work. 

 [hanneke   07, dhm   07, wang   09 , fridman   09, kolt10, bhw   08, bhlz   10, h   10, ailon   12,    ] 

general guarantees for a2 agnostic active learner  
   disagreement based   :  pick a few points at random from the current region of 
uncertainty, query their labels, throw out hypothesis if you are statistically 
confident they are suboptimal.  

[bbl   06] 

how quickly the region of disagreement 
collapses as we get closer and closer to 
optimal classifier 

guarantees for a2 [hanneke   07]: 

disagreement coefficient 

realizable case:  

linear separators, uniform distr.: 

c* 

disagreement based active learning 

   disagreement based     algos:  query points from current 
region of disagreement, throw out hypotheses when 
statistically confident they are suboptimal.  

    generic (any class), adversarial label noise. 

    computationally efficient for classes of small vc-dimension 

still, could be suboptimal in label complex & computationally 
inefficient in general. 

 

lots of subsequent work trying to make is more efficient computationally 
and more aggressive too: [hanneke07, dasguptahsumontleoni   07, wang   09 , 
fridman   09,  koltchinskii10, bhw   08, beygelzimerhsulangfordzhang   10, hsu   10, ailon   12,    ] 

other interesting altechniques 
used in practice 

interesting open question to analyze 
under what conditions they are successful. 

density-based sampling 

centroid of largest unsampled cluster 

[jaime g. carbonell] 

uncertainty sampling 

closest to decision boundary (active id166) 

[jaime g. carbonell] 

maximal diversity sampling 

maximally distant from labeled x   s 

[jaime g. carbonell] 

ensemble-based possibilities 

uncertainty + diversity criteria 

density + uncertainty criteria 

[jaime g. carbonell] 

graph-based active and 
semi-supervised methods 

graph-based methods 

    assume we are given a pairwise similarity fnc and that 

very similar examples probably have the same label. 

    if we have a lot of labeled data, this suggests a 

nearest-neighbor type of algorithm. 

    if you have a lot of unlabeled data, perhaps can use 

them as    stepping stones   . 

e.g., handwritten digits [zhu07]: 

graph-based methods 

idea: construct a graph with edges between very similar 
examples. 
unlabeled data can help    glue    the objects of the same 
class together. 

graph-based methods 

often, transductive approach.  (given l + u, output predictions on 
u). are alllowed to output any labeling of              . 

main idea: 

    construct graph g with edges 

between very similar examples. 

    might have also glued together in g 

examples of different classes. 

    run a graph partitioning algorithm to 

separate the graph into pieces. 

several methods: 

    minimum/multiway cut [blum&chawla01] 
    minimum    soft-cut    [zhughahramanilafferty   03] 
    spectral partitioning 
        

ssl using soft cuts 

[zhughahramanilafferty   03] 

 

solve for label function               0,1  to minimize: 

          =                                      (        )

                     (    ,    )

2

+

                                 
               

2

 

similar nodes get 

similar labels 

(weighted similarity) 

agreement with labels  

(agreement not strictly enforces) 

active learning with label propagation 

 

(using soft-cuts) 

how to choose 
which node to 

query? 

44 

active learning with label propagation 

one natural idea: query the most uncertain point. 

but this has only one edge.  query won   t have 
much impact! 
(even worse: a completely isolated node)  

(using soft-cuts) 

45 

active learning with label propagation 

instead, use a 1-step-lookahead heuristic: 

    for a node with label     , assume that querying will have prob 

     of returning answer 1, 1          of returning answer 0. 

    compute    average confidence    after running soft-cut in each case: 

    

1
    

  max     1          , 1         1          + (1         )

        

1
    

  max     0          , 1         0         

        

  

    query node s.t. this quantity is highest (you want to be more 

confident on average). 

(using soft-cuts) 

46 

active learning with label propagation  in 

practice 

    does well for video segmentation (fathi-balcan-ren-regh, bmvc 11). 

what you should know 

    active learning could be really helpful, could provide 
exponential improvements in label complexity (both 
theoretically and practically)! 

    common heuristics (e.g., those based on uncertainty 

sampling). need to be very careful due to  sampling bias. 

    safe disagreement based active learning schemes. 

    understand how they operate precisely in noise 

free scenarios. 

