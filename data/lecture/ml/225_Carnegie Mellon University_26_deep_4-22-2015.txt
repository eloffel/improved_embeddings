impact	
   of	
   deep	
   learning	
   

      	
   speech	
   recogni4on	
   
      	
   computer	
   vision	
   
      	
   recommender	
   systems	
   	
   
      	
   language	
   understanding	
   	
   
      	
   drug	
   discovery	
   and	
   medical	
   
image	
   analysis	
   	
   

[courtesy	
   of	
   r.	
   salakhutdinov]	
   

id50: training [hinton & salakhutdinov, 2006] 

very large scale use of dbn   s 
data: 10 million 200x200 unlabeled images, sampled from youtube 
training: use 1000 machines (16000 cores) for 1 week 
learned network: 3 multi-stage layers, 1.15 billion parameters 
achieves 15.8% (was 9.5%) accuracy classifying 1 of 20k id163 items 

[quoc le, et al., icml, 2012] 

real 
images 
that most 
excite the 
feature: 

image 
synthesized 
to most 
excite the 
feature: 

restricted	
   boltzmann	
   machines	
   

graphical	
   models:	
   powerful	
   
framework	
   for	
   represen4ng	
   
dependency	
   structure	
   between	
   
random	
   variables.	
   

	
   	
   hidden	
   variables	
   

pair-     wise	
   

unary	
   
feature	
   detectors	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

rbm	
   is	
   a	
   markov	
   random	
   field	
   with:	
   
      	
   stochas4c	
   binary	
   visible	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   stochas4c	
   binary	
   hidden	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   bipar4te	
   connec4ons.	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

markov	
   random	
      elds,	
   boltzmann	
   machines,	
   log-     linear	
   models.	
   	
   

model	
   learning	
   

	
   	
   hidden	
   units	
   

given	
   a	
   set	
   of	
   i.i.d.	
   training	
   examples	
   	
   

	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   we	
   want	
   to	
   learn	
   	
   

model	
   parameters

	
   

	
   

	
   	
   	
   	
   	
   	
   .	
   	
   	
   	
   

maximize	
   log-     likelihood	
   objec4ve:	
   

image	
   	
   	
   	
   	
   	
   visible	
   units	
   

deriva4ve	
   of	
   the	
   log-     likelihood:	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

deep	
   boltzmann	
   machines	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

input:	
   pixels	
   

image	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(salakhutdinov & hinton, neural computation 2012)

deep	
   boltzmann	
   machines	
   

learn	
   simpler	
   representa4ons,	
   
then	
   compose	
   more	
   complex	
   ones	
   

higher-     level	
   features:	
   
combina4on	
   of	
   edges	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

input:	
   pixels	
   

image	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(salakhutdinov 2008, salakhutdinov & hinton 2012)

h3

h2

h1

v

model	
   formula4on	
   

same	
   as	
   rbms	
   

w3

requires	
   approximate	
   id136	
   to	
   
train,	
   but	
   it	
   can	
   be	
   done   	
   
and	
   scales	
   to	
   millions	
   of	
   examples	
   

w2

w1

input	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

samples	
   generated	
   by	
   the	
   model	
   

training	
   data	
   

model-     generated	
   samples	
   

data	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

handwri4ng	
   recogni4on	
   

mnist	
   dataset	
   

60,000	
   examples	
   of	
   10	
   digits	
   
learning	
   algorithm	
   
logis4c	
   regression	
   
k-     nn	
   	
   
neural	
   net	
   (pla_	
   2005)	
   
id166	
   (decoste	
   et.al.	
   2002)	
   
deep	
   autoencoder	
   
(bengio	
   et.	
   al.	
   2007)	
   	
   
deep	
   belief	
   net	
   
(hinton	
   et.	
   al.	
   2006)	
   	
   
dbm	
   	
   

error	
   
12.0%	
   
3.09%	
   
1.53%	
   
1.40%	
   
1.40%	
   

0.95%	
   

1.20%	
   

op4cal	
   character	
   recogni4on	
   

42,152	
   examples	
   of	
   26	
   english	
   le_ers	
   	
   

learning	
   algorithm	
   
logis4c	
   regression	
   
k-     nn	
   	
   
neural	
   net	
   
id166	
   (larochelle	
   et.al.	
   2009)	
   
deep	
   autoencoder	
   
(bengio	
   et.	
   al.	
   2007)	
   	
   
deep	
   belief	
   net	
   
(larochelle	
   et.	
   al.	
   2009)	
   	
   
dbm	
   

error	
   
22.14%	
   
18.92%	
   
14.62%	
   
9.70%	
   
10.05%	
   

9.68%	
   

8.40%	
   

permuta4on-     invariant	
   version.	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

3-     d	
   object	
   recogni4on	
   

norb	
   dataset:	
   24,000	
   examples	
   

learning	
   algorithm	
   
logis4c	
   regression	
   
k-     nn	
   (lecun	
   2004)	
   
id166	
   (bengio	
   &	
   lecun	
   	
   2007)	
   
deep	
   belief	
   net	
   (nair	
   &	
   hinton	
   	
   
2009)	
   	
   
dbm	
   

error	
   
22.5%	
   
18.92%	
   
11.6%	
   
9.0%	
   

7.2%	
   

pa_ern	
   
comple4on	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

learning	
   shared	
   representa4ons	
   

across	
   sensory	
   modali4es	
   

   concept   	
   

sunset,	
   paci   c	
   ocean,	
   
baker	
   beach,	
   seashore,	
   

ocean	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

mul4modal	
   dbm	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   sojmax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   sojmax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   sojmax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

bo_om-     up	
   

+	
   

top-     down	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   sojmax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

bo_om-     up	
   

+	
   

top-     down	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   sojmax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

text	
   generated	
   from	
   images	
   

given
	
   
	
   	
   

	
   	
   

generated 	
   
dog,	
   cat,	
   pet,	
   ki_en,	
   
puppy,	
   ginger,	
   tongue,	
   
ki_y,	
   dogs,	
   furry	
   

given
	
   
	
   	
   

generated 	
   

	
   	
   
insect,	
   bu_er   y,	
   insects,	
   
bug,	
   bu_er   ies,	
   
lepidoptera	
   

sea,	
   france,	
   boat,	
   mer,	
   
beach,	
   river,	
   bretagne,	
   
plage,	
   bri_any	
   

portrait,	
   child,	
   kid,	
   
ritra_o,	
   kids,	
   children,	
   
boy,	
   cute,	
   boys,	
   italy	
   

gra   4,	
   streetart,	
   stencil,	
   
s4cker,	
   urbanart,	
   gra   ,	
   
sanfrancisco	
   

canada,	
   nature,	
   
sunrise,	
   ontario,	
   fog,	
   
mist,	
   bc,	
   morning	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

text	
   generated	
   from	
   images	
   

given
	
   
	
   	
   

generated 	
   

	
   	
   

portrait,	
   women,	
   army,	
   soldier,	
   
mother,	
   postcard,	
   soldiers	
   

obama,	
   barackobama,	
   elec4on,	
   
poli4cs,	
   president,	
   hope,	
   change,	
   
sanfrancisco,	
   conven4on,	
   rally	
   

water,	
   glass,	
   beer,	
   bo_le,	
   
drink,	
   wine,	
   bubbles,	
   splash,	
   
drops,	
   drop	
   

images	
   selected	
   from	
   text	
   

retrieved	
   

given
	
   
	
   	
   

water,	
   red,	
   
sunset	
   

nature,	
      ower,	
   
red,	
   green	
   

blue,	
   green,	
   
yellow,	
   colors	
   

chocolate,	
   cake	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

summary	
   

       e   cient	
   learning	
   algorithms	
   for	
   deep	
   learning	
   models.	
   learning	
   

more	
   adap4ve,	
   robust,	
   and	
   structured	
   representa4ons.	
   	
   
text	
   &	
   image	
   retrieval	
   /	
   	
   
object	
   recognigon	
   

learning	
   a	
   category	
   
hierarchy	
   

image	
   tagging	
   

	
   

speech	
   recognigon	
   

id48	
   decoder	
   

mosque,	
   tower,	
   
building,	
   cathedral,	
   
dome,	
   castle	
   

	
   	
   	
   	
   	
   

mulgmodal	
   data	
   

	
   	
   	
   	
   	
   

sunset,	
   paci   c	
   ocean,	
   
beach,	
   seashore	
   

capgon	
   generagon	
   

       deep	
   models	
   improve	
   the	
   current	
   state-     of-     the	
   art	
   in	
   many	
   

applica4on	
   domains:	
   
         object	
   recogni4on	
   and	
   detec4on,	
   text	
   and	
   image	
   retrieval,	
   handwri_en	
   

character	
   and	
   speech	
   recogni4on,	
   and	
   others.	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

