10-601 machine learning

maria-florina balcan

spring 2015

plan: id88 algorithm for learning linear separators.

1 learning linear separators
here we can think of examples as being from {0, 1}n or from rn. given a training set
of labeled examples (that is consistent with a linear separator),we can    nd a hyperplane
w    x = w0 such that all positive examples are on one side and all negative examples are
on other. i.e., w    x > w0 for positive x   s and w    x < w0 for negative x   s. we can solve
this using id135. the sample complexity results for classes of    nite vc-
dimension together with known results about id135 imply that the class of
linear separators is e   ciently learnable in the pac (distributional) model. today we will
talk about the id88 algorithm.

1.1 the id88 algorithm

one of the oldest algorithms used in machine learning (from early 60s) is an online algorithm
for learning a linear threshold function called the id88 algorithm.

we present the id88 algorithm in the online learning model.
following scenario is repeats:

in this model, the

1. the algorithm receives an unlabeled example.

2. the algorithm predicts a classi   cation of this example.

3. the algorithm is then told the correct answer.

we will call whatever is used to perform step (2), the algorithm   s    current hypothesis.   

as mentioned, the id88 algorithm is an online algorithm for learning linear separators.
for simplicity, we   ll use a threshold of 0, so we   re looking at learning functions like:

w1x1 + w2x2 + ... + wnxn > 0.

we can simulate a nonzero threshold with a    dummy    input x0 that is always 1, so this can
be done without loss of generality.

1

the id88 algorithm:

1. start with the all-zeroes weight vector w1 = 0, and initialize t to 1.
2. given example x, predict positive i    wt    x > 0.
3. on a mistake, update as follows:

    mistake on positive: wt+1     wt + x.
    mistake on negative: wt+1     wt     x.

t     t + 1.

so, this seems reasonable. if we make a mistake on a positive x we get wt+1  x = (wt+x)  x =
wt    x + ||x||2, and similarly if we make a mistake on a negative x we have wt+1    x =
(wt     x)    x = wt    x     ||x||2. so, in both cases we move closer (by ||x||2) to the value we
wanted.

we will show the following guarantee for the id88 algorithm :
theorem 1 let s be a sequence of labeled examples consistent with a linear threshold func-
tion w       x > 0, where w    is a unit-length vector. then the number of mistakes m on s
made by the online id88 algorithm is at most (r/  )2, where

r = max

x   s ||x||, and    = min

x   s |w       x|.

note that since w    is a unit-length vector, the quantity |w       x| is equal to the distance of x
to the separating hyperplane w       x = 0. the parameter          is often called the    margin    of
w   , or more formally, the l2 margin because we are measuring euclidean distance.
proof of theorem 1. we are going to look at the following two quantities wt    w    and ||wt||.
claim 1: wt+1    w        wt    w    +   . that is, every time we make a mistake, the dot-product
of our weight vector with the target increases by at least   .

if x was a positive example, then we get wt+1    w    = (wt + x)    w    =
proof:
wt    w    + x    w        wt    w    +    (by de   nition of   ). similarly, if x was a negative
example, we get (wt     x)    w    = wt    w        x    w        wt    w    +   .

claim 2: ||wt+1||2     ||wt||2 + r2. that is, every time we make a mistake, the length squared
of our weight vector increases by at most r2.

proof: if x was a positive example, we get ||wt + x||2 = ||wt||2 + 2wt    x + ||x||2.
this is less than ||wt||2 + ||x||2 because wt    x is negative (remember, we made
a mistake on x), and this in turn is at most ||wt||2 + r2. same thing (   ipping
signs) if x was negative but we predicted positive.

2

claim 1 implies that after m mistakes, wm +1    w          m . on the other hand, claim 2
implies that after m mistakes, ||wm +1||2     r2m . now, all we need to do is use the fact
that wm +1    w        ||wm +1||, since w    is a unit-length vector. so, this means we must have
  m     r

   
m , and thus m     (r/  )2.

discussion: in order to use the id88 algorithm to    nd a consistent linear separator
given a set s of labeled examples that is linearly separable by margin   , we do the following.
we repeatedly feed the whole set s of labeled examples into the id88 algorithm up to
(r/  )2 + 1 rounds, until we get to a point where the current hypothesis is consistent with
the whole set s. note that by theorem 1, we are guaranteed to reach such a point. the
runnning time is then polynomial in |s| and (r/  )2.
in the worst case,    can be exponentially small in n. on the other hand, if we   re lucky
and the data is well-separated,    might even be large compared to 1/n. this is called the
   large margin    case. (in fact, the latter is the more modern spin on things: namely, that in
many natural cases, we would hope that there exists a large-margin separator.) in fact, one
nice thing here is that the mistake-bound depends on just a purely geometric quantity: the
amount of    wiggle-room    available for a solution and doesn   t depend in any direct way on
the number of features in the space.

so, if data is separable by a large margin, then the id88 is a good algorithm to use.

1.2 additional more advanced notes

guarantee in a distributional setting: in order to get a distributional guarantee we can
do the following.1 let m = (r/  )2. for any  ,   , we draw a sample of size (m/ )   log(m/  ).
we then run id88 on the data set and look at the sequence of hypotheses produced:
. for each i, if hi is consistent with following 1/     log(m/  ) examples, then we
h1, h2, ...
stop and output hi. we can argue that with id203 at least 1       , the hypothesis we
output has error at most  . this can be shown as follows. if hi was a bad hypothsis with
true error greater than  , then the chance we stopped and output hi was at most   /m . so,
by union bound, there   s at most a    chance we are fooled by any of the hypotheses.

note that this implies that if the margin over the whole distribution is 1/poly(n), the per-
ceptron algorithm can be used to pac learn the class of linear separators.

what if there is no perfect separator? what if only most of the data is separable by
a large margin, or what if w    is not perfect? we can see that the thing we need to look at
is claim 1. claim 1 said that we make       amount of progress    on every mistake. now it   s
possible there will be mistakes where we make very little progress, or even negative progress.
one thing we can do is bound the total number of mistakes we make in terms of the total
distance we would have to move the points to make them actually separable by margin   .
let   s call that td  . then, we get that after m mistakes, wm +1    w          m     td  . so,

1this is not the most sample e   cient online to pac reduction, but it is the simplest to think about.

3

   

m       m     td  . we could solve the quadratic,
   td   is called

combining with claim 2, we get that r
but this implies, for instance, that m     (r/  )2 + (2/  )td  . the quantity 1
the total hinge-loss of w   .
so, this is not too bad: we can   t necessarily say that we   re making only a small multiple of
the number of mistakes that w    is (in fact, the problem of    nding an approximately-optimal
separator is np-hard), but we can say we   re doing well in terms of the    total distance   
parameter.

id88 for approximately maximizing margins. we saw that the id88
algorithm makes at most (r/  )2 mistakes on any sequence of examples that is linearly-
separable by margin   , i.e., any sequence for which there exists a unit-length vector w    such
that all examples x satisfy (cid:96)(x)(w       x)       , where (cid:96)(x)     {   1, 1} is the label of x.
suppose we are handed a set of examples s and we want to actually    nd a large-margin
separator for them. one approach is to directly solve for the maximum-margin separator
using convex programming (which is what is done in the id166 algorithm). however, if we
only need to approximately maximize the margin, then another approach is to use id88.
in particular, suppose we cycle through the data using the id88 algorithm, updating
not only on mistakes, but also on examples x that our current hypothesis gets correct by
margin less than   /2. assuming our data is separable by margin   , then we can show that
this is guaranteed to halt in a number of rounds that is polynomial in r/  . (in fact, we can
replace   /2 with (1      )   and have bounds that are polynomial in r/(   ).)

the margin id88 algorithm(  ):

1. initialize w1 = (cid:96)(x)x, where x is the    rst example seen and initialize t to 1.
2. predict positive if wt  x

||wt||          /2, and consider an

||wt||       /2, predict negative if wt  x
||wt||     (     /2,   /2).

example to be a margin mistake when wt  x

3. on a mistake (incorrect prediction or margin mistake), update as in the standard

id88 algorithm: wt+1     wt + (cid:96)(x)x; t     t + 1.

theorem 2 let s be a sequence of labeled examples consistent with a linear threshold func-
tion w       x > 0, where w    is a unit-length vector, and let
x   s ||x||, and    = min

x   s |w       x|.

r = max

then the number of mistakes (including margin mistakes) made by margin id88(  ) on
s is at most 8(r/  )2 + 4(r/  ).

proof: the argument for this new algorithm follows the same lines as the argument for the
original id88 algorithm.

4

as before, each update increases wt   w    by at least   . what is now a little more complicated
is to bound the increase in ||wt||. for the original algorithm, we had: ||wt+1||2     ||wt||2 +r2,
which implies ||wt+1||     ||wt|| + r2
for the new algorithm, we can show instead:

2||wt||.

||wt+1||     ||wt|| +

to see this note that:

  
2

r2
2||wt|| +
(cid:32)

||wt+1||2 = ||wt||2 + 2l(x)wt    x + ||x||2 = ||wt||2

.

(1)

(cid:33)

wt    x
2l(x)
||wt||
||wt|| +
1 +
   
1 +        1+   
(cid:33)

||x||2
||wt||2
2 and ||x||2     r2

(cid:32)

taking the square-root of both sides and using the inequality
we get:

wt    x
||wt|| +

r2

1 +

l(x)
||wt||

||wt+1||     ||wt||
||wt||       
combining this with the fact l(x)wt  x
we get the desired upper bound on ||wt+1||, namely: ||wt+1||     ||wt|| +   
note that (1) implies that if ||wt||     2r2/   then ||wt+1||     ||wt|| + 3  /4. note also that
||wt+1||     ||wt|| + ||x|| (by triangle inequality), so ||wt+1||     ||wt|| + r. these two facts
imply that after m updates we have:

2 (since wt made a mistake or margin mistake on x)

2||wt||2

2 + r2

2||wt|| .

.

||wm +1||     (2r2/   + r) + 3m   /4.

solving m        (2r2/   + r) + 3m   /4 we get m     8r2/  2 + 4r/  , as desired.

5

