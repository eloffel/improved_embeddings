machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

january 12, 2015 

today: 
       what is machine learning? 
       decision tree learning 
       course logistics 

readings: 
          the discipline of ml    
       mitchell, chapter 3 
       bishop, chapter 14.4 

machine learning: 

study of algorithms that 
       improve their performance p 
       at some task t 
       with experience e 

well-defined learning task: <p,t,e> 

1 

learning to predict emergency c-sections 

[sims et al., 2000] 

9714 patient records, 
each with 215 features 

learning to classify text documents 

spam 
 vs 
not spam 

2 

learning to detect objects in images 

(prof. h. schneiderman) 

example training images 

for each orientation 

learn to classify the 
word a person is 
thinking about, based 
on fmri brain activity 

3 

learning prosthetic control from 
neural implant 

[r. kass 
l. castellanos 
a. schwartz] 

machine learning - practice 

id103 

mining databases 

text analysis 

control learning 

object recognition 

       support vector machines 
       id110s 
       id48 
       deep neural networks 
       id23 
       .... 

4 

machine learning - theory 
other theories for 
       reinforcement skill learning 
       semi-supervised learning 
       active student querying 
           

pac learning theory 

(supervised concept learning) 

# examples (m) 

error rate (  ) 

representational 
complexity (h) 

failure 
id203 (  ) 

    also relating: 
       # of mistakes during learning 
       learner   s query strategy 
       convergence rate 
       asymptotic performance 
       bias, variance 

machine learning in computer science 

       machine learning already the preferred approach to 

       id103, natural language processing 
       id161 
       medical outcomes analysis 
       robot control 
           

ml apps. 

       this ml niche is growing (why?) 

all software apps. 

5 

machine learning in computer science 

       machine learning already the preferred approach to 

       id103, natural language processing 
       id161 
       medical outcomes analysis 
       robot control 
           

ml apps. 

all software apps. 

       this ml niche is growing 

       improved machine learning algorithms  
       increased volume of online data  
       increased demand for self-customizing software 

tom   s prediction: ml will be fastest-growing part of cs this century 

economics 

and 

organizational 

behavior 

computer science 

machine learning 

evolution 

statistics  

animal learning 
(cognitive science, 

psychology, 
neuroscience) 

adaptive control 

theory 

6 

what you   ll learn in this course 
       the primary machine learning algorithms 

       id28, bayesian methods, id48   s, id166   s, 
id23, decision tree learning, boosting, 
unsupervised id91,     

       how to use them on real data 

       text, image, structured data 
       your own project 

       underlying statistical and computational theory 

       enough to read and understand ml research papers 

course logistics 

7 

machine learning 10-601 
website: www.cs.cmu.edu/~ninamf/courses/601sp15            

see webpage for  
       office hours 
       syllabus details 
       recitation sessions 
       grading policy 
       honesty policy 
       late homework policy 
       piazza pointers 
      

... 

faculty 
       maria balcan 
       tom mitchell 
 
ta   s 
       travis dick 
       kirsten early 
       ahmed hefny 
       micol marchetti-bowick 
       willie neiswanger 
       abu saparov 
 
course assistant 
       sharon cavlovich 

 

highlights of course logistics 

on the wait list? 
       hang in there for first few weeks 
homework 1 
       available now, due friday 
grading: 
       30% homeworks (~5-6) 
       20% course project 
       25% first midterm (march 2) 
       25% final midterm (april 29) 
 
academic integrity: 
       cheating       fail class, be expelled 

from cmu 

full credit when due 

late homework: 
      
       half credit next 48 hrs 
       zero credit after that 
       we   ll  delete your lowest hw score 
       must turn in at least n-1 of the n 

homeworks, even if late 

being present at exams: 
       you must be there     plan now. 
       two in-class exams, no other final  

8 

maria-florina balcan: nina 
       foundations for modern machine learning 
       e.g., interactive, distributed, life-long learning 

       theoretical computer science, especially connections 

between learning theory & other fields 

approx. 
algorithms 

control 
theory 

game theory 

machine 
learning  
theory 

mechanism design 

discrete 
optimization 

matroid  
theory 

travis dick 

       when can we learn many concepts 

from mostly unlabeled data by 
exploiting relationships between 
between concepts. 

       currently: geometric relationships 

9 

kirstin early 
       analyzing and predicting  

energy consumption 

       reduce costs/usage and help  

people make informed decisions 

predicting energy costs 
from features of home  
and occupant behavior 

energy disaggregation: 
decomposing total electric signal  
into individual appliances 

ahmed hefny 

       how can we learn to track and predict the state of a 
dynamical system only from noisy observations ?  

      

 can we exploit supervised learning  methods to devise a 
flexible, local minima-free approach ? 

observations (oscillating pendulum) 

extracted 2d state 
trajectory 

10 

micol marchetti-bowick 
how can we use machine learning for 
biological and medical research? 
       using genotype data to build personalized 
models that can predict clinical outcomes 
integrating data from multiple sources to 
perform cancer subtype analysis 

       structured sparse regression models for 

      

genome-wide association studies 

gene expression data 
w/ dendrogram (or have 
one picture per task) 

sample weight

genetic relatedness

x

y

x

y

x

y

x

y

x

y

x

y

x

y

x

y

x

y

willie neiswanger 

if we want to apply machine learning 

      
    algorithms to big datasets    
       how can we develop parallel, low-communication machine 

learning algorithms? 

 

       such as embarrassingly parallel algorithms, where machines 

work independently, without communication. 

11 

abu saparov 

       how can knowledge about the 

world help computers understand 
natural language? 

       what kinds of machine learning 
tools are needed to understand 
sentences?    

   carolyn ate the cake with a fork.    

   carolyn ate the cake with vanilla.    

person_eats_food 

consumer 
food 
instrument 

carolyn 

cake 
fork 

consumer 
food 

person_eats_food 
carolyn 

cake 

topping 

vanilla 

tom mitchell 

how can we build never-ending learners? 
case study: never-ending language learner 

(nell) runs 24x7 to learn to read the web 

mean avg. precision top 

1000 

see  http://rtw.ml.cmu.edu 

# of beliefs 

vs.  

time (5 years) 

 

reading accuracy  

time (5 years) 

vs.  

 

12 

function approximation and 
decision tree learning 

function approximation 

problem setting: 
       set of possible instances x
       unknown target function f : x     y
       set of function hypotheses h={ h | h : x     y }

superscript: ith training example
input: 
       training examples {<x(i),y(i)>} of unknown target function f 

output: 
       hypothesis h     h that best approximates target function f

13 

simple training data set 

day   outlook  temperature  humidity   wind   playtennis? 

a decision tree for  
f: <outlook, temperature, humidity, wind>       playtennis? 

each internal node: test one discrete-valued attribute xi 
each branch from a node: selects one value for xi 
each leaf node: predict y  (or p(y|x     leaf)) 

14 

decision tree learning 
problem setting: 
       set of possible instances x 

       each instance x in x is a feature vector 
       e.g., <humidity=low, wind=weak, outlook=rain, temp=hot> 

       unknown target function f : x     y

       y=1 if we play tennis on this day, else 0 
       set of function hypotheses h={ h | h : x     y }

       each hypothesis h is a decision tree 
       trees sorts x to leaf, which assigns y 

decision tree learning 
problem setting: 
       set of possible instances x 

       each instance x in x is a feature vector  

       unknown target function f : x     y

x = < x1, x2     xn> 
       y is discrete-valued 

       set of function hypotheses h={ h | h : x     y }

       each hypothesis h is a decision tree 

input: 
       training examples {<x(i),y(i)>} of unknown target function f
output: 
       hypothesis h     h that best approximates target function f

15 

id90 
suppose x = <x1,    xn>  
where xi are boolean-valued variables 
 
 
how would you represent y = x2 x5 ?     y = x2     x5 

how would you represent  x2 x5      x3x4(  x1) 

 

16 

node = root 

[ , c4.5, quinlan] 

sample id178 

17 

id178 
id178 h(x) of a random variable x 
 
 
 
h(x) is the expected number of bits needed to encode a 

# of possible 
values for x

randomly drawn value of x  (under most efficient code)  

 
why?  id205: 
       most efficient possible code assigns  -log2 p(x=i)  bits 

to encode the message x=i 

       so, expected number of bits to code one random x is:  
 

id178 
id178 h(x) of a random variable x 
 
 
 
specific conditional id178 h(x|y=v) of x given y=v : 
 
 

conditional id178 h(x|y) of x given y : 

mutual information (aka information gain) of x and y : 

18 

information gain is the mutual information between 
input attribute a and target variable y 
 
information gain is the expected reduction in id178 
of target variable y for data sample s, due to sorting 
on variable a  
 

simple training data set 

day   outlook    temperature   humidity    wind   playtennis? 

19 

20 

final decision tree for 
f: <outlook, temperature, humidity, wind>       playtennis? 

each internal node: test one discrete-valued attribute xi 
each branch from a node: selects one value for xi 
each leaf node: predict y 

which tree should we output? 

         performs heuristic 

search through space of 
id90 

       it stops at smallest 

acceptable tree. why? 

occam   s razor: prefer the 
simplest hypothesis that 
fits the data 

21 

why prefer short hypotheses? (occam   s razor) 

arguments in favor: 
 
 
 
 
arguments opposed:  

why prefer short hypotheses? (occam   s razor) 

argument in favor: 
       fewer short hypotheses than long ones 
      a short hypothesis that fits the data is less likely to be 

a statistical coincidence 

      highly probable that a sufficiently complex hypothesis 

will fit the data 

 
argument opposed: 
       also fewer hypotheses with prime number of nodes 

and attributes beginning with    z    

       what   s so special about    short    hypotheses? 

22 

overfitting 
consider a hypothesis h and its 
       error rate over training data: 
       true error rate over all data:  
 
we say h overfits the training data if 
 
 
amount of overfitting =  

23 

24 

split data into training and validation set
create tree that classi   es training set correctly

25 

26 

you should know: 
       well posed function approximation problems: 

       instance space, x 
       sample of labeled training data { <x(i), y(i)>} 
       hypothesis space, h = { f: x     y } 

       learning is a search/optimization problem over h 

       various objective functions 

       minimize training error (0-1 loss)  
       among hypotheses that minimize training error, select smallest (?) 

       decision tree learning 

       greedy top-down learning of id90 ( , c4.5, ...) 
       overfitting and tree/rule post-pruning 
       extensions    

questions to think about (1) 
         and c4.5 are heuristic algorithms that 

search through the space of id90.  
why not just do an exhaustive search? 

27 

questions to think about (2) 
       consider target function f: <x1,x2>       y, 

where x1 and x2 are real-valued, y is 
boolean.  what is the set of decision surfaces 
describable with id90 that use each 
attribute at most once? 

questions to think about (3) 
       why use information gain to select attributes 
in id90?  what other criteria seem 
reasonable, and what are the tradeoffs in 
making this choice?   

28 

questions to think about (4) 
       what is the relationship between learning 
id90, and learning if-then rules 

29 

