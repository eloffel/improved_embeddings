coms 4721: machine learning for data science

lecture 14, 3/21/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

unsupervised learning

supervised learning

framework of supervised learning
given: pairs (x1, y1), . . . , (xn, yn). think of x as input and y as output.
learn: a function f (x) that accurately predicts yi     f (xi) on this data.
goal: use the function f (x) to predict new y0 given x0.

probabilistic motivation
if we think of (x, y) as a random variable with joint distribution p(x, y), then
supervised learning seeks to learn the conditional distribution p(y|x).

this can be done either directly or indirectly:

directly: e.g., with id28 where p(y|x) = sigmoid function
indirectly: e.g., with a bayes classi   er

y = arg max

k

p(y = k|x) = arg max

k

p(x|y = k)p(y = k)

unsupervised learning

some motivation
(cid:73) the bayes classi   er factorizes the joint density as p(x, y) = p(x|y)p(y).
(cid:73) the joint density can also be written as p(x, y) = p(y|x)p(x).
(cid:73) unsupervised learning focuses on the term p(x)     learning p(x|y) on a

class-speci   c subset has the same    feel.    what should this be?

(cid:73) (this implies an underlying classi   cation task, but often there isn   t one.)

unsupervised learning
given: a data set x1, . . . , xn, where xi     x , e.g., x = rd
de   ne: some model of the data (probabilistic or non-probabilistic).
goal: learn structure within the data set as de   ned by the model.
(cid:73) supervised learning has a clear performance metric: accuracy
(cid:73) unsupervised learning is often (but not always) more subjective

some types of unsupervised learning

overview of second half of course
we will discuss a few types of unsupervised learning approaches in the
second half of the course.

id91 models: learn a partition of data x1, . . . , xn into groups.
(cid:73) image segmentation, data quantization, preprocessing for other models
id105: learn an underlying dot-product representation.
(cid:73) user preference modeling, id96
sequential models: learn a model based on sequential information.
(cid:73) learn how to rank objects, target tracking

as will become evident, an unsupervised model can often be interpreted as a
supervised model, or very easily turned into one.

id91

problem
(cid:73) given data x1, . . . , xn, partition it into

groups called clusters.

(cid:73) find the clusters, given only the data.
(cid:73) observations in same group        similar,   

different groups        different.   

(cid:73) we will set how many clusters we learn.

cluster assignment representation
for k clusters, encode cluster assignments as an indicator c     {1, . . . , k},

ci = k        xi is assigned to cluster k

id91 feels similar to classi   cation in that we    label    an observation by
its cluster assignment. the difference is that there is no ground truth.

the id116 algorithm

id91 and id116

id116 is the simplest and most fundamental id91 algorithm.

input: x1, . . . , xn, where x     rd.
output: vector c of cluster assignments, and k mean vectors   

(cid:73) c = (c1, . . . , cn),

ci     {1, . . . , k}

    if ci = cj = k, then xi and xj are clustered together in cluster k.

(cid:73)    = (  1, . . . ,   k),

  k     rd (same space as xi)

    each   k (called a centroid) de   nes a cluster.

as usual, we need to de   ne an objective function. we pick one that:

1. tells us what are good c and   , and
2. that is easy to optimize.

id116 objective function

the id116 objective function can be written as

n(cid:88)

k(cid:88)

i=1

k=1

1{ci = k}(cid:107)xi       k(cid:107)2

     , c    = arg min
  ,c

some observations:

(cid:73) id116 uses the squared euclidean distance of xi to the centroid   k.
(cid:73) it only penalizes the distance of xi to the centroid it   s assigned to by ci.

n(cid:88)

k(cid:88)

k(cid:88)

(cid:88)

l =

1{ci = k}(cid:107)xi       k(cid:107)2 =

(cid:107)xi       k(cid:107)2

i=1

k=1

k=1

i:ci=k

(cid:73) the objective function is    non-convex   

(cid:73) this means that we can   t actually    nd the optimal       and c   .
(cid:73) we can only derive an algorithm for    nding a local optimum (more later).

optimizing the id116 objective

gradient-based optimization
we can   t optimize the id116 objective function exactly by taking
derivatives and setting to zero, so we use an iterative algorithm.

however, the algorithm we will use is different from gradient methods:

w     w          wl

(id119)

recall: with id119, when we update a parameter    w    we move in
the direction that decreases the objective function, but

(cid:73) it will almost certainly not move to the best value for that parameter.
(cid:73) it may not even move to a better value if the step size    is too big.
(cid:73) we also need the parameter w to be continuous-valued.

id116 and coordinate decent

coordinate descent
we will discuss a new and widely used optimization procedure in the context
of id116 id91. we want to minimize the objective function

n(cid:88)

k(cid:88)

l =

1{ci = k}(cid:107)xi       k(cid:107)2.

i=1

k=1

we split the variables into two unknown sets    and c. we can   t    nd their
best values at the same time to minimize l. however, we will see that
(cid:73) fixing    we can    nd the best c exactly.
(cid:73) fixing c we can    nd the best    exactly.

this optimization approach is called coordinate descent: hold one set of
parameters    xed, and optimize the other set. then switch which set is    xed.

coordinate descent

coordinate descent (in the context of id116)
input: x1, . . . , xn where xi     rd. randomly initialize    = (  1, . . . ,   k).
(cid:73) iterate back-and-forth between the following two steps:

1. given   ,    nd the best value ci     {1, . . . , k} for i = 1, . . . , n.
2. given c,    nd the best vector   k     rd for k = 1, . . . , k.

there   s a circular way of thinking about why we need to iterate:

1. given a particular   , we may be able to    nd the best c, but once we

change c we can probably    nd a better   .

2. then    nd the best    for the new-and-improved c found in #1, but now

that we   ve changed   , there is probably a better c.

we have to iterate because the values of    and c depend on each other.
this happens very frequently in unsupervised models.

id116 algorithm: updating c

assignment step
given    = (  1, . . . ,   k), update c = (c1, . . . , cn). by rewriting l, we notice
the independence of each ci given   ,

(cid:16) k(cid:88)
(cid:124)

k=1

(cid:123)(cid:122)

(cid:17)

(cid:125)

(cid:16) k(cid:88)
(cid:124)

k=1

(cid:123)(cid:122)

l =

1{c1 = k}(cid:107)x1       k(cid:107)2

+        +

1{cn = k}(cid:107)xn       k(cid:107)2

distance of x1 to its assigned centroid

distance of xn to its assigned centroid

(cid:17)

.

(cid:125)

we can minimize l with respect to each ci by minimizing each term above
separately. the solution is to assign xi to the closest centroid

ci = arg min
k

(cid:107)xi       k(cid:107)2.

because there are only k options for each ci, there are no derivatives. simply
calculate all the possible values for ci and pick the best (smallest) one.

id116 algorithm: updating   

update step
given c = (c1, . . . , cn), update    = (  1, . . . ,   k). for a given c, we can
break l into k clusters de   ned by c so that each   i is independent.

l =

(cid:16) n(cid:88)
(cid:124)

i=1

1{ci = 1}(cid:107)xi       1(cid:107)2(cid:17)
(cid:125)

(cid:123)(cid:122)

sum squared distance of data in cluster #1

for each k, we then optimize. let nk =(cid:80)n

+       +

(cid:16) n(cid:88)
(cid:124)

i=1

1{ci = k}(cid:107)xi       k(cid:107)2(cid:17)
(cid:125)

(cid:123)(cid:122)

sum squared distance of data in cluster #k

.

  k = arg min
  

1{ci = k}(cid:107)xi       (cid:107)2          k =

1
nk

xi1{ci = k}.

1{ci = k}. then

i=1

n(cid:88)

i=1

n(cid:88)

i=1

that is,   k is the mean of the data assigned to cluster k.

id116 id91 algorithm

algorithm: id116 id91
given: x1, . . . , xn where each x     rd

goal: minimize l =(cid:80)n

(cid:80)k

i=1

k=1

1{ci = k}(cid:107)xi       k(cid:107)2.

(cid:73) randomly initialize    = (  1, . . . ,   k).
(cid:73) iterate until c and    stop changing

1. update each ci :

ci = arg min

k

(cid:107)xi       k(cid:107)2

2. update each   k : set

n(cid:88)

i=1

nk =

1{ci = k} and

  k =

n(cid:88)

i=1

1
nk

xi1{ci = k}

id116 algorithm: example run

a random initialization

(a)   202   202id116 algorithm: example run

iteration 1

assign data to clusters

(b)   202   202id116 algorithm: example run

iteration 1

update the centroids

(c)   202   202id116 algorithm: example run

iteration 2

assign data to clusters

(d)   202   202id116 algorithm: example run

iteration 2

update the centroids

(e)   202   202id116 algorithm: example run

iteration 3

assign data to clusters

(f)   202   202id116 algorithm: example run

iteration 3

update the centroids

(g)   202   202id116 algorithm: example run

iteration 4

assign data to clusters

(h)   202   202id116 algorithm: example run

iteration 4

update the centroids

(i)   202   202convergence of id116

objective function after

(cid:73) the    assignment    step (blue: corresponding to c), and
(cid:73) the    update    step (red: corresponding to   ).

123405001000literationconvergence of id116

the outline of why this convergences is straightforward:
1. every update to ci or   k decreases l compared to the previous value.
2. therefore, l is monotonically decreasing.
3. l     0, so step 1 converges to some point (but probably not to 0).

when c stops changing, the algorithm has converged to a local optimal
solution. this is a result of l not being convex.

non-convexity means that different initializations will give different results:

(cid:73) often the results will be similar in quality, but no guarantees.
(cid:73) in practice, the algorithm can be run multiple times with different

initializations. then use the result with the lowest l.

selecting k

we don   t know how many clusters there are, but selecting k is tricky.
the id116 objective function decreases as k increases,

n(cid:88)

k(cid:88)

l =

1{ci = k}(cid:107)xi       k(cid:107)2.

for example, if k = n then let   k = xk and as a result l = 0.

i=1

k=1

methods for choosing k include:

(cid:73) using advanced knowledge. e.g., if you want to split a set of tasks

among k people, then you already know k.

(cid:73) looking at the relative decrease in l. if k    is best, then increasing k

when k     k    should decrease l much more than when k > k   .
(cid:73) often the id116 result is part of a larger application. the main

application may start to perform worse even though l is decreasing.

(cid:73) more advanced modeling techniques exist that address this issue.

two applications of id116

lossy data compression

approach: vectorize 2    2 patches from an image (so data is x     r4) and
cluster them with id116. replace each patch with its assigned centroid.
(left) original 1024  1024 image requiring 8 bits/pixel (1mb total)
(middle) approximation using 200 clusters (requires 239kb storage)

(right) approximation using 4 clusters (requires 62kb storage)

id174 (side comment)
id116 is also very useful for discretizing data as a preprocessing step.
this allows us to recast a continuous-valued problem as a discrete one.

extensions: k-medoids

algorithm: k-medoids id91
input: data x1, . . . , xn and distance measure d(x,   ). randomly initialize   .

(cid:73) iterate until c is no longer changing

1. for each ci : set

2. for each   k : set

ci = arg min

k

d(xi,   k)

(cid:88)

i:ci=k

  k = arg min
  

d(xi,   )

comment: step #2 may require an algorithm.

k-medoids is a straightforward extension of id116 where the distance
measure isn   t the squared error. that is,
(cid:73) id116 uses d(x,   ) = (cid:107)x       (cid:107)2.
(cid:73) could set d(x,   ) = (cid:107)x       (cid:107)1, which would be more robust to outliers.
(cid:73) if x (cid:54)    rd, we could de   ne d(x,   ) to be more complex.

