machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

february 4, 2015 

classifiers 

today: 
       generative     discriminative 
       id75 
       decomposition of error into 
bias, variance, unavoidable 

readings: 
 
       mitchell:    na  ve bayes and 

id28    

     (required) 
       ng and jordan paper (optional) 
       bishop, ch 9.1, 9.2 (optional) 

id28 

       consider learning f: x       y, where 

       x is a vector of real-valued features, < x1     xn > 
       y is boolean 
       assume all xi are conditionally independent given y 
       model p(xi | y = yk) as gaussian n(  ik,  i) 
       model p(y) as bernoulli (  ) 

       then p(y|x) is of this form, and we can directly estimate w 

       furthermore, same holds if the xi are boolean 

       trying proving that to yourself 

       train by gradient ascent estimation of w   s (no assumptions!) 

id113 vs map  

       maximum conditional likelihood estimate 

       maximum a posteriori estimate with prior w~n(0,  i) 

map estimates and id173 

       maximum a posteriori estimate with prior w~n(0,  i) 

called a    id173    term 
       helps reduce overfitting, especially when training 
data is sparse 
       keep weights nearer to zero (if p(w) is zero mean 
gaussian prior), or whatever the prior suggests 
       used very frequently in id28 

generative vs. discriminative classifiers 

training classifiers involves estimating f: x       y, or p(y|x) 
 
generative classifiers (e.g., na  ve bayes) 
       assume some functional form for p(y), p(x|y)  
       estimate parameters of p(x|y), p(y) directly from training data 
       use bayes rule to calculate p(y=y |x= x) 

discriminative classifiers (e.g., id28) 
       assume some functional form for p(y|x) 
       estimate parameters of p(y|x) directly from training data 
       note: even though our derivation of the form of p(y|x) made gnb-

style assumptions, the training procedure for id28 
does not! 

use na  ve bayes or logisitic regression? 

consider 
       restrictiveness of modeling assumptions (how 

well can we learn with infinite data?) 

 
 
       rate of convergence (in amount of training data) 

toward asymptotic (infinite data) hypothesis  
       i.e., the learning curve 

na  ve bayes vs id28 
consider y boolean, xi continuous, x=<x1 ... xn> 
 
number of parameters: 
       nb: 4n +1 
       lr: n+1 

estimation method: 
       nb parameter estimates are uncoupled 
       lr parameter estimates are coupled 
 

gaussian na  ve bayes     big picture 

assume p(y=1) = 0.5 

gaussian na  ve bayes     big picture 

assume p(y=1) = 0.5 

g.na  ve bayes vs. id28 

[ng & jordan, 2002] 

recall two assumptions deriving form of lr from gnbayes: 
1.   xi conditionally independent of xk given y 
2.   p(xi | y = yk)  =  n(  ik,  i),         not n(  ik,  ik) 

consider three learning methods: 
      gnb (assumption 1 only) 
      gnb2 (assumption 1 and 2) 
      lr  
 
which method works better if we have infinite training data, and... 
 
      both (1) and (2) are satisfied 

      neither (1) nor (2) is satisfied 

      (1) is satisfied, but not (2)  

g.na  ve bayes vs. id28 

[ng & jordan, 2002] 

recall two assumptions deriving form of lr from gnbayes: 
1.   xi conditionally independent of xk given y 
2.   p(xi | y = yk)  =  n(  ik,  i),         not n(  ik,  ik) 

consider three learning methods: 
      gnb (assumption 1 only)     -- decision surface can be non-linear 
      gnb2 (assumption 1 and 2)     decision surface linear 
      lr                                         -- decision surface linear, trained differently 
 
which method works better if we have infinite training data, and... 
 
      both (1) and (2) are satisfied:    lr = gnb2 = gnb 

      neither (1) nor (2) is satisfied:   lr > gnb2,   gnb>gnb2 

      (1) is satisfied, but not (2) :        gnb > lr,   lr > gnb2  

g.na  ve bayes vs. id28 

[ng & jordan, 2002] 

what if we have only finite training data? 
 
they converge at different rates to their asymptotic (    data) error 
 
let          refer to expected error of learning algorithm a after n training 
examples 
 
let d be the number of features: <x1     xd> 
 
 
 
 
 
 
 
 
 
so, gnb requires n = o(log d) to converge, but lr requires n = o(d) 

some experiments 
from uci data sets 
[ng & jordan, 2002]  

na  ve bayes vs. id28 
the bottom line: 
 
gnb2 and lr both use linear decision surfaces, gnb need not 
 
given infinite data, lr is better than gnb2 because training 
procedure does not make assumptions 1 or 2 (though our 
derivation of the form of p(y|x) did). 
 
but gnb2 converges more quickly to its perhaps-less-accurate 
asymptotic error 
 
and gnb is both more biased (assumption1) and less (no 
assumption 2) than lr, so either might beat the other 

rate of covergence: id28 

[ng & jordan, 2002] 

let hdis,m be id28 trained on m examples in n 
dimensions.  then with high id203: 

implication: if we want 
 for some constant      , it suffices to pick order n examples  
 
      convergences to its asymptotic classifier, in order n examples 
(result follows from vapnik   s structural risk bound, plus fact that 
vcdim of n dimensional linear separators is n ) 

rate of covergence: na  ve bayes parameters 
[ng & jordan, 2002] 

what you should know: 
       id28 

       functional form follows from na  ve bayes assumptions 

       for gaussian na  ve bayes assuming variance   i,k =   i 
       for discrete-valued na  ve bayes too 
       but training procedure picks parameters without the 
       mcle training: pick w to maximize p(y | x, w) 
       map training: pick w to maximize p(w | x,y) 

conditional independence assumption 

       id173:   e.g., p(w)  ~ n(0,  ) 
       helps reduce overfitting  

       gradient ascent/descent 

       general approach when closed-form solutions for id113, map are 

unavailable 

       generative vs. discriminative classifiers 

       bias vs. variance tradeoff 

machine learning 10-701 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

february 4, 2015 

today: 
       id75 
       decomposition of error into 
bias, variance, unavoidable 

readings: 
       mitchell:    na  ve bayes and 

id28    

     (see class website) 
       ng and jordan paper (class 
       bishop, ch 9.1, 9.2 

website) 

regression 
so far, we   ve been interested in learning p(y|x) where y has 

discrete values (called    classification   ) 

 
what if y is continuous? (called    regression   ) 
       predict weight from gender, height, age,     

       predict google stock price today from google, yahoo, 

msft prices yesterday 

       predict each pixel intensity in robot   s current camera 

image, from previous image and previous action 

regression 
wish to learn f:x     y, where y is real, given {<x1,y1>   <xn,yn>} 
 
approach: 
 
1.    choose some parameterized form for p(y|x;   ) 

(    is the vector of parameters) 
 

2.    derive learning algorithm as mcle or map estimate for    

1. choose parameterized form for p(y|x;   ) 
 

y 

x 

where 

assume y is some deterministic f(x), plus random noise 
 
 
therefore y is a random variable that follows the distribution 
 
 
and the expected value of y for any given x is f(x) 
 

1. choose parameterized form for p(y|x;   ) 
 

y 

x 

where 

assume y is some deterministic f(x), plus random noise 
 
 
therefore y is a random variable that follows the distribution 
 
 
and the expected value of y for any given x is f(x) 
 

consider id75 
  
 
e.g., assume f(x) is linear function of x 
 
 
 
 
 
notation: to make our parameters explicit, let   s write 
 
 
 
 

training id75 
  
 
how can we learn w from the training data? 
 
 

training id75 
  
 
how can we learn w from the training data? 
 
learn maximum conditional likelihood estimate! 
 
 
 
 
where 
 
 

training id75 
 learn maximum conditional likelihood estimate 
 
 
where 
 
 
 
 
 
 
 

training id75 
 learn maximum conditional likelihood estimate 
 
 
where 
 
 
 
 
 
 
 

training id75 
 learn maximum conditional likelihood estimate 
 
 
where 
 
 
 
 
 
so: 
 
 

training id75 
 learn maximum conditional likelihood estimate 
 
 
can we derive id119 rule for training? 
 
 
 
 
 
 
 
 
 

how about map instead of id113 estimate? 

regression     what you should know 
under general assumption 
 
1.    id113 corresponds to minimizing sum of squared prediction errors 

2.    map estimate minimizes sse plus sum of squared weights 

3.    again, learning is an optimization problem once we choose our 

objective function 
       maximize data likelihood 
       maximize posterior prob of w 

4.    again, we can use id119 as a general learning algorithm 

      
      

as long as our objective fn is differentiable wrt w 
though we might learn local optima ins  

5.    almost nothing we said here required that f(x) be linear in x    
 
 

bias/variance decomposition of error 

bias and variance 

given some estimator y for some parameter   , we 

define 

 
the bias of estimator y =  
the variance of estimator y = 
 
e.g., define y as the id113 estimator for id203 of 

heads, based on n independent coin flips 

 
biased or unbiased? 
 
variance decreases as sqrt(1/n) 

bias     variance decomposition of error  

reading: bishop chapter 9.1, 9.2 

       consider simple regression problem f:x     y  

y = f(x) +   

noise n(0,  ) 

deterministic 

what are sources of prediction error? 

  

learned 

estimate of f(x)  

sources of error 
       what if we have perfect learner, infinite 

data? 
      our learned h(x) satisfies h(x)=f(x) 
      still have remaining, unavoidable error 
                                
                                    2 

sources of error 
       what if we have only n training examples? 
       what is our expected error 

      taken over random training sets of size n, 
drawn from distribution d=p(x,y)

 

sources of error 

