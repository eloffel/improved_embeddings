machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

february 2, 2015 

today: 
       id28 
       generative/discriminative 

classifiers 

readings: (see class website) 
 
required: 
       mitchell:    na  ve bayes and 

id28     

 
optional 
       ng & jordan 

announcements 

 

       hw3 due wednesday feb 4 
       hw4 will be handed out next monday feb 9 

       new reading available:    

       estimating probabilities: id113 and map (mitchell) 
       see lecture tab of class website 

       required reading for today: 

       na  ve bayes and id28 (mitchell) 

gaussian na  ve bayes     big picture 

example:  y= playbasketball (boolean), x1=height,  x2=mlgrade 

assume p(y=1) = 0.5 

id28 
idea: 
       na  ve bayes allows computing p(y|x) by 

learning p(y) and p(x|y) 
 

       why not learn p(y|x) directly? 

       consider learning f: x       y, where 

       x is a vector of real-valued features, < x1     xn > 
       y is boolean 
       assume all xi are conditionally independent given y 
       model p(xi | y = yk) as gaussian n(  ik,  i) 
       model p(y) as bernoulli (  ) 

       what does that imply about the form of p(y|x)? 

derive form for p(y|x) for gaussian p(xi|y=yk) assuming   ik =   i 

very convenient! 

implies 

implies 

implies 

very convenient! 

implies 

implies 

implies 

linear 

classification 

rule! 

logistic function 

id28 more generally
       id28 when y not boolean (but 

still discrete-valued).  

       now y     {y1 ... yr} : learn r-1 sets of weights 

 
 

 for k<r 

 for k=r 

training id28: mcle 

       we have l training examples: 

       maximum likelihood estimate for parameters w 

       maximum conditional likelihood estimate 

 

training id28: mcle 
       choose parameters w=<w0, ... wn> to 

maximize conditional likelihood of training data 
where 

       training data d =  
       data likelihood =  
       data conditional likelihood =  

expressing conditional log likelihood 

maximizing conditional log likelihood 

good news: l(w) is concave function of w
bad news: no closed-form solution to maximize l(w)

 id119:  
batch gradient: use error           over entire training set d 
do until satisfied: 
    1. compute the gradient  

    2. update the vector of parameters:  

 
stochastic gradient: use error          over single examples 
do until satisfied: 
    1. choose (with replacement) a random training example  
    2. compute the gradient just for    : 

    3. update the vector of parameters:  
 
stochastic approximates batch arbitrarily closely as 
stochastic can be much faster when d is very large 
intermediate approach: use error over subsets of d  

maximize conditional log likelihood:          

gradient ascent 

maximize conditional log likelihood:          

gradient ascent 

gradient ascent algorithm: iterate until change <   
   for all i, repeat 
 
    

that   s all for m(c)le.  how about map? 
       one common approach is to define priors on w 
       normal distribution, zero mean, identity covariance 
       helps avoid very large weights and overfitting 
       map estimate 

      

let   s assume gaussian prior: w ~ n(0,   ) 

id113 vs map  

       maximum conditional likelihood estimate 

       maximum a posteriori estimate with prior w~n(0,  i) 

map estimates and id173 

       maximum a posteriori estimate with prior w~n(0,  i) 

called a    id173    term 
       helps reduce overfitting 
       keep weights nearer to zero (if p(w) is zero mean    
       gaussian prior), or whatever the prior suggests 
       used very frequently in id28 

the bottom line 

       consider learning f: x       y, where 

       x is a vector of real-valued features, < x1     xn > 
       y is boolean 
       assume all xi are conditionally independent given y 
       model p(xi | y = yk) as gaussian n(  ik,  i) 
       model p(y) as bernoulli (  ) 

       then p(y|x) is of this form, and we can directly estimate w 

       furthermore, same holds if the xi are boolean 

       trying proving that to yourself 

generative vs. discriminative classifiers 

training classifiers involves estimating f: x       y, or p(y|x) 
 
generative classifiers (e.g., na  ve bayes) 
       assume some functional form for p(x|y), p(x) 
       estimate parameters of p(x|y), p(x) directly from training data 
       use bayes rule to calculate p(y|x= xi) 

discriminative classifiers (e.g., id28) 
 
       assume some functional form for p(y|x) 
       estimate parameters of p(y|x) directly from training data 

use na  ve bayes or logisitic regression? 

consider 
       restrictiveness of modeling assumptions 
 
       rate of convergence (in amount of 

training data) toward asymptotic 
hypothesis 

na  ve bayes vs id28 
consider y boolean, xi continuous, x=<x1 ... xn> 
 
number of parameters: 
       nb: 4n +1 
       lr: n+1 

estimation method: 
       nb parameter estimates are uncoupled 
       lr parameter estimates are coupled 
 

g.na  ve bayes vs. id28 

recall two assumptions deriving form of lr from gnbayes: 
1.     xi conditionally independent of xk given y 
2.     p(xi | y = yk)  =  n(  ik,  i),         not n(  ik,  ik) 

consider three learning methods: 
       gnb (assumption 1 only) 
       gnb2 (assumption 1 and 2) 
       lr  
 
which method works better if we have infinite training data, and    
       both (1) and (2) are satisfied 
       neither (1) nor (2) is satisfied 
       (1) is satisfied, but not (2)  

g.na  ve bayes vs. id28 

[ng & jordan, 2002] 

recall two assumptions deriving form of lr from gnbayes: 
1.     xi conditionally independent of xk given y 
2.     p(xi | y = yk)  =  n(  ik,  i),         not n(  ik,  ik) 

consider three learning methods: 
      gnb (assumption 1 only) 
      gnb2 (assumption 1 and 2) 
      lr  
 
which method works better if we have infinite training data, and... 
 
      both (1) and (2) are satisfied 

      neither (1) nor (2) is satisfied 

      (1) is satisfied, but not (2)  

g.na  ve bayes vs. id28 

[ng & jordan, 2002] 

recall two assumptions deriving form of lr from gnbayes: 
1.     xi conditionally independent of xk given y 
2.     p(xi | y = yk)  =  n(  ik,  i),         not n(  ik,  ik) 

consider three learning methods: 
      gnb (assumption 1 only)     -- decision surface can be non-linear 
      gnb2 (assumption 1 and 2)     decision surface linear 
      lr                                         -- decision surface linear, trained without  

 

 

            assumption 1. 

 
which method works better if we have infinite training data, and... 
 
      both (1) and (2) are satisfied: lr = gnb2 = gnb 

      (1) is satisfied, but not (2) :     gnb > gnb2, gnb > lr,  lr > gnb2  

      neither (1) nor (2) is satisfied:   gnb>gnb2,  lr > gnb2, lr><gnb 

g.na  ve bayes vs. id28 

[ng & jordan, 2002] 

what if we have only finite training data? 
 
they converge at different rates to their asymptotic (    data) error 
 
let          refer to expected error of learning algorithm a after n training 
examples 
 
let d be the number of features: <x1     xd> 
 
 
 
 
 
 
 
 
 
so, gnb requires n = o(log d) to converge, but lr requires n = o(d) 

some experiments 
from uci data sets 
[ng & jordan, 2002]  

na  ve bayes vs. id28 
the bottom line: 
 
gnb2 and lr both use linear decision surfaces, gnb need not 
 
given infinite data, lr is better or equal to gnb2 because 
training procedure does not make assumptions 1 or 2 (though our 
derivation of the form of p(y|x) did). 
 
but gnb2 converges more quickly to its perhaps-less-accurate 
asymptotic error 
 
and gnb is both more biased (assumption1) and less (no 
assumption 2) than lr, so either might outperform the other 

what you should know: 
       id28 

       functional form follows from na  ve bayes assumptions 

       for gaussian na  ve bayes assuming variance   i,k =   i 
       for discrete-valued na  ve bayes too 

conditional independence assumption 

       but training procedure picks parameters without making 
       id113 training: pick w to maximize p(y | x, w) 
       map training: pick w to maximize p(w | x,y) 

          id173     
       helps reduce overfitting  

       gradient ascent/descent 

       general approach when closed-form solutions unavailable 

       generative vs. discriminative classifiers 

       bias vs. variance tradeoff 

extra slides 

what is the minimum possible error? 

best case: 
       conditional independence assumption is satistied 
       we know p(y), p(x|y) perfectly (e.g., infinite training data) 

questions to think about: 
       can you use na  ve bayes for a combination of 

discrete and real-valued xi?  

       how can we easily model the assumption that 

just 2 of the n attributes as dependent? 

       what does the decision surface of a na  ve bayes 

classifier look like? 

       how would you select a subset of xi   s? 
 

