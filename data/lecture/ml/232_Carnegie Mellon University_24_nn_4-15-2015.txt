machine learning 10-601 

 

 

tom m. mitchell 

machine learning department 

carnegie mellon university 

april 15, 2015 

today: 
       id158s 
       id26  
       recurrent networks 
       convolutional networks 
       id50 
       deep boltzman machines  

reading: 
       mitchell: chapter 4 
       bishop: chapter 5 
       quoc le tutorial: 
       ruslan salakhutdinov tutorial: 

id158s to learn f: x       y 

f might be non-linear function 

      
       x (vector of) continuous and/or discrete vars 
       y (vector of) continuous and/or discrete vars 

       represent f by network of logistic units 
       each unit is a logistic function 

       id113: train weights of all units to minimize sum of squared 

errors of predicted network outputs 

       map: train to minimize sum of squared errors plus weight 

magnitudes 

alvinn 

[pomerleau 1993] 

m(c)le training for neural networks 

       consider regression problem f:x     y , for scalar y 
assume noise n(0,    ), iid 

y = f(x) +   

deterministic 

       let   s maximize the conditional data likelihood 

  

learned 

neural network 

map training for neural networks 

       consider regression problem f:x     y , for scalar y 

y = f(x) +   

noise n(0,    ) 

deterministic 

 gaussian p(w) = n(0,    ) 

  

ln p(w)      c    i wi

2 

xd = input 
td = target output 
od = observed unit 
output 
wi = weight i 

(id113) 

xd = input 
td = target output 
od = observed unit 
output 
wij = wt from i to j 

left 

strt 

right 

up 

w0 

semantic memory model based on ann   s 

[mcclelland & rogers, nature 2003] 

no hierarchy given.  

train with assertions, 
e.g., can(canary,fly) 

training networks on time series 

       suppose we want to predict next state of world 

       and it depends on history of unknown length 
       e.g., robot with forward-facing sensors trying to predict next 

sensor reading as it moves and turns 

 

recurrent networks: time series 

       suppose we want to predict next state of world 

       and it depends on history of unknown length 
       e.g., robot with forward-facing sensors trying to predict next 

sensor reading as it moves and turns 

      

idea: use hidden layer in network to capture state history 

recurrent networks on time series 

how can we train recurrent net?? 

convolutional neural nets for image recognition 
[le cun, 1992] 

       specialized architecture: mix different types of units, not 
completely connected, motivated by primate visual cortex 

       many shared parameters, stochastic gradient training 
       very successful!  now many specialized architectures for 

vision, speech, translation,     

id50 
       problem: training networks with many hidden layers 

[hinton & salakhutdinov, 2006] 

doesn   t work very well 
       local minima, very slow training if initialize with zero weights 

       id50 

       autoencoder networks to learn low dimensional encodings  

       but more layers, to learn better encodings 

id50 

[hinton & salakhutdinov, 2006] 

original image 

reconstructed from 
2000-1000-500-30 dbn 
 reconstructed from  
2000-300, linear pca 

versus 

id50: training [hinton & salakhutdinov, 2006] 

encoding of digit images in two dimensions 

[hinton & salakhutdinov, 2006] 

784-2 linear encoding (pca) 

784-1000-500-250-2 dbnet 

very large scale use of dbn   s 
data: 10 million 200x200 unlabeled images, sampled from youtube 
training: use 1000 machines (16000 cores) for 1 week 
learned network: 3 multi-stage layers, 1.15 billion parameters 
achieves 15.8% (was 9.5%) accuracy classifying 1 of 20k id163 items 

[quoc le, et al., icml, 2012] 

real 
images 
that most 
excite the 
feature: 

image 
synthesized 
to most 
excite the 
feature: 

restricted boltzman machine 
       bipartite graph, logistic activation 
       id136: fill in any nodes, estimate other 

nodes 

       consider vi, hj are boolean variables 
 

h2 

h3 

h1 

v1  v2 

    

vn 

impact	
   of	
   deep	
   learning	
   

      	
   speech	
   recogni4on	
   
      	
   computer	
   vision	
   
      	
   recommender	
   systems	
   	
   
      	
   language	
   understanding	
   	
   
      	
   drug	
   discovery	
   and	
   medical	
   
image	
   analysis	
   	
   

[courtesy	
   of	
   r.	
   salakhutdinov]	
   

feature	
   representa4ons:	
   tradi4onally	
   

data 

feature 
extraction 

learning 
algorithm 

object	
   
detec4on	
   

audio	
   
classi   ca4on	
   

image	
   

vision	
   features	
   

recogni4on	
   

audio	
   

audio	
   features	
   

speaker	
   

iden4   ca4on	
   

[courtesy	
   of	
   r.	
   salakhutdinov]	
   

computer	
   vision	
   features	
   

sift	
   

textons	
   

hog	
   

gist	
   

rift	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

audio	
   features	
   

spectrogram	
   

mfcc	
   

flux	
   

zcr	
   

rollo   	
   
[courtesy,	
   r.	
   salakhutdinov]	
   

audio	
   features	
   

spectrogram	
   

representa4on	
   learning:	
   
mfcc	
   
can	
   we	
   automa4cally	
   learn	
   
these	
   representa4ons?	
   

flux	
   

zcr	
   

rollo   	
   
[courtesy,	
   r.	
   salakhutdinov]	
   

restricted	
   boltzmann	
   machines	
   

graphical	
   models:	
   powerful	
   
framework	
   for	
   represen4ng	
   
dependency	
   structure	
   between	
   
random	
   variables.	
   

	
   	
   hidden	
   variables	
   

pair-     wise	
   

unary	
   
feature	
   detectors	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

rbm	
   is	
   a	
   markov	
   random	
   field	
   with:	
   
      	
   stochas4c	
   binary	
   visible	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   stochas4c	
   binary	
   hidden	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   bipar4te	
   connec4ons.	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

markov	
   random	
      elds,	
   boltzmann	
   machines,	
   log-     linear	
   models.	
   	
   

learning	
   features	
   

observed	
   	
   data	
   	
   

subset	
   of	
   25,000	
   characters	
   

learned	
   w:	
   	
      edges   	
   
subset	
   of	
   1000	
   features	
   

new	
   image:	
   

=	
   

logis4c	
   func4on:	
   suitable	
   for	
   
modeling	
   binary	
   images	
   

sparse	
   
representa8ons	
   

   .	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

model	
   learning	
   

	
   	
   hidden	
   units	
   

given	
   a	
   set	
   of	
   i.i.d.	
   training	
   examples	
   	
   

	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   we	
   want	
   to	
   learn	
   	
   

model	
   parameters

	
   

	
   

	
   	
   	
   	
   	
   	
   .	
   	
   	
   	
   

maximize	
   log-     likelihood	
   objec4ve:	
   

image	
   	
   	
   	
   	
   	
   visible	
   units	
   

deriva4ve	
   of	
   the	
   log-     likelihood:	
   

di   cult	
   to	
   compute:	
   exponen4ally	
   many	
   	
   
con   gura4ons	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

rbms	
   for	
   real-     valued	
   data	
   

	
   	
   hidden	
   variables	
   

pair-     wise	
   

unary	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

gaussian-     bernoulli	
   rbm:	
   
      	
   stochas4c	
   real-     valued	
   visible	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   stochas4c	
   binary	
   hidden	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   bipar4te	
   connec4ons.	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(salakhutdinov & hinton, nips 2007; salakhutdinov & murray, icml 2008)

rbms	
   for	
   real-     valued	
   data	
   

	
   	
   hidden	
   variables	
   

pair-     wise	
   

unary	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

4	
   million	
   unlabelled	
   images	
   

learned	
   features	
   (out	
   of	
   10,000)	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

rbms	
   for	
   real-     valued	
   data	
   

	
   	
   hidden	
   variables	
   

pair-     wise	
   

unary	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

4	
   million	
   unlabelled	
   images	
   

learned	
   features	
   (out	
   of	
   10,000)	
   

=  0.9 *            +  0.8 *            + 0.6 *                

new	
   image	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

rbms	
   for	
   word	
   counts	
   

pair-     wise	
   

unary	
   

hjaj1a

exp0@
dxi=1

w k

ijvk

0	
   
0	
   
0	
   
1	
   
0	
   

i hj +

p   (vk

p   (v, h) =

fxj=1

kxk=1

1
z(   )

dxi=1
kxk=1
exp   bk
i +pf
q=1 exp   bq
i +pf
pk
replicated	
   soemax	
   model:	
   undirected	
   topic	
   model:	
   
      	
   stochas4c	
   1-     of-     k	
   visible	
   variables.	
   
      	
   stochas4c	
   binary	
   hidden	
   variables	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
      	
   bipar4te	
   connec4ons.	
   

i = 1|h) =

vk
i bk

i +

fxj=1
ij   

ij   

j=1 hjw k

j=1 hjw q

[courtesy,	
   r.	
   salakhutdinov]	
   

(salakhutdinov & hinton, nips 2010, srivastava & salakhutdinov, nips 2012)

rbms	
   for	
   word	
   counts	
   

pair-     wise	
   

unary	
   

0	
   
0	
   
0	
   
1	
   
0	
   

p   (v, h) =

1
z(   )

exp0@
dxi=1

kxk=1

p   (vk

i = 1|h) =

hjaj1a

w k

ijvk

i hj +

fxj=1

dxi=1
kxk=1
exp   bk
i +pf
q=1 exp   bq
i +pf
pk

vk
i bk

i +

fxj=1
ij   

ij   

j=1 hjw k

j=1 hjw q

reuters	
   dataset:	
   
804,414	
   unlabeled	
   
newswire	
   stories	
   
bag-     of-     words	
   	
   

learned	
   features:	
   ``topics      	
   

russian	
   
russia	
   
moscow	
   
yeltsin	
   
soviet	
   

clinton	
   
house	
   
president	
   
bill	
   
congress	
   

computer	
   
system	
   
product	
   
soeware	
   
develop	
   

trade	
   
country	
   
import	
   
world	
   
economy	
   

stock	
   
wall	
   
street	
   
point	
   
dow	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

di   erent	
   data	
   modali4es	
   

      	
   binary/gaussian/soemax	
   rbms:	
   all	
   have	
   binary	
   hidden	
   
variables	
   but	
   use	
   them	
   to	
   model	
   di   erent	
   kinds	
   of	
   data.	
   

binary	
   

real-     valued	
   

1-     of-     k	
   

0	
   
0	
   
0	
   
1	
   
0	
   

      	
   it	
   is	
   easy	
   to	
   infer	
   the	
   states	
   of	
   the	
   hidden	
   variables:	
   	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

product	
   of	
   experts	
   

the	
   joint	
   distribu4on	
   is	
   given	
   by:	
   

marginalizing	
   over	
   hidden	
   variables:	
   

product	
   of	
   experts	
   

government	
   
auhority	
   
power	
   
empire	
   
pu4n	
   

clinton	
   
house	
   
president	
   
bill	
   
congress	
   

bribery	
   
corrup4on	
   
dishonesty	
   
pu4n	
   
fraud	
   

oil	
   
barrel	
   
exxon	
   
pu4n	
   
drill	
   

stock	
   
wall	
   
street	
   
point	
   
dow	
   

   	
   
	
   
	
   
	
   
	
   

topics	
      government   ,	
      corrup4on   	
   
and	
      oil   	
   can	
   combine	
   to	
   give	
   very	
   high	
   
id203	
   to	
   a	
   word	
      pu4n   .	
   

pu4n	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012)

deep	
   boltzmann	
   machines	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

input:	
   pixels	
   

image	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(salakhutdinov & hinton, neural computation 2012)

deep	
   boltzmann	
   machines	
   

learn	
   simpler	
   representa4ons,	
   
then	
   compose	
   more	
   complex	
   ones	
   

higher-     level	
   features:	
   
combina4on	
   of	
   edges	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

input:	
   pixels	
   

image	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(salakhutdinov 2008, salakhutdinov & hinton 2012)

h3

h2

h1

v

model	
   formula4on	
   

same	
   as	
   rbms	
   

w3

model	
   parameters	
   
       dependencies	
   between	
   hidden	
   variables.	
   
requires	
   approximate	
   id136	
   to	
   
       all	
   connec4ons	
   are	
   undirected.	
   
train,	
   but	
   it	
   can	
   be	
   done   	
   
       bolom-     up	
   and	
   top-     down:	
   
and	
   scales	
   to	
   millions	
   of	
   examples	
   

w1

w2

input	
   

top-     down	
   

bolom-     up	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

samples	
   generated	
   by	
   the	
   model	
   

training	
   data	
   

model-     generated	
   samples	
   

data	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

handwri4ng	
   recogni4on	
   

mnist	
   dataset	
   

60,000	
   examples	
   of	
   10	
   digits	
   
learning	
   algorithm	
   
logis4c	
   regression	
   
k-     nn	
   	
   
neural	
   net	
   (plal	
   2005)	
   
id166	
   (decoste	
   et.al.	
   2002)	
   
deep	
   autoencoder	
   
(bengio	
   et.	
   al.	
   2007)	
   	
   
deep	
   belief	
   net	
   
(hinton	
   et.	
   al.	
   2006)	
   	
   
dbm	
   	
   

error	
   
12.0%	
   
3.09%	
   
1.53%	
   
1.40%	
   
1.40%	
   

0.95%	
   

1.20%	
   

op4cal	
   character	
   recogni4on	
   

42,152	
   examples	
   of	
   26	
   english	
   lelers	
   	
   

learning	
   algorithm	
   
logis4c	
   regression	
   
k-     nn	
   	
   
neural	
   net	
   
id166	
   (larochelle	
   et.al.	
   2009)	
   
deep	
   autoencoder	
   
(bengio	
   et.	
   al.	
   2007)	
   	
   
deep	
   belief	
   net	
   
(larochelle	
   et.	
   al.	
   2009)	
   	
   
dbm	
   

error	
   
22.14%	
   
18.92%	
   
14.62%	
   
9.70%	
   
10.05%	
   

9.68%	
   

8.40%	
   

permuta4on-     invariant	
   version.	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

3-     d	
   object	
   recogni4on	
   

norb	
   dataset:	
   24,000	
   examples	
   

learning	
   algorithm	
   
logis4c	
   regression	
   
k-     nn	
   (lecun	
   2004)	
   
id166	
   (bengio	
   &	
   lecun	
   	
   2007)	
   
deep	
   belief	
   net	
   (nair	
   &	
   hinton	
   	
   
2009)	
   	
   
dbm	
   

error	
   
22.5%	
   
18.92%	
   
11.6%	
   
9.0%	
   

7.2%	
   

palern	
   
comple4on	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

learning	
   shared	
   representa4ons	
   

across	
   sensory	
   modali4es	
   

   concept   	
   

sunset,	
   paci   c	
   ocean,	
   
baker	
   beach,	
   seashore,	
   

ocean	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

a	
   simple	
   mul4modal	
   model	
   

      	
   use	
   a	
   joint	
   binary	
   hidden	
   layer.	
   
      	
   problem:	
   	
   inputs	
   have	
   very	
   di   erent	
   sta4s4cal	
   
proper4es.	
   
      	
   di   cult	
   to	
   learn	
   cross-     modal	
   features.	
   

real-     valued	
   

0	
   
0	
   
0	
   
1	
   
0	
   

1-     of-     k	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

mul4modal	
   dbm	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   soemax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   soemax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   soemax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

bolom-     up	
   

+	
   

top-     down	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   soemax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

mul4modal	
   dbm	
   

bolom-     up	
   

+	
   

top-     down	
   

gaussian	
   model	
   

dense,	
   real-     valued	
   
image	
   features	
   

replicated	
   soemax	
   

word	
   
counts	
   

0	
   
0	
   
0	
   
1	
   
0	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	
   

text	
   generated	
   from	
   images	
   

given
	
   
	
   	
   

	
   	
   

generated 	
   
dog,	
   cat,	
   pet,	
   kilen,	
   
puppy,	
   ginger,	
   tongue,	
   
kily,	
   dogs,	
   furry	
   

given
	
   
	
   	
   

generated 	
   

	
   	
   
insect,	
   buler   y,	
   insects,	
   
bug,	
   buler   ies,	
   
lepidoptera	
   

sea,	
   france,	
   boat,	
   mer,	
   
beach,	
   river,	
   bretagne,	
   
plage,	
   brilany	
   

portrait,	
   child,	
   kid,	
   
ritralo,	
   kids,	
   children,	
   
boy,	
   cute,	
   boys,	
   italy	
   

gra   4,	
   streetart,	
   stencil,	
   
s4cker,	
   urbanart,	
   gra   ,	
   
sanfrancisco	
   

canada,	
   nature,	
   
sunrise,	
   ontario,	
   fog,	
   
mist,	
   bc,	
   morning	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

text	
   generated	
   from	
   images	
   

given
	
   
	
   	
   

generated 	
   

	
   	
   

portrait,	
   women,	
   army,	
   soldier,	
   
mother,	
   postcard,	
   soldiers	
   

obama,	
   barackobama,	
   elec4on,	
   
poli4cs,	
   president,	
   hope,	
   change,	
   
sanfrancisco,	
   conven4on,	
   rally	
   

water,	
   glass,	
   beer,	
   bolle,	
   
drink,	
   wine,	
   bubbles,	
   splash,	
   
drops,	
   drop	
   

images	
   generated	
   from	
   text	
   

retrieved	
   

given
	
   
	
   	
   

water,	
   red,	
   
sunset	
   

nature,	
      ower,	
   
red,	
   green	
   

blue,	
   green,	
   
yellow,	
   colors	
   

chocolate,	
   cake	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

mir-     flickr	
   dataset	
   
      	
   1	
   million	
   images	
   along	
   with	
   user-     assigned	
   tags.	
   

sculpture,	
   beauty,	
   
stone	
   

d80	
   

nikon,	
   abigfave,	
   
goldstaraward,	
   d80,	
   
nikond80	
   

food,	
   cupcake,	
   
vegan	
   

anawesomeshot,	
   
theperfectphotographer,	
   
   ash,	
   damniwishidtakenthat,	
   
spiritofphotography	
   

nikon,	
   green,	
   light,	
   
photoshop,	
   apple,	
   d70	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

white,	
   yellow,	
   
abstract,	
   lines,	
   bus,	
   
graphic	
   

sky,	
   geotagged,	
   
re   ec4on,	
   cielo,	
   
bilbao,	
   re   ejo	
   

huiskes	
   et.	
   al.	
   

results	
   

      	
   logis4c	
   regression	
   on	
   top-     level	
   representa4on.	
   
      	
   mul4modal	
   inputs	
   

mean	
   average	
   precision	
   

learning	
   algorithm	
   
random	
   
lda	
   [huiskes	
   et.	
   al.]	
   
id166	
   [huiskes	
   et.	
   al.]	
   
dbm-     labelled	
   
deep	
   belief	
   net	
   
autoencoder	
   
dbm	
   

map	
   
0.124	
   
0.492	
   
0.475	
   
0.526	
   
0.638	
   
0.638	
   
0.641	
   

precision@50	
   

0.124	
   
0.754	
   
0.758	
   
0.791	
   
0.867	
   
0.875	
   
0.873	
   

labeled	
   
25k	
   
examples	
   

+	
   1	
   million	
   
unlabelled	
   

[courtesy,	
   r.	
   salakhutdinov]	
   

id158s: summary 

       highly non-id75/classification 
       hidden layers learn intermediate representations 
       potentially millions of parameters to estimate 
       stochastic id119, local minima problems 

       deep networks have produced real progress in many fields 

       id161 
       id103 
       mapping images to text 
       recommender systems 
           

       they learn very useful non-linear representations 

