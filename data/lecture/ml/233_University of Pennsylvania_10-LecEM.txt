cis 519/419 

applied machine learning

www.seas.upenn.edu/~cis519

dan roth
danroth@seas.upenn.edu
http://www.cis.upenn.edu/~danroth/
461c, 3401 walnut

slides were created by dan roth (for cis519/419 at penn or cs446 at uiuc), eric eaton 
for cis519/419 at penn, or from other authors who have made their ml slides available. 

cis419/519 spring    18

administration

    exam:

    the exam will take place on the originally assigned date, 4/30. 

    similar to the previous midterm.
    75 minutes; closed books.

    what is covered:

    the focus is on the material covered after the previous mid-term.
    however, notice that the ideas in this class are cumulative!!
    everything that we present in class and in the homework assignments
    material that is in the slides but is not discussed in class is not part of the 

material required for the exam.

    example 1: we talked about boosting. but not about boosting the confidence.
    example 2: we talked about multiclass classification: ova, ava, but not error 

correcting codes,  and additional material in the slides.

    we will give a few practice exams.

cis419/519 spring    18

2

administration

    projects

    we will have a poster session 6-8pm on may 7

   

in the active learning room, 3401 walnut.

(72% of the cis 519 students that voted in our poll preferred that).
   
    the hope is that this will be a fun event where all of you have an 

opportunity to see and discuss the projects people have done. 

    all are invited!
    mandatory for cis519 students
    the final project report will be due on 5/8
   

logistics: you will send us you posters the a day earlier; we will print it 
and hang it; you will present it.
    if you haven   t done so already:

    come to my office hours at least once this or next week to discuss the 

project!!

go to bayes

cis419/519 spring    18

3

summary: basic id203

    product rule:   p(a,b) = p(a|b)p(b) = p(b|a)p(a)
    if a and b are independent:   

    p(a,b) = p(a)p(b);   p(a|b)= p(a), p(a|b,c)=p(a|c)

    sum rule: p(a   b) = p(a)+p(b)-p(a,b)
    bayes rule: p(a|b)  = p(b|a) p(a)/p(b)
    total id203: 

   
    p(b) =     p(b , ai) =    i p(b|ai) p(ai)
    total id155: 

if events a1, a2,   an are mutually exclusive: ai    aj = ,    i p(ai)= 1
if events a1, a2,   an are mutually exclusive: ai    aj = ,     i p(ai)= 1

   
    p(b|c) =     p(b , ai|c) =    i p(b|ai,c) p(ai|c)                   

cis419/519 spring    18

4

expectation of a random variable

    let x be a random variable with arity k that takes the values  

{x1,x2, ..., xk } with probabilities {p1,p2, ..., pk }, respectively,

    the, the expectation of the random variable x is:

with            =1                         =1
    e[x] =             =1                                         

    important property: 

    linearity:          e[x+y] = e[x] + e[y]

cis419/519 spring    18
based on slide by andrew moore

5

semi-supervised learning

    consider the problem of prepositional phrase attachment. 

   

buy car with money             ; buy car with wheel 

    there are several ways to generate features. given the 
limited representation, we can assume that all possible 
conjunctions of the 4 attributes are used. (15 feature in 
each example). 

    assume we will use na  ve bayes for learning to decide 

between [n,v]

    examples are:  (x1,x2,   xn,[n,v])

cis419/519 spring    18

6

using na  ve bayes

    to use na  ve bayes, we need to use the data to estimate:        

p(n)                      p(v)
p(x1|n)                p(x1|v)
p(x2|n)                p(x2|v)

      

p(xn|n)                p(xn|v)

    then, given an example (x1,x2,   xn,?), compare:
p(n|x)~=p(n) p(x1|n) p(x2|n)    p(xn|n)

and

p(v|x)~=p(v) p(x1|v) p(x2|v)    p(xn|v)

cis419/519 spring    18

7

using na  ve bayes

    after seeing 10 examples, we have:   
    p(n) =0.5; p(v)=0.5

p(x1|n)=0.75;p(x2|n) =0.5; p(x3|n) =0.5; p(x4|n) =0.5 
p(x1|v)=0.25; p(x2|v) =0.25;p(x3|v) =0.75;p(x4|v) =0.5

    then, given an example x=(1000), we have:

pn(x)~=0.5 0.75 0.5 0.5 0.5 = 3/64
pv(x)~=0.5 0.25 0.75 0.25 0.5=3/256 

    now, assume that in addition to the 10 labeled examples, 

we also have 100 unlabeled examples.

    will that help? 

cis419/519 spring    18

8

using na  ve bayes

    for example, what can be done with the example (1000)  ?

    we have an estimate for its label   
    but, can we use it to improve the classifier (that is, the estimation 

of the probabilities that we will use in the future)?

    option 1: we can make predictions, and believe them

    or some of them (based on what?)

    option 2: we can assume the example x=(1000) is a 
    an n-labeled  example with id203  pn(x)/(pn(x) + pv(x))
    a v-labeled example with id203  pv(x)/(pn(x) + pv(x))

    estimation of probabilities does not require working with 

integers!

cis419/519 spring    18

9

using unlabeled data

the discussion suggests several algorithms:

1. use a threshold. chose examples labeled with high 

confidence. label them [n,v]. retrain.

2. use fractional examples. label the examples with 

fractional labels [p of n, (1-p) of v]. retrain.

cis419/519 spring    18

10

comments on unlabeled data

both algorithms suggested can be used iteratively.

both algorithms can be used with other classifiers, not  only na  ve 
bayes. the only requirement     a robust confidence measure in the 
classification.

there are other approaches to semi-supervised learning: 

   

   

most are conceptually similar: id64 algorithms
some are    graph-based    algorithms: assume    similar    examples have    similar 
labels   .

   

   

   

    what happens if instead of 10 labeled examples we start with 0

labeled examples?

    make a guess; continue as above; a version of em 

cis419/519 spring    18

11

em

   

   

   

   

em is a class of algorithms that is used to estimate a 
id203 distribution in the presence of missing 
attributes. 
using it requires an assumption on the underlying 
id203 distribution.
the algorithm can be very sensitive to this assumption 
and to the starting point (that is, the initial guess of 
parameters. 
in general, known to converge to a local maximum of the 
maximum likelihood function. 

cis419/519 spring    18

12

three coin example

    we observe a series of coin tosses generated in the 

   

following way: 
a person has three coins.
   

coin 0: id203 of head is   
coin 1: id203 of head p 
coin 2: id203 of head q

   

   

   

consider the following coin-tossing scenarios:

cis419/519 spring    18

13

estimation problems

scenario i: toss one of the coins four times.

observing  hhth
question: which coin is more likely to produce this sequence ? 

scenario ii: toss coin 0. if head     toss coin 1; otherwise     toss coin 2

observing the sequence  hhhht,  ththt, hhhht, hhtth
produced by coin 0 , coin1 and coin2
question: estimate most likely values for p, q (the id203 of h in 

each coin) and the id203 to use each of the coins (  )

scenario iii: toss coin 0. if head     toss coin 1; o/w     toss coin 2

observing the sequence  hhht,  htht, hhht, htth
produced by coin 1 and/or coin 2 
question: estimate most likely values for p, q and   

coin 0

   

   

   

there is no known analytical solution to this problem (general 
setting). that is, it is not known how to compute the values of 
the parameters so as to maximize the likelihood of the data.
cis419/519 spring    18

1st toss

2nd toss

nth  toss
14

key intuition (1)

if we knew which of the data points (hhht), (htht), (htth)  came 
from coin1 and which from coin2, there was no problem.

recall that the    simple    estimation is the ml estimation:
assume that you toss a (p,1-p) coin m times and get k heads m-k 
tails.

log[p(d|p)] = log [ pk (1-p)m-k ]= k log p + (m-k) log (1-p) 

to maximize, set the derivative w.r.t. p equal to 0:

d log p(d|p)/dp = k/p     (m-k)/(1-p) = 0

solving this for p, gives:      p=k/m

   

   
   

   

   

cis419/519 spring    18

15

key intuition (2)

   

   

   

if we knew which of the data points (hhht), (htht), (htth)  came 
from coin1 and which from coin2, there was no problem.
instead, use an iterative approach for estimating the parameters:
guess the id203 that a given data point came from coin 1 or 2;   
generate fictional labels, weighted according to this id203.
now, compute the most likely value of the parameters. [recall nb 
example]
compute the likelihood of the data given this model.
re-estimate the initial parameter setting: set them to maximize  the 
likelihood of the data.
(labels        model parameters)       likelihood of the data

   

   

   

   

this process can be iterated and can be shown to converge to a local 
maximum of the likelihood function

cis419/519 spring    18

16

em algorithm (coins) -i

    we will assume (for a minute) that we know the parameters             

and use it to estimate which coin it is (problem 1)
then, we will use this    label    estimation of the observed tosses, to 
estimate the most likely parameters 

   

  ~~~ ,q,p

   

and so on...

    notation: n data points; in each one: m tosses, hi heads. 
    what is the id203 that the ith data point came from coin1 ?
   

step 1 (expectation step):                                                         (here h=hi )

i
p
1

=

p(coin1

i
)d|

=

i
p(d

|

p(coin1)
 
coin1)
i
)p(d

=

          

        
=

h

~ ~
(1p  
   
  
~
hm
)p
(1
   
   +

~
hm
)p
   
~~
(1q) 
  

h

~
)q

   

hm
   

~~
(1p 
  

h

   

cis419/519 spring    18

17

em algorithm (coins) - ii

ll=    i=1,n log    y=0,1 p(di, y | p,q,   ) =

=    i=1,n log    y=0,1 p(di|p,q,    )p(y|di,p,q,  ) = 
=    i=1,n log e_y p(di |p,q,   )   
      i=1,n e_y log p(di |p,q,   )

now, we would like to compute the likelihood of the data, and find 
the parameters that maximize it.

where the inequality is due to jensen   s inequality.
we maximize a lower bound on the likelihood. 

   

    we will maximize the log likelihood of the data (n data points)  

   

   

   

   
   

ll =    1,n logp(di |p,q,  )

but, one of the variables     the coin   s name - is hidden. we can 
marginalize:

ll=     i=1,n log    y=0,1 p(di, y | p,q,   ) 

however, the sum is inside the log, making ml solution difficult. 
since the latent variable y is not observed, we cannot use the 
complete-data log likelihood. instead, we use the expectation of the 
complete-data log likelihood under the posterior distribution of the 
latent variable to approximate log p(di| p   ,q   ,     )

    we think of the likelihood logp(di|p   ,q   ,     ) as a random variable that 
depends on the value y of the coin in the ith toss. therefore, instead 
of maximizing the ll we will maximize the expectation of this 
random variable (over the coin   s name).  [justified using jensen   s 
inequality; later & above] 

cis419/519 spring    18

18

em algorithm (coins) - iii

    we maximize the expectation of this random variable (over 

the coin name).

e[ll] = e[   i=1,n log p(di| p,q,   )] =    i=1,ne[log p(di| p,q,   )] = (some math; see above)     

=     i=1,n p1i log p(di, 1 | p,q,   )] + (1-p1i) log p(di, 0 | p,q,   )]  

- p1i log p1i - (1-p1i) log (1- p1i )     

(does not matter when we maximize)

   

this is due to the linearity of the expectation and the 
random variable definition:

cis419/519 spring    18

19

em algorithm (coins) - iv

   

   

explicitly, we get:

                                                   (                |           ,              ,      
                                                                      (        ,                |           ,              ,   ) +           (                                   )                                (        ,                |           ,              ,   ) =   
=                                                                                         (                   )                           +                                                           (                )                        (                   )                           = 
=                                    (                             +                                                 + (                           ) log(                   ) ) 
+           (                                   )(                        (                )+                                                 +(                           ) log(                   ) )

cis419/519 spring    18

20

em algorithm (coins) - v

when computing the derivatives, 
notice p1i here is a constant; it was 

computed using the current 

parameters in the e step

   

   
   

=

=

de
~
d
  
de
~
pd
de
~
qd

n

   

1i
=
n

   

1i
=

=

n

   

1i
=

  ~~~ ,q,p

finally, to find the most likely parameters, we maximize the 
derivatives with respect to             : 
step 2: maximization step
(sanity check: think of the weighted fictional points)
i
i
0p1-p
   
=
1
1
~1~
    
   
h(p
hm-
   
i
~
~
p1
p
   
hm-
   
~
q1
   

  
hp
   
i
i
1
m
  
   
i
p
1
h)p(1
   
   
i
m
   
i
)p(1-
1

   
i
p
1
n
~
p

h)(p(1-
i
~
q

~
q
        

        

        

       

       

0)
=

0)
=

       

~
  

   

   

   

  

=

=

=

i
1

i
1

i
1

i

i

cis419/519 spring    18

21

the general em procedure 

cis419/519 spring    18

e

m

26

em summary (so far)

   

em is a general procedure for learning in the presence of  
unobserved variables. 

    we have shown how to use it in order to estimate the most likely 

density function for a mixture of (bernoulli) distributions. 
em is an iterative algorithm that can be shown to converge to a local 
maximum of the likelihood function.

it depends on assuming a family of id203 distributions.
in this sense, it is a family of algorithms. the update rules you will 
derive depend on the model assumed.
it has been shown to be quite useful in practice, when the 
assumptions made on the id203 distribution are correct,  but 
can fail otherwise.

   

   
   

   

cis419/519 spring    18

27

em summary (so far)

em is a general procedure for learning in the presence of  
unobserved variables. 

the (family of ) id203 distribution is known; the problem is to 
estimate its parameters  

in the presence of hidden variables, we can often think about it as a 
problem of a mixture of distributions     the participating 
distributions are known, we need to estimate:  

   

   

   

   

   

parameters of the distributions 
the mixture policy

   

our previous example: mixture of bernoulli distributions

cis419/519 spring    18

28

example: id116 algorithm

k- means is a id91 algorithm.
we are given data points, known to be sampled independently 
from  a mixture of k normal distributions, with 
means    i, i=1,   k and the same standard variation     

p(x)

2  

1  

x

cis419/519 spring    18

29

example: id116 algorithm

first, notice that if we knew that all the data points are taken 
from a normal distribution with mean     , finding its most likely 
value is easy.

|p(x

)
  

 
=

exp[

(x

   

  

2
])

1
2
2
    

we get many data points, d = {x1,   ,xm}

|

ln(l(d

   
maximizing the log-likelihood is equivalent to minimizing: 

ln(p(d

))
  

))
  

)
  

(x

=

=

   

|

i

2

  
ml

=

argmin

  

       

(x
i

i

2

)
  

   

calculate the derivative with respect to   ,  we get that the 
1  
   =
minimal point, that is, the most likely mean is
m

ix
i

cis419/519 spring    18

30

   

1
2
2
  
1-
2   
2
  

i

a mixture of distributions

as in the coin example, the problem is that data is sampled from a 
mixture of k different normal distributions, and we do not know, 
for a given data point xi, where is it  sampled from. 

assume that we observe data point xi ;what is the id203 that it 
was sampled from the distribution   j

?
1
p(xk
=
1
    =
k
p(xk
1n

|x
i
=

)
 
    
=
j
|x
=
    
i
n

p
ij

=

p(

  
j

)x|
i

=

p(x

i

)

j

)p(
 
|
    
j
)p(x
i

=

=

exp[

   

1
2
2
  
exp[
   

(x
1
2
2
  

    =

k
1n

   

  
j

2
])

i

(x

i

   

  
n

2
])

cis419/519 spring    18

=

)

31

a mixture of distributions

as in the coin example, the problem is that data is sampled from a 
mixture of k different normal distributions, and we do not know, 
for a given each data point xi, where is it  sampled from. 

for a data point xi, define k binary hidden variables, zi1,zi2,   ,zik, s.t 
zij =1 iff xi is sampled from the j-th distribution. 

e[z
ij
          

1]
   =
0
   
   

p(x
p(x

i

i

cis419/519 spring    18

sampled
 
 was
 was
not 
 

from 
)
 
  
+
j
 
from 
sampled
  
j
 e[y]
=

)
p 
=
ij
   
p(yy
i
iy
 y]
=

e[x]

=

)y
i

+

e[y]
32

e[x

+

example: id116 algorithms

expectation: (here: h =                              ) 

p(y

i

|

z,p(x h)

=

i

,...,

i1

|z
ik

2

        
k
exp[

,
h)

,
1
=

,...,
1
2
2
    

   

1
2
2
  

   

j

(xz
ij

i

   

  
j

2
])

computing the likelihood given the observed data  d = {x1,   ,xm} 
and the hypothesis h  (w/o the constant coefficient)
   

h))

   

)

2

  
j

i

|

=

ln(p(y

   
   
e[ln(p(y |h))] e
[

m
1i
=

=

j

1-
2    
(xz
ij
2
  
1
   
m
-
2
2
  =
i 1
1-
2    
2
  

e[z

ij

j

j

z (x
ij

   

i

2
]  

)

    
=

j

](x

   

  
j

2
  )

i

=

   

m
1i
=

cis419/519 spring    18

33

example: id116 algorithms

maximization: maximizing
   

m
1i
=
with respect to      we get that:

)h'

=

1-
2    
2
  

e[z

](x

i

ij

   

2

  
j

)

  

j

=     =
c

m
1i

e[z

ij

](x

i

   

  
j

  0)
=

|q(h
j  
dq
d
  
j

which yields:

   
=  
j    

m
e[z
ij
1i
=
m
e[z
1i
=

]x
]

ij

i

 

cis419/519 spring    18

34

summary: id116 algorithms
given a set d = {x1,   ,xm} of data points,
,...,
guess initial parameters
compute (for all i,j)
   
ij =

,
1
2
exp[

 ]e[z
ij

2
])

  
j

p

   

=

,

i

(x

i

   

  
n

2
])

        
k
1
(x
2
2
  
1
    =
exp[
   
2
2
  
   
m
e[z
]x
ij
1i
=  
=
j    
m
e[z
]
1i
=

k
1n

ij

i

 

and a new set of means:

repeat to convergence

notice that this algorithm will find the best id116 in the 
sense of minimizing the sum of square distance.

cis419/519 spring    18

35

summary: em

    em is a general procedure for learning in the presence of   

unobserved variables. 

    we have shown how to use it in order to estimate the most likely 

density function for a mixture of id203 distributions.

    em is an iterative algorithm that can be shown to converge to a local 

maximum of the likelihood function. thus, might requires many 
restarts.

    it depends on assuming a family of id203 distributions.
    it has been shown to be quite useful in practice, when the 

assumptions made on the id203 distribution are correct,  but can 
fail otherwise.

    as examples, we have derived an important id91 algorithm,  the 
id116 algorithm and have shown how to use it in order to estimate 
the most likely density function for a mixture of id203 
distributions. 

cis419/519 spring    18

36

more thoughts about em

    training: a sample of data points, (x0, x1 ,   , xn) 2 {0,1}n+1
    task: predict the value of x0, given assignments to all n 

variables. 

cis419/519 spring    18

37

  z

z

more thoughts about em

pi
z

    assume that a set xi 2 {0,1}n+1 of data points is  generated 

as follows:

    postulate a hidden variable z, with k values, 1    z    k 

with id203   z,    1,k   z = 1

    having randomly chosen a value z for the hidden variable, 

we choose the value  xi for each observable  xi  to be 1 
with id203 pi

z and 0 otherwise, [i = 0, 1, 2,    .n]

    training: a sample of data points, (x0, x1 ,   , xn) 2 {0,1}n+1
    task: predict the value of x0, given assignments to all n 

variables. 

cis419/519 spring    18

38

  z

z

more thoughts about em

pi
z

    two options:
    parametric:   estimate the model using em.                  

once a model is known, use it to make predictions.
    problem: cannot use em directly without an additional 

assumption on the way data is generated.

    non-parametric:  learn x0 directly as a function of the 

other variables.
    problem: which function to try and learn? 

    x0  turns out to be a linear function of the other variables, 

when k=2   (what does it mean)?

    when k is known, the em approach performs well; if an 
another important distinction to attend to is the fact that, once you 
incorrect value is assumed the estimation fails; the linear 
estimated all the parameters with em, you can answer many prediction 
methods performs better [grove & roth 2001]
problems e.g., p(x0, x7,   ,x8 |x1, x2 ,   , xn) while with id88 (say) 
you need to learn separate models for each prediction problem.

cis419/519 spring    18

39

