evaluation

anietie andy (andy)

these slides were assembled by eric eaton, with grateful acknowledgement of the many others who made 
their course materials freely available online. feel free to reuse or adapt these slides for your own academic 
purposes, provided that you include proper attribution. please send comments and corrections to eric. 

robot image credit: viktoriya sukhanova    123rf.com

announcements

    ta office hours
    http://www.seas.upenn.edu/~cis519/spring2018/staff.html

    recitation at 3401 walnut st,  rm 401b 

    tuesdays:       6:30pm     7:30pm
    wednesdays: 5:30pm     6:30pm

    homework submission instructions

stages of (batch) machine learning
x, y = {hxi, yii}n

given: labeled training data

i=1

    assumes each                         with

xi     d(x )

yi = ftarget (xi)

x, y 

learner

model

x

yprediction

train the model:

model    classifier.train(x, y )

apply the model to new data:
    given: new unlabeled instance

yprediction    model.predict(x)

x     d(x )

classification metrics

accuracy =

# correct predictions

# test instances

error = 1   accuracy =

# incorrect predictions

# test instances

4

recall

recall = 

no. of relevant records 
retrieved
total no. of relevant 
records in the database

precision

precision = 

no. of relevant records 
retrieved

total no. of records 
retrieved from the database

precision vs. recall

an inverse relationship
as the level of recall rises the 
level of precision generally 
declines and vice versa.

the cranfield experiments (1957 & 1962)
cyril cleverdon, p.i.

confusion matrix

    given a dataset of p positive instances and n negative instances:

predicted class

yes
tp

no
fn

fp

tn

s
s
a
c

l

 
l

a
u
t
c
a

yes

no

accuracy =

t p + t n

p + n

    imagine using classifier to identify positive cases (i.e., for 

information retrieval)

t p

precision =

t p + f p
id203 that a randomly 
selected result is relevant

recall =

t p

t p + f n

id203 that a randomly 
selected relevant document 
is retrieved

9

example

n = 165

actual:
yes
actual: no
total

predicted:
yes
tp = 100

fp = 10
110

predicted: 
no
fn=5

tn = 50
55

total

105

60

accuracy = (tp+tn)/n = (100+50)/165 = 0.91

precision = tp/tp + fp = 100/110 = 0.91

recall =  tp / tp + fn = 100/105 = 0.95 

training data and test data

    training data: data used to build the model
    test data: new data, not used in the training process

    training performance is often a poor indicator of generalization 
performance 
    generalization is what we really care about in ml
    easy to overfit the training data
    performance on test data is a good indicator of generalization performance
    i.e., test accuracy is more important than training accuracy

11

training and test data

full data set

training data

test data

idea:
train each
model on the
(cid:2)training data(cid:3)...

...and then test
each model(cid:1)s
accuracy on
the test data

12

simple decision boundary

two-class data in a two-dimensional feature space

decision
region 1 

decision
region 2 

6

5

4

3

2

1

0

2

 

e
r
u

t

a
e
f

-1

2

3

4

slide by padhraic smyth, ucirvine

decision
boundary 

5

6

feature 1

7

8

9

10

13

more complex decision boundary

two-class data in a two-dimensional feature space

 

2
e
r
u
a
e
f

t

6

5

4

3

2

1

0

decision
region 1 

decision
region 2 

decision
boundary 

-1

2

3

4

5

feature 1

6

7

8

9

10

slide by padhraic smyth, ucirvine

14

overfitting

       fitting the data more than is warranted   

example: the overfitting phenomenon

y

slide by padhraic smyth, ucirvine

x

16

a complex model

y = high-order polynomial in x

y

slide by padhraic smyth, ucirvine

x

17

the true (simpler) model

y = a x  + b  +  noise

y

slide by padhraic smyth, ucirvine

x

18

underfitting and overfitting

underfitting

overfitting

complexity of a decision
tree := number of nodes 
it uses

underfitting: when model is too simple, both training and test errors are large
overfitting: when model is too complex and test errors are large although 
training errors are small.

complexity of the used model

christopher erick: learning models to predict and classify

notes on overfitting

    overfitting results in models that are more complex than necessary: 
after learning knowledge they    tend to learn noise   

    more complex models tend to have more complicated decision 
boundaries and tend to be more sensitive to noise, missing 
examples,   

    training error no longer provides a good estimate of how well the 
tree will perform on previously unseen records

comparing classifiers

say we have two classifiers, c1 and c2, and want to 
choose the best one to use for future predictions

can we use training accuracy to choose between them?
    no! 

    e.g., c1 = pruned decision tree, c2 = 1-nn

training_accuracy(1-nn) = 100%,  but may not be best

instead, choose based on test accuracy...

based on slide by padhraic smyth, ucirvine

24

n-fold cross validation

   instead of a single test-training split:

train
   split data into n equal-sized parts 

test

   train and test n different classifiers
   report average accuracy and standard deviation of the accuracy

cis419/519 spring    18

26

example 3-fold cv

full data set

1st partition

test data

training
data

2nd partition

training
data

test data
training
data

...

kth partition

training
data

test data

test

performance

test
performance

test

performance

summary statistics

over k test

performances

27

more on cross-validation

    cross-validation generates an approximate estimate of how well the 

classifier will do on (cid:1)unseen(cid:2) data
    as k    n, the model becomes more accurate (more training data)
    ...but, cv becomes more computationally expensive
    choosing k < n is a compromise

    averaging over different partitions is more robust than just a single 

train/validate partition of the data

    it is an even better idea to do cv repeatedly!

29

multiple trials of k-fold cv

1.) loop for t trials:

a.) randomize 

data set

full data set

shuffle 

full data set

b.) perform 
k-fold cv

1st partition
test data

training
data

2nd partition

training
data

test data
training
data

...

kth partition

training
data

test data

test
performance

test
performance

test
performance

2.) compute statistics over 
t x k test performances

30

comparing multiple classifiers

1.) loop for t trials:

a.) randomize 

data set

full data set

shuffle 

test each candidate learner on 

same training/testing splits

b.) perform 
k-fold cv

full data set

1st partition
test data

training
data

2nd partition

training
data

test data
training
data

...

kth partition

training
data

test data

test perf. 

c1

test perf. 

c2

test
c1

test
c2

test
c1

test
c2

2.) compute statistics over 
t x k test performances

allows us to do paired summary 

statistics (e.g., paired t-test)

31

building learning curves

1.) loop for t trials:

a.) randomize 

data set

full data set

shuffle 

compute learning curve over each 

training/testing split

b.) perform 
k-fold cv

full data set

1st partition
test data

training
data

2nd partition

training
data

test data
training
data

...

kth partition

training
data

test data

curve 
c1

curve
c2

curve 
c1

curve 
c2

curve 
c1

curve 
c2

2.) compute statistics over 

t x k learning curves

33

hypothesis testing

    you want to show that hypothesis h is true, based on your data  

    (e.g.  h  =    classifier a and b are different   ) 
    define a null hypothesis h0
    (h0 is the contrary of what you want to show)
    h0 defines a distribution p(m |h0) over some statistic

    e.g. a distribution over the difference in accuracy between a and b

    can you refute (reject) h0?

rejecting h0

    h0 defines a distribution p(m |h0) over some statistic m

    (e.g. m= the difference in accuracy between a and b)

    select a significance value s 

    (e.g. 0.05, 0.01, etc.)
    you can only reject h0 if p(m |h0)     s

    compute the test statistic m from your data

    e.g. the average difference in accuracy over your n folds

    compute p(m |h0) 

    refute h0 with p     s if p(m |h0)     s

paired t-test

    a paired t-test is used to compare two population means 
where you have two samples in which observations in one 
sample can be paired with observations in the other sample. 

    e.g. 

before-and-after observations on the same subjects (e.g.
students)

39

procedure for carrying out paired t-test
    calculate the difference between the two observations on each 

pair.

    calculate the mean difference

    calculate the standard deviation of the differences

    calculate the error of the mean difference

    calculate the t-statistic 

41

paired t-test example

    question: the downtimes (measured in hours) for computer systems in 

six branches of a major bank were recorded for year 1 and year 2. 
compute the test statistics for the paired t-test.

    solution: 

branch

year 1

year 2

a
b
c
d
e
f

40
54
32
36
55
46

30
41
24
38
56
37

dragonfly statistics

difference
(year 1     year 2)
10
13
8
-2
-1
9
sum = 37

square of 
difference
100
169
64
4
1
81
sum = 419

42

paired t-test

    sample size: n = 6
    sum of differences     di  = 37
    sum of squared differences       di2 = 419

    mean of case-wise differences: !  =	   &'( = 37/6 = 6.166
=	6.177
)*+	,-	  -.*--0			
    test statistics for the paired t-test:        2=	&3456 = 2.445

    standard deviation :                 =     

sd=   78,97:;
9,<

1

43

mcnemar   s test

    the test is often used for the situation where one tests for the presence 

(1) or absence (0) of something and variable a is the state at the first 
observation (i.e., pretest) and variable b is the state at the second 
observation (i.e., posttest).

46

mcnemar   s test

    an alternative to cross validation, when the test can be run only once
    divide the sample s into a training set r and a test set t.
    train algorithms a and b on r, yielding classifiers a,b
    record how each example in t is classified and compute the number of 

examples misclassified by both
a and b n00

examples misclassified by 
a but not b   n01

examples misclassified by
b but not a n10

examples misclassified by neither
a nor b  n11

where n is  the total number of examples in the test set t
n

n

n

n

n

=

+

+

+

01

11

10

00

47

mcnemar   s test

    the hypothesis: the two learning algorithms have the same error rate 

on a randomly drawn sample. that is, we expect that

n =

10 n

01

    the statistics we use to measure deviation from the expected counts:

n(|

-

01

01
n

|n
10
n
+

10

2

1)
-

experimental evaluation

cs446-spring06

48

end

49

