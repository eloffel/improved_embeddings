cis 519/419 

applied machine learning

www.seas.upenn.edu/~cis519

dan roth
danroth@seas.upenn.edu
http://www.cis.upenn.edu/~danroth/
461c, 3401 walnut

lecture given by daniel khashabi

slides were created by dan roth (for cis519/419 at penn or cs446 at uiuc), eric eaton 
for cis519/419 at penn, or from other authors who have made their ml slides available. 

cis419/519 spring    18

functions can be made linear

    data are not linearly separable in one dimension
    not separable if you insist on using a specific class of 

functions

x

cis419/519 spring    18

2

blown up feature space
    data are separable in <x, x2> space

x2

cis419/519 spring    18

3

x

multi-layer neural network
    multi-layer network were designed to overcome the 

computational (expressivity) limitation  of a single 
threshold element. 

activation

    the idea is to stack several 

layers of threshold elements, 
each layer using the output of 
the previous layer as input.  

output

hidden

input

    multi-layer networks can represent arbitrary 

functions, but  building effective learning methods 
for such network was [thought to be] difficult. 

cis419/519 spring    18

4

basic units 

    linear unit:  multiple layers of linear functions  
oj = w   x produce linear functions.  we want to 
represent nonlinear functions.

activation

output

    need to do it in a way that 

facilitates learning

    threshold units:  oj = sgn(w   x) 
are not differentiable,  hence 
unsuitable for id119. 

    the key idea was to notice that the discontinuity of 

the threshold element can be represents by a smooth 
non-linear approximation: oj = [1+ exp{-w   x}]-1

(rumelhart, hinton, williiam, 1986), (linnainmaa, 1970) , see: http://people.idsia.ch/~juergen/who-
invented-id26.html )

   

cis419/519 spring    18

w2

ij
hidden
w1

ij

input

5

model neuron (logistic)
    us a non-linear, differentiable output function such 

1x

7x

1
2
3
4
5
6

as the sigmoid or logistic function
17w

t

7

   

67w
    net input to a unit is defined as: 
    output of a unit is defined as:

jo

jt
net        
xw
 

=

ij

j

o

j

=

cis419/519 spring    18

1
(net
   +
e1

   

)t
j

j

i

6

learning with a multi-layer  

id88

    it   s easy to learn the top layer     it   s just a linear unit. 
    given feedback (truth) at the top layer, and the activation at the 

layer below it, you can use the id88 update rule (more 
generally, id119) to updated these weights.

    the problem is what to do with 
the other set of weights     we do
not get feedback in the 
intermediate layer(s). 

cis419/519 spring    18

activation

output

w2

ij
hidden
w1

ij

input

7

learning with a multi-layer  id88

   

   

activation

the problem is what to do with 
the other set of weights     we do 
not get feedback in the 
intermediate layer(s). 
solution: if all the activation 
functions are differentiable, then 
the output of the network is also 
a differentiable function of the input and weights in the network.

output

w2

ij
hidden
w1

ij

input

    define an error function (multiple options) that is a differentiable function 
of the output, that this error function is also a differentiable function of the 
weights. 

    we can then evaluate the derivatives of the error with respect to the 

weights, and use these derivatives to find weight values that minimize this 
error function.  this can be done, for example, using id119 .  
this results in an algorithm called back-propagation.

   

cis419/519 spring    18

8

neural networks 

    robust approach to approximating real-valued, discrete-

valued and vector valued target functions.

    among the most effective general purpose supervised 

learning method currently known.

    effective especially for complex and hard to interpret 

input data such as real-world sensory data, where a lot of 
supervision is available. 

    the id26 algorithm for neural networks has 

been shown successful in many practical problems
    handwritten character recognition, id103,  object 

recognition, some nlp problems

cis419/519 spring    18

9

neural networks 
    neural networks are functions: nn:                   
    where         = 0,1        , or {0,1}         and          = 0,1 ,{0,1}

    nn can be used as an approximation of a target classifier

   

in their general form, even with a single hidden layer, nn can 
approximate any function

    algorithms exist that can learn a nn representation from labeled 

training data  (e.g., id26).

cis419/519 spring    18

10

multi-layer neural networks
    multi-layer network were designed to overcome the 

computational (expressivity) limitation  of a single 
threshold element. 

activation

    the idea is to stack several 

layers of threshold elements, 
each layer using the output of 
the previous layer as input.  

cis419/519 spring    18

output

hidden

input

11

motivation for neural networks
    inspired by biological systems

    but don   t take this (as well as any other words in the new on 

   emergence    of intelligent behavior) seriously; 

    we are currently on rising part of a wave of interest in nn 
architectures, after a long downtime from the mid-90-ies.
    better computer architecture (gpus, parallelism) 
    a lot more data than before; in many domains, supervision is 

available.

    current surge of interest has seen very minimal 

algorithmic changes

cis419/519 spring    18

12

motivation for neural networks
    minimal to no algorithmic changes
    one potentially interesting perspective:

    before we looked at nn only as function approximators.
    now, we look at  the intermediate representations generated 

while learning as meaningful
ideas are being developed on the value of these intermediate 
representations for id21 etc. 

   

    we will present in the next two lectures a few of the basic 
architectures and learning algorithms, and provide some 
examples for applications

cis419/519 spring    18

13

basic unit in multi-layer neural network

    linear unit:                 =        .            multiple layers of linear functions 
    threshold units:                 =                        (        .                      ) are not 

produce linear functions.  we want to represent nonlinear 
functions.

differentiable,  hence unsuitable for id119

activation

output

hidden

input

15

cis419/519 spring    18

the parameters so far? 

model neuron (logistic)
    neuron is modeled by a unit           connected by weighted 
links                          to other units         . 
the set of connective weights:                          
the threshold value:                 
        1        2        3        4        5        6
        17
        7
        67

    use a non-linear, differentiable output function such as the 

   

                

sigmoid or logistic function

    net input to a unit is defined as: 

    output of a unit is defined as:

cis419/519 spring    18

                =

net        =                           .                
1
1+exp   (net                           )

16

history: neural computation 
    mccollough and pitts (1943) showed how linear 
threshold units can be used to compute logical 
net        =                           .                
functions 
1
1+exp   (net                           )

    and:                        =                /        
    or:                         =                

    can build basic logic gates

    not: use negative weight

                =

    can build arbitrary logic circuits, id122 

and computers given these basis gates.

    can specify any boolean function using two layer 

network (w/ negation)
    dnf and cnf are universal representations

cis419/519 spring    18

17

history: learning rules 

    hebb (1949) suggested that if two units are both active 

(firing) then the weights between them should increase:       

                        =                        +                                        
             and is a constant called the learning rate

    supported by physiological evidence

    rosenblatt (1959) suggested that when a target output 
value is provided for a single neuron with fixed input, it 
can incrementally change weights and learn to produce 
the output using the id88 learning rule.
    assumes binary output units; single linear threshold unit
    led to the id88 algorithm

    see: http://people.idsia.ch/~juergen/who-invented-id26.html

cis419/519 spring    18

18

   

   

   

    given:

id88 learning rule
the target output for the output unit is                 
the input the neuron sees is                 
the output it produces is                  
    update weights according to                                                      +                                                           
        1        2        3        4        5        6

if output is correct, don   t change the weights
if output is wrong, change weights for all inputs which are 1
    if output is low (0, needs to be 1) increment weights
    if output is high (1, needs to be 0) decrement weights

        17
        67

                

        7

                

   

   

   

cis419/519 spring    18

19

widrow-hoff rule 

    this incremental update rule provides an approximation 

    where: 

to the goal:
    find the best linear approximation of the data 

                                         =12                                                          2
                =                                    .                =                .           
                     = target output for example d

output of linear unit on example d

cis419/519 spring    18

20

    we use id119 determine the weight vector that  minimizes  

id119 
                                        
    fixing the set          of examples,          is a function of                  

produces the steepest descent along the error surface.

;

    at each step, the weight vector is modified in the direction that 

                        (        )

        3        2        1        0

        

21

cis419/519 spring    18

summary: single layer network 
    variety of update rules

    multiplicative
    additive

    batch and incremental algorithms
    various convergence and efficiency conditions
    there are other ways to learn linear functions

    id135  (general purpose)
    probabilistic classifiers ( some assumption)

    key algorithms are driven by id119 

cis419/519 spring    18

22

general stochastic gradient algorithms 

learning rate

gradient

the loss q: a function of x, w and y

wt+1 = wt     rt gw q(xt, yt, wt) = wt     rt gt

lms: q((x, y), w) =1/2 (y     wt x)2
leads to the update rule (also called widrow   s adaline):

wt+1 = wt + r (yt                             xt) xt

here, even though we make binary predictions based on sgn (wt x) we 
do not take the sign of the dot-product into account in the loss.

another common id168 is:
hinge loss: 
q((x, y), w) = max(0, 1 - y wt x)
this leads to the id88 update rule:

if yi                            xi > 1   (no mistake, by a margin):       no update

(mistake, relative to margin):  wt+1 = wt + r yt xt

otherwise 

here g = -yx

cis419/519 spring    18

good to think about the 
case of boolean examples

wt x

23

summary: single layer network 
    variety of update rules

    multiplicative
    additive

    batch and incremental algorithms
    various convergence and efficiency conditions
    there are other ways to learn linear functions

    id135  (general purpose)
    probabilistic classifiers ( some assumption)

    key algorithms are driven by id119 
    however, the representational restriction is limiting in 

many applications

cis419/519 spring    18

24

learning with a multi-layer  

id88

    it   s easy to learn the top layer     it   s just a linear unit. 
    given feedback (truth) at the top layer, and the activation at the layer 

below it, you can use the id88 update rule (more generally, 
id119) to updated these weights.

    the problem is what to do with 
the other set of weights     we do
not get feedback in the 
intermediate layer(s). 

cis419/519 spring    18

activation

output

w2

ij
hidden
w1

ij

input

25

learning with a multi-layer  

id88

   

   

activation

the problem is what to do with 
the other set of weights     we do 
not get feedback in the 
intermediate layer(s). 
solution: if all the activation 
functions are differentiable, then 
the output of the network is also 
a differentiable function of the input and weights in the network.

output

w2

ij
hidden
w1

ij

input

    define an error function (e.g., sum of squares) that is a differentiable function 

of the output, i.e. this error function is also a differentiable function of the 
weights. 

    we can then evaluate the derivatives of the error with respect to the weights, 

and use these derivatives to find weight values that minimize this error 
function, using id119 (or other optimization methods). 
this results in an algorithm called back-propagation.

   

cis419/519 spring    18

26

    simple chain rule

some facts from real analysis
if          is a function of         , and          is a function of         
    then          is a function of         , as well. 
    question:  how to find                                 

   

we will use these facts to derive 
the details of the id26  
algorithm. 
z will be the error (loss) function.
- we need to know how to 
differentiate z 
intermediate nodes use a logistics 
function (or another differentiable 
step function). 
- we need to know how to 
differentiate it. 

27

cis419/519 spring    18

some facts from real analysis

    multiple path chain rule 

cis419/519 spring    18

slide credit: richard socher

28

some facts from real analysis

    multiple path chain rule: general 

cis419/519 spring    18

slide credit: richard socher

29

id26 learning rule
    since there could be multiple output units, we define the 

error as the sum over all the network output units.

                                 =12                                            
                                                    2
    where          is the set of training examples, 
             is the set of output units
                           =                                                           

cis419/519 spring    18

    this is used to derive the (global) learning rule which performs 

id119 in the weight space in an attempt to minimize the 
error function. 

        1                   
(1,0,1,0,0)

function 1

30

reminder: model neuron (logistic)

    neuron is modeled by a unit           connected by weighted 
links                          to other units         . 
        1        2        3        4        5        6
        7

    use a non-linear, differentiable output function such as the 

        17
        67

   

                

sigmoid or logistic function

    net input to a unit is defined as: 

    output of a unit is defined as:

cis419/519 spring    18

                =

function 2

net        =                           .                
1
1+exp   (net                           )

function 3

31

derivatives

   

   

   

function 1 (error): 

   
function 2 (linear gate): 

            = 12                                                          2
                                        =                                      
            =                   .                
                                        =                
            =
1
1+exp{   (                   )}
(1+exp{   (                   )})2=         (1           )
                                    = exp{   (                   )}

   
function 3 (differentiable step function):

cis419/519 spring    18

        1                            
                                 

32

derivation of learning rule
    the weights are updated incrementally;  the error is 

computed for each example and the weight update is 
then derived.

                                             =12                                                          2
                             influences the output only through  net        
net        =                           .                        
                                                        =                                 net                net        
                                

    therefore:

        1                            
                                 

33

cis419/519 spring    18

    therefore:

derivation of learning rule (2)
                             influences the output only through net        
    weight updates of output units:
                                                        =                                 net                 net        
                                
        net        
                                net        
=                                o        
                                
=                                      
                1                   
                        
                                net        =                (1                   )
                                         =12                                                          2
net        =                           .                        
1
                =
1+exp{   (net                           )}

        

cis419/519 spring    18

        

                        

34

derivation of learning rule (3)
                             is changed by:
    weights of output units:
                           =                                                           1                                           
=                                                
                =                                                    1                   

        
                        

                

where 

                                 

cis419/519 spring    18

35

derivation of learning rule (4)
                             influences the output only through all the units whose direct 
    weights of hidden units:
                
input include         
                
net        =                           .                        
                                                        =                                 net                 net        
                                =
                                                                                              (        )                                 net                 net        
        net                                
=
        
                                                                                              (        )                            net        
=
        net                                

cis419/519 spring    18

                        

36

derivation of learning rule (5)
                             influences the output only through all the units whose direct 
    weights of hidden units:
                
input include         
                
                                                        =

                                                                                              (        )                            net        
        net                                =
                                net                                
                                                                                              (        )                            net                                
=
                                                                                              (        )                                                           (1                   )                         
=

cis419/519 spring    18

                        

        

37

derivation of learning rule (6)
                             is changed by:
    weights of hidden units:
                           =                        1                   .
=                                                
                =                1                   .

                                                                                                                                                 

                                                                                                                                                 

    first determine the error for the output units.
    then, backpropagate this error layer by layer through the network, 

                        

    where 

changing weights appropriately in each layer.

cis419/519 spring    18

                
                

                        

        

38

the id26 algorithm
    until all examples produce the correct output within          (or other 

    create a fully connected three layer network. initialize weights.

for each example in the training set do:

criteria)

1.
2.

1.

1.

1.

compute the network output for this example 
compute the error between the output and target value

for each output unit k, compute error term 

                =                1                   .

                =                                                    1                   
                                                                                                                                                 
                           =                                                

for each hidden unit, compute error term:

update network weights

end epoch

cis419/519 spring    18

39

more hidden layers

    the same algorithm holds for more hidden layers. 

input       1

   2

   3 output

cis419/519 spring    18

40

comments on training 

    no guarantee of convergence; may oscillate or reach a local 

minima.

    in practice, many large networks can be trained on large 

amounts of data for realistic problems.

    many epochs (tens of thousands) may be needed for adequate 

training. large data sets may require many hours of cpu 

    termination criteria: number of epochs;  threshold on training 
set error; no decrease in error; increased error on a validation 
set.

    to avoid local minima: several trials with different random 

initial weights with majority or voting techniques

cis419/519 spring    18

41

over-training prevention 
    running too many epochs may over-train the network and 

result in over-fitting. (improved result on training, decrease in 
performance on test set) 

    keep an hold-out validation set and test accuracy after every 

epoch

    maintain weights for best performing network on the validation 

set and return it when performance decreases significantly 
beyond that.
to avoid losing training data to validation:
    use 10-fold cross-validation to determine the average number of epochs 

   

that optimizes validation performance

    train on the full data set using this many epochs to produce the final 

results

cis419/519 spring    18

42

over-fitting prevention 

    too few hidden units prevent the system from adequately 

fitting the data and learning the concept.

    using too many hidden units leads to over-fitting.
    similar cross-validation method can  be used to determine 

an appropriate number of hidden units.  (general)

    another approach to prevent over-fitting is weight-decay: 

all weights are multiplied by some fraction in (0,1) after 
every epoch.
    encourages smaller weights and less complex hypothesis
    equivalently: change error function to include a term for the sum 

of the squares of the weights in the network. (general)

cis419/519 spring    18

43

dropout training

    proposed by (hinton et al, 2012)

    each time decide whether to delete one hidden unit with 

some id203 p

cis419/519 spring    18

44

dropout training

    dropout of 50% of the hidden units and 20% of the input units (hinton 

et al, 2012)

cis419/519 spring    18

45

dropout training

    model averaging effect 

    among 2h models, with shared parameters 

    h: number of units in the network 

    only a few get trained 
    much stronger than the known regularizer

    what about the input space?

    do the same thing! 

cis419/519 spring    18

46

input-output coding

    appropriate coding of inputs and outputs can make 
learning problem easier and improve generalization. 
    encode each binary feature as a separate input unit;

one way to do it, if you start with a collection of sparsely 
representation examples, is to use id84 
methods:
your m examples are represented as a m x 106 matrix
-
- multiple it by a random matrix of size 106 x 300, say.
-
- new representation: m x 300 dense rows 

    for multi-valued features include one binary unit per 
value rather than trying to encode input information in 
fewer units.
    very common today to use distributed representation of the input 

random matrix: normal(0,1) 

    real valued, dense representation. 

    for disjoint categorization problem, best to have one 
output unit for each category rather than encoding n 
categories into log n bits.

cis419/519 spring    18

47

representational power 

    the id26 version presented is for networks with a 

single hidden layer,

but:
    any boolean function can be represented by a two layer 

network (simulate a two layer and-or network)

    any bounded continuous function can be approximated with 

arbitrary small error by a two layer network.

    sigmoid functions provide a set of basis function from which 

arbitrary function can be composed. 

    any function can be approximated to arbitrary accuracy by a 

three layer network.

cis419/519 spring    18

48

hidden layer representation 

    weight tuning procedure sets weights that define 

whatever hidden units representation is most effective at 
minimizing the error.

    sometimes id26 will define new hidden layer 
features that are not explicit in the input representation, 
but which capture properties of the input instances that 
are most relevant to learning the target function.

    trained hidden units can be seen as newly constructed 
features that re-represent the examples so that they are 
linearly separable

cis419/519 spring    18

49

auto-associative network

    an auto-associative network trained with 8 inputs, 3 hidden units and 

8 output nodes, where the output must reproduce the input.

    when trained with vectors with only one bit on

1    0    0    0    1    0    0    0

input                hidden
1 0 0 0 0 0 0 0    .89   .40  0.8
0 1 0 0 0 0 0 0    .97   .99  .71
   .
0 0 0 0 0 0 0 1    .01   .11  .88

    learned the standard 3-bit encoding for the 8 bit vectors.
    illustrates also data compression aspects of learning

1    0    0    0    1    0    0    0

cis419/519 spring    18

51

sparse auto-encoder 

    encoding: 
    decoding: 

    goal: perfect reconstruction of 

        =        (                +        )
           =        (                   +           )
input vector         , by the output           =           (        )
    where         ={        ,           }
    minimize an error function         (                   ,        )
                           ,         =                                 2
|                |
min                                                ,         +           

    and regularize it 

    for example:

    after optimization drop the 
reconstruction layer and add a new layer

cis419/519 spring    18

52

stacking auto-encoder 
    add a new layer, and a reconstruction layer for it. 
    and try to tune its parameters such that 
    and continue this for each layer 

cis419/519 spring    18

53

beyond supervised learning

    so far what we had was purely supervised.

initialize parameters randomly 

   
    train in supervised mode typically, using backprop
    used in most practical systems  (e.g. speech and image recognition)

    unsupervised, layer-wise + supervised classifier on top 

   

train each layer unsupervised, one after the other 
train a supervised classifier on top, keeping the other layers fixed 

   
    good when very few labeled samples are available

    unsupervised, layer-wise + global supervised fine-tuning 

    train each layer unsupervised, one after the other 
    add a classifier layer, and retrain the whole thing supervised 
    good when label set is poor (e.g. pedestrian detection)  

we won   t talk about unsupervised pre-
training here.  but it   s good to have this in 
mind, since it is an active topic of research. 

cis419/519 spring    18

54

nn-2

cis419/519 spring    18

55

recap: multi-layer id88s
    multi-layer network 
    a global approximator
    different rules for training it 

activation

    the back-propagation

    forward step 
    back propagation of errors 

output

hidden

input

    congrats! now you know the hardest concept about 

neural networks!

    today: 

    convolutional neural networks 
    recurrent neural networks  

cis419/519 spring    18

56

receptive fields 

    the receptive field of an individual sensory neuron is the particular 
region of the sensory space (e.g., the body surface, or the retina) in 
which a stimulus will trigger the firing of that neuron.
   

in the auditory system, receptive fields can correspond to volumes in 
auditory space

    designing    proper    receptive fields for the input neurons is a 

significant challenge. 

    consider a task with image inputs

    receptive fields should give expressive features from the raw input to the 

system 

    how would you design the receptive fields for this problem? 

cis419/519 spring    18

57

    a fully connected layer: 

    example: 

    100x100 images 
    1000 units in the input 

    problems: 

    10^7 edges! 
    spatial correlations lost! 
    variables sized inputs. 

cis419/519 spring    18

slide credit: marc'aurelio ranzato

58

    consider a task with image inputs: 
    a locally connected layer: 

    example: 

    100x100 images 
    1000 units in the input 
    filter size: 10x10

    local correlations preserved!
    problems: 

    10^5 edges 
    this parameterization is good 
when input image is 
registered (e.g., face recognition).  
    variable sized inputs, again. 

cis419/519 spring    18

slide credit: marc'aurelio ranzato

59

convolutional layer 

    a solution: 

    filters to capture different patterns in the input space. 

    share parameters across different locations (assuming input is 

stationary) 

    convolutions with learned filters 
    filters will be learned during training. 
    the issue of variable-sized inputs will be 
resolved with a pooling layer.

so what is a 
convolution?

cis419/519 spring    18

slide credit: marc'aurelio ranzato

60

convolution operator 

    convolution operator:    

    one dimension:  

   

takes two functions and gives another function 

                        =                                                         
              [        ]=                              [                   ]

   convolution    is 
very similar to 

   cross-

correlation   , 
except that in 
convolution one 
of the functions 

is flipped. 

cis419/519 spring    18

61

convolution operator (2)

    convolution in two dimension:

    the same idea: flip one matrix and slide it on the other matrix 
    example: sharpen kernel: 

cis419/519 spring    18

try other kernels: http://setosa.io/ev/image-kernels/ 
62

convolution operator (3)

convolution in two dimension:
    the same idea: flip one matrix and slide it on the other 

matrix 

cis419/519 spring    18

slide credit: marc'aurelio ranza

63

complexity of convolution

inputs. 
    uses fast-fourier-transform (fft)

    complexity of convolution operator is                              , for         
    for two-dimension, each convolution takes                                             
time, where the size of input is                 . 

cis419/519 spring    18

slide credit: marc'aurelio ranzato

64

convolutional layer

    the convolution of the input (vector/matrix) with weights 

(vector/matrix) results in a response vector/matrix. 

    we can have multiple filters in each convolutional layer, each 

producing an output.  

    if it is an intermediate layer, it can have multiple inputs! 

convolutional 

layer

filterfilterfilterfilter

one can add nonlinearity 

at the output of 
convolutional layer

cis419/519 spring    18

65

pooling layer 

    how to handle variable sized inputs? 

    a layer which reduces inputs of different size, to a fixed size.
    pooling  

cis419/519 spring    18

slide credit: marc'aurelio ranzato

66

pooling layer 

    how to handle variable sized inputs? 

    a layer which reduces inputs of different size, to a fixed size.
    pooling  
    different variations 

    max pooling 

    average pooling 

                   (        )      [        ]
                    = max
                    =1                               (        )      [        ]
                      (        )      2[        ]
                    =1        

    l2-pooling 

    etc

cis419/519 spring    18

67

convolutional nets

    one stage structure: 

convol.

pooling

    whole system: 

input 
image

stage 1

stage 2

stage 3

fully 

connected 

layer

class 
label 

cis419/519 spring    18

slide credit: druv bhatra

68

training a convnet

    the same procedure from back-propagation applies here. 

    remember in backprop we started from the error terms in the last stage, 

and passed them back to the previous layers, one by one. 

    back-prop for the pooling layer: 

   

   

consider, for example, the case of    max    pooling. 
this layer only routes the gradient to the input that has the highest value in the 
forward pass. 

    hence, during the forward pass of a pooling layer it is common to keep track of the 

index of the max activation (sometimes also called the switches) so that gradient 
routing is efficient during id26.

   

therefore we have:          =                                                
                

                

convol.

pooling

input 
image

stage 1

stage 2

stage 3

cis419/519 spring    18

        last   layer=
                        
                first   layer

                        
                last   layer                

class 
label 

fully connected layer

        first   layer=

69

we derive the 

update rules for a 
1d convolution, 
but the idea is the 
same for bigger 
dimensions.  

    back-prop for the convolutional layer:

training a convnet
           1                                           =           =0
           =                                           =           =0
        =                                         =        (                   )            
                                                =
           1                                                                                                      =           =0
           1                                                                              
           =0
                                                                                                   =                                                           (           )
                                                   =
        =                                                =
           1                                                                              
           1                                                                                                       =           =0
           =0
                
        first   layer=

convol.

pooling

           1                                                       

the convolution

a differentiable nonlinearity 

now we have everything in 
this layer to update the filter

we need to pass the gradient 

to the previous layer 

        last   layer=
                        
                first   layer

                        
                last   layer                

class 
label 
70

stage 1

stage 2

stage 3

fully connected layer

now we can 
repeat this for 
each stage of 

convnet. 

                

input 
image

cis419/519 spring    18

convolutional nets

input 
image

stage 

1

stage 

2

stage 

3

fully 

connected 

layer

class 
label 

feature visualization of convolutional net trained on id163
from [zeiler & fergus 2013]

cis419/519 spring    18

71

convnet roots 

    fukushima, 1980s designed network with same basic structure but 

did not train by id26. 

    the first successful applications of convolutional networks by yann 

lecun in 1990's (lenet)
    was used to read zip codes, digits, etc.

    many variants nowadays, but the core idea is the same

    example: a system developed in google (googlenet) 

    compute different filters 
    compose one big vector from all of them
    layer this iteratively

cis419/519 spring    18

see more: http://arxiv.org/pdf/1409.4842v1.pdf

72

depth matters

slide from [kaiming he 2015]

cis419/519 spring    18

73

practical tips 

    before large scale experiments, test on a small subset of the data and 

check the error should go to zero. 
    overfitting on small training 

    visualize features (feature maps need to be uncorrelated) and have 

high variance

    bad training: many hidden units ignore the input and/or exhibit strong 

correlations.

cis419/519 spring    18

figure credit: marc'aurelio ranzato

74

debugging

    training diverges: 

learning rate may be too large     decrease learning rate 

   
    backprop is buggy     numerical gradient checking 

    loss is minimized but accuracy is low 

    check id168: is it appropriate for the task you want to solve? does 

it have degenerate solutions? 

    nn is underperforming / under-fitting 

    compute number of parameters     if too small, make network larger 

    nn is too slow 

    compute number of parameters     use distributed framework, use gpu, 

make network smaller

many of these points apply to many machine learning models, no just neural 

networks. 

cis419/519 spring    18

75

id98 for vector inputs
    let   s study another variant of id98 for language 
    first step: represent each word with a vector in            

example: sentence classification (say spam or not spam)

spam 

this 

not 

is 

a 

   

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

o o o o o o o
concatenate the vectors 

o o o

    now we can assume that the input to the system is a 

vector                     
    where the input sentence has length          (        =5 in our example )
    each word vector   s length          (        =7 in our example )

76

cis419/519 spring    18

convolutional layer on vectors
    think about a single convolutional layer

o o o o o o o o o o o o o o

    a bunch of vector filters

    find its (modified) convolution with the input vector 
o o o o o o o o o o o o o o
o o o o o o o o o o o o o o
o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o
o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o
o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o
o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o
o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

    each defined in               
    where     is the number of the words the filter covers 
    size of the word vector         
        1=        (        .        1:   )        2=        (        .           +1:2   )        3=        (        .        2   +1:3   )
        4=        (        .        3   +1:4   )
        =[        1,   .,                      +1]

convolution with a filter that spans 2 words, is operating on all of the bi-
grams (vectors of two consecutive word, concatenated):    this is   ,    is not   , 
   not a   ,    a spam   . 

o o o o o o o o o o o o o o
o o o o o o o o o o o o o o
o o o o o

    result of the convolution with the filter 

o o
o o

   

    regardless of whether it is grammatical  (not appealing linguistically)

cis419/519 spring    18

77

convolutional layer on vectors

this 

is 

not 

a 

spam 

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

*

o o o o o o o
o o o o o o o o o o o o o o

o o o o o o o o o o o o o o

o o o o o o o o o o o o o o o o o o o o o

o o o o o o o o o o o o o o o o o o o o o

get word 
vectors for 
each words 

concatenate 

vectors 

perform 

convolution 
with each filter 

#of filters

how are we going to 
handle the variable 

sized response 

vectors?
pooling!  

cis419/519 spring    18

o o o o o
o o o o

o o o o
o o o

o o o

#words - #length of filter + 1

filter 
bank

set of 

response 
vectors 

78

convolutional layer on vectors

this 

is 

not 

a 

spam 

get word 
vectors for 
each words 

concatenate 

vectors 

perform 

convolution 
with each 

filter 

pooling on 

filter 

responses 

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o

o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

*

o o o o o o o
o o o o o o o o o o o o o o

o o o o o o o o o o o o o o

o o o o o o o o o o o o o o o o o o o o o

o o o o o o o o o o o o o o o o o o o o o

#of filters

o o o o o
o o o o

o o o o
o o o

o o
oo o oo o o

o o o

o o oo o o
#words - #length of filter + 1

filter 
bank

some choices for 

pooling: 

k-max, mean, etc

   

now we can pass the fixed-sized vector to a logistic unit (softmax), or give it to multi-layer 
network (last session)

cis419/519 spring    18

79

recurrent neural networks 

    multi-layer feed-forward nn: dag

just computes a fixed sequence of 

   
non-linear learned transformations to convert an input patter into an 
output pattern

    recurrent neural network: digraph 

    has cycles. 
    cycle can act as a memory; 
    the hidden state of a recurrent net can carry along  information 

about a    potentially    unbounded number of previous inputs.
    they can model sequential data in a much more natural way.

cis419/519 spring    18

80

equivalence between id56 and feed-forward nn

    assume that there is a time delay of 1 in using each connection.
    the recurrent net is just a layered net that keeps reusing the same 

weights.

w1

w2 

w4

w3

w1    w2       w3   w4

w1    w2      w3     w4

w1      w2     w3   w4

time=3

time=2

time=1

time=0

cis419/519 spring    18

slide credit: geoff hinton

81

recurrent neural networks 

    training a general id56   s can be hard

    here we will focus on a special family of id56   s 

    prediction on chain-like input: 

    example: id52 words of a sentence 

        =
y=

   

issues : 

this
sample 
dt                      vbz             dt                   nn   

is 

a 

sentence

nn            

    structure in the output: there is connections between labels
   

interdependence between elements of the inputs: the final decision is based 
on an intricate interdependence of the words on each other. 

    variable size inputs:  e.g. sentences differ in size 
    how would you go about solving this task? 

cis419/519 spring    18

82

    a chain id56:

    has a chain-like structure 

recurrent neural networks 
    each input is replaced with its vector representation                 
    hidden (memory) unit             contain information about previous 
inputs and previous hidden units               1,              2, etc
                +1
           +1

                   1
              1

the sentence up to that time.

                
           

    computed from the past memory and current word. it summarizes 

o
o
o
o
o

o
o
o
o
o

o
o
o
o
o

o o o o o

o o o o o

o o o o o

 

 

 

input layer

memory layer

cis419/519 spring    18

83

    a popular way of formalizing it: 

recurrent neural networks 
    where          is a nonlinear, differentiable (why?) function. 

           =        (                         1+                                )

    outputs?

    many options; depending on problem and computational 

resource

o o o o o

                   1
              1

 

o
o
o
o
o

o o o o o

                
           

 

o
o
o
o
o

o o o o o

                +1
           +1

 

o
o
o
o
o

input layer

memory layer

cis419/519 spring    18

84

prediction for the whole chain

recurrent neural networks 
prediction for                 , with            
                =softmax                           
prediction for                 , with            ,   ,                      
                                                                           
                =softmax            =0
                =softmax                           
                +1
                
                   1
           +1
           
              1

o o o o o

o o o o o

o
o
o
o
o

o
o
o
o
o

o
o
o
o
o

o o o o o

input layer

 

 

 

memory layer

some inherent issues with id56s: 
    recurrent neural nets cannot capture phrases without prefix context 
   

they often capture too much of last words in final vector

   

   

   

   

cis419/519 spring    18

85

bi-directional id56

    one of the issues with id56: 

    hidden variables capture only one side context 

    a bi-directional structure

o o o o o

o o o o o

                

 

o
o
o
o
o

           

o o o o o

                   1
              1

 

o
o
o
o
o

 

o
o
o
o
o

                 1
                   1

 

o
o
o
o
o

              

                

 

o
o
o
o
o

                +1
           +1
                +1

 

o
o
o
o
o

              +1

           =        (                         1+                                )
              =        (                            +1+                                   )
                =softmax                           +                                 

cis419/519 spring    18

86

stack of bi-directional networks 
    use the same idea and make your model further 

complicated: 

cis419/519 spring    18

87

   

vectors for 

parameters? 

    how to train such model? 

generalize the same ideas from back-propagation 

training id56s
    total output error:                     ,            =           =1                                         ,                
                                =           =1                                                 
                ,                 ,             +
                                        =           =1                                                                                                                                                                                                    
                   1
              1

                +1
           +1

                
           

o
o
o
o
o

o
o
o
o
o

o o o o o

o o o o o

input

 

 

o o o o o

                   1

                

cis419/519 spring    18

reminder: 
                =softmax                           
           =        (                         1+                                )

this sometimes  is called 
   id26 through time   , 
since the gradients are 
propagated back through time. 

 

o
o
o
o
o

                +1

88

recurrent neural network 
                                =           =1                                                                                                                                                                                                    
reminder: 
                =softmax                           
           =        (                         1+                                )
                                         1=           dia               (                         1+                                )
dia            1,   ,                 =         1 0
0
0
0    
0                 
0
                                         1=            =                   +1
                                                 =            =                   +1
        
                    dia               (                         1+                                )
                +1
                
                   1
           +1
              1
           
                +1
                   1
                

o o o o o

o o o o o

o o o o o

o
o
o
o
o

o
o
o
o
o

o
o
o
o
o

 

 

 

89

cis419/519 spring    18

vanishing/exploding gradients 
                    dia               (                         1+                                )
        
dia               (                         1+                                )                =                   +1

                                                 =            =                   +1
                                                     =                   +1
        
           

gradient can become very small or very large quickly, and the locality assumption 
of id119 breaks down (vanishing gradient) [bengio et al 1994]

                =                          

    vanishing gradients are quite prevalent and a serious issue.  
    a real example 

    training a feed-forward network 
y-axis: sum of the gradient norms
   
    earlier layers have exponentially 
smaller sum of gradient norms
    this will make training earlier 
layers much slower. 

cis419/519 spring    18

90

vanishing/exploding gradients 

    in an id56 trained on long sequences (e.g. 100 time steps) the 

gradients can easily explode or vanish.
    so id56s have difficulty dealing with long-range dependencies.

    many methods proposed for reduce the effect of vanishing gradients; 

although it is still a problem 
   
    abandon stochastic id119 in favor of a much more 

introduce shorter path between long connections 

sophisticated hessian-free (hf) optimization

    add fancier modules that are robust to handling long memory; 

e.g. long short term memory (lstm) 

    one trick to handle the exploding-gradients: 

    clip gradients with bigger sizes: 

cis419/519 spring    18

defnne         =                                
if                                                                             then 
                                                                         
        
        

91

