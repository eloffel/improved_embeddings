cis 519/419 

applied machine learning

www.seas.upenn.edu/~cis519

dan roth
danroth@seas.upenn.edu
http://www.cis.upenn.edu/~danroth/
461c, 3401 walnut

slides were created by dan roth (for cis519/419 at penn or cs446 at uiuc), eric eaton 
for cis519/419 at penn, or from other authors who have made their ml slides available. 

cis419/519 spring    18

administration

    exam:

    the exam will take place on the originally assigned date, 4/30. 

    similar to the previous midterm.
    75 minutes; closed books.

    what is covered:

    the focus is on the material covered after the previous mid-term.
    however, notice that the ideas in this class are cumulative!!
    everything that we present in class and in the homework assignments
    material that is in the slides but is not discussed in class is not part of the 

material required for the exam.

    example 1: we talked about boosting. but not about boosting the confidence.
    example 2: we talked about multiclass classification: ova, ava, but not error 

correcting codes,  and additional material in the slides.

    we will give a few practice exams.

cis419/519 spring    18

2

administration

    projects

    we will have a poster session 6-8pm on may 7

   

in the active learning room, 3401 walnut.

(72% of the cis 519 students that voted in our poll preferred that).
   
    the hope is that this will be a fun event where all of you have an 

opportunity to see and discuss the projects people have done. 

    all are invited!
    mandatory for cis519 students
    the final project report will be due on 5/8
   

logistics: you will send us you posters the a day earlier; we will print it 
and hang it; you will present it.
    if you haven   t done so already:

    come to my office hours at least once this or next week to discuss the 

project!!

go to bayes

cis419/519 spring    18

3

recap: error driven learning 

    consider a distribution d over space x  y
    x - the instance space;   y - set of labels. (e.g. +/-1)

    can think about the data generation process as governed by d(x), and 

the labeling process as governed by d(y|x), such that 

d(x,y)=d(x) d(y|x)

    this can be used to model both the case where labels are generated 

by a function y=f(x), as well as noisy cases and probabilistic generation 
of the label. 

    if the distribution d is known, there is no learning. we can simply 

predict y = argmaxy d(y|x)

    if we are looking for a hypothesis, we can simply find the one that 

minimizes the id203 of mislabeling:

h = argminh e(x,y)~d [[h(x)    y]]

cis419/519 spring    18

recap: error driven learning (2)
    inductive learning comes into play when the distribution is 

not known. 

    then, there are two basic approaches to take.

    discriminative (direct) learning  

    and  

    bayesian learning (generative) 

    running example: text correction:
       i saw the girl it the park        i saw the girl in the park

cis419/519 spring    18

5

1: direct learning

    model the problem of text correction as a problem of 

learning from examples.

    goal: learn directly how to make predictions.

paradigm

    look at many (positive/negative) examples.
    discover some regularities in the data.
    use these to construct a prediction policy.
    a policy (a function, a predictor) needs to be specific.
if the occurs after the target    in
    assumptions comes in the form of a hypothesis class.

[it/in] rule:

bottom line: approximating h : x     y is estimating p(y|x).

cis419/519 spring    18

6

direct learning (2)

    consider a distribution d over space x  y
    x - the instance space;   y - set of labels. (e.g. +/-1)
    given a sample {(x,y)}1
m
    find  h   h that minimizes   

,, and a id168 l(x,y)          

  i=1,md(xi,yi)l(h(xi),yi) + reg

    l can be:   l(h(x),y)=1, h(x)   y, o/w l(h(x),y) = 0 (0-1 loss)

l(h(x),y)=(h(x)-y)2 ,                  (l2 ) 
l(h(x),y)= max{0,1-y h(x)}       (hinge loss)
l(h(x),y)= exp{- y h(x)}             (exponential loss)

    guarantees: if we find an algorithm that minimizes loss on the 
observed data. then, learning theory guarantees good future 
behavior (as a function of |h|).

cis419/519 spring    18

7

2: generative model

the model is called 
   generative    since it 
makes an assumption 
on how data x is 
generated given y
    model the problem of text correction as that of generating 

correct sentences.

    goal: learn a model of the language; use it to predict.

paradigm

    learn a id203 distribution over all sentences
in practice: make assumptions on the distribution   s type
    use it to estimate which sentence is more likely. 

   

   

   

pr(i saw the girl it the park) <>   pr(i saw the girl in the park)
in practice: a decision policy depends on the assumptions

bottom line: the generating paradigm approximates 

p(x,y) = p(x|y) p(y).

    guarantees:  we need to assume the    right     id203 distribution

cis419/519 spring    18

8

probabilistic learning

    there are actually two different notions.
    learning probabilistic concepts 

    the learned concept is a function c:x   [0,1]
    c(x) may be interpreted as the id203 that the label 1 is 

assigned to x

    the learning theory that we have studied before is applicable 

(with some extensions).

    bayesian learning: use of a probabilistic criterion in 

selecting a hypothesis
    the hypothesis can be deterministic, a boolean function.

    it   s not the hypothesis     it   s the process.
    in practice, as we   ll see, we will use the same principles as before, 

often similar justifications, and similar algorithms.

cis419/519 spring    18

9

probabilities

    30 years of ai research danced around the fact that the 

world was inherently uncertain

    bayesian id136:

    use id203 theory and information about independence 
    reason diagnostically (from evidence (effects) to conclusions 

(causes))...
...or causally (from causes to effects)

   

    probabilistic reasoning only gives probabilistic results

   

i.e., it summarizes uncertainty from various sources

    we will only use it as a tool in machine learning.

cis419/519 spring    18

concepts

    id203, id203 space and events
    joint events
    conditional probabilities
    independence

    go to the recitation on tuesday/thursday
    use the material we provided on-line

    next i will give a very quick refresher 

cis419/519 spring    18

11

(1) discrete random variables

    let x denote a random variable

    x is a mapping from a space of outcomes to r.
    [x=x] represents an event (a possible outcome) and, 
    each event has an associated id203

    examples of binary random variables:

    a = i have a headache
    a = sally will be the us president in 2020

    p(a=true) is    the fraction of possible worlds in which 

p(a)  
a is true   
    we could spend hours on the philosophy of this,    but we 

won   t

cis419/519 spring    18
adapted from slide by andrew moore

12

visualizing a

    universe u  is the event space of all possible worlds

its area is 1
   
    p(u) = 1

    p(a) = area of red oval

    therefore:

u

worlds in which 

a is true

worlds in which a is false

cis419/519 spring    18

13

axioms of id203

kolmogorov showed that three simple axioms lead to 
the rules of id203 theory

   

de finetti, cox, and carnap have also provided compelling 
arguments for these axioms

1. all probabilities are between 0 and 1:

0     p(a)     1

2. valid propositions (tautologies) have id203 1, 

and unsatisfiable propositions have id203 0:

p(true) = 1 ;    p(false) = 0
3. the id203 of a disjunction is given by:

p(a     b) = p(a) + p(b)     p(a     b)

cis419/519 spring    18

14

interpreting the axioms

a

a   b

b

    0     p(a)     1
    p(true) = 1
    p(false) = 0
    p(a     b) = p(a) + p(b)     p(a     b)

    from these you can prove other 

properties:

   

   

cis419/519 spring    18

15

multi-valued random variables

    a is a random variable with arity k if it can take on exactly one 

value out of {v1,v2, ..., vk }
    think about tossing a die
    thus   

cis419/519 spring    18
based on slide by andrew moore

16

multi-valued random variables

    we can also show that:

    this is called marginalization over a

cis419/519 spring    18

17

(2) joint probabilities

    joint id203: matrix of combined probabilities of a set 

of variables

russell & norvig   s alarm domain: (boolean rvs)
    a world has a specific instantiation of variables:

(alarm     burglary       earthquake)

    the joint id203 is given by:

p(alarm, burglary) =

burglary
  burglary

alarm
0.09
0.1

  alarm
0.01
0.8

id203 of 
burglary:
p(burglary) = 0.1
by marginalization 
over alarm

cis419/519 spring    18

the joint distribution

recipe for making a joint 

distribution of d variables:

e.g., boolean variables a, b, c

cis419/519 spring    18

slide    andrew moore

19

the joint distribution

recipe for making a joint 

distribution of d variables:

1. make a truth table listing all 

combinations of values of 
your variables (if there are d
boolean variables then the 
table will have 2d rows).

e.g., boolean variables a, b, c
a
0
0
0
0
1
1
1
1

b
0
0
1
1
0
0
1
1

c
0
1
0
1
0
1
0
1

cis419/519 spring    18

slide    andrew moore

20

the joint distribution

recipe for making a joint 

distribution of d variables:

1. make a truth table listing all 

combinations of values of 
your variables (if there are d
boolean variables then the 
table will have 2d rows).
1. for each combination of 

values, say how probable it is.

e.g., boolean variables a, b, c
a
0
0
0
0
1
1
1
1

prob
0.30
0.05
0.10
0.05
0.05
0.10
0.25
0.10

b
0
0
1
1
0
0
1
1

c
0
1
0
1
0
1
0
1

cis419/519 spring    18

slide    andrew moore

21

the joint distribution

recipe for making a joint 

distribution of d variables:

1. make a truth table listing all 

combinations of values of 
your variables (if there are d
boolean variables then the 
table will have 2d rows).
1. for each combination of 

2.

values, say how probable it is.
if you subscribe to the axioms 
of id203, those numbers 
must sum to 1.
cis419/519 spring    18

slide    andrew moore

e.g., boolean variables a, b, c
a
0
0
0
0
1
1
1
1

prob
0.30
0.05
0.10
0.05
0.05
0.10
0.25
0.10

b
0
0
1
1
0
0
1
1

c
0
1
0
1
0
1
0
1

0.05

a

0.25

0.10
0.10

0.05

0.05

c

b

0.10

0.30

22

inferring probabilities from the joint

alarm
earthquake
0.01
0.01

  earthquake
0.08
0.09

  alarm
earthquake
0.001
0.01

  earthquake
0.009
0.79

burglary
  burglary

cis419/519 spring    18

(3) id155

    p(a | b) =  fraction of worlds in which b is true that also 

have a true
u

a

b

what if we already know 
that b is true?
that knowledge changes 
the id203 of a
    because we know we   re in a 

world where b is true

cis419/519 spring    18

24

example:  conditional probabilities

p(alarm, burglary) =

burglary
  burglary

alarm
0.09
0.1

  alarm
0.01
0.8

p(burglary | alarm) 

= p(burglary     alarm) / p(alarm)
= 0.09 / 0.19 = 0.47

p(alarm | burglary) 

p(burglary     alarm)  
cis419/519 spring    18

= p(burglary     alarm) / p(burglary)
= 0.09 / 0.1 = 0.9
= p(burglary | alarm) p(alarm) 
= 0.47 * 0.19 = 0.09

25

(4) independence

    when two event do not affect each others    probabilities, we call 

them independent
    formal definition:

cis419/519 spring    18

26

exercise: independence

p(smart     study     prep)

prepared

  prepared

smart

  smart

study

0.432

0.048

  study

0.16

0.16

study

0.084

0.036

  study

0.008

0.072

is smart independent of study?

is prepared independent of study?

cis419/519 spring    18

27

exercise: independence

p(smart     study     prep)

prepared

  prepared

smart

  smart

study

0.432

0.048

  study

0.16

0.16

study

0.084

0.036

  study

0.008

0.072

is smart independent of study?

p(study     smart) = 0.432 + 0.048 = 0.48
p(study) = 0.432 + 0.048 + 0.084 + 0.036 = 0.6 
p(smart) = 0.432 + 0.048 + 0.16 + 0.16 = 0.8
p(study) x p(smart) = 0.6 x 0.8 = 0.48

is prepared independent of study?

cis419/519 spring    18

so yes!

28

conditional independence

    absolute independence of a and b:

conditional independence of a and b given c

    this lets us decompose the joint distribution:

    conditional independence is different than absolute independence, 

but still useful in decomposing the full joint
p(a,b|c) p(c) = 

    p(a,b,c) = [always] 

= [independence] p(a|c) p(b|c) p(c)

cis419/519 spring    18

29

exercise: independence

p(smart     study     prep)

prepared

  prepared

smart

  smart

study

0.432

0.048

  study

0.16

0.16

study

0.084

0.036

  study

0.008

0.072

is smart independent of study?

is prepared independent of study?

cis419/519 spring    18

30

take home exercise: 

conditional independence

p(smart     study     prep)

prepared

  prepared

smart

  smart

study

0.432

0.048

  study

0.16

0.16

study

0.084

0.036

  study

0.008

0.072

is smart conditionally independent of prepared, given study?

is study conditionally independent of prepared, given smart?

cis419/519 spring    18

31

summary: basic id203

    product rule:   p(a,b) = p(a|b)p(b) = p(b|a)p(a)
    if a and b are independent:   

    p(a,b) = p(a)p(b);   p(a|b)= p(a), p(a|b,c)=p(a|c)

    sum rule: p(a   b) = p(a)+p(b)-p(a,b)
    bayes rule: p(a|b)  = p(b|a) p(a)/p(b)
    total id203: 

   
    p(b) =     p(b , ai) =    i p(b|ai) p(ai)
    total id155: 

if events a1, a2,   an are mutually exclusive: ai    aj =   ,    i p(ai)= 1
if events a1, a2,   an are mutually exclusive: ai    aj =   ,     i p(ai)= 1

   
    p(b|c) =     p(b , ai|c) =    i p(b|ai,c) p(ai|c)                   

cis419/519 spring    18

32

summary: the monty hall problem

    suppose you're on a game show, and 
you're given the choice of three doors.
    behind one door is a car; behind the others, 

goats. 

    you pick a door, say no. 1, and the host, 

who knows what's behind the doors, opens 
another door, say no. 3, which has a goat.

    he then says to you, "do you want to switch 

to door no. 2?" 

    is it to your advantage to switch your 

choice?

    try to develop an argument using the 

concepts we discussed. 

cis419/519 spring    18

33

bayes    rule

    exactly the process we just used
    the most important formula in 
probabilistic machine learning

(super easy) derivation:

just set equal... 

and solve...

cis419/519 spring    18

bayes, thomas (1763) an essay towards 
solving a problem in the doctrine of 
chances. philosophical transactions of 
34
the royal society of london, 53:370-418

using bayes rule to gamble

the    win    envelope has a 
dollar and four beads in it

the    lose    envelope has 
three beads and no money

trivial question: someone draws an envelope at 
random and offers to sell it to you. 
how much should you pay?

cis419/519 spring    18

slide    andrew moore

35

using bayes rule to gamble

the    win    envelope has a 
dollar and four beads in it

the    lose    envelope has 
three beads and no money

a question: before deciding, you are allowed to see one 
bead drawn randomly from the chosen envelope.
- will that help?
- suppose it   s black:  how much should you pay? 

cis419/519 spring    18

slide    andrew moore

36

calculation   

    suppose it   s black:  how much should you pay?
    we know: 

    p(win) = 1/2 ; p(b | win) = 1/2 p(b | lose) = 2/3  

    p(win | b) = p(b | win) p(win)/p(b) = 
   
    p(lose | b) = p(b | lose) p(lose)/p(b)
   
    we can compute p(b):

= 1/2 x 1/2    = 0.25  

= 2/3 x 1/2    = 0.3333  

    1 = p(win | b) + p(lose | b) = 0.25   + 0.3333          = 1.714
   

(try a direct way of computing p(b))? 

    we get: 

    p(win | b)  = 0.4286 ; p(lose | b) = 0.5714

cis419/519 spring    18

based on example by andrew moore

   

37

bayes    rule for machine learning
    allows us to reason from evidence to hypotheses
    another way of thinking about bayes    rule:

cis419/519 spring    18

38

basics of bayesian learning
    goal: find the best hypothesis from some space h of 

hypotheses, given the observed data (evidence) d.

    define best to be: most probable hypothesis in h

    in order to do that, we need to assume a id203 

distribution over the class h.

    in addition, we need to know something about the relation 
between the data observed and the hypotheses (e.g., a coin 
problem.)

    as we will see, we will be bayesian about other things, e.g., the 

parameters of the model 

cis419/519 spring    18

39

basics of bayesian learning

    p(h) - the prior id203 of a hypothesis h

reflects background knowledge; before data is observed. if no 
information - uniform distribution.

    p(d) - the id203 that this sample of the data is observed. 

(no knowledge of the hypothesis)

    p(d|h): the id203 of observing the sample d, given that 

hypothesis h is the target

    p(h|d): the posterior id203 of  h. the id203 that h is 

the target, given that d has been observed. 

cis419/519 spring    18

40

id47

|p(h

d)

=

p(d

|

h)

p(d)
    p(h|d) increases with p(h) and with p(d|h)

p(h)

    p(h|d) decreases with p(d)

cis419/519 spring    18

41

learning scenario

    p(h|d)  = p(d|h) p(h)/p(d)

    the learner considers a set of candidate hypotheses h 

(models), and attempts to find the most probable one h    h, 
given the observed data.

    such maximally probable hypothesis is called maximum a 

posteriori hypothesis (map); id47 is used to 
compute it:

hmap = argmaxh    h p(h|d)  = argmaxh    h p(d|h) p(h)/p(d) 

= argmaxh    h p(d|h) p(h)

cis419/519 spring    18

42

learning scenario (2)

hmap = argmaxh    h p(h|d)  =  argmaxh    h p(d|h) p(h)

    we may assume that a priori,  hypotheses are equally 

probable:                 p(hi) = p(hj)     hi, hj 2 h

    we get the maximum likelihood hypothesis: 

hml = argmaxh    h p(d|h)

    here we just look for the hypothesis that best explains the 

data 

cis419/519 spring    18

43

examples

    hmap = argmaxh    h p(h|d)  =  argmaxh    h p(d|h) p(h)
    a given coin is either fair or has a 60% bias in favor of head.
    decide what is the bias of the coin [this is a learning problem!]

    two hypotheses:  h1: p(h)=0.5;   h2: p(h)=0.6

    prior: p(h): p(h1)=0.75   p(h2 )=0.25 
    now we need data. 1st experiment: coin toss is h.
    p(d|h):

p(d|h1)=0.5 ; p(d|h2) =0.6

    p(d):      

    p(h|d):

p(d)=p(d|h1)p(h1) +  p(d|h2)p(h2 ) 

=  0.5      0.75  +     0.6     0.25  = 0.525

p(h1|d) = p(d|h1)p(h1)/p(d) = 0.5   0.75/0.525 = 0.714
p(h2|d) = p(d|h2)p(h2)/p(d) = 0.6   0.25/0.525 = 0.286

cis419/519 spring    18

44

examples(2)

    hmap = argmaxh    h p(h|d)  =  argmaxh    h p(d|h) p(h)
    a given coin is either fair or has a 60% bias in favor of head.
    decide what is the bias of the coin [this is a learning problem!]

    two hypotheses:  h1: p(h)=0.5;   h2: p(h)=0.6

    prior: p(h): p(h1)=0.75   p(h2 )=0.25 

    after 1st coin toss is h we still think that the coin is more likely to be fair

    if we were to use maximum likelihood approach (i.e., assume equal priors) 

we would think otherwise. the data supports  the biased coin better.

    try: 100 coin tosses; 70 heads. 
    you will believe that the coin is biased.

cis419/519 spring    18

45

examples(2)

    hmap = argmaxh    h p(h|d)  =  argmaxh    h p(d|h) p(h)
    a given coin is either fair or has a 60% bias in favor of head.
    decide what is the bias of the coin [this is a learning problem!]

    two hypotheses:  h1: p(h)=0.5;   h2: p(h)=0.6

    prior: p(h): p(h1)=0.75   p(h2 )=0.25 
    case of  100 coin tosses; 70 heads. 

p(d) = p(d|h1) p(h1) + p(d|h2) p(h2) = 

= 0.5100   0.75 + 0.670   0.430   0.25 = 
= 7.9   10-31   0.75 + 3.4   10-28   0.25

0.0057 = p(h1|d) = p(d|h1) p(h1)/p(d) << p(d|h2) p(h2) /p(d) = p(h2|d) =0.9943

cis419/519 spring    18

46

example: a model of language

    model 1: there are 5 characters, a, b, c, d, e, and space
    at any point can generate any of them, according to:

   

p(a)= p1;   p(b) =p2;   p(c) =p3;   p(d)= p4;   p(e)= p5 p(sp)= p6 
      pi = 1 
this is a family of distributions; learning is identifying a member of this family.
e.g., p(a)= 0.3;   p(b) =0.1;   p(c) =0.2;   p(d)= 0.2;   p(e)= 0.1    p(sp)=0.1

    we assume a generative model of independent characters (fixed k): 
p(u) = p(x1, x2,   , xk)=   i=1,k p(xi| xi+1, xi+2,   , xk)=   i=1,k p(xi)

the parameters of the model are the character generation probabilities (unigram).

   
    goal: to determine which of two strings u, v is more likely.
   

the bayesian way: compute the id203 of each string, and decide which is 
more likely.

consider strings: aabbc & abbba

learning here is: learning the parameters of a known model family

you observe a string; use it to learn the language model. 
e.g., s= aabbabc;                      compute p(a)

47

   
    how?

cis419/519 spring    18

1. the model we assumed is binomial. you could assume a different model! 
next we will consider other models and see how to learn their parameters.

maximum likelihood estimate
    assume that you toss a (p,1-p) coin m times and get k heads, 

m-k tails.  what is p?

    if p is the id203 of head, the id203 of the data 

2. in practice, smoothing is advisable     deriving the 
right smoothing can be done by assuming a prior. 

observed is:    

    the log likelihood:

p(d|p) = pk (1-p)m-k

l(p) = log p(d|p) = k log(p) + (m-k)log(1-p)

    to maximize, set the derivative w.r.t. p equal to 0:

dl(p)/dp = k/p     (m-k)/(1-p) 

    solving this for p, gives:      p=k/m

cis419/519 spring    18

48

id203 distributions

    bernoulli distribution:  

    random variable x takes values {0, 1} s.t  p(x=1) = p = 1     p(x=0)
   

(think of tossing a coin)

    binomial distribution: 

    random variable x takes values {1, 2,   , n} representing  the number of 

successes (x=1) in n bernoulli trials.

    p(x=k) = f(n, p, k) = cn
    note that if x ~ binom(n, p) and y ~ bernulli (p),    x =    i=1,n y
   

(think of multiple coin tosses) 

k pk (1-p)n-k

cis419/519 spring    18

49

id203 distributions(2)

    categorical distribution:  

    random variable x takes on values in {1,2,   k}  s.t p(x=i) = pi and     1
   

(think of a dice) 

k pi = 1

    multinomial distribution:

    let the random variables xi (i=1, 2,   , k) indicates the number of times 

outcome i was observed over the n trials. 

    the vector x = (x1, ..., xk) follows a multinomial distribution (n,p) where 

p = (p1, ..., pk) and    1
f(x1, x2,   xk, n, p) = p(x1= x1,     xk = xk) =
(think of n tosses of a k sided dice)

k pi = 1 

   

   

cis419/519 spring    18

50

how do we model it?

a multinomial bag of words

our eventual goal will be: given a document, 
predict whether it   s    good    or    bad   

    we are given a collection of documents written in a three word language {a, b, c}. all the 

documents have exactly n words (each word can be either a, b or c). 

    we are given a labeled document collection {d1, d2 ... , dm}. the label yiof document di is 

   

   
   

1 or 0, indicating whether di is    good    or    bad   .
this model uses the multinominal distribution. that is, ai (bi, ci, resp.) is the number of 
times word a (b, c, resp.) appears in document di. 
therefore:                        ai + bi + ci = |di| = n.
in this generative model, we have: 

p(di|y = 1) =n!/(ai! bi! ci!)         1
ai        1
bi        1
ai        0 bi        0
p(di|y = 0) =n!/(ai! bi! ci!)          0

where         1 (        1,         1 resp.) is the id203 that a (b , c) appears in a    good     document. 
    note that:         0 +        0+        0=        1 +        1+        1 =1
    (multinomial, with         i (        i,         i ) 

    now, we observe documents, and estimate these parameters. 
cis419/519 spring    18
    once we have the parameters, we can predict the corresponding label. 

unlike the discriminative case, the    game    here is different: 
    we make an assumption on how the data is being generated. 

similarly, 

51

   

ci

ci

a multinomial bag of words (2)

    we are given a collection of documents written in a three word language {a, b, c}. all the 

documents have exactly n words (each word can be either a, b or c). 

    we are given a labeled document collection {d1, d2 ... , dm}. the label yiof document di is 

1 or 0, indicating whether di is    good    or    bad   .

   

   

the classification problem: given a document d, determine if it is good or bad; that is, 
determine p(y|d). 

this can be determined via bayes rule: p(y|d)  = p(d|y) p(y)/p(d)

    but, we need to know the parameters of the model to compute that. 

cis419/519 spring    18

notice that this is an important trick to write down the 

a multinomial bag of words (3)
experiment is. the ith expression evaluates to p(di , yi)
(could be written as a sum with multiplicative yi but less convenient) 

joint id203 without knowing what the outcome of the 

   

then:

bi        1

examples are independent

likelihood of the observed data. 
pd =   i p(yi , di )  =    i p(di |yi ) p(yi)  = 

    how do we estimate the parameters?
    we derive the most likely value of the parameters defined above, by maximizing the log 
labeled data, assuming that the 

    we want to maximize it with respect to each of the parameters. we first compute log (pd) 

and then differentiate: 
log(pd) =   

    we denote by p(yi=1) =          the id203  that an example is    good    (yi=1; otherwise yi=0).     
ai        0 bi        0
      i p(y, di ) =   i [(         n!/(ai! bi! ci!)         1
ai        1

ci )yi   ((1 -        )  n!/(ai! bi! ci!)          0
[ log(        ) + c + ai log(        1) + bi log(        1) + ci log(        1)] +            
(1- yi) [log(1-        ) + c    + ai log(        0) + bi log(        0) + ci log(        0) ]
    dlogpd/         =    i [yi /         - (1-yi)/(1-        )] = 0        i (yi -        ) = 0                 =    i yi /m
independent:         0+        0+        0=        1+          1+        1 =1 and also ai+ bi + ci = |di| = n.

the same can be done for the other 6 parameters. however, notice that they are not 

cis419/519 spring    18

makes sense?

ci )1-yi]

i yi

53

   

   

other examples (id48s)

    we can do the same exercise we did before. 

    consider data over 5 characters, x=a, b, c, d, e,  and 2 states s=b, i

    data: {(x1 ,x2,   xm ,s1 ,s2,   sm)}1

think about chunking a sentence to phrases; b is the beginning of each phrase, i is inside 
a phrase. 
(or: think about being in one of two rooms, observing x in it, moving to the other)

n

   

   

p(xi |si), p(si+1 |si), p(s1)

    find the most likely parameters of the model:

    we generate characters according to:
    initial state prob: p(b)= 1; p(i)=0
    state transition prob:

    given an unlabeled example 

x = (x1, x2,   xm)

    use bayes rule to predict the label  l=(s1, s2,   sm):

    p(b   b)=0.8 p(b   i)=0.2
    p(i   b)=0.5 p(i   i)=0.5

p(b)=1
p(i) = 0

    output prob:
l* = argmaxl p(l|x) = argmaxl p(x|l) p(l)/p(x)

    p(a|b) = 0.25,p(b|b)=0.10, p(c|b)=0.10,   . 
    the only issue is computational: there are 2m possible 
    p(a|i) = 0.25,p(b,i)=0,   

0.8
0.8

b

0.2
0.2

0.5
0.5

p(x|b)

values of l

    this is an id48 model, but nothing was hidden; 
0.5

    can follow the generation process to get the observed sequence.
i
    next week, s1 ,s2,   sm will be hidden (?)

0.5

0.5

0.2

i

i

1

b
0.25
a

0.25

c

0.25
d

0.25

a

b
0.4

d

cis419/519 spring    18

54

0.5
0.5

i

p(x|i)

bayes optimal classifier

    how should we use the general formalism?
    what should h be?

    h can be a collection of functions. given the training data, 

choose an optimal function. then, given new data, evaluate 
the selected function on it.

    h can be a collection of possible predictions. given the data, 

try to directly choose the optimal prediction. 

    could be different!

cis419/519 spring    18

55

bayes optimal classifier

    the first formalism suggests to learn a good hypothesis and 

use it. 

    (id38, grammar learning, etc. are here)
|p(d

argmax

argmax

|p(h

d)

=

=

hh
   

hh
   

h

map

h)p(h)

    the second one suggests to directly choose a decision.[it/in]:
    this is the issue of    thresholding    vs. entertaining all options 

until the last minute. (computational issues) 

cis419/519 spring    18

56

bayes optimal classifier: example

    assume a space of 3 hypotheses:

    p(h1|d) = 0.4; p(h2|d) = 0.3; p(h3|d) = 0.3    hmap = h1

    given a new instance x, assume that

    h1(x) = 1                h2(x) = 0                 h3(x) = 0

    in this case, 

    p(f(x) =1 ) = 0.4   ; p(f(x) = 0) = 0.6    but    hmap (x) =1

    we want to determine the most probable classification by 

combining the prediction of all hypotheses, weighted by 
their posterior probabilities

    think about it as deferring your commitment     don   t 
commit to the most likely hypothesis until you have to 
make a decision (this has computational consequences) 

cis419/519 spring    18

57

bayes optimal classifier: example(2)

j

let v be a set of possible classifications
   
 d)
|
=

   
)p(hd,h | p(v
p(v
i
    bayes optimal classification: 
 v
=

argmax

argmax

  d)
=

 d)
=

p(v

   

hh
   
i

|

|

i

j

j

vv
   
j

hh
   
i

h | p(v

j

i

)p(h

i

|

d)

       

hh
i

vv
   
j

h | p(v

j

i

)p(h

i

|

d)

    in the example: 
|p(1
|p(0

=        
=        

 d)
 d)

h | p(1
i
h | p(0

hh
i

hh
i

)p(h
i
)p(h

i

i

1 d)
|
   =
|
0 d)
   =

0.4
0.4

0 
   +
1 
   +

0.3
0.3

0 
   +
1 
   +

0.3
0.3

=

=

0.4
0.6

    and the optimal prediction is indeed 0.
    the key example of using a    bayes optimal classifier    is 

that of the na  ve bayes algorithm.

cis419/519 spring    18

58

bayesian classifier

f:x   v,  finite set of values
instances x   x can be described as a collection of features 

x = (x1, x2,     xn)    xi    {0,1} 

given an example, assign it the most probable value in v 
bayes rule:  
v

 
argmax
=

argmax
 
=

x,x|

p(v

p(v

,...,

x)

|

1

2

map

vv
   
j

)x
n

vv
   
j

j
j
p(x1,x2,...,xn | v j)p(vj)

vmap =  argmax v j    v
        =  argmax v j    vp(x1,x2,...,xn | v j)p(vj)
  
notational convention: p(y) means p(y=y)

p(x1,x2,...,xn )

 

cis419/519 spring   18

59

bayesian classifier
vmap = argmaxv p(x1, x2,    , xn | v )p(v)

given training data we can estimate the two terms.

estimating  p(v) is easy. e.g., under the binomial distribution 
assumption, count the number of times v appears in the training data. 

however, it is not feasible to estimate p(x1, x2,    , xn | v )

in this case we have to estimate, for each target value,  the id203 
of each instance (most of which will not occur).

in order to use a bayesian classifiers in practice, we need to make 
assumptions that will allow us to estimate these quantities.

cis419/519 spring   18

60

naive bayes

vmap = argmaxv p(x1, x2,    , xn | v )p(v)

)p(x
)p(x

2

2

,...,
x|

3

v|x
n
,...,

)
j
v,x
n

j

j

)

p(x

,...,

v|x
n

j

)

3

j

,...,
x|
x|

)v|x
=
n
j
,...,
v,x
n
,...,
v,x
n

2

2

1

p(x
x,
1
2
    
p(x
=
    
p(x 
=
1
    
.......
=
p(x
    
=

1

x|

,...,

v,x
n

j

)p(x

2

2

x|

,...,

v,x
n

j

)

p(x

3

3

x |

4

,...,

v,x
n

j

)...

p(x

n

v|

)

j

assumption: feature values are independent given the target value

    
=

    =

n
1i

p(x

i

)v|
j

cis419/519 spring   18

61

naive bayes (2)

vmap = argmaxv p(x1, x2,    , xn | v )p(v)

assumption: feature values are independent given the target 
value

p(x1 = b1, x2 = b2,   ,xn = bn | v = vj )            =1         p(xi = bi | v = vj )

generative model:
first choose a value vj    v                        according to p(v)
for each  vj :  choose  x1 x2,    , xn

according to p(xk |vj )

cis419/519 spring   18

62

naive bayes (3)
vmap = argmaxv p(x1, x2,    , xn | v )p(v)

assumption: feature values are independent given the target value

p(x1 = b1, x2 = b2,   ,xn = bn | v = vj ) =            =1         p(xi = bi | v = vj )

learning method: estimate n|v| + |v| parameters and use them to make 
a prediction.  (how to estimate?)

notice that this is learning without search. given a collection of training 
examples, you just compute the best hypothesis (given the assumptions). 

this is learning without trying to achieve consistency or even approximate 
consistency.
why does it work?

cis419/519 spring   18

63

conditional independence

    notice that the features values are conditionally independent 
given the target value, and are not required to be independent.

    example: the boolean features are x and y. 
we define the label to be  l = f(x,y)=x   y
over the product distribution:     p(x=0)=p(x=1)=1/2  and    p(y=0)=p(y=1)=1/2 
the distribution is defined so that x and y are independent:   p(x,y) = p(x)p(y)  

that is:   

x=0
   
   

(l  = 0)
(l = 0)

x=1
   
  

(l = 0)
(l = 1)

y=0
y=1

    but, given that  l =0:

p(x=1| l =0) = p(y=1| l =0) = 1/3

while:             p(x=1,y=1 | l =0) = 0
so x and y are not conditionally independent.

cis419/519 spring   18

64

conditional independence

    the other direction also does not hold. 

x and y can be conditionally independent but not independent.

example: we define a distribution s.t.:
l =0:   p(x=1| l =0) =1,  p(y=1| l =0) = 0
l =1:   p(x=1| l =1) =0,  p(y=1| l =1) = 1  
and assume, that:    p(l =0) = p(l =1)=1/2

x=0
0  (l= 0)
y=0
y=1    (l= 1)

x=1
     (l= 0)
0    (l= 1)

    given the value of l,      x and y are independent (check)
    what about unconditional independence ?
p(x=1) = p(x=1| l =0)p(l =0)+p(x=1| l =1)p(l =1) = 0.5+0=0.5 
p(y=1) = p(y=1| l =0)p(l =0)+p(y=1| l =1)p(l =1) = 0+0.5=0.5
but,
p(x=1, y=1)=p(x=1,y=1| l =0)p(l =0)+p(x=1,y=1| l =1)p(l =1) = 0 

so x and y are not independent.

cis419/519 spring   18

65

na  ve bayes example
v
)v|
j

argmax
 
=

)p(v
j

nb

   

p(x
i

vv
   
j

i

playtennis

day    outlook    temperature      humidity    wind
1       sunny            hot              high          weak            no
2       sunny            hot              high         strong           no
3       overcast        hot              high          weak            yes
4       rain              mild              high          weak            yes
5       rain              cool             normal       weak            yes
6       rain              cool             normal      strong           no
7       overcast        cool             normal      strong          yes 
8       sunny            mild             high          weak             no
9       sunny            cool             normal      weak            yes
10      rain              mild              normal      weak            yes 
11      sunny            mild              normal     strong           yes
12      overcast        mild              high         strong           yes
13      overcast         hot              normal     weak             yes
14      rain               mild              high        strong            no 

cis419/519 spring   18

66

estimating probabilities
argmax
 
observatio
|n
=

p(v)

{yes,
v
   

no}

v)

v

nb

    how do we estimate                                 ?
 v)

p(observat

|

i

i

p(x

    =
ion

cis419/519 spring   18

67

example
   
)p(v
j

vv
   
j

p(x
i

)v|
j

i

v

nb

argmax
 
=

    compute p(playtennis= yes);  p(playtennis= no)
    compute p(outlook= s/oc/r      | playtennis= yes/no) (6 numbers)
    compute p(temp= h/mild/cool | playtennis= yes/no) (6 numbers)
    compute p(humidity= hi/nor    | playtennis= yes/no) (4 numbers)
    compute p(wind= w/st            | playtennis= yes/no) (4 numbers)

cis419/519 spring   18

68

example
   
)p(v
j

vv
   
j

p(x
i

)v|
j

i

v

nb

argmax
 
=

    compute p(playtennis= yes);  p(playtennis= no)
    compute p(outlook= s/oc/r      | playtennis= yes/no) (6 numbers)
    compute p(temp= h/mild/cool | playtennis= yes/no) (6 numbers)
    compute p(humidity= hi/nor    | playtennis= yes/no) (4 numbers)
    compute p(wind= w/st            | playtennis= yes/no) (4 numbers)

   given a new instance:

(outlook=sunny;  temperature=cool; humidity=high; wind=strong)

    predict: playtennis= ?

cis419/519 spring   18

69

example
   
)p(v
j

vv
   
j

p(x
i

)v|
j

i

v

nb

argmax
 
=

   given: (outlook=sunny;  temperature=cool; humidity=high; wind=strong)

p(playtennis= yes)=9/14=0.64            p(playtennis= no)=5/14=0.36

p(outlook = sunny | yes)= 2/9         p(outlook = sunny | no)= 3/5 
p(temp = cool | yes)    = 3/9             p(temp = cool | no)  = 1/5
p(humidity = hi |yes)    = 3/9             p(humidity = hi | no)  =  4/5
p(wind = strong | yes)  = 3/9            p(wind = strong | no)= 3/5

p(yes,    ..) ~ 0.0053                          p(no,    ..) ~ 0.0206 

cis419/519 spring   18

70

example
   
)p(v
j

vv
   
j

p(x
i

)v|
j

i

v

nb

argmax
 
=

   given: (outlook=sunny;  temperature=cool; humidity=high; wind=strong)

p(playtennis= yes)=9/14=0.64            p(playtennis= no)=5/14=0.36

p(outlook = sunny | yes)= 2/9         p(outlook = sunny | no)= 3/5 
p(temp = cool | yes)    = 3/9             p(temp = cool | no)  = 1/5
p(humidity = hi |yes)    = 3/9             p(humidity = hi | no)  =  4/5
p(wind = strong | yes)  = 3/9            p(wind = strong | no)= 3/5

p(yes,    ..) ~ 0.0053                          p(no,    ..) ~ 0.0206 
p(no|instance) = 0.0206/(0.0053+0.0206)=0.795

what if we were asked about outlook=oc ?

cis419/519 spring   18

71

nb

v

estimating probabilities
 
argmax
word
|
=

    =
p(v)
|
 v)
    how do we estimate                          ?
    as we suggested before, we made a binomial assumption; then:
p(word
=

dislike}
{like,
v
   
p(word k

documents)

 #
(word

 v in 

  v)
=

p(x

appears
 
 (v#

training

 in 
documents)

v)

|

k

k

i

i

i

n
k
n

 

    sparsity of data is a problem
-- if       is small, the estimate is not accurate
-- if       is 0, it will dominate the estimate: we will never predict 

n 
kn 

v 
if a  word that never appeared in training (with    ) 
appears in the test data 

cis419/519 spring   18

v 

72

nb

v

robust estimation of probabilities

argmax
 
=

p(v)
    this process is called smoothing.
    there are many ways to do it, some better justified than others;
    an empirical issue.

word

    =

p(x

v
{like,
   

dislike}

v)

|

i

i

i

p(x

k

|

n  v)
=

mp
k
mn

+
+

 

here:  
    nk is # of occurrences of the word in the presence of  v
    n is # of occurrences of the label v
    p is a prior estimate of v (e.g., uniform)
    m is equivalent sample size (# of labels)

    is this a reasonable definition?

cis419/519 spring   18

73

robust estimation of probabilities
smoothing: 

p(x

k

|

n  v)
=

mp
k
mn

+
+

 

common values: 

laplace rule: for the boolean case, p=1/2 , m=2 

p(x

k

|

n  v)
=

1
+
k
2n
+

 

learn to classify text:     p = 1/(|values|)   (uniform)

m= |values|

cis419/519 spring   18

74

another comment on estimation

v

nb

argmax
 
=

)p(v
j

   

p(x
i

i

)v|
j

vv
   
j

    in practice, we will typically work in the log space:

                           = argmax                                                                                                     =
= argmax [ log                          +                                                                             ]

cis419/519 spring   18

76

na  ve bayes: two classes
v

argmax
 
=

p(x

p(v

)v|
j

nb

   
)

vv
   
j

j

i

i

    notice that the na  ve bayes method gives a method for predicting 
rather than an explicit classifier.
    in the case of two classes,  v   {0,1} we predict that v=1 iff:

p(v
p(v

j

j

=
=

1)
0)

   
   

   
   

n
1i
=
n
1i
=

p(x
p(x

i

i

v|
v|

j

j

=
=

1)
0)

>

1

cis419/519 spring   18

77

na  ve bayes: two classes
v

argmax
 
=

p(x

p(v

)v|
j

nb

   
)

vv
   
j

j

i

i

    notice that the na  ve bayes method gives a method for predicting 
rather than an explicit classifier.
    in the case of two classes,  v   {0,1} we predict that v=1 iff:

>

1

j

p(v
p(v

i

p 
:

denote

j
p(x
=
p(v
p(v
cis419/519 spring   18

          

         

j

j

   
   

=
=

1)
0)

   
   
v|1
=
=
i
   
1)
=
   
   
0)
=
   

p(x
p(x

n
1i
=
n
1i
=
q   
1),
n
x
p
1
i
=
n
q
1
i
=

x

i

i

i

i

j

v|
v|

=
=

1)
0)

i

i

j
p(x
i
x-1
)
)

x-1

i

i

=

i
p-(1
q-(1

=

v|1

=

0)

i

i

>

1

78

na  ve bayes: two classes
   in the case of two classes,  v   {0,1} we predict that v=1 iff:
p)(p-(1
i
p-1
q)(q-(1
i
q-1

)p-(1
i
)q-(1
i

   
   

n
1i
=
n
1i
=

1)
0)

   

   

p(v

p(v

p
q

=
=

n
1i
=

n
1i
=

   
   

0)

1)

=

=

=

x-1

x-1

   

   

x

x

j

j

i

i

i

i

j

i

i

i

i

j

 

p(v
p(v

>

1

x

i

)

x

i

)

i

i

cis419/519 spring   18

79

 

p(v
p(v

i

j

x

=
=

na  ve bayes: two classes
   in the case of two classes,  v   {0,1} we predict that v=1 iff:
p)(p-(1
i
p-1
q)(q-(1
i
q-1

n
1i
=
n
1i
=

   

   

1)
0)

   
   
j
take
logarithm;
 
 
 we 
p(v
   
p(v

)p-(1
i
)q-(1
i
predict 
p-1
q-1

1)
0)

0)
=
   
j
   1
iff
:
p
i
p-1

(log

p(v
v
=
   

   

log

i

q
i
q-1

i

)x

i

>

0

p(v

p
q

log

=
=

n
1i
=

n
1i
=

   
   

log

1)

+

+

=

=

x-1

x-1

   

x

j

j

j

i

i

i

i

i

i

i

i

i

i

i

>

1

x

i

)

x

i

)

i

i

cis419/519 spring   18

80

 

na  ve bayes: two classes
   in the case of two classes,  v   {0,1} we predict that v=1 iff:
p)(p-(1
i
p-1
q)(q-(1
i
q-1

n
1i
=
n
1i
=

   

   

p(v

=
=

n
1i
=

n
1i
=

1)

=

=

x-1

   

x

j

i

i

i

i

i

i

i

j

x

x-1

   
   

p
q

   
1)
p(v
)p-(1
i
   
0)
p(v
)q-(1
j
i
take
logarithm;
 
 
 we 
predict 
p(v
p-1
   
p(v
q-1

1)
0)

log

log

=
=

+

j

i

i

i

+

p(v
v
=
   

0)
   
=
j
   1
iff
:
p
i
p-1

(log

i

   

log

i

q
i
q-1

i

)x

i

>

0

j

i

>

1

x

i

)

x

i

)

i

i

    we get that naive bayes is a linear separator with 

wi = log pi
1- pi
if pi = qi then wi = 0 and the feature is irrelevant
  

    log qi
1- qi

= log pi
qi

1- qi
1- pi

cis419/519 spring   18

81

na  ve bayes: two classes

    in the case of two classes we have that:
   

log

=

j

i

=
=

)x  |1
)x  |0

p(v
p(v

    but since

j

xw
i

   

b

i

p(v

j

=

-1)x  |1

=

p(v

=

)x  |0

j

    we get: 

we have: 
a = 1-b; log(b/a) = -c. 
then:
exp(-c) = b/a = 
= (1-a)/a = 1/a     1 
= 1  + exp(-c) = 1/a 
a  = 1/(1+exp(-c))

p(v

j

=

)x  |1

=

1

+

exp(-

1
   

xw
i

i

+

b)

i

    which is simply the logistic function.

    the linearity of nb provides a better explanation for why it works.

cis419/519 spring   18

82

why does it work? 
|sx  {|
   
=
learning theory 
    probabilistic predictions are done via linear models 
    the low expressivity explains generalization + robustness
expressivity (1: methodology) 
    is there a reason to believe that these hypotheses minimize the empirical error?

errs

 /|} l

h(x)

|s|

(h)

   

    in general, no. (unless some probabilistic assumptions happen to hold).    

    but: if the hypothesis does not  fit the training data, 

    experimenters will augment set of features  (in fact, add dependencies)

    but now, this actually follows the learning theory protocol: 

    try to learn a hypothesis that is consistent with the data
    generalization will be a function of the low expressivity

expressivity (2: theory)
    (a) product distributions are    dense    in the space of all distributions. 

    consequently, the resulting predictor   s error is actually close to optimal classifier. 

    (b) nb classifiers cannot express all linear functions; but they converge faster to 

what they can express. 

cis419/519 spring   18

83

what   s next? 

(1) if probabilistic hypotheses are actually like other linear 
functions, can we interpret the outcome of other linear learning 
algorithms probabilistically?
    yes
(2) if probabilistic hypotheses are actually like other linear 
functions, can you train them similarly (that is, 
discriminatively)?
    yes.
    classification: logistics regression/max id178
    id48: can be learned as a linear model, e.g., with a version of 

id88 (structured models class)

cis419/519 spring   18

84

recall: na  ve bayes, two classes
in the case of two classes we have:
   

xw
i

log

b

   

=

j

i

i

p(v
p(v

j

=
=

)x  |1
)x  |0

but since

p(v
j
we get (plug in (2) in (1); some algebra): 

-1)x  |1

p(v

=

=

j

=

)x  |0

p(v

j

=

)x  |1

=

b)
which is simply the logistic (sigmoid) function used in the 
neural network representation.

exp(-

xw
i

1

+

+

i

i

1
   

cis419/519 spring   18

85

conditional probabilities
(1) if probabilistic hypotheses are actually like other 
linear functions, can we interpret the outcome of 
other linear learning algorithms probabilistically?
    yes
general recipe
    train a classifier f using your favorite algorithm (id88, 

id166, winnow, etc). then:

    use sigmoid1/1+exp{-(awtx + b)} to get an estimate for  p(y 

| x)

    a, b can be tuned using a held out that was not used for 

training.

    done in lbjava, for example

cis419/519 spring   18

86

id28

(2) if probabilistic hypotheses are actually like other linear 
functions, can you actually train them similarly (that is, 
discriminatively)?
the id28 model assumes the following model:

p(y= +/-1 | x,w)= [1+exp(-y(wtx + b)]-1

this is the same model we derived for na  ve bayes, only that 
now we will not assume any independence assumption. we 
will directly find the best w. 
therefore training will be more difficult. however, the weight 
vector derived will be more expressive.
    it can be shown that the na  ve bayes algorithm cannot represent all 

how?

linear threshold functions.

    on the other hand, nb converges to its performance faster. 

cis419/519 spring   18

87

id28 (2)

given the model:

p(y= +/-1 | x,w)= [1+exp(-y(wtx + b)]-1

the goal is to find the (w, b) that maximizes the log likelihood of 
the data: {x1,x2    xm}.
we are looking for (w,b) that minimizes the negative log-
likelihood

minw,b    1m log p(y= +/-1 | x,w)= minw,b    1m log[1+exp(-yi(wtxi + b)]

this optimization problem is called logistics regression

id28 is sometimes called the maximum id178 
model in the nlp community (since the resulting distribution is 
the one that has the largest id178 among all those that 
activate the same features).

cis419/519 spring   18

88

id28 (3)

using the standard mapping to linear separators through the 
origin, we would like to minimize: 
minw    1

m log p(y= +/-1 | x,w)= minw,    1

m log[1+exp(-yi(wtxi)]

to get good generalization, it is common to add a id173 
term, and the regularized logistics regression then becomes:

minw f(w) =    wtw + c    1
id173 term

where c is a user selected parameter that balances the two terms. 

empirical loss

m log[1+exp(-yi(wtxi)], 

cis419/519 spring   18

89

comments on discriminative learning

minw f(w) =    wtw + c    1
id173 term

m log[1+exp(-yiwtxi)], 

empirical loss

where c is a user selected parameter that balances the two terms. 

since the second term is the id168
therefore, regularized id28 can be  related to other learning 
methods, e.g., id166s. 
l1 id166 solves the following optimization problem:

minw f1(w) =    wtw + c    1

m max(0,1-yi(wtxi) 

l2 id166 solves the following optimization problem: 

minw f2(w) =    wtw + c    1

m (max(0,1-yiwtxi))2

cis419/519 spring   18

90

a few more nb examples

cis419/519 spring   18

91

example: learning to classify text

nb

v

argmax
 
=

      
    instance space x: text documents
    instances are labeled according to f(x)=like/dislike

p(v)

p(x

v)

vv

|

i

i

    goal: learn this function such that, given a new document
you can use it to decide if you like it or not

    how to represent the document ? 
    how to estimate the probabilities ? 
    how to classify?

cis419/519 spring   18

92

id194

    instance space x: text documents
    instances are labeled according to y = f(x) = like/dislike
    how to represent the document ? 
   
   

a document will be represented as a list of its words 
the representation question can be viewed as the generation question   

    we have a dictionary of n words  (therefore 2n parameters)
    we have documents of size n: can account for word position & count
    having a parameter for each word & position may be too much: 

    # of parameters: 2 x n x n (2 x 100 x 50,000 ~ 107) 

    simplifying assumption:

    the id203 of observing a word in a document is independent of its location
    this still allows us to think about two ways of generating the document

cis419/519 spring   18

93

classification via bayes rule (b)

    we want to compute 

argmaxy p(y|d)  = argmaxy p(d|y) p(y)/p(d) = 

= argmaxy p(d|y)p(y) 

    our assumptions will go into estimating p(d|y):
1. multivariate bernoulli

parameters:
1. priors: p(y=0/1) 

2.     wi    dictionary

p(wi =0/1 |y=0/1)

to generate a document, first decide if it   s good (y=1) or bad (y=0).

i.
ii. given that, consider your dictionary of words and choose w into your 

iii.

document with id203 p(w |y), irrespective of anything else. 
if the size of the dictionary is |v|=n, we can then write         

p(d|y) =   1         p(wi=1|y)bi p(wi=0|y)1   bi

p(w=1/0|y): the id203 that w appears/does-not in a y-labeled document. 
bi    {0,1} indicates whether word wi occurs in document d

    where: 

estimating p(wi =1|y) and p(y) is done in the ml way as before (counting).

    2n+2 parameters: 

cis419/519 spring   18

94

a multinomial model 

    we want to compute 

argmaxy p(y|d)  = argmaxy p(d|y) p(y)/p(d) = 

= argmaxy p(d|y)p(y) 

    our assumptions will go into estimating p(d|y):
2. multinomial

parameters:
1. priors: p(y=0/1) 

2.     wi    dictionary

p(wi =0/1 |y=0/1)
n dictionary items are  
chosen into d

to generate a document, first decide if it   s good (y=1) or bad (y=0).

i.
ii. given that, place n words into d, such that wi is placed with id203           

p(wi|y), and    i

n p(wi|y) =1.

iii. the id203 of a document is: 

p(d|y) =  n!/n1!...nk!  p(w1|y)n1   p(wk|y)nk

    where ni is the # of times wi appears in the document.
   

same # of parameters: 2n+2, where n = |dictionary|, but the estimation is 
done a bit differently. (hw).

cis419/519 spring   18

95

model representation
    the generative model in these two cases is different

label

documents d

appear

words w

label

documents d

appear (d)

position p

bernoulli: a binary variable corresponds 
to a document d and a dictionary word 
w, and it takes the value 1 if w appears in 
d. document topic/label is governed by a 
prior , its topic (label), and the variable in 
the intersection of the plates is governed 
by  and the bernoulli parameter  for the 
dictionary word w  

cis419/519 spring   18

multinomial: words do not correspond 
to dictionary words but to positions 
(occurrences) in the document d. the 
internal variable is then w(d,p). these 
variables are generated from the same 
multinomial distribution , and depend on 
the topic/label. 

96

general nb scenario

    we assume a mixture id203 model, parameterized by   .
    different components {c1,c2,    ck} of the model are parameterize by disjoint 
subsets of   .

the generative story: a document d is created by 
(1) selecting a component according to the priors, p(cj |  ), then 
(2) having the mixture component generate a document according to its 
own parameters, with distribution p(d|cj,   )
    so we have: 

p(d|  ) =    1k p(cj|  ) p(d|cj,  )

    in the case of document classification, we assume a one to one 
correspondence between components and labels.

cis419/519 spring   18

97

na  ve bayes: continuous features
    xi can be continuous
    we can still use 

    and

cis419/519 spring   18

98

na  ve bayes: continuous features
    xi can be continuous
    we can still use 

    and

    na  ve bayes classifier:

cis419/519 spring   18

99

na  ve bayes: continuous features
    xi can be continuous
    we can still use 

    and

    na  ve bayes classifier:

    assumption: p(xi|y) has a gaussian distribution  

cis419/519 spring   18

100

the gaussian id203 distribution
    gaussian id203 distribution also called normal distribution.
    it is a continuous distribution with pdf:

   = mean of distribution
  2 = variance of distribution
x is a continuous variable (-        x        )

)(
xp

=

2

(

)
x
  
   
2
2
  

   

e

1
2

    

    id203 of x being in the range [a, b] cannot be evaluated
analytically (has to be looked up in a table)

  

p(x) =

1

   2  

e

    (x      )2

2  2 gaussian

cis419/519 spring   18

x

101

na  ve bayes: continuous features
    p(xi|y) is gaussian

    training: estimate mean and standard deviation

note that the following slides abuse notation significantly. 
since p(x) =0 for continues distributions, we think of 
p (x=x| y=y), not as a classic id203 distribution, but 

just as a function f(x) = n(x,        ,         2).

f(x) behaves as a id203 distribution in the sense that 
8 x, f(x)    0 and the values add up to 1. also, note that 
f(x) satisfies bayes rule, that is, it is true that: 

fy(y|x = x) = fx (x|y = y) fy (y)/fx(x)

cis419/519 spring   18

102

na  ve bayes: continuous features
    p(xi|y) is gaussian

    training: estimate mean and standard deviation

x2

x3

x1
y
2         3         1         1
-1.2        2        .4         1
2       0.3        0         0
2.2      1.1        0         1     

cis419/519 spring   18

103

na  ve bayes: continuous features
    p(xi|y) is gaussian

    training: estimate mean and standard deviation

x2

x3

x1
y
2         3         1         1
-1.2        2        .4         1
2       0.3        0         0
2.2      1.1        0         1     

cis419/519 spring   18

104

recall: na  ve bayes, two classes

   in the case of two classes we have that:
   

log

=

=
=

)x  |1
)x  |0

p(v
p(v

i

   but since

xw

i    

i

b

p(v

=

-1)x  |1

=

p(v

=

)x  |0

   we get: 

p(v

=

)x  |1

=

1

+

exp(-

1
   

i

xw

i +

i

b)

    which is simply the logistic function (also used in the neural network
representation)
    the same formula can be written for continuous features

cis419/519 spring   18

105

logistic function: continuous features
    logistic function for gaussian features

note that we are 

using ratio of 

probabilities, since x 

is a continuous 

variable.

cis419/519 spring   18

106

