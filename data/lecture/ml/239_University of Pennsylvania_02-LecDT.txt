cis 519/419 

applied machine learning

www.seas.upenn.edu/~cis519

dan roth
danroth@seas.upenn.edu
http://www.cis.upenn.edu/~danroth/
461c, 3401 walnut

slides were created by dan roth (for cis519/419 at penn or cs446 at uiuc), eric eaton 
for cis519/419 at penn, or from other authors who have made their ml slides available. 

cis419/519 spring    18

introduction - summary

    we introduced the technical part of the class by giving two (very important) 

examples for learning approaches to linear discrimination.
there are many other solutions.

   
    question 1: our solution learns a linear function; in principle, the target 

function may not be linear, and this will have implications on the performance 
of our learned hypothesis. 
   

can we learn a function that is more flexible in terms of what it does with the 
feature space?

    question 2: can we say something about the quality of what we learn 

(sample complexity, time complexity; quality)

cis419/519 spring    18

2

id90

    earlier, we decoupled the generation of the feature space from 

the learning. 

    argued that we can map the given examples into another 
space, in which the target functions are linearly separable. 

    do we always want to do it? 
    how do we determine what are good mappings?

think about the badges problem

what   s the best learning algorithm? 

    the study of id90 may shed some light on this.
    learning is done directly from the given data representation.
    the algorithm ``transforms    the data itself.

cis419/519 spring    18

x2

x

3

this lecture
    id90 for (binary) classification

    non-linear classifiers

    learning id90 (  algorithm)

    greedy heuristic (based on information gain)

originally developed for discrete features

    some extensions to the basic algorithm

    overfitting

    some experimental issues

cis419/519 spring    18

4

representing data 

    think about a large table, n attributes, and assume you want to know 

something about the people represented as entries in this table.

    e.g. own an expensive car or not;
    simplest way: histogram on the first attribute     own
    then, histogram on first and second (own & gender)
    but, what if the # of attributes is larger: n=16
    how large are the 1-d histograms (contingency tables) ? 16 numbers
    how large are the 2-d histograms? 16-choose-2 = 120 numbers
    how many 3-d tables? 560 numbers
    with 100 attributes, the 3-d tables need 161,700 numbers

    we need to figure out a way to represent data in a better way, and 

figure out what  are the important attributes to look at first. 
id205 has something to say about it     we will use it to 
better represent the data. 

   

cis419/519 spring    18

5

id90

    a hierarchical data structure that represents data by 

implementing a divide and conquer strategy

    can be used as a non-parametric classification and 

regression method

    given a collection of examples, learn a decision tree that 

represents it.

    use this representation to classify new examples

b

c

cis419/519 spring    18

a

6

the representation

b

    id90 are classifiers for instances represented as 

feature vectors (color= ; shape= ; label= )
    nodes are tests for feature values
c
    there is one branch for each value of the feature
    leaves specify the category (labels)
    can categorize instances into multiple disjoint categories

(color= red ;shape=triangle)

a

    decision 

trees

evaluation of a 
decision tree

blue

shape

triangle

b
cis419/519 spring    18

square
a

color 

learning a 
decision tree

red
b

green

shape

circle

square

circle

b

c

a

7

expressivity of id90

    as boolean functions they can represent any boolean function.
    can be rewritten as rules in disjunctive normal form (dnf)

    green^square     positive
    blue^circle     positive
    blue^square     positive

    the disjunction of these rules is equivalent to the decision tree
    what did we show? what is the hypothesis space here?

   

2 dimensions, 3 values each    |x| = 9; |y| = 2; |h| = 29

color 

blue

shape

triangle

circle

red
+

green

shape

square

circle

    decision 

trees

-

cis419/519 spring    18

square
+

+

+

-

8

id90

    output is a discrete category. real valued outputs are 

possible (regression trees)

    there are efficient algorithms for processing large 

amounts of data (but not too many features)

    there are methods for handling noisy data (classification 

noise and attribute noise) and for handling missing 
attribute values

    decision 

trees

color 

blue

shape

triangle

circle

red
+

green

shape

square

circle

-

cis419/519 spring    18

square
+

+

+

-

9

decision boundaries

    usually, instances are represented as attribute-value pairs 

(color=blue, shape = square, +)

    numerical values can be used either by discretizing or by 

using thresholds for splitting nodes

    in this case, the tree divides the features space into axis-

parallel rectangles, each labeled with one of the labels

    decision 

trees

y
7

5

+

+ 

-

+

+

+

+

-

-

-

cis419/519 spring    18

1                        3                      x

no

y>7

no

x<3 

yes

no
++

yes

y<5

yes
x < 1
-
10

-

+

today   s key concepts

    learning id90 (  algorithm)
    greedy heuristic (based on information gain)

originally developed for discrete features

    overfitting

    what is it? how do we deal with it?

    some extensions of dts

how can this be avoided with linear classifiers?

    principles of experimental ml

cis419/519 spring    18

11

id90

    can represent any boolean function
    can be viewed as a way to compactly represent a lot of 

data.

    natural representation: (20 questions) 
    the evaluation of the decision tree classifier is easy

    clearly, given data, there are
many ways to represent it as 
a decision tree. 

    learning a good representation 

from data is the challenge.

outlook 

sunny
humidity

overcast rain
wind

yes

cis419/519 spring    18

high
no

normal
yes

strong
no

weak
yes

12

will i play tennis today? 

    features 
    outlook: 
    temperature:
    humidity:
    wind:

    labels

{sun, overcast, rain}
{hot, mild, cool}
{high, normal, low}
{strong, weak}

    binary classification task: y =  {+, -}

cis419/519 spring    18

13

will i play tennis today? 

c
c
c

h w play?
o t
h w
h
1 s
h
s
h
2 s
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
s
14 r m h

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

outlook:

s(unny), 
o(vercast), 
r(ainy)
temperature: h(ot), 

m(edium), 
c(ool)
h(igh),
n(ormal), 
l(ow)
s(trong), 
w(eak)

humidity:

wind:

cis419/519 spring    18

14

basic id90 learning algorithm

    data is processed in batch (i.e. all the data available)
    recursively build a decision tree top down.

algorithm?

c
c
c

h w play?
o t
h w
h
1 s
h
s
2 s
h
h w
3 o h
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
14 r m h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

cis419/519 spring    18

outlook 

sunny
humidity

overcast rain
wind

yes

high
no

normal
yes

strong
no

weak
yes

basic decision tree algorithm

    let  s be the set of examples

label  is the target attribute (the prediction) 
   
    attributes is the set of measured attributes

     (s, attributes, label)
if all examples are labeled the same return a single node tree with label

otherwise begin 
a =  attribute in attributes that best classifies s  (create a root node for tree)

for each possible value v of a

add a new tree branch corresponding to a=v
let sv be the subset of examples in s with a=v

if sv is empty:  add leaf node with the common value 

of label in s

else:  below this branch add the subtree

 (sv, attributes - {a}, label)

end
return root

cis419/519 spring    18

why? 
for evaluation time

16

picking the root attribute

    the goal is to have the resulting decision tree as small as 

possible (occam   s razor)
    but, finding the minimal decision tree consistent with the data is 

np-hard

    the recursive algorithm is a greedy heuristic search for a 

simple tree, but cannot guarantee optimality.

    the main decision in the algorithm is the selection of the 

next attribute to condition on.

cis419/519 spring    18

17

picking the root attribute

    consider data with two boolean attributes (a,b).

<  (a=0,b=0), - >:    50 examples
<  (a=0,b=1), - >:    50 examples
<  (a=1,b=0), - >:      0 examples
<  (a=1,b=1), + >: 100 examples

    what should be the first attribute we select?

    splitting on a: we get purely labeled nodes.
1
a

0
-
splitting on b: we don   t get purely labeled nodes.

0
-

b

+

1

a

1

0
-

+
cis419/519 spring    18

what if we have: <(a=1,b=0), - >: 3 examples?

18

picking the root attribute

    consider data with two boolean attributes (a,b).

<  (a=0,b=0), - >:    50 examples
<  (a=0,b=1), - >:    50 examples
<  (a=1,b=0), - >:      0 examples       3 examples
<  (a=1,b=1), + >: 100 examples

    what should be the first attribute we select?
    trees looks structurally similar; which attribute should we choose

1
a

b

0

1

0
-
53

+
100
cis419/519 spring    18

-
50

advantage a. but   
need a way to quantify things

1
b

1

+
100

a

0
-
3

0
-
100

19

picking the root attribute

    the goal is to have the resulting decision tree as small as 

possible (occam   s razor)

    the main decision in the algorithm is the selection of the 

next attribute to condition on.

    we want attributes that split the examples to sets that are 
relatively pure in one label; this way we are closer to a leaf 
node.

    the most popular heuristics is based on information gain, 

originated with the   system of quinlan.

cis419/519 spring    18

20

id178

a binary classification is:

    id178 (impurity, disorder) of a set of examples, s, relative to 

                                                                 =           +log        +               log           

    where p+ is the proportion of positive examples in s and

p-

   

is the proportion of  negatives.
if all the examples belong to the same category: id178 = 0 
if all the examples are equally mixed (0.5, 0.5): id178 = 1

   
    id178  = level of uncertainty. 

    in general, when pi is the fraction of examples labeled i:

                                                                        1,        2,   ,                 =      1                        log                

    id178 can be viewed as the number of bits required, on average, to  
encode the class of labels. if the id203 for + is 0.5, a single bit is 
required for each example; if it is 0.8 -- can use less then 1 bit.

cis419/519 spring    18

21

id178

    id178 (impurity, disorder) of a set of examples, s, 

relative to a binary classification is:

                                                                 =           +log        +               log           

    where p+ is the proportion of positive examples in s and      

is the proportion of  negatives.
if all the examples belong to the same category: id178 = 0 
if all the examples are equally mixed (0.5, 0.5): id178 = 1

p-

   

   
    id178  = level of uncertainty. 

1

1

1

-- +

cis419/519 spring    18

--

+

-- +

22

id178

    id178 (impurity, disorder) of a set of examples, s, 

relative to a binary classification is:

                                                                 =           +log        +               log           

    where p+ is the proportion of positive examples in s and      

is the proportion of  negatives.
if all the examples belong to the same category: id178 = 0 
if all the examples are equally mixed (0.5, 0.5): id178 = 1

p-

   

   
    id178  = level of uncertainty. 

1

1

1

cis419/519 spring    18

23

high id178     high level 
of uncertainty
low id178     no 
uncertainty. 

information gain

outlook 

sunny

overcast rain

    the information gain of an attribute a is the expected 

reduction in id178 caused by partitioning on this attribute

                                        ,         =                                                                                                                                   (        )|                |

|        |                                                         (                )

    where:

    sv is the subset of s for which attribute a has value v, and
   

the id178 of partitioning the data is calculated by weighing the 
id178 of each partition by its size relative to the original set

    partitions of low id178 (imbalanced splits) lead to high gain
    go back to check which of the a, b splits is better

cis419/519 spring    18

24

will i play tennis today? 

c
c
c

h w play?
o t
h w
h
1 s
h
s
h
2 s
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
s
14 r m h

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

outlook:

s(unny), 
o(vercast), 
r(ainy)
temperature: h(ot), 

m(edium), 
c(ool)
h(igh),
n(ormal), 
l(ow)
s(trong), 
w(eak)

humidity:

wind:

cis419/519 spring    18

25

will i play tennis today? 

c
c
c

t
h
h
h

h w
o
h w
1 s
h
s
2 s
h w
3 o
4 r m h w
5 r
n w
s
n
6 r
7 o
n
s
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o
14 r m h
s

h

c

current id178:
p  = 9/14
n = 5/14

h(y) = 
   (9/14) log2(9/14)  

   (5/14) log2(5/14) 
    0.94

play?

-
-
+
+
+
-
+
-
+
+
+
+
+
-

cis419/519 spring    18

26

information gain: outlook

c
c
c

h w play?
o t
h w
h
1 s
h
s
h
2 s
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
s
14 r m h

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

outlook = sunny: 

p = 2/5     n = 3/5

outlook = overcast:

p = 4/4     n = 0

outlook = rainy:

p = 3/5     n = 2/5

hs = 0.971

ho= 0

hr = 0.971

expected id178: 

(5/14)  0.971 + (4/14)  0 
+ (5/14)  0.971 = 0.694

information gain: 

0.940     0.694 = 0.246

cis419/519 spring    18

27

information gain: humidity

c
c
c

h w play?
o t
h w
h
1 s
h
s
h
2 s
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
s
14 r m h

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

humidity = high: 

p = 3/7     n = 4/7

humidity = normal:

p = 6/7     n = 1/7

hh = 0.985

ho= 0.592

expected id178: 

(7/14)  0.985 + (7/14)  0.592= 0.7785

information gain: 

0.940     0.151 = 0.1515

cis419/519 spring    18

28

which feature to split on? 

information gain: 

outlook:  0.246
humidity: 0.151
wind: 0.048
temperature: 0.029

    split on outlook

c
c
c

h w play?
o t
h w
h
1 s
h
s
h
2 s
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
s
14 r m h

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

cis419/519 spring    18

29

an illustrative example (iii)

outlook 

gain(s,humidity)=0.151
gain(s,wind) = 0.048
gain(s,temperature) = 0.029
gain(s,outlook) = 0.246

cis419/519 spring    18

30

an illustrative example (iii)

outlook 

sunny
1,2,8,9,11

2+,3-

?

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-
? 

c
c
c

h w play?
o t
h w
1 s
h
h
s
2 s
h
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
14 r m h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

cis419/519 spring    18

31

an illustrative example (iii)

outlook 

sunny
1,2,8,9,11

2+,3-

?

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-
? 

continue until:
    every attribute is included in path, or,
    all examples  in the leaf have same label

c
c
c

h w play?
o t
h w
1 s
h
h
s
2 s
h
3 o h
h w
4 r m h w
n w
5 r
6 r
n
s
7 o
s
n
8 s m h w
9 s
n w
10 r m n w
s
11 s m n
12 o m h
s
n w
13 o h
14 r m h
s

-
-
+
+
+
-
+
-
+
+
+
+
+
-

c

cis419/519 spring    18

32

an illustrative example (iv)

outlook 

gain(s sunny
,
humidity)
gain(s sunny
,
temp)
wind)
,
gain(s sunny

=
=

=

.97-(3/5) 0-(2/5) 0 = .97

.97- 0-(2/5) 1 = .57
.97-(2/5) 1 - (3/5) .92= .02

sunny
1,2,8,9,11

2+,3-

?

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-
? 

day    outlook  temperature      humidity    wind
1       sunny            hot              high          weak            no
2       sunny            hot              high         strong           no
8       sunny            mild             high          weak            no
9       sunny            cool            normal      weak           yes
11      sunny            mild             normal     strong          yes

playtennis

cis419/519 spring    18

33

an illustrative example (v)

outlook 

sunny
1,2,8,9,11

2+,3-

?

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-
? 

cis419/519 spring    18

34

an illustrative example (v)

outlook 

sunny
1,2,8,9,11

2+,3-
humidity

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-
? 

high
no

normal
yes

cis419/519 spring    18

35

    1. does s uniquely define a class? 

inducedecisiontree(s)

if all s     s have the same label y: return s;

i = argmax i gain(s, xi)

    2. find the feature with the most information gain:

    3. add children to s:

for k in values(xi): 

sk = {s     s | xi = k}

addchild(s, sk)
inducedecisiontree(sk)

return s;

cis419/519 spring    18

36

an illustrative example (vi)

outlook 

sunny
1,2,8,9,11

2+,3-
humidity

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-

wind

high
no

normal
yes

strong
no

weak
yes

cis419/519 spring    18

37

hypothesis space in decision tree 

induction

    conduct a search of the space of id90 which can 

represent all possible discrete functions. (pros and cons)

    goal: to find the best decision tree

    best could be    smallest depth   
    best could be    minimizing the expected number of tests   

    finding a minimal decision tree consistent with a set of 

data is np-hard.

    performs a greedy heuristic search:  hill climbing without 

backtracking

    makes statistically based decisions using all data

cis419/519 spring    18

38

history of decision tree research

    hunt and colleagues in psychology used full search decision tree 

methods to model human concept learning in the 60s

    quinlan developed  , with the information gain heuristics in the 

late 70s to learn id109 from examples

    breiman, freidman and colleagues in statistics developed cart 

(classification and regression trees simultaneously)

    a variety of improvements in the 80s: coping with noise, continuous 

attributes, missing data, non-axis parallel etc.

    quinlan   s updated algorithm, c4.5 (1993) is commonly used (new: c5)
    boosting (or id112) over dts is a very good general purpose 

algorithm

cis419/519 spring    18

39

example

    outlook = sunny, temp = hot,  humidity = normal,  wind = strong,  no

outlook 

sunny
1,2,8,9,11

2+,3-
humidity

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-

wind

high
no

normal
yes

strong
no

weak
yes

cis419/519 spring    18

40

overfitting - example

    outlook = sunny, temp = hot,  humidity = normal,  wind = strong,  no

outlook 

this can always be done    
may fit noise or other 
coincidental regularities

sunny
1,2,8,9,11

2+,3-
humidity

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-

wind

high
no

normal
wind

strong
no

weak
yes

cis419/519 spring    18

strong

no

weak
yes

41

our training data

cis419/519 spring    18

42

the instance space

cis419/519 spring    18

43

overfitting the data

    learning a tree that classifies the training data perfectly may not lead 

to the tree with the best generalization performance.
    there may be noise in the training data the tree is fitting
    the algorithm might be making decisions based on very little data

    a hypothesis h is said to overfit the training data if there is another 
hypothesis h   , such that h has a smaller error than h    on the training 
data but h has larger error on the test data than h   .

accuracy

on training

on testing

complexity of tree

cis419/519 spring    18

44

reasons for overfitting

    too much variance in the training data

    training data is not a representative sample 

of the instance space

    we split on features that are actually irrelevant

    too much noise in the training data

    noise = some feature values or class labels are incorrect
    we learn to predict the noise

    in both cases, it is a result of our will to minimize the 
empirical error when we learn, and the ability to do it 
(with dts) 

cis419/519 spring    18

45

pruning a decision tree

    prune = remove leaves and assign majority label of the 

parent to all items

    prune the children of s if:
    all children are leaves, and
   

the accuracy on the validation set does not decrease if we assign 
the most frequent class label to all items at s.

cis419/519 spring    18

46

avoiding overfitting

    two basic approaches

how can this be avoided with linear classifiers?

    pre-pruning: stop growing the tree at some point during construction 
when it is determined that there is not enough data to make reliable 
choices.

    post-pruning: grow the full tree and then remove nodes that seem not to 

have sufficient evidence.

    methods for evaluating subtrees to prune

    cross-validation: reserve hold-out set to evaluate utility
    statistical testing: test if the observed regularity can be dismissed as 

likely to occur by chance

    minimum description length: is the additional complexity of the 

hypothesis smaller than remembering the exceptions?

    this is related to the notion of id173 that we will see in other 

contexts     keep the hypothesis simple. 

hand waving, for now. 

next: a brief detour into explaining generalization and overfitting

cis419/519 spring    18

47

preventing overfitting

h1

h2

cis419/519 spring    18

48

the i.i.d. assumption

    training and test items are independently and identically 

distributed (i.i.d.): 
    there is a distribution p(x, y) from which the data d = 

{(x, y)} is generated.
    sometimes it   s useful to rewrite p(x, y) as p(x)p(y|x)

usually p(x, y) is unknown to us (we just know it exists)

    training and test data are samples drawn from the 

same p(x, y): they are identically distributed

    each (x, y) is drawn independently from p(x, y)

cis419/519 spring    18

52

overfitting

accuracy

why this shape 

of curves? 

on training data

on test data

size of tree

    a decision tree overfits the training data when its accuracy 

on the training data goes up but its accuracy on unseen data 
goes down

cis419/519 spring    18

53

overfitting

empirical 
error

??

model complexity

    empirical error (= on a given data set):

the percentage of items in this data set are misclassified by 
the classifier f.

cis419/519 spring    18

54

overfitting

empirical 
error

model complexity

    model complexity (informally):

how many parameters do we have to learn?

    id90: complexity = #nodes

cis419/519 spring    18

55

overfitting

expected
error

model complexity

    expected error:

what percentage of items drawn from p(x,y) do we expect to 
be misclassified by f? 

    (that   s what we really care about     generalization)

cis419/519 spring    18

56

variance of a learner (informally)

variance
model complexity
    how susceptible is the learner to minor changes in the training data? 

   

(i.e. to different samples from p(x, y))

    variance increases with model complexity 

    think about extreme cases: a hypothesis space with one function vs. all functions. 
    or, adding the    wind    feature in the dt earlier.
    the larger the hypothesis space is,  the more flexible the selection of the chosen 

hypothesis is as a function of the data. 

    more accurately: for each data set d, you will learn a different hypothesis h(d), that 

will have a different true error e(h); we are looking here at the variance of this 
random variable. 
57

cis419/519 spring    18

bias of a learner (informally)

bias

model complexity

    how likely is the learner to identify the target hypothesis? 
    bias is low when the model is expressive (low empirical error) 
    bias is high when the model is (too) simple 

    the larger the hypothesis space is,  the easiest it is to be close to the true 

hypothesis. 

    more accurately: for each data set d, you learn a different hypothesis h(d), that 
has a different true error e(h); we are looking here at the difference of the mean 
of this random variable from the true error. 

cis419/519 spring    18

58

impact of bias and variance

expected
error

variance
bias

model complexity

    expected error     bias + variance

cis419/519 spring    18

59

model complexity

expected
error

variance
bias

model complexity
complex models: 
high variance and low bias 

    simple models: 

high bias and low variance

cis419/519 spring    18

60

underfitting and overfitting

underfitting

overfitting

expected
error

variance
bias

    simple models: 

model complexity
complex models: 
high bias and low variance
high variance and low bias 
this can be made more accurate for some id168s. 
we will discuss a more precise and general theory that 
trades expressivity of models with empirical error

cis419/519 spring    18

61

avoiding overfitting

    two basic approaches

how can this be avoided with linear classifiers?

    pre-pruning: stop growing the tree at some point during construction 
when it is determined that there is not enough data to make reliable 
choices.

    post-pruning: grow the full tree and then remove nodes that seem not to 

have sufficient evidence.

    methods for evaluating subtrees to prune

    cross-validation: reserve hold-out set to evaluate utility
    statistical testing: test if the observed regularity can be dismissed as 

likely to occur by chance

    minimum description length: is the additional complexity of the 

hypothesis smaller than remembering the exceptions?

    this is related to the notion of id173 that we will see in other 

contexts     keep the hypothesis simple. 

hand waving, for now. 

next: a brief detour into explaining generalization and overfitting

cis419/519 spring    18

62

trees and rules
    id90 can be represented as rules

   

   

(outlook = sunny) and (humidity = normal) then yes
(outlook = rain) and (wind = strong) then no

    sometimes pruning can be done at the rules level

    rules are generalized by 

erasing a condition (different!)

outlook 

sunny
1,2,8,9,11

2+,3-
humidity

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-
wind

cis419/519 spring    18

high
no

normal
yes

strong
no

weak
yes
63

continuous attributes

    real-valued attributes can, in advance, be discretized into ranges, 

such as big, medium, small

    alternatively, one can develop splitting nodes based on thresholds of 

the form a<c that partition the data into examples that satisfy a<c
and a>=c. the information gain for these splits is calculated in the 
same way and compared to the information gain of discrete splits.

    how to find the split with the highest gain?
    for each continuous feature a:

    sort examples according to the value of a
    for each ordered pair (x,y) with different labels
    check the mid-point as a possible threshold, i.e.

sa    x, sa    y

cis419/519 spring    18

64

continuous attributes

   

length (l):  10  15  21  28  32  40  50 

example:  
   
    class:           -
    check thresholds:   l < 12.5;  l < 24.5;  l < 45
    subset of examples= {   },      split= k+,j-

+   +    - +    +    -

    how to find the split with the highest gain ?

   

for each continuous feature a:
   

sort examples according to the value of a
for each ordered pair (x,y) with different labels

   

   

check the mid-point as a possible threshold. i.e,          

sa    x, sa    y

cis419/519 spring    18

65

missing values

    diagnosis = < fever, blood_pressure,   , blood_test=?,   > 

    many times values are not available for all attributes 

during training or testing  (e.g., medical diagnosis)

    training: evaluate gain(s,a) where in some of the 

examples a value for a is not given 

cis419/519 spring    18

66

other suggestions?

outlook 

                                        ,         =                                                                       |                |

|        |                                                         (                )

missing values
temp)
,
gain(ssunny
gain(ssunny
,
humidity)

=

.97- 0-(2/5) 1 = .57      
=

sunny
1,2,8,9,11

2+,3-

?

overcast rain
3,7,12,134,5,6,10,14
4+,0-
yes

3+,2-
? 

   

fill in: assign the most likely value of xi to s:
argmax k p( xi = k ): normal
97-(3/5) ent[+0,-3] -(2/5) ent[+2,-0] = .97

   

    assign fractional counts p(xi =k) 

for each value of xi to s 

   

.97-(2.5/5) ent[+0,-2.5] - (2.5/5) ent[+2,-.5] < .97

day    outlook  temperature      humidity    wind

playtennis

1       sunny            hot              high          weak            no
2       sunny            hot              high          strong          no
8       sunny            mild             ??? 
weak             no
9       sunny            cool            normal     weak            yes
11      sunny            mild            normal     strong          yes

cis419/519 spring    18

missing values

    diagnosis = < fever, blood_pressure,   , blood_test=?,   > 

    many times values are not available for all attributes during training 

or testing  (e.g., medical diagnosis)

    training: evaluate gain(s,a) where in some of the examples a value 

for a is not given 

    testing:  classify an example without knowing the value of a

cis419/519 spring    18

68

missing values

normal/high

outlook = sunny, temp = hot,  humidity = ???,  wind = strong,  label = ??   

outlook = ???, temp = hot,  humidity = normal,  wind = strong,  label = ??

outlook 

1/3 yes + 1/3 yes +1/3 no = yes

other suggestions?

sunny

1,2,8,9,11

2+,3-
humidity

overcast
3,7,12,13
4+,0-
yes

rain

4,5,6,10,14

3+,2-

wind

high
no

normal
yes

strong
no

weak
yes

cis419/519 spring    18

69

other issues

    attributes with different costs 

    change information gain so that low cost attribute are preferred

    dealing with features with different # of values

    alternative measures for selecting attributes

    when different attributes have different number of values 

information gain tends to prefer those with many values

    oblique id90 

    decisions are not axis-parallel

    incremental id90 induction

    update an existing decision tree to account  for new examples 

incrementally  (maintain consistency?) 

cis419/519 spring    18

70

id90 as features

    rather than using id90 to represent the target function it is 

becoming common to use small id90 as features

    when learning over a large number of features, learning decision 

trees is difficult and the resulting tree may be very large  

    (over fitting)

    instead, learn small id90, with limited depth.
    treat them as    experts   ; they are correct, but only on a small region in 

the domain. (what dts to learn?  same every time?)

    then, learn another function, typically a linear function, over these as 

features. 

    boosting (but also other linear learners) are used on top of the small 

id90. (either boolean, or real valued features)

cis419/519 spring    18

71

experimental machine learning
    machine learning is an experimental field and we will 
spend some time (in problem sets) learning how to run 
experiments and evaluate results
    first hint: be organized; write scripts

    basics:

    split your data into two (or three) sets:

    training data (often 70-90%)
    test data (often 10-20%)
    development data (10-20%)

    you need to report performance on test data, but you are 

not allowed to look at it.
    you are allowed to look at the development data (and use it to 

tweak parameters)

cis419/519 spring    18

72

n-fold cross validation

    instead of a single test-training split:
test
    split data into n equal-sized parts 

train

    train and test n different classifiers
    report average accuracy and standard deviation of the 

accuracy

cis419/519 spring    18

73

evaluation: significance tests

    you have two different classifiers, a and b
    you train and test them on the same data set using n-fold 

cross-validation

    for the n-th fold: 

accuracy(a, n), accuracy(b, n)
pn = accuracy(a, n) - accuracy(b, n)

    is the difference between a and b   s accuracies significant?

cis419/519 spring    18

74

hypothesis testing

    you want to show that hypothesis h is true, based on your 

data  

   

(e.g.  h  =    classifier a and b are different   ) 

    define a null hypothesis h0

   

(h0 is the contrary of what you want to show)

    h0 defines a distribution p(m |h0) over some statistic

    e.g. a distribution over the difference in accuracy between a and 

b

    can you refute (reject) h0?

cis419/519 spring    18

75

rejecting h0

    h0 defines a distribution p(m |h0) over some statistic m

   

(e.g. m= the difference in accuracy between a and b)

    select a significance value s 

(e.g. 0.05, 0.01, etc.)

   
    you can only reject h0 if p(m |h0)     s

    compute the test statistic m from your data

    e.g. the average difference in accuracy over your n folds

    compute p(m |h0) 
    refute h0 with p     s if p(m |h0)     s

cis419/519 spring    18

76

paired t-test

    null hypothesis (h0; to be refuted): 

    there is no difference between a and b, i.e. the expected 

accuracies of a and b are the same 

    that is, the expected difference (over all possible data 

sets) between their accuracies is 0:

h0: e[pd]  = 0

    we don   t know the true e[pd]
    n-fold cross-validation gives us n samples of pd

cis419/519 spring    18

77

paired t-test

    null hypothesis h0: e[diffd]  =    = 0
    m: our estimate of    based on n samples of diffd

m = 1/n    n diffn
    the estimated variance s2:  

s2 = 1/(n-1)    1,n (diffn     m)2

    accept null hypothesis at significance level a if the     

following statistic lies in (-ta/2, n-1, +ta/2, n-1)

cis419/519 spring    18

78

id90 - summary

    hypothesis space: 

    variable size (contains all functions)
    deterministic;  discrete and continuous attributes 

    search algorithm

  - batch

   
    extensions: missing values

    issues:  

    what is the goal? 
    when to stop? how to guarantee good generalization?

    did not address: 

    how are we doing? (correctness-wise, complexity-wise)

cis419/519 spring    18

79

