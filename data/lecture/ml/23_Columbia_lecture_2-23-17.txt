coms 4721: machine learning for data science

lecture 11, 2/23/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

maximum margin classifiers

maximum margin idea

setting
linear classi   cation, two linearly separable classes.

recall id88

(cid:73) selects some hyperplane separating the classes.
(cid:73) selected hyperplane depends on several factors.

maximum margin
to achieve good generalization (low prediction error), place the hyperplane
   in the middle    between the two classes.

more precisely, choose a plane such that its distance to the closest point in
each class is maximized. this distance is called the margin.

generalization error

possible id88 solution

(dotted) poor generalization, (solid) better

maximum margin solution

example: gaussian data

(cid:73) intuitively, the classi   er on the left isn   t good because sampling more data

could lead to misclassi   cations.

(cid:73) if we imagine the data from each class as gaussian, we could frame the goal as
to    nd a decision boundary that cuts into as little id203 mass as possible.

(cid:73) with no distribution assumptions, we can argue that max-margin is best.

substituting convex sets

observation
where a separating hyperplane may be placed depends on the    outer    points
on the sets. points in the center do not matter.

in geometric terms, we can represent each class by the smallest convex set
which contains all point in the class. this is called a convex hull.

substituting convex sets

convex hulls
the thing to remember for this lecture is that a convex hull is de   ned by all
possible weighted averages of points in a set.

that is, let x1, . . . , xn be the above data coordinates. every point x0 in the
shaded region     i.e., the convex hull     can be reached by setting

n(cid:88)

n(cid:88)

x0 =

  ixi,   i     0,

  i = 1,

i=1

i=1

for some (  1, . . . ,   n). no point outside this region can be reached this way.

margin

de   nition
the margin of a classifying hyperplane h is the shortest distance between
the plane and any point in either set (equivalently, the convex hull)

when we maximize this margin, h is    exactly in the middle    of the two
convex hulls. of course, the dif   cult part is how do we    nd this h?

support vector machines

support vector machine

finding the hyperplane
for n linearly separable points (x1, y1), . . . , (xn, yn) with yi     {  1}, solve:

1

2(cid:107)w(cid:107)2
yi(xt

i w + w0)     1

for i = 1, . . . , n

min
w,w0
subject to

comments

(cid:73) recall that yi(xt
(cid:73) if there exists a hyperplane h that separates the classes, we can scale w

i w + w0) > 0 if yi = sign(xt

i w + w0).

so that yi(xt

i w + w0) > 1 for all i (this is useful later).

(cid:73) the resulting classi   er is called a support vector machine. this

formulation only has a solution when the classes are linearly separable.

(cid:73) it is not at all obvious why this maximizes the margin. this will

become more clear when we look at the solution.

support vector machine

skip to the end
q: first, can we intuitively say what the

solution should look like?

a: yes, but we won   t give the proof.

1. find the closest two points from the

convex hulls of class +1 and    1.

2. connect them with a line and put a perpendicular hyperplane in the middle.
3. if s1 and s0 are the sets of x in class +1 and    1 respectively, we   re looking

for two id203 vectors   1 and   0 such that we minimize

(cid:13)(cid:13)(cid:13) ((cid:80)
(cid:124)

(cid:123)(cid:122)

    ((cid:80)
(cid:124)

(cid:125)

(cid:123)(cid:122)

xi   s1

  1ixi)

xi   s0

  0ixi)

in conv. hull of s1

in conv. hull of s0

(cid:13)(cid:13)(cid:13)2

(cid:125)

4. then we de   ne the hyperplane using the two points found with   1 and   0.

primal and id78

primal problem
the primal optimization problem is the one we de   ned:

min
w,w0
subject to

1

2(cid:107)w(cid:107)2
yi(xt

i w + w0)     1

for i = 1, . . . , n

this is tricky, so we use lagrange multipliers to set up the    dual    problem.

lagrange multipliers
de   ne lagrange multipliers   i > 0 for i = 1, . . . , n. the lagrangian is

(cid:107)w(cid:107)2     n(cid:88)
(cid:107)w(cid:107)2     n(cid:88)

i=1

l =

=

1
2

1
2

  i(yi(xt

  iyi(xt

i w + w0)     1)
n(cid:88)

i w + w0) +

  i

i=1

i=1

we want to minimize l over w and w0 and maximize over (  1, . . . ,   n).

setting up the dual problem

first minimize over w and w0:

(cid:107)w(cid:107)2     n(cid:88)

l =

1
2

n(cid:88)

i=1

  i

  iyi(xt

i w + w0) +

i=1

   

   wl = w     n(cid:88)
=     n(cid:88)

i=1

   l
   w0

  iyixi = 0    

  iyi = 0    

i=1

i=1

n(cid:88)

i=1

  iyixi

  iyi = 0

w =

n(cid:88)

therefore,

1. we can plug the solution for w back into the problem.

2. we know that (  1, . . . ,   n) must satisfy(cid:80)n

i=1   iyi = 0.

id166 dual problem

lagrangian: l = 1

2(cid:107)w(cid:107)2    (cid:80)n

i w + w0) +(cid:80)n

i=1   i

i=1   iyi(xt

dual problem
plugging these values in from the previous slide, we get the dual problem

n(cid:88)

n(cid:88)

n(cid:88)

i=1

  i     1
2

  iyi = 0,

  i  jyiyj(xt

i xj)

i=1

j=1

  i     0

for i = 1, . . . , n

max
  1,...,  n

subject to

l =

n(cid:88)

i=1

comments

(cid:73) where did w0 go? the condition(cid:80)n

i=1   iyi = 0 gives 0    w0 in the dual.
(cid:73) we now maximize over the   i. this requires an algorithm that we won   t

discuss in class. many good software implementations are available.

after solving the dual

solving the primal problem
before discussing the solution of the dual, we ask:

after    nding each   i how do we predict a new y0 = sign(xt
i w + w0)     1)

0 w + w0) ?

we have: l = 1
with conditions:   i     0,
solve for w.

2(cid:107)w(cid:107)2    (cid:80)n
i=1   i(yi(xt
i w + w0)     1     0
n(cid:88)

yi(xt

   wl = 0     w =

  iyixi

(just plug in the learned   i   s)

i=1

what about w0?

(cid:73) we can show that at the solution,   i(yi(xt
(cid:73) therefore, pick i for which   i > 0 and solve yi(xt

i w + w0)     1) = 0 for all i.
i w + w0)     1 = 0 for
w0 using the solution for w (all possible i will give the same solution).

understanding the dual

dual problem
we can manipulate the dual problem to    nd out what it   s trying to do.

l =

max
  1,...,  n

subject to

n(cid:88)
since yi     {   1, +1}
(cid:73)

  iyi = 0     c =

i=1

n(cid:88)
n(cid:88)

i=1

n(cid:88)

i=1

j=1

n(cid:88)

n(cid:88)

n(cid:88)

i=1

  i     1
2

  iyi = 0,

  i  jyiyj(xt

i xj)

i=1

j=1

  i     0

for i = 1, . . . , n

  i =

(cid:88)
(cid:13)(cid:13)(cid:13) n(cid:88)

i   s1

i=1

  j

(cid:88)
(cid:13)(cid:13)(cid:13)2

j   s0

= c2(cid:13)(cid:13)(cid:13)(cid:88)

i   s1

xi    (cid:88)

j   s0

  i
c

(cid:13)(cid:13)(cid:13)2

  j
c

xj

(cid:73)

  i  jyiyj(xt

i xj) =

  iyixi

understanding the dual

dual problem
we can change notation to write the dual as

max
  1,...,  n

subject to

l = 2c     1
(cid:88)
2

c :=

  i =

i   s1

(cid:13)(cid:13)(cid:13)(cid:16)(cid:80)
(cid:124)

i   s1

(cid:123)(cid:122)

min
  1,...,  n

c xi
  i

c2(cid:13)(cid:13)(cid:13)(cid:88)
(cid:88)
   (cid:16)(cid:80)
(cid:124)

(cid:17)
(cid:125)

j   s0

xi    (cid:88)

(cid:13)(cid:13)(cid:13)2

  j
c

xj

  i
c

j   s0
i   s1
  j,   i     0

(cid:13)(cid:13)(cid:13)2

(cid:17)
(cid:125)

j   s0

(cid:123)(cid:122)

  j
c xj

in conv. hull of s1

in conv. hull of s0

we observe that the maximum of this function satis   es

therefore, the dual problem is trying to    nd the closest points in the convex
hulls constructed from data in class +1 and    1.

returning to the picture

recall
we wanted to    nd:

(cid:107)u     v(cid:107)2

min
u   h(s1)
v   h(s0)

the direction of w is u     v.

we previously claimed we can    nd the max-margin hyperplane as follows:

1. find shortest line connecting the convex hulls.
2. place hyperplane orthogonal to line and exactly at the midpoint.
with the id166 we want to minimize (cid:107)w(cid:107)2 and we can write this solution as

n(cid:88)

i=1

w =

  iyixi = c

      (cid:88)

i   s1

xi    (cid:88)

j   s0

  i
c

      

  j
c

xj

soft-margin id166

question: what if the data isn   t linearly separable?

answer: permit training data be on wrong side of hyperplane, but at a cost.

slack variables
replace the training rule yi(xt

i w + w0)     1 with

i w + w0)     1       i,

yi(xt
with   i     0.

the   i are called slack variables.

  >1  <1  =0  =0soft-margin id166

soft-margin objective function
adding the slack variables gives a new objective to optimize

min

w,w0,  1,...,  n

subject to

n(cid:88)

  i

(cid:107)w(cid:107)2 +   
i w + w0)     1       i

i=1

1
2
yi(xt
  i     0

for

i = 1, . . . , n

for

i = 1, . . . , n

we also have to choose the parameter    > 0. we solve the dual as before.
role of   

(cid:73) speci   es the    cost    of allowing a point on the wrong side.
(cid:73) if    is very small, we   re happy to misclassify.
(cid:73) for           , we recover the original id166 because we want   i = 0.
(cid:73) we can use cross-validation to choose it.

influence of margin parameter

   = 100000

   = 0.01

hyperplane is sensitive to   . either way, a linear classi   er isn   t ideal . . .

kernelizing the id166

primal problem with slack variables
let   s map the data into higher dimensions using the function   (xi),

min

w,w0,  1,...,  n

subject to

n(cid:88)

  i

1
(cid:107)w(cid:107)2 +   
2
yi(  (xi)tw + w0)     1       i
  i     0
i = 1, . . . , n

for

i=1

for

i = 1, . . . , n

dual problem
maximize over each (  i,   i) and minimize over w, w0,   1, . . . ,   n

n(cid:88)

  i     n(cid:88)

  i(yi(  (xi)tw + w0)     1 +   i)     n(cid:88)

l =

1
2

(cid:107)w(cid:107)2 +   

i=1

i=1

i=1

  i  i

subject to   i     0,   i     0,

yi(  (xi)tw + w0)     1 +   i     0

kernelizing the id166

dual problem
minimizing for w, w0 and each   i, we    nd

n(cid:88)

n(cid:88)

w =

  iyixi,

i=1

i=1

  iyi = 0,

         i       i = 0

if we plug w and   i =          i back into the l, we have the dual problem

l =

max
  1,...,  n

subject to (cid:80)n

n(cid:88)

i=1

  i     1
2

n(cid:88)

n(cid:88)

i=1

j=1

(cid:123)(cid:122)

(cid:125)

  i  jyiyj   (xi)t   (xj)
k(xi, xj)

(cid:124)

0       i       

i=1   iyi = 0,

classi   cation: using the solution w =(cid:80)n
(cid:17)

(cid:16) n(cid:88)

  iyi  (x0)t   (xi) + w0

y0 = sign

i=1   iyi  (xi), declare

(cid:16) n(cid:88)

= sign

  iyik(x0, xi) + w0

(cid:17)

i=1

i=1

kernelizing the id166

black solid line
id166 decision boundary

(cid:17)

classi   cation rule

(cid:16) n(cid:88)

sign

  iyik(x0, xi)+w0

i=1

dots
support vectors (  i > 0)

purple line
a bayes classi   er.

elementsofstatisticallearning(2nded.)c!hastie,tibshirani&friedman2009chap12id166 - degree-4 polynomial in feature space...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo                                             training error: 0.180test error:       0.245bayes error:    0.210id166 - radial kernel in feature space...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo                                                                                             training error: 0.160test error:       0.218bayes error:    0.210figure12.3.twononlinearid166sforthemix-turedata.theupperplotusesa4thdegreepolynomialsummary: support vector machine

basic id166

(cid:73) linear classi   er for linearly separable data.
(cid:73) position of af   ne hyperplane is determined to maximize the margin.
(cid:73) the dual is a convex, so we can    nd exact solution with optimization.

full-   edged id166

purpose

ingredient
maximum margin good generalization properties
slack variables
kernel

overlapping classes, robust against outliers
nonlinear decision boundary

use in practice

(cid:73) software packages (many options)
(cid:73) choose a id81 (e.g., rbf)
(cid:73) cross-validate    parameter and rbf kernel width

