cis 519/419 

applied machine learning

www.seas.upenn.edu/~cis519

dan roth
danroth@seas.upenn.edu
http://www.cis.upenn.edu/~danroth/
461c, 3401 walnut

slides were created by dan roth (for cis519/419 at penn or cs446 at uiuc), eric eaton 
for cis519/419 at penn, or from other authors who have made their ml slides available. 

cis419/519 spring    18

administration 

    registration    
    hw1 is due next week

    you should have started working on it already    

    hw2 will be out next week
    no lecture on tuesday next week (2/6)!!  

questions

cis419/519 spring    18

2

projects

    cis 519 students need to do a team project

    teams will be of size 2-3

    projects proposals are due on friday 3/2/18 

    details will be available on the website
    we will give comments and/or requests to modify / augment/ do a 

different project. 

    there may also be a mechanism for peer comments.

    please start thinking and working on the project now.

    your proposal is limited to 1-2 pages, but needs to include references 

and, ideally,  some preliminary results/ideas.

    any project with a significant machine learning component is good. 
    experimental work, theoretical work, a combination of both or a critical 

survey of results in some specialized topic. 

    the work has to include some reading of the literature . 
    originality is not mandatory but is encouraged. 
try to make it interesting! 

   

cis419/519 spring    18

3

project examples

    kdd cup 2013:

   

   

"author-paper identification": given an author and a small set of papers, we 
are asked to identify which papers are really written by the author. 

    https://www.kaggle.com/c/kdd-cup-2013-author-paper-identification-challenge

   author profiling   : given a set of document, profile the author: 
identification, gender, native language,    . 

    caption control: is it gibberish? spam? high quality text?

    adapt an nlp program to a new domain

    work on making learned hypothesis more comprehensible 

    explain the prediction

    develop a (multi-modal) people identifier  
    identify contradictions in news stories
    large scale id91 of documents + name the cluster

    e.g., cluster news documents and give a title to the document

    deep neural networks: convert a state of the art nlp program to a nn

cis419/519 spring    18

4

this lecture
    id90 for (binary) classification

    non-linear classifiers

    learning id90 (  algorithm)

    greedy heuristic (based on information gain)

originally developed for discrete features

    some extensions to the basic algorithm

    overfitting

    some experimental issues

cis419/519 spring    18

5

a guide

   

(stochastic) id119 (with lms)     

learning algorithms
   
    id90 
importance of hypothesis space (representation) 

   
    how are we doing? 

today: 
take a more general 
perspective and think 
more about learning, 
learning protocols, 
quantifying performance, 
etc. 
this will motivate some of 
the ideas we will see next. 

   

    quantification in terms of cumulative # of mistakes  
    our algorithms were driven by a different metric than the one we care about.
today: versions of id88
    how to deal better with large features spaces & sparsity?
    variations of id88
    dealing with overfitting

closing the loop: back to id119

   
    dual representations & kernels

    multilayer id88
    beyond binary classification? 

    multi-class classification and id170

    more general way to quantify learning performance (pac) 

    new algorithms (id166, boosting)

cis419/519 spring    18

6

quantifying performance

    we want to be able to say something rigorous about the 

performance of our learning algorithm.

    we will concentrate on discussing the number of 

examples one needs to see before we can say that our 
learned hypothesis is good. 

cis419/519 spring    18

7

learning conjunctions

    there is a hidden (monotone) conjunction the learner 

(you) is to learn 

f(x1, x2,   ,x100) = x2    x3    x4    x5     x100                                    

    how many examples are needed to learn it ?  how ?
    protocol i:  the learner proposes instances as queries to the 

teacher

    protocol ii:  the teacher (who knows f) provides training examples 
    protocol iii: some random source (e.g., nature) provides training 

examples; the teacher (nature) provides the labels (f(x))

cis419/519 spring    18

8

learning conjunctions (i)

    protocol i:  the learner proposes instances as queries to 

the teacher

    since we know we are after a monotone conjunction:
    is x100 in?   <(1,1,1   ,1,0), ?>   f(x)=0 (conclusion: yes)
in?   <(1,1,   1,0,1), ?>   f(x)=1 (conclusion: no)
    is x99
in ?  <(0,1,   1,1,1), ?>   f(x)=1 (conclusion: no)
    is x1

    a straight forward algorithm requires n=100 queries, and 
will produce as a result the hidden conjunction (exactly).
    h(x1, x2,   ,x100) = x2    x3    x4    x5     x100                                    

what happens here if the conjunction 
is not known to be monotone?
if we know of a positive example,
the same algorithm works. 

cis419/519 spring    18

9

learning conjunctions(ii)

    protocol ii:  the teacher (who knows f) provides training 

examples

cis419/519 spring    18

10

learning conjunctions (ii)

    protocol ii:  the teacher (who knows f) provides training 

examples

    <(0,1,1,1,1,0,   ,0,1), 1>

cis419/519 spring    18

11

learning conjunctions (ii)

    protocol ii:  the teacher (who knows f) provides training 

examples

    <(0,1,1,1,1,0,   ,0,1), 1> (we learned a superset of the good variables)

cis419/519 spring    18

12

learning conjunctions (ii)

    protocol ii:  the teacher (who knows f) provides training 

examples

    <(0,1,1,1,1,0,   ,0,1), 1> (we learned a superset of the good variables)
    to show you that all these variables are required   

cis419/519 spring    18

13

learning conjunctions (ii)

    protocol ii:  the teacher (who knows f) provides training 

examples

    <(0,1,1,1,1,0,   ,0,1), 1> (we learned a superset of the good variables)
    to show you that all these variables are required   

    <(0,0,1,1,1,0,   ,0,1), 0>   need x2
    <(0,1,0,1,1,0,   ,0,1), 0>   need x3
       ..
    <(0,1,1,1,1,0,   ,0,0), 0>   need x100

modeling teaching 
is tricky

    a straight forward algorithm requires k = 6 examples to 

produce the hidden conjunction (exactly).

h(x1, x2,   ,x100) = x2    x3    x4    x5     x100

cis419/519 spring    18

14

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples

    teacher (nature) provides the labels (f(x)) 

    <(1,1,1,1,1,1,   ,1,1), 1>
    <(1,1,1,0,0,0,   ,0,0), 0>
    <(1,1,1,1,1,0,...0,1,1), 1>
    <(1,0,1,1,1,0,...0,1,1), 0>
    <(1,1,1,1,1,0,...0,0,1), 1>
    <(1,0,1,0,0,0,...0,1,1), 0>
    <(1,1,1,1,1,1,   ,0,1), 1>
    <(0,1,0,1,0,0,...0,1,1), 0>
    how should we learn?
    skip

cis419/519 spring    18

15

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example

cis419/519 spring    18

16

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example

cis419/519 spring    18

17

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>

cis419/519 spring    18

18

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>         learned nothing:  h= x1    x2 ,   ,   x100 

    <(1,1,1,1,1,0,...0,1,1), 1>

cis419/519 spring    18

19

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>         learned nothing:  h= x1    x2 ,   ,   x100 

    <(1,1,1,1,1,0,...0,1,1), 1>       h= x1    x2    x3    x4    x5    x99   x100 

cis419/519 spring    18

20

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>         learned nothing
    <(1,1,1,1,1,0,...0,1,1), 1>     h= x1    x2    x3    x4    x5    x99   x100 
    <(1,0,1,1,0,0,...0,0,1), 0>       learned nothing
    <(1,1,1,1,1,0,...0,0,1), 1>

cis419/519 spring    18

21

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>         learned nothing
    <(1,1,1,1,1,0,...0,1,1), 1>       h= x1    x2    x3    x4    x5    x99   x100 
    <(1,0,1,1,0,0,...0,0,1), 0>       learned nothing
    <(1,1,1,1,1,0,...0,0,1), 1>       h= x1    x2    x3    x4    x5    x100 

cis419/519 spring    18

22

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>      learned nothing
    <(1,1,1,1,1,0,...0,1,1), 1>
    <(1,0,1,1,0,0,...0,0,1), 0>     learned nothing
    <(1,1,1,1,1,0,...0,0,1), 1>        h= x1    x2    x3    x4    x5    x100 
    <(1,0,1,0,0,0,...0,1,1), 0>
    <(1,1,1,1,1,1,   ,0,1), 1>
    <(0,1,0,1,0,0,...0,1,1), 0>

cis419/519 spring    18

23

learning conjunctions(iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

training examples
    teacher (nature) provides the labels (f(x)) 

    algorithm:  elimination 

is it  good

   
    performance ?
    # of examples ?

    start with the set of all literals as candidates
    eliminate a literal that is not active (0) in a positive example
    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>      learned nothing
    <(1,1,1,1,1,0,...0,1,1), 1>
    <(1,0,1,1,0,0,...0,0,1), 0>     learned nothing
    <(1,1,1,1,1,0,...0,0,1), 1>
    <(1,0,1,0,0,0,...0,1,1), 0>    final hypothesis: 
    <(1,1,1,1,1,1,   ,0,1), 1>        h= x1    x2    x3    x4    x5    x100 
    <(0,1,0,1,0,0,...0,1,1), 0>

cis419/519 spring    18

24

learning conjunctions (iii)

f= x2    x3    x4    x5     x100 

    protocol iii:  some random source (e.g., nature) provides 

    algorithm:        . 

training examples
    teacher (nature) provides the labels (f(x)) 

is it  good

   
    performance ?
    # of examples ?
    with the given data, we only learned an 

   approximation    to the true concept

    we don   t know how many examples we 

need to see to learn exactly. (do we care?)
    but we know that we can make a limited # 

    <(1,1,1,1,1,1,   ,1,1), 1>     
    <(1,1,1,0,0,0,   ,0,0), 0>      
    <(1,1,1,1,1,0,...0,1,1), 1>
    <(1,0,1,1,0,0,...0,0,1), 0>     
of mistakes. 
    <(1,1,1,1,1,0,...0,0,1), 1>
    <(1,0,1,0,0,0,...0,1,1), 0>   final hypothesis:
    <(1,1,1,1,1,1,   ,0,1), 1>     h= x1    x2    x3    x4    x5    x100 
    <(0,1,0,1,0,0,...0,1,1), 0>
    <(0,1,0,1,0,0,...0,1,1), 0>

cis419/519 spring    18

25

two directions

    can continue to analyze the probabilistic intuition:

    never saw x1=0 in positive examples, maybe we   ll never see it?
    and if we will, it will be with small id203, so the concepts we 

learn may be pretty good

    good: in terms of performance on future data
    pac framework

    mistake driven learning algorithms/on line algorithms
    now, we can only reason about #(mistakes), not #(examples)

    any relations?

    update your hypothesis only when you make mistakes

    not all on-line algorithms are mistake driven, so performance 

measure could be different.

cis419/519 spring    18

26

on-line learning

    new learning algorithms

(all learn a linear function over the feature space) 
    id88                   (+ many variations)
    general id119 view

    issues:

importance of representation

   
    complexity of learning
   
    more about features

idea of kernel based methods

cis419/519 spring    18

27

generic mistake bound algorithms

    is it clear that we can bound the number of mistakes ? 
    let c be a finite concept class. learn f 2 c
   

con:
in the ith stage of the algorithm:

   
    ci all concepts in c consistent with all i-1 previously seen examples
    choose randomly f 2 ci and use to predict the next example
    clearly, ci+1    ci and, if a mistake is made on the ith example, 

then |ci+1| < |ci|       so progress is made.

    the con algorithm makes at most |c|-1 mistakes
    can we do better ?

cis419/519 spring    18

28

the halving algorithm

    let c be a concept class. learn f 2 c
    algorithm:
    in the ith stage of the algorithm:

    ci all concepts in c consistent with all i-1 previously seen examples
    given an example et consider the value fj (et) for all fj 2  ci

and predict  by majority.

    predict 1 iff|{                                   ;                  (                )=0}|<|{                                   ;                  (                )=1}|
    clearly                    +1                   
example, then                 +1 < 1/2 |                |

    the halving algorithm makes at most log(|c|) mistakes

and if a mistake is made in the ith

    of course, this is a theoretical algorithm; can this ne achieved with an 

efficient algorithm?

cis419/519 spring    18

29

administration 

questions?

    hw1 is done    

    recall that this is an applied machine learning class. 
    we are not asking you to simply give us back what you   ve seen in class.
    the hw will try to simulate challenges you might face when you want 

to apply ml.

    allow you to experience various ml scenarios and make observations 

that are best experienced when you play with it yourself.  

    hw2 will be out tomorrow

    please start to work on it early. 
    this way, you will have a chance to ask questions in time.
    come to the recitations and to office hours.
    be organized     you will run a lot of experiments, but a good script can 

do a lot of the work.

    recitations

cis419/519 spring    18

30

projects

    cis 519 students need to do a team project

    teams will be of size 2-3

    projects proposals are due on friday 3/2/18 

    details will be available on the website
    we will give comments and/or requests to modify / augment/ do a 

different project. 

    there may also be a mechanism for peer comments.

    please start thinking and working on the project now.

    your proposal is limited to 1-2 pages, but needs to include references 

and, ideally,  some preliminary results/ideas.

    any project with a significant machine learning component is good. 
    experimental work, theoretical work, a combination of both or a critical 

survey of results in some specialized topic. 

    the work has to include some reading of the literature . 
    originality is not mandatory but is encouraged. 
try to make it interesting! 

   

cis419/519 spring    18

31

learning conjunctions

can this bound be 
achieved?

    there is a hidden conjunctions the learner is to learn

f(x1, x2,   ,x100) = x2    x3    x4    x5     x100                                    
can mistakes be 
bounded in the non-
finite case?

    log(|c|) = n
last time: 
    the elimination algorithm makes n mistakes
   
talked about various learning protocols & on algorithms for conjunctions. 
    discussed the performance of the algorithms in terms of bounding the 
number of mistakes that algorithm makes. 
    k-conjunctions:
    gave a    theoretical    algorithm with log|c| mistakes.

    the number of (all; not monotone) conjunctions: 3        
    the number of k-conjunctions:                  2        

log(|c|) = klog n
can we learn efficiently with this number of mistakes ? 

    assume that only k<<n attributes occur in the disjunction

    learn    ..

   

   

cis419/519 spring    18

32

representation

    assume that you want to learn conjunctions. should your hypothesis 

space be the class of conjunctions?

   

   

theorem:   given a sample on n attributes that is consistent with a conjunctive 
concept, it is np-hard to find a pure conjunctive hypothesis that is both consistent 
with the sample and has the minimum number of attributes. 
[david haussler, aij   88:    quantifying inductive bias: ai learning algorithms and valiant's learning framework   ] 

    same holds for disjunctions.
    intuition: reduction to minimum set cover problem.

    given a collection of sets that cover x, define a set of examples  so that learning 

the best (dis/conj)junction implies a minimal cover.

    consequently, we cannot learn the concept efficiently as a 

(dis/con)junction.

    but, we will see that we can do that, if we are willing to learn the 

concept as a linear threshold function.

    in a more expressive class, the search for a good hypothesis 

sometimes becomes combinatorially easier.

cis419/519 spring    18

so, there is a tradeoff!
(recall your dt results)

33

probabilistic classifiers as well

linear threshold functions  

    many functions are linear 

    conjunctions:

    y = x1    x3    x5                                        

f(x) = sgn {                           -   } = sgn{           =1                                          -    }
    y = sgn{1     x1 + 1     x3 + 1     x5 - 3};             w = (1, 0, 1, 0, 1)   =3
    y = sgn{1     x1 + 1     x3 + 1     x5 - 2} };           w = (1, 0, 1, 0, 1)   =2
    xor: y = (x1    x2 )   (           1      x2 )

    non trivial dnf: y = ( x1    x2 )    (  x3    x4 ) 

    y = at least 2 of {x1 ,x3, x5 }       

    at least m of n:

    many functions are not

    but can be made linear
    note: all the variables above are boolean variables

cis419/519 spring    18

34

wt x =   

-

-

-
-
-- -

-

w

--
-

-

- -

-

wt x = 0

cis419/519 spring    18

35

canonical representation

f(x) = sgn {                           -   } = sgn{           =1                                          -    }

    note: sgn {                           -   }  = sgn {                                 } 

    where: 

    x    = (x, -1)  and w    = (w,   ) 

    moved from an n dimensional representation to an (n+1) dimensional 

representation, but now can look for hyperplanes that go through the origin. 

    basically, that means that we learn both w and   
  

                            =   

0x

                                  = 0

1x

cis419/519 spring    18

1x

0x

36

id88 learning rule

    on-line, mistake driven algorithm.
    rosenblatt (1959) suggested that when a target output 
value is provided for a single neuron with fixed input, it 
can incrementally change weights and learn to produce 
the output using the id88 learning rule

    (id88 == linear threshold unit)

1x

6x

1
2
3
4
5
6

7

   

1w

6w

t

cis419/519 spring    18

y

37

id88 learning rule

    we learn f:x   {-1,+1} represented as f =sgn{wt   x)
    where x=  {0,1}n  or x= rn and w    rn
    given labeled examples:  {(x1, y1), (x2, y2),   (xm, ym)}

initialize w=0   

1.
2.   cycle through all examples  [multiple times]        

nr

a. predict the label of instance x to be y    = sgn{wt   x)
b. if y      y, update the weight vector: 

w = w + r y x
(r - a constant, learning rate)
otherwise, if y   =y, leave weights unchanged.

cis419/519 spring    18

38

id88 in action

x (with y = +1)
next item to be 

classified

wtx = 0
current 
decision 
boundary

w

current weight 

vector

(figures from bishop 2006)

cis419/519 spring    18

x as a vector

x as a vector added to 

w

w 

new weight 

vector

wtx = 0
new

decision 
boundary

positive
negative

39

id88 in action

wtx = 0
new

decision 
boundary

x (with y = +1)
next item to be 

classified

x as a vector

wtx = 0
current 
decision 
boundary

w

current weight 

vector

x as a vector added to 

w

w 

new weight 

vector

(figures from bishop 2006)

cis419/519 spring    18

positive
negative

40

id88 learning rule

    if x is boolean, only weights of active features are updated
    why is this important?

initialize w=0   

1.
2.   cycle through all examples          

nr

a. predict the label of instance x to be y    = sgn{wt   x)
b. if y      y, update the weight vector to 
w = w + r y x 
otherwise, if y   =y, leave weights unchanged.

(r - a constant, learning rate)

                            >0isequivalentto:                =+1         =

1+                                    >12
1

cis419/519 spring    18

41

id88 learnability
    obviously can   t learn what it can   t represent (???)

    only linearly separable functions

    minsky and papert (1969) wrote an influential book 

demonstrating id88   s representational limitations
    parity functions can   t be learned (xor)
   

in vision, if patterns are represented with local features, can   t 
represent symmetry, connectivity

    research on neural networks stopped for years

    id88

    rosenblatt himself (1959) asked,

       what pattern recognition problems can be transformed so as to 

become linearly separable?    

cis419/519 spring    18

42

(x1    x2) v (x3    x4)

y1    y2

cis419/519 spring    18

43

id88 convergence

    id88 convergence theorem:
    if there exist a set of weights that are consistent with the 
data (i.e., the data is linearly separable), the id88 
learning algorithm will converge
    how long would it take to converge ?

    id88 cycling theorem: 
    if the training data is not linearly separable the id88 
learning algorithm will eventually repeat the  same set of 
weights and therefore enter an infinite loop.
    how to provide robustness, more expressivity ? 

cis419/519 spring    18

44

id88

just to make sure we understand
that we learn both w and   

cis419/519 spring    18

45

id88: mistake bound theorem

    maintains a weight vector w   rn,    w0=(0,   ,0).
    upon receiving an example x     rn
    predicts according to the linear threshold function wt   x     0.

    theorem [novikoff,1963] let (x1; y1),   ,: (xt; yt), be a 

sequence of labeled examples with xi    < n, ||xi||   r and yi    {-
1,1} for all i. let u    < n,   > 0 be such that, ||u|| = 1 and 
yi ut     xi       for all i. 
complexity parameter
then id88 makes at most r2 /   2 mistakes on this 
example sequence.
(see additional notes)

cis419/519 spring    18

46

id88-mistake bound

proof: let vk be the hypothesis before the k-th mistake.  assume 
that the k-th mistake occurs on the input example (xi, yi).

assumptions
v1 = 0
||u|| = 1
yi ut     xi       

1. note that the bound does not 
depend on the dimensionality 
nor on the number of examples.

2. note that we place weight vectors
and examples in the same space.

3.      interpretation of the theorem

cis419/519 spring    18

k < r2 /   2

47

robustness to noise

    in the case of non-separable data , the extent to which a data point 
fails to have margin    via the hyperplane w can be quantified by a  
slack variable 

  i= max(0,        yi wtxi). 

    observe that when   i = 0, the example xi has margin at least   . 

otherwise, it grows linearly with     yi wt xi

    denote: d2 = [    {  i
    theorem: the id88 is 

2}]1/2

guaranteed to make no more than 
((r+d2)/  )2 mistakes on any sequence
of examples satisfying ||xi||2<r

    id88 is expected to 

have some robustness to noise. 

-
-
-
-
-- -

-

--
-

-

--

-

cis419/519 spring    18

48

id88 for boolean functions

    how many mistakes will the id88 algorithms make 

when learning a k-disjunction?

    try to figure out the bound 
    find a sequence of examples that will cause id88 to 

make o(n) mistakes on k-disjunction on n attributes. 

    (where is n coming from?)
    recall that halving suggested the possibility of a better 

bound     klog(n). 

    this can be achieved by winnow

    a multiplicative update algorithm [littlestone   88]
    see hw2

cis419/519 spring    18

49

practical issues and extensions

    there are many extensions that can be made to these basic algorithms.
    some are necessary for them to perform well

    id173 (next; will be motivated in the next section, colt)

    some are for ease of use and tuning

    converting the output of a id88/winnow to a id155

                =+1         =

1
1+                                           

    the parameter a can be tuned on a development set  

    multiclass classification (later)
    key efficiency issue: infinite attribute domain

   

sparse representation on the input

cis419/519 spring    18

50

id173 via averaged id88

    an averaged id88 algorithm is motivated by the following considerations:

   

   

   

   

in real life, we want more guarantees from our learning algorithm
in the mistake bound model:

    we don   t know when we will make the mistakes. 

every mistake-bound algorithm can be converted efficiently to a pac algorithm     to 
yield global guarantees on performance. 
in the pac model: 

    dependence is on number of examples seen and not number of mistakes.
    being consistent with more examples is better 
    which hypothesis will you choose   ??

   

to convert a given mistake bound algorithm (into a global guarantee algorithm):
    wait for a long stretch w/o mistakes  (there must be one)
    use the hypothesis at the end of this stretch.
   

its pac behavior is relative to the length of the stretch.

    averaged id88 returns a weighted average of a number of earlier 

hypotheses; the weights are a function of the length of no-mistakes 
stretch. 

cis419/519 spring    18

52

id173 via averaged id88

training: 

input: a labeled training set {(x1, y1),   (xm, ym)}

   
[m: #(examples); k: #(mistakes) = #(hypotheses); ci: consistency count for vi ]
   
   
   
   
    repeat t times:
    for i =1,   m:

output: a list of weighted id88s {(v1, c1),   ,(vk, ck)}

initialize: k=0; v1 = 0, c1 = 0

number of epochs t

   

    this can be done on top of any 
online mistake driven algorithm.
in hw two you will run it over 
three different algorithms.

    compute prediction y    = sgn(                         xi )

if y    = y,   then ck = ck + 1

   

else: vk+1 =  vk + yi x ; ck+1 = 1; k = k+1

    prediction:
   

given: a list of weighted id88s {(v1, c1),   (vk, ck)} ; a new example x
predict the label(x) as follows:

y(x)=  sgn [    1, k ci (                         x) ] 

averaged version of id88 
/winnow is as good as any other linear 
learning algorithm, if not better. 

53

cis419/519 spring    18

id88 with margin
    thick separator  (aka as id88 with margin)     

(applies both for id88 and winnow)

    promote if:
    wt x -    <   
    demote if:
    wt x -    >   

wt x = 0

wt x =   

-

-

-
-
-- -

-

--
-

-

- -

-

note:    is a functional margin. its effect could disappear as w grows.
nevertheless, this has been shown to be a very effective algorithmic addition.
(grove & roth 98,01; karov et. al 97) 

cis419/519 spring    18

54

other extensions 

    assume you made a mistake on example x.
    you then see example x again; will you make a mistake on it?
    threshold relative updating (aggressive id88)
    w     w + rx

            =                                    
|        |2

    equivalent to updating 
on the same example 
multiple times
cis419/519 spring    18

55

lbjava

    several of these extensions (and a couple more) are implemented in 
the lbjava learning architecture that supports several linear update 
rules (winnow, id88, na  ve bayes) 

    supports 

conversion to probabilities

    id173(averaged winnow/id88; thick separator)
   
    automatic parameter tuning 
true multi-class classification 
   
feature extraction and pruning 

   
    variable size examples 
    good support for large scale domains in terms of number of examples and number 

of features.

    very efficient 
    many other options 

    [download from: http://cogcomp.org/page/software/]

cis419/519 spring    18

56

general stochastic gradient algorithms 

the loss q: a function 

of x, w and y
    given examples {z=(x,y)}1, m from a distribution over xxy, we are trying 
to learn a linear function, parameterized by a weight vector w, so that 
we minimize the expected risk function

j(w) = ez q(z,w) ~=~ 1/m    1, m q(zi, wi)

    in stochastic id119 algorithms we approximate this 
minimization by incrementally updating the weight vector w as 
follows: 

wt+1 = wt     rt gw q(zt, wt) = wt     rt gt

where gt = gw q(zt, wt) is the gradient with respect to w at time t. 

    the difference between algorithms now amounts to choosing a 

different id168 q(z, w)

cis419/519 spring    18

57

general stochastic gradient algorithms 

learning rate

gradient

the loss q: a function of x, w and y

wt+1 = wt     rt gw q(xt, yt, wt) = wt     rt gt

lms: q((x, y), w) =1/2 (y     wt x)2
leads to the update rule (also called widrow   s adaline):

wt+1 = wt + r (yt                             xt) xt

here, even though we make binary predictions based on sgn (wt x) we 
do not take the sign of the dot-product into account in the loss.

another common id168 is:
hinge loss: 
q((x, y), w) = max(0, 1 - y wt x)
this leads to the id88 update rule:

if yi                            xi > 1   (no mistake, by a margin):       no update

(mistake, relative to margin):  wt+1 = wt + r yt xt

otherwise 

here g = -yx

cis419/519 spring    18

good to think about the 
case of boolean examples

wt x

58

new stochastic gradient algorithms 

wt+1 = wt     rt gw q(zt, wt) = wt     rt gt

(notice that this is a vector, each coordinate (feature) has its own wt,j and gt,j)

so far, we used fixed learning rates r = rt, but this can change. 
adagrad alters the update to adapt based on historical information
frequently occurring features in the gradients get small learning rates 
and infrequent features get higher ones. 
the idea is to    learn slowly    from frequent features but    pay attention    
to rare but informative features.

define a    per feature    learning rate for the feature j, as: 

rt,j = r/(gt,j)1/2

where gt,j =    k=1, t g2
until time t.
overall, the update rule for adagrad is:

k,j the sum of squares of gradients at feature j

wt+1,j = wt,j - gt,j r/(gt,j)1/2

easy to think about 

the case of 

id88, and on 
boolean examples. 

this algorithm is supposed to update weights faster than id88 
or lms when needed.

cis419/519 spring    18

59

id173

    the more general formalism adds a id173

term to the risk function, and minimize: 

j(w) =    1, m q(zi, wi) +    ri (wi)

    where r is used to enforce    simplicity    of the learned functions. 

    lms case: q((x, y), w) =(y     wt x)2

    r(w) = ||w||2
    r(w) = ||w||1 gives a problem called the lasso problem

2gives the optimization problem called ridge regression.

    hinge loss case:  q((x, y), w) = max(0, 1 - y wt x)

    r(w) = ||w||2

2 gives the problem called support vector machines

    logistics loss case:  q((x,y),w) = log (1+exp{-y wt x}) 
2 gives the problem called logistics regression

    r(w) = ||w||2

   

these are id76 problems and, in principle, the same gradient 
descent mechanism can be used in all cases. 

    we will see later why it makes sense to use the    size    of w as a way to control 

   simplicity   .

cis419/519 spring    18

60

algorithmic approaches

    focus:    two families of algorithms (one of the on-line 

representative) 
    additive update algorithms: id88

    id166 is a close relative of id88

    multiplicative update algorithms: winnow

    close relatives: boosting, max id178/id28

cis419/519 spring    18

61

which algorithm is better? 

how to compare? 

    generalization

    since we deal with linear learning algorithms, we know (???) that 

they will all converge eventually to a perfect representation. 

    all can represent the data
    so, how do we compare:

1. how many examples are needed to get to a given level of accuracy?
efficiency: how long does it take to learn a hypothesis and evaluate 
2.
it (per-example)? 
robustness (to noise);  
adaptation to a new domain,    .

3.
4.

    with (1) being the most fundamental question:

   

compare as a function of what? 
   

one key issue is the characteristics of the data

cis419/519 spring    18

62

sentence representation
s= i don   t know whether to laugh or cry

    define a set  of  features:

   

features are relations that  hold in the sentence

    map a sentence to its  feature-based representation

    the feature-based representation will give some of the 

information in the sentence

    use  this feature-based representation as an example to 

your algorithm

cis419/519 spring    18

63

sentence representation
s= i don   t know whether to laugh or cry

    define a set  of  features:

   

features are properties that  hold in the sentence

    conceptually, there are two steps in coming up with a 

feature-based representation
    what are  the information sources available? 

    sensors: words, order of words, properties (?) of words

    what features to construct based on these?

cis419/519 spring    18

why is this distinction needed?

64

embedding

whether

weather

xxx

21

3

   

xxx

41

3

cis419/519 spring    18

new discriminator in functionally simpler
xxx
   

y

y

y

   

   

3

2

5

1

5

4

65

domain characteristics

    the number of potential features is very large

    the instance space is sparse

    decisions depend on a small set of features: the function 

space is sparse

    want  to  learn  from a number of examples that is 

small  relative  to  the  dimensionality

cis419/519 spring    18

66

generalization

    dominated by the sparseness of the function space

    most features are irrelevant

    # of examples required by multiplicative algorithms 

depends mostly on # of relevant features
   

(generalization bounds depend on the target ||u|| )

    # of examples required by additive algorithms depends 

heavily on sparseness of features space: 
    advantage to  additive. generalization depend on input ||x||

    (kivinen/warmuth 95).

    nevertheless, today most people use additive algorithms.

cis419/519 spring    18

67

which algorithm to choose?

    generalization

the l1 norm: ||x||1 =    i|xi|              the l2 norm: ||x||2 =(   1

n|xi|2)1/2

the lp norm: ||x||p = (   1

n|xi|p )1/p

the l1 norm: ||x||1 = maxi|xi|

    multiplicative algorithms:

    bounds depend on ||u||, the separating hyperplane; i: example #)
    mw =2ln n ||u||1
    do not care much about data; advantage with sparse target u

2 /mini(u x(i))2

2 maxi||x(i)||1

    additive algorithms:

    bounds depend on ||x|| (kivinen / warmuth,    95)
    mp = ||u||2
    advantage with few active features per example

2/mini(u x(i))2

2 maxi||x(i)||2

cis419/519 spring    18

68

mw =2ln n ||u||1
mp = ||u||2

2 maxi||x(i)||2

2 maxi||x(i)||1

2/mini(u x(i))2

2 /mini(u x(i))2 

examples

    extreme scenario 1: assume the u has exactly k active features, and 
the other n-k are 0. that is, only k input features are relevant to the 
prediction. then:

||u||2, = k1/2  ; ||u||1, = k ; max ||x||2, = n1/2   ;; max ||x||1 , = 1

we get that: mp = kn;     mw = 2k2 ln n 
therefore, if k<<n, winnow behaves much better.

    extreme scenario 2: now assume that u=(1, 1,   .1) and the instances 

are very sparse, the rows of an nxn unit matrix. then:
||u||2, = n1/2  ; ||u||1, = n ; max ||x||2, = 1 ;; max ||x||1 , = 1

we get that: mp = n; mw = 2n2 ln n 
therefore, id88 has a better bound.

cis419/519 spring    18

69

mistakes bounds for 10 of 100 of n

`

function: at least 10 out of 
fixed 100 variables are active
dimensionality is n

id88,id166s

winnow

hw2

e
c
n
e
g
r
e
v
n
o
c
 
o
t
 
s
e
k
a
t
s
i

m

 
f
o
#

 

n: total # of variables (dimensionality)

cis419/519 spring    18

70

summary

    introduced multiple versions of on-line algorithms
    all turned out to be stochastic gradient algorithms

    for different id168s

    some turned out to be mistake driven
a term that minimizes error on 

    we suggested generic improvements via:

the training data

wt x =   
- - -
-
---- --- -- -
-

a term that forces 
simple hypothesis

    id173 via adding a term that forces a    simple hypothesis    

j(w) =    1, m q(zi, wi) +    ri (wi)

    id173 via the averaged trick

       stability    of a hypothesis is related to its ability to generalize

    an improved, adaptive, learning rate (adagrad)

    dependence on function space and the instance space properties. 
    today: 

    a way to deal with non-linear target functions (kernels)
    beginning of learning theory.

cis419/519 spring    18

71

efficiency

    dominated by the size of the feature space
    most features are functions (e.g. conjunctions) of raw 

attributes
,

(

1

,...

,
xxxx

(x)
    additive algorithms allow the use of kernels
    no need to explicitly generate complex features

      (
3

     

(x)
, 

, 
(x)

kx

)

2

1

3

2

... 

  
n

n
) 
        
(x)

>>

k

f(x)

   =

i

c
i

k(x,x
i

)

    could be more efficient since work is done in the original 
feature space, but expressivity is a function of the kernel 
expressivity.

cis419/519 spring    18

72

functions can be made linear
    data are not linearly separable in one dimension
    not separable if you insist on using a specific class of 

functions

x

cis419/519 spring    18

73

blown up feature space

    data are separable in <x, x2> space

x2

cis419/519 spring    18

74

x

making data linearly separable

f(x) = 1 iff  x1

2 + x2

2      1

cis419/519 spring    18

75

making data linearly separable

in order to deal with this, we 
introduce two new concepts: 

dual representation
kernel (& the kernel trick)

transform data: x = (x1, x2 )  => x    = (x1
f(x   ) = 1 iff  x   1 + x   2      1

2, x2

2 ) 

cis419/519 spring    18

76

examples x     {0,1}n ;  learned hypothesis w     rn

dual representation

f(x) = sgn {                           } = sgn{           =1                                          }

id88 update: 

if y      y, update:  w = w + ry x

    let w be an initial weight vector for id88. let (x1,+), (x2,+), (x3,-), (x4,-) be 

examples and assume mistakes are made on x1, x2 and x4. 

    what is the resulting weight vector? 

w = w + x1 + x2 - x4

    in general, the weight vector w can be written 

as a linear combination of examples: 

w =    1,m r   i yi xi

    where   i is the number of mistakes made on xi.
cis419/519 spring    18

note: we care about the dot 
product: f(x) = wt x =

= (   1,m r   i yi xi)t x            
=    1,m r   i yi (xit x) 

77

kernel based methods

f(x) = sgn {                           } = sgn{           =1                                          }

incurring the cost of keeping a very large weight vector. 

    a method to  run id88 on a very large feature set, without 

    computing the dot product can be done in the original feature space.
    notice: this pertains only to efficiency: the classifier is identical to the 

one you get by blowing up the feature space.

    generalization is still relative to the real dimensionality (or, related 

properties).

    kernels were popularized by id166s, but many other algorithms can 

make use of them (== run in the dual). 

   

linear kernels: no kernels; stay in the original space. a lot of applications  actually 
use linear kernels.

cis419/519 spring    18

78

kernel base methods

examples x     {0,1}n ;  learned hypothesis w     rn

f(x) = sgn {                           } = sgn{           =1                                          (x)}

    let i be the set t1,t2,t3    of monomials (conjunctions) over the 

feature space x1, x2    xn. 

    then we can write a linear function over this new feature space.

f(x) = sgn {                           } = sgn{                                            (x)}

(11010)

xx    1 
=
3

(11010)

xxx  0 
=
21

4

xxx  :
example
21
4
cis419/519 spring    18

4

1 
(11011)
=
79

examples x     {0,1}n ;  learned hypothesis w     rn

kernel based methods

f(x) = sgn {                           } = sgn{                                            (x)}

id88 update: 

if y      y, update:  w = w + ry x

    great increase in expressivity
    can run id88 (and winnow) but the convergence bound 

may suffer exponential growth.

    exponential number of monomials are true in each example. 
    also, will have to keep many weights.

cis419/519 spring    18

80

embedding

whether

weather

new discriminator in functionally simpler
xxx

y

y

y

   

   

3

2

5

1

5

4

xxx

21

3

   

xxx

41

3

   

cis419/519 spring    18

examples x     {0,1}n ;  learned hypothesis w     rn

the kernel trick(1)

f(x) = sgn {                           } = sgn{                                            (x)}

id88 update: 

if y      y, update:  w = w + ry x

    consider the value of w used in the prediction.
    each previous mistake, on example z, makes an additive 

contribution of +/-1 to some of the coordinates of w. 
    note: examples are boolean, so only coordinates of w that correspond 

to on terms in the example z  (ti(z) = 1) are being updated.

    the value of w is determined by the number and type of 

mistakes.  

cis419/519 spring    18

82

examples x     {0,1}n ;  learned hypothesis w     rn

the kernel trick(2)

f(x) = sgn {                           } = sgn{                                            (x)}

id88 update: 

if y      y, update:  w = w + ry x

    p     set of examples on which we promoted
    d     set of examples on which we demoted
    m = p [ d

f(x) = sgn                                            (x) =            [                      ,                        =11                         ,                        =11]                 (x) =

=           [                              (        )                                        (        )]

cis419/519 spring    18

83

84

the kernel trick(3)

    p     set of examples on which we promoted
    d     set of examples on which we demoted
    m = p [ d

f(x) = sgn                                            (x) =            [                      ,                        =11                         ,                        =11]                 (x) 

f(x) = sgn {                           } = sgn{                                            (x)}
=sgn{           [                                                                                      ]}
f(x) = sgn{                              (        )                   (        )                                        (        )}

    where s(z)=1 if z    p and s(z) = -1 if z    d. 
    reordering: 

cis419/519 spring    18

          

the kernel trick(4)
)xtw
f(x)
)(

(th  
=

         

i       
    s(y)=1 if y    p and s(y) = -1 if y    d. 
s(z)

(th  

      

f(x)

 
=

  

i

i

i

   

z
   

m

   

i
i
   

t

i

(

z)t
i

(

))x

    a mistake on z contributes the value +/-1 to all monomials 
satisfied by z. the total contribution of z to the sum is equal 
to the number of monomials that satisfy both x and z.

    define a dot product in the t-space: 
i   
t
 
z)
(
=
    we get the standard notation:

k(x,

      

i
i
   

z)t

i

(

)x

      

f(x)

 
=

(th  

       

z

m

s(z)k(x,

z))

cis419/519 spring    18

85

kernel based methods
      

s(z)k(x,

(th  

f(x)

z))

 
=

       

m

z

    what does this representation give us?

      

k(x,

z)

i   
 
t
=

i
i
   

(

z)t
i

(

)x

    we can view this kernel as the distance between x,z in the 

t-space. 

    but, k(x,z) can be measured in the  original space, without 

explicitly writing the t-representation of x, z 

cis419/519 spring    18

86

(

z)t
i

(

)x

x1x3 (001) = x1x3 (011) = 1             

x1 (001) =  x1 (011) = 1 ;    x3 (001) = x3 (011) = 1

   (001) =    (011) = 1

if any other variables appears in the monomial, 

it   s evaluation on x, z will be different.

kernel trick
       
s(z)k(x,
z))

z

m

 
=

f(x)

      

(th  

i   
 
t
=
i
i
   
    consider the space of all 3n monomials (allowing both 

k(x,

      

z)

positive and negative literals). then, 
=

    claim:
    when  same(x,z) is the number of features that have the 

=    
 

k(x,

      

same(x,

(z)t

(x)

z)

2

   ii

 z)

t

i

i

same value for both x and z. 

    we get: 

      

f(x)

 
=

(th  

       

z

m

s(z)(2

same(x,

z)

)

    example: take n=3; x=(001), z=(011), monomials of size 0,1,2,3
    proof: let k=same(x,z); construct a    surviving    monomials by: (1) 

choosing to include one of these k literals with the right polarity in the 
monomial, or (2) choosing to not include it at all. monomials with 
literals outside this set disappear. 

cis419/519 spring    18

87

      

f(x)

 
=

(th  

       

z

m

example 
s(z)k(x,
k(x,
      

z))

z)

i   
 
t
=

i
i
   

(

z)t
i

(

)x

    take x={x1, x2, x3, x4}
    i = the space of all 3n monomials; | i |= 81
    consider x=(1100), z=(1101)
    write down i(x), i(z), the representation of x, z in the i space.
    compute i(x)    i(z).
    show that 
    k(x,z) =i(x)     i(z) =       ti(z) ti(x) = 2same(x,z) = 8
    try to develop another kernel, e.g., where i is the space of 

all conjunctions of size 3 (exactly). 

cis419/519 spring    18

88

implementation: dual id88

      

 
f(x)
=
k(x,
      

       
(th  
z
m
i   
 
z)
t
(
=

z))

s(z)k(x,
z)t
i

)x

(

i
i
   

    simply run id88 in an on-line mode, but keep track 

of the set m.

    keeping the set m allows us to keep track of s(z).
    rather than remembering the weight vector w,    

remember the set m (p and d)     all those examples on 
which we made mistakes.

    dual representation

cis419/519 spring    18

89

example: polynomial kernel

    prediction with respect to a separating hyper planes (produced by 
id88, id166) can be computed as a function of dot products 
of  feature based representation of examples. 

    we want to define a dot product in a high dimensional space. 
    given two examples  x = (x1, x2,    xn) and y = (y1,y2,    yn) we want 
to map them to a high dimensional space [example- quadratic]: 

sq(2)

      (x1,x2,   ,xn) = (1, x1,   ,xn, x1
      (y1,y2,   ,yn) = (1, y1,   ,yn ,y1

2,   ,xn
2,   ,yn
and compute the dot product a  =   (x)t  (y)

2, x1x2,   ,xn-1xn) 
2, y1y2,   ,yn-1yn)

[takes time ]

    instead, in the original space, compute 

    b = k(x , y)= [1+ (x1,x2,    xn )t (y1,y2,    yn)]2

    theorem: a = b                              (coefficients do not really matter)

cis419/519 spring    18

90

kernels     general conditions

   

kernel trick: you want to work with degree 2 polynomial features,   (x). 
then, your dot product will be in a space of dimensionality n(n+1)/2. the 
kernel trick allows you to save and compute dot products in an n 
dimensional space. 

    can we use any k(.,.)? 

z))
    a function k(x,z) is a valid kernel if it corresponds to an inner product in some 

s(z)k(x,

       

(th  

      

f(x)

 
=

m

z

i
i
   

z)

      

k(x,

i   
 
t
=

(perhaps infinite dimensional) feature space. 
    take the quadratic kernel: k(x,z) = (xtz)2
    example: direct construction  (2 dimensional, for simplicity): 
    k(x,z) = (x1 z1 + x2 z2)2 = x12 z12 +2x1 z1 x2 z2 + x22 z22
   
   
   
    general condition: construct the kernel matrix {k(xi ,zj)}; check that it   s 

= (x12, sqrt{2} x1x2, x22) (z12, sqrt{2} z1z2, z22) t
=   (x)t    (z)     a dot product in an expanded space.

it is not necessary to explicitly show the feature function   .

positive semi definite.  

(

z)t
i

(

)x

we proved that k is a valid kernel by explicitly 
showing that it corresponds to a dot product. 

cis419/519 spring    18

91

polynomial kernels

    linear kernel: k(x, z) = xz

    polynomial kernel of degree d: k(x, z) = (xz)d

(only dth-order interactions) 

    polynomial kernel up to degree d: k(x, z) = (xz + c)d (c>0)

(all interactions of order d or lower)

cis419/519 spring    18

96

constructing new kernels

    you can construct new kernels k   (x, x   ) from 

existing ones:
    multiplying k(x, x   ) by a constant c:

k   (x, x   ) = ck(x, x   )

    multiplying k(x, x   ) by a function f applied to x and x   : 

k   (x, x   ) = f(x)k(x, x   )f(x   )

    applying a polynomial (with non-negative coefficients) to 

k(x, x   ): 
k   (x, x   ) = p( k(x, x   ) )  with p(z) =    i aizi and ai   0

    exponentiating k(x, x   ):
k   (x, x   ) = exp(k(x, x   ))

cis419/519 spring    18

97

constructing new kernels (2)
    you can construct k   (x, x   ) from k1(x, x   ), k2(x, x   ) by:

    adding k1(x, x   ) and k2(x, x   ):
k   (x, x   ) = k1(x, x   ) + k2(x, x   )

    multiplying k1(x, x   ) and k2(x, x   ):

k   (x, x   ) = k1(x, x   )k2(x, x   )

    also: 

if   (x)     rm and km(z, z   ) a valid kernel in rm, 

k(x, x   ) = km(  (x),   (x   )) is also a valid kernel

if a is a symmetric positive semi-definite matrix, 
k(x, x   ) = xax    is also a valid kernel

   

   

    in all cases, it is easy to prove these directly by construction. 

cis419/519 spring    18

98

gaussian kernel 

(aka radial basis function kernel)

    k(x, z) = exp(   (x     z)2/c   

(x     z)2: squared euclidean distance between x and z 

   
    c =   2: a free parameter 
    very small c: k     identity matrix  (every item is different) 
    very large c:  k     unit matrix  (all items are the same)

    k(x, z)     1 when x, z close
    k(x, z)     0 when x, z dissimilar 

cis419/519 spring    18

99

gaussian kernel

    k(x, z) = exp(   (x     z)2/c   
    is this a kernel?
    k(x, z)

= exp(   (x     z)2/2  2   
= exp(   (xx + zz     2xz)/2  2   
= exp(   xx/2  2   exp(xz/  2) exp(   zz/2  2   
= f(x) exp(xz/  2) f(z)  

   
   

   

exp(xz/  2)  is a valid kernel: 
xz is the linear kernel; 
we can multiply kernels by constants (1/  2) 
we can exponentiate kernels 

   
unlike the discrete kernels discussed earlier, here you cannot easily 
explicitly blow up the feature space to get an identical representation.

cis419/519 spring    18

100

summary     kernel based methods

      

f(x)

 
=

(th  

       

z

m

s(z)k(x,

z))

    a method to  run id88 on a very large feature set, 

without incurring the cost of keeping a very large weight vector. 

    computing the weight vector can be done in the original feature 

space.

    notice: this pertains only to efficiency: the classifier is identical 

to the one you get by blowing up the feature space.

    generalization is still relative to the real dimensionality (or, 

related properties).

    kernels were popularized by id166s but apply to a range of 

models, id88, gaussian models, pcas, etc. 

cis419/519 spring    18

102

explicit & implicit kernels: complexity
    is it always worthwhile to define kernels and work in 

the dual space? 

    computationally: 

    dual space     t1 m2 vs, primal space     t2 m
    where m is # of examples, t1, t2 are the sizes of the (dual, 

primal) feature spaces, respectively.

    typically, t1 << t2, so it boils down to the number of 

examples one needs to consider relative to the growth in 
dimensionality. 

    rule of thumb: a lot of examples     use primal space
    most applications today: people use explicit kernels. that is, 

they blow up the feature space explicitly. 

cis419/519 spring    18

104

kernels: generalization

    do we want to use the most expressive kernels we 

can? 
   

(e.g., when you want to add quadratic terms, do you really 
want to add all of them?)

    no; this is equivalent to working in a larger feature 

space, and will lead to overfitting. 

    it   s possible to give simple arguments that show that 

simply adding irrelevant features does not help. 

cis419/519 spring    18

105

conclusion- kernels

    the use of kernels to learn in the dual space is an important idea
    different kernels may expand/restrict the hypothesis space in useful ways.
    need to know the benefits and hazards

    to justify these methods we must embed in a space much larger 

than the training set size.
    can affect generalization

    expressive structures in the input data could give rise to specific 

kernels, designed to exploit these structures.
    e.g., people have developed kernels over parse trees: corresponds to 

features that are sub-trees.
it is always possible to trade these with explicitly generated features, but 
it might help one   s thinking about appropriate features. 

   

cis419/519 spring    18

107

functions can be made linear

    data are not linearly separable in one dimension
    not separable if you insist on using a specific class of 

functions

x

cis419/519 spring    18

108

blown up feature space
    data are separable in <x, x2> space

x2

cis419/519 spring    18

109

x

multi-layer neural network
    multi-layer network were designed to overcome the 

computational (expressivity) limitation  of a single 
threshold element. 

activation

    the idea is to stack several 

layers of threshold elements, 
each layer using the output of 
the previous layer as input.  

output

hidden

input

    multi-layer networks can represent arbitrary 

functions, but  building effective learning methods 
for such network was [thought to be] difficult. 

cis419/519 spring    18

110

basic units 

    linear unit:  multiple layers of linear functions  
oj = w   x produce linear functions.  we want to 
represent nonlinear functions.

activation

output

    need to do it in a way that 

facilitates learning

    threshold units:  oj = sgn(w   x) 
are not differentiable,  hence 
unsuitable for id119. 

    the key idea was to notice that the discontinuity of 

the threshold element can be represents by a smooth 
non-linear approximation: oj = [1+ exp{-w   x}]-1

(rumelhart, hinton, williiam, 1986), (linnainmaa, 1970) , see: http://people.idsia.ch/~juergen/who-
invented-id26.html )

   

cis419/519 spring    18

w2

ij
hidden
w1

ij

input

111

model neuron (logistic)
    us a non-linear, differentiable output function such 

1x

7x

1
2
3
4
5
6

as the sigmoid or logistic function
17w

t

7

   

67w
    net input to a unit is defined as: 
    output of a unit is defined as:

jo

jt
net        
xw
 

=

ij

j

i

o

j

=

cis419/519 spring    18

1
(net
   +
e1

   

)t
j

j

112

learning with a multi-layer  

id88

    it   s easy to learn the top layer     it   s just a linear unit. 
    given feedback (truth) at the top layer, and the activation at the 

layer below it, you can use the id88 update rule (more 
generally, id119) to updated these weights.

    the problem is what to do with 
the other set of weights     we do
not get feedback in the 
intermediate layer(s). 

cis419/519 spring    18

activation

output

w2

ij
hidden
w1

ij

input

113

learning with a multi-layer  id88

   

   

activation

the problem is what to do with 
the other set of weights     we do 
not get feedback in the 
intermediate layer(s). 
solution: if all the activation 
functions are differentiable, then 
the output of the network is also 
a differentiable function of the input and weights in the network.

output

w2

ij
hidden
w1

ij

input

    define an error function (multiple options) that is a differentiable function 
of the output, that this error function is also a differentiable function of the 
weights. 

    we can then evaluate the derivatives of the error with respect to the 

weights, and use these derivatives to find weight values that minimize this 
error function.  this can be done, for example, using id119 .  
this results in an algorithm called back-propagation.

   

cis419/519 spring    18

114

