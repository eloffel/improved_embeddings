cis 519/419 

applied machine learning

www.seas.upenn.edu/~cis519

dan roth
danroth@seas.upenn.edu
http://www.cis.upenn.edu/~danroth/
461c, 3401 walnut

slides were created by dan roth (for cis519/419 at penn or cs446 at uiuc), eric eaton 
for cis519/419 at penn, or from other authors who have made their ml slides available. 

cis419/519 spring    18

exams

mean: 62 (18.6 - 13.2 - 18.7 - 10.5)
std dev: 13.8 (2.5 - 6.7 - 4.4 - 5.8)
max: 94, min: 27.5

    1. overall:
   
   
   
    2. cis 519 (91 students):
   
   
   
    3. cis 419 (47 students):
   
   
   

mean: 63.6 (19 - 14 - 19 - 10)
std dev: 12 (2.2 - 5.9 - 4.1 - 5.8)
max: 93, min: 41

mean: 61.48 (18.4 - 12.8 - 18.5 - 10.75)
std dev: 14.7 (2.6 - 7.1 - 4.5 - 5.9)
max: 94   min: 27.5

   
solutions are available.
    midterms will be made 

available at the recitations, 
tuesday and wednesday. 
this will also be a good 
opportunity to ask the tas
questions about the grading.

   

questions?

cis419/519 spring    18

2

projects

    please start working!

    come to my office hours at least once in the next 3 weeks to discuss 

the project.

cis419/519 spring    18

3

colt approach to explaining 

learning

    no distributional assumption
    training distribution is the same as the test distribution

    generalization bounds depend

on this view and affects 
model selection.  
errd(h) < errtr(h)   +   

p(vc(h), log(1/  ),1/m)

    this is also called the 

   structural risk minimization    principle. 

cis419/519 spring    18

4

colt approach to explaining 

learning

    no distributional assumption
    training distribution is the same as the test distribution

    generalization bounds depend on this view and affect model 

selection.  

errd(h) < errtr(h)   +   p(vc(h), log(1/  ),1/m)

    as presented, the vc dimension is a combinatorial parameter that is 

associated with a class of functions. 

    we know that the class of linear functions has a lower vc dimension 

than the class of quadratic functions. 

    but, this notion can be refined to depend on a given data set, and this 

way directly affect the hypothesis chosen for a given data set.

cis419/519 spring    18

5

data dependent vc dimension

    so far we discussed vc dimension in the context of a fixed class of 

functions.  

    we can also parameterize the class of functions in interesting ways. 

    consider the class of linear functions, parameterized by their margin.  

note that this is a data dependent notion.

cis419/519 spring    18

6

linear classification

    let x = r2, y = {+1, -1}
    which of these classifiers would be likely to generalize 

better?

h1

h2

cis419/519 spring    18

7

vc and linear classification

    recall the vc based generalization bound:

err(h)    errtr(h) + poly{vc(h), 1/m, log(1/  )}

    here we get the same bound for both classifiers: 
    errtr (h1) = errtr (h2)= 0
    h1, h2 2 hlin(2), vc(hlin(2)) =  3

    how, then, can we explain our intuition that h2 should give better 

generalization than h1?

cis419/519 spring    18

8

linear classification

    although both classifiers separate the data, the distance 

with which the separation is achieved is different: 

h1

h2

cis419/519 spring    18

9

concept of margin

    the margin   i of a point xi    rn with respect to a linear 

classifier h(x) = sign(wt     x +b) is defined as the distance of 
xi from the hyperplane wt     x +b = 0:

  i = |(wt     xi +b)/||w||| 

    the margin of a set of points {x1,   xm} with respect to a 

hyperplane w, is defined as the margin of the point closest
to the hyperplane:

   = mini  i = mini|(wt     xi +b)/||w||| 

cis419/519 spring    18

10

vc and linear classification

    theorem: 

if h   is the space of all linear classifiers in rn that 
separate the training data with margin at least   , 
then: 

vc(h  )     min(r2/   2, n) +1,

    where r is the radius of the smallest sphere (in rn) that 

contains the data.

    thus, for such classifiers, we have a bound of the form: 

err(h)    errtr(h) + { (o(r2/  2) + log(4/  ))/m }1/2

cis419/519 spring    18

11

towards max margin classifiers 

    first observation:
    when we consider the class h   of linear hypotheses that separate a 

given data set with a margin   ,

    we see that 

    large margin        small vc dimension of h  

    consequently, our goal could be to find a separating hyperplane w  

that maximizes the margin of the set s of examples. 

    a second observation that drives an algorithmic approach is that:

small ||w||     large margin

    together, this leads to an algorithm: from among all those w   s that 

agree with the data, find the one with the minimal size ||w|| 
    but, if w separates the data, so does w/7   .
    we need to better understand the relations between w and the margin

cis419/519 spring    18

12

the distance between a point x and the hyperplane defined by  (w; b) is:   |wt x + b|/||w||

maximal margin

this discussion motivates the notion of a maximal margin.
the maximal margin of a data set s is define as:

a hypothesis (w,b) 
has many names

  (s) = max||w||=1 min(x,y)    s |y wt x|

1. for a given w: find the 

closest point.  

2. then, find the one the gives 
the maximal margin value across 
all w   s  (of size 1). 
note: the selection of the point  is in 
the min and therefore  the max does 
not change if we scale w, so it   s okay 
to only deal with normalized w   s. 

interpretation 1: among all w   s, 
choose the one that maximizes 

the margin. 

how does it help us to derive these h   s? 
argmax||w||=1 min(x,y)    s |y wt x|

cis419/519 spring    18

13

recap: margin and vc dimension
    theorem (vapnik): if h   is the space of all linear classifiers 
in rn that separate the training data with margin at least   , 
then

believe

    where r is the radius of the smallest sphere (in rn) that 

vc(h  )     r2/  2

contains the data.

approach.

    this is the first observation that will lead to an algorithmic 

we   ll 
prove

    the second observation is that: 

small ||w||     large margin

    consequently: the algorithm will be: from among all those

w   s that agree with the data, find the one with the 
minimal size ||w||

cis419/519 spring    18

14

from margin to ||w||

    we want to choose the hyperplane that achieves the largest margin. 

that is, given a data set s, find: 
    w* = argmax||w||=1 min(x,y)    s |y wt x|

interpretation 2: among all w   s 

that separate the data with 
margin 1, choose the one with 

minimal size. 

    how to find this w*?

    claim: define w0 to be the solution of the optimization problem:

w0 = argmin {||w||2 :     (x,y)    s, y wt x     1 }.

then:
w 0/||w0|| = argmax||w||=1 min(x,y)    s y wt x

that is, the id172 of w0 corresponds to the largest  margin 
separating hyperplane.  

cis419/519 spring    18

15

from margin to ||w||(2)
w0 = argmin {||w||2 :     (x,y)    s, y wt x     1 }    (**)
    claim: define w0 to be the solution of the optimization problem:

then:
w 0/||w0|| = argmax||w||=1 min(x,y)    s y wt x

that is, the id172 of w0 corresponds to the largest  margin 
separating hyperplane.  
    proof: define w    = w 0/||w0|| and let w* be the largest-margin 
separating hyperplane of size 1.  we need to show that w    = w*. 
note first that  w*/  (s) satisfies the constraints in (**); 
therefore:       ||w0||     ||w*/   (s)||  = 1/   (s) . 

    consequently:

    (x,y)    s   y w   t x  = 1/||w0|| y w0

def. of w   

but since ||w   || = 1 this implies that w    corresponds to the largest 
margin, that is w   = w*

prev. ineq.

def. of w0
t x     1/||w0||       (s)

def. of w0

cis419/519 spring    18

16

margin of a separating hyperplane

    a separating hyperplane: wt x+b = 0

assumption: data is linearly separable
let (x0 ,y0) be a point on wtx+b = 1
then its distance to the separating plane wt
x+b = 0 is: |wt x0  +b|/||w||= 1/||w||

distance between 
wt x+b = +1 and -1 is 2 / ||w||
what we did: 
1.

consider all possible w with 
different angles
scale w such that the 
constraints are tight
pick the one with largest 
margin/minimal size

2.

3.

wt xi +b   1   if  yi = 1
wt xi +b  
-1  if  yi = -1

=>                 (                                +        )   1

cis419/519 spring    18

wt x+b = 0
wt x+b = -1

17

cis419/519 spring    18

18

hard id166 optimization

    we have shown that the sought after weight vector w is 

the solution of the following optimization problem:

id166 optimization:  (***)
minimize:     ||w||2

subject to:    (x,y)    s:     y wt x     1 

    this is a quadratic optimization problem in (n+1) variables, 

with |s|=m inequality constraints.   

    it has a unique solution.

cis419/519 spring    18

19

maximal margin

the margin of a linear 
separator
wt x+b = 0 is 2 / ||w||

max 2 / ||w|| = min ||w|| 
= min    wtw

min        ,        
12                        
s.t  yi(wtxi+        )   1,                   ,                            

cis419/519 spring    18

20

support vector machines

    the name    support vector machine    stems from the fact 

that w* is supported by (i.e. is the linear span of) the 
examples that are exactly at a distance 1/||w*|| from the 
separating hyperplane. these vectors are therefore called 
support vectors. 

    theorem: let w* be the minimizer of
the id166 optimization problem (***)
for s = {(xi, yi)}.    let i= {i: w*tx = 1}. 
then there exists coefficients   i >0 such that:

w* =    i    i   i yi xi

cis419/519 spring    18

this representation 
should ring a bell   

21

duality

    this, and other properties of support vector machines are 

shown by moving to the dual problem.

    theorem: let w* be the minimizer of
the id166 optimization problem (***)
for s = {(xi, yi)}.   
let i= {i: yi (w*txi +b)= 1}. 
then there exists coefficients   i >0 
such that:

w* =    i    i   i yi xi

cis419/519 spring    18

22

(recap) kernel id88

examples
hypothesis

n

{0,1}
 x :
   
n'
 w:
r
;
   
f(x  
if

nonlinear

   ;
decision
function
 
(k)

mapping
x : 
 
   
sgn(
  
f(x)
 :
=
 yr  w
+   

    w,y)
   

t(x),
t(x)
r
 
   
    =
n'
t(xw
))
i
i
1i
(k)
(k)
 )
(x
t

(k)

n'

=

sgn(w

   

t(x))

    if n    is large, we cannot represent w explicitly. however, the weight vector w

m

can be written as a linear combination of examples: 
)

    where                 is the number of mistakes made on         (        )
    then we can compute f(x) based on {        (        )} and         

yr
  
j

 w 
=

   

t(x

1j
=

(j)

(j)

(j)

(j)

m

f(x)

  
=

sgn(w

   

t(x))

  
=

sgn(

t(x

yr
  
j

   

1j
=

)

   

t(x))

=

sgn(

cis419/519 spring    18

m

   

1j
=

(j)

yr
k  
j

(

(j)
xx
,

))

23

(recap) kernel id88

n'

n

r

x : 
  
=

nonlinear

{0,1}
 x :
   
n'
 w:
r
;
   

examples
hypothesis

 
t(x)
t(x),
   
   
sgn(w
t(x))
   

   ;
decision
function
 

 
mapping
 :
f(x)

    in the training phase, we initialize          to be an all-zeros vector.
    for training sample (        (        ),        (        )), instead of using the original id88 
update rule in the                     space
we maintain         by
based on the relationship between w and         :

  
then
    
k

    w,y)
   

 yr w

yr
   k
j

+   

+   

(j)
xx
,

f(x if

sgn(

f(x  

   

 y
   

  )
=

(x

   

if

 )

))

(k)

(k)

(k)

(k)

1j
=

1

(k)

(k)

(k)

(j)

(

m

t

(j)

(j)

m

k

t(x

)

 w 
=

yr
  
j

   

1j
=

cis419/519 spring    18

24

    similar to id88, we can augment vectors to handle the bias term

    then consider the following formulation 

footnote about the threshold

                      ,1;                        ,        
so that                               =                        +        
12                               s.t yi           t          i   1,                   ,                    s
12                        +12        2 s.t yi(        txi+        )   1,                   ,                    s

the bias term is included in the id173. 

equivalent to

min           
min        ,        

    however, this formulation is slightly different from (***), because it is 

this usually doesn   t matter

for simplicity, we ignore the bias term

cis419/519 spring    18

25

key issues

    computational issues

    training of an id166 used to be is very time consuming     solving 

quadratic program.

    modern methods are based on stochastic id119 and 

coordinate descent and are much faster.

    is it really optimal? 

   

is the objective function we are optimizing the    right    one?

cis419/519 spring    18

26

real data  

17,000 dimensional context sensitive spelling 
histogram of distance of points from the hyperplane

in practice, even in the separable 
case, we may not want to depend 
on the points closest to the 
hyperplane but rather on the 
distribution of the distance.  if 
only a few are close, maybe we 
can dismiss them.

cis419/519 spring    18

27

soft id166

    the hard id166 formulation assumes linearly separable data.
    a natural relaxation: 

    maximize the margin while minimizing the # of examples that violate the margin 

(separability) constraints. 

    however, this leads to non-convex problem that is hard to solve. 
    instead, we relax in a different way, that results in optimizing a 

surrogate id168 that is convex.  

cis419/519 spring    18

28

soft id166

    notice that the relaxation of the constraint:                                                      

example) and requiring:    

    now, we want to solve: 

    can be done by introducing a slack variable                  (per 

yiwtxi   1
yiwtxi   1                   ;                   0
12                        +                                   
min        ,                
s.t  yiwtxi   1                   ;                   0            

a large value of c means 
that misclassifications 
are bad     we focus on a 
small training error (at 
the expense of margin). 
a small c results in more 

training error, but 
hopefully better true 

error.

cis419/519 spring    18

29

soft id166 (2)

    now, we want to solve: 

s.t

12                        +                                   
12                        +                                   
min        ,                
min        ,                
s.t  yiwtxi   1                   ;                   0            
                   1   yiwtxi;                   0            
in  optimum,   i=max(0,1   yiwtxi)
12                        +                    max(0,1                                                   ).

min        

    what is the interpretation of this?

    which can be written as:

cis419/519 spring    18

30

    the problem we solved is:

id166 objective function
min    ||w||2 + c                    
    where                  > 0 is called a slack variable, and is defined by:
                     = max(0, 1     yi wtxi)
    equivalently, we can say that: yi wtxi    1 -                ;                      0
+             c                    

    and this can be written as:
min     ||w||2

id173 term

empirical loss

can be replaced by other id173 

functions

can be replaced by other id168s

    general form of a learning algorithm:

    minimize empirical loss, and regularize (to avoid over fitting) 
    theoretically motivated improvement over the original algorithm we   ve seen 

at the beginning of the semester.

cis419/519 spring    18

31

balance between id173 and empirical 

loss

cis419/519 spring    18

32

balance between id173 and empirical 

loss

cis419/519 spring    18

(demo)

33

underfitting and overfitting

underfitting

overfitting

expected
error

variance
bias

model complexity
complex models: 
high variance and low bias 

    simple models: 

high bias and low variance

smaller c

larger c

cis419/519 spring    18

34

what do we optimize?

cis419/519 spring    18

35

what do we optimize(2)?

    we get an unconstrained problem. we can use the gradient 

descent algorithm! however, it is quite slow.

    many other methods

   

iterative scaling; non-linear conjugate gradient; quasi-newton 
methods; truncated id77s; trust-region id77.
    all methods are iterative methods, that generate a sequence wk that 
converges to the optimal solution of the optimization problem above.

    currently: limited memory bfgs is very popular 

cis419/519 spring    18

36

optimization: how to solve
    1. earlier methods used quadratic programming. very slow.
    2. the soft id166 problem is an unconstrained optimization problems. it is 

possible to use the id119 algorithm.

    many options within this category: 

   

iterative scaling; non-linear conjugate gradient; quasi-id77s; 
truncated id77s; trust-region id77.

    all methods are iterative methods, that generate a sequence wk that 
converges to the optimal solution of the optimization problem above.
currently: limited memory bfgs is very popular 

   

   

    3. 3rd generation algorithms are based on stochastic gradient decent 
the runtime does not depend on n=#(examples); advantage when n is very large. 
stopping  criteria is a problem: method tends to be too aggressive at the beginning and 
reaches a moderate accuracy quite fast, but it   s convergence becomes slow if we are 
interested in more accurate solutions.

   

    4. dual coordinated descent (& stochastic version)

cis419/519 spring    18

37

. m: data size

m is here for mathematical correctness, it 
doesn   t matter in the view of modeling.

sgd for id166

1.

    goal:   min                              12                        +                           max0,1                                                   
    compute sub-gradient of                 :
if  1                                                      0 ; otherwise                          =        
                         =                                                   
initialize         =0                   
2.  for every example xi,yi            
if                                                    1 update the weight vector to 
(         - learning rate)
            1                   +                                                
otherwise               (1           )        

3. continue until convergence is achieved
convergence can be proved for a slightly 
complicated version of sgd (e.g, pegasos)

this algorithm 
should ring a bell   
38

cis419/519 spring    18

nonlinear id166

(demo)
(demo2)

    we can map data to a high dimensional space: x                   
    then use kernel trick:                        ,                 =                                                         
dual:min        
primal: min        ,                
12                q                                   
12                        +                                   
yiwt                            1                   
s.t  0                                 
                   0            
q                =                                                        ,                
problem,            betheminimizerofthedualproblem.
then w   =                      yixi

theorem: let w* be the minimizer of the primal 

s.t 

39

cis419/519 spring    18

nonlinear id166

    tradeoff between training time and accuracy
    complex model v.s. simple model

from: 
http://www.csie.ntu.edu.tw/~cjlin/papers/lowpoly_journal.pdf

cis419/519 spring    18

40

