coms 4721: machine learning for data science

lecture 4, 1/26/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

regression with/without id173

given:
a data set (x1, y1), . . . , (xn, yn), where x     rd and y     r. we standardize
such that each dimension of x is zero mean unit variance, and y is zero mean.
model:
we de   ne a model of the form

y     f (x; w).

we particularly focus on the case where f (x; w) = xtw.
learning:
we can learn the model by minimizing the objective (aka,    loss   ) function

i=1(yi     xt

i w)2 +   wtw     l = (cid:107)y     xw(cid:107)2 +   (cid:107)w(cid:107)2
we   ve focused on    = 0 (least squares) and    > 0 (ridge regression).

l =(cid:80)n

bias-variance trade-off

bias-variance for id75

we can go further and hypothesize a generative model y     n(xw,   2i) and
some true (but unknown) underlying value for the parameter vector w.

(cid:73) we saw how the least squares solution, wls = (xtx)   1xty, is unbiased

but potentially has high variance:

e[wls] = w, var[wls] =   2(xtx)   1.

(cid:73) by contrast, the ridge regression solution is wrr = (  i + xtx)   1xty.

using the same procedure as for least squares, we can show that

e[wrr] = (  i + xtx)   1xtxw, var[wrr] =   2z(xtx)   1zt ,

where z = (i +   (xtx)   1)   1.

bias-variance for id75

the expectation and covariance of wls and wrr gives insight into how well
we can hope to learn w in the case where our model assumption is correct.

(cid:73) least squares solution: unbiased, but potentially high variance
(cid:73) ridge regression solution: biased, but lower variance than ls

so which is preferable?

ultimately, we really care about how well our solution for w generalizes to
new data. let (x0, y0) be future data for which we have x0, but not y0.

(cid:73) least squares predicts y0 = xt
0 wls
(cid:73) ridge regression predicts y0 = xt

0 wrr

bias-variance for id75

in keeping with the square error measure of performance, we could calculate
the expected squared error of our prediction:
(y0     xt

0   w)2p(y|x, w)p(y0|x0, w) dy dy0.

e(cid:2)(y0     xt

0   w)2|x, x0

(cid:3) =

(cid:90)

(cid:90)

r

rn

(cid:73) the estimate   w is either wls or wrr.
(cid:73) the distributions on y, y0 are gaussian with the true (but unknown) w.
(cid:73) we condition on knowing x0, x1, . . . , xn.

in words this is saying:

(cid:73) imagine i know x, x0 and assume some true underlying w.
(cid:73) i generate y     n(xw,   2i) and approximate w with   w = wls or wrr.
(cid:73) i then predict y0     n(xt
0   w.
what is the expected squared error of my prediction?

0 w,   2) using y0     xt

bias-variance for id75

we can calculate this as follows (assume conditioning on x0 and x),
0 e[  w  wt ]x0

0]     2e[y0]xt

0   w)2] = e[y2

e[(y0     xt

0 e[  w] + xt

(cid:73) since y0 and   w are independent, e[y0   w] = e[y0]e[  w].
(cid:73) remember: e[  w  wt ] = var[  w] + e[  w]e[  w]t

e[y2

0] =   2 + (xt

0 w)2

bias-variance for id75

we can calculate this as follows (assume conditioning on x0 and x),
0 e[  w  wt ]x0

0]     2e[y0]xt

0   w)2] = e[y2

e[(y0     xt

0 e[  w] + xt

(cid:73) since y0 and   w are independent, e[y0   w] = e[y0]e[  w].
(cid:73) remember: e[  w  wt ] = var[  w] + e[  w]e[  w]t

e[y2

0] =   2 + (xt

0 w)2

plugging these values in:
e[(y0     xt

0   w)2] =   2 + (xt
=   2 + xt

0 w)2     2(xt
0 e[  w]) + (xt
0 (w     e[  w])(w     e[  w])tx0 + xt

0 w)(xt

0 e[  w])2 + xt
0 var[  w]x0

0 var[  w]x0

bias-variance for id75

we have shown that if
1. y     n(xw,   2) and y0     n(xt
2. we approximate w with   w according to some algorithm,

0 w,   2), and

then
e[(y0     xt

0   w)2|x, x0] =   2(cid:124)(cid:123)(cid:122)(cid:125)

noise

+ xt

(cid:124)
(cid:125)
0 (w     e[  w])(w     e[  w])tx0

(cid:123)(cid:122)

squared bias

(cid:124)

(cid:123)(cid:122)

+ xt

0 var[  w]x0
variance

(cid:125)

we see that the generalization error is a combination of three factors:

1. measurement noise     we can   t control this given the model.
2. model bias     how close to the solution we expect to be on average.
3. model variance     how sensitive our solution is to the data.

we saw how we can    nd e[  w] and var[  w] for the ls and rr solutions.

bias-variance trade-off

this idea is more general:

(cid:73) imagine we have a model: y = f (x; w) +  , e( ) = 0, var( ) =   2
(cid:73) we approximate f by minimizing a id168:   f = arg minf lf .
(cid:73) we apply   f to new data, y0       f (x0)       f0.
then integrating everything out (y, x, y0, x0):

e[(y0       f0)2] = e[y2

0]     2e[y0   f0] + e[  f 2
0 ]
0     2f0e[  f0] + e[  f0]2 + var[  f0]
(cid:125)
(cid:124)
+ (f0     e[  f0])2

(cid:124) (cid:123)(cid:122) (cid:125)

+ var[  f0]
variance

(cid:123)(cid:122)

squared bias

=   2 + f 2

=   2(cid:124)(cid:123)(cid:122)(cid:125)

noise

this is interesting in principle, but is deliberately vague (what is f ?) and
usually can   t be calculated (what is the distribution on the data?)

cross-validation

an easier way to evaluate the model is to use cross-validation.

the procedure for k-fold cross-validation is very simple:
1. randomly split the data into k roughly equal groups.
2. learn the model on k     1 groups and predict the held-out kth group.
3. do this k times, holding out each group once.
4. evaluate performance using the cumulative set of predictions.

for the case of the id173 parameter   , the above sequence can be
run for several values with the best-performing value of    chosen.

the data you test the model on should never be used to train the model!

bayes rule

prior information/belief

motivation
we   ve discussed the ridge regression objective function

n(cid:88)

l =

(yi     xt

i w)2 +   wtw.

i=1

the id173 term   wtw was imposed to penalize values in w that are
large. this reduced potential high-variance predictions from least squares.

in a sense, we are imposing a    prior belief    about what values of w we
consider to be good.

question: is there a mathematical way to formalize this?
answer: using id203 we can frame this via bayes rule.

review: id203 statements

imagine we have two events, a and b, that may or may not be related, e.g.,

(cid:73) a =    it is raining   
(cid:73) b =    the ground is wet   

we can talk about probabilities of these events,

(cid:73) p(a) = id203 it is raining
(cid:73) p(b) = id203 the ground is wet

we can also talk about their conditional probabilities,
(cid:73) p(a|b) = id203 it is raining given that the ground is wet
(cid:73) p(b|a) = id203 the ground is wet given that it is raining

we can also talk about their joint probabilities,

(cid:73) p(a, b) = id203 it is raining and the ground is wet

calculus of id203

there are simple rules for moving from one id203 to another
1. p(a, b) = p(a|b)p(b) = p(b|a)p(a)

2. p(a) =(cid:80)
3. p(b) =(cid:80)

b p(a, b = b)
a p(a = a, b)

using these three equalities, we automatically can say

p(a|b) =

p(b|a) =

p(b|a)p(a)

p(b)

p(a|b)p(b)

p(a)

=

=

this is known as    bayes rule.   

p(b|a)p(a)

(cid:80)
a p(b|a = a)p(a = a)
(cid:80)
b p(a|b = b)p(b = b)

p(a|b)p(b)

bayes rule

bayes rule lets us quantify what we don   t know. imagine we want to say
something about the id203 of b given that a happened.

bayes rule says that the id203 of b after knowing a is:

p(b|a)

(cid:124) (cid:123)(cid:122) (cid:125)

posterior

= p(a|b)

(cid:124) (cid:123)(cid:122) (cid:125)

likelihood

(cid:124)(cid:123)(cid:122)(cid:125)

p(b)

prior

(cid:124)(cid:123)(cid:122)(cid:125)

/ p(a)

marginal

notice that with this perspective, these probabilities take on new meanings.
that is, p(b|a) and p(a|b) are both    conditional probabilities,    but they
have different signi   cance.

bayes rule with continuous variables

bayes rule generalizes to continuous-valued random variables as follows.
however, instead of probabilities we work with densities.

(cid:73) let    be a continuous-valued model parameter.
(cid:73) let x be data we possess. then by bayes rule,

p(  |x) =

p(x|  )p(  )

(cid:82) p(x|  )p(  )d  

p(x|  )p(  )

p(x)

=

in this equation,
(cid:73) p(x|  ) is the likelihood, known from the model de   nition.
(cid:73) p(  ) is a prior distribution that we de   ne.
(cid:73) given these two, we can (in principle) calculate p(  |x).

example: coin bias

we have a coin with bias    towards    heads   . (encode: heads = 1, tails = 0)

we    ip the coin many times and get a sequence of n numbers (x1, . . . , xn).
assume the    ips are independent, meaning

n(cid:89)

n(cid:89)

p(x1, . . . , xn|  ) =

p(xi|  ) =

  xi(1       )1   xi.

we choose a prior for    which we de   ne to be a beta distribution,

i=1

i=1

p(  ) = beta(  |a, b) =

  (a + b)
  (a)  (b)

  a   1(1       )b   1.

what is the posterior distribution of    given x1, . . . , xn?

example: coin bias

from bayes rule,

p(  |x1, . . . , xn) =

(cid:82) 1
p(x1, . . . , xn|  )p(  )
0 p(x1, . . . , xn|  )p(  )d  

.

there is a trick that is often useful:

(                 proportional to   )

(cid:73) the denominator only normalizes the numerator, doesn   t depend on   .
(cid:73) we can write p(  |x)     p(x|  )p(  ).
(cid:73) multiply the two and see if we recognize anything:

  (a)  (b)   a   1(1       )b   1(cid:105)
i=1   xi(1       )1   xi(cid:3)(cid:104)   (a+b)
p(  |x1, . . . , xn)     (cid:2)(cid:81)n
(cid:80)n
(cid:80)n
i=1 xi + a,(cid:80)n
we recognize this as p(  |x1, . . . , xn) = beta((cid:80)n
i=1 xi+a   1(1       )
i=1(1   xi)+b   1

i=1(1    xi) + b).

      

maximum a posteriori

likelihood model

least squares and maximum likelihood
when we modeled data pairs (xi, yi) with a linear model, yi     xt
that the least squares solution,

i w, we saw

wls = arg min
w

(y     xw)t (y     xw),

was equivalent to the maximum likelihood solution when y     n(xw,   2i).

the question now is whether a similar probabilistic connection can be made
for the ridge regression problem.

prior model

ridge regression and bayesian modeling
the likelihood model is y     n(xw,   2i). what about a prior for w?
let us assume that the prior for w is gaussian, w     n(0,      1i). then

(cid:16)   

(cid:17) d

2  

p(w) =

2 e      

2 wt w.

we can now try to    nd a w that satis   es both the data likelihood, and our
prior conditions about w.

maximum a poseriori estimation

maximum a poseriori (map) estimation seeks the most probable value w
under the posterior:

wmap = arg max
w

= arg max

w

= arg max

w

ln p(w|y, x)

p(y|w, x)p(w)

p(y|x)

ln
ln p(y|w, x) + ln p(w)     ln p(y|x)

(cid:73) contrast this with ml, which only focuses on the likelihood.
(cid:73) the normalizing constant term ln p(y|x) doesn   t involve w. therefore,

we can maximize the    rst two terms alone.

(cid:73) in many models we don   t know ln p(y|x), so this fact is useful.

map for id75

map using our de   ned prior gives:

= arg max

wmap = arg max
w

ln p(y|w, x) + ln p(w)
    1
2  2 (y     xw)t (y     xw)       
2
calling this objective l, then as before we    nd w such that
  2 xtxw       w = 0

  2 xty     1

   wl =

1

w

wtw + const.

(cid:73) the solution is wmap = (    2i + xtx)   1xty.
(cid:73) notice that wmap = wrr
(cid:73) rr maximizes the posterior, while ls maximizes the likelihood.

(modulo a switch from    to     2)

