coms 4721: machine learning for data science

lecture 12, 2/28/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

id90

id90

a decision tree maps input x     rd to output y using binary decision rules:
(cid:73) each node in the tree has a splitting rule.
(cid:73) each leaf node is associated with an output value (outputs can repeat).

each splitting rule is of the form

h(x) = 1{xj > t}

for some dimension j of x and t     r.

using these transition rules, a path
to a leaf node gives the prediction.

(one-level tree = decision stump)

x1>1.7x2>2.8  y=1  y=2  y=3regression trees

motivation: partition the space so that data in a region have same prediction

left: dif   cult to de   ne a    rule   .
right: easy to de   ne a recursive splitting rule.

regression trees

      

if we think in terms of trees, we can de   ne a simple rule for partitioning the
space. the left and right    gures represent the same regression function.

regression trees

      

adding an output dimension to the    gure (right), we can see how regression
trees can learn a step function approximation to the data.

classification trees (example)

classifying irises using sepal and petal
measurements:
(cid:73) x     r2, y     {1, 2, 3}
(cid:73) x1 = ratio of sepal length to width
(cid:73) x2 = ratio of petal length to width

sepal length/width1.522.53petal length/width22.533.544.555.56classification trees (example)

classifying irises using sepal and petal
measurements:
(cid:73) x     r2, y     {1, 2, 3}
(cid:73) x1 = ratio of sepal length to width
(cid:73) x2 = ratio of petal length to width

sepal length/width1.522.53petal length/width22.533.544.555.56  y=2classification trees (example)

classifying irises using sepal and petal
measurements:
(cid:73) x     r2, y     {1, 2, 3}
(cid:73) x1 = ratio of sepal length to width
(cid:73) x2 = ratio of petal length to width

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7classification trees (example)

classifying irises using sepal and petal
measurements:
(cid:73) x     r2, y     {1, 2, 3}
(cid:73) x1 = ratio of sepal length to width
(cid:73) x2 = ratio of petal length to width

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7  y=1  y=3classification trees (example)

classifying irises using sepal and petal
measurements:
(cid:73) x     r2, y     {1, 2, 3}
(cid:73) x1 = ratio of sepal length to width
(cid:73) x2 = ratio of petal length to width

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7x2>2.8  y=1classification trees (example)

classifying irises using sepal and petal
measurements:
(cid:73) x     r2, y     {1, 2, 3}
(cid:73) x1 = ratio of sepal length to width
(cid:73) x2 = ratio of petal length to width

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7x2>2.8  y=1  y=2  y=3basic decision tree learning algorithm

      

      

the basic method for learning trees is with a top-down greedy algorithm.

(cid:73) start with a single leaf node containing all data
(cid:73) loop through the following steps:

(cid:73) pick the leaf to split that reduces uncertainty the most.
(cid:73) figure out the     decision rule on one of the dimensions.

(cid:73) stopping rule discussed later.

label/response of the leaf is majority-vote/average of data assigned to it.

  y=2x1>1.7  y=1  y=3x1>1.7x2>2.8  y=1  y=2  y=3growing a regression tree

how do we grow a regression tree?

(cid:73) for m regions of the space, r1, . . . , rm,

f (x) =

the prediction function is

m(cid:88)
goal: try to minimize(cid:80)

so for a    xed m, we need rm and cm.
i(yi     f (xi))2.

cm1{x     rm}.

m=1

1. find cm given rm: simply the average of all yi for which xi     rm.
2. how do we    nd regions? consider splitting region r at value s of dim j:

(cid:73) de   ne r   (j, s) = {xi     r|xi(j)     s} and r+(j, s) = {xi     r|xi(j) > s}
(cid:73) for each dimension j, calculate the best splitting point s for that dimension.
(cid:73) do this for each region (leaf node). pick the one that reduces the objective most.

growing a classification tree

for regression: squared error is a natural way to de   ne the splitting rule.

for classi   cation: need some measure of how badly a region classi   es data

and how much it can improve if it   s split.

k-class problem: for all x     rm, let pk be empirical fraction labeled k.

measures of quality of rm include
1. classi   cation error: 1     maxk pk

2. gini index: 1    (cid:80)
3. id178:    (cid:80)

k p2
k
k pk ln pk

(cid:73) these are all maximized when pk is uniform on the k classes in rm.
(cid:73) these are minimized when pk = 1 for some k (rm only contains one class)

growing a classification tree

search r1 and r2 for splitting options.

1. r1: y = 1 leaf classi   es perfectly
2. r2: y = 3 leaf has gini index

(cid:18) 1

101

(cid:19)2    

(cid:18) 50

101

(cid:19)2    

(cid:18) 50

(cid:19)2

101

u(r2) = 1    

= 0.5098

gini improvement from split rm to r   

u(rm)    (cid:16)

pr   m

   u(r   

m ) + pr+

m

(cid:17)

m & r+
m :
   u(r+
m )

pr+

m

: fraction of data in rm split into r+
m .

u(r+

m ) : new quality measure in region r+
m .

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7  y=1  y=3growing a classification tree

search r1 and r2 for splitting options.

1. r1: y = 1 leaf classi   es perfectly
2. r2: y = 3 leaf has gini index

(cid:18) 1

101

(cid:19)2    

(cid:18) 50

101

(cid:19)2    

(cid:18) 50

(cid:19)2

101

u(r2) = 1    

= 0.5098

check split r2 with 1{x1 > t}

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7  y=1  y=3t1.61.822.22.42.62.83reduction in uncertainty00.0050.010.0150.02growing a classification tree

search r1 and r2 for splitting options.

1. r1: y = 1 leaf classi   es perfectly
2. r2: y = 3 leaf has gini index

(cid:18) 1

101

(cid:19)2    

(cid:18) 50

101

(cid:19)2    

(cid:18) 50

(cid:19)2

101

u(r2) = 1    

= 0.5098

check split r2 with 1{x2 > t}

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7  y=1  y=3t22.533.544.5reduction in uncertainty00.050.10.150.20.25growing a classification tree

search r1 and r2 for splitting options.

1. r1: y = 1 leaf classi   es perfectly
2. r2: y = 3 leaf has gini index

(cid:18) 1

101

(cid:19)2    

(cid:18) 50

101

(cid:19)2    

(cid:18) 50

(cid:19)2

101

u(r2) = 1    

= 0.5098

check split r2 with 1{x2 > t}

sepal length/width1.522.53petal length/width22.533.544.555.56x1>1.7x2>2.8  y=1  y=2  y=3t22.533.544.5reduction in uncertainty00.050.10.150.20.25pruning a tree

q: when should we stop growing a tree?

a: uncertainty reduction is not best way.

example: any split of x1 or x2 at right
will show zero reduction in uncertainty.
however, we can learn a perfect tree on
this data by partitioning in quadrants.

pruning is the method most often used. grow the tree to a very large size.
then use an algorithm to trim it back.

(we won   t cover the algorithm, but mention that it   s non-trivial.)

x1x2overfitting

(cid:73) training error goes to zero as size of tree increases.
(cid:73) testing error decreases, but then increases because of over   tting.

numberofnodesintreeerrortrainingerrortrueerrorthe bootstrap

the bootstrap: a resampling technique

we brie   y present a technique called the bootstrap. this statistical technique
is used as the basis for learning ensemble classi   ers.

bootstrap
bootstrap (i.e., resampling) is a technique for improving estimators.

resampling = sampling from the empirical distribution of the data

application to ensemble methods

(cid:73) we will use resampling to generate many    mediocre    classi   ers.
(cid:73) we then discuss how    id112    these classi   ers improves performance.
(cid:73) first, we cover the bootstrap in a simpler context.

bootstrap: basic algorithm

input

(cid:73) a sample of data x1, . . . , xn.
(cid:73) an estimation rule   s of a statistic s. for example,   s = med(x1:n)

estimates the true median s of the unknown distribution on x.

bootstrap algorithm
1. generate bootstrap samples b1, . . . ,bb.

    create bb by picking points from {x1, . . . , xn} randomly n times.
    a particular xi can appear in bb many times (it   s simply duplicated).
2. evaluate the estimator on each bb by pretending it   s the data set:

  sb :=   s(bb)

3. estimate the mean and variance of   s:

b(cid:88)

b=1

  b =

1
b

  sb,

  2
b =

1
b

b(cid:88)

b=1

(  sb       b)2

example: variance estimation of the median

(cid:73) the median of x1, . . . , xn (for x     r) is found by simply sorting them

and taking the middle one, or the average of the two middle ones.

(cid:73) how con   dent can we be in the estimate median(x1, . . . , xn)?

(cid:73) find it   s variance.
(cid:73) but how? answer: by id64 the data.

1. generate bootstrap data sets b1, . . . ,bb.

2. calculate: (notice that   smean is the mean of the median)

  smean =

1
b

median(bb),

  svar =

1
b

median(bb)       smean

(cid:73) the procedure is remarkably simple, but has a lot of theory behind it.

b(cid:88)

b=1

b(cid:88)

(cid:16)

b=1

(cid:17)2

id112 and id79s

id112

id112 uses the bootstrap for regression or classi   cation:

id112 = bootstrap aggregation

algorithm
for b = 1, . . . , b:
1. draw a bootstrap sample bb of size n from training data.
2. train a classi   er or regression model fb on bb.
(cid:73) for a new point x0, compute:

b(cid:88)

b=1

favg(x0) =

1
b

fb(x0)

(cid:73) for regression, favg(x0) is the prediction.
(cid:73) for classi   cation, view favg(x0) as an average over b votes. pick the majority.

example: id112 trees

(cid:73) binary classi   cation, x     r5.

(cid:73) note the variation among

bootstrapped trees.

(cid:73) take-home message:

with id112, each tree doesn   t
have to be great, just    ok   .

(cid:73) id112 often improves results

when the function is non-linear.

elementsofstatisticallearning(2nded.)c!hastie,tibshirani&friedman2009chap8|x.1 < 0.39501010110original tree|x.1 < 0.55501001b = 1|x.2 < 0.205010101b = 2|x.2 < 0.28511010b = 3|x.3 < 0.985010111b = 4|x.4 <    1.3601101010b = 5|x.1 < 0.39511001b = 6|x.1 < 0.39501011b = 7|x.3 < 0.985010010b = 8|x.1 < 0.395010110b = 9|x.1 < 0.55510101b = 10|x.1 < 0.5550101b = 11figure8.9.id112treesonsimulateddataset.thlflhhiilelid79s

drawbacks of id112

(cid:73) id112 works on trees because of the

id160 (    bias,     variance).
(cid:73) however, the bagged trees are correlated.
(cid:73) in general, when bootstrap samples are

correlated, the bene   t of id112 decreases.

id79s
modi   cation of id112 where trees are designed to reduce correlation.

(cid:73) a very simple modi   cation.
(cid:73) still learn a tree on each bootstrap set, bb.
(cid:73) to split a region, only consider random subset of dimensions of x     rd.

id79s: algorithm

training
input parameter: m     a positive integer with m < d, often m        

d

for b = 1, . . . , b:
1. draw a bootstrap sample bb of size n from the training data.
2. train a tree classi   er on bb, where each split is computed as follows:

(cid:73) randomly select m dimensions of x     rd, newly chosen for each b.
(cid:73) make the best split restricted to that subset of dimensions.

(cid:73) id112 for trees: bag trees learned using the original algorithm.
(cid:73) id79s: bag trees learned using algorithm on this slide.

id79s

example problem

(cid:73) id79 classi   cation.

(cid:73) forest size: a few hundred trees.

(cid:73) notice there is a tendency to align
decision boundary with the axis.

elementsofstatisticallearning(2nded.)c!hastie,tibshirani&friedman2009chap15id79 classifierooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooootraining error: 0.000test error:       0.238bayes error:    0.2103   nearest neighborsooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooootraining error: 0.130test error:       0.242bayes error:    0.210figure15.11.randomforestsversus3-nnonthemixturedata.theaxis-orientednatureoftheindivid-ualtreesinarandomforestleadtodecisionregionswithanaxis-oriented   avor.