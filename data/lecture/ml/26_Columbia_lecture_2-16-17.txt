coms 4721: machine learning for data science

lecture 9, 2/16/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

id28

binary classification

linear classi   ers
given: data (x1, y1), . . . , (xn, yn), where xi     rd and yi     {   1, +1}
a linear classi   er takes a vector w     rd and scalar w0     r and predicts

yi = f (xi; w, w0) = sign(xt

i w + w0).

we discussed two methods last time:

(cid:73) least squares: sensitive to outliers
(cid:73) id88: convergence issues, assumes linear separability

can we combine the separating hyperplane idea with id203 to    x this?

bayes linear classification

id156
we saw an example of a linear classi   cation rule using a bayes classi   er.
for the model y     bern(  ) and x| y     n(  y,   ), declare y = 1 given x if

p(x|y = 1)p(y = 1)
p(x|y = 0)p(y = 0)

ln

> 0 .

in this case, the log odds is equal to

p(x|y = 1)p(y = 1)
p(x|y = 0)p(y = 0)

ln

= ln   1
  0

(cid:124)

    1
2

(  1 +   0)t      1(  1       0)

(cid:125)

(cid:123)(cid:122)
(cid:125)

a constant w0

+ xt      1(  1       0)

(cid:123)(cid:122)

a vector w

(cid:124)

log odds and bayes classification

original formulation
recall that originally we wanted to declare y = 1 given x if

p(y = 1|x)
p(y = 0|x)

ln

> 0

we didn   t have a way to de   ne p(y|x), so we used bayes rule:
(cid:73) use p(y|x) = p(x|y)p(y)
(cid:73) de   ne p(y) to be a bernoulli distribution (coin    ip distribution)
(cid:73) de   ne p(x|y) however we want (e.g., a single gaussian)

and let the p(x) cancel each other in the fraction

p(x)

now, we want to directly de   ne p(y|x). we   ll use the log odds to do this.

log odds and bayes classification

log odds and hyperplanes
classifying x based on the log odds

l = ln

p(y = +1|x)
p(y =    1|x)

,

we notice that
1. l (cid:29) 0 : more con   dent y = +1,
2. l (cid:28) 0 : more con   dent y =    1,
3. l = 0 : can go either way

x2

h

x

w

x1

   w0/(cid:107)w(cid:107)2

the linear function xtw + w0 captures these three objectives:

(cid:73) the distance of x to a hyperplane h de   ned by (w, w0) is(cid:12)(cid:12) xt w(cid:107)w(cid:107)2

(cid:73) the sign of the function captures which side x is on.
(cid:73) as x moves away/towards h, we become more/less con   dent.

(cid:12)(cid:12).

+ w0(cid:107)w(cid:107)2

log odds and hyperplanes

logistic link function
we can directly plug in the hyperplane representation for the log odds:

p(y = +1|x)
p(y =    1|x)

ln

= xtw + w0

question: what is different from the previous bayes classi   er?
answer: there was a formula for calculating w and w0 based on the prior
model and data x. now, we put no restrictions on these values.
setting p(y =    1|x) = 1     p(y = +1|x), solve for p(y = +1|x) to    nd

p(y = +1|x) =

exp{xtw + w0}
1 + exp{xtw + w0} =   (xtw + w0).

(cid:73) this is called the sigmoid function.
(cid:73) we have chosen xtw + w0 as the link function for the log odds.

logistic sigmoid function

(cid:73) red line: sigmoid function   (xtw + w0), which maps x to p(y = +1|x).
(cid:73) the function   (  ) captures our desire to be more con   dent as we move

away from the separating hyperplane, de   ned by the x-axis.

(cid:73) (blue dashed line: not discussed.)

   50500.51id28

as with regression, absorb the offset: w    

(cid:21)

(cid:20) w0

w

and x    

(cid:20) 1

x

(cid:21)

.

de   nition
let (x1, y1), . . . , (xn, yn) be a set of binary labeled data with y     {   1, +1}.
id28 models each yi as independently generated, with

p(yi = +1|xi, w) =   (xt

i w),

  (xi; w) =

ext
i w
1 + ext

i w .

discriminative vs generative classi   ers
(cid:73) this is a discriminative classi   er because x is not directly modeled.
(cid:73) bayes classi   ers are known as generative because x is modeled.

discriminative: p(y|x)

generative: p(x|y)p(y).

id28 likelihood

n(cid:89)
n(cid:89)

i=1

data likelihood
de   ne   i(w) =   (xt

i w). the joint likelihood of y1, . . . , yn is

p(y1, . . . , yn|x1, . . . , xn, w) =

=

p(yi|xi, w)

  i(w)1(yi=+1) (1       i(w))

1(yi=   1)

i=1

(cid:73) notice that each xi modi   es the id203 of a    +1    for its respective yi.
(cid:73) predicting new data is the same:

(cid:73) if xtw > 0, then   (xtw) > 1/2 and predict y = +1, and vice versa.
(cid:73) we now get a con   dence in our prediction via the id203   (xtw).

id28 and maximum likelihood

more notation changes
use the following fact to condense the notation:

(cid:17)1(yi=+1)(cid:16)

(cid:16) ext
(cid:123)(cid:122)
(cid:124)

i w
1 + ext
i w
  i(w)

(cid:125)

(cid:123)(cid:122)
(cid:124)
1     ext
i w
1 + ext
i w
1     i(w)

(cid:125)

(cid:17)1(yi=   1)

(cid:124)

(cid:123)(cid:122)

eyixt
i w
1 + eyixt
i w
  i(yi  w)

(cid:125)

=

therefore, the data likelihood can be written compactly as

p(y1, . . . , yn|x1, . . . , xn, w) =

  i(yi    w)

we want to maximize this over w.

i=1

n(cid:89)

id28 and maximum likelihood

maximum likelihood
the maximum likelihood solution for w can be written

n(cid:88)

i=1

l

ln   i(yi    w)

wml = arg max
w

= arg max

w

as with the id88, we can   t directly set    wl = 0, and so we need an
iterative algorithm. since we want to maximize l, at step t we can update

w(t+1) = w(t) +      wl,

   wl =

(1       i(yi    w)) yixi.

we will see that this results in an algorithm similar to the id88.

i=1

n(cid:88)

id28 algorithm (steepest ascent)

input: training data (x1, yi), . . . , (xn, yn) and step size    > 0

1. set w(1) = (cid:126)0
2. for iteration t = 1, 2, . . . do
    update w(t+1) = w(t) +   

n(cid:88)

i=1

(cid:16)

1       i(yi    w(t))

(cid:17)

yixi

id88: search for misclassi   ed (xi, yi), update w(t+1) = w(t) +   yixi.

id28: something similar except we sum over all data.

(cid:73) recall that   i(yi    w) picks out the id203 model gives to the observed yi.
(cid:73) therefore 1       i(yi    w) is the id203 the model picks the wrong value.
(cid:73) id88 is    all-or-nothing.    either it   s correctly or incorrectly classi   ed.
(cid:73) id28 has a probabilistic    fudge-factor.   

bayesian id28

problem: if a hyperplane can separate all training data, then (cid:107)wml(cid:107)2        .
this drives   i(yi    w)     1 for each (xi, yi).

even for nearly separable data it might get a few very wrong in order to be
more con   dent about the rest. this is a case of    over-   tting.   

a solution: regularize w with   wtw :

(cid:80)n
i=1 ln   i(yi  w)     wtw

wmap = arg maxw

we   ve seen how this corresponds to a
gaussian prior distribution on w.
how about the posterior p(w|x, y)?

   4   202468   8   6   4   2024laplace approximation

bayesian id28

posterior calculation
de   ne the prior distribution on w to be w     n(0,      1i). the posterior is

p(w)(cid:81)n
(cid:82) p(w)(cid:81)n

i=1   i(yi    w)
i=1   i(yi    w) dw

p(w|x, y) =

this is not a    standard    distribution and we can   t calculate the denominator.
therefore we can   t actually say what p(w|x, y) is.
can we approximate p(w|x, y)?

laplace approximation

one strategy
pick a distribution to approximate p(w|x, y). we will say

p(w|x, y)     normal(  ,   ).

now we need a method for setting    and   .

laplace approximations
using a condensed notation, notice from bayes rule that

p(w|x, y) =

(cid:82) eln p(y,w|x)dw

eln p(y,w|x)

.

we will approximate ln p(y, w|x) in the numerator and denominator.

laplace approximation

let   s de   ne f (w) = ln p(y, w|x).

taylor expansions
we can approximate f (w) with a second order taylor expansion.
recall that w     rd+1. for any point z     rd+1,

f (w)     f (z) + (w     z)t   f (z) +

1
2

(w     z)t(cid:0)   2f (z)(cid:1) (w     z)

the notation    f (z) is short for    wf (w)|z, and similarly for the matrix of
second derivatives. we just need to pick z.

the laplace approximation de   nes z = wmap.

laplace approximation (solving)

recall f (w) = ln p(y, w|x) and z = wmap. from bayes rule and the laplace
approximation we now have

p(w|x, y) =

   

e f (w)

(cid:82) e f (w)dw
(cid:82) e f (z)+(w   z)t   f (z)+ 1

e f (z)+(w   z)t   f (z)+ 1

2 (w   z)t(   2f (z))(w   z)
2 (w   z)t (   2f (z))(w   z)dw

this can be simpli   ed in two ways,

1. the term e f (wmap) in the numerator and denominator can be viewed as a
multiplicative constant since it doesn   t vary in w. they therefore cancel.
2. by de   nition of how we    nd wmap, the vector    w ln p(y, w|x)|wmap = 0.

laplace approximation (solving)

we   re therefore left with the approximation

p(w|x, y)    

e    1

(cid:82) e    1

2 (w   wmap)t(      2 ln p(y,wmap|x))(w   wmap)
2 (w   wmap)t (      2 ln p(y,wmap|x))(w   wmap)dw

the solution comes by observing that this is a multivariate normal,

where

p(w|x, y)     normal(  ,   ),

   = wmap,    =(cid:0)      2 ln p(y, wmap|x)(cid:1)   1

we can take the second derivative (hessian) of the log joint likelihood to    nd

   2 ln p(y, wmap|x) =      i     n(cid:88)

i=1

  i(yi    wmap) (1       i(yi    wmap)) xixt

i

bayesian id28

laplace approximation for id28
given labeled data (x1, y1), . . . , (xn, yn) and the model

p(yi|xi, w) =   i(yi    w), w     n(0,      1i),

  i(yi    w) =

eyixt
i w
1 + eyixt
i w

1. find: wmap = arg max
w

n(cid:88)
2. set:         1 =      i     n(cid:88)

i=1

ln   i(yi    w)       
2

wtw

  i(yi    wmap) (1       i(yi    wmap)) xixt

i

3. approximate: p(w|x, y) = n (wmap,   ).

i=1

