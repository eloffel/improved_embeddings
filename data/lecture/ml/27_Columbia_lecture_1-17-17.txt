coms 4721: machine learning for data science

lecture 1, 1/17/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

overview

this class will cover model-based techniques for extracting information
from data with an end-task in mind. such tasks include:

(cid:73) predicting an unknown    output    given its corresponding    input   
(cid:73) uncovering information within the data to better understand it
(cid:73) data-driven recommendation, grouping, classi   cation, ranking, etc.

there are a few ways we can divide up the material as we go along, e.g.,

supervised learning
probabilistic models
modeling approach

|
|
|

unsupervised learning
non-probabilistic models
optimization techniques

we   ll adopt the    rst method and work in the second two along the way.

overview: supervised learning

(a) regression

(b) classi   cation

regression: using set of inputs, predict real-valued output.

classi   cation: using set of inputs, predict a discrete label (aka class).

xt01   101example classification problem

given a set of inputs characterizing an item, assign it a label.

is this spam?
hi everyone,

i saw that close to my hotel there is a pub with bowling
(it   s on market between 9th and 10th avenue). meet
there at 8:30?

what about this?
enter for a chance to win a trip to universal orlando to
celebrate the arrival of dr. seuss   s the lorax on movies
on demand on august 21st!

click here now!

overview: unsupervised learning

(c) id96

(d) recommendations1

with unsupervised learning our goal is often to uncover structure in the data.
this helps with predictions, recommendations, ef   cient data exploration.

1figure from koren, y., robert b., and volinsky, c..    id105 techniques for recommender systems.    computer 42.8 (2009): 30-37.

governmentlaw            politicslegislation . . .0.040.020.010.01teambasketball         pointsscore . . .0.030.020.010.01businessmoney    economiccompany. . .0.040.020.020.01healthmedical       diseasehospital . . .0.030.030.020.01computersystem  softwareprogram . . .0.030.020.020.01topicsdocumentstopic assignmentstopic proportionsexample unsupervised problem

goal: learn the dominant topics from a set of news articles.

data modeling

(cid:73) supervised vs. unsupervised: blocks #1 and #4

(cid:73) probabilistic vs. non-probabilistic: primarily block #2 (some block #3)

(cid:73) model development (block #2) vs. optimization techniques (block #3)

gaussian distribution (multivariate)

gaussian density in d dimensions
(cid:73) block #1: data x1, . . . , xn. each xi     rd
(cid:73) block #2: an i.i.d. gaussian model
(cid:73) block #3: maximum likelihood
(cid:73) block #4: leave unde   ned

the density function is

p(x|  ,   ) :=

(2  )

2(cid:112)det(  )

1

d

(cid:16)   1

2

exp

(cid:17)

(x     )t      1(x     )

the central moments are:

e[x] =(cid:82)

rd x p(x|  ,   )dx =   ,

cov(x) = e[(x     e[x])(x     e[x])t ] = e[xxt ]     e[x]e[x]t =   .

block #2: a probabilistic model

probabilistic models
(cid:73) a probabilistic model is a set of id203 distributions, p(x|  ).
(cid:73) we pick the distribution family p(  ), but don   t know the parameter   .

example: model data with a gaussian distribution p(x|  ),    = {  ,   }.

the i.i.d. assumption
assume data is independent and identically distributed (iid). this is written

iid    p(x|  ),

xi

i = 1, . . . , n.

writing the density as p(x|  ), then the joint density decomposes as

p(x1, . . . , xn|  ) =

p(xi|  ).

n(cid:89)

i=1

block #3: id113

maximum likelihood approach
we now need to    nd   . maximum likelihood seeks the value of    that
maximizes the likelihood function:

    ml := arg max

  

p(x1, . . . , xn|  ),

this value best explains the data according to the chosen distribution family.

maximum likelihood equation
the analytic criterion for this maximum likelihood estimator is:

n(cid:89)

     

p(xi|  ) = 0.

simply put, the maximum is at a peak. there is no    upward    direction.

i=1

block #3: logarithm trick

(cid:81)n
i=1 p(xi|  ) can be complicated. we use the fact that the

logarithm trick
calculating      
logarithm is monotonically increasing on r+, and the equality

(cid:16)(cid:89)

(cid:17)

(cid:88)

ln

fi

=

ln(fi).

i

i

consequence: taking the logarithm does not change the location of a
maximum or minimum:

max

y

arg max

y

ln g(y) (cid:54)= max
ln g(y) = arg max

y

g(y)

y

the value changes.

g(y)

the location does not change.

block #3: analytic maximum likelihood

maximum likelihood and the logarithm trick

    ml = arg max

  

p(xi|  ) = arg max

  

ln

p(xi|  )

= arg max

  

(cid:17)

(cid:16) n(cid:89)

i=1

n(cid:88)

i=1

ln p(xi|  )

n(cid:89)

i=1

to then solve for     ml,    nd

n(cid:88)

i=1

     

ln p(xi|  ) =

n(cid:88)

i=1

      ln p(xi|  ) = 0.

depending on the choice of the model, we will be able to solve this

1. analytically (via a simple set of equations)
2. numerically (via an iterative algorithm using different equations)
3. approximately (typically when #2 converges to a local optimal solution)

example: multivariate gaussian id113

block #2: multivariate gaussian data model
model: set of all gaussians on rd with unknown mean        rd and
++ (positive de   nite d    d matrix).
covariance        sd
we assume that x1, . . . , xn are i.i.d. p(x|  ,   ), written xi

iid    p(x|  ,   ).

block #3: maximum likelihood solution
we have to solve the equation

n(cid:88)

   (  ,  ) ln p(xi|  ,   ) = 0

i=1

for    and   . (try doing this without the log to appreciate it   s usefulness.)

example: gaussian mean id113

(cid:17)

first take the gradient with respect to   .

(cid:16)   1
1(cid:112)(2  )d|  | exp
ln(2  )d|  |     1
(cid:16)
2

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

ln

   1
2
     

0 =      

=      

=    1
2

(xi       )t      1(xi       )

2
(xi       )t      1(xi       )

(cid:17)
i      1xi     2  t      1xi +   t      1  
xt

=         1

n(cid:88)

(xi       )

i=1

since    is positive de   nite, the only solution is

n(cid:88)

i=1

(xi       ) = 0

   

    ml =

n(cid:88)

i=1

1
n

xi

since this solution is independent of   , it doesn   t depend on     ml.

example: gaussian covariance id113

now take the gradient with respect to   .

n(cid:88)

i=1

   1
2

0 =      

=     n
2
=     n
2

ln(2  )d|  |     1
(cid:16)
2
     trace

(xi       )t      1(xi       )

n(cid:88)

(xi       )(xi       )t(cid:17)

     1

      ln|  |     1
n(cid:88)
2

     1 +

     2

1
2

i=1

(xi       )(xi       )t

solving for    and plugging in    =     ml,

i=1

n(cid:88)

i=1

    ml =

1
n

(xi         ml)(xi         ml)t .

example: gaussian id113 (summary)

so if we have data x1, . . . , xn in rd that we hypothesize is i.i.d. gaussian, the
maximum likelihood values of the mean and covariance matrix are

n(cid:88)

i=1

n(cid:88)

i=1

    ml =

1
n

xi,

    ml =

1
n

(xi         ml)(xi         ml)t .

are we done? there are many assumptions/issues with this approach that
makes    nding the    best    parameter values not a complete victory.

(cid:73) we made a model assumption (multivariate gaussian).
(cid:73) we made an i.i.d. assumption.
(cid:73) we assumed that maximizing the likelihood is the best thing to do.

comment: we often use   ml to make predictions about xnew (block #4).

how does   ml generalize to xnew?
if x1:n don   t    capture the space    well,   ml can over   t the data.

