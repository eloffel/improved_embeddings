coms 4721: machine learning for data science

lecture 13, 3/2/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

boosting

robert e. schapire and yoav freund, boosting: foundations and algorithms, mit press, 2012.
see this textbook for many more details. (i borrow some    gures from that book.)

id112 classifiers

algorithm: id112 binary classi   ers
given (x1, y1), . . . , (xn, yn), x     x , y     {   1, +1}
(cid:73) for b = 1, . . . , b

(cid:73) sample a bootstrap dataset bb of size n. for each entry in bb, select (xi, yi)
n . some (xi, yi) will repeat and some won   t appear in bb.

with id203 1

(cid:73) learn a classi   er fb using data in bb.

(cid:73) de   ne the classi   cation rule to be

fbag(x0) = sign

(cid:32) b(cid:88)

(cid:33)

fb(x0)

.

b=1

(cid:73) with id112, we observe that a committee of classi   ers votes on a label.
(cid:73) each classi   er is learned on a bootstrap sample from the data set.
(cid:73) learning a collection of classi   ers is referred to as an ensemble method.

boosting

how is it that a committee of blockheads can somehow arrive at highly reasoned decisions,
despite the weak judgment of the individual members?

- schapire & freund,    boosting: foundations and algorithms   

boosting is another powerful method for id108. it is similar to
id112 in that a set of classi   ers are combined to make a better one.

it works for any classi   er, but a    weak    one that is easy to learn is usually
chosen. (weak = accuracy a little better than random guessing)

short history
1984 : leslie valiant and michael kearns ask if    boosting    is possible.
1989 : robert schapire creates    rst boosting algorithm.
1990 : yoav freund creates an optimal boosting algorithm.
1995 : freund and schapire create adaboost (adaptive boosting),

the major boosting algorithm.

id112 vs boosting (overview)

training sampleweighted sampleweighted sampleweighted sampletraining samplebootstrap samplebootstrap samplebootstrap samplef3(x)f2(x)f3(x)f2(x)f1(x)f1(x)id112boostingthe adaboost algorithm (sampling version)

(cid:33)

  t ft(x0)

(cid:32) t(cid:88)

t=1

fboost(x0) = sign

training sampleweighted sampleweighted sampleweighted sample  3, f3(x)  2, f2(x)  1, f1(x)boostingsample and classify b3weightederror   1 sample and classify b2sample and classify b1weightederror   2the adaboost algorithm (sampling version)

algorithm: boosting a binary classi   er
given (x1, y1), . . . , (xn, yn), x     x , y     {   1, +1}, set w1(i) = 1

n for i = 1 : n

(cid:73) for t = 1, . . . , t

1. sample a bootstrap dataset bt of size n according to distribution wt.

notice we pick (xi, yi) with id203 wt(i) and not 1
n .

2. learn a classi   er ft using data in bt.

3. set  t =(cid:80)n

i=1 wt(i)1{yi (cid:54)= ft(xi)} and   t = 1

2 ln

4. scale   wt+1(i) = wt(i)e     tyi ft(xi) and set wt+1(i) =

(cid:73) set the classi   cation rule to be

fboost(x0) = sign

(cid:16)(cid:80)t

t=1   t ft(x0)

(cid:17)

.

 t

(cid:16) 1    t
(cid:80)
(cid:17)

.

  wt+1(i)
j   wt+1(j)

.

comment: description usually simpli   ed to    learn classi   er ft using distribution wt.   

boosting a decision stump (example 1)

original data

uniform distribution, w1
learn weak classi   er

here: use a decision stump

+++++-----x1>1.7  y=1  y=3boosting a decision stump (example 1)

round 1 classi   er

weighted error:  1 = 0.3
weight update:   1 = 0.42

+++++-----boosting a decision stump (example 1)

weighted data

after round 1

+++++-----boosting a decision stump (example 1)

round 2 classi   er

weighted error:  2 = 0.21
weight update:   2 = 0.65

+++++-----boosting a decision stump (example 1)

weighted data

after round 2

+++++-----boosting a decision stump (example 1)

round 2 classi   er

weighted error:  3 = 0.14
weight update:   3 = 0.92

+++++-----boosting a decision stump (example 1)

classi   er after three rounds

+++++-----++0.42x0.65x0.92xboosting a decision stump (example 2)

example problem
random guessing
50% error

decision stump
45.8% error

full decision tree
24.7% error

boosted stump
5.8% error

boosting

point = one dataset. location = error rate w/ and w/o boosting. the boosted
version of the same classi   er almost always produces better results.

boosting

(left) boosting a bad classi   er is often better than not boosting a good one.
(right) boosting a good classi   er is often better, but can take more time.

boosting and feature maps

q: what makes boosting work so well?
a: this is a well-studied question. we will present one analysis later, but

we can also give intuition by tying it in with what we   ve already learned.

the classi   cation for a new x0 from boosting is

(cid:32) t(cid:88)

(cid:33)

fboost(x0) = sign

  t ft(x0)

.

t=1

de   ne   (x) = [ f1(x), . . . , ft (x)](cid:62), where each ft(x)     {   1, +1}.
(cid:73) we can think of   (x) as a high dimensional feature map of x.
(cid:73) the vector    = [  1, . . . ,   t ](cid:62) corresponds to a hyperplane.
(cid:73) so the classi   er can be written fboost(x0) = sign(  (x0)(cid:62)  ).
(cid:73) boosting learns the feature mapping and hyperplane simultaneously.

application: face detection

face detection (viola & jones, 2001)

problem: locate the faces in an image or video.
processing: divide image into patches of different scales, e.g., 24    24,
48    48, etc. extract features from each patch.
classify each patch as face or no face using a boosted decision stump. this
can be done in real-time, for example by your digital camera (at 15 fps).

(cid:73) one patch from a larger image. mask it with many    feature extractors.   
(cid:73) each pattern gives one number, which is the sum of all pixels in black

region minus sum of pixels in white region (total of 45,000+ features).

144violaandjonesfigure5.the   rstandsecondfeaturesselectedbyadaboost.thetwofeaturesareshowninthetoprowandthenoverlayedonatyp-icaltrainingfaceinthebottomrow.the   rstfeaturemeasuresthedifferenceinintensitybetweentheregionoftheeyesandaregionacrosstheuppercheeks.thefeaturecapitalizesontheobservationthattheeyeregionisoftendarkerthanthecheeks.thesecondfeaturecomparestheintensitiesintheeyeregionstotheintensityacrossthebridgeofthenose.featurestotheclassi   er,directlyincreasescomputationtime.4.theattentionalcascadethissectiondescribesanalgorithmforconstructingacascadeofclassi   erswhichachievesincreaseddetec-tionperformancewhileradicallyreducingcomputationtime.thekeyinsightisthatsmaller,andthereforemoreef   cient,boostedclassi   erscanbeconstructedwhichrejectmanyofthenegativesub-windowswhiledetect-ingalmostallpositiveinstances.simplerclassi   ersareusedtorejectthemajorityofsub-windowsbeforemorecomplexclassi   ersarecalledupontoachievelowfalsepositiverates.stagesinthecascadeareconstructedbytrainingclassi   ersusingadaboost.startingwithatwo-featurestrongclassi   er,aneffectiveface   ltercanbeobtainedbyadjustingthestrongclassi   erthresholdtomini-mizefalsenegatives.theinitialadaboostthreshold,12!tt=1  t,isdesignedtoyieldalowerrorrateonthetrainingdata.alowerthresholdyieldshigherdetec-tionratesandhigherfalsepositiverates.basedonper-formancemeasuredusingavalidationtrainingset,thetwo-featureclassi   ercanbeadjustedtodetect100%ofthefaceswithafalsepositiverateof50%.seefig.5foradescriptionofthetwofeaturesusedinthisclassi   er.thedetectionperformanceofthetwo-featureclas-si   erisfarfromacceptableasafacedetectionsystem.neverthelesstheclassi   ercansigni   cantlyreducethenumberofsub-windowsthatneedfurtherprocessingwithveryfewoperations:1.evaluatetherectanglefeatures(requiresbetween6and9arrayreferencesperfeature).2.computetheweakclassi   erforeachfeature(re-quiresonethresholdoperationperfeature).3.combinetheweakclassi   ers(requiresonemultiplyperfeature,anaddition,and   nallyathreshold).atwofeatureclassi   eramountstoabout60mi-croprocessorinstructions.itseemshardtoimaginethatanysimpler   ltercouldachievehigherrejectionrates.bycomparison,scanningasimpleimagetem-platewouldrequireatleast20timesasmanyoperationspersub-window.theoverallformofthedetectionprocessisthatofadegeneratedecisiontree,whatwecalla   cascade   (quinlan,1986)(seefig.6).apositiveresultfromthe   rstclassi   ertriggerstheevaluationofasecondclassi   erwhichhasalsobeenadjustedtoachieveveryhighdetectionrates.apositiveresultfromthesecondclassi   ertriggersathirdclassi   er,andsoon.anegativeoutcomeatanypointleadstotheimmediaterejectionofthesub-window.thestructureofthecascadere   ectsthefactthatwithinanysingleimageanoverwhelmingmajorityofsub-windowsarenegative.assuch,thecascadeat-temptstorejectasmanynegativesaspossibleattheearlieststagepossible.whileapositiveinstancewillfigure6.schematicdepictionofathedetectioncascade.aseriesofclassi   ersareappliedtoeverysub-window.theinitialclassi   ereliminatesalargenumberofnegativeexampleswithverylittlepro-cessing.subsequentlayerseliminateadditionalnegativesbutrequireadditionalcomputation.afterseveralstagesofprocessingthenum-berofsub-windowshavebeenreducedradically.furtherprocessingcantakeanyformsuchasadditionalstagesofthecascade(asinourdetectionsystem)oranalternativedetectionsystem.face detection (example results)

152violaandjonesfigure10.outputofourfacedetectoronanumberoftestimagesfromthemit+cmutestset.6.conclusionswehavepresentedanapproachforfacedetectionwhichminimizescomputationtimewhileachievinghighdetectionaccuracy.theapproachwasusedtocon-structafacedetectionsystemwhichisapproximately15timesfasterthananypreviousapproach.preliminaryexperiments,whichwillbedescribedelsewhere,showthathighlyef   cientdetectorsforotherobjects,suchaspedestriansorautomobiles,canalsobeconstructedinthisway.thispaperbringstogethernewalgorithms,represen-tations,andinsightswhicharequitegenericandmaywellhavebroaderapplicationincomputervisionandimageprocessing.the   rstcontributionisanewatechniqueforcom-putingarichsetofimagefeaturesusingtheintegralimage.inordertoachievetruescaleinvariance,almostallfacedetectionsystemsmustoperateonmultipleimagescales.theintegralimage,byeliminatingtheneedtocomputeamulti-scaleimagepyramid,reducestheinitialimageprocessingrequiredforfacedetectionanalysis of boosting

analysis of boosting

training error theorem
we can use analysis to make a statement about the accuracy of boosting on
the training data.

theorem: under the adaboost framework, if  t is the weighted error of

classi   er ft, then for the classi   er fboost(x0) = sign((cid:80)t
(cid:16)     2

1{yi (cid:54)= fboost(xi)}     exp

training error =

n(cid:88)

t=1   tft(x0)),

t(cid:88)

2      t)2(cid:17)

.

( 1

1
n

i=1

t=1

even if each  t is only a little better than random guessing, the sum over t
classi   ers can lead to a large negative value in the exponent when t is large.

for example, if we set:

 t = 0.45, t = 1000     training error     0.0067.

proof of theorem

setup
we break the proof into three steps. it is an application of the fact that

if

and

then

(cid:124)(cid:123)(cid:122)(cid:125)

a < b
step 2

(cid:124)(cid:123)(cid:122)(cid:125)

b < c
step 3

(cid:124)(cid:123)(cid:122)(cid:125)

a < c
conclusion

(cid:73) step 1 calculates the value of b.
(cid:73) steps 2 and 3 prove the two inequalities.

also recall the following step from adaboost:

(cid:73) update   wt+1(i) = wt(i)e     tyi ft(xi).
(cid:73) normalize wt+1(i) =

(cid:80)

  wt+1(i)
j   wt+1(j)

       de   ne zt =(cid:80)

j   wt+1(j).

proof of theorem (a     b     c)

step 1
we    rst want to expand the equation of the weights to show that

wt+1(i) =

1
n

e   yi

(cid:80)t
(cid:81)t
t=1   t ft(xi)
t=1 zt

(cid:81)t

e   yi ht (xi)
t=1 zt

:=

1
n

t(cid:88)

t=1

    ht (x) :=

  t ft(xi)

derivation of step 1:
notice the update rule: wt+1(i) =

wt(i)e     tyi ft(xi)

1
zt

do the same expansion for wt(i) and continue until reaching w1(i) = 1
n,

the product(cid:81)t

wt+1(i) = w1(i)

e     1yi f1(xi)

z1

             e     t yi ft (xi)

zt

t=1 zt is    b    above. we use this form of wt+1(i) in step 2.

proof of theorem (a     b     c)

step 2
next show the training error of f (t)
currently we know

wt+1(i) =

(cid:81)t

e   yi ht (xi)
t=1 zt

1
n

    wt+1(i)

t=1

boost (boosting after t steps) is     (cid:81)t
t(cid:89)

e   yi ht (xi) & f (t)

zt =

1
n

boost(x) = sign(ht (x))

t=1 zt.

derivation of step 2:
observe that 0 < ez1 and 1 < ez2 for any z1 < 0 < z2. therefore

n(cid:88)

i=1

1
n

(cid:124)

1{yi (cid:54)= f (t)

boost(xi)}
(cid:125)

(cid:123)(cid:122)

a

    1
n

n(cid:88)
n(cid:88)

i=1

e   yi ht (xi)

t(cid:89)

=

wt+1(i)

zt =

   a    is the training error     the quantity we care about.

i=1

t=1

t(cid:89)
(cid:124)(cid:123)(cid:122)(cid:125)

t=1

zt

b

proof of theorem (a     b     c)
the    nal step is to calculate an upper bound on zt, and by extension(cid:81)t
(cid:16) 1    t
(cid:17)

derivation of step 3:
this step is slightly more involved. it also shows why   t := 1

step 3

2 ln

.

 t

t=1 zt.

zt =

n(cid:88)
(cid:88)
remember we de   ned  t =(cid:80)

i=1

=

wt(i)e     tyi ft(xi)

e     twt(i) +

(cid:88)

i : yi(cid:54)=ft(xi)

e  twt(i)

i : yi=ft(xi)

= e     t (1      t) + e  t  t

i : yi(cid:54)=ft(xi) wt(i), the id203 of error for wt.

proof of theorem (a     b     c)

derivation of step 3 (continued):
remember from step 2 that

training error =

1
n

n(cid:88)

1{yi (cid:54)= fboost(xi)}     t(cid:89)

i=1

t=1

zt .

and we just showed that zt = e     t (1      t) + e  t  t.

we want the training error to be small, so we pick   t to minimize zt.
minimizing, we get the value of   t used by adaboost:

(cid:19)
plugging this value back in gives zt = 2(cid:112) t(1      t).

(cid:18)1      t

  t =

1
2

ln

 t

.

proof of theorem (a     b     c)

derivation of step 3 (continued):

next, re-write zt as

zt = 2(cid:112) t(1      t)

(cid:114)

=

1     4(

1
2

     t)2

then, use the inequality 1     x     e   x to conclude that

zt =(cid:0)1     4( 1

2    (cid:16)
2      t)2(cid:1) 1

2    t)2(cid:17) 1

2

e   4( 1

= e   2( 1

2    t)2

.

   2   1012   1.5   1   0.500.511.522.533.5e-x1-xproof of theorem

concluding the right inequality (a     b     c)
because both sides of zt     e   2( 1
= e   2(cid:80)t
this concludes the    b     c    portion of the proof.

zt     t(cid:89)

t(cid:89)

e   2( 1

2    t)2

t=1( 1

t=1

t=1

2    t)2

.

2    t)2 are positive, we can say that

combining everything

(cid:125)(cid:124)

a

(cid:122)

1
n

n(cid:88)

i=1

(cid:123)

b(cid:122)(cid:125)(cid:124)(cid:123)
t(cid:89)

zt    

t=1

(cid:122)
(cid:125)(cid:124)
e   2(cid:80)t

c

t=1( 1

2    t)2

.

(cid:123)

training error =

1{yi (cid:54)= fboost(xi)}    

we set out to prove    a < c    and we did so by using    b    as a stepping-stone.

training vs testing error

q: driving the training error to zero leads one to ask, does boosting over   t?
a: sometimes, but very often it doesn   t!

c4.5 (tree) testing erroradaboost testing erroradaboost training errorrounds of boostingerror