coms 4721: machine learning for data science

lecture 10, 2/21/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

feature expansions

feature expansions

feature expansions (also called basis expansions) are names given to a
technique we   ve already discussed and made use of.

problem: a linear model on the original feature space x     rd doesn   t work.
solution: map the features to a higher dimensional space   (x)     rd, where

d > d, and do linear modeling there.

examples

(cid:73) for polynomial regression on r, we let   (x) = (x, x2, . . . , xp).
(cid:73) for jump discontinuities,   (x) = (x, 1{x < a}).

mapping example for regression

(a) data for id75

(b) same data mapped to higher dimension

high-dimensional maps can transform the data so output is linear in inputs.

left: original x     r and response y.
right: x mapped to r2 using   (x) = (x, cos x)t.

xyyxcos(x)mapping example for regression

using the mapping   (x) = (x, cos x)t, learn the id75 model

y     w0 +   (x)tw

    w0 + w1x + w2 cos x.

left: learn (w0, w1, w2) to approximate data on the left with a plane.
right: for each point x, map to   (x) and predict y. plot as a function of x.

yxcos(x)xymapping example for classification

(e) data for binary classi   cation

(f) same data mapped to higher dimension

high-dimensional maps can transform data so it becomes linearly separable.

left: original data in r2.
right: data mapped to r3 using   (x) = (x2

1, x1x2, x2

2)t.

x1x2x1x2x12x22mapping example for classification

using the mapping   (x) = (x2

1, x1x2, x2

2)t, learn a linear classi   er

y = sign(w0 +   (x)tw)

= sign(w0 + w1x2

1 + w2x1x2 + w3x2
2).

left: learn (w0, w1, w2, w3) to linearly separate classes with hyperplane.
right: for each point x, map to   (x) and classify. color decision regions in r2.

x1x2x12x22x1x2feature expansions and dot products

what expansion should i use?
this is not obvious. the illustrations required knowledge about the data that
we likely won   t have (especially if it   s in high dimensions).

one approach is to use the    kitchen sink   : if you can think of it, then use it.
select the useful features with an (cid:96)1 penalty

w(cid:96)1 = arg min
w

f (yi,   (xi), w) +   (cid:107)w(cid:107)1.

n(cid:88)

i=1

we know that this will    nd a sparse subset of the dimensions of   (x) to use.

often we only need to work with dot products   (xi)t   (xj)     k(xi, xj). this
is called a kernel and can produce some interesting results.

kernels

id88 (some motivation)

w =(cid:80)

id88 classi   er
let xi     rd+1 and yi     {   1, +1} for i = 1, . . . , n observations. we saw that
the id88 constructs the hyperplane from data,

i   m yixi,

(assume    = 1 and m has no duplicates)

where m is the sequentially constructed set of misclassi   ed examples.
predicting new data
we also discussed how we can predict the label y0 for a new observation x0:

y0 = sign(xt

0 w) = sign(cid:0)(cid:80)
y0 = sign(  (x0)tw) = sign(cid:0)(cid:80)

i   m yixt
0 xi

i   m yi  (x0)t   (xi)(cid:1)

(cid:1)

we   ve taken feature expansions for granted, but we can explicitly write it as

we can represent the decision using dot products between data points.

kernels

kernel de   nition
a kernel k(  ,  ) : rd    rd     r is a symmetric function de   ned as follows:
de   nition: if for any n points x1, . . . , xn     rd, the n    n matrix k, where

kij = k(xi, xj), is positive semide   nite, then k(  ,  ) is a    kernel.   

intuitively, this means k satis   es the properties of a covariance matrix.

mercer   s theorem
if the function k(  ,  ) satis   es the above properties, then there exists a
mapping    : rd     rd (d can equal    ) such that
k(xi, xj) =   (xi)t   (xj).

if we    rst de   ne   (  ) and then k, this is obvious. however, sometimes we
   rst de   ne k(  ,  ) and avoid ever using   (  ).

gaussian kernel (radial basis function)

the most popular kernel is the gaussian kernel, also called the radial basis
function (rbf),

k(x, x(cid:48)) = a exp

(cid:107)x     x(cid:48)(cid:107)2

   1
b

(cid:27)

.

(cid:73) this is a good, general-purpose kernel that usually works well.
(cid:73) it takes into account proximity in rd. things close together in space

have larger value (as de   ned by kernel width b).

in this case, the the mapping   (x) that produces the rbf kernel is in   nite
dimensional (it   s a continuous function instead of a vector). therefore

(cid:26)

(cid:90)

k(x, x(cid:48)) =

  t(x)  t(x(cid:48)) dt.

(cid:73)   t(x) can be thought of as a function of t with parameter x that also has

a gaussian form.

kernels

another kernel

   

   

   

2x1, . . . ,

map :   (x) = (1,

1, . . . , x2
kernel :   (x)t   (x(cid:48)) = k(x, x(cid:48)) = (1 + xtx(cid:48))2
in fact, we can show k(x, x(cid:48)) = (1 + xtx(cid:48))b, for b > 0 is a kernel as well.

2xixj, . . . )

2xd, x2

d, . . . ,

kernel arithmetic
certain functions of kernels can produce new kernels.
let k1 and k2 be any two kernels, then constructing k in the following ways
produces a new kernel (among many other ways):

k(x, x(cid:48)) = k1(x, x(cid:48))k2(x, x(cid:48))
k(x, x(cid:48)) = k1(x, x(cid:48)) + k2(x, x(cid:48))
k(x, x(cid:48)) = exp{k1(x, x(cid:48))}

kernelized id88

returning to the id88
we write the feature-expanded decision as

y0 = sign(cid:0)(cid:80)
= sign(cid:0)(cid:80)
(cid:16)(cid:80)

y0 = sign

i   m yi  (x0)t   (xi)(cid:1)
i   m yik(x0, xi)(cid:1)
b(cid:107)x0   xi(cid:107)2(cid:17)

i   m yi e    1

we can pick the kernel we want to use. let   s pick the rbf (set a = 1). then

notice that we never actually need to calculate   (x).

what is this doing?
(cid:73) notice 0 < k(x0, xi)     1, with bigger values when x0 is closer to xi.
(cid:73) this is like a    soft voting    among the data picked by id88.

kernelized id88

learning the kernelized id88

i   mt
1. find a new x(cid:48) such that y(cid:48) (cid:54)= sign(x(cid:48)tw(t))

recall: given a current vector w(t) =(cid:80)
2. add the index of x(cid:48) to m and set w(t+1) =(cid:80)
1. find a new x(cid:48) such that y(cid:48) (cid:54)= sign((cid:80)

yik(x(cid:48), xi))

i   mt

i   mt+1

yixi

again we only need dot products, meaning these steps are equivalent to

yixi, we update it as follows,

2. add the index of x(cid:48) to m but don   t bother calculating w(t+1)

the trick is to realize that we never need to work with   (x).

(cid:73) we don   t need   (x) to do step 1 above.
(cid:73) we don   t need   (x) to classify new data (previous slide).
(cid:73) we only ever need to calculate k(x, x(cid:48)) between two points.

kernel id92

an extension
we can generalize kernelized id88 to soft id92 with a simple change.
instead of summing over misclassi   ed data m, sum over all the data:

(cid:16)(cid:80)n

b(cid:107)x0   xi(cid:107)2(cid:17)

.

y0 = sign

i=1 yi e    1

next, notice the decision doesn   t change if we divide by a positive constant.

let : z =(cid:80)n

j=1 e    1

b(cid:107)x0   xj(cid:107)2

(cid:16)(cid:80)n

(cid:17)

construct : vector p(x0), where pi(x0) = 1

z e    1

b(cid:107)x0   xi(cid:107)2

declare : y0 = sign

i=1 yipi(x0)

(cid:73) we let all data vote for the label based on a    con   dence score    p(x0).
(cid:73) set b so that most pi(x0)     0 to only focus on neighborhood around x0.

kernel regression

nadaraya-watson model
the developments are almost limitless.

here   s a regression example almost identical to the kernelized id92:

before: y     {   1, +1}
now: y     r

using the rbf kernel, for a new (x0, y0) predict

n(cid:88)

i=1

(cid:80)n

yi

y0 =

k(x0, xi)
j=1 k(x0, xj)

.

what is this doing?
we   re taking a locally weighted average of all yi for which xi is close to x0
(as decided by the kernel width). gaussian processes are another option. . .

gaussian processes

kernelized bayesian id75

regression setup: for n observations, with response vector y     rn and their
feature matrix x, we de   ne the likelihood and prior

y     n(xw,   2i), w     n(0,      1i).

marginalizing: what if we integrate out w? we can solve this,

p(y|x) =

p(y|x, w)p(w)dw = n(0,   2i +      1xxt ).

kernelization: notice that (xxt )ij = xt
i xj. replace each x with   (x) after
which we can say [  (x)  (x)t ]ij = k(xi, xj). we can de   ne k directly, so

p(y|x) =

p(y|x, w)p(w)dw = n(0,   2i +      1k).

this is called a gaussian process. we never use w or   (x), but just k(xi, xj).

(cid:90)

(cid:90)

gaussian processes

de   nition

    let f (x)     r and x     rd.
    de   ne the kernel k(x, x(cid:48)) between two points x and x(cid:48).
    then f (x) is a gaussian process and y(x) the noise-added process if for

n observed pairs (x1, y1), . . . , (xn, yn), where x     x and y     r,
y| f     n(f ,   2i),
f     n(0, k)        y     n(0,   2i + k)
where y = (y1, . . . , yn)t and k is n    n with kij = k(xi, xj).

comments:

(cid:73) we assume    = 1 to reduce notation.
(cid:73) typical breakdown: f (x) is the gp and y(x) equals f (x) plus i.i.d. noise.
(cid:73) the kernel is what keeps this from being    just a gaussian.   

gaussian processes

above: a gaussian process f (x) generated using

(cid:26)

   (cid:107)xi     xj(cid:107)2

b

(cid:27)

.

k(xi, xj) = exp

right: the covariance of f (x) de   ned by k.

00.10.20.30.40.50.60.70.80.91   2   10123xf(x)0011gaussian processes

top: unobserved underlying function,
bottom: noisy observed data sampled from this function

00.10.20.30.40.50.60.70.80.91   2   10123xf(x)00.10.20.30.40.50.60.70.80.91   2   10123xy(x)ooooooooooooooooooooooooopredictions with gaussian vectors

bayesian id75
imagine we have n observation pairs d = {(xi, yi)}n
y0 given x0. integrating out w and setting    = 1, the joint distribution is

i=1 and want to predict

(cid:21)

(cid:20) y0

y

(cid:18)

(cid:20) xt

    normal

0 ,   2i +

0 x0
xx0

(xx0)t
xxt

(cid:21)(cid:19)

we want to predict y0 given d and x0. calculations can show that

y0|d, x0     normal(  0,   2
0)

  0 = (xx0)t (  2i + xxt )   1y
0 =   2 + xt
  2

0 x0     (xx0)t (  2i + xxt )   1(xx0)

the since the in   nite gaussian process is only evaluated at a    nite set of
points, we can use this fact.

predictions with gaussian processes

predictive distribution of y(x)
given measured data dn = {(x1, y1), . . . , (xn, yn)}, the distribution of y(x)
can be calculated at any new x to make predictions.
let k(x,dn) = [k(x, x1), . . . , k(x, xn)] and kn be the n    n kernel matrix
restricted points in dn. then we can show

y(x)|dn     n (  (x),   (x)) ,

  (x) = k(x,dn)(  2i + kn)   1y,
  (x) =   2 + k(x, x)     k(x,dn)(  2i + kn)   1k(x,dn)t

for the posterior of f (x) instead of y(x), just remove   2.

gaussian processes posterior

what does the posterior distribution of f (x) look like?
(cid:73) we have data marked by an   .
(cid:73) these values pin down the function f (x) nearby
(cid:73) we get a mean and variance for every possible x from a previous slide.
(cid:73) the distribution on y(x) adds variance   2 (very small above) point-wise.

meanstandarddevobservedvaluestruthxf(x)