cmsc 422 introduction to machine learning
lecture 4 geometry and nearest neighbors

furong huang / furongh@cs.umd.edu

what we know so far

id90
    what is a decision tree, and how to induce it from data

fundamental machine learning concepts
    difference between memorization and generalization
    what inductive bias is, and what is its role in learning
    what underfitting and overfitting means
    how to take a task and cast it as a learning problem
    why you should never ever touch your test data!!

today   s topics

    nearest neighbors (nn) algorithms for 

classification
   id92, epsilon ball nn

    fundamental machine learning 

concepts
   decision boundary

id202

   

   

   

   
   

provides compact representation of data

  

  

for a given example, all its features can be represented as a 
single vector
an entire dataset can be represented as a single matrix
provide ways of manipulating these objects

  

dot products, vector/matrix operations, etc

provides formal ways of describing and discovering 
patterns in data

examples are points in a vector space

  
   we can use norms and distances to compare them
some are valid for feature data types
some can be made valid, with generalization ...

mathematical view of vectors

    ordered set of numbers: (1,2,3,4)
    example: (x,y,z) coordinates of a point in space.
the 16384 pixels in a 128  128 image of a face
   
list of choices in the tennis example
   
    vectors usually indicated with bold lower case letters. 

    usual mathematical operations with vectors:

scalars with lower case

   addition operation !	+$, with:
v zero%
$+%=$
inverse    $+    $ =%

v

   scalar multiplication:
v distributive rule: 

(!	+$ =(!+($
((+*)!=(!+*!

    the dot product or, more generally, inner product of 

dot product

!!	    !"=$!$"+&!&"+'!'"

two vectors is a scalar:

    useful for many purposes

   computing the euclidean length of a vector:

length(v) = sqrt(v    v)

(in 3d)

   normalizing a vector, making it unit-length
   computing the angle between two vectors: u    v= |u| |v| cos(  )
   checking two vectors for orthogonality
   projecting one vector onto another

    other ways of measuring length and distance are 

possible

vector norms !=($%,$',   ,$))
)
    two norm (euclidean norm)! '= +$,'
,-%
if ! '=1, ! is a unit vector
! /=max $%,$',   ,$)
)
! %=+$,
,-%

one norm (manhattan distance)

infinity norm

   

   

law of large numbers

    suppose that !!,!",   !# are independent and 

identically distributed random variables

    the empirical sample average approaches the 

population average as the number of sample 
goes to infinity

pr

lim#   %1*+!&
&

=-! =1

nearest neighbor

   

intuition   points close in a feature space 
are likely to belong to the same class
   choosing right features is very important

    nearest neighbors (nn) algorithms for 

    fundamental machine learning 

classification
   id92, epsilon ball nn

concepts
   decision boundary

intuition for nearest
neighbor classification

this    rule of nearest neighbor    has considerable 
elementary intuitive appeal and probably 
corresponds to practice in many situations. for 
example, it is possible that much medical 
diagnosis is influenced by the doctor   s recollection 
of the subsequent history of an earlier patient 
whose symptoms resemble in some way those of 
the current patient.

(fix and hodges, 1952)

intuition for nearest
neighbor classification

    simple idea

   store all training examples
   classify new examples based on label for k 

closest training examples

   training may just involve making structures to 

make computing closest examples cheaper

k nearest neighbor classification

based on

k: number of 
neighbors that 
classification is 

{	   1;	+1	}

test instance with 
unknown class in 

training data

2 approaches to learning

eager learning
(eg id90)
    learn/train

   induce an abstract model 

from data

    test/predict/classify

   apply learned model to new 

data

lazy learning
(eg nearest neighbors)
    learn

   just store data in memory

    test/predict/classify

   compare new data to stored 

data

    properties

training 

   retains all information seen in 

   complex hypothesis space
   classification can be very slow

components of a id92 classifier

    distance metric

   how do we measure distance between instances?
   determines the layout of the example space

    the k hyperparameter

   how large a neighborhood should we consider?
   determines the complexity of the hypothesis space

distance metrics
    we can use any distance function to select 

nearest neighbors.

    different distances yield different neighborhoods

l2 distance 
( = euclidean distance)

l1 distance

max norm

k=1 and voronoi diagrams

   

imagine we are given a 
bunch of training 
examples

    find regions in the 

feature space which are 
closest to every training 
example

    algorithm   if our test point 

is in the region 
corresponding to a given 
input point   return its label

decision boundary of a classifier

   

it is simply the line that separates 
positive and negative regions in the 
feature space

    why is it useful?

   it helps us visualize how examples will be 

classified for the entire feature space

   it helps us visualize the complexity of the 

learned model

decision boundaries for 1-nn

decision  boundaries change with the 
distance function

decision boundaries change with k

the k hyperparameter

    tunes the complexity of the hypothesis 

space
   if k = 1, every training example has its own 

   if k = n, the entire feature space is one 

    higher k yields smoother decision 

neighborhood

neighborhood!

boundaries

    how would you set k in practice?

what is the inductive bias of
id92?

    nearby instances should have the same label

    all features are equally important

    complexity is tuned by the k parameter

variations on id92:
weighted voting

    weighted voting

   default: all neighbors have equal weight
   extension: weight neighbors by (inverse) 

distance

variations on id92:
epsilon ball nearest neighbors

    same general principle as id92, but change 

the method for selecting which training 
examples vote

   

instead of using k nearest neighbors, use all 
examples x such that

!"#$%&'()*,)    	.

exercise:  how would you modify 
knn-predict  to perform epsilon ball 
nn?

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

