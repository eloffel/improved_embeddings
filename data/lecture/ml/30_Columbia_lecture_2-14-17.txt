coms 4721: machine learning for data science

lecture 8, 2/14/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

linear classification

binary classification

we focus on binary classi   cation, with input xi     rd and output yi     {  1}.
(cid:73) we de   ne a classi   er f , which makes prediction yi = f (xi,   ) based on
a function of xi and parameters   . in other words f : rd     {   1, +1}.

last lecture, we discussed the bayes classi   cation framework.

(cid:73) here,    contains: (1) class prior probabilities on y,

(2) parameters for class-dependent distribution on x.

this lecture we   ll introduce the linear classi   cation framework.
(cid:73) in this approach the prediction is linear in the parameters   .
(cid:73) in fact, there is an intersection between the two that we discuss next.

a bayes classifier

bayes decisions
with the bayes classi   er we predict the class of a new x to be the most
probable label given the model and training data (x1, y1), . . . , (xn, yn).

in the binary case, we declare class y = 1 if

(cid:123)(cid:122)
(cid:125)
p(x|y = 1) p(y = 1)

(cid:124)

  1

(cid:123)(cid:122)
(cid:125)
> p(x|y = 0) p(y = 0)

(cid:124)

  0

ln

p(x|y = 1)p(y = 1)
p(x|y = 0)p(y = 0)

(cid:109)
> 0

this second line is referred to as the log odds.

a bayes classifier

gaussian with shared covariance
let   s look at the log odds for the special case where

(i.e., a single gaussian with a shared covariance matrix)

p(x|y) = n(x|  y,   )

ln

p(x|y = 1)p(y = 1)
p(x|y = 0)p(y = 0)

= ln   1

  0    

(cid:124)

(cid:125)
   1(  1       0)

(cid:123)(cid:122)
1
2 (  1 +   0)t   
(cid:125)
(cid:123)(cid:122)
   1(  1       0)

a constant, call it w0

a vector, call it w

(cid:124)

+ xt   

this is also called    id156    (used to be called lda).

a bayes classifier

so we can write the decision rule for this bayes classi   er as a linear one:

f (x) = sign(xtw + w0).

(cid:73) this is what we saw last lecture
(but now class 0 is called    1)
(cid:73) the bayes classi   er produced a
linear decision boundary in the
data space when   1 =   0.

(cid:73) w and w0 are obtained through a

speci   c equation.

2.6.discriminantfunctionsforthenormaldensity21-2240.10.20.30.4p(  1)=.5p(  2)=.5p(x|  i)x  1  2r1r2-2024-202400.050.10.15-2024-2024r1r2p(  2)=.5p(  1)=.5-2-1012012-2-1012-2-1012012p(  1)=.5p(  2)=.5r2r1figure2.10:ifthecovariancesoftwodistributionsareequalandproportionaltotheidentitymatrix,thenthedistributionsaresphericalinddimensions,andtheboundaryisageneralizedhyperplaneofd   1dimensions,perpendiculartothelineseparatingthemeans.inthese1-,2-,and3-dimensionalexamples,weindicatep(x|  i)andtheboundariesforthecasep(  1)=p(  2).inthe3-dimensionalcase,thegridplaneseparatesr1fromr2.wi=1  2  i(52)andwi0=   12  2  ti  i+lnp(  i).(53)wecallwi0thethresholdorbiasintheithdirection.thresholdbiasaclassi   erthatuseslineardiscriminantfunctionsiscalledalinearmachine.thislinearmachinekindofclassi   erhasmanyinterestingtheoreticalproperties,someofwhichwillbediscussedindetailinchap.??.atthispointwemerelynotethatthedecisionsurfacesforalinearmachinearepiecesofhyperplanesde   nedbythelinearequationsgi(x)=gj(x)forthetwocategorieswiththehighestposteriorprobabilities.forourparticularcase,thisequationcanbewrittenaswt(x   x0)=0,(54)wherew=  i     j(55)andx0=12(  i+  j)     2     i     j   2lnp(  i)p(  j)(  i     j).(56)thisequationde   nesahyperplanethroughthepointx0andorthogonaltothevectorw.sincew=  i     j,thehyperplaneseparatingriandrjisorthogonaltothelinelinkingthemeans.ifp(  i)=p(  j),thesecondtermontherightofeq.56vanishes,andthusthepointx0ishalfwaybetweenthemeans,andthehyperplaneistheperpendicularbisectorofthelinebetweenthemeans(fig.2.11).ifp(  i)  =p(  j),thepointx0shiftsawayfromthemorelikelymean.note,however,thatifthevariancelinear classifiers

this bayes classi   er is one instance of a linear classi   er

f (x) = sign(xtw + w0)

where

w0 = ln   1

  0    
   1(  1       0)
with id113 used to    nd values for   y,   y and   .

w =   

1
2 (  1 +   0)t   

   1(  1       0)

setting w0 and w this way may be too restrictive:

(cid:73) this bayes classi   er assumes single gaussian with shared covariance.
(cid:73) maybe if we relax what values w0 and w can take we can do better.

linear classifiers (binary case)

de   nition: binary linear classi   er
a binary linear classi   er is a function of the form

f (x) = sign(xtw + w0),

where w     rd and w0     r. since the goal is to learn w, w0 from data, we are
assuming that linear separability in x is an accurate property of the classes.

de   nition: linear separability
two sets a, b     rd are called linearly separable if

(cid:40)

xtw + w0

> 0
< 0

if x     a (e.g, class +1)
if x     b (e.g, class    1)

the pair (w, w0) de   nes an af   ne hyperplane. it is important to develop the
right geometric understanding about what this is doing.

hyperplanes

geometric interpretation of linear classi   ers:

x2

h

a hyperplane in rd is a linear subspace of
dimension (d     1).
(cid:73) a r2-hyperplane is a line.
(cid:73) a r3-hyperplane is a plane.
(cid:73) as a linear subspace, a hyperplane

always contains the origin.

x1

w

a hyperplane h can be represented by a
vector w as follows:

(cid:110)
x     rd | xtw = 0

(cid:111)

.

h =

which side of the plane are we on?

h

x

w

  

(cid:107)x(cid:107)2    cos   

distance from the plane

(cid:73) how close is a point x to h?
(cid:73) cosine rule: xtw = (cid:107)x(cid:107)2(cid:107)w(cid:107)2 cos   
(cid:73) the distance of x to the hyperplane is
(cid:107)x(cid:107)2    | cos   | = |xtw|/(cid:107)w(cid:107)2.

so |xtw| gives a sense of distance.

which side of the hyperplane?
(cid:73) the cosine satis   es cos    > 0 if        (      
(cid:73) so the sign of cos(  ) tells us the side of h, and by the cosine rule

2 ,   

2 ).

sign(cos   ) = sign(xtw).

affine hyperplanes

x2

h

af   ne hyperplanes

(cid:73) an af   ne hyperplane h is a hyperplane

translated (shifted) using a scalar w0.

(cid:73) think of: h = xtw + w0 = 0.
(cid:73) setting w0 > 0 moves the hyperplane in the
opposite direction of w. (w0 < 0 in    gure)

w

x1

   w0/(cid:107)w(cid:107)2

which side of the hyperplane now?
(cid:73) the plane has been shifted by distance    w0(cid:107)w(cid:107)2
(cid:73) for a given w, w0 and input x the inequality xtw + w0 > 0 says that x is

in the direction w.

on the far side of an af   ne hyperplane h in the direction w points.

classification with affine hyperplanes

   w0(cid:107)w(cid:107)2

w

sign(xtw + w0) > 0

h

sign(xtw + w0) < 0

polynomial generalizations

the same generalizations from regression also hold for classi   cation:

(cid:73) (left) a linear classi   er using x = (x1, x2).
(cid:73) (right) a linear classi   er using x = (x1, x2, x2

1, x2
2).

the decision boundary is linear in r4, but isn   t when plotted in r2.

another bayes classifier

gaussian with different covariance
let   s look at the log odds for the general case where p(x|y) = n(x|  y,   y)
(i.e., now each class has its own covariance)
(cid:125)

= something complicated not involving x

p(x|y = 1)p(y = 1)
p(x|y = 0)p(y = 0)

(cid:123)(cid:122)

(cid:124)

ln

(cid:124)
(cid:124)

+ xt (  

a constant
   1
0   0)

(cid:123)(cid:122)
   1
1   1       
(cid:123)(cid:122)
   1
0 /2       

a part that   s linear in x
   1
1 /2)x

+ xt (  

a part that   s quadratic in x

(cid:125)
(cid:125)

also called    quadratic discriminant analysis,    but it   s linear in the weights.

another bayes classifier

(cid:73) we also saw this last lecture.

(cid:73) notice that

f (x) = sign(xtax + xtb + c)

is linear in a, b, c.
(cid:73) when x     r2, rewrite as
x     (x1, x2, 2x1x2, x2

1, x2
2)

and do linear classi   cation in r5.

whereas the bayes classi   er with shared covariance is a version of linear
classi   cation, using different covariances is like polynomial classi   cation.

26chapter2.bayesiandecisiontheory-1001020-10010200.01.02.03.04p-1001020-20-1001020-100102000.0050.01p-20-1001020-1001020-1001000.010.020.03p-1001020-1001020-1001000.010.020.030.040.05p-1001020-1001020-100102000.0050.01p-100102000.0050.01p-505-50500.050.10.15p-505figure2.14:arbitrarygaussiandistributionsleadtobayesdecisionboundariesthataregeneralhyperquadrics.conversely,givenanyhyperquadratic,onecan   ndtwogaussiandistributionswhosebayesdecisionboundaryisthathyperquadric.least squares on {   1, +1}

how do we de   ne more general classi   ers of the form

f (x) = sign(xtw + w0) ?

(cid:73) one simple idea is to treat classi   cation as a regression problem:

1. let y = (y1, . . . , yn)t, where yi     {   1, +1} is the class of xi.
2. add dimension equal to 1 to xi and construct the matrix x = [x1, . . . , xn]t.
3. learn the least squares weight vector w = (xtx)   1xty.
4. for a new point x0 declare y0 = sign(xt

0 w)        w0 is included in w.

(cid:73) another option: instead of ls, use (cid:96)p id173.

(cid:73) these are    baseline    options. we can use them, along with id92, to get

a quick sense what performance we   re aiming to beat.

sensitivity to outliers

least squares can do well, but it is sensitive to outliers. in general we can
   nd better classi   ers that focus more on the decision boundary.

(cid:73) (left) least squares (purple) does well compared with another method
(cid:73) (right) least squares does poorly because of outliers

   4   202468   8   6   4   2024   4   202468   8   6   4   2024the id88 algorithm

easy case: linearly separable data

(assume data xi has a 1 attached.)

suppose there is a linear classi   er
with zero training error:

yi = sign(xt

i w),

for all i.

then the data is    linearly separable   

left: can separate classes with a line.
(can    nd an in   nite number of lines.)

id88 (rosenblatt, 1958)

using the linear classi   er

y = f (x) = sign(xtw),

n(cid:88)

the id88 seeks to minimize

i w)}.

i=1

i w)1{yi (cid:54)= sign(xt

(yi  xt
(cid:40)

l =    
because y     {   1, +1},
yi  xt

i w is

> 0 if yi = sign(xt
< 0 if yi (cid:54)= sign(xt
by minimizing l we   re trying to
always predict the correct label.

i w)
i w)

learning the id88

(cid:73) unlike other techniques we   ve talked about, we can   t    nd the minimum

of l by taking a derivative and setting to zero:

   wl = 0 cannot be solved for w analytically.

however    wl does tell us the direction in which l is increasing in w.

(cid:73) therefore, for a suf   ciently small   , if we update

w(cid:48)

    w          wl,

then l(w(cid:48)) < l(w)     i.e., we have a better value for w.
(cid:73) this is a very general method for optimizing an objective functions

called id119. id88 uses a    stochastic    version of this.

learning the id88

input: training data (x1, y1), . . . , (xn, yn) and a positive step size   

1. set w(1) = (cid:126)0
2. for step t = 1, 2, . . . do

a) search for all examples (xi, yi)     d such that yi (cid:54)= sign(xt
b) if such a (xi, yi) exists, randomly pick one and update

i w(t))

w(t+1) = w(t) +   yixi,

else: return w(t) as the solution since everything is classi   ed correctly.

(cid:88)
if mt indexes the misclassi   ed observations at step t, then we have

n(cid:88)

l =    

i=1

(yi    xt

i w)1{yi (cid:54)= sign(xt

i w)},

   wl =    

i   mt

yixi .

the full gradient step is w(t+1) = w(t)          wl. stochastic optimization just
picks out one element in    wl    we could have also used the full summation.

learning the id88

red = +1, blue =    1,    = 1
1. pick a misclassi   ed (xi, yi)
2. set w     w +   yixi

   1   0.500.51   1   0.500.51learning the id88

red = +1, blue =    1,    = 1
the update to w de   nes a new
decision boundary (hyperplane)

   1   0.500.51   1   0.500.51learning the id88

red = +1, blue =    1,    = 1
1. pick another misclassi   ed (xj, yj)
2. set w     w +   yjxj

   1   0.500.51   1   0.500.51learning the id88

red = +1, blue =    1,    = 1
again update w, i.e., the hyperplane

this time we   re done.

   1   0.500.51   1   0.500.51drawbacks of id88

the id88 represents a    rst attempt at linear classi   cation by directly
learning the hyperplane de   ned by w. it has some drawbacks:

1. when the data is separable, there are an in   nite # of hyperplanes.

(cid:73) we may think some are better than others, but this algorithm doesn   t take

   quality    into consideration. it converges to the    rst one it    nds.

2. when the data isn   t separable, the algorithm doesn   t converge. the

hyperplane of w is always moving around.

(cid:73) it   s hard to detect this since it can take a long time for the algorithm to

converge when the data is separable.

later, we will discuss algorithms that use the same idea of directly learning
the hyperplane w, but alters the objective function to    x these problems.

