coms 4721: machine learning for data science

lecture 3, 1/24/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

regression: problem definition

data
measured pairs (x, y), where x     rd+1 (input) and y     r (output)

goal
find a function f : rd+1     r such that y     f (x; w) for the data pair (x, y).
f (x; w) is the regression function and the vector w are its parameters.

de   nition of id75
a regression method is called linear if the prediction f is a linear function of
the unknown parameters w.

least squares (continued)

least squares id75

least squares solution
least squares    nds the w that minimizes the sum of squared errors. the least
squares objective in the most basic form where f (x; w) = xtw is

n(cid:88)

l =

(yi     xt

i w)2 = (cid:107)y     xw(cid:107)2 = (y     xw)t (y     xw).

i=1

we de   ned y = [y1, . . . , yn]t and x = [x1, . . . , xn]t.

taking the gradient with respect to w and setting to zero, we    nd that
   wl = 2xtxw     2xty = 0     wls = (xtx)   1xty.

in other words, wls is the vector that minimizes l.

probabilistic view

(cid:73) last class, we discussed the geometric interpretation of least squares.

(cid:73) least squares also has an insightful probabilistic interpretation that

allows us to analyze its properties.

(cid:73) that is, given that we pick this model as reasonable for our problem,

we can ask: what kinds of assumptions are we making?

probabilistic view

recall: gaussian density in n dimensions
(cid:16)    1
assume a diagonal covariance matrix    =   2i. the density is
2  2 (y       )t (y       )

p(y|  ,   2) =

(2    2)

exp

1

n
2

(cid:17)

.

what if we restrict the mean to    = xw
and    nd the maximum likelihood
solution for w?

probabilistic view

maximum likelihood for gaussian id75
plug    = xw into the multivariate gaussian distribution and solve for w
using maximum likelihood.

wml = arg max
w

= arg max

w

ln p(y|   = xw,   2)
2  2(cid:107)y     xw(cid:107)2     n
    1

2

ln(2    2).

least squares (ls) and maximum likelihood (ml) share the same solution:

ls: arg min
w

(cid:107)y     xw(cid:107)2     ml: arg max

w

    1
2  2(cid:107)y     xw(cid:107)2

probabilistic view

(cid:73) therefore, in a sense we are making an independent gaussian noise

assumption about the error,  i = yi     xt

i w.

(cid:73) other ways of saying this:

1) yi = xt

i w +  i,

 i
i w,   2),

ind    n(xt

iid    n(0,   2),
for i = 1, . . . , n,

2) yi
3) y     n(xw,   2i), as on the previous slides.

for i = 1, . . . , n,

(cid:73) can we use this probabilistic line of analysis to better understand the

maximum likelihood (i.e., least squares) solution?

probabilistic view

expected solution
given: the modeling assumption that y     n(xw,   2i).

we can calculate the expectation of the ml solution under this distribution,

(cid:18)

=

(cid:19)
(cid:90) (cid:2)(xtx)   1xty(cid:3) p(y|x, w) dy

e[wml] = e[(xtx)   1xty]
= (xtx)   1xte[y]
= (xtx)   1xtxw
= w

therefore wml is an unbiased estimate of w, i.e., e[wml] = w.

review: an equality from id203

(cid:73) even though the    expected    maximum likelihood solution is the correct

one, should we actually expect to get something near it?

review: an equality from id203

(cid:73) even though the    expected    maximum likelihood solution is the correct

one, should we actually expect to get something near it?

(cid:73) we should also look at the covariance. recall that if y     n(  ,   ), then

var[y] = e[(y     e[y])(y     e[y])t ] =   .

review: an equality from id203

(cid:73) even though the    expected    maximum likelihood solution is the correct

one, should we actually expect to get something near it?

(cid:73) we should also look at the covariance. recall that if y     n(  ,   ), then

var[y] = e[(y     e[y])(y     e[y])t ] =   .

(cid:73) plugging in e[y] =   , this is equivalently written as

var[y] = e[(y       )(y       )t ]

= e[yyt     y  t       yt +     t ]
= e[yyt ]         t

(cid:73) immediately we also get e[yyt ] =    +     t.

probabilistic view

variance of the solution
returning to least squares id75, we wish to    nd

var[wml] = e[(wml     e[wml])(wml     e[wml])t ]

= e[wmlwt

ml]     e[wml]e[wml]t .

1aside: for matrices a, b and vector c, recall that (abc)t = ct bt at.

probabilistic view

variance of the solution
returning to least squares id75, we wish to    nd

var[wml] = e[(wml     e[wml])(wml     e[wml])t ]

= e[wmlwt

ml]     e[wml]e[wml]t .

the sequence of equalities follows:1

var[wml] = e[(xtx)   1xtyytx(xtx)   1]     wwt

1aside: for matrices a, b and vector c, recall that (abc)t = ct bt at.

probabilistic view

variance of the solution
returning to least squares id75, we wish to    nd

var[wml] = e[(wml     e[wml])(wml     e[wml])t ]

= e[wmlwt

ml]     e[wml]e[wml]t .

the sequence of equalities follows:1

var[wml] = e[(xtx)   1xtyytx(xtx)   1]     wwt
= (xtx)   1xte[yyt ]x(xtx)   1     wwt

1aside: for matrices a, b and vector c, recall that (abc)t = ct bt at.

probabilistic view

variance of the solution
returning to least squares id75, we wish to    nd

var[wml] = e[(wml     e[wml])(wml     e[wml])t ]

= e[wmlwt

ml]     e[wml]e[wml]t .

the sequence of equalities follows:1

var[wml] = e[(xtx)   1xtyytx(xtx)   1]     wwt
= (xtx)   1xte[yyt ]x(xtx)   1     wwt
= (xtx)   1xt (  2i + xwwtxt )x(xtx)   1     wwt

1aside: for matrices a, b and vector c, recall that (abc)t = ct bt at.

probabilistic view

variance of the solution
returning to least squares id75, we wish to    nd

var[wml] = e[(wml     e[wml])(wml     e[wml])t ]

= e[wmlwt

ml]     e[wml]e[wml]t .

the sequence of equalities follows:1

var[wml] = e[(xtx)   1xtyytx(xtx)   1]     wwt
= (xtx)   1xte[yyt ]x(xtx)   1     wwt
= (xtx)   1xt (  2i + xwwtxt )x(xtx)   1     wwt
= (xtx)   1xt   2ix(xtx)   1 +       

(xtx)   1xtxwwtxtx(xtx)   1     wwt

1aside: for matrices a, b and vector c, recall that (abc)t = ct bt at.

probabilistic view

variance of the solution
returning to least squares id75, we wish to    nd

var[wml] = e[(wml     e[wml])(wml     e[wml])t ]

= e[wmlwt

ml]     e[wml]e[wml]t .

the sequence of equalities follows:1

var[wml] = e[(xtx)   1xtyytx(xtx)   1]     wwt
= (xtx)   1xte[yyt ]x(xtx)   1     wwt
= (xtx)   1xt (  2i + xwwtxt )x(xtx)   1     wwt
= (xtx)   1xt   2ix(xtx)   1 +       

(xtx)   1xtxwwtxtx(xtx)   1     wwt

=   2(xtx)   1

1aside: for matrices a, b and vector c, recall that (abc)t = ct bt at.

probabilistic view

(cid:73) we   ve shown that, under the gaussian assumption y     n(xw,   2i),

e[wml] = w, var[wml] =   2(xtx)   1.

(cid:73) when there are very large values in   2(xtx)   1, the values of wml are

very sensitive to the measured data y (more analysis later).

(cid:73) this is bad if we want to analyze and predict using wml.

ridge regression

regularized least squares

(cid:73) we saw how with least squares, the values in wml may be huge.

(cid:73) in general, when developing a model for data we often wish to

constrain the model parameters in some way.

(cid:73) there are many models of the form

wopt = arg min
w

(cid:107)y     xw(cid:107)2 +    g(w).

(cid:73) the added terms are

1.    > 0 : a id173 parameter,
2. g(w) > 0 : a penalty function that encourages desired properties about w.

ridge regression

ridge regression is one g(w) that addresses variance issues with wml.

it uses the squared penalty on the regression coef   cient vector w,

wrr = arg min
w

(cid:107)y     xw(cid:107)2 +   (cid:107)w(cid:107)2

the term g(w) = (cid:107)w(cid:107)2 penalizes large values in w.

however, there is a tradeoff between the    rst and second terms that is
controlled by   .
(cid:73) case        0 : wrr     wls
(cid:73) case            : wrr     (cid:126)0

ridge regression solution

objective: we can solve the ridge regression problem using exactly the
same procedure as for least squares,

l = (cid:107)y     xw(cid:107)2 +   (cid:107)w(cid:107)2

= (y     xw)t (y     xw) +   wtw.

solution: first, take the gradient of l with respect to w and set to zero,

   wl =    2xty + 2xtxw + 2  w = 0

then, solve for w to    nd that

wrr = (  i + xtx)   1xty.

ridge regression geometry

there is a tradeoff between
squared error and penalty on w.

we can write both in terms of
level sets: curves where function
evaluation gives the same number.

the sum of these gives a new set
of levels with a unique minimum.

you can check that we can write:

(cid:107)y   xw(cid:107)2 +   (cid:107)w(cid:107)2 = (w   wls)t (xtx)(w   wls) +   wtw + (const. w.r.t. w).

w1w2x1wls  wtw(w-wls)t(xtx)(w-wls)id174

ridge regression is one possible id173 scheme. for this problem, we
   rst assume the following preprocessing steps are done:

1. the mean is subtracted off of y:

y     y     1
n

2. the dimensions of xi have been standardized before constructing x:

xij     (xij       x  j)/    j,

    j =

(xij       x  j)2.

i.e., subtract the empirical mean and divide by the empirical standard
deviation for each dimension.

3. we can show that there is no need for the dimension of 1   s in this case.

yi.

i=1

n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)1

n

n(cid:88)

i=1

some analysis of ridge

regression

ridge regression vs least squares

the solutions to least squares and ridge regression are clearly very similar,

wls = (xtx)   1xty     wrr = (  i + xtx)   1xty.

(cid:73) we can use id202 and id203 to compare the two.

(cid:73) this requires the singular value decomposition, which we review next.

review: singular value decompositions

(cid:73) we can write any n    d matrix x (assume n > d) as x = usvt, where

1. u: n    d and orthonormal in the columns, i.e. utu = i.
2. s: d    d non-negative diagonal matrix, i.e. sii     0 and sij = 0 for i (cid:54)= j.
3. v: d    d and orthonormal, i.e. vtv = vvt = i.

(cid:73) from this we have the immediate equalities

xtx = (usvt )t (usvt ) = vs2vt , xxt = us2ut .

(cid:73) assuming sii (cid:54)= 0 for all i (i.e.,    x is full rank   ), we also have that

(xtx)   1 = (vs2vt )   1 = vs   2vt .

proof: plug in and see that it satis   es de   nition of inverse

(xtx)(xtx)   1 = vs2vtvs   2vt = i.

least squares and the svd

using the svd we can rewrite the variance,

var[wls] =   2(xtx)   1 =   2vs   2vt .

this inverse becomes huge when sii is very small for some values of i.
(aside: this happens when columns of x are highly correlated.)

the least squares prediction for new data is

ynew = xt

newwls = xt

new(xtx)   1xty = xt

newvs   1uty.

when s   1 has very large values, this can lead to unstable predictions.

ridge regression vs least squares i

relationship to least squares solution
recall for two symmetric matrices, (ab)   1 = b   1a   1.

wrr = (  i + xtx)   1xty

= (  i + xtx)   1(xtx) (xtx)   1xty

(cid:124)

(cid:123)(cid:122)

wls

(cid:125)

= [(xtx)(  (xtx)   1 + i)]   1(xtx)wls
= (  (xtx)   1 + i)   1(xtx)   1(xtx)wls
= (  (xtx)   1 + i)   1wls

can use this to prove that the solution shrinks toward zero: (cid:107)wrr(cid:107)2     (cid:107)wls(cid:107)2.

ridge regression vs least squares ii

continue analysis with the svd: x = usvt     (xtx)   1 = vs   2vt:

wrr = (  (xtx)   1 + i)   1wls
= (  vs   2vt + i)   1wls
= v(  s   2 + i)   1vtwls
:= vmvtwls

m is a diagonal matrix with mii = s2
  +s2
ii

ii

wrr = vs   1

   uty,

s   1
   =

. we can pursue this to show that

             s11

0

  +s2
11

            

...

0

sdd
  +s2
dd

compare with wls = vs   1uty, which is the case where    = 0 above.

ridge regression vs least squares iii

ridge regression can also be seen as a special case of least squares.
de   ne   y       xw in the following way,

                         

                     

                     

y

0
...
0

    x
   

  

...

0

   

0
   

  

                     

          w1

...
wd

         

if we solved wls for this regression problem, we    nd wrr of the original
problem: calculating (  y       xw)t (  y       xw) in two parts gives
   

(  y       xw)t (  y       xw) = (y     xw)t (y     xw) + (

   
  w)t (

  w)

= (cid:107)y     xw(cid:107)2 +   (cid:107)w(cid:107)2

selecting   

degrees of freedom:

df (  ) = trace(cid:2)x(xtx +   i)   1xt(cid:3)

d(cid:88)

i=1

=

s2
ii

   + s2
ii

this gives a way of
visualizing relationships.

we will discuss methods for
picking    later.

