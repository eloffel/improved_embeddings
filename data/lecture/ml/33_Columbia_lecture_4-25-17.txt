coms 4721: machine learning for data science

lecture 24, 4/25/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

model selection

model selection

the model selection problem
we   ve seen how often model parameters need to be set in advance and
discussed how this can be done using using cross-validation.

another type of model selection problem is learning model order.

model order: the complexity of a class of models

(cid:73) gaussian mixture model: how many gaussians?
(cid:73) id105: what rank?
(cid:73) id48: how many states?

in each of these problems, we can   t simply look at the log-likelihood
because a more complex model can always    t the data better.

model selection

model order
we will discuss two methods for selecting an    appropriate    complexity of
the model. this assumes a good model type was chosen to begin with.

example: maximum likelihood

n(cid:88)

notation
we write l for the log-likelihood of a parameter under a model p(x|  ):

iid    p(x|  )        l =

xi

log p(xi|  )

the maximum likelihood solution is:   ml = arg max   l.

i=1

example: how many clusters? (wrong way)
the parameters    could be those of a gmm. we could    nd   ml for different
numbers of clusters and pick the one with the largest l.

problem: we can perfectly    t the data by putting each observation in its
own cluster. then shrink the variance of each gaussian to zero.

number of parameters

the general problem

(cid:73) models with more degrees of freedom are more prone to over   tting.
(cid:73) the degrees of freedom is roughly the number of scalar parameters, k.
(cid:73) by increasing k (done by increasing #clusters, rank, #states, etc.) the

model can add more degrees of freedom.

some common solutions

(cid:73) stability: bootstrap sample the data, learn a model, calculate the
likelihood on the original data set. repeat and pick the best model.

(cid:73) bayesian nonparametric methods: each possible value of k is

assigned a prior id203. the posterior learns the best k.

(cid:73) penalization approaches: a penalty term makes adding parameters
expensive. must be overcome by a greater improvement in likelihood.

penalizing model complexity

general form
de   ne a penalty function on the number of model parameters. instead of
maximizing l, minimize    l and add the de   ned penalty.

two popular penalties are:
(cid:73) akaike information criterion (aic):     l + k
(cid:73) bayesian information criterion (bic):     l + 1
when 1
example: for nmf with an m1    m2 matrix and rank r factorization,

2 ln n > 1, bic encourages a simpler model (happens when n     8).

2 k ln n

aic     (m1 + m2)r,

bic     1
2

(m1 + m2)r ln(m1m2)

example of aic output

example: aic vs bic on id48

notice:

(cid:73) likelihood is always improving
(cid:73) only compare location of aic
and bic minima, not the values.

derivation of bic

aic and bic

recall the two penalties:
(cid:73) akaike information criterion (aic):     l + k
(cid:73) bayesian information criterion (bic):     l + 1

2 k ln n

algorithmically, there is no extra work required:
1. find the ml solution of the selected models and calculate l.
2. add the aic or bic penalty to get a score useful for picking a model.

q: where do these penalties come from? currently they seem arbitrary.

a: we will derive bic next. aic also has a theoretical motivation, but we

will not discuss that derivation.

deriving the bic

imagine we have r candidate models, m1, . . . ,mr. for example, r id48s
each having a different number of states.
we also have data d = {x1, . . . , xn}. we want the posterior of each mi.

p(mi|d) =

(cid:80)
p(d|mi)p(mi)
j p(d|mj)p(mj)
(cid:90)

if we assume a uniform prior distribution on models, then because the
denominator is constant in mi, we can pick

m = arg maxmi

ln p(d|mi) =

ln p(d|  ,mi)p(  |mi)d  

we   re choosing the model with the largest marginal likelihood of the data by
integrating out all parameters of the model. this is usually not solvable.

deriving the bic

we will see how the bic arises from the approximation,

m = arg maxmi

ln p(d|mi)     arg maxmi

ln p(d|  ml,mi)     1
2

k ln n

step 1: recognize that the dif   culty is with the integral

ln p(d|mi) = ln

p(d|  )p(  )d  .

(cid:90)

mi determines p(d|  ), p(  )   we will suppress this conditioning.

step 2: approximate this integral using a second-order taylor expansion.

deriving the bic

1. we want to calculate:

ln p(d|m) = ln

(cid:90)

(cid:90)

p(d|  )p(  )d   = ln

exp{ln p(d|  )}p(  )d  

2. we use a second-order taylor expansion of ln p(d|  ) at the point   ml,

ln p(d|  )     ln p(d|  ml) + (         ml)t     ln p(d|  ml)

(cid:125)

(cid:124)

(cid:123)(cid:122)

= 0

(cid:125)

+

1
2

(         ml)t    2 ln p(d|  ml)

(         ml)

3. approximate p(  ) as uniform and plug this approximation back in,

ln p(d|m)     ln p(d|  ml) + ln

exp

(       ml)tj (  ml)(       ml)

d  

(cid:111)

=    j (  ml)

(cid:124)

(cid:123)(cid:122)
(cid:110)   1

2

(cid:90)

deriving the bic

observation: the integral is the normalizing constant of a gaussian,

(cid:90)

(cid:110)     1

2

exp

(         ml)tj (  ml)(         ml)

d   =

(cid:111)

(cid:19)k/2

(cid:18) 2  

|j (  ml)|

remember the de   nition that

   j (  ml) =    2 ln p(d|  ml)

(a)
= n

n(cid:88)
(cid:124)

i=1

1
n

   2 ln p(xi|  ml)

(cid:123)(cid:122)

(cid:125)

converges as n increases

(a) is by the i.i.d. model assumption made at the beginning of the lecture.

deriving the bic

4. plugging this in,

ln p(d|m)     ln p(d|  ml) + ln

and |j (  ml)| = n(cid:12)(cid:12)(cid:80)n

i=1

n   2 ln p(xi|  ml)(cid:12)(cid:12).

1

(cid:19)k/2

(cid:18) 2  

|j (  ml)|

therefore we arrive at the bic,

ln p(d|m)     ln p(d|  ml)     1
2

(cid:124)

(cid:123)(cid:122)

k ln n + something not growing with n

o(1) term, so we ignore it

(cid:125)

some next steps

icml sessions (subset)

the international conference on machine learning (icml) is a major ml
conference. many of the session titles should look familiar:

(cid:73) bayesian optimization and gaussian processes
(cid:73) pca and subspace models
(cid:73) supervised learning
(cid:73) matrix completion and graphs
(cid:73) id91 and nonparametrics
(cid:73) active learning
(cid:73) id91
(cid:73) boosting and ensemble methods
(cid:73) id105 i & ii
(cid:73) kernel methods i & ii
(cid:73) topic models
(cid:73) time series and sequences
(cid:73) etc.

icml sessions (subset)

other sessions might not look so familiar:

(cid:73) id23 i & ii
(cid:73) bandits i & ii
(cid:73) optimization i, ii & iii
(cid:73) bayesian nonparametrics i & ii
(cid:73) online learning i & ii
(cid:73) id114 i & ii
(cid:73) neural networks and deep learning i & ii
(cid:73) metric learning and feature selection
(cid:73) etc.

many of these topics are taught in advanced machine learning courses at
columbia in the cs, statistics, ieor and ee departments.

