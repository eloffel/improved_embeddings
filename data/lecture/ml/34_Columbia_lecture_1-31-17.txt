coms 4721: machine learning for data science

lecture 5, 1/31/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

bayesian id75

model
have vector y     rn and covariates matrix x     rn  d. the ith row of y and x
correspond to the ith observation (yi, xi).

in a bayesian setting, we model this data as:

likelihood :

y     n(xw,   2i)
prior : w     n(0,      1i)

the unknown model variable is w     rd.
(cid:73) the    likelihood model    says how well the observed data agrees with w.
(cid:73) the    model prior    is our prior belief (or constraints) on w.

this is called bayesian id75 because we have de   ned a prior on
the unknown parameter and will try to learn its posterior.

review: maximum a posteriori id136

map solution
map id136 returns the maximum of the log joint likelihood.
p(y, w|x) = p(y|w, x)p(w)

joint likelihood :

using bayes rule, we see that this point also maximizes the posterior of w.

wmap = arg max
w
= arg max

w

= arg max

w

ln p(w|y, x)
ln p(y|w, x) + ln p(w)     ln p(y|x)
    1
2  2 (y     xw)t (y     xw)       

2

wtw + const.

we saw that this solution for wmap is the same as for ridge regression:

wmap = (    2i + xtx)   1xty     wrr

point estimates vs bayesian id136

point estimates
wmap and wml are referred to as point estimates of the model parameters.

they    nd a speci   c value (point) of the vector w that maximizes an objective
function     the posterior (map) or likelihood (ml).

(cid:73) ml: only considers the data model: p(y|w, x).
(cid:73) map: takes into account model prior: p(y, w|x) = p(y|w, x)p(w).

bayesian id136
bayesian id136 goes one step further by characterizing uncertainty about
the values in w using bayes rule.

bayes rule and id75

posterior calculation
since w is a continuous-valued random variable in rd, bayes rule says that
the posterior distribution of w given y and x is

p(w|y, x) =

p(y|w, x)p(w)

(cid:82)
rd p(y|w, x)p(w) dw

that is, we get an updated distribution on w through the transition

prior     likelihood     posterior

quote:    the posterior of

is proportional to the likelihood times the prior.   

fully bayesian id136

bayesian id75
in this case, we can update the posterior distribution p(w|y, x) analytically.

we work with the proportionality    rst:

p(w|y, x)     p(y|w, x)p(w)

    (cid:104)

2  2 (y   xw)t (y   xw)(cid:105)(cid:104)

2 wt w(cid:105)

e    1
    e    1
2{wt (  i+     2xt x)w   2     2wt xt y}

e      

the     sign lets us multiply and divide this by anything as long as it doesn   t
contain w. we   ve done this twice above. therefore the 2nd line (cid:54)= 3rd line.

bayesian id136 for id75

we need to normalize:

p(w|y, x)     e    1

2{wt (  i+     2xt x)w   2     2wt xt y}

there are two key terms in the exponent:
wt (  i +      2xtx)w

(cid:124)

(cid:123)(cid:122)

quadratic in w

(cid:125)

(cid:124)

(cid:123)(cid:122)

    2wtxty/  2
linear in w

(cid:125)

we can conclude that p(w|y, x) is gaussian. why?
1. we can multiply and divide by anything not involving w.
2. a gaussian has (w       )t      1(w       ) in the exponent.
3. we can    complete the square    by adding terms not involving w.

bayesian id136 for id75

compare: in other words, a gaussian looks like:

p(w|  ,   ) =

1
2 |  | 1
(2  )

2

d

e    1

2 (wt      1w   2wt      1  +  t      1  )

and we   ve shown for some setting of z that

p(w|y, x) =

1
z

e    1

2 (wt (  i+     2xt x)w   2wt xt y/  2)

conclude: what happens if in the above gaussian we de   ne:
     1   = xty/  2 ?

     1 = (  i +      2xtx),

using these speci   c values of    and    we only need to set

z = (2  )

d

2 |  | 1

2 e 1

2   t      1  

bayesian id136 for id75

the posterior distribution
therefore, the posterior distribution of w is:

p(w|y, x) = n(w|  ,   ),

   = (  i +      2xtx)   1,
   = (    2i + xtx)   1xty     wmap

things to notice:

(cid:73)    = wmap after a rede   nition of the id173 parameter   .
(cid:73)    captures uncertainty about w, like var[wls] and var[wrr] did before.
(cid:73) however, now we have a full id203 distribution on w.

uses of the posterior distribution

understanding w
we saw how we could calculate the variance of wls and wrr. now we have
an entire distribution. some questions we can ask are:

q: is wi > 0 or wi < 0? can we con   dently say wi (cid:54)= 0?
a: use the marginal posterior distribution: wi     n(  i,   ii).

q: how do wi and wj relate?
a: use their joint marginal posterior distribution:

(cid:21)

(cid:20) wi

wj

(cid:18)(cid:20)   i

  j

(cid:21)

(cid:20)   ii   ij

  ji   jj

,

(cid:21)(cid:19)

    n

predicting new data
the posterior p(w|y, x) is perhaps most useful for predicting new data.

predicting new data

predicting new data

recall: for a new pair (x0, y0) with x0 measured and y0 unknown, we can
predict y0 using x0 and the ls or rr (i.e., ml or map) solutions:

y0     xt

0 wls

or

y0     xt

0 wrr

with bayes rule, we can make a probabilistic statement about y0:

p(y0|x0, y, x) =

=

p(y0, w|x0, y, x) dw

p(y0|w, x0, y, x) p(w|x0, y, x) dw

(cid:90)
(cid:90)

rd

rd

notice that conditional independence lets us write

p(y0|w, x0, y, x) = p(y0|w, x0)

and p(w|x0, y, x) = p(w|y, x)

(cid:124)

(cid:123)(cid:122)

(cid:125)

likelihood

(cid:124)

(cid:123)(cid:122)

posterior

(cid:125)

predicting new data

predictive distribution (intuition)
this is called the predictive distribution:

(cid:90)

rd

p(y0|x0, y, x) =

p(y0|x0, w)

p(w|y, x)

dw

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:123)(cid:122)

(cid:124)

likelihood

posterior

intuitively:

1. evaluate the likelihood of a value y0 given x0 for a particular w.
2. weight that likelihood by our current belief about w given data (y, x).
3. then sum (integrate) over all possible values of w.

predicting new data

we know from the model and bayes rule that

model: p(y0|x0, w) = n(y0|xt

0 w,   2),

p(w|y, x) = n(w|  ,   ).

bayes rule:

with    and    calculated on a previous slide.

the predictive distribution can be calculated exactly with these distributions.
again we get a gaussian distribution:

p(y0|x0, y, x) = n(y0|  0,   2
0),

  0 = xt
0 =   2 + xt
  2

0   ,

0   x0.

notice that the expected value is the map prediction since   0 = xt
we now quantify our con   dence in this prediction with the variance   2
0.

0 wmap, but

active learning

prior     posterior     prior

bayesian learning is naturally thought of as a sequential process. that is, the
posterior after seeing some data becomes the prior for the next data.

let y and x be    old data    and y0 and x0 be some    new data   . by bayes rule

p(w|y0, x0, y, x)     p(y0|w, x0)p(w|y, x).
the posterior after (y, x) has become the prior for (y0, x0).

simple modi   cations can be made sequentially in this case:
p(w|y0, x0, y, x) = n(w|  ,   ),

   = (  i +      2(x0xt
   = (    2i + (x0xt

0 +(cid:80)n
0 +(cid:80)n

i ))   1,

i ))   1(x0y0 +(cid:80)n

i=1 xixt
i=1 xixt

i=1 xiyi).

intelligent learning

notice we could also have written

p(w|y0, x0, y, x)     p(y0, y|w, x, x0)p(w)

but often we want to use the sequential aspect of id136 to help us learn.

learning w and making predictions for new y0 is a two-step procedure:
(cid:73) form the predictive distribution p(y0|x0, y, x).
(cid:73) update the posterior distribution p(w|y, x, y0, x0).
question: can we learn p(w|y, x) intelligently?

that is, if we   re in the situation where we can pick which yi to measure with
knowledge of d = {x1, . . . , xn}, can we come up with a good strategy?

active learning

an    active learning    strategy
imagine we already have a measured dataset (y, x) and posterior p(w|y, x).
we can construct the predictive distribution for every remaining x0     d.

p(y0|x0, y, x) = n(y0|  0,   2
0),

  0 = xt
0 =   2 + xt
  2

0   ,

0   x0.

for each x0,   2
0 tells how con   dent we are. this suggests the following:
1. form predictive distribution p(y0|x0, y, x) for all unmeasured x0     d
2. pick the x0 for which   2
3. update the posterior p(w|y, x) where y     (y, y0) and x     (x, x0)
4. return to #1 using the updated posterior

0 is largest and measure y0

active learning

id178 (i.e., uncertainty) minimization
when devising a procedure such as this one, it   s useful to know what
objective function is being optimized in the process.

we introduce the concept of the id178 of a distribution. let p(z) be a
continuous distribution, then its (differential) id178 is:

h(p) =    (cid:82) p(z) ln p(z)dz.

this is a measure of the spread of the distribution. more positive values
correspond to a more    uncertain    distribution (larger variance).

the id178 of a multivariate gaussian is

h(n(w|  ,   )) =

1
2

ln

(cid:16)

(2  e)d|  |(cid:17)

.

active learning

the id178 of a gaussian changes with its covariance matrix. with
sequential bayesian learning, the covariance transitions from

prior : (  i +      2xtx)   1

      

   

posterior : (  i +      2(x0xt

0 + xtx))   1     (     1 +      2x0xt

0 )   1

using a    rank-one update    property of the determinant, we can show that the
id178 of the prior hprior relates to the id178 of the posterior hpost as:

hpost = hprior     d
2

ln(1 +      2xt

0   x0)

therefore, the x0 that minimizes hpost also maximizes   2 + xt
minimizing h myopically, so this is called a    greedy algorithm   .

0   x0. we are

model selection

selecting   

we   ve discussed    as a    nuisance    parameter that can impact performance.

bayes rule gives a principled way to do this via evidence maximization:

p(w|y, x,   ) = p(y|w, x)

(cid:124)

(cid:123)(cid:122)

likelihood

(cid:125)

p(w|  )

(cid:124) (cid:123)(cid:122) (cid:125)

prior

/ p(y|x,   )

.

(cid:124)

(cid:123)(cid:122)

(cid:125)

evidence

the    evidence    gives the likelihood of the data with w integrated out. it   s a
measure of how good our model and parameter assumptions are.

selecting   

if we want to set   , we can also do it by maximizing the evidence.1

     = arg max
  

ln p(y|x,   ).

we notice that this looks exactly like maximum likelihood, and it is:

type-i ml: maximize the likelihood over the    main parameter    (w).
type-ii ml: integrate out    main parameter    (w) and maximize over
the    hyperparameter    (  ). also called empirical bayes.

the difference is only in their perspective.

this approach requires us to solve this integral, but we often can   t for more
complex models. cross-validation is an alternative that   s always available.

1we can show that the distribution of y is p(y|x,   ) = n(y|0,   2i +      1xxt ). this would

require an algorithm to maximize over   . the key point here is the general technique.

