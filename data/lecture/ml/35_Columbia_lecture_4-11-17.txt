coms 4721: machine learning for data science

lecture 20, 4/11/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

sequential data

so far, when thinking probabilistically we have focused on the i.i.d. setting.

(cid:73) all data are independent given a model parameter.
(cid:73) this is often a reasonable assumption, but was also done for convenience.

in some applications this assumption is bad:
(cid:73) modeling rainfall as a function of hour
(cid:73) daily value of currency exchange rate
(cid:73) acoustic features of speech audio

the distribution on the next value clearly
depends on the previous values.

a basic way to model sequential information
is with a discrete,    rst-order markov chain.

markov chains

example: zombie walker1

imagine you see a zombie in an alley. each time it moves forward it steps

( left, straight, right ) with id203

( pl, ps, pr ),

unless it   s next to the wall, in which case it steps straight with id203 pw
s
and toward the middle with id203 pw
m.

the distribution on the next location only depends on the current location.

1this problem is often introduced with a    drunk,    so our maturity is textbook-level.

random walk notation

we simplify the problem by assuming there are only a    nite number of
positions the zombie can be in, and we model it as a random walk.

the distribution on the next position only depends on the current position.
for example, for a position i away from the wall,

st+1 |{st = i} =

          i + 1

i
i     1

w.p. pr
ps
w.p.
w.p. pl

this is called the    rst-order markov property. it   s the simplest type. a
second-order model would depend on the previous two positions.

position 4position 20matrix notation

a more compact notation uses a matrix.

for the random walk problem, imagine we have 6 different positions, called
states. we can write the transition matrix as

                        

m =

                        

pw
s
pl
0
0
0
0

pw
m
ps
pl
0
0
0

0
pr
ps
pl
0
0

0
0
pr
ps
pl
0

0
0
0
pr
ps
pw
m

0
0
0
0
pr
pw
s

mij is the id203 that the next position is j given the current position is i.

of course we can jumble this matrix by moving rows and columns around in
a correct way, as long as we can map the rows and columns to a position.

first-order markov chain (general)

let s     {1, . . . , s}. a sequence (s1, . . . , st) is a    rst-order markov chain if

t(cid:89)

t(cid:89)

p(s1, . . . , st)

(a)
= p(s1)

p(su|s1, . . . , su   1)

(b)
= p(s1)

p(su|su   1)

u=2

u=2

from the two equalities above:
(a) this equality is always true, regardless of the model (chain rule).
(b) this simpli   cation results from the markov property assumption.

notice the difference from the i.i.d. assumption
u=2 p(su|su   1)

p(s1, . . . , st) =

(cid:40) p(s1)(cid:81)t
(cid:81)t

u=1 p(su)

markov assumption
i.i.d. assumption

from a modeling standpoint, this is a signi   cant difference.

first-order markov chain (general)

again, we encode this more general id203 distribution in a matrix:

mij = p(st = j|st   1 = i)

we will adopt the notation that rows are distributions.

(cid:73) m is a transition matrix, or markov matrix.
(cid:73) m is s    s and each row sums to one.
(cid:73) mij is the id203 of transitioning to state j given we are in state i.

given a starting state, s0, we generate a sequence (s1, . . . , st) by sampling

st | st   1     discrete(mst   1,:).

we can model the starting state with its own separate distribution.

maximum likelihood

t   1(cid:88)

s(cid:88)

u=1

i,j

given a sequence, we can approximate the transition matrix using ml,

mml = arg max
m

p(s1, . . . , st|m) = arg max

m

1(su = i, su+1 = j) ln mij.

since each row of m has to be a id203 distribution, we can show that

(cid:80)t   1

u=1

(cid:80)t   1

u=1

1(su = i, su+1 = j)

.

1(su = i)

mml(i, j) =

empirically, count how many times we observe a transition from i     j and
divide by the total number of transitions from i.

example: model id203 it rains (r) tomorrow given it rained today with
observed fraction #{r   r}
#{r} . notice that #{r} = #{r     r} + #{r     no-r}.

property: state distribution

q: can we say at the beginning what state we   ll be in at step t + 1?

a: imagine at step t that we have a id203 distribution on which state
we   re in, call it p(st = u). then the distribution on st+1 is

s(cid:88)

u=1

(cid:124)

s(cid:88)

u=1

(cid:124)

p(st+1 = j) =

p(st+1 = j|st = u)p(st = u)

.

(cid:123)(cid:122)

p(st+1= j, st= u)

p(st+1 = j)

=

(cid:124)

(cid:123)(cid:122)

wt+1(j)

(cid:125)

p(st+1 = j|st = u)

p(st = u)

.

(cid:123)(cid:122)

muj

(cid:125)

(cid:124)

(cid:123)(cid:122)

wt(u)

(cid:125)

(cid:125)

represent p(st = u) with the row vector wt (the state distribution). then

we can calculate this for all j with the matrix-vector product wt+1 = wtm.
therefore, wt+1 = w1mt and w1 can be indicator if starting state is known.

property: stationary distribution

s(cid:88)

given current state distribution wt, the distribution on the next state is

wt+1(j) =

mujwt(u)        wt+1 = wtm

what happens if we project an in   nite number of steps out?

u=1

de   nition: let w    = limt       wt. then w    is the stationary distribution.
(cid:73) there are many technical results that can be proved about w   .
(cid:73) property: if the following are true, then w    is the same vector for all w0

1. we can eventually reach any state starting from any other state,
2. the sequence doesn   t loop between states in a pre-de   ned pattern.

(cid:73) clearly w    = w   m since wt is converging and wt+1 = wtm.

this last property is related to the    rst eigenvector of mt:

mtq1 =   1q1 =      1 = 1, w    =

q1(cid:80)s

u=1 q1(u)

a ranking algorithm

example: ranking objects

we show an example of using the stationary distribution of a markov chain
to rank objects. the data are pairwise comparisons between objects.

for example, we might want to rank

(cid:73) sports teams or athletes competing against each other
(cid:73) objects being compared and selected by users
(cid:73) web pages based on popularity or relevance

our goal is to rank objects from    best    to    worst.   

(cid:73) we will construct a random walk matrix on the objects. the stationary

distribution will give us the ranking.

(cid:73) notice: we don   t consider the sequential information in the data itself.

the markov chain is an arti   cial modeling construct.

example: team rankings

problem setup
we want to construct a markov chain where each team is a state.

(cid:73) we encourage transitions from teams that lose to teams that win.
(cid:73) predicting the    state    (i.e., team) far in the future, we can interpret a

more probable state as a better team.

one speci   c approach to this speci   c problem:

(cid:73) transitions only occur between teams that play each other.
(cid:73) if team a beats team b, there should be a high id203 of

transitioning from b   a and small id203 from a   b.

(cid:73) the strength of the transition can be linked to the score of the game.

example: team rankings

how about this?

initialize (cid:98)m to a matrix of zeros. for a particular game, let j1 be the index of

team a and j2 the index of team b. then update

(cid:98)mj1j1     (cid:98)mj1j1 + 1{team a wins} +
(cid:98)mj2j2     (cid:98)mj2j2 + 1{team b wins} +
(cid:98)mj1j2     (cid:98)mj1j2 + 1{team b wins} +
(cid:98)mj2j1     (cid:98)mj2j1 + 1{team a wins} +

pointsj1

pointsj1 +pointsj2

pointsj2

pointsj1 +pointsj2

pointsj2

pointsj1 +pointsj2

pointsj1

pointsj1 +pointsj2

,

,

,

.

after processing all games, let m be the matrix formed by normalizing the

rows of (cid:98)m so they sum to 1.

example: 2016-2017 college basketball season

  xxxx1,570 teams22,426 gamesscore = w   8 < 13 : proof of intelligence?a classification algorithm

semi-supervised learning

imagine we have data with very few labels.

we want to use the structure in the dataset to
help classify the unlabeled data.

we can do this with a markov chain.

semi-supervised learning uses partially labeled data to do classi   cation.

(cid:73) many or most yi will be missing in the pair (xi, yi).
(cid:73) still, there is structure in x1, . . . , xn that we don   t want to throw away.
(cid:73) in the example above, we might want the inner ring to be one class

(blue) and the outer ring another (red).

a random walk classifier

we will de   ne a classi   er where, starting from any data point xi,

(cid:73) a    random walker    moves around from point to point
(cid:73) a transition between nearby points has higher id203
(cid:73) a transition to a labeled point terminates the walk
(cid:73) the label of a point xi is the label of the terminal point

one possible random walk matrix

1. let the unnormalized transition matrix be

(cid:26)

(cid:98)mij = exp

(cid:27)
   (cid:107)xi     xj(cid:107)2
2. normalize rows of (cid:98)m to get m

b

3. if xi has label yi, re-de   ne mii = 1

starting pointhigher id203transitionlower id203transitionproperty: absorbing states

imagine we have s states. if p(st = i|st   1 = i) = 1, then the ith state is
called an absorbing state since we can never leave it.
q: given initial state s0 = j and set of absorbing states {i1, . . . , ik}, what is
the id203 a markov chain terminates at a particular absorbing state?

(cid:73) aside: for the semi-supervised classi   er, the answer gives the

id203 on the label of xj.

a: start a random walk at j and keep track of the distribution on states.

(cid:73) w0 is a vector of 0   s with a 1 in entry j because we know s0 = j
(cid:73) if m is the transition matrix, we know that wt+1 = wtm.
(cid:73) so we want w    = w0m   .

property: absorbing state distribution

group the absorbing states and break up the transition matrix into quadrants:

(cid:21)

(cid:20) a b

0

i

m =

the bottom half contains the self-transitions of the absorbing states.
observation: wt+1 = wtm = wt   1m2 =        = w0mt+1

so we need to understand what   s going on with mt. for the    rst two we have

(cid:20) a b
(cid:21)(cid:20) a b
(cid:21)(cid:20) a2 ab + b

0

0

i

i

(cid:21)
(cid:21)

m2 =

(cid:20) a b

0

i

0

i

(cid:20) a2 ab + b
(cid:21)
(cid:20) a3 a2b + ab + b

0

i

0

i

=

=

(cid:21)

m3 =

geometric series

detour: we will use the matrix version of the following scalar equality.

de   nition: let 0 < r < 1. then(cid:80)t   1

1   r and so(cid:80)   

u=0 ru = 1   rt

u=0 ru = 1

1   r .

proof: first de   ne the top equality and create the bottom equality

ct = 1 + r + r2 +        + rt   1
r ct =

r + r2 +        + rt   1 + rt

and so

therefore

t   1(cid:88)

u=0

ct =

ct     r ct = 1     rt.

ru =

1     rt
1     r

and

c    =

1
1     r .

property: absorbing state distribution

a matrix version of the geometric series appears here. we see the pattern

(cid:34)

u=0 au(cid:17)
at (cid:16)(cid:80)t   1

0

i

b

(cid:35)

.

mt =

two key things that can be shown are:

   (cid:88)

a    = 0,

au = (i     a)   1

summary:

u=0

(cid:104) 0

(cid:105)

(cid:73) after an in   nite # of steps, w    = w0 m    = w0
i
(cid:73) the non-zero dimension of w0 picks out a row of (i     a)   1b.
(cid:73) the id203 that a random walk started at xj terminates at the ith

0

.

(i     a)   1b

absorbing state is [(i     a)   1b]ji.

classification example

using a gaussian kernel normalized on the rows. the color indicates the
distribution on the terminal state for each starting point.

kernel width was tuned to give this result.

classification example

using a gaussian kernel normalized on the rows. the color indicates the
distribution on the terminal state for each starting point.

kernel width is larger here. therefore, purple points may leap to the center.

