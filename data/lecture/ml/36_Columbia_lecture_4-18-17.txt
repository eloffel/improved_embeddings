coms 4721: machine learning for data science

lecture 22, 4/18/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

markov models

the sequence (s1, s2, s3, . . . ) has the markov property, if for all t

p(st|st   1, . . . , s1) = p(st|st   1).

our    rst encounter with markov models assumed a    nite state space,
meaning we can de   ne an indexing such that s     {1, . . . , s}.

this allowed us to represent the transition probabilities in a matrix,

aij     p(st = j|st   1 = i).

s1s2s3s4id48

the hidden markov model modi   ed this by assuming the sequence of states
was a latent process (i.e., unobserved).
an observation xt is associated with each st, where xt | st     p(x|  st ).

like a mixture model, this allowed for a few distributions to generate the
data. it adds an extra transition rule between distributions.

sn   1snsn+1xn   1xnxn+1s1s2x1x2discrete state spaces

in both cases, the state space was discrete and
relatively small in number.

(cid:73) for the markov chain, we gave an example
where states correspond to positions in rd.

(cid:73) a continuous hidden markov model might
perturb the latent state of the markov chain.

(cid:73) for example, each si can be modi   ed by

continuous-valued noise, xi = si +  i.

(cid:73) but s1:t is still a discrete markov chain.

a12a23a31a21a32a13a11a22a33k=1k=2k=3k=1k=2k=300.5100.51discrete vs continuous state spaces

markov and id48 both assume a discrete state space.

for markov models:

(cid:73) the state could be a data point xi (markov chain classi   er)
(cid:73) the state could be an object (object ranking)
(cid:73) the state could be the destination of a link (internet search engines)

for id48 we can simplify complex data:

(cid:73) sequences of discrete data may come from a few discrete distributions.
(cid:73) sequences of continuous data may come from a few distributions.

what if we model the states as continuous too?

continuous-state markov model

continuous markov models extend the state space to a continuous domain.
instead of s     {1, . . . , s}, s can take any value in rd.

again compare:

(cid:73) discrete-state markov models: the states live in a discrete space.
(cid:73) continuous-state markov models: the states live in a continuous space.

the simplest example is the process

st = st   1 +  t,

 t     n(0, ai).

each successive state is a perturbed version of the current state.

linear gaussian markov model

the most basic continuous-state version of the hidden markov model is
called a linear gaussian markov model (also called the kalman    lter).

(cid:124)

(cid:123)(cid:122)

st = cst   1 +  t   1

,

latent process

(cid:125)

(cid:124)

(cid:123)(cid:122)

xt = dst +   t
observed process

(cid:125)

(cid:73) st     rp is a continuous-state latent (unobserved) markov process
(cid:73) xt     rd is a continuous-valued observation
(cid:73) the process noise  t     n(0, q)
(cid:73) the measurement noise   t     n(0, v)

example applications

difference from id48: st and xt are both from continuous distributions.

the linear gaussian markov model (and its variants) has many applications.

(cid:73) tracking moving objects
(cid:73) automatic control systems
(cid:73) economics and    nance (e.g., stock modeling)
(cid:73) etc.

sn   1snsn+1xn   1xnxn+1s1s2x1x2example: tracking

we get (very) noisy measurements of an object   s position in time, xt     r2.

the time-varying state vector is s = [pos1 vel1 accel1 pos2 vel2 accel2]t.

motivated by the underlying physics, we model this as:

1    t
1
0
0
0
0
0
0
0
0
0

                        
(cid:124)
(cid:20) 1 0
(cid:124)

0 0

st+1 =

xt+1 =

2 (   t)2
1
   t
e        t

0
0
0

0
0
0
0
0
0
1    t
0
1
0
0
    c

(cid:123)(cid:122)

0
0
0

2 (   t)2
1
   t
e        t

(cid:21)
(cid:125)

(cid:123)(cid:122)

0 0 0
0 1 0
    d

0
0

st+1 +   t+1

                        

(cid:125)

st +  t

therefore, st not only approximates where the target is, but where it   s going.

example: tracking

the learning problem

as with the hidden markov model, we   re given the sequence (x1, x2, x3, . . . ),
where each x     rd. the goal is to learn state sequence (s1, s2, s3, . . . ).

all distributions are gaussian,

p(st+1 = s|st) = n(cst, q),

p(xt = x|st) = n(dst, v).

notice that with the discrete id48 we wanted to learn   , a and b, where

(cid:73)    is the initial state distribution
(cid:73) a is the transition matrix among the discrete set of states
(cid:73) b contains the state-dependent distributions on discrete-valued data

the situation here is very different.

the learning problem

no    b    to learn: in the linear gaussian markov model, each state is unique
and so the distribution on xt is different for each t.

no    a    to learn: in addition, each state transition is to a brand new state, so
each st has its own unique id203 distribution.

what we can learn are the two posterior distributions.
1. p(st|x1, . . . , xt) : a distribution on the current state given the past.
2. p(st|x1, . . . , xt ) : a distribution on each latent state in the sequence

(cid:73) #1: kalman    ltering problem. we   ll focus on this one today.
(cid:73) #2: kalman smoothing problem. requires extra step (not discussed).

the kalman filter

goal: learn the sequence of distributions p(st|x1, . . . , xt) given a sequence
of data (x1, x2, x3, . . . ) and the model
st+1 | st     n(cst, q),

xt | st     n(dst, v).

this is the (linear) kalman    ltering problem and is often used for tracking.

setup: we can use bayes rule to write

p(st|x1, . . . , xt)     p(xt|st) p(st|x1, . . . xt   1)

and represent the prior as a marginal distribution

p(st|x1, . . . , xt   1) =

p(st|st   1) p(st   1|x1, . . . , xt   1) dst   1

(cid:90)

the kalman filter

we   ve decomposed the problem into parts that we do and don   t know (yet)

p(st|x1, . . . , xt)     p(xt|st)

p(st|st   1)

p(st   1|x1, . . . , xt   1)

dst   1

(cid:124) (cid:123)(cid:122) (cid:125)

n(dst,v)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

n(cst   1,q)

(cid:123)(cid:122)

?

(cid:125)

(cid:90)

observations and considerations:

1. the left is the posterior on st and the right has the posterior on st   1.
2. we want the integral to be in closed form and a known distribution.
3. we want the prior and likelihood terms to lead to a known posterior.
4. we want future calculations, e.g. for st+1, to be easy.

we will see how choosing the gaussian distribution makes this all work.

the kalman filter: step 1

calculate the marginal for prior distribution
hypothesize (temporarily) that the unknown distribution is gaussian,

p(st|x1, . . . , xt)     p(xt|st)

p(st|st   1)

(cid:124) (cid:123)(cid:122) (cid:125)

n(dst,v)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

n(cst   1,q)

(cid:123)(cid:122)

p(st   1|x1, . . . , xt   1)
n(  ,  ) by hypothesis

(cid:125)

dst   1

(cid:90)

(cid:90)

a property of the gaussian is that marginals are still gaussian,

n(st|cst   1, q)n(st   1|  ,   )dst   1 = n(st|c  , q + c  ct ).

we know c and q (by design) and    and    (by hypothesis).

the kalman filter: step 2

calculate the posterior
we plug in the marginal distribution for the prior and see that

p(st|x1, . . . , xt)     n(xt|dst, v) n(st|c  , q + c  ct ).

though the parameters look complicated, the posterior is just a gaussian

p(st|x1, . . . , xt) = n(st|  (cid:48),   (cid:48))

  (cid:48) = (cid:2)(q + c  ct )   1 + dtv   1d(cid:3)   1
  (cid:48) =   (cid:48)(cid:0)dtv   1xt + (q + c  ct )   1c  (cid:1)

we can plug the relevant values into these two equations.

addressing the gaussian assumption

by making the assumption of a gaussian in the prior,

p(st|x1, . . . , xt)     p(xt|st)
n(xt|dst,v)

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124)

(cid:123)(cid:122)

p(st|st   1)
n(st|cst   1,q)

(cid:125)

(cid:124)

(cid:123)(cid:122)

p(st   1|x1, . . . , xt   1)
n(  ,  ) by hypothesis

(cid:125)

dst   1

(cid:90)

we found that the posterior is also gaussian with a new mean and covariance.

(cid:73) we therefore only need to de   ne a gaussian prior on the    rst state to

keep things moving forward. for example,
p(s0)     n(0, i).

once this is done, all future calculations are in closed form.

kalman filter: one final quantity

making predictions
we know how to update the sequence of state posterior distributions

p(st|x1, . . . , xt).

what about predicting xt+1?
p(xt+1|x1, . . . , xt) =

(cid:90)
(cid:90)

=

(cid:90)

p(xt+1|st+1)p(st+1|x1, . . . , xt)dst+1
p(xt+1|st+1)
n(xt+1|dst+1,v)

p(st+1|st)
n(st+1|cst,q)

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:124)

(cid:125)

(cid:124)

(cid:125)

(cid:124)

(cid:123)(cid:122)

n(st|  (cid:48),  (cid:48))

(cid:125)

p(st|x1, . . . , xt)

dst dst+1

again, gaussians are nice because these operations stay gaussian.

this is a multivariate gaussian that looks even more complicated than the
previous one (omitted). simply perform the previous integral twice.

algorithm: kalman filtering

the kalman    ltering algorithm can be run in real time.

0. set the initial state distribution p(s0) = n(0, i)
1. prior to observing each new xt     rd predict

xt     n(  x

t ,   x
t )

(using previously discussed marginalization)

2. after observing each new xt     rd update

p(st|x1, . . . , xt) = n(  s

t ,   s
t )

(using equations on previous slide)

example

learning state trajectory

green: true trajectory

blue: observed trajectory

red: state distribution

intuitions about what this is doing:

(cid:73) in the prior distribution notice that we add q to the covariance,

p(st|x1, . . . , xt   1) = n(st|c  , q + c  ct ).

this allows the state st to    drift    away from st   1.

(cid:73) in the posterior p(st|x1, . . . , xt), xt    pulls    the distribution away.

some final model comparisons

gaussian mixture model
(cid:73) st     discrete(  )
(cid:73) xt|st     n(  st ,   st )

continuous hidden markov model
(cid:73) st|st   1     discrete(ast   1 )
(cid:73) xt|st     n(  st ,   st )

we saw how the transition from gmm     id48 involves using a markov
chain to index the distribution on clusters.

sn   1snsn+1xn   1xnxn+1s1s2x1x2sn   1snsn+1xn   1xnxn+1s1s2x1x2some final model comparisons

probabilistic pca
(cid:73) st     n(0, q)
(cid:73) xt|st     n(dst, v)

linear gaussian markov model
(cid:73) st|st   1     n(cst   1, q)
(cid:73) xt|st     n(dst, v)

there is a similar relationship between probabilistic pca and the kalman
   lter. (probabilistic pca also learns d, while the kalman    lter doesn   t).

sn   1snsn+1xn   1xnxn+1s1s2x1x2sn   1snsn+1xn   1xnxn+1s1s2x1x2extensions

there are a variety of extensions to this framework. the equations in the
corresponding algorithms would all look familiar given our discussion.

extended kalman    lter: nonlinear kalman    lters use nonlinear function
of the state, h(st). the ekf approximates h(st)     h(z) +    h(z)(st     z)

st+1 | st     n(dst, q),

xt | st     n(h(st), v).

continuous time: sometimes the time between observations varies. let    t
be the time between observation xt and xt+1, then model

st+1 | st     n(st,    tq),

xt | st     n(dst, v).

adding control: in dynamic models, we can add control to the state using a
vector ut whose values we choose (e.g., thrusters).

st+1 | st     n(cst + gut, q),

xt | st     n(dst, v).

