coms 4721: machine learning for data science

lecture 2, 1/19/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

id75

example: old faithful

example: old faithful

can we meaningfully predict the time between eruptions only using the
duration of the last eruption?

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1.52.02.53.03.54.04.55.05060708090current eruption time (min)waiting time (min)example: old faithful

can we meaningfully predict the time between eruptions only using the
duration of the last eruption?

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1.52.02.53.03.54.04.55.05060708090current eruption time (min)waiting time (min)example: old faithful

one model for this

(wait time)     w0 + (last duration)    w1

(cid:73) w0 and w1 are to be learned.
(cid:73) this is an example of id75.

refresher
w1 is the slope, w0 is called the intercept, bias, shift, offset.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1.52.02.53.03.54.04.55.05060708090current eruption time (min)waiting time (min)higher dimensions

two inputs

(output)     w0 + (input 1)    w1 + (input 2)    w2

with two inputs the intuition
is the same       

y = w0 + x1w1 + x2w2x1x2yregression: problem definition

data
input: x     rd
output: y     r (i.e., response, dependent variable)

(i.e., measurements, covariates, features, indepen. variables)

goal
find a function f : rd     r such that y     f (x; w) for the data pair (x, y).
f (x; w) is called a regression function. its free parameters are w.

de   nition of id75
a regression method is called linear if the prediction f is a linear function of
the unknown parameters w.

least squares id75 model

model
the id75 model we focus on now has the form

d(cid:88)

yi     f (xi; w) = w0 +

xijwj.

j=1

model learning
we have the set of training data (x1, y1) . . . (xn, yn). we want to use this data
to learn a w such that yi     f (xi; w). but we    rst need an objective function to
tell us what a    good    value of w is.

least squares
the least squares objective tells us to pick the w that minimizes the sum of
squared errors

wls = arg min
w

(yi     f (xi; w))2     arg min

l.

w

n(cid:88)

i=1

least squares in pictures

observations:
vertical length is error.

the objective function l is the
sum of all the squared lengths.

find weights (w1, w2) plus an
offset w0 to minimize l.

(w0, w1, w2) de   nes this plane.

example: education, seniority and income

2-dimensional problem
input: (education, seniority)     r2.
output: (income)     r
model: (income)     w0 + (education)w1 + (seniority)w2

question: both w1, w2 > 0. what does this tell us?

answer: as education and/or seniority goes up, income tends to go up.

(caveat: this is a statement about correlation, not causation.)

least squares id75 model

thus far
we have data pairs (xi, yi) of measurements xi     rd and a response yi     r.
we believe there is a linear relationship between xi and yi,

d(cid:88)

j=1

yi = w0 +

xijwj +  i

and we want to minimize the objective function

n(cid:88)

l =

 2
i =

n(cid:88)

(yi     w0    (cid:80)d

j=1 xijwj)2

i=1

i=1

with respect to (w0, w1, . . . , wd).
can math notation make this easier to look at/work with?

notation: vectors and matrices

we think of data with d dimensions as a column vector:

               
               

xi1
xi2
...
xid

x11
x21
...
xn1

                (e.g.)    
                =

. . .
. . .

x1d
x2d
...
xnd

. . .

               
               

               
               

age
height

...

income

1    
    xt
    xt
2    
...
    xt
n    

xi =

x =

a set of n vectors can be stacked into a matrix:

assumptions for now:
(cid:73) all features are treated as continuous-valued (x     rd)
(cid:73) we have more observations than dimensions (d < n)

notation: regression (and classification)

usually, for id75 (and classi   cation) we include an intercept
term w0 that doesn   t interact with any element in the vector x     rd.

it will be convenient to attach a 1 to the    rst dimension of each vector xi
(which we indicate by xi     rd+1) and in the    rst column of the matrix x:

                      ,

                     

1
xi1
xi2
...
xid

xi =

x =

               

1
1
...
1

x11
x21

xn1

. . .
. . .

...

. . .

x1d
x2d

xnd

                =

               

                .

1     xt
1     xt
...
1     xt

1    
2    

n    

we also now view w = [w0, w1, . . . , wd]t as w     rd+1.

least squares in vector form

original least squares objective function: l =(cid:80)n
using vectors, this can now be written: l =(cid:80)n

i=1(yi     w0    (cid:80)d

j=1 xijwj)2

i=1(yi     xt

i w)2

least squares solution (vector version)
we can    nd w by setting,

   wl = 0     n(cid:88)
(cid:17)

(cid:16) n(cid:88)

i=1

solving gives,

    n(cid:88)

2yixi +

i=1

i=1

2xixt
i

w = 0     wls =

(cid:16) n(cid:88)

i=1

(cid:17)   1(cid:16) n(cid:88)

(cid:17)

.

yixi

i=1

xixt
i

   w(y2

i     2wtxiyi + wtxixt

i w) = 0.

least squares in matrix form

least squares solution (matrix version)
least squares in matrix form is even cleaner.

n(cid:88)

start by organizing the yi in a column vector, y = [y1, . . . , yn]t. then

l =

(yi     xt

i w)2 = (cid:107)y     xw(cid:107)2 = (y     xw)t (y     xw).

i=1

if we take the gradient with respect to w, we    nd that

   wl = 2xtxw     2xty = 0     wls = (xtx)

   1xty.

i=1 yixi)

recall from id202

recall: matrix    vector (xty =(cid:80)n
                = y1
       |
       |
       + y2
recall: matrix    matrix (xtx =(cid:80)n

               

y1
y2
...
yn

      

|
x2
|

|
xn
|

x1
|

x1|

. . .

       |

x1
|

|
x2
|

. . .

      

|
xn
|

               

i=1 xixt
i )

                = x1xt

    xt
1    
2    
    xt
...
n    
    xt

       |

x2|

       +        + yn

      

       |

xn|

1 +        + xnxt
n .

least squares id75: key equations

two notations for the key equation

(cid:16) n(cid:88)

(cid:17)   1(cid:16) n(cid:88)

(cid:17)        wls = (xtx)

yixi

   1xty.

wls =

xixt
i

i=1

i=1

making predictions
we use wls to make predictions.

given xnew, the least squares prediction for ynew is

ynew     xt

newwls

least squares solution

potential issues
calculating wls = (xtx)   1xty assumes (xtx)   1 exists.

when doesn   t it exist?

answer: when xtx is not a full rank matrix.

when is xtx full rank?

answer: when the n    (d + 1) matrix x has at least d + 1 linearly
independent rows. this means that any point in rd+1 can be reached by
a weighted combination of d + 1 rows of x.

obviously if n < d + 1, we can   t do least squares. if (xtx)   1 doesn   t exist,
there are an in   nite number of possible solutions.
takeaway: we want n (cid:29) d (i.e., x is    tall and skinny   ).

broadening id75

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   2   10123   50510xybroadening id75

y = w0 + w1x

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   2   10123   50510xybroadening id75

y = w0 + w1x + w2x2 + w3x3

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll   2   10123   50510xypolynomial regression in r

recall: de   nition of id75
a regression method is called linear if the prediction f is a linear function of
the unknown parameters w.

(cid:73) therefore, a function such as y = w0 + w1x + w2x2
(cid:73) e.g., let (x1, y1) . . . (xn, yn) be the data, x     r, y     r. for a pth-order

the ls solution is the same, only the preprocessing is different.

is linear in w.

polynomial approximation, construct the matrix

               

1
1
...
1

x =

               

x1
x2

x2
1
x2
2

xp
1
xp
2

. . .
. . .

...

xp
n
(cid:73) then solve exactly as before: wls = (xtx)   1xty.

. . .

x2
n

xn

polynomial regression (mth order)

xtm=001   101polynomial regression (mth order)

xtm=101   101polynomial regression (mth order)

xtm=301   101polynomial regression (mth order)

xtm=901   101polynomial regression in two dimensions

example: 2nd and 3rd order polynomial regression in r2
the width of x grows as (order)    (dimensions) + 1.

2nd order:
3rd order:

yi = w0 + w1xi1 + w2xi2 + w3x2
yi = w0 + w1xi1 + w2xi2 + w3x2

i1 + w4x2
i2
i1 + w4x2
i2 + w5x3

i1 + w6x3
i2

(a) 1st order

(b) 3rd order

further extensions

more generally, for xi     rd+1 least squares id75 can be
performed on functions f (xi; w) of the form

s(cid:88)

yi     f (xi, w) =

gs(xi)ws.

for example,

s=1

gs(xi) = x2
ij
gs(xi) = log xij
gs(xi) = i(xij < a)
gs(xi) = i(xij < xij(cid:48))

as long as the function is linear in w1, . . . , ws, we can construct the matrix
x by putting the transformed xi on row i, and solve wls = (xtx)   1xty.

one caveat is that, as the number of functions increases, we need more data
to avoid over   tting.

geometry of least squares regression

thinking geometrically about least squares regression helps a lot.

(cid:73) we want to minimize (cid:107)y     xw(cid:107)2. think of the vector y as a point in rn.

we want to    nd w in order to get the product xw close to y.

(cid:73) if xj is the jth column of x, then xw =(cid:80)d+1

j=1 wjxj.

(cid:73) that is, we weight the columns in x by values in w to approximate y.

(cid:73) the ls solutions returns w such that xw is as close to y as possible in

the euclidean sense (i.e., intuitive    direct-line    distance).

geometry of least squares regression

arg min
w

(cid:107)y     xw(cid:107)2     wls = (xtx)

   1xty.

the columns of x de   ne a d + 1-dimensional
subspace in the higher dimensional rn.

the closest point in that subspace is the
orthonormal projection of y into the column
space of x.
right: y     r3 and data xi     r.

x1 = [1, 1, 1]t and x2 = [x1, x2, x3]t

the approximation is   y = xwls = x(xtx)   1xty.

geometry of least squares regression

(a) yi     w0 + xt

i w for i = 1, . . . , n

(b) y     xw

there are some key difference between (a) and (b) worth highlighting as you
try to develop the corresponding intuitions.

(a) can be shown for all n, but only for xi     r2 (not counting the added 1).

(b) this corresponds to n = 3 and one-dimensional data: x =

(cid:34) 1

1
1

(cid:35)

.

x1
x2
x3

y = w0 + x1w1 + x2w2x1x2y