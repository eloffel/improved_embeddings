coms 4721: machine learning for data science

lecture 6, 2/2/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

underdetermined linear equations

we now consider the regression problem y = xw where x     rn  d is    fat   
(i.e., d (cid:29) n). this is called an    underdetermined    problem.
(cid:73) there are more dimensions than observations.
(cid:73) w now has an in   nite number of solutions satisfying y = xw.

       y

       =

      

x

      

                            w

                           

these sorts of high-dimensional problems often come up:

(cid:73) in gene analysis there are 1000   s of genes but only 100   s of subjects.
(cid:73) images can have millions of pixels.
(cid:73) even polynomial regression can quickly lead to this scenario.

minimum (cid:96)2 regression

one solution (least norm)

one possible solution to the underdetermined problem is

wln = xt (xxt )   1y     xwln = xxt (xxt )   1y = y.

we can construct another solution by adding to wln a vector        rd that is in
the null space n of x:

       n (x)     x   = 0 and    (cid:54)= 0

and so x(wln +   ) = xwln + x   = y + 0.

in fact, there are an in   nite number of possible   , because d > n.

we can show that wln is the solution with smallest (cid:96)2 norm. we will use the
proof of this fact as an excuse to introduce two general concepts.

tools: analysis

we can use analysis to prove that wln satis   es the optimization problem

wln = arg min
w

(cid:107)w(cid:107)2

subject to xw = y.

(think of mathematical analysis as the use of inequalities to prove things.)
proof : let w be another solution to xw = y, and so x(w     wln) = 0. also,

(w     wln)twln = (w     wln)txt (xxt )   1y

= (x(w     wln)

)t (xxt )   1y = 0

(cid:124)

(cid:123)(cid:122)

= 0

(cid:125)

as a result, w     wln is orthogonal to wln. it follows that
(cid:107)w(cid:107)2 = (cid:107)w   wln +wln(cid:107)2 = (cid:107)w   wln(cid:107)2 +(cid:107)wln(cid:107)2 +2 (w     wln)twln

(cid:124)

(cid:123)(cid:122)

= 0

(cid:125)

> (cid:107)wln(cid:107)2

tools: lagrange multipliers

instead of starting from the solution, start from the problem,

wln = arg min
w

wtw subject to xw = y.

(cid:73) introduce lagrange multipliers: l(w,   ) = wtw +   t (xw     y).
(cid:73) minimize l over w maximize over   . if xw (cid:54)= y, we can get l = +   .
(cid:73) the optimal conditions are

   wl = 2w + xt    = 0,

     l = xw     y = 0.

we have everything necessary to    nd the solution:
1. from    rst condition: w =    xt   /2
2. plug into second condition:    =    2(xxt )   1y
3. plug this back into #1: wln = xt (xxt )   1y

sparse (cid:96)1 regression

ls and rr in high dimensions

usually not suited for high-dimensional data

(cid:73) modern problems: many dimensions/features/predictors
(cid:73) only a few of these may be important or relevant for predicting y
(cid:73) therefore, we need some form of    feature selection   

(cid:73) least squares and ridge regression:

(cid:73) treat all dimensions equally without favoring subsets of dimensions
(cid:73) the relevant dimensions are averaged with irrelevant ones
(cid:73) problems: poor generalization to new data, interpretability of results

regression with penalties

penalty terms
recall: general ridge regression is of the form

n(cid:88)

l =

(yi     f (xi; w))2 +   (cid:107)w(cid:107)2

we   ve referred to the term (cid:107)w(cid:107)2 as a penalty term and used f (xi; w) = xt

i w.

i=1

penalized    tting
the general structure of the optimization problem is

total cost = goodness-of-   t term + penalty term

(cid:73) goodness-of-   t measures how well our model f approximates the data.
(cid:73) penalty term makes the solutions we don   t want more    expensive   .
what kind of solutions does the choice (cid:107)w(cid:107)2 favor or discourage?

quadratic penalties

w2
j

intuitions

(cid:73) quadratic penalty: reduction in

cost depends on |wj|.

(cid:73) suppose we reduce wj by    w.
the effect on l depends on the
starting point of wj.

(cid:73) consequence: we should favor
vectors w whose entries are of
similar size, preferably small.

   w

   w

wj

sparsity

setting
(cid:73) regression problem with n data points x     rd, d (cid:29) n.
(cid:73) goal: select a small subset of the d dimensions and switch off the rest.
(cid:73) this is sometimes referred to as    feature selection   .

what does it mean to    switch off    a dimension?

(cid:73) each entry of w corresponds to a dimension of the data x.
(cid:73) if wk = 0, the prediction is

f (x, w) = xtw = w1x1 +        + 0    xk +        + wdxd,

so the prediction does not depend on the kth dimension.

(cid:73) feature selection: find a w that (1) predicts well, and (2) has only a

small number of non-zero entries.

(cid:73) a w for which most dimensions = 0 is called a sparse solution.

sparsity and penalties

penalty goal
find a penalty term which encourages sparse solutions.
quadratic penalty vs sparsity

(cid:73) suppose wk is large, all other wj are very small but non-zero
(cid:73) sparsity: penalty should keep wk, and push other wj to zero
(cid:73) quadratic penalty: will favor entries wj which all have similar size, and

so it will push wk towards small value.

overall, a quadratic penalty favors many small, but non-zero values.
solution
sparsity can be achieved using linear penalty terms.

lasso

sparse regression
lasso: least absolute shrinkage and selection operator

with the lasso, we replace the (cid:96)2 penalty with an (cid:96)1 penalty:

wlasso = arg min
w

where

(cid:107)y     xw(cid:107)2

2 +   (cid:107)w(cid:107)1

d(cid:88)

j=1

(cid:107)w(cid:107)1 =

|wj|.

this is also called (cid:96)1-regularized regression.

quadratic penalties

quadratic penalty
|wj|2

linear penalty
|wj|

wj

wj

reducing a large value wj achieves a
larger cost reduction.

cost reduction does not depend on the
magnitude of wj.

ridge regression vs lasso

w2

w2

wls

wls

w1

w1

this    gure applies to d < n, but gives intuition for d (cid:29) n.
(cid:73) red: contours of (w     wls)t (xtx)(w     wls) (see lecture 3)
(cid:73) blue: (left) contours of (cid:107)w(cid:107)1, and (right) contours of (cid:107)w(cid:107)2

2

coefficient profiles: rr vs lasso

(a) (cid:107)w(cid:107)2 penalty

(b) (cid:107)w(cid:107)1 penalty

(cid:96)p regression

(cid:96)p-norms
these norm-penalties can be extended to all norms:

(cid:16) d(cid:88)

|wj|p(cid:17) 1

p

(cid:107)w(cid:107)p =

for 0 < p        

j=1

(cid:96)p-regression
the (cid:96)p-regularized id75 problem is
(cid:107)y     xw(cid:107)2

w(cid:96)p := arg min
w

2 +   (cid:107)w(cid:107)p

p

we have seen:

(cid:73) (cid:96)1-regression = lasso
(cid:73) (cid:96)2-regression = ridge regression

(cid:96)p penalization terms

p = 4

p = 2

p = 1

p = 0.5

p = 0.1

p

behavior of (cid:107) .(cid:107)p

p =     norm measures largest absolute entry, (cid:107)w(cid:107)    = maxj |wj|
p > 2
p = 2
p = 1
p < 1
p     0

norm focuses on large entries
large entries are expensive; encourages similar-size entries
encourages sparsity
encourages sparsity as for p = 1, but contour set is not convex
(i.e., no    line of sight    between every two points inside the shape)

simply records whether an entry is non-zero, i.e. (cid:107)w(cid:107)0 =(cid:80)

i{wj (cid:54)= 0}

j

computing the solution for (cid:96)p

solution of (cid:96)p problem
(cid:96)2 aka ridge regression. has a closed form solution
(cid:96)p (p     1, p (cid:54)= 2)     by    id76   . we won   t discuss convex

analysis in detail in this class, but two facts are important

(cid:73) there are no    local optimal solutions    (i.e., local minimum of l)
(cid:73) the true solution can be found exactly using iterative algorithms

(p < 1)     we can only    nd an approximate solution (i.e., the best in
its    neighborhood   ) using iterative algorithms.

three techniques formulated as optimization problems

method

least squares
ridge regression
lasso

good-o-   t
(cid:107)y     xw(cid:107)2
(cid:107)y     xw(cid:107)2
(cid:107)y     xw(cid:107)2

2

2

2

penalty

none
(cid:107)w(cid:107)2
2
(cid:107)w(cid:107)1

solution method
analytic solution exists if xtx invertible
analytic solution exists always
numerical optimization to    nd solution

