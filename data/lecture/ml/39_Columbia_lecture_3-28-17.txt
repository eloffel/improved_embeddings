coms 4721: machine learning for data science

lecture 16, 3/28/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

soft id91 vs

hard id91 models

hard id91 models

review: id116 id91 algorithm
given: data x1, . . . , xn, where x     rd

goal: minimize l =(cid:80)n

(cid:80)k

i=1

k=1

1{ci = k}(cid:107)xi       k(cid:107)2.

(cid:73) iterate until values no longer changing

1. update c: for each i, set ci = arg mink (cid:107)xi       k(cid:107)2

2. update   : for each k, set   k =(cid:0)(cid:80)

i xi1{ci = k}(cid:1) /(cid:0)(cid:80)

1{ci = k}(cid:1)

i

id116 is an example of a hard id91 algorithm because it assigns
each observation to only one cluster.
in other words, ci = k for some k     {1, . . . , k}. there is no accounting for
the    boundary cases    by hedging on the corresponding ci.

soft id91 models

a soft id91 algorithm breaks the data across clusters intelligently.

(left) true cluster assignments of data from three gaussians.

(middle) the data as we see it.

(right) a soft-id91 of the data accounting for borderline cases.

(a)00.5100.51(b)00.5100.51(c)00.5100.51weighted id116 (soft id91 example)

weighted id116 id91 algorithm
given: data x1, . . . , xn, where x     rd
k=1   i(k)

goal: minimize l =(cid:80)n
conditions:   i(k) > 0,(cid:80)k

(cid:80)k
k=1   i(k) = 1, h(  i) = id178. set    > 0.

   (cid:80)

(cid:107)xi     k(cid:107)2

i h(  i) over   i and   k

i=1

  

(cid:73) iterate the following

1. update   : for each i, update the cluster allocation weights

for k = 1, . . . , k

  i(k) =

(cid:80)
exp{    1
j exp{    1

  (cid:107)xi       k(cid:107)2}
  (cid:107)xi       j(cid:107)2} ,
(cid:80)
(cid:80)

i xi  i(k)
i   i(k)

  k =

2. update   : for each k, update   k with the weighted average

soft id91 with weighted id116

   i  = 0.75 on green cluster & 0.25 blue cluster   i = 1 on blue clusterx  1  -de   ned regionwhen  i is binary,we get back the hard id91 modelmixture models

probabilistic soft id91 models

probabilistic vs non-probabilistic soft id91
the weight vector   i is like a id203 of xi being assigned to each cluster.

a mixture model is a probabilistic model where   i actually is a id203
distribution according to the model.

mixture models work by de   ning:

(cid:73) a prior distribution on the cluster assignment indicator ci
(cid:73) a likelihood distribution on observation xi given the assignment ci

intuitively we can connect a mixture model to the bayes classi   er:
(cid:73) class prior     cluster prior. this time, we don   t know the    label   
(cid:73) class-conditional likelihood     cluster-conditional likelihood

mixture models

(a) a id203 distribution on r2.

(b) data sampled from this distribution.

before introducing math, some key features of a mixture model are:

1. it is a generative model (de   nes a id203 distribution on the data)
2. it is a weighted combination of simpler distributions.

(cid:73) each simple distribution is in the same distribution family (i.e., a gaussian).
(cid:73) the    weighting    is de   ned by a discrete id203 distribution.

mixture models

generating data from a mixture model
data: x1, . . . , xn, where each xi     x (can be complicated, but think x = rd)
model parameters: a k-dim distribution    and parameters   1, . . . ,   k.
generative process: for observation number i = 1, . . . , n,

1. generate cluster assignment: ci
2. generate observation: xi     p(x|  ci).

iid    discrete(  )     prob(ci = k|  ) =   k.

some observations about this procedure:

(cid:73) first, each xi is randomly assigned to a cluster using distribution   .
(cid:73) ci indexes the cluster assignment for xi

(cid:73) this picks out the index of the parameter    used to generate xi.
(cid:73) if two x   s share a parameter, they are clustered together.

mixture models

(a) uniform mixing weights

(b) data sampled from this distribution.

(c) uneven mixing weights

(d) data sampled from this distribution.

gaussian mixture models

illustration

gaussian mixture models are mixture models where p(x|  ) is gaussian.
mixture of two gaussians

the red line is the density function.

in   uence of mixing weights

   = [0.5, 0.5]

(  1,   2
(  2,   2

1) = (0, 1)
2) = (2, 0.5)

the red line is the density function.

   = [0.8, 0.2]

(  1,   2
(  2,   2

1) = (0, 1)
2) = (2, 0.5)

gaussian mixture models (gmm)

the model
parameters: let    be a k-dimensional id203 distribution and (  k,   k)
be the mean and covariance of the kth gaussian in rd.

generate data: for the ith observation,
1. assign the ith observation to a cluster, ci     discrete(  )
2. generate the value of the observation, xi     n(  ci,   ci)
de   nitions:    = {  1, . . . ,   k} and    = {  1, . . . ,   k}.

goal: we want to learn   ,    and   .

gaussian mixture models (gmm)

maximum likelihood
objective: maximize the likelihood over model parameters   ,    and    by
treating the ci as auxiliary data using the em algorithm.

n(cid:89)

n(cid:89)

k(cid:88)

p(x1, . . . , xn|  ,   ,   ) =

p(xi|  ,   ,   ) =

p(xi, ci = k|  ,   ,   )

i=1

i=1

k=1

the summation over values of each ci    integrates out    this variable.

we can   t simply take derivatives with respect to   ,   k and   k and set to zero
to maximize this because there   s no closed form solution.

we could use gradient methods, but em is cleaner.

em algorithm

q: why not instead just include each ci and maximize(cid:81)n

since (we can show) this is easy to do using coordinate ascent?

i=1 p(xi, ci|  ,   ,   )

a: we would end up with a hard-id91 model where ci     {1, . . . , k}.

our goal here is to have soft id91, which em does.

em and the gmm
we will not derive everything from scratch. however, we can treat c1, . . . , cn
as the auxiliary data that we integrate out.

therefore, we use em to

n(cid:88)

maximize

ln p(xi|  ,   ,   ) by using

i=1

let   s look at the outlines of how to derive this.

n(cid:88)

i=1

ln p(xi, ci|  ,   ,   )

the em algorithm and the gmm

(cid:90)

from the last lecture, the generic em objective is

ln p(x|  1) =

q(  2) ln

p(x,   2|  1)

q(  2)

d  2 +

q(  2) ln

q(  2)

p(  2|x,   1)

d  2

(cid:90)

the em objective for the gaussian mixture model is

n(cid:88)

i=1

ln p(xi|  ,   ,   ) =

n(cid:88)
n(cid:88)

i=1

k(cid:88)
k(cid:88)

k=1

i=1

k=1

q(ci = k) ln

p(xi, ci = k|  ,   ,   )

q(ci = k)

+

q(ci = k) ln

q(ci = k)

p(ci|xi,   ,   ,   )

because ci is discrete, the integral becomes a sum.

em setup (one iteration)

first: set q(ci = k)     p(ci = k|xi,   ,   ,   ) using bayes rule:
p(ci = k|xi,   ,   ,   )     p(ci = k|  )p(xi|ci = k,   ,   )

we can solve the posterior of ci given   ,    and   :

q(ci = k) =

(cid:80)
  kn(xi|  k,   k)
j   jn(xi|  j,   j)

=      i(k)

e-step: take the expectation using the updated q   s

n(cid:88)

k(cid:88)

l =

  i(k) ln p(xi, ci = k|  ,   k,   k) + constant w.r.t.   ,   ,   

i=1

k=1

m-step: maximize l with respect to    and each   k,   k.

m-step close up

aside: how has em made this easier?

original objective function:

n(cid:88)

k(cid:88)

i=1

k=1

l =

ln

p(xi, ci = k|  ,   k,   k) =

n(cid:88)

ln

k(cid:88)

i=1

k=1

  kn(xi|  k,   k).

the log-sum form makes optimizing   , and each   k and   k dif   cult.

using em here, we have the m-step:

n(cid:88)

k(cid:88)

i=1

k=1

(cid:124)

(cid:123)(cid:122)

(cid:125)

ln p(xi,ci=k|  ,  k,  k)

l =

  i(k){ln   k + ln n(xi|  k,   k)}

+ constant w.r.t.   ,   ,   

the sum-log form is easier to optimize. we can take derivatives and solve.

em for the gmm

algorithm: maximum likelihood em for the gmm
given: x1, . . . , xn where x     rd

goal: maximize l =(cid:80)n

i=1 ln p(xi|  ,   ,   ).

(cid:73) iterate until incremental improvement to l is    small   

1. e-step: for i = 1, . . . , n, set

  i(k) =

for k = 1, . . . , k

2. m-step: for k = 1, . . . , k, de   ne nk =(cid:80)n

  k =

nk
n ,   k =

1
nk

i=1

i=1   i(k) and update the values

  i(k)(xi     k)(xi     k)t

n(cid:88)

i=1

1
nk

,

(cid:80)
  kn(xi|  k,   k)
j   jn(xi|  j,   j)
n(cid:88)

  i(k)xi   k =

comment: the updated value for   k is used when updating   k.

gaussian mixture model: example run

a random initialization

(a)   202   202gaussian mixture model: example run

iteration 1 (e-step)

assign data to clusters

(b)   202   202gaussian mixture model: example run

iteration 1 (m-step)

update the gaussians

(c)l=1   202   202gaussian mixture model: example run

iteration 2

assign data to clusters
and update the gaussians

(d)l=2   202   202gaussian mixture model: example run

iteration 5 (skipping ahead)

assign data to clusters
and update the gaussians

(e)l=5   202   202gaussian mixture model: example run

iteration 20 (convergence)

assign data to clusters
and update the gaussians

(f)l=20   202   202gmm and the bayes classifier

the gmm feels a lot like a k-class bayes classi   er, where the label of xi is

label(xi) = arg max

k

  k n(xi|  k,   k).

(cid:73)   k = class prior, and n(  k,   k) = class-conditional density function.
(cid:73) we learned   ,    and    using maximum likelihood there too.

for the bayes classi   er, we could    nd   ,    and    with a single equation
because the class label was known. compare with the gmm update:

  k =

nk
n ,   k =

1
nk

  i(k)xi   k =

1
nk

  i(k)(xi       k)(xi       k)t

n(cid:88)

i=1

n(cid:88)

i=1

they   re almost identical. but since   i(k) is changing we have to update these
values. with the bayes classi   er,      i    encodes the label, so it was known.

choosing the number of clusters

maximum likelihood for the gaussian mixture model can over   t the data. it
will learn as many gaussians as it   s given.

there are a set of techniques for this based on the dirichlet distribution.

a dirichlet prior is used on    which encourages many gaussians to
disappear (i.e., not have any data assigned to them).

em for a generic mixture model

algorithm: maximum likelihood em for mixture models
given: data x1, . . . , xn where x     x

goal: maximize l =(cid:80)n

i=1 ln p(xi|  ,   ), where p(x|  k) is problem-speci   c.

(cid:73) iterate until incremental improvement to l is    small   

1. e-step: for i = 1, . . . , n, set

(cid:80)
  k p(xi|  k)
j   j p(xi|  j)

  i(k) =

,

for k = 1, . . . , k

2. m-step: for k = 1, . . . , k, de   ne nk =(cid:80)n
n(cid:88)

  k = arg max

  k =

nk
n ,

  

i=1

i=1   i(k) and set

  i(k) ln p(xi|  )

comment: similar to generalization of the bayes classi   er for any p(x|  k).

