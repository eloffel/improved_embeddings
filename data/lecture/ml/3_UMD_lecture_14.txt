slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 14 (sub)id119

furong huang / furongh@cs.umd.edu

regrading deadline extended

march 29 11:00 am
submit along with you exam sheets
don   t forget to put your name

recap: linear models

general framework for binary classification
cast learning as optimization problem
optimization objective combines 2 terms

id168: measures how well classifier fits training 
data 
regularizer: measures how simple classifier is

    does not assume data is linearly separable
lets us separate model definition from 
training algorithm (id119)

casting linear classification
as an optimization problem

objective 
function

id168

measures how well 
classifier fits training 

data

regularizer

prefers solutions 
that generalize 

well

indicator function: 1 if (.) is true, 0 otherwise

the id168 above is called the 0-1 loss

id119

a general solution for our optimization problem

idea: take iterative steps to update parameters in the direction 
of the gradient

id119 algorithm

objective function 

to minimize

number of 

steps

step size

illustrating id119
in 1-dimensional case

id119

2 questions

when to stop?

when the gradient gets close to zero
when the objective stops changing much
when the parameters stop changing much
early
when performance on held-out dev set plateaus

how to choose the step size?

start with large steps, then take smaller steps

now let   s calculate gradients for  
multivariate objectives

consider the following learning objective

what do we need to do to run gradient 
descent?

(1) derivative with respect to b

(2) gradient with respect to w

subgradients

problem: some objective functions are not 
differentiable everywhere

hinge loss, l1 norm

solution: subgradient optimization

let   s ignore the problem, and just try to apply 
id119 anyway!!
we will just differentiate by parts   

subgradient review

    subgradient generalized the 
derivative to functions which 
are not differentiable.

    for any x0 in the domain of 
the function one can draw a 
line which goes through the 
point (x0, f(x0)) and which is 
everywhere either touching
or below the graph of f.

    set-valued

subgradient review 

rigorously, a subgradient of a convex function f:i   r at a point x0 in the

open interval i is a real number c such that

for all x in i. one may show that the set of subgradients at x0 for a 
convex function is a nonempty closed interval [a, b], where a and b are 
the one-sided limits.

which are guaranteed to exist and satisfy a     b.
    the set [a, b] of all subgradients is called the subgradients of the 
function f at x0. since f is convex, if its subdifferential at x0 contains 
exactly one subgradient, then f is differentiable at x0.

approximating the 0-1 loss with 
surrogate id168s

examples (with b = 0)

hinge loss
log loss
exponential loss

what if !   0?

example: subgradient of
hinge loss

for a given example n

subid119 
for hinge loss

what is the id88 optimizing?

id168 is a variant of the hinge loss

recap: linear models

lets us separate model definition from 
training algorithm (id119)

summary

id119

a generic algorithm to minimize objective functions
works well as long as functions are well behaved (ie
convex)
subid119 can be used at points where 
derivative is not defined
choice of step size is important

optional: can we do better? 

for some objectives, we can find closed form solutions 
(see ciml 6.6)

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

