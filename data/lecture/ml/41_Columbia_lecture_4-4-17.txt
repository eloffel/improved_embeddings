coms 4721: machine learning for data science

lecture 18, 4/4/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

id96

models for text data

given text data we want to:

(cid:73) organize
(cid:73) visualize
(cid:73) summarize
(cid:73) search
(cid:73) predict
(cid:73) understand

topic models allow us to

1. discover themes in text
2. annotate documents
3. organize, summarize, etc.

id96

id96

a probabilistic topic model
(cid:73) learns distributions on words called    topics    shared by documents
(cid:73) learns a distribution on topics for each document
(cid:73) assigns every word in a document to a topic

id96

however, none of these things are known in advance and must be learned
(cid:73) each document is treated as a    bag of words   
(cid:73) need to de   ne (1) a model, and (2) an algorithm to learn it
(cid:73) we will review the standard topic model, but won   t cover id136

id44

there are two essential ingredients to id44 (lda).

1. a collection of distributions on words (topics).
2. a distribution on topics for each document.

youthvotepoliticsrateballreasoninterestpowersenseproofboyscorebrainordersetseasonsenatetax  1  2  3id44

there are two essential ingredients to id44 (lda).

1. a collection of distributions on words (topics).
2. a distribution on topics for each document.

  1id44

there are two essential ingredients to id44 (lda).

1. a collection of distributions on words (topics).
2. a distribution on topics for each document.

  2id44

there are two essential ingredients to id44 (lda).

1. a collection of distributions on words (topics).
2. a distribution on topics for each document.

  3id44

there are two essential ingredients to id44 (lda).

1. a collection of distributions on words (topics).
2. a distribution on topics for each document.

the generative process for lda is:

1. generate each topic, which is a distribution on words

  k     dirichlet(  ),

k = 1, . . . , k

2. for each document, generate a distribution on topics

  d     dirichlet(  ),
3. for the nth word in the dth document,

d = 1, . . . , d

a) allocate the word to a topic, cdn     discrete(  d)
b) generate the word from the selected topic, xdn     discrete(  cdn )

dirichlet distribution

a continuous distribution on discrete id203 vectors. let   k be a
id203 vector and    a positive parameter vector,

  ((cid:80)
(cid:81)v

v   v)
v=1   (  v)

v(cid:89)

v=1

    v   1

k,v

p(  k|  ) =

this de   nes the dirichlet distribution. some examples of   k generated from
this distribution for a constant value of    and v = 10 are given below.

   = 1

dirichlet distribution

a continuous distribution on discrete id203 vectors. let   k be a
id203 vector and    a positive parameter vector,

  ((cid:80)
(cid:81)v

v   v)
v=1   (  v)

v(cid:89)

v=1

    v   1

k,v

p(  k|  ) =

this de   nes the dirichlet distribution. some examples of   k generated from
this distribution for a constant value of    and v = 10 are given below.

   = 10

dirichlet distribution

a continuous distribution on discrete id203 vectors. let   k be a
id203 vector and    a positive parameter vector,

  ((cid:80)
(cid:81)v

v   v)
v=1   (  v)

v(cid:89)

v=1

    v   1

k,v

p(  k|  ) =

this de   nes the dirichlet distribution. some examples of   k generated from
this distribution for a constant value of    and v = 10 are given below.

   = 100

dirichlet distribution

a continuous distribution on discrete id203 vectors. let   k be a
id203 vector and    a positive parameter vector,

  ((cid:80)
(cid:81)v

v   v)
v=1   (  v)

v(cid:89)

v=1

    v   1

k,v

p(  k|  ) =

this de   nes the dirichlet distribution. some examples of   k generated from
this distribution for a constant value of    and v = 10 are given below.

   = 1

dirichlet distribution

a continuous distribution on discrete id203 vectors. let   k be a
id203 vector and    a positive parameter vector,

  ((cid:80)
(cid:81)v

v   v)
v=1   (  v)

v(cid:89)

v=1

    v   1

k,v

p(  k|  ) =

this de   nes the dirichlet distribution. some examples of   k generated from
this distribution for a constant value of    and v = 10 are given below.

   = 0.1

dirichlet distribution

a continuous distribution on discrete id203 vectors. let   k be a
id203 vector and    a positive parameter vector,

  ((cid:80)
(cid:81)v

v   v)
v=1   (  v)

v(cid:89)

v=1

    v   1

k,v

p(  k|  ) =

this de   nes the dirichlet distribution. some examples of   k generated from
this distribution for a constant value of    and v = 10 are given below.

   = 0.01

lda output

lda outputs two main things:

1. a set of distributions on words (topics). shown above are ten topics
from nyt data. we list the ten words with the highest id203.

2. a distribution on topics for each document (not shown). this indicates

its thematic breakdown and provides a compact representation.

lda and id105

q: for a particular document, what is p(xdn = i|      ,   d)?
a: find this by integrating out the cluster assignment,

k(cid:88)
k(cid:88)

k=1

k=1

(cid:124)

p(xdn = i|      ,   ) =

=

p(xdn = i, cdn = k|      ,   d)

p(xdn = i,|      , cdn = k)

p(cdn = k|  d)

(cid:125)

(cid:124)

(cid:123)(cid:122)

=   dk

(cid:125)

(cid:123)(cid:122)

=   ki

let b = [  1, . . . ,   k] and    = [  1, . . . ,   d], then p(xdn = i|  ,   ) = (b  )id

in other words, we can read the probabilities from a matrix formed by taking
the product of two matrices that have nonnegative entries.

nonnegative matrix

factorization

nonnegative id105

lda can be thought of as an instance of nonnegative id105.

(cid:73) it is a probabilistic model.
(cid:73) id136 involves techniques not taught in this course.

we will discuss two other related models and their algorithms. these two
models are called nonnegative id105 (nmf)

(cid:73) they can be used for the same tasks as lda
(cid:73) though    nonnegative id105    is a general technique,

   nmf    usually just refers to the following two methods.

nonnegative id105

we use notation and think about the problem slightly differently from pmf
(cid:73) data x has nonnegative entries. none missing, but likely many zeros.
(cid:73) the learned factorization w and h also have nonnegative entries.

(cid:73) the value xij    (cid:80)

k wikhkj, but we won   t write this with vector notation

(cid:73) later we interpret the output in terms of columns of w and h.

n2 "objects"n1 dimensions{{(i,j)-th entry, xij>_0~~wik>_0hkj>_ 0{rank = knonnegative id105

what are some data modeling problems that can constitute x?

(cid:73) text data:

(cid:73) word term frequencies
(cid:73) xij contains the number of times word i appears in document j.

(cid:73) image data:

(cid:73) face identi   cation data sets
(cid:73) put each vectorized n    m image of a face on a column of x.

(cid:73) other discrete grouped data:

(cid:73) quantize continuous sets of features using id116
(cid:73) xij counts how many times group j uses cluster i.
(cid:73) for example: group = song, features = d    n spectral information matrix

two objective functions

nmf minimizes one of the following two objective functions over w and h.

choice 1: squared error objective

(cid:107)x     wh(cid:107)2 =

choice 2: divergence objective

d(x(cid:107)wh) =    (cid:88)

(cid:88)
(cid:88)

i

(cid:88)

(xij     (wh)ij)2

j

[xij ln(wh)ij     (wh)ij]

i

j

(cid:73) both have the constraint that w and h contain nonnegative values.
(cid:73) nmf uses a fast, simple algorithm for optimizing these two objectives.

minimization and multiplicative algorithms1

recall what we should look for in minimizing an objective    min

h

f(h)   :

1. a way to generate a sequence of values h1, h2, . . . , such that

f(h1)     f(h2)     f(h3)           

2. convergence of the sequence to a local minimum of f

the following algorithms ful   ll these
requirements. in this case:

(cid:73) minimization is done via an

   auxiliary function.   

(cid:73) leads to a    multiplicative
algorithm    for w and h.

(cid:73) we   ll skip details (see reference).

1for details, see d.d. lee and h.s. seung (2001).    algorithms for non-negative matrix

factorization.    advances in neural information processing systems.

multiplicative update for (cid:107)x     wh(cid:107)2

problem

min (cid:80)

ij(xij     (wh)ij)2

subject to wik     0, hkj     0.

algorithm

(cid:73) randomly initialize h and w with nonnegative values.
(cid:73) iterate the following,    rst for all values in h, then all in w:

hkj     hkj

wik     wik

(wtx)kj
(wtwh)kj
(xht )ik
(whht )ik

,

,

until the change in (cid:107)x     wh(cid:107)2 is    small.   

visualization and maximum likelihood

a visualization that may be helpful. use the color-coded de   nition above.
(cid:73) use element-wise multiplication/division across three columns below.
(cid:73) use id127 within each outlined box.

probabilistically, the squared error penalty implies a gaussian distribution,

xij     n((cid:80)

k wikhkj,   2)

since xij     0 (and often isn   t continuous), we are making an incorrect
modeling assumption. nevertheless, as with pmf it still works well.

~~xwh.*. /.*. /multiplicative update for d(x(cid:107)wh)

(cid:105)

xij ln

1

(wh)ij

+ (wh)ij

problem

min (cid:80)

ij

(cid:104)

algorithm

subject to wik     0, hkj     0.

(cid:73) randomly initialize h and w with nonnegative values.
(cid:73) iterate the following,    rst for all values in h, then all in w:

(cid:80)
(cid:80)

i wikxij/(wh)ij

j hkjxij/(wh)ij

(cid:80)
(cid:80)

i wik

j hkj

,

,

hkj     hkj

wik     wik

until the change in d(x(cid:107)wh) is    small.   

visualization

visualizing the update for the divergence penalty is more complicated.

(cid:73) use the color-coded de   nition above.
(cid:73)    purple    is the data matrix    dot-divided    by the approximation of it.

__./defxwhnormalize the rows ofthis transposed matrixso they sum to one.*.*normalize the columns of this matrix so they sum to onemaximum likelihood

the maximum likelihood interpretation of the divergence penalty is more
interesting than for the squared error penalty.

if we model the data as independent poisson random variables

xij     pois((wh)ij),

pois(x|  ) =

e     , x     {0, 1, 2, . . .},

  x
x!

then the negative divergence penalty is maximum likelihood for w and h.

(cid:88)
(cid:88)

ij

   d(x(cid:107)wh) =

[xij ln(wh)ij     (wh)ij]

we use: p(x|w, h) =(cid:81)

=

ij

ij p(xij|w, h) =(cid:81)

ij pois(xij|(wh)ij).

ln p(xij|w, h) + constant

nmf and id96

as discussed, nmf can be used for id96. in fact, one can show
that the divergence penalty is closely related mathematically to lda.

step 1. form the term-frequency matrix x. (xij = # times word i in doc j)
step 2. run nmf to learn w and h using d(x(cid:107)wh) penalty
step 3. as an added step, after step 2 is complete, for k = 1, . . . , k

1. set ak =(cid:80)

i wik

2. divide wik by ak for all i
3. multiply hkj by ak for all j
notice that this is does not change the id127 wh.

interpretation: the kth column of w can be interpreted as the kth topic. the
jth column of h can be interpreted as how much document j uses each topic.

nmf and face modeling

for face modeling, put the face images along the columns of x and factorize.
show columns of w as image. compare this with id116 and svd.

id116 (i.e., vq): equivalent to each column of h having a single 1.
id116 learns averages of full faces.

nmf and face modeling

for face modeling, put the face images along the columns of x and factorize.
show columns of w as image. compare this with id116 and svd.

svd: finds the singular value decomposition of x.
results not interpretable because of    values and orthogonality constraint

nmf and face modeling

for face modeling, put the face images along the columns of x and factorize.
show columns of w as image. compare this with id116 and svd.

nmf learns a    parts-based    representation. each column captures something
interpretable. this is a result of the nonnegativity constraint.

