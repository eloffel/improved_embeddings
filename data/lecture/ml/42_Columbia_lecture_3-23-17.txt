coms 4721: machine learning for data science

lecture 15, 3/23/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

maximum likelihood

approaches to data modeling

our approaches to modeling data thus far have been either probabilistic or
non-probabilistic in motivation.

(cid:73) probabilistic models: id203 distributions de   ned on data, e.g.,

1. bayes classi   ers
2. id28
3. least squares and ridge regression (using ml and map interpretation)
4. bayesian id75

(cid:73) non-probabilistic models: no id203 distributions involved, e.g.,

1. id88
2. support vector machine
3. id90
4. id116

in every case, we have some objective function we are trying to optimize
(greedily vs non-greedily, locally vs globally).

maximum likelihood

as we   ve seen, one probabilistic objective function is maximum likelihood.

setup: in the most basic scenario, we start with

1. some set of model parameters   
2. a set of data {x1, . . . , xn}
3. a id203 distribution p(x|  )
iid    p(x|  )
4. an i.i.d. assumption, xi

maximum likelihood seeks the    that maximizes the likelihood

  ml = arg max

  

p(x1, . . . , xn|  )

(a)
= arg max

  

p(xi|  )

(b)
= arg max

  

n(cid:89)

i=1

n(cid:88)

i=1

ln p(xi|  )

(a) follows from i.i.d. assumption.
(b) follows since f (y) > f (x)     ln f (y) > ln f (x).

maximum likelihood

we   ve discussed maximum likelihood for a few models, e.g., least squares
id75 and the bayes classi   er.

both of these models were    nice    because we could    nd their respective   ml
analytically by writing an equation and plugging in data to solve.

gaussian with unknown mean and covariance
in the    rst lecture, we saw if xi

iid    n(  ,   ), where    = {  ,   }, then

n(cid:89)

i=1

      ln

p(xi|  ) = 0

gives the following maximum likelihood values for    and   :

n(cid:88)

i=1

n(cid:88)

i=1

  ml =

1
n

xi,

  ml =

1
n

(xi       ml)(xi       ml)t

coordinate ascent and maximum likelihood

in more complicated models, we might split the parameters into groups
  1,   2 and try to maximize the likelihood over both of these,

  1,ml,   2,ml = arg max
  1,  2

ln p(xi|  1,   2),

n(cid:88)

i=1

although we can solve one given the other, we can   t solve it simultaneously.

coordinate ascent (probabilistic version)
we saw how id116 presented a similar situation, and that we could
optimize using coordinate ascent. this technique is generalizable.

algorithm: for iteration t = 1, 2, . . . ,

1. optimize   (t)
2. optimize   (t)

1 = arg max  1
2 = arg max  2

(cid:80)n
(cid:80)n
i=1 ln p(xi|  1,   (t   1)
i=1 ln p(xi|  (t)
1 ,   2)

2

)

coordinate ascent and maximum likelihood

there is a third (subtly) different situation, where we really want to    nd

  1,ml = arg max
  1

ln p(xi|  1).

n(cid:88)

i=1

except this function is    tricky    to optimize directly. however, we    gure out
that we can add a second variable   2 such that

ln p(xi,   2|  1)

(function 2)

is easier to work with. we   ll make this clearer later.

i=1

(cid:73) notice in this second case that   2 is on the left side of the conditioning

bar. this implies a prior on   2, (whatever      2    turns out to be).

(cid:73) we will next discuss a fundamental technique called the em algorithm

for    nding   1,ml by using function 2 instead.

n(cid:88)

expectation-maximization

algorithm

a motivating example

let xi     rd, be a vector with missing data. split this vector into two parts:
1. xo
2. xm
3. the missing dimensions can be different for different xi.

i     observed portion (the sub-vector of xi that is measured)
i     missing portion (the sub-vector of xi that is still unknown)

we assume that xi

iid    n(  ,   ), and want to solve

  ml,   ml = arg max
  ,  

ln p(xo

i |  ,   ).

this is tricky. however, if we knew xm

i (and therefore xi), then

  ml,   ml = arg max
  ,  

ln p(xo

(cid:124)

(cid:125)
(cid:123)(cid:122)
i |  ,   )
i , xm
= p(xi|  ,  )

is very easy to optimize (we just did it on a previous slide).

n(cid:88)

i=1

n(cid:88)

i=1

connecting to a more general setup

we will discuss a method for optimizing(cid:80)n

missing values {xm

1 , . . . , xm

n }. this is a very general technique.

i=1 ln p(xo

i |  ,   ) and imputing its

general setup
imagine we have two parameter sets   1,   2, where
p(x,   2|  1) d  2

p(x|  1) =

(cid:90)

(marginal distribution)

example: for the previous example we can show that

p(xo

i |  ,   ) =

p(xo

i , xm

i |  ,   ) dxm

i = n(  o

i ,   o
i ),

(cid:90)

where   o

i and   o

i are the sub-vector/sub-matrix of    and    de   ned by xo
i .

the em objective function

we need to de   ne a general objective function that gives us what we want:
1. it lets us optimize the marginal p(x|  1) over   1,
2. it uses p(x,   2|  1) in doing so purely for computational convenience.

the em objective function
before picking it apart, we claim that this objective function is

ln p(x|  1) =

q(  2) ln

p(x,   2|  1)

q(  2)

d  2 +

q(  2) ln

q(  2)

p(  2|x,   1)

d  2

(cid:90)

(cid:90)

some immediate comments:

(cid:73) q(  2) is any id203 distribution (assumed continuous for now)
(cid:73) we assume we know p(  2|x,   1). that is, given the data x and    xed

values for   1, we can solve the conditional posterior distribution of   2.

deriving the em objective function

(cid:90)
(cid:90)

let   s show that this equality is actually true

ln p(x|  1) =

=

q(  2) ln

q(  2) ln

p(x,   2|  1)

q(  2)

d  2 +
p(x,   2|  1)q(  2)
p(  2|x,   1)q(  2)

d  2

(cid:90)

q(  2) ln

q(  2)

p(  2|x,   1)

d  2

remember some rules of id203:

p(a, b|c) = p(a|b, c)p(b|c)     p(b|c) =

p(a, b|c)
p(a|b, c)

.

letting a =   1, b = x and c =   1, we conclude

(cid:90)

ln p(x|  1) =

q(  2) ln p(x|  1) d  2

= ln p(x|  1)

the em objective function

the em objective function splits our desired objective into two terms:

ln p(x|  1) =

q(  2) ln

d  2

+

q(  2) ln

(cid:90)
(cid:124)

p(x,   2|  1)

q(  2)

(cid:123)(cid:122)

(cid:125)

(cid:90)
(cid:124)

q(  2)

p(  2|x,   1)

(cid:123)(cid:122)

d  2

(cid:125)

a function only of   1, we   ll call it l

id181

some more observations about the right hand side:
1. the kl diverence is always     0 and only = 0 when q = p.
2. we are assuming that the integral in l can be calculated, leaving a
function only of   1 (for a particular setting of the distribution q).

bigger picture

q: what does it mean to iteratively optimize ln p(x|  1) w.r.t.   1?

a: one way to think about it is that we want a method for generating:
1 )     ln p(x|  (t   1)
1. a sequence of values for   1 such that ln p(x|  (t)
2. we want   (t)
1

to converge to a local maximum of ln p(x|  1).

1

).

it doesn   t matter how we generate the sequence   (1)
1

,   (2)
1

,   (3)
1

, . . .

we will show how em generates #1 and just mention that em satis   es #2.

the em algorithm

the em objective function

(cid:90)
(cid:124)

ln p(x|  1) =

q(  2) ln

p(x,   2|  1)

q(  2)

(cid:123)(cid:122)

de   ne this to be l(x,   1)

(cid:90)
(cid:124)

+

d  2

(cid:125)

q(  2) ln

q(  2)

p(  2|x,   1)

(cid:123)(cid:122)

d  2

(cid:125)

id181

de   nition: the em algorithm
given the value   (t)
e-step: set qt(  2) = p(  2|x,   (t)

1 ,    nd the value   (t+1)

1

(cid:90)

1 ) and calculate
qt(  2) ln p(x,   2|  1) d  2    

lqt (x,   1) =

as follows:

(cid:90)
(cid:124)

(cid:123)(cid:122)

qt(  2) ln qt(  2) d  2

.

can ignore this term

(cid:125)

m-step: set   (t+1)

1

= arg max  1 lqt (x,   1).

proof of monotonic improvement

once we   re comfortable with the moving parts, the proof that the sequence
1 monotonically improves ln p(x|  1) just requires analysis:
  (t)
q(  2)(cid:107) p(  2|x1,   (t)
1 )

1 ) = l(x,   (t)

ln p(x|  (t)

1 ) + kl

(cid:16)

(cid:17)
(cid:125)

(cid:124)

(cid:123)(cid:122)

= 0 by setting q = p

    e-step
)     m-step

(cid:16)

(cid:124)

(cid:16)

) + kl

qt(  2)(cid:107) p(  2|x1,   (t+1)

)

1

(cid:123)(cid:122)

> 0 because q(cid:54)=p

q(  2)(cid:107) p(  2|x1,   (t+1)

1

)

(cid:17)
(cid:125)

(cid:17)

= lqt (x,   (t)
1 )
    lqt (x,   (t+1)
    lqt (x,   (t+1)

1

1

) + kl

= l(x,   (t+1)
= ln p(x|  (t+1)

1

1

)

one iteration of em

start: current setting of   1 and q(  2)

for reference:
ln p(x|  1) = l + kl

(cid:90)
(cid:90)

l =

kl =

q(  2) ln

q(  2) ln

p(x,   2|  1)

q(  2)

d  2

q(  2)

p(  2|x,   1)

d  2

lkl(q||p)}lnp(x|  1)(x|  1)some arbitrary point < 0one iteration of em

e-step: set q(  2) = p(  2|x,   1) and update l.

for reference:
ln p(x|  1) = l + kl

(cid:90)
(cid:90)

l =

kl =

q(  2) ln

q(  2) ln

p(x,   2|  1)

q(  2)

d  2

q(  2)

p(  2|x,   1)

d  2

some arbitrary point < 0lnp(x|  1)lkl(q||p)=0(x|  1)one iteration of em

m-step: maximize l wrt   1. now q (cid:54)= p.

for reference:
ln p(x|  1) = l + kl

(cid:90)
(cid:90)

l =

kl =

q(  2) ln

q(  2) ln

p(x,   2|  1)

q(  2)

d  2

q(  2)

p(  2|x,   1)

d  2

lnp(x|  1up)lkl(q||p)}some arbitrary point < 0(x|  1up)em for missing data

the problem

we have a data matrix with missing entries. we model the columns as

iid    n(  ,   ).

xi

our goal could be to

1. learn    and    using maximum likelihood
2. fill in the missing values    intelligently    (e.g., using a model)
3. both

we will see how to achieve both of these goals using the em algorithm.

em for single gaussian model with missing data

the original, generic em objective is
p(x,   2|  1)

ln p(x|  1) =

q(  2) ln

(cid:90)

(cid:90)

d  2 +

q(  2) ln

q(  2)

p(  2|x,   1)

d  2

the em objective for this speci   c problem and notation is

n(cid:88)

i=1

ln p(xo

i |  ,   ) =

q(  2)

(cid:90)
(cid:90)

n(cid:88)
n(cid:88)

i=1

i=1

q(xm

i ) ln

p(xo

i |  ,   )
i , xm
q(xm
i )

dxm

i +

q(xm

i ) ln

q(xm
i )
i |xo
i ,   ,   )

p(xm

dxm
i

we can calculate everything required to do this.

e-step (part one)

i ,   ,   ) using current   ,   

i represent the observed and missing dimensions of xi. for

i ) = p(xm

set q(xm
let xo
notational convenience, think

i and xm

i |xo
(cid:20) xo

i
xm
i

xi =

then we can show that p(xm

(cid:21)(cid:19)

(cid:21)

    n

(cid:21)

(cid:20)   oo

(cid:18)(cid:20)   o
i ,   ,   ) = n((cid:98)  i,(cid:98)  i), where

  om
  mm

i
  mo
i

i
  m
i

i
i

,

i |xo

(cid:98)  i =   m

i +   mo

i (  oo

i )   1(xo

i       o
i ),

i       mo

i (  oo

i )   1  om

i

.

(cid:98)  i =   mm

it doesn   t look nice, but these are just functions of sub-vectors of    and
sub-matrices of    using the relevant dimensions de   ned by xi.

e-step (part two)

e-step: eq(xm
for each i we will need to calculate the following term,

i )[ln p(xo

i , xm

i |  ,   )]

eq[(xi       )t      1(xi       )] = eq[trace{     1(xi       )(xi       )t}]
= trace{     1eq[(xi       )(xi       )t ]}

i ) = p(xm

i |xo

i ,   ,   ). so only the xm
i

the expectation is calculated using q(xm
portion of xi will be integrated.

i ) = n((cid:98)  i,(cid:98)  i). we de   ne

to this end, recall q(xm

1. (cid:98)xi : a vector where we replace the missing values in xi with(cid:98)  i.
2. (cid:98)vi : a matrix of 0   s, plus sub-matrix(cid:98)  i in the missing dimensions.

m-step

m-step: maximize(cid:80)n

we   ll omit the derivation, but the expectation can now be solved and

i=1

i |  ,   )]

i , xm

  up,   up = arg max
  ,  

eq[ln p(xo
n(cid:88)
can be found. recalling the(cid:98) notation,
(cid:98)xi,
{((cid:98)xi       up)((cid:98)xi       up)t +(cid:98)vi}

n(cid:88)
n(cid:88)

eq[ln p(xo

i |  ,   )]

  up =

  up =

i , xm

1
n

i=1

i=1

1
n

i=1

then return to the e-step to calculate the new p(xm

i |xo

i ,   up,   up).

implementation details

we need to initialize    and   , for example, by setting missing values to zero
and calculating   ml and   ml. (we can also use random initialization.)

the em objective function is then calculated after each update to    and   
and will look like the    gure above. stop when the change is    small.   

the output is   ml,   ml and q(xm

i ) for all missing entries.

