coms 4721: machine learning for data science

lecture 17, 3/30/2017

prof. john paisley

department of electrical engineering

& data science institute

columbia university

id185

object recommendation

matching consumers to products is an important practical problem.

we can often make these connections using user feedback about subsets of
products. to give some prominent examples:

(cid:73) net   ix lets users to rate movies
(cid:73) amazon lets users to rate products and write reviews about them
(cid:73) yelp lets users to rate businesses, write reviews, upload pictures
(cid:73) youtube lets users like/dislike a videos and write comments

id126s use this information to help recommend new
things to customers that they may like.

content filtering

one strategy for object recommendation is:

content    ltering: use known information about the products and users to
make recommendations. create pro   les based on

(cid:73) products: movie information, price information, product descriptions
(cid:73) users: demographic information, questionnaire information

example: a fairly well known example is the online radio pandora, which
uses the    music genome project.   

(cid:73) an expert scores a song based on hundreds of characteristics
(cid:73) a user also provides information about his/her music preferences
(cid:73) recommendations are made based on pairing these two sources

id185

content    ltering requires a lot of information that can be dif   cult and
expensive to collect. another strategy for object recommendation is:

collaborative    ltering (cf): use previous users    input/behavior to make
future recommendations. ignore any a priori user or object information.

(cid:73) cf uses the ratings of similar users to predict my rating.
(cid:73) cf is a domain-free approach. it doesn   t need to know what is being

rated, just who rated what, and what the rating was.

one cf method uses a neighborhood-based approach. for example,

1. de   ne a similarity score between me and other users based on how

much our overlapping ratings agree, then

2. based on these scores, let others    vote    on what i would like.

these    ltering approaches are not mutually exclusive. content information
can be built into a collaborative    ltering system to improve performance.

location-based cf methods (intuition)

location-based approaches embed users and objects into points in rd.

1koren, y., robert b., and volinsky, c..    id105 techniques for recommender systems.    computer 42.8 (2009): 30-37.

id105

id105

id105 (mf) gives a way
to learn user and object locations.

first, form the rating matrix m:

(cid:73) contains every user/object pair.
(cid:73) will have many missing values.
(cid:73) the goal is to    ll in these

missing values.

mf and id126s:
(cid:73) we have prediction of every

missing rating for user i.

(cid:73) recommend the highly rated
objects among the predictions.

n2 objectsn1 users{{(i,j)-th entry, mij, contains the rating for user i of object j singular value decomposition

our goal is to factorize the matrix m. we   ve discussed one method already.

singular value decomposition: every matrix m can be written as
m = usvt, where utu = i, vtv = i and s is diagonal with sii     0.

r = rank(m). when it   s small, m has fewer    degrees of freedom.   

collaborative    ltering with id105 is intuitively similar.

id105

we will de   ne a model for learning a low-rank factorization of m. it should:

1. account for the fact that most values in m are missing
2. be low-rank, where d (cid:28) min{n1, n2} (e.g., d     10)
3. learn a location ui     rd for user i and vj     rd for object j

n2 objectsn1 users{{(i,j)-th entry, mij, contains the rating for user i of object j ~~uivj{rank = dlow-rank id105

why learn a low-rank matrix?

(cid:73) we think that many columns should look similar. for example, movies

like caddyshack and animal house should have correlated ratings.

(cid:73) low-ranid116 that the n1-dimensional columns don   t       ll up    rn1.
(cid:73) since > 95% of values may be missing, a low-rank restriction gives

hope for    lling in missing data because it models correlations.

n2 objectsn1 users{{~~{rank = dcaddyshackuser ratingsanimal houseuser ratingscaddyshacklocationanimal houselocationprobabilistic matrix

factorization

some notation

    let the set     contain the pairs (i, j)
that are observed. in other words,
    = {(i, j) : mij is measured}.
so (i, j)         if user i rated object j.
    let    ui be the index set of objects

rated by user i.

    let    vj be the index set of users who

rated object j.

n2 objectsn1 users{{(i,j)-th entry, mij, contains the rating for user i of object j probabilistic id105

generative model
for n1 users and n2 objects, generate

user locations: ui     n(0,      1i),
vj     n(0,      1i),

object locations:

i = 1, . . . , n1

j = 1, . . . , n2

given these locations the distribution on the data is

mij     n(ut

i vj,   2),

for each (i, j)         .

comments:

(cid:73) since mij is a rating, the gaussian assumption is clearly wrong.
(cid:73) however, the gaussian is a convenient assumption. the algorithm will

be easy to implement, and the model works well.

model id136

q: there are many missing values in the matrix m. do we need some sort of

em algorithm to learn all the u   s and v   s?
(cid:73) let mo be the part of m that is observed and mm the missing part. then

(cid:90)

p(mo|u, v) =

p(mo, mm|u, v)dmm.

(cid:73) recall that em is a tool for maximizing p(mo|u, v) over u and v.
(cid:73) therefore, it is only needed when
1. p(mo|u, v) is hard to maximize,
2. p(mo, mm|u, v) is easy to work with, and
3. the posterior p(mm|mo, u, v) is known.

a: if p(mo|u, v) doesn   t present any problems for id136, then no.
(similar conclusion in our map scenario, maximizing p(mo, u, v).)

model id136

to test how hard it is to maximize p(mo, u, v) over u and v, we have to

1. write out the joint likelihood
2. take its natural logarithm
3. take derivatives with respect to ui and vj and see if we can solve

the joint likelihood of p(mo, u, v) can be factorized as follows:

(cid:104) (cid:89)
(cid:124)

(i,j)      

(cid:105)
(cid:125)

  (cid:104) n1(cid:89)
(cid:124)

i=1

p(mij|ui, vj)

(cid:123)(cid:122)

(cid:105)(cid:104) n2(cid:89)
(cid:123)(cid:122)

j=1

(cid:105)
(cid:125)

p(ui)

p(vj)

.

conditionally independent likelihood

independent priors

p(mo, u, v) =

by de   nition of the model, we can write out each of these distributions.

maximum a posteriori

log joint likelihood and map
the map solution for u and v is the maximum of the log joint likelihood

(cid:88)

(i,j)      

n1(cid:88)

n2(cid:88)

ln p(mij|ui, vj) +

ln p(ui) +

ln p(vj)

i=1

j=1

umap, vmap = arg max
u,v

calling the map objective function l, we want to maximize

l =     (cid:88)

(i,j)      

1

2  2(cid:107)mij     ut

i vj(cid:107)2    

(cid:107)ui(cid:107)2    

  
2

(cid:107)vj(cid:107)2 + constant

  
2

n1(cid:88)

i=1

n2(cid:88)

j=1

the squared terms appear because all distributions are gaussian.

maximum a posteriori

to update each ui and vj, we take the derivative of l and set to zero.

(cid:88)
(cid:88)

j      ui

i      vj

   uil =

   vjl =
(cid:16)
(cid:16)

ui =

vj =

    2i +(cid:80)
    2i +(cid:80)

1

  2 (mij     ut
  2 (mij     vt

1

i vj)vj       ui = 0

j ui)ui       vi = 0

(cid:17)   1(cid:16)(cid:80)
(cid:17)   1(cid:16)(cid:80)

(cid:17)
(cid:17)

j      ui

mijvj

i      vj

mijui

j      ui

i      vj

vjvt
j

uiut
i

we can solve for each ui and vj individually (therefore em isn   t required),

however, we can   t solve for all ui and vj at once to    nd the map solution.
thus, as with id116 and the gmm, we use a coordinate ascent algorithm.

probabilistic id105

map id136 coordinate ascent algorithm
input: an incomplete ratings matrix m, as indexed by the set    . rank d.
output: n1 user locations, ui     rd, and n2 object locations, vj     rd.
initialize each vj. for example, generate vj     n(0,      1i).

for each iteration do

(cid:73) for i = 1, . . . , n1 update user location

(cid:16)
    2i +(cid:80)
(cid:16)
    2i +(cid:80)

ui =

vj =

j      ui

vjvt
j

j      ui

(cid:17)   1(cid:16)(cid:80)
(cid:17)   1(cid:16)(cid:80)

(cid:17)
(cid:17)

mijvj

mijui

i      vj

uiut
i

i      vj

(cid:73) for j = 1, . . . , n2 update object location

predict that user i rates object j as ut

i vj rounded to closest rating option

algorithm output for movies

hard to show in r2, but we get locations for movies and users. their relative
locations captures relationships (that can be hard to explicitly decipher).

1koren, y., robert b., and volinsky, c..    id105 techniques for recommender systems.    computer 42.8 (2009): 30-37.

algorithm output for movies

returning to animal house (j) and caddyshack (j(cid:48)), it   s easy to understand
the relationship between their locations vj and vj(cid:48):

(cid:73) for these two movies to have similar rating patterns, their respective v   s

must be similar (i.e., close to each other in rd).

(cid:73) the same holds for users who have similar tastes across movies.

n2 objectsn1 users{{~~{rank = dcaddyshackuser ratingsanimal houseuser ratingscaddyshacklocationanimal houselocationid105 and

ridge regression

id105 and ridge regression

there is a close relationship between this algorithm and ridge regression.

(cid:73) think from the perspective of object location vj.
(cid:73) minimize the sum squared error 1
(cid:73) this is ridge regression for vj, as the update also shows:

  2 (mij     ut

i vj)2 with penalty   (cid:107)vj(cid:107)2.

(cid:16)
    2i +(cid:80)

vj =

(cid:17)   1(cid:16)(cid:80)

(cid:17)

i      vj

uiut
i

i      vj

mijui

(cid:73) so this model is a set of n1 + n2 coupled ridge regression problems.

n2 objectsn1 users{{~~vj{rank = d~~vjid105 and least squares

we can also connect it to least squares.

(cid:73) remove the gaussian priors on ui and vj. the update for, e.g., vj is then

(cid:16)(cid:80)

(cid:17)   1(cid:16)(cid:80)

vj =

i      vj

uiut
i

i      vj

mijui

(cid:17)

(cid:73) this is the least squares solution. it requires that every user has rated at

least d objects and every object is rated by at least d users.

(cid:73) this probably isn   t the case, so we see why a prior is necessary here.

n2 objectsn1 users{{~~vj{rank = d~~vj