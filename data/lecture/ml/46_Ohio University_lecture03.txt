machine learning

cs 4900/5900

lecture 03

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

machine learning is optimization

    parametric ml involves minimizing an objective function 

j(w):
    also called cost function, id168, or error function.

    (    )

    want to find     "=argmin
															    
1. start with some guess for w0, set      = 0.
increment      =      + 1.
2. update w     to w    +1 such that j(w    +1)     j(w    ).

    numerical optimization procedure:

3.
4. repeat from 2 until j cannot be improved anymore.

lecture 03

2

gradient-based optimization

    how to update w     to w    +1 such that j(w    +1)     j(w    )?

    move w in the direction of steepest descent:

	    /01	=	    /+	        
         is the learning rate and controls the magnitude of the change.

    g is the direction of steepest descent, i.e. direction along which j

decreases the most.

lecture 03

3

gradient-based optimization

	    /01	=	    /+	        
		    /01	=	    /               (    /)

    move w in the direction of steepest descent:

    what is the direction of steepest descent of j(w) at w    ?

    the gradient    j(w) is in the direction of steepest ascent.
    set g =       j(w) => the id119 update:

lecture 03

4

id119 algorithm

    want to minimize a function  j : rn    r.

    j  is differentiable and convex.
    compute gradient of j i.e. direction of steepest increase:

             =                 1,                 ;,   ,                 =
1. set learning rate      = 0.001 (or other small value).
2. start with some guess for w0, set      = 0.
     =      + 1.
    /01	=	    /                   /

3. repeat for epochs e or until j does not improve:
4.
5.

lecture 03

5

id119: large updates

lecture 03

6

id119: small updates

https://www.safaribooksonline.com/library/view/hands-on-machine-learning
7

lecture 03

the learning rate

1. set learning rate      = 0.001 (or other small value).
2. start with some guess for w0, set      = 0.

     =      + 1.
    /01	=	    /                   /

3. repeat for epochs e or until j does not improve:
4.
5.

   how big should the learning rate be?

o if learning rate too small => slow convergence.
o if learning rate too big => oscillating behavior => may not even 

converge.

lecture 03

8

learning rate too small

lecture 03

9

learning rate too large

lecture 03

10

the learning rate

    how big should the learning rate be?

    if learning rate too big => oscillating behavior.
    if learning rate too small => hinders convergence.

o use line search (backtracking line search, conjugate gradient,    ).
o use second order methods (newton   s method, l-bfgs, ...).

    requires computing or estimating the hessian. 

o use a simple learning rate annealing schedule:

    start with a relatively large value for the learning rate.
    decrease the learning rate as a function of the number of epochs or 

as a function of the improvement in the objective. 

o use adaptive learning rates:

    adagrad, adadelta, rmsprop, adam.
lecture 03

11

id119: nonconvex objective

saddle point

lecture 03

12

convex multivariate objective

w0

w1

lecture 03

13

gradient step and contour lines

w0

w1

lecture 03

14

id119: nonconvex objectives

lecture 03

15

id119 & plateaus

lecture 03

16

id119 & saddle points

lecture 03

17

id119 & ravines

lecture 03

18

id119 & ravines

    ravines are areas where the surface curves much more 

steeply in one dimension than another.
    common around local optima.
    gd oscillates across the slopes of the ravines, making slow progress 

towards the local optimum along the bottom.

    use momentum to help accelerate gd in the relevant 

directions and dampen oscillations:
    add a fraction of the past update vector to the current update vector.

    the momentum term increases for dimensions whose previous 

gradients point in the same direction.

    it reduces updates for dimensions whose gradients change sign.
    also reduces the risk of getting stuck in local minima.

lecture 03

19

id119 & momentum

vanilla id119:

		    /01	=	            (    /)
			    /01	=	    /   		    /01

id119 w/ momentum:

		    /01	=	        /+            (    /)
		    /01	=	    /   		    /01

     is usually set to 0.9 or similar.

lecture 03

20

momentum & nesterov accelerated gradient

gd with momentum:

		    /01	=	        /+            (    /)
		    /01	=	    /   		    /01

nesterov accelerated gradient:

		    /01	=	        /+            (    /   	        /)
		    /01	=	    /   		    /01

by making an anticipatory update, nags prevents gd from going too fast 
=> significant improvements when training id56s.

lecture 03

21

id119 optimization algorithms

    momentum.
    nesterov accelerated gradient (nag).
    adaptive learning rates methods:

    idea is to perform larger updates for infrequent params and smaller 

updates for frequent params, by accumulating previous gradient 
values for each parameter.
    adagrad.
    adadelta.
    rmsprop.
    adaptive moment estimation (adam)

lecture 03

22

visualization

    adagrad, rmsprop, adadelta, and adam are very similar 

algorithms that do well in similar circumstances.
    insofar, adam might be the best overall choice.

lecture 03

23

variants of id119

    /01	=	    /       	            /

    depeding on how much data is used to compute the 

gradient at each step:
    batch id119:

    use all the training examples.

    stochastic id119 (sgd).

    use one training example, update after each.

    minibatch id119.

    use a constant number of training examples (minibatch).

lecture 03

24

    sum-of-squares error: 

batch id119

         = 12    c	       (    (=))	   	         ;
h
=i1
    /01	=	    /       	            /
    /01	=	    /       1    c	h

=i1        (    (=))	   	        	    (=)

lecture 03

25

    sum-of-squares error: 

stochastic id119

         = 12    c	       (    (=))	   	         ;
= 12    c	        /,    (=)  
h
h
=i1
=i1
    /01	=	    /       	            /,    (=)
    /01	=	    /              (    (=))	   	        	    (=)

    update parameters w after each example, sequentially:

=> the least-mean-square (lms) algorithm.

lecture 03

26

batch gd vs. stochastic gd

    accuracy:

    time complexity:

    memory complexity:

    online learning:

lecture 03

27

batch gd vs. stochastic gd

lecture 03

28

pre-processing features

    features may have very different scales, e.g.  x1 = rooms 

vs. x2 = size in sq ft.
    right (different scales): gd goes first towards the bottom of the 

bowl, then slowly along an almost flat valley.

    left (scaled features): gd goes straight towards the minimum.

lecture 03

29

feature scaling

    scaling between [0, 1] or [   1, +1]:

    for each feature xj, compute minj and maxj over the training 

examples.
    scale x(n)

j as follows:

    for each feature xj, compute sample     j and sample     j over the 

    scaling to standard normal distribution:

training examples.

    scale x(n)

j as follows:

lecture 03

30

implementation: gradient checking

    want to minimize j(  ), where    is a scalar.

    mathematical definition of derivative:
j(  +  )     j(       )

d
d  

j(  ) = lim

        

2  

    numerical approximation of derivative:

d
d  

j(  )    

j(  +  )     j(       )

2  

where    = 0.0001

lecture 03

31

implementation: gradient checking

   

if    is a vector of parameters   i, 
    compute numerical derivative with respect to each   i.
    aggregate all derivatives into numerical gradient gnum(  ).

    compare numerical gradient gnum(  ) with implementation 

of gradient gimp(  ):

gnum(  )    gimp(  )
gnum(  ) +gimp(  )    10   6

lecture 03

32

id119 vs. normal equations

    need to select learning rate     .

    id119:

    may need many iterations:

    can do early stopping on validation data for id173.

    scalable when number of training examples n is large.

    normal equations:

    no iterations => easy to code.

    computing (xmx)-1 has cubic time complexity => slow for large n.
    xmx may be singular:

1. redundant (linearly dependent) features.
2.

#features > #examples => do feature selection or id173.

lecture 03

33

