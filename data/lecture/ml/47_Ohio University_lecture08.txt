machine learning: k-nearest neighbors

lecture 08

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

nonparametric methods: k-nearest neighbors

input:

    a training dataset (x1, t1), (x2, t2),     (xn, tn).
    a test instance x.

output:

   

estimated class label y(x).

1. find k instances x1, x2,    , xk nearest to x.
2. let 

xy
)(

arg

=

k

t
)(
i

max
tt
  

1
  
  
0
  

i

  
d
t
1
=
x
x

t
t

=
  
lecture 04

where                                  is the kronecker delta function.

=

xt
)(d

2

k-nearest neighbors (id92)

    euclidean distance, k = 4

lecture 04

3

k-nearest neighbors (id92)

    euclidian distance, k = 1.

voronoi diagram

decision boundary

lecture 04

4

id92 for classification: probabilistic 

justification 

    assume a dataset with nj points in class cj.
jn

   total number of points is

n

  =

j

    draw a sphere centered at x containing k points:

    sphere has volume v.
    sphere contains kj points from class cj.
if v sufficiently small and k sufficiently large, we can estimate [2.5.1]:

   

p

(x

|

c

j =)

k
j
vn
j

p

=)(x

    bayes    theorem   

cp
(

j

|

x

=)

k
nv
k
j
k

cp
(

j =)

j

n
n

   choose class cj with most neighbors.

lecture 04

5

distance metrics

    euclidean distance:
yx
-

yx
),(

=

d

=

2

(

yx

-

()
t

yx

-

)

    hamming distance:

# of (discrete) features that have different values in x and y.

    mahalanobis distance:
yx
t
)

yx
),(

d

=

-

(

(sample) covariance matrix

-s
1

(

yx

-

)

    scale-invariant metric that normalizes for variance.

    if s = i    euclidean distance.
    if s = diag(s1
-2 ,     sk

-2 , s2

-2)    normalized euclidean distance.

lecture 04

6

distance metrics

    cosine similarity:

d

yx
,(

1)
-=

cos(

yx
,

1)
-=

yx
t
yx

    used for text and other high-dimensional data.

    levenshtein distance (id153):

    distance metric on strings (sequences of symbols).
    min. # of basic edit operations that can transform one string into 

the other (delete, insert, substitute).

x =    athens   
y =    hints    

   d(x,y) = 4

    used in bioinformatics.

lecture 04

7

efficient indexing

    linear searching for k-nearest neighbors is not efficient for 

large training sets:
    o(n) time complexity.

    for euclidean distance use a kd-tree:

    instances stored at leaves of the tree.
    internal nodes branch on threshold test on individual features.
    expected time to find the nearest neighbor is o(log n)

   

indexing structures depend on distance function:
    inverted index for text retrieval with cosine similarity.

lecture 04

8

id92 and the curse of dimensionality

    standard metrics weigh each feature equally:

    problematic when many features are irrelevant.

    one solution is to weigh each feature differently:

    use measure indicating ability to discriminate between classes, 

such as:

    information gain, chi-square statistic
    pearson correlation, signal to noise ration, t test.

       stretch    the axes: 

    lengthen for relevant features, shorten for irrelevant features.
    equivalent with mahalanobis distance with diagonal covariance.

lecture 04

9

distance-weighted id92

for any test point x, weight each of the k neighbors according 
to their distance from x.

1. find k instances x1, x2,    , xk nearest to x.

2. let 

xy
)(

=

arg

max
tt
  

where

iw

=

xx
-

i

k

  

i

1
=
2-

tw
)(
d
i
i
t

measures the similarity between x and xi

lecture 04

10

kernel-based distance-weighted nn

for any test point x, weight all training instances according 
to their similarity with x.

1. assume binary classification, t = {+1, -1}.

2. compute weighted majority:

y

x
)(

=

sign

  
  
  

n

  

i

1
=

k

xx
,(

i t
)
i

  
  
  

lecture 04

11

regression with k-nearest neighbor

input:

    a training dataset (x1, t1), (x2, t2),     (xn, tn).
    a test instance x.

output:

   

estimated function value y(x).

1. find k instances x1, x2,    , xk nearest to x.
2. let 

xy
)(

=

k

1
k

it

  

i

1
=

lecture 04

12

3 datasets & linear interpolation

[http://www.autonlab.org/tutorials/mbl08.pdf]

linear interpolation does not always lead to good models of the data. 

lecture 04

13

regression with 1-nearest neighbor

lecture 04

14

regression with 1-nearest neighbor

lecture 04

15

regression with 1-nearest neighbor

  1-nn has high variance

lecture 04

16

regression with 9-nearest neighbor

k = 1

k = 9

lecture 04

17

regression with 9-nearest neighbor

k = 1

k = 9

lecture 04

18

regression with 9-nearest neighbor

k = 1

k = 9

lecture 04

19

distance-weighted id92 for regression

for any test point x, weight each of the k neighbors according 
to their similarity with x.

1. find k instances x1, x2,    , xk nearest to x.

2. let 

xy
)(

=

k

  

i

1
=

tw
ii

k

  

i

1
=

w
i

where

iw

=

xx
-

i

2-

for k = n    shepard   s method [shepard, acm    68].

lecture 04

20

kernel-based distance weighted nn 

regression 

for any test point x, weight all training instances according 
to their similarity with x.

1. return weighted average:
  
i
== n
1
  

x
)(

y

n

k

xx
,(

)

t
i

i

k

xx
,(

)

i

i

1
=

lecture 04

21

nn regression with gaussian kernel

2s2=10

2s2=20

2s2=80

k

xx
,(

)

i

=

e

2

-

xx
-
i
2
2
s

increased kernel width means more influence from 
distant points.  

lecture 04

22

nn regression with gaussian kernel

2s2=1/16 of x axis

2s2=1/32 of x axis

2s2=1/32 of x axis

k

xx
,(

)

i

=

e

2

-

xx
-
i
2
2
s

lecture 04

23

k-nearest neighbor summary

    training: memorize the training examples.
    testing: compute distance/similarity with training examples.
    trades decreased training time for increased test time.
    use kernel trick to work in implicit high dimensional space.
    needs feature selection when many irrelevant features.
    an instance-based learning (ibl) algorithm:

    memory-based learning
    lazy learning
    exemplar-based
    case-based

lecture 04

24

