machine learning: fisher   s linear 

discriminant

lecture 05

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

lecture 05

supervised learning

    task = learn an (unkown) function t : x    t that maps input 

instances x    x to output targets t(x)    t:
    classification:

    the output t(x)    t is one of a finite set of discrete categories.

    regression:

    the output t(x)    t is continuous, or has a continuous component.

    target function t(x) is known (only) through (noisy) set of 

training examples:

(x1,t1), (x2,t2),     (xn,tn)

lecture 05

three parametric approaches to 

classification

1) discriminant functions: construct f : x   t that directly 

assigns a vector x to a specific class ck.
   

id136 and decision combined into a single learning 
problem.

    linear discriminant: the decision surface is a 

hyperplane in x:
   
   
   

fisher    s linear discriminant
id88
support vector machines

lecture 03

3

three parametric approaches to 

classification

2) probabilistic discriminative models: directly model the 

id136 and decision are separate.

posterior class probabilities p(ck | x).
   
    less data needed to estimate p(ck | x) than p(x |ck).
    can accommodate many overlapping features.

    id28
    id49

lecture 03

4

three parametric approaches to 

classification

3) probabilistic generative models: 

    model class-conditional p(x |ck) as well as the priors 

p(ck), then use bayes   s theorem to find p(ck | x).
or model p(x,ck) directly, then marginalize to obtain the 
   
posterior probabilities p(ck | x).

id136 and decision are separate.

   
    can use p(x) for outlier or novelty detection.
    need to model dependencies between features.

    na  ve bayes.
    id48.

lecture 03

5

generative vs. discriminative

left-hand mode has no effect on posterior class probabilities.

lecture 03

6

linear discriminant functions: 

two classes (k = 2)
    use a linear function of the input vector:

y

wx
)(

=

t
j

x
)(

+

w
0

weight vector

bias =-threshold

    decision:

x    c1 if  y(x)    0, otherwise x    c2.

   decision boundary is hyperplane y(x) = 0.

    properties:

    w is orthogonal to vectors lying within the decision surface.
    w0 controls the location of the decision hyperplane.

lecture 03

7

linear discriminant functions:

two classes (k = 2)

lecture 03

8

linear discriminant functions:

multiple classes (k > 2)

1) train k or k-1 one-versus-the-rest classifiers.
2) train k(k-1)/2 one-versus-one classifiers.

3) train k linear functions:

k

y

wx
)(
=
    decision:

t
j
k

x
)(

+

w
k

0

x    ck if  yk(x) > yj(x), for all j    k.
   decision boundary between classes ck and cj is hyperplane defined 

by  yk(x) = yj(x) i.e. 

-
   same geometrical properties as in binary case.

ww

x
)(

w
k

j

t
)

+

-

(

(

0

k

j

w

j

0

0)
=

lecture 03

9

linear discriminant functions:

multiple classes (k > 2)

4) more general ranking approach:
   where

xw
t
j

t
),(

arg

x
)(

=

y

max
tt
  

t
   

=

cc
,{
1
2

...,

c
k

}

   

it subsumes the approach with k separate linear functions. 

    useful when t is very large (e.g. exponential in the size 
of input x), assuming id136 can be done efficiently.

lecture 03

10

linear discriminant functions:

two classes (k = 2)

    what algorithms can be used to learn y(x) = wtj(x) + w0?

assume a training dataset of n = n1 + n2 examples in c1 and c2.

    fisher   s linear discriminant
    id88:

    voted/averaged id88
    kernel id88

    support vector machines:

    linear
    kernel

lecture 03

11

fisher   s linear discriminant

    discriminant function y(x) = wtx + w0 can be interpreted 

as follows:
1. project d-dimensional x down to one dimension    wtx
2. use a threshold -w0 to classify x   

x  c1, if wtx    - w0
x  c2, otherwise.

   

fisher   s idea:
    maximize the between-class separation of projected dataset.
    minimize the within-class variance of projected dataset.

lecture 03

12

fisher   s linear discriminant

line joining the class means vs. line inferred with fisher   s criterion.

lecture 03

13

fisher   s linear discriminant

1) measure of the separation between the classes is the 

between class variance:

m
1

=

m2 =

1
n
1
1
n2

x

n

  

cn
  
1

xn

   
n   c2

mm
1

-

2

=

t

mmw
1

-

(

2

)

(

mm -
1

2

)

2

lecture 03

14

fisher   s linear discriminant

2) measure of the within-class variance: 

s1
2 =

s2
2 =

   
n   c1
   
n   c2

(wtxn     m1)2
(wtxn     m2)2

s +
2
1

s
2
2

lecture 03

15

fisher   s linear discriminant

    maximize the between-class separation and minimize the 

*

within-class variance    fisher   s criterion:
mm
-
2
1
s
s
2
2
+
1
2
    the objective function can be rewritten as:

, where

max
w

=w

arg

w

w

=

j

j

)

(

(

)

(

2

)

where
s
s

b

w

=
=

j

(

w

)

=

t

wsw
wsw
t

b

w

mmmm
(
-
-
2
1
2
  
mxmx
(
)
t
-
-
n
1

)(
)(

1

n

1

t

)

cn
  
1

(

+

  
cn
  
2
lecture 03

mxmx
n

)(

-

-

n

2

t

)

2

16

fisher   s linear discriminant

    optimization formulation:
arg

arg

w

w

=

=

j

)

(

*

max
w

max
w

t

wsw
wsw
t

b

w

    solution:
w
j
)
(
  
w
  

  =

0

(

  

s

wwww

s

=

(

t

b

s)

w

w

t

w

w
s
www
=
w

s)
b
t
s
s

w

b

b

t

w

s 

w

w

   

  

s   

w

s 
l=

w

w

b

   

if sw is nonsingular:
wss  
-1

  

b

generalized eigenvalue problem

w l=
w

lecture 03

conventional eigenvalue problem

17

fisher   s linear discriminant

    no need to solve the eigenvalue problem:

wmmmmws
b

)(

-

=

-

)

(

t

1

2

1

2

is a vector in the direction (m2     m1)

    the norm of w is immaterial, only its direction is important.

   can take

    how to find w0:

w

=

(s
-1
w

mm
1

-

2

)

    assume p(wtx|c1) and p(wtx|c2) are gaussians.
    estimate means and variances using maximum likelihood.
    use decision theory to find w0 i.e. p(-w0|c1) = p(-w0|c2)

lecture 03

18

supplementary reading

    prml section 1.4 (the curse of dimensionality).
    prml section 1.5 (decision theory).
    prml section 4 (linear models for classification):

    4.1.1 to 4.1.4.

lecture 03

19

