machine learning: the id88

lecture 06

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

lecture 06

1

mcculloch-pitts neuron function

activation / output

function

  

wixi   

f

hw(x)

w0
w1
w2
w3

1x0

x1

x2

x3

    algebraic interpretation:

    the output of the neuron is a linear combination of  inputs from other neurons, 

rescaled by the synaptic weights.

    weights wi correspond to the synaptic weights (activating or inhibiting).
    summation corresponds to combination of signals in the soma.

    it is often transformed through a monotonic activation function.

lecture 06

2

id180

unit step

f (z) =

id88

1

0 if z < 0
1 if z     0

"
$
#
%$

f (z) =

logistic

1
1+e   z
id28

f (z) = z

identity
id75

3

0

lecture 06

id88

  

wixi   

activation
function f

f (z) =

0 if z < 0
1 if z     0

"
$
#
%$

w0
w1
w2
w3

1x0

x1

x2

x3

    assume classes t = {c1, c2} = {1, 0}.
    training set is (x1,t1), (x2,t2),     (xn,tn).

x = [1, x1, x2, ..., xk]t
h(x) = step(wtx)

hw(x) 
=

!
#
"
$#

1 if wtx > 0
0
otherwise

lecture 06

4

id88 learning

    learning = finding the    right    parameters wt = [w0, w1,     , wk ]
    find w that minimizes an error function  e(w) which measures the 

misfit between h(xn,w) and tn.

    expect that h(x,w) performing well on training examples xn    h(x,w) 

will perform well on arbitrary test examples x    x.

    least squares error function?

e(w) =

1
2

n
   
n=1

{h(xn,w)    tn}2

# of mistakes

lecture 06

5

least squares vs. id88 criterion

    least squares => cost is  # of misclassified patterns:

    piecewise constant function of w with discontinuities.
    cannot find closed form solution for w that minimizes cost.
    cannot use gradient methods (gradient zero almost everywhere).

    id88 criterion:

    set labels to be +1 and     1. want wtxn > 0 for tn = 1, and wtxn < 0 

for tn =     1.
   would like to have wtxntn > 0 for all patterns.
   want to minimize    wtxntn for all missclassified patterns m.

   minimize     "     =      

           )    )

 )   -

lecture 06

6

stochastic id119

           )    )

 )   -

    id88 criterion:

minimize     "     =      
=    (/)+        )    )
    (/34)=    (/)+    )    )

    update parameters w sequentially after each mistake:

w(  +1) = w(  )         ep(w(  ),xn)

    the magnitude of w is inconsequential => set h= 1.

lecture 06

7

the id88 algorithm: two classes

1.
2.
3.
4.
5.

initialize parameters w = 0
for n = 1     n

hn = sgn(wtxn)
if hn    tn then

w = w + tnxn

repeat:
a)
b)

until convergence.
for a number of epochs e.

theorem [rosenblatt, 1962]:

if the training dataset is linearly separable, the id88 learning 
algorithm is guaranteed to find a solution in a finite number of steps.
   
see theorem 1 (block, novikoff)  in [freund & schapire, 1999].

lecture 06

8

averaged id88: two classes

initialize parameters w = 0, t = 1, 
for n = 1     n

0=w

1.
2.
3.
4.
5.
6.
7.
8.

hn = sgn(wtxn)
if hn    tn then

w = w + tnxn
=

www
+
t = t + 1
t/w

during testing: h(    )=            (    9       )

return

repeat:
a)
b)

until convergence.
for a number of epochs e.

lecture 06

9

the id88 algorithm: two classes

initialize parameters w = 0
for n = 1     n

1.
2.
3.
4.
5.

hn = sgn(wtxn)
if hn    tn then

w = w + tnxn

loop invariant: w is a weighted sum of training vectors:

    =:    )    )    )
)

           =:    )    )    )       

)

  

repeat:
a)
b)

until convergence.
for a number of epochs e.

lecture 06

10

 
 
kernel id88: two classes

         =           =:    )    )    )       =:    )    )    (    ),    )

1. define 

)

)

initialize dual parameters an = 0
for n = 1     n

2.
3.
4.
5.
6.

hn = sgn f(xn)
if hn    tn then
an = an + 1

during testing: h(x) = sgn f(x)

lecture 06

11

 
 
kernel id88: two classes

         =           =:    )    )       =:    )    (    ),    )

1. define 

)

)

initialize dual parameters an = 0
for n = 1     n

2.
3.
4.
5.
6.

hn = sgn f(xn)
if hn    tn then
an = an + tn

during testing: h(x) = sgn f(x)

lecture 06

12

 
 
the id88 vs. boolean functions

and
=xj
)(
=w
[

,

txx
]
1
2
twww
,
]
0

,1[
,

2

1

?

xor

or

=>

t

w j

x
)(

=

[

ww
1
2

,

[]
t

xx
,
1

2

]

+

w
0

lecture 06

13

id88 with quadratic kernel

    discriminant function: 
x
t
)

jja

x
)(

=

(

f

t
ii

i

  

i

x
)(

=

kt
a
ii

(

xx
),
i

  

i

    quadratic kernel:
yx
t

yx
),(

k

=

(

2

)

=

(

yx
11

+

yx
22

2

)

   corresponding feature space j(x) = ?

conjunctions of two atomic features

lecture 06

14

id88 with quadratic kernel

x

j(x)

d
1

a

c

b

1

c

2

1

b

1

a

linear kernel

k

yx
=),(

yx
t

d

quadratic kernel

k

yx
),(

=

(

yx
t

2)

lecture 06

15

quadratic kernels

    circles, hyperbolas, and ellipses as separating surfaces:

k

yx
),(

1(
+=

yx
t

)

2

x
)(
t
jj=
x
)(
=j

y
)(
xx
,2,2,1[
2
2
1

x
1

2,

txxx
]
21

2
2

,

x2

x1

lecture 06

16

quadratic kernels

k

yx
),(

=

(

yx
t

)

2

=

jj t

x
)(

y
)(

x

j(x)

lecture 06

17

explicit features vs. kernels

    explicitly enumerating features can be prohibitive:

    1,000 basic features for xty =>               quadratic features for (xty)2
    much worse for higher order features.

500,500

    solution:

    do not compute the feature vectors, compute kernels instead (i.e. 

compute dot products between implicit feature vectors).

    (xty)2 takes 1001 multiplications.
    j(x)t j(y) in feature space takes 500,500 multiplications.

lecture 06

18

id81s

    definition:

a function k : x    x    r is a id81 if there 
exists a feature mapping j: x    rn such that:

k(x,y) = j(x)tj(y)

    theorem:

k : x    x    r is a valid kernel    the gram matrix k 
whose elements are given by k(xn,xm) is positive 
semidefinite for all possible choices of the set {xn}.

lecture 06

19

techniques for constructing kernels

lecture 06

20

kernel examples

    linear kernel:

k

yx
=),(

yx
t

    quadratic kernel:

k

yx
),(

=

(

c

+

yx
t

2)

    contains constant, linear terms and terms of order two (c > 0).

    polynomial kernel:

yx
t
    contains all terms up to degree m (c > 0).

yx
),(

k

+

=

c

(

m

)

    gaussian kernel:

k

yx
),(

=

exp(

--

2 syx
2

2/

)

    corresponding feature space has infinite dimensionality.

lecture 06

21

kernels over discrete structures

    subsequence kernels [lodhi et al., jmlr 2002]:

    s is a finite alphabet (set of symbols).
    x,y  s* are two sequences of symbols with lengths |x| and |y|
    k(x,y) is defined as the number of common substrings of length n.
    k(x,y) can be computed in o(n|x||y|) time complexity.

    tree kernels [collins and duffy, nips 2001]:

    t1 and t2 are two trees with n1 and n2 nodes respectively.
    k(t1, t2) is defined as the nummber of common subtrees.
    k(t1, t2) can be computed in o(n1n2) time complexity.
    in practice, time is linear in the size of the trees.

lecture 06

22

supplementary reading

    prml chapter 6:

    section 6.1 on dual representations for id75 

models.

    section 6.2 on techniques for constructing new kernels.

lecture 06

23

the id88 algorithm: k classes

initialize parameters w = 0
for i = 1     n

arg

max
yi =
tt
  
if yi    ti then

t

xw j
(

ti
),

w = w + j(xi,ti) - j(xi,yi)

repeat:
a)
b)

until convergence.
for a number of epochs e.

1.
2.
3.
4.
5.

during testing:
t* = argmax
t   t

wt  (x,t)

lecture 06

24

averaged id88: k classes

initialize parameters w = 0, t = 1, 
for i = 1     n

0=w

1.
2.
3.
4.
5.
6.
7.
8.

arg

max
yi =
tt
  
if yi    ti then

t

xw j
(

ti
),

w = w + j(xi,ti) - j(xi,yi)
=

www
+
t = t + 1
t/w
t
*

=

return

during testing:

repeat:
a)
b)

until convergence.
for a number of epochs e.

arg

max
tt
  

t

xw j

t
),(

lecture 06

25

the id88 algorithm: k classes

initialize parameters w = 0
for i = 1     n

arg

max
cj =
tt
  
if cj    ti then

t

xw j
(

ti
),

w = w + j(xi,ti) - j(xi, cj)

repeat:
a)
b)

until convergence.
for a number of epochs e.

1.
2.
3.
4.
5.

loop invariant: w is a weighted sum of training vectors:

w =   ij(  (xi,
  

   
i, j
wt  (x,t) =   ij(  (xi,

 ti)      (xi,cj))
   
i, j

lecture 06

 ti)t  (x,t)      (xi,cj)t  (x,t))

26

kernel id88: k classes

1. define 

f (x,t) =   ij(  (xi,

 ti)t  (x,t)      (xi,cj)t  (x,t))

   
i, j

initialize dual parameters aij = 0
for i = 1     n

f

arg

max
cj =
tt
  
if yi    ti then
aij = aij + 1

(

x

i

t
),

repeat:
a)
b)

until convergence.
for a number of epochs e.

2.
3.
4.
5.
6.

during testing:

*

t

=

arg

max
tt
  

f

x
t
),(

lecture 06

27

kernel id88: k classes

    discriminant function:
f (x,t) =   i, j(  (xi,
   
i, j
=   ij(k(xi,
   
i, j

 ti)t  (x,t)      (xi,cj)t  (x,t))
 ti,x,t)     k(xi,cj,x,t))

where:
k
t
t
),(),
(
i
k(xi, yi,x,t) =  t(xi, yi)  (x,t)

t
j=

t
),

j

x

x

x

x

t
i

(

,

,

i

i

lecture 06

28

