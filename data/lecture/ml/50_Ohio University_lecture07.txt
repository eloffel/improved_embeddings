machine learning: support vector machines

lecture 07

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

max-margin classifiers: separable case

    linear model for binary classification:

y

xwx
)(
)(
t
j

=

+

b

    training examples:

(x1,t1), (x2,t2),     (xn,tn), where tn  {+1,-1}

    assume training data is linearly separable:

 , 0)
>

xyt
(
n
n

nn
   id88 solution depends on:

    

1 all

for 

    initial values of w and b.
    order of processing of data points.
lecture 07

2

maximum margin classifiers

    which hyperplane has the smallest generalization error?

    the one that maximizes the margin [computational learning theory]
    margin = the distance between the decision boundary and the 

closest sample.

lecture 07

3

maximum margin classifiers

    the distance between a point xn and a hyperplane y(x)=0 is:

)

n

y

x
(
w

=

)

n

=

x
yt
(
n
w

t

(

n

w

x
(
t
j
w

)

+

n

b

)

lecture 07

4

maximum margin classifiers

    margin = the distance between hyperplane y(x)=0 and closest sample:

min
n

t
n

(

xw
t
(
j
w

n

  
  
  

)

+

b

)

  
  
  

    find parameters w and b that maximize the margin:

arg
max
w
b
,

  
  
  

1
w

[
t
min
n
n

(

xw
t
(
j

)

b
+ )

n

  
]
  
  

    rescaling w and b does not change distances to the hyperplane:

w j
t
(
   for the closest point(s), set 
n
,1{
!  "
lecture 07

+xw j

,1)
  

     

  

b

(

)

(

(

t

t

t

n

n

n

1)
=

+ b

x
)
n
n
},

5

max-margin: quadratic optimization

    constrained optimization problem:

minimize:

1
2

2

w

w =b

(

j

),
subject to:
t

t

n

(

+xw j

)

(

n

b

,1)
  

     

!  "

,1{

n

n
},

    solved using the technique of lagrange multipliers.

lecture 07

6

id76

    id76 problem in standard form (primal):

solution x*

minimize:
)(0 xf
subject to:

fi
hi

  x
i
   ,0)(
=x
i
   ,0)(

,1
,
!=
,
,1
!=

m
p

    fi : rn  r are all convex functions, for i = 0,    , m
    hi : rn  r are all afine functions , for i = 0,    , p (e.g. hi(x)=ax+b) 

lecture 07

7

lagrange multipliers

    define lagrangian function lp : rn    rm    rp    r:
xh
)(
i

    x
,(
),

xf
)(
0

xf
)(
i

l
i

l
p

+

+

=

m

p

  
u
i

  

i

1
=

i

1
=

    li    0, and ni are the lagrange multipliers.

    define lagrange dual function ld : rm    rp    r:

l
d

    
),(

=

inf
x

l
p

    x
,(
),

lecture 07

8

id76

    lagrange dual problem:

solution (l*,n*)

maximize:

),(     dl

subject to:
i
   ,0

  l
i

,
!=

,1

m

l
d

    
),(

=

inf
x

l
p

    x
,(
),

lecture 07

9

strong duality

minimize:
)(0 xf
subject to:
  x
i
   ,0)(
=x
i
   ,0)(

fi
hi

,
,1
!=
,1
,
!=

m
p

maximize:

),(     dl

subject to:

  

  l
i

i
   ,0

,
!=

,1

m

solution x*

solution (l*,n*)

    optimum for primal problem = optimum for dual problem:

f

(0

x
*)

=

*)

    
dl
*,
(
lecture 07

10

karush   kuhn   tucker (kkt) conditions

2. dual constraints:

1. primal constraints:

assume (x, l, n) are the primal & dual solutions. then (x, l, n)
satisfy the following constraints:
  x
fi
i
  ,0)(
=x
i
hi
  ,0)(
i
,1
,
   ,0
!=
3. complementary slackness:
=xl
fi
i

m
4. gradient of lagrangian with respect to x vanishes:
xh
)(
=
i

m
,1
,
!=
p
,1
,
!=
m

i
   ,0)(

,
!=

xf
)(
0

xf
)(
i

l
  
p

l
  
i

  l
i

  =

x
)(

,1

+

+

  
  
u
i

  

m

p

i

1
=
lecture 07

i

1
=

0

11

max-margin: quadratic optimization

    constrained optimization problem:

minimize:

1
2

2

w

w =b

(

j

),
subject to:
t

t

n

(

+xw j

)

(

n

b

,1)
  

     

!  "

,1{

n

n
},

    let   s solve it using the technique of lagrange multipliers.

lecture 07

12

max-margin: quadratic optimization

    lagrangian function:
1
2

{
t
( 
n
    an    0 are the lagrangian multipliers.

  w
b
,

  

l
p

w

-

=

1
=

)

(

,

n

a w

n

n

2

t
(
j

x
n

)

+

b

}
1)
-

    lagrangian dual function:
,

  
)(

=

(

l
d

l
p

inf
w
b
,

  w
b
),

    solve:

pl
  
w
  
lp
  
b
  

0=

0=

  

n

=

t
nn

(jaw
n

  
n
1
=
=  
nnta

0

n

1
=

lecture 07

x

n

)

13

max-margin: quadratic optimization

    dual representation:

n

  

n

1
=

a
n

-

1
2

n

n

    

n

1
=

m

ktt
mnmn

aa
1
=

(

xx
,
n

m

)

l
d

maximize:
  
)(
=
subject to:
n
   ,0
  a
n
n
=  
nnta

0

n

1
=

,
!=

,1

n

    k(xn,xm)=j(xn)tj(xn) is the id81.

lecture 07

14

kkt conditions

1. primal constraints:

xyt
(
n
0  na
2. complementary slackness:

1. dual constraints:

01)
  -n

 tny(xn)    1

} = 0

{

  n

   for any data point, either an = 0 or tn y(xn) = 1

s = {n | tn y(xn) = 1} is the set of 

support vectors

lecture 07

15

max-margin solution

    after solving the dual problem    know an, for n = 1    n

n

  
n
1
=
1
s

|

jaw
(

=

t
nn

x

n

)

=

b

=

|

  

sn
  

  
  
  

t

n

-

  

sm
  

  

(
ja

t
mm

x

)

m

sm
  
xxa
kt
,
mm

(

n

)

m

  
  
  

    linear discriminant function becomes:

xy
)(

=   

sm
  

a

xxkt
,(
mm
m

)

+

b

   in both training and testing, examples are used only through 

the id81!

lecture 07

16

an id166 with gaussian kernel

lecture 07

17

max-margin classifiers: non-separable case

    allow data points to be on the wrong side of the margin boundary.

    penalty that increases with the distance from the boundary.

lecture 07

18

max-margin: quadratic optimization

    optimization problem:

=

1
2

2

w

+

c

n

  
x
n

n

1
=

(

j

w

minimize:
b
),
subject to:
jx
(
t

w

(

t

n

)

n

x
n

b
1)
+
-  
0  nx

     
,

!  "

,1{

n

n
},

    solve it using the technique of lagrange multipliers.

lecture 07

19

max-margin: quadratic optimization

    dual representation:

a
n

-

1
2

n

n

    

n

1
=

m

ktt
mnmn

aa
1
=

(

xx
,
n

m

)

n

n

1
=

l
d

  

maximize:
  
)(
=
subject to:
ncn
0
   ,
  a
  
n
=  
nnta

0

n

1
=

,
!=

,1

n

    k(xn,xm)=j(xn)tj(xn) is the id81.

lecture 07

20

(some of the) kkt conditions

1. primal constraints:

1. dual constraints:

1)
+-

x
n

  

0

xyt
(
n
n
cn   
  a0
  n

2. complementary slackness:

 tny(xn)    1+  n

} = 0

{

   for any data point, either an = 0 or tn y(xn) = 1-xn

s  = {n | tn y(xn) = 1-xn} is the set of 
m ={n | 0 < an< c} is the set of svs that lie on the margin.

support vectors

lecture 07

21

max-margin solution

    after solving the dual problem    know an, for n = 1    n

n

  
n
1
=
1
m

jaw
(

=

t
nn

x

n

)

=

b

=

|

  

mn
  

|

  
  
  

t
n

-

  

sm
  

  

(
ja

t
mm

x

m

)

sm
  
xxa
,

kt
mm

(

n

)

m

  
  
  

    linear discriminant function becomes:

xy
)(

=   

sm
  

a

xxkt
,(
mm
m

)

+

b

   in both training and testing, examples are used only through 

the id81!

lecture 07

22

support vector machines

    optimization problem:

1
2

2

w

+

c

n

  
  
x
n

n

1
=

=

(

j

w

minimize:
b
),
subject to:
jx
(
t

w

(

t

n

)

n

x
n

b
1)
+
-  
0  nx

     
,

!  "

,1{

n

n
},

upper bound on the missclassification error on the training data.

lecture 07

23

id166s for regression

    use an e-insensitive error function (e > 0) to obtain sparse solutions.

    penalty that increases with the distance from the e-insensitive    tube   .

lecture 07

24

id166s for regression: quadratic optimization

    optimization problem:

)  
xx  
+

(

n

2

n

n

(

j

+

c

w

1
2

= w

minimize:
b
),
subject to:
w
(x
b
)
t
j
+++
xe
n
  
xw
b
t
)
(
j
--+
xe
n
n
n
n
     
},
,0
,1{
!  "

t
n
t
n
  xx
n

  
  
  ,

1
=

n

n

n

    solve it using the technique of lagrange multipliers.

lecture 07

25

id166s for regression: sparse solution

    after solving the dual problem    know an,     for n = 1    n

n

n

n

)

1
=

=

-

=

w

x
n

  

()  
(
jaa
n

  
    s is the set of support vectors:
i.e. points for which either an    0 or 
   points that lie on the boundary of the e-insensitive tube or outside 

     na

sm
  

0

)

na  
()  
x
(
jaa
m
m

-

m

the tube
y

xwx)(

t +

=

b

=   

sm
  

,()  
xxk
( aa
m
m

-

m

)

+

b

   in both training and testing, examples are used only through 

the id81!

lecture 07

26

id166s for regression: sparse solution

lecture 07

27

id166s for ranking

[joachims, kdd   02]

    problem:

    for a query q, a search engine returns a set of documents d.
    want to rank di higher than dj if di is more relevant to q than dj.

    solution:

    learn a ranking function f(q,d) = wtj(q,d)
    rank di higher than dj if f(q,di)    f(q,dj)    wtj(q,di)    wtj(q,dj)
    training data:

    set {(qk, di, dj) | di ranked higher than dj for query qk}.
    relative rankings obtained from clicktrough data.

lecture 07

28

id166s for ranking

[joachims, kdd   02]

    optimization problem:

1
2

2

w

  +
c
x
jik
,,

(

j

w

minimize:
)
=
subject to:
w
t
(
j

i

)

dq
,
k
,,   jikx

   w
t
(
j
0

dq
,
k

i

1)
-+

x
jik
,,

t

w

(
(
j

dq
,
k

i

)

-

(
j

dq
,
k

i

)

)

1
-  

x
jik
,,

   equivalent with a classification problem

lecture 07

29

id166s for ranking

[joachims, kdd   02]

    after solving the quadratic problem:

(jaw

  =

lk
,

dq
,
k

l

)

lk
,
dqf
,(

)

  

tjw=
dq
,(
ja  =
t
(
lk  = a
(

lk
,

lk
,

,

lk
,

)
dq
,
k

l

dq
,()
j

)

dqdqk
,,

,

)

k

l

   in both training and testing, examples are used only 

through the id81!

lecture 07

30

learning scenarios for id166s

    classification.
    ranking.
    regression.
    ordinal regression.
    one class learning.
    learning with positive and unlabeled examples.
    transductive learning.
    semi-supervised learning.
    multiple instance learning.

lecture 07

31

practical issues

    data scaling:

    between [-1,+1] or [0, 1].
    use same scaling factors in training and testing!

    parameter tuning:

    most id166 packages specify reasonable default values.

    tuning helps, especially with kernels that tend to overfit.

    grid search is simple and effective:

    for rbf kernels, need to tune c and   :

    c    {2-5, 2-3, ..., 215},       {2-15, 2-13, ..., 23}

    read libid166   s    a practical guide to id166 classification   .

lecture 07

32

conclusion

    id166s were originally proposed by boser, guyon, and vapnik in 1992.
    id166s are currently among the best performers on a number of 

classification tasks ranging from text to genomic data.

    id166s can be applied to complex data types, e.g. graphs, trees, 

sequences, by designing id81s for such data.
    also to id203 distributions        learning from distributions via support 

measure machines    [muandet et al., nips 2012]

    kernel trick has been extended to other methods such as id88, 

pca, knn, etc.

    popular optimization algorithms for id166s use decomposition to hill-

climb over a subset of an   s at a time, e.g. smo [platt    99].
    but training and testing with linear id166s are much faster.

    read lin   s    machine learning software: design and practical use   

lecture 07

33

