machine learning: id28

lecture 04

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

lecture 04

supervised learning

    task = learn an (unkown) function t : x    t that maps input 

instances x    x to output targets t(x)    t:
    classification:

    the output t(x)    t is one of a finite set of discrete categories.

    regression:

    the output t(x)    t is continuous, or has a continuous component.

    target function t(x) is known (only) through (noisy) set of 

training examples:

(x1,t1), (x2,t2),     (xn,tn)

lecture 04

supervised learning

training

training examples

(xk, tk)

testing

test examples

(x, t)

learning 
algorithm

model h

generalization 
performance

model h

lecture 04

parametric approaches to supervised 

learning 

    task = build a function h(x) such that:

    h matches t well on the training data:

=> h is able to fit data that it has seen.

    h also matches t well on test data:

=> h is able to generalize to unseen data.

    task = choose h from a    nice    class of functions that 

depend on a vector of parameters w:
    h(x)    hw(x)    h(w,x)
    what classes of functions are    nice   ?

lecture 04

neurons

soma is the central part of the neuron:
    where the input signals are combined.

dendrites are cellular extensions:
    where majority of the input occurs.

axon is a fine, long projection:
    carries nerve signals to other neurons.

synapses are molecular structures between 
axon terminals and other neurons:
    where the communication takes place.

lecture 04

neuron models

https://www.research.ibm.com/software/ibmresearch/multimedia/ijid982013.neuron-
model.pdf

lecture 04

spiking/lif neuron function 
http://ee.princeton.edu/research/prucnal/sites/default/files/06497478.pdf

lecture 04

neuron models

https://www.research.ibm.com/software/ibmresearch/multimedia/ijid982013.neuron-
model.pdf

lecture 04

mcculloch-pitts neuron function

activation / output

function

  

wixi   

f

hw(x)

w0
w1
w2
w3

1x0

x1

x2

x3

    algebraic interpretation:

    the output of the neuron is a linear combination of  inputs from other neurons, 

rescaled by the synaptic weights.

    weights wi correspond to the synaptic weights (activating or inhibiting).
    summation corresponds to combination of signals in the soma.

    it is often transformed through an activation / output function.

lecture 04

id180

unit step

f (z) =

id88

1

0 if z < 0
1 if z     0

"
$
#
%$

f (z) =

logistic

1
1+e   z
id28

f (z) = z

identity
id75

0

lecture 04

id75

w0
w1
w2
w3

1x0

x1

x2

x3

activation / output

function

  

wixi   

f
f (z) = z

hw(x) =

wixi   

    polynomial curve fitting is id75:

x =   (x) = [1, x, x2, ..., xm]t
h(x) = wtx

lecture 04

mcculloch-pitts neuron function

activation / output

function

  

wixi   

f

hw(x)

w0
w1
w2
w3

1x0

x1

x2

x3

    algebraic interpretation:

    the output of the neuron is a linear combination of  inputs from other neurons, 

rescaled by the synaptic weights.

    weights wi correspond to the synaptic weights (activating or inhibiting).
    summation corresponds to combination of signals in the soma.

    it is often transformed through a monotonic activation / output function.

lecture 04

id28

w0
w1
w2
w3

1x0

x1

x2

x3

activation
function f

  

wixi   

f (z) =

1

1+exp(   z)

hw(x) 
=

1

1+exp(   wtx)

    training set is (x1,t1), (x2,t2),     (xn,tn).

x = [1, x1, x2, ..., xk]t
h(x) =   (wtx)

    can be used for both classification and regression:

    classification: t = {c1, c2} = {1, 0}.
    regression: t = [0, 1] (i.e. output needs to be normalized).

id28 for binary classification

    model output can be interpreted as posterior class 

probabilities:

p(c1 | x) =  (wtx) =

p(c2 | x) =1     (wtx) =

1

1+exp(   wtx))
exp(   wtx)
1+exp(   wtx)

    how do we train a id28 model?

    what error/cost function to minimize?

lecture 04

id28 learning

    learning = finding the    right    parameters wt = [w0, w1,     , wk ]
    find w that minimizes an error function  e(w) which measures the 

misfit between h(xn,w) and tn.

    expect that h(x,w) performing well on training examples xn   

h(x,w) will perform well on arbitrary test examples x    x.

    least squares error function?

e(w) =

1
2

{h(xn,w)    tn}2

n
   
n=1

    differentiable => can use id119    
    non-convex => not guaranteed to find the global optimum     

lecture 04

maximum likelihood

training set is d = {  xn, tn   | tn    {0,1}, n    1   n}
let

hn = p(c1 | xn)     hn = p(tn =1| xn) =  (wtxn)

maximum likelihood (ml) principle: find parameters that 
maximize the likelihood of the labels.

    the likelihood function is

p(t | w) =

n
   
n=1

tn(1   hn)(1   tn)
hn

    the negative log-likelihood (cross id178) error function:

e(w) =    ln p(t | x) =    

n
   
n=1

tn lnhn +(1   tn)ln(1   hn)
{
}
lecture 04

maximum likelihood learning

for id28

    the ml solution is:

wml = argmaxw p(t | w) = argminw e(w)
    ml solution is given by   e(w) = 0.

convex in w

    cannot solve analytically => solve numerically with gradient 

based methods: (stochastic) id119, conjugate gradient, 
l-bfgs, etc.

    gradient is (prove it):
   e(w) =

(hn    tn)xn
t

n
   
n=1

lecture 04

regularized id28

    use a gaussian prior over the parameters:

(

p

w

w = [w0, w1,     , wm]t
  =
  
  

i
1
-
a
    bayes    theorem:
wwt
|(

0
,(

  

p

=

)

)

p

(

tw
)|

=

p
()
t
)(

p

a
  
  
2
p
  

(

m

2/)1
+

exp

  -
  
  

a
2

t

ww

  
  
  

)

  

p

wwt
|(

p
()

)

    map solution:
arg

map =

w

max
w

p

(

tw
)|

lecture 04

regularized id28

    map solution:

p

wwt
|(

p
()

)

w

map =
=

=
=

=

arg
arg
arg
arg
arg

tw
p
max
arg
)|
(
max
=
w
w
wwt
p
p
min
ln
|(
()
)
-
w
wt
p
p
(
)
|(
ln
min
ln
-
-
w
w
w
ed
p
(
)
ln
(
)
min
-
w
a
w
ww
de
t
min
(
)
2
w

+

w

)

=

arg

min
w

ed

(

w
)

+

e
w

(

w
)

e

d

(w

)

-=

e

ww
(

)

=

ln

n

n

{
t

  
n
1
=
a
ww
2

t

y

n

1(
-+

t

n

)

1ln(

-

}
)

y

n

data term

id173 term

lecture 04

regularized id28

    map solution:

w

map

=

arg

min
w

ed

(

w
)

+

e
w

(

w
)

    ml solution is given by   e(w) = 0.

still convex in w

  e(w) =    ed(w) +   ew(w)

(hn    tn)xn
where hn =  (wtxn)
    cannot solve analytically => solve numerically:

t +  wt

=

n
   
n=1

    (stochastic) id119 [prml 3.1.3], newton raphson 

iterative optimization [prml 4.3.3], conjugate gradient, lbfgs.

lecture 04

softmax regression = id28

for multiclass classification

    multiclass classification:

t = {c1, c2, ..., ck} = {1, 2, ..., k}.

    training set is (x1,t1), (x2,t2),     (xn,tn).

x = [1, x1, x2, ..., xm]
t1, t2,     tn    {1, 2, ..., k}

    one weight vector per class [prml 4.3.4]:

p(ck | x) =

exp(wk
tx))
tx)
exp(w j
j   

lecture 04

softmax regression (k    2)

   

c
*

id136:
max
arg
=
c
k
= argmax
ck
= argmax
ck
= argmax
ck
    training using:

xk
cp
)
(
|
exp(wk
tx)
exp(w j
tx)
j   
tx)
exp(wk
wk
tx

z(x) a id172 
constant

    maximum likelihood (ml)
    maximum a posteriori (map) with a gaussian prior on w.

lecture 04

softmax regression

    the negative log-likelihood error function is:

ed(w) =    

=    

=    

1
n ln
1
n
   
n
n=1
1
n
   
n
n=1

n
p(tn | xn)
   
n=1
ln exp(wtn
t xn)
z(xn)
  k(tn)ln exp(wk
k
   
z(xn)
k=1

txn)

convex in w

where                                  is the kronecker delta function.

=

xt
)(d

1
  
  
0
  

x
x

=
  

t
t

lecture 04

softmax regression

    the ml solution is:
e
(
d

arg

w

=

ml

min
w

w
)

    the gradient is (prove it):

   wked(w) =    

=    

1
n
1
n

n
   
n=1
n
   
n=1

  k(tn)     p(ck | xn)
xn
(
)
txn)

exp(wk
z(xn)

   
xn
   
   
t ed(w),   ,     wk

   
  k(tn)    
   
   

lecture 04

   ed(w) =     w1

t ed(w),     w2

"
#

t
t ed(w)
$
%

regularized softmax regression

    the new cost function is:
e(w) = ed(w) + ew(w)

=    

1
n

n
   
n=1

  k(tn)ln exp(wk
k
   
z(xn)
k=1

txn)

+

    the new gradient is (prove it):

k

t

  

2 wk
    wk

k=1

   wke(w) =    

1
n

n
   
n=1

  k(tn)     p(ck | xn)
(

t

)xn

+  wk
t

lecture 04

softmax regression

    ml solution is given by   ed(w) = 0 .

    cannot solve analytically.
    solve numerically, by pluging [cost, gradient] = [ed(w),   ed(w)] 

values into general convex solvers:

    l-bfgs
    id77s
    conjugate gradient
    (stochastic / minibatch) gradient-based methods.
    id119 (with / without momentum).
    adagrad, adadelta
    rmsprop
    adam, ...

lecture 04

implementation 

    need to compute [cost, gradient]:

   cost

=    

1
n

n
   
n=1

   gradientk

=    

k
   
k=1
1
n

n
   
n=1

  k(tn)ln p(ck | xn)

+

  k(tn)     p(ck | xn)
(

k

  

t

2 wk
    wk
k=1
)xn

+  wk
t

t

=> need to compute, for k = 1, ..., k: 
exp(wk
txn))
txn)
exp(w j
j   

p(ck | xn) =

   output

lecture 04

overflow when wk
are too large.

txn

implementation: preventing overflows

    subtract from each product wk

txn the maximum product:

n

c = max1   k   k
p(ck | xn) =

wk
txn
exp(wk
exp(w j
j   

txn    c))
txn    c)

n

n

lecture 04

implementation: gradient checking

    want to minimize j(  ), where    is a scalar.

    mathematical definition of derivative:

                     =lim(   *        +            (           )

2    

    numerical approximation of derivative:

d
d  

j(  )    

j(  +  )     j(       )

2  

where    = 0.0001

lecture 04

implementation: gradient checking

   

if    is a vector of parameters   i, 
    compute numerical derivative with respect to each   i.

    create a vector v that is    in position i and 0 everywhere else:

    how do you do this without a for loop in numpy?

    compute gnum(  i) = (j(   +v)     j(       v)) / 2  

    aggregate all derivatives into numerical gradient gnum(  ).

    compare numerical gradient gnum(  ) with implementation 

of gradient gimp(  ):

gnum(  )    gimp(  )
gnum(  ) +gimp(  )    10   6

lecture 04

implementation: vectorization of lr

    version 1: compute gradient component-wise.

   e(w) =

(hn    tn)xn
t

n
   
n=1

    assume example xn is stored in column x[:,n] in data matrix x.

grad = np.zeros(k)
for n in range(n):

h = sigmoid(w.dot(x[:,n])
temp = h     t[n]
for k in range(k):
grad[k] = grad[k] + temp * x[k,n]

lecture 03

def sigmoid(x):

return 1 / (1 + np.exp(   x)) 

implementation: vectorization of lr

    version 2: compute gradient, partially vectorized.

   e(w) =

(hn    tn)xn
t

n
   
n=1

grad = np.zeros(k)
for n in range(n):

grad = grad + (sigmoid(w.dot(x[n]))     t[n]) * x[n]

def sigmoid(x):

return 1 / (1 + np.exp(   x)) 

lecture 03

implementation: vectorization of lr

    version 3: compute gradient, vectorized.

   e(w) =

(hn    tn)xn
t

n
   
n=1

grad = x.dot(sigmoid(w.dot(x))     t)

def sigmoid(x):

return 1 / (1 + np.exp(   x)) 

lecture 03

vectorization of softmax

    need to compute [cost, gradient]:

1
n

n
   
n=1

k
   
k=1
1
n

   cost

=    

  k(tn)ln p(ck | xn)

+

   gradientk

=> compute ground truth matrix g such that g[k,n] =     k(tn)

  k(tn)     p(ck | xn)
(

+  wk
t

n
   
n=1

=    

t

k

  

t

2 wk
    wk
k=1
)xn

from scipy.sparse import coo_matrix
groundtruth = coo_matrix((np.ones(n, dtype = np.uint8),

(labels, np.arange(n)))).toarray()
lecture 04

vectorization of softmax

  k(tn)ln p(ck | xn)

+

k

t

  

2 wk
    wk

k=1

    compute cost

=    

1
n

n
   
n=1

k
   
k=1

    compute matrix of     34    6.
    compute matrix of     34    6       6.
    compute matrix of exp	(    34    6       6).
    compute matrix of ln    (    3|    6).

    compute log-likelihood.

lecture 04

vectorization of softmax

    compute gradk

=    

1
n

n
   
n=1

  k(tn)     p(ck | xn)
(

t

)xn

+  wk
t

   gradient = [grad1 | grad2 |     | gradk]

    compute matrix of     (    3|    6).

    compute matrix of gradient of data term.

    compute matrix of gradient of id173 term.

lecture 04

vectorization of softmax

    useful numpy functions:

    np.dot()
    np.amax()
    np.argmax()
    np.exp()
    np.sum()
    np.log()
    np.mean()

lecture 04

import scipy

    scipy.sparse.coo_matrix()

groundtruth = coo_matrix((np.ones(numcases, dtype = np.uint8),

(labels, np.arange(numcases)))).toarray()

    scipy.optimize:

    scipy.optimize.fmin_l_bfgs_b()

theta, _, _ = fmin_l_bfgs_b(softmaxcost, theta,

args = (numclasses, inputsize, decay, images, labels),
maxiter = 100, disp = 1)

    scipy.optimize.fmin_cg()
    scipy.minimize
https://docs.scipy.org/doc/scipy-0.10.1/reference/tutorial/optimize.html

lecture 03

multiclass id28 (k    2)

1) train one weight vector per class [prml chapter 4.3.4]:

kcp
(

|

x

)

exp(

xw
t
(
))
j
k
xw
t
(
j
j

exp(

  =

j

))

2) more general approach:

cp
(
k

|

x

)

exp(

xw
c
t
,(
))
j
k
xw
c
t
,(
j

j

exp(

  =

j

- id136:
=

c
*

arg

max
c
k

cp
(

)

xk
|
lecture 07

))

39

id28 (k    2)

2)

id136 in more general approach:

c
*

=

arg

max
c
k

cp
(

)

xk
|
xw
c
t
,(
))
j
k
xw
c
t
,(
j

exp(

j

t

xw j
,(

c
k

))

exp(
  
j
exp(

t

w j

x
,(

c

k

)

=

arg

max
c

k

=

arg

=

arg

k

max
c
max
c

k

z(x) the partition 
function.

))

    training using:

    maximum likelihood (ml)
    maximum a posteriori (map) with a gaussian prior on w.

lecture 07

40

id28 (k    2) with ml

    the negative log-likelihood error function is:

e

d
w

(

w

)

-=

=

arg

ml

x

n

)

-=

n

|

ln

  
n
1
=
e
min
d
w

tp
(
n
w
)

(

n

  

n

1
=

exp(

ln

xw
t
(
j
x
z
(
)

n

,

t
n

))

n

convex in w

    the gradient is (prove it):
w
,)

e
  

w

=

)

(

d

  
  
  

e
(
  
d
w
  
0
  

(
j
i

n

n

1
=

x

n

,

,
!

w

,)

e
(
  
d
w
  
1
n
    

+

t
n

)

k

n

1
=

k

1
=

)

w

e
(
  
d
w
  
m

  
  
  

cp
(
k

|

x

)
(
j
i

x

n

,

c
k

)

n

)

w
e
(
  
d
w
  
i

-=

lecture 07

41

id28 (k    2) with ml

    set   ed(w) = 0    ml solution satisfies:

n

  

n

1
=

(
j
i

x

,

t

n

)

n

=

k

n

    

n

1
=

k

1
=

cp
(
k

|

x

(
)
j
i

x

n

,

c
k

)

n

   for every feature ji, the observed value on d should be the same as 

the expected value on d!

    solve numerically:

    stochastic id119 [chapter 3.1.3].
    newton raphson iterative optimization (large hessian!).
    limited memory id77s (e.g. l-bfgs).

lecture 07

42

the maximum id178 principle

    principle of insufficient reason
    principle of indifference

    can be traced back to pierre laplace and jacob bernoulli.

   a. l. berger, s. a. della pietra, and v. j. della pietra. 1996.
a maximum id178 approach to natural language processing.
computational linguistics, 22(1).
       model all that is known and assume nothing about that which is 

unknown   .

       given a collection of facts, choose a model consistent with all the 

facts, but otherwise as uniform as possible   .

lecture 07

43

maximum likelihood    maximum id178

1) maximize conditional likelihood:

w

ml =

arg

max
w

wt
p
|(
)
wt
p
)
|(

  
1
=
2) maximize conditional id178:

=

n

n

tp
(
w
n

|

x

n

)

=

n

  

n

1
=

exp(

xw
t
(
j
x
z
(
)

n

,

t
n

))

n

p

me

=

arg

max
p

subject to:

k

n

    

n

1
=

k

1
=

-

cp
(
k

|

x

n

log)

cp
(
k

|

x

n

)

n

  

n

1
=

(
j

x

,

t

n

)

n

=

k

n

    

n

1
=

k

1
=

cp
(
k

|

x

()
j

x

,

c
k

)

n

n

   solution is: 

p

(

t
n

|

x

n

)

=

me

t

(

x
p
|
w
lecture 07

ml

n

exp(

)

=

n

x
w
t
(
j
ml
x
z
(
)

n

,

t
n

))

n

44

