machine learning

cs 4900/5900

lecture 02

razvan c. bunescu

school of electrical engineering and computer science

bunescu@ohio.edu

supervised learning

    task = learn an (unknown) function t : x    t that maps 

input instances x    x to output targets t(x)    t:
    classification:

    the output t(x)    t is one of a finite set of discrete categories.

    regression:

    the output t(x)    t is continuous, or has a continuous 

component.

    target function t(x) is known (only) through (noisy) set of 

training examples:

(x1,t1), (x2,t2),     (xn,tn)

lecture 01

2

supervised learning 

    task = learn an (unknown) function t : x    t that maps 

input instances x    x to output targets t(x)    t:
    function t  is known (only) through (noisy) set of training examples:

    training/test data: (x1,t1), (x2,t2),     (xn,tn)

    task = build a function h(x) such that:

    h matches t well on the training data:

=> h is able to fit data that it has seen.
    h also matches target t well on test data:

=> h is able to generalize to unseen data.

lecture 01

parametric approaches to supervised 

learning 

    task = build a function h(x) such that:

    h matches t well on the training data:

=> h is able to fit data that it has seen.

    h also matches t well on test data:

=> h is able to generalize to unseen data.

    task = choose h from a    nice    class of functions that 

depend on a vector of parameters w:
    h(x)    hw(x)    h(w,x)
    what classes of functions are    nice   ?

lecture 01

id75

1. univariate id75

    house price prediction

2. id75 with polynomial features

polynomial curve fitting

   
    id173
    ridge regression

3. multivariate id75

    house price prediction
    normal equations

lecture 01

5

house price prediction

    given the floor size in square feet, predict the selling price:

    x is the size, t is the price
    need to learn a function h such that h(x)     t(x).

   

is this classification or regression?
    regression, because price is real valued.

    and there are many possible prices.

    univariate regression, because one input value.
    would a problem with only two labels t1 = 0.5 and t2 = 1.0 still be 

regression?

lecture 01

6

house prices in athens

50 houses, randomly selected from the 106 
houses or townhomes:
   
   

sold recently in athens, oh.
built 1990 or later.

lecture 01

7

house prices in athens

lecture 01

8

parametric approaches to supervised 

learning 

    task = build a function h(x) such that:

    h matches t well on the training data:

=> h is able to fit data that it has seen.

    h also matches t well on test data:

=> h is able to generalize to unseen data.

    task = choose h from a    nice    class of functions that 

depend on a vector of parameters w:
    h(x)    hw(x)    h(w,x)
    what classes of functions are    nice   ?

lecture 01

house prices in athens

lecture 01

10

house prices in athens

lecture 01

11

univariate id75

    use a linear function approximation:
    hw(x) = wtx = [w0, w1]t[1, x] = w1x+w0.
    w0 is the intercept (or the bias term).
    w1 controls the slope.

    learning = optimization:

    find w that obtains the best fit on the training data, i.e. find w

that minimizes the sum of square errors:

         = 12    1	       (    n)	   	         7
8
9:;
    "=argmin
								    

    (    )

lecture 01

12

univariate id75

    learning = finding the    right    parameters wt = [w0, w1]
    find w that minimizes an error function  e(w) = j(w) which 

measures the misfit between h(xn,w) and tn.

    expect that h(x,w) performing well on training examples xn   

h(x,w) will perform well on arbitrary test examples x   x.

    sum-of-squares error function:

         = 12    1	       (    n)	   	         7

8
9:;

inductive learning hyphotesis

lecture 01

13

minimizing sum-of-squares error

why squared?

    sum-of-squares error function:

    how do we find w* that minimizes e(w)?

         = 12    1	       (    n)	   	         7
8
9:;
    "=arg	min
																							         (    )
=1    98
    =    +    ;1    9
8
9:;
9:;

    least square solution is found by solving a system of 2 linear equations:

    =1    9
9:; +    ;1    97
8
8
9:;

=1    98
9:;

    9

lecture 01

14

polynomial basis functions

    q: what if the raw feature is insufficient for good 

performance?
    example: non-linear dependency between label and raw feature.

    a: engineer [cs 4900] /learn [cs 6890] higher-level features, 

as functions of the raw feature.

    polynomial curve fitting:

    add new features, as polynomials of the original feature.

lecture 01

15

regression: curve fitting

target t

    training: build a function h(x), based on (noisy) training 

examples (x1,t1), (x2,t2),     (xn,tn)

lecture 01

regression: curve fitting

learned h

target t

    training: build a function h(x), based on (noisy) training 

examples (x1,t1), (x2,t2),     (xn,tn)

lecture 01

regression: curve fitting

learned h

target t

    testing: for arbitrary (unseen) instance x    x , compute 

target output h(x); want it to be close to t(x).

lecture 01

regression: polynomial curve fitting

t =?

h(x) = h(x,w) = w0 + w1x + w2x2 +   + wmxm = wjx j

m
   
j=0

parameters

features

polynomial curve fitting

    parametric model:

h(x) = h(x,w) = w0 + w1x + w2x2 +   + wmxm = wjx j

m
   
j=0

    polynomial curve fitting is (multivariate) id75:

x = [1, x, x2, ..., xm]t
h(x) = h(x,w) = wtx

    "=arg	min
																							         (    )

    learning = minimize the sum-of-squares error function:

         = 12    1	       (    n)	   	         7

8
9:;

lecture 01

20

sum-of-squares error function

         = 12    1	       (    n)	   	         7

8
9:;

    how to find w* that minimizes e(w), i.e.
    solve    j(w) = 0.

w
*

=

arg

min
w

e

(

w

)

lecture 01

21

polynomial curve fitting

    least square solution is found by solving a set of m + 1 

linear equations:

a    =t

m

  

j

=

0

twa
ij
i

=

j

 where
 
, 

a
ij

=

n

  

n

1
=

x
i
+
n

j

t
and ,
 
i

=

n

  

n

1
=

xt
i
nn

    prove it.

lecture 01

polynomial curve fitting

    generalization = how well the parameterized h(x,w) 

performs on arbitrary (unseen) test instances x  x.

    generalization performance depends on the value of m.

lecture 01

0th order polynomial

lecture 01

24

1st order polynomial

lecture 01

25

3rd order polynomial

lecture 01

26

9th order polynomial

lecture 01

27

polynomial curve fitting

    model selection: choosing the order m of the polynomial.

    best generalization obtained with m = 3.
    m = 9 obtains poor generlization, even though it fits training 

examples perfectly:

    but m = 9 polynomials subsume m = 3 polynomials!

    overfitting    good performance on training examples, poor 

performance on test examples.

lecture 01

28

overfitting

    measure fit using the root-mean-square (rms) error:

erms(w) =

n   

(

wtxn    tn

)2

n

    use 100 random test examples, generated in the same way:

lecture 01

over-fitting and parameter values

lecture 01

30

overfitting vs. data set size

m =9

m =9

    more training data    less overfitting.
    what if we do not have more training data?

    use id173.

lecture 01

31

id173

    parameter norm penalties (term in the objective).
    limit parameter norm (constraint).
    dataset augmentation.
    dropout.
    ensembles.
    semi-supervised learning.
    early stopping.
    noise robustness.
    sparse representations.
    adversarial training.

lecture 01

id173

    penalize large parameter values:

         = 12    1	       (    n)	   	         7+    2      7

8
9:;

regularizer

w
*

=

arg

min
w

e

(

w

)

lecture 01

9th order polynomial with id173

lecture 01

34

9th order polynomial with id173

lecture 01

35

training & test error vs.

how do we find the optimal value of l?

lecture 01

36

model selection

    put aside an independent validation set.
    select parameters giving best performance on validation set.

ln

-  l

,40{

,35
-

,30
-

,25
-

,20
-

}15
-

validation

training

ln l
erms

-40
1.05

-35
0.30

-30
0.25

-25
0.27

-20
0.52

-15
0.55

lecture 01

37

model evaluation

    k-fold cross-validation

    randomly partition dataset in k equally sized subsets p1, p2,     pk
    for each fold i in {1, 2,    , k}:

    test on pi, train on p1           pi-1    pi+1           pk

    compute average error/accuracy across k folds.

4-fold cross validation

lecture 01

38

multivariate id75

    q: what if the raw feature is insufficient for good 

performance?
    example: house prices depend not only on floor size, but also 

number of bedrooms, age, location,    

    a: use multivariate id75.

lecture 01

39

multivariate id75

    polynomial curve fitting:

x = [1, x, x2, ..., xm]t
= [x0, x1,    , xm]t
h(x) = h(x,w) = wtx

    multivariate id75:

x = [x0, x1,    , xm]t
h(x) = h(x,w) = wtx

    training examples: (x(1),t1), (x(2),t2),     (x(n),tn)

lecture 01

40

    learning = minimize the sum-of-squares error function:

multivariate id75

         = 12    1	       (    (9))	   	         7

8
9:;

    "=arg	min
																							         (    )
8
1    c    (9)	   	         	    (9)=0
9:;

    = xcx f;xc    

    computing the gradient    j(w) and setting it to zero:

    solving for w yields

    prove it.

lecture 01

41

    solution is 

    x is the data matrix, or the design matrix:

normal equations

    = xcx f;xc    
=     =(;)	    ;(;)   	    j(;)
    =     ; c
    7 c          8 c
    =(7)	    ;(7)   	    j(7)
      
    =(8)	    ;(8)   	    j(8)

   

t = [t1, t2,    , tn]t is the vector of labels.

lecture 01

42

    multivariate id75 with l2 id173:

ridge regression
         = 12    1	       (    n)	   	         7+    2      7
8
9:;
    "=arg	min
																							         (    )
    =           +xcx f;xc    

    solution is
    prove it.

lecture 01

43

id173: ridge vs. lasso

    ridge regression:

         = 12    1	       (    n)	   	         7+    2
         = 12    1	       (    n)	   	         7+    2

8
9:;
8
9:;

j
1	    m7
m:;
j
1	    m
m:;

    lasso:

    if    is sufficiently large, some of the coefficients wj are driven to 0 

=> sparse model.

lecture 01

44

id173

    id173 alleviates overfitting when using models 

with high capacity (e.g. high degree polynomials):
    want high capacity because we do not know how complicated the 

data is.

    q: can we achieve high capacity when doing curve fitting 

without using high degree polynomials?

    a: use piecewise polynomial curves.

    example: cubic spline smoothing.

lecture 01

45

cubic spline smoothing

    cubic spline smoothing is a regularized version of cubic 

spline interpolation.
    cubic spline interpolation: given n points {(xi , yi)}, connect 

adjacent points using cubic functions si , requiring that the spline 
and its first and second derivative remain continuous at all points: 

    cubic spline smoothing: the spline s = {si} is allowed to deviate 

from the data points and has low curvature => constrained 
optimization problem with objective:

lecture 01

46

cubic spline smoothing

http://ace.cs.ohio.edu/~razvan/papers/icmla11.pdf

lecture 01

47

polynomial curve fitting (revisited)

t =?

xy
)(

=

xy
,(

w

)

=

xwxww
0
2

+

+

1

2

+

!

+

m

xw
m

=

m

  

j

=

0

j

xw
j

parameters

lecture 01

features

48

generalization: basis functions as features

    generally

where jj(x) are known as basis functions.

    typically j0(x) = 1, so that w0 acts as a bias.

   

in the simplest case, use linear basis functions : jd(x) = xd.

linear basis function models (1)

    polynomial basis functions:

    global behavior:

    a small change in x affect all basis 

functions.

linear basis function models (2)

    gaussian basis functions:

    local behavior:

    a small change in x only 

affects nearby basis functions.
      j and s control location and 

scale (width).

linear basis function models (3)

    sigmoidal basis functions:

where

    local behavior:

    a small change in x only affect 

nearby basis functions. 

      j and s control location and 

scale (slope).

lecture 01

53

