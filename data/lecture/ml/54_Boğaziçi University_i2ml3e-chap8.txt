lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 8: 

nonparametric 
methods 

nonparametric estimation 

3 

    parametric (single global model), semiparametric 

(small number of local models) 

    nonparametric: similar inputs have similar outputs 
    functions (pdf, discriminant, regression) change 

smoothly 

    keep the training data;   let the data speak for 

itself    

    given x, find a small number of closest training 

instances and interpolate from these 

    aka lazy/memory-based/case-based/instance-

based learning 

density estimation 

4 

    given the training set x={xt}t drawn iid from p(x) 
    divide data into bins of size h 

    histogram: 

 

    naive estimator: 

 

  or 

 

 

            nhxxxpt as bin same the in #                 nhhxxhxxpt2               #                                                                 otherwise if   012111uuwhxxwnhxpntt/  5 

6 

kernel estimator 

7 

    id81, e.g., gaussian kernel: 

 

 

 

    kernel estimator (parzen windows) 

                                          ntthxxknhxp11                                2212uukexp   8 

k-nearest neighbor estimator 

9 

    instead of fixing bin width h and counting the 

number of instances, fix the instances (neighbors) k 
and check bin width 

 

 

  dk(x), distance to kth closest instance to x 

            xndkxpk2     10 

multivariate data 

11 

    kernel density estimator 

 
 
 

  multivariate gaussian kernel 
 
 
 
  ellipsoid 

spheric 

                                          nttdhknhp11xxx                                                                                               uuuuu121222121221sstddkkexpexp//      nonparametric classification 

12 

    estimate p(x|ci) and use bayes    rule 
    kernel estimator 

 

 

 

 

    id92 estimator 

                              tinttdiiiiitinttdiirhknhcpcpgnncprhkhncp                                                                              1111xxxxxxx  |       |                                      kkpcpcpcpvnkcpiiiikiii         xxxxx    |  |     |  condensed nearest neighbor 

13 

    time/space complexity of id92 is o (n) 

    find a subset z of x that is small and is accurate in 

classifying x (hart, 1968) 

 

            zzxxz         ||'eecondensed nearest neighbor 

14 

    incremental algorithm: add instance if needed 

distance-based classification 

15 

    find a distance function d(xr,xs) such that  

 

if xrand xsbelong to the same class, distance is small 
and if they belong to different classes, distance is 
large  

    assume a parametric model and learn its 

parameters using data, e.g., 

learning a distance function 

16 

    the three-way relationship between distances, 

id84, and feature extraction. 

    m=ltl is dxd and l is kxd  

 
 
 

    similarity-based representation using similarity 

scores 

    large-margin nearest neighbor (chapter 13) 

euclidean distance (circle) is not suitable,  
mahalanobis distance using an m (ellipse) is suitable. 
after the data is projected along l, euclidean distance can be used. 
 

17 

outlier detection 

18 

    find outlier/novelty points 

    not a two-class problem because outliers are very 

few, of many types, and seldom labeled 

    instead, one-class classification problem: find 

instances that have low id203 

    in nonparametric case: find instances far away from 

other instances 

local outlier factor 

19 

nonparametric regression 

20 

    aka smoothing models 

    regressogram 

 

                                                   otherwise  withbin same the in is  ifwhere 0111xxxxbxxbrxxbxgttntttntt,,,  21 

22 

running mean/kernel smoother 

23 

    running mean smoother 

    kernel smoother 

 

 

 

 

 

 

 

    running line smoother 

 

 

 

 

where k( ) is gaussian 

    additive models (hastie 

and tibshirani,  1990)  

                                                                                                otherwise 1if where   0111uuwhxxwrhxxwxgntttntt                                                                           ntttntthxxkrhxxkxg11   24 

25 

26 

how to choose k or h ? 

27 

    when k or h is small, single instances matter; bias is 

small, variance is large (undersmoothing): high 
complexity 

    as k or h increases, we average over more instances 

and variance decreases but bias increases 
(oversmoothing): low complexity 

    cross-validation is used to finetune k or h. 

28 

