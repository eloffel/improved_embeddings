lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 6: 

dimensionality 
reduction 

why reduce dimensionality? 

3 

    reduces time complexity: less computation 

    reduces space complexity: fewer parameters 

    saves the cost of observing the feature 

    simpler models are more robust on small datasets 

    more interpretable; simpler explanation 

    data visualization (structure, groups, outliers, etc) if 

plotted in 2 or 3 dimensions 

feature selection vs extraction 

4 

    feature selection: choosing k<d important features, 

ignoring the remaining d     k 

 

 

subset selection algorithms 

    feature extraction: project the  

 

 

 

 

original xi , i =1,...,d dimensions to  
new k<d dimensions, zj , j =1,...,k 

subset selection 

5 

    there are 2d subsets of d features 
    forward search: add the best feature at each step 

    set of features f initially   . 
    at each iteration, find the best new feature 

j = argmini e ( f     xi )  

    add xj to f  if e ( f     xj ) < e ( f )  
 

    hill-climbing o(d2) algorithm 
    backward search: start with all features and remove 

 

one at a time, if possible. 

    floating search (add k, remove l) 

iris data: single feature 

6 

chosen 

iris data: add one more feature to f4 

chosen 

7 

principal components analysis 

8 

    find a low-dimensional space such that when x is 

projected there, information loss is minimized. 

    the projection of x on the direction of w is: z = wtx 
    find w such that var(z) is maximized 
 
 
 
 
  where var(x)= e[(x       )(x      )t] =     

var(z) = var(wtx) = e[(wtx     wt  )2]  
 
 
 

= e[(wtx     wt  )(wtx     wt  )] 
= e[wt(x       )(x       )tw] 
= wt e[(x       )(x      )t]w = wt     w  

 
 
 
 

    maximize var(z) subject to ||w||=1 
 
 

   w1 =   w1 that is, w1 is an eigenvector of     
choose the one with the largest eigenvalue for var(z) to be 

max 

    second principal component: max var(z2), s.t., 

||w2||=1 and orthogonal to w1 
 

 

 
    w2 =    w2 that is, w2 is another eigenvector of     

  and so on. 

9 

      111111         wwwwwtt   max            011222222               wwwwwwwttt      maxwhat pca does 

10 

 

 

 

z = wt(x     m) 

  where the columns of w are the eigenvectors of     

and m is sample mean 

  centers the data at the origin and rotates the axes 

how to choose k ? 

11 

    proportion of variance (pov) explained 

 

 

 

 
  when   i are sorted in descending order  
    typically, stop at pov>0.9 

    scree graph plots of pov vs k, stop at    elbow    

dkk                                                      212112 

13 

feature embedding 

14 

    when x is the nxd data matrix, 
xtx is the dxd matrix (covariance of features, if mean-

centered) 

xxt is the nxn matrix (pairwise similarities of instances) 
    pca uses the eigenvectors of xtx which are d-dim and can 

be used for projection 

    feature embedding uses the eigenvectors of xxt which are 

n-dim and which give directly the coordinates after 
projection 

    sometimes, we can define pairwise similarities (or distances) 

between instances, then we can use feature embedding 
without needing to represent instances as vectors. 

factor analysis 

15 

    find a small number of factors z, which when 

combined generate x : 
 

xi       i = vi1z1 + vi2z2 + ... + vikzk +   i  

 
 
  where zj, j =1,...,k are the latent factors with  
 
e[ zj ]=0, var(zj)=1, cov(zi ,, zj)=0, i     j ,  
    i are the noise sources  
 
  and vij are the factor loadings 

 

 

e[   i ]=   i, cov(  i ,   j) =0, i     j, cov(  i , zj) =0 , 

 

pca vs fa 

16 

    pca 

    fa   

from x to z     

z = wt(x       ) 

from z to x   

x        = vz +     

 

x 

 

z 

z 

x 

factor analysis 

17 

    in fa, factors zj are stretched, rotated and 

translated to generate x 

singular value decomposition and 
id105 

18 

    singular value decomposition: x=vawt 
  v is nxn and contains the eigenvectors of xxt 

  w is dxd and contains the eigenvectors of xtx 

  and a is nxd and contains singular values on its first 

k diagonal 

    x=u1a1v1

t+...+ukakvk

t where k is the rank of x 

id105 

19 

    id105: x=fg 

  f is nxk and g is kxd 

 

 

id45 

multidimensional scaling 

20 

    given pairwise distances between n points,  

 

 

dij, i,j =1,...,n 

  place on a low-dim map s.t. distances are preserved 

(by feature embedding) 

    z = g (x |    ) 

find    that min sammon stress   

                                                                  srsrsrsrsrsrsrsre,,|||2222xxxxxgxgxxxxzz         xmap of europe by mds 

21 

map from cia      the world  factbook:  http://www.cia.gov/ 

id156  

    find a low-dimensional 
space such that when x 
is projected, classes are 
well-separated.  

    find w that maximizes 

 

22 

                                             ttttttttttrmsrrmssmmj212112221221xwxww        between-class scatter: 

 

 

 

 

    within-class scatter: 

23 

                              2121211111112121ssssss                                                wwtttttttttttttttttssrrrms  where    wherewwmxmxwwwmxmxwxw                                    tbbtttttmm21212121221221mmmmwwwmmmmwmwmw                              ss  where  fisher   s linear discriminant 

24 

    find w that max 

 

 

    lda soln: 

 

    parametric soln: 

 

      211mmw            wcs            wwmmwwwwwwwttwtbtjsss221                                          ,~|  when       iicp      211nxwk>2 classes 

25 

    within-class scatter:  

 

 

    between-class scatter: 

 

 

    find w that max 

the largest eigenvectors of sw

-1sb; maximum rank of k-1 

            tititttiikiiwrmxmx                     sss      1                                    kiikitiiibkn111mmmmmm      s      wswwswwwtbtj   26 

pca vs lda 

27 

canonical correlation analysis 

28 

    x={xt,yt}t ; two sets of variables x and y x 
    we want to find two projections w and v st when x 
is projected along w and y is projected along v, the 
correlation is maximized: 

cca 

29 

    x and y may be two different views or modalities; 
e.g., image and word tags, and cca does a joint 
mapping 

isomap 

30 

    geodesic distance is the distance along the 

manifold that the data lies in, as opposed to the 
euclidean distance in the input space 

  

isomap  

31 

    instances r and s are connected in the graph if  

||xr-xs||<e or if xs is one of the k neighbors of xr  
the edge length is ||xr-xs|| 

    for two nodes r and s not connected, the distance is 

equal to the shortest path between them 

    once the nxn distance matrix is thus formed, use 

mds to find a lower-dimensional mapping 

32 

matlab source from http://web.mit.edu/cocosci/isomap/isomap.html 

-150-100-50050100150-150-100-50050100150optdigits after isomap (with neighborhood graph).0074625508719530478478591206187076919394921996432827146204637102252481730337791334342889847169401362locally linear embedding 

33 

1. given xr find its neighbors xs

(r) 

find wrs that minimize 

2.

 

 

 

3.

find the new coordinates zr that minimize 

2            rssrrsrxe)()|(xwxw2            rssrrsrzze)()|(wwz34 

lle on optdigits 

35 

matlab source from http://www.cs.toronto.edu/~roweis/lle/code.html 

-3.5-3-2.5-2-1.5-1-0.500.511.51111110074625508719530478478591206187076919394921996432827146204637102252481730337791334342889847169401362laplacian eigenmaps 

36 

    let r and s be two instances and brs is their similarity, we 

want to find zr and zs that  
 

 

     brs can be defined in terms of similarity in an original 

space: 0 if xr and xs are too far, otherwise 
 

 

    defines a graph laplacian, and feature embedding 

returns zr  

 

laplacian eigenmaps on iris 

37 

spectral id91 (chapter 7) 

