lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 7: 

id91 

semiparametric density estimation 

3 

    parametric: assume a single model for p (x | ci) 

(chapters 4 and 5) 

    semiparametric: p (x|ci) is a mixture of densities 
  multiple possible explanations/prototypes: 

  different handwriting styles, accents in speech 

    nonparametric: no model; data speaks for itself 

(chapter 8) 

mixture densities 

4 

 
where gi the components/groups/clusters,  
p ( gi ) mixture proportions (priors), 
 
p ( x | gi) component densities 
 
gaussian mixture where p(x|gi) ~ n (   i ,    i ) 

 
 

parameters    = {p ( gi ),   i ,    i }k

i=1  

  unlabeled sample x={xt}t (unsupervised learning) 

                           kiiigpgpp1|xxclasses vs. clusters  

5 

    supervised: x = {xt,rt }t  
    classes ci i=1,...,k 
 

 

where p(x|ci) ~ n(  i ,   i )  
       = {p (ci ),   i ,    i }k
i=1 
 

 

 

 

    unsupervised : x = { xt }t  
    clusters gi i=1,...,k 

 

 

 

  where p(x|gi) ~ n (   i ,    i )  
       = {p ( gi ),   i ,    i }k
 

i=1 

 

 

labels rt

i ? 

                           kiiigpgpp1|xx                           kiiippp1cc|xx                                                ttitittittiittitttiittiirrrrnrcpmxmxxms     id116 id91 

6 

    find k reference vectors (prototypes/codebook 
vectors/codewords) which best represent data 

    reference vectors, mj, j =1,...,k 
    use nearest (most similar) reference: 

 

 

    reconstruction error 

jtjitmxmx         min                                                      otherwiseminif 011jtjittitiittikiibbemxmxmxmxencoding/decoding 

7 

                           otherwise0minif 1jtjittibmxmxid116 id91 

8 

9 

expectation-maximization (em) 

10 

    log likelihood with a mixture model 

 
 
 
 

    assume hidden variables z, which when known, make 

optimization much simpler 

    complete likelihood, lc(   |x,z), in terms of x and z 
    incomplete likelihood, l(   |x), in terms of x  

                                                tkiiitttgpgpp1|log|log|xxxle- and m-steps 

11 

iterate the two steps 

1. e-step: estimate z given x and current    

2. m-step: find new       given z, x, and old   .  

 

 

 

 

  an increase in q increases incomplete likelihood  

                        lllcle                                 |maxarg:step-m|||:step-eqx,zx,lq1            xlxl||ll            1em in gaussian mixtures 

12 

    zt

i = 1 if xt belongs to gi, 0 otherwise (labels r t
supervised learning); assume p(x|gi)~n(  i,   i) 

i of 

    e-step:  

 

 

    m-step:  

 

 

 

use estimated labels in 
place of unknown labels 

                                    tiltijjljtilitltihgpgpgpgpgpze                        ,,,xxxx|||,                                                            ttitlittlittilittitttilittiihhhhnhp1111mxmxxms       gp(g1|x)=h1=0.5 

13 

mixtures of latent variable models 

14 

regularize clusters 
1. assume shared/diagonal covariance matrices 
2. use pca/fa to decrease dimensionality: mixtures 

of pca/fa 
 

 
  can use em to learn vi (ghahramani and hinton, 

1997; tipping and bishop, 1999) 

            itiiiitgp  mx      vv,|nafter id91 

15 

    id84 methods find correlations 

between features and group features 

    id91 methods find similarities between 

instances and group instances 

    allows knowledge extraction through  

  number of clusters, 
  prior probabilities,  

  cluster parameters, i.e., center, range of features. 

  example: crm, customer segmentation 

id91 as preprocessing 

16 

    estimated group labels hj (soft) or bj (hard) may be 

seen as the dimensions of a new k dimensional 
space, where we can then learn our discriminant or 
regressor. 

    local representation (only one bj is 1, all others are 

0; only few hj are nonzero) vs 

  distributed representation (after pca; all zj are 

nonzero) 

mixture of mixtures 

17 

    in classification, the input comes from a mixture of 

classes (supervised).  

    if each class is also a mixture, e.g., of gaussians, 

(unsupervised), we have a mixture of mixtures: 

                                                      kiiikjijijipppgpgppi11ccc|||xxxxspectral id91 

18 

    cluster using predefined pairwise similarities brs 

instead of using euclidean or mahalanobis distance 

    can be used even if instances not vectorially 

represented 

    steps: 

i.

ii.

use laplacian eigenmaps (chapter 6) to map to a 
new z space using brs 
use id116 in this new z space for id91 

hierarchical id91 

19 

    cluster based on similarities/distances 

    distance measure between instances xr and xs 

  minkowski (lp) (euclidean for p = 2) 
 

 

  city-block distance 

                  pdjpsjrjsrmxxd/,11            xx                  djsjrjsrcbxxd1xx,agglomerative id91 

20 

    start with n groups each with one instance and merge 

two closest groups at each iteration 

    distance between two groups gi and gj: 

    single-link:  
 

 

    complete-link: 

 

 
    average-link, centroid 
 

            srjidggdjsirxxxx,min,,gg                     srjidggdjsirxxxx,max,,gg                     srjidggdjsirxxxx,,,gg         aveexample: single-link id91 

21 

dendrogram 

choosing k 

22 

    defined by the application, e.g., image quantization 

    plot data (after pca) and check for clusters 

    incremental (leader-cluster) algorithm: add one at a 

time until    elbow    (reconstruction error/log 
likelihood/intergroup distances) 

    manually check for meaning 

 

