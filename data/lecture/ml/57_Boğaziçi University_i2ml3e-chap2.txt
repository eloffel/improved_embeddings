lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 2:  
supervised learning 

learning a class from examples 

3 

    class c of a    family car    

    prediction: is car x a family car? 

    knowledge extraction: what do people expect from a 

family car? 

    output:  

 

  positive (+) and negative (   ) examples 

    input representation:  

 

 

x1: price, x2 : engine power 

training set x 

4 

nttt,r1}{      xx            negative is  if positive is  if xx01r                     21xxxclass c 

5 

            2121  power  engine   and  price eepp            hypothesis class h 

6 

error of h on h 

            negative is  says  if positive is  says  if )(xxxhhh01                        ntttrhhe11x)|(xs, g, and the version space 

7 

most specific hypothesis, s 

most general hypothesis, g 

h       h, between s and g is 
consistent and make up the  
version space 
(mitchell, 1997) 

margin 

8 

    choose h with largest margin 

vc dimension 

9 

    n points can be labeled in 2n ways as +/    
    h shatters n if there  
  exists h     h consistent  
  for any of these:  
  vc(h ) = n 
 

 

an axis-aligned rectangle shatters 4 points only ! 

probably approximately correct (pac) 
learning 

10 

    how many training examples n should we have, such that with id203 

at least 1       , h has error at most    ? 

(blumer et al., 1989) 

 

 

    each strip is at most   /4 

    pr that we miss a strip 1      /4 

    pr that n instances miss a strip (1       /4)n 

    pr that n instances miss 4 strips 4(1       /4)n 

    4(1       /4)n        and (1     x)   exp(     x) 

    4exp(      n/4)         and n     (4/  )log(4/  ) 

noise and model complexity 

11 

use the simpler one because 
    simpler to use  

 

 

(lower computational  

complexity) 

    easier to train (lower  

 

space complexity) 

    easier to explain  

 

(more interpretable) 

    generalizes better (lower  

  variance - occam   s razor) 

multiple classes, ci i=1,...,k 

train hypotheses  
hi(x), i =1,...,k: 

12 

nttt,r1}{      xx                         , if   if  ijrjtitticcxx01                               ,  if   if ijhjtitticcxxx01regression 

13 

      01wxwxg            0122wxwxwxg                                       ntttxgrnge121x|                                 ntttwxwrnwwe1201011x|,                                 tttntttxfrrrx1,xmodel selection & generalization 

14 

    learning is an ill-posed problem; data is not 

sufficient to find a unique solution 

    the need for inductive bias, assumptions about h 
    generalization: how well a model performs on new 

data 

    overfitting: h more complex than c or f  
    underfitting: h less complex than c or f 

triple trade-off 

15 

   

there is a trade-off between three factors 
(dietterich, 2003): 

1.

2.

complexity of h, c (h), 
training set size, n,  

3. generalization error, e, on new data 

    as n         e    
    as c (h)         first e      and then e    

cross-validation 

16 

    to estimate generalization error, we need data 

unseen during training. we split the data as 

    training set (50%) 

    validation set (25%) 

    test (publication) set (25%) 

    resampling when there is few data 

dimensions of a supervised learner 

1. model:  

2.

 

 

 

 

id168: 

 

 

3. optimization procedure: 

   

   

   

 

17 

         |xg                        tttgrle      |,|xx      x|min arg*         e   