lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 13:  
kernel machines 

kernel machines 

3 

    discriminant-based: no need to estimate densities 

first 

    define the discriminant in terms of support vectors 

    the use of id81s, application-specific 

measures of similarity 

    no need to represent instances as vectors 

    id76 problems with a unique solution 

 

optimal separating hyperplane 

4 

(cortes and vapnik, 1995; vapnik, 1995) 

            1111111000021                                                                  wrrwrwwccrrtttttttttttttttxwxwxwwxxxas rewritten be can which for  for that such  and  find if if  where,xmargin 

5 

    distance from the discriminant  to the closest instances 

on either side 

    distance of x to the hyperplane is 

 

    we require 

 

    for a unique sol   n, fix   ||w||=1, and to max margin 

wxw0wtt         twrttt         ,   wxw0      twrttt            ,12102xww to subject  minmargin 

6 

7 

                        00021121121101110210202                                                                                                ntttpnttttpnttntttttntttttptttrwlrlwrwrltwr               xwwxwwxwwxww      to subject  min,most   t are 0 and only a small number have   t >0; they are 
the support vectors 

8 

                                                                                 t and  to subject          trrrrwrltttttsttsttsstttttttttttttttd,002121210                           xxwwxwwwsoft margin hyperplane 

9 

    not linearly separable 

 

 

    soft error 

 

 

    new primal is 

      ttttwxr            10w   tt                                             tttttttttttpwxrcl               12102ww10 

hinge loss 

11 

                  otherwise iftttttthingeryryryl110),(n-id166 

12 

n controls the fraction of  support vectors  

            n                              n                                                         tttttntssttststdtttttttnrxxrrlwrn,,,,100210021102t     to subject     to subject1   -  minxwwkernel trick 

13 

    preprocess input x by basis functions 

   

   

z =   (x) 

 

 

 

 

    the id166 solution  

g(z)=wtz   

g(x)=wt   (x) 

                                                                     ttttttttttttttttttkrgrgrrxxxx  x  x  wxx  zw,            vectorial kernels 

14 

    polynomials of degree q: 
 

 

 

 

 

 

 

            qtttk1      xxxx,                              ttxxxxxxyxyxyyxxyxyxyxyxk2221212122222121212122112221122221222111,,,,,,                                    xyxyx   vectorial kernels 

15 

    radial-basis  functions: 

                                       222skttxxxxexp,defining kernels 

16 

    kernel    engineering    

    defining good measures of similarity 

    string kernels, graph kernels, image kernels, ... 

    empirical kernel map: define a set of templates mi 

and score function s(x,mi) 
    (xt)=[s(xt,m1), s(xt,m2),..., s(xt,mm)] 

 

  and  
  k(x,xt)=    (x)t     (xt) 

multiple kernel learning 

17 

    fixed kernel combination 

 

 

    adaptive kernel combination 

 

 

 

 

    localized kernel combination 

                                                         yxyxyxyxyxyx,,,,,,2121kkkkckk                                                            itiittttsistiiststttdimiikrgkrrlkkxxxxxyxyx,)(,,,                     211                     itiitttkrgxxxx,|)(         multiclass kernel machines 

18 

    1-vs-all 

    pairwise separation 

    error-correcting output codes (section 17.5) 

    single multiclass optimization 

02210012                                       tittiittizttzittikiiziwwctt                to subject   min,,xwxwwid166 for regression 

19 

    use a linear model (possibly kernelized) 

     

f(x)=wtx+w0 

    use the   -sensitive  error function 

 

 

 

 

 

                                                      otherwiseif ,         ttttttfrfrfrexxx0            000                                       ttttttttrwwr                  ,xwxw                     tttc      221wmin20 

kernel regression 

21 

    polynomial kernel 

    gaussian kernel 

kernel machines for ranking 

22 

    we require not only that scores be correct order 

but at least +1 unit margin. 

    linear case: 

01212                     tivutvtutttiirrtc                 to subject   min,:,   xwxwwone-class kernel machines 

23 

    consider a sphere with center a and radius r 

                                                            tttntssttststtstttdtttttcxxrrxxlrar100122                        ,,     to subject     to subjectc   minx24 

large margin nearest neighbor 

25 

    learns the matrix m of mahalanobis metric 
  d(xi, xj)=(xi-xj)tm(xi-xj) 

    for three instances i, j, and l, where i and j are of 

the same class and l different, we require 

  d(xi, xl) > d(xi, xj)+1 

  and if this is not satisfied, we have a slack for the 
difference and we learn m to minimize the sum of 
such slacks over all i,j,l triples (j and l being one of k 
neighbors of i, over all i) 

learning a distance measure  

26 

    lmnn algorithm (weinberger and saul 2009) 

 

 

 

 

    lmca algorithm (torresani and lee 2007) uses  a 

similar approach where m=ltl and learns l 

kernel id84 

27 

    kernel pca does 

pca on the 
kernel matrix 
(equal to 
canonical pca 
with a linear 
kernel) 

    kernel lda, cca 

