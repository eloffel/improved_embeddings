lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 3: 
bayesian decision 
theory 

id203 and id136 

3 

    result of tossing a coin is       {heads,tails} 

    random var x    {1,0} 

 

 

bernoulli: p {x=1} = po

x (1     po)(1     x) 

    sample: x = {xt }n

 
t =1

  estimation: po = # {heads}/#{tosses} =    
    prediction of next toss: 

t xt / n 

 

 

heads if po >   , tails otherwise 

classification 

  output is low-risk vs high-risk 

    credit scoring: inputs are income and savings.  
 
    input: x = [x1,x2]t ,output: c    {0,1} 
    prediction:  

 

4 

                                             otherwise 0 )|()|( if 1 choose orotherwise 0 )|( if 1 choosecccc ,xxcp ,xxcp.  ,xxcp21212101501bayes    rule 

5 

posterior 

prior 

likelihood 

evidence 

                        xxxppppccc| |                                                         1100011110                                          xxxxx||||ccccccccpppppppppbayes    rule: k>2 classes 

6 

                                                            kkkkiiiiicpcpcpcppcpcpcp1||||xxxxx                        xx| max  | if   choose and 1kkiikiiicpcpccpcp               10losses and risks 

    actions:   i   
    loss of   i when the state is ck :   ik  
    expected risk (duda and hart, 1973) 

7 

                        xxxx|min|  if  choose||kkiikkkikirrcpr                           1losses and risks: 0/1 loss 

8 

for minimum risk, choose the most probable  class 

                  kikiik if  if 10                           xxxx||||iikkkkkikicpcpcpr                        11      losses and risks: reject 

9 

10110                                             otherwise     if     if    ,kikiik                              xxxxx|||||iikkikkkkcpcprcpr                              111                              otherwise         reject| and   || if    choose                  1xxxikiicpikcpcpcdifferent losses and reject 

10 

equal losses 

unequal losses 

with reject 

discriminant functions 

k decision regions r1,...,rk 

11 

      kigi,, ,   1   x            xxkkiiggcmax if  choose                     xxxkkiiggmax|      r                                                   iiiiicpcpcprg|||xxxx   k=2 classes 

    dichotomizer (k=2) vs polychotomizer (k>2) 
    g(x) = g1(x)     g2(x) 

 
 
 

    log odds:  

12 

                  otherwise  if  choose210cgcx            xx||log21cpcputility theory 

    prob of state k given exidence x: p (sk|x) 
    utility of   i when state is k: uik 
    expected utility: 

13 

                        xxxx| max| if   choose||jjiikkikieueu  spueu                  association rules 

    association rule: x     y 

    people who buy/click/visit/enjoy x are also likely to 

buy/click/visit/enjoy y. 

    a rule implies association, not necessarily causation. 

14 

association measures 

15 

    support (x     y):  

 

 

 

    confidence (x     y): 

 

 

    lift (x     y): 

 

                  customers and  bought  whocustomers##,yxyxp                           xyxxpyxpxyp bought  whocustomers and  bought  whocustomers|##)(,            )()|()()(,ypxypypxpyxp      example 

16 

apriori algorithm (agrawal et al., 
1996) 

17 

    for (x,y,z), a 3-item set, to be frequent (have 

enough support), (x,y), (x,z), and (y,z) should be 
frequent. 

    if (x,y) is not frequent, none of its supersets can be 

frequent. 

    once we find the frequent k-item sets, we convert 

them to rules: x, y     z, ... 

  and x     y, z, ... 

 

