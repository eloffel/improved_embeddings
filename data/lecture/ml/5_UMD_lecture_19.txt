slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 19 neural networks ii

furong huang / furongh@cs.umd.edu

neural networks

last time

what are neural networks? (key components)

how to make a prediction given an input?

why are neural networks powerful? 

neural networks

last time

what are neural networks?

multilayer id88

how to make a prediction given an input?

simple matrix operations + nonlinearities
why are neural networks powerful? 

universal function approximators!

today

how to train neural networks?

example: a neural network to solve the 
xor problem

  0(x1) = {-1, 1}
  2

x

  0(x2) = {1, 1}
o

  1(x3) = {-1, 1}

o

  1[1]

  1

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

  1[0]

o

  1(x2) = {1, -1}

x

  1(x1) = {-1, -1}
  1(x4) = {-1, -1}

  1[0]

  1[1]

1
1
-1

-1
-1
-1

example
   in new space, the examples are linearly separable!

  0(x1) = {-1, 1}
  0[1]

x

  0(x2) = {1, 1}
o

  0[0]

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

-1
-1
-1

  2[0] = y

1
1
-1

-1
-1
-1

  1[0]

  1[1]

  1(x3) = {-1, 1}

o

  1[1]

  1(x1) = {-1, -1} x
  1(x4) = {-1, -1}

  1[0]

o   1(x2) = {1, -1}

example

   the final net

  0[0]
  0[1]
1

  0[0]
  0[1]
1

1
1

-1

-1
-1

-1

tanh

  1[0]

-1

tanh

  2[0]

-1

tanh

  1[1]

1 -1

id180

    examples:

    sign: s"#$(&)=)   1,&<0
0,&=0
1,&>0
    tanh: tanh& =456475
458475=4956:
4958:
derivative: ;<=tanh& =1   tanh>(&)

   

id180

    sigmoid: !" = $$%&'(= &($%&( , logistic function
derivative: ))*!" = &($%&(=!" (1   !")

   

id180

    relu: !"#$% =%'=max(0,%) rectified 
    the derivative of softplus is logistic function /7% = 89:'89

/% =log(1+exp%)

a smooth approximation: softplus

linear unit

   

exercise

    learn softmax function 

https://en.wikipedia.org/wiki/softmax_fu
nction

forward propagation:
given input x, compute network output

neural network training

id26 algorithm 

=

id119 + chain rule 

recall: id119
for linear classifiers

objective function 

to minimize

number of 

steps

step size

what   s our training objective?

we   ll consider the following objective

i.e. our goal is to find parameters w, v that minimize 
squared error

other objectives are possible (e.g., other loss 
functions, add regularizer)

backprop in a 2-layer network

backprop in a 2-layer network

compute gradient g and g

what   s our training objective?

we   ll consider the following objective

i.e. our goal is to find parameters w, v that minimize 
squared error

other objectives are possible (e.g., other loss 
functions, add regularizer)

gradient of objective 
w.r.t. output layer weights v

!"   $!"

error at example n:

vector of activations 
of hidden units for 

example n

w.r.t. hidden unit weights !"

gradient of objective

chain rule

(this is on one example only)

backprop in a 2-layer network

forward 
propagation

update 
gradients

update 
parameters

tricky issues with
neural network training
sensitive to initialization

objective is non-convex, many local optima
in practice: start with random values rather than 
zeros

many other hyperparameters

number of hidden units (and potentially hidden 
layers)
id119 learning rate
stopping criterion

neural networks 
vs. linear classifiers

advantages of neural networks:
more expressive
less feature engineering

inconvenients of neural networks:
harder to train
harder to interpret

neural network architectures

we focused on a 2-layer feedforward
network

other architectures are possible
more than 2 layers (aka deep learning)
recurrent network (i.e. network has cycles)
can still be trained with id26

but more issues arise when networks get more complex 
(e.g., vanishing gradients)

try different architectures and training 
parameters here:

http://playground.tensorflow.org

what you should know

what are neural networks?

multilayer id88

how to make a prediction given an input?

forward propagation: simple matrix operations + nolinearities

why are neural networks powerful? 

universal function approximators!
how to train neural networks?

the id26 algorithm

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

