lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 5:  
multivariate methods 

multivariate data 

3 

    multiple measurements (sensors) 

    d inputs/features/attributes: d-variate  

    n instances/observations/examples 

                                             ndnnddxxxxxxxxx            212222111211xmultivariate parameters 

4 

                                                                                    2212222111221    dddddte                                       xxxcov                        jiijijjijiijtdxxxxe                                    ,,,...,corr :ncorrelatiocov:covariance:mean1  xparameter estimation 

5 

            jiijijjtjntitiijnttiisssrnmxmxsdinxm                              ::,...,,:rsmmatrix  ncorrelatiomatrix  covariance mean sample111estimation of missing values 

6 

    what to do if certain instances have missing 

attributes? 

    ignore those instances: not a good idea if the 

sample is small 

    use    missing    as an attribute: may give information 

    imputation: fill in the missing value 

    mean imputation: use the most likely value (e.g., mean) 

    imputation by regression: predict based on other 

attributes 

multivariate normal distribution 

7 

                                                                 x  xx  x1212  21  21  tddpexp~//   ,nmultivariate normal distribution 

8 

    mahalanobis distance: (x       )t       1 (x       )  

  measures the distance from x to    in terms of     (normalizes 

for difference in variances and correlations) 

    bivariate: d = 2 

 

                        22212121                                                iiiixzzzzzxxp                        /exp,                                          2221212221212121121bivariate normal 

9 

10 

independent inputs: naive bayes 

11 

    if xi are independent, offdiagonals of     are 0, 

mahalanobis distance reduces to weighted (by 1/  i) 
euclidean distance: 

 

 

 

 

    if variances are also equal, reduces to euclidean 

distance 

 

                                                                                             diiiidiiddiiixxpp121212121            exp/   xparametric classification 

    if p (x | ci ) ~ n (   i ,    i ) 

 

 

 

    discriminant functions 

12 

                                                         iitiidicp  x  xx1212  21  21exp|//                                       iiitiiiiicpdcpcpg log loglog log| log                                    21  21221xxxx   estimation of parameters 

13 

                                                ttitittittiittitttiittiirrrrnrcpmxmxxms                          iiitiiicpg   log log                     mxmxx12121ssdifferent si  

    quadratic discriminant 

14 

                        iiiitiiiiiiiitiitiiitiiititiicpwwcpg   log logwhere   log log                                                                  sssswwssss2121212212110110111mmmwxwxxmmmxxxxlikelihoods 

discriminant:  
p (c1|x ) = 0.5 

posterior for c1 

15 

common covariance matrix s 

16 

    shared common sample covariance s 

 

    discriminant reduces to 

 

 

which is a linear discriminant 

      iiicp  ss                              iitiicp  g log211                  mxmxxs            iitiiiiitiicpwwg   log   where                        mmmwxwx101021sscommon covariance matrix s 

17 

diagonal s  

18 

    when xj j = 1,..d, are independent,     is diagonal 
  p (x|ci) =    
 p (xj |ci) (naive bayes    assumption) 
j
 
 
 
 
  classify based on weighted euclidean distance (in sj 

units) to the nearest mean 

            idjjijtjicpsmxg   log                                          1221xdiagonal s 

19 

variances may be 
different 

diagonal s, equal variances 

20 

    nearest mean classifier: classify based on euclidean 

distance to the nearest mean 
 
 
 
 
 

    each mean can be considered a prototype or template 

and this is template matching 

                        idjijtjiiicpmxscpsg   log   log                              21222212mxxdiagonal s, equal variances 

21 

* ? 

model selection 

22 

assumption 

covariance matrix 

no of parameters 

shared, hyperspheric 

shared, axis-aligned 

shared, hyperellipsoidal 

different, hyperellipsoidal 

 
 
 
 
 

si=s=s2i 

si=s, with sij=0 

si=s 

si 

1 

d 

d(d+1)/2 

k d(d+1)/2 

    as we increase complexity (less restricted s), bias 

decreases and variance increases 

    assume simple models (allow some bias) to control 

variance (id173) 

23 

discrete features 

24 

    binary features: 

 

 

 

 

 

 

  

if xj are independent (naive bayes   ) 

the discriminant is linear 

estimated parameters 

                                          ijijjijjiiicppxpxcpcpg log log  log  log| log                        11xx      ijijcxpp|1                                       djxijxijijjppcxp111|         ttittitjijrrxp  discrete features 

25 

    multinomial (1-of-nj) features: xj    {v1, v2,..., vnj
 

} 

if xj are independent 

 

 

 

            ikjijkijkcvxpczpp||            1                                                      ttittitjkijkiijkjkjkidjnkzijkirrzpcppzgpcpjjk   log log |xx11multivariate regression 

26 

multivariate linear model 

 
 
 
 

multivariate polynomial model:  
 
 
 
 

define new higher-order variables  
2, z5=x1x2 
2, z4=x2
 
and use the linear model in this new z space  
(basis functions, kernel trick: chapter 13) 

z1=x1, z2=x2, z3=x1

 
 
 
 

               dttwwwxgr,...,,|10            2110102211021                              ttddttdtddttxwxwwrwwwexwxwxww      x|,...,,