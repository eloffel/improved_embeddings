lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 9: 

id90 

tree uses nodes and leaves 

3 

divide and conquer 

4 

    internal decision nodes 

    univariate: uses a single attribute, xi 
    numeric xi : binary split : xi  > wm 
    discrete xi : n-way split for n possible values 

    multivariate: uses all attributes, x 

    leaves 

    classification: class labels, or proportions 
    regression: numeric; r average, or local fit 

    learning is greedy; find the best split recursively 

(breiman et al, 1984; quinlan, 1986, 1993)  

classification trees ( ,cart,c4.5) 

5 

    for node m, nm instances reach m, ni

m belong to ci 

 

 
 

    node m is pure if pi

m is 0 or 1 
    measure of impurity is id178 

 

      mimiminnpmcp      ,|  x            kiimimmpp12logibest split 

6 

    if node m is pure, generate a leaf and stop, otherwise 

split and continue recursively 

    impurity after split: nmj of nm take branch j. ni

mj 

belong to ci 
 

 

    find the variable  and split that min impurity (among 

all variables  -- and split positions for numeric 
variables) 

      mjimjimjinnpjmcp      ,,|  x                  kiimjimjnjmmjmppnn121logi'7 

regression trees 

8 

    error at node m: 

 

 

 

 

    after splitting: 

                                                               ttmtttmmtmtmtmmmmbrbgbgrnembxxxxxx         otherwise node reaches :if 2101x                                                                  ttmjtttmjmjjtmjtmjtmmmjmjbrbgbgrnejmbxxxxxx        'otherwise  branch  and   node  reaches  :if 2101xmodel selection in trees 

9 

pruning trees 

10 

    remove subtrees for better generalization 

(decrease variance) 

    prepruning: early stopping 

    postpruning: grow the whole tree then prune subtrees that 

overfit on the pruning set 

    prepruning is faster, postpruning is more accurate 

(requires a separate pruning set) 

rule extraction from trees 

c4.5rules  
(quinlan, 1993) 

11 

learning rules 

12 

    rule induction is similar to tree induction but  

     

tree induction is breadth-first,  

     

rule induction is depth-first; one rule at a time 

    rule set contains rules; rules are conjunctions of terms 

    rule covers an example if all terms of the rule evaluate 

to true for the example 

    sequential covering: generate rules one at a time until 

all positive examples are covered 

    irep (f  rnkrantz and widmer, 1994), ripper (cohen, 

1995) 

13 

14 

multivariate trees 

15 

