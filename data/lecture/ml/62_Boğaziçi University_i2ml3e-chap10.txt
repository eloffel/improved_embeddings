lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 10:  
linear discrimination 

likelihood- vs. discriminant-based 
classification 

3 

    likelihood-based: assume a model for p(x|ci), use 

bayes    rule to calculate p(ci|x)  
 

gi(x) = log p(ci|x) 

 
    discriminant-based: assume a model for gi(x|  i); 

no density estimation 

    estimating the boundaries is enough; no need to 

accurately estimate the densities inside the 
boundaries 

linear discriminant 

4 

    linear discriminant: 

 

 

    advantages: 

    simple: o(d) space/computation  

    knowledge extraction: weighted sum of attributes; 

positive/negative weights, magnitudes (credit scoring) 

    optimal when p(x|ci) are gaussian with shared cov matrix; 

useful when classes are (almost) linearly separable 

      0100ijdjijitiiiiwxwwwg                  xwwx,|generalized linear model 

5 

    quadratic discriminant: 

 

 

    higher-order (product) terms: 

 

 

 

  map from x to z using nonlinear basis functions and use a linear 

discriminant in z-space 

2152242132211xxzxzxzxzxz                 ,  ,  ,  ,      00itiitiiiiwwg         xwxxwxww,,|                     kjjijiwg1xx   two classes 

6 

                                          020102120210121wwwwwgggtttt                                    xwxwwxwxwxxx                  otherwise if choose210cgcxgeometry 

7 

multiple classes 

8 

classes are 
linearly separable 

      00itiiiiwwg      xwwx,|            xxjkjiiggc1      maxif   choosepairwise separation 

9 

      00ijtijijijijwwg      xwwx,|                                    otherwisecare tdon' if ifjiijid35xxx00      0         xijigijc,if    choosefrom discriminants to posteriors  

10 

when p (x | ci ) ~ n (   i ,    ) 

            iitiiiiitiiiicpwwwg log   ,|                                  21  10100wxwwx                              otherwise  and  logif  choose|  and  |21210111501cyyyyycycpcpy                                       //.xx11 

                                                                                                                                                                                          001011211210211021212212111212212121111111        21        212    2121wwcpwcpcpwwcpcpcpcpcpcpcpcpcpcpcpttttttdtd                                                                                                                                    xwxwxxwxxwxwxxxxxxxxxxxexpsigmoid|||log logit of inverse the     wherelogexpexploglog||log||log||log|logit//////      sigmoid (logistic) function 

12 

                  5001010.                  ycwygcwgtt if  choose and sigmoid calculateor , if  choose and  calculatexwxxwxgradient-descent 

13 

 

 

    e(w|x) is error with parameters w on sample x 
 
 
    gradient 

w*=arg minw e(w | x) 

 
 

    gradient-descent:  

  starts from random w and updates w iteratively in the 

negative direction of gradient  

tdwwewewee                                          ,...,,21gradient-descent 

14 

e (wt) 

e (wt+1) 

wt  wt+1 

   
 

iiiiiwwwiwew                           ,   logistic discrimination 

15 

two classes: assume log likelihood ratio is linear 

                                                                                          01210002121111021111wcpycpcpwwwcpcpcpcpcpcpcpwcpcptotot                                             xwxxwxxxxxxwxxexp|log wherelog||log||log|logit||log  training: two classes 

16 

                                                                              tttttrttrttttttttyryrweleyywlwcpyyrrtt                                                      1111101001 log log|log|exp|bernoulli|   xxx,,~,wwxwxxxtraining: gradient-descent 

17 

                                                                                                                                                                        tttttjtttjtttttttjjtttttyrwewdjxyrxyyyryrwewyydadyyyryrwe               0001111111,...,,,   asigmoid if log log|xw18 

100 

1000 

10 

19 

k>2 classes 

20 

softmax 

                                                                                                                                                                  ttjtjjtttjtjjtittiiiitirtiiiikjjtjitiioitikitktttttyrwyryrweywlkiwwcpywcpcprti      000100011    log|,   |,,...,,expexp|  ||log,mult~|   ,xwwwxwxwxxwxxyxrxxxx21 

example 

22 

generalizing the linear model 

23 

    quadratic: 

 

 

    sum of basis functions: 

 

 

  where   (x) are basis functions. examples:  

    hidden units in neural networks (chapters 11 and 12) 

    kernels in id166 (chapter 13) 

            0itiitkiwcpcp         xwxxxxw||log                  0itikiwcpcp      xwxx   ||logdiscrimination by regression 

24 

    classes are not mutually exclusive and exhaustive 

                                                            tttttttttttttttttttyyyryrweyrwlwwyyrxwwwxwxw                                                                                    12122111020220002                     xxn|exp|expsigmoid  where,,,~learning to rank 

25 

    ranking: a different problem than classification or 

regression 

    let us say xu and xv are two instances, e.g., two 

movies  

  we prefer u to v implies that g(xu)>g(xv) 

  where g(x) is a score function, here linear: 

 

 

 

g(x)=wtx  

    find a direction w such that we get the desired 

ranks when instances are projected along w 

ranking error 

26 

    we prefer u to v implies that g(xu)>g(xv), so  
  error is g(xv)-g(xu), if g(xu)<g(xv)  

 

 

 

 

27 

