lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 15:  
id48 

introduction 

3 

    modeling dependencies in input; no longer iid 

    sequences:  

    temporal: in speech; phonemes in a word (dictionary), words 

in a sentence (syntax, semantics of the language).  

 

in handwriting, pen movements 

    spatial: in a dna sequence; base pairs 

discrete markov process 

4 

     p(qt+1=sj | qt=si, qt-1=sk ,...) = p(qt+1=sj | qt=si)  

    n states: s1, s2, ..., sn  state at    time    t, qt = si 
    first-order markov 
 
 
    transition probabilities 
 

     aij     p(qt+1=sj | qt=si)       aij     0 and   j=1
aij=1 

n 

 
    initial probabilities 
 

       i     p(q1=si)           j=1

n   i=1 

stochastic automaton 

5 

                  ttqqqqqttttaaqqpqp,qop1211211||                              aexample: balls and urns 

6 

    three urns each full of balls of one color 

  s1: red, s2: blue, s3: green 

                                          048080304050801010206020303040302050331311133131113311.....                                  |||,|,,,.........       .,.,.                                                                                    aaasspsspsspspopssssot   aaballs and urns: learning 

7 

    given k example sequences of length t 

                                                                                          kt-tiktkt-tjktiktijiijkikiisqsqsqsssaksqs111111111 and      from stransition to  from stransitionsequences   withstarting  sequences##  ##     id48 

8 

    states are not observable 
    discrete observations {v1,v2,...,vm} are recorded; a 

probabilistic function of the state 

    emission probabilities   
 
    example: in each urn, there are balls of different 

bj(m)     p(ot=vm | qt=sj) 

 

colors, but with different probabilities. 

    for each observation sequence, there are multiple 

state sequences 

id48 unfolded in time 

9 

elements of an id48 

10 

    n: number of states 

    m: number of observation symbols 

    a = [aij]: n by n state transition id203 matrix 
    b = bj(m): n by m observation id203 matrix 
       = [  i]: n by 1 initial state id203 vector 

 

     = (a, b,   ), parameter set of id48 

three basic problems of id48s 

11 

1. evaluation: given   , and o, calculate p (o |   ) 
2. state sequence: given   , and o, find q* such that  

  p (q* | o,    ) = maxq p (q | o ,    )  

 
3. learning: given x={ok}k, find   * such that  
 

  p ( x |   * )=max   p ( x |    ) 

 

(rabiner, 1989) 

evaluation 

12 

    forward variable: 

 

                                                                                                         nittjniijttiiitttiopobaijobisqoopi1111111                        |        :recursion        :tioninitializa|,   backward variable:  

 

13 

                                                               njttjijttittttjobaiisqoopi11111                       :recursion        :tioninitializa|,   finding the state sequence 

14 

choose the state that has the highest id203,  
for each time step: 
 

*= arg maxi   t(i) 

qt

no! 

                                                   njttttittjjiiosqpi1                  ,q1q2          qt-1 p(q1q2         qt-1,qt =si,o1         ot |   ) 

viterbi   s algorithm 

15 

    t(i)     max
 
    initialization:  
 
    recursion: 
 

 
1(i)aij 

    1(i) =   ibi(o1),   1(i) = 0 

   t(j) = maxi   t-1(i)aijbj(ot),   t(j) = argmaxi   t-

    termination: 
  p* = maxi   t(i), qt
 
    path backtracking: 

*= argmaxi   t (i) 

  qt

* =   t+1(qt+1

* ), t=t-1, t-2, ..., 1  

learning 

16 

                                                                                                                           otherwise and  if   otherwise if(em) algorithm welch-baum|0101111111jtittijittiklttlkltttjijttjtittsqsqzsqzlobakjobaijiosqsqpji:,,,,                     baum-welch (em) 

17 

                                                                              ,                 :stepm,     :stepe                                                                                          kkttktkkttmktktjkkttktkkttktijkkkittijttikkkkivojmbijiakijizeize111111111111111                        continuous observations 

18 

    discrete: 

 

 

    gaussian mixture (discretize using id116): 

 

 

    continuous: 

use em to learn parameters, e.g.,  

            2jjjttsqop         ,~,|n                                          otherwise  if     |011mttmrmmjjttvormbsqoptm   ,                        llljttlljljttsqoppsqop                  ,~                                                       ,,| ,|         ngg1                     tttttjjoj           id48 with input 

19 

    input-dependent observations: 

 
 

    input-dependent transitions (meila and jordan, 

1996; bengio and frasconi, 1996): 
 
 

    time-delay input: 

      titjtxsqsqp,|         1                  2jjtjtjttxgxsqop         ,,,|~|n         1         tttoo,...,   fxid48 as a graphical model 

20 

21 

model selection in id48 

22 

    left-to-right id48s: 

 

 

 

 

    in classification, for each ci, estimate p (o |   i) by a 

separate id48 and use bayes    rule 

 

                                       4434332423221312110000000aaaaaaaaaa                                    jjjiiipoppopop               |||