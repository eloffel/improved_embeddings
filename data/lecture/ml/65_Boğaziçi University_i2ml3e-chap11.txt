lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 11:  
multilayer id88s 

neural networks 

3 

    networks of processing units (neurons) with 

connections (synapses) between them 

    large number of neurons: 1010 
    large connectitivity: 105 

    parallel processing 

    distributed computation/memory 

    robust to noise, failures 

understanding the brain 

4 

    levels of analysis (marr, 1982) 

1. computational theory 

2. representation and algorithm 

3. hardware implementation 

    reverse engineering: from hardware to theory 

    parallel processing: simd vs mimd 

  neural net: simd with modifiable local memory 

 

learning: update by training/experience 

id88 

5 

(rosenblatt, 1962) 

            tdtdtdjjjxxwwwwxwy,...,,,...,,110011                     xwxwwhat a id88 does  

6 

    regression: y=wx+w0 

    classification:y=1(wx+w0>0) 

y 

w 

x 

w0 

x0=+1 

y 

x 

w0 

s 

y 

w 

x 

w0 

            xwtoy            exp sigmo regression: 

k outputs 

7 

classification: 

kkiikkiitiiyycooyomax  if chooseexpexp            xwxyxww                  tiidjjijiwxwy01training 

8 

    online (instances seen one by one) vs batch (whole 

sample) learning: 
    no need to store the whole sample 
    problem may change in time 
    wear and degradation in system components  

    stochastic gradient-descent: update after a single 

pattern 

    generic update rule (lms rule): 

            inpututactualoutpputdesiredoutctorlearningfaupdate                     tjtititijxyrw   training a id88: regression 

    regression (linear output): 

 

 

 

 

 

 

9 

                              tjtttjttttttttxyrwryrre                        222121xwxw,|classification 

10 

    single sigmoid output 

 

 

 

    k>2 softmax outputs 

 

                              tjtttjttttttttttxyrwyryrey                              11 log  log | sigmoidrxwxw,                  tjtititijititittiitkttkttitxyrwyrey                            log |     exp exprxwxwxw,learning boolean and 

11 

xor 

 

 

 

 

 

    no w0, w1, w2 satisfy: 

(minsky and papert, 1969) 

12 

000002101020                        wwwwwwwwmultilayer id88s 

13 

(rumelhart et al., 1986) 

                                                      djhjhjthhhhihihtiiwxwzvzvy101011exp sigmoidxwzv14 

x1 xor x2 = (x1 and ~x2) or (~x1 and x2) 

id26 

15 

                  hjhhiihjdjhjhjthhhhihihtiiwzzyyewewxwzvzvy                                                                         exp            sigmo 1011xwzvregression 

forward 

x 

16 

backward 

      xwthhz sigmoid               hhthhtvzvy10                        tjthththtttjthththtthjththttthjhjxzzvyrxzzvyrwzzyyewew                                                                        11                        221         tttyrex|,vw      thttthzyrv            regression with multiple outputs 

17 

yi 

whj 

vih 

zh 

xj 

                              tjththtiihtitihjthttitiihihhthihtitititixzzvyrwzyrvvzvyyre                                                                        121012      x|,vw18 

19 

whx+w0 

zh 

vhzh 

20 

two-class discrimination 

21 

    one sigmoid output yt for p(c1|xt) and  
  p(c2|xt)     1-yt 

 

                                    tjththhttthjthttthttttthhthhtxzzvyrwzyrvyryrevzvy                                                                           11110       log  log | sigmoidxv,wk>2 classes 

22 

                              tjththtiihtitihjthttitiihtitititiktktitihhithihtixzzvyrwzyrvyrecpooyvzvo                                                                                 110       log|| exp exp     xvx,wmultiple hidden layers 

23 

    mlp with one hidden layer is a universal 

approximator (hornik et al., 1989), but using 
multiple layers may lead to simpler networks 

                                                                                                               211022210212122110111111hlllthhlhlhtlldjhjhjthhvzvyhlwzwzhhwxwzzvzwxw,...,,,...,, sigmoid sigmoid sigmoid sigmoidimproving convergence 

24 

    momentum 

 

 

 

    adaptive learning rate 

1                        tiittiwwew                                 otherwise if         beeattoverfitting/overtraining 

25 

number of weights: h (d+1)+(h+1)k 

26 

structured mlp 

27 

    convolutional networks (deep learning) 

(le cun et al, 1989) 

weight sharing 

28 

hints 

29 

    invariance to translation, rotation, size 

 

 

 

    virtual examples 

(abu-mostafa, 1995) 

    augmented error: e   =e+  heh 
if x    and x are the    same   : eh=[g(x|  )- g(x   |  )]2 
approximation hint: 

                                                                                 xxxxxxhbxgbxgaxgaxgbaxge               |if ||if |,|if 220tuning the network size 

30 

    destructive 

  weight decay: 

 

 

    constructive 

  growing networks 

(ash, 1989) 

(fahlman and lebiere, 1989) 

                           iiiiiweewwew22         'bayesian learning 

31 

    consider weights wi as random vars, prior p(wi) 

 

 

 

 

 

 

 

    weight decay, ridge regression, id173 

 

 

cost=data-misfit +    complexity 

  more about bayesian methods in chapter 14 

                                                                  22212wwwwwwwwwww                                                            eewcwpwppcppppppppiiiimap')/(  exp    where log| log| log| log max arg   ||xxxxxxid84 

32 

autoencoder networks 

33 

learning time 

34 

    applications: 

    sequence recognition: id103 

    sequence reproduction: time-series prediction 

    sequence association 

    network architectures 

    time-delay networks (waibel et al., 1989) 

    recurrent networks (rumelhart et al., 1986) 

time-delay neural networks 

35 

recurrent networks 

36 

unfolding in time 

37 

deep networks 

38 

    layers of feature extraction units 

    can have local receptive fields as in convolution 

networks, or can be fully connected 

    can be trained layer by layer using an autoencoder 

in an unsupervised  manner 

    no need to craft the right features or the right basis 

functions or the right id84 method; 
learns multiple layers of abstraction  all by itself given 
a lot of data and a lot of computation 

    applications in vision, language  processing,  ... 

