lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 16:  
id114 

id114 

3 

    aka id110s, probabilistic networks 
    nodes are hypotheses (random vars) and the 

probabilities corresponds to our belief in the truth 
of the hypothesis 

    arcs are direct influences between hypotheses 
    the structure is represented as a directed acyclic 

graph (dag) 

    the parameters are the conditional probabilities in 

the arcs 
lauritzen, 1996) 

(pearl, 1988, 2000; jensen, 1996; 

causes and bayes    rule 

4 

causal 

diagnostic id136: 
knowing that the grass is wet,  
what is the id203 that rain is  
the cause? 

diagnostic 

                                                            750602040904090.......~|~||||                           rprwprprwprprwpwprprwpwrpconditional independence 

5 

    x and y are independent if  

 

 

 

 

p(x,y)=p(x)p(y) 

    x and y are conditionally independent given z if  

 

 

    or 

 

 

 

 

 

 

p(x,y|z)=p(x|z)p(y|z) 

p(x|y,z)=p(x|z) 

    three canonical cases: head-to-tail, tail-to-tail, 

head-to-head 

 

case 1: head-to-head 

6 

    p(x,y,z)=p(x)p(y|x)p(z|y) 

 
 
 
 
 
 
 

    p(w|c)=p(w|r)p(r|c)+p(w|~r)p(~r|c) 

case 2: tail-to-tail  

7 

    p(x,y,z)=p(x)p(y|x)p(z|x) 

case 3: head-to-head 

8 

    p(x,y,z)=p(x)p(y)p(z|x,y) 

causal vs diagnostic id136 

9 

causal id136: if the  
sprinkler is on, what is the  
id203 that the grass is wet? 
 
p(w|s) = p(w|r,s) p(r|s) +  
 
 = p(w|r,s) p(r) +  
 
 = 0.95 0.4 + 0.9 0.6 = 0.92 
 

p(w|~r,s) p(~r|s) 

p(w|~r,s) p(~r) 

 

diagnostic id136: if the grass is wet, what is the id203 
that the sprinkler is on?  p(s|w) = 0.35 > 0.2 p(s) 
p(s|r,w) = 0.21 explaining away: knowing that it has rained 
 

decreases the id203 that the sprinkler is on.  

causes 

10 

p(w|~r,s) p(~r,s|c) +  
p(w|r,~s) p(r,~s|c) +  
p(w|~r,~s) p(~r,~s|c) 

causal id136: 
p(w|c) = p(w|r,s) p(r,s|c) + 
 
 
 
 
and use the fact that 
  p(r,s|c) = p(r|c) p(s|c) 
 
 

diagnostic: p(c|w ) = ? 

exploiting the local structure 

11 

p (f | c) = ? 

                                    rfprswpcrpcspcpfwrscp||||,,,,,                              diiidxxpxxp11 parents|   ,classification 

12 

diagnostic 
 
p (c|x ) 

bayes    rule inverts the arc: 

                        xxxpcpcpcp||   naive bayes    classifier 

13 

given c, xj are independent: 
 
 

p(x|c) = p(x1|c) p(x2|c) ... p(xd|c)  

id75 

14 

wwwxwxwrwwxrwxwrxwwxxrxdprprpdppprpdprprpttt                     )(),|(),'|'()()(),|(),'|'(),|(),'|'(),,'|'(d-separation 

15 

    a path from node a to node b 

is blocked if 

a)

b)

the directions of edges on 
the path meet head-to-tail 
(case 1) or tail-to-tail (case 
2) and the node is in c, or 
the directions of edges meet 
head-to-head (case 3) and 
neither that node nor any of 
its descendants is in c. 

    if all paths are blocked, a 

and b are d-separated 
(conditionally independent) 
given c. 

bcdf is blocked given c.  
befg is blocked by f. 
befd is blocked unless f (or g) is 
given. 

belief propagation (pearl, 1988) 

16 

    chain: 

                                                )()()|()|()|,(||xxepxpxepxepepxpxeepepxpxepexp                                 )()|()()()|()(yxypxuuxpxyu                        trees 

17 

               xxzyxuxpxuxxxepx)|()()()()()|()(               )()()()()|()|()(xxxuuxpexpxzyuxx                                 polytrees 

18 

how can we model p(x|u1,u2,...,uk) cheaply? 

)()()()(),,,|()|()(xxxuuuuxpexpxjsyyuuukiixkxsjk                                                   12121                                 mjyirrxxukixxxuuuuxpxujir121)()()(),,,|()()(                     junction trees 

19 

    if x does not separate e+ and e-, we convert it into 

a junction tree and then apply the polytree 
algorithm 

tree of moralized, 
clique nodes 

undirected graphs: markov random 
fields 

20 

    in a markov random field, dependencies are 

symmetric, for example, pixels in an image 

    in an undirected graph, a and b are independent if 

removing c makes them unconnected. 

    potential function yc(xc) shows how favorable is the 

particular configuration x over the clique c  

    the joint is defined in terms of the clique potentials 

               xccccccxzxzxp)()()(yy normalizer  where1factor graphs 

21 

    define new factor nodes and write the joint in terms 

of them  

 )()(      sssxfzxp1learning a graphical model 

22 

    learning the conditional probabilities, either as 

tables (for discrete case with small number of 
parents), or as parametric functions 

    learning the structure of the graph: doing a state-
space search over a score function that uses both 
goodness of fit to data and some measure of 
complexity 

 

influence diagrams 

23 

decision node 

chance node 

utility node 

