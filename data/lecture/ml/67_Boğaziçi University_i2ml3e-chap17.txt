lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 17:  
combining multiple 
learners 

rationale 

3 

    no free lunch theorem: there is no algorithm that is 

always the most accurate 

    generate a group of base-learners which when 

combined has higher accuracy 
    different learners use different 

    algorithms 
    hyperparameters 
    representations /modalities/views 
    training sets 
    subproblems 

    diversity vs accuracy 
 

voting 

    linear combination 

 

 

 

 

 

    classification 

4 

         ljjijidwy11011                     ljjjljjjwwdwy  and      bayesian perspective: 

 

 

    if dj are iid  
 

 

 

  bias does not change, variance decreases by l 

    if dependent, error increase with positive correlation 

 

5 

                                    jjjjjjjjjjdldlldldlydedelldleyevarvarvarvarvar11111122                                                                                                                        jjiipxcpxcpjmmm ,|| models all                                                                                    jjjijijjjddcovdldly),(varvarvar21122fixed combination rules 

6 

error-correcting output codes 

7 

    k classes; l problems (dietterich and bakiri, 1995) 
    code matrix w codes classes in terms of learners 

 

    one per class 
 

l=k 

 
 
 

    pairwise 
  l=k(k-1)/2 

                                                                           110100101010011001000111w                                                                                       1111111111111111w    full code l=2(k-1)-1 

 
 
 
 

    with reasonable l, find w such that the hamming 

distance btw rows and columns are maximized. 

    voting scheme 

 
 

    subproblems may be more difficult than one-per-k 

8 

                                                                                                                           1111111111111111111111111111w         ljjijidwy1id112  

9 

    use id64 to generate l training sets and 
train one base-learner with each (breiman, 1996) 

    use voting (average or median with regression) 

    unstable algorithms profit from id112 

adaboost 

10 

generate a 
sequence of 
base-
learners 
each 
focusing on 
previous 
one   s errors 

(freund and 
schapire, 
1996) 

mixture of experts 

11 

voting where weights are input-dependent (gating) 
 
 
 
(jacobs et al., 1991) 
experts or gating  
can be nonlinear 
 

 

         ljjjdwy1stacking 

12 

    combiner f () is 
another learner 
(wolpert, 1992) 

fine-tuning an ensemble 

13 

    given an ensemble of dependent classifiers, do not 

use it as is, try to get independence 

1. subset selection: forward (growing)/backward 

(pruning) approaches to improve 
accuracy/diversity/independence 

2.

train metaclassifiers: from the output of correlated 
classifiers, extract new combinations that are 
uncorrelated. using pca, we get    eigenlearners.    

    similar to feature selection vs feature extraction 

cascading 

14 

use dj only if 
preceding ones are 
not confident 

 

cascade learners in 
order of complexity 

combining multiple sources/views 

15 

    early integration: concat all features and train a 

single learner 

    late integration: with each feature set, train one 
learner, then either use a fixed rule or stacking to 
combine decisions 

    intermediate integration: with each feature set, 

calculate a kernel, then use a single id166 with 
multiple kernels 

    combining features vs decisions vs kernels 

