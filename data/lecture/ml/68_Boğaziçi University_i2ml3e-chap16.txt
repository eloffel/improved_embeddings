lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 16:  
bayesian estimation 

rationale 

3 

    parameters q not constant, but random variables 

with a prior, p(q) 

 

    bayes    rule: 

 

 

            )()|(|            ppppqqqgenerative model 

4 

bayesian approach 

5 

1. prior p(q) allows us to concentrate on region where 
q is likely to lie, ignoring regions where it   s unlikely 

2.

instead of a single estimate with a single q, we 
generate several estimates using several q and 
average, weighted by how their probabilities 

even if prior p(q) is uninformative, (2) still helps. 

map estimator does not make use of (2): 

bayesian approach 

6 

 

 

     in certain cases, it is easy to integrate 
    conjugate prior: posterior has the same density as prior 

    sampling (id115): sample from 

the posterior and average 

    approximation: approximate the posterior with a 

model easier to integrate 
    laplace approximation: use a gaussian 
    variational approximation: split the multivariate density 

into a set of simpler densities using independencies  

estimating the parameters of a 
distribution: discrete case 

    xt

i=1 if in instance t is in state i, id203 of state i is qi  

    dirichlet prior, ai are hyperparameters 

 

    sample likelihood 

  

 

 

    posterior 

 

 

    dirichlet is a conjugate prior 

    with k=2, dirichlet reduced to beta 

 

 

7 

tixntkiiqxp               11)|(q                                       kiiikqdirichlet1110aaaa   )|(  q                  )|()|(n  q  q                                       dirichletqpkininnniikk11110aaaa   estimating the parameters of a 
distribution: continuous case 

8 

    p(xt)~n(m,s2) 
    gaussian prior for m, p(m)~ n(m0, s0
    posterior is also gaussian  p(m|x)~ n(mn, sn

2) 

2) 

where 

 

 

 

2202220200220211ssssssmsssmnmnnnn                  gaussian: prior on variance 

9 

    let   s define a prior (gamma) on precision l=1/s2 

joint prior and making a prediction 

10 

multivariate gaussian 

11 

estimating the parameters of a 
function: regression 

12 

    r=wtx+ e, p(e)~n(0,1/b), and p(rt|xt,w, b)~n(wtxt, 1/b) 
    log likelihood 

 

 

 

 

  ml solution 

    gaussian conjugate prior: p(w)~n(0,1/a) 

    posterior: p(w|x)~n(mn,sn    where 
 

 

aka ridge regression/parameter shrinkage/ 
l2 id173/weight decay 

                                    tttttttid56rplxwwxwxr22bb   bbloglog),,|(log),,|(rxxxwttml1      )(1            )(xxi  rx    tntnnbab13 

prior on noise variance 

14 

id115 (mcmc) sampling 

15 

basis/id81s 

16 

    for new x   , the estimate r    is calculated  as 

 

 

 

 

dual representation 

    linear kernel 

    for any other f(x), we can write k(x   ,x)=f(x   )tf(x) 
 

 

16 16 

            tttnttnttrrx  xrx  xx)'()'()'('bb         ttttttntrkrr),'()'('xxx  xbbid81s 

17 

what   s in a prior? 

18 

    defining a prior is subjective 

    uninformative prior if no prior preference 

    how high to go? 

 

 

    empirical bayes: use one good a* 

bayesian model comparison 

19 

    marginal likelihood of a model: 

 

    posterior id203 of model given data: 

 

    bayes    factor: 

 

    approximations: 
  bic: 

  aic: 

mixture model 

20 

models in increasing complexity.  
a complex model can fit more 
datasets but is spread thin,  
a simple model can fit few datasets 
but has higher marginal 
likelihood where it does  
(mackay 2003) 

21 

nonparametric bayes 

22 

    model complexity can increase with more data (in 

practice up to n, potentially to infinity) 

    similar to id92 and parzen windows we saw 

before where training set is the parameters 

gaussian processes 

23 

    nonparametric model for supervised learning 
    assume gaussian prior p(w)~n(0,1/a) 
  y=xw, where e[y]=0 and cov(y)=k with kij= (xi)txi 
  k is the covariance function, here linear 
    with basis function f(x), kij= (f(xi))tf(xi) 
 
    with new x    added as xn+1,  rn+1~nn+1(0,cn+1) 

r~nn(0,cn) where cn= (1/b)i+k 

 

 
 
 

  where k = [k(x   ,xt)t]t and c=k(x   ,x   )+1/b. 
  p(r   |x   ,x,r)~n(ktcn-1r,c-ktcn-1k) 

 
 
 

23 23 23 

                        id98kkcc124 

dirichlet processes 

25 

    nonparametric bayesian approach for id91 

    chinese restaurant process 

    customers arrive and either join one of the existing 

tables or start a new one, based on the table  

  occupancies: 

 

 

nonparametric gaussian mixture 

26 

    tables are gaussian components and decisions 

based both on prior and also on input x: 

id44 

27 

    bayesian feature extraction 

beta processes 

28 

    nonparametric bayesian approach for feature 

extraction 

    id105: 

 

 

 

    nonparametric version: allow j to increase with more 

data probabilistically  

    indian buffet process: customer can take one of the 
existing dishes with prob mj or add a new dish to the 
buffet 

