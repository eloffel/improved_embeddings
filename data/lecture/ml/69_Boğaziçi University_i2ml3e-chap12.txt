lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 12:  
local models 

introduction 

3 

    divide the input space into local regions and learn 

simple (constant/linear) models in each patch 
 
 
 

 

 

    unsupervised: competitive, online id91 
    supervised: radial-basis functions, mixture of 

experts 

competitive learning 

 
 
 
 
 
 
 

4 

                  ijtjtiijtijttitttiiltlittitiittikiimxbmemkbbkbbe                                                                         :means-  online :means-  batchotherwise min ifxmmxmxmxm011xwinner-take-all 
network 

5 

adaptive resonance theory 

    incremental; add a new cluster if 

not covered; defined by vigilance, 
   

(carpenter and grossberg, 1988) 

6 

                                                   otherwise ifminitiitkltklittibbmxmxmmxmx      11self-organizing maps 

7 

    units have a neighborhood defined; mi is    between    

mi-1 and mi+1, and are all updated together 

    one-dim map: 

(kohonen, 1990) 

                                                            22221            ilileileltlexp,,mxmradial-basis functions 

    locally-tuned units: 

8 

01wpwyhhthht                                             222hhtthspmxexplocal vs distributed representation 

9 

training rbf 

10 

    hybrid learning: 

    first layer centers and spreads:  

 

  unsupervised id116 

    second layer weights:  

  supervised gradient-descent 

    fully supervised 

 

(broomhead and lowe, 1988; moody and darken, 
1989) 

 

regression 

11 

                                          32201221hhtthtiihtitihhhjtjthtiihtitihjtthtitiihihhthihtitititihiihhhspwyrssmxpwyrmpyrwwpwyyrwsemxm                                                                                                                     x|,,,classification 

12 

                                                      khkthkhhithihtitititihiihhhwpwwpwyyrwse00expexp log |,,,xmrules and exceptions 

13 

exceptions 

default  
rule 

01vpwytthhthht               xvrule-based knowledge 

14 

 
 
 
 
 
 

    incorporation of prior knowledge (before training) 
    rule extraction (after training) (tresp et al., 1997) 
    fuzzy membership functions and fuzzy rules 

                                          1021022102232321222221211321...                                                                                                   wscxpwsbxsaxpycxbxax  withexp  withexpexp then  or   and ifnormalized basis functions 

15 

                                    212222122hhjtjtithtiihtitihjthttitiihhhthihtilllthhthltlththsmxgywyrmgyrwgwyssppg                                                                           /exp/expmxmxcompetitive basis functions 

16 

    mixture model: 

                           hhttttthphpp1xrxxr,|||                                                                  llltlhhththltttsasaglplphphphp222222/exp/exp|||mxmxxxxregression 

17 

                                          2221         titiittyrpexp|xr                                                                                                                                                                                                      llitiltitlitihtithththhjtjththhjtthtihtiihihtihthitihtithhiihhhlplphphphpyrgyrgid122xgfmfyrwwyyrgwsxrxxrxxrm,,,//,,,|||||expexpfit constant the is                                           explog|     2222212121      xlclassification 

18 

                                                                                          litiltitlitihtiththkkhihtihthitihtiththirtihthhiihhhyrgyrgfwwyyrgygwsti log exp log expexp exp                  log  exp log                               log|,,,xlmem for rbf (supervised em) 

19 

    e-step: 

 

    m-step: 

      tthhpfxr,|                                                tthttithihtththtthtthhtthttthhfrfwffsffmxmxxmlearning vector quantization 

20 

    h units per class prelabeled (kohonen, 1990) 

    given x, mi is the closest: 

x 

mi 

mj 

                                             otherwise )label)label( ifitiititimxmmxmxm      (mixture of experts 

    in rbf, each local fit is a 

constant, wih, second 
layer weight 

    in moe, each local fit is 
a linear function of x, a 
local expert: 

(jacobs et al., 1991) 

21 

ttihtihwxv   moe as models combined 

    radial gating: 

 

 

 

 

    softmax gating: 

22 

                              lllthhtthssg222222/exp/expmxmx                  lttltththgxmxmexpexpcooperative moe 

23 

    regression 

 

 

 

 

 

                                    tjthttitihtihtihjttthtihtiihtititihiihhhxgywyrmgyryrwse                                             xvm221x|,,,competitive moe: regression 

24 

                              ttththhtthttihtiihtihihtihthitihtithhiihhhgffyrwyyrgwsxmxvxvm                                                                                                                                                         exp log|,,2,21xlcompetitive moe: classification 

25 

                                                                        ktkhtihkkhihtihthitihtiththirtihthhiihhhwwyyrgygwstixvxvmexp expexp exp                  log  exp log                               log|,,,xlhierarchical mixture of experts 

26 

    tree of moe where each moe is an expert in a 

higher-level moe 

    soft decision tree: takes a weighted (gating) 

average of all leaves (experts), as opposed to 
using a single path and a single leaf 

    can be trained using em (jordan and jacobs, 

1994) 

