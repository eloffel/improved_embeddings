cmsc 422 introduction to machine learning

furong huang / furongh@cs.umd.edu / department of computer science 

what is machine learning?
machine learning studies representations and 
algorithms that allow machines to improve their 
performance on a task from experience. 

what is machine learning?
    data driven science

    finding and exploiting patterns in data

    algorithms

    data independent algorithms

    prediction

    ultimate goal

what can ml do?

analyze text & speech

what can ml do?

stock price prediction

what can ml do?

recognize objects in images

teach robots how to cook from youtube videos

what can ml do?

exploration and understanding of correlations between various genome features

analyze genomics data

what can ml do?

   

intelligent machine
    as smart as humans

    mechanical turk evaluation, cloud sourcing

    smarter than human
computation power

   
    memory

what can ml do?

what can ml do?

id53 system 
beats jeopardy champion 
ken jennings at quiz bowl!

other ml applications?

machine learning
    paradigm:    programming by example   
    replace    human writing code    with    human 

supplying data      

    most central issue: generalization

    how to abstract from    training       examples to 
   test    examples? memorization vs learning?

    analogy with human learning?

machine learning vs programming
    algorithm

    ml: usually with uncertainty
   

programming: usually deterministic

    goal

    ml: prediction, do better in the future using data in the past
   

programming: computing

    data dependency

    ml: universal for all or a wide class of data
   

programming: data specific

a fast moving field
    broad applicability

    daily life: finance, natural language processing, 
id161, robotics, healthcare, medicine, 
biology

    open/young field 

    fear of ai/ml

    deep learning, gap between theory and practice

    understand what   s in the black box

cmsc 422 goal
    this is a broad overview of existing 
methods for machine learning and 
an introduction to adaptive systems 
in general.

cmsc 422 goal
    by the end of the semester, you should be able to

    look at a problem
   
   
    apply those algorithms

identify if ml is an appropriate solution
if so, identify what types of algorithms might be applicable

    this course is not

    a survey of ml algorithms
    a tutorial on ml toolkits such as weka, tensorflow,    

cmsc 422 topics
foundations of supervised learning
   
id90 and inductive bias 
   
geometry and nearest neighbors 
   
id88 
   
practical concerns: feature design, evaluation, debugging 
   
beyond binary classification 
advanced supervised learning
   
   
   
   
   
   

linear models and id119 
support vector machines 
naive bayes models and probabilistic modeling 
neural networks 
kernels 
id108 

cmsc 422 topics
unsupervised learning
    id116 
    pca 
    expectation maximization

selected advanced topics (as time permits)
    expectation maximization

   online learning
   id100

   

imitation learning 

teaching assistants

xuchen you

joseph thomas bergman

justin shen

what you can expect from the instructors

   

introducing concepts from multiple 
perspectives
   

lecture, reading material, office hours, online 
discussion (piazza)

    providing opportunities to practice

    homework, programming projects

what i expect from you
    math background 

calculus, id202, id203

    come to class prepared

readings required

    attend the lectures

ask questions, take notes if necessary

    complete hw independently
    complete projects collaboratively
    be active on piazza

   

   

   

prerequisites
    cmsc351 (algorithms) and cmsc330 (programming 

languages)

   

    recommended: stat400 (applied id203 and 

statistics) and id202.
these previous courses require cmsc250 (discrete 
structures), cmsc216 (computer systems)

    which in turn require cmsc131 (object oriented 

programming) and math141 (calculus)

    course is about data, representations, mathematical 

modeling, and programming

math in ml

sections
    two sections

   
   

prof. marine carpuat, 0101
this section, 0201

different slides/notes

    same textbook
    common online homework
    different exams/ exam dates

    cover the same material, but using somewhat 

course logistics
grading

   

   

   

released on canvas weekly, due thursday 10:30am
no late submissions (absolute)

homework (20%)
   
   
three projects (30%)
   

form groups of two or three

    midterm exam(20%)

   
final exam(20%)
   

march 8th 11:00 am - 12:15 pm, in class, closed book/notes, one letter size 
cheat sheet

may 12th  8:00     10:00 am, in class, closed book/notes, one letter size 
cheat sheet

course logistics
quick links
    class website
    textbook: a course in machine learning
    assignments release and submission: canvas
    grades: canvas
    after class discussions/surveys, ask questions 

and everything else: piazza

please use piazza instead of email

   

what is learning?

   
   

ability to use previous data to perform future actions
biological systems do it all the time

a definition due to simon (1983) is one of the best:
   learning denotes changes in the system that are adaptive in the
sense that they enable the system to do the task or tasks drawn
from the same population more efficiently and more effectively
the next time.   
simon, herbert a. "why should machines learn?." machine learning. springer berlin heidelberg, 1983. 25-37.

today   s topics
what does it mean to    learn by example   ?

    classification tasks

   

inductive bias

    formalizing learning

classification tasks
    how would you write a program to 

distinguish a picture of me from a picture of 
someone else?

    provide examples pictures of me and 

pictures of other people and let a classifier
learn to distinguish the two.

classification tasks
    how would you write a program to 

distinguish a sentence is grammatical or 
not?

    provide examples of grammatical and 
ungrammatical sentences and let a 
classifier learn to distinguish the two.

classification tasks
    how would you write a program to 

distinguish cancerous cells from normal 
cells?

    provide examples of cancerous and normal 

cells and let a classifier learn to 
distinguish the two.

let   s try it out   
    your task: 
learn a classifier to distinguish 
class a from class b
from examples

examples of class a

examples of class b

let   s try it out   
    learn a classifier from examples
    now: predict class on new examples 

using what you   ve learned

what if i told you   

plausibly other hypotheses: is 
the background in focus or not?

but nobody comes up with them: 
why? inductive bias --- some 
hypotheses are more probable 
than others.

key ingredients 
needed for learning

training vs. test examples

    memorizing the training examples is not enough!
    need to generalize to make good predictions on test examples

inductive bias

    many classifier hypotheses are plausible
    need assumptions about the nature of the relation between examples 

and classes

machine learning 
as function approximation

problem setting

    set of possible instances !
    unknown target function ":!	   &
    set of function hypotheses '=    		   :!	   &}
    training examples {,-,/- ,    ,1,/1 } of unknown 
target function "
    hypothesis    	   '	that best approximates target function "

output

input

formalizing induction:
id168

!(#,%(&)) where #	is the truth and %&	is the 
e.g.	!#,%(&)	 =	*0					,%	#=%(&)
1							./   123,41

system   s prediction

captures our notion of what is important to learn

formalizing induction:
data generating distribution
where does the data come from?

    data generating distribution

a id203 distribution ! over (#,%) pairs
    we don   t know what ! is!

we only get a random sample from it: our training data

formalizing induction:
expected loss

    ! should make good predictions
    as measured by loss "
    on future examples that are also drawn from #
$	, the expected loss of !	over # with respect to " should be 
$   '(,*~,"(.,!(/)) =	   
#/,."(.,!(/))
((,*)

    formally

small

   

is

formalizing induction:
training error

    we can   t compute expected loss because we don   t know what !
    we only have a sample of !
training examples {#$,&$ ,    #(,&( }
*     	.101(&3,4(#3))
(
36$

    all we can compute is the training error

   

   
   

formalizing induction
    given

a id168 !
a sample from some unknown data distribution "
expected error over " with respect to !.
#$,&~(!(*,+(,)) =		0",,*!(*,+(,))

    our task is to compute a function f that has low 

($,&)

recap: introducing 
machine learning
    what does    learning by example    mean?
    classification tasks

    learning requires examples + inductive bias

    generalization vs. memorization
    formalizing the learning problem

    function approximation
    learning as minimizing expected loss

your task before next class
    check out course webpage, canvas, and 

piazza

    do the readings
    get started on hw01: due thursday 10:30am
    let me know dates of religious holiday you 

observe this semester

    let me know if you will need dss 

arrangements

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

