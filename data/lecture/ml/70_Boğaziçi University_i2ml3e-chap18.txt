lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 18:  
id23 

introduction 

3 

    game-playing: sequence of moves to win a game 

    robot in a maze: sequence of actions to find a goal 

    agent has a state in an environment, takes an action 

and sometimes receives reward and the state 
changes 

    credit-assignment 

    learn a policy 

single state: k-armed bandit 

4 

    among k levers, choose  
   the one that pays best 
  q(a): value of action a 
 
 
 
 
  
    rewards stochastic (keep an expected reward): 

reward is ra 
set q(a) = ra 
choose a* if  
  q(a*)=maxa q(a) 

                              aqaraqaqtttt               11   elements of rl (markov decision 
processes) 

5 

    st : state of agent at time t 
    at: action taken at time t 
    in st, action at is taken, clock ticks and reward rt+1 is 

received and state changes to st+1 

    next state prob: p (st+1 | st , at ) 
    reward prob: p (rt+1 | st , at ) 
    initial state(s), goal state(s) 
    episode (trial) of actions from initial state to goal 
    (sutton and barto, 1998; kaelbling et al., 1996) 

policy and cumulative reward 

6 

    policy, 

    value of a policy, 

    finite-horizon: 

 

 

    infinite horizon:   

      ttsa                 :as      tsv                                                                  tiittttttrerrresv121                  rate  discount  the is   10113221                                                                              iitittttrerrresv   bellman   s equation 

7 

                                                                                    111111111111111111                                                                                                                                                                              ttasttttttttttattsttttatttaiititaiitiatttasqasspreasqsaasqsvsvasspresvsvrerreressvsvtttttttt,,,,,,********max| in  of     valuemax|maxmaxmaxmaxmax                        model-based learning 

8 

    environment, p (st+1 | st , at ), p (rt+1 | st , at ) known 
    there is no need for exploration 

    can be solved using id145 

    solve for 

 

 

    optimal policy 

                                                                     1111tsttttatsvasspresvtt**,|max                                                                        1111tsttttttatsvasspasrestt*,|,|max arg*      value iteration 

9 

policy iteration 

10 

temporal difference learning 

11 

    environment, p (st+1 | st , at ), p (rt+1 | st , at ), is not 

known; model-free learning 

    there is need for exploration to sample from  

  p (st+1 | st , at ) and p (rt+1 | st , at ) 
    use the reward received in the next time step to 

update the value of current state (action) 

    the temporal difference between the value of the 

current action and the value discounted from the 
next state  

exploration strategies 

12 

      -greedy: with pr   ,choose one action at random 
uniformly; and choose the best action with pr 1-   

    probabilistic: 

 
 

    move smoothly from exploration/exploitation.  
    decrease    
    annealing 

                           a1bbsqasqsap,exp,exp|                                       a1btbsqtasqsap/,exp/,exp|deterministic rewards and actions 

13 

 
 

    deterministic: single possible reward and next state 

 
 

  used as an update rule (backup) 
 
 
  starting at zero, q values increase, never decrease 

                        111111                           ttasttttttasqasspreasqtt,max,|,**               1111                  ttatttasqrasqt,max,               1111                  ttatttasqrasqt,  max,       =0.9 

consider the value of action marked by    *   : 

if path a is seen first, q(*)=0.9*max(0,81)=73 
then b is seen, q(*)=0.9*max(100,81)=90 

or, 

if path b is seen first, q(*)=0.9*max(100,0)=90 
then a is seen, q(*)=0.9*max(100,81)=90 

q values increase but never decrease 

14 

nondeterministic rewards and 
actions 

15 

    when next states and rewards are nondeterministic 

(there is an opponent or randomness in the environment), 
we keep averages (expected values) instead as 
assignments 

    id24 (watkins and dayan, 1992): 

 

 

    off-policy vs on-policy (sarsa) 

backup 
    learning v (td-learning: sutton, 1988) 

                              tttttsvsvrsvsv                  11                                                                        ttttatttttasqasqrasqasqt,  ,  max,  ,  1111      id24 

16 

sarsa 

17 

eligibility traces 

18 

keep a record of previously visited states (actions) 

                                          asaseasqasqasqasqraseaassasetttttttttttttttt,,,,,,,,,                                                                  11111otherwise  and  ifsarsa (  ) 

19 

generalization 

20 

    tabular: q (s , a) or v (s) stored in a table 

    regressor: use a learner to estimate q(s,a) or v(s) 

 

 

                                                                  zeros all   withyeligibilit0  11111112111eeee        tttttttttttttttttttttttttasqasqasqrasqasqasqrasqasqrett,,,,,,,,                                                                                                         partially observable states 

21 

    the agent does not know its state but receives an 

observation  p(ot+1|st,at) which can be used to infer 
a belief about states 

    partially observable  

  mdp 

 

the tiger problem 

22 

    two doors, behind one of which there is a tiger 

    p: prob that tiger is behind the left door 

 

 

    r(al)=-100p+80(1-p), r(ar)=90p-100(1-p) 
    we can sense with a reward of r(as)=-1 
    we have unreliable sensors 

 

    if we sense ol, our belief in tiger   s position changes 

23 

1130100709011009013080701001801001307070                                                                           )|()()(.)(.)'(')|(),()|(),()|()()(.)(.)'(')|(),()|(),()|()(...)()()|()|('lslllrrrlllrlrlllrrllllllllllllloaroppoppppozpzarozpzaroaroppoppppozpzarozpzaroarpppopzpzopozpp24 

                                                                                             )()()()(max)())|(),|(),|(max()())|(),|(),|(max()()|(max'ppppppppopoaroaroaropoaroaroaropoarvrrsrrrlllslrlljjjii110090126331464318010025 

    let us say the tiger can move from one room to the 

other with prob 0.8 

26 

                                                               )'()'()'('max')(..'ppppppvppp1100901263318010018020    when planning for episodes of two, we can take al, 

ar, or sense and wait: 

27 

                                                   11100901801002'max)()(maxvppppv