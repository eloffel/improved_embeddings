lecture slides for 
introduction  
to  
machine  
learning 
3rd edition 

ethem alpaydin 
   the mit press, 2014 
 
alpaydin@boun.edu.tr 
http://www.cmpe.boun.edu.tr/~ethem/i2ml3e 

chapter 19:  
design and analysis of 
machine learning experiments 

introduction 

3 

    questions: 

    assessment of the expected error of a learning algorithm: is 

the error rate of 1-nn less than 2%? 

    comparing the expected errors of two algorithms: is id92 

more accurate than mlp ? 

    training/validation/test sets 

    resampling methods: k-fold cross-validation 

algorithm preference 

4 

    criteria (application-dependent): 

    misclassification error, or risk (id168s) 

    training time/space complexity 

    testing time/space complexity 

    interpretability 

    easy programmability 

    cost-sensitive learning 

 

factors and response 

5 

    response function based 

on output to be 
maximized 

    depends on controllable 

factors 

    uncontrollable factors 
introduce randomness 

    find the configuration of 
controllable factors that 
maximizes response and 
minimally affected by 
uncontrollable factors 

strategies of experimentation 

6 

how to search the factor space? 

response surface design for approximating  and maximizing  
the response function in terms of the controllable factors 

guidelines for ml experiments 

7 

a. aim of the study 

b. selection of the response variable 

c. choice of factors and levels 

d. choice of experimental design 

e. performing the experiment 

f. statistical analysis of the data 

g. conclusions and recommendations 

resampling and  
k-fold cross-validation 

8 

    the need for multiple training/validation  sets 
 
    k-fold cross-validation:  divide x into k, xi,i=1,...,k 

{xi,vi}i: training/validation  sets of fold i 

 
 
 
 
 
 

    ti share k-2 parts 

12131223211                                                kkkkkkxxxtxvxxxtxvxxxtxv                            215  2 cross-validation 

9 

    5 times 2 fold cross-validation (dietterich, 1998) 

                                                                        15102510259159124224223123112212211111                            xvxtxvxtxvxtxvxtxvxtxvxt                                       id64 

10 

    draw instances from a dataset with replacement 

    prob that we do not pick an instance after n draws 

 

 

 

that is, only 36.8% is new! 

 

 

3680111.                              ennperformance measures 

11 

 
 
 
 
 

 

 

= # of found positives / # of positives  
= tp / (tp+fn) = sensitivity = hit rate 

    error rate   = # of errors / # of instances = (fn+fp) / n 
    recall  
 
    precision   = # of found positives / # of found 
 
= tp / (tp+fp) 
    specificity   = tn / (tn+fp) 
    false alarm rate = fp / (fp+tn) = 1 - specificity 

 

 

roc curve 

12 

13 

precision and recall 

14 

interval estimation 

15 

    x = { xt }t where xt ~ n (   ,   2) 
    m ~ n (   ,   2/n) 

 

100(1-   ) percent 
confidence interval 

                                                                                                                                                            195096196195096196122nzmnzmpnmnmpmnpmn//......~z100(1-   ) percent one-sided 
confidence interval 

when   2 is not known: 

16 

                                                                                                                  1950641950641nzmpnmpmnp....                                                                                             111212122nstmnstmptsmnnmxsnnntt,/,/~     /hypothesis testing 

17 

    reject a null hypothesis if not supported by the sample 

with enough confidence 

 

  x = { xt }t where xt ~ n (   ,   2) 
h0:    =   0 vs. h1:          0  
 
  accept h0 with level of significance    if   0 is in the  
 
 
 
  two-sided test 

100(1-   ) confidence interval 

 

 

            220//,            zzmn          

 

 

 

    one-sided test: h0:           0 vs. h1:    >   0  
  accept if 

 

    variance unknown: use t, instead of z  
  accept h0:    =   0 if  

18 

                     zmn,            0            12120               nnttsmn,/,/,         assessing error: h0:p     p0 vs. h1:p > p0  

19 

    single training/validation set: binomial test 

 

 

if error prob is p0, prob that there are e errors or 
less in n validation trials is 

 

 

 

 

 

 

n=100, e=20 

1-    

accept if this prob is less than 1-    

            jnjjejppjnexp                                          0011normal approximation to the binomial 

20 

    number of errors x is approx n with mean np0 and 

var np0(1-p0) 

 

 

1-    

accept if this prob for x = e is  
less than z1-   

      z~0001pnpnpx      paired t test 

21 

    multiple training/validation sets 
    xt
    error rate of fold i: 

i = 1 if instance t misclassified on fold i 

 

    with m and s2 average and var of pi , we accept p0 or 

less error if 

 
 
 

is less than t  ,k-1 

nxpnttii         1      10      ktspmk~comparing classifiers: h0:  0=  1 vs. 
h1:  0     1  

22 

    single training/validation set: mcnemar   s test 

 

 

 

    under h0, we expect e01= e10=(e01+ e10)/2 

accept if < x2
  

  ,1 

      211001210011x~eeee         k-fold cv paired t test 

23 

    use k-fold cv to get k training/validation folds 

2: errors of classifiers 1 and 2 on fold i 
1     pi

1, pi
    pi
  pi = pi
    the null hypothesis is whether pi has mean 0 

2 : paired difference on fold i 

                  121211221000100                                                   kkkkiikiitttsmksmkkmpskpmhh,/,/,~::             in if  accept       vs.  5  2 cv paired t test 

24 

    use 5  2 cv to get 2 folds of 5 tra/val replications 

(dietterich, 1998)  

    pi

(j) :  difference btw errors of 1 and 2 on fold j=1, 

2 of replication i=1,...,5 

two-sided test: accept h0:   0 =    1 if in (-t  /2,5,t  /2,5)  
one-sided test:  accept h0:   0        1 if < t  ,5  

                                                551211222122152tspppppspppiiiiiiiiii~/      /                        5  2 cv paired f test 

25 

two-sided test: accept h0:   0 =    1 if < f  ,10,5 
 

            510512512122,~fspiiijji                  comparing l>2 algorithms:  
analysis of variance (anova) 

26 

 

    errors of l algorithms on k folds 

 

    we construct two estimators to   2 .  
  one is valid if h0 is true, the other is always valid. 
  we reject h0  if the two estimators disagree.  

lh                     210:      kiljxjij,...,  ,,...,,,~112            n27 

                              212022122122222121011                                                                     ljjljjljjjjljjkiijjssbid48kssbkmmlmmksklmmslmmkkxmhxxn~~/  /,~:                  have  wetrue, is   whenso   namely, , is  of estimator an thus           true is  if228 

                                                                        11210112221221222212122201111111                                                                                                                                                   klllkllklkjjijijjijijljjkijijjjfhfklsswlssbklsswlssbsswskmxsswklmxlskmxssh,,,:~/////~~                                 if         : variances group of averagethe is  to estimator second our  of regardless22   xxanova table 

29 

if anova rejects, we do pairwise posthoc tests 

)(~: vs :1102               klwjijijitmmthh               comparison over multiple datasets 

30 

    comparing two algorithms:  

  sign test: count how many times a beats b over n 

datasets, and check if this could have been by chance if 
a and b did have the same error rate 

    comparing multiple algorithms 

  kruskal-wallis test: calculate the average rank of all 

algorithms on n datasets, and check if these could have 
been by chance if they all had equal error 

 

if kw rejects, we do pairwise posthoc tests to find 
which ones have significant rank difference 

multivariate tests 

31 

    instead of testing using a single performance 
measure, e.g., error, use multiple measures for 
better discrimination, e.g., [fp-rate,fn-rate] 

    compare p-dimensional distributions 

    parametric case: assume p-variate gaussians 

multivariate pairwise comparison 

32 

    paired differences: 

 

 

    hotelling   s multivariate t2 test  

 

    for p=1, reduces to paired t test 

multivariate anova 

33 

    comparsion of l>2 algorithms 

