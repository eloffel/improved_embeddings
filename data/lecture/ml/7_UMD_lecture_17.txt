slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 17 unsupervised learning 
    principal component analysis

furong huang / furongh@cs.umd.edu

unsupervised learning

discovering hidden structure in data

what algorithms do we know for 
unsupervised learning?

id116 id91

today:  how can we learn better 
representations of our data points?

id84

goal: extract hidden lower-dimensional 
structure from high dimensional datasets

why?

to visualize data more easily
to remove noise in data
to lower resource requirements for 
storing/processing data
to improve classification/id91

examples of data points in d dimensional space 
that can be effectively represented in a d-
dimensional subspace (d < d)

principal component analysis

goal: find a projection of the data onto 
directions that maximize variance of the 
original data set

intuition: those are directions in which most 
information is encoded

definition: principal components are 
orthogonal directions that capture most of 
the variance in the data

pca: finding principal components

1st pc

projection of data points along 1st pc 
discriminates data most along any one 
direction

2nd pc

next orthogonal direction of greatest 
variability
and so on   

pca: notation

data points

represented by matrix x of size nxd

!! is the i-th row, i.e., the i-th example
!!" is the value of j-th feature for example i
let   s assume data is centered, i.e.,    !!!=0
principal components are d vectors:  #%,#&,   #'
(!&("=0,*   , and #!"#!=1,   *
   !,-. (0!1#)3= 4# 14#

the sample variance data projected on vector v is

pca formally

finding vector that maximizes sample 
variance of projected data:

!"#$!%&'()()' such that '('=1
!"#$!%!'")")'   -('"'   1)
  solutions are vectors v such that )")'=-'
  i.e. eigenvectors of !"!(sample covariance matrix)

a constrained optimization problem(method of 
lagrange multipliers)

  lagrangian folds constraint into objective: 

lagrange multiplier

    a strategy for finding the local maxima and minima of a 

function subject to equality constraints

    consider the optimization problem

introduce a new variable+ called a lagrange 

   

max! $(&)
subject to (& =0
   &,+ =$&    +((&)

    study the lagrange (lagrangian) function

multiplier

relationship between pca and eigen 
value decomposition 

!"#$!%&'()()'   +'('   1
where x is the n by d data matrix
)!)'   +'=0
matrix )() is a stationary point of the 

derivative with respect to v, set the gradient 
to 0 to solve for the stationary point

    therefore the eigenvector of covariance 

    for this optimization problem, taking 

(*)

optimization problem (*).

pca formally

    the eigenvalue ! denotes the amount of variability 
captured along dimension "
sample variance of projection "!#!#"=!
the 1st pc is the eigenvector of #!# associated with 
the 2nd pc is the eigenvector of #!# associated with 

if we rank eigenvalues from large to small

largest eigenvalue

2nd largest eigenvalue
   

alternative interpretation of pca

pca finds vectors v such that projection 
on to these vectors minimizes 
reconstruction error

resulting pca algorithm

x

how to choose the hyperparameter k?

i.e. the number of dimensions

we can ignore the components of smaller 
significance

an example: eigenfaces

pca pros and cons

pros

eigenvector method
no tuning of the parameters
no local optima

cons

only based on covariance (2nd order statistics)
limited to linear projections

what you should know

principal components analysis

goal: find a projection of the data onto directions 
that maximize variance of the original data set

pca optimization objectives and resulting 
algorithm

why this is useful!

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

