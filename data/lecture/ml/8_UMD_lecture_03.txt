cmsc 422 introduction to machine learning

lecture 3 id90 & limits of learning

furong huang / furongh@cs.umd.edu

last lecture: id90

    what is a decision tree?

    how to learn a decision tree from data?

    what is the inductive bias?

    generalization?

an example training set

a decision tree
to decide whether to play tennis

today: id90

    what is a decision tree?

    how to learn a decision tree from data?

    what is the inductive bias?

    generalization?

a decision tree to distinguish homes in 
new york from homes in san francisco

http://www.r2d3.us/visual-intro-to-machine-learning-part-1/

top-down induction
of id90

currentnode = root

dttrain (examples for currentnode, features at 
currentnode):

1. find f, the    best    decision feature for next 

node

node

2. for each value of f, create new descendant of 

3. sort training examples to leaf nodes
4. if training examples perfectly classified

stop
else
recursively apply dttrain over new leaf nodes 

picking the root attribute

    the goal is to have the resulting decision tree 

as small as possible (occam   s razor)
   however, finding the minimal decision tree 

consistent with the data is np-hard

    the recursive algorithm is a greedy heuristic 

search for a simple tree, but cannot guarantee 
optimality.

    the main decision in the algorithm is the 

selection of the next attribute to condition on. 
(sub optimality)

information gain

    the information gain of an attribute ! is the 
"!#$%,! =($)*+,-%     / |%1|
|%|($)*+,-(%1)
1   156789(5)
   original/current set of examples is %.	
	%1	 is the subset of % for which attribute ! has value <. 

expected reduction in id178 caused by 
partitioning on this attribute

  

  

the id178 of partitioning the data is calculated by weighing 
the id178 of each partition by its size relative to the original 
set.

v

partitions of low id178 (imbalanced splits) lead to high gain

   select the attribute with largest information gain.

2. find the feature with the most information gain:

inducedecisiontree(s)

1. does ! uniquely define a class?
if all "   ! have the same label y: return!;
3. add children to !	:
%=argmax,-.%/(!,2,)
for5 in values(2,):
!6= "   !2,=5
addchild(!	,!6)
inducedecisiontree(!6)

return s;

today: id90

what is a decision tree?

how to learn a decision tree from data?

what is the inductive bias?

generalization?

inductive bias in 
decision tree learning

    our learning algorithm 

performs heuristic search 
through space of decision 
trees
it stops at smallest acceptable 
tree

   

    why do we prefer small trees?

   occam   s razor: prefer 

the simplest hypothesis 
that fits the data

why prefer short hypotheses?

pros

fewer short hypotheses than long ones

a short hypothesis that fits the data is less likely to be a 
statistical coincidence

cons

what   s so special about short hypotheses?

evaluating the learned hypothesis    	
   we   ve learned a tree     using the top-down 

    assume

induction algorithm

   it fits the training data perfectly

    are we done? can we guarantee we 

have found a good hypothesis?

    given

recall: formalizing induction

   a id168 !
   a sample from some unknown data distribution "
has low expected error over " with 
respect to !.
#$,&~(!(*,+(,)) =		0",,*!(*,+(,))

    our task is to compute a function f that 

($,&)

training error is not sufficient

    we care about generalization to new 

examples

    a tree can classify training data perfectly, 

yet classify new examples incorrectly 
   because training examples are only a sample of 

data distribution
v a feature might correlate with class by coincidence
   because training examples could be noisy

v e.g., accident in labeling

let   s add a noisy training example.
how does this affect the learned decision 
tree?

d15     sunny           hot            normal     strong        no

overfitting

    consider a hypothesis     and its:
   error rate over training data "##$#%&'()(   ):
"##$#%&'()    =-1/0(1),   (3)))
4
   true error rate over all data "##$#%&78    : 
)56
"##$#%&78    =9:,;~=0(1,   (3))	=		-?3,10(1,   (3))
    we say     overfits the training data if
(:,;)
"##$#%&'()    <"##$#%&78   
"##$#%&78       	"##$#%&'()   

amount of overfitting =

   

    solution:

evaluating on test data

    problem: we don   t know !""#"$%&'    !
!""#"$')$   
!""#"$')$    =+1-.(01!213,   (51!213))

   we don   t look at them during training!
   after learning a decision tree, we calculate 

v some examples that will be used for evaluation

   we set aside a test set

7
389

effect of overfitting
in id90

overfitting

    another way of putting it

    a hypothesis h is said to overfit the 

training data, if there is another 
hypothesis h   , such that
   h has a smaller error than h    on the training data
   but h has larger error on the test data than h   .

underfitting/overfitting

    underfitting

    overfitting

   learning algorithm had the opportunity to learn more from 

training data, but didn   t

   or didn   t have sufficient data to learn from

   learning algorithm paid too much attention to 

idiosyncracies of the training data; the resulting tree 
doesn   t generalize

    what we want:

   a decision tree that neither underfits nor overfits
   because it is expected to do best in the future

pruning a decision tree

    prune = remove leaves and assign 

majority label of the parent to all items

    prune the children of s if:

   all children are leaves, and 
   the accuracy on the validation set does not 

decrease if we assign the most frequent class 
label to all items at s.

avoid overfitting

   

two basic approaches

  

  

pre-pruning: stop growing the tree at some point during 
construction when it is determined that there is not enough 
data to make reliable choices.
post-pruning: grow the full tree and then remove nodes that 
seem not to have sufficient evidence.

    methods for evaluating subtrees to prune

   cross-validation: reserve hold-out set to evaluate utility
   statistical testing: test if the observed regularity can be 

dismissed as likely to occur by chance

   

hypothesis smaller than remembering the exceptions?

   minimum description length: is the additional complexity of the 
this is related to the notion of id173 that we 
will see in other contexts   keep the hypothesis 
simple.

overfitting

    a decision tree overfits the training data 

when its accuracy on the training data 
goes up but its accuracy on unseen data 
goes down.

overfitting

    empirical error (= on a given data set):
the percentage of items in this data set are 
misclassified by the classifier f.

variance of a learner (informally)

    how susceptible is the learner to minor changes in the 

training data?
   (i.e. to different samples from p(x, y))

    variance increases with model complexity

bias of a learner (informally)

   

   

   

how likely is the learner to identify the target hypothesis?
bias is low when the model is expressive (low empirical error)
bias is high when the model is (too) simple
  

the larger the hypothesis space is, the easier it is to be close 
to the true hypothesis.

impact of bias and variance

model complexity

underfitting and overfitting

   

this can be made more accurate for some loss 
functions. 

    we will develop a more precise and general theory that 

trades expressivity of models with empirical error

expectation

    ! is a discrete random variable with 
distribution "(!) :
    expectation of ! (%[!]), aka. the mean of !
(())
%! =    "!=,,:=()
    expectation of a function of ! (%[/(!)])
.
%/(!) =0"!=,/(!=,)	
	
.

   

if x is continuous, replace sums with integrals

    squared difference between x and its mean:

    variance of x:

variance and standard deviation

!   #! $= &   '( $
v*+! =# !   '( $ =,-$
,-= ,-$= .*+(!)
	(= the square root of the variance)

between x and its mean
    standard deviation of x:

   

    the expected value of the square difference 

variance (continued)

    the variance of ! is equal to the expected 
value of !"	 minus the square of its mean
$%&! =	(!     (! "
=(!"    *+"
$%&! =( !   *+ "
=(!"   2*+!+*+"
=(!"    2*+(! +*+"
=(!"    *+"

    proof:

practical impact 
on decision tree learning

what we want:

a decision tree that neither underfits nor overfits
because it is expected to do best in the future

how can we encourage that behavior?

set a maximum tree depth d

your thoughts?

what are the pros and cons 

of id90?

dealing with data

what real data looks like   
class y

example

1 robocop is an intelligent science fiction thriller and 
social satire , one with class and style .  the film , set 
in old detroit in the year 1991 , stars peter weller as 
murphy , a lieutenant on the city's police force .  1991's 
detroit suffers from rampant crime and a police department 
run by a private contractor ( security concepts inc . ) 
whose employees ( the cops ) are threatening to strike .  to 
make matters worse , a savage group of cop-killers has been 
terrorizing the city . [   ]
0 do the folks at disney have no common decency ?  they have 
resurrected yet another cartoon and turned it into a live 
action hodgepodge of expensive special effects , 
embarrassing writing and kid-friendly slapstick .  wasn't mr
. magoo enough , people ?  obviously not .  inspector gadget 
is not what i would call ideal family entertainment .  [   ]

how would you define input 
vectors x to represent each 
example? what features 

would you use?

train/dev/test sets

in practice, we always split examples into 3 distinct sets
   

training set
   used to learn the parameters of the ml model
  

e.g., what are the nodes and branches of the decision tree

    development set

aka tuning set, aka validation set, aka held-out data

  
   used to learn hyperparameters

v parameter that controls other parameters of the model
v

e.g., max depth of decision tree

   

test set
   used to evaluate how well we   re doing on new unseen 

examples

cardinal rule of machine 
learning:

never evertouch 
your test data!

summary: what you should know 

    id90

   what is a decision tree, and how to induce it from data

    fundamental machine learning concepts

   difference between memorization and generalization
   what inductive bias is, and what is its role in learning. 
   what underfitting and overfitting means
   how to take a task and cast it as a learning problem

why you should never ever 
touch your test data!!

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

