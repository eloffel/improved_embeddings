slides adapted from prof carpuat and duraiswami

cmsc 422 introduction to machine learning

lecture 15 a probabilistic view of machine learning i 

furong huang / furongh@cs.umd.edu

approximating the 0-1 loss with 
surrogate id168s

examples (with b = 0)

hinge loss
log loss
exponential loss

what if !   0?

example: subgradient of
hinge loss

for a given example n

   

   
   

subid119 
for hinge loss

what is the id88 optimizing?

id168 is a variant of the hinge loss

recap: linear models

lets us separate model definition from 
training algorithm (id119)

summary

id119

a generic algorithm to minimize objective functions
works well as long as functions are well behaved (ie
convex)
subid119 can be used at points where 
derivative is not defined
choice of step size is important

optional: can we do better? 

for some objectives, we can find closed form solutions 
(see ciml 6.6)

today   s topics

    bayes rule review

    a probabilistic view of machine learning

joint distributions

   
    bayes optimal classifier

    statistical estimation

    maximum likelihood estimates
    derive relative frequency as the solution to a 

constrained optimization problem

bayes rule

exercise: applying bayes rule

consider the 2 random variables

a = you have the flu
b = you just coughed

assume

p(a) = 0.05
p(b|a) = 0.8
p(b|not a) = 0.2

what is p(a|b)?

answer

    via logic

   assume 100 students     5 have the flu. 80% (4) 

of the students who have the flu cough; 20% 
(19) of the students who don   t have the flu 
cough; so the chance that you have the flu is 
4/23 

    via bayes rule

   p(a|b)p(b)=p(b|a)p(a). 
   p(b)=0.8*0.05+0.2*(1-0.05)=0.04+0.19=0.23
   p(a|b)=0.8*0.05/0.23 =0.04/0.23=4/23 

using a joint distribution

using a joint distribution

given the joint distribution, 
we can find the id203 
of any logical expression e 
involving these variables

using a joint distribution

given the joint distribution,
we can make id136s

e.g., p(male | poor)?
or p(wealth | gender, hours)?

recall: machine learning 
as function approximation

problem setting

    set of possible instances !
    unknown target function ":!   %
    set of function hypotheses &=        :!   %}
    training examples {+,,., ,    +0,.0 } of unknown 
target function "
    hypothesis       & that best approximates target function "

output

input

recall: formal definition of binary 
classification (from ciml)

the bayes optimal classifier

assume we know the data generating distribution !

we define the bayes optimal classifier as

theorem: of all possible classifiers, the bayes optimal classifier 
achieves the smallest zero/one loss

bayes error rate

defined as the error rate of the bayes optimal classifier
best error rate we can ever hope to achieve under zero/one loss

we define the bayes optimal classifier as

the bayes optimal classifier

assume we know the data generating distribution !
if we had access to !, 
we don   t have access to !

bayes error rate

theorem: of all possible classifiers, the bayes optimal classifier 
achieves the smallest zero/one loss

finding an optimal classifier would be trivial!

so let   s try to estimate it instead!
defined as the error rate of the bayes optimal classifier
best error rate we can ever hope to achieve under zero/one loss

what does    training    mean in 
probabilistic settings?

    training = estimating ! from a finite training set
    we typically assume that ! comes from a specific 

family of id203 distributions
e.g., bernouilli, gaussian, etc

   
    learning means inferring parameters of that 

distributions
e.g., mean and covariance of the gaussian

   

training assumption: training 
examples are iid

   

independently and identically 
distributed

i.e. as we draw a sequence of examples from !, 

the n-th draw is independent from the previous 
n-1 sample

   

    this assumption is usually false!
    but sufficiently close to true to be useful

how can we estimate the joint id203 
distribution from data?
what are the challenges?

id113

    find the parameters that maximize the 

id203 of the data

    example: how to model a biased coin?

(on board)

id113

    example: how to model a k-sided die?

(on board)

today   s topics

bayes rule review

a probabilistic view of machine learning

joint distributions
bayes optimal classifier

statistical estimation

maximum likelihood estimates
derive relative frequency as the solution to a 
constrained optimization problem

furong huang

3251 a.v. williams, college park, md 20740

301.405.8010 / furongh@cs.umd.edu

