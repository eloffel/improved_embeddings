nlp
introduction to nlp
introduction to deep learning
neural networks
remember id28?
input: vector of features
output: one or more nodes
trained using sgd
one-layer lr (maxent model) is a simple neural network
neuron (id88): output is a non-linear combination of the inputs
sample code:
http://ai.stanford.edu/~ajoulin/code/nn.zip
id88
x1
x2
x3
x4
y
w1
w2
w3
w4
non-linearities (e.g., sigmoid, tanh)
multi-layer id88
(multi-layer lr)
x1
x2
x3
x4
y1
v1
w2,1
w3,1
w4,1
y2
w1,2
w2,2
w3,2
w4,2
z
w1,1
v2
[hidden layer]
[output layer]
[input layer]
network with a hidden layer
it is a universal function approximator
it can model any continuous function (hornik 1991)
this is not true for a simpler network
it cannot learn xor, for example (no one-layer linear classifier can)
the deeper network can represent xor 
as a disjunction of two lower-level features
an even deeper network can also represent an arbitrary function
but with fewer nodes

multi-layer id88
neural networks
remember id28?
input: vector of features
output: one or more nodes
trained using sgd
one-layer lr (maxent model) is a simple neural network
neuron (id88): output is a non-linear combination of the inputs
non-linearities (e.g., sigmoid, tanh)
deeper networks
multiple layers
input layer
low-level feature layer
higher-level feature layer
output layer
intuition
learning features automatically
+
=
example (from lee et al. 2009)
example (from lee et al. 2009)
what is deep learning
architecture 
neural networks with multiple layers
non-linearities (e.g., sigmoid, tanh)
use
single-layer nn used as simple classifier
multi-layer nn can express more complex functions and learn complex features
hot area of research
very popular these days, especially in speech and vision, but also in nlp
works best with high-performance computers using gpus and very large data sets
why deep learning?
amazing results
speech: dahl et al. 2010     30% improvement in wer (word error rate) 
vision: id163 (krizhevsky et al. 2012)     scarily complex architecture 
the big players (google, facebook, baidu, microsoft, ibm   ) are investing billions in dl
the hot new thing?
actually, many of the architectures that we   ll talk about were invented in the 1980s and 1990s
what   s new is hardware (gpu) that can use these architectures at scale  
id163 architecture
[krizhevsky et al. 2012]
[krizhevsky et al. 2012]
[clarifai.com demo]
autoencoder
x1
x2
x3
x4
y1
y2
z2
[hidden layer]
[output layer]
[input layer]
z1
z3
z4
z = x
autoencoder
the size of the hidden layer should be smaller than the size of the input layer.
otherwise the hidden layer will learn the identity mapping
minimize loss
difference between zi and xi
denoising autoencoder
add some domain-independent noise to the input
example: rotate or stretch the image
still try to have the output match the input
the autoencoder is closely related to svd


quick math review
3
x

2
y

5
z

7
this equation
can be written as a dot product of two vectors: 
3
x
2
y
5
z
id202
likewise, these equations
can be written as a dot product of a matrix and a vector: 



7
3
x
2
y
5
z



12
-1
x
3
y
9
z



8
4
x
y
0
z
supervised machine learning

sigmoid or other nonlinearity
supervised machine learning
w


  

  

y
cost(     ,     )

update parameters
a simplified diagram
w

x
y
  
nlp
