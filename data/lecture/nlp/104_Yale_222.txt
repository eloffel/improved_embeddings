nlp
introduction to nlp
learning in id48
id48 learning
supervised
training sequences are labeled 
unsupervised
training sequences are unlabeled
known number of states
semi-supervised
some training sequences are labeled
supervised id48 learning
estimate the static transition probabilities using id113


estimate the observation probabilities using id113


use smoothing

unsupervised id48 training
given: 
observation sequences
goal: 
build the id48
use em (expectation maximization) methods 
forward-backward (baum-welch) algorithm
baum-welch finds an approximate solution for p(o|  )
outline of baum-welch
algorithm
randomly set the parameters of the id48
until the parameters converge repeat:
e step     determine the id203 of the various state sequences for generating the observations
m step     reestimate the parameters based on these probabilities
notes
the algorithm guarantees that at each iteration the likelihood of the data p(o|  ) increases
it can be stopped at any point and give a partial solution
it converges to a local maximum
nlp
