nlp
introduction to nlp
k-nearest neighbors
k-nearest neighbor classifier
keep all training examples
find k examples that are most similar to the new document (   neighbor    documents)
assign the category that is most common in these neighbor documents (neighbors vote for the category)
can be improved by considering the distance of a neighbor ( a closer neighbor has more influence)

example of id92 classifier
?


id160
http://scott.fortmann-roe.com/docs/biasvariance.html
example
http://scott.fortmann-roe.com/docs/biasvariance.html
example
http://scott.fortmann-roe.com/docs/biasvariance.html
example
http://scott.fortmann-roe.com/docs/biasvariance.html
example (n=1)
http://scott.fortmann-roe.com/docs/biasvariance.html
example (n=15)
http://scott.fortmann-roe.com/docs/biasvariance.html
example (n=65)
http://scott.fortmann-roe.com/docs/biasvariance.html
id92 demo
http://sleepyheads.jp/apps/knn/knn.html
k-nearest neighbor classifier
advantages
no training needed
can be applied to any distance measure and id194
empirically effective
disadvantages
finding nearest neighbors has high time complexity
imprecise when the number of examples is small, which is often true in high-dimensional spaces (neighbors cannot be trusted)
nlp
