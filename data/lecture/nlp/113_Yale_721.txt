nlp
id27s
deep learning
what is the feature vector x?
typically a vector representation of a single character or word
often reflects the context in which that word is found
could just do counts, but that leads to sparse vectors
commonly used techniques: id97 or glove id27s
https://code.google.com/p/id97/ 
includes the models and pre-trained embeddings
pre-trained is good, because training takes a lot of data	
gensim: python library that works with id97
https://radimrehurek.com/gensim/
embeddings are magic, part 1
4
image courtesy of jurafsky & martin
embeddings are magic, part 2
glove vectors for comparative and superlative adjectives

http://nlp.stanford.edu/projects/glove/images/comparative_superlative.jpg
more examples
examples from richard socher
skip-grams
predict each neighboring word 
in a context window of 2c words 
from the current word.
e.g., for c=2, we are given word wt and predicting these 4 words:

skip-grams learn 2 embeddings for each w
input embedding v, in the input matrix w
column i of the input matrix w is the 1  d embedding vi for word i in the vocabulary. 

output embedding v   , in output matrix w   
row i of the output matrix w    is a d    1 vector embedding v   i for word i in the vocabulary.

[jurafsky & martin]
setup
walking through corpus pointing at word w(t), whose index in the vocabulary is j, so we   ll call it wj 
(1 < j < |v|). 
let   s predict w(t+1), whose index in the vocabulary is k (1 < k < |v |). hence our task is to compute p(wk|wj). 


slide courtesy of jurafsky & martin
one-hot vectors
a vector of length |v| 
example:
[0,0,0,0,1,0,0,0,0      .0]
cbow and skipgram (mikolov 2013) 
wi-2
wi-1
wi+1
wi+2

wi
skip-gram
slide courtesy of jurafsky & martin
skip-gram
h = vj
o = w   h
o = w   h
slide courtesy of jurafsky & martin
notes
sparse vs. dense vectors
100,000 dimensions vs. 300 dimensions
<10 non-zero dimensions vs. 300 non-zero dimensions
dense vectors
semantic similarity (cf. lsa)

similarity computation

softmax
evaluating embeddings
nearest neighbors
analogies
(a:b)::(c:?)
information retrieval
semantic hashing

similarity data sets
[table from faruqui et al. 2016]
[mikolov et al. 2013]
semantic hashing
[salakhutdinov and hinton 2007]
wevi (xin rong)
https://ronxin.github.io/wevi/ 
eat|apple, eat|orange, eat|rice, drink|juice, drink|milk, drink|water, orange|juice, apple|juice, rice|milk, milk|drink, water|drink, juice|drink
embeddings for word senses
[rothe and schuetze 2015]
non-compositionality
black cat = black + cat
black market     black + market
notes
id27s perform id105 of the co-occurrence matrix
id97 is a simple feed-forward neural network
training is done using id26 using sgd
negative sampling for training
nlp
