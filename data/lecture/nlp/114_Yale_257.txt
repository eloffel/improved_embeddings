nlp
introduction to nlp
transition-based id33
transition-based parsing
similar to shift-reduce
produces a single (projective) tree
data structures
stack of partially processed (unattached) words
input buffer
set of dependency arcs
attach the word on the top of the stack to the word at the current position in the buffer (or in the other direction)
transition-based parsing
initial configuration
stack (including the root token w0)
buffer (sentence)
arcs (empty)
goal configuration
stack (empty)
buffer (empty)
arcs (complete tree)
maltparser (nivre 2008)
the reduce operations combine an element from the stack and one from the buffer
arc-standard parser
the actions are shift, left-arc, right-arc
arc-eager parser
the actions are shift, reduce, left-arc, right-arc

(arc-eager) maltparser actions
[example from nivre and kuebler]
[example from kuebler, mcdonald, nivre]
example
example:    people want to be free   
            [root] 			[people, want, to, be, free] 	  
shift     [root, people] 	[want, to, be, free] 
lansubj  [root] 			[want, to, be, free]  			a1 = {nsubj(want, people)}
raroot   [root, want]	[to, be, free]  					a2 = a1     {root(root, want)}
characteristics
the next action is chosen locally using a classifier (e.g. id166)
there is no search
the final list of arcs is returned as the dependency tree
trained on a dependency treebank
very fast method
parsing
oracle-based
assuming an oracle, parsing is deterministic
in practice
approximate the oracle with a classifier
o(c) = argmaxt w.f(c,t)
[example from mcdonald and nivre]
greedy transition-based parsing
id125 with q=1
score is computed using a linear model
because of the greedy property, errors can propagate
parse (sent = w1...wn)
c = cs (sent)
while c is not in ct
	t* = argmaxt score (c,t)
	c = t*(c)
return gc
feature model
[example from kuebler, mcdonald, nivre]
feature vectors
[example from kuebler, mcdonald, nivre]
complexity
arc-eager is o(n3)     like eisner
arc-standard is o(n5)
nlp
