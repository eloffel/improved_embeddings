nlp
text similarity
vector semantics
what does    acerola    mean?
acerola is a significant source of vitamin c.
the pulp of the acerola is very soft
acerola are now found growing in most sub-tropical regions of the world.
acerola can be eaten fresh or used to make jams or jellies.
distributional similarity
two words that appear in similar contexts are likely to be semantically related, e.g.,
schedule a test drive and investigate honda's financing options
volkswagen debuted a new version of its front-wheel-drive golf
the jeep reminded me of a recent drive
our test drive took place at the wheel of loaded ford el model
   you will know a word by the company that it keeps.    (j.r. firth 1957)
basic ideas
represent words as vectors
for example, based on nearby words
similar words (synonyms) should have similar representations
different senses of the same word should have different representations
relations should be preserved
for example,    cat   -   kitten    should be similar to    dog   -   puppy   
context features
the context features can be any of the following:
the word before the target word
the word after the target word
any word within n words of the target word
any word within a specific syntactic relationship with the target word (e.g., the head of the dependency or the subject of the sentence)
any word within the same sentence
any word within the same document
example
s1: schedule a test drive and investigate honda's financing options
s2: volkswagen debuted a new version of its front-wheel-drive golf
s3: the jeep reminded me of a recent drive
s4: our test drive took place at the wheel of loaded ford el model
association strength
frequency matters
we want to ignore spurious word pairings.
however, frequency alone is not sufficient, e.g., being near the words    and    or    the is not very informative
pointwise mutual information (pmi). 
here w is a word and c is a feature from the context
association strength
positive pmi (ppmi):
if pmi(w,c)<0, that indicates that the word and the feature are negatively associated
in ppmi, negative values are replaced by zeros
smoothing may be needed
if the vectors are probabilities
use id181
or jensen-shannon divergence
nlp
