nlp
introduction to nlp
probabilities
probabilities in nlp
very important for language processing
example in id103:
   recognize speech    vs    wreck a nice beach   
example in machine translation:
   l   avocat general   :    the attorney general    vs.    the general avocado   
example in information retrieval:
if a document includes three occurrences of    stir    and one of    rice   , what is the id203 that it is a recipe
probabilities make it possible to combine evidence from multiple sources in a systematic way
probabilities
id203 theory
predicting how likely it is that something will happen
experiment (trial)
e.g., throwing a coin
possible outcomes
heads or tails
sample spaces
discrete (number of occurrences of    rice   ) or continuous (e.g., temperature)
events
    is the certain event 
    is the impossible event
event space - all possible events
sample space
random experiment: an experiment with uncertain outcome
e.g., flipping a coin, picking a word from text
sample space: all possible outcomes, e.g., 
tossing 2 fair coins,     ={hh, ht, th, tt}

events
event: a subspace of the sample space
e       , e happens iff outcome is in e, e.g., 
e={hh} (all heads) 
e={hh,tt} (same face)
impossible event (   )
certain event (   )	
id203 of event : 0     p(e)    1, s.t.
p(   )=1 (outcome always in    )
p(a    b)=p(a)+p(b), if (a   b)=     (e.g., a=same face, b=different face)
example: toss a die
sample space:     = {1,2,3,4,5,6}
fair die:
p(1) = p(2) = p(3) = p(4) = p(5) = p(6) = 1/6
unfair die: p(1) = 0.3, p(2) = 0.2, ...
n-dimensional die:
    = {1, 2, 3, 4,    , n}
example in modeling text:
toss a die to decide which word to write in the next position
    = {cat, dog, tiger,    }
example: flip a coin
    : {head, tail}
fair coin: 
p(h) = 0.5, p(t) = 0.5
unfair coin, e.g.:
p(h) = 0.3, p(t) = 0.7
flipping two fair coins:
sample space: {hh, ht, th, tt}
example in modeling text:
flip a coin to decide whether or not to include a word in a document
sample space = {appear, absence}
probabilities
probabilities
numbers between 0 and 1
id203 distribution
distributes a id203 mass of 1 throughout the sample space    .
example: 
a fair coin is tossed three times. 
what is the id203 of 3 heads?
what is the id203 of 2 heads?
meaning of probabilities
frequentist
i threw the coin 10 times and it turned up heads 5 times
subjective
i am willing to bet 50 cents on heads
probabilities
joint id203: p(a   b), also written as p(a, b)
id155: p(b|a)=p(a   b)/p(a)
p(a   b) = p(a)p(b|a) = p(b)p(a|b)
so, p(a|b) = p(b|a)p(a)/p(b) (bayes    rule)
for independent events, p(a   b) = p(a)p(b), so p(a|b)=p(a)
total id203: if a1,    , an form a partition of s, then
p(b) = p(b   s) = p(b, a1) +     + p(b, an) (why?)
so, p(ai|b) = p(b|ai)p(ai)/p(b) = p(b|ai)p(ai)/[p(b|a1)p(a1)+   +p(b|an)p(an)] 
this allows us to compute p(ai|b) based on p(b|ai)
                      
id155
prior and posterior id203
id155
p(a|b) = 
p(a     b)
p(b) 

properties of probabilities
p(   ) = 0 
p(certain event)=1
p(x)     p(y), if x     y
p(x     y) = p(x) + p(y), if x     y=   

id155
six-sided fair die
p(d even)=?
p(d>=4)=?
p(d even|d>=4)=?
p(d odd|d>=4)=?
multiple conditions
p(d odd|d>=4, d<=5)=?
answers
six-sided fair die
p(d even)=3/6=1/2
p(d>=4)=3/6=1/2
p(d even|d>=4)=2/3
p(d odd|d>=4)=1/3
multiple conditions
p(d odd|d>=4, d<=5)=1/2
the chain rule
p(w1,w2,w3   wn) = ?
using the chain rule:
p(w1,w2,w3   wn) =p(w1) p(w2|w1) p(w3|w1,w2)    p(wn|w1,w2   wn-1)
this rule is used in many ways in statistical nlp, more specifically in markov models

independence
two events are independent when 
p(a   b) = p(a)p(b)
unless p(b)=0 this is equivalent to saying that
p(a) = p(a|b)
if two events are not independent, they are considered dependent

adding vs. removing constraints
adding constraints
p(walk=yes|weather=nice)
p(walk=yes|weather=nice,freetime=yes,crowded=yes)
more accurate
but more difficult to estimate
removing constraints (backoff)
p(walk=yes|weather=nice,freetime=yes,crowded=yes)
p(walk=yes|weather=nice,freetime=yes)
p(walk=yes|weather=nice)
note that it is not possible to do backoff on the left hand side of the conditional
[example modified from jason eisner]
x:         rn
random variables
simply a function:
the numbers are generated by a stochastic process with a certain id203 distribution
example
the discrete random variable x that is the sum of the faces of two randomly thrown fair dice
id203 mass function (pmf) which gives the id203 that the random variable has different numeric values:


p(x) = p(x = x)  = p(ax) where ax = {            : x(   ) = x}
random variables
if a random variable x is distributed according to the pmf p(x), then we write x ~ p(x)
for a discrete random variable, we have

sp(xi) = p(   ) = 1
six-sided fair die
p(1) = 1/6
p(2) = 1/6
etc.
p(d)=?
p(d) = {1/6, 1/6, 1/6, 1/6, 1/6, 1/6}
p(d|odd) = {1/3, 0, 1/3, 0, 1/3, 0}

nlp
