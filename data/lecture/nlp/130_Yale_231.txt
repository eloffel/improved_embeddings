nlp
introduction to nlp
statistical id52
id52 methods
rule-based
stochastic
id48 (generative)
maximum id178 mm (discriminative)
transformation-based
id48-based id52
find tag sequence that maximizes the id203 formula
p(word|tag) * p(tag|previous n tags) 
a bigram-based id48 tagger chooses the tag ti for word wi that is most probable given the previous tag ti-1 and the current word wi:
ti = argmaxj p(tj|ti-1,wi)
ti = argmaxj p(tj|ti-1)p(wi|tj) : id48 equation for a single tag

id48 tagging
t = argmax p(t|w)
where t=t1,t2,   ,tn
by bayes    theorem
p(t|w) = p(t)p(w|t)/p(w)
thus we are attempting to choose the sequence of tags that maximizes the right hand side of the equation
p(w) can be ignored
p(t) is called the prior, p(w|t) is called the likelihood.
id48 tagging
complete formula
p(t)p(w|t) =   p(wi|w1t1   wi-1ti-1ti)p(ti|t1   ti-2ti-1)
simplification 1: 
p(w|t) =   p(wi|ti)
simplification 2: 
p(t)=   p(ti|ti-1)
bigram approximation
t = argmax p(t|w) = argmax   p(wi|ti) p(ti|ti-1)
maximum likelihood estimates
transitions
p(nn|jj) = c(jj,nn)/c(jj)=22301/89401 = .249
emissions
p(this|dt) = c(dt,this)/c(dt)=7037/103687 = .068
example
the/dt rich/jj like/vbp to/to travel/vb ./.
example
evaluating taggers
data set
training set
development set
test set
tagging accuracy
how many tags right
results
baseline is very high     90% for english
trigram id48 about 95% total accuracy, 55% on unknown words
highest accuracy around 97% on ptb trained on 800,000 words
(50-85% on unknown words; 50% for trigrams)
upper bound 97-98% 
noise (e.g., errors and inconsistencies in the data, e.g., nn vs jj)
notes on pos
new domains
lower performance
new languages
morphology matters! also availability of training data
distributional id91
combine statistics about semantically related words
example: names of companies
example: days of the week
example: animals
notes on pos
british national corpus
http://www.natcorp.ox.ac.uk/
tagset sizes
ptb 45, brown 85, universal 12, twitter 25
dealing with unknown words
look at features like twodigitnum, allcaps, initcaps, containsdigitandslash (bikel  et al. 1999)


brown id91
words with similar vector representations are clustered together, in an agglomerative (recursive) way
for example,    monday   ,    tuesday   , etc. may form a new vector    day of the week   
published by brown et al. [1992]
example
friday monday thursday wednesday tuesday saturday sunday weekends sundays
people guys folks fellows ceos chaps doubters commies unfortunates blokes
down backwards ashore sideways southward northward overboard aloft downwards adrift
water gas coal liquid acid sand carbon steam shale iron
great big vast sudden mere sheer gigantic lifelong scant colossal
american indian european japanese german african catholic israeli italian arab
mother wife father son husband brother daughter sister boss uncle
machine device controller processor cpu printer spindle subsystem compiler plotter
john george james bob robert paul william jim david mike
feet miles pounds degrees inches barrels tons acres meters bytes
had hadn't hath would've could've should've must've might've
that tha theat
head body hands eyes voice arm seat eye hair mouth
example
input:
this is one document . it has two sentences but the program only cares about spaces .
here is another document . it also has two sentences .
and here is a third document with one sentence .
this document is short .
the dog ran in the park .
the cat was chased by the dog .
the dog chased the cat .
[code by michael heilman: https://github.com/mheilman/tan-id91]
[code by michael heilman]
.       	1011    	9
the     	011    	7
is      	110     	4
document	1110    	4
dog     	000     	3
it      	101001  	2
one     	11111   	2
sentences	1010111 	2
chased  	00111   	2
two     	1010100 	2
has     	1010110 	2
here    	111101  	2
this    	1000    	2
cat     	0010    	2
and     	11110010	1
sentence	11110011	1
ran     	01011		1
in      	0100		1
spaces  	10101011011	1
another 	1010001	1
cares   	101010111	1
also    	1010000	1
only    	10101011010	1
program 	10101011001	1
was     	001100		1
park    	01010		1
but     	10101011000	1
short   	1001		1
with    	111100001	1
by      	001101		1
a       	111100000	1
about   	10101010	1
third   	11110001	1
external link
jason eisner   s awesome interactive spreadsheet about learning id48s
http://cs.jhu.edu/~jason/papers/#eisner-2002-tnlp 
http://cs.jhu.edu/~jason/papers/eisner.id48.xls 

introduction to nlp
transformation-based learning
transformation based learning
idea: change some labels given specific input patterns
[brill 1995]
example
p(nn|sleep) = .9
p(vb|sleep) = .1
change nn to vb when the previous tag is to
types of rules:
the preceding (following) word is tagged z
the word two before (after) is tagged z
one of the two preceding (following) words is tagged z
one of the three preceding (following) words is tagged z
the preceding word is tagged z and the following word is tagged w

transformation based tagger
transformation based tagger
unknown words
nlp
