decoding and id136 

with  

syntactic translation models

machine translation 

lecture 15 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

id18s

s     np  vp
vp     np  v

v     tabeta
np     jon-ga
np     ringo-o

s

np

vp

jon-ga np

v

ringo-o

tabeta

output:

jon-ga ringo-o tabeta

synchronous id18s

s     np  vp
vp     np  v

v     tabeta
np     jon-ga
np     ringo-o

synchronous id18s

s     np  vp
vp     np  v

v     tabeta
np     jon-ga
np     ringo-o

:

:

:
:
:

1

2

2

1

(monotonic)

(inverted)

ate
john
an apple

synchronous generation

s

np

vp

jon-ga

np

s

np

vp

v

john

v

np

ringo-o

tabeta

ate

an apple

output:

(
)
jon-ga ringo-o tabeta : john ate an apple

translation as parsing

parse source

project to target

s

vp

np

np

v

np

s

v

vp

np

jon-ga ringo-o tabeta

  john

    ate

an apple

a closer look at parsing
    parsing is usually done with id145
    share common computations and structure 
    represent exponential number of alternatives in 
    with sid18s there are two kinds of ambiguity

polynomial space

    source parse ambiguity
    translation ambiguity
    parse forests can represent both!

cky or variants on the cky algorithm)

a closer look at parsing
    any monolingual parser can be used (most often: 
    parsing complexity is o(|n3|)
    cubic in the length of the sentence (n3)
    cubic in the number of non-terminals (|g|3)
    adding nonterminal types increases parsing 
    with few nts, exhaustive parsing is tractable

complexity substantially!

parsing as deduction

antecedents

side conditions

a : u b : v

c : w

 

consequent

   if a and b are true with weights u and v, and 
phi is also true, then c is true with weight w.   

example: cky

inputs:

f = hf1, f2, . . . , f`i
g context-free grammar in chomsky 

normal form.

item form:
[x, i, j] a subtree rooted with nt type x 
spanning i to j has been recognized.

example: cky

goal:

axioms:

[s, 0,  ]

[x, i   1, i] : w

w

(x ! fi) 2 g

id136 rules:

[x, i, k] : u [y, k, j] : v

[z, i, j] : u     v     w

w

(z ! xy ) 2 g

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
v     saw
nn    
duck
v     duck
prp     i
prp     her

0

np,2,4

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

sbar
2,4

np,2,4

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

vp,1,4

sbar
2,4

np,2,4

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

vp,1,4

sbar
2,4

np,2,4

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

s     prp  vp
vp     v  np
vp     v  sbar
sbar     prp  v
np     prp nn
nn    v     saw
duck
v     duck
prp     i
prp     her

0

s,0,4

vp,1,4

sbar
2,4

np,2,4

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

i       saw        her        duck
1

2

3

4

what is this object?

s,0,4

vp,1,4

sbar
2,4

np,2,4

prp,0,1

v,1,2

prp,2,3

nn,3,4

v,3,4

0

i       saw        her        duck
1

3

2

4

semantics of hypergraphs

arity of the edge is the number of tails)

    generalization of directed graphs
    special node designated the    goal   
    every edge has a single head and 0 or more tails (the 
    node labels correspond to lhs   s of id18 rules
    a derivation is the generalization of the graph concept 
    weights multiply along edges in the derivation, and add 

of path to hypergraphs

at nodes (cf. semiring parsing)

edge labels

sites (non-terminals)

source and target languages

    edge labels may be a mix of terminals and substitution 
    in translation hypergraphs, edges are labeled in both the 
    the number of substitution sites must be equal to the 
arity of the edge and must be the same in both languages
    the two languages may have different orders of the 
    there is no restriction on the number of terminal 

substitution sites

symbols

edge labels

la lectura : reading 

ayer : yesterday

x

x

  de      :      's 
1

2

2

1

s

2

  de      :       from 
1

2

1

{
(
(

,
la lectura de ayer : yesterday     s  reading
)
la lectura de ayer : reading from yesterday
)

}

id136 algorithms
    viterbi 

o(|e| + |v |)

    find the maximum weighted derivation
    requires a partial ordering of weights

    inside - outside

o(|e| + |v |)

    compute the marginal (sum) weight of all 
derivations passing through each edge/node
o(|e| + |dmax|k log k)

    k-best derivations

    enumerate the k-best derivations in the hypergraph
    see iwpt paper by huang and chiang (2005)

things to keep in mind

bound on the number of edges:

|e|   o(n3|g|3)

bound on the number of nodes:

|v |   o(n2|g|)

decoding again

    translation hypergraphs are a    lingua 
franca    for translation search spaces
    note that fst lattices are a special case
    decoding problem: how do i build a 

translation hypergraph?

representational limits

consider this very simple sid18 translation model:

   glue    rules:

s     s
s     s

s :
s :

1

2

2

1

representational limits

consider this very simple sid18 translation model:

   glue    rules:

   lexical    rules:

s     s
s :
s     s
s :
s     tabeta
s     jon-ga
s     ringo-o

:
:
:

1

2

2

1

ate
john
an apple

representational limits

    phrase-based decoding runs in exponential 
time
    all permutations of the source are 
modeled (traveling salesman problem!)
    typically distortion limits are used to 
    but parsing is polynomial...what   s going on?

mitigate this

representational limits

binary sid18s cannot model this 
(however, ternary sid18s can):
c

a

b

d

b

d

a

c

representational limits

binary sid18s cannot model this 
(however, ternary sid18s can):
c

a

b

d

b

d

a

c

but can   t we binarize any grammar?

representational limits

binary sid18s cannot model this 
(however, ternary sid18s can):
c

a

b

d

b

d

a

c

but can   t we binarize any grammar?

no. synchronous id18s cannot 
generally be binarized!

does this matter?

    the    forbidden    pattern is observed in real data (melamed, 2003)
    does this matter?

    learning

    translation

pattern

    phrasal units and higher rank grammars can account for the 
    sentences can be simpli   ed or ignored

    the pattern does exist, but how often must it exist (i.e., is 

there a good translation that doesn   t violate the sid18 
matching property)?

tree-to-string

    how do we generate a hypergraph for a tree-to-

string translation model?
    simple linear-time (given a    xed translation 
model) top-down matching algorithm
    recursively cover    uncovered    sites in tree
    each node in the input tree becomes a node in 
    for details, huang et al. (amta, 2006) and huang 

the translation forest

et al. (emnlp, 2010)

s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

}

tree-to-string grammar

s

vp

np

v

np
jon-ga

ringo-o tabeta

s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

s

vp

np

v

np
jon-ga

ringo-o tabeta

s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

s

1

2

np
1

vp

n
h
o
j

np
2

l

e
p
p
a

 

n
a

2

1

v

e

t

a

s

vp

np

v

np
jon-ga

ringo-o tabeta

s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

s

1

2

np
1

vp

n
h
o
j

np
2

l

e
p
p
a

 

n
a

2

1

v

e

t

a

s

vp

np

v

np
jon-ga

ringo-o tabeta

s(x1:np x2:vp) ! x1 x2
s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

s

1

2

np
1

vp

n
h
o
j

np
2

l

e
p
p
a

 

n
a

2

1

v

e

t

a

s

vp

np

v

np
jon-ga

ringo-o tabeta

s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

s

1

2

np
1

vp

n
h
o
j

np
2

l

e
p
p
a

 

n
a

2

1

v

e

t

a

s

vp

np

v

np
jon-ga

ringo-o tabeta

s(x1:np x2:vp) ! x1 x2
vp(x1:np x2:v) ! x2 x1

tabeta ! ate
ringo-o ! an apple
jon-ga ! john

s

1

2

np
1

vp

n
h
o
j

np
2

l

e
p
p
a

 

n
a

2

1

v

e

t

a

language models

hypergraph review

la lectura : reading 

ayer : yesterday

x

x

  de      :      's 
1

2

2

1

  de      :       from 
1

2

1

s

2

goal node

source label

target label

hypergraph review

la lectura : reading 

ayer : yesterday

x

x

  de      :      's 
1

2

2

1

s

2

  de      :       from 
1

2

1

substitution sites / variables / non-terminals

hypergraph review

la lectura : reading 

ayer : yesterday

x

x

  de      :      's 
1

2

2

1

s

2

  de      :       from 
1

2

1

for lm integration, we ignore the source!

hypergraph review

la lectura : reading 

ayer : yesterday

x

x

  de      :      's 
1

2

2

1

s

2

  de      :       from 
1

2

1

for lm integration, we ignore the source!

hypergraph review

la lectura : reading 

ayer : yesterday

x

x

  de      :      's 
1

2

2

1

s

2

  de      :       from 
1

2

1

{
(
(

,
)
yesterday     s  reading
)
reading from yesterday

}

how can we add the lm score to each string derived 

by the hypergraph?

lm integration

    if lm features were purely local ...
       unigram    model
    ... integration would be a breeze
    add an    lm feature    to every edge
    but, lm features are non-local!

why is it hard?

x

x

  de      :       saw 

1

2

x

two problems:

1. what is the content of the variables?

why is it hard?

x

x

two problems:

x

  de      :       saw 

1

2

john mary

1. what is the content of the variables?

why is it hard?

x

x

two problems:

x

1

  de      :       saw 
i think i mary

2

1. what is the content of the variables?

why is it hard?

x

x

x

  de      :       saw 
1
i think i

2
somebody

two problems:

1. what is the content of the variables?

why is it hard?

x

x

x

  de      :       saw 
1
i think i

2
somebody

two problems:

1. what is the content of the variables?
2. what will be the left context when this   
string is substituted somewhere?

why is it hard?

x

x

<s>

1

</s>

x

s

  de      :       saw 
1
i think i

2
somebody

two problems:

1. what is the content of the variables?
2. what will be the left context when this   
string is substituted somewhere?

why is it hard?

x

x

fortunately, 

1

x

x

  de      :       saw 
1
i think i

2
somebody

two problems:

1. what is the content of the variables?
2. what will be the left context when this   
string is substituted somewhere?

why is it hard?

x

x

two problems:

x

x

2

1

x

  de      :       saw 
1
i think i

2
somebody

1. what is the content of the variables?
2. what will be the left context when this   
string is substituted somewhere?

naive solution

    extract the all (k-best?) translations from 
the translation model
    score them with an lm
    what   s the problem with this?

outline of dp solution

    use n-order markov assumption to help us

the local (conditional) id203 of a word in context

    in an id165 lm, words more than n words away will not affect 
    this is not generally true, just the markov assumption! 

    general approach

along edges.

    restructure the hypergraph so that lm probabilities decompose 
    solves both    problems   

   enough   .

    we will not know the full value of variables, but we will know 
    defer scoring of left context until the context is established.

hypergraph restructuring

    note the following three facts:

    if you know n or more consecutive words, the conditional 
probabilities of the nth, (n+1)th, ... words can be computed.
    therefore: add a feature weight to the edge for words.

    (n-1) words of context to the left is enough to determine the 

id203 of any word
    therefore: split nodes based on the (n-1) words on the right side 

of the span dominated by every node

    (n-1) words on the left side of a span cannot be scored with 

certainty because the context is not known
    therefore: split nodes based on the (n-1) words on the left side 

of the span dominated by every node

hypergraph restructuring

    note the following three facts:

    if you know n or more consecutive words, the conditional 
probabilities of the nth, (n+1)th, ... words can be computed.
    therefore: add a feature weight to the edge for words.
split nodes by the (n-1) words on both sides of 
id203 of any word
    therefore: split nodes based on the (n-1) words on the right side 

    (n-1) words of context to the left is enough to determine the 

the convergent edges.

of the span dominated by every node

    (n-1) words on the left side of a span cannot be scored with 

certainty because the context is not known
    therefore: split nodes based on the (n-1) words on the left side 

of the span dominated by every node

hypergraph restructuring

    algorithm (   cube intersection   ):

    for each node v (proceeding in topological order through the 
nodes)
    for each edge e with head-node v, compute the (n-1) words 
on the left and right; call this qe
    do this by substituting the (n-1)x2 word string from the 
    if node vqe does not exist, create it, duplicating all outgoing 
    disconnect e from v and attach it to vqe 

edges from v so that they also proceed from vqe

tail node corresponding to the substitution variable

    delete v

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

1

0.6

x

x
  de      :       from 

21

1

2

0.4

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

1

0.6

x

x
  de      :       from 

21

1

2

0.4

-lm viterbi:

the stain   s the man

hypergraph restructuring

let   s add a bi-gram language model!

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

1

0.6

x

x
  de      :       from 

21

1

2

0.4

hypergraph restructuring

let   s add a bi-gram language model!

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

1

0.6

x

x
  de      :       from 

21

1

2

0.4

hypergraph restructuring

p(mancha|la)

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

1

0.6

x

x
  de      :       from 

21

1

2

0.4

hypergraph restructuring

p(mancha|la)

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

xla mancha

1

0.6

x

x
  de      :       from 

21

1

2

0.4

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

p(stain|the)

  de      :      's 
1
x

2

2

xla mancha

1

0.6

x

x
  de      :       from 

21

1

2

0.4

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

p(stain|the)

  de      :      's 
1
x

2

2

xla mancha

1

0.6

x

x
  de      :       from 

21

1

2

0.4

xthe stain

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

p(gray|the) x

p(stain|gray)

  de      :      's 
1
x

2

2

xla mancha

1

0.6

x

x
  de      :       from 

21

1

2

0.4

xthe stain

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

p(gray|the) x

p(stain|gray)

  de      :      's 
1
x

2

2

xla mancha

1

0.6

x

x
  de      :       from 

21

1

2

0.4

xthe stain

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

xla mancha

1

0.6

x

  de      :       from 

21

1

2

0.4

xthe stain

hypergraph restructuring

0.6
0.4

0.1
0.7
0.2

the man
the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2

2

1

0.6

x

xla mancha
1

  de      :       from 

21

2

0.4

xthe stain

hypergraph restructuring

0.6

the man

xthe man

0.4

0.1
0.7
0.2

the husband

la mancha
the stain
the gray stain

  de      :      's 
1
x

2
the husband

2

1

0.6

x

xla mancha
1

  de      :       from 

21

2

0.4

xthe stain

hypergraph restructuring

0.6

the man

xthe man

  de      :      's 
1
x

2
the husband

2

1

0.6

0.4

the husband

every node    remembers    enough 
for edges to compute lm costs

x

0.1
0.7
0.2

la mancha
the stain
the gray stain

  de      :       from 

21

2

0.4

xla mancha
1

xthe stain

complexity

    what is the run-time of this algorithm?

complexity

    what is the run-time of this algorithm?
o(|v ||e|| |2(n 1))

going to longer id165s is exponentially expensive!

cube pruning

    expanding every node like this exhaustively is 
impractical
    polynomial time, but really, really big!
    cube pruning: minor tweak on the above 
algorithm
    compute the k-best expansions at each node
    use an estimate (usually a unigram id203) 

of the unscored left-edge to rank the nodes

cube pruning

mt

    widely used for phrase-based and syntax-based 
    may be applied in conjunction with a bottom-up 
decoder, or as a second    rescoring    pass
    nodes may also be grouped together (for 

example, all nodes corresponding to a certain 
source span)

    requirement for topological ordering means 
translation hypergraph may not have cycles

reading

textbook

    chapter 11 from the 
    research papers listed in 

the syllabus 

