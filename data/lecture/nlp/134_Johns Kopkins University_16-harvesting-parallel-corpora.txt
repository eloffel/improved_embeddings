harves   ng	
   parallel	
   documents	
   	
   

from	
   the	
   web

april 3, 2014

thanks to jakob uszkoreit and ashish venugopal for many of today   s slides!  

                                              (cid:2)        .
sentence	
   aligned	
   bitexts

. . .

anc calls for steps to prevent deaths in 

french

arabic

english

english

l' espagne a refus   de con   rmer que l' 
espagne avait refus   d' aider le maroc.

!"#$%&'( ) '*+, -"./ 012 34"5 6+78 

declined to aid morocco.

spain declined to con   rm that spain 

torture is still being practised on a wide 

scale.

nous voyons que le gouvernement 
fran  ais a envoy   un m  diateur.   

69$: 0;1<"= +)0$>", 6+)?$@"a b6c 7d( efg.h 

we see that the french government has 
.69<i<j 

sent a mediator.

place routinely.

arrest and detention without cause take 

force est de constater que la situation 

6?"c 6kl +#$m12 e"#df<nh 6+#o@"0j +#p<"7<j .

we note that the situation is changing 

this is a time for vision and political 

  volue chaque jour. 

every day.

. . .

. . .

chinese

courage 

. . .

english

                           (cid:1)    (cid:1)              (cid:2) .

                                              (cid:2)        .

. . .

china's energy and raw materials 

production up.

anc calls for steps to prevent deaths in 

police custody .

. . .

english

2

torture is still being practised on a wide 

goals	
   for	
   today   s	
   lecture

    understand	
   how	
   to	
   mine	
   bitexts	
   from	
   the	
   web	
   
    web	
   crawling	
   101	
   
    review	
   recent	
   research	
   into	
   extrac   ng	
   parallel	
   
documents	
   from	
   the	
   web	
   and	
   from	
   unstructured	
   
collec   ons	
   
    what	
   to	
   do	
   if	
   you   re	
   google	
   and	
   you   re	
   worried	
   
about	
   harves   ng	
   your	
   own	
   machine	
   transla   on	
   
output

3

the	
   web	
   as	
   a	
   parallel	
   corpus

    old	
   idea:	
   
    philip resnik, "parallel strands: a preliminary investigation into mining the web for 
bilingual text", in machine translation and the information soup: third conference of 
the association for machine translation in the americas (amta-98), october, 1998. 	
   
    heuris   cally	
   iden   fy	
   web	
   pages	
   that	
   are	
   
poten   al	
   transla   ons	
   of	
   each	
   other	
   
    download	
   them	
   	
   
    do	
      ltering	
   to	
   check	
   whether	
   they	
   are	
   really	
   
transla   ons

4

heuris   c	
   iden      ca   on

    use	
   link	
   text	
   
    if	
   a	
   page	
   is	
   wrinen	
   in	
   english,	
   and	
   contains	
   a	
   link	
   
with	
   the	
   text	
   fran  ais	
   
    if	
   the	
   target	
   page	
   is	
   wrinen	
   in	
   french	
   and	
   
contains	
   a	
   link	
   with	
   the	
   text	
   english	
   
    then	
   the	
   pair	
   of	
   documents	
   may	
   be	
   transla   ons	
   
of	
   each	
   other

5

6

7

check	
   for	
   transla   on	
   equivalence

    how	
   would	
   you	
   check	
   to	
   see	
   if	
   two	
   documents	
   
were	
   transla   ons	
   of	
   each	
   other	
   or	
   not?	
   
    how	
   would	
   your	
   strategy	
   di   er	
   if	
   

   	
   you	
   didn   t	
   have	
   any	
   bilingual	
   resources	
   
   	
   you	
   had	
   a	
   normal	
   bilingual	
   dic   onary	
   
   	
   you	
   had	
   a	
   small	
   amount	
   of	
   bitexts	
   already	
   
!

    discuss	
   with	
   your	
   neighbor

8

the chunk length is measured in nonwhitespace bytes, and the html tags are nor-
malized for case. attribute-value pairs within the tags are treated as nonmarkup text
(e.g., <font color="blue"> produces [start:font] followed by [chunk:12]).

page	
   structure	
   similarity

the second step is to align the linearized sequences using a standard dynamic pro-
gramming technique (hunt and mcilroy 1975). for example, consider two documents
that begin as follows:

<html>
<title>emergency exit</title>
<body>
<h1>emergency exit</h1>
if seated at an exit and
...

<html>
<title>sortie de secours</title>
<body>
si vous   etes assis `a
c  ot  e d   une . . .
...

the aligned linearized sequence would be as follows:

[start:html]
[start:title]
[chunk:13]
[end:title]
[start:body]
[start:h1]
[chunk:13]
[end:h1]
[chunk:112]

[start:html]
[start:title]
[chunk:15]
[end:title]
[start:body]

[chunk:122]

9

353

strand

    %	
   of	
   non-     shared	
   material	
   
    number	
   of	
   aligned	
   non-     markup	
   text	
   chunks	
   that	
   
are	
   di   erent	
   in	
   length	
   
    correla   on	
   of	
   lengths	
   of	
   the	
   text	
   chunks	
   
    signi   cance	
   level	
   of	
   the	
   correla   on	
   
!

   set	
   the	
   value	
   of	
   each	
   of	
   those	
   elements	
   empirically	
   
against	
   a	
   set	
   of	
   manually	
   classi   ed	
   real-     world	
   pages

10

bilingual	
   dic   onary

bilingual internet text search (bits) (ma and liberman 1999) starts with a given
list of domains to search for parallel text. it operates by sampling pages from each
domain and identifying their languages; if a domain is deemed to be multilingual,
all pages on the site are crawled exhaustively. bits appears to consider all possible
combinations of web page pairs in the two languages (i.e., the full cross product within
each site) and    lters out bad pairs by using a large bilingual dictionary to compute a
content-based similarity score and comparing that score to a threshold. for each page
pair, the similarity score is

    use	
   a	
   bilingual	
   dic   onary	
   to	
   do	
   a	
   word-     for-     word	
   
lookup	
   of	
   all	
   the	
   words	
   in	
   document	
   a,	
   compare	
   
them	
   to	
   document	
   b	
   
!

number of translation token pairs

number of tokens in a

!
    in	
   addi   on	
   to	
   dic   onary	
   transla   ons,	
   can	
   also	
   
count	
   iden   cal	
   strings	
   (numbers	
   and	
   names)	
   or	
   
near	
   iden   cal	
   strings	
   (cognates)

translation token pairs are considered within a    xed window (i.e., a distance-based
measure of co-occurrence is used).5 in addition to cross-lingual lexical matching, bits
   lters out candidate pairs that do not match well in terms of    le size, anchors (num-
bers, acronyms, and some named entities), or paragraph counts. using an english-
german bilingual lexicon of 117,793 entries, ma and liberman report 99.1% precision
and 97.1% recall on a hand-picked set of 600 documents (half in each language) con-
taining 240 translation pairs (as judged by humans). this technique yielded a 63mb

11

similarity(a, b) =

url	
   similarity
www.aecb.org/fra/publisher.asp?id=4090         
www.aecb.org/eng/publisher.asp?id=4090 
!
portal.unesco.org/fr/ev.php-url_id=3737  
portal.unesco.org/en/ev.php-url_id=3737 
what about translated urls? 
!
!
www.csps-efpc.gc.ca/about/dthe-dfva/ex_year_f.asp   
www.banqueducanada.ca/2012/04/discours/vieillir-
www.csps-efpc.gc.ca/about/dthe-dfva/ex_year_e.asp 
en-beaute-inevitable-evolution/ 
!
www.bankofcanada.ca/2012/04/speeches/aging-
www.ecml.at/edl/detailsprint.asp?l=f&e=2406    
gracefully-canadas-inevitable/
www.ecml.at/edl/detailsprint.asp?l=e&e=2406 
!
www.rwanda-botschaft.de/embassy3/pages/
341763a3c5e7f86ced395a8f0e32b8d7nw.php?
lg=fr&src=ns0000501151840&nid=44&diflg=nodif     
www.rwanda-botschaft.de/embassy3/pages/
341763a3c5e7f86ced395a8f0e32b8d7nw.php?

12

sites	
   with	
   translated	
   content

93236 rparticle.web-p.cisti.nrc.ca 
53973 www.ec.gc.ca 
52318 www.hc-sc.gc.ca 
45118 portal.unesco.org 
42737 www.cra-arc.gc.ca 
34617 www.dfo-mpo.gc.ca 
29445 www.canadianheritage.gc.ca 
28170 www.idrc.ca 
26823 www.agr.gc.ca 
21255 www.dfait-maeci.gc.ca 
19827 www.forces.gc.ca 
16922 www.ic.gc.ca 
16492 www.ceaa-acee.gc.ca 
16289 www.gg.ca 
15002 www.canadianencyclopedia.ca

!
14380 www2.parl.gc.ca 
14089 www.fin.gc.ca 
13706 www.aecb.org 
13264 www.cihr-irsc.gc.ca 
12161 www.cprn.org 
12145 www.civilisations.ca 
11632 www.cbsa.gc.ca 
11632 www.cbsa-asfc.gc.ca 
11005 www.hockeycanada.ca 
10382 www.crr.ca 
10338 www.commonlaw.uottawa.ca 
10150 www.ourroots.ca 
9224 www.cws-scf.ec.gc.ca 
8440 www.elections.ca 
8099 www.collectionscanada.gc.ca

13

web	
   crawling	
   101

    mirror	
   web	
   sites	
   
    extract	
   text	
   page	
   contents	
   
    perform	
   language	
   id	
   
    segment	
   into	
   sentences	
   
    align	
   document	
   pairs	
   
    align	
   sentences	
   
    remove	
   duplicates

14

mirror	
   web	
   sites

    we	
   would	
   like	
   to	
   crawl	
   the	
   web,	
   saving	
   pages	
   to	
   
extract	
   translated	
   documents	
   from	
   
    useful	
   cross-     pladorm	
   gnu	
   u   lity	
   called	
   wget	
   
    basic	
   usage	
   to	
   download	
   a	
   single	
      le: 
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   wget http://europa.eu/ 
    download	
   an	
   en   re	
   web	
   site,	
   preserving	
   
directory	
   structures:	
    
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   wget --mirror http://europa.eu/

15

no	
   robots

there is a protocol that 
web sites use to instruct 
search engines and 
other web crawlers not 
to index certain pages.   
!
sites contain a file called 
robots.txt that indicates 
who is allowed to look at 
what.

16

that   s	
   robo-     prejudice!

    wget	
   lets	
   you	
   ignore	
   this	
   protocol: 
   wget -robots=off --mirror http://akhbarlive.com/ 
    some	
   sites	
   will	
   block	
   wget	
   directly,	
   you	
   can	
   
pretend	
   to	
   be	
   some	
   other	
   browser: 
   wget -robots=off --mirror  -u "mozilla/5.0 (compatible; 
konqueror/3.2; linux)"  http://akhbarlive.com	
   
    don   t	
   do	
   this.	
   but	
   if	
   you	
   do,	
   please	
   do	
   this	
   too: 
   wget --wait=5 --random-wait --limit-rate=512k --
timeout=5  -robots=off --mirror  -u "mozilla/5.0 
(compatible; konqueror/3.2; linux)"  http://akhbarlive.com

17

extract	
   text	
   content

    for	
   bilingual	
   parallel	
   corpora,	
   we	
   really	
   only	
   care	
   
about	
   the	
   text.	
   	
   html	
   markup	
   will	
   mess	
   us	
   up.	
   
    convert	
   web	
   pages	
   to	
   text	
   (surprisingly	
   not	
   easy)	
   
    i	
   use	
   two	
   programs	
   

   	
   apple   s	
   textu   l	
   for	
   html	
   and	
   word	
   
   	
   xpdf	
   for	
   pdf

18

perform	
   language	
   id

    how	
   do	
   we	
   know	
   that	
   a	
   page	
   is	
   wrinen	
   in	
   the	
   
language	
   that	
   we	
   are	
   expec   ng?	
   
    html	
      meta   	
   tag	
   with	
   iso	
   639	
   2-     lener	
   language	
   
codes: 
<meta http-equiv="content-language" content="en">   
<meta http-equiv="content-language" content="fr">	
   
    this	
   meta-     data	
   is	
   oken	
   missing	
   or	
   in	
   accurate	
   
    sta   s   cal	
   nlp	
   to	
   the	
   rescue!

19

sta   s   cal	
   language	
   id

    intui   on:	
   some	
   character	
   strings	
   are	
   more	
   
probable	
   in	
   one	
   language	
   than	
   in	
   others

20

languagechar	
   sequencedutchvndenglisheryfrencheuxgaelicmhgermanderitaliancchiportugueseseuserbo-     croatljspanishirexamined grows, he reports 99,9% accuracy for 500 bytes of text to be classi   ed. the

b. now the id203 that a string s has been produced by the language model a can

advantages of statistical methods are manifold. no linguistic knowledge is needed to

be computed as follows:

dunning	
   (1994)

perform identi   cation and the input can be character based. this way the approach can

also be used for documents written in languages which cannot be easily tokenized and no

preprocessing is needed. i will come back to this point in section 6.

p(s | a) = p(s1 . . . sk | a)

p(si | si   k . . . si   k | a)

k

nyi=k+1

(dunning 94) reports that it is better to compare logarithms of the conditional probabil-

ities in order to avoid numeric under   ow. rearranging equation 4 gives us the following

formula to calculate the id203 of string s given the language model a.

log p(s | a) = xw1...wk+1   s

t (w1 . . . wk+1, s)logp(wk+1 | w1 . . . wk | a)

where t (w1 . . . wk+1, s) stands for the number of times that the k+1 gram w1 . . . wk+1

21

segment	
   into	
   sentences

    but	
   prof.	
   callison-     burch,	
   yahoo!	
   answers.com	
   tells	
   
me	
   that	
   this	
   is	
   a	
   99.66%	
   of	
   the	
      me	
   this	
   is	
   super	
   
easy	
   to	
   do... 
but	
   prof. 
callison-     burch,	
   yahoo! 
answers. 
com	
   tells	
   me	
   that	
   this	
   is	
   a	
   99. 
66%	
   of	
   the	
      me	
   this	
   is	
   super	
   easy	
   to	
   do. 
. 
.

22

sentence	
   segmenters

    nltk	
   has	
   one	
   called	
   punkt	
   that	
   is	
   trainable	
   to	
   
other	
   languages	
   
    download	
   several	
   from	
   the	
   wmt	
   workshops	
   

   	
   hnp://statmt.org/wmt08/scripts.tgz

23

align	
   document	
   pairs

    write	
   a	
   regular	
   expression	
   to	
      nd	
   pairs	
   of	
   urls	
   
that	
   are	
   equivalent	
   (s/_e/_f/)	
   and	
   see	
   if	
   there	
   are	
   
matching	
      les	
   from	
   your	
   crawl	
   
    use	
   link	
   structure	
   across	
   pages	
   with	
   the	
   strand	
   
trick	
   
    validate	
   that	
   the	
   document	
   pairs	
   are	
   plausible

24

align	
   sentences

    aker	
   we	
   have	
   iden      ed	
   parallel	
   documents	
   we	
   
need	
   to	
   align	
   the	
   sentences	
   within	
   them	
   
    this	
   is	
   not	
   straighdorward	
   because	
   human	
   
translators	
   do	
   not	
   always	
   translate	
   things	
   in	
   a	
   1-     
to-     1	
   fashion	
   
   	
   sentences	
   tend	
   to	
   be	
   translated	
   in	
   same	
   order	
   linear	
   
   	
   can	
   join	
   two	
   sentences	
   into	
   one	
   
   	
   can	
   split	
   one	
   sentence	
   into	
   two	
   
   	
   can	
   omit	
   a	
   sentence	
   (by	
   mistake)	
   
   	
   can	
   add	
   a	
   sentence	
   (for	
   elabora   on)

25

sentence	
   alignment

    use	
   dynamic	
   programming	
   to	
      nd	
   the	
   best	
   
alignment	
   between	
   sentences	
   in	
   a	
   document	
   
   	
   use	
   sentence	
   lengths	
   in	
   absence	
   of	
   other	
   info	
   
   	
   use	
   bilingual	
   dic   onaries	
   to	
   score	
   alignments	
   
   	
   use	
   model-     1	
   probabili   es	
   to	
   score	
   alignments	
   
    open	
   source	
   tool	
   from	
   bob	
   moore: 
hnp://research.microsok.com/en-     us/downloads/
aafd5dcf-     4dcc-     49b2-     8a22-     f7055113e656/

26

remove	
   duplicates

    with	
   large	
   scale	
   crawls,	
   there	
   are	
   oken	
   duplicates	
   at	
   
page	
   level	
   or	
   sub-     page	
   level	
   
   with	
   www.	
   pre   x	
   and	
   without	
   
   printable	
   versions	
   of	
   ar   cles	
   and	
   regular	
   versions	
   
   template	
   text	
   like	
   budgets	
   that	
   vary	
   only	
   in	
   $	
   amount	
   
   naviga   on	
   gets	
   replicated	
   across	
   an	
   en   re	
   site	
   
   remove	
   text	
   that	
   is	
   lek	
   untranslated	
   

    we	
   would	
   like	
   to	
   remove	
   duplicate	
   pages,	
   or	
   bener	
   
yet,	
   duplicate	
   sentences	
   
    problem:	
   too	
   much	
   data	
   to	
   store	
   in	
   a	
   hashtable/
hashset	
   and	
   check	
   strings	
   against

27

28

lossy	
   data	
   structures

    lossy	
   data	
   structures	
   like	
   bloom	
   filters	
   are	
   a	
   
poten   al	
   solu   on	
   
    bloom	
   filters	
   allow	
   you	
   to	
   test	
   for	
   set	
   
membership	
   
    instead	
   of	
   storing	
   the	
   object	
   itself	
   (string)	
   they	
   
store	
   a	
   highly	
   compressed	
   bit	
   signature	
   	
   
    one	
   tailed	
   error:	
   never	
   have	
   false	
   nega   ves,	
   
have	
   false	
   posi   ves	
   with	
   some	
   small,	
   
quan      able	
   id203	
   

29

harves   ng	
   data	
   from	
   the	
   web

    mirror	
   web	
   sites	
   
    extract	
   text	
   page	
   contents	
   
    perform	
   language	
   id	
   
    segment	
   into	
   sentences	
   
    align	
   document	
   pairs	
   
    align	
   sentences	
   
    remove	
   duplicates	
   
    ...	
   pro   t!

30

what	
   i	
   did

1000m

50m

european 
parliament

french-english  
10^9 word webcrawl

31

what	
   google	
   does
large scale parallel document mining 
for machine translation   

jakob uszkoreit, jay ponte, ashok popat, moshe dubiner

2.5 billion general web pages

    czech, english, french, german, hungarian and spanish

1.5 million ocred public-domain books

    english, french and a few spanish volumes


32

how	
   is	
   this	
   di   erent?

    how	
   is	
   the	
   google	
   set-     up	
   di   erent	
   from	
   mine?	
   
    what	
   resources	
   and	
   data	
   do	
   they	
   have	
   that	
   i	
   
don   t?	
   
    how	
   do	
   you	
   think	
   this	
   might	
   change	
   their	
   
strategy?	
   
!
    discuss	
   with	
   your	
   neighbor.

33

high	
   level	
   strategy

    document	
   transla   on	
   pairs	
   are	
   simply	
   near-     
duplicates,	
   albeit	
   annoyingly	
   in	
   di   erent	
   
languages	
   
    use	
   machine	
   transla   on	
   system	
   to	
   factor	
   out	
   
di   erences	
   in	
   language	
   and	
   apply	
   ir-     inspired	
   
near	
   duplicate	
   detec   on	
   techniques	
   
    pick-     out	
   small	
   candidate	
   sets	
   of	
   documents	
   
sharing	
   a	
   few	
   rare	
   matching	
   features	
   
    score	
   all	
   pairs	
   of	
   documents	
   in	
   every	
   candidate	
   
set	
   using	
   full	
   features	
   

34

step	
   1:	
   transla   on

    translate	
   all	
   input	
   documents	
   into	
   a	
   single	
   
language	
   (e.g.	
   english)	
   
    transla   on	
   quality	
   has	
   only	
   limited	
   e   ect	
   on	
   data	
   
quality	
   
    we   ll	
   see	
   that	
   later	
   in	
   numbers	
   
    preprocess	
   transla   ons	
   by	
   removing	
   stopwords	
   
and	
      boilerplate   	
   text

35

step	
   2:	
   feature	
   extrac   on

    extract	
   2	
   types	
   of	
   features	
   from	
   translated	
   documents	
   
    matching	
   features	
   such	
   that	
   

   every	
   transla   on	
   pair	
   is	
   likely	
   to	
   have	
   some	
   of	
   these	
   features	
   
in	
   common	
   
   any	
   given	
   feature	
   is	
   unlikely	
   to	
   be	
   shared	
   by	
   many	
   documents	
   
   they	
   use:	
   5-     grams	
   
    scoring	
   features	
   

   with	
   higher	
   overlap	
   between	
   the	
   contents	
   of	
   two	
   transla   ons	
   
   without	
   frequency	
   constraints	
   
   they	
   use:	
   bigrams

36

step	
   2:	
   feature	
   extrac   on

    generate	
   two	
   indexes	
   
    inverted	
   index	
   with	
   every	
   n-     gram	
   lis   ng	
   all	
   
document	
   ids	
   with	
   that	
   n-     gram	
   
    forward	
   index	
   with	
   the	
   set	
   of	
   scoring	
   n-     grams	
   for	
   
each	
   document	
   
    (embarrassingly	
   parallel	
   task)

37

step	
   3:	
   prune	
   indexes

    discard	
   matching	
   n-     grams	
   from	
   inverted	
   index	
   
   that	
   are	
   shared	
   by	
   more	
   than	
   a	
   few	
   (50)	
   documents	
   
   that	
   do	
   not	
   occur	
   in	
   more	
   than	
   one	
   language	
   

    e   cient	
   opera   on	
   on	
   inverted	
   index	
   
    in	
   parallel,	
   annotate	
   every	
   occurrence	
   of	
   each	
   scoring	
   
n-     gram	
   in	
   the	
   forward	
   index	
   with	
   global	
   informa   on	
   
from	
   the	
   inverted	
   index	
   
   frequency	
   
   number	
   of	
   original	
   languages	
   
   prune	
   very	
   frequent	
   scoring	
   n-     grams	
   (>	
   100,000	
   occurrences)	
   
   prune	
   scoring	
   n-     grams	
   that	
   occur	
   only	
   in	
   one	
   language38

step	
   4:	
   pairwise	
   scoring

    get	
   all	
   pairs	
   of	
   document	
   ids	
   that	
   

   share	
   a	
   given	
   minimum	
   number	
   of	
   matching	
   n-     grams	
   
   have	
   similar	
   lengths	
   
   are	
   in	
   two	
   di   erent,	
   original	
   languages	
   

    since	
   frequent	
   n-     grams	
   have	
   been	
   discarded,	
   this	
   
generates	
   rela   vely	
   few	
   candidate	
   pairings	
   and	
   
prevents	
   n2	
   explosion	
   of	
   comparisons	
   
    gather	
   all	
   candidate	
   pairs	
   for	
   each	
   document	
   id

39

step	
   4:	
   pairwise	
   scoring

    score	
   candidate	
   pairings	
   and	
   genera   ng	
   one	
   n-     
best	
   list	
   per	
   document,	
   per	
   language	
   
   cosine	
   similarity	
   between	
   idf	
   n-     gram	
   vectors	
   

    further	
      lter	
   pairings	
   by	
   looking	
   at	
   rela   ve	
   order	
   
of	
   shared	
   n-     grams	
   
    (again	
   straighdorward	
   to	
   parallelize	
   -     -     	
   google	
   
loves	
   that!)

40

final	
   steps

    discard	
   pairings	
   with	
   scores	
   below	
   a	
   threshold	
   
    discard	
   pairings	
   that	
   are	
   not	
   symmetric	
   
   document	
   a	
   is	
   required	
   to	
   be	
   in	
   n-     best	
   list	
   of	
   
document	
   b	
   and	
   vice-     versa	
   

    sentence-     align	
   the	
   original	
   documents	
   using	
   a	
   
standard	
   dynamic	
   programming	
   algorithm	
   
    do	
   lang	
   id	
   and	
   discard	
   sentence	
   pairs	
   that	
   are	
   
not	
   detected	
   to	
   be	
   in	
   two	
   di   erent	
   languages	
   
    discard	
   those	
   that	
   with	
   low	
   ibm	
   model	
   1	
   probs

41

number of words of mined english-foreign parallel text

!
!
!

on the web data set, the system

    extracts 430 billion distinct 5-grams

    stores 500 billion bigram occurrences in forward index

    but performs less than 50 billion pairwise comparisons

takes less than 24h on a cluster of 2,000 state-of-the-art cpus

42

baselinebookswebczech27.5m-271.9mfrench479.8m228.5m4,914.3mgerman54.2m-3,787.6mhungarian26.9m-198.9mspanish441.0m15.0m4,846.8m   
   
how	
   much	
   data	
   did	
   they	
   get?

    number	
   of	
   words	
   of	
   mined	
   english-     x	
   parallel	
   text	
   
!

!

!

!
    on	
   the	
   web	
   data	
   set,	
   the	
   system	
   
   extracts	
   430	
   billion	
   dis   nct	
   5-     grams	
   
   stores	
   500	
   billion	
   bigram	
   occurrences	
   in	
   forward	
   index	
   
   but	
   performs	
   less	
   than	
   50	
   billion	
   pairwise	
   comparisons	
   

    takes	
   less	
   than	
   24h	
   on	
   a	
   cluster	
   of	
   2,000	
   cpus43

baselinebookswebczech27.5m-271.9mfrench479.8m228.5m4,914.3mgerman54.2m-3,787.6mhungarian26.9m-198.9mspanish441.0m15.0m4,846.8mhow	
   much	
   did	
   it	
   improve	
   their	
   mt?

test set 1

test set 2

baseline+books+webczech english16.46-23.25 (+6.76)german english20.03-23.35 (+3.32)hungarian english11.02-14.68 (+3.66)french english26.3927.15 (+0.76)28.34 (+1.95)spanish english26.8827.16 (+0.28)28.50 (+1.62)baseline+books+webczech english21.59-29.26 (+7.67)german english27.99-32.35 (+4.36)french english34.2634.73 (+0.47)36.65 (+2.39)spanish english43.6744.07 (+0.40)46.21 (+2.54)google   s	
   approach	
   is	
   great!

    google   s	
   approach	
   is	
   computa   onal	
   e   cient	
   and	
   
is	
   embarrassingly	
   simple	
   to	
   parallelize	
   
    generalizes	
   across	
   di   erent	
   types	
   of	
   documents	
   
    does	
   not	
   require	
   presence	
   of	
   any	
   metadata	
   or	
   
document	
   structure	
   
    it	
   employs	
   many	
   simple	
   queries	
   (matching	
   n-     
grams)	
   
    it	
   has	
   been	
   applied	
   to	
   truly	
   web-     scale	
   input	
   data	
   
    but	
   there	
   is	
   a	
   problem...

45

problem:	
   everyone	
   loves	
   google!

    there   s	
   a	
   problem:	
   google	
   translate	
   is	
   too	
   good	
   
    everyone	
   is	
   using	
   it	
   to	
   translate	
   their	
   web	
   sites	
   
!
    ...	
   so	
   google	
   ends	
   up	
   harves   ng	
   its	
   own	
   
transla   ons	
   as	
   parallel	
   corpora	
   to	
   train	
   its	
   
system!	
   
    when	
   they	
   train	
   a	
   new	
   version	
   of	
   the	
   system	
   it	
   
reverts	
   back	
   to	
   behaving	
   like	
   the	
   old	
   version

46

solu   on:	
   digital	
   watermarking

47

watermarking	
   smt	
   output
watermarking the output of id170 
with an application in id151
ashish venugopal, jakob uszkoreit, david talbot, franz j. och, juri ganitkevitch

   back-of-the-envelope    study:  

corpora identified by uskzoreit et al 2010 

pages using translate plugins to serve content in multiple languages

\

48

language pair% in set / all identifiedtagalog-english50.6%hindi-english44.5%galician-english41.9%task:	
   iden   fy	
   one   s	
   own	
   mt	
   output

assumption: each translation output has k relatively similar alternatives

selected from:

q

...   dk(q) 

intuition: rather than simply selecting the    best    translation according to the 
model, systematically select alternative results such that we can identify 
them.

49

watermarking	
   selec   on

r0 =

argmax
r 2 dk(q)

w(r, dk(q), h)

    r:	
   the	
   machine	
   translated	
   output	
   sentence	
   
    h:	
   a	
   random	
   hash	
   func   on	
   
    w:	
   a	
   selector	
   func   on	
   to	
   choose	
   from	
   the	
   set	
   of	
   k	
   
alterna   ves

50

watermarking	
   evalua   on

    false	
   posi   ve	
   rate:	
   how	
   oken	
   are	
   non-     
watermarked	
   collec   ons	
   falsely	
   iden      ed	
   as	
   
watermarked	
   
    recall	
   rate:	
   how	
   oken	
   watermarked	
   collec   ons	
   
are	
   correctly	
   iden      ed	
   as	
   watermarked	
   
    quality	
   degrada   on:	
   how	
   does	
   the	
   selected	
   
transla   on	
   di   er	
   from	
   best	
   transla   on	
   under	
   
id7?

51

random	
   hashing	
   

h

h

h

h

010011010111100100

001001111010110010

111000011010110000

111000011010110000

cn

q1
q2

qn

a good h produces independent bits, implying the number of #1s:

x    binomial(p = 0.5, n = |h(cn)|)

52

random	
   hashing

h

111000011010110000

cn

q1
q2

qn

null hypothesis: an un-marked collection would generate bit 
sequences where #1s follows:

x    binomial(p = 0.5, n = |h(cn)|)

53

systema   cally	
   selec   ng	
   improbable	
   results

q

...   dk(q) 

0011...1001
1111...1101
0011...1001

improbable result 
lots more 1s.

54

evalua   on:	
   false	
   posi   ve	
   rates

id7 loss can be held to -0.2 for most languages

55

languagefalse positive rate: full sentences: %false positive rate: using 3-5 grams arabic2.45.8french1.87.5hindi5.63.5turkish5.56.2evalua   on:	
   bound	
   at	
   -     0.2	
   id7	
   loss

l
l

a
c
e
r

 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6

sentence-level
3-to-5 grams

arabic

french

hindi

turkish

56

watermarking	
   wrap	
   up

    on	
   several	
   languages	
   it	
   is	
   possible	
   to	
   achieve:	
   

   high	
   recall	
   rates	
   (over	
   80%)	
   
   low	
   false	
   posi   ve	
   rates	
   (5-     8%)	
   
   minimal	
   quality	
   degrada   on	
   (-     0.2	
   id7)	
   	
   
   allowing	
   for	
   local	
   edit	
   opera   ons	
   
!

    problem	
   solved!	
   	
   

57

ques   ons?

58

