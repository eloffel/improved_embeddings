course wrap up
machine translation 

lecture 26 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

course co-inventors

adam lopez
edinburgh

matt post

jhu

chris dyer

cmu

statistical machine 

translation

develop a statistical  model of translation that can be 

learned from data and used to predict the correct english 

translation of new foreign sentences.

lexical translation models

p(ei | fai)

  pid113(e | haus) =

0.696
0.279
0.014
0.011
0

if e = house
if e = home
if e = shell
if e = household
otherwise

8>>>>>><>>>>>>:

lexical translation models

p(e | f, m) = xa2[0,n]m

p(a | f, m)    

p(ei | fai)

myi=1

   

alignment translation | alignment

alignment

    alignments can be visualized in by drawing 
links between two sentences, and they are 
represented as vectors of positions:

a = (1, 2, 3, 4)>

reordering
    words may be reordered during 

translation.

a = (3, 4, 2, 1)>

one-to-many translation
    a source word may translate into more 

than one target word

a = (1, 2, 3, 4, 4)>

many-to-one translation
    more than one source word may not 
translate as a unit in lexical translation

1
das

2

haus

3

brach

4

zusammen

the
1

house

2

collapsed

3

a =???

a = (1, 2, (3, 4)>)> ?

ibm model 1

    simplest possible lexical translation model
    additional assumptions
    the m alignment decisions are independent
    the alignment distribution for each    is uniform 

ai

over all source words and null

for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

historical note

   why ibm models?

   the validity of a statistical (information 
some of us started to wonder in the mid 
theoretic) approach to mt has indeed been 
1980s whether our [id103] 
recognized, as the authors mention, by weaver 
methods could be successfully applied to 
as early as 1949. and was universally 
new    elds. bob mercer and i spent many of 
recognized as mistaken by 1950 (cf. hutchins, 
our after-lunch    periphery    walks 
mt     past, present, future, ellis horwood, 
discussing possible candidates. we soon 
1986, p. 30ff and references therein). the crude 
came up with two: machine translation and 
force of computers is not science. the paper is 
simply beyond the scope of coling.   

stock market modeling

fred jelinek
(1932-2010)

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

learning lexical 
translation models

p(e | f )

    how do we learn the parameters
       chicken and egg    problem
    if we had the alignments, we could 
    if we had parameters, we could    nd the 

estimate the parameters (id113)

most likely alignments

you implemented your 

own word aligner

phrase-based translation
    what are the atomic units?
    lexical translation: words
    phrase-based translation: phrases
    bene   ts
    many-to-many translation
    use of local context in translation
    standard model used by google, microsoft ...

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

f = morgen fliege ich nach baltimore zur konferenz

we can then marginalize to get p(f|e):

p(f | e)

e =

tomorrow

i

will fly

to the conference

in baltimore

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

morgen

fliege ich nach baltimore zur konferenz

we can then marginalize to get p(f|e):

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

morgen

fliege ich nach baltimore zur konferenz

we can then marginalize to get p(f|e):

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow)

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

fliege

morgen

we can then marginalize to get p(f|e):

ich nach baltimore zur konferenz

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow) x p(   iege|will    y)

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

fliege

ich

morgen

we can then marginalize to get p(f|e):

nach baltimore zur konferenz

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow) x p(   iege|will    y)

x p(ich|i)

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

nach baltimore

fliege

ich

morgen

we can then marginalize to get p(f|e):

zur konferenz
p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow) x p(   iege|will    y)

x p(ich|i)

x ... 

phrase-based translation

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

marginalize to get p(f|e):

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

p(f | e)

extracting phrase pairs

   
   

                       
   

       
   

   
   

   
   

australia
is
one
of
the
few
countries
that
have
diplomatic
relations
with
north
korea

      ,  australia
   , is
      , one of
      , few
      , countries
   , have
      , diplomatic relations
   , with
   , north
   , korea
         ,  australia is
             ,  few countries
         ,  have diplomatic relations
      ,  with north
      ,  north korea
                ,  the few countries that
         ,  with north korea
                      ,  one of the the few 
countries that
                   ,  have diplomatic 
relations with north korea
                          ,  the few countries 
that have diplomatic relations

phrase tables

f

das thema

es gibt

morgen

   iege ich

e

the issue
the point
the subject
the thema
there is
there are
tomorrow
will i    y
will    y
i will    y

p(f | e)
0.41
0.72
0.47
0.99
0.96
0.72
0.9
0.63
0.17
0.13

phrases

constituents

    contiguous strings of words
    phrases are not necessarily syntactic 
    usually have maximum limits
    phrases subsume words (individual words 

are phrases of length 1)

linguistic phrases

    model is not limited to linguistic phrases   
    non-constituent phrases are useful

(nps, vps, pps, cps...)

es gibt    there is | there are

syntax hurts

koehn et al (2003)

ibm model 4
pbmt
pbmt w/syntactic phrases

10k

20k

40k

80k

160k

320k

training corpus size

e
r
o
c
s
 
u
e
l
b

28.0

25.5

23.0

20.5

18.0

182022222325212425262728182023242526decoding: find best path

decoding

er

geht

ja

nicht

nach

hause

he

are

it

yes

goes

home

does not

home

go

to

backtrack from highest scoring complete hypothesis

decoding

stacks

goes

does not

he

are

it

yes

no word
translated

one word
translated

two words
translated

three words
translated

    hypothesis expansion in a stack decoder

stack decoding algorithm

decoding

1: place empty hypothesis into stack 0
2: for all stacks 0...n   1 do

for all hypotheses in stack do

3:

4:

5:

6:

7:

8:

9:

10:

11:

for all translation options do

if applicable then

create new hypothesis
place in stack
recombine with existing hypothesis if possible
prune stack if too big

end if
end for

end for

12:
13: end for

decoding complexity

    finding the best hypothesis is np-hard
    even with no language model, there are 
    solution 1: limit reordering
    solution 2: (lossy) pruning

an exponential number of states!

search errors

the search space

(like in id67)

    we are using a heuristic search to prune 
    there are no guarantees of admissibility 
    we may therefore prune out a partial 
hypothesis that would have lead to the 
most probable translation, if we hadn   t 
pruned it early on

future cost estimation

cost estimates from translation options

the   tourism  initiative addresses  this    for     the       rst   time

-1.0

-2.0

-1.5

-2.4

-1.4

-1.0

-1.0

-1.9

-1.6

-4.0

-2.5

-2.2

-1.3

-2.4

-2.7

-2.3

-2.3

-2.3

cost of cheapest translation options for each input span (log-probabilities)

cost estimates for all spans

future cost estimation

    compute cost estimate for all contiguous spans by combining cheapest options

future cost estimate for n words (from    rst)

   rst
word
the

7
-9.6
-9.6
-7.6

1
-1.0
tourism -2.0
initiative
-1.5
addresses
-2.4
-1.4
-1.0
-1.0
-1.9
combining score and future cost
-1.6

2
-3.0
-3.5
-3.9
-3.8
-2.4
-1.3
-2.2
-2.4

3
-4.5
-5.9
-5.3
-4.8
-2.7
-2.3
-2.3

4
-6.9
-7.3
-6.3
-5.1
-3.7
-2.3

this
for
the
   rst
time

5
-8.3
-8.3
-6.6
-6.1
-3.7

6
-9.3
-8.6
-7.6
-6.1

9

-10.6

8

-10.6
-9.6

    function words cheaper (the: -1.0) than content words (tourism -2.0)
    common phrases cheaper (for the    rst time: -2.3)
than unusual ones (tourism initiative addresses: -5.9)

the tourism initiative
die touristische 

the    rst time

-6.1 +

-9.3 +

-9.1 +

-9.3

-6.9

-2.2

-6.1

das erste mal
tm:-0.56,lm:-2.81
d:-0.74. all:-4.11 

-4.11

=

-13.41

this for ... time
f  r diese zeit
tm:-0.82,lm:-2.98
d:-1.06. all:-4.86 

initiative

-5.88

=

tm:-1.21,lm:-4.67

-11.98

chapter 6: decoding

d:0, all:-5.88

-4.86

=

-13.96

28

(nps, vps, pps, cps...)

linguistic phrases
    model is not limited to linguistic phrases   
    non-constituent phrases are useful
es gibt    there is | there are
    is a    good    phrase more likely to be   
     [p np]      or        [governor  p]   
why? how would you    gure this out?

you implemented your 

own phrase-based 

decoder

evaluating translation 

quality

    why do we want to do it?
    want to rank systems
    want to evaluate incremental changes
    what to make scienti   c claims

goals for 

automatic evaluation
    no cost evaluation for incremental changes
    ability to rank systems
    ability to identify which sentences we're 
doing poorly on,  and categorize errors
    correlation with human judgments
    interpretability of the score

methodology

    comparison against reference translations
    intuition: closer we get to human 
translations, the better we're doing
    can   t use wer like in id103
    this shows how easy it is to recognize speech
    this shows how easy it is to wreck a nice 

beach

problems with wer

    in machine translation there can be many 

possible (and equally valid) ways of 
translating a sentence

spoken word

    this shows how easy it is to recognize speech
    it illustrates how simple it is to transcribe the 
    clauses can move around
    this shows that recognizing speech is easy

id7

    bilingual evaluation understudy
    uses multiple reference translations
    look for id165s that occur anywhere in 

the sentence

multiple references

ref 1 orejuela appeared calm as he was led to the american plane 

which will take him to miami, florida.

ref 2 orejuela appeared calm while being escorted to the plane that 

would take him to miami, florida.

ref 3 orejuela appeared calm as he was being led to the american 

plane that was to carry him to miami in florida.

ref 4 orejuela seemed quite calm as he was being led to the american 

plane that would take him to miami in florida.

in order to match higher order id165s.

id165 precision

papineni et al. (2002) de   ne id7 in terms of id165 precision. they calculate an
id165 precision score, pn, for each id165 length by summing over the matches for
every hypothesis sentence s in the complete corpus c as:

pn =

  s2c   ngram2scountmatched(ngram)

  s2c   ngram2scount(ngram)

id7   s id165 precision is modi   ed slightly to eliminate repetitions that occur across
sentences. for example, even though the bigram    to miami    is repeated across all four
reference translations in table 6.1, it is counted only once in a hypothesis translation.
these is referred to as clipped id165 precision.

american, florida, miami, orejuela, 
appeared, as, being, calm, carry, escorted, he, 
him, in, led, plane, quite, seemed, take, that, 
the, to, to, to, was , was, which, while, will, 
would, ,, .

1-gram precision = 15/18 

hyp

appeared calm when he was taken to the american 
plane , which will to miami , florida .

american plane, florida ., miami ,, miami 
in, orejuela appeared, orejuela seemed, 
appeared calm, as he, being escorted, being 
led, calm as, calm while, carry him, escorted 
to, he was, him to, in florida, led to, plane 
that, plane which, quite calm, seemed quite, 
take him, that was, that would, the american, 
the plane, to miami, to carry, to the, was 
being, was led, was to, which will, while 
being, will take, would take, , florida

2-gram precision = 10/17 

hyp

appeared calm when he was taken to the american 
plane , which will to miami , florida .

brevity penalty

bp

1.00

0.75

mt is longer

0.50

0.25

0.00

-50

-25

0

25

50

75

100

difference with effective reference length (%)

rank sentences

you have judged 25 sentences for wmt09 spanish-english news corpus, 427 sentences total taking 64.9 seconds per 
sentence.

manual evaluation

source: estos tejidos est  n analizados, transformados y congelados antes de ser almacenados en hema-
qu  bec, que gestiona tambi  n el   nico banco p  blico de sangre del cord  n umbilical en quebec.

reference: these tissues are analyzed, processed and frozen before being stored at h  ma-qu  bec, which 
manages also the only bank of placental blood in quebec.

translation
these weavings are analyzed, transformed and frozen before being 
stored in hema-quebec, that negotiates also the public only bank of 
blood of the umbilical cord in quebec.
these tissues analysed, processed and before frozen of stored in hema-
qu  bec, which also operates the only public bank umbilical cord blood 
in quebec.
these tissues are analyzed, processed and frozen before being stored in 
hema-qu  bec, which also manages the only public bank umbilical cord 
blood in quebec.
these tissues are analyzed, processed and frozen before being stored in 
hema-quebec, which also operates the only public bank of umbilical 
cord blood in quebec.
these fabrics are analyzed, are transformed and are frozen before being 
stored in hema-qu  bec, who manages also the only public bank of 
blood of the umbilical cord in quebec.
annotator: ccb task: wmt09 spanish-english news corpus
instructions: 

rank

1

best

1

best

1

best

1

best

1

best

2

2

2

2

2

3

3

3

3

3

4

4

4

4

4

5

worst

5

worst

5

worst

5

worst

5

worst

correlation with 
human judgments

6.2 segment-level metric analysis
we measured the metrics    segment-level scores with
the human rankings using kendall   s tau rank corre-
lation coef   cient. the reference was not included as
    kendall   s tau
an extra translation.

we calculated kendall   s tau as:
  = num concordant pairs - num discordant pairs

total pairs

where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,

you implemented your 
own evaluation metric

id87

e    = arg max

e

= arg max

e

= arg max

e

= arg max

e

= arg max

p(e | g)
p(g | e)   p(e)

p(g)

p(g | e)   p(e)
log p(g | e) + log p(e)

e    1
1 >
|{z}w>

   log p(g | e)
log p(e)  
}
{z
|

h(g,e)

id87

e    = arg max

e

= arg max

e

= arg max

e

= arg max

e

= arg max

p(e | g)
p(g | e)   p(e)

p(g)

p(g | e)   p(e)
log p(g | e) + log p(e)

e    1
1 >
|{z}w>

   log p(g | e)
log p(e)  
}
{z
|

h(g,e)
log-linear model

discriminative 

modeling 

    depart from generative modeling 
    goal: 
    directly optimize for translation 

performance by discriminating between 
good/bad translation

discriminative 

modeling 

    represent possible translations using a set 

of features h

of the translation

    each feature hi derives from one property 
    its feature weight wi indicates its relative 

importance

the noisy channel

-log p(g | e)

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)
improvement 1:

change      to    nd better translations

~w

~wg

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

improvement 2:

add dimensions to make points separable

~w

-log p(e)

you discriminatively 

re-ranked n-best 

translations

id151

linguisticsalgorithmsmachine learningformal language informationtheorythe	
   syntax	
   bet
every time i fire a linguist 
my performance goes up

    longstanding	
   debate	
   about	
   whether	
   linguis9c	
   
informa9on	
   can	
   help	
   sta9s9cal	
   transla9on	
   
    two	
   camps

syntax will improve 
translation

simpler data-driven 
models will always win

68

reordering model

values of    

1.00

0.75

0.50

0.25

y
t
i
l
i

b
a
b
o
r
p

0.00

0

1

  =
0.99
0.75
0.5
0.25
0.1

4

5

70

2

3

distance of move

lexicalized	
   reordering	
   model

l

i

e
v
e
w

i

e
t
l
l

o
s

n
a
m

d
n
u
g
r
f
u
a

s
e
n
e
s

i

s

l
i
f
o
r
p

k
o
o
b
e
c
a
nf

i

n
e
n
e
d
r
e
v

i

m: monotone (keep order)
s: swap order
d: become discontinuous

reordering features are 
id203 estimates of s, 
d, and m

how
much
should
you
charge
for
your
facebook
profile

m

m
m

m
m

d
d
s

d
d

71

lexicalized	
   reordering	
   table

    iden9cal	
   phrase	
   pairs	
   <f,e>	
   as	
   in	
   the	
   phrase	
   
transla9on	
   table	
   
    contains	
   values	
   for	
   p(monotone|e,f),	
   p(swap|e,f),	
   
p(discon9nuous|e,f)

transla5on p(m|e,f)

source
natuerlich of	
   course
natuerlich naturally
natuerlich of	
   course	
   ,
natuerlich ,	
   of	
   course	
   

0.52
0.42
0.5
0.27

p(s|e,f)
0.08
0.1
0.001
0.17

p(d|e,f)

0.4
0.48
0.499
0.56

72

empirically	
   berer

koehn et al, iwslt 2005

arabic

japanese

korean

chinese en-chinese

baseline

lexicalized reordering

73

60.0

45.0

30.0

15.0

0.0

16.638.642.347.650.915.234.635.745.149.9the	
   awful	
   german	
   language

   
 the germans have another kind 
of parenthesis, which they make 
by splitting a verb in two and 
putting half of it at the beginning 
of an exciting chapter and the 
other half at the end of it. 
can any one conceive of 
anything more confusing than 
that? these things are called 
   separable verbs.    the wider the 
two portions of one of them are 
spread apart, the better the 
author of the crime is pleased 
with his performance.   

mark twain

german	
   verbs

ich  werde  ihnen   den  report  aushaendigen . 
  i    will      to_you  the   report   pass_on          .

ich  werde  ihnen   die entsprechenden anmerkungen  aushaendigen .  
  i    will      to_you  the   corresponding  comments       pass_on          .

ich werde ihnen die entsprechenden anmerkungen am dienstag aushaendigen 
  i    will   to_you the   corresponding  comments      on  tuesday   pass_on

75

collins   	
   pre-     ordering	
   model

step 1: reorder the source language

ich  werde  ihnen   den  report  aushaendigen ,  
damit    sie   den  eventuell  uebernehmen  koennen  . 

ich  werde  aushaendigen ihnen   den  report  ,  
damit    sie  koennen uebernehmen den  eventuell   . 

(i will pass_on to_you the report, so_that you can adopt it perhaps .)

step 2: apply the phrase-based machine translation pipeline   
             to the reordered input.

76

clause	
   restructuring

rule 1:  verbs are initial in vps 
              within a vp, move the head to the initial position

s

vp-oc

...

pds-oa 

den 
that

adjd-mo 
eventuell 
perhaps

vinf-hd 
koennen 

can

vvinf-hd 

uebernehmen 

adopt

77

synchronous	
   context	
   free	
   grammars

    a	
   common	
   way	
   of	
   represen9ng	
   syntax	
   in	
   nlp	
   is	
   
through	
   context	
   free	
   grammars	
   
    synchronous	
   context	
   free	
   grammars	
   generate	
   pairs	
   
of	
   corresponding	
   strings	
   	
   
    can	
   be	
   used	
   to	
   describe	
   transla9on	
   and	
   re-     ordering	
   
between	
   languages	
   
    sid18s	
   translate	
   sentences	
   by	
   parsing	
   them

78

pp   

np   

hamd ansary

np   
na}b sdr

s   

vp   

vp   

v   
namzd

aux   
taa

p   
kylye
s   

vp   

vp   

np   

hamid ansari

aux   
was

v   
nominated

p   
for

pp   

np   

vice president

extrac9ng	
   syntac9c	
   rules

s

p
v

p
n

p
n
n

p
n

p
p

p
n

australia
is
one
of
the
few
countries
that
have
diplomatic
relations
with
north
korea

p
n

p
m
o
c

p
v

p
n

p
p

p
n

p
n

   
   

                       
   

       
   

   
   

   
   

vp                           , 
have diplomatic relations 
with north korea
np                            
                 , the few 
countries that have 
diplomatic relations with 
north korea
np     vp                  , 
the few countries that vp
np     vp     np,  
the np that vp

syntax	
   v.	
   no	
   syntax

id7 score on blind nist urdu-english test set

32.0

29.0

26.0

23.0

20.0

no syntax (hiero) syntax (samt)

best system

81

31.231.025.0state	
   of	
   the	
   art	
   urdu	
   results

all system scores on nist09 urdu-english constrained 

task 

31

25

pbmt(moses)

hiero baseline syntax

32.0

28.8

25.5

22.3

19.0

31312524242323232220joshua	
   decoder	
   

    an	
   open	
   source	
   decoder	
   
    uses	
   synchronous	
   context	
   
free	
   grammars	
   to	
   translate	
   
    implements	
   all	
   algorithms	
   
needed	
   for	
   transla9ng	
   with	
   
sid18s	
   
   grammar	
   extrac9on	
   (thrax!)	
   
   chart-     parsing	
   
   n-     gram	
   language	
   model	
   
integra9on	
   
   pruning,	
   and	
   k-     best	
   extrac9on

83

history	
   of	
   decoders
giza++ was an open source implementation of the ibm 
alignment models developed at the 1999 clsp summer 
workshop
pharaoh was a id125 decoder
for phrase-based id151 models

moses is an open source decoder

for phrase-based id151 models

joshua is an open source syntax based smt models

84

advanced topics

86

google   s	
   e   orts

    number	
   of	
   words	
   of	
   mined	
   english-     x	
   parallel	
   text	
   

czech
french
german
hungarian
spanish

baseline
27.5m
479.8m
54.2m
26.9m
441.0m

books
-
228.5m
-
-
15.0m

web
271.9m
4,914.3m
3,787.6m
198.9m
4,846.8m

    transla9on	
   improvements	
   

czech english
german english
french english
spanish english

baseline
21.59
27.99
34.26
43.67

+web
+books
29.26 (+7.67)
-
-
32.35 (+4.36)
34.73 (+0.47) 36.65 (+2.39)
44.07 (+0.40) 46.21 (+2.54)

avoiding	
   die9ng	
   to	
   
prevent	
   from	
      u

this	
   research	
   of	
   
american	
   scien9sts	
   
came	
   in	
   front	
   afer	
   
experimen9ng	
   on	
   
mice.

absten9on	
   from	
   
die9ng	
   in	
   order	
   to	
   
avoid	
   flu
this	
   research	
   from	
   
the	
   american	
   
scien9sts	
   have	
   come	
   
up	
   afer	
   the	
   
experiments	
   on	
   rats.

abstain	
   from	
   decrease	
   
ea9ng	
   in	
   order	
   to	
   
escape	
   from	
      ue
this	
   research	
   of	
   
american	
   scien9sts	
   
was	
   shown	
   afer	
   
many	
   experiments	
   
on	
   mouses.

experiments	
   proved	
   
that	
   mice	
   on	
   a	
   lower	
   
calorie	
   diet	
   had	
   
compara9vely	
   less	
   
ability	
   to	
      ght	
   the	
   
   u	
   virus.
research	
   has	
   proven	
   
this	
   old	
   myth	
   wrong	
   
that	
   its	
   berer	
   to	
   fast	
   
during	
   fever.

in	
   has	
   been	
   proven	
   
from	
   experiments	
   
that	
   rats	
   put	
   on	
   diet	
   
with	
   less	
   calories	
   
had	
   less	
   ability	
   to	
   
resist	
   the	
   flu	
   virus.
research	
   disproved	
   
the	
   old	
   axiom	
   that	
   "	
   
it	
   is	
   berer	
   to	
   fast	
   
during	
   fever"

it	
   was	
   proved	
   by	
   
experiments	
   the	
   low	
   
calories	
   eaters	
   
mouses	
   had	
   low	
   
defending	
   power	
   for	
   
   ue	
   in	
   ra9o.
the	
   research	
   proved	
   
this	
   old	
   talk	
   that	
   
decrease	
   ea9ng	
   is	
   
useful	
   in	
   fever.

in	
   order	
   to	
   be	
   safer	
   
from	
      u	
   quit	
   die9ng

according	
   to	
   the	
   
american	
   scien9st	
   
this	
   research	
   has	
   
come	
   out	
   afer	
   much	
   
experimenta9ons	
   on	
   
rats.
experimentaions	
   have	
   
proved	
   that	
   those	
   rats	
   
on	
   less	
   calories	
   diet	
   
have	
   developed	
   a	
   
tendency	
   of	
   not	
   
overcoming	
   the	
      u	
   
virus.
this	
   research	
   has	
   
proved	
   the	
   very	
   old	
   
saying	
   wrong	
   that	
   it	
   
is	
   good	
   to	
   starve	
   
while	
   in	
   fever.

88

avoiding	
   die9ng	
   to	
   
prevent	
   from	
      u

this	
   research	
   of	
   
american	
   scien9sts	
   
came	
   in	
   front	
   afer	
   
experimen9ng	
   on	
   
mice.

absten9on	
   from	
   
die9ng	
   in	
   order	
   to	
   
avoid	
   flu
this	
   research	
   from	
   
the	
   american	
   
scien9sts	
   have	
   come	
   
up	
   afer	
   the	
   
experiments	
   on	
   rats.

abstain	
   from	
   decrease	
   
ea9ng	
   in	
   order	
   to	
   
escape	
   from	
      ue
this	
   research	
   of	
   
american	
   scien9sts	
   
was	
   shown	
   afer	
   
many	
   experiments	
   
on	
   mouses.

experiments	
   proved	
   
that	
   mice	
   on	
   a	
   lower	
   
calorie	
   diet	
   had	
   
compara9vely	
   less	
   
ability	
   to	
      ght	
   the	
   
   u	
   virus.
research	
   has	
   proven	
   
this	
   old	
   myth	
   wrong	
   
that	
   its	
   berer	
   to	
   fast	
   
during	
   fever.

in	
   has	
   been	
   proven	
   
from	
   experiments	
   
that	
   rats	
   put	
   on	
   diet	
   
with	
   less	
   calories	
   
had	
   less	
   ability	
   to	
   
resist	
   the	
   flu	
   virus.
research	
   disproved	
   
the	
   old	
   axiom	
   that	
   "	
   
it	
   is	
   berer	
   to	
   fast	
   
during	
   fever"

it	
   was	
   proved	
   by	
   
experiments	
   the	
   low	
   
calories	
   eaters	
   
mouses	
   had	
   low	
   
defending	
   power	
   for	
   
   ue	
   in	
   ra9o.
the	
   research	
   proved	
   
this	
   old	
   talk	
   that	
   
decrease	
   ea9ng	
   is	
   
useful	
   in	
   fever.

in	
   order	
   to	
   be	
   safer	
   
from	
      u	
   quit	
   die9ng

according	
   to	
   the	
   
american	
   scien9st	
   
this	
   research	
   has	
   
come	
   out	
   afer	
   much	
   
experimenta9ons	
   on	
   
rats.
experimentaions	
   have	
   
proved	
   that	
   those	
   rats	
   
on	
   less	
   calories	
   diet	
   
have	
   developed	
   a	
   
tendency	
   of	
   not	
   
overcoming	
   the	
      u	
   
virus.
this	
   research	
   has	
   
proved	
   the	
   very	
   old	
   
saying	
   wrong	
   that	
   it	
   
is	
   good	
   to	
   starve	
   
while	
   in	
   fever.

89

avoiding	
   die9ng	
   to	
   
prevent	
   from	
      u

this	
   research	
   of	
   
american	
   scien9sts	
   
came	
   in	
   front	
   afer	
   
experimen9ng	
   on	
   
mice.

absten9on	
   from	
   
die9ng	
   in	
   order	
   to	
   
avoid	
   flu
this	
   research	
   from	
   
the	
   american	
   
scien9sts	
   have	
   come	
   
up	
   afer	
   the	
   
experiments	
   on	
   rats.

abstain	
   from	
   decrease	
   
ea9ng	
   in	
   order	
   to	
   
escape	
   from	
      ue
this	
   research	
   of	
   
american	
   scien9sts	
   
was	
   shown	
   afer	
   
many	
   experiments	
   
on	
   mouses.

experiments	
   proved	
   
that	
   mice	
   on	
   a	
   lower	
   
calorie	
   diet	
   had	
   
compara9vely	
   less	
   
ability	
   to	
      ght	
   the	
   
   u	
   virus.
research	
   has	
   proven	
   
this	
   old	
   myth	
   wrong	
   
that	
   its	
   berer	
   to	
   fast	
   
during	
   fever.

in	
   has	
   been	
   proven	
   
from	
   experiments	
   
that	
   rats	
   put	
   on	
   diet	
   
with	
   less	
   calories	
   
had	
   less	
   ability	
   to	
   
resist	
   the	
   flu	
   virus.
research	
   disproved	
   
the	
   old	
   axiom	
   that	
   "	
   
it	
   is	
   berer	
   to	
   fast	
   
during	
   fever"

it	
   was	
   proved	
   by	
   
experiments	
   the	
   low	
   
calories	
   eaters	
   
mouses	
   had	
   low	
   
defending	
   power	
   for	
   
   ue	
   in	
   ra9o.
the	
   research	
   proved	
   
this	
   old	
   talk	
   that	
   
decrease	
   ea9ng	
   is	
   
useful	
   in	
   fever.

in	
   order	
   to	
   be	
   safer	
   
from	
      u	
   quit	
   die9ng

according	
   to	
   the	
   
american	
   scien9st	
   
this	
   research	
   has	
   
come	
   out	
   afer	
   much	
   
experimenta9ons	
   on	
   
rats.
experimentaions	
   have	
   
proved	
   that	
   those	
   rats	
   
on	
   less	
   calories	
   diet	
   
have	
   developed	
   a	
   
tendency	
   of	
   not	
   
overcoming	
   the	
      u	
   
virus.
this	
   research	
   has	
   
proved	
   the	
   very	
   old	
   
saying	
   wrong	
   that	
   it	
   
is	
   good	
   to	
   starve	
   
while	
   in	
   fever.

90

professional	
   quality	
   from	
   non-     professionals

47

42

37

32

27

22

full details in zaidan and 
callison-burch (acl 2011a) 
& zaidan (phd thesis 2012)

 ldc


professional

translation


turker


translation w/o

quality control


turker


translation w/

qc model


best 


sentence

oracle


best 

turker

oracle


91

4144392842gathering	
   data	
   about	
   arabic	
   dialects

arabic has different varieties.  msa is the standardized form 
but there are many distinct regional dialects. 

maghrebi

levantine 

iraqi

egyptian 

gulf

other

92

examples	
   of	
   dialect	
   transla9on

dialect input msa system 

                                                 
egy 
                                            
                         !! 
                                              
egy 
                                                  
                                      
                                                 
                                                      

lev 

you are working 
for a declaration 
and not?
myself feel to see 
this image.

god you the 
atmosphere.

dialect 
system 
you are making 
the advertisement 
for him or what?
i wish to check on 
him after he saw 
this picture.
this is why the 
weather is so cool

reference

are you promoting 
it or what?!!

i want to be sure 
that he is fine after 
he saw the images.
this is why the 
weather is so cool

                                                   
lev 
                 

do you think 
about a joke long.

calm down we are 
kidding

calm down, we are 
only kidding

93

how	
   to	
   improve	
   machine	
   transla9on

30

25

20

15

10

5

0

y
t
i
l

a
u
q
 
n
o
i
t
a
s
n
a
r
t

l

    better models 

    more bilingual training data 

    eliminate the need for bitexts 

1

20000

40000

60000

82000

bilingual training data

94

          

scoring	
   transla9ons:	
   time

s
e
c
n
e
r
r
u
c
c
o

s
e
c
n
e
r
r
u
c
c
o

terrorist (en)
terrorista (es)

similar

terrorist (en)
riqueza (es)

dissimilar

time

96

scoring	
   transla9ons:	
   time

e  lica
wind
renewable
solar
sources
renewables
energy
energies
electricity
photovoltaic
grid

estambul
istanbul
erdogan
turkish
turkey
turks
ankara
membership
negotiations
undcp
talks

terrorista
terrorist
terrorism
terrorists
attacks
fight
attack
terror
acts
threat
september

vacuno
beef
cattle
bse
compulsory
meat
cows
veal
cow
labelling
papayannakis

97

id141

... 5 farmers were

thrown into jail

in ireland ...

... f  nf landwirte
...
oder wurden

festgenommen 
festgenommen

, weil
...
, gefoltert

...

...

or have been

imprisoned

,

tortured

...

many equivalent english 

expressions

thrown into jail

arrested 
detained 
imprisoned 
incarcerated 

jailed 

locked up 

be thrown in prison 
been thrown into jail 

being arrested 

in jail 

in prison 

put in prison for 

arrest 
cases 
custody 
maltreated 
owners 
protection 
thrown

taken into custody 
thrown into prison

were thrown into jail 

who are held in detention

natural language 
understanding

hypothesis

s

vp

np

np

nns

jj

illustrations insulting
were 
12
cd

sparked  

vb

by

s

riots
np

cd
jj
  
twelve
in denmark

pp

np
text

np

the prophet

np

np

vb
caused unrest

editorial

jj

cartoons
nns

that were

offensive

jj

to

np

np

vp

pp
  
muhammad

np

guest lecturers

liang huang

cuny

wei xu   
penn

matt post

jhu

will lewis

msr

christian buck

edinburgh

ken hea   eld
bloomberg

you guys!

leaderboard
hw 2 

hw 1 
aer

model score 

hw 0 
# correct

alias

okyurp
do_not_set_yo
urself_on_   re
   y
@jim
sogeking
lilies
direkt 
translation
stopitron
cloud9
etaoin shrdlu
   
mstag
dhrubeel
class the mt 
is.
aa   kins
aoc
sqq
kailoo   
ohayyy
john e. mason
baby 
eigensheep
mithrandir
chief relief
to   
luck
mogjuice
madan-mohan 
das

hw 3 
accuracy

hw 4 
id7

28.3847
0.548676
27.3337
0.540384
27.9741
0.544452
27.6029
0.547972
28.3220
0.556538
27.5001
0.560918
28.1127
0.541636
28.1132
0.532835
28.6879
0.544687
28.5747
0.539797
27.8708
0.533657
27.5254
0.527281
27.9579
0.562209
28.5456
0.544256
28.4572
0.561818
27.4141
0.553448
28.0958
0.555912
28.4231
0.557868
27.2392
0.538076
28.6509
0.540345
27.0876
0.550045
28.2648
0.560488
27.3528
0.556616
0.528533
28.2981
0.541714 27.1022
27.8330
0.540462
0.537333
23.6436

moses 


(o    the shelf):


-1286.92

     

-1211.39
-1224.20
-1228.65
-1229.52
-1230.32
-1237.72
-1238.51
-1239.82
-1242.32
-1242.62
-1243.78
-1244.77
-1245.83
-1246.62
-1249.20
-1254.07
-1254.24
-1257.08
-1263.84
-1269.91
-1270.60
-1274.59
-1279.60
-1283.94
-1295.99
-1298.83
-1299.82

10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

15.12
18.73
26.94
16.59
18.02
14.73
21.81
34.04
27.09
19.03
26.81
31.41
27.59
28.32
27.59
20.51
25.45
25.76
26.53
26.48
30.60
27.51
27.19
27.40
26.70
31.10
23.22

ibm research
what it takes to compete against top human jeopardy!  players
our analysis reveals the winner   s cloud

each dot     actual historical human jeopardy! games

winning human 
winning human 
performance
performance

grand champion 
grand champion 
human performance
human performance

computers?
not so good.

2007 qa computer system
2007 qa computer system

55

more confident
more confident

less confident
less confident

   2010 ibm corporation

ibm research

deepqa: incremental progress in answering precision 
on the jeopardy challenge: 6/2007-11/2010 

ibm watson

playing in the winners cloud

v0.8  11/10

v0.7  04/10

v0.6  10/09

v0.5  05/09

v0.4  12/08

v0.1  12/07

v0.3 08/08

v0.2  05/08

baseline 12/06

10

   2010 ibm corporation

your term projects did this.

    you de   ned a challenge problem
    you de   ned a scoring function that allowed 
    you and your classmates tried different 

you to plot your progress over time.

algorithms and developed different models 
to solve the problem.

what you can do to 

stay involved

    if you will be back at penn next year:
    take cis 530 - computation linguistics
    take cis 520 - machine learning
    do an independent research project 
    if you   re graduating, then stay in touch!

with me!

thanks!

stay	
   in	
   touch!	
   

email:	
   ccb@cis.upenn.edu	
   

twirer:	
   @ccb	
   

facebook:	
   chris.callisonburch

