human ranking of 
machine translation

matt post 
johns hopkins university 

university of pennsylvania 
april 9, 2015

some slides and ideas borrowed 
from adam lopez (edinburgh)

review

    in translation, human evaluations are what matter 

    but they are expensive to run 
    this holds up science! 

    the solution is automatic metrics 

    fast, cheap, (usually) easy to compute 
    deterministic

2

review

    automatic metrics produce a ranking 

    they are evaluated using correlation statistics 

against human judgments
outputs

metrics

system a

system b

system c

system d

id7

humans

ranking

a   
b, d 
c

b   
a 
d 
c

??

3

review

    the human judgments are the    gold standard    

    questions: 

1. how do we get this gold   

standard? 

2. how do we know it   s correct?

4

today

    how we produce the gold-standard ranking
    how we know it   s correct

5

at the end of this lecture   

    you should understand 

    how to rank with incomplete 
    how to evaluate truth claims in science 

    you might come away with 

    a desire to submit your metric to the wmt 

metrics task (deadline: may 25, 2015) 

    a desire to buy an xbox 
    a preference for simplicity

6

producing a ranking

    then, we take this data and produce a ranking 

    outline of the rest of the talk

human ranking methods

model selection

id91

7

goal

system g }

system a
system b
system c
system d
system e
system f

7. system e{

1. system c
2. system d
3. system a
4. system b
5. system g
6. system f

slide from adam lopez

8

goal

    produce a ranking of systems 

    there are many ways to do this: 
    reading comprehension tests 
    time spent on human post-editing 
    aggregating sentence-level judgments 

    this last one is what is used by the workshop on 
id151 (statmt.org/wmt15)

9

inherent problems
    translation is used for a range of tasks   

understanding 

the past

technical 
manuals

conversing

information

    what best (or suf   cient) means likely varies by person 

and situation

10

   
   
   
   
   
   
   
collecting data

    data: k systems translate an n-sentence document 

    we use human judges to compare translations of an 

input sentence and select whether    
the    rst is  better,    
  
  

   worse, or    
    equivalent to  the second 

 
 

 
 

    we use a large pool of judges

11

   
12

collecting data

c > a > b > d > e

c > a   a > b  b > d   d > e   
c > b   a > d  b > e 
c > d   a > e 
c > e

13

ten pairwise judgments

dataset

    this yields ternary-valued pairwise judgments of the 

following form   
judge    dredd    ranked onlineb > jhu on sent #74   
  
judge    judy    ranked uedin > uu on sent #1734   
judge    reinhold    ranked jhu > uu on sent #1   
judge    jay    ranked onlinea = uedin on sent #953   
   

14

   
the sample space

    how much data is there to collect?   

(number of ways to pick two systems)   
x (number of sentences) x (number of judges)
    for 10 systems there are 135k comparisons 
    for 20 systems, 570k 
    more with multiple judges 

    too much to collect, also wasteful; instead we 

sample

15

   
   
design of the wmt evaluation (2008-2011)

system a
system b
system c
system d
system e
system f
system g
reference =
while (evaluation period is not over):

1. reference
2. system c
3. system a, system f
4. system d

   {wmt raw data:

pairwise rankings 
reference     system a
reference     system c
reference     system d
reference     system f
system a     system c
system a     system d
system a     system f
system c     system d
system c     system f
system d     system f

 
 
 
 
 
 
 
 
 

    sample input sentence.
    sample    ve translators of it from systems     {reference}.
    sample a judge.
    receive set of pairwise judgments from the judge.

how much data do we collect?

of tens of 
millions 
possible

17

producing a ranking

    then, we take this data and produce a ranking 

    human ranking methods 

expected wins and variants

bayesian model (relative ability) 

trueskill   

18

expected wins (1)
    this most appealing and intuitive approach 

    de   ne wins(a), ties(a), and loses(a) as the 

number of times system a won, tied, or lost 

    score each system as follows   

score(a)	
   =	
   

wins(a)	
   +	
   ties(a)	
   

wins(a)	
   +	
   ties(a)	
   +	
   loses(a)

    now sort by scores

19

   
   
expected wins (2)

    do you see any problems with this?   

score(a)	
   =	
   

wins(a)	
   +	
   ties(a)	
   

wins(a)	
   +	
   ties(a)	
   +	
   loses(a)

    look at a judgments:   
one winner, one loser
one winner, one loser
one winner, one loser
two winners, no losers

judge    dredd    ranked onlineb > jhu on sent #74   
judge    judy    ranked uedin > uu on sent #1734   
judge    reinhold    ranked jhu > uu on sent #1   
judge    jay    ranked onlinea = uedin on sent #953

20

   
   
   
   
expected wins (3)

    a system is rewarded as much for a tie as for a win 

       and most systems are variations of    
the same underlying architecture, data 

    new formula: throw away ties   

score(a)	
   =	
   

wins(a)

wins(a)	
   +	
   loses(a)

    wait: is this better?

a grain of salt for the wmt manual evaluation (bojar et al., 2012)

21

   
   
   
expected wins (4)

    problem 2: the luck of the draw   

aggregation over

different sets of inputs 
different competitors 

different judges

    consider a case where in reality b > c, but  

    b gets compared to a bunch of good systems 
    c gets compared to a bunch of bad systems 
    we could get score(c) > score(b

22

   
   
   
   
expected wins (5)

    this can happen! 

    systems include a human reference translation 
    also include really good unconstrained 

commercial systems

23

expected wins (6)  

 

 

 

 

    even more problems: 

    remember that the scores for 
a system is the percentage of 
time it won in comparisons 
across all systems 

    what if score(b) > score(c), 
but in direct comparisons, c 
was almost always better 
than b? 

    this leads to cycles in the 

ranking 

    is this a problem?

 

onlineb

rwth-combo

 

 

cmu-hyposel-combo

cambridge

lium

dcu-combo

cmu-hea   eld-combo

upv-combo

nrc
uedin
jhu
limsi

rali
lig

jhu-combo
lium-combo

bbn-combo

rwth

cmu-statxfer

onlinea
huicong

dfki

cu-zeman
geneva

 

24

summary

    list of problems: 

    including ties biases similar systems, excluding 

discredits 

    comparisons do not factor in dif   culty of the 
   match    (i.e., losing to the best system should 
count less) 

    there are cycles in the judgments 

    we made intuitive changes, but how do we know 

whether they   re correct?

25

relative ability model

models of translation competitions (hopkins & may, 2013)

    in expected wins, we estimate a id203 of 

each system winning a competition 

    we now move to a setup that models the relative 

ability of a system 
    assume each system si has an inherent ability,   j 
    its translations are then represented by draws 

from a gaussian distribution centered at   j

26

relative ability

  i

better

27

relative ability
    a    competition    proceeds as follows: 

    choose two systems, si and sj, from the set {s} 
    sample a    translation    from their distributions   

   
   

 
 

  qi ~ n(si;   i,   2)   
  qj ~ n(sj;   j,   2) 

    compare their values to determine who won 

    de   ne d as a    decision radius    
    record a tie if |qi     qj| < d 
    else record a win or loss

28

visually

better

tie

si wins

sj wins

d
qi
qj

d

d

qi

qj

qi

qj

29

observations

    we can compute exact probabilities for all these 

events (difference of gaussians) 

    on average, a system with a higher    ability    will 

have higher draws, and will win 

    systems with close   s will tie more often

30

learning the model

    if we knew the system means, we could rank them 

    we assume the data was generated by the process 
above; we need to infer values for hidden params: 
    system means {  } 
    sampled translation qualities {q} 

    we   ll use id150 

    uses simple random steps to learn a 

complicated joint distributions 

    converges under certain conditions

31

id150

judge    dredd    ranked onlineb > jhu on sent #74   
judge    judy    ranked uedin > uu on sent #1734   
judge    reinhold    ranked jhu > uu on sent #1   
judge    jay    ranked onlinea = uedin on sent #953
    represent data as tuples   (si,	
   sj,	
     ,	
   qi,	
   qj)   
unknown

known

(onlineb, jhu, >, ?, ?)   
(uedin, uu, >, ?, ?)   
(jhu, uu, >, ?, ?)   
(onlinea, uedin, =, ?, ?)

    iterate back and forth between guessing {q}s and 

{  }s

32

   
   
   
iterative process

[collect	
   all	
   the	
   judgments]	
   
until	
   convergence	
   
	
   	
   #	
   resample	
   translation	
   qualities	
   
	
   	
   for	
   each	
   judgment	
   
	
   	
   	
   	
   qi	
   ~	
   n(  i,  2)   
	
   	
   	
   	
   qj	
   ~	
   n(  j,  2)	
   
	
   	
   	
   	
   #	
   (adjust	
   samples	
   to	
   respect	
   judgment	
     )	
   
	
   	
   #	
   resample	
   the	
   system	
   means   
	
   	
   for	
   each	
   system	
   
	
   	
   	
   	
     i	
   =	
   mean({qi})	
   

33

visually

(onlineb, 0.4, >, jhu, 0.2)

iteration 1

(onlineb, 0.15, >, jhu,    0.1)

iteration 2

(onlineb, 0.35, >, jhu, 0.05)

iteration 3

d

qi

qj

d

qi

qj

d

qi

qj

better

34

summary

    summary 

    model provides us with an explanation of how the 

data was generated 

    we infer the abilities of the systems to rank using 

the human judgments 

    problems 

    still no notion of evenness of the match 
    judges are not modeled 
    actual sentences are ignored

35

trueskill    ranking system
    used to rate players in xbox live 

    based on the elo system for chess 
    models player ability (  ) and the system   s 

con   dence about that estimate (  ) 
    when a game is played, the outcome (win, loss, 

or tie) is used to update these parameters 
    a more surprising outcome results in larger 

updates 

    these values are also used to    nd even matches

36

visualization

37

visualization

observation: s1 defeats s2

not pictured: confidences are 
separate for each system

38

updating

if s1 defeats s2, 

outcome surprisal

39

trueskill for mt

    in the mt setting: 

    each system is a player 
    each pairwise annotation is a game 

    we consider the judgments sequentially, an update 

the system parameters after each one 

    differences from xbox: 

    systems don   t improve between games

40

procedure

until	
   convergence	
   
	
   	
   create	
   a	
   new	
   match	
   
	
   	
   observe	
   the	
   outcome	
   
	
   	
   update	
   the	
   parameters	
   of	
   both	
   systems	
   

41

advantages of trueskill

    the system parameter updates re   ect how 

surprising the outcome was 

    trueskill is an online algorithm (as opposed to 

batch) 
    instead of sampling system pairs uniformly, we 
can gather more judgments from systems that 
are closely matched 

    this presents some potential for reducing the 

amount of data we need to collect

42

partial orderings
    what is the best university in the world? 

    best is not always well-de   ned or 

meaningful 

    instead of total orderings, we present 

partial orderings, which are equivalence 
clusters of systems that can   t be 
distinguished

simulating human judgment in machine 
translation evaluation campaigns (koehn, 2012)

43

computing clusters

    to compute clusters, we use a 

statistical technique called bootstrap 
resampling 
    estimate variance by sampling the 
sample many times and compute 
statistics over the samples 

    we run each model 1,000 times 

    for each system, extract rank from 

each fold, throw out outliers 

    use resulting rank range to cluster

x
1 
2 2
2
2
2
x
3
2
2
2
2
1
2
2
2
1

44

hindi   english (wmt 2014)

rank range

constrained

unconstrained

1

2   4

5

6   7

8

9

uedin-syntax, cmu

uedin-phrase

afrl, iit-bombay

dcu-lingo24

online-b

online-a

iit-hyderabad

45

model selection

    we have multiple ways of ranking the systems 

    expected wins 
    model of relative ability 
    trueskill 

    which is best? 

    which one does the best job of making 

predictions?

46

model selection

    experimental setup 

    split the complete data 

into 100 folds 
    for each fold 

    build a model on the 

other 99 folds 

    compute accuracy on 

the current fold 
    report average 

accuracy across all 
folds

dataset: 328k judgments   

10 language pairs 

47

results

48

analysis

    the different methods don   t have that much of an 

effect (surprising?) 
    in fact, the ordering of systems was exactly the 

same for eight of the language pairs 

    however, this hides the amount of data used

49

data requirements

0.5

0.488

0.475

0.463

y
c
a
r
u
c
c
a

0.45

400

ew
h&m
ts

800

1600

3200

6400

training data size

50

analysis

    the different methods don   t have that much of an 

effect (surprising?) 
    in fact, the ordering of systems was exactly the 

same for eight of the language pairs 

    however, this hides the amount of data used 

    trueskill needs much less data 
    also has much smaller variance (so we get 

tighter clusters)

51

cluster counts

52

summary

    there are many ways of producing the human 

ranking, from simple models to more elegant ones 

    we use the model   s ability to predict unseen data 

as a test of how good it is 
    there are many dimensions to goodness, 
including accuracy and data requirements 

    translation quality is inherently subjective and task-

speci   c 
    publishing clusters is a step towards capturing 

this

53

54

55

