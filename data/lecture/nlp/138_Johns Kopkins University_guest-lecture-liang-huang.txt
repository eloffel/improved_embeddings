trees and forests in 
machine translation

liang huang

city university of new york

joint work with kevin knight (isi), aravind joshi (penn), haitao mi and qun liu (ict), 2006--2010

university of pennsylvania, march 31st, 2015

nlp is hard

    how many interpretations?

aravind 

joshi

i saw her duck

lexical ambiguity

liang huang (cuny)

2

nlp is hard

    how many interpretations?

aravind 

joshi

i  eat  sushi  with  tuna

structural ambiguity

liang huang (cuny)

3

ambiguity explosion

    how many interpretations?

nlp: ambiguity explosion

tcs: combinatorial explosion

i saw her duck

with a telescope in the garden ...

liang huang (cuny)

...

4

unexpected structural ambiguity

liang huang (cuny)

5

ambiguity in translation

   zi    zhu     zhong   duan
                           

   self  help terminal device

help oneself terminating machine

translation 
requires 

understanding!

(atm,     self-service terminal   )

liang huang (cuny)

6

ambiguity in translation

   id20    problem in machine learning

liang huang (cuny)

7

ambiguity in translation

liang   s rule: if you see 
   x carefully    in china, 

just don   t do it.

       np <=> be aware of np
         vp <=> be careful not to vp

liang huang (usc)

8

translate server error

clear evidence that mt is used in real life.

liang huang (cuny)

9

how do people translate?

1. understand the source language sentence
2. generate the target language translation
      

      

      

         

   

   

b  sh  

yu

shal  ng

jux  ng

le

hu  t  n

bush

and/
with

sharon

hold

[past.]

meeting

   bush   held   a  meeting   with   sharon   

liang huang (cuny)

10

how do compilers translate?
1. parse high-level language program into a syntax tree
2. generate intermediate or machine code accordingly

x3 = y + 3;

ld     r1,   
addf   r1,  r1, #3.0  // add float
rtoi   r2,  r1        // real to int
st     id1, r2

syntax-directed translation (~1960)

liang huang (cuny)

11

syntax-directed machine translation

1. parse the source-language sentence into a tree
2. recursively convert it into a target-language sentence

bush

and/
with

sharon

hold

[past.]

meeting

liang huang (cuny)

(irons 1961; lewis, stearns 1968;  aho, ullman 1972)  ==>  (huang, knight, joshi 2006)12

syntax-directed machine translation

    recursive rewrite by pattern-matching

with

liang huang (cuny)

(huang, knight, joshi 2006); rules from (galley et al., 04)

13

syntax-directed machine translation?

    recursively solve un   nished subproblems

bush

with

held          

liang huang (cuny)

(huang, knight, joshi 2006); rules from (galley et al., 04)

14

syntax-directed machine translation?

    continue pattern-matching

bush

held

with

a meeting

sharon

liang huang (cuny)

(huang, knight, joshi 2006); rules from (galley et al., 04)

15

syntax-directed machine translation?

    continue pattern-matching

bush

held

a meeting

with

sharon

liang huang (cuny)

(huang, knight, joshi 2006); rules from (galley et al., 04)

16

pros: simple, fast, and expressive
    simple architecture: separate parsing and translation
    ef   cient linear-time id145
       soft decision    at each node on which rule to use
    (trivial) depth-   rst traversal with memoization
    expressive multi-level rules for syntactic divergence

                                              (beyond id18)

liang huang (cuny)

(huang, knight, joshi 2006); rules from (galley et al., 04)

17

cons: parsing errors

    ambiguity is a fundamental problem in natural languages
    probably will never have perfect parsers (unlike compiling)
    parsing errors affect translation quality!

emergency exit
or    safe exports   ?

liang huang (cuny)

mind your head

or    meet cautiously   ?

18

exponential explosion of ambiguity

i saw her duck.

...

    how about...
    i saw her duck with a telescope.
    i saw her duck with a telescope in the garden...

liang huang (cuny)

nlp == dealing with ambiguities.

19

tackling ambiguities in translation
    simplest idea: take top-k trees rather than 1-best parse
    but only covers tiny fraction of the exponential space
...
    and these k-best trees are very similar
..
    e.g., 50-best trees ~ 5-6 binary ambiguities (25 < 50 <26)
    very inef   cient to translate on these very similar trees

    most ambitious idea: combining parsing and translation
    start from the input string, rather than 1-best tree
    essentially considering all trees (search space too big)
    our approach:  packed forest (poly. encoding of exp. space) 
    almost as fast as 1-best,   almost as good as combined

liang huang (cuny)

20

outline

    overview: tree-based translation
    forest-based translation
    packed forest
    translation on a forest
    experiments
    forest-based rule extraction
    large-scale experiments

liang huang (cuny)

21

from lattices to forests

    common theme: polynomial encoding of exponential space
    forest generalizes    lattice/graph    from    nite-state world
    paths => trees  (in dp: knapsack vs. matrix-chain multiplication)
    graph => hypergraph;   regular grammar => id18

liang huang (cuny)

22

(earley 1970; billot and lang 1989)

packed forest

    a compact representation of many many parses
    by sharing common sub-derivations
    polynomial-space encoding of exponentially large set

nodes

hyperedges

a hypergraph

0  i 1 saw  2  him  3  with 4 a 5 mirror 6

liang huang (cuny)

(klein and manning, 2001; huang and chiang, 2005)

23

forest-based translation

   and    /    with   

liang huang (cuny)

24

forest-based translation

pattern-matching

on forest

(linear-time in forest size)

   

   and   

      

   

liang huang (cuny)

   and    /    with   

         

      

   

      

25

forest-based translation

pattern-matching

on forest

(linear-time in forest size)

   

   and   

      

   

liang huang (cuny)

   and    /    with   

         

      

   

      

26

translation forest

   bush held a meeting with sharon   

   held a meeting   

   bush   
liang huang (cuny)

   sharon   

27

the whole pipeline

input sentence
parser
parse forest

s
t
s
e
r
o
f
 
d
e
k
c
a
p

pattern-matching w/

translation rules (exact)

translation forest

integrating language models
                   (cube pruning)

translation+lm forest

alg. 3

1-best translation

k-best translations

liang huang (cuny)

(huang and chiang, 2005; 2007; chiang, 2007)

28

the whole pipeline

input sentence
parser
parse forest

s
t
s
e
r
o
f
 
d
e
k
c
a
p

forest pruning

pruned forest

pattern-matching w/

translation rules (exact)

translation forest

integrating language models
                   (cube pruning)

translation+lm forest

alg. 3

1-best translation

k-best translations

liang huang (cuny)

(huang and chiang, 2005; 2007; chiang, 2007)

29

parse forest pruning

    prune unpromising hyperedges
    principled way: inside-outside
       rst compute viterbi inside   , outside   
    then     (e) =   (v) + c(e) +   (u) +   (w)
    cost of best deriv that traverses e
    similar to    expected count    in em
    prune away hyperedges that have
         (e) -     (top) > p
for some threshold p

liang huang (cuny)

jonathan graehl: relatively useless pruning

outside  (v)

v
e

...
u

w

  (w)
inside

30

  (u)
inside

small-scale experiments

    chinese-to-english translation
    on a tree-to-string system similar to (liu et al, 2006)
    31k sentences pairs (0.8m chinese & 0.9m english words)
    giza++ aligned
    trigram language model trained on the english side
    dev: nist 2002 (878 sent.); test: nist 2005 (1082 sent.)
    chinese-side parsed by the parser of xiong et al. (2005)
    modi   ed to output a forest for each sentence (huang 2008)
    id7 score: 1-best baseline: 0.2430  vs.   pharaoh: 0.2297

liang huang (cuny)

31

k-best trees vs. forest-based

1.7 id7 improvement over 1-best, 
0.8 over 30-best, and even faster!

k = ~6.1  108  trees

~2  104  trees

liang huang (cuny)

32

forest as virtual    -best list
    how often is the ith-best tree picked by the decoder?

(~6.1  108 -best)

suggested by
mark johnson

 
d
n
o
y
e
b
 
%
2
3

t
s
e
b
-
0
0
1

 
d
n
o
y
e
b
 
%
0
2

t
s
e
b
-
0
0
0
1

1000

33

liang huang (cuny)

wait a sec... where are the rules from?

         vp   <=>  be careful not to vp
         np  <=>  be careful of np 
                  . . .
xi  ox  n  g  u
              <=>  be aware of  dog

xi  ox  n
         x   <=>  be careful not to x

outline

    overview: tree-based translation
    forest-based translation
    forest-based rule extraction
    background: tree-based rule extraction (galley et al., 2004)
    extension to forest-based
    large-scale experiments

liang huang (cuny)

35

where are the rules from?

    data: source parse tree, target sentence, and alignment
    intuition: fragment the tree; contiguous span

liang huang (cuny)

ghkm - (galley et al 2004; 2006)

36

where are the rules from?
    source parse tree, target sentence, and alignment
    compute target spans

liang huang (cuny)

ghkm - (galley et al 2004; 2006)

37

where are the rules from?
    source parse tree, target sentence, and alignment
    well-formed fragment: contiguous and faithful t-span

admissible 

set

liang huang (cuny)

ghkm - (galley et al 2004; 2006)

38

where are the rules from?
    source parse tree, target sentence, and alignment
    well-formed fragment: contiguous and faithful t-span

admissible 

set

liang huang (cuny)

ghkm - (galley et al 2004; 2006)

39

forest-based rule extraction
    same cut set computation; different fragmentation

liang huang (cuny)

also in (wang, knight, marcu, 2007)

40

forest-based rule extraction
    same cut set computation; different fragmentation

liang huang (cuny)

also in (wang, knight, marcu, 2007)

41

forest-based rule extraction

    same admissible set de   nition; different fragmentation

liang huang (cuny)

42

forest-based rule extraction
    forest can extract smaller chunks of rules

liang huang (cuny)

43

the forest2 pipeline

training time

source sentence

parser

1-best/
forest

aligner

word alignment

target sentence

source sentence

parser

1-best/forest

translation time
liang huang (cuny)

pattern-
matcher

target sentence

r
o
t
c
a
r
t
x
e
 
 
 
e
u
r

l

translation 

ruleset

forest vs. k-best extraction

1.0 id7 improvement over 1-best,
twice as fast as 30-best extraction

~108  trees

liang huang (cuny)

45

forest2

    fbis: 239k sentence pairs (7m/9m chinese/english words)
    forest in both extraction and decoding
    forest2 results is 2.5 points better than 1-best2
    and outperforms hiero (chiang 2007) by quite a bit

translating on ...

l

r
u
e
s
 
f
r
o
m

 
.
.
.

1-best tree
30-best trees

forest
hiero

1-best tree

0.2560
0.2634
0.2679

forest
0.2674
0.2767
0.2816

0.2738
0.2738

liang huang (cuny)

46

translation examples

    src                                                                          

            b  ow  ir   sh  o  y        al  f  t      hu  t  n   h  n    zh  ngy  o
            powell    say   with   arafat    talk     very   important

    1-best2  powell said the very important talks with arafat 
    forest2  powell said his meeting with arafat is very important 
    hiero    powell said very important talks with arafat

liang huang (cuny)

47

conclusions

    main theme: ef   cient syntax-directed translation
    forest-based translation

    forest =    underspeci   ed syntax   :  polynomial vs. exponential
    still fast (with pruning), yet does not commit to 1-best tree
    translating millions of trees is faster than just on top-k trees
    forest-based rule extraction: improving rule set quality
    very simple idea, but works well in practice
    signi   cant improvement over 1-best syntax-directed
       nal result outperforms hiero by quite a bit

liang huang (cuny)

48

forest is your friend in machine translation.

help save the forest.

larger decoding experiments (acl)
    2.2m sentence pairs (57m chinese and 62m english words)
    larger trigram models (1/3 of xinhua gigaword)
    also use bilingual phrases (bp) as    at translation rules
    phrases that are consistent with syntactic constituents
    forest enables larger improvement with bp

1-best tree
30-best trees

forest

improvement

t2s
0.2666
0.2755
0.2839

1.7

t2s+bp
0.2939
0.3084
0.3149

2.1

liang huang (cuny)

50

