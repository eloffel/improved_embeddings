crf word alignment & 
noisy channel translation

machine translation 

lecture 6 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

last time ...

alignment

translation
)
,

p(

p(

=

)=

translation

xalignment
xalignment
|
p(e | f, m) = xa2[0,n]m

p(

p(
)
alignment
   

)
translation | alignment

{z
}|

z

} |
{ z
myi=1

{z
{
}|

p(ei | fai)

p(a | f, m)    

}

map alignment

ibm model 4 alignment

our model's alignment

figure 2: example english-urdu alignment under ibm model 4 (left) and our discriminative model (right). model

a few tricks...

p(f|e)

p(e|f)

a few tricks...

p(f|e)

p(e|f)

a few tricks...

p(f|e)

p(e|f)

another view
myi=1

p(a | f, m)    

with this model:

p(e | f, m) = xa2[0,n]m

p(ei | fai)

the problem of word alignment is as:

a    = arg max
a2[0,n]m

p(a | e, f, m)

can we model this distribution directly?

markov random fields (mrfs)

a

x

a

x

b

y

b

y

c

z

c

z

p(a, b, c, x, y, z) =
p(a)     p(b | a)     p(c | b)   
p(x | a)p(y | b)p(z | c)

1
z    

p(a, b, c, x, y, z) =
 1(a, b)      2(b, c)      3(c, d)   
 4(x)      5(y )      6(z)

   factors   

x

y

x = {a, b, c}
x 2 x
y 2 x

computing z
z = xx2xxy2x

 1(x, y) 2(x) 3(y)

when the graph has certain 
structures (e.g., chains), you can 
factor to get polynomial time 
id145 algorithms.

z = xx2x

 2(x)xy2x

 1(x, y) 3(y)

id148

a

x

b

y

c

z

1
z    

p(a, b, c, x, y, z) =
 1(a, b)      2(b, c)      3(c, d)   
 4(x)      5(y )      6(z)

 1,2,3(x, y) = expxk

weights (learned)

wkfk(x, y)

feature functions   

(speci   ed)

random fields

    bene   ts 
    potential functions can be de   ned with respect 
to arbitrary features (functions) of the variables
    great way to incorporate knowledge
    drawbacks 
    likelihood involves computing z
    maximizing likelihood usually requires computing 

z (often over and over again!)

id49

    use mrfs to parameterize a conditional 
distribution. very easy: let feature functions 
look at anything they want in the    input   

p(y | x) =

1

zw(y)

expxf2gxk

wkfk(f, x)

all factors in the graph of 

y

parameter learning

    crfs are trained to maximize conditional likelihood

  wid113 = arg max

w y(xi,yi)2d
    recall we want to directly model

p(yi | xi ; w)

p(a | e, f)

    the likelihood of what alignments?

gold reference alignments!

crf for alignment
    one of many possibilities, due to blunsom & 

cohn (2006)

1

|e|xi=1xk

exp

zw(e, f)

p(a | e, f) =
wkf (ai, ai 1, i, e, f)
    a has the same form as in the lexical translation 
models (still make a one-to-many assumption)
    wk are the model parameters
    fk are the feature functions

o(n2m)     o(n3)

model

    labels (one per target word) index source sentence
    train model (e,f) and (f,e) [inverting the reference alignments]

alignment experiments
    french-english canadian hansards corpus
    484 manually word-aligned sentence pairs 
(100 training, 37 development, 347 testing)
    1.1 million sentence-aligned pairs
    baseline for comparison: giza++ 
implementation of ibm model 4
    (also experimented on romanian-english)

pervez musharrafs

langer

abschied

identical word

pervez musharraf

   s

long

goodbye

identical word

17

pervez musharrafs

langer

abschied

matching pre   x

pervez musharraf

   s

long

goodbye

identical word
matching pre   x

18

pervez musharrafs

langer

abschied

matching suf   x

pervez musharraf

   s

long

goodbye

identical word
matching pre   x
matching suf   x

19

pervez musharrafs

langer

abschied

orthographic similarity

pervez musharraf

   s

long

goodbye

identical word
matching pre   x
matching suf   x
orthographic similarity

20

pervez musharrafs

langer

abschied

in dictionary

pervez musharraf

   s

long

goodbye

identical word
matching pre   x
matching suf   x
orthographic similarity

in dictionary
...

21

lexical features

    word   word indicator features 
    various word   word co-occurrence scores
    ibm model 1 probabilities (t   s , s   t)
    geometric mean of model 1 probabilities
    dice   s coef   cient [binned]
    products of the above

lexical features

    word class   word class indicator

    nn translates as nn                     (nn_nn=1)
    nn does not translate as md         (nn_md=1)

    identical word feature

    identical pre   x feature

    2010 = 2010                   (identword=1 identnum=1)

    obama ~ obamu          (identprefix=1)

    orthographic similarity measure [binned]

    al-qaeda ~ al-kaida  (orthosim050_080=1)

other features

    compute features from large amounts of 
unlabeled text
    does the model 4 alignment contain this 
    what is the model 1 posterior 

alignment point?

id203 of this alignment point?

results

summary

    crfs outperform unsupervised / latent 

variable alignment models, even when only 
a small number of word-aligned sentences 
are available

    diverse range of features can be 

incorporated and are bene   cial to word-
alignment quality

    features from unsupervised models can 

also be incorporated

unfortunately, you need gold alignments!

putting the pieces together

    we have seen how to model the following:

p(e)
p(e | f, m)
p(e, a | f, m)
p(a | e, f)

putting the pieces together

    we have seen how to model the following:

p(e)
p(e | f, m)
p(e, a | f, m)
p(a | e, f)

    goal: a better model of                    that knows about

p(e | f, m)

p(e)

one naturally wonders if the problem 
of translation could conceivably be 
treated as a problem in cryptography.  
when i look at an article in russian, i 

say:    this is really written in 

english, but it has been coded in 
some strange symbols. i will now 

proceed to decode.   

warren weaver to norbert wiener, march, 1947

m

encoder

y

message

sent   

transmission

   noisy    
channel

x

decoder

received   
transmission

m0

recovered   
message

claude shannon.    a mathematical theory of 
communication    1948.

m

encoder

y

message

sent   

transmission

   noisy    
channel

x

decoder

received   
transmission

p(y)

p(x|y)

p(x)

m0

recovered   
message

claude shannon.    a mathematical theory of 
communication    1948.

m

encoder

y

message

sent   

transmission

   noisy    
channel

x

decoder

received   
transmission

p(y)

p(x|y)

p(x)

m0

recovered   
message

claude shannon.    a mathematical theory of 
communication    1948.

m

encoder

y

message

sent   

transmission

   noisy    
channel

received   
transmission

x

decoder

m0

recovered   
message

p(y) p(x|y)
shannon   s theory tells us:
1) how much data you can send   
2) the limits of compression   
3) why your download is so slow
4) how to translate   
claude shannon.    a mathematical theory of 
communication    1948.

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

p(y) p(x|y)

y 0
m0

recovered   
message

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

p(y) p(x|y)

y 0
m0

recovered   
message

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

p(y) p(x|y)

y 0
m0

recovered   
message

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

p(y) p(x|y)

y 0
m0

recovered   
message

y0 = arg max

y

= arg max

y

p(y|x)
p(x|y)p(y)

p(x)

= arg max

y

p(x|y)p(y)

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

y 0
m0

recovered   
message

p(y) p(x|y)

6=

y

y0 = arg max

p(y|x)
p(x|y)p(y)

p(x)

= arg max

i can help.
y

= arg max

y

p(x|y)p(y)

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

y 0
m0

recovered   
message

y0 = arg max

y

= arg max

y

p(y|x)
p(x|y)p(y)

p(x)

= arg max

y

p(x|y)p(y)

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

y 0
m0

recovered   
message

y0 = arg max

y

= arg max

y

p(y|x)
p(x|y)p(y)

p(x)

p(x|y)p(y)
denominator doesn   t depend on    .y

= arg max

y

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

y 0
m0

recovered   
message

y0 = arg max

y

= arg max

y

p(y|x)
p(x|y)p(y)

p(x)

= arg max

y

p(x|y)p(y)

   noisy    
channel

y

sent   

transmission

x

decoder

received   
transmission

y 0
m0

recovered   
message

y0 = arg max

y

p(x|y)p(y)

y

sent   

transmission
english

   noisy    
channel

x

decoder

m0y 0

received   
transmission
   french   

recovered   
message
english   

y0 = arg max

y

e0 = arg max

e

p(x|y)p(y)
p(f|e)p(e)

y

sent   

transmission
english

   noisy    
channel

x

decoder

m0y 0

received   
transmission
   french   

recovered   
message
english   

y0 = arg max

y

e0 = arg max

e

p(x|y)p(y)
p(f|e)p(e)

translation model

y

sent   

transmission
english

   noisy    
channel

x

decoder

m0y 0

received   
transmission
   french   

recovered   
message
english   

y0 = arg max

y

e0 = arg max

e

p(x|y)p(y)
p(f|e)p(e)

translation model

language model

other noisy channel applications: ocr, speech 

recognition, id147...

division of labor

source

    translation model
    id203 of translation back into the 
    ensures adequacy of translation
    language model
    is a translation hypothesis    good    english?
    ensures    uency of translation

p(e)

english

p(f | e)

e    = arg max

e

= arg max

e

p(e | f)
p(f | e)     p(e)

announcements

tonight at 11:59pm

    hw1 leaderboard submissions are due 
    hw1 writeup and code are due 24 hours 
    next week: phrase-based machine 

later

translation

