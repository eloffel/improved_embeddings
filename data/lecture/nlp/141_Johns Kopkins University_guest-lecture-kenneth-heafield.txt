faster decoding for phrases and syntax

kenneth hea   eld

translation is expensive

   speed-up in tuning time but a   ects the performance   

   18 days using 12 cores   
[williams et al wmt 2014]

   time-sensitive id7 score   

[chung and galley, 2012]

   due to time constraints, this procedure was not used   

[servan et al, wmt 2012]

=) routine quality compromises

introduction

problem

cube pruning

incremental

conclusion

2

introduction

problem

cube pruning

incremental

conclusion

3

blame the language model

   lm queries often account for more than 50% of the cpu   

[green et al, wmt 2014]

introduction

problem

cube pruning

incremental

conclusion

4

blame the language model

   lm queries often account for more than 50% of the cpu   

[green et al, wmt 2014]

faster queries (kenlm)
more e   ective queries

introduction

problem

cube pruning

incremental

conclusion

5

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1 decoding problem
2 cube pruning
3 incremental

introduction

problem

cube pruning

incremental

conclusion

8

decoding example: input

le gar  con

a vu

l   homme

avec un t  elescope

introduction

problem

cube pruning

incremental

conclusion

9

decoding example: parse with sid18

s:s

x :np

x :vp

x :vp

x :v

x :np

x :pp

le gar  con

a vu

l   homme

avec un t  elescope

introduction

problem

cube pruning

incremental

conclusion

10

decoding example: read target side

s:s

x :np

x :vp

x :vp

le gar  con
the boy
a boy

x :v

a vu
seen
saw
view

x :np

x :pp

l   homme
man
the man
some men

avec un t  elescope
with the telescope
to an telescope
with a telescope

introduction

problem

cube pruning

incremental

conclusion

11

decoding example: one constituent

s:s

x :np

x :vp

x :vp

le gar  con
the boy
a boy

x :v

a vu
seen
saw
view

x :np

x :pp

l   homme
man
the man
some men

avec un t  elescope
with the telescope
to an telescope
with a telescope

introduction

problem

cube pruning

incremental

conclusion

12

x :vp

x :v

x :np

a vu

hyp
seen
saw
view

l   homme

hyp
man
the man
some men

introduction

problem

cube pruning

incremental

conclusion

13

x :vp

x :v

x :np

a vu

hyp
seen
saw
view

l   homme

hyp
man
the man
some men

x :vp

a vu l   homme

hypothesis
seen man
seen the man
seen some men
saw man
saw the man
saw some men
view man
view the man
view some men

introduction

problem

cube pruning

incremental

conclusion

14

x :vp

x :vp

x :v

x :np

a vu

hyp score
seen
-3.8
-4.0
saw
view
-4.0

l   homme

hyp
man
the man
some men

score
-3.6
-4.3
-6.3

a vu l   homme

hypothesis score
-8.8
seen man
seen the man
-7.6
-9.5
seen some men
saw man
-8.3
-6.9
saw the man
-8.5
saw some men
view man
-8.5
view the man
-8.9
view some men -10.8

introduction

problem

cube pruning

incremental

conclusion

15

x :vp

x :vp

x :v

x :np

a vu

hyp score
seen
-3.8
-4.0
saw
view
-4.0

l   homme

hyp
man
the man
some men

score
-3.6
-4.3
-6.3

a vu l   homme

hypothesis score
-6.9
saw the man
seen the man
-7.6
-8.3
saw man
saw some men
-8.5
-8.5
view man
-8.8
seen man
view the man
-8.9
seen some men
-9.5
view some men -10.8

introduction

problem

cube pruning

incremental

conclusion

16

x :vp

x :vp

x :v

x :np

a vu

hyp score
seen
-3.8
-4.0
saw
view
-4.0

l   homme

hyp
man
the man
some men

score
-3.6
-4.3
-6.3

scores do not sum

introduction

problem

cube pruning

incremental

a vu l   homme

hypothesis score
-6.9
saw the man
seen the man
-7.6
-8.3
saw man
saw some men
-8.5
-8.5
view man
-8.8
seen man
view the man
-8.9
seen some men
-9.5
view some men -10.8

conclusion

17

x :vp

x :vp

x :v

x :np

a vu

hyp score
seen
-3.8
-4.0
saw
view
-4.0

l   homme

hyp
man
the man
some men

score
-3.6
-4.3
-6.3

pruning is approximate

introduction

problem

cube pruning

incremental

a vu l   homme

hypothesis score
-6.9
saw the man
seen the man
-7.6
-8.3
saw man
saw some men
-8.5
-8.5
view man
-8.8
seen man
view the man
-8.9
seen some men
-9.5
view some men -10.8

conclusion

18

appending strings

hypotheses are built by string concatenation.

language model id203 changes when this is done:

p(saw the man)
p(saw)p(the man)

=

p(the | saw)p(man | saw the)
p(the)

p(man | the)

introduction

problem

cube pruning

incremental

conclusion

19

appending strings

hypotheses are built by string concatenation.

language model id203 changes when this is done:

p(saw the man)
p(saw)p(the man)

=

p(the | saw)p(man | saw the)
p(the)

p(man | the)

log id203 is part of the score

=) scores do not sum
=) local decisions may not be globally optimal
=) search is hard.

introduction

problem

cube pruning

incremental

conclusion

20

1 decoding problem
2 cube pruning
3 incremental

introduction

problem

cube pruning

incremental

conclusion

21

id125

man

seen  3.8 seen man  8.8
saw  4.0 saw man  8.3
view  4.0 view man  8.5

 3.6 the man

seen the man  7.6
saw the man  6.9
view the man  8.9

 4.3 some men

 6.3
seen some men  9.5
saw some men  8.5
view some men  10.8

[lowerre, 1976; chiang, 2005]

introduction

problem

cube pruning

incremental

conclusion

22

cube pruning

man  3.6 the man  4.3 some men  6.3

seen  3.8 queue
saw  4.0
view  4.0

hypothesis
seen man

queue

sum
 3.8 3.6= 7.4

[chiang, 2007]

introduction

problem

cube pruning

incremental

conclusion

23

cube pruning

man

 3.6 the man  4.3 some men  6.3

seen  3.8 seen man  8.8 queue
saw  4.0 queue
view  4.0

queue

sum
hypothesis
 4.0 3.6= 7.6
saw man
seen the man  3.8 4.3= 8.1

[chiang, 2007]

introduction

problem

cube pruning

incremental

conclusion

24

cube pruning

man

 3.6 the man  4.3 some men  6.3

seen  3.8 seen man  8.8 queue
saw  4.0 saw man  8.3 queue
view  4.0 queue

queue

sum
hypothesis
 4.0 3.6= 7.6
view man
seen the man  3.8 4.3= 8.1
saw the man  4.0 4.3= 8.3

[chiang, 2007]

introduction

problem

cube pruning

incremental

conclusion

25

cube pruning

man

 3.6 the man  4.3 some men  6.3

seen  3.8 seen man  8.8 queue
saw  4.0 saw man  8.3 queue
view  4.0 view man  8.5 queue

queue

sum
hypothesis
seen the man  3.8 4.3= 8.1
saw the man  4.0 4.3= 8.3
view the man  4.0 4.3= 8.3

[chiang, 2007]

introduction

problem

cube pruning

incremental

conclusion

26

make every dish. keep the best k, throw the rest out.

id125

cube pruning

combine the best ingredients. only make k dishes.

introduction

problem

cube pruning

incremental

conclusion

27

cube pruning hypotheses are atomic

string
is a
are a

string
countries that
countries which
country

string
is a countries that
are a countries that
are a countries which
...

no notion that    a countries    is bad.

introduction

problem

cube pruning

incremental

conclusion

28

make every dish. keep the best k, throw the rest out.

id125

cube pruning

combine the best ingredients. only make k dishes.

make small portions, taste, and order the best ones.

coarse-to-fine

introduction

problem

cube pruning

incremental

conclusion

29

coarse-to-fine

decode multiple times, adding detail each time:

increased lm order, words instead of classes

detect and prune    a countries    with a bigram lm.

[zhang et al, 2008; petrov et al, 2008]

introduction

problem

cube pruning

incremental

conclusion

30

coarse-to-fine

decode multiple times, adding detail each time:

increased lm order, words instead of classes

detect and prune    a countries    with a bigram lm.

[zhang et al, 2008; petrov et al, 2008]

requires tuning each pruning pass.
operates in lock step.

introduction

problem

cube pruning

incremental

conclusion

31

coarse-to-fine

decode multiple times, adding detail each time:

increased lm order, words instead of classes

detect and prune    a countries    with a bigram lm.

[zhang et al, 2008; petrov et al, 2008]

requires tuning each pruning pass.
operates in lock step.

can coarse-to-   ne be done on the    y?

introduction

problem

cube pruning

incremental

conclusion

32

1 decoding problem
2 cube pruning
3 incremental

introduction

problem

cube pruning

incremental

conclusion

33

observations

competing translations have words in common:

is a, are a

introduction

problem

cube pruning

incremental

conclusion

34

observations

competing translations have words in common:

is a, are a

words at the boundary matter most:

a + country, a + countries

introduction

problem

cube pruning

incremental

conclusion

35

observations

competing translations have words in common:

is a, are a

words at the boundary matter most:

a + country, a + countries

emphasize boundary words

introduction

problem

cube pruning

incremental

conclusion

36

make every dish. keep the best k, throw the rest out.

id125

cube pruning

combine the best ingredients. only make k dishes.

make small portions, taste, and order the best ones.

coarse-to-fine

incremental

taste during cooking. share ingredients.

introduction

problem

cube pruning

incremental

conclusion

37

boundary words

1 left-to-right phrase-based: one side
2 bottom-up syntax: both sides

introduction

problem

cube pruning

incremental

conclusion

38

partial translations

plain text

the united kingdom is a + . . .
scotland and wales are a + . . .

tree
the united kingdom

scotland and wales

is

are

a

   

introduction

problem

cube pruning

incremental

conclusion

39

phrase continuations

plain text

. . . + countries that
. . . + countries which
. . . + country

tree

countries

country

that

which

   

introduction

problem

cube pruning

incremental

conclusion

40

the united kingdom

is

scotland and wales

are

a

   

+

   

that

countries

country

which

introduction

problem

cube pruning

incremental

conclusion

41

the united kingdom

is

scotland and wales

are

a

   

+

   

that

countries

country

which

introduction

problem

cube pruning

incremental

conclusion

42

the united kingdom

is

scotland and wales

are

a

   

+

   

that

countries

country

which

introduction

problem

cube pruning

incremental

conclusion

43

the united kingdom

is

scotland and wales

are

a

   

+

   

that

countries

country

which

does the model like    a + countries   ?

introduction

problem

cube pruning

incremental

conclusion

44

exploring and backtracking

does the model like    a + countries   ?

yes try more detail.
no consider alternatives.

introduction

problem

cube pruning

incremental

conclusion

45

exploring and backtracking

does the model like    a + countries   ?

yes try more detail.
no consider alternatives.

formally: best-   rst search with a priority queue.

introduction

problem

cube pruning

incremental

conclusion

46

the queue entry

   a +       

splits into

best child    a + countries   
other children    a + country   

introduction

problem

cube pruning

incremental

conclusion

47

scores come from the best descendant:

score(a) = max{score(is a), score(are a)}

introduction

problem

cube pruning

incremental

conclusion

48

scores come from the best descendant:

score(a) = max{score(is a), score(are a)}

the language model updates scores:
score(a + countries) < score(a) + score(countries)

introduction

problem

cube pruning

incremental

conclusion

49

scores come from the best descendant:

score(a) = max{score(is a), score(are a)}

the language model updates scores:
score(a + countries) < score(a) + score(countries)
formally: p(countries | a) replaces p(countries)

introduction

problem

cube pruning

incremental

conclusion

50

best-first algorithm summary

populate the queue with     +    

loop until k complete options have been found:

split the top-scoring option

build a tree from the k complete options

introduction

problem

cube pruning

incremental

conclusion

51

summary

translations are assembled from left to right.

partial translations often share su   xes.
phrases often share pre   xes.

test su   xes and pre   xes before full combinations.

introduction

problem

cube pruning

incremental

conclusion

52

experiment

task chinese   english

source stanford
model phrase-based

software my own decoder, mtplz, versus moses

introduction

problem

cube pruning

incremental

conclusion

53

phrase-based results

-27.5

-28.0

-28.5

-29.0

e
r
o
c
s

l
e
d
o
m
e
g
a
r
e
v
a

-29.5

0

1

mtplz with incremental
moses with cube pruning
4

3

2

cpu seconds/sentence

introduction

problem

cube pruning

incremental

conclusion

54

phrase-based results

15

14

u
e
l
b
d
e
s
a
c
n
u

13

0

1

mtplz with incremental
moses with cube pruning
4

3

2

cpu seconds/sentence

introduction

problem

cube pruning

incremental

conclusion

55

search

the language model cares most about adjacent words.

test them    rst.

share work for shared words.

introduction

problem

cube pruning

incremental

conclusion

56

boundary words

1 left-to-right phrase-based: one side
2 bottom-up syntax: both sides

introduction

problem

cube pruning

incremental

conclusion

57

bottom-up syntax: both sides

is a x :np1 </s>
is a x :np1 that

how do we    nd the best value to substitute?

manage words on both sides.

introduction

problem

cube pruning

incremental

conclusion

58

example hypotheses

left state
countries that maintain diplomatic relations with north korea .
countries that have an embassy in dpr korea .
country that maintains some diplomatic ties in north korea .
nations which has some diplomatic ties with dpr korea .
country that maintains some diplomatic ties with dpr korea .

right state

relations

ties

introduction

problem

cube pruning

incremental

conclusion

59

example hypotheses

right state
left state
(countries that
    with north korea .)
(nations which has     with dpr korea .)
(countries that have    
dpr korea .)
(country
in north korea .)
   
(country
    with dpr korea .)

    words the language model does not care about

introduction

problem

cube pruning

incremental

conclusion

60

idea: alternate between left and right side

introduction

problem

cube pruning

incremental

conclusion

61

group by leftmost word

(countries that     with north korea .)

(countries that     korea .)

(countries that have     dpr korea .)

(           )

(nations which has     with dpr korea .)

(country     korea .)

(country     in north korea .)

(country     with dpr korea .)

introduction

problem

cube pruning

incremental

conclusion

62

reveal common words in each group
(countries that     with north korea .)

(countries that     korea .)

(countries that have     dpr korea .)

(           )

(nations which has     with dpr korea .)

(country     korea .)

(country     in north korea .)

(country     with dpr korea .)

introduction

problem

cube pruning

incremental

conclusion

63

alternate sides until tree is full

(countries that     with north korea .)

(countries that     korea .)

(countries that have     dpr korea .)

(           )

(nations which has     with dpr korea .)

(country     korea .)

(country     in north korea .)

(country     with dpr korea .)

introduction

problem

cube pruning

incremental

conclusion

64

using rules

is a x :np1 </s>

turns into
is a (           ) </s>

x :v 1 the x :n2
turns into
(           ) the (           )
|{z}
|{z}
x :n2
x :v 1

introduction

problem

cube pruning

incremental

conclusion

65

exploring and backtracking

does the lm like    is a (countries that     korea .) </s>   ?

yes try more detail.
no consider alternatives.

introduction

problem

cube pruning

incremental

conclusion

66

exploring and backtracking

does the lm like    is a (countries that     korea .) </s>   ?

yes try more detail.
no consider alternatives.

formally: priority queue containing breadcrumbs.

introduction

problem

cube pruning

incremental

conclusion

67

split and leave breadcrumbs

(countries that     with north korea .)

(countries that     korea .)

(countries that have     dpr korea .)

(           )

(nations which has     with dpr korea .)

(country     korea .)

(country     in north korea .)

(country     with dpr korea .)

introduction

problem

cube pruning

incremental

conclusion

68

split and leave breadcrumbs

(countries that     with north korea .)

(countries that     korea .)

(countries that have     dpr korea .)

(           )
[1+]

(nations which has     with dpr korea .)

(country     korea .)

(country     in north korea .)

(country     with dpr korea .)

introduction

problem

cube pruning

incremental

conclusion

69

the queue entry

is a (           ) </s>
splits into

zeroth child    is a (countries that     korea .) </s>   
other children    is a (           )[1+] </s>   
children except the zeroth.

introduction

problem

cube pruning

incremental

conclusion

70

a priority queue contains competing entries:

is a (countries that     korea .) </s>
(           ) the (           )
is a (           )[1+] </s>
the algorithm pops the top entry,
splits a non-terminal, and pushes.

introduction

problem

cube pruning

incremental

conclusion

71

best-first algorithm

populate the queue with rules like    is a (           ) </s>   
loop until k complete options have been found:

split the top-scoring option, leave a breadcrumb

build a tree from the k complete options

introduction

problem

cube pruning

incremental

conclusion

72

syntax

same as phrase-based, just concatenate on left and right.

introduction

problem

cube pruning

incremental

conclusion

73

experiment

task wmt 2011 german-english
model hierarchical

decoder moses

introduction

problem

cube pruning

incremental

conclusion

74

moses hierarchical

 101.4

 101.5

e
r
o
c
s

 101.6

 101.7

l
e
d
o
m
e
g
a
r
e
v
a

 101.8

 101.9

0

0.5

incremental
cube pruning
additive cube pruning

1

1.5

2

cpu seconds/sentence

introduction

problem

cube pruning

incremental

conclusion

2.5

75

moses hierarchical

22.2

22.0

21.8

21.6

u
e
l
b
d
e
s
a
c
n
u

21.4

0

0.5

incremental
cube pruning
additive cube pruning

1

1.5

cpu seconds/sentence

2

introduction

problem

cube pruning

incremental

conclusion

2.5

76

hiero zh-en

hiero en-de

hiero de-en

hiero de-en cdec

syntax en-de

syntax de-en

tree-to-tree fr-en cdec

0.0

0.5

1.0

1.5

2.0

speed ratio

2.5

3.0

3.5

4.0

introduction

problem

cube pruning

incremental

conclusion

77

beam < 20
beam   20

hiero zh-en

hiero en-de

hiero de-en

hiero de-en cdec

syntax en-de

syntax de-en

tree-to-tree fr-en cdec

0.0

0.5

1.0

1.5

2.0

speed ratio

2.5

3.0

3.5

4.0

introduction

problem

cube pruning

incremental

conclusion

78

incremental

a series of coarse-to-   ne estimates.

continually taste the dish and adjust.

introduction

problem

cube pruning

incremental

conclusion

79

takeaway

search limits what translation can do.

long-distance models like gender and number are harder.

open the black box.

language models can produce intermediate scores.

introduction

problem

cube pruning

incremental

conclusion

80

