phrase-based mt
machine translation 

lecture 7 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

translational equivalence

er hat die pr  fung bestanden, jedoch nur knapp

he insisted on the test, but just barely.
he passed the test, but just barely.

how do lexical translation models deal with   
contextual information?

translational equivalence

er hat die pr  fung bestanden, jedoch nur knapp

he insisted on the test, but just barely.
he passed the test, but just barely.

f

e

bestanden insisted

were
existed

was
been
passed
consist

prob
0.06
0.06
0.04
0.04
0.04
0.03
0.01

translational equivalence

er hat die pr  fung bestanden, jedoch nur knapp

he insisted on the test, but just barely.
he passed the test, but just barely.

lexical translation

what is wrong with this?
how can we improve this?

translation model

    what are the atomic units?
    lexical translation: words
    phrase-based translation: phrases 
    standard model used by google, microsoft ...
    bene   ts
    many-to-many translation
    use of local context in translation
    downsides
    where do phrases comes from?

translation model

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

f = morgen fliege ich nach baltimore zur konferenz

we can then marginalize to get p(f|e):

p(f | e)

e =

tomorrow

i

will fly

to the conference

in baltimore

translation model

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

morgen

fliege ich nach baltimore zur konferenz

we can then marginalize to get p(f|e):

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

translation model

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

morgen

fliege ich nach baltimore zur konferenz

we can then marginalize to get p(f|e):

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow)

translation model

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

fliege

morgen

we can then marginalize to get p(f|e):

ich nach baltimore zur konferenz

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow) x p(   iege|will    y)

translation model

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

fliege

ich

morgen

we can then marginalize to get p(f|e):

nach baltimore zur konferenz

p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow) x p(   iege|will    y)

x p(ich|i)

`

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

nach baltimore

fliege

ich

morgen

we can then marginalize to get p(f|e):

zur konferenz
p(f | e)

tomorrow

i

will fly

to the conference

in baltimore

f =
a =
e =

p(morgen|tomorrow) x p(   iege|will    y)

x p(ich|i)

x ... 

translation model

    with a latent variable, we introduce a decomposition 

into phrases which translate independently:

p(f | e)

marginalize to get p(f|e):

p(f, a | e) = p(a) y   e,f    a
p(a) y   e,f    a
p(f | e) =xa a

p(f | e)

phrases

constituents

    contiguous strings of words
    phrases are not necessarily syntactic 
    usually have maximum limits
    phrases subsume words (individual words 

are phrases of length 1)

(nps, vps, pps, cps...)

linguistic phrases
    model is not limited to linguistic phrases   
    non-constituent phrases are useful
es gibt    there is | there are
    is a    good    phrase more likely to be   
     [p np]      or        [governor  p]   
why? how would you    gure this out?

phrase tables

f

das thema

es gibt

morgen

   iege ich

e

the issue
the point
the subject
the thema
there is
there are
tomorrow
will i    y
will    y
i will    y

p(f | e)
0.41
0.72
0.47
0.99
0.96
0.72
0.9
0.63
0.17
0.13

p(a)

    two responsibilities
    divide the source sentence into phrases
    standard approach: uniform distribution over 
    how many segmentations are there?
    reorder the phrases
    standard approach: markov model on phrases 

all possible segmentations

(parameterized with log-linear model)

reordering model

learning phrases

    latent segmentation variable
    latent phrasal inventory
    parallel data
    em?

computational problem: summing over all segmentations   
and alignments is #p-complete

modeling problem: id113 has a degenerate solution.

learning phrases

    three stages
    word alignment
    extraction of phrases
    estimation of phrase probabilities

consistent phrases

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

akemasu / open 

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

watashi wa / i 

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

watashi / i 

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

watashi / i    

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

hako wo / box

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

hako wo / the box

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

hako wo / open the box

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

hako wo / open the box

   

phrase extraction

i   open   the    box

watashi

wa
hako
wo

akemasu

hako wo akemasu / open the box

translation process

    task: translate this sentence from german into english

er

geht

ja

nicht

nach

hause

chapter 6: decoding

2

translation process

    task: translate this sentence from german into english

geht

ja

nicht

nach

hause

er

er

he

    pick phrase in input, translate

chapter 6: decoding

3

translation process

    task: translate this sentence from german into english

er

er

geht

ja

nicht

nach

hause

ja nicht

he

does not

    pick phrase in input, translate

    it is allowed to pick words out of sequence reordering
    phrases may have multiple words: many-to-many translation

chapter 6: decoding

4

translation process

    task: translate this sentence from german into english

er

er

geht

geht

ja

nicht

nach

hause

ja nicht

he

does not

go

    pick phrase in input, translate

chapter 6: decoding

5

translation process

    task: translate this sentence from german into english

er

er

geht

geht

ja

nicht

nach

hause

ja nicht

nach hause

he

does not

go

home

    pick phrase in input, translate

chapter 6: decoding

6

computing translation id203

    probabilistic model for phrase-based translation:

ebest = argmaxe

iyi=1

 (   fi|  ei) d(starti   endi 1   1) plm(e)

    score is computed incrementally for each partial hypothesis
    components

phrase translation picking phrase   fi to be translated as a phrase   ei
! look up score  (   fi|  ei) from phrase translation table
reordering previous phrase ended in endi 1, current phrase starts at starti
! compute d(starti   endi 1   1)
language model for id165 model, need to keep track of last n   1 words
! compute score plm(wi|wi (n 1), ..., wi 1) for added words wi

chapter 6: decoding

7

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

,

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

    many translation options to choose from

    in europarl phrase table: 2727 matching phrase pairs for this sentence
    by pruning to the top 20 per phrase, 202 translation options remain

chapter 6: decoding

8

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

    the machine translation decoder does not know the right answer

    picking the right translation options
    arranging them in the right order

! search problem solved by heuristic id125

chapter 6: decoding

9

decoding: precompute translation options

er

geht

ja

nicht

nach

hause

consult phrase translation table for all input phrases

chapter 6: decoding

10

decoding: start with initial hypothesis

er

geht

ja

nicht

nach

hause

initial hypothesis: no input words covered, no output produced

chapter 6: decoding

11

decoding: hypothesis expansion

er

geht

ja

nicht

nach

hause

are

pick any translation option, create new hypothesis

chapter 6: decoding

12

decoding: hypothesis expansion

er

geht

ja

nicht

nach

hause

he

are

it

create hypotheses for all other translation options

chapter 6: decoding

13

decoding: hypothesis expansion

er

geht

ja

nicht

nach

hause

he

are

it

yes

goes

home

does not

home

go

to

also create hypotheses from created partial hypothesis

chapter 6: decoding

14

decoding: find best path

er

geht

ja

nicht

nach

hause

he

are

it

yes

goes

home

does not

home

go

to

backtrack from highest scoring complete hypothesis

chapter 6: decoding

15

reading

    read chapter 5 and 6 

from the textbook

announcements

    hw2 will be released soon
    hw2 due thursday feb 19th at 11:59pm

