phrase-based mt
machine translation 

lecture 8 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

translation process

    task: translate this sentence from german into english

er

geht

ja

nicht

nach

hause

chapter 6: decoding

2

translation process

    task: translate this sentence from german into english

geht

ja

nicht

nach

hause

er

er

he

    pick phrase in input, translate

chapter 6: decoding

3

translation process

    task: translate this sentence from german into english

er

er

geht

ja

nicht

nach

hause

ja nicht

he

does not

    pick phrase in input, translate

    it is allowed to pick words out of sequence reordering
    phrases may have multiple words: many-to-many translation

chapter 6: decoding

4

translation process

    task: translate this sentence from german into english

er

er

geht

geht

ja

nicht

nach

hause

ja nicht

he

does not

go

    pick phrase in input, translate

chapter 6: decoding

5

translation process

    task: translate this sentence from german into english

er

er

geht

geht

ja

nicht

nach

hause

ja nicht

nach hause

he

does not

go

home

    pick phrase in input, translate

chapter 6: decoding

6

computing translation id203

    probabilistic model for phrase-based translation:

ebest = argmaxe

iyi=1

 (   fi|  ei) d(starti   endi 1   1) plm(e)

    score is computed incrementally for each partial hypothesis
    components

phrase translation picking phrase   fi to be translated as a phrase   ei
! look up score  (   fi|  ei) from phrase translation table
reordering previous phrase ended in endi 1, current phrase starts at starti
! compute d(starti   endi 1   1)
language model for id165 model, need to keep track of last n   1 words
! compute score plm(wi|wi (n 1), ..., wi 1) for added words wi

chapter 6: decoding

7

reordering model

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

,

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

    many translation options to choose from

    in europarl phrase table: 2727 matching phrase pairs for this sentence
    by pruning to the top 20 per phrase, 202 translation options remain

chapter 6: decoding

8

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

,

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

    many translation options to choose from

    in europarl phrase table: 2727 matching phrase pairs for this sentence
    by pruning to the top 20 per phrase, 202 translation options remain

chapter 6: decoding

8

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

,

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

    many translation options to choose from

    in europarl phrase table: 2727 matching phrase pairs for this sentence
    by pruning to the top 20 per phrase, 202 translation options remain

chapter 6: decoding

8

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

    the machine translation decoder does not know the right answer

    picking the right translation options
    arranging them in the right order

! search problem solved by heuristic id125

chapter 6: decoding

9

decoding: precompute translation options

er

geht

ja

nicht

nach

hause

consult phrase translation table for all input phrases

chapter 6: decoding

10

decoding: start with initial hypothesis

er

geht

ja

nicht

nach

hause

initial hypothesis: no input words covered, no output produced

chapter 6: decoding

11

decoding: hypothesis expansion

er

geht

ja

nicht

nach

hause

are

pick any translation option, create new hypothesis

chapter 6: decoding

12

decoding: hypothesis expansion

er

geht

ja

nicht

nach

hause

he

are

it

create hypotheses for all other translation options

chapter 6: decoding

13

decoding: hypothesis expansion

er

geht

ja

nicht

nach

hause

he

are

it

yes

goes

home

does not

home

go

to

also create hypotheses from created partial hypothesis

chapter 6: decoding

14

decoding: find best path

er

geht

ja

nicht

nach

hause

he

are

it

yes

goes

home

does not

home

go

to

backtrack from highest scoring complete hypothesis

chapter 6: decoding

15

size of the search space

    decoding is an np-complete problem  

    the search space is exponential 

    how can we reduce the search? 

    id145 (risk free) 

    heuristic search (risky)

recombination

    two hypothesis paths lead to two matching hypotheses

    same number of foreign words translated
    same english words in the output
    di   erent scores

    worse hypothesis is dropped

it

is

it is

it

is

chapter 6: decoding

17

recombination

    two hypothesis paths lead to hypotheses indistinguishable in subsequent search

    same number of foreign words translated
    same last two english words in output (assuming trigram language model)
    same last foreign word translated
    di   erent scores

    worse hypothesis is dropped

does not

does not

does not

he

it

he

it

chapter 6: decoding

18

restrictions on recombination

    translation model: phrase translation independent from each other

! no restriction to hypothesis recombination
    language model: last n  1 words used as history in id165 language model
! recombined hypotheses must match in their last n   1 words
    reordering model: distance-based reordering model based on distance to

end position of previous input phrase
! recombined hypotheses must have that same end position
    other feature function may introduce additional restrictions

chapter 6: decoding

19

pruning

    recombination reduces search space, but not enough
(we still have a np complete problem on our hands)

    pruning: remove bad hypotheses early

    put comparable hypothesis into stacks

(hypotheses that have translated same number of input words)

    limit number of hypotheses in each stack

chapter 6: decoding

20

stacks

goes

does not

he

are

it

yes

no word
translated

one word
translated

two words
translated

three words
translated

    hypothesis expansion in a stack decoder

    translation option is applied to hypothesis
    new hypothesis is dropped into a stack further down

chapter 6: decoding

21

stack decoding algorithm

1: place empty hypothesis into stack 0
2: for all stacks 0...n   1 do

for all hypotheses in stack do

3:

4:

5:

6:

7:

8:

9:

10:

11:

for all translation options do

if applicable then

create new hypothesis
place in stack
recombine with existing hypothesis if possible
prune stack if too big

end if
end for

end for

12:
13: end for

chapter 6: decoding

22

decoding complexity

    finding the best hypothesis is np-hard 

    even with no language model, there are an 

exponential number of states! 

    solution 1: limit reordering 

    solution 2: (lossy) pruning

pruning

    pruning strategies

    histogram pruning: keep at most k hypotheses in each stack
    stack pruning: keep hypothesis with score         best score (    < 1)
    computational time complexity of decoding with histogram pruning
o(max stack size     translation options     sentence length)
    number of translation options is linear with sentence length, hence:

o(max stack size     sentence length2)

    quadratic complexity

chapter 6: decoding

23

reordering limits

    limiting reordering to maximum reordering distance
    typical reordering distance 5   8 words

    depending on language pair
    larger reordering limit hurts translation quality

    reduces complexity to linear

o(max stack size     sentence length)

    speed / quality trade-o    by setting maximum stack size

chapter 6: decoding

24

search errors

    we are using a heuristic search to prune the 

search space 

    there are no guarantees of admissibility (like in a* 

search) 

    we may therefore prune out a partial hypothesis 

that would have lead to the most probable 
translation, if we hadn   t pruned it early on

model errors

    model errors are different than search errors 

    if the model scores an incorrect translation higher 
than higher than the correct translation, then it is 
the model   s fault not the search strategy   s fault 

    how can these two interact? 

translating the easy part first?

the tourism initiative addresses this for the    rst time

the
die

tourism

touristische

initiative
initiative

tm:-0.19,lm:-0.4,

tm:-1.16,lm:-2.93

tm:-1.21,lm:-4.67

d:0, all:-0.65

d:0, all:-4.09

d:0, all:-5.88

the    rst time

das erste mal
tm:-0.56,lm:-2.81
d:-0.74. all:-4.11 

both hypotheses translate 3 words
worse hypothesis has better score

chapter 6: decoding

25

estimating future cost

    future cost estimate: how expensive is translation of rest of sentence?
    optimistic: choose cheapest translation options
    cost for each translation option

    translation model: cost known

    language model: output words known, but not context

! estimate without context

    reordering model: unknown, ignored for future cost estimation

chapter 6: decoding

26

cost estimates from translation options

the   tourism  initiative addresses  this    for     the       rst   time

-1.0

-2.0

-1.5

-2.4

-1.4

-1.0

-1.0

-1.9

-1.6

-4.0

-2.5

-2.2

-1.3

-2.4

-2.7

-2.3

-2.3

-2.3

cost of cheapest translation options for each input span (log-probabilities)

chapter 6: decoding

27

cost estimates for all spans

    compute cost estimate for all contiguous spans by combining cheapest options

   rst
word
the

1
-1.0
tourism -2.0
initiative
-1.5
addresses
-2.4
-1.4
-1.0
-1.0
-1.9
-1.6

this
for
the
   rst
time

future cost estimate for n words (from    rst)

9

-10.6

8

-10.6
-9.6

7
-9.6
-9.6
-7.6

6
-9.3
-8.6
-7.6
-6.1

5
-8.3
-8.3
-6.6
-6.1
-3.7

4
-6.9
-7.3
-6.3
-5.1
-3.7
-2.3

3
-4.5
-5.9
-5.3
-4.8
-2.7
-2.3
-2.3

2
-3.0
-3.5
-3.9
-3.8
-2.4
-1.3
-2.2
-2.4

    function words cheaper (the: -1.0) than content words (tourism -2.0)
    common phrases cheaper (for the    rst time: -2.3)
than unusual ones (tourism initiative addresses: -5.9)

chapter 6: decoding

28

combining score and future cost

-6.1

-9.3

-6.9

-2.2

the tourism initiative
die touristische 

initiative

tm:-1.21,lm:-4.67

d:0, all:-5.88

-6.1 +

-5.88

=

-11.98

the    rst time

das erste mal
tm:-0.56,lm:-2.81
d:-0.74. all:-4.11 

-9.3 +

-4.11

=

-13.41

this for ... time
f  r diese zeit
tm:-0.82,lm:-2.98
d:-1.06. all:-4.86 

-9.1 +

-4.86

=

-13.96

    hypothesis score and future cost estimate are combined for pruning

    left hypothesis starts with hard part: the tourism initiative

score: -5.88, future cost: -6.1 ! total cost -11.98

    middle hypothesis starts with easiest part: the    rst time

score: -4.11, future cost: -9.3 ! total cost -13.41
    right hypothesis picks easy parts: this for ... time
score: -4.86, future cost: -9.1 ! total cost -13.96

chapter 6: decoding

29

alternate to future cost 

estimation
stacks, n=number of source words 

    in the setup that we described so far, we had n 

    hypotheses were added to the stack based on 

how many source words they covered 

    we could group hypotheses into stacked based on 

which source words they cover 

    this eliminates the needs for fce, but requires 

more stacks

other decoding algorithms

    id67
    greedy hill-climbing
    using    nite state transducers (standard toolkits)

chapter 6: decoding

30

id67

    alternative path leading to 
hypothesis beyond threshold

e

t

a
m

i
t
s
e

 
c
i
t
s
i
r
u
e
h
+
 
y
t
i
l
i

 

b
a
b
o
r
p

cheapest score

    recombination

    depth-   rst

expansion to completed path

number of words covered 

    uses admissible future cost heuristic: never overestimates cost
    translation agenda: create hypothesis with lowest score + heuristic cost
    done, when complete hypothesis created

chapter 6: decoding

31

greedy hill-climbing

    create one complete hypothesis with depth-   rst search (or other means)
    search for better hypotheses by applying change operators

    change the translation of a word or phrase
    combine the translation of two words into a phrase
    split up the translation of a phrase into two smaller phrase translations
    move parts of the output into a di   erent position
    swap parts of the output with the output at a di   erent part of the sentence

    terminates if no operator application produces a better translation

chapter 6: decoding

32

reading

    read 6 from the textbook

announcements

    hw2 was released on friday
    hw2 due thursday feb 19th at 11:59pm
    start early!!  
    this assignment takes longer to run than the 
last hw, and a good strategy for doing well 
involves running the search with wider beam 
widths (taking more cpu cycles).

