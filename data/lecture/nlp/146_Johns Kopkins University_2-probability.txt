introduction to 

id203 and statistics

machine translation 

lecture 2 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

last time ...

1) formulate a model of pairs of sentences.   
2) learn an instance of the model from data.   
3) use it to infer translations of new inputs.

why id203?

    id203 formalizes ...
    the concept of models
    the concept of data
    the concept of learning
    the concept of id136 (prediction)

id203 is expectation founded 
upon partial knowledge.

p(x | partial knowledge)

   partial knowledge    is an apt description of   
what we know about language and translation!

id203 models

    the space of events (   or     )

    key components of a id203 model

    the assumptions about conditional 
independence / dependence among events
    functions assigning id203 (density) to 
    we will assume discrete distributions.

events

events and random variables
a random variable is a function from a random event 
from a set of possible outcomes (  ) and a id203 

distribution (    ), a function from outcomes to 

probabilities.

    = {1, 2, 3, 4, 5, 6}

x(!) = !

   x(x) =( 1

6
0

if x = 1, 2, 3, 4, 5, 6
otherwise

probabilities.

events and random variables
a random variable is a function from a random event 
from a set of possible outcomes (  ) and a id203 

distribution (    ), a function from outcomes to 
y (!) =(0
   y (y) =( 1

if ! 2 {2, 4, 6}
otherwise

    = {1, 2, 3, 4, 5, 6}

if y = 0, 1
otherwise

2
0

1

what is our event space?

what are our random 

variables?

id203 distributions

a id203 distribution (    x) assigns probabilities to   

the values of a random variable (x).
there are a couple of philosophically different ways   
to de   ne probabilities, but we will give only the invariants 
in terms of random variables.

   x(x) = 1

xx2x

   x(x)   0 8x 2 x

id203 distributions of a random variable may be 
speci   ed in a number of ways.

specifying distributions
    engineering/mathematical convenience
    important techniques in this course
    id203 mass functions
    tables (   stupid multinomials   )
    log-linear parameterizations (maximum 
id178, random    eld, multinomial logistic 
regression)

    construct random variables from other r.v.   s 

with known distributions

sampling notation

x = 4     z + 1.7
y     distribution(   )

distribution

expression

variable

random variable

parameter

sampling notation

x = 4     z + 1.7
y     distribution(   )

distribution

random variable

parameter

sampling notation

x = 4     z + 1.7
y     distribution(   )
y0 = y     x

multivariate r.v.   s

id203 theory is particularly useful because it lets   
us reason about (cor)related and dependent events.
a joint id203 distribution is a id203   
distribution over r.v.   s with the following form:

z =   x(!)
y (!) 
   z      x

y       0 8x 2 x , y 2 y

xx2x ,y2y

   z      x

y     = 1

    = {1, 2, 3, 4, 5, 6}

x(!) = !

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

x(!) = !1 y (!) = !2

   x,y (x, y) =( 1

36
0

if (x, y) 2    
otherwise

    = {1, 2, 3, 4, 5, 6}

x(!) = !

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

x(!) = !1 y (!) = !2

   x,y (x, y) =( x+y

252
0

if (x, y) 2    
otherwise

marginal id203

p(x = x, y = y) =    x(x, y)

p(x = x) = xy0=y
p(y = y) = xx0=x

p(x = x, y = y0)

p(x = x0, y = y)

p(x = 4) = xy02[1,6]

p(x = 4, y = y0)

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

p(y = 3) = xx02[1,6]

p(x = x0, y = 3)

   x,y (x, y) =( 1

36
0

if (x, y) 2    
otherwise

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

6
36

=

1
6

   x,y (x, y) =( x+y

252
0

if (x, y) 2    
otherwise

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

4 + 1 + 4 + 2 + 4 + 3 + 4 + 4 + 4 + 5 + 4 + 6

252

=

45
252

id155

the id155 of one random variable given 
another is de   ned as follows:

p(x = x, y = y)

joint id203

marginal

=

p(y = y)

p(x = x | y = y) =
given that  p(y) 6= 0
id155 distributions are   
useful for specifying joint distributions since:
p(x | y)p(y) = p(x, y) = p(y | x)p(x)

why might this be useful?

id155 

distributions

a id155 distribution is a   
id203 distribution over r.v.   s x and y with the   
form                . 
   x|y =y(x)

xx2x

   x|y =y(x) 8y 2 y

=1

chain rule

the chain rule is derived from a repeated application   
of the de   nition of id155:

p(a, b, c, d) = p(a | b, c, d)p(b, c, d)

= p(a | b, c, d)p(b | c, d)p(c, d)
= p(a | b, c, d)p(b | c, d)p(c | d)p(d)

use as many times as necessary!

bayes    rule

posterior

p(x | y)p(y) = p(x, y) = p(y | x)p(x)
likelihood

p(x | y)p(y) = p(y | x)p(x)

prior

p(x | y) =

p(y | x)p(x)

p(y)

evidence

   =

p(y | x)p(x)

px0 p(y | x0)p(x0)   

independence

two random variables are independent iff

p(x = x, y = y) = p(x = x)p(y = y)

equivalently, (use def. of cond. prob to prove)

p(x = x | y = y) = p(x = x)

equivalently again:

p(y = y | x = x) = p(y = y)

   knowing about x doesn   t tell me about y   

   x,y (x, y) =( 1

36
0

if (x, y) 2    
otherwise

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

   x,y (x, y) =( x+y

252
0

if (x, y) 2    
otherwise

    = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6),
(2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6),
(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6),
(4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6),
(5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6),
(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6),}

independence

independence has practical bene   ts. think about
how many parameters you need for a naive 
parameterization of                 vs          and

   x,y (x, y)

   y (y)

   x(x)

o(xy)

vs

o(x + y)

conditional independence
two equivalent statements of conditional   
independence:

and:

p(a, c | b) = p(a | b)p(c | b)

p(a | b, c) = p(a | b)

   if i know b, then c doesn   t tell me about a   

conditional independence

p(a, b, c) = p(a | b, c)p(b, c)

= p(a | b, c)p(b | c)p(c)

   if i know b, then c doesn   t tell me about a   

p(a | b, c) = p(a | b)

p(a, b, c) = p(a | b, c)p(b, c)

= p(a | b, c)p(b | c)p(c)
= p(a | b)p(b | c)p(c)
do we need more parameters or fewer 
parameters in conditional independence?

independence
    some variables are independent in nature
    how do we know?
    some variables we pretend are independent for 
computational convenience
    examples?
    assuming independence is equivalent to letting our 
model    forget    something that happened in its past
    what should we forget in language?

a word about data
    when we formulate our models there will 
be two kinds of random variables: observed 
and latent
    observed: words, sentences(?), parallel 
    latent: parameters, syntax,    meaning   , 

corpora, web pages, formatting...

word alignments, translation 
dictionaries...

interlingua
   meaning   

report_event[	
   
	
   	
   	
   	
   factivity=true   
	
   	
   	
   	
   explode(e,	
   bomb,	
   car)	
   
	
   	
   	
   	
   loc(e,	
   downtown)	
   
]

explodieren   
	
   	
   	
   	
   :arg0	
   bombe	
   
	
   	
   	
   	
   :arg1	
   auto	
   
	
   	
   	
   	
   :loc	
   innenstadt	
   
	
   	
   	
   	
   :tempus	
   imperf

hidden

detonate   
	
   	
   	
   	
   :arg0	
   bomb	
   
	
   	
   	
   	
   :arg1	
   car	
   
	
   	
   	
   	
   :loc	
   downtown	
   
	
   	
   	
   	
   :time	
   past

in	
   der	
   innenstadt	
   explodierte	
   eine	
   autobombe

a	
   car	
   bomb	
   exploded	
   downtown

!

!

in	
   der	
   innenstadt	
   explodierte	
   eine	
   autobombe

a	
   car	
   bomb	
   exploded	
   downtown

la empresa tiene enemigos fuertes en europa .
the company has strong enemies in europe .
garcia and associates .

observed

the clients and the associates are enemies .

garcia y asociados .

carlos garcia has three associates .

los clientes y los asociados son enemigos .

the company has three groups .

carlos garcia tiene tres asociados .

his associates are not strong .

la empresa tiene tres grupos .

its groups are in europe .

sus asociados no son fuertes .
garcia has a company also .

sus grupos estan en europa .

the modern groups sell strong pharmaceuticals .

garcia tambien tiene una empresa .

its clients are angry .

los grupos modernos venden medicinas fuertes .

the groups do not sell zanzanine .

sus clientes estan enfadados .
the associates are also angry .

los grupos no venden zanzanina .
the small groups are not modern .

los asociados tambien estan enfadados .

los grupos pequenos no son modernos .

la empresa tiene enemigos fuertes en europa .
the company has strong enemies in europe .
garcia and associates .

hidden

the clients and the associates are enemies .

garcia y asociados .

carlos garcia has three associates .

los clientes y los asociados son enemigos .

the company has three groups .

carlos garcia tiene tres asociados .

his associates are not strong .

la empresa tiene tres grupos .

its groups are in europe .

sus asociados no son fuertes .
garcia has a company also .

sus grupos estan en europa .

the modern groups sell strong pharmaceuticals .

garcia tambien tiene una empresa .

its clients are angry .

los grupos modernos venden medicinas fuertes .

the groups do not sell zanzanine .

sus clientes estan enfadados .
the associates are also angry .

los grupos no venden zanzanina .
the small groups are not modern .

los asociados tambien estan enfadados .

los grupos pequenos no son modernos .

learning

    let   s say we have formulated a model of a 
phenomenon
    made independence assumptions
    figured out what kinds of parameters we 
    let   s say we have collected data we assume to 
be generated by this model
    e.g. some parallel data

want

what do we do now?

parameter estimation
    inputs

    given a model with unspeci   ed parameters
    given some data

    goal: learn model parameters
    how?

that look like the data do

    find parameters that make the model make predictions 
    what do we mean    look like the data?   

    id203 (other options: accuracy, moment matching)

strategies

    id113 
    what is the id203 of generating the data?
    accuracy 
    using an auxiliary similarity function,    nd 
parameters that maximize the (expected?) 
accuracy of data

    bayesian techniques

p(heads)

1   p(heads)

p(heads) ?

p(data) = p(heads)7
p(data) = p(heads)7

   p(tails)3
   [1     p(heads)]3

p(data)

0

p(heads)

1

p(data)

0

.7

1

p(heads)

optimization

    for the most part, we will be working with maximum 
likelihood estimation
    the general recipe is:

    come up with an expression of the likelihood of your 
id203 model, as a function of data and the model 
parameters

    set the parameters to maximize the likelihood 
    this optimization is generally dif   cult

sum to 1, etc)

    you must respect any constraints on the parameters (>0, 
    there may not be analytical solutions (id148)

id203 lets us

1) formulate a model of pairs of sentences.   
2) learn an instance of the model from data.   
3) use it to infer translations of new inputs.

key concepts

     joint probabilities
    marginal probabilities 
    conditional probabilities 
    chain rule
    bayes    rule
    independence
    latent versus observed variables
    id113 

supplemental reading

    if this was unfamiliar to 
you, then please read 
chapter 3 from the 
textbook "statistical 
machine translation" by 
philipp koehn

announcements

    hw 0 has been posted on the web site.
    it   s a setup assignment to make sure that 
you can upload results, have them scored, 
and that they correctly appear on the 
leaderboard

