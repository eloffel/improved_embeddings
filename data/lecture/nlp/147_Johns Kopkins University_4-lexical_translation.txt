lexical translation 

models 1

machine translation 

lecture 4 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

lexical translation
    how do we translate a word? look it up in the 
haus : house, home, shell, household

dictionary

    multiple translations
    different word senses, different registers, 
    house, home are common
    shell is specialized (the haus of a snail is a shell)

different in   ections (?)

how common is each?

translation

count

house

home

shell

household

5000

2000

100

80

id113

  pid113(e | haus) =

0.696
0.279
0.014
0.011
0

if e = house
if e = home
if e = shell
if e = household
otherwise

8>>>>>><>>>>>>:

lexical translation

    goal: a model   p(e | f, m)
    where  e and  f  are complete english and foreign sentences
    lexical translation makes the following assumptions:

f

e

in

ei

    each word in     in    is generated from exactly one word 
    thus, we have an alignment     that indicates which word   
    given the alignments    , translation decisions are 

ei
fai
        came from   , speci   cally it came from       .

conditionally independent of each other and depend only 
on the aligned source word    .

ai

a

lexical translation

    goal: a model   p(e | f, m)
    where  e and  f  are complete english and foreign sentences
    lexical translation makes the following assumptions:

ei
e = he1, e2, . . . , emi

    each word in     in    is generated from exactly one word 
    thus, we have an alignment     that indicates which word   
    given the alignments    , translation decisions are 

ei
fai
        came from   , speci   cally it came from       .

conditionally independent of each other and depend only 
on the aligned source word    .

ai

in

a

e

f

lexical translation

    goal: a model   p(e | f, m)
    where  e and  f  are complete english and foreign sentences
    lexical translation makes the following assumptions:

ei
e = he1, e2, . . . , emi

    each word in     in    is generated from exactly one word 
    thus, we have an alignment     that indicates which word   
    given the alignments    , translation decisions are 

f = hf1, f2, . . . , fni
ei
fai
        came from   , speci   cally it came from       .

conditionally independent of each other and depend only 
on the aligned source word    .

ai

in

a

e

f

lexical translation

    goal: a model    p(e | f, m)
    where  e and  f  are complete english and foreign sentences 
    lexical translation makes the following assumptions:

    each word ei in e is generated from exactly one word in f
    thus, we have an alignment ai that indicates which word   
ei    came from   , speci   cally it came from fai .
    given the alignments a, translation decisions are 

conditionally independent of each other and depend only 
on the aligned source word fai .

lexical translation

    putting our assumptions together, we have:
p(e | f, m) = xa2[0,n]m

p(a | f, m)    

p(ei | fai)

myi=1

alignment translation | alignment

   

lexical translation
p(ei | fai)

lexical translation
p(ei | fai)

p(house | haus)

lexical translation
p(ei | fai)

p(house | haus)

p(shell | haus)

lexical translation
p(ei | fai)

p(house | haus)

p(shell | haus)

remember bigram models...

lexical translation

    putting our assumptions together, we have:
p(e | f, m) = xa2[0,n]m

p(a | f, m)    

p(ei | fai)

myi=1

alignment translation | alignment

   

alignment

p(a | f, m)

most of the action for the    rst 10 years   
of mt was here. words weren   t the problem,   
word order was hard.

alignment

    alignments can be visualized in by drawing 
links between two sentences, and they are 
represented as vectors of positions:

a = (1, 2, 3, 4)>

reordering
    words may be reordered during 

translation.

a = (3, 4, 2, 1)>

word dropping

    a source word may not be translated at all

a = (2, 3, 4)>

word insertion

    words may be inserted during translation
english just does not have an equivalent   
but it must be explained - we typically assume   
every source sentence contains a null token

a = (1, 2, 3, 0, 4)>

one-to-many translation
    a source word may translate into more 

than one target word

a = (1, 2, 3, 4, 4)>

many-to-one translation
    more than one source word may not 
translate as a unit in lexical translation

1
das

2

haus

3

brach

4

zusammen

the
1

house

2

collapsed

3

a =???

many-to-one translation
    more than one source word may not 
translate as a unit in lexical translation

1
das

2

haus

3

brach

4

zusammen

the
1

house

2

collapsed

3

a =???

a = (1, 2, (3, 4)>)> ?

ibm model 1

    simplest possible lexical translation model
    additional assumptions
    the m alignment decisions are independent
    the alignment distribution for each    is uniform 

ai

over all source words and null

for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

historical note

   why ibm models?

historical note

   why ibm models?

historical note

   why ibm models?

fred jelinek
(1932-2010)

historical note

   why ibm models?

some of us started to wonder in the mid 
1980s whether our [id103] 
methods could be successfully applied to 
new    elds. bob mercer and i spent many of 

our after-lunch    periphery    walks 

discussing possible candidates. we soon 
came up with two: machine translation and 

stock market modeling

fred jelinek
(1932-2010)

historical note

   why ibm models?

fred jelinek
(1932-2010)

historical note

   why ibm models?

   the validity of a statistical (information 
theoretic) approach to mt has indeed been 

recognized, as the authors mention, by weaver 

as early as 1949. and was universally 

recognized as mistaken by 1950 (cf. hutchins, 
mt     past, present, future, ellis horwood, 

1986, p. 30ff and references therein). the crude 
force of computers is not science. the paper is 

simply beyond the scope of coling.   

fred jelinek
(1932-2010)

historical note

   why ibm models?

fred jelinek
(1932-2010)

historical note

   why ibm models?

fred jelinek
(1932-2010)

historical note

   why ibm models?

fred jelinek
(1932-2010)

ibm model 1
for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

p(e, a | f, m) =

myi=1

ibm model 1
for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

p(e, a | f, m) =

myi=1

1

1 + n

ibm model 1
for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

p(e, a | f, m) =

myi=1

1

1 + n

p(ei | fai)

ibm model 1
for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

p(e, a | f, m) =

myi=1

1

1 + n

p(ei | fai)

ibm model 1
for each i 2 [1, 2, . . . , m]

ai     uniform(0, 1, 2, . . . , n)
ei     categorical(   fai

)

p(e, a | f, m) =

myi=1

1

1 + n

p(ei | fai)

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)

p(e, a | f, m) =

myi=1

p(ei, ai | f, m)

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

nxai=0

recall our independence assumption: all alignment decisions are 
independent of each other, and given the alignments then all 
translation decisions are independent of each other, so all 
translation decisions are independent of each other.

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

nxai=0

recall our independence assumption: all alignment decisions are 
independent of each other, and given the alignments then all 
translation decisions are independent of each other, so all 
translation decisions are independent of each other.

p(a, b, c, d) = p(a)p(b)p(c)p(d)

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

nxai=0

recall our independence assumption: all alignment decisions are 
independent of each other, and given the alignments then all 
translation decisions are independent of each other, so all 
translation decisions are independent of each other.

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

nxai=0

recall our independence assumption: all alignment decisions are 
independent of each other, and given the alignments then all 
translation decisions are independent of each other, so all 
translation decisions are independent of each other.

p(e | f, m) =

myi=1

p(ei | f, m)

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

p(e | f, m) =

p(ei | f, m)

nxai=0
myi=1

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

p(e | f, m) =

p(ei | f, m)

nxai=0
myi=1
myi=1

=

=

nxai=0

1

(1 + n)m

1

1 + n

p(ei | fai)

myi=1

nxai=0

p(ei | fai)

marginal id203

p(ei, ai | f, m) =

1

1 + n

p(ei | fai)
1

p(ei | f, m) =

1 + n

p(ei | fai)

p(e | f, m) =

p(ei | f, m)

nxai=0
myi=1
myi=1

=

=

nxai=0

1

(1 + n)m

1

1 + n

p(ei | fai)

myi=1

nxai=0

p(ei | fai)

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

4

start with a foreign sentence and a target length.

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

house

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

house

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

house

2

is
3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

house

2

is
3

4

example

0

null

1
das

2

haus

3
ist

4

klein

the
1

house

2

is
3

small

4

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

4

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

the
4

example

0

null

1
das

2

haus

3
ist

4

klein

1

2

3

the
4

example

0

null

1
das

2

haus

3
ist

4

klein

house

1

2

3

the
4

example

0

null

1
das

2

haus

3
ist

4

klein

house

1

2

3

the
4

example

0

null

1
das

2

haus

3
ist

4

klein

house

1

is
2

the
4

3

example

0

null

1
das

2

haus

3
ist

4

klein

house

1

is
2

the
4

3

example

0

null

1
das

2

haus

3
ist

4

klein

house

1

is
2

small

3

the
4

finding the viterbi 

alignment

a    = arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

p(a | e, f)

p(e, a | f)

pa0 p(e, a0 | f)

p(e, a | f)

a   i = arg

= arg

n

max
ai=0

n

max
ai=0

p(ei | fai)

1

1 + n
p(ei | fai)

finding the viterbi 

alignment

a    = arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

p(a | e, f)

p(e, a | f)

pa0 p(e, a0 | f)

p(e, a | f)

a   i = arg

= arg

n

max
ai=0

n

max
ai=0

p(ei | fai)

1

1 + n
p(ei | fai)

finding the viterbi 

alignment

a    = arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

p(a | e, f)

p(e, a | f)

pa0 p(e, a0 | f)

p(e, a | f)

a   i = arg

= arg

n

max
ai=0

n

max
ai=0

p(ei | fai)

1

1 + n
p(ei | fai)

finding the viterbi 

alignment

a    = arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

p(a | e, f)

p(e, a | f)

pa0 p(e, a0 | f)

p(e, a | f)

a   i = arg

= arg

n

max
ai=0

n

max
ai=0

p(ei | fai)

1

1 + n
p(ei | fai)

finding the viterbi 

alignment

a    = arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

= arg max

a2[0,1,...,n]m

p(a | e, f)

p(e, a | f)

pa0 p(e, a0 | f)

p(e, a | f)

a   i = arg

= arg

n

max
ai=0

n

max
ai=0

p(ei | fai)

1

1 + n
p(ei | fai)

historical note #2

the viterbi algorithm is a dynamic 
programming algorithm for    nding the 
most likely sequence of hidden states 
    called the viterbi path     that 
results in a sequence of observed 
events, especially in the context of 
markov information sources and 
id48.

   andrew viterbi
   professor at usc 
   co-founder of qualcomm
   classmates with fred jelinek

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

finding the viterbi 

alignment

0

null

1
das

2

haus

3
ist

4

klein

the
1

home

2

is
3

little

4

learning lexical 
translation models

p(e | f )

    how do we learn the parameters
       chicken and egg    problem
    if we had the alignments, we could 
    if we had parameters, we could    nd the 

estimate the parameters (id113)

most likely alignments

em algorithm

    pick some random (or uniform) parameters
    repeat until you get bored (~ 5 iterations for lexical translation 

models)
    using your current parameters, compute    expected    alignments 

for every target word token in the training data
(on board)

p(ai | e, f)

throughout the whole corpus

    keep track of the expected number of times f translates into e 
    keep track of the expected number of times that f is used as 
    use these expected counts as if they were    real    counts in the 

the source of any translation

standard id113 equation

em for model 1

em for model 1

em for model 1

em for model 1

em for model 1

convergence

evaluation

    since we have a probabilistic model, we can 

evaluate perplexity.

ppl = 2 

1

p(e,f)2d |e|

logq(e,f)2d p(e|f)

iter 1 iter 2 iter 3 iter 4

-log likelihood

perplexity

-

-

7.66

7.21

6.84

2.42

2.3

2.21

iter    

-6

2

...

...

...

alignment error rate

alignment error rate

possible links

p

alignment error rate

possible links

p

alignment error rate

possible links

p

sure links

s

alignment error rate

possible links

p

sure links

s

alignment error rate

possible links

p

sure links

s

precision(a, p ) = |p \ a|

|a|

alignment error rate

possible links

p

sure links

s

precision(a, p ) = |p \ a|

|a|

recall(a, s) = |s \ a|

|s|

alignment error rate

possible links

p

sure links

s

precision(a, p ) = |p \ a|

recall(a, s) = |s \ a|

|a|

|s|

aer(a, p, s) = 1   |s \ a| + |p \ a|

|s| + |a|

alignment error rate

possible links

p

sure links

s

precision(a, p ) = |p \ a|

recall(a, s) = |s \ a|

|a|

|s|

aer(a, p, s) = 1   |s \ a| + |p \ a|

|s| + |a|

alignment error rate

possible links

p

sure links

s

precision(a, p ) = |p \ a|

recall(a, s) = |s \ a|

|a|

|s|

aer(a, p, s) = 1   |s \ a| + |p \ a|

|s| + |a|

reading

    read chapter 4 from the 

textbook (today we 
covered 4.1 and 4.2)

announcements

    hw 1 is due in 1 week

