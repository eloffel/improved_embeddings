discriminative training

part 2

machine translation 

lecture 12 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

the noisy channel

-log p(g | e)

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)
improvement 1:

change      to    nd better translations

~w

~wg

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

improvement 2:

add dimensions to make points separable

~w

-log p(e)

linear models

e    = arg max

e

w>h(g, e)

    improve the modeling capacity of the noisy 
channel in two ways
    reorient the weight vector
    add new dimensions (new features)
    questions
    what features?
    how do we set the weights?

h(g, e)

w

parameter learning

12

hypothesis space

h1

hypotheses

h2

13

hypothesis space

h1

references

h2

14

preliminaries

we assume a decoder that computes:

he   , a   i = arg max
he,ai

w>h(g, e, a)

and k-best lists of, that is:

{ e   i , a   i   }i=k

i=1 = arg ith- max
he,ai

w>h(g, e, a)

standard, ef   cient algorithms exist for this.

15

cost-sensitive training
    assume we have a cost function that gives 

a score for how good/bad a translation is

 (  e,e)      [0, 1]

    optimize the weight vector by making 
reference to this function
    we will talk about two ways to do this

16

mert
    minimum error rate training
    directly optimize for an automatic evaluation 
    maximize the id7 score on a held out 
    iteratively update the parameters by re-

metric instead of likelihood

development set 

scoring n-best lists and comparing the highest 
scoring translation to the reference

17

mert

    even with 10-15 features it   s not possible to 

exhaustively search the space of possible 
feature values

search the space

    we need a good heuristic method to 
    another problem: the initial parameters 
might be so bad that the original n-best list 
is not a good sample of the translations

18

iterative parameter 

tuning

powell search

a better point along one line in the space

    explore a high-dimensional space by    nding 
    simplest form:  vary one parameter at a time
    if the optimal value is better than the 

current value, then change it and move to 
the next parameter

    iterate until there are no single parameter 

updates that increase the score

powell search

    problem: searching for the best value for a 
single parameter is still daunting
    parameters are real-valued #s, so they 
have an in   nite number of possible values
    key insight of mert: only a small number of 

threshold values will change the 1-best 
translation
    only 1-best translations change id7

finding the threshold 
points for 1 sentence
given weight vector    , any hypothesis             
he, ai
will have a (scalar) score
m = w>h(g, e, a)
now pick a search vector v, and consider   
how the score of this hypothesis will change:

w

wnew = w +  v

m = (w +  v)>h(g, e, a)

= w>h(g, e, a)

+  v>h(g, e, a)

b
= a  + b

m

{z

|

}

|

a

{z

}

linear function in 2d!

mert

m

 

23

mert

m

recall our k-best set { e   i , a   i   }k

i=1

 

24

mert

m

recall our k-best set { e   i , a   i   }k

i=1

 

25

mert

m

 

26

mert

m

he   162, a   162i

he   73, a   73i

he   28, a   28i

 

27

mert

m

he   162, a   162i

he   73, a   73i

he   28, a   28i

 

28

he   162, a   162i

mert

m

he   28, a   28i

errors

he   73, a   73i

 

 

29

he   162, a   162i

mert

m

he   28, a   28i

errors

he   73, a   73i

 

 

30

mert

m

errors

 

 

31

mert

m

errors

 
 

 

32

errors

    

 

let

wnew =     v + w

33

the effect on id7 
varying one parameter

the effect on id7 
varying one parameter

mert

    minimum error rate training 
    can maximize or minimize!
    in practice    errors    are suf   cient statistics 
for id74 (e.g., id7,  amber, 
ter, etc)

    downside: mert can only be used to 

optimize a small handful of features

36

training as classi   cation
    pairwise ranking optimization

    reduce training problem to binary classi   cation with a 

linear model 

    algorithm 

    for i=1 to n 

    pick random pair of hypotheses (a,b) from k-best list
    use cost function to determine if is a or b better
    create ith training instance
    train binary linear classi   er

37

k-best list example

h1

#2#1

~w

#3

#6

#4#5

#7

#8

#10

#9

h2

38

k-best list example

h1

#2#1

~w

#3

#6

#4#5

#7

#8

#10

#9

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

h2

39

h1

h1

#2 #1

#3

#6

#5

#4

#7

#8

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#10

#9

h2

h2

40

h1

h1

#2 #1

worse!

#3

#6

#5

#4

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#7

#8

#10

#9

h2

h2

41

h1

h1

#2 #1

worse!

#3

#6

#5

#4

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#7

#8

#10

#9

h2

h2

42

h1

h1

#2 #1

#3

#6

#5

#4

#7

#8

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#10

#9

h2

h2

43

h1

h1

#2 #1

#3

#6

#5

#4

#7

#8

better!

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#10

#9

h2

h2

44

h1

h1

#2 #1

#3

#6

#5

#4

#7

#8

better!

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#10

#9

h2

h2

45

h1

h1

#2 #1

worse!

#3

#6

#5

#4

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#7

#8

#10

#9

h2

h2

46

h1

h1

#2 #1

#3

#6

#5

#4

#7

#8

better!

#10

#9

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

h2

h2

47

h1

h1

#2 #1

#3

#6

#5

#4

#7

#8

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

#10

#9

h2

h2

48

h1

fit a linear model

h2

49

h1

~w

fit a linear model

h2

50

k-best list example

h1

#2#1

#3

#6

#4#5

#7

#8

~w

#10

#9

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

h2

51

summary

    id74
    figure out how well we   re doing
    figure out if a feature helps
    train your system
    what   s a great way to improve translation?
    improve evaluation!

52

reading

textbook

    read chapter 9 from the 
    hw4 will be a 

discriminative re-ranking 
project

announcements

from thursday.

    hw3 has been released.  it is due a week 
    upcoming: 
    term project (25% of your    nal grade) and 
    these are group projects (2-6 students), 
where the work scales to the group size
    speci   cations will be posted soon

the   language research project (10%)

term project

and  evaluate its performance

    problem description     similar to the 
descriptions on the hw assignments
    data collection     used to train a model, 
    objective function     score submissions on 
    default system     an implementation of 
    baseline system      an implementation of a 

the simplest possible solution

a leaderboard

published baseline

language research

the language

    gather monolingual and bilingual data for 
    investigate where it is spoken, and what 
other languages its speakers are exposed to
    collect information about the syntax and 
morphology of the language
    describe its writing system
    create your own nlp tools for the 
language (# will vary by team size)

