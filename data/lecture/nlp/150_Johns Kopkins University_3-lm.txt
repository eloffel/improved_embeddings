language models

machine translation 

lecture 3 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

no mt yet

    today we will talk about models of p(sentence)
    the rest of this semester will deal with   
    why do it this way?

     p(translated sentence | input sentence)

    conditioning on more stuff makes modeling more 

complicated. that is: p(sentence) is easier than 
p(translated sentence | input sentence).

    language models are arguably the most important 

models in statistical mt

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

my legal name is alexander perchov. but all of my 
many friends dub me alex, because that is a more 
   accid-to-utter version of my legal name. mother 
dubs me alexi-stop-spleening-me!, because i am 
always spleening her. if you want to know why i 
am always spleening her, it is because i am always 
elsewhere with friends, and disseminating so much 
currency, and performing so many things that can 
spleen a mother. father used to dub me shapka, 
for the fur hat i would don even in the summer 
month. he ceased dubbing me that because i 
ordered him to cease dubbing me that. it sounded 
boyish to me, and i have always thought of myself 
as very potent and generative. i have many many 
girls, believe me, and they all have a different 
name for me. one dubs me baby, not because i am 
a baby, but because she attends to me.

language models matter

    language models play the role of ...
    a judge of grammaticality
    a judge of semantic plausibility
    an enforcer of stylistic consistency
    a repository of knowledge (?)

what is the id203 

of a sentence?

string of words)

    requirements
    assign a id203 to every sentence (i.e., 
    questions
    how many sentences are there in 
    too many :)

english?

what is the id203 

of a sentence?

string of words)

    requirements
    assign a id203 to every sentence (i.e., 
    questions
xe2      
    how many sentences are there in 
plm(e)   0 8e 2       
    too many :)

plm(e) = 1

english?

why do we want to 

estimate the id203 

of a sentence?

    goal:  assign a higher id203 to good 

sentences in english

plm(the house is small) > plm(small the is house)

translations of german haus: home, house    
plm(i am going home) > plm(i am going house)

id165 lms

plm(e) =p(e1, e2, e3, . . . , e`)

=

p(e1)   
p(e2 | e1)   
p(e3 | e1, e2)   
vector-valued random variable
p(e4 | e1, e2, e3)   
         
p(e` | e1, e2, . . . , e` 2, e` 1)

id165 lms

plm(e) =p(e1, e2, e3, . . . , e`)

   

p(e1)   
p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
         
p(e` | e1, e2, . . . , e` 2, e` 1)

chain rule

the chain rule is derived from a repeated application   
of the de   nition of id155:

p(a, b, c, d) = p(a | b, c, d)p(b, c, d)

= p(a | b, c, d)p(b | c, d)p(c, d)
= p(a | b, c, d)p(b | c, d)p(c | d)p(d)

conditional independence

p(a, b, c) = p(a | b, c)p(b, c)

= p(a | b, c)p(b | c)p(c)

   if i know b, then c doesn   t tell me about a   

p(a | b, c) = p(a | b)

p(a, b, c) = p(a | b, c)p(b, c)

= p(a | b, c)p(b | c)p(c)
= p(a | b)p(b | c)p(c)

is the markov assumption 

valid for language?

    the old man are/is
    the pictures are/is
    the old man in the pictures is my dad.

id165 lms

plm(e) =p(e1, e2, e3, . . . , e`)

plm(e) =p(e1, e2, e3, . . . , e`)

   
p(e1)   
p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
         
p(e` | e1, e2, . . . , e` 2, e` 1)

   
p(e1)   
p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
         
p(e` | e1, e2, . . . , e` 2, e` 1)

which do you think is better? why?

id165 lms

plm(e) =p(e1, e2, e3, . . . , e`)

   
p(e1)   
p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
         
p(e` | e1, e2, . . . , e` 2, e` 1)

= p(e1 | start)    

p(ei | ei 1)     p(stop | e`)

`yi=2

start my

friends

call

me

alex

stop

p(my | start)

   p(friends | my)

   p(call | friends)

   p(me | call)

   p(alex | me)

   p(stop | alex)

start my

friends

dub

me

alex

stop

p(my | start)

   p(friends | my)

   p(dub | friends)    p(me | dub)

   p(alex | me)

   p(stop | alex)

these sentences have many terms in common.

categorical distributions
a categorical distribution characterizes   
a random event that can take on exactly one of   
k possible outcomes.   

(note: we often call these    multinomial distributions   )

p(x) =

if x = 1
if x = 2

p1
p2
. . .
pk if x = k
otherwise
0

8>>>>>><>>>>>>:

xi

pi = 1

pi   0 8i

p(  )

outcome

the
and
said
says
of
why
why

p
0.3
0.1
0.04
0.004
0.12
0.008
0.0007
0.00009

restaurant
destitute

0.00000064

id203 tables like this are the workhorses of   
language (and translation) modeling.

p(   | some context)

p(   | other context)

outcome

the
and
said
says
of
why
why

restaurant
destitute

p
0.6
0.04
0.009
0.00001

0.1
0.1

0.00008
0.0000008
0.00000064

outcome

the
and
said
says
of
why
why

restaurant
destitute

p
0.01
0.01
0.003
0.009
0.002
0.003
0.0006

0.2
0.1

p(   | some context)

p(   | other context)

p(   | in)
p
0.6
0.04
0.009
0.00001

0.1
0.1

0.00008
0.0000008
0.00000064

outcome

the
and
said
says
of
why
why

restaurant
destitute

outcome

the
and
said
says
of
why
why

p(   | the)
p
0.01
0.01
0.003
0.009
0.002
0.003
0.0006

restaurant
destitute

0.2
0.1

lm evaluation

for some task (mt, asr, etc.)

    extrinsic evaluation: build a new language model, use it 
    intrinsic: measure how good we are at modeling 
language
we will use perplexity to evaluate models   

given: w, plm

log2 plm(w)

1
|w|

ppl = 2
0     ppl     1

perplexity

translation quality for id165 models

    generally fairly good correlations with machine 
    perplexity is a generalization of the notion of branching 

factor
    how many choices do i have at each position?

choices per position

    state-of-the-art english lms have ppl of ~100 word 
    a uniform lm has a perplexity of 
    humans do much better
    ... and bad models can do even worse than uniform!

|   |

whence parameters?

estimation.

p(x | y) =

  pid113(x) =

  pid113(x, y) =

  pid113(x | y) =

p(x, y)
p(y)
count(x)

n

count(x, y)

n

count(x, y)

n

=

count(x, y)
count(y)

n

count(y)

   

  pid113(call | friends) =

count(friends call)

count(friends)

id113 & perplexity

    what is the lowest (best) perplexity 

possible for your model class?

    compute the id113!
    well, that   s easy...

start my

friends

call

me

alex

stop

p(my | start)

   p(friends | my)
-2.07101

id113

-3.65172

   p(call | friends)

-3.32231

   p(me | call)
-0.271271

   p(alex | me)

   p(stop | alex)

-4.961

-1.96773

start my

friends

dub

me

alex

stop

p(my | start)

   p(friends | my)
-2.07101

id113

-3.65172

   p(dub | friends)    p(me | dub)

   p(alex | me)

   p(stop | alex)

-   

-2.54562

-4.961

-1.96773

id113 assigns id203 zero   
to unseen events

zeros

    two kinds of zero probs:

impoverished observations

    sampling zeros: zeros in the id113 due to 
    structural zeros: zeros that should be there. do 

these really exist?

mean it doesn   t exist.

    just because you haven   t seen something, doesn   t 
    in practice, we don   t like id203 zero, even if 
there is an argument that it is a structural zero.

the a    s are nearing the end of their lease in oakland
the a 

smoothing

smoothing an refers to a family of estimation   
techniques that seek to model important general   
patterns in data while avoiding modeling noise or sampling   
artifacts. in particular, for id38, we seek   

p(e) > 0 8e 2       

we will assume that      is known and    nite.

   

add-1 smoothing

p(x | y) =

  pid113(x) =

  pid113(x, y) =

  pid113(x | y) =

=

p(x, y)
p(y)
count(x)

n

count(x, y)

n

count(x, y)

count(x, y)
count(y)

   
+1
+v

n

n

count(y)

what   s wrong with this?

p(x | y) =

  pid113(x) =

  pid113(x, y) =

  pid113(x | y) =

=

p(x, y)
p(y)
count(x)

n

count(x, y)

n

count(x, y)

count(x, y)
count(y)

   
+1
+  

n

n

count(y)

add-   smoothing

   

    add   <<1
    an optimal    can be analytically derived so 
that it gives an appropriate weight to 
unseen id165s

    simplest possible smoother
    but it doesn   t work well for language 

models

interpolation

       mixture of id113s   

  p(dub | my friends) = 3   pid113(dub | my friends)
+  2   pid113(dub | friends)
+  1   pid113(dub)

+  0

1
|   |

where do the lambdas come from?

discounting

discounting adjusts the frequencies of observed   
events downward to reserve id203 for the   
things that have not been observed.

note                            only when 

f (w3 | w1, w2) > 0

count(w1, w2, w3) > 0

we introduce a discounted frequency:

the total discount is the zero-frequency id203:

0     f   (w3 | w1, w2)     f (w3 | w1, w2)
 (w1, w2) = 1  xw0

f   (w0 | w1, w2)

back-off

recursive formulation of id203:

  pbo(w3 | w1, w2) =(f   (w3 | w1, w2)

   w1,w2      (w1, w2)       pbo(w3 | w1, w2)

{

if f   (w3 | w1, w2) > 0
otherwise

   back-off weight   

question: how do we discount?

witten-bell discounting
let   s assume that the id203 of a zero off   
can be estimated as follows:

a b c a b c a b x a b c c a b a b x c

 (a, b) /

1+1+1

=3

t(a, b) = |{x : count(a, b, x) > 0}|

 (a, b) =

t(a, b)

count(a, b) + t(a, b)

f   (c | a, b) =

count(a, b, c)

count(a, b) + t(a, b)

example: spite v. constant
    spite and constant both appear in the europarl corpus 993 

times

    spite only has 9 words that follow it (979 times it was 

followed by of because it is used in the phrase in spite of)

    constant has 415 words that follow it: and (42 times), 
concern (27 times), pressure (26 times), plus long tail 
including 268 that only appear once

    how likely is it that we   ll see a previously unseen bigram 

that starts with spite v. constant?

example: spite v. constant

t(spite) = 9

  (spite) = t(spite) / (count(spite) + t(spite))

  (spite) = 9/(9+993) = .00898

t(constant) = 415

  (constant) = t(constant) / (count(constant) + t(constant))

  (constant) = 415/(415+993) = .29474

previously unseen bigrams starting with spite are 

multiplied by a smaller value and are therefore less likely

kneser-ney discounting
    state-of-the-art in id38 for 15 years
    two major intuitions

    some contexts have lots of new words
    some words appear in lots of contexts

    procedure

in a backoff context

    only register a lower-order count the    rst time it is seen 
    example: bigram model

       san francisco    is a common bigram
    but, we only count the unigram    francisco    the    rst 
time we see the bigram    san francisco    - we change 
its unigram id203

other formulations
    id165 class-based language models

p(w) =

p(ci | ci n+1, . . . , ci 1)     p(wi | ci)

`yi=1

    syntactic language models
    generative syntactic models induce 

distributions over strings

p(w) = x    :yield(    )=w

p(    , w)

pauls & klein (2012)

p(    , w) = p(    )     p(w |     )

pauls & klein (2012)

p(    , w) = p(    )     p(w |     )

google (2007)

    "stupid backoff"
    a simpler method than kneser ney 

smoothing, that is easier to estimate using 
mapreduce

    approaches the same level of performance 
on tasks like mt when using large amounts 
of data

summary

    id165 language models are the standard 
method for estimating the id203 of 
sentences for mt and for asr

    although the markov assumption does not 

hold for language, it allows us to easily 
estimate probabilities by counting 
sequences of words in data

    since data is    nite, sparse counts are still a 
problem even when dealing with id165s 
instead of sentences

    smoothing is the most common solution.

questions?

    please read chapter 7 
from the textbook for 
more information on 
language models

announcements 

    hw0 is due tuesday before class
    hw1 will be released over the weekend.  
    hw1 is due on *february 2* 
    note: short time frame (1 week to 
complete), no partners for this assignment.
    goal: let you evaluate whether you should 

take the class before drop deadline.

