discriminative training

march 4, 2014

noisy channels again

p(e)

source

english

noisy channels again

p(e)

source

english

p(g | e)

german

noisy channels again

p(e)

source

english

p(g | e)

german

decoder

e    = arg max

e

= arg max

e

p(e | g)
p(g | e)   p(e)

p(g)

= arg max

e

p(g | e)   p(e)

noisy channels again

e    = arg max

e

= arg max

e

p(e | g)
p(g | e)   p(e)

p(g)

= arg max

e

p(g | e)   p(e)

noisy channels again

e    = arg max

e

= arg max

e

= arg max

e

= arg max

e

= arg max

p(e | g)
p(g | e)   p(e)

p(g)

p(g | e)   p(e)
log p(g | e) + log p(e)

e    1
1 >
|{z}w>

   log p(g | e)
log p(e)  
}
{z
|

h(g,e)

noisy channels again

e    = arg max

e

= arg max

e

= arg max

e

= arg max

e

= arg max

p(e | g)
p(g | e)   p(e)

p(g)

p(g | e)   p(e)
log p(g | e) + log p(e)

e    1
1 >
|{z}w>

   log p(g | e)
log p(e)  
}
{z
|

h(g,e)
log-linear model

discriminative 

modeling 

    depart from generative modeling 	

    goal: 	

    directly optimize for translation 

performance by discriminating between 
good/bad translation, and adjusting our 
model to give preference to good 
translations

discriminative 

modeling 

    possible translations of a sentence are 
represented using a set of features h	


of the translation	


    each feature hi derives from one property 
    its feature weight wi indicates its relative 
    the feature weights and feature values are 

importance	


combined into an overall score 

discriminative 

modeling 

    re-ranking - a two stage process	

    1: generate a candidate set of translations	

    2: add additional features and re-score 

the candidates according to the 
discriminative model

discriminative 

modeling 

model and translation model	


    optimize the features used in decoding	

    use more features than just the language 
    tune their parameters and use the 
    we can optimize a small handful of features, 

weights during decoding	


or we can use large-scale discriminative 
training for millions of features

n-best translations

    discriminative training operates on 
candidate translations of a sentence	

    in theory we could enumerate all possible 
translations, but in practice there are too 
many	


    typically, we operate on the 1,000-best or 
the 10,000-best translations, or the n-best

n-best translations

n-best translations

the noisy channel

-log p(g | e)

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)
improvement 1:

change      to    nd better translations

~w

~wg

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

~w

-log p(e)

as a linear model

-log p(g | e)

improvement 2:

add dimensions to make points separable

~w

-log p(e)

linear models

e    = arg max

e

w>h(g, e)

    improve the modeling capacity of the noisy 
channel in two ways	

    reorient the weight vector	

    add new dimensions (new features)	

    questions	

    what features?	

    how do we set the weights?

h(g, e)

w

mann

bei  t
x  bites y

hund

   25

mann

bei  t
x  bites y

hund

bei  t
mann
man bites

mann
man

bei  t
bite

hund
cat

hund
cat

bei  t

hund
mann
man chase dog

mann
man

bei  t
bite

mann
dog

bei  t
bites

hund
man

bei  t
mann
man bites

hund
dog

hund
dog

   26

feature classes

lexical

are lexical choices appropriate?
bank =    river bank    vs.    financial institution   

con   gurational

are semantic/syntactic relations preserved? 
   dog bites man    vs.    man bites dog   

grammatical

is the output    uent / well-formed?
   man bites dog    vs.    man bite dog   

   27

what do lexical features look like?

bei  t
mann
man bites

hund
cat

first attempt:

score(g, e) = w>h(g, e)

h15,342(g, e) =(1, 9i, j : gi = hund, ej = cat

otherwise

0,

but what if a cat is being chased by a hund?

   28

what do lexical features look like?

bei  t
mann
man bites

hund
cat

latent variables enable more precise features:

score(g, e, a) = w>h(g, e, a)

h15,342(g, e, a) = x(i,j)2a(1,

0,

if gi = hund, ej = cat
otherwise

   29

standard features

    target side features 

    log p(e)               [ id165 language model ]	

    number of words in hypothesis	

    non-english character count	


    source + target features 

    log relative frequency e|f of each rule     [ log #(e,f) - log #(f) ]	

    log relative frequency f|e of each rule     [ log #(e,f) - log #(e) ]	

       lexical translation    log id203 e|f of each rule    [     log pmodel1(e|f) ]	

       lexical translation    log id203 f|e of each rule    [     log pmodel1(f|e) ]	


    other features 

    count of rules/phrases used	

    reordering pattern probabilities

   30

parameter learning

   31

hypothesis space

h1

hypotheses

h2

   32

hypothesis space

h1

references

h2

   33

preliminaries

we assume a decoder that computes:

he   , a   i = arg max
he,ai

w>h(g, e, a)

and k-best lists of, that is:

{ e   i , a   i   }i=k

i=1 = arg ith- max
he,ai

w>h(g, e, a)

standard, ef   cient algorithms exist for this.

   34

learning weights
    try to match the reference translation exactly	


    conditional random    eld 

translations	


    maximize the id155 of the reference 
       average    over the different latent variables	


    max-margin 

translation from others by the maximal margin	


    find the weight vector that separates the reference 
    maximal setting of the latent variables

   35

problems

    these methods give    full credit    when the model 

exactly produces the reference and no credit 
otherwise	


    what is the problem with this? 
    there are many ways to translate a sentence	

    what if we have multiple reference 
    what about partial credit?

translations?	


   36

cost-sensitive training
    assume we have a cost function that gives 
a score for how good/bad a translation is	


!

!

 (  e,e)      [0, 1]

    optimize the weight vector by making 
reference to this function	

    we will talk about two ways to do this

   37

k-best list example

h1

#2#1

~w

#3

#6

#4#5

#7

#8

#10

#9

h2

   38

k-best list example

h1

#2#1

~w

#3

#6

#4#5

#7

#8

#10

#9

0.8       < 1.0
0.6       < 0.8
0.4       < 0.6
0.2       < 0.4
0.0       < 0.2

h2

   39

training as classi   cation
    pairwise ranking optimization	


    reduce training problem to binary classi   cation with a 

linear model 

    algorithm 

    for i=1 to n 

    pick random pair of hypotheses (a,b) from k-best list	

    use cost function to determine if is a or b better	

    create ith training instance	


    train binary linear classi   er

   40

reading

    read 9 from the textbook

announcements
    hw3 due on thursday at 11:59pm	

    jonny has of   ce hours tomorrow 2-3pm	

    1st term project is due by the end of spring 
    upcoming language-in-10	

    thursday: anshul - japanese 

break	


