lexical translation 

models 1i

machine translation 

lecture 5 

instructor: chris callison-burch 
tas: mitchell stern, justin chiu 

website: mt-class.org/penn

last time ...

alignment

translation
)
,

p(

p(

=

)=

translation

xalignment
xalignment
|
p(e | f, m) = xa2[0,n]m

p(

p(
)
alignment
   

)
translation | alignment

{z
}|

z

} |
{ z
myi=1

{z
{
}|

p(ei | fai)

p(a | f, m)    

}

p(e | f, m) = xa2[0,n]m

p(a | f, m)    

p(ei | fai)

alternate ways of de   ning 
p(ei | fai, fai 1)
the translation id203 

myi=1
myi=1
myi=1
myi=1
myi=1

p(ei | fai, fai 1)

p(ei | fai, ei 1)

p(ei, ei+1 | fai)

what is the problem here?

p(a | f, m)    

p(ei | fai)

myi=1
myi=1

   

p(ei | fai)

p(e | f, m) = xa2[0,n]m
= xa2[0,n]m
= xa2[0,n]m
= xa2[0,n]m

myi=1
|
myi=1
myi=1

1

1 + n

{z

p(a|f,m)
1

}

can we do something better here?

1 + n

p(ei | fai)

p(ai)     p(ei | fai)

0

null

1
das

2

haus

3
ist

4

klein

the
1

house

2

is
3

small

4

0

null

1
das

2

haus

3
ist

4

klein

house

1

is
2

small

3

the
4

p(e | f, m)

model 2

= xa2[0,n]m
= xa2[0,n]m

myi=1
myi=1

p(ai)     p(ei | fai)

p(ai | i, m, n)     p(ei | fai)

model 2

= xa2[0,n]m

myi=1

p(ai | i, m, n)     p(ei | fai)

    model alignment with an absolute position distribution
    id203 of translating a foreign word at position     
ai

to generate the word at position    (with target 
length     and source length    )
n

m

i

p(ai | i, m, n)

    em training of this model is almost the same as with 

model 1 (same conditional independencies hold)

model 2

= xa2[0,n]m

myi=1

p(ai | i, m, n)     p(ei | fai)

nat  rlich ist das haus

klein

nat  rlich nat  rlich

das haus

ist

klein

of

course

the house

is

small

model 2

p(ai | i, m, n)     p(ei | fai)

myi=1

= xa2[0,n]m
    pros 
    non-uniform alignment model
    fast em training / marginal id136
    cons 
    absolute position is very naive
    how many parameters to model

p(ai | i, m, n)

model 2

= xa2[0,n]m

myi=1

p(ai | i, m, n)     p(ei | fai)

how much do we know
when we only know the
source & target lengths   
and the current position?

how many parameters   
do we actually need to model this?

m = 6

}

null

j0 = 1
j0 = 2
j0 = 3
j0 = 4
j0 = 5

=
5

}n

i

i

i

i

i

i

=

=

=

=

=

1

2

3

4

5

=

6

model 2

= xa2[0,n]m

myi=1

p(ai | i, m, n)     p(ei | fai)

pos in target

pos in source

m = 6

h(j, i, m, n) =      

target len

i
m  

j

n    

source len

b(j | i, m, n) =

exp  h(j, i, m, n)

pj0 exp  h(j0, i, m, n)

p(ai | i, m, n) =(p0

(1   p0)b(ai | i, m, n)

}

null

j0 = 1
j0 = 2
j0 = 3
j0 = 4
j0 = 5

i

i

i

i

i

i

=

=

=

=

=

1

2

3

4

5

if ai = 0
otherwise

=

6

=
5

}n

words reorder in groups. model this!

p(e | f, m)

model 2

id48

= xa2[0,n]m
= xa2[0,n]m
= xa2[0,n]m

myi=1
myi=1
myi=1

p(ai)     p(ei | fai)

p(ai | i, m, n)     p(ei | fai)

p(ai | ai 1)     p(ei | fai)

id48

= xa2[0,n]m

myi=1

p(ai | ai 1)     p(ei | fai)

    insight: words translate in groups
    condition on previous alignment position
    id203 of translating a foreign word at position     
ai
ai 1

given that the previous position translated was

p(ai | ai 1)

    em training of this model using forward-backward 

algorithm (id145)

id48

myi=1

p(ai | ai 1)     p(ei | fai)

= xa2[0,n]m
    improvement: model    jumps    through the 

source sentence

p(ai | ai 1) = j(ai   ai 1)

    relative position model rather than 

absolute position model

-4
-3
-2
-1
0
1
2
3
4

0.0008
0.0015
0.08
0.18
0.0881
0.4
0.16
0.064
0.0256

id48

myi=1

here is one option (due to och):

p(ai | ai 1)     p(ei | fai)

= xa2[0,n]m
    be careful! nulls must be handled carefully. 
p(ai | ai ni) =(p0
ni    is the index of the    rst non-null aligned   
word in the alignment to the left of   .i

(1   p0)j(ai   ai ni)

if ai = 0
otherwise

id48

myi=1

p(ai | ai 1)     p(ei | fai)

= xa2[0,n]m
    other extensions: certain word-types are 

more likely to be reordered
j(  | f )
condition the jump id203 on the previous   
word translated

j(  | c(f ))

j(  | a(f ),b(e))

j(  | f, e)
condition the jump id203 on the previous   
word translated, and how it was translated

fertility models

been ef   cient

    the models we have considered so far have 
    this ef   ciency has come at a modeling cost:
    what is to stop the model from 
   translating    a word 0, 1, 2, or 100 times?
    we introduce fertility models to deal with 

this

ibm model 3

fertility

word

    fertility: the number of english words generated by a foreign 
    modeled by categorical distribution
    examples:

n(  | f )

unabhaengigkeitserklaerung

zum = (zu + dem)

haus

0
1
2
3
4
5

0.00008

0.1

0.0002

0.8
0.009

0

0
1
2
3
4
5

0.01

0
0.9

0.0009
0.0001

0

0
1
2
3
4
5

0.01
0.92
0.07

0
0
0

fertility

myi=1
p(e | f, m) = xa2[0,n]m
    fertility models mean that we can no 

p(a | f, m)    

p(ei | fai)

longer exploit conditional independencies 
to write             as a series of local   
alignment decisions.

p(a | f, m)

    how do we compute the statistics required for 

em training?

em recipe reminder

    if alignment points were visible, training 
fertility models would be easy
    we would _______ and ________

n(  = 3 | f = unabhaenigkeitserklaerung) =

count(3, unabhaenigkeitserklaerung)
count(unabhaenigkeitserklaerung)

    but, alignments are not visible 

n(  = 3 | f = unabhaenigkeitserklaerung) = e[count(3, unabhaenigkeitserklaerung)]
e[count(unabhaenigkeitserklaerung)]

expectation & fertility

    we need to compute expected counts 

under p(a | f,e,m)

nicely. :( 

    unfortunately p(a | f,e,m) doesn   t factorize 
    can we sum exhaustively? how many 
different a   s are there?
    what to do?

sampling alignments

    monte-carlo methods

    id150
    importance sampling
    particle    ltering

    for historical reasons

    use model 2 alignment to start (easy!)
    weighted sum over all alignment con   gurations that are 
    is this correct? no! does it work? sort of.

   close    to this alignment con   guration

lexical translation

    ibm models 1-5 [brown et al., 1993]

    model 1: lexical translation, uniform alignment
    model 2: absolute position model
    model 3: fertility
    model 4: relative position model (jumps in target string)
    model 5: non-de   cient model

    id48 translation model [vogel et al., 1996]

    relative position model (jumps in source string)

    latent variables are more useful these days than the translations
    widely used giza++ toolkit

pitfalls of conditional 

models

ibm model 4 alignment

our model's alignment

a few tricks...

p(f|e)

p(e|f)

a few tricks...

p(f|e)

p(e|f)

a few tricks...

p(f|e)

p(e|f)

model 1

suggestions for hw1
    matching the baseline will get you a b
    implement ibm model 2 in addition to ibm 
    try the heuristics for merging the many-to-
    try to reduce sparse counts by pre-
    other ideas?

one and one-to-many alignments

processing your training data

reading

    read chapter 4 from the 
textbook (today we 
covered 4.4 through 4.6)

announcements

by tuesday at 11:59pm 

    hw1 leaderboard submissions are due 
    hw1 write ups and code are due 24 

hours later

