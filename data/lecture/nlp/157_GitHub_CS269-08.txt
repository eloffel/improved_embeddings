lecture 8:

learning to search    

imitation learning in nlp

kai-wei chang
cs @ ucla

kw@kwchang.net

couse webpage: https://uclanlp.github.io/cs269-17/

ml in nlp

1

learning to search approaches
shift-reduce parser
v maintain a buffer and a stack
v make predictions from left to right
v three (four) types of actions:

shift, reduce, left, right

kai-wei chang (university of virginia)

2

credit:	google	research	blog

id170 as a search 
problem 
v decomposition of y often implies an ordering

    a sequential decision making process

can
i
pro md

can
vb

a
dt

can
nn

kai-wei chang

3

notations
v input:
v truth:

		           
y          (    )
v predicted:    (    )       (    )
		                    ,       
find            such that    x        (    )
minimizing		    4,5~7                    ,       
samples     9,    9~    

v loss:

a

can can

can
i
pro md vb dt nn
pro md md dt vb
pro md md dt nn
pro md nn dt md
pro md nn dt vb

based on     

goal: make joint prediction to minimize a joint loss

kai-wei chang ( msr -> u of virginia)

4

credit assignment problem

when making a mistake, which local decision should 
be blamed?

sentence

kai-wei chang (university of virginia)

5

an analogy from playing mario
from	mario	ai	competition	2009

input:

output:

jump	in	{0,1}
right	in	{0,1}
left	in	{0,1}
speed	in	{0,1}

v
i
d
e
o

 
c
r
e
d
i
t
:
 
s
t
  
p
h
a
n
e
r
o
s
s
,
 

g
e
o
f
f
 

high level goal:

extracted	27k+	binary	features
from	last	4	observations
(14	binary	features	for	every	cell)

watch an expert play and
learn to mimic her behavior

 

g
o
r
d
o
n
 
a
n
d
d
r
e
w
b
a
g
n
e
l
l

 

example of search space

i

can

can

a

can

pro md

i

can

can

decision

pro md vb dt nn

decision

action

pro md vb dt nn action

decision

pro md vb dt nn action

kai-wei chang

7

example of search space

i

can

pro md

can

vb

a

dt

can

vb

i
can
can
a
can

encodes an output

   =   (e)

from which
loss(y,   )

can be computed
(at training time)

pro md vb dt nn

e

end

kai-wei chang

8

policies

obs.

v a policy maps observations to actions

p(    ) =a

x
input:
t
timestep:
  
partial traj:
    anything else

kai-wei chang

9

labeled data     reference policy

v given partial traj.     <,    =,       ?@<and true 
label     , the minimum achievable loss is
(    ?   ,    ?a<   
min
(jk,jklm,   jn) loss(y,   (a))

,       b   )=arg
    <    =    .     ?@<
(    ?,    ?a<,       b)

loss(y,   (a))
e

kai-wei chang

10

labeled data     reference policy

v given partial traj.     <,    =,       ?@<and true 
label     , the minimum achievable loss is
(    ?   ,    ?a<   
min
v the optimal action is the corresponding     ?   
(jk,jklm,   jn) loss(y,   (a))

,       b   )=arg

v the optimal policy is the policy that always 

selects the optimal action

v reference policy can be constructed by the 

gold label in the training phase

kai-wei chang

11

ingredients for learning to search

structured	learning

input:            
truth:	y       (    )
outputs:	    (    )
loss:                     ,    p

search	space:

learning	to	search	

- state:	           
- action:	           (    )
- end	state	           
policies:	                
reference	policy:	    vwx

kai-wei chang

12

a simple approach

v collect trajectories from expert     vwx
v store as dataset     ={(    ,    vwx(    ))|    ~    vwx}
v train classifier      on     
v let      play the game!

kai-wei chang

13

learning a policy[chang+ 15, ross+15]

v at    ?    state, we construct a cost-sensitive multi-class 

example (?, [0, .2, .8])

e

loss=0

e

loss=.2

e

loss=.8
rollout

?

rollin

one-step
deviations

kai-wei chang

14

example: sequence labeling
receive input:

x = the monster ate the sandwich
y = dt    nn

vb dt     nn

make a sequence of predictions:

x = the monster ate the sandwich
   = dt    dt

dt dt dt

pick a timestep and try all perturbations there:

x = the monster ate the sandwich
  dt = dt    dt
  nn = dt    nn
  vb = dt    vb

vb dt     nn
vb dt     nn
vb dt     nn

l=1
l=0
l=1

compute losses and construct example:

( { w=monster, p=dt,    },  [1,0,1] )

learning a policy[chang+ 15, ross+15]

v at    ?    state, we construct a cost-sensitive multi-class 

example (?, [0, .2, .8])

e

loss=0

e

loss=.2

e

loss=.8
rollout

?

rollin

one-step
deviations

kai-wei chang

16

analysis

[icml 15]: learning to search better than your teacher

17

analysis

roll-in with ref:
unbounded structured regret

18

analysis

roll-out with ref: 

no local optimal if reference is sub-optimal

19

analysis

ignore ref     reinforcement	learning

roll-in & roll-out with current policy 

20

analysis

minimizes a combination of regret to ref and 
regret to its own one-step deviations.
v competes with ref when ref is good.
v competes with local deviations to improve 

on suboptimal ref

21

how to program?

v sample codes are available in vw

cs xxx lecture 1

22

