lecture 3:

multi-class classification

kai-wei chang
cs @ ucla

kw@kwchang.net

couse webpage: https://uclanlp.github.io/cs269-17/

ml in nlp

1

previous lecture

v binary linear classification models
v id88, id166s, id28

v given an example     , prediction is                 &x

v prediction is simple:

v note that all these linear classifier have the same 

v in id28, we can further estimate the 

id136 rule

id203

v question?

cs6501 lecture 3

2

this lecture

v multiclass classification overview
v reducing multiclass to binary 

v one-against-all & one-vs-one
v error correcting codes

v training a single classifier 

v multiclass id88: kesler   s construction
v multiclass id166s: crammer&singer formulation
v multinomial id28

cs6501 lecture 3

3

what is multiclass

v output     1,2,3,       

v in some cases, output space can be very large 

(i.e., k is very large)

v each input belongs to exactly one class

(c.f. in multilabel, input belongs to many classes)

cs6501 lecture 3

4

multi-class applications in nlp?

ml in nlp

5

two key ideas to solve multiclass

v reducing multiclass to binary 

v decompose the multiclass prediction into 

multiple binary decisions

v make final decision based on multiple binary 

classifiers

v training a single classifier 

v minimize the empirical risk
v consider all classes simultaneously 

cs6501 lecture 3

6

reduction v.s. single classifier

v reduction

v future-proof: binary classification improved so 

does muti-class

v easy to implement

v single classifier

v global optimization:  directly minimize the 

empirical loss; easier for joint prediction

v easy to add constraints and domain knowledge

cs6501 lecture 3

7

a general formula

    0=argmax               (    ;    ,    )
v id136/test: given     ,    , solve argmax
v learning/training: find a good     
v today:               ,    ={1,2,       } (multiclass)

output space

input
model parameters 

cs6501 lecture 3

8

this lecture

v multiclass classification overview
v reducing multiclass to binary 

v one-against-all & one-vs-one
v error correcting codes

v training a single classifier 

v multiclass id88: kesler   s construction
v multiclass id166s: crammer&singer formulation
v multinomial id28

cs6501 lecture 3

9

one against all strategy

cs6501 lecture 3

10

one against all learning

v multiclass classifier

v function   f : rn    {1,2,3,...,k}

v decompose into binary problems

cs6501 lecture 3

11

one-again-all learning algorithm

v learning: given a dataset     =     c,    c
    c       e,    c    1,2,3,       
v learn  k models:     f,    g,    h,       i

v decompose into k binary classification tasks

v for class k, construct a binary classification 

task as: 
v positive examples: elements of d with label k 
v negative examples: all other elements of d

v the binary classification can be solved by any 

algorithm we have seen

cs6501 lecture 3

12

one against all learning

v multiclass classifier

v function   f : rn    {1,2,3,...,k}

v decompose into binary problems

ideal	case:	only	the	correct	label	will	have	a	positive	score

    jklmn
&

	    >0

    jkrs&

	    >0

    tusse
&

	    >0

cs6501 lecture 3

13

one-again-all id136

v decompose into k binary classification tasks

v learning: given a dataset     =     c,    c
    c       e,    c    1,2,3,       
v learn  k models:     f,    g,    h,       i
v    0=argmaxv   {f,g,   i}	    v&	    
	    )
	    	,		    jkrs&
for	example:				y=argmax	(    jklmn
&
    0=argmax               (    ;    ,    )
    ={    f,    g,       i},	        ;    ,     =    v&	    

v id136:    winner takes all   

	    ,    tusse
&

v an instance of the general form

cs6501 lecture 3

14

one-again-all analysis

v not always possible to learn 

v assumption: each class individually separable from 

all the others

v no theoretical justification 

v need to make sure the range of all classifiers is 
the same     we  are comparing scores produced 
by k classifiers trained independently.
v easy to implement; work well in practice

cs6501 lecture 3

15

one v.s. one (all against all) strategy

cs6501 lecture 3

16

one v.s. one learning

v multiclass classifier

v function   f : rn    {1,2,3,...,k}

v decompose into binary problems

training

test

cs6501 lecture 3

17

one-v.s-one learning algorithm

v learning: given a dataset     =     c,    c
    c       e,    c    1,2,3,       
v learn c(k,2) models:     f,    g,    h,       i   (iyf)/g

v decompose into c(k,2) binary classification tasks

v for each class pair (i,j), construct a binary 

classification task as: 
v positive examples: elements of d with label i
v negative examples elements of d with label j
v the binary classification can be solved by any 

algorithm we have seen

cs6501 lecture 3

18

one-v.s-one id136 algorithm

v decision options: 

v more complex; each label gets k-1 votes
v output of binary classifier may not cohere. 
v majority: classify example x to take label i

if i wins on x more often than j (j=1,   k) 

v a tournament: start with n/2 pairs; continue with 

winners

cs6501 lecture 3

19

classifying with one-vs-one

tournament

majority	vote

1	red,	2	yellow,	2	green

   ?

all are post-learning and might cause weird stuff

cs6501 lecture 3

20

one-v.s.-one assumption

v every pair of classes is separable

it	is	possible	to	
separate	all	k	
classes	with	the	
o(k2)	classifiers

decision 
regions

cs6501 lecture 3

21

comparisons

v one against all

v o(k) weight vectors to train and store
v training set of the binary classifiers may unbalanced
v less expressive; make a strong assumption

v one v.s. one (all v.s. all)

v o(    g) weight vectors to train and store
    overfitting of the binary classifiers

v size of training set for a pair of labels could be small 

v need large space to store model 

cs6501 lecture 3

22

problems with decompositions

v learning optimizes over local metrics

v does not guarantee good global performance
v we don   t care about the performance of the 

local classifiers

v poor decomposition    poor performance

v difficult local problems
v irrelevant local problems

v efficiency: e.g., all vs. all vs. one vs. all
v not clear how to generalize multi-class to 

problems with a very large # of output

cs6501 lecture 3

23

still an ongoing research direction

key questions:

v how to deal with large number of classes
v how to select    right samples    to train binary classifiers

v error-correcting tournaments

[beygelzimer, langford, ravikumar 09]

v logarithmic time one-against-some 

[daume, karampatziakis, langford, mineiro 16]

v label embedding trees for large multi-class tasks.

[bengio, weston, grangier 10]

v    

cs6501 lecture 3

24

decomposition methods: summary

v general ideas:

v decompose the multiclass problem into many 

binary problems

v prediction depends on the decomposition

v constructs the multiclass label from the output of 

the binary classifiers

v learning optimizes local correctness

v each binary classifier don   t need to be globally 

correct and isn   t aware of the prediction procedure

cs6501 lecture 3

25

this lecture

v multiclass classification overview
v reducing multiclass to binary 

v one-against-all & one-vs-one
v error correcting codes

v training a single classifier 

v multiclass id88: kesler   s construction
v multiclass id166s: crammer&singer formulation
v multinomial id28

cs6501 lecture 3

26

revisit one-again-all learning algorithm

v learning: given a dataset     =     c,    c
    c       e,    c    1,2,3,       
v learn  k models:     f,    g,    h,       i
v    n:	separate class      from others
v prediction     0=argmaxv   {f,g,   i}	    v&	    

v decompose into k binary classification tasks

cs6501 lecture 3

27

observation

v at training time, we require     c&    	to be positive for 
examples of class     .
v really, all we need is for     c&     to be more than all 
others     this is a weaker requirement
for	examples	with	label	    ,	we	need	
    c&    >    _&    
for	all	    

cs6501 lecture 3

28

for	examples	with	label	    ,	we	need	
for	all	    

id88-style algorithm

    c&    >    _&    
v for each training example     ,    
v if  for some y   ,     v&           vb&     mistake!
v    v       v+        	
v    vb       vb           
why	add	         to	    v	promote	label	    :
before	update		sy =	<    vikj,    >
after	update					sy =<    vesk,    >=<    vikj+        ,    >
=	<    vikj,    >+    <    ,    >
note!		<    	,    >	=    &    >0	

update to promote y
update to demote y   

cs6501 lecture 3

29

a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
for    b       
if			    v&    <    vb&    
    v       v+        										promote y
    vb       vb           
return    
prediction: argmaxq		    v&    

demote y   

how	to	analyze	this	algorithm
and	simplify	the	update	rules?

make a  mistake

cs6501 lecture 3

30

linear separability with multiple classes

v let   s rewrite the equation

for all     

    c&    >    _&    
v instead of having     f,    g,    h,       i, we want to 
represent the model using a single vector     
    &							>	    &		       for all j
let   s	define	    (    ,    ),	such	that
    &        ,     >    &        ,    			       

change	the	input	representation

v how?  

?

?

multiple	models	v.s.	multiple	data	points

cs6501 lecture 3

31

assume	we	have	a	multi-class	problem	with	k	class	and	n	features.	

    &        ,     >    &        ,    			   	    

kesler construction

    c&	    >    _&	    			   	j
    n       e
    f,    g,       i,
           e

v models:

v input:

cs6501 lecture 3

32

assume	we	have	a	multi-class	problem	with	k	class	and	n	features.	

kesler construction

    c&	    >    _&	    			   	j
    n       e
    f,    g,       i,
           e

v models:

v input:

    &        ,     >    &        ,    			   	    
           e  i

v only one model:

cs6501 lecture 3

33

assume	we	have	a	multi-class	problem	with	k	class	and	n	features.	

kesler construction

    c&	    >    _&	    			   	    
    n       e
    f,    g,       i,
           e

v models:

v input:

v only one model:

    &        ,     >    &        ,    			   	    
           ei
v define         ,     for label y
    	in	    }~ block;
        ,     = 0e          0eei  f

being associated to input x

zeros elsewhere	

cs6501 lecture 3

34

kesler construction

assume	we	have	a	multi-class	problem	with	k	class	and	n	features.	

    c&	    >    c&	    			   	i
    f,    g,       i,		
    n       e
           e

v models:

v input:

    &        ,     >    &        ,    			   	    
    =     f       v       eei  f
	        ,     = 0e          0eei  f
    &        ,     =    v&	    

    	in	    }~ block;

zeros elsewhere	

cs6501 lecture 3

35

assume	we	have	a	multi-class	problem	with	k	class	and	n	features.	

kesler construction

    &        ,     >    &        ,    			   	    
       &        ,                ,     >0			       
    =     f       v       eei  f
[        ,                ,    ]=

binary	classification	problem
    	in	    }~ block;
       	in	    }~ block;

0e                    0e ei  f

cs6501 lecture 3

36

linear separability with multiple classes

what	we	want
    &        ,     >    &        ,    			   	    
       &        ,                ,     >0			       
dataset,	     in	nk dimension	should	linearly	separate		
        ,                ,     and	   [        ,                ,    ]

for	all	example	(x,	y)	with	all	other	labels	y   	in	

cs6501 lecture 3

37

how can we predict?

argmaxq		    &    (    ,    )
    =     f       v       eei  f
	        ,     = 0e          0eei  f

for	input	an	input	x,
the	model	predict	label	is	3

    (    ,3)
    

    (    ,2)
    (    ,1)

    (    ,4)
    (    ,5)

cs6501 lecture 3

38

how can we predict?

argmaxq		    &    (    ,    )
    =     f       v       eei  f
	        ,     = 0e          0eei  f
this	is	equivalent	to
argmaxv   {f,g,   i}	    v&	    

for	input	an	input	x,
the	model	predict	label	is	3

         (    ,3)

    (    ,2)
    (    ,1)

    (    ,4)
    (    ,5)

cs6501 lecture 3

39

constraint classification

            ,                ,        0			       

v goal:
v training:

v for each example     ,    
v update model if     &        ,                ,     <0	,   	    

transform examples

x

-x

2>1
2>3
2>4

2>4

2>1

2>3

cs6501 lecture 3

40

a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
for    b       
if			    &        ,                ,        <0
				           +    	        ,                ,       
return    
prediction:     argmaxq		    &    (    ,    )

this	needs	           updates,	

do	we	need	all	of	them?

how	to	interpret	this	update	rule?

cs6501 lecture 3

41

v goal:

an alternative training algorithm

            ,                ,        0			       
       &        ,    		   max_(cid:146)c     &        ,        0	
       &        ,    		   max_     &        ,        0	
v for each example     ,    
    0=	argmax(cid:145)		    &    (    ,    )
v update model if     &        ,                ,    0 <0	,   	       

v find the prediction of the current model:

v training:

cs6501 lecture 3

42

a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
      =argmaxqb		    &    (    ,       ) 
if			    &        ,                ,    0 <0
				           +    	        ,                ,    (cid:148)
return    
prediction:     argmaxq		    &    (    ,    )

how	to	interpret	this	update	rule?

cs6501 lecture 3

43

a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
      =argmaxqb		    &    (    ,       ) 
if			    &        ,                ,    0 <0
				           +    	        ,                ,    0
return    
prediction:     argmaxq		    &    (    ,    )

1.	    0=    :		        ,                ,    0 =0
2.	    &        ,                ,    0 <0

there	are	only	two	situations:

how	to	interpret	this	update	rule?

cs6501 lecture 3

44

a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
      =argmaxqb		    &    (    ,       ) 
	           +    	        ,                ,    0
return    
prediction:     argmaxq		    &    (    ,    )

how	to	interpret	this	update	rule?

cs6501 lecture 3

45

consider multiclass margin

cs6501 lecture 3

46

marginal constraint classifier

v goal: for every (x,y) in the training data set

minv(cid:149)(cid:146)v    &        ,                ,               
       &        ,        maxv(cid:146)vb    &        ,               
       &        ,        [maxv(cid:146)vb    &        ,     +    ]   0
constraints		violated	    need		an	update
let   s	define:	      ,    b =(cid:155)    					if	              
	     	     	    
0						        	    =       	
    =                        v(cid:149)    &        ,        +  (    ,    b)

check	if	

cs6501 lecture 3

47

a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
	    	    	    
for(    ,    ) in    :
      =argmaxqb		    &        ,        +  (    ,y   ) 
	           +    	        ,                ,    0
return    
prediction:     argmaxq		    &    (    ,    )

how	to	interpret	this	update	rule?

cs6501 lecture 3

48

remarks

v this approach can be generalized to train a 

ranker; in fact, any output structure
v we have preference over label assignments
v e.g., rank search results, rank movies / products

cs6501 lecture 3

49

a peek of a generalized id88 model 

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
      =argmaxqb		    &    (    ,       )+  (    ,y   ) 
	           +    	        ,                ,    0
return    
prediction:     argmaxq		    &    (    ,    )

model	update

structured	output

structural	prediction/id136	

structural	loss

cs6501 lecture 3

50

recap: a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
for    b       
if			    v&    <    vb&    
    v       v+        										promote y
    vb       vb           
return    
prediction:     argmaxq		    v&    

demote y   

make a  mistake

cs6501 lecture 3

51

recap: kesler construction

assume	we	have	a	multi-class	problem	with	k	class	and	n	features.	

    c&	    >    c&	    			   	i
    f,    g,       i,		
    n       e
           e

v models:

v input:

    &        ,     >    &        ,    			   	    
    =     f       v       eei  f
	        ,     = 0e          0eei  f
    &        ,     =    v&	    

    	in	    }~ block;

zeros elsewhere	

cs6501 lecture 3

52

geometric interpretation

in     (cid:159) space

    g

    h
      
argmaxv   {f,g,   i}	    v&	    

    f
    (cid:160)

         (    ,3)

#	features	=	n;	#	classes	=	k

in     (cid:159)i space
    (    ,2)
    (    ,4)
    (    ,1)
    (    ,5)
argmaxq		    &    (    ,    )

cs6501 lecture 3

53

recap: a id88-style algorithm

given a training set     ={    ,    }
initialize                 e
for epoch 1       :
for(    ,    ) in    :
      =argmaxqb		    &    (    ,       ) 
	           +    	        ,                ,    0
return    
prediction:     argmaxq		    &    (    ,    )

how	to	interpret	this	update	rule?

cs6501 lecture 3

54

multi-category to constraint classification

v multiclass

v (x, a)     (x, (a>b, a>c, a>d) )

v multilabel

v (x, (a, b))    (x, ( (a>c, a>d, b>c, b>d) ) 

v label ranking

v (x, (5>4>3>2>1))      (x, ( (5>4, 4>3, 3>2, 2>1) )

cs6501 lecture 3

55

generalized constraint classifiers
v in all cases, we have examples (x,y)  with  y    sk
v where sk : partial order over class labels {1,...,k}

v defines    preference    relation ( > ) for class labeling
v consequently, the constraint classifier is:  h: x    sk

v h(x) is a partial order
v h(x) is consistent with y if (i<j)    y    (i<j)   h(x)

just	like	in	the	multiclass	we	learn	one	wi    rn for	each	
label,	the	same	is	done	for	multi-label	and	ranking.	the	
weight	vectors	are	updated	according	with	the	
requirements	from	y	   sk

cs6501 lecture 3

56

multi-category to constraint classification

solving id170 problems by ranking algorithms
v multiclass

v (x, a)     (x, (a>b, a>c, a>d) )

v multilabel

v (x, (a, b))    (x, ( (a>c, a>d, b>c, b>d) ) 

v label ranking

v (x, (5>4>3>2>1))      (x, ( (5>4, 4>3, 3>2, 2>1) )

y	   sk

h:	x    sk

cs6501 lecture 3

57

properties of construction (zimak et. al 2002, 2003)

v can learn any argmax wi.x function (even when i isn   t 

linearly separable from the union of the others) 
v can use any algorithm to find linear separation

v id88 algorithm

v winnow algorithm

v ultraconservative online algorithm [crammer, singer 2001]

v multiclass winnow [ masterharm 2000 ] 

v defines a multiclass margin by binary margin in rkn

v multiclass id166 [crammer, singer 2001]

cs6501 lecture 3

58

this lecture

v multiclass classification overview
v reducing multiclass to binary 

v one-against-all & one-vs-one
v error correcting codes

v training a single classifier 

v multiclass id88: kesler   s construction
v multiclass id166s: crammer&singer formulation
v multinomial id28

cs6501 lecture 3

59

recall: margin for binary classifiers

v the margin of a hyperplane for a dataset is 

the distance between the hyperplane and 
the data point nearest to it.

-

-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

+

+ ++
+
+
+

+

margin	with	respect	to	this	hyperplane

cs6501 lecture 3

60

v in a risk minimization framework

multi-class id166

v goal: d= x  ,y  
1.    v  &    c>    vb&    c

c   f(cid:159)
for all     ,y   

2. maximizing the margin

cs6501 lecture 3

61

multiclass margin

cs6501 lecture 3

62

multiclass id166 (intuition)

v binary id166

v maximize margin. equivalently, 

minimize norm of weights such that the closest 
points to the hyperplane have a score 1

v multiclass id166

v each label has a different weight vector 

(like one-vs-all)

v maximize multiclass margin. equivalently,

minimize total norm of the weights such that the true label 
is scored at least 1 more than the second best one

cs6501 lecture 3

63

multiclass id166 in the separable case

recall	hard	binary	id166

size	of	the	weights.	
effectively,	regularizer

the	score	for	the	true	label	is	higher	than	the	
score	for	any other	label	by	1

cs6501 lecture 3

64

multiclass id166: general case

size	of	the	weights.	
effectively,	regularizer

total	slack.	effectively,	
don   t	allow	too	many	
examples	to	violate	the	

margin	constraint

the	score	for	the	true	label	is	higher	
than	the	score	for	any other	label	by	1

slack	variables.	not	all	

examples	need	to	
satisfy		the	margin	

constraint.	

slack	variables	can	
only	be	positive

cs6501 lecture 3

65

multiclass id166: general case

size	of	the	weights.	
effectively,	regularizer

total	slack.	effectively,	
don   t	allow	too	many	
examples	to	violate	the	

margin	constraint

the	score	for	the	true	label	is	higher	
than	the	score	for	any other	label	by	
1	-   i

slack	variables.	not	all	

examples	need	to	
satisfy		the	margin	

constraint.	

slack	variables	can	
only	be	positive

cs6501 lecture 3

66

recap: an alternative id166 formulation

min    ,j,    			fg    &    +           c
 c
s.t			y  (             +    )   1       c;	    c   0						       	
    c   1   	y  (             +    );	    c   0        	
v in the optimum,    c=max(0,	1   	y  (             +    ))
min    ,j			fg    &    +       max(0,	1   	y  (             +    ))

v soft id166 can be rewritten as:

v rewrite the constraints:

 c

id173		term

empirical	loss

cs6501 lecture 3

67

rewrite it as unconstraint problem

let   s	define:	      ,    b =(cid:155)    					if	              
0						        	    =       	
    n+    	   (maxn       c,     +    n&            v  &    	)
 c

mink fg	        n&
 n

cs6501 lecture 3

68

multiclass id166

v generalizes binary id166 algorithm

v if we have only two classes, this reduces to the 

binary (up to scale)

v comes with similar generalization guarantees 

as the binary id166

v can be trained using different optimization 

methods
v stochastic sub-id119 can be generalized 

cs6501 lecture 3

69

exercise!

v write down sgd for multiclass id166

v write down multiclas id166 with  kesler

construction

cs6501 lecture 3

70

this lecture

v multiclass classification overview
v reducing multiclass to binary 

v one-against-all & one-vs-one
v error correcting codes

v training a single classifier 

v multiclass id88: kesler   s construction
v multiclass id166s: crammer&singer formulation
v multinomial id28

cs6501 lecture 3

71

recall: (binary) id28

min     			12    &    +    (cid:176)log(1+    yq   (              ))

c

assume labels are generated using the following 
id203 distribution:

cs6501 lecture 3

72

 
partition		function

v assumption:

(multi-class) log-linear model

		            ,     =
            ,     =
   

exp    v&    
   
exp	(    vb&    )
 vb   {f,g,   i}
exp    &    (    ,    )
exp	(    &    (    ,       ))
 vb   {f,g,   i}

v this is a valid id203 assumption. why?
v another way to write this (with kesler

construction) is

this	often	called	soft-max	

cs6501 lecture 3

73

softmax

v softmax: let s(y) be the score for output y

computed by other metric.

here s(y)=    &    (    ,    )
(or     v&    ) but it can be 
exp    (    )
    (    )=
   
exp	(    (    ))
 vb   {f,g,   i}
exp        /    
    (    |    )=
   
exp	(    (    /    ))
 vb   {f,g,   i}

v we can control the peakedness of the distribution

cs6501 lecture 3

74

example

s(dog)=.5;				s(cat)=1;				s(mouse)=0.3;				s(duck)=-0.2

1.2
1
0.8
0.6
0.4
0.2
0
-0.2 
-0.4 

dog

softmax	(    =0.5) 

score

cat

softmax	(    =2) 

softmax

mouse

(hard)	max	(       0)

duck

cs6501 lecture 3

75

log linear model

            ,     =
         k        
   
         	(k   (cid:149)     )
log            ,     =log	(exp    v&    )   log	(   
    (cid:149)   {   ,(cid:190),     }
=    v&       log	(   
exp	(    v(cid:149)&    ))
 vb   {f,g,   i}

 vb   {f,g,   i}

exp	(    v(cid:149)&    ))

linear	function

except	this	term	

note:

cs6501 lecture 3

76

maximum log-likelihood estimation

v training can be done by maximum log-likelihood 

estimation i.e. maxk 	log    (        
d={(    c,    c)}
    (         =  c
         k            
   
         	(k   (cid:149)       )
    (cid:149)   {   ,(cid:190),     }
log   
log    (         =   [    v  &    c   
 vb   {f,g,   i}
 c

exp	(    v(cid:149)&    c)

]

cs6501 lecture 3

77

can	you	use	kesler construction	to	
rewrite	this	formulation?

maximum a posteriori

d={(    c,    c)}
                                    
m        k	   fg        v&    v+	
       [    v  &    c   
log   
exp	(    v(cid:149)&    c)
]
 vb   {f,g,   i}
 c
 v
       [ clog   
ormink 		fg        v&    v+	
]  
exp     v(cid:149)&    c        v  &    c
 vb   {f,g,   i}

 v

cs6501 lecture 3

78

v multi-class id166:

v log-linear model w/ map (multi-class)

comparisons
mink fg	        n&
    n+    	   (maxn (      c,     +    n&           v  &    ))
 c
 n
mink fg        n&    n+    
   [ clog   
 n   {f,g,   i}
 n
							min     			fg    &    +       max(0,	1   	y  (             ))
 c
							min     			fg    &    +       log(1+    yq   (              ))
 c

v log-linear mode (id28)

v binary id166:

exp    n&    c        v  &    c

]  

cs6501 lecture 3

79

