lecture 4:

id170 models

kai-wei chang
cs @ ucla

kw@kwchang.net

couse webpage: https://uclanlp.github.io/cs269-17/

ml in nlp

1

v id88, id166s, id28, 

previous lecture

v binary linear classification

na  ve bayes

v output:        {1,   1}
v output:         1,2,3,       

v multi-class classification

v multiclass id88, multiclass id166   

ml in nlp

2

what we have seen: multiclass

v reducing multiclass to binary 

v one-against-all & one-vs-one
v error correcting codes
v extension: learning to search
v training a single classifier 

v multiclass id88: kesler   s construction
v multiclass id166s: crammer&singer formulation
v multinomial id28
v extension: id114

ml in nlp

3

this lecture

v what is structured output?
v multiclass as structure
v sequence as structure
v general graph structure

ml in nlp

4

global decisions

v    understanding    is a global decision

v several local decisions play a role
v there are mutual dependencies on their 

outcome.

v essential to make coherent decisions 

v joint, global id136

ml in nlp

5

id136 with constraints 
[roth&yih   04,07,   .]

bernie   s	wife,	 jane,	is	a	native	of	brooklyn
e3
e1

e2

r12

r23

models	could	be	learned	separately/jointly;	constraints	may	come	up	only	at	decision	time.

ml in nlp

6

id136 with constraints [roth&yih   04,07,   .]

models	could	be	learned	separately/jointly;	constraints	may	come	up	only	at	decision	time.

ml in nlp

7

id136 with constraints [roth&yih   04,07,   .]

other
other

per
per

loc
loc

0.05
0.05
0.85
0.85
0.10
0.10

other
other

per
per

loc
loc

0.10
0.10
0.60
0.60
0.30
0.30

other
other
other

per
per
per

loc
loc
loc

0.05
0.05
0.05
0.50
0.50
0.50
0.45
0.45
0.45

bernie   s	wife,	 jane,	is	a	native	of	brooklyn
e3
e1

e2

r12

0.05
0.05
0.05
0.45
0.45
0.45
0.50
0.50
0.50

irrelevant
irrelevant
irrelevant

spouse_of
spouse_of
spouse_of

born_in
born_in
born_in

r23
irrelevant
irrelevant

spouse_of
spouse_of

born_in
born_in

0.10
0.10
0.05
0.05
0.85
0.85

models	could	be	learned	separately/jointly;	constraints	may	come	up	only	at	decision	time.

ml in nlp

8

structured output is   

v a predefine structure

v can be represented as a graph

ml in nlp

9

sequential tagging

v the process of assigning a part-of-speech 

to each word in a collection (sentence).

word

tag

the
koala
put 
the 
keys
on
the
table

det
n
v
det
n
p
det
n

ml in nlp

10

let   s try

don   t	worry!	there	is	no	problem	
with	your	eyes	or	computer.

a/dt d6g/nn 0s/vbz chas05g/vbg a/dt
cat/nn ./.
a/dt f6x/nn 0s/vbz 9u5505g/vbg ./.
a/dt b6y/nn 0s/vbz s05g05g/vbg ./.
a/dt ha77y/jj b09d/nn

what is the pos tag sequence of the following sentence?
a ha77y cat was s05g05g .

ml in nlp

11

let   s try

v a/dt d6g/nn 0s/vbz chas05g/vbg a/dt

cat/nn ./.
a/dt dog/nn is/vbz chasing/vbg a/dt cat/nn ./.
v a/dt f6x/nn 0s/vbz 9u5505g/vbg ./.

a/dt fox/nn is/vbz running/vbg ./. 

v a/dt b6y/nn 0s/vbz s05g05g/vbg ./.

a/dt boy/nn is/vbz singing/vbg ./.

v a/dt ha77y/jj b09d/nn

a/dt happy/jj bird/nn

v a ha77y cat was s05g05g .

a happy cat was singing . 

ml in nlp

12

how you predict the tags?

v two types of information are useful

v relations between words and tags
v relations between tags and tags

v dt nn, dt jj nn   
v fed in    the fed    is a noun because it follows a 

determiner

ml in nlp

13

combinatorial optimization problem

    ,=argmax               (    ;    ,    )
v id136/test: given     ,    , solve argmax
v learning/training: find a good     

input
model parameters 

output space

ml in nlp

14

challenges with structured output

v we cannot train a separate weight vector for 

each possible id136 outcome (why?)
v for multi-class we train one weight vector for 

each class

v we cannot enumerate all possible 

structures for id136
v id136 for multiclass was easy

ml in nlp

15

deal with combinatorial output

v decompose the output into parts that are labeled
v define a graph to represent

v how the parts interact with each other 
v these labeled interacting parts are scored; the total 
score for the graph is the sum of scores of each part
v an id136 algorithm to assign labels to all the parts

ml in nlp

16

a history-based model

v each token is dependent on all the tokens 

that came before it
v simple conditioning
v each p(xi |    ) is a multinomial id203 

distribution over the tokens

ml in nlp

17

example: a language model 

it was a bright cold day in april.

id203	of	a	word	starting	a	sentence

id203	of	a	word	following	   it   
id203	of	a	word	following	   it	was   
id203	of	a	word	following	   it	was	a   

ml in nlp

18

a history-based model

v each token is dependent on all the tokens 

that came before it
v simple conditioning
v each p(xi |    ) is a multinomial id203 

distribution over the tokens
v what is the problem here?

v how many parameters do we have? 
v grows with the size of the sequence!

ml in nlp

19

solution: lose the history

discrete	markov	process

v a system can be in one of k states at a time
v state at time t is xt
v first-order markov assumption 

the state of the system at any time is independent
of the full sequence history given the previous state

v defined by two sets of probabilities: 

v initial state distribution: p(x1 = sj)
v state transition probabilities: p(xi = sj | xi-1 = sk)

ml in nlp

20

example: another language model

it was a bright cold day in april

id203	of	a	word	starting	a	sentence
id203	of	a	word	following	   it   
id203	of	a	word	following	   was   
id203	of	a	word	following	   a   

if	there	are	k	tokens/states,	how	many	parameters	
do	we	need?	 o(k2)

ml in nlp

21

example: the weather

v three states: rain, cloudy, sunny

state	transitions:

v observations are markov chains:

eg: cloudy sunny sunny rain
id203 of the sequence = 
p(cloudy) p(sunny|cloudy) p(sunny | sunny) p(rain | sunny)

initial	id203

transition	probabilities

ml in nlp

22

example: the weather

v three states: rain, cloudy, sunny

state	transitions:

v observations are markov chains:

these	probabilities	define	the	model;	
can	find	p(any	sequence)
eg: cloudy sunny sunny rain
id203 of the sequence = 
p(cloudy) p(sunny|cloudy) p(sunny | sunny) p(rain | sunny)

initial	id203

transition	probabilities

ml in nlp

23

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

v structured id88 for sequences

ml in nlp

24

hidden markov model

v discrete markov model: 

v states follow a markov chain
v each state is an observation

v hidden markov model:

v states follow a markov chain
v states are not observed
v each state stochastically emits an observation

ml in nlp

25

toy part-of-speech example

the fed raises interest rates

transitions

emissions

p(the	|	determiner)	=	0.5
p(a	|	determiner)	=	0.3
p(an	|	determiner)	=	0.1
p(fed	|	determiner)	=	0
   

p(fed|	noun)	=	0.001
p(raises|	noun)	=	0.04
p(interest|	noun)	=	0.07
p(the|	noun)	=	0
   

determiner

noun

verb

start

determiner

noun

verb

noun

noun

initial

the

fed

raises

interest

rates

ml in nlp

26

joint model over states and observations

v notation

v number of states = k, number of observations = m
v   : initial id203 over states  (k dimensional vector)
v a: transition probabilities (k  k matrix)
v b: emission probabilities (k  m matrix)

v id203 of states and observations

v denote states by y1, y2, ! and observations by x1, x2, !

ml in nlp

27

other applications

v id103
v input: speech signal
v output: sequence of words

v nlp applications

v information extraction
v text chunking 

v computational biology

v aligning protein sequences
v labeling nucleotides in a sequence as exons, 

introns, etc.

questions?

ml in nlp

28

three questions for id48s

[rabiner 1999]

1. given an observation sequence, x1, x2, ! xn

and a model (  , a, b), how to efficiently 
calculate the id203 of the observation?
2. given an observation sequence, x1, x2, !, xn

and a model (  , a, b), how to efficiently 
calculate the most probable state sequence?

id136

3. how to calculate (  , a, b) from observations?

learning	

ml in nlp

29

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

v structured id88 for sequences

ml in nlp

30

most likely state sequence

v input:

v a hidden markov model (  , a, b)
v an observation sequence x = (x1, x2, !, xn)

v output: a state sequence y = (y1, y2, !, yn) 

that corresponds to
v maximum a posteriori id136 (map id136)

v computationally: combinatorial optimization 

ml in nlp

31

map id136

v we want

v we have defined

v but 

p (y|x,    , a, b) / p (x, y|   , a, b)

v and we don   t care about p(x) we are maximizing 

over y

v so, 

ml in nlp

32

how many possible sequences? 

the

fed

raises

interest

rates

determiner

1

verb
noun

2

verb
noun

2

verb
noun

2

verb
noun

2

list	of	allowed	tags	for	each	word

in	this	simple	case,	16	sequences	(1  2  2  2  2)

ml in nlp

33

na  ve approaches

1. try out every sequence

v score the sequence y as p(y|x,   , a, b)
v return the highest scoring one
v what is the problem?

v correct, but slow, o(kn)

2. greedy search

v construct the output left to right
v for each i, elect the best yi using yi-1 and xi
v what is the problem?

v incorrect but fast, o(nk)

ml in nlp

34

solution: use the independence assumptions

recall: the first order markov assumption
the state at token i is only influenced by the 
previous state, the next state and the token 
itself

given the adjacent labels, the others do not 
matter

suggests a recursive algorithm

ml in nlp

35

jason   s ice cream

scroll to the bottom to see a graph of what states and transitions the 
model thinks are likely on each day.  those likely states and transitions 
can be used to reestimate the red probabilities (this is the "forward-
backward" or baum-welch algorithm), incr
p(   |h)
0.1
0.2
0.7
0.2
0.8

p(   |c)
0.5
0.4
0.1
0.8
0.2

p(1|   )
p(2|   )
p(3|   )
p(c|   )
p(h|   )

p(   |start)

0.5
0.5

#cones

v best tag sequence for p(   1,2,1   )?

0.5

0.5

0.5
c

h

0.1

0.8

0.2

0.2

0.8

0.4
c

h

0.2

0.8

0.2

0.2

0.8

0.5
c

h

0.1

ml in nlp

36

deriving the recursive algorithm

transition	probabilities

emission	probabilities

initial	id203

y1

x1

y2

x2

y3

x3

   

yn

xn

ml in nlp

37

deriving the recursive algorithm

the	only	terms	that	depend	on	y1

y1

x1

y2

x2

y3

x3

   

y
n

xn

ml in nlp

38

deriving the recursive algorithm

abstract	away	the	score	for	all	
decisions	till	here	into	score

y1

x1

y2

x2

y3

   

x3
ml in nlp

y
n

xn

39

deriving the recursive algorithm

y1

x1

y2

x2

only	terms	that	depend	on	y2
   

y
n

xn

40

y3

x3
ml in nlp

deriving the recursive algorithm

y1

x1

y2

x2

y3

   

x3
ml in nlp

y
n

xn

41

abstract	away	the	score	for	all	decisions	till	here	into	score

deriving the recursive algorithm

y1

y2

y3

   

y
n

x1

xn
abstract	away	the	score	for	all	decisions	till	here	into	score

x3
ml in nlp

x2

42

deriving the recursive algorithm

ml in nlp

43

viterbi algorithm

max-product	algorithm	for	first	order	sequences

initial: for each state s, calculate

1.

  :	initial	probabilities
a:	transitions
b:	emissions

1. recurrence: for i = 2 to n, for every state s, calculate

1. final state: calculate

this	only	calculates	the	max.	to	get	final	answer	(argmax),	
    keep	track	of	which	state	corresponds	to	the	max	at	each	step	
    build	the	answer	using	these	back	pointers

ml in nlp

questions?

44

general idea

v id145

v the best solution for the full problem relies on 

best solution to sub-problems 
v memoize partial computation

v examples

v viterbi algorithm
v dijkstra   s shortest path algorithm
v    

ml in nlp

45

complexity of id136

v complexity parameters
v input sequence length: n
v number of states: k

v memory

v storing the table: nk (scores for all states at 

each position)

v runtime

v at each step, go over pairs of states
v o(nk2)

ml in nlp

46

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

ml in nlp

47

learning id48 parameters

v assume we know the number of states in the id48
v two possible scenarios

1. we are given a data set d = {<xi, yi>} of sequences 

labeled with states
and we have to learn the parameters of the id48 (  , a, b)

supervised	learning	with	complete	data

2. we are given only a collection of sequences d = {xi}

and we have to learn the parameters of the id48 (  , a, b)

unsupervised	learning,	with	incomplete	data

ml in nlp

48

supervised learning of id48

v we are given a dataset d = {<xi, yi>}

v each xi is a sequence of observations and yi is a 

sequence of states that correspond to xi
goal: learn initial, transition, emission distributions (  , a, b)

v how do we learn the parameters of the id203 

distribution?
v the maximum likelihood principle

where	have	we	seen	this	before?

and	we	know	how	to	write	this	in	terms	of	the	parameters	of	the	id48

ml in nlp

49

supervised learning details

  , a, b can be estimated separately just by counting

v makes learning simple and fast

[exercise: derive the following using derivatives of the log likelihood. 
requires lagrangian multipliers.]

number	of	instances	where	the	first	state	is	s

initial	probabilities

number	of	examples

transition	probabilities

emission	probabilities

ml in nlp

50

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

ml in nlp

51

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

ml in nlp

52

modeling next-state directly

v instead of modeling the joint distribution 

p(x, y) only focus on p(y|x)
v which is what we care about eventually anyway

v for sequences, different formulations

v maximum id178 markov model [mccallum, et al 2000]
v projection-based markov model [punyakanok and roth, 

2001]

(other names: discriminative/conditional markov model,    )

ml in nlp

53

generative vs discriminative models

v generative models 

v learn p(x, y)
v characterize how the data is generated (both inputs and 

outputs)

v eg: na  ve bayes, hidden markov model

v discriminative models 

v learn p(y | x)
v directly characterizes the decision boundary only
v eg: id28, conditional models (several names)

ml in nlp

54

generative vs discriminative models

v generative models 

v learn p(x, y)
v characterize how the data is generated (both inputs and outputs)
v eg: na  ve bayes, hidden markov model

a	generative	model	tries	to	characterize	
the	distribution	of	the	inputs,	a	
discriminative	model	doesn   t	care

v discriminative models 

v learn p(y | x)
v directly characterizes the decision boundary only
v eg: id28, conditional models (several names)

ml in nlp

55

another independence assumption

yt-1

id48

yt

xt

yt-1

conditional
model

yt

xt

this assumption lets us write the conditional 
id203 of the output as

ml in nlp

56

modeling p(yi | yi-1, xi)
v different approaches possible

1. train a log-linear classifier
2. or, ignore the fact that we are predicting a id203, we 

only care about maximizing some score. train any classifier 
(e.g, id88 algorithm)

v for both cases:

v use rich features that depend on input and previous state
v we can increase the dependency to arbitrary neighboring xi   s

v eg. neighboring words influence this words pos tag

ml in nlp

57

id148 for multiclass

consider multiclass classification

v output: y    {1, 2, ! , k}
v feature representation:     (x, y)

v inputs: x

v we have seen this before

v define id203 of an input x taking a label y as 
interpretation:	score	for	label,	
converted	to	a	well-formed	
id203	distribution	by	
exponentiating +	normalizing
v a generalization of id28 to 

multiclass

ml in nlp

58

training a log-linear model (multi-class)

given a data set d = {<xi, yi>}

v apply the maximum likelihood principle

v maybe with a regularizer

here

ml in nlp

59

training a log-linear model

v gradient based methods to minimize

v usual stochastic id119

v initialize            
v for each example     >	    >
v update            	   r@        (    ,    >,    >)
v return     

the loss at that example

v iterate through examples for multiple epochs

take gradient step for 

ml in nlp

60

back	to	sequences
the next-state model

yt-1

id48

yt

xt

yt-1

conditional
model

yt

xt

this assumption lets us write the id155 
of the output as

we	need	to	learn	this	function

ml in nlp

61

maximum id178 markov model

goal:	compute	p(y |	x)

start

determiner

noun

verb

noun

noun

the

fed

raises

interest

rates

the	prediction	task:	
using	the	entire	input	and	the	current	label,	predict	the	next	label

ml in nlp

62

maximum id178 markov model

goal:	compute	p(y |	x)

start

determiner

noun

verb

noun

noun

the

fed

raises

interest

rates

to	model	the	id203,	first,	we	need	to	define	
features	for	the	current	classification	problem

word
caps
-es
previous

ml in nlp

63

maximum id178 markov model

goal:	compute	p(y |	x)

start

determiner

noun

verb

noun

noun

word
caps
-es
previous

fed

raises

interest

rates

the
the
y
n
start

    (x,	0,	start,	y0)

ml in nlp

64

maximum id178 markov model

goal:	compute	p(y |	x)

start

determiner

noun

verb

noun

noun

the

fed

raises

interest

rates

word
caps
-es
previous

the
y
n
start

  (x,	0,	start,	y0)   (x,	1,	y0, y1)

determiner

fed
y
n

raises
n
y

noun

interest

n
n
verb

rates
n
n
noun

can	get	very	
creative	here

  (x,	2,	y1,	y2)

  (x,	3,	y2,	y3)

  (x,	4,	y3,	y4)

compare	to	id48:	only	depends	on	the	word	and	the	previous	tag

ml in nlp

questions?

65

maximum id178 markov model

goal:	compute	p(y |	x)

start

determiner

noun

verb

noun

noun

word
caps
-es
previous

the
the
y
n
start

  (x,	0,	start,	y0)

fed
fed
y
n

determiner

  (x,	1,	y0, y1)

raises

raises
n
y

noun

  (x,	2,	y1,	y2)

interest
interest

n
n
verb

rates
rates
n
n
noun

  (x,	3,	y2,	y3)   (x,	4,	y3,	y4)

can	get	very	
creative	here

compare	to	id48:	only	depends	on	the	word	and	the	previous	tag

ml in nlp

questions?

66

maximum id178 markov model

goal:	compute	p(y |	x)

start

determiner

noun

verb

noun

noun

word
caps
-es
previous

the
the
y
n
start

  (x,	0,	start,	y0)

fed
fed
y
n

determiner

  (x,	1,	y0, y1)

raises

raises
n
y

noun

  (x,	2,	y1,	y2)

interest
interest

n
n
verb

rates
rates
n
n
noun

  (x,	3,	y2,	y3)   (x,	4,	y3,	y4)

can	get	very	
creative	here

compare	to	id48:	only	depends	on	the	word	and	the	previous	tag

ml in nlp

questions?

67

using memm

v training

v next-state predictor locally as maximum likelihood

v similar to any maximum id178 classifier

v prediction/decoding

v modify the viterbi algorithm for the new 

independence assumptions

id48

conditional	
markov	model

ml in nlp

68

generalization: any multiclass classifier 

v viterbi decoding: we only need a score for 

each decision
v so far, probabilistic classifiers

v in general, use any learning algorithm to build 

get a score for the label yi given yi-1 and x
v multiclass versions of id88, id166
v just like memm, these allow arbitrary features to be 

defined

exercise: viterbi needs to be re-defined to work with sum of 
scores rather than the product of probabilities

ml in nlp

69

comparison to id48

what we gain

1. rich feature representation for inputs

v helps generalize better by thinking about 

properties of the input tokens rather than the entire 
tokens

v eg: if a word ends with    es, it might be a present 
tense verb (such as raises). could be a feature; 
id48 cannot capture this

2. discriminative predictor

v model p(y | x) rather than p(y, x)
v joint vs conditional 

ml in nlp

70

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

ml in nlp

71

outline

v conditional models for predicting 

sequences

v id148 for multiclass 

classification

v maximum id178 markov models

v the label bias problem

ml in nlp

72

the next-state model for sequences

yt-1

id48

yt

xt

yt-1

conditional
model

yt

xt

this assumption lets us write the id155 of the output 
as

we	need	to	train	local	multiclass	classifiers	that	predicts	
the	next	state	given	the	previous	state	and	the	input

ml in nlp

73

   local classifiers! label bias problem

let   s look at the independence assumption

   next-state   	classifiers	
are	locally	normalized

eg:	part-of-speech	tagging	the	sentence

wheels
1

n

the

robot

0.8

1

d

n

1

0.2

v

are

round

1

1

v

n

a

r

suppose	these	are	the	only	state	transitions	allowed

ml in nlp

example	based	on	[wallach	2002]

wheels)	  

are)	  

wheels)	  

are)	  

round)

option	1:	p(d	|	the)	  

p(n	|	d,	robot)	  

option	2:	p(d	|	the)	  
round)

p(n	|	d,	robot)	  

p(n	|	n,	

p(v	|	n,	

p(a	|	v,	

p(v	|	n,	

p(n	|	v,	

p(	r|	n,	

74

   local classifiers! label bias problem

let   s look at the independence assumption

   next-state   	classifiers	
are	locally	normalized

eg:	part-of-speech	tagging	the	sentence

wheels
1

n

the

robot

0.8

1

d

n

1

0.2

v

are

round

1

1

v

n

a

r

suppose	these	are	the	only	state	transitions	allowed

ml in nlp

example	based	on	[wallach	2002]

wheels)	  

are)	  

wheels)	  

are)	  

round)

option	1:	p(d	|	the)	  

p(n	|	d,	robot)	  

option	2:	p(d	|	the)	  
round)

p(n	|	d,	robot)	  

p(n	|	n,	

p(v	|	n,	

p(a	|	v,	

p(v	|	n,	

p(n	|	v,	

p(	r|	n,	

75

   local classifiers     label bias problem

ml in nlp

76

   local classifiers     label bias problem

the	path	scores	are	the	same	
even	if	the	word	fred	is	never	observed	as	a	verb	in	the	data,	
it	will	be	predicted	as	one	
the	input	fred	does	not	influence	the	output	at	all

ml in nlp

77

example: label bias problem

very	hot dinosaur	attack

c

0.9

0.6

0.8

0.3

0.4

0

0

c	,	0

c	,	1

   

c	,	5

h,	1

h,	2

       f ,			0
    f ,			1

   

h

0.1

0.2

0

0.7

0.6

0

0

in	memm:	even	you   ve	never	seen	this	event,	
you	still	need	to	make	this	row	sum	up	to	1.	

for	crf,	there	is	no	such	restriction.

p(    f5,c =0.51
p(    5,c =0.49
        
        
    f

0

0

0.2

0

0

0

0

0

0

0

0

0

1

1

0

0

0

0

0

0

0

ml in nlp

78

label bias

v states with a single outgoing transition effectively ignore 

their input
v states with lower-id178 next states are less influenced 

by observations

v why? 

v each the next-state classifiers are locally normalized. 
v if a state has fewer next states, each of those will get a higher 

id203 mass 
   and hence preferred

v side note: surprisingly doesn   t affect some tasks

v eg: part-of-speech tagging

ml in nlp

79

summary: local models for sequences

v conditional models

v use rich features in the mode

v possibly suffer from label bias problem

ml in nlp

80

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

ml in nlp

81

outline

v sequence models

v id48

v id136 with id48
v learning

v conditional models and local classifiers

v global models

v id49

ml in nlp

82

so far   

v id48

v pros: decomposition of total id203 with 

tractable

v cons: doesn   t allow use of features for 

representing inputs
v also, generative model

v not really a downside, but we may get better performance 

with conditional models if we care only about predictions

v local, conditional markov models 

v pros: conditional model, allows features to be used
v cons: label bias problem

ml in nlp

83

global models

v train the predictor globally

v instead of training local decisions independently

v normalize globally

v make each edge in the model undirected
v not associated with a id203, but just a    score   

v recall the difference between local vs. global 

for multiclass

ml in nlp

84

id48 vs. a local model vs. a global model

p(yt |	yt-1)

yt-1

id48

yt

xt

p(xt |	yt)

p(yt |	yt-1,	xt)

yt-1

conditional
model

yt

xt

ft(yt,	yt-1)

yt-1

global
model

yt

xt

fe(yt,	xt)

generative

discriminative

local:	p	is	locally	
normalized	to	add	up	
to	one	for	each	t

global:	the	functions	ft
and	fe are	scores	that	
are	not	normalized

ml in nlp

85

id48 vs. a local model vs. a global 
model

p(yt |	yt-1)

yt-1

yt

id48

p(xt |	yt)
xt

p(yt |	yt-1,	xt)

yt-1

conditional
model

yt

xt

ft(yt,	yt-1)

yt-1

global
model

yt

xt

fe(yt,	xt)

generative

discriminative

local:	p	is	locally	
normalized	to	add	up	
to	one	for	each	t

global:	the	functions	ft
and	fe are	scores	that	
are	not	normalized

ml in nlp

86

id48 vs. a local model vs. a global 
model

p(yt |	yt-1)

yt-1

yt

id48

p(xt |	yt)
xt

p(yt |	yt-1,	xt)

yt-1

conditional
model

yt

xt

ft(yt,	yt-1)

yt-1

global
model

yt

xt

fe(yt,	xt)

generative

discriminative

local:	p	is	locally	
normalized	to	add	up	
to	one	for	each	t

global:	the	functions	ft
and	fe are	scores	that	
are	not	normalized

ml in nlp

87

conditional random field

y0

y1

y2

y3

x

each	node	is	a	random	variable

we	observe	some	nodes	and	the	rest	are	unobserved

the	goal:	to	characterize	a	id203	
distribution	over	the	unobserved	variables,	
given	the	observed

ml in nlp

88

conditional random field

y0

y1

y2

y3

x

score(x,	y0,	y1)

score(x,	y1,	y2)

score(x,	y2,	y3)

each	node	is	a	random	variable

we	observe	some	nodes	and	need	to	assign	the	rest

each	clique is	associated	with	a	score

ml in nlp

89

conditional random field

y0

y1

y2

y3

    q    (x,	y0,	y1)

x

    q    (x,	y1,	y2)     q    (x,	y2,	y3)

each	node	is	a	random	variable

we	observe	some	nodes	and	need	to	assign	the	rest

each	clique is	associated	with	a	score

ml in nlp

90

conditional random field: factor graph

factors

y0

y1

y2

y3

    q    (x,	y0,	y1)

x

    q    (x,	y1,	y2)

    q    (x,	y2,	y3)

each	node	is	a	random	variable
we	observe	some	nodes	and	need	to	assign	the	rest
each	clique is	associated	with	a	score

factor

ml in nlp

91

conditional random field: factor graph

a	different	factorization:	
recall	decomposition	of	structures	into	parts.	same	idea

each	node	is	a	random	variable
we	observe	some	nodes	and	need	to	assign	the	rest
each	factor is	associated	with	a	score

ml in nlp

92

conditional random field: factor graph

each	node	is	a	random	variable
we	observe	some	nodes	and	need	to	assign	the	rest
each	factor is	associated	with	a	score

ml in nlp

93

conditional random field for sequences

               =	1    texp	(    q    (    ,    >,    >wx)

>    =	ztexp	(    q    (    ,    ,>,    ,>wx)
[,

>

z:	normalizing	constant,	
sum	over	all	sequences

ml in nlp

94

 
 
 
v define a feature vector for the entire input and output 

v input: x, output: y, both sequences (for now)

crf: a different view

sequence:	    (x, y)
               =	1    texp	(    q    (    ,    >,    >wx)

v define a giant log-linear model, 

p(y | x) parameterized by w

>

   exp     qz    (    ,    >,    >wx)

>

v just like any other log-linear model, except

v space of y is the set of all possible sequences of the correct length
v id172 constant sums over all sequences

in	an	memm,	probabilities	were	locally	normalized

ml in nlp

95

 
 
global features

the feature function decomposes over the 
sequence

ml in nlp

96

prediction

goal: to predict most probable sequence y an input x

but the score decomposes as

prediction via viterbi (with sum instead of product)

ml in nlp

97

training a chain crf

v input: 

v dataset with labeled sequences, d = {<xi, yi>}
v a definition of the feature function 

v how do we train?

v maximize the (regularized) log-likelihood

recall:	empirical	loss	minimization

ml in nlp

98

training with id136

v many methods for training

v numerical optimization
v can use a gradient or hessian based method

v simple gradient ascent

v training involves id136! 

v a different kind than what we have seen so far 
v summing over all sequences is just like viterbi

v with summation instead of maximization

ml in nlp

99

training with id136

v many methods for training

v numerical optimization
v can use a gradient or hessian based method

v simple gradient ascent

v training involves id136! 

v a different kind than what we have seen so far 
v summing over all sequences is just like viterbi

v with summation instead of maximization

ml in nlp

100

crf summary

v an undirected graphical model

v decompose the score over the structure into a collection of factors
v each factor assigns a score to assignment of the random variables 

it is connected to

v final prediction via argmax    q    (x, y)

v training and prediction

v train by maximum (regularized) likelihood (also need id136)

v relation to other models

v effectively a linear classifier
v a generalization of id28 to structures
v an instance of markov random field, with some random variables 

observed (we will see this soon)

ml in nlp

101

from generative models to crf

[figure	from	sutton	and	mccallum,	   05]

ml in nlp

102

general crfs

y1

x1

y2

x2

y3

x3

ml in nlp

103

general crfs

            (y1,	y2,	y3)

            (x1,	y1)

y1

x1

y2

            (x1,	x2,	y2)

x2

            (x3,	y2,	y3)

y3

x3

    (x,	y)	=	    (x1,	y1)	+	    (y1,	y2,	y3)	+	    (x3,	y2,	y3)	+	    (x1,	x2,	y2)

ml in nlp

104

computational questions
1. learning: given a training set {<xi, yi>}

v train via maximum likelihood (typically 

regularized)

v need to compute partition function during training

1. prediction

v go over all possible assignments to the y   s 
v find the one with the highest id203/score

ml in nlp

105

computational questions
1. learning: given a training set {<xi, yi>}

v train via maximum likelihood (typically 

regularized)

v need to compute partition function during training

1. prediction

v go over all possible assignments to the y   s 
v find the one with the highest id203/score

ml in nlp

106

computational questions
1. learning: given a training set {<xi, yi>}

v train via maximum likelihood (typically 

regularized)

v need to compute partition function during training

2. prediction:

v go over all possible assignments to the y   s 
v find the one with the highest id203/score

ml in nlp

107

id136 in id114

1

2

3

4
in general, compute id203 of a subset of states

5

v p(xa), for some subsets of random variables xa

v exact id136

v variable elimination

v marginalize by summing out variables in a    good    order 
v think about what we did for viterbi

v belief propagation (exact only for graphs without loops)

what	makes	an	ordering	good?
v nodes pass messages to each other about their estimate of what 

the neighbor   s state should be

v generally efficient for trees, sequences (and maybe other 

graphs too)

v    approximate    id136

ml in nlp

108

id136 in id114

in general, compute id203 of a subset of states

v p(xa), for some subsets of random variables xa

1

2

3

v exact id136 
v    approximate    id136

np-hard	in	general,	works	for	simple	graphs

4

5

v id115 

v id150/metropolis-hastings

v variational algorithms

v frame id136 as an optimization problem, perturb it to an approximate 

one and solve the approximate problem

v loopy belief propagation

v run bp and hope it works!

ml in nlp

109

