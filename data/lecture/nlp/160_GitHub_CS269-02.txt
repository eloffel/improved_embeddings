lecture 2:

binary classification

kai-wei chang
cs @ ucla

kw@kwchang.net

couse webpage: https://uclanlp.github.io/cs269-17/

ml in nlp

1

announcements

v waiting list: if you   re not enrolled, please sign up. 

v we will use piazza as an online discussion 

platform. please sign up here: 
piazza.com/ucla/fall2017/cs269

ml in nlp

2

this lecture

v supervised learning
v linear classifiers

v id88 algorithm
v support vector machine
v id28 

v optimization in machine learning

cs6501 lecture 2

3

the badges game

+	naoki	abe

- eric	baum

v conference attendees to the icml 1994 were 

given name badges labeled with + or    .

v what function was used to assign these labels? 

cs6501 lecture 2

4

training data

+	naoki	abe
- myriam abramson
+	david	w.	aha
+	kamal	m.	ali
- eric	allender
+	dana	angluin
- chidanand apte
+	minoru	asada
+	lars	asker
+	javed aslam
+	jose	l.	balcazar
- cristina	baroglio

+	peter	bartlett
- eric	baum
+	welton becket
- shai ben-david
+	george	berg
+	neil	berkman
+	malini bhandaru
+	bir bhanu
+	reinhard blasig
- avrim blum
- anselm	blumer
+	justin	boyan

+	carla	e.	brodley
+	nader	bshouty
- wray	buntine
- andrey burago
+	tom	bylander
+	bill	byrne
- claire	cardie
+	john	case
+	jason	catlett
- philip	chan
- zhixiang chen
- chris	darken

cs6501 lecture 2

5

raw test data

gerald	f.	dejong
chris	drummond
yolanda	gil
attilio giordana
jiarong hong
j.	r.	quinlan

priscilla	rasmussen
dan	roth
yoram singer
lyle	h.	ungar

cs6501 lecture 2

6

why we need machine learning?

v there is no (or limited numbers of) human 

expert for some problems
v e.g.: identify dna binding sites, predicting disease 

progression, predicting protein folding structure

cs6501 lecture 2

7

why we need machine learning?

v there is no (or limited numbers of) human 

expert for some problems

v humans can perform a task, but can   t 

describe how they do it
v e.g.: object recognition

cs6501 lecture 2

8

why we need machine learning?

v there is no (or limited numbers of) human 

expert for some problems

v humans can perform a task, but can   t 

describe how they do it

v the desired function is hard to be written 

down in a closed form 
v e.g.,: predict stock price 

cs6501 lecture 2

9

supervised learning

input

x    x

target	function

y	=	       (x)
y	=	        

learned	model

output

y    y

an	item	x

drawn	from	an	
instance	space	x

an	item	y

drawn	from	a	label	

space	y

cs6501 lecture 2

10

supervised learning

input

x    x

an	item	x

drawn	from	an	
instance	space	x

- typically	        0,1( or	    *

x	is	represented	in	a	feature	space

- usually	represented	as	a	vector
- we	call	it	input	vector

cs6501 lecture 2

11

supervised learning

y	is	represented	in	output	space	
(label	space)
different	kinds	of	output:

    multiclass	classification:

    binary	classification:	

    	    {-1,1}	
       {1,2,3,       }
           
        1,2,3,        *

    regression:

    structured	output

output

y    y

an	item	y

drawn	from	a	label	

space	y

cs6501 lecture 2

12

learning the mapping

input

x    x

target	function

y	=	       (x)
y	=	        

learned	model

output

y    y

an	item	x

drawn	from	an	
instance	space	x

an	item	y

drawn	from	a	label	

space	y

cs6501 lecture 2

13

goal

v find a good approximation of          

v good in what sense?

cs6501 lecture 2

14

under-fitting and over-fitting

v which classifier (blue line) is the best one?

cs6501 lecture 2

15

bias v.s. variance

v remember, training data are subsamples drawn from 

the true distribution

v exam strategy:

v study every chapter well

v a+: low var & bias

v study only a few chapters

v a+? b? c? low bias; high var

v study every chapter roughly

v b+:  low var; high bias

v go to sleep 

v b ~d: high var, high bias

cs6501 lecture 2

16

questions of interest

v representation

v modeling

v what assumptions we made

v how to represent x, y (and latent factors)

v i.e., what is the hypothesis set of     	?
v (learn) give data, how to learn     ?
v (id136) give test instance x and      , how to 
evaluate     (x)?

v algorithms

v learning protocols

v what is the goal of the learning algorithm?

cs6501 lecture 2

17

different learning protocols 
(more technical terms)
v supervision signals?

v supervised learning, semi-supervised learning, 

unsupervised learning, bandit feedback

v what to be optimized?

v batch learning: minimize the risk 

(expected average loss)

v online learning:

receive one sample and make prediction
receive the label; then update
minimize the accumulated loss

cs6501 lecture 2

18

linear classification

cs6501 lecture 2

19

linear classifiers

v for now, we consider binary classification

v given a training set     ={    ,    }, find a  
linear threshold units classify an example     

using the classification rule:

1x

6x

12
345
6

1w

6w

7

  

t

y

cs6501 lecture 2

20

the geometry interpretation

cs6501 lecture 2

21

a simple trick to remove the bias term b

					    8    +    
=     8	        	    1
=    ;       <
    ;=     8	    8
    <=		    8		18			

for simplicity, i   ll write     ; and     < as      and     

when there is no confusion  

cs6501 lecture 2

22

some data are not linearly separable

cs6501 lecture 2

23

but they can be made liner

using	a	different	representation
e.g.,	feature	conjunctions,	

non-linear	mapping

cs6501 lecture 2

24

exercise: can you make these data 
points linearly separable?

cs6501 lecture 2

25

exercise: can you make these data 
points linearly separable?

adding	feature	    =	    >

cs6501 lecture 2

26

linear classifiers

v let   s take a look at a few linear classifiers
v we will show later, they can be written in 

the same framework!

v id88
v (linear) support vector machines
v id28

cs6501 lecture 2

27

the id88 algorithm [rosenblatt 1958]

v goal: find a separating hyperplane
v can be used in an online setting:
considers one example at a time

v converges if data is separable

-- mistake bound 

cs6501 lecture 2

28

the id88 algorithm [rosenblatt 1958]

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
      =sg    (    h    )
if             ,            +            
return    
prediction:      ?@a?   sg    (    h    ?@a?)

1.
2.
3.
4.
5.
6.

(predict)
(update)

learning rate

cs6501 lecture 2

29

the id88 algorithm [rosenblatt 1958]

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
if         h     <    
								           +            
return    
prediction:      ?@a?   sg    (    h    ?@a?)

1.
2.
3.
4.
5.
6.

cs6501 lecture 2

30

geometry interpretation 

x	(with	y	=	+1)
1
next	item	to	be	

classified

0.5

wx =	0
0
current	
decision	
boundary
   0.5

   1
   1

   0.5

w

current	

weight	vector
0

0.5

1

x	as	a	vector

0.5

0

   0.5

1

0.5

0

   0.5

   1
   1

1

   0.5

0

x	as	a	vector	
1
added	to	w

0.5

   1
   1

weight	vector	points	to	the	positive side

(figures	from	bishop	2006)

cs6501 lecture 2

wx =	0
new

decision	
boundary

w	

new weight	

vector
   0.5

0

0.5

1

positive
negative

31

id88 in action

x	(with	y	=	+1)
next	item	to	be	

classified

x	as	a	vector

1

0.5

0

1

0.5
wx =	0
current	
decision	
0
boundary

   0.5

   1
   1

   0.5

0

w

   0.5
current	weight	

x	as	a	vector	added	

to	w

vector
0.5

   1
   1

1

   0.5

0

0.5

wx =	0
new

decision	
boundary

   0.5

0

0.5

1

1

0.5

0

   0.5

   1
1
   1

w	

new weight	

vector

(figures	from	bishop	2006)

cs6501 lecture 2

positive
negative

32

convergence of id88

v mistake bound

v if data is linearly separable (i.e., a good linear 

model exists), the id88 will converge 
after a fixed number of mistakes [novikoff 1962]

cs6501 lecture 2

33

marginal id88 -- motivation

v which separating hyper-plane is better?

smaller	(

cs6501 lecture 2

34

the id88 algorithm [rosenblatt 1958]

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
if         h     <    
								           +            
return    
prediction:      ?@a?   sg    (    h    ?@a?)

1.
2.
3.
4.
5.
6.

cs6501 lecture 2

35

the id88 algorithm [rosenblatt 1958]

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
if         h     <    
								           +            
return    
prediction:      ?@a?   sg    (    h    ?@a?)

1.
2.
3.
4.
5.
6.

cs6501 lecture 2

36

how about batch setting?

1. collect training data			    s={    ,    }

v learning as loss minimization

2. pick a hypothesis class

v e.g., linear classifiers, deep neural networks

3. choose a id168

v hinge loss, negative log-likelihood
v we can impose a preference (i.e., prior)  over 

hypotheses, e.g., simpler is better

4. minimize the expected loss

v sgd, coordinate descent, id77s, lbfgs

cs6501 lecture 2

37

batch learning setup

v    s={    ,    } drawn from a fixed, unknown 
distribution     	
v a hidden oracle classifier        ,     =       (    )
v we wish to find a hypothesis             that mimics        
v we define a id168     (        ,           ) that 
v what is the ideal     ?
argmin[   \    ^~   	            ,           

penalizes mistakes

expected loss

cs6501 lecture 2

38

batch learning setup

v    s={    ,    } drawn from a fixed, unknown distribution     	
v a hidden oracle classifier        ,     =       (    )
v we wish to find a hypothesis             that mimics        
v we define a id168     (        ,           ) that penalizes 
    mb=    ,    n =1		        	              
																											0		        	    =       
v what is the ideal     ?
=	min[   \	    ^~   [	#                                ]	
min[   \    ^~   	    ab=        ,           

let   s	define	

mistakes

cs6501 lecture 2

39

how can we learn f from    s
v we don   t know     , we only see samples in     s
min[   \ 1    s   ^,p      s             ,    

v instead, we minimize empirical loss

cs6501 lecture 2

40

how can we prevent over-fitting?

v with sufficient data,      s    d 
v however, if data is insufficient     overfitting
min[   \	        		+		1    s   ^,p      s             ,    
v we will discuss the choices of 	         later

v we can impose a preference over models

cs6501 lecture 2

41

how about the id168?

v usually, we cannot minimize 0-1 loss

v it is a combinatorial optimization problem: np-hard

v idea: minimizing its upper-bound

)

    

    

,

    
(
    

    	    (    )

cs6501 lecture 2

42

how about the id168?

v usually, we cannot minimize 0-1 loss

v it is a combinatorial optimization problem: np-hard

v idea: minimizing its upper-bound

)

    

    

,

    
(
    

    	    (    )

cs6501 lecture 2

43

v we are minimizing with r, l, h with your choice 

many choices

min[   \	        		+		1    s   ^,p      s             ,    
        :	    8       0,           u

v let consider h is a set of d-dimensional  linear function 
v h can be parameterized as

cs6501 lecture 2

44

back to linear model

v we are minimizing with r, l, h with your choice 

min[   \	        		+		1    s   ^,p      s             ,    
        :	    8       0,           u
v we are going to fine the best one based on      s

v let decide h to be a set of d-dimensional  linear 

v h can be parameterized as

function 

v i.e., find the best setting of w and b

cs6501 lecture 2

45

v minimizing the empirical loss:

rewrite our optimization problem

min[   \	        		+		1    s   ^,p      s             ,    
minv   wx	        		+		1    s   ^,p      s         ,    ,    

v minimizing the empirical loss with linear function

v what choices of r and l we have?

cs6501 lecture 2

46

many choices of id168 (l)

cs6501 lecture 2

47

cs6501 lecture 2

48

function

many choices of r(w)
v minimizing the empirical loss with linear 

minv   wx	        		+		1    s   ^,p      s         ,    ,    
         = #non-zero elements in w   (l0 regularizer)
         =        y
 y
         =       y>
=    8    
 y

v prefer simpler model: (how?)

v gaussian prior (large margin w/ hinge loss):

(l1 regularizer)

(l2 regularizer)

v sparse: 

cs6501 lecture 2

49

support vector machines 

cs6501 lecture 2

cmu	ml	protest

50

support vector machines (id166s)

vr(w): l2-loss,         ,    ,     : hinge loss
min     			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))

 y

v maximizing margin  (why?!!)

cs6501 lecture 2

51

the hinge loss

cs6501 lecture 2

52

cs6501 lecture 2

53

solving:

let   s view it from another direction

v id166 learns a model      on    ={        ,    y} by 
min    ,(cid:134)			=>    8    
s.t 				y(cid:129)(    (cid:131)    (cid:129)+    )   1,       y,    y        
>||v	||?
argmax >||v	||
=arg	min	||    	||
=arg	min	||    	||>
		=arg	min			    8    

why	the	margin	is	

(hard	id166)

cs6501 lecture 2

54

why?

soft id166s

v data is not separable     hard id166 fails
v introduce a set of slack variable {    y}
    relax the constraints
v given	    =         ,    y
, soft id166 solves:
min    ,(cid:134),    			=>    8    +           y
 y
s.t			y(cid:129)(    (cid:131)    (cid:129)+    )   1       y;	    y   0						       	

(soft	id166)

penalty	parameter

cs6501 lecture 2

55

an alternative formulation

min    ,(cid:134),    			=>    8    +           y
 y
s.t			y(cid:129)(    (cid:131)    (cid:129)+    )   1       y;	    y   0						       	
    y   1   	y(cid:129)(    (cid:131)    (cid:129)+    );	    y   0        	
v in the optimum,    y=max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)+    ))
min    ,(cid:134)			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)+    ))

v soft id166 can be rewritten as:

v rewrite the constraints:

 y

id173		term

empirical	loss

cs6501 lecture 2

56

an alternative formulation

min    ,(cid:134),    			=>    8    +           y
 y
s.t			y(cid:129)(    (cid:131)    (cid:129)+    )   1       y;	    y   0						       	
    y   1   	y(cid:129)(    (cid:131)    (cid:129)+    );	    y   0        	
v in the optimum,    y=max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)+    ))
min    ,(cid:134)			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)+    ))

we	can	simply	   b   	using	the	trick
however,	we	will	add	b	into	the	id173	term
it	is	often	okay,	if	we	have	many	features	

v soft id166 can be rewritten as:

v rewrite the constraints:

 y

id173		term

empirical	loss

cs6501 lecture 2

57

balance between id173 and 
empirical loss

58

cs6501 lecture 2

balance between id173 and 
empirical loss

59

(demo)

cs6501 lecture 2

regularized loss minimization 
v l1-loss id166

						min     			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))
	min     			=>    8    +       max(0,1   	y(cid:129)(    (cid:131)    (cid:129)))>
					min     			=>    8    +       log(1+    b(cid:147)(cid:148)(    (cid:149)    (cid:148)))

v id28  (regularized)

v l2-loss id166

 y
 y

 y

v loss over training data + regularizer

cs6501 lecture 2

60

id168s

cs6501 lecture 2

61

id28 

regression                    id28 

logistic	function

cs6501 lecture 2

62

logistic function / sigmoid function

v when            what is         ?
v when               what is         ?
what is         ?
v when     =0

cs6501 lecture 2

63

why sigmoid?

cs6501 lecture 2

64

probabilistic interpretation

min     			12    8    +    (cid:154)log(1+    b(cid:147)(cid:148)(    (cid:149)    (cid:148)))

y

assume labels are generated using the following 
id203 distribution:

cs6501 lecture 2

65

 
how	to	make	prediction?

predict	y=1	if	p(y=1|x,w	)	>	p	(y=	-1|x,	w)

cs6501 lecture 2

66

decision boundary?

v the decision boundary?

cs6501 lecture 2

67

alternative view

v predict y=1 if p(y=1|x,w ) > p(y= -1|x, w)

v when does this happen?

=(cid:155)@(cid:156)(cid:157)	(bv(cid:158)^)>0.5
=
   1+exp       8     <2
   exp       8     <1
       8    >0

v

cs6501 lecture 2

68

id113 

v probabilistic model assumption:

v the log-likelihood of seeing a dataset 

d = {(x , y )} if the true weight vector was w: 

             =  y	        y    y,    
   log             =   ylog        y    y,     	

cs6501 lecture 2

69

minimizing negative log-likelihood

v log likelihood

 y

v id28

min    ,(cid:134)			   log(1+    b(cid:147)(cid:148)(    (cid:149)    (cid:148)))
v simpler is better     add gaussian prior 

v let   s add some prior

cs6501 lecture 2

70

add gaussian prior

v simpler is better     add gaussian prior 
centered at zero with variance     >

v suppose each element in w is drawn 

independently from the normal distribution 

v bias towards smaller weights

cs6501 lecture 2

71

regularized id28

v remember we are in the log space

v             	           ,     =        	        (    )

v put them together

v learning: 

find weight vector by maximizing the posterior 
distribution p(w | d) 

cs6501 lecture 2

72

maximum a posteriori estimation

v             	           ,     =        	        (    )

v put them together

v learning:  find weight vector by maximizing the 

posterior distribution p(w | d) 

	min     			12    8    +    (cid:154)log(1+    b(cid:147)(cid:148)(    (cid:149)    (cid:148)))

prior

log-likelihood

y

cs6501 lecture 2

73

 
regularized loss minimization 
v l1-loss id166

						min     			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))
	min     			=>    8    +       max(0,1   	y(cid:129)(    (cid:131)    (cid:129)))>
					min     			=>    8    +       log(1+    b(cid:147)(cid:148)(    (cid:149)    (cid:148)))

v id28  (regularized)

v l2-loss id166

 y
 y

 y

v loss over training data + regularizer

cs6501 lecture 2

74

how to learn 
(how to optimize the objective function?)

min     			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))

 y

v this function is convex
v many id76 methods can be used

v stochastic (sub)-id119
v coordinate descent methods
v id77s
v lbfgs

cs6501 lecture 2

75

convexity

cs6501 lecture 2

76

non convex minimization is hard

v you may end up with some local minimum

cs6501 lecture 2

77

id76 is relatively easy

v ensure that there are no local minima
v note: need special design for functions that 

are not differentiable (e.g., hinge loss)

cs6501 lecture 2

78

different optimization techniques

some	methods	(e.g.,	sgd,	cd)	
are	fast	in	the	early	stage	of	
optimization

some	methods	(e.g.,	newton	
methods)	converge	faster

results	from	http://www.cs.virginia.edu/~kc2wc/papers/changhsli08.pdf

cs6501 lecture 2

79

id119

cs6501 lecture 2

80

v for some functions, gradient may not exist

 y

why not id119?

v solution: use sub-gradient

min     			=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))
    n=           	
		         =	=>    8    +       max(0,	1   	y(cid:129)    (cid:131)    (cid:129))
											=	     |    |	   (	=>    8    +       max(0,	1   	y(cid:129)    (cid:131)    (cid:129))
)
 y
                   y    
 y
             = =|   |           y(    )
 y
        y               																					        	y(cid:129)(    (cid:131)    (cid:129))>1
                  y    y			                                   																			,	
	

 y

cs6501 lecture 2

81

stochastic id119 

		         =	=>    8    +       max(0,	1   	y(cid:129)    (cid:131)    (cid:129))
											=	     |    |	   (	=>    8    +       max(0,	1   	y(cid:129)    (cid:131)    (cid:129))
)
 y
   		    y    
             = =|   |           y(    )
 y

 y
=    y~           y(    )
randomly	pick	one	sample	(    y,    y)
update	w   w           f(cid:129)(w)

v approximate the true gradient by a 

gradient at a single example at a time

repeat	until	converge:

cs6501 lecture 2

82

stochastic sub-id119

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
update                   	    	    (    )
return    
            	=>    8    +       max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))

1.
2.
3.
4.
5.

 y

cs6501 lecture 2

83

stochastic (sub)-id119 for id166

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
if         h     <    
								        1           +    	    	        
else        1           
return    

1.
2.
3.
4.
5.
6.
7.
8.

cs6501 lecture 2

84

the id88 algorithm [rosenblatt 1958]

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
if         h     <    
								           +            
return    
prediction:      ?@a?   sg    (    h    ?@a?)

1.
2.
3.
4.
5.
6.

cs6501 lecture 2

85

the id88 algorithm [rosenblatt 1958]

given a training set     ={    ,    }
initialize                 (
for epoch 1       :
for(    ,    ) in    :
if         h     <    
								           +            
return    
prediction:      ?@a?   sg    (    h    ?@a?)
(cid:154)max(0,	1   	y(cid:129)(    (cid:131)    (cid:129)))
y

1.
2.
3.
4.
5.
6.

id88	effectively	minimizing:

cs6501 lecture 2

86

 
a general formula

    (cid:176)=argmax               (    ;    ,    )
v id136/test: given     ,    , solve argmax
v learning/training: find a good     
v today:               ,    ={   1,1} (binary classification)

input
model parameters 

output space

cs6501 lecture 2

87

binary linear classifiers

    (cid:176)=argmax               (    ;    ,    )
v              ,    ={   1,1}
v        ;    ,                h    +     =           y    y
+    
 y
vargmax               (    ;    ,    )=  			1,    h    +       0
	   1,    h    +    <0
=sgn    h    +    

(break ties arbitrarily)

cs6501 lecture 2

88

