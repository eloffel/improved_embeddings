lecture 5:

representation learning

kai-wei chang
cs @ ucla

kw@kwchang.net

couse webpage: https://uclanlp.github.io/cs269-17/

ml in nlp

1

this lecture

v review: neural network
v recurrent nn
v representation learning in nlp

ml in nlp

2

neural network

based	on	slide	by	andrewng

10

neural network (feed forward)

slide	by	andrewng

12

feed-forward process

vinput	layer	units	are	features	(in	nlp,	e.g.,	words)

v usually, one-hot vector or id27

vworking	forward	through	the	network,	the	input		

function	is	applied	to	compute	the	input		value
ve.g.,	weighted	sum	of	the	input

vthe	activation	function	transforms	this	input		

function	into	a	   nal value
vtypically	a	nonlinear	function	(e.g,	sigmoid)

based	on	slide	by	t.	finin,	m.	desjardins,	l	getoor,	r.par

13

slide	by	andrewng

14

vector representation

based	on	slide	by	andrewng

15

can extend to multi-class

pedestrian

car

motorcycle

truck

slide	by	andrewng

17

why staged predictions?

based	on	slide	and	example	by	andrewng

21

representing boolean functions

22

combining representations to create  

non-linear functions

based	on	example	by	andrewng

23

layering representations

x1 ... x20
x21 ... x40
x41 ... x60

.
.
.

x381 ... x400

20    20 pixel images
d = 400
10 classes

each image is    unrolled    into a vector x of pixel intensities

2
4

layering representations

x1
x2
x3
x4  
x5

xd

input layer

output layer

hidden layer

visualization of  hidden layer

   0   
   1   

   9   

2
5

this lecture

v review: neural network

v learning nn

v recursive and recurrent nn
v representation learning in nlp

ml in nlp

14

stochastic sub-id119

given a training set     ={    ,    }
initialize                 &
for epoch 1       :
for(    ,    ) in    :
update                   	    	    (    )
return    

1.
2.
3.
4.
5.

ml in nlp

15

recap: id28

min     			    2        a    +1    clog(1+    hij(    k    j))
	
(id203    =1 given    n)
lethp(    n)=1/(1+    hpstu)
vw&    a    +x&   y[	log(   p(    n))+(1       n)(	log(1      p(    n))
	
 n

n

ml in nlp

16

 
         =         +        ,

cost function

			         =    	    a    

based	on	slide	by	andrewng

3
2

optimizing the neural network

based	on	slide	by	andrewng

3
3

forward propagation

based	on	slide	by	andrewng

3
4

id26: compute gradient

based	on	slide	by	andrewng

36

this lecture

v review: neural network
v recurrent nn
v representation learning in nlp

ml in nlp

21

how to deal with input with variant size?

v use same parameters

today									is															a													   

</s>

<s>										today											is													   

day		

advanced ml: id136

22

recurrent neural networks

recurrent neural networks

unroll id56s

u

v

id56 training

v back-propagation over time

vanishing gradients

v for the traditional id180, 

each gradient term has the value in range 
(-1, 1).

v multiplying n of these small numbers to 

compute gradients

v the longer the sequence is, the more 

severe the problems are. 

id56s characteristics

v model hidden states (input) dependencies
v errors    back propagation over time   
v id171 methods
v vanishing gradient problem: 

cannot model long-distant dependencies of 
the hidden states.

long-short term memory networks 
(lstms)

use gates to control the information to
be	added	from	the	input,	forgot	from	the
previous memories, and outputted.
   and f are sigmoid and tanh function 
respectively,	to	map	the	value	to	[-1,	1]

another visualization

capable of modeling long-distant dependencies between states. 

figure	credit:	christopher	olah

bidirectional lstms

how to deal with sequence output?

v idea 1: combine dl with crf

v idea 2: introduce structure in dl

ml in nlp

32

lstms for sequential tagging

yt

yt =wht +b
l(yt, y^
min

   
t

t)

yt =wht

sophisticated model of input + local predictions.

recall crfs for sequential tagging

arbitrary features	on	the	input	side
markov assumption	on	the	output	side

lstms for sequential tagging

v completely ignored the interdependencies 

of the outputs

v will this work? yes. 

v liang et. al. (2008), structure compilation: 

trading structure for features

v is this the best model? not necessarily.

combining crfs with lstms

traditional crfs v.s. 
lstm-crfs
v traditional crfs:

p(y | x;  ) =

n
   
n=1

   
y

v lstm-crfs:

1

exp(  f (yi, yi   1, x1:n))

n
   
n=1

exp(  f (yi, yi   1, x1:n))

p(y | x;  ) =

n
   
n=1

   
y

1

exp(  f (yi, yi   1,lstm(x1:n)))

n
   
n=1

exp(  f (yi, yi   1,lstm(x1:n)))

   ={  ,  } where	   is	lstm	parameters

combining two benefits

    directly model output dependencies by crfs.
    powerful automatic id171 using bilstms.
    jointly training all the parameters to    share the 

modeling responsibilities   

id21 with lstm-crfs

v neural networks as feature learner
v share the feature learner for different tasks
v jointly train the feature learners so that it 

learns the common features.

v use different crfs for different tasks to 

encode task-specific information
v going forward, one can imagine using other 
id114 besides linear chain crfs.

id21 cws + ner

shared

joint training

v simply linearly combine two objectives.

v alternative updates for each module   s 

parameters.

how to deal with sequence output?

v idea 1: combine dl with crf

v idea 2: introduce structure in dl

ml in nlp

42

advanced ml: id136

43

advanced ml: id136

44

advanced ml: id136

45

advanced ml: id136

46

advanced ml: id136

47

