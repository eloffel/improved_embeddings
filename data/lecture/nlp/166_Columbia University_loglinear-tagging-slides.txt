id148 for tagging

(maximum-id178 markov models

(memms))

michael collins, columbia university

part-of-speech tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall street,
as their ceo alan mulally announced    rst quarter results.

output:

pro   ts/n soared/v at/p boeing/n co./n ,/, easily/adv topping/v
forecasts/n on/p wall/n street/n ,/, as/p their/poss ceo/n
alan/n mulally/n announced/v    rst/adj quarter/n results/n ./.

= noun
= verb
= preposition

n
v
p
adv = adverb
adj
. . .

= adjective

id39

input: pro   ts soared at boeing co., easily topping forecasts on
wall street, as their ceo alan mulally announced    rst quarter
results.

output: pro   ts soared at [company boeing co.], easily
topping forecasts on [location wall street], as their ceo [person
alan mulally] announced    rst quarter results.

named entity extraction as tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall street,
as their ceo alan mulally announced    rst quarter results.

output:

pro   ts/na soared/na at/na boeing/sc co./cc ,/na easily/na
topping/na forecasts/na on/na wall/sl street/cl ,/na as/na
their/na ceo/na alan/sp mulally/cp announced/na    rst/na
quarter/na results/na ./na

na
sc
cc
sl
cl
. . .

= no entity
= start company
= continue company
= start location
= continue location

our goal

training set:
1 pierre/nnp vinken/nnp ,/, 61/cd years/nns old/jj ,/, will/md
join/vb the/dt board/nn as/in a/dt nonexecutive/jj director/nn
nov./nnp 29/cd ./.
2 mr./nnp vinken/nnp is/vbz chairman/nn of/in elsevier/nnp
n.v./nnp ,/, the/dt dutch/nnp publishing/vbg group/nn ./.
3 rudolph/nnp agnew/nnp ,/, 55/cd years/nns old/jj and/cc
chairman/nn of/in consolidated/nnp gold/nnp fields/nnp plc/nnp
,/, was/vbd named/vbn a/dt nonexecutive/jj director/nn of/in
this/dt british/jj industrial/jj conglomerate/nn ./.
. . .
38,219 it/prp is/vbz also/rb pulling/vbg 20/cd people/nns out/in
of/in puerto/nnp rico/nnp ,/, who/wp were/vbd helping/vbg
huricane/nnp hugo/nnp victims/nns ,/, and/cc sending/vbg
them/prp to/to san/nnp francisco/nnp instead/rb ./.

(cid:73) from the training set, induce a function/algorithm that maps new

sentences to their tag sequences.

overview

(cid:73) recap: the tagging problem

(cid:73) log-linear taggers

id148 for tagging

(cid:73) we have an input sentence w[1:n] = w1, w2, . . . , wn

(wi is the i   th word in the sentence)

id148 for tagging

(cid:73) we have an input sentence w[1:n] = w1, w2, . . . , wn

(wi is the i   th word in the sentence)

(cid:73) we have a tag sequence t[1:n] = t1, t2, . . . , tn

(ti is the i   th tag in the sentence)

id148 for tagging

(cid:73) we have an input sentence w[1:n] = w1, w2, . . . , wn

(wi is the i   th word in the sentence)

(cid:73) we have a tag sequence t[1:n] = t1, t2, . . . , tn

(ti is the i   th tag in the sentence)

(cid:73) we   ll use an log-linear model to de   ne

p(t1, t2, . . . , tn|w1, w2, . . . , wn)

for any sentence w[1:n] and tag sequence t[1:n] of the same length.
(note: contrast with id48 that de   nes p(t1 . . . tn, w1 . . . wn))

id148 for tagging

(cid:73) we have an input sentence w[1:n] = w1, w2, . . . , wn

(wi is the i   th word in the sentence)

(cid:73) we have a tag sequence t[1:n] = t1, t2, . . . , tn

(ti is the i   th tag in the sentence)

(cid:73) we   ll use an log-linear model to de   ne

p(t1, t2, . . . , tn|w1, w2, . . . , wn)

for any sentence w[1:n] and tag sequence t[1:n] of the same length.
(note: contrast with id48 that de   nes p(t1 . . . tn, w1 . . . wn))

(cid:73) then the most likely tag sequence for w[1:n] is

t   
[1:n] = argmaxt[1:n]

p(t[1:n]|w[1:n])

how to model p(t[1:n]|w[1:n])?
p(t[1:n]|w[1:n]) =(cid:81)n

a trigram log-linear tagger:

j=1 p(tj | w1 . . . wn, t1 . . . tj   1)

chain rule

a trigram log-linear tagger:

how to model p(t[1:n]|w[1:n])?
p(t[1:n]|w[1:n]) =(cid:81)n
=(cid:81)n

j=1 p(tj | w1 . . . wn, t1 . . . tj   1)

chain rule

j=1 p(tj | w1, . . . , wn, tj   2, tj   1)

(cid:73) we take t0 = t   1 = *

independence assumptions

a trigram log-linear tagger:

how to model p(t[1:n]|w[1:n])?
p(t[1:n]|w[1:n]) =(cid:81)n
=(cid:81)n

j=1 p(tj | w1 . . . wn, t1 . . . tj   1)

chain rule

j=1 p(tj | w1, . . . , wn, tj   2, tj   1)

independence assumptions

(cid:73) we take t0 = t   1 = *
(cid:73) independence assumption: each tag only depends on previous

two tags
p(tj|w1, . . . , wn, t1, . . . , tj   1) = p(tj|w1, . . . , wn, tj   2, tj   1)

an example

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .
    there are many possible tags in the position ??
y = {nn, nns, vt, vi, in, dt, . . .}

representation: histories

(cid:73) a history is a 4-tuple (cid:104)t   2, t   1, w[1:n], i(cid:105)
(cid:73) t   2, t   1 are the previous two tags.
(cid:73) w[1:n] are the n words in the input sentence.
(cid:73) i is the index of the word being tagged
(cid:73) x is the set of all possible histories

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .

(cid:73) t   2, t   1 = dt, jj
(cid:73) w[1:n] = (cid:104)hispaniola, quickly, became, . . . , hemisphere, .(cid:105)
(cid:73) i = 6

recap: feature vector representations in log-linear
models

(cid:73) we have some input domain x , and a    nite label set y. aim
is to provide a id155 p(y | x) for any x     x
and y     y.

(cid:73) a feature is a function f : x    y     r

(often binary features or indicator functions
f : x    y     {0, 1}).

(cid:73) say we have m features fk for k = 1 . . . m

    a feature vector f (x, y)     rm for any x     x and y     y.

an example (continued)

(cid:73) x is the set of all possible histories of form (cid:104)t   2, t   1, w[1:n], i(cid:105)
(cid:73) y = {nn, nns, vt, vi, in, dt, . . .}
(cid:73) we have m features fk : x    y     r for k = 1 . . . m

for example:

f1(h, t) =

f2(h, t) =

. . .

(cid:26) 1 if current word wi is base and t = vt
(cid:26) 1 if current word wi ends in ing and t = vbg

0 otherwise

0 otherwise

f1((cid:104)jj, dt, (cid:104) hispaniola, . . .(cid:105), 6(cid:105), vt) = 1
f2((cid:104)jj, dt, (cid:104) hispaniola, . . .(cid:105), 6(cid:105), vt) = 0
. . .

the full set of features in [(ratnaparkhi, 96)]

(cid:73) word/tag features for all word/tag pairs, e.g.,

(cid:26) 1 if current word wi is base and t = vt

f100(h, t) =

0 otherwise

(cid:73) spelling features for all pre   xes/su   xes of length     4, e.g.,

(cid:26) 1 if current word wi ends in ing and t = vbg
(cid:26) 1 if current word wi starts with pre and t = nn

0 otherwise

f101(h, t) =

f102(h, t) =

0 otherwise

the full set of features in [(ratnaparkhi, 96)]

(cid:73) contextual features, e.g.,

0 otherwise

0 otherwise

(cid:26) 1 if (cid:104)t   2, t   1, t(cid:105) = (cid:104)dt, jj, vt(cid:105)
(cid:26) 1 if (cid:104)t   1, t(cid:105) = (cid:104)jj, vt(cid:105)
(cid:26) 1 if (cid:104)t(cid:105) = (cid:104)vt(cid:105)
(cid:26) 1 if previous word wi   1 = the and t = vt
(cid:26) 1 if next word wi+1 = the and t = vt

0 otherwise

0 otherwise

0 otherwise

f103(h, t) =

f104(h, t) =

f105(h, t) =

f106(h, t) =

f107(h, t) =

id148

(cid:73) we have some input domain x , and a    nite label set y. aim
is to provide a id155 p(y | x) for any x     x
and y     y.

(cid:73) a feature is a function f : x    y     r

(often binary features or indicator functions
f : x    y     {0, 1}).

(cid:73) say we have m features fk for k = 1 . . . m

    a feature vector f (x, y)     rm for any x     x and y     y.

(cid:73) we also have a parameter vector v     rm
(cid:73) we de   ne

p(y | x; v) =

(cid:80)

ev  f (x,y)
y(cid:48)   y ev  f (x,y(cid:48))

training the log-linear model

(cid:73) to train a log-linear model, we need a training set (xi, yi) for

i = 1 . . . n. then search for

                  

(cid:88)
(cid:124)

i

v    = argmaxv

log p(yi|xi; v)

(cid:123)(cid:122)

log   likelihood

                  

(cid:88)
(cid:124) (cid:123)(cid:122) (cid:125)

v2
k

k

regularizer

      
2

(cid:125)

(see last lecture on id148)

(cid:73) training set is simply all history/tag pairs seen in the training

data

the viterbi algorithm

problem: for an input w1 . . . wn,    nd

p(t1 . . . tn | w1 . . . wn)

arg max
t1...tn

we assume that p takes the form

n(cid:89)

p(t1 . . . tn | w1 . . . wn) =

q(ti|ti   2, ti   1, w[1:n], i)

(in our case q(ti|ti   2, ti   1, w[1:n], i) is the estimate from a
log-linear model.)

i=1

the viterbi algorithm

(cid:73) de   ne n to be the length of the sentence
(cid:73) de   ne

k(cid:89)

r(t1 . . . tk) =

q(ti|ti   2, ti   1, w[1:n], i)

i=1

(cid:73) de   ne a id145 table

  (k, u, v) = maximum id203 of a tag sequence ending

in tags u, v at position k

that is,

  (k, u, v) = max

(cid:104)t1,...,tk   2(cid:105) r(t1 . . . tk   2, u, v)

a recursive de   nition

base case:

  (0, *, *) = 1

recursive de   nition:
for any k     {1 . . . n}, for any u     sk   1 and v     sk:

(cid:0)  (k     1, t, u)    q(v|t, u, w[1:n], k)(cid:1)

  (k, u, v) = max
t   sk   2

where sk is the set of possible tags at position k

the viterbi algorithm with backpointers

input: a sentence w1 . . . wn, log-linear model that provides q(v|t, u, w[1:n], i) for any
tag-trigram t, u, v, for any i     {1 . . . n}
initialization: set   (0, *, *) = 1.
algorithm:

(cid:73) for k = 1 . . . n,

(cid:73) for u     sk   1, v     sk,

  (k, u, v) = max
t   sk   2

bp(k, u, v) = arg max
t   sk   2
(cid:73) set (tn   1, tn) = arg max(u,v)   (n, u, v)
(cid:73) for k = (n     2) . . . 1, tk = bp(k + 2, tk+1, tk+2)
(cid:73) return the tag sequence t1 . . . tn

(cid:0)  (k     1, t, u)    q(v|t, u, w[1:n], k)(cid:1)
(cid:0)  (k     1, t, u)    q(v|t, u, w[1:n], k)(cid:1)

faq segmentation: mccallum et. al

(cid:73) mccallum et. al compared id48 and log-linear taggers on a

faq segmentation task

(cid:73) main point: in an id48, modeling

p(word|tag)

is di   cult in this domain

faq segmentation: mccallum et. al

<head>x-nntp-poster: newshound v1.33
<head>
<head>archive name: acorn/faq/part2
<head>frequency: monthly
<head>

<question>2.6) what configuration of serial cable should i use

<answer>
<answer>
here follows a diagram of the necessary connections
<answer>programs to work properly. they are as far as i know t
<answer>agreed upon by commercial comms software developers fo
<answer>
<answer>
pins 1, 4, and 8 must be connected together inside
<answer>is to avoid the well known serial port chip bugs. the

faq segmentation: line features

begins-with-number
begins-with-ordinal
begins-with-punctuation
begins-with-question-word
begins-with-subject
blank
contains-alphanum
contains-bracketed-number
contains-http
contains-non-space
contains-number
contains-pipe
contains-question-mark
ends-with-question-mark
first-alpha-is-capitalized
indented-1-to-4

faq segmentation: the log-linear tagger

<head>x-nntp-poster: newshound v1.33
<head>
<head>archive name: acorn/faq/part2
<head>frequency: monthly
<head>

<question>2.6) what configuration of serial cable should i use

here follows a diagram of the necessary connections

       tag=question;prev=head;begins-with-number   
   tag=question;prev=head;contains-alphanum   
   tag=question;prev=head;contains-nonspace   
   tag=question;prev=head;contains-number   
   tag=question;prev=head;prev-is-blank   

faq segmentation: an id48 tagger

<question>2.6) what configuration of serial cable should i use

(cid:73) first solution for p(word | tag):

p(   2.6) what con   guration of serial cable should i use    | question) =
e( 2.6) | question)  
e(w hat | question)  
e(conf iguration | question)  
e(of | question)  
e(serial | question)  
. . .

(cid:73) i.e. have a language model for each tag

faq segmentation: mccallum et. al

(cid:73) second solution:    rst map each sentence to string of features:

<question>2.6) what configuration of serial cable should i use
   

<question>begins-with-number contains-alphanum contains-nonspace
contains-number prev-is-blank

(cid:73) use a language model again:

p(   2.6) what con   guration of serial cable should i use    | question) =
e(begins-with-number | question)  
e(contains-alphanum | question)  
e(contains-nonspace | question)  
e(contains-number | question)  
e(prev-is-blank | question)  

faq segmentation: results

precision recall
method
0.362
me-stateless
0.038
tokenid48 0.276
0.140
0.529
featureid48 0.413
memm
0.867
0.681

(cid:73) precision and recall results are for recovering segments

faq segmentation: results

precision recall
method
0.362
me-stateless
0.038
tokenid48 0.276
0.140
0.529
featureid48 0.413
memm
0.867
0.681

(cid:73) precision and recall results are for recovering segments
(cid:73) me-stateless is a log-linear model that treats every sentence

seperately (no dependence between adjacent tags)

faq segmentation: results

precision recall
method
0.362
me-stateless
0.038
tokenid48 0.276
0.140
0.529
featureid48 0.413
memm
0.867
0.681

(cid:73) precision and recall results are for recovering segments
(cid:73) me-stateless is a log-linear model that treats every sentence

seperately (no dependence between adjacent tags)

(cid:73) tokenid48 is an id48 with    rst solution we   ve just seen

faq segmentation: results

precision recall
method
0.362
me-stateless
0.038
tokenid48 0.276
0.140
0.529
featureid48 0.413
memm
0.867
0.681

(cid:73) precision and recall results are for recovering segments
(cid:73) me-stateless is a log-linear model that treats every sentence

seperately (no dependence between adjacent tags)

(cid:73) tokenid48 is an id48 with    rst solution we   ve just seen
(cid:73) featureid48 is an id48 with second solution we   ve just seen

faq segmentation: results

precision recall
method
0.362
me-stateless
0.038
tokenid48 0.276
0.140
0.529
featureid48 0.413
memm
0.867
0.681

(cid:73) precision and recall results are for recovering segments
(cid:73) me-stateless is a log-linear model that treats every sentence

seperately (no dependence between adjacent tags)

(cid:73) tokenid48 is an id48 with    rst solution we   ve just seen
(cid:73) featureid48 is an id48 with second solution we   ve just seen
(cid:73) memm is a log-linear trigram tagger (memm stands for

   maximum-id178 markov model   )

summary

(cid:73) key ideas in log-linear taggers:

(cid:73) decompose

p(t1 . . . tn|w1 . . . wn) =

n(cid:89)

i=1

p(ti|ti   2, ti   1, w1 . . . wn)

(cid:73) estimate

p(ti|ti   2, ti   1, w1 . . . wn)

using a log-linear model

(cid:73) for a test sentence w1 . . . wn, use the viterbi algorithm to

   nd

arg max
t1...tn

(cid:32) n(cid:89)

i=1

(cid:33)

p(ti|ti   2, ti   1, w1 . . . wn)

(cid:73) key advantage over id48 taggers:    exibility in the features

they can use

