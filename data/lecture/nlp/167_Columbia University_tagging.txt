tagging problems, and id48

michael collins, columbia university

overview

(cid:73) the tagging problem

(cid:73) generative models, and the noisy-channel model, for

supervised learning

(cid:73) hidden markov model (id48) taggers

(cid:73) basic de   nitions
(cid:73) parameter estimation
(cid:73) the viterbi algorithm

part-of-speech tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall street,
as their ceo alan mulally announced    rst quarter results.

output:
pro   ts/n soared/v at/p boeing/n co./n ,/, easily/adv topping/v
forecasts/n on/p wall/n street/n ,/, as/p their/poss ceo/n
alan/n mulally/n announced/v    rst/adj quarter/n results/n ./.

= noun
= verb
= preposition

n
v
p
adv = adverb
adj
. . .

= adjective

id39

input: pro   ts soared at boeing co., easily topping forecasts on wall
street, as their ceo alan mulally announced    rst quarter results.

output: pro   ts soared at [company boeing co.], easily topping
forecasts on [location wall street], as their ceo [person alan mulally]
announced    rst quarter results.

named entity extraction as tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall street,
as their ceo alan mulally announced    rst quarter results.

output:
pro   ts/na soared/na at/na boeing/sc co./cc ,/na easily/na
topping/na forecasts/na on/na wall/sl street/cl ,/na as/na
their/na ceo/na alan/sp mulally/cp announced/na    rst/na
quarter/na results/na ./na

na
sc
cc
sl
cl
. . .

= no entity
= start company
= continue company
= start location
= continue location

our goal

training set:
1 pierre/nnp vinken/nnp ,/, 61/cd years/nns old/jj ,/, will/md
join/vb the/dt board/nn as/in a/dt nonexecutive/jj director/nn
nov./nnp 29/cd ./.
2 mr./nnp vinken/nnp is/vbz chairman/nn of/in elsevier/nnp
n.v./nnp ,/, the/dt dutch/nnp publishing/vbg group/nn ./.
3 rudolph/nnp agnew/nnp ,/, 55/cd years/nns old/jj and/cc
chairman/nn of/in consolidated/nnp gold/nnp fields/nnp plc/nnp
,/, was/vbd named/vbn a/dt nonexecutive/jj director/nn of/in
this/dt british/jj industrial/jj conglomerate/nn ./.
. . .
38,219 it/prp is/vbz also/rb pulling/vbg 20/cd people/nns out/in
of/in puerto/nnp rico/nnp ,/, who/wp were/vbd helping/vbg
huricane/nnp hugo/nnp victims/nns ,/, and/cc sending/vbg
them/prp to/to san/nnp francisco/nnp instead/rb ./.

(cid:73) from the training set, induce a function/algorithm that maps

new sentences to their tag sequences.

two types of constraints

in   uential/jj members/nns of/in the/dt house/nnp ways/nnp and/cc
means/nnp committee/nnp introduced/vbd legislation/nn that/wdt
would/md restrict/vb how/wrb the/dt new/jj savings-and-loan/nn
bailout/nn agency/nn can/md raise/vb capital/nn ./.

(cid:73)    local   : e.g., can is more likely to be a modal verb md

rather than a noun nn

(cid:73)    contextual   : e.g., a noun is much more likely than a

verb to follow a determiner

(cid:73) sometimes these preferences are in con   ict:

the trash can is in the garage

overview

(cid:73) the tagging problem

(cid:73) generative models, and the noisy-channel model, for

supervised learning

(cid:73) hidden markov model (id48) taggers

(cid:73) basic de   nitions
(cid:73) parameter estimation
(cid:73) the viterbi algorithm

supervised learning problems

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. each x(i)

is an input, each y(i) is a label.

(cid:73) task is to learn a function f mapping inputs x to labels f (x)

supervised learning problems

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. each x(i)

is an input, each y(i) is a label.

(cid:73) task is to learn a function f mapping inputs x to labels f (x)

(cid:73) conditional models:

(cid:73) learn a distribution p(y|x) from training examples
(cid:73) for any test input x, de   ne f (x) = arg maxy p(y|x)

generative models

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. task is

to learn a function f mapping inputs x to labels f (x).

generative models

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. task is

to learn a function f mapping inputs x to labels f (x).

(cid:73) generative models:

(cid:73) learn a distribution p(x, y) from training examples
(cid:73) often we have p(x, y) = p(y)p(x|y)

generative models

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. task is

to learn a function f mapping inputs x to labels f (x).

(cid:73) generative models:

(cid:73) learn a distribution p(x, y) from training examples
(cid:73) often we have p(x, y) = p(y)p(x|y)

(cid:73) note: we then have

where p(x) =(cid:80)

p(y|x) =

p(y)p(x|y)

p(x)

y p(y)p(x|y)

decoding with generative models

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. task is

to learn a function f mapping inputs x to labels f (x).

decoding with generative models

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. task is

to learn a function f mapping inputs x to labels f (x).

(cid:73) generative models:

(cid:73) learn a distribution p(x, y) from training examples
(cid:73) often we have p(x, y) = p(y)p(x|y)

decoding with generative models

(cid:73) we have training examples x(i), y(i) for i = 1 . . . m. task is

to learn a function f mapping inputs x to labels f (x).

(cid:73) generative models:

(cid:73) learn a distribution p(x, y) from training examples
(cid:73) often we have p(x, y) = p(y)p(x|y)

(cid:73) output from the model:
p(y|x)
p(y)p(x|y)

f (x) = arg max

y

= arg max

y

= arg max

y

p(x)

p(y)p(x|y)

overview

(cid:73) the tagging problem

(cid:73) generative models, and the noisy-channel model, for

supervised learning

(cid:73) hidden markov model (id48) taggers

(cid:73) basic de   nitions
(cid:73) parameter estimation
(cid:73) the viterbi algorithm

id48

(cid:73) we have an input sentence x = x1, x2, . . . , xn

(xi is the i   th word in the sentence)

(cid:73) we have a tag sequence y = y1, y2, . . . , yn

(yi is the i   th tag in the sentence)

(cid:73) we   ll use an id48 to de   ne

p(x1, x2, . . . , xn, y1, y2, . . . , yn)

for any sentence x1 . . . xn and tag sequence y1 . . . yn of the
same length.

(cid:73) then the most likely tag sequence for x is

arg max
y1...yn

p(x1 . . . xn, y1, y2, . . . , yn)

trigram id48 (trigram id48s)
for any sentence x1 . . . xn where xi     v for i = 1 . . . n, and any
tag sequence y1 . . . yn+1 where yi     s for i = 1 . . . n, and
yn+1 = stop, the joint id203 of the sentence and tag
sequence is

p(x1 . . . xn, y1 . . . yn+1) =

q(yi|yi   2, yi   1)

e(xi|yi)

n+1(cid:89)

n(cid:89)

where we have assumed that x0 = x   1 = *.

i=1

i=1

parameters of the model:

(cid:73) q(s|u, v) for any s     s     {stop}, u, v     s     {*}
(cid:73) e(x|s) for any s     s, x     v

an example

if we have n = 3, x1 . . . x3 equal to the sentence the dog laughs,
and y1 . . . y4 equal to the tag sequence d n v stop, then

p(x1 . . . xn, y1 . . . yn+1)

= q(d|   ,   )    q(n|   , d)    q(v|d, n)    q(stop|n, v)

  e(the|d)    e(dog|n)    e(laughs|v)

(cid:73) stop is a special tag that terminates the sequence
(cid:73) we take y0 = y   1 = *, where * is a special    padding    symbol

why the name?

p(x1 . . . xn, y1 . . . yn) = q(stop|yn   1, yn)

q(yj | yj   2, yj   1)

(cid:125)

n(cid:89)
(cid:123)(cid:122)

j=1

(cid:124)
   n(cid:89)
(cid:124)

j=1

e(xj | yj)

(cid:123)(cid:122)

(cid:125)

xj   s are observed

markov chain

overview

(cid:73) the tagging problem

(cid:73) generative models, and the noisy-channel model, for

supervised learning

(cid:73) hidden markov model (id48) taggers

(cid:73) basic de   nitions
(cid:73) parameter estimation
(cid:73) the viterbi algorithm

smoothed estimation

q(vt | dt, jj) =   1    count(dt, jj, vt)
count(dt, jj)
+  2    count(jj, vt)
count(jj)
+  3    count(vt)
count()

  1 +   2 +   3 = 1,

and for all i,   i     0

e(base | vt) =

count(vt, base)

count(vt)

dealing with low-frequency words: an example

pro   ts soared at boeing co. , easily topping forecasts on wall
street , as their ceo alan mulally announced    rst quarter results .

dealing with low-frequency words

a common method is as follows:

(cid:73) step 1: split vocabulary into two sets

frequent words
low frequency words = all other words

= words occurring     5 times in training

(cid:73) step 2: map low frequency words into a small,    nite set,

depending on pre   xes, su   xes etc.

dealing with low-frequency words: an example

[bikel et. al 1999] (named-entity recognition)
word class
twodigitnum
fourdigitnum
containsdigitandalpha
containsdigitanddash
containsdigitandslash
containsdigitandcomma
containsdigitandperiod
othernum
allcaps
capperiod
   rstword
initcap
lowercase
other

example
90
1990
a8956-67
09-96
11/9/89
23,000.00
1.00
456789
bbn
m.
   rst word of sentence
sally
can
,

intuition
two digit year
four digit year
product code
date
date
monetary amount
monetary amount, percentage
other number
organization
person name initial
no useful capitalization information
capitalized word
uncapitalized word
punctuation marks, all other words

dealing with low-frequency words: an example

pro   ts/na soared/na at/na boeing/sc co./cc ,/na easily/na
topping/na forecasts/na on/na wall/sl street/cl ,/na as/na their/na
ceo/na alan/sp mulally/cp announced/na    rst/na quarter/na
results/na ./na

   

firstword/na soared/na at/na initcap/sc co./cc ,/na easily/na
lowercase/na forecasts/na on/na initcap/sl street/cl ,/na as/na
their/na ceo/na alan/sp initcap/cp announced/na    rst/na
quarter/na results/na ./na

na
sc
cc
sl
cl
. . .

= no entity
= start company
= continue company
= start location
= continue location

overview

(cid:73) the tagging problem

(cid:73) generative models, and the noisy-channel model, for

supervised learning

(cid:73) hidden markov model (id48) taggers

(cid:73) basic de   nitions
(cid:73) parameter estimation
(cid:73) the viterbi algorithm

the viterbi algorithm

problem: for an input x1 . . . xn,    nd

arg max
y1...yn+1

p(x1 . . . xn, y1 . . . yn+1)

where the arg max is taken over all sequences y1 . . . yn+1 such
that yi     s for i = 1 . . . n, and yn+1 = stop.

we assume that p again takes the form

p(x1 . . . xn, y1 . . . yn+1) =

q(yi|yi   2, yi   1)

n+1(cid:89)

i=1

n(cid:89)

i=1

e(xi|yi)

recall that we have assumed in this de   nition that y0 = y   1 = *,
and yn+1 = stop.

brute force search is hopelessly ine   cient

problem: for an input x1 . . . xn,    nd

arg max
y1...yn+1

p(x1 . . . xn, y1 . . . yn+1)

where the arg max is taken over all sequences y1 . . . yn+1 such
that yi     s for i = 1 . . . n, and yn+1 = stop.

the viterbi algorithm

(cid:73) de   ne n to be the length of the sentence
(cid:73) de   ne sk for k =    1 . . . n to be the set of possible tags at

position k:

(cid:73) de   ne

s   1 = s0 = {   }

sk = s for k     {1 . . . n}

k(cid:89)

r(y   1, y0, y1, . . . , yk) =

q(yi|yi   2, yi   1)

(cid:73) de   ne a id145 table

i=1

k(cid:89)

i=1

e(xi|yi)

  (k, u, v) = maximum id203 of a tag sequence

ending in tags u, v at position k

that is,
  (k, u, v) = max(cid:104)y   1,y0,y1,...,yk(cid:105):yk   1=u,yk=v r(y   1, y0, y1 . . . yk)

an example

  (k, u, v) = maximum id203 of a tag sequence

ending in tags u, v at position k

the man saw the dog with the telescope

a recursive de   nition

base case:

  (0, *, *) = 1

recursive de   nition:
for any k     {1 . . . n}, for any u     sk   1 and v     sk:

  (k, u, v) = max
w   sk   2

(  (k     1, w, u)    q(v|w, u)    e(xk|v))

justi   cation for the recursive de   nition
for any k     {1 . . . n}, for any u     sk   1 and v     sk:

  (k, u, v) = max
w   sk   2

(  (k     1, w, u)    q(v|w, u)    e(xk|v))

the man saw the dog with the telescope

the viterbi algorithm

input: a sentence x1 . . . xn, parameters q(s|u, v) and e(x|s).
initialization: set   (0, *, *) = 1
de   nition: s   1 = s0 = {   }, sk = s for k     {1 . . . n}
algorithm:

(cid:73) for k = 1 . . . n,

(cid:73) for u     sk   1, v     sk,

  (k, u, v) = max
w   sk   2

(  (k     1, w, u)    q(v|w, u)    e(xk|v))

(cid:73) return maxu   sn   1,v   sn (  (n, u, v)    q(stop|u, v))

the viterbi algorithm with backpointers

input: a sentence x1 . . . xn, parameters q(s|u, v) and e(x|s).
initialization: set   (0, *, *) = 1
de   nition: s   1 = s0 = {   }, sk = s for k     {1 . . . n}
algorithm:

(cid:73) for k = 1 . . . n,

(cid:73) for u     sk   1, v     sk,
  (k, u, v) = max
w   sk   2

bp(k, u, v) = arg max
w   sk   2

(  (k     1, w, u)    q(v|w, u)    e(xk|v))

(  (k     1, w, u)    q(v|w, u)    e(xk|v))

(cid:73) set (yn   1, yn) = arg max(u,v) (  (n, u, v)    q(stop|u, v))
(cid:73) for k = (n     2) . . . 1, yk = bp(k + 2, yk+1, yk+2)
(cid:73) return the tag sequence y1 . . . yn

the viterbi algorithm: running time

(cid:73) o(n|s|3) time to calculate q(s|u, v)    e(xk|s) for
all k, s, u, v.
(cid:73) n|s|2 entries in    to be    lled in.
(cid:73) o(|s|) time to    ll in one entry
(cid:73)     o(n|s|3) time in total

pros and cons

(cid:73) hidden markov model taggers are very simple to

train (just need to compile counts from the
training corpus)

(cid:73) perform relatively well (over 90% performance on

id39)

(cid:73) main di   culty is modeling

e(word | tag)

can be very di   cult if    words    are complex

