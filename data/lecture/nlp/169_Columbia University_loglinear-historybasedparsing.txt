id148 for history-based parsing

michael collins, columbia university

log-linear taggers: summary

(cid:73) the input sentence is w[1:n] = w1 . . . wn

(cid:73) each tag sequence t[1:n] has a id155

p(t[1:n] | w[1:n]) =(cid:81)n
=(cid:81)n

j=1 p(tj | w1 . . . wn, t1 . . . tj   1)
j=1 p(tj | w1 . . . wn, tj   2, tj   1)

chain rule

independence
assumptions

(cid:73) estimate p(tj | w1 . . . wn, tj   2, tj   1) using id148
(cid:73) use the viterbi algorithm to compute

argmaxt[1:n]

log p(t[1:n] | w[1:n])

a general approach:
(conditional) history-based models

(cid:73) we   ve shown how to de   ne p(t[1:n] | w[1:n]) where t[1:n] is a

tag sequence

(cid:73) how do we de   ne p(t | s) if t is a parse tree (or another

structure)? (we use the notation s = w[1:n])

a general approach:
(conditional) history-based models

(cid:73) step 1: represent a tree as a sequence of decisions d1 . . . dm

t = (cid:104)d1, d2, . . . dm(cid:105)

m(cid:89)

m is not necessarily the length of the sentence

(cid:73) step 2: the id203 of a tree is

p(t | s) =

p(di | d1 . . . di   1, s)

i=1

(cid:73) step 3: use a log-linear model to estimate

p(di | d1 . . . di   1, s)

(cid:73) step 4: search?? (answer we   ll get to later: beam or

heuristic search)

an example tree

s(questioned)

np(lawyer)

dt

nn

the

lawyer

vp(questioned)

vt

np(witness)

pp(about)

questioned

dt

nn

in

np(revolver)

the

witness

about

dt

nn

the

revolver

ratnaparkhi   s parser: three layers of structure

1. part-of-speech tags
2. chunks
3. remaining structure

layer 1: part-of-speech tags

dt

nn

vt

dt

nn

in

dt

nn

the

lawyer

questioned

the

witness

about

the

revolver

(cid:73) step 1: represent a tree as a sequence of decisions d1 . . . dm

t = (cid:104)d1, d2, . . . dm(cid:105)

(cid:73) first n decisions are tagging decisions

(cid:104)d1 . . . dn(cid:105) = (cid:104) dt, nn, vt, dt, nn, in, dt, nn (cid:105)

layer 2: chunks

np

vt

np

in

np

dt

nn

questioned

dt

nn

about

dt

nn

the

lawyer

the

witness

the

revolver

chunks are de   ned as any phrase where all children are
part-of-speech tags

(other common chunks are adjp, qp)

layer 2: chunks

start(np)

join(np)

other

start(np)

join(np)

other

start(np)

join(np)

dt

the

nn

vt

lawyer

questioned

dt

the

nn

in

witness

about

dt

the

nn

revolver

(cid:73) step 1: represent a tree as a sequence of decisions d1 . . . dm

t = (cid:104)d1, d2, . . . dm(cid:105)

(cid:73) first n decisions are tagging decisions

next n decisions are chunk tagging decisions
(cid:104)d1 . . . d2n(cid:105) = (cid:104) dt, nn, vt, dt, nn, in, dt, nn,

start(np), join(np), other, start(np), join(np),
other, start(np), join(np)(cid:105)

layer 3: remaining structure

alternate between two classes of actions:

(cid:73) join(x) or start(x), where x is a label (np, s, vp etc.)
(cid:73) check=yes or check=no

meaning of these actions:

(cid:73) start(x) starts a new constituent with label x

(always acts on leftmost constituent with no start or join
label above it)

(cid:73) join(x) continues a constituent with label x

(always acts on leftmost constituent with no start or join
label above it)

(cid:73) check=no does nothing
(cid:73) check=yes takes previous join or start action, and converts

it into a completed constituent

np

vt

np

in

np

dt

nn

questioned

dt

nn

about

dt

nn

the

lawyer

the

witness

the

revolver

start(s)

vt

np

in

np

np

questioned

dt

nn

about

dt

nn

dt

nn

the

lawyer

the

witness

the

revolver

start(s)

vt

np

in

np

np

questioned

dt

nn

about

dt

nn

the

witness

the

revolver

dt

nn

the

lawyer

check=no

start(s)

start(vp)

np

in

np

np

vt

dt

nn

about

dt

nn

dt

nn

questioned

the

witness

the

revolver

the

lawyer

start(s)

start(vp)

np

in

np

np

vt

dt

nn

about

dt

nn

dt

nn

questioned

the

witness

the

revolver

the

lawyer

check=no

start(s)

start(vp)

join(vp)

in

np

np

vt

np

about

dt

nn

dt

nn

questioned

dt

nn

the

revolver

the

lawyer

the

witness

start(s)

start(vp)

join(vp)

in

np

np

vt

np

about

dt

nn

dt

nn

questioned

dt

nn

the

revolver

the

lawyer

the

witness

check=no

start(s)

start(vp)

join(vp)

start(pp)

np

np

vt

np

in

dt

nn

dt

nn

questioned

dt

nn

about

the

revolver

the

lawyer

the

witness

start(s)

start(vp)

join(vp)

start(pp)

np

np

vt

np

in

dt

nn

dt

nn

questioned

dt

nn

about

the

revolver

the

lawyer

the

witness

check=no

start(s)

start(vp)

join(vp)

start(pp)

join(pp)

np

vt

np

in

np

dt

nn

questioned

dt

nn

about

dt

nn

the

lawyer

the

witness

the

revolver

start(s)

start(vp)

join(vp)

pp

np

vt

np

in

np

dt

nn

questioned

dt

nn

about

dt

nn

the

lawyer

the

witness

the

revolver

check=yes

start(s)

start(vp)

join(vp)

join(vp)

np

vt

np

pp

dt

nn

questioned

dt

nn

in

np

the

lawyer

the

witness

about

dt

nn

the

revolver

start(s)

np

dt

nn

the

lawyer

vp

np

vt

pp

questioned

dt

nn

in

np

the

witness

about

dt

nn

the

revolver

check=yes

start(s)

np

dt

nn

the

lawyer

join(s)

vp

vt

np

pp

questioned

dt

nn

in

np

the

witness

about

dt

nn

the

revolver

s

vt

vp

np

pp

questioned

dt

nn

in

np

the

witness

about

dt

nn

the

revolver

np

dt

nn

the

lawyer

check=yes

the final sequence of decisions

(cid:104)d1 . . . dm(cid:105) = (cid:104) dt, nn, vt, dt, nn, in, dt, nn,

start(np), join(np), other, start(np), join(np),
other, start(np), join(np),
start(s), check=no, start(vp), check=no,
join(vp), check=no, start(pp), check=no,
join(pp), check=yes, join(vp), check=yes,
join(s), check=yes (cid:105)

a general approach:
(conditional) history-based models

(cid:73) step 1: represent a tree as a sequence of decisions d1 . . . dm

t = (cid:104)d1, d2, . . . dm(cid:105)

m is not necessarily the length of the sentence

p(t | s) =(cid:81)m

(cid:73) step 2: the id203 of a tree is

i=1 p(di | d1 . . . di   1, s)

(cid:73) step 3: use a log-linear model to estimate
p(di | d1 . . . di   1, s)

(cid:73) step 4: search?? (answer we   ll get to later: beam or heuristic

search)

applying a log-linear model

(cid:73) step 3: use a log-linear model to estimate
p(di | d1 . . . di   1, s)

(cid:73) a reminder:

p(di | d1 . . . di   1, s) =

(cid:80)

ef ((cid:104)d1...di   1,s(cid:105),di)  v
d   a ef ((cid:104)d1...di   1,s(cid:105),d)  v

where:
(cid:104)d1 . . . di   1, s(cid:105)
di
f
v
a

is the history
is the outcome
maps a history/outcome pair to a feature vector
is a parameter vector
is set of possible actions

applying a log-linear model

(cid:73) step 3: use a log-linear model to estimate

p(di | d1 . . . di   1, s) =

(cid:80)

ef ((cid:104)d1...di   1,s(cid:105),di)  v
d   a ef ((cid:104)d1...di   1,s(cid:105),d)  v

(cid:73) the big question: how do we de   ne f ?
(cid:73) ratnaparkhi   s method de   nes f di   erently depending on

whether next decision is:

(cid:73) a tagging decision

(same features as before for id52!)

(cid:73) a chunking decision
(cid:73) a start/join decision after chunking
(cid:73) a check=no/check=yes decision

layer 3: join or start

(cid:73) looks at head word, constituent (or pos) label, and

start/join annotation of n   th tree relative to the decision,
where n =    2,   1

(cid:73) looks at head word, constituent (or pos) label of n   th tree

relative to the decision, where n = 0, 1, 2

(cid:73) looks at bigram features of the above for (-1,0) and (0,1)

(cid:73) looks at trigram features of the above for (-2,-1,0), (-1,0,1)

and (0, 1, 2)

(cid:73) the above features with all combinations of head words

excluded

(cid:73) various punctuation features

layer 3: check=no or check=yes

(cid:73) a variety of questions concerning the proposed constituent

the search problem

(cid:73) in id52, we could use the viterbi algorithm because
p(tj | w1 . . . wn, j, t1 . . . tj   1) = p(tj | w1 . . . wn, j, tj   2 . . . tj   1)

(cid:73) now: decision di could depend on arbitrary decisions in the

   past        no chance for id145

(cid:73) instead, ratnaparkhi uses a id125 method

