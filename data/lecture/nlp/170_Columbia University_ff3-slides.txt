id27s in feedforward networks;

tagging and id33 using

feedforward networks

michael collins, columbia university

overview

(cid:73) introduction
(cid:73) multi-layer feedforward networks
(cid:73) representing words as vectors (   id27s   )
(cid:73) the id33 problem
(cid:73) id33 using a shift-reduce neural-network

model

multi-layer feedforward networks

(cid:73) an integer d specifying the input dimension. a set y of

output labels with |y| = k.

network.

hidden units in the j   th layer.

(cid:73) an integer j specifying the number of hidden layers in the
(cid:73) an integer mj for j     {1 . . . j} specifying the number of
(cid:73) a matrix w 1     rm1  d and a vector b1     rm1 associated with
(cid:73) for each j     {2 . . . j}, a matrix w j     rmj  mj   1 and a
(cid:73) for each j     {1 . . . j}, a transfer function gj : rmj     rmj
(cid:73) a matrix v     rk  mj and a vector        rk specifying the

the    rst layer.
vector bj     rmj associated with the j   th layer.

associated with the j   th layer.

parameters in the output layer.

multi-layer feedforward networks (continued)

(cid:73) calculate output of    rst layer:

z1     rm1 = w 1xi + b1
h1     rm1 = g1(z1)

(cid:73) calculate outputs of layers 2 . . . j:

for j = 2 . . . j:
zj     rmj = w jhj   1 + bj
hj     rmj = gj(zj)
(cid:73) calculate output value:
l     rk = v hj + bj
q     rk = ls(l)
o     r =     log qyi

overview

(cid:73) introduction
(cid:73) multi-layer feedforward networks
(cid:73) representing words as vectors (   id27s   )
(cid:73) the id33 problem
(cid:73) id33 using a shift-reduce neural-network

model

an example: part-of-speech tagging

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .
    there are many possible tags in the position ??
{nn, nns, vt, vi, in, dt, . . .}
    the task: model the distribution p(tj|t1, . . . , tj   1, w1 . . . wn)
where tj is the j   th tag in the sequence, wj is the j   th word
    the input to the neural network will be (cid:104)t1 . . . tj   1, w1 . . . wn, j(cid:105)

one-hot encodings of words, tags etc.

(cid:73) a dictionary d with size s(d) maps each word w in the

vocabulary to an integer index(w, d) in the range 1 . . . s(d).

index(the, d) = 1
index(dog, d) = 2
index(cat, d) = 3
index(saw, d) = 4

. . .

(cid:73) for any word w, dictionary d, onehot(w, d) maps a word w
to a    one-hot vector    u = onehot(w, d)     rs(d). we have

uj = 1 for j = index(w, d)
uj = 0 otherwise

one-hot encodings of words, tags etc. (continued)

(cid:73) a dictionary d with size s(d) maps each word w in the

vocabulary to an integer in the range 1 . . . s(d).

index(the, d) = 1
index(dog, d) = 2
index(cat, d) = 3

. . .

onehot(the, d) = [1, 0, 0, . . .]
onehot(dog, d) = [0, 1, 0, . . .]
onehot(cat, d) = [0, 0, 1, . . .]

. . .

the concatenation operation

(cid:73) given column vectors vi     rdi for i = 1 . . . n,
z     rd = concat(v1, v2, . . . vn)

where d =(cid:80)n
(cid:73) z is a column vector of dimension(cid:80)

i=1 di

i di

(cid:73) z is a vector formed by concatenating the vectors v1 . . . vn

the concatenation operation (continued)

(cid:73) given vectors vi     rdi for i = 1 . . . n,

where d =(cid:80)n

(cid:73) the jacobians:

z     rd = concat(v1, v2, . . . vn)
i=1 di

   z

   vi     rd  di
(cid:20)    z
(cid:21)
(cid:21)
(cid:20)    z

   vi

j,k

= 1

= 0

have entries

if j = k +(cid:80)

i(cid:48)<i di(cid:48),

otherwise

   vi

j,k

a single-layer computational network for tagging

inputs: a training example xi = (cid:104)t1 . . . tj   1, w1 . . . wn, j(cid:105), yi     y. a
word dictionary d with size s(d), a tag dictionary t with size s(t ).
parameters of a single-layer feedforward network.
computational graph:

   2     rs(t ) = onehot(tj   2, t )
t(cid:48)
   1     rs(t ) = onehot(tj   1, t )
t(cid:48)
   1     rs(d) = onehot(wj   1, d)
w(cid:48)
0     rs(d) = onehot(wj, d)
w(cid:48)
+1     rs(d) = onehot(wj+1, d)
w(cid:48)
   1, w(cid:48)
z = w u + b, h = g(z),
o = qyi

   2, t(cid:48)

u     r2s(t )+3s(d) = concat(t(cid:48)

   1, w(cid:48)

0, w(cid:48)

+1)

l = v h +   , q = ls(l)

the number of parameters

   2     rs(t ) = onehot(tj   2, t )
t(cid:48)

. . .

+1     rs(d) = onehot(wj+1, d)
w(cid:48)
   1, w(cid:48)

u = concat(t(cid:48)
z     rm = w u + b

   2, t(cid:48)

   1, w(cid:48)

0, w(cid:48)

+1)

. . .

(cid:73) an example: s(t ) = 50 (50 tags), s(d) = 10, 000 (10,000

words), m = 1000 (1000 neurons in the single layer)

(cid:73) then

w     rm  (2s(t )+3s(d))

and m = 1000, 2s(t ) + 3s(d) = 30, 100, so there are
m    (2s(t ) + 3s(d)) = 30, 100, 000 parameters in the matrix w

an example

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .

t

   2     rs(t )
(cid:48)
   1     rs(t )
(cid:48)
t
   1     rs(d)
(cid:48)
0     rs(d)
(cid:48)
w
+1     rs(d)
(cid:48)

w

w

= onehot(tj   2, t )

= onehot(tj   1, t )

= onehot(wj   1, d)

= onehot(wj , d)

= onehot(wj+1, d)

u = concat(t

. . .

(cid:48)
   2, t

(cid:48)
   1, w

(cid:48)
   1, w

(cid:48)
0, w

(cid:48)
+1)

embedding matrices

(cid:73) given a word w, a word dictionary d we can map w to a

one-hot representation

w(cid:48)     rs(d)  1 = onehot(w, d)

(cid:73) now assume we have an embedding dictionary e     re  s(d)
where e is some integer. typical values of e are e = 100 or
e = 200

(cid:73) we can now map the one-hot representation w(cid:48) to
= e    onehot(w, d)

w(cid:48)(cid:48)(cid:124)(cid:123)(cid:122)(cid:125)

e  1

= e(cid:124)(cid:123)(cid:122)(cid:125)

e  s(d)

w(cid:48)(cid:124)(cid:123)(cid:122)(cid:125)

s(d)  1

(cid:73) equivalently, a word w is mapped to a vector e(: j)     re

where j = index(w, d) is the integer that word w is mapped
to, and e(: j) is the j   th column in the matrix.

embedding matrices vs. one-hot vectors

(cid:73) one-hot representation:

w(cid:48)     rs(d)  1 = onehot(w, d)

this representation is high-dimensional, sparse

(cid:73) embedding representation:

w(cid:48)(cid:48)(cid:124)(cid:123)(cid:122)(cid:125)

e  1

= e(cid:124)(cid:123)(cid:122)(cid:125)

e  s(d)

w(cid:48)(cid:124)(cid:123)(cid:122)(cid:125)

s(d)  1

= e    onehot(w, d)

this representation is low-dimensional, dense

(cid:73) the embedding matrices can be learned using stochastic

id119 and id26 (each entry of e is a
new parameter in the model)

(cid:73) critically, embeddings allow shared information between

words: e.g., words with similar meaning or syntax get
mapped to    similar    embeddings

a single-layer computational network for tagging

inputs: a training example xi = (cid:104)t1 . . . tj   1, w1 . . . wn, j(cid:105), yi     y. a
word dictionary d with size s(d), a tag dictionary t with size s(t ). a
id27 matrix e     re  s(d). a tag embedding matrix
a     ra  s(d). parameters of a single-layer feedforward network.
computational graph:

   2     ra = a    onehot(tj   2, t )
t(cid:48)
   1     ra = a    onehot(tj   1, t )
t(cid:48)
   1     re = e    onehot(wj   1, d)
w(cid:48)
0     re = e    onehot(wj, d)
w(cid:48)
+1     re = e    onehot(wj+1, d)
w(cid:48)
u     r2a+3e = concat(t(cid:48)
   1, w(cid:48)

   2, t(cid:48)

   1, w(cid:48)
z = w u + b, h = g(z),
o = qyi

0, w(cid:48)
l = v h +   , q = ls(l)

+1)

an example

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .

w

   2     ra
(cid:48)
t
   1     ra
(cid:48)
t
   1     re
(cid:48)
0     re
(cid:48)
w
+1     re
(cid:48)
u     r2a+3e

w

= a    onehot(tj   2, t )
= a    onehot(tj   1, t )
= e    onehot(wj   1, d)
= e    onehot(wj , d)
= e    onehot(wj+1, d)

= concat(t

(cid:48)
   2, t

(cid:48)
   1, w

(cid:48)
   1, w

(cid:48)
0, w

(cid:48)
+1)

calculating jacobians

0     re = e    onehot(w, d)
w(cid:48)

equivalently:

(w(cid:48)

0)j =

ej,k    onehotk(w, d)

(cid:88)

k

(cid:73) need to calculate the jacobian
   w(cid:48)
e

0

this has entries

(cid:20)    w(cid:48)

0

(cid:21)

e

j,(j(cid:48),k)

= 1 if j = j(cid:48) and onehotk(w, e) = 1, 0 otherwise

an additional perspective

   2     ra = onehot(tj   2, t )
t(cid:48)

   2     ra = a    onehot(tj   2, t )
t(cid:48)

. . .

+1     re = onehot(wj+1, d)
w(cid:48)
   2 . . . w(cid:48)

u = concat(t(cid:48)
z     rm = w u + b

+1)

. . .

+1     re = e    onehot(wj+1, d)
w(cid:48)
+1)

  u = concat(t(cid:48)
  z     rm =   w   u + b

   2 . . . w(cid:48)

(cid:73) if we set

w(cid:124)(cid:123)(cid:122)(cid:125)

m  (2s(t )+3s(e))

  w(cid:124)(cid:123)(cid:122)(cid:125)

m  (2a+3e)

=

(cid:124)

   diag(a, a, e, e, e)
(2a+3e)  (2s(t )+3s(d))

(cid:123)(cid:122)

(cid:125)

then w u + b =   w   u + b hence z =   z

an additional perspective (continued)

(cid:73) if we set

w(cid:124)(cid:123)(cid:122)(cid:125)

m  (2s(t )+3s(e))

  w(cid:124)(cid:123)(cid:122)(cid:125)

m  (2a+3e)

=

(cid:124)

   diag(a, a, e, e, e)
(2a+3e)  (2s(t )+3s(d))

(cid:123)(cid:122)

(cid:125)

then w u + b =   w   u + b hence z =   z

(cid:73) an example: s(t ) = 50 (50 tags), s(d) = 10, 000 (10,000
words), a = e = 100 (recall a, e are size of embeddings for
tags and words respectively), m = 1000 (1000 neurons)

(cid:73) then we have parameters

w(cid:124)(cid:123)(cid:122)(cid:125)

1000  30,100

vs.

  w(cid:124)(cid:123)(cid:122)(cid:125)

1000  500

a(cid:124)(cid:123)(cid:122)(cid:125)

100  50

e(cid:124)(cid:123)(cid:122)(cid:125)

100  10,000

overview

(cid:73) introduction
(cid:73) multi-layer feedforward networks
(cid:73) representing words as vectors (   id27s   )
(cid:73) the id33 problem
(cid:73) id33 using a shift-reduce neural-network

model

unlabeled dependency parses

(cid:73) root is a special root symbol

(cid:73) each dependency is a pair (h, m) where h is the index of a head

word, m is the index of a modi   er word. in the    gures, we
represent a dependency (h, m) by a directed edge from h to m.
(cid:73) dependencies in the above example are (0, 2), (2, 1), (2, 4), and

(4, 3). (we take 0 to be the root symbol.)

rootjohnsawamoviethe (unlabeled) id33 problem

john saw a movie

   

rootjohnsawamovieconditions on dependency structures

(cid:73) the dependency arcs form a directed tree, with the root

symbol at the root of the tree.
(de   nition: a directed tree rooted at root is a tree, where for
every word w other than the root, there is a directed path
from root to w.)

(cid:73) there are no    crossing dependencies   .

dependency structures with no crossing dependencies are
sometimes referred to as projective structures.

sawamoviejohnroothelikedtodaythatall dependency parses for john saw mary

rootjohnsawmaryrootjohnsawmaryrootjohnsawmaryrootjohnsawmaryrootjohnsawmarythe labeled id33 problem

i live in new york city .

   

overview

(cid:73) introduction
(cid:73) multi-layer feedforward networks
(cid:73) representing words as vectors (   id27s   )
(cid:73) the id33 problem
(cid:73) id33 using a shift-reduce neural-network

model

shift-reduce id33: con   gurations

(cid:73) a con   guration consists of:

1. a stack    consisting of a sequence of words, e.g.,

   = [root0, i1, live2]

2. a bu   er    consisting of a sequence of words, e.g.,

   = [in3, new4, york5, city6, .7]

3. a set    of labeled dependencies, e.g.,

   = {{1    nsubj 2},{6    nn 5}

the initial con   guration

   = [root0],    = [i1, live2, in3, new4, york5, city6, .7],    = {}

shift-reduce actions: the shift action

the shift action takes the    rst word in the bu   er, and adds it to
the end of the stack.

   = [root0],    = [i1, live2, in3, new4, york5, city6, .7],    = {}

shift

   

   = [root0, i1],    = [live2, in3, new4, york5, city6, .7],    = {}

shift-reduce actions: the shift action

the shift action takes the    rst word in the bu   er, and adds it to
the end of the stack.

   = [root0, i1],    = [live2, in3, new4, york5, city6, .7],    = {}

shift

   

   = [root0, i1, live2],    = [in3, new4, york5, city6, .7],    = {}

shift-reduce actions: the left-arc action

the left-arcnsubj action takes the top two words on the stack,
adds a dependency between them in the left direction with label
nsubj, and removes the modi   er word from the stack. there is a
left-arcl action for each possible dependency label l.

   = [root0, i1, live2],    = [in3, new4, york5, city6, .7],    = {}

left-arcnsubj

   

   = [root0, live2],    = [in3, new4, york5, city6, .7],    = {{2    nsubj 1}}

shift-reduce actions: the right-arc action

the right-arcprep action takes the top two words on the stack,
adds a dependency between them in the right direction with label
prep, and removes the modi   er word from the stack. there is a
right-arcl action for each possible dependency label l.

   = [root0, live2, in3],    = [.7],    = {{2    nsubj 1},}

right-arcprep

   

   = [root0, live2],    = [.7],    = {{2    nsubj 1},{2    prep 3}}

each dependency parse is mapped to a sequence of
actions
action
shift
shift
left-arcnsubj
shift
shift
shift
shift
left-arid98
left-arid98
right-arcpobj
right-arcprep
shift
right-arcpunct
right-arcroot
terminal

  
[root0]
[root0, i1]
[root0, i1, live2]
[root0, live2]
[root0, live2, in3]
[root0, live2, in3, new4]
[root0, live2, in3, new4, york5]
[root0, live2, in3, new4, york5, city6]
[root0, live2, in3, new4, city6]
[root0, live2, in3, city6]
[root0, live2, in3]
[root0, live2]
[root0, live2, .7]
[root0, live2]
[root0]

  
[i1,live2, in3, new4, york5, city6, .7]
[live2, in3, new4, york5, city6, .7]
[in3, new4, york5, city6, .7]
[in3, new4, york5, city6, .7]
[new4, york5, city6, .7]
[york5, city6, .7]
[city6, .7]
[.7]
[.7]
[.7]
[.7]
[.7]
[]
[]
[]

h l       d

nsubj             1

2

6 nn       5
6 nn       4
pobj          6
3
prep             3
2

punct             7
2
0 root          2

each dependency parse is mapped to a sequence of
actions

(cid:73) input w1 . . . wn = i live in new york city .
(cid:73) dependency parse requires actions a1 . . . am, e.g.,

a1 . . . am = (cid:104)shift, shift, left-arcnsubj, shift, shift, shift, shift,

left-arid98, left-arid98, right-arcpobj, right-arcprep,
shift, right-arcpunc, right-arcroot(cid:105)

(cid:73) we use a feedforward neural network to model

p(a1 . . . am|w1 . . . wn) =

p(ai|a1 . . . ai   1, w1 . . . wn)

m(cid:89)

i=1

feature extractors

m(cid:89)

(cid:73) we use a feedforward neural network to model

p(a1 . . . am|w1 . . . wn) =

p(ai|a1 . . . ai   1, w1 . . . wn)

i=1

(cid:73) note that the action sequence a1 . . . ai   1 maps to a

con   guration ci = (cid:104)  i,   i,   i(cid:105)

(cid:73) a feature extractor maps a (ci, w1 . . . wn) pair to either a

word, part-of-speech tag, or dependency label

(cid:73) weiss et al. 2015 (see also chen and manning 2014) have 20

word-based feature extractors, 20 tag-based feature
extractors, 12 dependency label feature extractors

(cid:73) this gives 20 + 20 + 12 = 52 one-hot vectors as input to a

neural network that estimates p(a|c, w1 . . . wn)

word-based feature extractors

(cid:73) a feature extractor maps a (ci, w1 . . . wn) pair to either a

word, part-of-speech tag, or dependency label

(cid:73) si for i = 1 . . . 4 is the index of the i   th element on the stack.

bi for i = 1 . . . 4 is the index of the i   th element on the
bu   er. lc1(si) is the    rst left-child of word si, lc2(si) is the
second left-child. rc1(si) and rc2(si) are the    rst and second
right-children of si.

(cid:73) we then have features:

word(s1) word(s2) word(s3) word(s4) word(b1) word(b2) word(b3)
word(b4) word(lc1(s1)) word(lc1(s2)) word(lc2(s1)) word(lc2(s2))
word(rc1(s1)) word(rc1(s2)) word(rc2(s1)) word(rc2(s2))
word(lc1(lc1(s1)) word(lc1(lc1(s2)) word(rc1(rc1(s1)) word(rc1(rc1(s2))

some results

method

unlabeled dep. accuracy

global linear model1

neural network, greedy2
neural network, beam3

neural network, beam, global training4

92.9%
93.0%
93.6%
94.6%

1. hand-constructed features very similar to features in log-linear
models. uses id125 in conjunction with a global linear model.
transition-based id33 with rich non-local features,
zhang and nivre 2011.
2, 3: feedforward neural network with greedy search, or id125.
globally normalized transition-based neural networks. andor et al.,
acl 2016. see also a fast and accurate dependency parser using
neural network chen and manning, acl 2014.
4: neural network with global training, related to training of global
linear models (but with id27s, and non-linearities from a
neural network). see andor et al. 2016.

