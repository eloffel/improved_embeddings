the id6

michael collins, columbia university

recap: the id87

(cid:73) goal: translation system from french to english
(cid:73) have a model p(e| f ) which estimates id155
of any english sentence e given the french sentence f . use
the training corpus to set the parameters.

(cid:73) a id87 has two components:

p(e)
p(f | e)

the language model

the translation model

(cid:73) giving:

and

(cid:80)

p(e, f )
p(f )

p(e | f ) =

p(e)p(f | e)
e p(e)p(f | e)
argmaxep(e | f ) = argmaxep(e)p(f | e)

=

roadmap for the next few lectures

(cid:73) ibm models 1 and 2

(cid:73) phrase-based models

overview

(cid:73) ibm model 1

(cid:73) ibm model 2

(cid:73) em training of models 1 and 2

ibm model 1: alignments

(cid:73) how do we model p(f | e)?

(cid:73) english sentence e has l words e1 . . . el,

french sentence f has m words f1 . . . fm.

(cid:73) an alignment a identi   es which english word each french

word originated from

(cid:73) formally, an alignment a is {a1, . . . am}, where each

aj     {0 . . . l}.

(cid:73) there are (l + 1)m possible alignments.

ibm model 1: alignments

(cid:73) e.g., l = 6, m = 7

e = and the program has been implemented

f = le programme a ete mis en application

(cid:73) one alignment is

{2, 3, 4, 5, 6, 6, 6}

(cid:73) another (bad!) alignment is

{1, 1, 1, 1, 1, 1, 1}

alignments in the ibm models

(cid:73) we   ll de   ne models for p(a | e, m) and p(f | a, e, m),

p(f, a | e, m) = p(a | e, m)p(f | a, e, m)

(cid:88)

a   a

p(f | e, m) =

p(a | e, m)p(f | a, e, m)

where a is the set of all possible alignments

giving

(cid:73) also,

a by-product: most likely alignments

(cid:73) once we have a model p(f, a | e, m) = p(a | e)p(f | a, e, m)

we can also calculate

p(a | f, e, m) =

(cid:80)
p(f, a | e, m)
a   a p(f, a | e, m)

for any alignment a

(cid:73) for a given f, e pair, we can also compute the most likely

alignment,

a    = arg max

a

p(a | f, e, m)

(cid:73) nowadays, the original ibm models are rarely (if ever) used
for translation, but they are used for recovering alignments

an example alignment

french:
le conseil a rendu son avis , et nous devons `a pr  esent adopter un
nouvel avis sur la base de la premi`ere position .

english:
the council has stated its position , and now , on the basis of the
   rst position , we again have to give our opinion .

alignment:
the/le council/conseil has/`a stated/rendu its/son position/avis ,/,
and/et now/pr  esent ,/null on/sur the/le basis/base of/de the/la
   rst/premi`ere position/position ,/null we/nous again/null
have/devons to/a give/adopter our/nouvel opinion/avis ./.

ibm model 1: alignments

(cid:73) in ibm model 1 all allignments a are equally likely:

p(a | e, m) =

1

(l + 1)m

(cid:73) this is a major simplifying assumption, but it gets things

started...

ibm model 1: translation probabilities

(cid:73) next step: come up with an estimate for

p(f | a, e, m)

(cid:73) in model 1, this is:

p(f | a, e, m) =

m(cid:89)

j=1

t(fj | eaj )

(cid:73) e.g., l = 6, m = 7

e = and the program has been implemented

f = le programme a ete mis en application

(cid:73) a = {2, 3, 4, 5, 6, 6, 6}

p(f | a, e) = t(le | the)   

t(programme | program)   
t(a | has)   
t(ete | been)   
t(mis | implemented)   
t(en | implemented)   
t(application | implemented)

ibm model 1: the generative process

to generate a french string f from an english string e:

(cid:73) step 1: pick an alignment a with id203

1

(l+1)m

(cid:73) step 2: pick the french words with id203

m(cid:89)

p(f | a, e, m) =

t(fj | eaj )

j=1

the    nal result:

p(f, a | e, m) = p(a | e, m)  p(f | a, e, m) =

m(cid:89)

j=1

1

(l + 1)m

t(fj | eaj )

an example lexical entry

id203
english
french
position position
0.756715
position situation 0.0547918
position mesure
0.0281663
0.0169303
position vue
0.0124795
position point
position attitude
0.0108907

. . . de la situation au niveau des n  egociations de l     ompi . . .
. . . of the current position in the wipo negotiations . . .

nous ne sommes pas en mesure de d  ecider , . . .
we are not in a position to decide , . . .

. . . le point de vue de la commission face `a ce probl`eme complexe .
. . . the commission    s position on this complex problem .

overview

(cid:73) ibm model 1

(cid:73) ibm model 2

(cid:73) em training of models 1 and 2

ibm model 2

(cid:73) only di   erence: we now introduce alignment or distortion

parameters

q(i | j, l, m) = id203 that j   th french word is connected
to i   th english word, given sentence lengths of
e and f are l and m respectively

(cid:73) de   ne

p(a | e, m) =

where a = {a1, . . . am}

(cid:73) gives

p(f, a | e, m) =

m(cid:89)

j=1

m(cid:89)

q(aj | j, l, m)

j=1

q(aj | j, l, m)t(fj | eaj )

an example

l = 6

m = 7

e = and the program has been implemented

f = le programme a ete mis en application
a = {2, 3, 4, 5, 6, 6, 6}

p(a | e, 7) = q(2 | 1, 6, 7)   
q(3 | 2, 6, 7)   
q(4 | 3, 6, 7)   
q(5 | 4, 6, 7)   
q(6 | 5, 6, 7)   
q(6 | 6, 6, 7)   
q(6 | 7, 6, 7)

an example

l = 6

m = 7

e = and the program has been implemented

f = le programme a ete mis en application
a = {2, 3, 4, 5, 6, 6, 6}

p(f | a, e, 7) = t(le | the)   

t(programme | program)   
t(a | has)   
t(ete | been)   
t(mis | implemented)   
t(en | implemented)   
t(application | implemented)

ibm model 2: the generative process

to generate a french string f from an english string e:

(cid:73) step 1: pick an alignment a = {a1, a2 . . . am} with

id203

m(cid:89)

j=1

q(aj | j, l, m)

m(cid:89)

p(f | a, e, m) =

(cid:73) step 3: pick the french words with id203
t(fj | eaj )
m(cid:89)

the    nal result:
p(f, a | e, m) = p(a | e, m)p(f | a, e, m) =

j=1

q(aj | j, l, m)t(fj | eaj )

j=1

recovering alignments

(cid:73) if we have parameters q and t, we can easily recover the most

likely alignment for any sentence pair

(cid:73) given a sentence pair e1, e2, . . . , el, f1, f2, . . . , fm, de   ne

aj = arg max

a   {0...l} q(a|j, l, m)    t(fj|ea)

for j = 1 . . . m

e = and the program has been implemented

f = le programme a ete mis en application

overview

(cid:73) ibm model 1

(cid:73) ibm model 2

(cid:73) em training of models 1 and 2

the parameter estimation problem

(cid:73) input to the parameter estimation algorithm: (e(k), f (k)) for
k = 1 . . . n. each e(k) is an english sentence, each f (k) is a
french sentence

(cid:73) output: parameters t(f|e) and q(i|j, l, m)

(cid:73) a key challenge: we do not have alignments on our

training examples, e.g.,

e(100) = and the program has been implemented
f (100) = le programme a ete mis en application

parameter estimation if the alignments are observed

(cid:73) first: case where alignments are observed in training data.

e.g.,

e(100) = and the program has been implemented

f (100) = le programme a ete mis en application
a(100) = (cid:104)2, 3, 4, 5, 6, 6, 6(cid:105)

(cid:73) training data is (e(k), f (k), a(k)) for k = 1 . . . n. each e(k) is

an english sentence, each f (k) is a french sentence, each a(k)
is an alignment

(cid:73) maximum-likelihood parameter estimates in this case are

trivial:
tm l(f|e) =

count(e, f )

count(e)

qm l(j|i, l, m) =

count(j|i, l, m)
count(i, l, m)

input: a training corpus (f (k), e(k), a(k)) for k = 1 . . . n, where
f (k) = f (k)

, a(k) = a(k)

mk , e(k) = e(k)

. . . a(k)
mk .

. . . f (k)

. . . e(k)
lk

1

1

1

algorithm:

(cid:73) set all counts c(. . .) = 0
(cid:73) for k = 1 . . . n

(cid:73) for i = 1 . . . mk, for j = 0 . . . lk,

, f (k)

i

) +   (k, i, j)

c(e(k)

j

i

, f (k)
c(e(k)

)     c(e(k)
j )     c(e(k)

j

j ) +   (k, i, j)

c(j|i, l, m)     c(j|i, l, m) +   (k, i, j)
c(i, l, m)     c(i, l, m) +   (k, i, j)

where   (k, i, j) = 1 if a(k)

i = j, 0 otherwise.

output: tm l(f|e) = c(e,f )

c(e) , qm l(j|i, l, m) = c(j|i,l,m)

c(i,l,m)

parameter estimation with the em algorithm

(cid:73) training examples are (e(k), f (k)) for k = 1 . . . n. each e(k) is

an english sentence, each f (k) is a french sentence

(cid:73) the algorithm is related to algorithm when alignments are

observed, but two key di   erences:

1. the algorithm is iterative. we start with some initial (e.g.,

random) choice for the q and t parameters. at each iteration
we compute some    counts    based on the data together with
our current parameter estimates. we then re-estimate our
parameters with these counts, and iterate.

2. we use the following de   nition for   (k, i, j) at each iteration:

  (k, i, j) =

(cid:80)lk
q(j|i, lk, mk)t(f (k)
|e(k)
j )
|e(k)
j=0 q(j|i, lk, mk)t(f (k)
j )

i

i

1

. . . f (k)

mk , e(k) = e(k)

input: a training corpus (f (k), e(k)) for k = 1 . . . n, where
f (k) = f (k)
initialization: initialize t(f|e) and q(j|i, l, m) parameters (e.g.,
to random values).

. . . e(k)
lk

.

1

for s = 1 . . . s

(cid:73) set all counts c(. . .) = 0
(cid:73) for k = 1 . . . n

(cid:73) for i = 1 . . . mk, for j = 0 . . . lk

, f (k)

i

) +   (k, i, j)

c(e(k)

j

i

, f (k)
c(e(k)

)     c(e(k)
j )     c(e(k)

j

j ) +   (k, i, j)

c(j|i, l, m)     c(j|i, l, m) +   (k, i, j)
c(i, l, m)     c(i, l, m) +   (k, i, j)

where

  (k, i, j) =

(cid:80)lk
|e(k)
q(j|i, lk, mk)t(f (k)
j )
j=0 q(j|i, lk, mk)t(f (k)
|e(k)
j )

i

i

(cid:73) recalculate the parameters:

t(f|e) =

c(e, f )

c(e)

q(j|i, l, m) =

c(j|i, l, m)
c(i, l, m)

the em algorithm for ibm model 1

for s = 1 . . . s

(cid:73) set all counts c(. . .) = 0
(cid:73) for k = 1 . . . n

(cid:73) for i = 1 . . . mk, for j = 0 . . . lk

, f (k)

i

) +   (k, i, j)

c(e(k)

j

i

, f (k)
c(e(k)

)     c(e(k)
j )     c(e(k)

j

j ) +   (k, i, j)

c(j|i, l, m)     c(j|i, l, m) +   (k, i, j)
c(i, l, m)     c(i, l, m) +   (k, i, j)

where

(cid:80)lk
(cid:73) recalculate the parameters: t(f|e) = c(e, f )/c(e)

|e(k)
j )
|e(k)
(1+lk) t(f (k)
j )

(1+lk) t(f (k)

(cid:80)lk

  (k, i, j) =

j=0

=

1

1

i

i

i

|e(k)
t(f (k)
j )
|e(k)
j=0 t(f (k)
j )

i

  (k, i, j) =

(cid:80)lk
q(j|i, lk, mk)t(f (k)
|e(k)
j )
|e(k)
j=0 q(j|i, lk, mk)t(f (k)
j )

i

i

e(100) = and the program has been implemented

f (100) = le programme a ete mis en application

justi   cation for the algorithm

(cid:73) training examples are (e(k), f (k)) for k = 1 . . . n. each e(k) is

an english sentence, each f (k) is a french sentence

(cid:73) the log-likelihood function:

l(t, q) =

log p(f (k)|e(k)) =

n(cid:88)

k=1

n(cid:88)

k=1

log

(cid:88)

a

p(f (k), a|e(k))

(cid:73) the maximum-likelihood estimates are

arg max

t,q

l(t, q)

(cid:73) the em algorithm will converge to a local maximum of the

log-likelihood function

summary

(cid:73) key ideas in the id6:

(cid:73) alignment variables
(cid:73) translation parameters, e.g., t(chien|dog)
(cid:73) distortion parameters, e.g., q(2|1, 6, 7)

(cid:73) the em algorithm: an iterative algorithm for training the q

and t parameters

(cid:73) once the parameters are trained, we can recover the most

likely alignments on our training examples

e = and the program has been implemented

f = le programme a ete mis en application

