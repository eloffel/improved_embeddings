id38

michael collins, columbia university

overview

(cid:73) the id38 problem
(cid:73) trigram models
(cid:73) evaluating language models: perplexity
(cid:73) estimation techniques:
(cid:73) linear interpolation
(cid:73) discounting methods

the id38 problem

(cid:73) we have some (   nite) vocabulary,

say v = {the, a, man, telescope, beckham, two, . . .}

(cid:73) we have an (in   nite) set of strings, v   

the stop
a stop
the fan stop
the fan saw beckham stop
the fan saw saw stop
the fan saw beckham play for real madrid stop

the id38 problem (continued)

(cid:73) we have a training sample of example sentences in

english

the id38 problem (continued)

(cid:73) we have a training sample of example sentences in

english

(cid:73) we need to    learn    a id203 distribution p

i.e., p is a function that satis   es

p(x) = 1,

p(x)     0 for all x     v   

(cid:88)

x   v   

the id38 problem (continued)

(cid:73) we have a training sample of example sentences in

english

(cid:73) we need to    learn    a id203 distribution p

i.e., p is a function that satis   es

(cid:88)

p(x) = 1,

p(x)     0 for all x     v   

x   v   

p(the stop) = 10   12
p(the fan stop) = 10   8
p(the fan saw beckham stop) = 2    10   8
p(the fan saw saw stop) = 10   15
. . .
p(the fan saw beckham play for real madrid stop) = 2    10   9
. . .

why on earth would we want to do this?!

(cid:73) id103 was the original motivation.

(related problems are id42,
handwriting recognition.)

why on earth would we want to do this?!

(cid:73) id103 was the original motivation.

(related problems are id42,
handwriting recognition.)

(cid:73) the estimation techniques developed for this problem will

be very useful for other problems in nlp

a naive method

(cid:73) we have n training sentences

(cid:73) for any sentence x1 . . . xn, c(x1 . . . xn) is the number of

times the sentence is seen in our training data

(cid:73) a naive estimate:

p(x1 . . . xn) =

c(x1 . . . xn)

n

overview

(cid:73) the id38 problem
(cid:73) trigram models
(cid:73) evaluating language models: perplexity
(cid:73) estimation techniques:
(cid:73) linear interpolation
(cid:73) discounting methods

markov processes

(cid:73) consider a sequence of random variables x1, x2, . . . xn.
each random variable can take any value in a    nite set v.
for now we assume the length n is    xed (e.g., n = 100).

(cid:73) our goal: model

p (x1 = x1, x2 = x2, . . . , xn = xn)

first-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)

first-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)

= p (x1 = x1)

p (xi = xi|x1 = x1, . . . , xi   1 = xi   1)

n(cid:89)

i=2

first-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)

= p (x1 = x1)

= p (x1 = x1)

p (xi = xi|x1 = x1, . . . , xi   1 = xi   1)

p (xi = xi|xi   1 = xi   1)

n(cid:89)
n(cid:89)

i=2

i=2

first-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)

= p (x1 = x1)

= p (x1 = x1)

p (xi = xi|x1 = x1, . . . , xi   1 = xi   1)

p (xi = xi|xi   1 = xi   1)

n(cid:89)
n(cid:89)

i=2

i=2

the    rst-order markov assumption: for any i     {2 . . . n}, for
any x1 . . . xi,
p (xi = xi|x1 = x1 . . . xi   1 = xi   1) = p (xi = xi|xi   1 = xi   1)

second-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)

second-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)
= p (x1 = x1)    p (x2 = x2|x1 = x1)

p (xi = xi|xi   2 = xi   2, xi   1 = xi   1)

   n(cid:89)

i=3

second-order markov processes

p (x1 = x1, x2 = x2, . . . xn = xn)
= p (x1 = x1)    p (x2 = x2|x1 = x1)

p (xi = xi|xi   2 = xi   2, xi   1 = xi   1)

   n(cid:89)
n(cid:89)

i=3

=

p (xi = xi|xi   2 = xi   2, xi   1 = xi   1)

i=1

(for convenience we assume x0 = x   1 = *, where * is a
special    start    symbol.)

modeling variable length sequences

(cid:73) we would like the length of the sequence, n, to also be a

random variable

(cid:73) a simple solution: always de   ne xn = stop where

stop is a special symbol

modeling variable length sequences

(cid:73) we would like the length of the sequence, n, to also be a

random variable

(cid:73) a simple solution: always de   ne xn = stop where

stop is a special symbol

(cid:73) then use a markov process as before:

n(cid:89)

p (x1 = x1, x2 = x2, . . . xn = xn)

=

p (xi = xi|xi   2 = xi   2, xi   1 = xi   1)

i=1

(for convenience we assume x0 = x   1 = *, where * is a
special    start    symbol.)

trigram language models

(cid:73) a trigram language model consists of:

1. a    nite set v
2. a parameter q(w|u, v) for each trigram u, v, w such that

w     v     {stop}, and u, v     v     {*}.

trigram language models

(cid:73) a trigram language model consists of:

1. a    nite set v
2. a parameter q(w|u, v) for each trigram u, v, w such that

w     v     {stop}, and u, v     v     {*}.

(cid:73) for any sentence x1 . . . xn where xi     v for

i = 1 . . . (n     1), and xn = stop, the id203 of the
sentence under the trigram language model is

n(cid:89)

p(x1 . . . xn) =

q(xi|xi   2, xi   1)

where we de   ne x0 = x   1 = *.

i=1

an example

for the sentence

we would have

the dog barks stop

p(the dog barks stop) = q(the|*, *)

  q(dog|*, the)
  q(barks|the, dog)
  q(stop|dog, barks)

the trigram estimation problem

remaining estimation problem:

for example:

q(wi | wi   2, wi   1)

q(laughs | the, dog)

the trigram estimation problem

remaining estimation problem:

for example:

q(wi | wi   2, wi   1)

q(laughs | the, dog)

a natural estimate (the    maximum likelihood estimate   ):

q(wi | wi   2, wi   1) =

count(wi   2, wi   1, wi)

count(wi   2, wi   1)

q(laughs | the, dog) =

count(the, dog, laughs)

count(the, dog)

sparse data problems

a natural estimate (the    maximum likelihood estimate   ):

q(wi | wi   2, wi   1) =

count(wi   2, wi   1, wi)

count(wi   2, wi   1)

q(laughs | the, dog) =

count(the, dog, laughs)

count(the, dog)

say our vocabulary size is n = |v|, then there are n 3
parameters in the model.
e.g., n = 20, 000     20, 0003 = 8    1012 parameters

overview

(cid:73) the id38 problem
(cid:73) trigram models
(cid:73) evaluating language models: perplexity
(cid:73) estimation techniques:
(cid:73) linear interpolation
(cid:73) discounting methods

evaluating a language model: perplexity

(cid:73) we have some test data, m sentences

s1, s2, s3, . . . , sm

evaluating a language model: perplexity

(cid:73) we have some test data, m sentences

s1, s2, s3, . . . , sm

(cid:81)m

(cid:73) we could look at the id203 under our model

i=1 p(si). or more conveniently, the log id203

m(cid:89)

m(cid:88)

log

p(si) =

log p(si)

i=1

i=1

evaluating a language model: perplexity

(cid:73) we have some test data, m sentences

s1, s2, s3, . . . , sm

(cid:81)m

(cid:73) we could look at the id203 under our model

i=1 p(si). or more conveniently, the log id203

m(cid:89)

m(cid:88)

log

p(si) =

log p(si)

i=1

i=1

(cid:73) in fact the usual evaluation measure is perplexity

perplexity = 2   l where

l =

1
m

log p(si)

and m is the total number of words in the test data.

m(cid:88)

i=1

some intuition about perplexity

(cid:73) say we have a vocabulary v, and n = |v| + 1

and model that predicts

q(w|u, v) =

1
n

for all w     v     {stop}, for all u, v     v     {*}.

(cid:73) easy to calculate the perplexity in this case:

perplexity = 2   l where

l = log

1
n

   

perplexity is a measure of e   ective    branching factor   

perplexity = n

typical values of perplexity

(cid:73) results from goodman (   a bit of progress in language

modeling   ), where |v| = 50, 000

(cid:73) a trigram model: p(x1 . . . xn) =(cid:81)n

perplexity = 74

i=1 q(xi|xi   2, xi   1).

typical values of perplexity

(cid:73) results from goodman (   a bit of progress in language

modeling   ), where |v| = 50, 000

(cid:73) a trigram model: p(x1 . . . xn) =(cid:81)n
(cid:73) a bigram model: p(x1 . . . xn) =(cid:81)n

perplexity = 74

perplexity = 137

i=1 q(xi|xi   2, xi   1).

i=1 q(xi|xi   1).

typical values of perplexity

(cid:73) results from goodman (   a bit of progress in language

perplexity = 74

modeling   ), where |v| = 50, 000

(cid:73) a trigram model: p(x1 . . . xn) =(cid:81)n
(cid:73) a bigram model: p(x1 . . . xn) =(cid:81)n
(cid:73) a unigram model: p(x1 . . . xn) =(cid:81)n

perplexity = 137

perplexity = 955

i=1 q(xi|xi   2, xi   1).

i=1 q(xi|xi   1).

i=1 q(xi).

some history

(cid:73) shannon conducted experiments on id178 of english

i.e., how good are people at the perplexity game?

c. shannon. prediction and id178 of printed
english. bell systems technical journal,
30:50   64, 1951.

some history

chomsky (in syntactic structures (1957)):

second, the notion    grammatical    cannot be identi   ed with
   meaningful    or    signi   cant    in any semantic sense.
sentences (1) and (2) are equally nonsensical, but any speaker
of english will recognize that only the former is grammatical.

(1) colorless green ideas sleep furiously.
(2) furiously sleep ideas green colorless.
. . .
. . . third, the notion    grammatical in english    cannot be
identi   ed in any way with the notion    high order of statistical
approximation to english   . it is fair to assume that neither
sentence (1) nor (2) (nor indeed any part of these sentences)
has ever occurred in an english discourse. hence, in any
statistical model for grammaticalness, these sentences will be
ruled out on identical grounds as equally    remote    from
english. yet (1), though nonsensical, is grammatical, while
(2) is not. . . .

overview

(cid:73) the id38 problem
(cid:73) trigram models
(cid:73) evaluating language models: perplexity
(cid:73) estimation techniques:
(cid:73) linear interpolation
(cid:73) discounting methods

sparse data problems

a natural estimate (the    maximum likelihood estimate   ):

q(wi | wi   2, wi   1) =

count(wi   2, wi   1, wi)

count(wi   2, wi   1)

q(laughs | the, dog) =

count(the, dog, laughs)

count(the, dog)

say our vocabulary size is n = |v|, then there are n 3
parameters in the model.
e.g., n = 20, 000     20, 0003 = 8    1012 parameters

the bias-variance trade-o   

(cid:73) trigram maximum-likelihood estimate

qml(wi | wi   2, wi   1) =

count(wi   2, wi   1, wi)

count(wi   2, wi   1)

(cid:73) bigram maximum-likelihood estimate

qml(wi | wi   1) =

count(wi   1, wi)

count(wi   1)

(cid:73) unigram maximum-likelihood estimate

qml(wi) =

count(wi)
count()

linear interpolation

(cid:73) take our estimate q(wi | wi   2, wi   1) to be

q(wi | wi   2, wi   1) =   1    qml(wi | wi   2, wi   1)

+  2    qml(wi | wi   1)
+  3    qml(wi)

where   1 +   2 +   3 = 1, and   i     0 for all i.

linear interpolation (continued)

our estimate correctly de   nes a distribution (de   ne
v(cid:48) = v     {stop}):
(cid:80)
w   v(cid:48) q(w | u, v)

linear interpolation (continued)

our estimate correctly de   nes a distribution (de   ne
v(cid:48) = v     {stop}):
(cid:80)
w   v(cid:48) q(w | u, v)
=(cid:80)

w   v(cid:48) [  1    qml(w | u, v) +   2    qml(w | v) +   3    qml(w)]

linear interpolation (continued)

our estimate correctly de   nes a distribution (de   ne
v(cid:48) = v     {stop}):
(cid:80)
w   v(cid:48) q(w | u, v)
=(cid:80)
w   v(cid:48) [  1    qml(w | u, v) +   2    qml(w | v) +   3    qml(w)]
(cid:80)
w qml(w | u, v) +   2

(cid:80)
w qml(w | v) +   3

w qml(w)

(cid:80)

=   1

linear interpolation (continued)

our estimate correctly de   nes a distribution (de   ne
v(cid:48) = v     {stop}):
(cid:80)
w   v(cid:48) q(w | u, v)
=(cid:80)
w   v(cid:48) [  1    qml(w | u, v) +   2    qml(w | v) +   3    qml(w)]
(cid:80)
w qml(w | u, v) +   2

(cid:80)
w qml(w | v) +   3

w qml(w)

(cid:80)

=   1

=   1 +   2 +   3

linear interpolation (continued)

our estimate correctly de   nes a distribution (de   ne
v(cid:48) = v     {stop}):
(cid:80)
w   v(cid:48) q(w | u, v)
=(cid:80)
w   v(cid:48) [  1    qml(w | u, v) +   2    qml(w | v) +   3    qml(w)]
(cid:80)
w qml(w | u, v) +   2

(cid:80)
w qml(w | v) +   3

w qml(w)

(cid:80)

=   1

=   1 +   2 +   3

= 1

linear interpolation (continued)

our estimate correctly de   nes a distribution (de   ne
v(cid:48) = v     {stop}):
(cid:80)
w   v(cid:48) q(w | u, v)
=(cid:80)
w   v(cid:48) [  1    qml(w | u, v) +   2    qml(w | v) +   3    qml(w)]
(cid:80)
w qml(w | u, v) +   2

(cid:80)
w qml(w | v) +   3

w qml(w)

(cid:80)

=   1

=   1 +   2 +   3

= 1
(can show also that q(w | u, v)     0 for all w     v(cid:48))

how to estimate the    values?

(cid:73) hold out part of training set as    validation    data

how to estimate the    values?

(cid:73) hold out part of training set as    validation    data

(cid:73) de   ne c(cid:48)(w1, w2, w3) to be the number of times the

trigram (w1, w2, w3) is seen in validation set

how to estimate the    values?

(cid:73) hold out part of training set as    validation    data

(cid:73) de   ne c(cid:48)(w1, w2, w3) to be the number of times the

trigram (w1, w2, w3) is seen in validation set

(cid:73) choose   1,   2,   3 to maximize:

(cid:88)

l(  1,   2,   3) =

c(cid:48)(w1, w2, w3) log q(w3 | w1, w2)

w1,w2,w3

such that   1 +   2 +   3 = 1, and   i     0 for all i, and
where
q(wi | wi   2, wi   1) =   1    qml(wi | wi   2, wi   1)

+  2    qml(wi | wi   1)
+  3    qml(wi)

allowing the      s to vary

(cid:73) take a function    that partitions histories

e.g.,

  (wi   2, wi   1) =

                     

1 if count(wi   1, wi   2) = 0
2 if 1     count(wi   1, wi   2)     2
3 if 3     count(wi   1, wi   2)     5
4 otherwise

(cid:73) introduce a dependence of the      s on the partition:

1

q(wi | wi   2, wi   1) =     (wi   2,wi   1)
+    (wi   2,wi   1)
+    (wi   2,wi   1)
+     (wi   2,wi   1)
    0 for all i.

where     (wi   2,wi   1)
and     (wi   2,wi   1)

2

3

2

1

i

   qml(wi | wi   2, wi   1)
   qml(wi | wi   1)
   qml(wi)
+     (wi   2,wi   1)

= 1,

3

overview

(cid:73) the id38 problem
(cid:73) trigram models
(cid:73) evaluating language models: perplexity
(cid:73) estimation techniques:
(cid:73) linear interpolation
(cid:73) discounting methods

discounting methods

(cid:73) say we   ve seen the following counts:

qml(wi | wi   1)

x
the

the, dog
the, woman
the, man
the, park
the, job
the, telescope
the, manual
the, afternoon
the, country
the, street

count(x)

48

15
11
10
5
2
1
1
1
1
1

15/48
11/48
10/48
5/48
2/48
1/48
1/48
1/48
1/48
1/48

(cid:73) the maximum-likelihood estimates are high

(particularly for low count items)

discounting methods

(cid:73) now de   ne    discounted    counts,

count   (x) = count(x)     0.5

(cid:73) new estimates:

x

the

the, dog
the, woman
the, man
the, park
the, job
the, telescope
the, manual
the, afternoon
the, country
the, street

count(x) count   (x)

   
count
(x)
count(the)

48

15
11
10
5
2
1
1
1
1
1

14.5
10.5
9.5
4.5
1.5
0.5
0.5
0.5
0.5
0.5

14.5/48
10.5/48
9.5/48
4.5/48
1.5/48
0.5/48
0.5/48
0.5/48
0.5/48
0.5/48

discounting methods (continued)

(cid:73) we now have some    missing id203 mass   :
count   (wi   1, w)
count(wi   1)

  (wi   1) = 1    (cid:88)

w

e.g., in our example,   (the) = 10    0.5/48 = 5/48

katz back-o    models (bigrams)
(cid:73) for a bigram model, de   ne two sets

a(wi   1) = {w : count(wi   1, w) > 0}
b(wi   1) = {w : count(wi   1, w) = 0}

(cid:73) a bigram model

qbo(wi | wi   1) =

                     

   
count

(wi   1,wi)

count(wi   1)

(cid:80)

  (wi   1)

qml(wi)

w   b(wi   1) qml(w)

if wi     a(wi   1)

if wi     b(wi   1)

where

  (wi   1) = 1     (cid:88)

count   (wi   1, w)
count(wi   1)

w   a(wi   1)

katz back-o    models (trigrams)

(cid:73) for a trigram model,    rst de   ne two sets

a(wi   2, wi   1) = {w : count(wi   2, wi   1, w) > 0}
b(wi   2, wi   1) = {w : count(wi   2, wi   1, w) = 0}
(cid:73) a trigram model is de   ned in terms of the bigram model:

   
count

qbo(wi | wi   2, wi   1) =

                                             
  (wi   2, wi   1) = 1    (cid:88)

where

(cid:80)

(wi   2,wi   1,wi)

count(wi   2,wi   1)

if wi     a(wi   2, wi   1)

  (wi   2,wi   1)qbo(wi|wi   1)
w   b(wi   2,wi   1) qbo(w|wi   1)

if wi     b(wi   2, wi   1)

count   (wi   2, wi   1, w)
count(wi   2, wi   1)

w   a(wi   2,wi   1)

summary

(cid:73) three steps in deriving the language model probabilities:

1. expand p(w1, w2 . . . wn) using chain rule.
2. make markov independence assumptions

p(wi | w1, w2 . . . wi   2, wi   1) = p(wi | wi   2, wi   1)

3. smooth the estimates using low order counts.

(cid:73) other methods used to improve language models:

(cid:73)    topic    or    long-range    features.
(cid:73) syntactic models.

it   s generally hard to improve on trigram models though!!

