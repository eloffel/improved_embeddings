id148

michael collins, columbia university

the id38 problem

(cid:73) wi is the i   th word in a document
(cid:73) estimate a distribution p(wi|w1, w2, . . . wi   1) given previous

   history    w1, . . . , wi   1.

(cid:73) e.g., w1, . . . , wi   1 =

third, the notion    grammatical in english    cannot be
identi   ed in any way with the notion    high order of
statistical approximation to english   . it is fair to assume
that neither sentence (1) nor (2) (nor indeed any part of
these sentences) has ever occurred in an english
discourse. hence, in any statistical

trigram models

(cid:73) estimate a distribution p(wi|w1, w2, . . . wi   1) given previous

   history    w1, . . . , wi   1 =

third, the notion    grammatical in english    cannot be identi   ed in any way
with the notion    high order of statistical approximation to english   . it is fair
to assume that neither sentence (1) nor (2) (nor indeed any part of these
sentences) has ever occurred in an english discourse. hence, in any statistical

(cid:73) trigram estimates:

q(model|w1, . . . wi   1) =   1qm l(model|wi   2 = any, wi   1 = statistical) +

where   i     0,(cid:80)

  2qm l(model|wi   1 = statistical) +
  3qm l(model)

i   i = 1, qm l(y|x) = count(x,y)

count(x)

trigram models
q(model|w1, . . . wi   1) =   1qm l(model|wi   2 = any, wi   1 = statistical) +

  2qm l(model|wi   1 = statistical) +
  3qm l(model)

(cid:73) makes use of only bigram, trigram, unigram estimates
(cid:73) many other    features    of w1, . . . , wi   1 may be useful, e.g.,:

qm l(model
qm l(model
qm l(model
qm l(model
qm l(model
qm l(model

| wi   2 = any)
| wi   1 is an adjective)
| wi   1 ends in    ical   )
| author = chomsky)
|    model    does not occur somewhere in w1, . . . wi   1)
|    grammatical    occurs somewhere in w1, . . . wi   1)

a naive approach

q(model|w1, . . . wi   1) =
  1qm l(model|wi   2 = any, wi   1 = statistical) +
  2qm l(model|wi   1 = statistical) +
  3qm l(model) +
  4qm l(model|wi   2 = any) +
  5qm l(model|wi   1 is an adjective) +
  6qm l(model|wi   1 ends in    ical   ) +
  7qm l(model|author = chomsky) +
  8qm l(model|   model    does not occur somewhere in w1, . . . wi   1) +
  9qm l(model|   grammatical    occurs somewhere in w1, . . . wi   1)

this quickly becomes very unwieldy...

a second example: part-of-speech tagging

input:
pro   ts soared at boeing co., easily topping forecasts on wall street,
as their ceo alan mulally announced    rst quarter results.

output:

pro   ts/n soared/v at/p boeing/n co./n ,/, easily/adv topping/v
forecasts/n on/p wall/n street/n ,/, as/p their/poss ceo/n
alan/n mulally/n announced/v    rst/adj quarter/n results/n ./.

= noun
= verb
= preposition

n
v
p
adv = adverb
adj
. . .

= adjective

a second example: part-of-speech tagging

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .
    there are many possible tags in the position ??
{nn, nns, vt, vi, in, dt, . . .}
    the task: model the distribution

p(ti|t1, . . . , ti   1, w1 . . . wn)

where ti is the i   th tag in the sequence, wi is the i   th word

a second example: part-of-speech tagging

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .
    the task: model the distribution

p(ti|t1, . . . , ti   1, w1 . . . wn)

where ti is the i   th tag in the sequence, wi is the i   th word
    again: many    features    of t1, . . . , ti   1, w1 . . . wn may be relevant

qm l(nn | wi = base)
qm l(nn |
ti   1 is jj)
qm l(nn | wi ends in    e   )
qm l(nn | wi ends in    se   )
qm l(nn | wi   1 is    important   )
qm l(nn | wi+1 is    from   )

overview

(cid:73) id148

(cid:73) the maximum-id178 property

(cid:73) smoothing, feature selection etc. in id148

the general problem

(cid:73) we have some input domain x
(cid:73) have a    nite label set y
(cid:73) aim is to provide a id155 p(y | x)

for any x, y where x     x , y     y

id38

(cid:73) x is a    history    w1, w2, . . . wi   1, e.g.,

third, the notion    grammatical in english    cannot be identi   ed in any
way with the notion    high order of statistical approximation to english   .
it is fair to assume that neither sentence (1) nor (2) (nor indeed any
part of these sentences) has ever occurred in an english discourse.
hence, in any statistical

(cid:73) y is an    outcome    wi

feature vector representations

(cid:73) aim is to provide a id155 p(y | x) for

   decision    y given    history    x

(cid:73) a feature is a function fk(x, y)     r

(often binary features or indicator functions
f (x, y)     {0, 1}).

(cid:73) say we have m features fk for k = 1 . . . m

    a feature vector f (x, y)     rm for any x, y

id38

(cid:73) x is a    history    w1, w2, . . . wi   1, e.g.,

third, the notion    grammatical in english    cannot be identi   ed in any
way with the notion    high order of statistical approximation to english   .
it is fair to assume that neither sentence (1) nor (2) (nor indeed any
part of these sentences) has ever occurred in an english discourse.
hence, in any statistical

(cid:73) y is an    outcome    wi
(cid:73) example features:

(cid:26) 1
(cid:26) 1
(cid:26) 1

0

0

0

f1(x, y) =

f2(x, y) =

f3(x, y) =

if y = model
otherwise
if y = model and wi   1 = statistical
otherwise
if y = model, wi   2 = any, wi   1 = statistical
otherwise

0 otherwise

0 otherwise

(cid:26) 1 if y = model, wi   2 = any
(cid:26) 1 if y = model, wi   1 is an adjective
(cid:26) 1 if y = model, wi   1 ends in    ical   
(cid:26) 1 if y = model, author = chomsky
(cid:26) 1 if y = model,    model    is not in w1, . . . wi   1
(cid:26) 1 if y = model,    grammatical    is in w1, . . . wi   1

0 otherwise

0 otherwise

0 otherwise

0 otherwise

f4(x, y) =

f5(x, y) =

f6(x, y) =

f7(x, y) =

f8(x, y) =

f9(x, y) =

de   ning features in practice

(cid:73) we had the following    trigram    feature:

(cid:26) 1 if y = model, wi   2 = any, wi   1 = statistical

f3(x, y) =

0 otherwise

(cid:73) in practice, we would probably introduce one trigram feature

for every trigram seen in the training data: i.e., for all
trigrams (u, v, w) seen in training data, create a feature

(cid:26) 1 if y = w, wi   2 = u, wi   1 = v

fn (u,v,w)(x, y) =

0 otherwise

where n (u, v, w) is a function that maps each (u, v, w)
trigram to a di   erent integer

the pos-tagging example

(cid:73) each x is a    history    of the form (cid:104)t1, t2, . . . , ti   1, w1 . . . wn, i(cid:105)
(cid:73) each y is a pos tag, such as nn, nns, vt, vi, in, dt, . . .
(cid:73) we have m features fk(x, y) for k = 1 . . . m

for example:

f1(x, y) =

f2(x, y) =

. . .

(cid:26) 1 if current word wi is base and y = vt
(cid:26) 1 if current word wi ends in ing and y = vbg

0 otherwise

0 otherwise

the full set of features in ratnaparkhi, 1996

(cid:73) word/tag features for all word/tag pairs, e.g.,

(cid:26) 1 if current word wi is base and y = vt

f100(x, y) =

0 otherwise

(cid:73) spelling features for all pre   xes/su   xes of length     4, e.g.,

(cid:26) 1 if current word wi ends in ing and y = vbg
(cid:26) 1 if current word wi starts with pre and y = nn

0 otherwise

f101(x, y) =

f102(h, t) =

0 otherwise

the full set of features in ratnaparkhi, 1996

(cid:73) contextual features, e.g.,

0 otherwise

0 otherwise

(cid:26) 1 if (cid:104)ti   2, ti   1, y(cid:105) = (cid:104)dt, jj, vt(cid:105)
(cid:26) 1 if (cid:104)ti   1, y(cid:105) = (cid:104)jj, vt(cid:105)
(cid:26) 1 if (cid:104)y(cid:105) = (cid:104)vt(cid:105)
(cid:26) 1 if previous word wi   1 = the and y = vt
(cid:26) 1 if next word wi+1 = the and y = vt

0 otherwise

0 otherwise

0 otherwise

f103(x, y) =

f104(x, y) =

f105(x, y) =

f106(x, y) =

f107(x, y) =

the final result

(cid:73) we can come up with practically any questions (features)

regarding history/tag pairs.

(cid:73) for a given history x     x , each label in y is mapped to a

di   erent feature vector

f ((cid:104)jj, dt, (cid:104) hispaniola, . . .(cid:105), 6(cid:105), vt) = 1001011001001100110
f ((cid:104)jj, dt, (cid:104) hispaniola, . . .(cid:105), 6(cid:105), jj) = 0110010101011110010
f ((cid:104)jj, dt, (cid:104) hispaniola, . . .(cid:105), 6(cid:105), nn) = 0001111101001100100
f ((cid:104)jj, dt, (cid:104) hispaniola, . . .(cid:105), 6(cid:105), in) = 0001011011000000010

. . .

parameter vectors

(cid:73) given features fk(x, y) for k = 1 . . . m,
also de   ne a parameter vector v     rm

(cid:73) each (x, y) pair is then mapped to a    score   

v    f (x, y) =

vkfk(x, y)

(cid:88)

k

id38

(cid:73) x is a    history    w1, w2, . . . wi   1, e.g.,

third, the notion    grammatical in english    cannot be identi   ed in any
way with the notion    high order of statistical approximation to english   .
it is fair to assume that neither sentence (1) nor (2) (nor indeed any
part of these sentences) has ever occurred in an english discourse.
hence, in any statistical

(cid:73) each possible y gets a di   erent score:

v    f (x, model) = 5.6
v    f (x, is) = 1.5
v    f (x, models) = 4.5

v    f (x, the) =    3.2
v    f (x, of ) = 1.3

. . .

id148

(cid:73) we have some input domain x , and a    nite label set y. aim is
to provide a id155 p(y | x) for any x     x and
y     y.

(cid:73) a feature is a function f : x    y     r

(often binary features or indicator functions
fk : x    y     {0, 1}).
    a feature vector f (x, y)     rm for any x     x and y     y.

(cid:73) say we have m features fk for k = 1 . . . m
(cid:73) we also have a parameter vector v     rm
(cid:73) we de   ne

p(y | x; v) =

(cid:80)

ev  f (x,y)
y(cid:48)   y ev  f (x,y(cid:48))

why the name?

log p(y | x; v) = v    f (x, y)
linear term

(cid:123)(cid:122)

(cid:125)

(cid:124)

    log

ev  f (x,y(cid:48))

(cid:88)
(cid:123)(cid:122)

y(cid:48)   y

(cid:124)

(cid:125)

id172 term

maximum-likelihood estimation

(cid:73) maximum-likelihood estimates given training sample (xi, yi)

for i = 1 . . . n, each (xi, yi)     x    y:

vm l = argmaxv   rml(v)

where

l(v) =

n(cid:88)

i=1

log p(yi | xi; v) =

n(cid:88)

i=1

v    f (xi, yi)     n(cid:88)

log

i=1

(cid:88)

y(cid:48)   y

ev  f (xi,y(cid:48))

calculating the maximum-likelihood estimates

(cid:73) need to maximize:

l(v) =

i=1
(cid:73) calculating gradients:

dl(v)
dvk

=

=

=

(cid:88)

y(cid:48)   y

log

ev  f (xi,y(cid:48))

i=1

v    f (xi, yi)     n(cid:88)
n(cid:88)
(cid:80)
fk(xi, yi)     n(cid:88)
n(cid:88)
(cid:80)
(cid:88)
n(cid:88)
fk(xi, yi)     n(cid:88)
    n(cid:88)
(cid:88)
n(cid:88)
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:124)

fk(xi, yi)

y(cid:48)   y

y(cid:48)   y

i=1

i=1

i=1

i=1

i=1

i=1

empirical counts

y(cid:48)   y fk(xi, y(cid:48))ev  f (xi,y(cid:48))

z(cid:48)   y ev  f (xi,z(cid:48))

fk(xi, y(cid:48))

ev  f (xi,y(cid:48))
z(cid:48)   y ev  f (xi,z(cid:48))
fk(xi, y(cid:48))p(y(cid:48) | xi; v)

(cid:80)
(cid:123)(cid:122)

(cid:125)

expected counts

gradient ascent methods

(cid:73) need to maximize l(v) where

n(cid:88)

f (xi, yi)     n(cid:88)

i=1

i=1

(cid:88)

y(cid:48)   y

f (xi, y(cid:48))p(y(cid:48) | xi; v)

dl(v)

dv

=

initialization: v = 0
iterate until convergence:

(cid:73) calculate     = dl(v)
dv
(cid:73) calculate       = argmax  l(v +      ) (line
(cid:73) set v     v +         

search)

conjugate gradient methods

(cid:73) (vanilla) gradient ascent can be very slow

(cid:73) conjugate gradient methods require calculation of gradient at
each iteration, but do a line search in a direction which is a
function of the current gradient, and the previous step
taken.

(cid:73) conjugate gradient packages are widely available

in general: they require a function

calc gradient(v)    

and that   s about it!

(cid:18)

l(v),

dl(v)

dv

(cid:19)

overview

(cid:73) id148

(cid:73) the maximum-id178 property

(cid:73) smoothing, feature selection etc. in id148

overview

(cid:73) id148

(cid:73) the maximum-id178 property

(cid:73) smoothing, feature selection etc. in id148

smoothing in maximum id178 models

(cid:73) say we have a feature:

(cid:26) 1 if current word wi is base and t = vt

f100(h, t) =

0 otherwise

(cid:73) in training data, base is seen 3 times, with vt every time
(cid:73) maximum likelihood solution satis   es

(cid:88)

(cid:88)

(cid:88)

f100(xi, yi) =

p(y | xi; v)f100(xi, y)

i

i

y

    p(vt | xi; v) = 1 for any history xi where wi = base
    v100         at maximum-likelihood solution (most likely)
    p(vt | x; v) = 1 for any test data history x where w = base

(cid:88)

y(cid:48)   y

ev  f (xi,y(cid:48))     
2

m(cid:88)

k=1

v2
k

log

id173

(cid:73) modi   ed id168

l(v) =

dl(v)
dvk

=

n(cid:88)

i=1

i=1

v    f (xi, yi)     n(cid:88)
    n(cid:88)
n(cid:88)
(cid:124)
(cid:124)

fk(xi, yi)

(cid:123)(cid:122)

(cid:125)

i=1

i=1

empirical counts

(cid:88)

y(cid:48)   y

(cid:73) calculating gradients:

fk(xi, y(cid:48))p(y(cid:48) | xi; v)

     vk

(cid:123)(cid:122)

(cid:125)

expected counts

(cid:73) can run conjugate gradient methods as before

(cid:73) adds a penalty for large weights

experiments with gaussian priors

(cid:73) [chen and rosenfeld, 1998]: apply id148 to

id38: estimate q(wi | wi   2, wi   1)

(cid:73) unigram, bigram, trigram features, e.g.,

(cid:26) 1 if trigram is (the,dog,laughs)
(cid:26) 1 if bigram is (dog,laughs)
(cid:26) 1 if unigram is (laughs)

0 otherwise

0 otherwise

f1(wi   2, wi   1, wi) =

f2(wi   2, wi   1, wi) =

f3(wi   2, wi   1, wi) =

q(wi | wi   2, wi   1) =

0 otherwise

(cid:80)

ef (wi   2,wi   1,wi)  v
w ef (wi   2,wi   1,w)  v

experiments with gaussian priors

(cid:73) in regular (unregularized) id148, if all id165

features are included, then it   s equivalent to
maximum-likelihood estimates!
q(wi | wi   2, wi   1) =

count(wi   2, wi   1, wi)

count(wi   2, wi   1)

(cid:73) [chen and rosenfeld, 1998]: with gaussian priors, get very
good results. performs as well as or better than standardly
used    discounting methods    (see lecture 2).

(cid:73) downside: computing(cid:80)

w ef (wi   2,wi   1,w)  v is slow.

