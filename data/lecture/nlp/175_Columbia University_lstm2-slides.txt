recurrent networks, and attention, for

id151

michael collins, columbia university

mapping sequences to sequences

(cid:73) learn to map input sequences x1 . . . xn to output sequences

y1 . . . ym where ym = stop.

(cid:73) can decompose this as

p(y1 . . . ym|x1 . . . xn) =

m(cid:89)

j=1

p(yj|y1 . . . yj   1, x1 . . . xn)

(cid:73) encoder/decoder framework: use an lstm to map x1 . . . xn

to a vector h(n), then model

p(yj|y1 . . . yj   1, x1 . . . xn) = p(yj|y1 . . . yj   1, h(n))

using a    decoding    lstm

the computational graph

training a recurrent network for translation

inputs: a sequence of source language words x1 . . . xn where each
xj     rd. a sequence of target language words y1 . . . ym where
ym = stop.
de   nitions:   f = parameters of an    encoding    lstm.   d =
parameters of a    decoding    lstm. lstm(x(t), h(t   1);   ) maps an
input x(t) together with a hidden state h(t   1) to a new hidden state
h(t). here    are the parameters of the lstm

training a recurrent network for translation
(continued)

computational graph:

(cid:73) initialize h(0) to some values (e.g. vector of all zeros)
(cid:73) (encoding step:) for t = 1 . . . n

(cid:73) h(t) = lstm(x(t), h(t   1);   f )

(cid:73) initialize   (0) to some values (e.g., vector of all zeros)
(cid:73) (decoding step:) for j = 1 . . . m

(cid:73)   (j) = lstm(concat(yj   1, h(n)),   (j   1);   d)
(cid:73) l(j) = v    concat(  (j), yj   1, h(n)) +   , q(j) = ls(l(j)),

o(j) =    q(j)

yj

(cid:73) (final loss is sum of losses:)

m(cid:88)

j=1

o(j)

o =

the computational graph

greedy decoding with a recurrent network for
translation

(cid:73) encoding step: calculate h(n) from the input x1 . . . xn
(cid:73) j = 1. do:

(cid:73) yj = arg maxy p(y|y1 . . . yj   1, h(n))
(cid:73) j = j + 1
(cid:73) until: yj   1 = stop

greedy decoding with a recurrent network for
translation

computational graph:

(cid:73) initialize h(0) to some values (e.g. vector of all zeros)
(cid:73) (encoding step:) for t = 1 . . . n

(cid:73) h(t) = lstm(x(t), h(t   1);   f )

(cid:73) initialize   (0) to some values (e.g., vector of all zeros)
(cid:73) (decoding step:) j = 1. do:

(cid:73)   (j) = lstm(concat(yj   1, h(n)),   (j   1);   d)
(cid:73) l(j) = v    concat(  (j), yj   1, h(n)) +   
(cid:73) yj = arg maxy l(j)
y
(cid:73) j = j + 1
(cid:73) until yj   1 = stop

(cid:73) return y1 . . . yj   1

a bi-directional lstm (bi-lstm) for encoding

inputs: a sequence x1 . . . xn where each xj     rd.
de   nitions:   f and   b are parameters of a forward and backward
lstm.
computational graph:

(cid:73) h(0),   (n+1) are set to some inital values.
(cid:73) for t = 1 . . . n

(cid:73) h(t) = lstm(x(t), h(t   1);   f )

(cid:73) for t = n . . . 1

(cid:73)   (t) = lstm(x(t),   (t+1);   b)

(cid:73) for t = 1 . . . n

(cid:73) u(t) = concat(h(t),   (t))     encoding for position t

the computational graph

incorporating attention

(cid:73) old decoder:

(cid:73) c(j) = h(n)     context used in decoding at j   th step
(cid:73)   (j) = lstm(concat(yj   1, c(j)),   (j   1);   d)
(cid:73) l(j) = v    concat(  (j), yj   1, c(j)) +   
(cid:73) yj = arg maxy l(j)
y

incorporating attention

(cid:73) new decoder:

(cid:73) de   ne

where

and

n(cid:88)
(cid:80)n

i=1

ai,ju(i)

exp{si,j}
i=1 si,j

c(j) =

ai,j =

si,j = a(  (j   1), u(i);   a)

where a(. . .) is a non-linear function (e.g., a feedforward
network) with parameters   a

greedy decoding with attention

(cid:73) (decoding step:) j = 1. do:

(cid:73) for i = 1 . . . n,

si,j = a(  (j   1), u(i);   a)

(cid:80)n

exp{si,j}
i=1 si,j

ai,j =

(cid:73) for i = 1 . . . n,

(cid:73) set c(j) =(cid:80)n

i=1 ai,ju(i)

(cid:73)   (j) = lstm(concat(yj   1, c(j)),   (j   1);   d)
(cid:73) l(j) = v    concat(  (j), yj   1, c(j)) +   
(cid:73) yj = arg maxy l(j)
y
(cid:73) j = j + 1
(cid:73) until yj   1 = stop

(cid:73) return y1 . . . yj   1

training with attention

(cid:73) (decoding step:) for j = 1 . . . m

(cid:73) for i = 1 . . . n,

si,j = a(  (j   1), u(i);   a)

(cid:80)n

exp{si,j}
i=1 si,j

ai,j =

(cid:73) for i = 1 . . . n,

(cid:73) set c(j) =(cid:80)n

i=1 ai,ju(i)

(cid:73)   (j) = lstm(concat(yj   1, c(j)),   (j   1);   d)
(cid:73) l(j) = v    concat(  (j), yj   1, c(j)) +   , q(j) = ls(l(j)),

o(j) =    q(j)

yj

(cid:73) (final loss is sum of losses:)

m(cid:88)

j=1

o(j)

o =

the computational graph

results from wu et al. 2016

(cid:73) from google   s id4 system: bridging the gap

between human and machine translation, wu et al. 2016. human
evaluations are on a 1-6 scale (6 is best). pbmt is a phrase-based
translation system, using ibm alignment models as a starting point.

results from wu et al. 2016 (continued)

conclusions

(cid:73) directly model

p(y1 . . . ym|x1 . . . xn) =

m(cid:89)

j=1

p(yj|y1 . . . yj   1, x1 . . . xn)

(cid:73) encoding step: map x1 . . . xn to u(1) . . . u(n) using a

bidirectional lstm

(cid:73) decoding step: use an lstm in decoding together with

attention

