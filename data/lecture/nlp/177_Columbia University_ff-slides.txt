feedforward neural networks

michael collins, columbia university

recap: id148

a log-linear model takes the following form:

p(y|x; v) =

(cid:80)

exp (v    f (x, y))
y(cid:48)   y exp (v    f (x, y(cid:48)))

(cid:73) f (x, y) is the representation of (x, y)
(cid:73) advantage: f (x, y) is highly    exible in terms of the features

that can be included

(cid:73) disadvantage: can be hard to design features by hand
(cid:73) neural networks allow the representation itself to be
learned. recent empirical results across a broad set of
domains have shown that learned representations in neural
networks can give very signi   cant improvements in accuracy
over hand-engineered features.

example 1: the id38 problem

(cid:73) wi is the i   th word in a document
(cid:73) estimate a distribution p(wi|w1, w2, . . . wi   1) given previous

   history    w1, . . . , wi   1.

(cid:73) e.g., w1, . . . , wi   1 =

third, the notion    grammatical in english    cannot be
identi   ed in any way with the notion    high order of
statistical approximation to english   . it is fair to assume
that neither sentence (1) nor (2) (nor indeed any part of
these sentences) has ever occurred in an english
discourse. hence, in any statistical

example 2: part-of-speech tagging

hispaniola/nnp quickly/rb became/vb an/dt important/jj
base/?? from which spain expanded its empire into the rest of the
western hemisphere .
    there are many possible tags in the position ??
{nn, nns, vt, vi, in, dt, . . .}
    the task: model the distribution

p(ti|t1, . . . , ti   1, w1 . . . wn)

where ti is the i   th tag in the sequence, wi is the i   th word

overview

(cid:73) basic de   nitions
(cid:73) stochastic id119
(cid:73) de   ning the input to a neural network
(cid:73) a single neuron
(cid:73) a single-layer feedforward network
(cid:73) motivation: the xor problem

an alternative form for id148

old form:

p(y|x; v) =

(cid:80)

exp (v    f (x, y))
y(cid:48)   y exp (v    f (x, y(cid:48)))

(1)

new form:

p(y|x; v) =

(cid:80)
exp (v(y)    f (x) +   y)
y(cid:48)   y exp (v(y(cid:48))    f (x) +   y(cid:48))
(cid:73) feature vector f (x) maps input x to f (x)     rd.
(cid:73) parameters: v(y)     rd,   y     r for each y     y.
(cid:73) the score v    f (x, y) in eq. 1 has essentially been replaced by

(2)

v(y)    f (x) +   y in eq. 2.
bias values: that is, v = {(v(y),   y) : y     y}

(cid:73) we will use v to refer to the set of all parameter vectors and

introducing learned representations

p(y|x;   , v) =

(cid:80)

exp (v(y)      (x;   ) +   y)
y(cid:48)   y exp (v(y(cid:48))      (x;   ) +   y(cid:48))

(3)

(cid:73) replaced f (x) by   (x;   ) where    are some additional

parameters of the model

(cid:73) the parameter values    will be estimated from training

examples: the representation of x is then    learned   

(cid:73) in this lecture we   ll show how feedforward neural networks

can be used to de   ne   (x;   ).

de   nition (multi-class feedforward models)
a multi-class feedforward model consists of:

(cid:73) a set x of possible inputs. a    nite set y of possible labels. a

positive integer d specifying the number of features in the
feedforward representation.

(cid:73) a parameter vector    de   ning the feedforward parameters of the

network. we use     to refer to the set of possible values for   .

(cid:73) a function    : x            rd that maps any (x,   ) pair to a

   feedforward representation      (x;   ).

(cid:73) for each label y     y, a parameter vector v(y)     rd, and a bias

value   y     r.

for any x     x , y     y,

p(y|x;   , v) =

y(cid:48)   y exp(cid:0)v(y(cid:48))      (x;   ) +   y(cid:48)(cid:1)
(cid:80)

exp (v(y)      (x;   ) +   y)

two questions

(cid:73) how can we de   ne the feedforward representation   (x;   )?
(cid:73) given training examples (xi, yi) for i = 1 . . . n, how can we

train the parameters    and v?

overview

(cid:73) basic de   nitions
(cid:73) stochastic id119
(cid:73) de   ning the input to a neural network
(cid:73) a single neuron
(cid:73) a single-layer feedforward network
(cid:73) motivation: the xor problem

a simple version of stochastic id119

inputs: training examples (xi, yi) for i = 1 . . . n. a feedforward
representation   (x;   ). an integer t specifying the number of
updates. a sequence of learning rate values   1 . . .   t where each
  t > 0.
initialization: set v and    to random parameter values.

a simple version of stochastic id119
(continued)

algorithm:

(cid:73) for t = 1 . . . t

(cid:73) select an integer i uniformly at random from {1 . . . n}
(cid:73) de   ne l(  , v) =     log p(yi|xi;   , v)
(cid:73) for each parameter   j,   j =   j       t    dl(  ,v)
d  j
(cid:73) for each label y, for each parameter vk(y),

vk(y) = vk(y)       t    dl(  ,v)

dvk(y)

(cid:73) for each label y,   y =   y       t    dl(  ,v)

d  y

output: parameters    and v

overview

(cid:73) basic de   nitions
(cid:73) stochastic id119
(cid:73) de   ning the input to a neural network
(cid:73) a single neuron
(cid:73) a single-layer feedforward network
(cid:73) motivation: the xor problem

de   ning the input to a feedforward network

(cid:73) given an input x, we need to de   ne a function f (x)     rd

that speci   es the input to the network

(cid:73) in general it is assumed that the representation f (x) is

   simple   , not requiring careful hand-engineering.

(cid:73) the neural network will take f (x) as input, and will produce
a representation   (x;   ) that depends on the input x and the
parameters   .

linear models

we could build a log-linear model using f (x) as the
representation:

p(y|x; v) =

(cid:80)
exp{v(y)    f (x) +   y}
y(cid:48) exp{v(y(cid:48))    f (x) +   y(cid:48)}

(4)

this is a    linear    model, because the score v(y)    f (x) is linear in
the input features f (x). the general assumption is that a model
of this form will perform poorly or at least non-optimally. neural
networks enable    non-linear    models that often perform at much
higher levels of accuracy.

an example: digit classi   cation

(cid:73) task is to map an image x to a label y
(cid:73) each image contains a hand-written digit in the set

{0, 1, 2, . . . 9}

(cid:73) the representation f (x) simply represents pixel values in the

image.

(cid:73) for example if the image is 16    16 grey-scale pixels, where
each pixel takes some value indicating how bright it is, we
would have d = 256, with f (x) just being the list of values
for the 256 di   erent pixels in the image.

(cid:73) linear models under this representation perform poorly,

neural networks give much better performance

simplifying notation

(cid:73) from now on assume that x = f (x): that is, the input x is

already de   ned as a vector
(cid:73) this will simplify notation
(cid:73) but remember that when using a neural network you will

have to de   ne a representation of the inputs

overview

(cid:73) basic de   nitions
(cid:73) stochastic id119
(cid:73) de   ning the input to a neural network
(cid:73) a single neuron
(cid:73) a single-layer feedforward network
(cid:73) motivation: the xor problem

a single neuron

(cid:73) a neuron is de   ned by a weight vector w     rd, a bias b     r,

and a transfer function g : r     r.

(cid:73) the neuron maps an input vector x     rd to an output h as

follows:

h = g(w    x + b)

(cid:73) the vector w     rd and scalar b     r are parameters of the

model, which are learned from training examples.

transfer functions

(cid:73) it is important that the transfer function g(z) is non-linear
(cid:73) a linear transfer function would be

g(z) =       z +   

for some constants    and   

the recti   ed linear unit (relu) transfer function

the relu transfer function is de   ned as

g(z) = {z if z     0, or 0 if z < 0}

or equivalently, g(z) = max{0, z}
it follows that the derivative is

dg(z)

dz

= {1 if z > 0, or 0 if z < 0, or unde   ned if z = 0}

the tanh transfer function

the tanh transfer function is de   ned as
e2z     1
e2z + 1

g(z) =

it can be shown that the derivative is

dg(z)

dz

= (1     g(z))2

calculating derivatives

given

h = g(w    x + b)

it will be useful to calculate derivatives

dh
dwj

for the parameters w1, w2, . . . wd, and also

dh
db

for the bias parameter b

calculating derivatives (continued)

we can use the chain rule of di   erentiation. first introduce an
intermediate variable z     r:

z = w    x + b,

h = g(z)

then by the chain rule we have

dh
dwj

=

dh
dz

   dz
dwj

=

dg(z)

dz

   xj

here we have used dh

dz = dg(z)

dz , dz
dwj

= xj.

calculating derivatives (continued)

we can use the chain rule of di   erentiation. first introduce an
intermediate variable z     r:

z = w    x + b,

h = g(z)

then by the chain rule we have

dh
db

=

dh
dz

   dz
db

=

dg(z)

dz

   1

here we have used dh

dz = dg(z)

dz , and dz

db = 1.

de   nition (single-layer feedforward representation)
a single-layer feedforward representation consists of the following:

(cid:73) an integer d specifying the input dimension. each input to

the network is a vector x     rd.

for each k     {1, 2, . . . m} to refer to the k   th row of w .

(cid:73) an integer m specifying the number of hidden units.
(cid:73) a parameter matrix w     rm  d. we use the vector wk     rd
(cid:73) a vector b     rm of bias parameters.
(cid:73) a transfer function g : r     r. common choices are

g(x) = relu(x) or g(x) = tanh(x).

de   nition (single-layer feedforward representation
(continued))
we then de   ne the following:

(cid:73) for k = 1 . . . m, the input to the k   th neuron is

zk = wk    x + bk.

(cid:73) for k = 1 . . . m, the output from the k   th neuron is

hk = g(zk).
(cid:73) finally, de   ne the vector   (x;   )     rm as   k(x;   ) = hk for
k = 1 . . . m. here    denotes the parameters w     rm  d and
b     rm. hence    contains m    (d + 1) parameters in total.

some intuition

the neural network employs m units, each with their own
parameters wk and bk, and these neurons are used to construct a
   hidden    representation h     rm.

matrix form

we can for example replace the operation

zk = wk    x + b

for k = 1 . . . m

with

z = w x + b

where the dimensions are as follows (note that an m-dimensional
column vector is equivalent to a matrix of dimension m    1):

z(cid:124)(cid:123)(cid:122)(cid:125)

m  1

= w(cid:124)(cid:123)(cid:122)(cid:125)
x(cid:124)(cid:123)(cid:122)(cid:125)
(cid:125)
(cid:123)(cid:122)
(cid:124)

m  d

d  1

m  1

+ b(cid:124)(cid:123)(cid:122)(cid:125)

m  1

de   nition (single-layer feedforward representation
(matrix form))
a single-layer feedforward representation consists of the following:

(cid:73) an integer d specifying the input dimension. each input to

the network is a vector x     rd.

(cid:73) an integer m specifying the number of hidden units.
(cid:73) a matrix of parameters w     rm  d.
(cid:73) a vector of bias parameters b     rm
(cid:73) a transfer function g : rm     rm. common choices would be

to de   ne g(z) to be a vector with components
relu(z1), relu(z2), . . . , relu(zm) or
tanh(z1), tanh(z2), . . . , tanh(zm).

de   nition (single-layer feedforward representation
(matrix form) (continued))
we then de   ne the following:

(cid:73) the vector of inputs to the hidden layer z     rm is de   ned as

z = w x + b.

(cid:73) the vector of outputs from the hidden layer h     rm is

de   ned as h = g(z)

(cid:73) finally, de   ne   (x;   ) = h. here the parameters    contain

the matrix w and the vector b.

(cid:73) it follows that

  (x;   ) = g(w x + b)

overview

(cid:73) basic de   nitions
(cid:73) stochastic id119
(cid:73) de   ning the input to a neural network
(cid:73) a single neuron
(cid:73) a single-layer feedforward network
(cid:73) motivation: the xor problem

a motivating example: the xor problem (from deep

learning, ian goodfellow and yoshua bengio and aaron courville)

we will assume a training set where each label is in the set
y = {   1, +1}, and there are 4 training examples, as follows:

x1 = [0, 0],
x2 = [0, 1],
x3 = [1, 0],
x4 = [1, 1],

y1 =    1
y2 = 1
y3 = 1
y4 =    1

a useful lemma

assume we have a model of the form

p(y|x; v) =

(cid:80)

exp{v(y)    x +   y}
y exp{v(y)    x +   y }

and the set of possible labels is y = {   1, +1}. then for any x,

if and only if

p(+1|x; v) > 0.5

u    x +    > 0

where u = v(+1)    v(   1) and    =   +1          1. similarly for any x,

p(   1|x; v) > 0.5

if and only if u    x +    < 0

proof: we have
p(+1|x; v) =

=

exp{v(+1)    x +   +1}

exp{v(+1)    x +   +1} + exp{v(   1)    x +      1}
1 + exp{   (u    x +   )}

1

it follows that p(+1|x; v) > 0.5 if and only if
exp{   (u    x +   )} < 1 from which it follows that u    x +    > 0.
a similar proof applies to the condition p(   1|x; v) > 0.5.

theorem
assume we have examples (xi, yi) for i = 1 . . . 4 as de   ned above.
assume we have a model of the form

p(y|x; v) =

(cid:80)

exp{v(y)    x +   y}
y exp{v(y)    x +   y }

then there are no parameter settings for v(+1), v(   1),   +1,      1
such that

p(yi|xi; v) > 0.5

for i = 1 . . . 4

proof sketch:

from the previous lemma, p(yi = 1|xi; v) > 0.5 if and only if

u    xi +    > 0

where u = v(+1)     v(   1) and    =   +1          1.
similarly p(yi =    1|xi; v) > 0.5 if and only if

u    xi +    < 0

where u = v(+1)     v(   1) and    =   +1          1.
hence to satisfy p(yi|xi; v) > 0.5 for i = 1 . . . 4, there must exist
parameters v and    such that

u    [0, 0] +    < 0
u    [0, 1] +    > 0
u    [1, 0] +    > 0
u    [1, 1] +    < 0

(5)
(6)
(7)
(8)

the constraints can not be satis   ed

u    [0, 0] +    < 0
u    [0, 1] +    > 0
u    [1, 0] +    > 0
u    [1, 1] +    < 0

the constraints can not be satis   ed

u    [0, 0] +    < 0
u    [0, 1] +    > 0
u    [1, 0] +    > 0
u    [1, 1] +    < 0

theorem
assume we have examples (xi, yi) for i = 1 . . . 4 as de   ned above.
assume we have a model of the form

p(y|x;   , v) =

(cid:80)
exp{v(y)      (x;   ) +   y}
y exp{v(y)      (x;   ) +   y}

where   (x;   ) is de   ned by a single layer neural network with
m = 2 hidden units, and the relu(z) activation function. then
there are parameter settings for v(0), v(1),   0,   1,    such that

p(yi|xi; v) > 0.5

for i = 1 . . . 4

proof sketch: de   ne w1 = [1, 1], w2 = [1, 1], b1 = 0, b2 =    1.
then for each input x we can calculate the value for the vectors z
and h corresponding to the inputs and the outputs from the
hidden layer:

x = [0, 0]     z = [0,   1]     h = [0, 0]
x = [1, 0]     z = [1, 0]     h = [1, 0]
x = [0, 1]     z = [1, 0]     h = [1, 0]
x = [1, 1]     z = [2, 1]     h = [2, 1]

proof sketch (continued)

hence to satisfy p(yi|xi; v) > 0.5 for i = 1 . . . 4, there must exist
parameters u = v(+1)     v(   1) and    =   +1          1 such that

u    [0, 0] +    < 0
u    [1, 0] +    > 0
u    [1, 0] +    > 0
u    [2, 1] +    < 0

(9)
(10)
(11)
(12)

it can be veri   ed that u = [1,   2],    =    0.5 satisi   es these
contraints.

