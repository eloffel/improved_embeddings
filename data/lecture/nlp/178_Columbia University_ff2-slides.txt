computational graphs, and id26

michael collins, columbia university

a key problem: calculating derivatives
exp (v(y)      (x;   ) +   y)
y(cid:48)   y exp (v(y(cid:48))      (x;   ) +   y(cid:48))

p(y|x;   , v) =

(cid:80)

(1)

where

and

  (x;   ) = g(w x + b)

(cid:73) m is an integer specifying the number of hidden units
(cid:73) w     rm  d and b     rm are the parameters in   .

g : rm     rm is the transfer function

(cid:73) key question, given a training example (xi, yi), de   ne

l(  , v) =     log p(yi|xi;   , v)
how do we calculate derivatives such as dl(  ,v)
dwk,j

?

a simple version of stochastic id119
(continued)

algorithm:

(cid:73) for t = 1 . . . t

(cid:73) select an integer i uniformly at random from {1 . . . n}
(cid:73) de   ne l(  , v) =     log p(yi|xi;   , v)
(cid:73) for each parameter   j,   j =   j       t    dl(  ,v)
d  j
(cid:73) for each label y, for each parameter vk(y),

vk(y) = vk(y)       t    dl(  ,v)

dvk(y)

(cid:73) for each label y,   y =   y       t    dl(  ,v)

d  y

output: parameters    and v

overview

(cid:73) introduction
(cid:73) the chain rule
(cid:73) derivatives in a single-layer neural network
(cid:73) computational graphs
(cid:73) id26 in computational graphs
(cid:73) justi   cation for id26

partial derivatives

assume we have scalar variables z1, z2 . . . zn, and y, and a
function f , and we de   ne

y = f (z1, z2, . . . zn)

then the partial derivative of f with respect to zi is written as

   f (z1, z2, . . . zn)

we will also write the partial derivative as

   zi

(cid:12)(cid:12)(cid:12)(cid:12)f

   y
   zi

z1...zm

which can be read as    the partial derivative of y with respect to
zi, under function f , at values z1 . . . zm   

partial derivatives (continued)

we will also write the partial derivative as

(cid:12)(cid:12)(cid:12)(cid:12)f

   y
   zi

z1...zm

which can be read as    the partial derivative of y with respect to
zi, under function f , at values z1 . . . zm   

the notation including f is non-standard, but helps to alleviate a
lot of potential confusion...

we will sometimes drop f and/or z1 . . . zm when this is clear from
context

the chain rule

assume we have equations

y = f (z),

z = g(x)

h(x) = f (g(x))

then

or equivalently,

dh(x)

df (g(x))

   dg(x)
dx

=

dx

   y
   x

(cid:12)(cid:12)(cid:12)(cid:12)h

x

dz

(cid:12)(cid:12)(cid:12)(cid:12)f

=

   y
   z

      z
   x

g(x)

(cid:12)(cid:12)(cid:12)(cid:12)g

x

the chain rule

assume we have equations

then

y = f (z),

z = g(x)

h(x) = f (g(x))

dh(x)

dx

=

df (g(x))

dz

   dg(x)
dx

for example, assume f (z) = z2 and g(x) = x3. assume in
addition that x = 2. then:

z = x3 = 8,

dg(x)

dx

= 3x2 = 12,

f (z) = z2 = 64,

df (z)

dz

= 2z = 16

from which it follows that dh(x)

dx = 12    16 = 192

the chain rule (continued)

assume we have equations

y = f (z)

z1 = g1(x), z2 = g2(x), . . . , zn = gn(x)

for some functions f , g1 . . . gn, where z is a vector z     rn, and x
is a vector x     rm.
de   ne the function

h(x) = f (g1(x), g2(x), . . . gn(x))

then we have

   h(x)
   xj

=

   f (z)

   gi(x)

   zi

   xj

where z is the vector g1(x), g2(x), . . . gn(x).

(cid:88)

i

the jacobian matrix

assume we have a function f : rn     rm that takes some vector
x     rn and then returns a vector y     rm:

the jacobian j     rm  n is de   ned as the matrix with entries

y = f (x)

ji,j =

   fi(x)

   xj

hence the jacobian contains all partial derivatives of the function.

the jacobian matrix

assume we have a function f : rn     rm that takes some vector
x     rn and then returns a vector y     rm:

the jacobian j     rm  n is de   ned as the matrix with entries

y = f (x)

ji,j =

   fi(x)

   xj

hence the jacobian contains all partial derivatives of the function.
we will also use

(cid:12)(cid:12)(cid:12)(cid:12)f

x

   y
   x

for vectors y and x to refer to the jacobian matrix with respect to
a function f mapping x to y, evaluated at x

an example of the jacobian: the log-softmax
function

we de   ne ls : rk     rk to be the function such that for
k = 1 . . . k,

(cid:18) exp{lk}
(cid:80)
k(cid:48) exp{lk(cid:48)}

(cid:19)

(cid:88)

k(cid:48)

= lk     log

exp{lk(cid:48)}

the jacobian then has entries

=

   lsk(l)

   lk(cid:48)

   l

k,k(cid:48)

= [[k = k(cid:48)]]     exp{lk(cid:48)}
k(cid:48)(cid:48) exp{lk(cid:48)(cid:48)}

(cid:80)

where [[k = k(cid:48)]] = 1 if k = k(cid:48), 0 otherwise.

lsk(l) = log

(cid:20)   ls(l)

(cid:21)

the chain rule (continued)

assume we have equations

y = f (z1, z2, . . . zn)

zi = gi(x1, x2, . . . xm)

for i = 1 . . . n where y is a vector, zi for all i are vectors, and xj
for all j are vectors. de   ne h(x1 . . . xm) to be the composition of
f and g, so y = h(x1 . . . xm). then

(cid:12)(cid:12)(cid:12)(cid:12)h
(cid:124) (cid:123)(cid:122) (cid:125)

   y
   xj

d(y)  d(xj )

=

n(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)f
(cid:124)(cid:123)(cid:122)(cid:125)

   y
   zi

(cid:12)(cid:12)(cid:12)(cid:12)gi
(cid:124) (cid:123)(cid:122) (cid:125)

      zi
   xj

d(y)  d(zi)

d(zi)  d(xj )

where d(v) is the dimensionality of vector v.

overview

(cid:73) introduction
(cid:73) the chain rule
(cid:73) derivatives in a single-layer neural network
(cid:73) computational graphs
(cid:73) id26 in computational graphs
(cid:73) justi   cation for id26

derivatives in a feedforward network

de   nitions: the set of possible labels is y. we de   ne k = |y|.
g : rm     rm is a transfer function. we de   ne
ls = log-softmax.
inputs: xi     rd, yi     y, w     rm  d, b     rm, v     rk  m,        rk.

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

jacobian involving matrices

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

if w     rm  d, z     rm, the jacobian
   z
   w

is a matrix of dimension m    m(cid:48) where m(cid:48) = (m    d) is the
number of entries in w . so we treat w as a vector with (m    d)
elements.

local functions

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

leaf variables: w , xi, b, v ,   , yi

intermediate variables: z, h, l, q

output variable: o

each intermediate variable has a    local    function:

f z(w, xi, b) = w xi + b,

f h(z) = g(z),

f l(h) = v h +   ,

. . .

global functions

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

leaf variables: w , xi, b, v ,   , yi

intermediate variables: z, h, l, q

output variable: o

global functions: for the output variable o, we de   ne   f o to be the
function that maps the leaf values w , xi, b, v ,   , yi to the
output value o =   f o(w, xi, b, v,   , yi). we use similar de   nitions
for   f z(w, xi, b, v,   , yi),   f h(w, xi, b, v,   , yi), etc.

applying the chain rule

derivative:

(cid:12)(cid:12)(cid:12)(cid:12)   f o

=

   o
   w

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

applying the chain rule

derivative:

(cid:12)(cid:12)(cid:12)(cid:12)   f o

=

   o
   w

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

(cid:12)(cid:12)(cid:12)(cid:12)f o

   o
   q

(cid:12)(cid:12)(cid:12)(cid:12)   f q

      q
   w

applying the chain rule

derivative:

(cid:12)(cid:12)(cid:12)(cid:12)   f o

(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o

   o
   q

   o
   q

(cid:12)(cid:12)(cid:12)(cid:12)   f q
(cid:12)(cid:12)(cid:12)(cid:12)f q

      q
   w
      q
   l

      l
   w

(cid:12)(cid:12)(cid:12)(cid:12)   f l

=

=

   o
   w

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

applying the chain rule

   o
   w

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

derivative:

(cid:12)(cid:12)(cid:12)(cid:12)   f o

(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o

   o
   q

   o
   q

   o
   q

(cid:12)(cid:12)(cid:12)(cid:12)   f q
(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q

      q
   w
      q
   l
      q
   l

(cid:12)(cid:12)(cid:12)(cid:12)   f l
(cid:12)(cid:12)(cid:12)(cid:12)f l

      l
   w
      l
   h

      h
   w

(cid:12)(cid:12)(cid:12)(cid:12)   f h

=

=

=

applying the chain rule

   o
   w

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

derivative:

(cid:12)(cid:12)(cid:12)(cid:12)   f o

(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o

(cid:12)(cid:12)(cid:12)(cid:12)   f q
(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q

      q
   w
      q
   l
      q
   l
      q
   l

   o
   q

   o
   q

   o
   q

   o
   q

(cid:12)(cid:12)(cid:12)(cid:12)   f l
(cid:12)(cid:12)(cid:12)(cid:12)f l
(cid:12)(cid:12)(cid:12)(cid:12)f l

      l
   w
      l
   h
      l
   h

(cid:12)(cid:12)(cid:12)(cid:12)   f h
(cid:12)(cid:12)(cid:12)(cid:12)f h

      h
   w
      h
   z

(cid:12)(cid:12)(cid:12)(cid:12)   f z

      z
   w

=

=

=

=

applying the chain rule

   o
   w

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

derivative:

(cid:12)(cid:12)(cid:12)(cid:12)   f o

(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o

(cid:12)(cid:12)(cid:12)(cid:12)   f q
(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q

      q
   w
      q
   l
      q
   l
      q
   l
      q
   l

   o
   q

   o
   q

   o
   q

   o
   q

   o
   q

(cid:12)(cid:12)(cid:12)(cid:12)   f l
(cid:12)(cid:12)(cid:12)(cid:12)f l
(cid:12)(cid:12)(cid:12)(cid:12)f l
(cid:12)(cid:12)(cid:12)(cid:12)f l

      l
   w
      l
   h
      l
   h
      l
   h

(cid:12)(cid:12)(cid:12)(cid:12)   f h
(cid:12)(cid:12)(cid:12)(cid:12)f h
(cid:12)(cid:12)(cid:12)(cid:12)f h

      h
   w
      h
   z
      h
   z

(cid:12)(cid:12)(cid:12)(cid:12)   f z
(cid:12)(cid:12)(cid:12)(cid:12)f z

      z
   w
      z
   w

=

=

=

=

=

another derivative

equations:
z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

(cid:12)(cid:12)(cid:12)(cid:12)   f o

(cid:12)(cid:12)(cid:12)(cid:12)f o

(cid:12)(cid:12)(cid:12)(cid:12)f q

      q
   l

(cid:12)(cid:12)(cid:12)(cid:12)f l

      l
   v

=

   o
   q

   o
   v

a computational graph

equations:

z     rm = w xi + b
h     rm = g(z)
l     rk = v h +   
q     rk = ls(l)
o     r =    qyi

derivatives:

(cid:12)(cid:12)(cid:12)(cid:12)   f o
(cid:12)(cid:12)(cid:12)(cid:12)   f o

   o
   v

   o
   w

(cid:12)(cid:12)(cid:12)(cid:12)f o
(cid:12)(cid:12)(cid:12)(cid:12)f o

(cid:12)(cid:12)(cid:12)(cid:12)f q
(cid:12)(cid:12)(cid:12)(cid:12)f q

(cid:12)(cid:12)(cid:12)(cid:12)f l
(cid:12)(cid:12)(cid:12)(cid:12)f l

      l
   v
      l
   h

      q
   l
      q
   l

(cid:12)(cid:12)(cid:12)(cid:12)f h

      h
   z

(cid:12)(cid:12)(cid:12)(cid:12)f z

      z
   w

=

=

   o
   q

   o
   q

overview

(cid:73) introduction
(cid:73) the chain rule
(cid:73) derivatives in a single-layer neural network
(cid:73) computational graphs
(cid:73) id26 in computational graphs
(cid:73) justi   cation for id26

computational graphs: a formal de   nition

a computational graph consists of:

(cid:73) an integer n specifying the number of vertices in the graph.

an integer l < n specifying the number of leaves in the
graph. vertices 1 . . . l are leaves in the graph. vertex n is a
special    output    vertex.

(cid:73) a set of directed edges e. each member of e is an ordered
pair (j, i) where j     {1 . . . n}, i     {(l + 1) . . . n}, and i > j.
for any i we de   ne   (i) to be the set of parents of i in the
graph:

  (i) = {j : (j, i)     e}

computational graphs (continued)

(cid:73) a variable ui     rdi is associated with each vertex in the

graph. here di for i = 1 . . . n speci   es the dimensionality of
ui. we assume dn = 1, hence the output variable is a scalar.
(cid:73) a function f i is associated with each non-leaf vertex in the
graph (i     {(l + 1) . . . n}). the function maps a vector ai
de   ned as

ai = (cid:104)uj|j       (i)(cid:105)

to a vector f i(ai)     rdi

an example

(cid:73) de   ne n = 4, l = 2
(cid:73) de   ne di = 1 for all i (all variables are scalars)
(cid:73) de   ne e = {(1, 3), (2, 3), (2, 4), (3, 4)}
(cid:73) de   ne

f 3(u1, u2) = u1 + u2
f 4(u2, u3) = u2    u3

two questions

(cid:73) note that the computational graph de   nes a function, which
we call   f n, from the values of the leaf variables to the output
variable:

un =   f n(u1 . . . ul)

(cid:73) given a computational graph, and values for the leaf variables

u1 . . . ul:

1. how do we compute the output un?
2. how do we compute the partial derivatives

(cid:12)(cid:12)(cid:12)(cid:12)   f n

   un
   ui

for all i     {1 . . . l} ?

forward computation

input: values for leaf variables u1 . . . ul
algorithm:

(cid:73) for i = (l + 1) . . . n

where

ui = f i(ai)

ai = (cid:104)uj|j       (i)(cid:105)

an example

(cid:73) de   ne n = 4, l = 2
(cid:73) de   ne di = 1 for all i (all variables are scalars)
(cid:73) de   ne e = {(1, 3), (2, 3), (2, 4), (3, 4)}
(cid:73) de   ne

f 3(u1, u2) = u1 + u2
f 4(u2, u3) = u2    u3

de   ning and calculating derivatives

(cid:73) for any k     {(l + 1) . . . n}, there is a function   f k such that

uk =   f k(u1, u2, . . . ul)

(cid:73) we want to calculate

for j = 1 . . . l

(cid:12)(cid:12)(cid:12)(cid:12)   f n

u1...ul

   un
   uj

computational graphs (continued)

(cid:73) a function j j   i is associated with each edge (j, i)     e. the

function maps a vector ai de   ned as

ai = (cid:104)uj|j       (i)(cid:105)

to a matrix j j   i(ai)     rdi  dj .

j j   i(ai) =

   f i(ai)

   uj =

   ui
   uj

(cid:12)(cid:12)(cid:12)(cid:12)f i

ai

forward pass

input: values for leaf variables u1 . . . ul
algorithm:

(cid:73) for i = (l + 1) . . . n

ai = (cid:104)uj|j       (i)(cid:105)

ui = f i(ai)

backward pass

(cid:73) pn = 1
(cid:73) for j = (n     1) . . . 1:

pj =

(cid:88)

i:(j,i)   e

pij j   i(ai)

(cid:73) output: pi for i = 1 . . . l satisfying

(cid:12)(cid:12)(cid:12)(cid:12)   f n

u1...ul

pi =

   o
   ui

an example

pn = 1
for j = (n     1) . . . 1:

(cid:88)

pj =

i:(j,i)   e

pij j   i(ai)

overview

(cid:73) introduction
(cid:73) the chain rule
(cid:73) derivatives in a single-layer neural network
(cid:73) computational graphs
(cid:73) id26 in computational graphs
(cid:73) justi   cation for id26

products of jacobians over paths in the graph

(cid:73) a directed path between vertices j and k is a sequence of

edges (i1, i2), (i2, i3), . . . (in   1, in) with n     2 such that each
edge is in e, and i1 = j, and in = k.

(cid:73) for any j, k, we write p(j, k) to denote the set of all

directed paths between j and k

(cid:73) for convenience we de   ne da   b = j a   b(ab) for all edges

(a, b).

(cid:73) theorem: for any j     {1 . . . l}, k     {(l + 1) . . . n},

(cid:12)(cid:12)(cid:12)(cid:12)   f k

   uk
   uj

=

(cid:88)

(cid:89)

p   p(j,k)

(a,b)   p

da   b

an example

(cid:12)(cid:12)(cid:12)(cid:12)   f k

   uk
   uj

(cid:88)

(cid:89)

p   p(j,k)

(a,b)   p

=

da   b

proof sketch

(cid:73) for any j, j(cid:48), k, we write p(j, j(cid:48), k) to denote the set of all
directed paths between j and k such that the last edge in the
sequence is (j(cid:48), k).

(cid:73) proof sketch: by induction over the graph. by the chain rule

we have

   uk
   uj

(cid:12)(cid:12)(cid:12)(cid:12)   f k

(cid:88)
(cid:88)
(cid:88)
(cid:88)

j(cid:48):(j(cid:48),k)   e

j(cid:48):(j(cid:48),k)   e

j(cid:48):(j(cid:48),k)   e

   uj

(cid:12)(cid:12)(cid:12)(cid:12)   f j(cid:48)
dj(cid:48)   k       uj(cid:48)
dj(cid:48)   k    (cid:88)
(cid:88)
(cid:89)
(cid:89)

p   p(j,j(cid:48),k)
da   b

p   p(j,j(cid:48))

(a,b)   p

p   p(j,k)

(a,b)   p

(cid:89)

(a,b)   p
da   b

=

=

=

=

da   b

backward pass

(cid:73) pn = 1
(cid:73) for j = (n     1) . . . 1:

pj =

(cid:73) output: pi for i = 1 . . . l satisfying

pidj   i

i:(j,i)   e

(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)   f o

   o
   ui

pi =

u1,u2,...ul

correctness of the backward pass

(cid:73) theorem: for all pi we have

(cid:88)

pi =

da   b

(a,b)   p
it follows that for any i     {1 . . . l},

p   p(i,n)

(cid:89)
(cid:12)(cid:12)(cid:12)(cid:12)   f n

pi =

   un
   ui

proof

(cid:73) theorem: for all pi we have

(cid:88)

(cid:89)

p   p(i,n)

(a,b)   p

pi =

da   b

(cid:73) proof sketch: by induction on

i = n, i = (n     1), i = (n     2), . . . i = 1. for i = n we have
pn = 1 so the proposition is true. for j = (n     1) . . . 1 we
have

(cid:88)
(cid:88)

i:(j,i)   e

i:(j,i)   e

pj =

=

pidj   i

       (cid:88)

p   p(i,n)

       dj   i =

da   b

(cid:89)

(a,b)   p

(cid:88)

(cid:89)

p   p(j,n)

(a,b)   p

da   b

