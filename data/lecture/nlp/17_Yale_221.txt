nlp
introduction to nlp
id48
markov models
sequence of random variables that aren   t independent
examples 
weather reports
text
stock market numbers
properties
limited horizon:
p(xt+1 = sk|x1,   ,xt) = p(xt+1 = sk|xt)
time invariant (stationary)
p(xt+1 = sk|xt) = p(x2=sk|x1)
definition: in terms of a transition matrix a and initial state probabilities    .

example
start
visible mm
p(x1,   xt) = p(x1) p(x2|x1) p(x3|x1,x2)     p(xt|x1,   ,xt-1)
              
                    = p(x1) p(x2|x1) p(x3|x2)     p(xt|xt-1)



p(d, a, b) = p(x1=d) p(x2=a|x1=d) p(x3=b|x2=a)
              = 1.0 x 0.7 x 0.8
              = 0.56
hidden mm
motivation
observing a sequence of symbols
the sequence of states that led to the generation of the symbols is hidden
the states correspond to hidden (latent) variables
definition
q = states
o = observations, drawn from a vocabulary
q0,qf = special (start, final) states
a = state transition probabilities
b = symbol emission probabilities
    = initial state probabilities
   = (a,b,   ) = complete probabilistic model
hidden mm
uses
id52
id103
gene sequencing


hidden markov model (id48)
can be used to model state sequences and observation sequences
example:
p(s,w) =     i p(si|si-1)p(wi|si)
generative algorithm
pick start state from     
for t = 1..t
move to another state based on a
emit an observation based on b
state transition probabilities
emission probabilities
p(ot=k|xt=si,xt+1=sj) = bijk
all parameters of the model
initial
p(g|start) = 1.0, p(h|start) = 0.0
transition
p(g|g) = 0.8, p(g|h) = 0.6, p(h|g) = 0.2, p(h|h) = 0.4
emission
p(x|g) = 0.7, p(y|g) = 0.2, p(z|g) = 0.1
p(x|h) = 0.3, p(y|h) = 0.5, p(z|h) = 0.2
observation sequence    yz   
starting in state g (or h), p(yz) = ?
possible sequences of states:
gg
gh
hg
hh
p(yz) = p(yz|gg) + p(yz|gh) + p(yz|hg) + p(yz|hh) =
	= .8 x .2 x .8 x .1 
	+ .8 x .2 x .2 x .2
	+ .2 x .5 x .4 x .2
	+ .2 x .5 x .6 x .1
	= .0128+.0064+.0080+.0060 =.0332
states and transitions
an id48 is essentially a weighted finite-state transducer
the states encode the most recent history
the transitions encode likely sequences of states
e.g., adj-noun or noun-verb 
or perhaps art-adj-noun
use id113 to estimate the probabilities
another way to think of an id48
it   s a natural extension of na  ve bayes to sequences
emissions
estimating the emission probabilities
harder than transition probabilities
there may be novel uses of word/pos combinations
suggestions
it is possible to use standard smoothing
as well as heuristics (e.g., based on the spelling of the words)
sequence of observations
the observer can only see the emitted symbols
observation likelihood
given the observation sequence s and the model     = (a,b,   ), what is the id203 p(s|   ) that the sequence was generated by that model.
being able to compute the id203 of the observations sequence turns the id48 into a language model

tasks with id48
given     = (a,b,   ), find p(o|   )
uses the forward algorithm
given o,    , find (x1,   xt+1)
uses the viterbi algorithm
given o and a space of all possible    1..m, find model    i that best describes the observations
uses expectation-maximization

id136
find the most likely sequence of tags, given the sequence of words
t* = argmaxt p(t|w)
given the model   , it is possible to compute p (t|w) for all values of t
in practice, there are way too many combinations
greedy search
id125 
one possible solution
uses partial hypotheses
at each state, only keep the k best hypotheses so far
may not work
viterbi algorithm
find the best path up to observation i and state s
characteristics
uses id145
memoization
backpointers

id48 trellis
id48 trellis
p(g,t=1)
p(g,t=1) = 
p(start) x p(g|start) x p(y|g)
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
p(h,t=1)
p(h,t=1) = 
p(start) x p(h|start) x p(y|h)
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
p(h,t=2) = 
max (p(g,t=1) x p(h|g) x p(z|h),
	 p(h,t=1) x p(h|h) x p(z|h))
p(h,t=2)
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
p(h,t=2)
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
p(end,t=3)
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
p(end,t=3)
p(end,t=3) = 
max (p(g,t=2) x p(end|g),
	 p(h,t=2) x p(end|h))
start
end
h
h
h
h
g
g
g
g
y
z
start
start
start
end
end
end
id48 trellis
.
p(end,t=3)
p(end,t=3) = best score for the sequence

use the backpointers to find the sequence of states.
p(end,t=3) = 
max (p(g,t=2) x p(end|g),
	 p(h,t=2) x p(end|h))
some observations
advantages of id48s
relatively high accuracy
easy to train
higher-order id48
the previous example was about bigram id48s
how can you modify it to work with trigrams?
how to compute p(o)
viterbi was used to find the most likely sequence of states that matches the observation
what if we want to find all sequences that match the observation
we can add their probabilities (because they are mutually exclusive) to form the id203 of the observation
this is done using the forward algorithm
the forward algorithm
very similar to viterbi
instead of max we use sum

source: wikipedia
nlp
