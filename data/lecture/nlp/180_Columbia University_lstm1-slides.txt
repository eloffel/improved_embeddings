recurrent networks, and lstms, for nlp

michael collins, columbia university

representing sequences

(cid:73) often we want to map some sequence x[1:n] = x1 . . . xn to a

label y or a distribution p(y|x[1:n])

(cid:73) examples:

(cid:73) id38: x[1:n] is    rst n words in a document, y

is the (n + 1)   th word

(cid:73) id31: x[1:n] is a sentence (or document), y is

label indicating whether the sentence is
positive/neutral/negative about a particular topic (e.g., a
particular restaurant)

(cid:73) machine translation: x[1:n] is a source-language sentence, y
is a target language sentence (or the    rst word in the target
language sentence)

representing sequences (continued)

(cid:73) slightly more generally: map a sequence x[1:n] and a
position i     {1 . . . n} to a label y or a distribution
p(y|x[1:n], i)
(cid:73) examples:

(cid:73) tagging: x[1:n] is a sentence, i is a position in the sentence,

y is the tag for position i
(cid:73) id33: x[1:n] is a sentence, i is a position in
the sentence, y     {1 . . . n}, y (cid:54)= i is the head for word xi in
the dependency parse

a simple recurrent network

inputs: a sequence x1 . . . xn where each xj     rd. a label
y     {1 . . . k}. an integer m de   ning size of hidden dimension.
parameters w hh     rm  m, w hx     rm  d, bh     rm, h0     rm,
v     rk  m,        rk. transfer function g : rm     rm.
de   nitions:

   = {w hh, w hx, bh, h0}

r(x(t), h(t   1);   ) = g(w hxx(t) + w hhh(t   1) + bh)

computational graph:

(cid:73) for t = 1 . . . n

(cid:73) h(t) = r(x(t), h(t   1);   )

(cid:73) l = v h(n) +   , q = ls(l), o =    qy

the computational graph

a problem in training: exploding and vanishing
gradients

(cid:73) calculation of gradients involves multiplication of long chains

of jacobians

(cid:73) this leads to exploding and vanishing gradients

lstms (long short-term memory units)

(cid:73) old de   nitions of the recurrent update:

   = {w hh, w hx, bh, h0}

r(x(t), h(t   1);   ) = g(w hxx(t) + w hhh(t   1) + bh)
(cid:73) lstms give an alternative de   nition of r(x(t), h(t   1);   ).

de   nition of sigmoid function, element-wise
product

(cid:73) given any integer d     1,   d : rd     rd is the function that
maps a vector v to a vector   d(v) such that for i = 1 . . . d,

  d
i (v) =

evi

1 + evi

(cid:73) given vectors a     rd and b     rd, c = a (cid:12) b has components

ci = ai    bi

for i = 1 . . . d

lstm equations (from ilya sutskever, phd thesis)

maintain st,   st, ht as hidden state at position t. st is memory,
intuitively allows long-term memory. the function
st,   st, ht = lstm(xt, st   1,   st   1, ht   1;   ) is de   ned as:

ut = concat(ht   1, xt,   st   1)
ht = g(w hut + bh)
it = g(w iut + bi)

(   input   )

(hidden state)

  t =   (w   ut + b  )
ot =   (w out + bo)
f t =   (w f ut + bf )

(   input gate   )

(   output gate   )

(   forget gate   )

st = st   1 (cid:12) f t + it (cid:12)   t
  st = st (cid:12) ot

forget and input gates control update of memory

output gate controls information that can leave the unit

an lstm-based recurrent network

inputs: a sequence x1 . . . xn where each xj     rd. a label
y     {1 . . . k}.

computational graph:

(cid:73) h(0), s(0),   s(0) are set to some inital values.
(cid:73) for t = 1 . . . n

(cid:73) s(t),   s(t), h(t) = lstm(x(t), s(t   1),   s(t   1), h(t   1);   )

(cid:73) l = v lhh(n) + v ls  s(n) +   , q = ls(l), o =    qy

the computational graph

an lstm-based recurrent network for tagging

inputs: a sequence x1 . . . xn where each xj     rd. a sequence
y1 . . . yn of tags.

computational graph:

(cid:73) h(0), s(0),   s(0) are set to some inital values.
(cid:73) for t = 1 . . . n

(cid:73) s(t),   s(t), h(t) = lstm(x(t), s(t   1),   s(t   1), h(t   1);   )

(cid:73) for t = 1 . . . n

(cid:73) lt = v    concat(h(t),   s(t)) +   , qt = ls(lt), ot =    qyt

(cid:73) o =(cid:80)n

t=1 ot

the computational graph

a bi-directional lstm (bi-lstm) for tagging
inputs: a sequence x1 . . . xn where each xj     rd. a sequence
y1 . . . yn of tags.
de   nitions:   f and   b are parameters of a forward and backward
lstm.
computational graph:

(cid:73) h(0), s(0),   s(0),   (n+1),   (n+1),     (n+1) are set to some inital values.
(cid:73) for t = 1 . . . n

(cid:73) s(t),   s(t), h(t) = lstm(x(t), s(t   1),   s(t   1), h(t   1);   f )

(cid:73) for t = n . . . 1

(cid:73)   (t),     (t),   (t) = lstm(x(t),   (t+1),     (t+1),   (t+1);   b)

(cid:73) lt = v    concat(h(t),   s(t),   (t),     t) +   , qt = ls(lt),

(cid:73) for t = 1 . . . n

(cid:73) o =(cid:80)n

ot =    qyt
t=1 ot

the computational graph

results on id38

(cid:73) results from one billion word benchmark for measuring

progress in statistical id38, ciprian chelba,
tomas mikolov, mike schuster, qi ge, thorsten brants.

results on id33

(cid:73) deep bia   ne attention for neural id33,

dozat and manning.

(cid:73) uses a bidirectional lstm to represent each word
(cid:73) uses lstm representations to predict head for each word in

the sentence

(cid:73) unlabeled dependency accuracy: 95.75%

conclusions

(cid:73) recurrent units map input sequences x1 . . . xn to

representations h1 . . . hn. the vector hn can be used to
predict a label for the entire sentence. each vector hi for
i = 1 . . . n can be used to make a prediction for position i
(cid:73) lstms are recurrent units that make use of more involved

recurrent updates. they maintain a    memory    state.
empirically they perform extremely well

(cid:73) bi-directional lstms allow representation of both the

information before and after a position i in the sentence
(cid:73) many applications: id38, tagging, parsing,
id103, we will soon see machine translation

