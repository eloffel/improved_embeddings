morphology parsing
informatics 2a: lecture 14

shay cohen

school of informatics
university of edinburgh
scohen@inf.ed.ac.uk

20 october 2017

1 / 31

formal languages

2 / 31

natural languages

3 / 31

essential epistemology

and

deals in...

exact sciences empirical sciences
axioms
facts and theories
theorems
truth is ...
forever
examples... maths,
theory

temporary
physics, biology,
linguistics

cs

engineering
artifacts

it works!
applied
and nlp

cs

4 / 31

1 morphology parsing: the problem

2 finite-state transducers

3 finite-state transducers

4 fsts for morphology parsing and generation

(this lecture is based on jurafsky & martin chapter 3, sections
1   7.)

5 / 31

morphology in other languages

morphology is the study of the structure of words

english has relatively impoverished morphology.

languages with rich morphology: turkish, arabic, hungarian,
korean, and many more (actually, english is rather unique in
having relatively simple morphology)

for example, turkish is an agglutinative language, and words are
constructed by concatenating morphemes together without
changing them much:
evlerinizden:    from your houses    (morphemes: ev-ler-iniz-den)

this lecture will mostly discuss how to build an english
morphological analyser.

6 / 31

morphology

stems and a   xes (pre   xes, su   xes, in   xes and circum   xes)
combine together

four methods to combine them:

in   ection (stem + grammar a   x) - word does not change its
grammatical class (walk     walking)
derivation (stem + grammar a   x) - word changes its
grammatical form (computerize     computerization)
compounding (stems together) - doghouse

cliticization - i   ve, we   re, he   s

morphology can be concatenative or non-concatenative (e.g.
templatic morphology as in arabic)

7 / 31

wals - su   xing versus pre   xing

8 / 31

examples: in   ection in di   erent languages

in english, nouns are in   ected for number, while verbs are in   ected
for person and tense. number: book / books person: you read /
she reads tense: we talk / we talked

in german, nouns are in   ected for number and case, e.g house
becomes:

nominative
genitive
dative
accusative

singular
das haus
des hauses
dem haus / dem hause
das haus

plural
die h  auser
der h  auser
den h  ausern
die h  auser

in spanish, in   ection depends on gender, e.g. el sol / la luna.

in luganda, nouns have ten genders, each with di   erent in   ections
for singular/ plural!

9 / 31

examples: agglutination and compounding

ostoskeskuksessa

ostos#keskus+n+sg+loc:in

shopping#center+n+sg+loc:in
in the shopping center (finnish)

qangatasuukkuvimmuuriaqalaaqtunga

   i   ll have to go to the airport    (inuktitut)

avrupallatramadklarmzdanmsnzcasna

   as if you are reportedly of those of ours that we were unable to

europeanize    (turkish)

in the most extreme examples, the meaning of the word is the

meaning of a sentence!

10 / 31

morphological parsing: the problem

english has concatenative morphology. words can be made up of a
main stem (carrying the basic dictionary meaning) plus one or
more a   xes carrying grammatical information. e.g.:

surface form:
lexical form:

cats
cat+n+pl walk+v+prespart

walking

smoothest
smooth+adj+sup

morphological parsing is the problem of extracting the lexical form
from the surface form. (for speech processing, this includes
identifying the word boundaries.)

we should take account of:

irregular forms (e.g. goose     geese)
systematic rules (e.g.    e    inserted before su   x    s    after
s,x,z,ch,sh:

fox     foxes, watch     watches)

11 / 31

why bother?

any nlp tasks involving grammatical parsing will typically
involve morphology parsing as a prerequisite.

search engines: e.g. a search for    fox    should return
documents containing    foxes   , and vice versa.

even a humble task like spell checking can bene   t: e.g. is
   walking    a possible word form?

but why not just list all derived forms separately in our wordlist
(e.g. walk, walks, walked, walking)?

might be ok for english, but not for a morphologically rich
language     e.g. in turkish, can pile up to 10 su   xes on a
verb stem, leading to 40,000 possible forms for some verbs!

even for english, morphological parsing makes
adding/learning new words easier.

in speech processing, word breaks aren   t known in advance.

12 / 31

how expressive is morphology?

morphemes are tacked together in a rather    regular    way.

this means that    nite-state machines are a good way to model
morphology. there is no need for    unbounded memory    to model
it (there are no long range dependencies).

this is as opposed to syntax, the study of the order of words in a
sentence, which we will learn about in another lecture

13 / 31

parsing and generation

parsing here means going from the surface to the lexical form.
e.g. foxes     fox +n +pl.
generation is the opposite process: fox +n +pl     foxes. it   s
helpful to consider these two processes together.

either way, it   s often useful to proceed via an intermediate form,
corresponding to an analysis in terms of morphemes (= minimal
meaningful units) before orthographic rules are applied.

surface form:
intermediate form:
lexical form:

foxes
fox    s #
fox +n +pl

(   means morpheme boundary, # means word boundary.)

n.b. the translation between surface and intermediate form is
exactly the same if    foxes    is a 3rd person singular verb!

14 / 31

finite-state transducers

we can consider  -nfas (over an alphabet   ) in which transitions
may also (optionally) produce output symbols (over a possibly
di   erent alphabet   ).
e.g. consider the following machine with input alphabet {a, b} and
output alphabet {0, 1}:

such a thing is called a    nite state transducer.
in e   ect, it speci   es a (possibly multi-valued) translation from one
regular language to another.

15 / 31

finite-state transducers

we can consider  -nfas (over an alphabet   ) in which transitions
may also (optionally) produce output symbols (over a possibly
di   erent alphabet   ).
e.g. consider the following machine with input alphabet {a, b} and
output alphabet {0, 1}:

such a thing is called a    nite state transducer.
in e   ect, it speci   es a (possibly multi-valued) translation from one
regular language to another.

16 / 31

a:0a:1b:  b:  quick exercise

what output will this produce, given the input aabaaabbab?

1 001110

2 001111

3 0011101

4 more than one output is possible.

17 / 31

a:0a:1b:  b:  formal de   nition

formally, a    nite state transducer t with inputs from    and
outputs from    consists of:

sets q, s, f as in ordinary nfas,
a transition relation         q    (     { })    (     { })    q

from this, one can de   ne a many-step transition relation
          q                      q, where (q, x, y , q(cid:48))           means    starting
from state q, the input string x can be translated into the output
string y , ending up in state q(cid:48).    (details omitted.)
note that a    nite state transducer can be run in either direction!
from t as above, we can obtain another transducer t just by
swapping the roles of inputs and outputs.

18 / 31

fsts and fsas

formally, a    nite state transducer t with inputs from    and
outputs from    consists of:

sets q, s, f as in ordinary nfas,
a transition relation         q    (     { })    (     { })    q

reminder: formally, an nfa with alphabet    consists of:

a    nite set q of states.
a transition relation         q    (     { })    q,
a set s     q of possible starting states,
a set f     q of accepting states.

19 / 31

stage 1: from lexical to intermediate form

consider the problem of translating a lexical form like    fox+n+pl   
into an intermediate form like    fox    s #    , taking account of
irregular forms like goose/geese.

we can do this with a transducer of the following schematic form:

we treat each of +n, +sg, +pl as a single symbol.
the    transition    labelled +pl :   s# abbreviates three transitions:
+pl :   ,

  : #.

  : s,

20 / 31

+n:  +n:  +n:  regular noun(copied to output)(copied to output)irregular nounirregular noun(replaced by plural)+pl : ^s#+sg : #+sg : #+pl : #the stage 1 transducer    eshed out

the left hand part of the preceding diagram is an abbreviation for
something like this (only a small sample shown):

here, for simplicity, a single label u abbreviates u : u.

21 / 31

o:eo:eegoosesfcatoxstage 1 in full

22 / 31

stage 2: from intermediate to surface form

to convert a sequence of morphemes to surface form, we apply a
number of orthographic rules such as the following.

e-insertion: insert e after s,z,x,ch,sh before a word-   nal
morpheme -s.

(fox     foxes)

e-deletion: delete e before a su   x beginning with e,i.
(love     loving)
consonant doubling: single consonants b,s,g,k,l,m,n,p,r,s,t,v
are doubled before su   x -ed or -ing.

(beg     begged)

we shall consider a simpli   ed form of e-insertion, ignoring ch,sh.

(note that this rule is oblivious to whether -s is a plural noun su   x
or a 3rd person verb su   x.)

23 / 31

a transducer for e-insertion (adapted from j+m)

here ? may stand for any symbol except z,s,x,  ,#.
(treat # as a    visible space character   .)

at a morpheme boundary following z,s,x, we arrive in state 2.
if the ensuing input sequence is s#, our only option is to go via
states 3 and 4. note that there   s no #-transition out of state 5.

state 5 allows e.g.    ex  service  men#    to be translated to
   exservicemen   .

24 / 31

^:  ^:  ^:  ?sz,x:e  s14523#0   ?#??#z,s,xz,s,xz,s,x0?,z,s,xputting it all together

fsts can be cascaded: output from one can be input to another.

to go from lexical to surface form, use    stage 1    transducer
followed by a bunch of orthographic rule transducers like the
above. (made more e   cient by back-end compilation into one
single transducer.)

the results of this generation process are typically deterministic
(each lexical form gives a unique surface form), even though our
transducers make use of non-determinism along the way.

running the same cascade backwards lets us do parsing (surface to
lexical form). because of ambiguity, this process is frequently
non-deterministic: e.g.    foxes    might be analysed as fox+n+pl or
fox+v+pres+3sg.

such ambiguities are not resolved by morphological parsing itself:
left to a later processing stage.

25 / 31

quick exercise 2

apply this backwards to translate from surface to int. form.

starting from state 0, how many sequences of transitions are
compatible with the input string    asses    ?

1 1
2 2
3 3
4 4
5 more than 4

26 / 31

^:  ^:  ^:  ?sz,x:e  s14523#0   ?#??#z,s,xz,s,xz,s,x0?,z,s,xsolution

on the input string    asses   , 10 transition sequences are possible!

output ass  s

output ass  es

s    1 s    1      2 e    3 s    4,
s    1 s    1      2 e    0(cid:48)
s    1,
s    1 s    1 e    0(cid:48)
s    1,
s    1      2 s    5      2 e    3 s    4,
s    1      2 s    5      2 e    0(cid:48)
s    1,
s    1,
s    1      2 s    5 e    0(cid:48)

0 a    0(cid:48)
0 a    0(cid:48)
0 a    0(cid:48)
0 a    0(cid:48)
0 a    0(cid:48)
0 a    0(cid:48)
four of these can also be followed by 1      2 (output   ).

output as  s  es

output as  ses

output as  s  s

output asses

27 / 31

^:  ^:  ^:  ?sz,x:e  s14523#0   ?#??#z,s,xz,s,xz,s,x0?,z,s,xthe porter stemmer

lexicon can be quite large with    nite state transducers

sometimes need to extract the stem in a very e   cient fashion
(such as in ir)

the porter stemmer: a lexicon-free method for getting the stem of
a given word
ational     ate (e.g., relation     relate)
ing       if stem comtains a vowel (e.g. motoring     motor)
sses     ss (e.g., grasses     grass)
makes errors:
organization     organ
doing     doe
numerical     numerous
policy     police

28 / 31

current methods for morphological analysis

a vibrant area of study

mostly done by learning from data, just like many other nlp
problems. two main paradigms: unsupervised morphological
parsing and supervised one.

nlp solvers are not perfect! they can make mistakes. sometimes
ambiguity can   t even be resolved. but, for english, morphological
analysis is highly accurate. with other languages, there is still a
long way to go.

29 / 31

finite state transducers in nlp

one of the basic tools that is used for many applications

id103

machine translation

part-of-speech tagging

... and many more

30 / 31

next class

part-of-speech tagging:

what are parts of speech?

what are they useful for?

zipf   s law and the ambiguity of id52

one problem nlp solves really well (... for english)

31 / 31

