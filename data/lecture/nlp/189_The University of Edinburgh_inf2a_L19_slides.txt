phrase structure and parsing as search

informatics 2a: lecture 19

shay cohen

school of informatics
university of edinburgh

30 october 2017

1 / 66

last class

part-of-speech tagging and its applications

the use of id48 for id52

the viterbi algorithm

weighted    nite-state automata and their use in applications like
id103, machine translation (similarly: optical
character recognition, predictive text, poetry generation ...)

2 / 66

1 phrase structure

heads and phrases
desirable properties of a grammar
a fragment of english

2 grammars and parsing

recursion
structural ambiguity
recursive descent parsing
id132

3 / 66

computing meaning

a well-studied, di   cult, and un-
solved problem.

fortunately, we know enough to
have made partial progress (wat-
son won).

over the next few weeks, we will work up to the study of systems
that can assign logical forms that mathematically state the
meaning of a sentence, so that they can be processed by machines.

our    rst stop will be natural language syntax.

4 / 66

natural language syntax

syntax provides the sca   olding for semantic composition.

the brown dog on the mat saw the striped cat through the
window.

5 / 66

natural language syntax

syntax provides the sca   olding for semantic composition.

the brown dog on the mat saw the striped cat through the
window.
the brown cat saw the striped dog through the window on the
mat.

5 / 66

natural language syntax

syntax provides the sca   olding for semantic composition.

the brown dog on the mat saw the striped cat through the
window.
the brown cat saw the striped dog through the window on the
mat.

do the two sentences above mean the same thing? what is the
process by which you computed their meanings?

5 / 66

constituents

words in a sentence often form groupings that can combine with
other units to produce meaning. these groupings, called
consituents can often be identi   ed by substitution tests (much
like parts of speech!)

kim [read a book], [gave it to sandy], and [left]

you said i should read the book and [read it] i did.

kim read [a very interesting book about grammar].

6 / 66

heads and phrases

noun (n): noun phrase (np)
adjective (a): adjective phrase (ap)
verb (v): verb phrase (vp)
preposition (p): prepositional phrase (pp)

so far we have looked at terminals (words or pos tags).

today, we   ll look at non-terminals, which correspond to
phrases.

the part of speech that a word belongs to is closely linked to
the type of constituent that it is associated with.

in a x-phrase (eg np), the key occurrence of x (eg n) is
called the head, and controls how the phrase interacts (both
syntactically and semantically) with the rest of the sentence.

in english, the head tends to appear in the middle of a phrase.

7 / 66

constituents have structure

english nps are commonly of the form:
(det) adj* noun (pp | relclause)*
np: the angry duck that tried to bite me,

vps are commonly of the form:

(aux) adv* verb arg* adjunct*
arg     np | pp
adjunct     pp | advp | . . .
vp: usually eats artichokes for dinner,

.

in japanese, korean, hindi, urdu, and other head-   nal languages,
the head is at the end of its associated phrase.

in irish, welsh, scots gaelic and other head-initial languages, the
head is at the beginning of its associated phrase.

8 / 66

constituents have structure

english nps are commonly of the form:
(det) adj* noun (pp | relclause)*
np: the angry duck that tried to bite me, head: duck.

vps are commonly of the form:

(aux) adv* verb arg* adjunct*
arg     np | pp
adjunct     pp | advp | . . .
vp: usually eats artichokes for dinner,

.

in japanese, korean, hindi, urdu, and other head-   nal languages,
the head is at the end of its associated phrase.

in irish, welsh, scots gaelic and other head-initial languages, the
head is at the beginning of its associated phrase.

8 / 66

constituents have structure

english nps are commonly of the form:
(det) adj* noun (pp | relclause)*
np: the angry duck that tried to bite me, head: duck.

vps are commonly of the form:

(aux) adv* verb arg* adjunct*
arg     np | pp
adjunct     pp | advp | . . .
vp: usually eats artichokes for dinner, head: eat.

in japanese, korean, hindi, urdu, and other head-   nal languages,
the head is at the end of its associated phrase.

in irish, welsh, scots gaelic and other head-initial languages, the
head is at the beginning of its associated phrase.

8 / 66

desirable properties of a grammar

chomsky speci   ed two properties that make a grammar
   interesting and satisfying   :

it should be a    nite speci   cation of the strings of the
language, rather than a list of its sentences.

it should be revealing, in allowing strings to be associated
with meaning (semantics) in a systematic way.

we can add another desirable property:

it should capture structural and distributional properties of
the language. (e.g. where heads of phrases are located; how a
sentence transforms into a question; which phrases can    oat
around the sentence.)

9 / 66

desirable properties of a grammar

context-free grammars (id18s) provide a pretty good
approximation.

some features of nls are more easily captured using mildly
context-sensitive grammars, as well see later in the course.

there are also more modern grammar formalisms that better
capture structural and distributional properties of human
languages. (e.g. id35.)

but ll(1) grammars and the like de   nitely aren   t enough for
nls. even if we could make a nl grammar ll(1), we
wouldn   t want to: this would arti   cially suppress ambiguities,
and would often mutilate the    natural    structure of sentences.

10 / 66

a tiny fragment of english

let   s say we want to capture in a grammar the structural and
distributional properties that give rise to sentences like:

a duck walked in the park.
the man walked with a duck.
you made a duck.
you made her duck.
a man with a telescope saw you.
a man saw you with a telescope.
you saw a man with a telescope.

np,v,pp
np,v,pp
pro,v,np
? pro,v,np
np,pp,v,pro
np,v,pro,pp
pro,v,np,pp

we want to write grammatical rules that generate these phrase
structures, and lexical rules that generate the words appearing in
them.

11 / 66

a tiny fragment of english

let   s say we want to capture in a grammar the structural and
distributional properties that give rise to sentences like:

a duck walked in the park.
the man walked with a duck.
you made a duck.
you made her duck.
a man with a telescope saw you.
a man saw you with a telescope.
you saw a man with a telescope.

np,v,pp
np,v,pp
pro,v,np
? pro,v,np
np,pp,v,pro
np,v,pro,pp
pro,v,np,pp

we want to write grammatical rules that generate these phrase
structures, and lexical rules that generate the words appearing in
them.

11 / 66

a tiny fragment of english

let   s say we want to capture in a grammar the structural and
distributional properties that give rise to sentences like:

a duck walked in the park.
the man walked with a duck.
you made a duck.
you made her duck.
a man with a telescope saw you.
a man saw you with a telescope.
you saw a man with a telescope.

np,v,pp
np,v,pp
pro,v,np
? pro,v,np
np,pp,v,pro
np,v,pro,pp
pro,v,np,pp

we want to write grammatical rules that generate these phrase
structures, and lexical rules that generate the words appearing in
them.

11 / 66

a tiny fragment of english

let   s say we want to capture in a grammar the structural and
distributional properties that give rise to sentences like:

a duck walked in the park.
the man walked with a duck.
you made a duck.
you made her duck.
a man with a telescope saw you.
a man saw you with a telescope.
you saw a man with a telescope.

np,v,pp
np,v,pp
pro,v,np
? pro,v,np
np,pp,v,pro
np,v,pro,pp
pro,v,np,pp

we want to write grammatical rules that generate these phrase
structures, and lexical rules that generate the words appearing in
them.

11 / 66

a tiny fragment of english

let   s say we want to capture in a grammar the structural and
distributional properties that give rise to sentences like:

a duck walked in the park.
the man walked with a duck.
you made a duck.
you made her duck.
a man with a telescope saw you.
a man saw you with a telescope.
you saw a man with a telescope.

np,v,pp
np,v,pp
pro,v,np
? pro,v,np
np,pp,v,pro
np,v,pro,pp
pro,v,np,pp

we want to write grammatical rules that generate these phrase
structures, and lexical rules that generate the words appearing in
them.

11 / 66

grammar for the tiny fragment of english

grammar g1 generates the sentences on the previous slide:

lexical rules
det     a | the | her (determiners)
n     man | park | duck | telescope (nouns)
pro     you (pronoun)
v     saw | walked | made (verbs)
prep     in | with | for (prepositions)

grammatical rules
s     np vp
np     det n
np     det n pp
np     pro
vp     v np pp
vp     v np
vp     v
pp     prep np

does g1 produce a    nite or an in   nite number of sentences?

12 / 66

recursion

recursion in a grammar makes it possible to generate an in   nite
number of sentences.

in direct recursion, a non-terminal on the lhs of a rule also
appears on its rhs. the following rules add direct recursion to g1:
vp     vp conj vp
conj     and | or

in indirect recursion, some non-terminal can be expanded (via
several steps) to a sequence of symbols containing that
non-terminal:
np     det n pp
pp     prep np

13 / 66

structural ambiguity

you saw a man with a telescope.

s

np

pro

you

v

saw

vp

np

pp

det

n

a

man

prep

np

with

det

n

a

telescope

14 / 66

structural ambiguity

you saw a man with a telescope.

s

v

saw

np

pro

you

vp

np

det

a

n

pp

man

prep

np

with

det

n

a

telescope

15 / 66

structural ambiguity

you saw a man with a telescope.

s

np

pro

you

v

saw

s

v

saw

np

pro

you

pp

np

det

n

a

telescope

vp

det

a

np

n

man

vp

np

det

n

a

man

prep

with

prep

with

pp

np

det

n

a

telescope

this illustrates attachment ambiguity: the pp can be a part of the
vp or of the np. note that there   s no pos ambiguity here.

16 / 66

structural ambiguity

you saw a man with a telescope.

s

np

pro

you

v

saw

s

v

saw

np

pro

you

pp

np

det

n

a

telescope

vp

det

a

np

n

man

vp

np

det

n

a

man

prep

with

prep

with

pp

np

det

n

a

telescope

this illustrates attachment ambiguity: the pp can be a part of the
vp or of the np. note that there   s no pos ambiguity here.

16 / 66

structural ambiguity

you saw a man with a telescope.

s

np

pro

you

v

saw

s

v

saw

np

pro

you

pp

np

det

n

a

telescope

vp

det

a

np

n

man

vp

np

det

n

a

man

prep

with

prep

with

pp

np

det

n

a

telescope

this illustrates attachment ambiguity: the pp can be a part of the
vp or of the np. note that there   s no pos ambiguity here.

16 / 66

structural ambiguity

grammar g1 only gives us one analysis of you made her duck.

np

pro

you

s

vp

v

np

made

det

n

her

duck

there is another, ditransitive (i.e., two-object) analysis of this
sentence     one that underlies the pair:

what did you make for her?
you made her duck.

17 / 66

structural ambiguity

for this alternative, g1 also needs rules like:
np     n
vp     v np np
pro     her

np

pro

you

s

vp

v

np

made

det

n

her

duck

np

pro

you

s

vp

v

np

np

made

pro

n

her

duck

in this case, the structural ambiguity is rooted in pos ambiguity.

18 / 66

structural ambiguity

there is a third analysis as well, one that underlies the pair:

what did you make her do?
you made her duck.

here, the small clause (her duck) is the direct object of a verb.

similar small clauses are possible with verbs like see, hear and
notice, but not ask, want, persuade, etc.

g1 needs a rule that requires accusative case-marking on the
subject of a small clause and no tense on its verb.:
vp     v s1
s1     np(acc) vp(untensed)
np(acc)     her | him | them

19 / 66

structural ambiguity

there is a third analysis as well, one that underlies the pair:

what did you make her do?
you made her duck. (move head or body quickly downwards)

here, the small clause (her duck) is the direct object of a verb.

similar small clauses are possible with verbs like see, hear and
notice, but not ask, want, persuade, etc.

g1 needs a rule that requires accusative case-marking on the
subject of a small clause and no tense on its verb.:
vp     v s1
s1     np(acc) vp(untensed)
np(acc)     her | him | them

19 / 66

structural ambiguity

now we have three analyses for you made her duck:

how can we compute these analyses automatically?

20 / 66

npvpsvpronpyoumadeduckdetnhernpvpsvproyoumadeducknpnpproheid56pvpsvproyoumadeducksnp(acc)hervpva fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched the train from the window with my binoculars

21 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched the train from the window with my binoculars
e

21 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

a new one?

(d)

(f)
watched the train from the window on the wall

(e)

22 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

a new one?

(d)

(f)
watched the train from the window on the wall
a

(e)

22 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched a show by net   ix about the starship

23 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched a show by net   ix about the starship
d

23 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched a show about the expedition to space

24 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched a show about the expedition to space
c

24 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched a video about the commet on my mobile

25 / 66

a fun exercise - which is the vp?

(a)

(b)

(c)

(d)

(e)

a new one?

(f)

watched a video about the commet on my mobile
b

25 / 66

parsing algorithms

a parser is an algorithm that computes a structure for an input
string given a grammar. all parsers have two fundamental
properties:

directionality: the sequence in which the structures are
constructed (e.g., top-down or bottom-up).

search strategy: the order in which the search space of
possible analyses is explored (e.g., depth-   rst, breadth-   rst).

for instance, ll(1) parsing is top-down and depth-   rst.

26 / 66

coming up: a zoo of parsing algorithms

as we   ve noted, ll(1) isn   t good enough for nl. we   ll be looking
at other parsing algorithms that work for more general id18s.

recursive descent parsers (top-down). simple and very
general, but ine   cient. other problems

shift-reduce parsers (bottom-up).

the cocke-younger-kasami algorithm (bottom up). works for
any id18 with reasonable e   ciency.

the earley algorithm (top down). chart parsing enhanced
with prediction.

27 / 66

recursive descent parsing

a recursive descent parser treats a grammar as a speci   cation of
how to break down a top-level goal into subgoals. therefore:

parser searches through the trees licensed by the grammar to
   nd the one that has the required sentence along its yield.
directionality = top-down: it starts from the start symbol of
the grammar, and works its way down to the terminals.
search strategy = depth-   rst: it expands a given terminal as
far as possible before proceeding to the next one.

28 / 66

algorithm sketch: recursive descent parsing

1 the top-level goal is to derive the start symbol (s).

2 choose a grammatical rule with s as its lhs

(e.g, s     np vp), and replace s with the rhs of the rule
(the subgoals; e.g., np and vp).

3 choose a rule with the leftmost subgoal as its lhs (e.g.,

np     det n). replace the subgoal with the rhs of the rule.

4 whenever you reach a lexical rule (e.g., det     the), match

its rhs against the current position in the input string.

if it matches, move on to next position in the input.
if it doesn   t, try next lexical rule with the same lhs.
if no rules with same lhs, backtrack to most recent choice of
grammatical rule and choose another rule with the same lhs.
if no more grammatical rules, back up to the previous subgoal.

5

iterate until the whole input string is consumed, or you fail to
match one of the positions in the input. backtrack on failure.

29 / 66

recursive descent parsing

30 / 66

thedogsawamanintheparksrecursive descent parsing

31 / 66

thedogsawamanintheparksnpvprecursive descent parsing

32 / 66

thedogsawamanintheparksvpnpdetnpprecursive descent parsing

33 / 66

thedogsawamanintheparksvpnpnppdettherecursive descent parsing

34 / 66

thedogsawamanintheparksvpnpnppdettherecursive descent parsing

35 / 66

thedogsawamanintheparksvpnpppdetthenmanrecursive descent parsing

36 / 66

thedogsawamanintheparksvpnpppdetthenparkrecursive descent parsing

37 / 66

thedogsawamanintheparksvpnpppdetthendogrecursive descent parsing

38 / 66

thedogsawamanintheparksvpnpdetthendogpppnprecursive descent parsing

39 / 66

thedogsawamanintheparksvpnpdetthendogppnppinrecursive descent parsing

40 / 66

thedogsawamanintheparksvpnpndettherecursive descent parsing

41 / 66

thedogsawamanintheparksvpnpdetthendogrecursive descent parsing

42 / 66

thedogsawamanintheparksnpdetthendogvpnpppvsawrecursive descent parsing

43 / 66

thedogsawamanintheparksnpdetthendogvpppvsawnpnppdetarecursive descent parsing

44 / 66

thedogsawamanintheparksnpdetthendogvpppvsawnpppdetanmanrecursive descent parsing

45 / 66

thedogsawamanintheparksnpdetthendogvpppvsawnpdetanmanppnppinrecursive descent parsing

46 / 66

thedogsawamanintheparksnpdetthendogvpppvsawnpdetanmanpppinnpdetnpprecursive descent parsing

47 / 66

thedogsawamanintheparksnpdetthendogvpppvsawnpdetanmanpppinnpdetthenparkppnpprecursive descent parsing

48 / 66

thedogsawamanintheparksnpdetthendogvpvsawppnpdetnrecursive descent parsing

49 / 66

thedogsawamanintheparksnpdetthendogvpvsawppnpdetanmanrecursive descent parsing

50 / 66

thedogsawamanintheparksnpdetthendogvpvsawnpdetanmanpppinnpdetthenparkid132

a shift-reduce parser tries to    nd sequences of words and phrases
that correspond to the righthand side of a grammar production
and replace them with the lefthand side:

directionality = bottom-up: starts with the words of the
input and tries to build trees from the words up.
search strategy = breadth-   rst: starts with the words, then
applies rules with matching right hand sides, and so on until
the whole sentence is reduced to an s.

51 / 66

algorithm sketch: id132

until the words in the sentences are substituted with s:

scan through the input until we recognise something that
corresponds to the rhs of one of the production rules (shift)

apply a production rule in reverse; i.e., replace the rhs of the
rule which appears in the sentential form with the lhs of the
rule (reduce)

a shift-reduce parser implemented using a stack:

1

start with an empty stack

2 a shift action pushes the current input symbol onto the stack

3 a reduce action replaces n items with a single item

52 / 66

id132

53 / 66

stackremaining textmydogsawamanintheparkwithastatueid132

54 / 66

stackremaining textmydogsawamanintheparkwithastatuedetid132

55 / 66

stackremaining textmydogsawamanintheparkwithastatuedetnid132

56 / 66

stackremaining textmydogsawamanintheparkwithastatuedetnnpid132

57 / 66

stackremaining textnpdetmyndogvsawnpdetanmanintheparkwithastatueid132

58 / 66

stackremaining textnpdetmyndogvsawnpdetanmanpinnpdetthenparkwithastatueppid132

59 / 66

stackremaining textnpdetmyndogvsawnpnpdetanmanpppinnpdetthenparkwithastatueid132

60 / 66

stackremaining textnpdetmyndogvsawnpnpdetanmanpppinnpdetthenparkwithastatuevpid132

61 / 66

stackremaining textnpdetmyndogvsawnpnpdetanmanpppinnpdetthenparkwithastatuevpsshift-reduce parsers and pushdown automata

id132 is equivalent to a pushdown automaton
constructed from the id18 (with one state q0):

start with empty stack
shift: a transition in the pda from q0 (to q0) putting a
terminal symbol on the stack

reduce: whenever the righthand side of a rule appears on top
of the stack, pop the rhs and push the lefthand side (still
staying in q0). don   t consume anything from the input.
accept the string if the start symbol is in the stack and the
end of string has been reached

if there is some derivation for a given sentence under the id18,
there will be a sequence of actions for which this npda accepts
the string

62 / 66

generalised lr parsing

if there is some derivation for a given sentence under the
id18, there will be a sequence of actions for which this
npda accepts the string

but how do we    nd this derivation?

one way to do this is using so-called generalised lr parsing, which
explores all possible paths of the above npda

modern parsers do it di   erently, because glr can be expontential
in the worst-case

63 / 66

modern shift-reduce parsers

shift-reduce parsers are highly e   cient, they are linear in the
length of the string they parse, if they explore only one path

how to do that? learn from data what actions to take at each
point, and try to make the optimal decisions so that the correct
parse tree will be found

this keeps the parser linear in the length of the string, but one
small error can propagate through the whole parse, and lead to the
wrong parse tree

64 / 66

try it out yourselves!

recursive descent parser

>>> from nltk.app import rdparser
>>> rdparser()

shift-reduce parser
>>> from nltk.app import srparser
>>> srparser()

65 / 66

summary

we use id18s to represent nl grammars

grammars need recursion to produce in   nite sentences

most nl grammars have structural ambiguity

a parser computes structure for an input automatically

recursive descent and id132

we   ll examine more parsers in lectures 17   22

reading:

j&m (2nd edition) chapter 12 (intro     section
12.3), chapter 13 (intro     section 13.3)

next lecture: the cyk algorithm

66 / 66

