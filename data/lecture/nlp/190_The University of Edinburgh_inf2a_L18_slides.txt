fun with weighted fsts

informatics 2a: lecture 18

shay cohen

school of informatics
university of edinburgh

27 october 2017

1 / 33

de   nition of id48

for our purposes, a hidden markov model (id48) consists of:

a set q = {q0, q1, . . . , qt} of states, with q0 the start state.
(our non-start states will correspond to parts-of-speech).

a transition id203 matrix
a = (aij | 0     i     t , 1     j     t ), where aij is the id203
aij = 1.
of jumping from qi to qj . for each i, we require

t(cid:80)

for each non-start state qi and word type w , an emission
id203 bi (w ) of outputting w upon entry into qi . (ideally,

for each i, we   d have(cid:80)

w bi (w ) = 1.)

j=1

we also suppose we   re given an observed sequence w1, w2 . . . , wn
of word tokens generated by the id48.

2 / 33

transition probabilities

3 / 33

emission probabilities

4 / 33

transition and emission probabilities

vb
<s> .019
vb
.0038
to
.83
nn
.0040
prp .23

to
.0043
.035
0
.016
.00079

nn
.041
.047
.00047
.087
.001

prp
.67
.0070
0
.0045
.00014

i
0
0
0

vb
to
nn
prp .37

want
.0093
0
.000054
0

to
0
.99
0
0

race
.00012
0
.00057
0

5 / 33

the id48 trellis

nn

nn

nn

nn

to

to

to

to

start

vb

vb

vb

vb

prp

prp

prp

prp

i

want

to

race

6 / 33

the viterbi algorithm

keep a chart of the form table(pos, i) where pos ranges over
the pos tags and i ranges over the indices in the sentence.

for all t and i:

table(t , i + 1)     max

t (cid:48) table(t (cid:48), i)    p(t|t (cid:48))    p(wi+1|t )

and

table(t , 1)     p(t|(cid:104)s(cid:105))p(w1|t )

table(., n) will contain the id203 of the most likely sequence.
to get the actual sequence, we need backpointers.

7 / 33

the viterbi algorithm: second example

q4 nn

q3 to

q2 vb

0

0

0

q1 prp 0

qo

start

1.0

<s>

i
w1

want
w2

to
w3

race
w4

for each state qj at time i, compute
vi (j) =

vi   1(k)akj bj (wi )

n

max
k=1

8 / 33

the viterbi algorithm

0
q4 nn
0
q3 to
q2 vb
0
q1 prp 0
qo

start

1.0
<s> i
w1

want
w2

to
w3

race
w4

1 create id203 matrix, with one column for each

observation (i.e., word token), and one row for each non-start
state (i.e., pos tag).

2 we proceed by    lling cells, column by column.
3 the entry in column i, row j will be the id203 of the

most probable route to state qj that emits w1 . . . wi .

9 / 33

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
qo

start

1.0
<s>

1.0    .041    0
1.0    .0043    0
1.0    .19    0
1.0    .67    .37

i
w1

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

10 / 33

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
q0

start

1.0
<s>

0
0
0
.025

i
w1

.025    .0012    0.000054
.025    .00079    0
.025    .23    .0093
.025    .00014    0

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

11 / 33

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
q0

start

1.0
<s>

0
0
0
.025

i
w1

.000000002
0
.00053
0

.000053    .047    0
.000053    .035    .99
.000053    .0038    0
.000053    .0070    0

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

12 / 33

the viterbi algorithm

q4 nn 0
q3 to 0
q2 vb 0
q1 prp0
q0

start1.0

.0000018    .00047    .00057

.0000000020
0
.00053

0
0
0
.025 0

.0000018.0000018  0  0
0
0

.0000018  .83  .00012
.0000018    0    0

<s> i
w1

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

13 / 33

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
q0

start

1.0
<s>

0
0
0
.025

i
w1

.000000002
0
.00053
0

0
.0000018
0
0

4.8222e-13
0
1.7928e-10
0

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

14 / 33

example demo

http://nlp.stanford.edu:8080/parser/

relies both on    distributional    and    morphological    criteria

uses a model similar to id48

15 / 33

input as an fst

16 / 33

emission table as an fst

notice the weights on the fst

17 / 33

transition table as an fst

18 / 33

input fst composed with emission fst

19 / 33

input fst composed with emission fst

19 / 33

... composed with transition fst

20 / 33

... composed with transition fst

20 / 33

language models

rather than generate tag conditioned on previous tag, generate
word conditioned on previous word.

21 / 33

language models

rather than generate tag conditioned on previous tag, generate
word conditioned on previous word.

bigrams:

months the my and issue of year foreign new exchanges september
were recession exchange new endorsed a acquire to six executives

21 / 33

language models

rather than generate tag conditioned on previous tag, generate
word conditioned on previous word.

bigrams:

months the my and issue of year foreign new exchanges september
were recession exchange new endorsed a acquire to six executives

trigrams:

last december through the way to preserve the hudson
corporation n. b. e. c. taylor would seem to complete the maj or
central planners one point    ve percent of u. s. e. has already old
m. x. corporation of living on information such as more frequently
   shing to keep her.

21 / 33

language models

4-grams:

they also point to ninety nine point six billion dollars from two
hundred four oh six three percent of the rates of interest stores as
mexico and brazil on market conditions.

22 / 33

language models

4-grams:

they also point to ninety nine point six billion dollars from two
hundred four oh six three percent of the rates of interest stores as
mexico and brazil on market conditions.

this basic idea is fundamental in any system that generates
language: machine translation, id103, optical
character recognition, image captioning.

as we   ve just seen, can be (and is) implemented as a very large
weighted fst!

22 / 33

id103

task: convert soundwaves to corresponding text.

intuition: both sound and text are (noisy) representations of the
same underlying set of phonemes.

mapping from words to phonemes is just transduction! (from
phonemes to sound, signal processing). coupled with a very large
language model...

23 / 33

id103 transducers (1)

(a) language model; (b) phonemes to words

24 / 33

id103 transducers (1)

(c) composition of the lm and phoneme to word transducer; (d)
determinisation of c.

25 / 33

machine tanslation

watashi wa hako wo akemasu     i open the box
(japanese gloss:    i the box open   , with two case markers)

two basic operation of a machine translation system:

1

substitute words or sequences of words.

2 permute word sequences.

26 / 33

machine translation models

27 / 33

the segmentation transducer

28 / 33

the permutation transducer

29 / 33

the insertion transducer

30 / 33

machine translation models (again)

every step can be encoded as an fst. compose and run viterbi!

31 / 33

other applications

a search on google scholar for       nite state transducers    leads to
over 200,000 results.

other applications for natural language:

id39

text normalisation

information extraction

fsts are modular (can break the problem into di   erent
sub-problems and cascade the resulting fsts together), highly
e   cient and are simple to understand and work with.

32 / 33

takeaways:

1 viterbi algorithm

2 weighted    nite state transducers are incredibly useful!

next week: parsing natural language.

33 / 33

