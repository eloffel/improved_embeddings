part-of-speech tagging

informatics 2a: lecture 17

shay cohen

school of informatics
university of edinburgh

24 october 2017

1 / 48

last class

we discussed the pos tag lexicon

when do words belong to the same class? three criteria

what tagset should we use?

what are the sources of ambiguity for id52?

2 / 48

1 automatic id52: the problem

2 methods for tagging

unigram tagging
bigram tagging
tagging using id48: viterbi algorithm
rule-based tagging

reading: jurafsky & martin, chapters (5 and) 6.

3 / 48

1 automatic id52: the problem

2 methods for tagging

unigram tagging
bigram tagging
tagging using id48: viterbi algorithm
rule-based tagging

reading: jurafsky & martin, chapters (5 and) 6.

4 / 48

1 automatic id52: the problem

2 methods for tagging

unigram tagging
bigram tagging
tagging using id48: viterbi algorithm
rule-based tagging

reading: jurafsky & martin, chapters (5 and) 6.

5 / 48

bene   ts of id52

essential preliminary to (anything that involves) parsing.
can help with id133. for example, try saying the
sentences below out loud.
can help with determining authorship: are two given
documents written by the same person? forensic linguistics.

1 have you read    the wind in the willows    ? (noun)

2 the clock has stopped. please wind it up. (verb)

3 the students tried to protest. (verb)

4 the students    protest was successful. (noun)

6 / 48

before we get started

question you should always ask yourself:

how hard is this problem?

7 / 48

before we get started

question you should always ask yourself:

how hard is this problem?

for id52, this boils down to:

how ambiguous are parts of speech, really?

if most words have unambiguous pos, then we can probably write
a simple program that solves id52 with just a lookup
table. e.g.    whenever i see the word the, output dt.   

7 / 48

before we get started

question you should always ask yourself:

how hard is this problem?

for id52, this boils down to:

how ambiguous are parts of speech, really?

if most words have unambiguous pos, then we can probably write
a simple program that solves id52 with just a lookup
table. e.g.    whenever i see the word the, output dt.   

this is an empirical question. to answer it, we need data.

7 / 48

corpus annotation

a corpus (plural corpora) is a computer-readable collection of nl
text (or speech) used as a source of information about the
language: e.g. what words/constructions can occur in practice, and
with what frequencies.
the usefulness of a corpus can be enhanced by annotating each
word with a pos tag, e.g.

our/prp\$ enemies/nns are/vbp innovative/jj and/cc
resourceful/jj ,/, and/cc so/rb are/vb we/prp ./.
they/prp never/rb stop/vb thinking/vbg about/in new/jj
ways/nns to/to harm/vb our/prp\$ country/nn and/cc
our/prp\$ people/nn, and/cc neither/dt do/vb we/prp ./.

typically done by an automatic tagger, then hand-corrected by a
native speaker, in accordance with speci   ed tagging guidelines.

8 / 48

id52: di   cult cases

even for humans, tagging sometimes poses di   cult decisions.
e.g. words in -ing: adjectives (jj), or verbs in gerund form (vbg)?

a boring/jj lecture

the falling/vbg leaves

sparkling/jj? lemonade

a very boring lecture
? a lecture that bores
the lecture seems boring
*the very falling leaves
the leaves that fall
? very sparkling lemonade
lemonade that sparkles
the lemonade seems sparkling

in view of such problems, we can   t expect 100% accuracy from an
automatic tagger.
in the id32, annotators disagree around 3.5% of the
time. put another way: if we assume that one annotator tags
perfectly, and then measure the accuracy of another annotator by
comparing with the    rst, they will only be right about 96.5% of
the time. we can hardly expect a machine to do better!

9 / 48

word types and tokens

need to distinguish word tokens (particular occurrences in a
text) from word types (distinct vocabulary items).
we   ll count di   erent in   ected or derived forms (e.g. break,
breaks, breaking) as distinct word types.
a single word type (e.g. still) may appear with several pos.
but most words have a clear most frequent pos.

question: how many tokens and types in the following? ignore
case and punctuation.

esau sawed wood. esau wood would saw wood. oh, the
wood wood would saw!

1 14 tokens, 6 types
2 14 tokens, 7 types
3 14 tokens, 8 types
4 none of the above.

10 / 48

extent of pos ambiguity

the brown corpus (1,000,000 word tokens) has 39,440 di   erent
word types.

35340 have only 1 pos tag anywhere in corpus (89.6%)

4100 (10.4%) have 2 to 7 pos tags

so why does just 10.4% pos-tag ambiguity by word type lead to
di   culty?
this is thanks to zip   an distribution: many high-frequency words
have more than one pos tag.
in fact, more than 40% of the word tokens are ambiguous.

he wants to/to go.
he went to/in the store.

he wants that/dt hat.
it is obvious that/cs he wants a hat.
he wants a hat that/wps    ts.

11 / 48

word frequencies in di   erent languages

ambiguity by part-of-speech tags:

language type-ambiguous token-ambiguous
english
greek
japanese
czech
turkish

56.2%
19.14%
50.2%
14.5%
35.2%

13.2%
<1%
7.6%
<1%
2.5%

12 / 48

some tagging strategies

we   ll look at several methods or strategies for automatic tagging.

one simple strategy: just assign to each word its most
common tag. (so still will always get tagged as an adverb    
never as a noun, verb or adjective.) call this unigram tagging,
since we only consider one token at a time.

surprisingly, even this crude approach typically gives around
90% accuracy. (state-of-the-art is 96   98%).

can we do better? we   ll look brie   y at bigram tagging, then
at hidden markov model tagging.

13 / 48

bigram tagging

we can do much better by looking at pairs of adjacent tokens.
for each word (e.g. still), tabulate the frequencies of each possible
pos given the pos of the preceding word.

example (with made-up numbers):

. . .

still dt md jj
6
nn 8
14
23
jj
2
1
vb
rb
6
3

0
0
12
45

given a new text, tag the words from left to right, assigning each
word the most likely tag given the preceding one.

could also consider trigram (or more generally id165) tagging,
etc. but the frequency matrices would quickly get very large, and
also (for realistic corpora) too    sparse    to be really useful.

14 / 48

bigram model

example
and a member of both countries , a serious the services of the dole
of .     ross declined to buy beer at the winner of his wife , i can
live with her hand who sleeps below 50 @-@ brick appealed to
make his last week the size , radovan karadzic said .     the dow
jones set aside from the economy that samuel adams was half
@-@    lled with it ,     but if that yeltsin .     but analysts and goes
digital popcorn , you don    t .     this far rarer cases it is educable .

15 / 48

trigram model

example

change his own home ; others ( such disagreements have
characterized diller    s team quickly launched deliberately raunchier
, more recently ,     said michael pasano , a government and ruling
party     presidential power , and estonia , which published
photographs by him in running his own club

16 / 48

4-gram model

example
not to let nature take its course .     we   ve got one time to do it in
three weeks and was criticized by lebanon and syria to use the
killing of thousands of years of involvement in the plots .

17 / 48

problems with bigram tagging

one incorrect tagging choice might have unintended e   ects:

the
intended: dt
bigram: dt

still
rb
jj

smoking
vbg
nn

remains
nns
vbz

of
in
. . .

camp   re

the
dt nn

no lookahead: choosing the    most probable    tag at one stage
might lead to highly improbable choice later.

the

still was

intended: dt nn vbd
vbd?

bigram: dt jj

smashed
vbn

we   d prefer to    nd the overall most likely tagging sequence given
the bigram frequencies. this is what the hidden markov model
(id48) approach achieves.

18 / 48

id48

the idea is to model the agent that might have generated the
sentence by a semi-random process that outputs a sequence of
words.

think of the output as visible to us, but the internal states of
the process (which contain pos information) as hidden.

for some outputs, there might be several possible ways of
generating them i.e. several sequences of internal states. our
aim is to compute the sequence of hidden states with the
highest id203.

speci   cally, our processes will be    nfas with probabilities   .
simple, though not a very    attering model of human language
users!

19 / 48

de   nition of id48

for our purposes, a hidden markov model (id48) consists of:

a set q = {q0, q1, . . . , qt} of states, with q0 the start state.
(our non-start states will correspond to parts-of-speech).

a transition id203 matrix
a = (aij | 0     i     t , 1     j     t ), where aij is the id203
aij = 1.
of jumping from qi to qj . for each i, we require

t(cid:80)

for each non-start state qi and word type w , an emission
id203 bi (w ) of outputting w upon entry into qi . (ideally,

for each i, we   d have(cid:80)

w bi (w ) = 1.)

j=1

we also suppose we   re given an observed sequence w1, w2 . . . , wn
of word tokens generated by the id48.

20 / 48

transition probabilities

21 / 48

emission probabilities

22 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh

has

a

very

rich

history

.

23 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh
nnp

p(nnp|(cid:104)s(cid:105))    p(edinburgh|nnp)

24 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh
nnp

has
vbz

p(nnp|(cid:104)s(cid:105))    p(edinburgh|nnp)

p(vbz|nnp)    p(has|vbz )

25 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh
nnp

a

has
vbz dt

p(nnp|(cid:104)s(cid:105))    p(edinburgh|nnp)

p(vbz|nnp)    p(has|vbz )

p(dt|vbz )    p(a|dt )

26 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh
nnp

a

has
very
vbz dt rb

p(nnp|(cid:104)s(cid:105))    p(edinburgh|nnp)

p(vbz|nnp)    p(has|vbz )

p(dt|vbz )    p(a|dt )
p(rb|dt )    p(very|rb)

27 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh
nnp

a

has
very
vbz dt rb

rich
jj

p(nnp|(cid:104)s(cid:105))    p(edinburgh|nnp)

p(vbz|nnp)    p(has|vbz )

p(dt|vbz )    p(a|dt )
p(rb|dt )    p(very|rb)
p(jj|rb)    p(rich|jj)

28 / 48

generating a sequence

id48 can be thought of as devices that generate
sequences with hidden states:

edinburgh
nnp

a

has
very
vbz dt rb

rich
jj

history
nn

p(nnp|(cid:104)s(cid:105))    p(edinburgh|nnp)

p(vbz|nnp)    p(has|vbz )

p(dt|vbz )    p(a|dt )
p(rb|dt )    p(very|rb)
p(jj|rb)    p(rich|jj)

p(nn|jj)    p(history|nn)

29 / 48

transition and emission probabilities

vb
<s> .019
vb
.0038
to
.83
nn
.0040
prp .23

to
.0043
.035
0
.016
.00079

nn
.041
.047
.00047
.087
.001

prp
.67
.0070
0
.0045
.00014

i
0
0
0

vb
to
nn
prp .37

want
.0093
0
.000054
0

to
0
.99
0
0

race
.00012
0
.00057
0

30 / 48

how do we search for best tag sequence?

we have de   ned an id48, but how do we use it? we are given a
word sequence and must    nd their corresponding tag sequence.

it   s easy to compute the id203 of generating a word
sequence w1 . . . wn via a speci   c tag sequence t1 . . . tn: let t0
denote the start state, and compute

t(cid:89)

p(ti|ti   1).p(wi|ti )

(1)

i=1

using the transition and emission probabilities.

but how do we    nd the most likely tag sequence?
we can do this e   ciently using id145 and
the viterbi algorithm.

31 / 48

question

given n word tokens and a tagset with t choices per token, how
many tag sequences do we have to evaluate?

1

|t| tag sequences
2 n tag sequences

|t|    n tag sequences
|t|n tag sequences

3

4

32 / 48

the id48 trellis

nn

nn

nn

nn

to

to

to

to

start

vb

vb

vb

vb

prp

prp

prp

prp

i

want

to

race

33 / 48

the viterbi algorithm

keep a chart of the form table(pos, i) where pos ranges over
the pos tags and i ranges over the indices in the sentence.

for all t and i:

table(t , i + 1)     max

t (cid:48) table(t (cid:48), i)    p(t|t (cid:48))    p(wi+1|t )

and

table(t , 1)     p(t|(cid:104)s(cid:105))p(w1|t )

table(., n) will contain the id203 of the most likely sequence.
to get the actual sequence, we need backpointers.

34 / 48

the viterbi algorithm

let   s now tag the newspaper headline:

deal talks fail

note that each token here could be a noun (n) or a verb (v).
we   ll use a toy id48 given as follows:

from start

to n to v
.8
from n .4
from v .8

.2
.6
.2

transitions

deal

n .2
v .3

fail
.05
.3

talks
.2
.3

emissions

35 / 48

the viterbi matrix

deal

talks

fail

n

v

from start

to n to v
.8
from n .4
from v .8

.2
.6
.2

deal

n .2
v .3

fail
.05
.3

talks
.2
.3

transitions

emissions

table(t , i + 1)     max

t (cid:48) table(t (cid:48), i)    p(t|t (cid:48))    p(wi+1|t )

36 / 48

the viterbi matrix

deal

talks

n .8x.2 = .16     .16x.4x.2 = .0128 (cid:46) .0288x.8x.05 = .001152
(since .0128x.4 < 0.0288x.8)
v .2x.3 = .06 (cid:45) .16x.6x.3 = .0288 (cid:45) .0128x.6x.3 = .002304
(since .0128x.6 > 0.0288x.2)

(since .16x.6 > .06x.2)

(since .16x.4 > .06x.8)

fail

looking at the highest id203 entry in the    nal column and
chasing the backpointers, we see that the tagging n n v wins.

37 / 48

the viterbi algorithm: second example

q4 nn

q3 to

q2 vb

0

0

0

q1 prp 0

qo

start

1.0

<s>

i
w1

want
w2

to
w3

race
w4

for each state qj at time i, compute
vi (j) =

vi   1(k)akj bj (wi )

n

max
k=1

38 / 48

the viterbi algorithm

0
q4 nn
0
q3 to
q2 vb
0
q1 prp 0
qo

start

1.0
<s> i
w1

want
w2

to
w3

race
w4

1 create id203 matrix, with one column for each

observation (i.e., word token), and one row for each non-start
state (i.e., pos tag).

2 we proceed by    lling cells, column by column.
3 the entry in column i, row j will be the id203 of the

most probable route to state qj that emits w1 . . . wi .

39 / 48

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
qo

start

1.0
<s>

1.0    .041    0
1.0    .0043    0
1.0    .19    0
1.0    .67    .37

i
w1

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

40 / 48

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
q0

start

1.0
<s>

0
0
0
.025

i
w1

.025    .0012    0.000054
.025    .00079    0
.025    .23    .0093
.025    .00014    0

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

41 / 48

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
q0

start

1.0
<s>

0
0
0
.025

i
w1

.000000002
0
.00053
0

.000053    .047    0
.000053    .035    .99
.000053    .0038    0
.000053    .0070    0

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

42 / 48

the viterbi algorithm

q4 nn 0
q3 to 0
q2 vb 0
q1 prp0
q0

start1.0

.0000018    .00047    .00057

.0000000020
0
.00053

0
0
0
.025 0

.0000018.0000018  0  0
0
0

.0000018  .83  .00012
.0000018    0    0

<s> i
w1

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

43 / 48

the viterbi algorithm

q4 nn
0
q3 to
0
q2 vb
0
q1 prp 0
q0

start

1.0
<s>

0
0
0
.025

i
w1

.000000002
0
.00053
0

0
.0000018
0
0

4.8222e-13
0
1.7928e-10
0

want
w2

to
w3

race
w4

n

max
k=1

vi   1(k)akj bj (wi )

for each state qj at time i, compute
vi (j) =
vi   1(k) is previous viterbi path id203, akj is
transition id203, and bj (wi ) is emission id203.
there   s also an (implicit) backpointer from cell (i, j) to the
relevant (i     1, k), where k maximizes vi   1(k)akj .

44 / 48

connection between id48s and    nite state machines

id48 are    nite state machines with probabilities
added to them.

if we think of    nite state automaton as generating a string when
randomly going through states (instead of scanning a string), then
id48 are such id122s where there is a speci   c
id203 for generating each symbol at each state, and a speci   c
id203 for transitioning from one state to another.

as such, the viterbi algorithm can be used to    nd the most likely
sequence of states in a probabilistic id122, given a speci   c input
string.

question: where do the probabilities come from?

45 / 48

example demo

http://nlp.stanford.edu:8080/parser/

relies both on    distributional    and    morphological    criteria

uses a model similar to id48

46 / 48

rule-based tagging

basic idea:

1 assign each token all its possible tags.

2 apply rules that eliminate all tags for a token that are

inconsistent with its context.

example

the dt (determiner)
can md (modal)

nn (sg noun)
vb (base verb)

   

the dt (determiner)
can md (modal)

nn (sg noun)
vb (base verb)

   
x

x

assign any unknown word tokens a tag that is consistent with its
context (eg, the most frequent tag).

47 / 48

rule-based tagging

rule-based tagging often used a large set of hand-crafted
context-sensitive rules.

example (schematic):

if (-1 dt)
elim md, vb /* eliminate modals and base verbs */

/* if previous word is a determiner */

problem: cannot eliminate all pos ambiguity.

48 / 48

