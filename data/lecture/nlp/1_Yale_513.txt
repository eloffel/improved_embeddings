nlp
introduction to nlp
generative vs. discriminative models
generative vs. discriminative models
learn a model of the joint id203 p(d, c)
use bayes    rule to calculate p(c|d) 
build a model of each class; given example, return the model most likely to have generated that example
examples: na  ve bayes, gaussian discriminant analysis, id48, probabilistic id18
generative
discriminative
model posterior id203 p(c|d) directly
class is a function of document vector
find the exact function that minimizes classification errors on the training data 
examples: id28, neural networks (nns), support vector machines (id166s), id90
assumptions of discriminative classifiers
data examples (documents) are represented as vectors of features (words, phrases, ngrams, etc)
looking for a function that maps each vector into a class. 
this function can be found by minimizing the errors on the training data (plus other various criteria)
different classifiers vary on what the function looks like, and how they find the function
discriminative vs. generative classifiers
discriminative classifiers are generally more effective, since they directly optimize the classification accuracy. but
they are all sensitive to the choice of features, and so far these features are extracted heuristically
also, overfitting can happen if data is sparse
generative classifiers are the    opposite   
they directly model text, an unnecessarily harder problem than classification
they can easily exploit unlabeled data
introduction to nlp
generative classifier:
na  ve bayes
na  ve bayes intuition
simple (   na  ve   ) classification method based on bayes rule
relies on very simple representation of document
bag of words

bag of words
remember bayes    rule?
d is the document (represented as a list of features of a document, x1,    , xn)
c is a class (e.g.,    not spam   )
bayes    rule:
na  ve bayes classifier (i)
map is    maximum a posteriori     = most likely class
bayes rule
dropping the denominator
na  ve bayes classifier (ii)
document d represented as features x1..xn
but where will we get these probabilitites?
na  ve bayesian classifiers
na  ve bayesian classifier


assuming statistical independence




features = words (or phrases) typically
multinomial na  ve bayes independence assumptions
bag of words assumption
assume position doesn   t matter
conditional independence
assume the feature probabilities p(xi|c) are independent given the class c.
[jurafsky and martin]
multinomial na  ve bayes classifier
[jurafsky and martin]
learning the multinomial na  ve bayes model
first attempt: maximum likelihood estimates
simply use the frequencies in the data
[jurafsky and martin]
create mega-document for topic j by concatenating all docs in this topic
use frequency of w in mega-document


parameter estimation
fraction of times word wi appears 
among all words in documents of topic cj
[jurafsky and martin]
problem with maximum likelihood
what if we have seen no training documents with the word fantastic  and classified in the topic positive (thumbs-up)?





zero probabilities cannot be conditioned away, no matter the other evidence!
[jurafsky and martin]
laplace smoothing
[jurafsky and martin]
multinomial na  ve bayes: learning
calculate p(cj) terms
for each cj in c do
 docsj     all docs with  class =cj

calculate p(wk | cj) terms
textj     single doc containing all docsj
for each word wk in vocabulary
    nk     # of occurrences of wk in textj
from training corpus, extract vocabulary
[jurafsky and martin]
example
features = {i hate love this book}
training
i hate this book
love this book
what is p(y|x)?
prior p(y) 
testing
hate book
different conditions
a = 0 (no smoothing)
a = 1 (smoothing)


example
ways naive bayes is not so naive
very fast, low storage requirements
robust to irrelevant features
irrelevant features cancel each other without affecting results
very good in domains with many equally important features
id90 suffer from fragmentation in such cases     especially if little data
optimal if the independence assumptions hold
if assumed independence is correct, then it is the bayes optimal classifier for problem
a good, dependable baseline for text classification
but other classifiers give better accuracy

[jurafsky and martin]
nlp
