informatics 2a 2017   18

lecture 32

revision lecture

john longley
shay cohen

reminder: pass criteria

by 4pm on friday, you will have completed your coursework.
the two courseworks together account for 25% of the course
mark.

the remaining 75% of the course mark is provided by the exam.

for a pass in inf2a, you need a combined mark of at least 40%.

(no separate exam and coursework hurdles this year.)

1

the 2017 inf2a exam

december exam time and location:

infr08008 - informatics 2a
location: appleton tower concourse (split a-s), the pleasance
sports hall (split t-z)
date: friday, 15/12/2017
time: 14:30 to 16:30
duration: 02:00

this is copied from the registry exam timetable

http://www.scripts.sasg.ed.ac.uk/registry/examinations/

which is the o   cial exam timetable. make sure that you use this
link to double-check all your exam times (including inf2a).

a resit exam will be held in august 2018.

2

exam structure

the exam is pen-and-paper, and lasts 2 hours.
calculators may be used, but you must bring your own. it must
be one from an approved list of models speci   ed by college:

http://edin.ac/1rnrsfa

the exam consists of:

    part a: 5 compulsory short questions, worth 10% each.

guideline time per question: 10 minutes

    part b: a choice of 2 out of 3 longer questions, worth 25%

each.
guideline time per question: 30 minutes

the guideline times allow 10 minutes for reading and familiarising
yourself with the exam paper.

3

part a questions

the 5 compulsory short questions were new in 2012 and replaced
20 multiple-choice questions in previous years.

the questions will be similar in style and length (but not neces-
sarily in topic) to the questions on this week   s tutorial 9.

the multiple-choice questions of previous years still provide good
revision material in terms of coverage of topics.

4

examinable material

examinable material: formal language thread

lecture 2 (the course roadmap) should be considered exam-
inable.

all of the material on regular and context-free languages (lec-
tures 3   14) is examinable, except:

    use of    nite automata in veri   cation (lecture 7, slides 12   

15)

    speci   c details of micro-haskell (lecture 13, slides 7   12)

    examples of english palindromes (lecture 14, slides 16   19)

5

examinable material: formal language thread (contd.)

lecture 28 (semantics of programming languages, in particular
mh) may be considered non-examinable.

lecture 29 (context-sensitive languages): mostly examinable    
but for the context-free pumping lemma, just the general idea
will su   ce.

lecture 30 (turing machines, linear bounded automata): gen-
eral
ideas and concepts examinable, but not detailed de   ni-
tions/proofs. should know e.g. that csls correspond to nl-
bas, rels correspond to tms, and should know some standard
examples of languages at each level.

lecture 31 (undecidability): should know what    decidable    and
   semidecidable    mean, what the halting problem is, and that it   s
undecidable. rest is non-examinable.

6

kinds of exam question: formal language thread

broadly speaking, 2 styles of question in exam.

algorithmic problems: minimizing a dfa, converting nfa to
dfa, executing a pda, ll(1) parsing using parse table, gener-
ating parse table from ll(1) grammar, . . .

when the algorithm is complex (e.g., minimization, calculating
   rst and follow sets), it may be easier to work with your un-
derstanding of the concepts rather than following the algorithm
strictly to the letter.

non-algorithmic problems: converting dfa to regular expres-
sion, designing regular expression patterns, applying pumping
lemma, designing id18s, converting id18 to ll(1), parsing us-
ing id19 or noncontracting grammar, . . .

7

examinable material: natural language thread

the main thing being tested is your ability to apply and under-
stand the methods for solving certain standard kinds of problems.

algorithmic problems:

    id52 via bigrams or viterbi algorithm (lecture 17).
    cyk and id117 (lectures 20, 21).
    tree probabilities; probabilistic cyk; inferring probabilities

from a corpus; lexicalization of rules (lectures 22, 23).

    computing semantics,

including   -reduction (lectures 25,

26).

8

examinable material: natural language thread (continued)

non-algorithmic problems (simple examples only!)

    design of a transducer for some morphology parsing task

(lecture 15).

    design of context-free rules for some feature of english. (in-

cludes parameterized rules for agreement     lecture 22.)

    adding semantic clauses to a given context-free grammar

(lectures 25, 26).

    converting an english sentence to a formula of fopl (lec-

ture 25).

9

examinable material: natural language thread (continued)

general topics

    the language processing pipeline (lecture 2).
    kinds of ambiguity (lectures 2, 16, 19, 25).
    the id154, and where human languages sit (lec-

tures 2, 27).

    the general idea of parts of speech (lecture 16).
    word distribution and zipf   s law (lecture 16).
    very basic python.

the ideas of recursive descent and id132 (lecture
19) are only weakly examinable.

10

non-examinable material: natural language thread

    speci   c knowledge of linguistics (everything you need will be

given in the question).

    details of particular pos tagsets; ability to do id52

by hand (lectures 16, 17).

11

follow-on informatics courses

12

compiling techniques (ug3)

covers the entire language-processing pipeline for programming
languages, aiming at e   ective compilation: translating code in a
high-level source language (java, c, haskell, . . . ) to equivalent
code in a low-level target language (machine code, bytecode)

syllabus includes lexing and parsing from a more practical per-
spective than in inf2a.

majority of course focused on latter stages of language-processing
pipeline. converting lexed and parsed source-language code into
equivalent target-language code.

currently an assignment-only course, no exam: you build a com-
piler!

13

introduction to theoretical computer science (ug3)

this will look at models of computation (register machines, tur-
ing machines, lambda-calculus) and their di   erent in   uences on
computing practice.

one thread will address the boundaries between what is not
computable at all (undecidable problems), what is computable in
principle (decidable problems), and what is computable in prac-
tice (tractable problems). a major goal
is to understand the
famous p = np question.

another thread will
look at the in   uence lambda-calculus has
had, as a model of computation, on programming language de-
sign and practice, including lisp, ocaml, haskell and java.

14

natural languages: what we   ve done, what we haven   t.

nls are endlessly complex and fascinating.
have barely scratched the surface.

in this course, we

there   s a world of di   erence between doing nlp with small
toy grammars (as in this course) and wide-coverage grammars
intended to cope with real-world speech/text.

    ambiguity is the norm rather than the exception.
    empirical and statistical techniques (involving text corpora)
come to the fore, as distinct from logical and symbolic ones.

coping with the richness and complexity of real-world language
is still a largely unsolved problem!

15

discourse structure.

in this course, we haven   t considered any structure above the
level of sentences. in practice, higher level discourse structure is
crucial. e.g.

the tin man went to the emerald city to see the wizard
of oz and ask for a heart. then he waited to see whether
he would give it to him.

or compare:

    john hid bill   s car keys. he was drunk.
    john hid bill   s car keys. he likes spinach. (??)

16

deep vs. shallow processing.

roughly, the further we go along the nlp pipeline, the deeper
our analysis.

    many apparently    shallow    nlp tasks (e.g. spell checking;
speech transcription) can bene   t from the use of    deeper   
techniques such as parsing.

    on the other hand, for many seemingly    deep    tasks (e.g.
machine translation), current state-of-the-art techniques are
surprisingly    shallow    (e.g. use of id165 techniques with
massive corpora).

17

follow-on courses in nlp

    foundations of natural language processing [ug3]. em-
pirical rather than theoretical in focus. material on text cor-
pora, id165s, the    noisy channel    model. a bit on the
discourse level.

    machine translation [ug4]. mainly on shallow techniques
for mt: e.g. phrase-based models. find out how google
translate works!

    natural language understanding [ug4]. considers the
lp pipeline much in the spirit of inf2a, but including dis-
course level. surveys both deep and shallow approaches.

    topics in natural language processing [ug4]. get ac-
quainted with state of the art in nlp and read cutting-edge
research papers in nlp and machine learning.

18

thank you!!

hope you   ve enjoyed inf2a,
and good luck with the exam!

please complete the online course questionnaire when it

becomes available.

19

