parameter estimation and lexicalisation for pid18s

informatics 2a: lecture 23

shay cohen

7 november 2017

1 / 35

last class

probabilistic id18s attach to each rule in the grammar a
id203

the cyk algorithm can be turned probabilistic: we have a
chart with three indices ranging over nonterminals, beginning
index and end index

each such element in the chart has the maximal id203 of
generating a tree spanning the corresponding phrase headed
by that nonterminal

but where do the probabilities come from?

what forms of grammar can we have?

2 / 35

1 standard pid18s

parameter estimation
problem 1: assuming independence
problem 2: ignoring lexical information

2 lexicalized pid18s

lexicalization
head lexicalization

reading:

j&m 2nd edition, ch. 14.2   14.6,
nltk book, chapter 8,    nal section on weighted
grammar.

3 / 35

question

s     np vp
(1.0)
np     det n
(0.7)
np     npr
(0.3)
vp     v pp
(0.7)
vp     v np
(0.3)
pp     prep np (1.0)

npr     john
npr     mary
v     saw
v     loves
det     a
n     cat
n     saw

(0.5)
(0.5)
(0.4)
(0.6)
(1.0)
(0.6)
(0.4)

what is the id203 of the sentence john saw a saw ?

1 0.02

2 0.00016

3 0.00504

4 0.0002

4 / 35

where the probabilities come from?

the case of id48:

i/prp was/vbd walking/vbg down/in the/dt high/jj
street/nn yesterday/nn when/cc i/prp noticed/vbd an/dt
old/jj man/nn acting/vbg suspiciously/rb . he/prp was/vbd
peering/vbg into/in various/jj shop/nn windows/nns and/cc
writing/vbg things/nns in/in a/dt notebook/nn .
when/wrb he/prp spotted/vbd me/prp, he/prp
stu   ed/vbd the/dt notebook/nn into/in his/prp$ pocket/nn
and/cc wandered/vbd o   /rp ./.

count the number of times word w occurs with tag t.

p(w | t) = count(w , t)/

count(w(cid:48), t)

count the number of times tag t appears after tag t(cid:48).

p(t | t(cid:48)) = count(t(cid:48), t)/

(cid:88)
(cid:88)

w(cid:48)

t(cid:48)(cid:48)

count(t(cid:48), t(cid:48)(cid:48))

5 / 35

parameter estimation

in a pid18 every rule is associated with a id203.
but where do these rule probabilities come from?

use a large parsed corpus such as the id32.

( (s

(np-sbj (dt that) (jj cold)

(, ,)
(jj empty) (nn sky) )

(vp (vbd was)

(adjp-prd (jj full)

(pp (in of)

(np (nn fire)

(cc and)
(nn light) ))))

(. .) ))

s     np-sbj vp
vp     vbd adjp-prd
pp     in np
np     nn cc nn
etc.

6 / 35

parameter estimation

in a pid18 every rule is associated with a id203.
but where do these rule probabilities come from?

use a large parsed corpus such as the id32.

obtain grammar rules by reading them o    the trees.
calculate number of times lhs     rhs occurs over number
of times lhs occurs.

p(         |  ) =

(cid:80)
count(         )
   count(         )

count(         )

count(  )

=

7 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
r 1
r 2

rule
  
s     np vp
s
s     np vp ap s

p(r|  )
2/4
2/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
r 1
r 2

rule
  
s     np vp
s
s     np vp ap s

p(r|  )
2/4
2/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
r 1
r 2

rule
  
s     np vp
s
s     np vp ap s

p(r|  )
2/4
2/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
rule
  
s     np vp
r 1
s
s     np vp ap s
r 2
r 3 np     grass
np

p(r|  )
2/4
2/4
3/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
rule
  
s     np vp
r 1
s
s     np vp ap s
r 2
r 3 np     grass
np
r 4 np     bananas
np

p(r|  )
2/4
2/4
3/4
1/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
rule
  
s     np vp
r 1
s
s     np vp ap s
r 2
r 3 np     grass
np
r 4 np     bananas
np
r 5 vp     grows
vp

p(r|  )
2/4
2/4
3/4
1/4
3/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
rule
  
s     np vp
r 1
s
s     np vp ap s
r 2
r 3 np     grass
np
r 4 np     bananas
np
r 5 vp     grows
vp
r 6 vp     grow
vp

p(r|  )
2/4
2/4
3/4
1/4
3/4
1/4

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
rule
  
s     np vp
r 1
s
s     np vp ap s
r 2
r 3 np     grass
np
r 4 np     bananas
np
r 5 vp     grows
vp
r 6 vp     grow
vp
r 7 ap     fast
ap

p(r|  )
2/4
2/4
3/4
1/4
3/4
1/4
1/2

8 / 35

parameter estimation

corpus of parsed sentences:
   s1: [s [np grass] [vp grows]]   
   s2: [s [np grass] [vp grows] [ap slowly]]   
   s3: [s [np grass] [vp grows] [ap fast]]   
   s4: [s [np bananas] [vp grow]]   

compute pid18 probabilities:

r
rule
  
s     np vp
r 1
s
s     np vp ap s
r 2
r 3 np     grass
np
r 4 np     bananas
np
r 5 vp     grows
vp
r 6 vp     grow
vp
r 7 ap     fast
ap
r 8 ap     slowly
ap

p(r|  )
2/4
2/4
3/4
1/4
3/4
1/4
1/2
1/2

8 / 35

parameter estimation

with these parameters (rule probabilities), we can now compute
the probabilities of the four sentences s1   s4:

p(s1) = p(r 1|s)p(r 3|np)p(r 5|vp)

= 2/4    3/4    3/4 = 0.28125

p(s2) = p(r 2|s)p(r 3|np)p(r 5|vp)p(r 7|ap)

= 2/4    3/4    3/4    1/2 = 0.140625

p(s3) = p(r 2|s)p(r 3|np)p(r 5|vp)p(r 7|ap)

= 2/4    3/4    3/4    1/2 = 0.140625

p(s4) = p(r 1|s)p(r 4|np)p(r 6|vp)

= 2/4    1/4    1/4 = 0.03125

9 / 35

motivation behind such estimation

one criterion for    nding rule weights of a pid18 (or parameters in
general) is the maximum likelihood criterion.

it means we want to    nd rule weights which make the treebank we
observe most likely if we multiply in all probabilities together (we
assume the trees are independent)

counting and normalising satis   es this criterion

10 / 35

parameter estimation: intuition

suppose that we have a bag containing two types of marbles: red
and black. how would you estimate the ratio of red to black
marbles in the bag?
more precisely, what is p(red)? (note: p(black) = 1     p(red)).

11 / 35

parameter estimation: intuition

suppose that we have a bag containing two types of marbles: red
and black. how would you estimate the ratio of red to black
marbles in the bag?
more precisely, what is p(red)? (note: p(black) = 1     p(red)).
experiment. draw ten marbles from the bag (replacing them each
time). suppose you draw 7 red and 3 black marbles. what is
p(red)?

11 / 35

parameter estimation: intuition

suppose that we have a bag containing two types of marbles: red
and black. how would you estimate the ratio of red to black
marbles in the bag?
more precisely, what is p(red)? (note: p(black) = 1     p(red)).
experiment. draw ten marbles from the bag (replacing them each
time). suppose you draw 7 red and 3 black marbles. what is
p(red)?

1

2

3

.3

.5

.7

4 1

11 / 35

parameter estimation: intuition

suppose that we have a bag containing two types of marbles: red
and black. how would you estimate the ratio of red to black
marbles in the bag?
more precisely, what is p(red)? (note: p(black) = 1     p(red)).
experiment. draw ten marbles from the bag (replacing them each
time). suppose you draw 7 red and 3 black marbles. what is
p(red)?

1

2

3

.3

.5

.7

4 1

why?

11 / 35

parameter estimation: maximum likelihood

since we saw 7 red and 3 black marbles, we can write the likelihood
of the observed data in terms of the unknown parameter p(red):

p(data) = p(red)7    (1     p(red))3

(1)

p(red) is unknown. what   s a reasonable way to set it?

12 / 35

parameter estimation: maximum likelihood

since we saw 7 red and 3 black marbles, we can write the likelihood
of the observed data in terms of the unknown parameter p(red):

p(data) = p(red)7    (1     p(red))3

(1)

p(red) is unknown. what   s a reasonable way to set it?

how about this?

arg max

p(red)   [0,1]

p(data) = p(red)7    (1     p(red))3

(2)

12 / 35

parameter estimation: maximum likelihood

now we have a basic calculus problem. solve:

arg max

p(red)   [0,1]

p(data) = p(red)7    (1     p(red))3

(3)

13 / 35

parameter estimation: maximum likelihood

now we have a basic calculus problem. solve:

arg max

p(red)   [0,1]

p(data) = p(red)7    (1     p(red))3

(3)

what p(data) looks like:

13 / 35

id113

id113 is one of the most basic parameter estimation methods.
when you have lots of data, it   s a reasonable    rst choice.

what are some cases where it might not work?

14 / 35

id113

id113 is one of the most basic parameter estimation methods.
when you have lots of data, it   s a reasonable    rst choice.

what are some cases where it might not work?

question. what if you don   t have lots of data (for the parameter
you want to estimate)?

14 / 35

id113

id113 is one of the most basic parameter estimation methods.
when you have lots of data, it   s a reasonable    rst choice.

what are some cases where it might not work?

question. what if you don   t have lots of data (for the parameter
you want to estimate)?

14 / 35

parameter estimation

what if we don   t have a treebank, but we do have an unparsed
corpus and (non-probabilistic) parser?

1 take a id18 and set all rules to have equal id203.

2 parse the (   at) corpus with the id18.

3 adjust the probabilities.

4 repeat steps two and three until probabilities converge.

this is the inside-outside algorithm (baker, 1979), a type of
expectation maximisation algorithm. it can also be used to induce
a grammar, but only with limited success.

15 / 35

problems with standard pid18s

while standard pid18s are already useful for some purposes, they
can produce poor result when used for disambiguation.

why is that?

1 they assume the rule choices are independent of one another.

2 they ignore lexical information until the very end of the
analysis, when word classes are rewritten to word tokens.

how can this lead to bad choices among possible parses?

16 / 35

problem 1: assuming independence

by de   nition, a id18 assumes that the expansion of non-terminals
is completely independent. it doesn   t matter:

where a non-terminal is in the analysis;

what else is (or isn   t) in the analysis.

the same assumption holds for standard pid18s: the id203 of
a rule is the same, no matter

where it is applied in the analysis;

what else is (or isn   t) in the analysis.

but this assumption is too simple!

17 / 35

problem 1: assuming independence

s     np vp
vp     vbd np

np     pro
np     dt nom

the above rules assign the same id203 to both these trees,
because they use the same re-write rules, and id203
calculations do not depend on where rules are used.

s

s

np

vp

np

vp

vbd

np

pro

vbd

np

wrote

pro

they

wrote

them

18 / 35

problem 1: assuming independence

but in speech corpora, 91% of 31021 subject nps are pronouns:

(1)

she   s able to take her baby to work with her.

a.
b. my wife worked until we had a family.

while only 34% of 7489 object nps are pronouns:

(2)

a.
b.

some laws absolutely prohibit it.
it wasn   t clear how nl and mr. simmons would
respond if georgia gulf spurns them again.

so the id203 of np     pro should depend on where in the
analysis it applies (e.g., subject or object position).

19 / 35

another example of independence

question: which tree will get higher id203?

20 / 35

addressing the independence problem

one way of introducing greater sensitivity into pid18s is via parent
annotation: subdivide (all or some) non-terminal categories
according to the non-terminal that appears as the node   s
immediate parent. e.g. np subdivides into np s , np vp , . . .

s     np s vp s
vp s     vbd vp np vp

np s     pro
np vp     pro, etc.

s

s

np s

vp s

np s

vp s

vbd

np vp

pro

vbd

np vp

wrote

pro

they

wrote

them

21 / 35

addressing the independence problem

node-splitting via parent annotation allows di   erent probabilities
to be assigned e.g. to the rules
np s     pro,

np vp     pro

however, too much node-splitting can mean not enough data to
obtain realistic rule probabilities, unless we have an enormous
training corpus.

there are even algorithms that try to identify the optimal amount
of node-splitting for a given training set!

22 / 35

problem 2: ignoring lexical information

      
s     np vp
np     nns | nn
vp     vbd np | vbd np pp v     dumped | spotted
pp     p np
np     dt nn

n     sack | bin |
nns     students
dt     a | the
p     in

consider the sentences:

(3)

a. the students dumped the sack in the bin.
b. the students spotted the    aw in the plan.

because rules for rewriting non-terminals ignore word tokens until
the very end, let   s consider these simply as strings of pos tags:

(4)

dt nns vbd dt nn in dt nn

23 / 35

problem 2: ignoring lexical information

s

s

np

vp

np

dt

nns

vp

dt

nns

vbd

np

vbd

np

pp

dt

nn

in

np

dt

nn

np

pp

dt

nn

in

np

dt

nn

which do we want for the students dumped the sack in the bin?
which for the students spotted the    aw in the plan?

the most appropriate analysis depends in part on the actual words
occurring. the word dumped, implying motion, is more likely to
have an associated prepositional phrase than spotted.

24 / 35

lexicalized pid18s

a pid18 can be lexicalised by associating a word with every
non-terminal in the grammar.

it is head-lexicalised if the word is the head of the constituent
described by the non-terminal.

each non-terminal has a head that determines syntactic properties
of phrase (e.g., which other phrases it can combine with).

example

noun phrase (np): noun
adjective phrase (ap): adjective
verb phrase (vp): verb
prepositional phrase (pp): preposition

25 / 35

lexicalization

we can lexicalize a pid18 by annotating each non-terminal with its
head word, starting with the terminals     replacing
vp     v np pp
np     dt nn
np     nns

vp     v np
np     np pp
pp     p np

with rules such as
vp(dumped)     v(dumped) np(sack) pp(in)
vp(spotted)     v(spotted) np(   aw) pp(in)
vp(dumped)     v(dumped) np(sack)
vp(spotted)     v(spotted) np(   aw)
np(   aw)
pp(in)
pp(in)

    dt(the) nn(   aw)
    p(in) np(bin)
    p(in) np(plan)

26 / 35

head lexicalization

in principle, each of these rules can now have its own id203.
but that would mean a ridiculous expansion in the set of grammar
rules, with no parsed corpus large enough to estimate their
probabilities accurately.

instead we just lexicalize the head of phrase:
vp(dumped)     v(dumped) np pp
vp(spotted)     v(spotted) np pp
vp(dumped)     v(dumped) np
vp(spotted)     v(spotted) np
    dt nn(   aw)
np(   aw)
    p(in) np
pp(in)

such grammars are called lexicalized pid18s or, alternatively,
probabilistic lexicalized id18s.

27 / 35

head lexicalization

for lexicalized pid18s, rule probabilities can be reasonably
estimated from a corpus.

in the simplest version, the lexicalized rules are supplemented by
head selection rules, whose probabilities can also be estimated
from a corpus:
vp     vp(dumped)
vp     vp(spotted)
np     np(sack)
np     np(   aw)
pp     pp(in)

28 / 35

how many parameters in a lexicalized pid18?

when all phrases are annotated with head words (say the grammar
is in chomsky normal form, and we have a vocabulary of size v
and n nonterminals)?

when only the head phrase is annotated with a head word?

29 / 35

combining approaches

we can also combine the ideas of head lexicalization with parent
annotation, leading to rules like
np vp(dumped)     np(sack)vp(dumped)
np vp(spotted)     np(   aw )vp(spotted)
pp vp(dumped)     pp(in)vp(dumped)

the probabilities for such rules can be used to take account of
commonly occurring word combinations, e.g. of verb-object or
verb-preposition. these include long-distance correlations invisible
to id165 technology.

grammars with these doubly-lexicalized rules are still feasible,
given enough training data. this is roughly the idea behind the
collins parser.

30 / 35

a highly simpli   ed version of collins model 1

let a rule be lhs     lnln   1 . . . l1hr1 . . . rn   1rn
idea: break it into smaller probabilities,    generating    the head and
then left dependents and right dependents:

p(h|lhs)    pl(l1|h, lhs) . . . pl(ln|h, lhs)    pl(stop | ln, lhs)
   pr (r1|h, lhs) . . . pr (rn|h, lhs)    pr (stop | rn, lhs)

now can estimate each term separately, and the sparsity issue will
be less severe

31 / 35

hidden state grammars

lexicalization and node-splitting require careful thought about the
linguistic meaning of the splitting

we can try to automatically induce these splittings by introducing
hidden re   nements of the nonterminals

s

s1

np

vp

=   

np3

vp2

d

n

v

p

d1

n2

v4

p1

the

dog

saw

him

the

dog

saw

him

32 / 35

hidden state grammars

now our pid18 looks like:
s[1]     np[2] vp[3]
vp[4]     vbd[1] np[3]

np[1]     pro[2]
np[2]     pro[1], etc.

question: what kind of id172 do we expect now (say we
have m hidden states)?

33 / 35

how to estimate hidden state grammars?

similar to the inside-outside algorithm:

start with equal probabilities for all rules with hidden states

parse the treebank such that for each sentence the correct
tree has to be returned, while maximising the id203 of
the states

h = arg max p(h, t , w )

(w is the sentence, t is the treebank sentence, and h is a
set of hidden states associated with each node in t )

re-estimate the hidden state pid18 rules as if you have node
splitting on the nonterminals dictated by h

experiments on the id32 show that approximately 30-60
states are needed for getting good coverage

34 / 35

summary

the rule probabilities of a pid18 can be estimated by
counting how often the rules occur in a corpus.

the usefulness of pid18s is limited by the lack of lexical
information and by strong independence assumptions.

these limitations can be overcome by lexicalizing the
grammars, i.e., by conditioning the rule probabilities on the
head word of the rule.

demo: the stanford parser:
http://nlp.stanford.edu:8080/parser/

35 / 35

