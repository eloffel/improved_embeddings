cse 447 / 547

natural language processing

winter 2018

id48

yejin choi 

university of washington

[many slides from dan klein, michael collins, luke zettlemoyer]

overview

   id48
   learning

   supervised: maximum likelihood

   id136 (or decoding)

   viterbi
   forward backward

   id165 taggers

pairs of sequences

   consider the problem of jointly modeling a pair of strings

   e.g.: id52

dt    nnp      nn   vbd vbn  rp  nn        nns
the georgia branch had taken on loan commitments    

dt     nn     in     nn        vbd   nns      vbd
the average of interbank offered rates plummeted    
   q: how do we map each word in the input sentence onto the 

appropriate label?

   a: we can learn a joint distribution:

p(x1 . . . xn, y1 . . . yn)

   and then compute the most likely assignment:

arg max
y1...yn

p(x1 . . . xn, y1 . . . yn)

classic solution: id48s

   we want a model of sequences y and observations x

y0

y1

x1

y2

x2

yn+1

yn

xn

nyi=1

p(x1...xn, y1...yn+1) = q(stop|yn)

q(yi|yi 1)e(xi|yi)

where y0=start and we call q(y   |y) the transition distribution and e(x|y) the 
emission (or observation) distribution.

   assumptions:

   tag/state sequence is generated by a markov model
   words are chosen independently, conditioned only on the tag/state
   these are totally broken assumptions: why?

example: id52

the georgia branch had taken on loan commitments    

dt     nnp        nn        vbd    vbn   rp   nn        nns

   id48 model:

   states y = {dt, nnp, nn, ... } are the pos tags
   observations x = v are words
   transition dist   n q(yi |yi -1) models the tag sequences
   emission dist   n e(xi |yi) models words given their pos

   q:  how do we represent id165 pos taggers?

example: chunking

   goal: segment text into spans with certain properties
   for example, named entities: per, org, and loc
germany    s representative to the european union    s 
veterinary committee werner zwingman said on wednesday 
consumers should    

[germany]loc    s representative to the [european union]org    s 
veterinary committee [werner zwingman]per said on 
wednesday consumers should    
   q:  is this a tagging problem?

example: chunking

[germany]loc    s representative to the [european union]org    s 
veterinary committee [werner zwingman]per said on wednesday 
consumers should    

germany/bl    s/na representative/na to/na the/na european/bo 
union/co    s/na veterinary/na committee/na werner/bp zwingman/cp 
said/na on/na wednesday/na consumers/na should/na    

   id48 model:

   states y = {na,bl,cl,bo,co,bp,cp} represent beginnings 

(bl,bo,bp) and continuations (cl,co,cp) of chunks, as well 
as other words (na)

   observations x = v are words
   transition dist   n q(yi |yi -1) models the tag sequences
   emission dist   n e(xi |yi) models words given their type

example: id48 translation model

e:

a:

f:

1

2
thank you

3
,

4
i

5

6
shall do

8

7
so gladly

9
.

1

3

7

6

8

8

8

8

gracias ,

lo

har   de muy buen grado

9

.

emissions:  e( f1 = gracias | ea1 = thank )

transitions:  p( a2 = 3 | a1 = 1)

model parameters

id48 id136 and learning
   learning

   maximum likelihood: transitions q and emissions e
q(yi|yi 1)e(xi|yi)

p(x1...xn, y1...yn+1) = q(stop|yn)
   id136 (linear time in sentence length!)

nyi=1

   viterbi:

y    = argmax

y1...yn

p(x1...xn, y1...yn+1)
where yn+1 = stop

   forward backward:

p(x1 . . . xn, yi) = xy1...yi 1 xyi+1...yn

p(x1 . . . xn, y1 . . . yn)

learning: maximum likelihood

p(x1...xn, y1...yn+1) = q(stop|yn)
   learning (supervised learning)

nyi=1

   maximum likelihood methods for estimating 

transitions q and emissions e

q(yi|yi 1)e(xi|yi)

c(yi 1, yi)
c(yi 1)

em l(x|y) =
qm l(yi|yi 1) =
   will these estimates be high quality?
   which is likely to be more sparse, q or e?

c(y, x)
c(y)

   can use all of the same smoothing tricks we saw for 

language models!

learning: low frequency words

p(x1...xn, y1...yn+1) = q(stop|yn)
q(yi|yi 1)e(xi|yi)
   typically, linear interpolation works well for transitions

nyi=1

q(yi|yi 1) =  1qm l(yi|yi 1) +  2qm l(yi)
   however, other approaches used for emissions

   step 1: split the vocabulary

   frequent words: appear more than m (often 5) times
   low frequency: everything else

   step 2: map each low frequency word to one of a small, finite 

set of possibilities
   for example, based on prefixes, suffixes, etc.

   step 3: learn model for this new space of possible word 

sequences

low frequency words: an example
dealing with low-frequency words: an example
id39 [bickel et. al, 1999]
[bikel et. al 1999] (named-entity recognition)
   used the following word classes for infrequent words:
word class
twodigitnum
fourdigitnum
containsdigitandalpha
containsdigitanddash
containsdigitandslash
containsdigitandcomma
containsdigitandperiod
othernum
allcaps
capperiod
   rstword
initcap
lowercase
other

intuition
two digit year
four digit year
product code
date
date
monetary amount
monetary amount,percentage
other number
organization
person name initial
no useful capitalization information
capitalized word
uncapitalized word
punctuation marks, all other words

example
90
1990
a8956-67
09-96
11/9/89
23,000.00
1.00
456789
bbn
m.
   rst word of sentence
sally
can
,

18

low frequency words: an example

   profits/na soared/na at/na boeing/sc co./cc ,/na easily/na 
topping/na forecasts/na on/na wall/sl street/cl ,/na as/na 
their/na ceo/na alan/sp mulally/cp announced/na first/na 
quarter/na results/na ./na

   firstword/na soared/na at/na initcap/sc co./cc ,/na easily/na 

lowercase/na forecasts/na on/na initcap/sl street/cl ,/na as/na 
their/na ceo/na alan/sp initcap/cp announced/na first/na 
quarter/na results/na ./na

na = no entity 
sc = start company 
cc = continue company 
sl  = start location 
cl  = continue location
   

id136 (decoding)

   problem: find the most likely (viterbi) sequence under the model

y    = argmax

y1...yn

p(x1...xn, y1...yn+1)

   given model parameters, we can score any sequence pair

nnp        vbz               nn          nns           cd             nn           .
fed    raises     interest   rates      0.5      percent    .

q(nnp|   ) e(fed|nnp) q(vbz|nnp) e(raises|vbz) q(nn|vbz)   ..

   in principle, we   re done     list all possible tag sequences, 

score each one, pick the best one (the viterbi state sequence) 

nnp  vbz   nn  nns  cd  nn
nnp  nns  nn  nns  cd  nn
nnp  vbz  vb   nns  cd  nn

logp = -23
logp = -29
logp = -27

id145!

p(x1...xn, y1...yn+1) = q(stop|yn)

nyi=1

q(yi|yi 1)e(xi|yi)

y    = argmax
length i ending in tag yi

y1...yn

   define   (i,yi) to be the max score of a sequence of 

p(x1...xn, y1...yn+1)

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

= max
= max
= max
yi 1
yi 1
yi 1
= max
= max
= max
yi 1
yi 1
yi 1

e(xi|yi)q(yi|yi 1) max
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
y1...yi 2
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)

p(x1 . . . xi 1, y1 . . . yi 1)

   we now have an efficient algorithm. start with i=0 and 

work your way to the end of the sentence!

time flies like an arrow; 
fruit flies like a banana

16

fruit  

    (1,    )
    (1,    )
    (1,        )

flies  

    (2,    )
    (2,    )
    (2,        )

like       bananas

    (3,    )
    (3,    )
    (3,        )

    (4,    )
    (4,    )
    (4,        )

p
o
t
s

t
r
a
t
s

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

17

t
r
a
t
s

fruit  

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

flies  

    (2,    )
    (2,    )
    (2,        )

like       bananas

    (3,    )
    (3,    )
    (3,        )

    (4,    )
    (4,    )
    (4,        )

p
o
t
s

=0

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

18

t
r
a
t
s

fruit  

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

flies  

=0.005

    (2,    )
    (2,    )
    (2,        )

like       bananas

    (3,    )
    (3,    )
    (3,        )

    (4,    )
    (4,    )
    (4,        )

p
o
t
s

=0

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

19

t
r
a
t
s

fruit  

flies  

like       bananas

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

    (3,    )
    (3,    )
    (3,        )

    (4,    )
    (4,    )
    (4,        )

p
o
t
s

=0

=0

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

20

fruit  

flies  

like       bananas

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

t
r
a
t
s

=0.0001

    (3,    )
    (3,    )
    (3,        )

=0.0007

    (4,    )
    (4,    )
    (4,        )

p
o
t
s

=0

=0

=0.0003

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

21

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

t
r
a
t
s

=0.0001

    (3,    )
    (3,    )
    (3,        )

=0.0007

    (4,    )
    (4,    )
    (4,        )

p
o
t
s

=0

=0

=0.0003

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

= max
yi 1
= max
= max
= max
yi 1
yi 1
yi 1

e(xi|yi)q(yi|yi 1) max
y1...yi 2
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)

p(x1 . . . xi 1, y1 . . . yi 1)

fruit  

flies  

like       bananas

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

t
r
a
t
s

=0.0001

    (3,    )
    (3,    )
    (3,        )

=0.0007

=0.00003

    (4,    )
    (4,    )
    (4,        )

=0.00001

=0

=0

   (i, yi) = max

y1...yi 1

=0.0003

=0
p(x1 . . . xi, y1 . . . yi)

23

p
o
t
s

fruit  

flies  

like       bananas

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

t
r
a
t
s

=0.0001

    (3,    )
    (3,    )
    (3,        )

=0.0007

=0.00003

    (4,    )
    (4,    )
    (4,        )

=0.00001

=0

=0

   (i, yi) = max

y1...yi 1

=0.0003

=0
p(x1 . . . xi, y1 . . . yi)

24

p
o
t
s

fruit  

flies  

like       bananas

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

t
r
a
t
s

=0.0001

    (3,    )
    (3,    )
    (3,        )

=0.0007

=0.00003

    (4,    )
    (4,    )
    (4,        )

=0.00001

=0

=0

   (i, yi) = max

y1...yi 1

=0.0003

=0
p(x1 . . . xi, y1 . . . yi)

25

p
o
t
s

t
r
a
t
s

why is this not a greedy algorithm?
why does this find the max p(.)?

what is the runtime?

=0.03

    (1,    )
    (1,    )
    (1,        )

=0.01

=0.005

    (2,    )
    (2,    )
    (2,        )

=0.007

=0.0001

    (3,    )
    (3,    )
    (3,        )

=0.0007

=0.00003

    (4,    )
    (4,    )
    (4,        )

=0.00001

=0

=0

   (i, yi) = max

y1...yi 1

=0.0003

=0
p(x1 . . . xi, y1 . . . yi)

26

p
o
t
s

id145!

p(x1...xn, y1...yn+1) = q(stop|yn)

nyi=1

q(yi|yi 1)e(xi|yi)

y    = argmax
length i ending in tag yi

y1...yn

   define   (i,yi) to be the max score of a sequence of 

p(x1...xn, y1...yn+1)

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

= max
yi 1
= max
= max
= max
yi 1
yi 1
yi 1

e(xi|yi)q(yi|yi 1) max
y1...yi 2
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)

p(x1 . . . xi 1, y1 . . . yi 1)

   we now have an efficient algorithm. start with i=0 and 

work your way to the end of the sentence!

viterbi algorithm
   dynamic program for computing (for all i)

   (i, yi) = max

p(x1 . . . xi, y1 . . . yi)

   iterative computation

y1...yi 1

   (0, y0) =    1 if y0 == st art

0 otherwise

for i = 1 ... n:

   (i, yi) = max
yi 1

   also, store back pointers

e(xi|yi)q(yi|yi 1)   (i   1, yi 1)

bp(i, yi) = arg max
yi 1

e(xi|yi)q(yi|yi 1)   (i   1, yi 1)
   what is the final solution to                                                  ?
y    = argmax

p(x1...xn, y1...yn+1)

y1...yn

the viterbi algorithm: runtime
   linear in sentence length n
   polynomial in the number of possible tags |k|
e(xi|yi)q(yi|yi 1)   (i   1, yi 1)

   (i, yi) = max
yi 1

   specifically:

o(n|k|) entries in    (i, yi)
o(|k|) time to compute each    (i, yi)

   total runtime:

o(n|k|2)

   q: is this a practical algorithm?
   a: depends on |k|   .

broader context

   id125: viterbi decoding with k best sub-

solutions (beam size = k)

   viterbi algorithm - a special case of max-product

algorithm

   forward-backward - a special case of sum-product

algorithm (belief propagation algorithm)

   viterbi decoding can be also used with general graphical 

models (factor graphs, markov random fields, 
id49,    ) with non-probabilistic 
scoring functions (potential functions).

30

reflection

   viterbi: why argmax over joint distribution?
p(x1 . . . xn, y1 . . . yn)

y    = arg max
y1...yn

   why not this:

   same thing!

marginal id136
   problem: find the marginal id203 of each tag for yi

p(x1 . . . xn, yi) = xy1...yi 1 xyi+1...yn

p(x1 . . . xn, y1 . . . yn+1)

   given model parameters, we can score any sequence pair

nnp        vbz               nn          nns           cd             nn           .
fed    raises     interest   rates      0.5      percent    .

q(nnp|   ) e(fed|nnp) q(vbz|nnp) e(raises|vbz) q(nn|vbz)   ..

   in principle, we   re done     list all possible tag sequences, 
score each one, sum over all of the possible values for yi

nnp  vbz   nn  nns  cd  nn
nnp  nns  nn  nns  cd  nn
nnp  vbz  vb   nns  cd  nn

logp = -23
logp = -29
logp = -27

marginal id136
   problem: find the marginal id203 of each tag for yi

p(x1 . . . xn, yi) = xy1...yi 1 xyi+1...yn

p(x1 . . . xn, y1 . . . yn+1)

compare it to    viterbi id136   

   (i, yi) = max

y1...yi 1

p(x1 . . . xi, y1 . . . yi)

the state lattice / trellis: viterbi

^

n

v

j

d

$

^

e(fed|n)

n

v

j

d

$

^

n

^

n

e(raises|v) e(interest|v)

^

n

v

q(v|v)

v

j

d

$

v

j

d

$

e(rates|j)

j

d

$

^

n

e(stop|v)

v

j

d

$

start       fed           raises       interest         rates         stop

the state lattice / trellis: marginal
p(x1 . . . xn, yi) = xy1...yi 1 xyi+1...yn
p(x1 . . . xn, y1 . . . yn+1)

^

^

^

^

^

^

n

v

j

d

$

n

v

j

d

$

n

v

j

d

$

n

v

j

d

$

n

v

j

d

$

n

v

j

d

$

start       fed           raises       interest         rates         stop

id145!
p(x1 . . . xn, yi) = p(x1 . . . xi, yi)p(xi+1 . . . xn|yi)
   sum over all paths, on both sides of each yi
 (i, yi) = p(x1 . . . xi, yi) = xy1...yi 1
 (i, yi) = p(xi+1 . . . xn|yi) = xyi+1...yn

e(xi|yi)q(yi|yi 1) (i   1, yi 1)

=xyi 1

p(x1 . . . xi, y1 . . . yi)

=xyi+1

e(xi+1|yi+1)q(yi+1|yi) (i + 1, yi+1)

p(xi+1 . . . xn, yi+1 . . . yn+1|yi)

the state lattice / trellis: forward
 (i, yi) = p(x1 . . . xi, yi) = xy1...yi 1
p(x1 . . . xi, y1 . . . yi)

e(xi|yi)q(yi|yi 1) (i   1, yi 1)

=xyi 1

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

start       fed           raises       interest         rates         stop

the state lattice / trellis: backward
 (i, yi) = p(xi+1 . . . xn|yi) = xyi+1...yn
p(xi+1 . . . xn, yi+1 . . . yn+1|yi)

e(xi+1|yi+1)q(yi+1|yi) (i + 1, yi+1)

=xyi+1

^

n

v

j

d

^

n

v

j

d

^

n

v

j

d

^

n

v

j

d

^

n

v

j

d

^

n

v

j

d

$

$
start       fed           raises       interest         rates         stop

$

$

$

$

forward backward algorithm

   two passes: one forward, one back

   forward:

   for i = 1     n

   backward:

0 otherwise

 (0, y0) =    1 if y0 == st art
 (i, yi) =xyi 1
 (n, yn) =    1 if yn == st op
 (i, yi) =xyi+1

   for i = n-1 ... 0

q(yn+1|yn) if yn+1 = stop
0 otherwise

e(xi|yi)q(yi|yi 1) (i   1, yi 1)

e(xi+1|yi+1)q(yi+1|yi) (i + 1, yi+1)

forward backward: runtime

   linear in sentence length n
   polynomial in the number of possible tags |k|

 (i, yi) =xyi 1
 (i, yi) =xyi+1

   specifically:

   total runtime:

e(xi|yi)q(yi|yi 1) (i   1, yi 1)
e(xi+1|yi+1)q(yi+1|yi) (i + 1, yi+1)

o(n|k|) entries in  (i, yi) and    (i, yi)
o(|k|) time to compute each entry
o(n|k|2)

   q: how does this compare to viterbi?
   a: exactly the same!!! 

   we   ve been doing this:

other marginal id136
p(x1 . . . xn, yi) = xy1...yi 1 xyi+1...yn

p(x1 . . . xn, y1 . . . yn+1)

   can we compute this?

= xy1...yn

p(x1 . . . xn, y1 . . . yn+1)

other marginal id136

   can we compute this?

   relation with forward quantity?

 (i, yi) = p(x1 . . . xi, yi) = xy1...yi 1

p(x1 . . . xn, y1 . . . yn+1)

p(x1 . . . xi, y1 . . . yi)

= xy1...yn

unsupervised learning (em) intuition
   we   ve been doing this:
p(x1 . . . xn, yi) = xy1...yi 1 xyi+1...yn

p(x1 . . . xn, y1 . . . yn+1)

   what we really want is this: (which we now know how to compute!)

   this means we can compute the expected count of things

unsupervised learning (em) intuition
   what we really want is this: (which we now know how to compute!)

   this means we can compute the expected count of things:

   if we have this:

   we can also compute expected transition counts:

   above marginals can be computed as

unsupervised learning (em) intuition

   expected emission counts:

   maximum likelihood parameters (supervised learning):

qm l(yi|yi 1) =

c(yi 1, yi)
c(yi 1)

em l(x|y) =

c(y, x)
c(y)

   for unsupervised learning, replace the actual counts with the 

expected counts.

expectation maximization

   initialize transition and emission parameters

   random, uniform, or more informed initialization

   iterate until convergence

   e-step:

   compute expected counts 

   m-step:

   compute new transition and emission parameters (using the 

expected counts computed above)

c(yi 1, yi)
c(yi 1)
   convergence? yes. global optimum? no

qm l(yi|yi 1) =

em l(x|y) =

c(y, x)
c(y)

equivalent to the procedure given 
in the textbook (j&m)     slightly 
different notations

how is unsupervised learning possible (at all)?
   i water the garden everyday
   saw a weird bug in that garden    
   while i was thinking of an equation    

noun
s: (n) garden (a plot of ground where plants are cultivated)
s: (n) garden (the flowers or vegetables or fruits or herbs that are cultivated in a garden)
s: (n) garden (a yard or lawn adjoining a house)
verb
s: (v) garden (work in the garden) "my hobby is gardening"
adjective
s: (adj) garden (the usual or familiar type) "it is a common or garden sparrow"

48

does em learn good id48 pos-taggers?

      why doesn   t em find good id48 pos-taggers   , 

johnson, emnlp 2007

id48s estimated by em 
generally assign a roughly 
equal number of word 
tokens to each hidden state, 
while the empirical 
distribution of tokens to 
pos tags is highly skewed

49

unsupervised learning results
   em for id48

   pos accuracy: 74.7%

   bayesian id48 learning [goldwater, griffiths 07]

   significant effort in specifying prior distriubtions
   integrate our parameters e(x|y) and t(y   |y)
   pos accuracy: 86.8%

   unsupervised, feature rich models [smith, eisner 05]

   challenge: represent p(x,y) as a log-linear model, which requires 

normalizing over all possible sentences x

   smith presents a very clever approximation, based on local 

neighborhoods of x

   pos accuracy: 90.1%

   newer, feature rich methods do better, not near 

supervised sota

quiz: p(s1) vs. p(s2)
   s1 = colorless green ideas sleep furiously.
   s2 = furiously sleep ideas green colorless

      it is fair to assume that neither sentence (s1) nor (s2) had ever 

occurred in an english discourse. hence, in any statistical model for 
grammaticalness, these sentences will be ruled out on identical 
grounds as equally "remote" from english    (chomsky 1957)

   how would p(s1) and p(s2) compare based on (smoothed) 

bigram language models?

   how would p(s1) and p(s2) compare based on marginal 

id203 based on pos-tagging id48s? 
   i.e., marginalized over all possible sequences of pos tags

51

