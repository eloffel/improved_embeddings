we have a sitting situation

   447 enrollment: 67 out of 64
   547 enrollment: 10 out of 10
   2 special approved cases for audits
   ----------------------------------------
   67 + 10 + 2 = 79 students in the class!
   there are 80 chairs in this classroom.

engineering practice for hws
   for hw#1

   no lm software/apis, no unk handling, no 

smoothing

   do use basic data structure apis (e.g., 
hashmap of java, dictionary of python)

   when in doubt, ask on canvas!
   python vs c++?

   importance of coding skills

announcements

   hw#1 is out! 

   due jan 19th fri 11:59pm
   small dataset v.s. full dataset
   two fairly common struggles:

   reasonably efficient coding to handle a 
moderately sized corpus (data structure)

   correct understanding of conditional probabilities 

& handling of unknowns

   start early!

announcements

   447 vs 547

   4 paper reading & discussions

   due every 2 weeks

   1 final report on literature survey

   ~ 3 pages on ~ 5 papers
   on topic of your choosing
   due at the end of the quarter

cse 447/547 

natural language processing

winter 2018

language models

yejin choi

slides adapted from dan klein, michael collins, luke zettlemoyer, dan jurafsky

overview

   the id38 problem
   id165 language models
   evaluation: perplexity
   smoothing

   add-n 
   linear interpolation
   discounting methods 

the id38 problem

n setup: assume a (finite) vocabulary of words

n we can construct an (infinite) set of strings

v    = {the, a, the a, the fan, the man, the man with the telescope, ...}

x   v   
n data: given a training set of example sentences             
n problem: estimate a id203 distribution

p(x) = 1

xx v   

and p(x)   0 for all x     v   

p(the) = 10 12
p(a) = 10 13
p(the fan) = 10 12
p(the fan saw beckham) = 2     10 8
p(the fan saw saw) = 10 15
. . .

n question: why would we ever want to do this?

id103

   automatic id103 (asr)

   audio in, text out
   sota: 0.3% error for digit strings, 5% dictation, 50%+ tv

   wreck a nice beach?   

      recognize speech   
      i ate a cherry   

   eye eight uh jerry?   

the noisy-channel model

n we want to predict a sentence given acoustics:

n the noisy channel approach:

acoustic model: distributions 
over acoustic waves given a 

sentence 

language model: 

distributions over sequences 

of words (sentences)

acoustically scored hypotheses

the station signs are in deep in english
the stations signs are in deep in english
the station signs are in deep into english
the station 's signs are in deep in english
the station signs are in deep in the english
the station signs are indeed in english
the station 's signs are indeed in english
the station signs are indians in english
the station signs are indian in english
the stations signs are indians in english
the stations signs are indians and english

-14732
-14735
-14739
-14740
-14741
-14757
-14760
-14790
-14799
-14807
-14815

asr system components

language model

source
p(w)

best
w

w

decoder

acoustic model
channel
p(a|w)

a

observed     

a

argmax p(w|a) = argmax p(a|w)p(w)

w

w

translation: codebreaking?

   also knowing nothing official about, but having guessed and 
inferred considerable about, the powerful new mechanized 
methods in cryptography   methods which i believe succeed 
even when one does not know what language has been 
coded   one naturally wonders if the problem of translation 
could conceivably be treated as a problem in cryptography.  
when i look at an article in russian, i say: 
   this is really written in english, but it has 
been coded in some strange symbols. i will 
now proceed to decode.       

   warren weaver (1955:18, quoting a letter he 

wrote in 1947)

mt system components

language model

source
p(e)

best
e

e

decoder

translation model
channel
p(f|e)

f

observed     

f

argmax p(e|f) = argmax p(f|e)p(e)

e

e

learning language models

   goal: assign useful probabilities p(x) to sentences x

   input: many observations of training sentences x
   output: system capable of computing p(x)

   probabilities should broadly indicate plausibility of sentences

   p(i saw a van) >> p(eyes awe of an)
   not grammaticality: p(artichokes intimidate zippers)    0
   in principle,    plausible    depends on the domain, context, speaker   

   one option: empirical distribution over training sentences   

p(x1 . . . xn) =

c(x1 . . . xn)

n

for sentence x = x1 . . . xn

   problem: does not generalize (at all)
   need to assign non-zero id203 to previously unseen sentences!

   assumption: each word xi is generated i.i.d.

unigram models
nyi=1

q(xi) where xxi2v   

p(x1...xn) =

q(xi) = 1 and v    := v [ {stop}

   generative process: pick a word, pick a word,     until you pick stop
   as a graphical model:

x1

x2

            .

xn-1

stop

   examples:

  
  
  
  
  

[fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass.]
[thrift, did, eighty, said, hard, 'm, july, bullish]
[that, or, limited, the]
[]
[after, any, on, consistently, hospital, lake, of, of, other, and, factors, raised, analyst, too, allowed, 
mexico, never, consider, fall, bungled, davison, that, obtain, price, lines, the, to, sass, the, the, further, 
board, a, details, machinists, the, companies, which, rivals, an, because, longer, oakes, percent, a, 
they, three, edward, it, currier, an, within, in, three, wrote, is, you, s., longer, institute, dentistry, pay, 
however, said, possible, to, rooms, hiding, eggs, approximate, financial, canada, the, so, workers, 
advancers, half, between, nasdaq]

   big problem with unigrams: p(the the the the) vs p(i like ice cream) ?

bigram models
nyi=1
q(xi|xi 1) where xxi2v   

p(x1...xn) =

q(xi|xi 1) = 1

x0 = start & v    := v [ {stop}
   generative process: (1) generate the very first word conditioning on the special 
symbol start, then, (2) pick the next word conditioning on the previous word, 
then repeat (2) until the special word stop gets picked.

   graphical model:

start

x1

x2

xn-1

stop

   subtleties:

  

if we are introducing the special start symbol to the model, then we are making the 
assumption that the sentence always starts with the special start word start, thus 
when we talk about                        it is in fact                                             

p(x1...xn)

   while we add the special stop symbol to the vocabulary       , we do not add the 

p(x1...xn|x0 = start)

special start symbol to the vocabulary. why?

v   

bigram models

   alternative option:

p(x1...xn) = q(x1)

nyi=2

q(xi|xi 1) where xxi2v   

q(xi|xi 1) = 1

   generative process: (1) generate the very first word based on the unigram 
model, then, (2) pick the next word conditioning on the previous word, then 
repeat (2) until the special word stop gets picked.

   graphical model:

x1

x2

xn-1

stop

   any better?

  

  
  

  

[texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., 
gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, 
fifty, five, yen]
[outside, new, car, parking, lot, of, the, agreement, reached]
[although, common, shares, rose, forty, six, point, four, hundred, dollars, from, thirty, 
seconds, at, the, greatest, play, disingenuous, to, be, reset, annually, the, buy, out, of, 
american, brands, vying, for, mr., womack, currently, sharedata, incorporated, believe, 
chemical, prices, undoubtedly, will, be, as, much, is, scheduled, to, conscientious, 
teaching]
[this, would, be, a, record, november]

id165 model decomposition

   k-gram models (k>1): condition on k-1 previous words

1.3. trigram language models

p(x1 . . . xn) =

q(xi|xi (k 1) . . . xi 1)

nyi=1

1.3. trigram language models

where xi 2v[{ st op} and x k+2 . . . x0 =    

we would have
   example: tri-gram
p(the dog barks stop) = q(the|*, *)   q(dog|*, the)   q(barks|the, dog)   q(stop|dog, barks)
note that in this expression we have one term for each word in the sentence (the,
dog, barks, and stop). each word depends only on the previous two words: this
   learning: estimate the distributions  
is the trigram assumption.

p(the dog barks stop) = q(the|*, *)   q(dog|*, the)   q(barks|the, dog)   q(stop|dog, barks)
note that in this expression we have one term for each word in the sentence (the,
dog, barks, and stop). each word depends only on the previous two words: this
is the trigram assumption.

the parameters satisfy the constraints that for any trigram u, v, w,

q(xi|xi (k 1) . . . xi 1)

7

the parameters satisfy the constraints that for any trigram u, v, w,

q(w|u, v)   0

generating sentences by sampling from id165 models

assignment 2: id48-based part-of-speech tagging

    due nov 30 11:59 pm, submission to blackboard    

unigram lms are well defined dist   ns*

1 overall goal

   simplest case: unigrams

nyi=1

p(x1...xn) =

q(xi)

in this homework you will implement hidden markov model (id48) part-of-
speech tagger. brie   y, the input to the system will be a sentence (a sequence
of words and punctuation tokens), and the output will be the part-of-speech
tags of that sentence. you may work in groups of 2 students. students in
the same group get the same grade for this assignment.

   generative process: pick a word, pick a word,     until you pick stop
   for all strings x (of any length): p(x)   0 
   claim:  the sum over string of all lengths is 1 :   xp(x) = 1
(1)

(2)

p(x) =

xx
xx1...xn
p(x1...xn) = xx1...xn
=xx1
p(x1)     ...    xxn
xx
1xn=1

1xn=1
(1   ps)n 1ps = ps

(1)+(2)

p(x) =

p(x) =

p(x1...xn)

1xn=1 xx1...xn
nyi=1
p(x1...xn) = xx1...xn
q(xi) =xx1
nyi=1
...xxn
p(xi) =xx1
q(x1)     ...    xxn
q(xn) = (1   qs)n 1qs where qs = q(stop)
p(xn) = (1   ps)n 1ps where ps = p(stop)
(1   qs)n 1 = qs

p(x1)     ...     p(xn)

q(x1)     ...     q(xn)

...xxn

1

1xn=1
(1   ps)n 1 = ps

(1   qs)n 1qs = qs
1xn=1

1

= 1

1   (1   ps)

= 1

1   (1   qs)

id165 model parameters

   the parameters of an id165 model:

   maximum likelihood estimate: relative frequency

qm l(w) =

c(w)
c()

, qm l(w|v) =

c(v, w)

c(v)

, qm l(w|u, v) =

c(u, v, w)

c(u, v)

,

. . .

where c is the empirical counts on a training set

   general approach

   take a training set d and a test set d   
   compute an estimate of the q(.) from d
   use it to assign probabilities to other sentences, such as those in d   

 

s
t
n
u
o
c
g
n
n
a
r
t

i

i

198015222 the first
194623024 the same
168504105 the following
158562063 the world
   
14112454 the door
-----------------
23135851162 the *

q(door|the) =

14112454

2313581162

= 0.0006

measuring model quality

   the goal isn   t to pound out fake sentences!

   obviously, generated sentences get    better    as we increase the 

model order

   more precisely: using ml estimators, higher order is always 

better likelihood on train, but not test

   what we really want to know is:

   will our model prefer good sentences to bad ones?
   bad     ungrammatical!
   bad    unlikely
   bad = sentences that our acoustic model really likes but aren   t 

the correct answer

measuring model quality

   the shannon game:

   how well can we predict the next word?

when i eat pizza, i wipe off the ____
many children are allergic to ____
i saw a ____

grease 0.5
sauce 0.4
dust 0.05
   .
mice 0.0001
   .
the     1e-100

   unigrams are terrible at this game.  (why?)

   how good are we doing?

compute per word log likelihood (m words, m test sentences si):

l =

1
m

mxi=1

log p(si)

claude shannon

perplexity

the best language model is 
one that best predicts an 
unseen test set

perplexity is the inverse 
id203 of the test set, 
normalized by the 
number of words (why?)

pp(w) = p(w1w2...wn)   

1
n

            =

n

1

p(w1w2...wn)

equivalently :
pp(w) = 2   l 
1
n logp(w1w2...wn)
where  l =

2 l where l =

1
m

mxi=1

log p(si)

the shannon game intuition for perplexity

   how hard is the task of recognizing digits    0,1,2,3,4,5,6,7,8,9    

at random
   perplexity 10

   how hard is recognizing (30,000) names at random 

   perplexity = 30,000

   if a system has to recognize

   operator (1 in 4)
   sales (1 in 4)
   technical support (1 in 4)
   30,000 names (1 in 120,000 each)
   perplexity is 53

   perplexity is weighted equivalent branching factor

perplexity as branching factor
   language with higher perplexity means the number of 

words branching from a previous word is larger on 
average.

   the difference between the perplexity of a language 
model and the true perplexity of the language is an 
indication of the quality of the model.

lower perplexity = better model
   training 38 million words, test 1.5 million words, wsj

id165 order
perplexity

unigram bigram
962

170

trigram
109

   "an estimate of an upper bound for the id178 of 

english". brown, peter f.; et al. (march 1992). 
computational linguistics 18 (1)

   important note:

   it   s easy to get bogus perplexities by having bogus probabilities 
that sum to more than one over their event spaces.  be careful in 
homeworks!

extrinsic evaluation

   intrinsic evaluation: e.g., perplexity 

   easier to use, but does not necessarily correlate the model 

performance when situated in a downstream application.

   extrinsic evaluation: e.g., id103, machine translation

   harder to use, but shows the true quality of the model in the 

context of a specific downstream application.

   better perplexity might not necessarily lead to better word error 

rate (wer) for id103.

   word error rate (wer) :=

insertions + deletions + substitutions

true sentence size

correct answer:

andy saw a part of the movie

recognizer output: and he saw apart of the movie

wer: 4/7 
= 57%

sparsity

   problems with id165 models:

   new words appear all the time:

   synaptitute
   132,701.03
   multidisciplinarization

 

n
e
e
s
n
o
i
t
c
a
r
f

1

0.8

0.6

0.4

0.2

0

   new id165s: even more often

   zipf   s law

200000
   types (words) vs. tokens (word occurrences)
   broadly: most word types are rare ones
   specifically: 

0

   rank word types by token frequency
   frequency inversely proportional to rank

unigrams
bigrams
rules

400000
600000
number of words

800000

1000000

   not special to language: randomly generated character strings 

have this property (try it!)

   this is particularly problematic when   

   training set is small (does this happen for id38?)
   transferring domains: e.g., newswire, scientific literature, twitter

zeros

    test set

    denied the offer
    denied the loan

   training set:

    denied the allegations
    denied the reports
    denied the claims
    denied the request

p(   offer    | denied the) = 0

zero id203 bigrams

   bigrams with zero id203

   mean that we will assign 0 id203 to the test set!
   it also means that we cannot compute perplexity (can   t 

divide by 0)!

pp(w) = p(w1w2...wn)   

1
n

            =

n

1

p(w1w2...wn)

equivalently :
pp(w) = 2   l 
1
n logp(w1w2...wn)
where  l =

parameter estimation

   maximum likelihood estimates won   t get us 

very far

qm l(w) =

c(w)
c()

, qm l(w|v) =

c(v, w)

c(v)

, qm l(w|u, v) =

c(u, v, w)

c(u, v)

,

. . .

   need to smooth these estimates
   general method (procedurally)

   take your empirical counts
   modify them in various ways to improve 

estimates

   general method (mathematically)

   often can give estimators a formal statistical 

interpretation     but not always

   approaches that are mathematically obvious 

aren   t always what works

3516 wipe off the excess 
1034 wipe off the dust
547 wipe off the sweat
518 wipe off the mouthpiece
   
120 wipe off the grease
0 wipe off the sauce
0 wipe off the mice
-----------------
28048 wipe off the *

smoothing

   we often want to make estimates from sparse statistics:

p(w | denied the)
3 allegations
2 reports
1 claims
1 request
7 total

s
n
o

i
t

a
g
e

l
l

a

s
t
r
o
p
e
r

s
e
g
r
a
h
c

t
s
e
u
q
e
r

s
m
a
c

l

i

n
o

i
t

o
m

s
t
i
f

e
n
e
b

   

   smoothing flattens spiky distributions so they generalize better

p(w | denied the)
2.5 allegations
1.5 reports
0.5 claims
0.5 request
2 other
7 total

s
n
s
o
n
i
o
t
a
i
t
g
a
e
g
e
a
a

l
l

l
l

s
t
r
o
p
e
r

s
e
g
r
a
h
c

t
s
e
u
q
e
r

s
m
a
c

i

l

n
o

i
t

o
m

s
t
i
f

e
n
e
b

   

   very important all over nlp (and ml more generally), but easy to do badly!
   question: what is the best way to do it?

add-one estimation

   also called laplace smoothing
   pretend we saw each word one more time than we did
   just add one to all the counts!

   id113 estimate:

qid113(xi|xi 1) =

c(xi 1, xi)
c(xi 1)

   add-1 estimate:

qadd-1(xi|xi 1) =

c(xi 1, xi) + 1
c(xi 1) + |v   |

more general formulations

add-k: 

qadd-k(xi|xi 1) =

c(xi 1, xi) + k
c(xi 1) + k|v   |

qadd-k(xi|xi 1) =

c(xi 1, xi) + m 1
|v   |
c(xi 1) + m

unigram prior smoothing:

quniform-prior(xi|xi 1) =

c(xi 1, xi) + m q(xi)

c(xi 1) + m

add-1 estimation is a blunt instrument

   so add-1 isn   t used for id165s: 

   we   ll see better methods

   but add-1 is used to smooth other nlp models

   for text classification 
   in domains where the number of zeros isn   t so huge.

linear interpolation
   problem:                         is supported by few counts
   classic solution: mixtures of related, denser histories:

qm l(w|u, v)

q(w|u, v) =  3qm l(w|u, v) +  2qm l(w|v) +  1qm l(w)

   is this a well defined distribution?

   yes, if all   i   0 and they sum to 1

   the mixture approach tends to work better than add-   

approach for several reasons
   can flexibly include multiple back-off contexts
   good ways of learning the mixture weights with em (later)
   not entirely clear why it works so much better

   all the details you could ever want: [chen and goodman, 98]

experimental design

   important tool for optimizing how models generalize:

training data

validation

data

test
data

   training data: use to estimate the base id165 models without smoothing
   validation data (or    development    data): use to pick the values of    hyper-

parameters    that control the degree of smoothing by maximizing the (log-) 
likelihood of the validation data

   can use any optimization technique (line search or em usually easiest)

   examples:

qadd-k(xi|xi 1) =

c(xi 1, xi) + k
c(xi 1) + k|v   |

l

q(w|u, v) =  3qm l(w|u, v) +  2qm l(w|v) +  1qm l(w)

k

handling unknown words

   if we know all the words in advanced

   vocabulary v is fixed
   closed vocabulary task

   often we don   t know this

   out of vocabulary = oov words
   open vocabulary task

   instead: create an unknown word token <unk>

   training of <unk> probabilities

   create a fixed lexicon l of size v
   at text id172 phase, any training word not in l changed to  

<unk>

   at decoding time

   now we train its probabilities like a normal word

   if text input: use unk probabilities for any word not in training

practical issues

   we do everything in log space

   avoid underflow
   (also adding is faster than multiplying)
   (though log can be slower than multiplication)

log(p1    p2    p3    p4) = log p1 +log p2 +log p3 +log p4

* advanced topics for 

smoothing

held-out reweighting

   what   s wrong with add-d smoothing?
   let   s look at some real bigram counts [church and gale 91]:

count in 22m words
1
2
3
4
5

actual c* (next 22m)
0.448
1.25
2.24
3.23
4.21

add-one   s c*
2/7e-10
3/7e-10
4/7e-10
5/7e-10
6/7e-10

add-0.0000027   s c*
~1
~2
~3
~4
~5

mass on new 
ratio of 2/1

9.2%
2.8

~100%
1.5

9.2%
~2

   big things to notice:

   add-one vastly overestimates the fraction of new bigrams
   add-0.0000027 vastly underestimates the ratio 2*/1*

   one solution: use held-out data to predict the map of c to c*

absolute discounting

   idea 1: observed id165s occur more in training than they will later:

future c* (next 22m)
0.448
1.25
2.24
3.23
   absolute discounting (bigram case)

count in 22m words
1
2
3
4

   no need to actually have held-out data; just subtract 0.75 (or some d)

c   (v, w) = c(v, w)   0.75 and q(w|v) =

   but, then we have    extra    id203 mass

   (v) = 1  xw

c   (v, w)

c(v)

   question: how to distribute    between the unseen words?

c   (v, w)

c(v)

katz backoff

   absolute discounting, with backoff to unigram estimates

c   (v, w) = c(v, w)    

   define the words into seen and unseen

   (v) = 1  xw

c   (v, w)

c(v)

words

a(v) = {w : c(v, w) > 0} b(v) = {w : c(v, w) = 0}
qbo(w|v) =( c   (v,w)

   now, backoff to maximum likelihood unigram estimates for unseen 
if w 2 a(v)
if w 2 b(v)
   can consider hierarchical formulations: trigram is recursively backed 

pw02b(v) qm l(w0)

   (v)    

qm l(w)

c(v)

off to katz bigram estimate, etc

   can also have multiple count thresholds (instead of just 0 and >0)

good-turing discounting*

   question: why the same d for all id165s?
   good-turing discounting: invented during wwii by alan turing and 

later published by good. frequency estimates were needed for 
enigma code-breaking effort.

   let nr be the number of id165s x for which c(x) = r
   now, use the modified counts

c   (x) = (r + 1)

i    c(x) = r, r > 0

nr+1
nr

   then, our estimate of the missing mass is:
n1
n

   (v) =

   where n is the number of tokens in the training set

kneser-ney backoff*

   idea: type-based fertility

   shannon game:  there was an unexpected ____?

   delay?
   francisco?

      francisco    is more common than    delay   
       but    francisco    (almost) always follows    san   
       so it   s less    fertile   

   solution: type-continuation probabilities

   in the back-off model, we don   t want the unigram estimate pml
   instead, want the id203 that w is allowed in a novel context
   for each word, count the number of bigram types it completes

   kn smoothing repeatedly proven effective
   [teh, 2006] shows it is a kind of approximate id136 in a hierarchical 

pitman-yor process (and other, better approximations are possible)

what actually works?

   trigrams and beyond:
   unigrams, bigrams 

generally useless

   trigrams much better (when 

there   s enough data)

   4-, 5-grams really useful in 

mt, but not so much for 
speech
   discounting

   absolute discounting, good-
turing, held-out estimation, 
witten-bell, etc   

   see [chen+goodman] 

reading for tons of graphs   

[graphs from

joshua goodman]

data vs. method?

   having more data is better   

y
p
o
r
t
n
e

10
9.5
9
8.5
8
7.5
7
6.5
6
5.5

100,000 katz
100,000 kn
1,000,000 katz
1,000,000 kn
10,000,000 katz
10,000,000 kn
all katz
all kn

1

2

3

4

5

6

7

8

9 10 20

id165 order

       but so is using a better estimator
   another issue: n > 3 has huge costs in speech recognizers

tons of data?

   tons of data closes gap, for extrinsic mt evaluation

beyond id165 lms

   lots of ideas we won   t have time to discuss:

   caching models: recent words more likely to appear again
   trigger models: recent words trigger other words
   topic models

   a few recent ideas

   syntactic models: use tree models to capture long-distance 

syntactic effects [chelba and jelinek, 98]

   discriminative models: set id165 weights to improve final task 
accuracy rather than fit training set density [roark, 05, for asr;  
liang et. al., 06, for mt]

   structural zeros: some id165s are syntactically forbidden, keep 

estimates at zero [mohri and roark, 06]

   bayesian document and ir models [daume 06]

google id165 release, august 2006

   

google id165

   serve as the incoming 92
   serve as the incubator 99
   serve as the independent 794
   serve as the index 223
   serve as the indication 72
   serve as the indicator 120
   serve as the indicators 45
   serve as the indispensable 111
   serve as the indispensible 40
   serve as the individual 234
http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-you.html

huge web-scale id165s

   how to deal with, e.g., google id165 corpus
   pruning

   only store id165s with count > threshold.

   remove singletons of higher-order id165s

   id178-based pruning

   efficiency

   efficient data structures like tries
   bloom filters: approximate language models
   store words as indexes, not strings

   use huffman coding to fit large numbers of words into two bytes

   quantize probabilities (4-8 bits instead of 8-byte float)

smoothing for web-scale id165s
      stupid backoff    (brants et al. 2007)
   no discounting, just use relative frequencies 

s(wi | wi   k+1

i   1 ) =

"
$$
#
$
$
%

i

)

count(wi   k+1
count(wi   k+1
0.4s(wi | wi   k+2
i   1

i   1 )   if  count(wi   k+1
)      otherwise

i

) > 0

s(wi) =

count(wi)

n

* additional details on 

1. good turing
2. kneser-ney

notation: nc = frequency of frequency c
   nc = the count of things we   ve seen c times
   sam i am i am sam i do not eat
i   3
sam 2
am  2
do  1
not 1
eat 1

n1 = 3
n2 = 2
n3 = 1

good-turing smoothing intuition
   you are fishing (a scenario from josh goodman), and 

caught:
   10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish

   how likely is it that next species is trout?

   1/18

   how likely is it that next species is new (i.e. catfish or bass)
   let   s use our estimate of things-we-saw-once to estimate the new 

things.

   3/18 (because n1=3)

   assuming so, how likely is it that next species is trout?

   must be less than 1/18
   how to estimate? 

good turing calculations
(c +1)nc+1

* (things with zero frequency) =
pgt

c* =

n1
n

nc

unseen (bass or catfish)

   c = 0:
   id113 p = 0/18 = 0
   p*

gt (unseen) = n1/n = 3/18

seen once (trout)

   c = 1
   id113 p = 1/18
   c*(trout) = 2 * n2/n1 = 2 * 1/3 = 2/3 
   p*

gt(trout) = 2/3 / 18 = 1/27

ney et al.   s good turing intuition

h.	ney,	u.	essen,	and	r.	kneser,	1995.	on	the	estimation	of	'small'	probabilities	by	leaving-one-out.		ieee	trans.	pami.	
17:12,1202-1212

held-out words:

60

ney et al. good turing intuition
n1

   intuition from leave-one-out validation

   take each of the c training words out in turn
   c training sets of size c   1, held-out of size 1
   what fraction of held-out words are unseen in 

training?
   n1/c

training?
   (k+1)nk+1/c

   what fraction of held-out words are seen k times in 

   so in the future we expect (k+1)nk+1/c of the words 

to be those with training count k

   there are nk words with training count k
   each should occur with id203:
k* =

      or expected count:

   (k+1)nk+1/c/nk

training held out

n0

n1

n2

n2

n3

(k +1)nk+1

nk

.
 
.
 
.
 
.

.
 
.
 
.
 
.

n3511
n4417

n3510
n4416

good-turing complications

   problem: what about    the   ?  (say 

c=4417)
   for small k, nk > nk+1
   for large k, too jumpy, zeros wreck 

estimates

   simple good-turing [gale and 

sampson]: replace empirical nk with 
a best-fit power law once counts get 
unreliable

n1

n2 n3

n1

n2

c* =

(c +1)nc+1

   numbers from church and gale (1991)
   22 million words of ap newswire

resulting good-turing numbers
good	turing	
c*
.0000270
0.446
1.26
2.24
3.24
4.22
5.19
6.21
7.24
8.25

coun
t	c
0
1
2
3
4
5
6
7
8
9

   it	sure	looks	like	c*	=	(c	- .75)

nc

absolute discounting interpolation
   save ourselves some time and just subtract 0.75 

(or some d)!
pabsolutediscounting(wi | wi   1) =

discounted bigram

c(wi   1,wi)    d

c(wi   1)

interpolation weight

+  (wi   1)p(w)

unigram

   (maybe keeping a couple extra values of d for counts 1 and 2)
   but should we really just use the regular unigram p(w)?

kneser-ney smoothing i

   better estimate for probabilities of lower-order unigrams!

   shannon game:  i can   t see without my reading___________?
      francisco    is more common than    glasses   
       but    francisco    always follows    san   

francisco
glasses

   the unigram is useful exactly when we haven   t seen this 

bigram!

   instead of  p(w):    how likely is w   
   pcontinuation(w):     how likely is w to appear as a novel 

continuation?
   for each word, count the number of bigram types it completes
   every bigram type was a novel continuation the first time it was seen

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

kneser-ney smoothing ii

   how many times does w appear as a novel continuation:

pcontinuation(w)     {wi   1 :c(wi   1,w) > 0}
   normalized by the total number of word bigram types

{(wj   1,wj):c(wj   1,wj) > 0}

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}

{(wj   1,wj):c(wj   1,wj) > 0}

kneser-ney smoothing iii

   alternative metaphor: the number of  # of word types seen to precede w

|{wi   1 :c(wi   1,w) > 0}|

   normalized by the # of words preceding all words:

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}
{w'i   1 :c(w'i   1,w') > 0}

   
w'

   a frequent word (francisco) occurring in only one context (san) will have 

a low continuation id203

kneser-ney smoothing iv

pkn(wi |wi   1) =

max(c(wi   1,wi)   d,0)

c(wi   1)

+  (wi   1)pcontinuation(wi)

   is a normalizing constant; the id203 mass we   ve discounted

  (wi   1) =

d
c(wi   1) {w :c(wi   1,w) > 0}

the normalized discount

the number of word types that can follow wi-1
= # of word types we discounted
= # of times we applied normalized discount

kneser-ney smoothing: recursive formulation

pkn(wi | wi   n+1
i   1

) =

ckn(   ) =

!
#
"
$#

)pkn(wi | wi   n+2
i   1

)

i

)    d,0)
)

max(ckn(wi   n+1
ckn(wi   n+1
i   1
count(   )   for the highest order

+  (wi   n+1
i   1

continuationcount(   )    for lower order

continuation count = number of unique single word contexts for   

