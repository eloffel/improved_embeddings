cse 447/547

natural language processing

winter 2018

feature rich models
(log linear models)

yejin choi 

university of washington

[many slides from dan klein, luke zettlemoyer]

announcements

   hw #3 due 
   feb 16 fri?
   feb 19 mon?

   feb 5     guest lecture by max forbes!

   verbphysics (using a    factor graph    model)

   related models: id49, 

markov random fields, id148

   related algorithms: belief propagation, sum-

product algorithm, forward backward

2

goals of this class

   how to construct a feature vector f(x)
   how to extend the feature vector to f(x,y)
   how to construct a id203 model using any 

given f(x,y)

   how to learn the parameter vector w for maxent

(log-linear) models

   knowing the key differences between maxent

and na  ve bayes

   how to extend maxent to sequence tagging

3

structure in the output variable(s)? 

no structure

structured id136

generative models
(classical probabilistic 
models)

na  ve bayes

id48s
pid18s
ibm models

id148
(discriminatively 
trained feature-rich
models)
neural network 
models 
(representation 
learning)

id88
maximum id178
id28

feedforward nn
id98

memm 
crf

id56
lstm
gru    

?
n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
t
u
p
n

i
 

e
h
t
 
s
i
 
t
a
h
w

feature rich models

   throw anything (features) you want 

into the stew (the model)

   id148
   often lead to great performance.
(sometimes even a best paper award) "11,001 new features for statistical 
machine translation", d. chiang, k. knight, and w. wang, naacl, 2009. 

why want richer features?

   id52: more information about the context?

   is previous word    the   ?
   is previous word    the    and the next word    of   ?
   is previous word capitalized and the next word is numeric?

   is there a word    program    within [-5,+5] window?
   is the current word part of a known idiom?
   conjunctions of any of above?

   desiderata:

   lots and lots of features like above: > 200k
   no independence assumption among features

   classical id203 models, however

   permit very small amount of features
   make strong independence assumption among features

id48s: p(tag sequence|sentence)

   we want a model of sequences y and observations x

y0

y1

x1

y2

x2

yn+1

yn

xn

p(x1...xn, y1...yn+1) = q(stop|yn)

nyi=1

q(yi|yi 1)e(xi|yi)

where y0=start and we call q(y   |y) the transition distribution and e(x|y) the emission 
(or observation) distribution.

   assumptions:

   tag/state sequence is generated by a markov model
   words are chosen independently, conditioned only on the tag/state
   these are totally broken assumptions: why?

a probabilistic context-free grammar (pid18)
pid18s: p(parse tree|sentence)

pid18 example

0.2

vp

1.0
vi     sleeps
1.0
vt     saw
0.7
nn     man
pp
0.2
nn     woman
in
nn     telescope 0.1
0.5
1.0
dt     the
dt
1.0
0.5
in     with
the man saw the woman with the telescope 
in     in
0.5

s     np vp
vp     vi
t2=
vp     vt np
vp     vp pp
np     dt nn
nn
np     np pp
0.7
np
pp     p
    id203 of a tree t with rules

p(ts)=1.8*0.3*1.0*0.7*0.2*0.4*1.0*0.3*1.0*0.2*0.4*0.5*0.3*1.0*0.1

nn
0.2

nn
0.1

dt
1.0

np

np

vt

0.3

0.3

0.4

1.0

0.3

1.0

s
1.0
0.4
vp
0.4
0.4
0.2
np
0.3
dt
0.7
1.0
1.0

is

  1       1,   2       2, . . . ,   n       n

n

p(t) =

q(  i       i)

where q(         ) is the id203 for rule          .

!i=1

44

rich features for long range dependencies

   what  s different between basic pid18 scores here?
   what (lexical) correlations need to be scored?

lms: p(text)
nyi=1
q(xi|xi 1) where xxi2v   

p(x1...xn) =

q(xi|xi 1) = 1

x0 = start & v    := v [ {stop}

   generative process: (1) generate the very first word conditioning on the special 
symbol start, then, (2) pick the next word conditioning on the previous word, 
then repeat (2) until the special word stop gets picked.

   graphical model:

start

x1

x2

xn-1

stop

   subtleties:

  

if we are introducing the special start symbol to the model, then we are making the 
assumption that the sentence always starts with the special start word start, thus 
when we talk about                        it is in fact                                             

p(x1...xn)

p(x1...xn|x0 = start)

   while we add the special stop symbol to the vocabulary       , we do not add the 

special start symbol to the vocabulary. why?

v   

internals of probabilistic models:

nothing but adding log-prob

   lm:     + log p(w7 | w5, w6) + log p(w8 | w6, w7) +    
   pid18: log p(np vp | s) + log p(papa | np) + log p(vp pp | vp)    
   id48 tagging:     + log p(t7 | t5, t6) + log p(w7 | t7) +    

   noisy channel: [log p(source)] + [log p(data | source)]
   na  ve bayes:

log p(class) + log p(feature1 | class) + log p(feature2 | class)    

arbitrary scores instead of log probs?

change log p(this | that) to   (this ; that)
   lm:     +         (w7 ; w5, w6) +          (w8 ; w6, w7) +    
   pid18:     (np vp ; s) +       (papa ; np) +     (vp pp ; vp)    
   id48 tagging:     +         (t7 ; t5, t6) +       (w7 ; t7) +    

   noisy channel: [      (source)] + [       (data ; source)]
   na  ve bayes:

   (class) +      (feature1 ; class) +       (feature2 ; class)    

arbitrary scores instead of log probs?

change log p(this | that) to   (this ; that)
   lm:     +         (w7 ; w5, w6) +          (w8 ; w6, w7) +    
   pid18:     (np vp ; s) +       (papa ; np) +     (vp pp ; vp)    
   id48 tagging:     +         (t7 ; t5, t6) +       (w7 ; t7) +    
memm or crf
   noisy channel: [      (source)] + [       (data ; source)]
   na  ve bayes:
id28 / max-ent

   (class) +      (feature1 ; class) +       (feature2 ; class)    

running example: id52

   roadmap of (known / unknown) accuracies:
   strawman baseline:

   most freq tag: 

~90% / ~50%

   generative models:

   trigram id48: 
   tnt (id48++): 

   feature-rich models?

~95% / ~55%
96.2% / 86.0% (with smart unk   ing)

   upper bound: 

~98%

structure in the output variable(s)? 

no structure

structured id136

generative models
(classical probabilistic 
models)

na  ve bayes

id48s
pid18s
ibm models

id148
(discriminatively 
trained feature-rich
models)
neural network 
models 
(representation 
learning)

id88
maximum id178
id28

feedforward nn
id98

memm 
crf

id56
lstm
gru    

?
n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
t
u
p
n

i
 

e
h
t
 
s
i
 
t
a
h
w

rich features for rich contextual information

   throw in various features about the context:

   f1 := is previous word    the    and the next word    of   ?
   f2 := is previous word capitalized and the next word is numeric?
   f3 := frequencies of    the    within [-15,+15] window?
   f4 := is the current word part of a known idiom?

given a sentence    the blah     the truth of     the blah    
let   s say x =    truth    above, then

f(x) := (f1, f2, f3, f4)
f(truth) = (true, false, 3, false)
=>
f(x) = (1, 0, 3, 0)

rich features for rich contextual information

   throw in various features about the context:

   f1 := is previous word    the    and the next word    of   ?
   f2 :=    

   you can also define features that look at the output    y   !

   f1_n := is previous word    the    and the next tag is    n   ?
   f2_n :=    
   f1_v := is previous word    the    and the next tag is    v   ?
      . (replicate all features with respect to different values of y)

f(x) := (f1, f2, f3, f4)
f(x,y) := (f1_n, f2_n, f3_n, f4_n,    
f1_v, f2_v, f3_v, f4_v,     
f1_d, f2_d, f3_d, f4_d,
   .)

rich features for rich contextual information

   you can also define features that look at the output    y   !

   f1_n := is previous word    the    and the next tag is    n   ?
   f2_n :=    
   f1_v := is previous word    the    and the next tag is    v   ?
      . (replicate all features with respect to different values of y)

given a sentence    the blah     the truth of     the blah    
let   s say x =    truth    above, and y =    n   , then

f(truth) = (true, false, 3, false)
f(x,y) := (f1_n, f2_n, f3_n, f4_n,    
f1_v, f2_v, f3_v, f4_v,     
f1_d, f2_d, f3_d, f4_d,
   .)

f(truth, n) = ?

rich features for rich contextual information

   throw in various features about the context:

   f1 := is previous word    the    and the next word    of   ?
   f2 := is previous word capitalized and the next word is numeric?
   f3 := frequencies of    the    within [-15,+15] window?
   f4 := is the current word part of a known idiom?

   you can also define features that look at the output    y   !

   f1_n := is previous word    the    and the next tag is    n   ?
   f1_v := is previous word    the    and the next tag is    v   ?

   you can also take any conjunctions of above.
f (x, y) = [0, 0, 0, 1, 0, 0, 0, 0, 3, 0.2, 0, 0, ....]
   create a very long feature vector with dimensions often >200k 
   overlapping features are fine     no independence assumption among 

features

goals of this class

   how to construct a feature vector f(x)
   how to extend the feature vector to f(x,y)
   how to construct a id203 model using any 

given f(x,y)

   how to learn the parameter vector w for maxent

(log-linear) models

   knowing the key differences between maxent

and na  ve bayes

   how to extend maxent to sequence tagging

20

maximum id178 (maxent) models
   output:	y
y3

   one	pos	tag	for	one	word	(at	a	time)
   input:	x (any	words	in	the	context)

   represented	as	a	feature	vector	f(x, y)

x2

x3

x4

   model	parameters:	w
   make	id203	using	softmax function:
   also	known	as	   log-linear   	models	(linear	if	you	take	log)

p(y|x) =

exp(w    f (x, y))

py0 exp(w    f (x, y0))

make positive!

normalize!

training maxent models

   make id203 using softmax function 

p(y|x) =
   training: 

exp(w    f (x, y))

py0 exp(w    f (x, y0))

   maximize log likelihood of training data 

l(w) = logyi

p(yi|xi) =xi

log

i=1

{(xi, yi)}n
exp(w    f (xi, yi))
py0 exp(w    f (xi, y0))

   which also incidentally maximizes the id178 (hence 

   maximum id178   )

training maxent models

   make id203 using softmax function 

p(y|x) =
   training: 

   maximize log likelihood

exp(w    f (x, y))

py0 exp(w    f (x, y0))
p(yi|xi) =xi

log

=xi    w    f (xi, yi)   logxy0

l(w) = logyi

exp(w    f (xi, yi))

py0 exp(w    f (xi, y0))
exp(w    f (xi, y0))   

training maxent models

take partial derivative for each           in the weight vector w:

l(w) =xi    w    f (xi, yi)   logxy0
=xi    fk(xi, yi)  xy0

exp(w    f (xi, y0))   
p(y0|xi)fk(xi, y0))   

@l(w)
@wk

wk

total count of feature k 
with respect to the 
correct predictions

expected count of feature k 
with respect to the 
predicted output

id76 for training

   the	likelihood	function	is	convex.	(can	get	global	optimum)
   many	optimization	algorithms/software	available.

   gradient	ascent	(descent),	conjugate	gradient,	l-bfgs,	etc

   all	we	need	are:

(1)	evaluate	the	function	at	current	   w   
(2)	evaluate	its	derivative	at	current	   w   

goals of this class

   how to construct a feature vector f(x)
   how to extend the feature vector to f(x,y)
   how to construct a id203 model using any 

given f(x,y)

   how to learn the parameter vector w for maxent

(log-linear) models

   knowing the key differences between maxent

and na  ve bayes

   how to extend maxent to sequence tagging

26

graphical representation of maxent

p(y|x) =

exp(w    f (x, y))

py0 exp(w    f (x, y0))

output

y

input

x2     xn

x1

graphical representation of na  ve bayes

p(x|y) =yj

p(xj|y)

output

y

input

x2     xn

x1

y

y

na  ve 
bayes

x1

x2     xn

maxent

x1

x2     xn

na  ve bayes classifier

maximum id178 classifier

   generative    models
   p(input | output)
   for instance, for text categorization, 

   discriminative     models
   p(output | input)
   for instance, for text categorization, 

p(words | category)

   unnecessary efforts on generating input

p(category | words)

   focus directly on predicting the output

   independent assumption among input 
variables: given the category, each word is 
generated independently from other words 
(too strong assumption in reality!)

   by conditioning on the entire input, we 

don   t need to worry about the 
independent assumption among input 
variables

   cannot incorporate 
arbitrary/redundant/overlapping features

   can incorporate arbitrary features: 
redundant and overlapping features

overview: id52 accuracies

   roadmap of (known / unknown) accuracies:

   most freq tag: 
   trigram id48: 
   tnt (id48++): 
   maxent p(si|x): 

~90% / ~50%
~95% / ~55%
96.2% / 86.0%
96.8% / 86.8%

   q: what   s missing in maxent compared to id48?

   upper bound: 

~98%

structure in the output variable(s)? 

no structure

structured id136

generative models
(classical probabilistic 
models)

na  ve bayes

id48s
pid18s
ibm models

id148
(discriminatively 
trained feature-rich
models)
neural network 
models 
(representation 
learning)

id88
maximum id178
id28

feedforward nn
id98

memm 
crf

id56
lstm
gru    

?
n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
t
u
p
n

i
 

e
h
t
 
s
i
 
t
a
h
w

goals of this class

   how to construct a feature vector f(x)
   how to extend the feature vector to f(x,y)
   how to construct a id203 model using any 

given f(x,y)

   how to learn the parameter vector w for maxent

(log-linear) models

   knowing the key differences between maxent

and na  ve bayes

   how to extend maxent to sequence tagging

32

memm taggers

   one step up: also condition on previous tags
p(si|s1 . . . si 1, x1 . . . xm)

p(s1 . . . sm|x1 . . . xm) =

myi=1
myi=1

=

p(si|si 1, x1 . . . xm)

   train up p(si|si-1,x1...xm) as a discrete log-linear (maxent) model, 

then use to score sequences

p(si|si 1, x1 . . . xm) =
   this is referred to as an memm tagger [ratnaparkhi 96]

ps0 exp (w     (x1 . . . xm, i, si 1, s0))

exp (w     (x1 . . . xm, i, si 1, si))

id48

memm

nnp

vbz

vbn

secretariat

is

expected

nnp

vbz

vbn

secretariat

is

expected

to

to

to

to

vb

nr

race

tomorrow

vb

nr

race

tomorrow

id48

memm

   generative    models
   joint id203 p( words, tags )
     generate    input (in addition to tags)
   but we need to predict tags, not words!  
id203 of each slice = 
emission * transition = 
p(word_i | tag_i) * p(tag_i | tag_i-1) =

   cannot incorporate long distance 
features

   discriminative    or    conditional    models
   id155 p( tags | words)
     condition    on input 
   focusing only on predicting tags
id203 of each slice = 
p( tag_i | tag_i-1, word_i)

or

p( tag_i | tag_i-1, all words)

   can incorporate long distance features

the id48 state lattice / trellis (repeat slide)

^

n

v

j

d

$

^

e(fed|n)

n

v

j

d

$

^

n

^

n

e(raises|v) e(interest|v)

q(v|v)

v

j

d

$

v

j

d

$

^

n

v

e(rates|j)

j

d

$

^

n

e(stop|v)

v

j

d

$

start       fed           raises       interest         rates         stop

the  memm state lattice / trellis

^

n

v

j

d

$

^

n

v

j

d

$

p(v|v,x)

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

x = start       fed           raises       interest         rates         stop

decoding:

p(s1 . . . sm|x1 . . . xm) =
=

   decoding maxent taggers:

myi=1
myi=1

   just like decoding id48s
   viterbi, id125, posterior decoding

   viterbi algorithm (id48s):

p(si|s1 . . . si 1, x1 . . . xm)
p(si|si 1, x1 . . . xm)

   define   (i,si) to be the max score of a sequence of length i ending in tag si
 (i, si) = max
si 1

e(xi|si)q(si|si 1) (i   1, si 1)

   viterbi algorithm (maxent):

   can use same algorithm for memms, just need to redefine   (i,si) !
 (i, si) = max
si 1

p(si|si 1, x1 . . . xm) (i   1, si 1)

overview: accuracies

   roadmap of (known / unknown) accuracies:

   most freq tag: 
   trigram id48: 
   tnt (id48++): 
   maxent p(si|x): 
   memm tagger: 

~90% / ~50%
~95% / ~55%
96.2% / 86.0%
96.8% / 86.8%
96.9% / 86.9%

   upper bound: 

~98%

structure in the output variable(s)? 

no structure

structured id136

generative models
(classical probabilistic 
models)

na  ve bayes

id48s
pid18s
ibm models

id148
(discriminatively 
trained feature-rich
models)
neural network 
models 
(representation 
learning)

id88
maximum id178
id28

feedforward nn
id98

memm 
crf

id56
lstm
gru    

?
n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
t
u
p
n

i
 

e
h
t
 
s
i
 
t
a
h
w

memm v.s. crf

(id49)

memm

nnp

vbz

vbn

to

vb

nr

secretariat

is

expected

to

race

tomorrow

crf

nnp

vbz

vbn

to

vb

nr

secretariat

is

expected

to

race

tomorrow

nnp

memm

secretariat

nnp

crf

secretariat

vb
z

is

vb
z

is

vbn

expected

vbn

expected

to

to

to

to

vb

nr

race

tomorrow

vb

nr

race

tomorrow

memm

crf

directed graphical model

undirected graphical model

   discriminative    or    conditional    models
   id155 p( tags | words)

id203 is defined for each slice = 

p ( tag_i | tag_i-1, word_i)

or

p ( tag_i | tag_i-1, all words)

instead of id203, potential (energy function) 
is defined for each slide = 
f ( tag_i, tag_i-1 ) * f (tag_i, word_i)

or

f ( tag_i, tag_i-1, all words ) * f (tag_i, all words)

   can incorporate long distance features

id49 (crfs)

   maximum id178 (id28)

[lafferty, mccallum, pereira 01] 

   learning: maximize the (log) conditional likelihood of training 

sentence: x=x1   xm

tag sequence: s=s1   sm

p(s|x; w) =

data

{(xi, si)}n

i=1

@
@wj

l(w) =

nxi=1  j(xi, si)  xs

exp (w     (x, s))

ps0 exp (w     (x, s0))
p(s|xi; w) j(xi, s)!    wj

   computational challenges?

   most likely tag sequence,  id172 constant, gradient

   crfs

decoding

s    = arg max

s

p(s|x; w)

   features must be local, for x=x1   xm, and s=s1   sm

p(s|x; w) =

arg max

s

exp (w     (x, s))

ps0 exp (w     (x, s0))
ps0 exp (w     (x, s0))

exp (w     (x, s))

= arg max

w     (x, s)

s

 (x, s) =

mxj=1

 (x, j, sj 1, sj)

= arg max

exp (w     (x, s))

s

   viterbi recursion
   (i, si) = maxsi 1w     (x, i, si 1, si) +    (i   1, si 1)

crfs: computing id172*

 (x, j, sj 1, sj)

p(s|x; w) =
xs0

exp (w     (x, s))

ps0 exp (w     (x, s0))
exp0@xj
=xs0
=xs0 yj

exp w     (x, s0) 

 (x, s) =

mxj=1
w     (x, j, sj 1, sj)1a

exp (w     (x, j, sj 1, sj))

define norm(i,si) to sum of scores for sequences ending in position i

exp (w     (x, i, si 1, si)) norm(i   1, si 1)

norm(i, yi) =xsi 1
 (i, yi) =xyi 1

   could also use backward?

   forward algorithm! remember id48 case:

e(xi|yi)q(yi|yi 1) (i   1, yi 1)

crfs: computing gradient*

p(s|x; w) =

l(w) =

exp (w     (x, s))

ps0 exp (w     (x, s0))
nxi=1  j(xi, si)  xs
p(s|xi; w) j(xi, s) =xs
mxj=1xa,b xs:sj 1=a,sb=b

=

@
@wj

xs

mxj=1

 (x, s) =

 (x, j, sj 1, sj)

p(s|xi; w) j(xi, s)!    wj
p(s|xi; w)

mxj=1

 k(xi, j, sj 1, sj)

p(s|xi; w) k(xi, j, sj 1, sj)

   need forward and backward messages

see notes for full details!

overview: accuracies

   roadmap of (known / unknown) accuracies:

   most freq tag: 
   trigram id48: 
   tnt (id48++): 
   maxent p(si|x): 
   memm tagger: 
   crf (untuned)

~90% / ~50%
~95% / ~55%
96.2% / 86.0%
96.8% / 86.8%
96.9% / 86.9%
95.7% / 76.2%

   upper bound: 

~98%

cyclic tagging [toutanova et al 03]

cyclic network
    another idea: train a bi-directional memm
(a) left-to-right cmm

[toutanova et al 03]

ditioned quantities, these local models may have to be
estimated in some sophisticated way; it is typical in tag-
ging to populate these models with little maximum en-
tropy models. for example, we might populate a model
for

   train two memms, 

multiple together to 
score

   and be very careful
    tune id173
    try lots of different 

(b) right-to-left cmm

(a) left-to-right cmm

features

    see paper for full 

(c) bidirectional dependency network

(b) right-to-left cmm

details

figure 1: dependency networks: (a) the (standard) left-to-right
   rst-order cmm, (b) the (reversed) right-to-left cmm, and (c)
the bidirectional dependency network.
the model.
having expressive templates leads to a large number
of features, but we show that by suitable use of a prior

(c) bidirectional dependency network

in this case, the
but there are not joint features involving all three vari-
ables (though there could have been such features). we
say that this model uses the feature templates
(previous tag features) and
tures).

clearly, both the preceding tag

directional models do not ignore this in   uence; in the
case of a left-to-right cmm, the in   uence of
is explicit in the
   uence of

overview: accuracies

   roadmap of (known / unknown) accuracies:

   most freq tag: 
   trigram id48: 
   tnt (id48++): 
   maxent p(si|x): 
   memm tagger: 
   id88
   crf (untuned)
   cyclic tagger: 
   upper bound: 

~90% / ~50%
~95% / ~55%
96.2% / 86.0%
96.8% / 86.8%
96.9% / 86.9%
96.7% / ??
95.7% / 76.2%
97.2% / 89.0%
~98%

   locally normalized models

   id48s, memms
   local scores are probabilities
   however: one issue in local models

      label bias    and other explaining away effects
   memm taggers   local scores can be near one without having 
both good    transitions    and    emissions   
   this means that often evidence doesn  t flow properly
   why isn  t this a big deal for id52?

   globally normalized models

   local scores are arbitrary scores
   id49 (crfs)
   slower to train (structured id136 at each iteration of learning)
   neural networks (global training w/o structured id136)

structure in the output variable(s)? 

no structure

structured id136

generative models
(classical probabilistic 
models)

na  ve bayes

id48s
pid18s
ibm models

id148
(discriminatively 
trained feature-rich
models)
neural network 
models 
(representation 
learning)

id88
maximum id178
id28

feedforward nn
id98

memm 
crf

id56
lstm
gru    

?
n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
t
u
p
n

i
 

e
h
t
 
s
i
 
t
a
h
w

supplementary material

id114

y1

x1

y2

x2

y3

x3

   id155 for each node

   e.g. p( y3 | y2, x3 ) for y3
   e.g. p( x3 ) for x3

   conditional independence

   e.g. p( y3 | y2, x3 ) = p( y3 | y1, y2, x1, x2, x3)

   joint id203 of the entire graph

= product of id155 of each node

undirected graphical model basics

y1

x1

y2

x2

y3

x3

   conditional independence

   e.g. p( y3 | all other nodes ) = p( y3 | y3    neighbor )

   no id155 for each node
   instead,    potential function    for each clique

   e.g. f ( x1, x2, y1 )  or  f ( y1, y2 )

   typically, log-linear potential functions

   f ( y1, y2 ) = exp sk wk fk (y1, y2)

undirected graphical model basics

y1

x1

y2

x2

y3

x3

   joint id203 of the entire graph
c)

p(y      ) =

1
z

   
clique c

  (y      
         
  (y

c)

z =

            

y

   

clique c

