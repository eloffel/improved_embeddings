cse 447/547

natural language processing

winter 2018

parsing (trees)

yejin choi - university of washington

[slides from dan klein, michael collins, luke zettlemoyer and ray mooney]

ambiguities

i shot [an elephant] [in my pajamas]

examples from j&m       

syntactic ambiguities i

   prepositional phrases:

they cooked the beans in the pot on the stove with 
handles.

   particle vs. preposition:

   complement structures

the puppy tore up the staircase.
the tourists objected to the guide that they couldn  t hear.
she knows you like the back of her hand.

   gerund vs. participial adjective
visiting relatives can be boring.
changing schedules frequently confused passengers.

syntactic ambiguities ii

   modifier scope within nps

impractical design requirements
plastic cup holder

   multiple gap constructions
the chicken is ready to eat.
the contractors are rich enough to sue.

   coordination scope:

small rats and mice can squeeze into holes or cracks in 
the wall.

dark ambiguities

   dark ambiguities: most analyses are shockingly bad 
(meaning, they don  t have an interpretation you can 
get your mind around)

this analysis corresponds 

to the correct parse of 
   this will panic buyers !    

   unknown words and new usages
   solution: we need mechanisms to focus attention on 

the best ones, probabilistic techniques do this

probabilistic 

id18s

   a context-free grammar is a tuple <n,    ,s, r>

in the following sections we answer these questions through de   ning proba-
bilistic context-free grammars (pid18s), a natural generalization of context-free
id140
grammars.
3.2 de   nition of pid18s
   phrasal categories: s, np, vp, adjp, etc.
id140 (pid18s) are de   ned as follows:
   parts-of-speech (pre-terminals): nn, jj, dt, vb, etc.
de   nition 1 (pid18s) a pid18 consists of:

   n : the set of non-terminals

      : the set of terminals (the words)
   s : the start symbol
1. a context-free grammar g = (n,   , s, r).
   often written as root or top
   not usually the sentence non-terminal s
2. a parameter

   r : the set of rules

q(         )

   of the form x     y1 y2     yn, with x     n, n   0, yi     (n       )

   examples: s     np vp,   vp     vp cc vp

for each rule               r. the parameter q(         ) can be interpreted as
the conditional probabilty of choosing rule           in a left-most derivation,
   a pid18 adds a distribution q:
given that the non-terminal being expanded is   . for any x     n, we have
the constraint

   id203 q(r) for each r     r, such that for all x     n:

!          r:  =x

q(         ) = 1

in addition we have q(         )     0 for any               r.

a probabilistic context-free grammar (pid18)

pid18 example

s     np vp
vp     vi
vp     vt np
vp     vp pp
np     dt nn
np     np pp
pp     p
np
    id203 of a tree t with rules

1.0
0.4
0.4
0.2
0.3
0.7
1.0

1.0
vi     sleeps
1.0
vt     saw
0.7
nn     man
0.2
nn     woman
nn     telescope 0.1
dt     the
1.0
0.5
in     with
0.5
in     in

is

  1       1,   2       2, . . . ,   n       n

n

p(t) =

q(  i       i)

where q(         ) is the id203 for rule          .

!i=1

44

a probabilistic context-free grammar (pid18)

s

1.0

np

pid18 example
1.0
vi     sleeps
1.0
vt     saw
0.7
nn     man
vp
t1=
nn     woman
0.2
0.3
nn     telescope 0.1
nn
vi
0.7
1.0
dt     the
0.5
in     with
p(t1)=1.0*0.3*1.0*0.7*0.4*1.0
in     in
0.5
s

1.0
s     np vp
0.4
vp     vi
0.4
vp     vt np
0.2
vp     vp pp
0.3
np     dt nn
a probabilistic context-free grammar (pid18)
0.7
np     np pp
1.0
np
pp     p
1.0
vi     sleeps
    id203 of a tree t with rules
1.0
vt     saw
t2=
0.7
nn     man
is
0.2
nn     woman
nn     telescope 0.1
p(t) =
dt     the
1.0
nn
0.2
0.5
in     with
where q(         ) is the id203 for rule          .
0.5
in     in

  1       1,   2       2, . . . ,   n       n
0.4
np

dt
1.0
the man sleeps     

!i=1

in
0.5

dt
1.0

np

vp

vp

0.4

1.0

1.0

n

0.2

pp

0.4

0.3

vt
1.0
q(  i       i)
dt
1.0
the man saw the woman with the telescope 

nn
0.7

nn
0.1

dt
1.0

np

0.3

0.3

p(ts)=1.8*0.3*1.0*0.7*0.2*0.4*1.0*0.3*1.0*0.2*0.4*0.5*0.3*1.0*0.1
44

    id203 of a tree t with rules

  1       1,   2       2, . . . ,   n       n

pid18s: learning and id136

   model

   the id203 of a tree t with n rules   i      i, i = 1..n

   learning

probabilities

   read the rules off of labeled sentences, use ml estimates for 

p(t) =

nyi=1

q( i !    i)

qm l(  !    ) =

count(  !    )

count( )

   and use all of our standard smoothing tricks!

   id136

   for input sentence s, define t(s) to be the set of trees whole yield is s 

(whole leaves, read left to right, match the words in s)

t (s) = arg max
t   t (s)

p(t)

t   t (i,j,x)

  (i, j, x) = max

i=1 q(  i       i)).

note in particular, that

  (1, n, s) = arg max
t   tg(s)

i=1 q(  i       i)).
p(t)

non-terminal x
note in particular, that

    for a given sentence x1 . . . xn, de   ne t (i, j, x) for any x     n, for any
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m
(i, j) such that 1     i     j     n, to be the set of all parse trees for words
thus   (i, j, x) is the highest score for any parse tree that dominates words
id145
xi . . . xj such that non-terminal x is at the root of the tree.
xi . . . xj, and has non-terminal x as its root. the score for a tree t is again taken
to be the product of scores for the rules that it contains (i.e. if the tree t contains
    de   ne
   we will store: score of the max parse of xi to xj with root 
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m
because by de   nition   (1, n, s) is the score for the highest id203 parse tree
spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
   so we can compute the most likely parse:
(we de   ne   (i, j, x) = 0 if t (i, j, x) is the empty set).
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
tion of the    values, which allows a simple bottom-up id145 algo-
p(t)
for all x     n,
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
thus   (i, j, x) is the highest score for any parse tree that dominates words
   via the recursion:
values for the cases where j = i, then the cases where j = i + 1, and so on.
xi . . . xj, and has non-terminal x as its root. the score for a tree t is again taken
because by de   nition   (1, n, s) is the score for the highest id203 parse tree
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
  (i, j, x) = max
to be the product of scores for the rules that it contains (i.e. if the tree t contains
spanning words x1 . . . xn, with s as its root.
all x     n,
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m
i=1 q(  i       i)).
the key observation in the cky algorithm is that we can use a recursive de   ni-
the next section of this note gives justi   cation for this recursive de   nition.
   with base case:
note in particular, that
tion of the    values, which allows a simple bottom-up id145 algo-
  (i, i, x) = " q(x     xi)
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
values for the cases where j = i, then the cases where j = i + 1, and so on.
this is a natural de   nition: the only way that we can have a tree rooted in node
values for   (i, j, x) such that j = i + 2; and so on.
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
x spanning word xi is if the rule x     xi is in the grammar, in which case the
note that the algorithm also stores backpointer values bp(i, j, x) for all values
because by de   nition   (1, n, s) is the score for the highest id203 parse tree
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
all x     n,

= max
  (1, n, s) = arg max
t2tg(s)
t   tg(s)

  (1, n, s) = arg max
t   tg(s)

(q(x     y z)      (i, s, y )      (s + 1, j, z))

if x     xi     r
otherwise

s   {i...(j   1)}

x   y z   r,

0

initialization:
for all i     {1 . . . n}, for all x     n,

spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
tion of the    values, which allows a simple bottom-up id145 algo-
the cky algorithm
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
if x     xi     r
  (i, i, x) = ! q(x     xi)
values for the cases where j = i, then the cases where j = i + 1, and so on.
otherwise
   input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
all x     n,
   initialization: for i = 1     n and all x in n
    for l = 1 . . . (n     1)

algorithm:

  (i, i, x) = " q(x     xi)

if x     xi     r
otherwise

0

0

    for i = 1 . . . (n     l)

   for l = 1     (n-1) 
    set j = i + l
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
    for all x     n, calculate
   for all x in n

this is a natural de   nition: the only way that we can have a tree rooted in node
[iterate all phrase lengths]
x spanning word xi is if the rule x     xi is in the grammar, in which case the
   for i = 1     (n-l) and j = i+l
[iterate all phrases of length l]
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
there are no trees rooted in x spanning word xi.

[iterate all non-terminals]

for all x     n,

(q(x     y z)      (i, s, y )      (s + 1, j, z))
(q(x     y z)      (i, s, y )      (s + 1, j, z))

(1)

  (i, j, x) = max

  (i, j, x) = max
x   y z   r,
s   {i...(j   1)}

x   y z   r,
s   {i...(j   1)}

12

and

the next section of this note gives justi   cation for this recursive de   nition.
   also, store back pointers
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
bp(i, j, x) = arg max
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
values for   (i, j, x) such that j = i + 2; and so on.
note that the algorithm also stores backpointer values bp(i, j, x) for all values
of (i, j, x). these values record the rule x     y z and the split-point s leading to

output: return   (1, n, s) = maxt   t (s) p(t), and backpointers bp which allow recovery
of arg maxt   t (s) p(t).

(q(x     y z)      (i, s, y )      (s + 1, j, z))

s   {i...(j   1)}

x   y z   r,

probabilistic cky parser

0.8
0.1
1.0

0.05
0.03

book       the        flight    through  houston
s:.03*.0135*.032
s :.01, 
verb:.5 
=.00001296
s:.05*.5*
nominal:.03
.000864
=.0000216

vp:.5*.5*.054

s:.05*.5*.054

=.00135

none

none

=.0135

det:.6

np:.6*.6*.15

=.054

none

nominal:.15

none

np:.6*.6*
.0024
=.000864

nominal:
.5*.15*.032
=.0024

prep:.2

pp:1.0*.2*.16

=.032

np:.16

s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
0.01     0.004    0.006

s     verb np
s     vp pp
np      i   |  he  |  she |  me
0.1   0.02  0.02    0.06

np     houston | nwa
0.16           .04

det    the |  a  |   an 

0.6    0.1   0.05    

np     det nominal
nominal     book | flight | meal | money

0.6

0.03    0.15   0.06     0.06

nominal     nominal nominal
nominal     nominal pp
verb    book | include | prefer

0.5      0.04        0.06

vp     verb np
vp     vp pp
prep     through | to | from
0.2          0.3   0.3

pp     prep np

0.2
0.5

0.5
0.3

1.0

probabilistic cky parser
book       the        flight    through  houston

s :.01, 
verb:.5 
nominal:.03

none

det:.6

s:.05*.5*.054

=.00135

vp:.5*.5*.054

=.0135

none

np:.6*.6*.15

=.054

none

nominal:.15

none

s:.0000216

np:.6*.6*
.0024
=.000864

nominal:
.5*.15*.032
=.0024

prep:.2

pp:1.0*.2*.16

=.032

np:.16

parse 
tree
#1

pick most 
probable
parse, i.e. take 
max to
combine 
probabilities
of multiple 
derivations
of each 
constituent in
each cell.

probabilistic cky parser
parse 
book       the        flight    through  houston
tree
#2

s:.05*.5*.054

=.00135

s: 00001296
s:.0000216

s :.01, 
verb:.5 
nominal:.03

none

vp:.5*.5*.054

=.0135

none

np:.6*.6*.15

=.054

none

det:.6

nominal:.15

none

np:.6*.6*
.0024
=.000864

nominal:
.5*.15*.032
=.0024

prep:.2

pp:1.0*.2*.16

=.032

np:.16

pick most 
probable
parse, i.e. take 
max to
combine 
probabilities
of multiple 
derivations
of each 
constituent in
each cell.

memory
   how much memory does this require?

   have to store the score cache
   cache size: |symbols|*n2

   pruning: id125

   score[x][i][j] can get too large (when?)
   can keep beams (truncated maps score[i][j]) which only store 

the best k scores for the span [i,j]

   pruning: coarse-to-fine

   use a smaller grammar to rule out most x[i,j]
   much more on this later   

time: theory

   how much time will it take to parse?

   for each diff (:= j     i) (<= n)

   for each i (<= n)

   for each rule x     y z 

   for each split point k

do constant work

x

y

z

i                       k                      j

   total time: |rules|*n3
   something like 5 sec for an unoptimized parse 

of a 20-word sentences

time: practice

   parsing with the vanilla treebank grammar:

~ 20k rules

(not an 

optimized 
parser!)

observed 
exponent: 

3.6

   why   s it worse in practice?

   longer sentences    unlock    more of the grammar
   all kinds of systems issues don   t scale

other dynamic programs

can also compute other quantities: 

   best inside: score of the max parse 
of wi to wj with root non-terminal x

x

   best outside: score of the max 

parse of w0 to wn with a gap from wi
to wj rooted with non-terminal x
   see notes for derivation, it is a bit more 

complicated

   sum inside/outside: do sums 

instead of maxes 

1

i

j

n

x

1

i

j

n

why chomsky normal form?
book       the        flight    through  houston
s:.03*.0135*.032
s :.01, 
verb:.5 
=.00001296
s:.05*.5*
nominal:.03
.000864
=.0000216

vp:.5*.5*.054

s:.05*.5*.054

=.00135

none

none

=.0135

id136:
det:.6
  can we keep n-ary (n > 2) rules and 
still do id145?
  can we keep unary rules and still do 
id145?
learning:
  can we reconstruct the original 
trees?

np:.6*.6*.15

=.054

none

nominal:.15

none

np:.6*.6*
.0024
=.000864

nominal:
.5*.15*.032
=.0024

prep:.2

pp:1.0*.2*.16

=.032

np:.16

treebanks

data for parsing experiments

the id32: size

i penn wsj treebank = 50,000 sentences with associated trees

i usual set-up: 40,000 training sentences, 2400 test sentences

an example tree:

top

s

np

vp

nnp

nnps

vbd

np

pp

np

pp

advp

in

np

cd

nn

in

np

qp

$

cd

cd

punc,

rb

np

prp$

jj

nn

cc

jj

nn

nns

in

pp

np

np

sbar

nnp

punc,

whadvp

s

wrb

np

vp

dt

nn

vbz

np

qp

nns

punc.

rb

cd

canadian

utilities

had

1988

revenue

of

c$

1.16

billion

,

mainly

from

its

natural

gas

and

electric

utility

businesses

in

alberta

,

where

the

company

serves

about

800,000

customers

.

canadian utilities had 1988 revenue of c$ 1.16 billion ,

9
the id32: an overview
id32 non-terminals
table 1.2. the id32 syntactic tagset
adjp
advp
np
pp
s
sbar
sbarq
sinv
sq
vp
whadvp
whnp
whpp
x
0
t

adjective phrase
adverb phrase
noun phrase
prepositional phrase
simple declarative clause
subordinate clause
direct question introduced by wh-element
declarative sentence with subject-aux inversion
yes/no questions and subconstituent of sbarq excluding wh-element
verb phrase
wh-adverb phrase
wh-noun phrase
wh-prepositional phrase
constituent of unknown or uncertain category
   understood    subject of in   nitive or imperative
zero variant of that in subordinate clauses
trace of wh-constituent

predicate-argument structure.

the new style of annotation provided

treebank grammars

   need a pid18 for broad coverage parsing.
   can take a grammar right off the trees (doesn  t work well):

root     s
s     np vp .
np     prp
vp     vbd adjp
   ..

1

1

1

1

   better results by enriching the grammar (e.g., lexicalization).
   can also get reasonable parsers without lexicalization.

treebank grammar scale

   treebank grammars can be enormous

   as fsas, the raw grammar has ~10k states, excluding the 

lexicon

   better parsers usually make the grammars larger, not smaller

np:

det

adj

noun

det

noun

plural noun

np

np

pp

np

conj

list

trie

min fsa

grammar encodings: non-black states are active, non-white states are 
accepting, and bold transitions are phrasal. fsas for a subset of the 
rules for the category np. 

typical experimental setup

   corpus: id32, wsj

training:
development:
test:

sections
section
section

02-21
22 (here, first 20 files)
23

   accuracy     f1: harmonic mean of per-node labeled 

precision and recall.

   here: also size     number of symbols in grammar.

   passive / complete symbols: np, np^s
   active / incomplete symbols: np     np cc    

how to evaluate?

correct tree t

computed tree p

s

vp

verb          np
book

det    nominal

the

nominal     pp
noun
flight

prep        np
through houston

s

vp

vp

verb          np
book

det    nominal

pp

the

noun
flight

prep        np

through

proper-noun
houston

parseval example

correct tree t

computed tree p

s

vp

verb          np
book

det    nominal

the

nominal     pp
noun
flight

prep        np
through houston

s

vp

vp

verb          np
book

det    nominal

pp

the

noun
flight

prep        np

through

proper-noun
houston

# constituents: 12

# constituents: 11

# correct constituents: 10

recall = 10/11= 90.9% precision = 10/12=83.3%

f1 = 87.4%

evaluation metric

   parseval metrics measure the fraction of the 

constituents that match between the computed and 
human parse trees.  if p is the system   s parse tree and t 
is the human parse tree (the    gold standard   ):
   recall = (# correct constituents in p) / (# constituents in t)
   precision = (# correct constituents in p) / (# constituents in p)
   labeled precision and labeled recall require getting 

the non-terminal label on the constituent node correct 
to count as correct.

   f1 is the harmonic mean of precision and recall.

   f1= (2 * precision * recall) / (precision + recall)

performance with vanilla pid18s

   use pid18s for broad coverage parsing
   take the grammar right off the trees

[charniak 96]

root     s
s     np vp . 
np     prp
vp     vbd adjp

   ..

model
baseline

1

1

1

1

f1
72.0

grammar refinements

1. markovization

conditional independence?

   not every np expansion can fill every np slot

   a grammar with symbols like    np    won  t be context-free
   statistically, conditional independence too strong

non-independence

   independence assumptions are often too strong.

all nps

nps under s

nps under vp

   example: the expansion of an np is highly dependent 

on the parent of the np (i.e., subjects vs. objects).

   also: the subject and object expansions are correlated!

vertical markovization

order 1

order 2

   vertical markov 
order: rewrites 
depend on past k
ancestor nodes.
(cf. parent 
annotation)

horizontal markovization

order 1

order    

vertical and horizontal

model
v=h=2v

f1
77.8

size
7.5k

unlexicalized pid18 grammar size

39

grammar refinements

2. lexicalization

problems with pid18s

   these trees differ only in one rule:

   vp     vp pp
   np     np pp

   lexicalization allows us to be sensitive to specific words

lexicalize trees!

   add    headwords    to 
each phrasal node
   headship not in (most) 

treebanks

   usually use (handwritten) 

head rules, e.g.:
   np:

   take leftmost np
   take rightmost n*
   take rightmost jj
   take right child

   vp:

   take leftmost vb*
   take leftmost vp
   take left child

lexicalized pid18s?

   problem: we now have to estimate probabilities like

   never going to get these atomically off of a treebank

   solution: break up derivation into smaller steps

lexical derivation steps

[collins 99]

   main idea: define a linguistically-motivated markov 

process for generating children given the parent

step 1: choose a head tag 
and word

step 2: choose a complement bag

step 3: generate children 
(incl. adjuncts)

step 4: recursively derive children

lexicalized cky

(vp->vbd...np    )[saw]

(vp->vbd    )[saw]

np[her]

x[h]

y[h] z[h   ]

i           h          k         h   

j

still cubic time?

bestscore(i,j,x, h)

if (j = i+1)
else

return tagscore(x,s[i])
return 

max  max  score(x[h]->y[h] z[h   ]) *

k,h   ,
x->yz

k,h   ,
x->yz

max   score(x[h]->y[h   ] z[h]) *

bestscore(i,k,y, h) *
bestscore(k+1,j,z, h   )
bestscore(i,k,y, h   ) *
bestscore(k+1,j,z, h)

pruning with beams

   the collins parser prunes with 

per-cell beams [collins 99]
   essentially, run the o(n5) cky
   if we keep k hypotheses at each 
span, then we do at most o(nk2) 
work per span (why?)

x[h]

y[h] z[h   ]

   keeps things more or less cubic

i           h          k         h   

j

   also: certain spans are 

forbidden entirely on the basis 
of punctuation (crucial for 
speed)

model
na  ve treebank 
grammar
klein & 
manning   03
collins 99

f1
72.6

86.3

88.6

grammar refinements
3. using latent sub-categories

manual annotation

   manually split categories

   np: subject vs object
   dt: determiners vs demonstratives
   in: sentential vs prepositional 

   advantages:

   fairly compact grammar
   linguistic motivations

   disadvantages:

   performance leveled out
   manually annotated

learning latent annotations

latent annotations:

   brackets are known
   base categories are known
   hidden variables for 

subcategories

forward/outside

x1

x4

x5

x6

x2
x3

x7

.

can learn with em: like forward-
backward for id48s.

was

right

he
backward/inside

final results

parser
klein & manning   03
matsuzaki et al.   05
collins   99
charniak & johnson   05
petrov et. al. 06 

f1

    40 words

f1

all words

86.3

86.7

88.6

90.1

90.2

85.7

86.1

88.2

89.6

89.7

   grammar as foreign language    (deep learning)

vinyals et al., 2015

john has a dog   

john has a dog   

   linearize a tree into a sequence
   then parsing problem becomes similar to machine translation

   input: sequence
   output: sequence (of different length)

   encoder-decoder lstms (id137)

   grammar as foreign language    (deep learning)

vinyals et al., 2015

john has a dog   

john has a dog   

   id32 (~40k sentences) is too small to train lstms
   create a larger training set with 11m sentences automatically parsed 

by two state-of-the-art parsers (and keep only those sentences for 
which two parsers agreed)

   grammar as foreign language    (deep learning)

vinyals et al., 2015

supplementary topics

i. cnf conversion

chomsky normal form

   chomsky normal form:

   all rules of the form x     y z or x     w
   in principle, this is no limitation on the space of (p)id18s

   n-ary rules introduce new non-terminals

vp

vbd   np   pp   pp

[vp     vbd np pp    ]

vp

[vp     vbd np    ]
vbd            np

pp

pp

   unaries / empties are    promoted   

   in practice it  s kind of a pain:
   reconstructing n-aries is easy
   reconstructing unaries is trickier
   the straightforward transformations don  t preserve tree scores

   makes parsing algorithms simpler!

original grammar
s     np vp
s     aux np vp

s     vp

np     pronoun

np     proper-noun

np     det nominal
nominal     noun 

nominal     nominal noun
nominal     nominal pp
vp     verb

0.8
0.1

0.1

0.2

0.2

0.6
0.3

0.2
0.5
0.2

vp     verb np
vp     vp pp
pp     prep np
lexicon:
noun     book | flight | meal | money

0.5
0.3
1.0

0.1     0.5      0.2     0.2
verb     book | include | prefer

0.5      0.2        0.3

cnf conversion 

example

det     the | a   | that | this
0.6  0.2  0.1    0.1

pronoun     i    | he | she | me
0.5  0.1  0.1    0.3

proper-noun     houston | nwa

0.8         0.2

aux     does
1.0

prep     from | to   | on | near | through

0.25  0.25  0.1    0.2     0.2

chomsky normal form

s     np vp
s     x1 vp
x1     aux np

0.8
0.1
1.0

original grammar
s     np vp
s     aux np vp

s     vp

np     pronoun

np     proper-noun

np     det nominal
nominal     noun 

nominal     nominal noun
nominal     nominal pp
vp     verb

0.8
0.1

0.1

0.2

0.2

0.6
0.3

0.2
0.5
0.2

vp     verb np
vp     vp pp
pp     prep np
lexicon (see previous slide for full list) :
noun     book | flight | meal | money

0.5
0.3
1.0

0.1     0.5      0.2     0.2
verb     book | include | prefer

0.5      0.2        0.3

chomsky normal form

s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
0.01     0.004    0.006

s     verb np
s     vp pp

0.8
0.1
1.0

0.05
0.03

original grammar
s     np vp
s     aux np vp

s     vp

np     pronoun

np     proper-noun

np     det nominal
nominal     noun 

nominal     nominal noun
nominal     nominal pp
vp     verb

0.8
0.1

0.1

0.2

0.2

0.6
0.3

0.2
0.5
0.2

vp     verb np
vp     vp pp
pp     prep np
lexicon (see previous slide for full list) :
noun     book | flight | meal | money

0.5
0.3
1.0

0.1     0.5      0.2     0.2
verb     book | include | prefer

0.5      0.2        0.3

chomsky normal form

s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
0.01     0.004    0.006

s     verb np
s     vp pp
np     i   |  he  |  she |  me
0.1   0.02  0.02    0.06

np     houston | nwa
0.16           .04

np     det nominal
nominal     book | flight | meal | money

0.03    0.15   0.06     0.06

nominal     nominal noun
nominal     nominal pp
vp     book | include | prefer
0.1      0.04        0.06

vp     verb np
vp     vp pp
pp     prep np

0.8
0.1
1.0

0.05
0.03

0.6

0.2
0.5

0.5
0.3
1.0

original grammar
s     np vp
s     aux np vp

s     vp

np     pronoun

np     proper-noun

np     det nominal
nominal     noun 

nominal     nominal noun
nominal     nominal pp
vp     verb

0.8
0.1

0.1

0.2

0.2

0.6
0.3

0.2
0.5
0.2

vp     verb np
vp     vp pp
pp     prep np
lexicon (see previous slide for full list) :
noun     book | flight | meal | money

0.5
0.3
1.0

0.1     0.5      0.2     0.2
verb     book | include | prefer

0.5      0.2        0.3

advanced topics

i. cky with unary rules

cnf + unary closure

we need unaries to be non-cyclic
   calculate closure close(r) for unary rules in r 

   add x   y if there exists a rule chain x   z1, z1   z2,..., zk    y with q(x   y) 

= q(x   z1)*q(z1   z2)*   *q(zk    y)

   if no unary rule exist for x, add x   x with q(x   x)=1 for all x in n

vp

vbd

np

dt

nn

vp

vbd

np

np

dt

nn

sbar

s

vp

warning: watch out 
for unary cycles!

sbar

vp

   rather than zero or more unaries, always exactly one
   alternate unary and binary layers
   what about x   y with different unary paths (and scores)?

initialization:
for all i     {1 . . . n}, for all x     n,

spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
tion of the    values, which allows a simple bottom-up id145 algo-
the cky algorithm
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
if x     xi     r
  (i, i, x) = ! q(x     xi)
values for the cases where j = i, then the cases where j = i + 1, and so on.
otherwise
   input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
all x     n,
   initialization: for i = 1     n and all x in n
    for l = 1 . . . (n     1)

algorithm:

  (i, i, x) = " q(x     xi)

if x     xi     r
otherwise

0

0

    for i = 1 . . . (n     l)

   for l = 1     (n-1) 
    set j = i + l
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
    for all x     n, calculate
   for all x in n

this is a natural de   nition: the only way that we can have a tree rooted in node
[iterate all phrase lengths]
x spanning word xi is if the rule x     xi is in the grammar, in which case the
   for i = 1     (n-l) and j = i+l
[iterate all phrases of length l]
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
there are no trees rooted in x spanning word xi.

[iterate all non-terminals]

for all x     n,

(q(x     y z)      (i, s, y )      (s + 1, j, z))
(q(x     y z)      (i, s, y )      (s + 1, j, z))

(1)

  (i, j, x) = max

  (i, j, x) = max
x   y z   r,
s   {i...(j   1)}

x   y z   r,
s   {i...(j   1)}

12

and

the next section of this note gives justi   cation for this recursive de   nition.
   also, store back pointers
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
bp(i, j, x) = arg max
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
values for   (i, j, x) such that j = i + 2; and so on.
note that the algorithm also stores backpointer values bp(i, j, x) for all values
of (i, j, x). these values record the rule x     y z and the split-point s leading to

output: return   (1, n, s) = maxt   t (s) p(t), and backpointers bp which allow recovery
of arg maxt   t (s) p(t).

(q(x     y z)      (i, s, y )      (s + 1, j, z))

s   {i...(j   1)}

x   y z   r,

spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
cky with unary closure
tion of the    values, which allows a simple bottom-up id145 algo-
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
values for the cases where j = i, then the cases where j = i + 1, and so on.
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
all x     n,

   input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
   initialization: for i = 1     n:

   step 1: for all x in n:

  (i, i, x) = " q(x     xi)

0

   step 2: for all x in n:

if x     xi     r
otherwise

   u (i, i, x) =

this is a natural de   nition: the only way that we can have a tree rooted in node
(q(x ! y )        (i, i, y ))
x spanning word xi is if the rule x     xi is in the grammar, in which case the
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
[iterate all phrase lengths]
there are no trees rooted in x spanning word xi.
   for i = 1     (n-l) and j = i+l
[iterate all phrases of length l]

x!y 2close(r)

   for l = 1     (n-1) 

max

   step 1: (binary) 
   for all x in n

   b(i, j, x) =

12
x!y z2r,s2{i...(j 1)}

max

[iterate all non-terminals]

(q(x ! y z)        u (i, s, y )        u (s + 1, j, z)

   step 2: (unary)

   for all x in n

[iterate all non-terminals]

   u (i, j, x) =

max

x!y 2close(r)

(q(x ! y )        b(i, j, y ))

advanced topics

2. grammar refinements :tag splits

tag splits

   problem: treebank 
tags are too coarse.

   example: sentential, 

pp, and other 
prepositions are all 
marked in.

   partial solution:

   subdivide the in tag.

annotation
v=h=2v
split-in

f1
78.3
80.3

size
8.0k
8.1k

other tag splits

   unary-dt: mark demonstratives as dt^u

   unary-rb: mark phrasal adverbs as rb^u

(   the x    vs.    those   )
(   quickly    vs.    very   )
parents (   not    is an rb^vp)

   tag-pa: mark tags with non-canonical 

[cf. charniak 97]

   split-aux: mark auxiliary verbs with    aux 
   split-cc: separate    but    and    &    from other 
   split-%:    %    gets its own tag.

conjunctions

f1
80.4

size
8.1k

80.5

8.1k

81.2

8.5k

81.6

9.0k

81.7

9.1k

81.8

9.3k

a fully annotated (unlex) tree

some test set results

parser

lp

magerman 95 84.9

collins 96

unlexicalized

charniak 97

collins 99

86.3

86.9

87.4

88.7

lr

84.6

85.8

85.7

87.5

88.6

f1
84.7
86.0
86.3
87.4
88.6

   beats    first generation    lexicalized parsers.
   lots of room to improve     more complex models next.

