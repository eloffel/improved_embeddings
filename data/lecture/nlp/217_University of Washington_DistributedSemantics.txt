cse 447/547

natural language processing

winter 2018

distributed semantics & embeddings

yejin choi - university of washington

[slides adapted from dan jurafsky]

why vector models of meaning?
computing the similarity between 

words

   fast    is similar to    rapid   
   tall    is similar to    height   

id53:
q:    how tall is mt. everest?   
candidate a:    the official height of mount everest is 29029 
feet   

similar words in plagiarism detection

problems with thesaurus-based meaning

   we don   t have a thesaurus for every language
   we can   t have a thesaurus for every year

   for historical linguistics, we need to compare word 

meanings in year t to year t+1

   thesauruses have problems with recall

   many words and phrases are missing
   thesauri work less well for verbs, adjectives

intuition of distributional word similarity

   suppose i asked you what is tesg  ino?
a bottle of tesg  ino is on the table
everybody likes tesg  ino
tesg  ino makes you drunk
we make tesg  ino out of corn.

   from context words humans can guess tesg  ino means

   an alcoholic beverage like beer

   intuition for algorithm: 

   two words are similar if they have similar word 

contexts.

distributional models of meaning
= vector-space models of meaning 

= vector semantics

intuitions:  zellig harris (1954):

      oculist and eye-doctor     occur in almost the same 

environments   

      if a and b have almost identical environments we say 

that they are synonyms.   

firth (1957): 

      you shall know a word by the company it keeps!   

four kinds of vector models

sparse vector representations

1. word co-occurrence matrices 

-- weighted by mutual-information

dense vector representations:

2. singular value decomposition (and latent semantic 

analysis)

3. neural-network inspired models (skip-grams, 

cbow)

4. brown clusters

shared intuition

   model the meaning of a word by    embedding    it in a vector 

space.

   the meaning of a word is a vector of numbers

   vector models are also called    embeddings   .

vector	semantics

i. words and co-occurrence vectors

co-occurrence matrices

   we represent how often a word occurs in a document

   term-document matrix

   or how often a word occurs with another

   term-term matrix 

(or word-word co-occurrence matrix
or word-context matrix)

term-document matrix

   each cell: count of word w in a document d:

   each document is a count vector in    v: a column below 

as#you#like#it
1
2
37
6

twelfth#night
1
2
58
117

julius#caesar
8
12
1
0

battle
soldier
fool
clown

henry#v
15
36
5
0

similarity in term-document matrices

two documents are similar if their vectors are similar

as#you#like#it
1
2
37
6

twelfth#night
1
2
58
117

julius#caesar
8
12
1
0

battle
soldier
fool
clown

henry#v
15
36
5
0

the words in a term-document matrix

   each word is a count vector in    d: a row below 

as#you#like#it
1
2
37
6

twelfth#night
1
2
58
117

julius#caesar
8
12
1
0

battle
soldier
fool
clown

henry#v
15
36
5
0

the words in a term-document matrix

   two words are similar if their vectors are similar

as#you#like#it
1
2
37
6

twelfth#night
1
2
58
117

julius#caesar
8
12
1
0

battle
soldier
fool
clown

henry#v
15
36
5
0

the word-word or word-context matrix

   instead of entire documents, use smaller contexts

   window of    4 words

   paragraph

   a word is now defined by a vector over counts of context 

words

   instead of each vector being of length d
   each vector is now of length |v|
   the word-word matrix is |v|x|v|

sample contexts    7 words

word co-occur in some context in some training corpus. the context could be the
document, in which case the cell represents the number of times the two words
appear in the same document. it is most common, however, to use smaller contexts,
such as a window around the word, for example of 4 words to the left and 4 words
to the right, in which case the cell represents the number of times (in some training
corpus) the column word occurs in such a   4 word window around the row word.
for example here are 7-word windows surrounding four sample words from the
brown corpus (just one example of each word):

word-word matrix

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

preserve or jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

   

0
0
aardvark
0
0
   

aardvark computer
0
0
2
0
1
0
2
1

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 17.2 shows a selection from the word-word
apricot
co-occurrence matrix computed from the brown corpus for these four words.
pineapple
digital
apricot
information
pineapple
   
information
figure 19.2 co-occurrence vectors for four words, computed from the brown corpus,
showing only six of the dimensions (hand-picked for pedagogical purposes). note that a
real vector would be vastly more sparse.

data pinch result
0
0
result
1
4

sugar
1
1
0
0

1
1
pinch
0
0

computer

...
...
...
...
...

0
0
1
6

digital

sugar

0
0
1
6

1
1
0
0

1
1
0
0

0
0
1
4

data

0
0
0
0

...

word-word matrix

   we showed only 4x6, but the real matrix is 50,000 x 50,000

   so it   s very sparse (most values are 0)
   that   s ok, since there are lots of efficient algorithms for 

   the size of windows depends on your goals

   the shorter the windows   

sparse matrices.

   the more syntactic the representation (   1-3 words)
   the more semantic the representation (   4-10 words)

   the longer the windows   

vector	semantics

positive pointwise mutual information 

(ppmi)

informativeness of a context word x 

for a target word y

   freq(the, beer)     vs   freq(drink, beer) ?
   how about joint id203?
   p(the, beer) 
   frequent words like    the    and    of    are not quite 

vs   (drink, beer) ?

informative

   normalize by the individual word frequencies!

   pointwise mutual information (pmi)

pointwise mutual information

pointwise mutual information: 

do events x and y co-occur more than if they were independent?

p m i(x = x, y = y) = log2

p (x, y)

p (x)p (y)

do words x and y co-occur more than if they were independent? 

pmi between two words:  (church & hanks 1989)

pmi                ),                + =log+     (                ),                +)
                    )    (                +)

positive pointwise mutual information

   pmi ranges from       		to	+   

   but the negative values are problematic

   things are co-occurring less than we expect by chance
   unreliable without enormous corpora

   imagine w1 and w2 whose id203 is each 10-6
   hard to be sure p(w1,w2) is significantly different than 10-12

   plus it   s not clear people are good at    unrelatedness   

   so we just replace negative pmi values by 0

   positive pmi (ppmi) between word1 and word2:

ppmi                ),                + =max log+     (                ),                +)
                    )    (                +),0

computing ppmi on a term-context matrix

   matrix f with w rows (words) and c columns (contexts)
   fij is # of times wi occurs in context cj

j=1 fij

fij

pij =

pw
i=1pc
pi    = pc
pw
i=1pc
p   j = pw
pw
i=1pc

j=1 fij

j=1 fij

i=1 fij

j=1 fij

pmiij = log

pij
pi   p   j

ppmiij = max(0, pmiij)

pij =

fij

pw
i=1pc

j=1 fij

p(wi) = pc

j=1 fij
n

this image cannot currently be displayed.

= .32

p(w=information,c=data) = 
p(w=information) =
p(c=data) =

6/19
11/19 = .58

7/19 = .37

apricot
pineapple
digital
information

computer
0.00
0.00
0.11
0.05

p(w,context)

data pinch result
0.00
0.00
0.00
0.00
0.05
0.05
0.21
0.32

0.05
0.05
0.00
0.00

sugar
0.05
0.05
0.00
0.00

p(w)

0.11
0.11
0.21
0.58

p(context)

0.16

0.37

0.11

0.26

0.11

pmiij = log

pij
pi   p   j

apricot
pineapple
digital
information
p(context)

computer
0.00
0.00
0.11
0.05

p(w,context)

data pinch result
0.00
0.00
0.00
0.00
0.05
0.05
0.21
0.32

0.05
0.05
0.00
0.00

p(w)

0.11
0.11
0.21
0.58

sugar
0.05
0.05
0.00
0.00

0.16

0.37

0.11

0.26

0.11

   pmi(information,data) = log2 (

.32 / (.37*.58) ) = .58

(.57 using full precision)

apricot
pineapple
digital
information

computer
1
1
1.66
0.00

ppmi(w,context)

data pinch result
1
1
0.00
0.47

1
1
0.00
0.57

2.25
2.25
1
1

sugar
2.25
2.25
1
1

weighting pmi

   pmi is biased toward infrequent events

   very rare words have very high pmi values

   two solutions:

   give rare words slightly higher probabilities
   use add-one smoothing (which has a similar effect)

0

0

0.47

0.57

information
figure 19.4 the ppmi matrix showing the association between words and context words,
weighting pmi: giving rare context words 
computed from the counts in fig. 17.2 again showing six dimensions.
pmi has the problem of being biased toward infrequent events; very rare words
tend to have very high pmi values. one way to reduce this bias toward low frequency
events is to slightly change the computation for p(c), using a different function pa (c)
that raises contexts to the power of a (levy et al., 2015):

   raise the context probabilities to     =0.75:

slightly higher id203

0

(19.8)

(19.9)

ppmia (w,c) = max(log2

p(w,c)

p(w)pa (c)

,0)

pa (c) =

count(c)a

   this helps because     ?     >         for rare c
pc count(c)a
.cc.def.g).de=.03
.cc.def.g).de=.97    ?     = .g).de
       ?     = .cc.de

levy et al. (2015) found that a setting of a = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams (mikolov et al., 2013a) and glove (pennington et al., 2014)). this works
because raising the id203 to a = 0.75 increases the id203 assigned to rare
contexts, and hence lowers their pmi (pa (c) > p(c) when c is rare).

another possible solution is laplace smoothing: before computing pmi, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. the larger the k, the more the non-zero counts
are discounted.

   consider two events, p(a) = .99 and p(b)=.01

tf-idf: alternative to ppmi for 

measuring association

   tf-idf (that   s a hyphen not a minus sign)
   the combination of two factors

   term frequency (luhn 1957): frequency of the word
   inverse document frequency (idf) (sparck jones 1972)

= # of documents with word i

   n is the total number of documents
   dfi =    document frequency of word i   
!
idfi = log n
#
dfi
#
"
= weight of word i in document j

$
&
&
%

wij = tfijidfi

  

vector	semantics

measuring similarity: the cosine

19.2

dot product
inner product

vector length

apricot
pineapple

    sparse vector models: positive pointwise mutual information
sugar
0.56
0.56
0
0

measuring similarity
pinch
computer
0.56
0.56
0
0

0
0
0
   given 2 target words v and w
0.58
   we   ll need a way to measure their similarity.
   most measure of vectors similarity are based on:

information
figure 19.6 the add-2 laplace smoothed ppmi matrix from the add-2 smoothing counts
in fig. 17.5.

0
0
0.62
0

0
0
0
0.37

digital

result

data

the cosine   like most measures for vector similarity used in nlp   is based on

   dot product or inner product from id202

the dot product operator from id202, also called the inner product:

nxi=1

dot-product(~v,~w) =~v   ~w =

viwi = v1w1 + v2w2 + ... + vnwn

(19.10)

   high when two vectors have large values in same 

intuitively, the dot product acts as a similarity metric because it will tend to be
dimensions. 
high just when the two vectors have large values in the same dimensions. alterna-
   low (in fact 0) for orthogonal vectors with zeros in 
tively, vectors that have zeros in different dimensions   orthogonal vectors    will be
complementary distribution
very dissimilar, with a dot product of 0.
this raw dot-product, however, has a problem as a similarity metric: it favors

long vectors. the vector length is de   ned as

nxi=1

in fig. 17.5.
dot-product(~v, ~w) = ~v   ~w =
the dot product operator from id202, also called the inner product:

problem with dot product

the cosine   like most measures for vector similarity used in nlp   is based on

viwi = v1w1 + v2w2 + ... + vnwn

intuitively, the dot product acts as a similarity metric because it will tend to be
high just when the two vectors have large values in the same dimensions. alterna-
tively, vectors that have zeros in different dimensions   orthogonal vectors    will be
dot-product(~v,~w) =~v   ~w =
very dissimilar, with a dot product of 0.

viwi = v1w1 + v2w2 + ... + vnwn

this raw dot-product, however, has a problem as a similarity metric: it favors

long vectors. the vector length is de   ned as

   dot product is longer if the vector is longer. vector length:
intuitively, the dot product acts as a similarity metric because it will tend to be
high just when the two vectors have large values in the same dimensions. alterna-
tively, vectors that have zeros in different dimensions   orthogonal vectors    will be
very dissimilar, with a dot product of 0.

(19.10)

(19.10)

(19.11)

this raw dot-product, however, has a problem as a similarity metric: it favors

the dot product is higher if a vector is longer, with higher values in each dimension.
   vectors are longer if they have higher values in each dimension
long vectors. the vector length is de   ned as
more frequent words have longer vectors, since they tend to co-occur with more
   that means more frequent words will have higher dot products
words and have higher co-occurrence values with each of them. raw dot product
thus will be higher for frequent words. but this is a problem; we   d like a similarity
   that   s bad: we don   t want a similarity metric to be sensitive to 
metric that tells us how similar two words are irregardless of their frequency.

(19.11)

nxi=1
|~v| =vuut
nxi=1
|~v| =vuut

the simplest way to modify the dot product to normalize for the vector length is
to divide the dot product by the lengths of each of the two vectors. this normalized
dot product turns out to be the same as the cosine of the angle between the two
vectors, following from the de   nition of the dot product between two vectors ~a and

the dot product is higher if a vector is longer, with higher values in each dimension.
more frequent words have longer vectors, since they tend to co-occur with more

word frequency

nxi=1

v2
i

v2
i

|~v| =vuut

dot product turns out to be the same as the cosine of the angle between the two
the dot product is higher if a vector is longer, with higher values in each dimension.
vectors, following from the de   nition of the dot product between two vectors ~a and
more frequent words have longer vectors, since they tend to co-occur with more
~b:
words and have higher co-occurrence values with each of them. raw dot product
thus will be higher for frequent words. but this is a problem; we   d like a similarity
metric that tells us how similar two words are irregardless of their frequency.

solution: cosine

vectors!

   just divide the dot product by the length of the two 

the simplest way to modify the dot product to normalize for the vector length is
to divide the dot product by the lengths of each of the two vectors. this normalized
dot product turns out to be the same as the cosine of the angle between the two
vectors, following from the de   nition of the dot product between two vectors ~a and

~a  ~b = |~a||~b|cosq
~a  ~b
|~a||~b|

= cosq

the cosine similarity metric between two vectors ~v and ~w thus can be computed
   this turns out to be the cosine of the angle between them!

as:

the cosine similarity metric between two vectors ~v and ~w thus can be computed

as:

~a  ~b = |~a||~b|cosq
~a  ~b
|~a||~b|

= cosq
~v   ~w
|~v||~w|

=

cosine(~v,~w) =

(19.12)

viwi

nxi=1
ivuut
nxi=1

v2

nxi=1

w2
i

vuut
nxi=1

cosine for computing similarity

sec. 6.3

dot product

unit vectors

cos(   v,    w) =

   v    
   w
   v    w =

   v
   v    

   w
   w =

n
   
i=1
n
2
vi
i=1

viwi
n
   
i=1

   

2
wi

vi is	the	ppmi	value	for	word	v in	context	i
wi is	the	ppmi	value	for	word	w in	context	i.
cos(v,w)	is	the	cosine	similarity	of	v and	w

 

   
e
g
r
a
l
   
 
:
1
n
o
i
s
n
e
m
d

i

visualizing vectors and angles

apricot
digital
information

large
2
0
1

data
0
1
6

apricot

information

3

2

1

digital

1

2

3

5
dimension 2:    data   

4

6

7

vector	semantics

dense vectors 

sparse versus dense vectors

   ppmi vectors are

   long (length |v|= 20,000 to 50,000)
   sparse (most elements are zero)

   alternative: learn vectors which are

   short (length 200-1000)
   dense (most elements are non-zero)

sparse versus dense vectors

   why dense vectors?

   short vectors may be easier to use as features in machine 

learning (less weights to tune)

   dense vectors may generalize better than storing explicit 

counts

   they may do better at capturing synonymy:

   car and automobile are synonyms; but are represented as 
distinct dimensions; this fails to capture similarity between 
a word with car as a neighbor and a word with automobile
as a neighbor

three methods for short dense vectors

   singular value decomposition (svd)

   a special case of this is called lsa (latent semantic analysis)
   (see supplementary topics)

   embeddings

   skip-grams and cbow

   brown id91

vector	semantics

embeddings inspired by neural language 

models: skip-grams and cbow

prediction-based models:

an alternative way to get dense vectors

   skip-gram (mikolov et al. 2013a)  cbow (mikolov et al. 2013b)
   learn embeddings as part of the process of word prediction.
   train a neural network to predict neighboring words

   inspired by neural net language models (sans nonlinearity).
   in so doing, learn dense embeddings for the words in the 

training corpus.

   advantages:

   fast, easy to train (much faster than svd)
   available online in the id97 package
   including sets of pretrained embeddings!

cbow et al. 2013a), draw inspiration from the neural methods for id38 intro-
duced in chapter 5. like the neural language models, these models train a network
to predict neighboring words, and while doing so learn dense embeddings for the
words in the training corpus. the advantage of these methods is that they are fast,
ef   cient to train, and easily available online in the id97 package; code and
pretrained embeddings are both available.

   predict each neighboring word 

skip-grams

   in a context window of 2c words 
   from the current word. 

we   ll begin with the skip-gram model. the skip-gram model predicts each
   so for c=2, we are given word wt and predicting these 4 words:
neighboring word in a context window of 2c words from the current word. so
for a context window c = 2 the context is [wt 2,wt 1,wt+1,wt+2] and we are pre-
dicting each of these from word wt. fig. 17.12 sketches the architecture for a sample

the skip-gram model actually learns two d-dimensional embeddings for each
word w: the input embedding v and the output embedding v0. these embeddings
are encoded in two matrices, the input matrix w and the output matrix w0. each
column i of the input matrix w is the 1    d vector embedding vi for word i in the
vocabulary. each row i of the output matrix w0 is a d     1 vector embedding v0i for

skip-grams learn 2 embeddings for each w

output embedding v  , in the output matrix w   
   embedding of the context word
   column i of the output matrix w   is a  1 x d 
embedding v  i for word i in the vocabulary.

input embedding v, in the input matrix w
   embedding of the target word
   row i of the input matrix w is the d x 1 

embedding vi for word i in the vocabulary

1
2
.
.
d

1
2
.
.
.
.

i

.
.
.
.
|v|

w   
w
i

.
.

1 2

   

|v|

d x  |v|

w   
w
d   

1 2

 |v| x d

setup

   walking through corpus pointing at word w(t), whose 

index in the vocabulary is j, so we   ll call it wj (1 < j < |v |). 
   let   s predict w(t+1) , whose index in the vocabulary is k 

(1 < k < |v |). hence our task is to compute p(wk|wj). 

one-hot vectors

   a vector of length |v| 
   1 for the target word and 0 for other words
   so if    popsicle    is vocabulary word 5
   the one-hot vector is
   [0,0,0,0,1,0,0,0,0      .0]

skip-gram

projection layer
embedding for wt

output layer

probabilities of
context words

y1
y2

w    d     |v|

yk

wt-1

w    d     |v|

1   d

y|v|
y1
y2

yk

y|v|

wt+1

input layer
1-hot input vector
x1
x2

wt

xj

w

|v|   d

x|v|
1   |v|

skip-gram

w t wt = vj

projection layer
embedding for wt

input layer
1-hot input vector
x1
x2

wt

xj

w

|v|   d

x|v|
1   |v|

1   d

w 0t vj = wt 1
yk = v0t
k vj
output layer

probabilities of
context words

y1
y2

yk

wt-1

y|v|
y1
y2

yk

y|v|

wt+1

w    d     |v|

w    d     |v|

the output matrix w0. the result for each context word, o = w0h, is a 1   |v|
dimensional output vector giving a score for each of the |v| vocabulary words.
in doing so, the element ok was computed by multiplying h by the output
embedding for word wk: ok = v0kh.

turning outputs into probabilities

3. finally, for each context word we normalize this score vector, turning the

score for each element ok into a id203 by using the soft-max function:

   we use softmax to turn into probabilities

yk = v0t

k vj = v0k    vj

p(wk|w j) =

exp(v0k    v j)
pw02|v|

exp(v0w    v j)

the next section explores how the embeddings, the matrices w and w0, are
learned. once they are learned, we   ll have two embeddings for each word wi: vi and
v0i. we can just choose to use the input embedding vi from w, or we can add the
two and use the embedding vi + v0i as the new d-dimensional embedding, or we can
concatenate them into an embedding of dimensionality 2d.

as with the simple count-based methods like ppmi, the context window size c

embeddings from w and w   

   since we have two embeddings, vj and v   j for each word wj
   we can either:
   just use vj
   sum them
   concatenate them to make a double-length embedding

we   ll also assume that the probabilities of each context (output) word is independent
argmax
of the other outputs:
   

(19.26)

(19.27)

the goal of the model is to learn representations (the embedding matrices w and
w0; we   ll refer to them collectively as the parameters q) that do well at predicting
the context words, maximizing the log likelihood of the corpus, text.

independent of the other input words,
independent of the other input words,

independent of the other input words,
argmax

we   ll    rst make the naive bayes assumptions that the input word at time t is

training embeddings
log
p(w(t c), ...,w(t 1),w(t+1), ...,w(t+c))
p(w(t c), ...,w(t 1),w(t+1), ...,w(t+c))
log p(text)
independent of the other input words,

log
q
argmax
log
(19.26)
we   ll also assume that the probabilities of each context (output) word is independent
q
of the other outputs:

we   ll    rst make the naive bayes assumptions that the input word at time t is

we   ll    rst make the naive bayes assumptions that the input word at time t is
|w(t))

we   ll also assume that the probabilities of each context (output) word is independent
of the other outputs:
we   ll also assume that the probabilities of each context (output) word is independent
(19.27)
of the other outputs:

argmax
independent of the other input words,

p(w(t c), ...,w(t 1),w(t+1), ...,w(t+c))

p(w(t c), ...,w(t 1),w(t+1), ...,w(t+c))
q

argmax
argmax
q

log p(text)

tyt=1

(19.25)

(19.25)

(19.26)

argmax

(19.26)

(19.26)

q

q

q

argmax

tyt=1

log p(w(t+ j)|w(t))

q x c    j   c, j6=0

we now substitute in eq. 17.24:
log

log
argmax
argmax
we now substitute in eq. 17.24:
we now substitute in eq. 17.24:
q

= argmax
argmax
with some rearrangements::
we now substitute in eq. 17.24:

= argmax
= argmax
with some rearrangements::
with some rearrangements::
q

tyt=1
tyt=1
tyt=1
q x c    j   c, j6=0
log p(w(t+ j)|w(t))
q x c    j   c, j6=0
txt=1
log p(w(t+ j)|w(t))
p(w(t c), ...,w(t 1),w(t+1), ...,w(t+c))
= argmax
txt=1 x c    j   c, j6=0
exp(v0(t+ j)    v(t))
log
pw2|v|
q x c    j   c, j6=0
exp(v0w    v(t))
log p(w(t+ j)|w(t))
txt=1 x c    j   c, j6=0
exp(v0(t+ j)    v(t))
pw2|v|
exp(v0w    v(t))
txt=1 x c    j   c, j6=0
exp(v0(t+ j)    v(t))
pw2|v|
exp(v0w    v(t))
24v0(t+ j)    v(t)   logxw2|v|
exp(v0w    v(t))35
log p(w(t+ j)|w(t))
txt=1 x c    j   c, j6=0
txt=1 x c    j   c, j6=0
exp(v0(t+ j)    v(t))
log
24v0(t+ j)    v(t)   logxw2|v|
exp(v0w    v(t))35
pw2|v|
exp(v0w    v(t))
exp(v0w    v(t))35
24v0(t+ j)    v(t)   logxw2|v|

q x c    j   c, j6=0
txt=1 x c    j   c, j6=0
txt=1 x c    j   c, j6=0
txt=1 x c    j   c, j6=0

with some rearrangements::

q
= argmax

log
log

= argmax

argmax

= argmax

q

q

we   ll also assume that the probabilities of each context (output) word is independent
of the other outputs:

(19.28)

eq. 17.29 shows that we are looking to set the parameters q (the embedding
(19.29)
matrices w and w0) in a way that maximizes the similarity between each word w(t)
q

(19.28)

(19.29)

(19.28)

we now substitute in eq. 17.24:

(19.27)

(19.27)

(19.27)

(19.28)

= argmax

training: noise contrastive estimation (nce)
with some rearrangements::

the goal of the model is to learn representations (the embedding matrices w and
txt=1 x c    j   c, j6=0
exp(v0(t+ j)    v(t))
w0; we   ll refer to them collectively as the parameters q) that do well at predicting
pw2|v|
exp(v0w    v(t))
the context words, maximizing the log likelihood of the corpus, text.
24v0(t+ j)    v(t)   logxw2|v|

we   ll    rst make the naive bayes assumptions that the input word at time t is

exp(v0w    v(t))35

independent of the other input words,
q

txt=1 x c    j   c, j6=0

log p(text)

= argmax

argmax

(19.25)

log

q

q

we   ll also assume that the probabilities of each context (output) word is independent
of the other outputs:

argmax

q

tyt=1

p(w(t c), ...,w(t 1),w(t+1), ...,w(t+c))

eq. 17.29 shows that we are looking to set the parameters q (the embedding
   the id172 factor is too expensive to compute 
log
(19.26)
matrices w and w0) in a way that maximizes the similarity between each word w(t)
exactly (why?)
and its nearby context words w(t+ j), while minimizing the similarity between word
   negative sampling: sample only a handful of negative 
w(t) and all the words in the vocabulary.

examples to compute the id172 factor
the actual training objective for skip-gram, the negative sampling approach, is
   (some engineering detail) the actual skip-gram training 
somewhat different; because it   s so time-consuming to sum over all the words in
the vocabulary v, the algorithm merely chooses a few negative samples to minimize
argmax
rather than every word. the training proceeds by stochastic id119, using
error id26 as described in chapter 5 (mikolov et al., 2013a).

also converts the problem into binary classification 
(id28) of predicting whether a given word 
is a context word or not
there is an interesting relationship between skip-grams, svd/lsa, and ppmi.
if we multiply the two context matrices w   w0t , we produce a |v|   |v| matrix x,
each entry mi j corresponding to some association between input word i and output

q x c    j   c, j6=0

log p(w(t+ j)|w(t))

(19.27)

we now substitute in eq. 17.24:

txt=1 x c    j   c, j6=0

relation between skipgrams and pmi!

   if we multiply ww    
   we get a |v|x|v| matrix m , each entry mij corresponding to 
some association between input word i and output word j 
   levy and goldberg (2014b) show that skip-gram reaches 

its optimum just when this matrix is a shifted version of 
pmi:

ww   =mpmi    log k 

   so skip-gram is implicitly factoring a shifted version of the 

pmi matrix into the two embedding matrices.

cbow (continuous bag of words)

input layer
1-hot input vectors
for each context word

wt-1

x1
x2

xj

x|v|
x1
x2

wt+1

xj

x|v|
1   |v|

w

|v|   d

w

|v|   d

projection layer
sum of embeddings
 for context words

output layer
id203 of wt

w    d     |v|

y1
y2

yk

y|v|

wt

1   d

vector, turning the score for each element ok into a id203 by using the soft-max
function.
properties of embeddings

19.5 properties of embeddings

   nearest words to some embeddings (mikolov et al. 2013)

we   ll discuss in section 17.8 how to evaluate the quality of different embeddings.
but it is also sometimes helpful to visualize them. fig. 17.14 shows the words/phrases
that are most similar to some sample words using the phrase-based version of the
skip-gram algorithm (mikolov et al., 2013a).

target:

redmond
redmond wash.
redmond washington
microsoft

capitulate
capitulation
capitulated
capitulating
figure 19.14 examples of the closest tokens to some target words using a phrase-based
extension of the skip-gram algorithm (mikolov et al., 2013a).

havel
vaclav havel
president vaclav havel martial arts
velvet revolution

graf   ti
spray paint
gra   tti
taggers

ninjutsu
ninja

swordsmanship

one semantic property of various kinds of embeddings that may play in their

usefulness is their ability to capture relational meanings

mikolov et al. (2013b) demonstrates that the offsets between vector embeddings
can capture some relations between words, for example that the result of the ex-
pression vector(   king   ) - vector(   man   ) + vector(   woman   ) is a vector close to vec-
tor(   queen   ); the left panel in fig. 17.15 visualizes this by projecting a representation
down into 2 dimensions. similarly, they found that the expression vector(   paris   )
- vector(   france   ) + vector(   italy   ) results in a vector that is very close to vec-

embeddings capture relational meaning!

vector(   king   ) - vector(   man   ) + vector(   woman   )    	vector(   queen   )
vector(   paris   ) - vector(   france   ) + vector(   italy   )     vector(   rome   )

vector	semantics

brown id91

brown id91

   an agglomerative id91 algorithm that clusters 
words based on which words precede or follow them

   these word clusters can be turned into a kind of vector
   we   ll give a very brief sketch here.

tions with the preceding or following words.

class-based language model

brown id91 (brown et al., 1992) is an agglomerative id91 algorithm for
deriving vector representations of words by id91 words based on their associa-
tions with the preceding or following words.

the algorithm makes use of the class-based language model (brown et al.,
1992), a model in which each word w 2 v belongs to a class c 2 c with a id203
the algorithm makes use of the class-based language model (brown et al.,
p(w|c). class based lms assigns a id203 to a pair of words wi 1 and wi by
1992), a model in which each word w 2 v belongs to a class c 2 c with a id203
modeling the transition between classes rather than between words:
p(w|c). class based lms assigns a id203 to a pair of words wi 1 and wi by
modeling the transition between classes rather than between words:

   suppose each word was in some class ci:
p(wi|wi 1) = p(ci|ci 1)p(wi|ci)
p(wi|wi 1) = p(ci|ci 1)p(wi|ci)
p(ci|ci 1)p(wi|ci)
p(corpus|c) =
a particularly id91 c as follows:

a particularly id91 c as follows:
the class-based lm can be used to assign a id203 to an entire corpus given

the class-based lm can be used to assign a id203 to an entire corpus given

(19.32)

(19.33)

class-based language models are generally not used as a language model for ap-
plications like machine translation or id103 because they don   t work
as well as standard id165s or neural language models. but they are an important
component in brown id91.

p(ci|ci 1)p(wi|ci)

class-based language models are generally not used as a language model for ap-
plications like machine translation or id103 because they don   t work

brown id91 is a hierarchical id91 algorithm. let   s consider a naive

p(corpus|c) =

nyi 1
nyi 1

brown id91 algorithm

   each word is initially assigned to its own cluster. 
   we now consider consider merging each pair of clusters. 

highest quality merge is chosen.
   quality = merges two words that have similar 
probabilities of preceding and following words

   (more technically quality = smallest decrease in the 
likelihood of the corpus according to a class-based 
language model) 

   id91 proceeds until all words are in one big 

cluster. 

brown clusters as vectors

   by tracing the order in which clusters are merged, the 

model builds a binary tree from bottom to top.

   each word represented by binary string = path from root 

to leaf

   each intermediate node is a cluster 
brown algorithm
   chairman is 0010,    months    = 01, and verbs = 1

000
ceo

0

00

01

001

010

011

0011
president

november october

0010
chairman
    words merged according to contextual 

similarity

1

10

100
101
run sprint

11
walk

based on immediately neighboring words, brown clusters are most commonly used
for representing the syntactic properties of words, and hence are commonly used as
a feature in parsers. nonetheless, the clusters do represent some semantic properties
as well. fig. 19.17 shows some examples from a large id91 from brown et al.
(1992).

brown cluster examples

friday monday thursday wednesday tuesday saturday sunday weekends sundays saturdays
june march july april january december october november september august
pressure temperature permeability density porosity stress velocity viscosity gravity tension
anyone someone anybody somebody
had hadn   t hath would   ve could   ve should   ve must   ve might   ve
asking telling wondering instructing informing kidding reminding bothering thanking deposing
mother wife father son husband brother daughter sister boss uncle
great big vast sudden mere sheer gigantic lifelong scant colossal
down backwards ashore sideways southward northward overboard aloft downwards adrift
figure 19.17 some sample brown clusters from a 260,741-word vocabulary trained on 366
million words of running text (brown et al., 1992). note the mixed syntactic-semantic nature
of the clusters.

note that the naive version of the brown id91 algorithm described above is
extremely inef   cient     o(n5): at each of n iterations, the algorithm considers each
of n2 merges, and for each merge, compute the value of the id91 by summing
over n2 terms. because it has to consider every possible pair of merges. in practice
we use more ef   cient o(n3) algorithms that use tables to pre-compute the values for
each merge (brown et al. 1992, liang 2005).

brown id91 on twitter!

http://www.cs.cmu.edu/~ark/tweetnlp/cluster_viewer.htm

l

1110101010010 (52) 

sorry gutted sry srry soz #thankful sory sorrry sowwy sori
thankgod sorryy sowi sorri sorryyy sorrrry luckyyy sowwie paiseh
sowie soory sorry- sorrrrry sowee -sorry sorryyyy
#didntwannatellyou sorreh sorr sowy soorry sorrryyy apols sawry
#iforgiveyou sryy sorrie sowwwy offski s0rry sorrryy soryy sorrrrrry
sawwy sorryyyyy sozz nitm sowry thankgoodness sowwi

00101110 (79) 

really rly realy genuinely rlly reallly realllly reallyy rele realli relly reallllly reli
reali sholl rily reallyyy reeeeally realllllly reaally reeeally rili reaaally
reaaaally reallyyyy rilly reallllllly reeeeeally reeally shol realllyyy reely relle
reaaaaally shole really2 reallyyyyy _really_ realllllllly reaaly realllyy reallii
reallt genuinly relli realllyyyy reeeeeeally weally reaaallly reallllyyy

1110101100111 (582) 

:( :/ -_- -.- :-( :'( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /:     :(( 
>_< =[ :[ #fml      -_____- =\ >:(      -,- >.> >:o ;/      d; .-. -______-
>_> :((( -_-" =s (cid:0) ;_; #ugh :-\ =.= (cid:0) -_______-

summary

   distributional (vector) models of meaning

   sparse (ppmi-weighted  word-word co-occurrence 

matrices)

   dense:

   word-word  svd 50-2000 dimensions
   skip-grams and cbow 
   brown clusters 5-20 binary dimensions.

supplementary topics

vector	semantics

dense vectors via svd

intuition

   approximate an n-dimensional dataset using fewer 

dimensions

   by first rotating the axes into a new space
   in which the highest order dimension captures the most 

variance in the original dataset

   and the next dimension captures the next most variance, etc.
   many such (related) methods:

   pca     principle components analysis
   factor analysis
   svd

id84

pca dimension 1

pca dimension 2

6
6

5
5

4
4

3
3

2
2

1
1

1
1

2
2

3
3

4
4

5
5

6
6

here.  suffice  it  to  say  that  cookbook  versions  of  svd  adequate  for 
small (e.g.,  100      100)  matrices  are  available  in  several places  (e.g., 
mathematica,  1991 ), and a free software version (berry,  1992)  suitable 

singular value decomposition

any (w x c) matrix x equals the product of 3 matrices:

contexts 

3= 

m x m  

m x c  

wxc 

w 

xm 

figure  a1.  schematic  diagram  of  the  singular  value  decomposition 

singular value decomposition

any (w x c) matrix x equals the product of 3 matrices:

x = w s c

w: (w x m) matrix: rows corresponding to original but m columns 
represents a dimension in a new latent space, such that 

    m column vectors are orthogonal to each other
    m =    rank    of x. 

s: (m x m) matrix: diagonal matrix of singular values expressing 
the importance of each dimension.
c: (m x c) matrix: columns corresponding to original but m rows 
corresponding to singular values

mathematica,  1991 ), and a free software version (berry,  1992)  suitable 

singular value decomposition

contexts 

3= 

m x m  

m x c  

wxc 

w 

xm 

figure  a1.  schematic  diagram  of  the  singular  value  decomposition 
(svd)  of  a  rectangular  word  (w)  by  context  (c)  matrix  (x).  the 
original matrix is decomposed into three matrices: w  and c, which are 

landuaer and dumais 1997

svd applied to term-document matrix:

latent semantic analysis (lsa)

analysis (lsa),  if one or more factor is omitted (that is, if one or more 
singular  values  in  the  diagonal  matrix  along  with  the  corresponding 
singular vectors of the other two matrices are deleted), the reconstruction 
is a least-squares best approximation to the original given the remaining 
dimensions.  thus,  for  example,  after  constructing  an  svd,  one  can 
reduce the number of dimensions systematically by, for example, remov- 
ing those with the smallest effect on the sum-squared error of the approx- 
imation simply by deleting those with the smallest singular values. 

deerwester et al (1988)

   often m is not small enough!
   if instead of keeping all m dimensions, we just keep the top k

the actual algorithms used to compute svds for large sparse matrices 
of the sort involved in lsa are rather sophisticated and are not described 
here.  suffice  it  to  say  that  cookbook  versions  of  svd  adequate  for 
small (e.g.,  100      100)  matrices  are  available  in  several places  (e.g., 
mathematica,  1991 ), and a free software version (berry,  1992)  suitable 

singular values. let   s say 300.

   the result is a least-squares approximation to the original x
   but instead of multiplying,                                                                    

contexts 

we   ll just make use of w.

   each row of w:

   a k-dimensional vector
   representing word w

3= 

wxc 

w 

k
xm 
/

m x m  
/
k

/
k

/
m x c  
k

figure  a1.  schematic  diagram  of  the  singular  value  decomposition 
(svd)  of  a  rectangular  word  (w)  by  context  (c)  matrix  (x).  the 

lsa more details

   300 dimensions are commonly used
   the cells are commonly weighted by a product of two weights

   local weight:  log term frequency
   global weight: either idf or an id178 measure

let   s return to ppmi word-word matrices

   can we apply svd to them?

original m. since the    rst dimensions encode the most variance, one way to view
the reconstruction is thus as modeling the most important information in the original
dataset.

svd applied to term-term matrix

svd applied to co-occurrence matrix x:

2666664

x

=

w

3777775

2666664

2666664

s1 0
0 . . . 0
0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sv
0
0
|v|   |v|

...

3777775

2666664

c

3777775

|v|   |v|

|v|   |v|
taking only the top k dimensions after svd applied to co-occurrence matrix x:

|v|   |v|

(simplifying assumption: the matrix has rank |v|)

2666664

x

|v|   |v|

3777775

2666664

w

=

|v|    k

s1 0
0 . . . 0
0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sk
0
0
k    k

...

3777775

h

i

c

k   |v|

3777775
2666664

3777775

2666664

3777775

2666664

0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sv
0
0
|v|   |v|

...

3777775

2666664

x

w

truncated svd on term-term matrix

=

c

|v|   |v|

|v|   |v|
taking only the top k dimensions after svd applied to co-occurrence matrix x:

|v|   |v|

2666664

x

|v|   |v|

3777775

=

2666664

w

|v|    k

s1 0
0 . . . 0
0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sk
0
0
k    k

...

i

c

k   |v|

h

3777775

figure 19.11 svd factors a matrix x into a product of three matrices, w, s, and c. taking
the    rst k dimensions gives a |v|    k matrix wk that has one k-dimensioned row per word that
can be used as an embedding.

2666664

3777775
2666664

3777775

2666664

3777775

2666664

2666664

embedding

for 
word i
=

x

3777775

2666664

w

|v|    k

2666664

3777775
2666664

3777775

s1 0

|v|   |v|
truncated svd produces embeddings

|v|   |v|

taking only the top k dimensions after svd applied to co-occurrence matrix x:

   each row of w matrix is a k-dimensional 

representation of each word w
   k might range from 50 to 1000
   generally we keep the top k dimensions, 

|v|   |v|

but some experiments suggest that 
getting rid of the top 1 dimension or  
even the top 50 dimensions is helpful 
(lapesa and evert 2014).

figure 19.11 svd factors a matrix x into a product of three matrices, w, s, and c. taking
the    rst k dimensions gives a |v|    k matrix wk that has one k-dimensioned row per word that
can be used as an embedding.

using only the top k dimensions (corresponding to the k most important singular
values), leads to a reduced |v|   k matrix wk, with one k-dimensioned row per word.
this row now acts as a dense k-dimensional vector (embedding) representing that
word, substituting for the very high-dimensional rows of the original m.3

embeddings versus sparse vectors

dense svd embeddings sometimes work better than sparse 
ppmi matrices at tasks like word similarity
   denoising: low-order dimensions may represent 

unimportant information

   truncation may help the models generalize better to 

unseen data.

   having a smaller number of dimensions may make it easier 

for classifiers to properly weight the dimensions for the 
task.

   dense models may do better at capturing higher order co-

occurrence. 

