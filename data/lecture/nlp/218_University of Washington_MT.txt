natural language processing

cse 517

winter 2017

machine translation

yejin choi

slides from dan klein, luke zettlemoyer, dan jurafsky, ray mooney

translation: codebreaking?

when i look at an article in russian, i say: 
   this is really written in english, but it has 
been coded in some strange symbols. i will 
now proceed to decode.       

   warren weaver (1955:18, quoting a letter he 

wrote in 1947)

brief history of nlp

   mid 1950   s     mid 1960   s:  birth of nlp and linguistics

   at first, people thought mt would be easy! researchers predicted that 

   machine translation    can be solved in 3 years or so.

   mid 1960   s     mid 1970   s: a dark era

   people started believing that machine translation is impossible.

   1970   s and  early 1980   s     slow revival of nlp

   small toy problems, linguistic heavy, weak empirical evaluation

   late 1980   s and 1990   s     statistical revolution!

   by this time, the computing power increased substantially .
   data-driven, statistical approaches with simple representation.

     whenever i fire a linguist, our mt performance improves.    (jelinek,1988)
   2000   s     statistics powered by linguistic insights

   more complex statistical models & richer linguistic representations.

machine translation: examples

corpus-based mt

modeling correspondences between languages

sentence-aligned parallel corpus:

yo lo har   ma  ana
i will do it tomorrow

hasta pronto
see you soon

hasta pronto
see you around

machine translation system:

yo lo har   pronto

model of 
translation

i will do it soon

i will do it around

see you tomorrow

levels of transfer

   vauquois triangle   

general approaches

   rule-based  approaches

   expert system-like rewrite systems
   interlingua methods (analyze and generate)
   lexicons come from humans
   can be very fast, and can accumulate a lot of knowledge over 

time (e.g. systran)

   statistical approaches

   word-to-word translation
   phrase-based translation
   syntax-based translation (tree-to-tree, tree-to-string)
   trained on parallel corpora
   usually noisy-channel (at least in spirit)

translation is hard!

   zi    zhu     zhong   duan
               (cid:6)         

   self  help terminal device

help oneself terminating machine

(atm,     self-service terminal   )

2

examples from liang huang

translation is hard!

3

examples from liang huang

translation is hard!

3

examples from liang huang

translation is hard!

3

examples from liang huang

translation is hard!

3

examples from liang huang

or even...

4

examples from liang huang

human evaluation

madame la pr  sidente, votre pr  sidence de cette institution a   t   marquante.
mrs fontaine, your presidency of this institution has been outstanding.
madam president, president of this house has been discoveries.
madam president, your presidency of this institution has been impressive.

je vais maintenant m'exprimer bri  vement en irlandais.
i shall now speak briefly in irish .
i will now speak briefly in ireland . 
i will now speak briefly in irish .

nous trouvons en vous un pr  sident tel que nous le souhaitions.
we think that you are the type of president that we want.
we are in you a president as the wanted. 
we are in you a president as we the wanted.

evaluation questions:
    are translations fluent/grammatical?
    are they adequate (you understand the meaning)?

mt: automatic evaluation

   human evaluations: subject measures, 

fluency/adequacy

   automatic measures: id165 match to 

references
   nist measure: id165 recall (worked poorly)
   id7: id165 precision (no one really likes it, but 

everyone uses it)

   id7:

   p1 = unigram precision
   p2, p3, p4 = bi-, tri-, 4-gram precision
   weighted geometric mean of p1-4
   brevity penalty (why?)
   somewhat hard to game   

automatic metrics work (?)

mt system components     id87

language model

source
p(e)

best
e

e

decoder

translation model
channel
p(f|e)

f

observed     

f

argmax p(e|f) = argmax p(f|e)p(e)

e

e

part i     word alignment 

models

word alignment

x

what is the anticipated 
cost of collecting fees 
under the new proposal?

en vertu des nouvelles 
propositions, quel est le 
co  t pr  vu de perception 
des droits?

z

what
is 
the
anticipated
cost
of
collecting 
fees 
under 
the 
new 
proposal
?

en 
vertu 
de
les
nouvelles 
propositions
, 
quel 
est 
le 
co  t 
pr  vu 
de 
perception 
de 
les 
droits
?

word alignment

unsupervised word alignment

   input: a bitext, pairs of translated sentences

nous acceptons votre opinion .
we accept your view .

   output: alignments: pairs of

translated words
   when words have unique
sources, can represent as
a (forward) alignment
function a from french to
english positions

the id6

the  mathematics  of  statistical  machine 
translation:  parameter  estimation 

[brown et al 1993]

peter e  brown* 
ibm t.j. watson research center 
vincent j. della pietra* 
ibm t.j. watson research center 

stephen  a. della pietra* 
ibm t.j. watson research center 
robert  l. mercer* 
ibm t.j. watson research center 

we describe a series o,f five statistical models o,f the translation process and give algorithms,for 
estimating  the parameters o,f these models given a set o,f pairs o,f sentences  that are translations 
o,f one another.  we define a concept o,f word-by-word alignment between such pairs o,f sentences. 
for any  given  pair of such  sentences  each  o,f our  models  assigns  a id203  to each  of the 
possible word-by-word alignments.  we give an algorithm for seeking the most probable o,f these 
alignments. although the algorithm is suboptimal, the alignment thus obtained accounts well for 
the word-by-word  relationships in the pair o,f sentences.  we have a great deal o,f data in french 
and english from  the proceedings  o,f the canadian parliament.  accordingly,  we have restricted 
our work to these two languages; but we,feel that because our algorithms have minimal linguistic 
content  they  would  work well on  other pairs  o,f languages.  we also ,feel, again  because  of the 
minimal  linguistic  content  o,f our algorithms,  that it is reasonable to argue that word-by-word 
alignments are inherent in any sufficiently large bilingual corpus. 
1.  introduction 

ibm model 1 (brown 93)

   peter f. brown, vincent j. della pietra, stephen a. della pietra, 

robert l. mercer

   the mathematics of id151:

parameter estimation. in: computational linguistics 19 (2), 1993. 

   3667 citations.

ibm model 1 (brown 93)
t(f|e) := p(0e0 is translated into 0f0|e)

   model parameters: 
   a (hidden) alignment vector                      where                means 

(a1, ..., am)

ai = j

   i   th target word is translated from    j   th source word. 

   include a    null    word on the source side
   this alignment vector defines 1-to-many mappings. (why?)

null0

p(f1 . . . fm, a1 . . . am|e1 . . . el, m) =

uniform alignment model!

=

myi=1
myi=1

q(ai|i, l, m)t(fi|eai)
1

l + 1

t(fi|eai)

ibm model 1: learning
   if given data with alignment {(e1...el,a1   am,f1...fm)k|k=1..n}

tm l(f|e) =

c(e, f )
c(e)

where

i = j, 0 otherwise

 (k, i, j) = 1 if a(k)

c(e, f ) =xk xi s.t. ei=e xj s.t. fj =f

 (k, i, j)

   in practice, no such data available at large scale.
   thus, learn the translation model parameters while keeping 

alignment as latent variables, using em, 
   repeatedly re-compute the expected counts:

 (k, i, j) =

i

j

t(f (k)

|e(k)
)
pj0 t(f (k)
|e(k)
j0 )

i

   basic idea: compute expected source for each word, update co-

occurrence statistics, repeat

sample em trace for alignment
(ibm model 1 with no null generation)

training
corpus

green house
casa verde

the house
la casa

verde       casa           la
1/3
1/3
1/3
1/3
1/3
1/3

1/3
1/3
1/3

green
house
the

assume uniform
initial probabilities

green house
casa verde
1/3 x 1/3 = 1/9 1/3 x 1/3 = 1/9 1/3 x 1/3 = 1/9 1/3 x 1/3 = 1/9

green house
casa verde

the house
la casa

the house
la casa

9/1
9/2

=

1
2

9/1
9/2

=

1
2

9/1
9/2

=

1
2

9/1
9/2

=

1
2

translation
probabilities

compute
alignment
probabilities
p(a, f | e)
normalize 
to get
p(a | f, e)

example cont.

the house
la casa
1/2

green house
casa verde

1/2

compute 
weighted 
translation 
counts

green
house
the

1/2

green house
casa verde

the house
la casa
1/2
verde       casa           la
1/2
0
1/2
0

1/2
1/2 + 1/2 1/2
1/2
1/2

normalize
rows to sum 
to one to 
estimate p(f | e)

green
house
the

verde       casa           la
1/2
0
1/4
1/4
1/2
0

1/2
1/2
1/2

example cont.

translation
probabilities

verde       casa           la
1/2
0
1/4
1/4
0
1/2

1/2
1/2
1/2

green
house
the

recompute
alignment
probabilities
p(a, f | e)

normalize 
to get
p(a | f, e)

green house
casa verde
1/2 x 1/4=1/8

green house
casa verde
1/2 x 1/2=1/4 1/2 x 1/2=1/4 1/2 x 1/4=1/8

the house
la casa

the house
la casa

8/1
8/3

=

1
3

4/1
8/3

=

2
3

4/1
8/3

=

2
3

8/1
8/3

=

1
3

continue em iterations until translation

parameters converge

ibm model 1 - em intuition

16

em algorithm

... la maison ... la maison blue ... la fleur ...

... the house ... the blue house ... the flower ...

step 1

step 2

step 3

   

... la maison ... la maison id7 ... la fleur ...

january 2013

step n

... la maison ... la maison blue ... la fleur ...

... la maison ... la maison blue ... la fleur ...

... the house ... the blue house ... the flower ...

17

em algorithm

... la maison ... la maison blue ... la fleur ...

    initial step: all alignments equally likely
    model learns that, e.g., la is often aligned with the

... the house ... the blue house ... the flower ...

em algorithm

18

... la maison ... la maison id7 ... la fleur ...

miles osborne

    after one iteration
    alignments, e.g., between la and the are more likely

machine translation

january 2013

19

... the house ... the blue house ... the flower ...

    after one iteration
    alignments, e.g., between la and the are more likely

miles osborne

... la maison ... la maison id7 ... la fleur ...

... the house ... the blue house ... the flower ...

em algorithm

... the house ... the blue house ... the flower ...

miles osborne

... la maison ... la maison id7 ... la fleur ...

    after another iteration
    it becomes apparent that alignments, e.g., between    eur and    ower are more

machine translation

january 2013

likely

    convergence
    inherent hidden structure revealed by em

... the house ... the blue house ... the flower ...

... the house ... the blue house ... the flower ...

    it becomes apparent that alignments, e.g., between    eur and    ower are more

miles osborne

    convergence
    inherent hidden structure revealed by em

machine translation

january 2013

example from philipp koehn

miles osborne

ibm model 1: id136

   model parameters: 
   a (hidden) alignment vector                      where                means 

t(f|e) := p(0e0 is translated into 0f0|e)

(a1, ..., am)

ai = j

   i   th target word is translated from    j   th source word. 

null0

p(f1 . . . fm, a1 . . . am|e1 . . . el, m) =

uniform alignment model!

=

myi=1
myi=1

q(ai|i, l, m)t(fi|eai)
1

l + 1

t(fi|eai)

   id136: find the best alignment a given (f,e) pairs. is this hard?

evaluating alignments

   how do we measure quality of a word-to-word 

model?
   method 1: use in an end-to-end translation system

   hard to measure translation quality
   option: human judges
   option: reference translations (nist, id7)
   option: combinations (hter)
   actually, no one uses word-to-word models alone as tms

   method 2: measure quality of the alignments 

produced
   easy to measure
   hard to know what the gold alignments should be
   often does not correlate well with translation quality (like 

perplexity in lms)

alignment error rate

   alignment error rate

=
=
=  

sure align.
possible  align.
predicted 

align.

   a := predicted alignments
   s := sure alignments
   p := possible alignments   

(including sure alignments)

problems with model 1

   there   s a reason they 
designed models 2-5!

   problems: alignments jump 
around, align everything to 
rare words

   experimental setup:

   training data: 1.1m sentences 

of french-english text, 
canadian hansards

   evaluation metric: alignment 

error rate (aer)

   evaluation data: 447 hand-

aligned sentences

intersected model 1

   post-intersection: standard 
practice to train models in 
each direction then 
intersect their predictions 
[och and ney, 03]

   second model is basically 

a filter on the first
   precision jumps, recall drops
   end up not guessing hard 

alignments

model
model 1 e   f
model 1 f   e
model 1 and

p/r aer
30.6
28.7
34.8

82/58
85/58
96/46

joint training?

      alignment by agreement    (liang et al, 2006)

   similar high precision to post-intersection
   but recall is much higher
   more confident about positing non-null alignments

model
model 1 e   f
model 1 f   e
model 1 and
model 1 int

p/r
82/58
85/58
96/46
93/69

aer
30.6
28.7
34.8
19.5

independent training

nous
ne
avons
pas
cru
bon
de
assister
`a
la
r  eunion
et
en
avons
inform  e
le
cojo
en
cons  equence
.

nous
ne
avons
pas
cru
bon
de
assister
`a
la
r  eunion
et
en
avons
inform  e
le
cojo
en
cons  equence
.

nous
ne
avons
pas
cru
bon
de
assister
`a
la
r  eunion
et
en
avons
inform  e
le
cojo
en
cons  equence
.

e
w

t
i

o
t

d
e
m
e
e
d

e
h
t

o
s

d
n
a

d
n
e
t
t
a

g
n
i
t
e
e
m

d
e
m
r
o
f
n
i

o.
j
o
c

e
l
b
a
s
i
v
d
a
n
i

e
w

t
i

o
t

d
e
m
e
e
d

e
h
t

o
s

d
n
a

d
n
e
t
t
a

g
n
i
t
e
e
m

d
e
m
r
o
f
n
i

o.
j
o
c

e
l
b
a
s
i
v
d
a
n
i

e
w

t
i

o
t

d
e
m
e
e
d

e
h
t

o
s

d
n
a

d
n
e
t
t
a

g
n
i
t
e
e
m

d
e
m
r
o
f
n
i

o.
j
o
c

e
l
b
a
s
i
v
d
a
n
i

e   f: 84.2/92.0/13.0

f   e: 86.9/91.1/11.5

intersection: 97.0/86.9/7.6

nous
ne
avons
pas
cru

nous
ne
avons
pas
cru

nous
ne
avons
pas
cru

e   f: 84.2/92.0/13.0

f   e: 86.9/91.1/11.5
joint training

intersection: 97.0/86.9/7.6

nous
ne
avons
pas
cru
bon
de
assister
`a
la
r  eunion
et
en
avons
inform  e
le
cojo
en
cons  equence
.

nous
ne
avons
pas
cru
bon
de
assister
`a
la
r  eunion
et
en
avons
inform  e
le
cojo
en
cons  equence
.

nous
ne
avons
pas
cru
bon
de
assister
`a
la
r  eunion
et
en
avons
inform  e
le
cojo
en
cons  equence
.

e
w

t
i

o
t

d
e
m
e
e
d

e
h
t

o
s

d
n
a

d
n
e
t
t
a

g
n
i
t
e
e
m

d
e
m
r
o
f
n
i

o.
j
o
c

e
l
b
a
s
i
v
d
a
n
i

e
w

t
i

o
t

d
e
m
e
e
d

e
h
t

o
s

d
n
a

d
n
e
t
t
a

g
n
i
t
e
e
m

d
e
m
r
o
f
n
i

o.
j
o
c

e
l
b
a
s
i
v
d
a
n
i

e
w

t
i

o
t

d
e
m
e
e
d

e
h
t

o
s

d
n
a

d
n
e
t
t
a

g
n
i
t
e
e
m

d
e
m
r
o
f
n
i

o.
j
o
c

e
l
b
a
s
i
v
d
a
n
i

e   f: 89.9/93.6/8.7

f   e: 92.2/93.5/7.3

intersection: 96.5/91.4/5.7

figure 1: an example of the viterbi output of a pair of independently trained id48s (top) and a pair of
jointly trained id48s (bottom), both trained on 1.1 million sentences. rounded boxes denote possible
alignments, square boxes are sure alignments, and solid boxes are model predictions. for each model, the
overall precision/recall/aer on the development set is given. see section 4 for details.

monotonic translation

japan shaken by two new quakes

le japon secou   par deux nouveaux s  ismes 

local order change

japan is at the junction of four tectonic plates

le japon est au confluent de quatre plaques tectoniques

ibm model 2 (brown 93)

   alignments: a hidden vector called an alignment specifies which 

english source is responsible for each french target word.

null0

p(f1 . . . fm, a1 . . . am|e1 . . . el, m) =

myi=1

q(ai|i, l, m)t(fi|eai)

   same decomposition as model 1, but we will use a multi-nomial

distribution for q!

ibm model 2: learning

   given data {(e1...el,a1   am,f1...fm)k|k=1..n}

tm l(f|e) =

c(e, f )
c(e)

qm l(j|i, l, m) =

c(j|i, l, m)
c(i, l, m)

where
 (k, i, j) = 1 if a(k)

i = j, 0 otherwise

c(e, f ) =xk xi s.t. ei=e xj s.t. fj =f

 (k, i, j)

   better approach: re-estimated generative models 

with em, 
   repeatedly compute counts, using redefined deltas:

 (k, i, j) =

i

q(j|i, lk, mk)t(f (k)
|e(k)
pj0 q(j0|i, lk, mk)t(f (k)

j

i

)
|e(k)
j0 )

   basic idea: compute expected source for each 

word, update co-occurrence statistics, repeat

   q: what about id136? is it hard?

example

phrase movement

on tuesday nov. 4, earthquakes rocked japan once again

des tremblements de terre ont    nouveau touch   le japon jeudi 4 novembre. 

the id48 model

1

2
thank you

3
,

4
i

5

6
shall do

8

7
so gladly

9
.

1

3

7

6

8

8

8

8

gracias ,

lo

har   de muy buen grado

9

.

emissions:  p( f1 = gracias | ea1 = thank )

transitions:  p( a2 = 3 | a1 = 1)

model parameters

e:

a:

f:

the id48 model

   model 2 can learn complex alignments
   we want local monotonicity:

   most jumps are small

   id48 model (vogel 96)

   re-estimate using the forward-backward algorithm
   handling nulls requires some care

   what are we still missing?

-2 -1  0  1  2  3

id48 examples

aer for id48s

model
model 1 int
id48 e   f
id48 f   e
id48 and
id48 int
giza m4 and

aer
19.5
11.4
10.8
7.1
4.7
6.9

ibm models 3/4/5

mary did not slap the green witch

mary not slap slap slap the green witch 

n(3|slap)

p(null)

t(la|the)

mary not slap slap slap null the green witch

mary no daba una botefada a la verde bruja

mary no daba una botefada a la bruja verde

d(j|i)

[from al-onaizan and knight, 1998]

overview of alignment models

  

some results

   [och and ney 03]

part ii - phrase translation 

model

phrase-based systems

sentence-aligned 

corpus

word alignments

cat ||| chat ||| 0.9 
the cat ||| le chat ||| 0.8
dog ||| chien ||| 0.8 
house ||| maison ||| 0.6 
my house ||| ma maison ||| 0.9
language ||| langue ||| 0.9 
   

phrase table

(translation model)

phrase translation tables

   defines the space of possible translations

5

   each entry has an associated    id203   

real example

   one learned example, for    den vorschlag    from europarl 
    phrase translations for den vorschlag learned from the europarl corpus:

data 

english
the proposal
   s proposal
a proposal
the idea
this proposal
proposal
of the proposal
the proposals

 (  e|   f)

0.6227
0.1068
0.0341
0.0250
0.0227
0.0205
0.0159
0.0159

english
the suggestions
the proposed
the motion
the idea of
the proposal ,
its proposal
it
...

 (  e|   f)

0.0114
0.0114
0.0091
0.0091
0.0068
0.0068
0.0068
...

   this table is noisy, has errors, and the entries do not necessarily 

miles osborne

match our linguistic intuitions about consistency   .

machine translation

13 february 2012

extracting phrases
   we will use word alignments to find phrases

word alignment

9

a
    
r
a
m

o
n

a
b
a
d

a
n
u

a
d
a
t
e
f
o
b

a
j
u
r
a b

a l

e
d
r
e
v

mary
did
not
slap
the
green
witch

miles osborne

   question: what is the best set of phrases?

machine translation

13 february 2012

learning a phrase translation table

extracting phrases

8

word alignment

9

    task: learn the model from a parallel corpus

   phrase alignment must

   contain at least one alignment edge
   contain all alignments for phrase pair

    word alignment: using ibm models or other method
    extraction of phrase pairs
    scoring phrase pairs

a
    
r
a
m

o
n

a
b
a
d

a
n
u

a
d
a
t
e
f
o
b

a
j
u
r
a b

a l

e
d
r
e
v

10

mary
did
not
slap
the
green
witch

phrase extraction criteria

maria no daba

miles osborne

maria no daba

machine translation

13 february 2012

machine translation
mary

maria no daba

13 february 2012
mary

did
not

slap

did

not

slap

x

mary

did

not

slap

x

consistent

inconsistent

inconsistent

   extract all such phrase pairs!

    phrase alignment has to contain all alignment points for all covered words

phrase pair extraction example

word alignment induced phrases (5)

word alignment induced phrases
word alignment induced phrases
word alignment induced phrases
word alignment induced phrases

maria no daba una
maria no daba una
maria no daba una
maria no daba una
maria no daba una

bofetada
bofetada
bofetada
bofetada
bofetada
a
a
a
a
a

bruja
bruja
bruja
bruja
bruja

la
la
la
la
la

verde
verde
verde
verde
verde

(maria, mary), (no, did not), (slap, 
daba una bofetada), (a la, the), 
(bruja, witch), (verde, green)
(maria no, mary did not), (no daba 
una bofetada, did not slap), (daba 
una bofetada a la, slap the), (bruja 
verde, green witch)
(maria no daba una bofetada, mary 
did not slap), (no daba una bofetada 
a la, did not slap the), (a la bruja 
verde, the green witch)
(maria no daba una bofetada a la, 
mary did not slap the), (daba una 
bofetada a la bruja verde, slap the 
green witch)
(maria no daba una bofetada a la 
bruja verde, mary did not slap the 
green witch)

mary
mary
mary
mary
mary

did
did
did
did
did

not
not
not
not
not

slap
slap
slap
slap
slap

the
the
the
the
the

green
green
green
green
green

witch
witch
witch
witch
witch

(maria, mary), (no, did not), (slap, daba una bofetada), (a la, the), (bruja, witch), (verde, green),
(maria, mary), (no, did not), (slap, daba una bofetada), (a la, the), (bruja, witch), (verde, green),
(maria, mary), (no, did not), (slap, daba una bofetada), (a la, the), (bruja, witch), (verde, green),
(maria, mary), (no, did not), (slap, daba una bofetada), (a la, the), (bruja, witch), (verde, green)
(maria, mary), (no, did not), (slap, daba una bofetada), (a la, the), (bruja, witch), (verde, green),

(maria no, mary did not), (no daba una bofetada, did not slap), (daba una bofetada a la, slap the),
(maria no, mary did not), (no daba una bofetada, did not slap), (daba una bofetada a la, slap the),
(maria no, mary did not), (no daba una bofetada, did not slap), (daba una bofetada a la, slap the),
(maria no, mary did not), (no daba una bofetada, did not slap), (daba una bofetada a la, slap the),
(bruja verde, green witch), (maria no daba una bofetada, mary did not slap),
(bruja verde, green witch)
(bruja verde, green witch), (maria no daba una bofetada, mary did not slap),
(bruja verde, green witch), (maria no daba una bofetada, mary did not slap),
(no daba una bofetada a la, did not slap the), (a la bruja verde, the green witch),
(no daba una bofetada a la, did not slap the), (a la bruja verde, the green witch),
(no daba una bofetada a la, did not slap the), (a la bruja verde, the green witch)
(maria no daba una bofetada a la, mary did not slap the),
(maria no daba una bofetada a la, mary did not slap the), (daba una bofetada a la bruja verde,

phrase size

   phrases do help

   but they don   t need 

to be long

   why should this be?

why not learn phrases w/ em?

em training of the phrase model

    we presented a heuristic set-up to build phrase translation table

(word alignment, phrase extraction, phrase scoring)

    alternative: align phrase pairs directly with em algorithm
    initialization: uniform model, all  (  e,   f) are the same
    expectation step:

    maximization step:

    estimate likelihood of all possible phrase alignments for all sentence pairs
    collect counts for phrase pairs (  e,   f), weighted by alignment id203
    update phrase translation probabilties p(  e,   f)

    however: method easily over   ts

(learns very large phrase pairs, spanning entire sentences)

chapter 5: phrase-based models

25

phrase scoring

g(f, e) = log

c(e, f )
c(e)

g(les chats, cats) = log

c(cats, les chats)

c(cats)

aiment

poisson

les chats

le

frais

.

cats
like
fresh

fish
.

.

   learning weights has 

been tried, several times:
   [marcu and wong, 02]
   [denero et al, 06]
       and others

   seems not to work well, 
for a variety of partially 
understood reasons

   main issue: big chunks 

get all the weight, 
obvious priors don   t help
   though, [denero et al 08]

part iii - decoding

phrase-based translation 

(cid:18)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)7(cid:3)(cid:1)(cid:1)(cid:1)(cid:2)(cid:5)(cid:10)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:16)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:8)(cid:1)(cid:1)(cid:1)(cid:7)(cid:1)(cid:1)(cid:1)(cid:4)(cid:15)(cid:11)(cid:1)(cid:1)(cid:1)(cid:14)(cid:1)(cid:1)(cid:1)(cid:1)(cid:9)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1). 
 

scoring:  try to use phrase pairs that have been frequently observed. 
                try to output a sentence with frequent english word sequences. 

phrase-based translation 

(cid:18)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)7(cid:3)(cid:1)(cid:1)(cid:1)(cid:2)(cid:5)(cid:10)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:16)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:8)(cid:1)(cid:1)(cid:1)(cid:7)(cid:1)(cid:1)(cid:1)(cid:4)(cid:15)(cid:11)(cid:1)(cid:1)(cid:1)(cid:14)(cid:1)(cid:1)(cid:1)(cid:1)(cid:9)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1). 
 

scoring:  try to use phrase pairs that have been frequently observed. 
                try to output a sentence with frequent english word sequences. 

phrase-based translation 
phrase-based translation 

(cid:18)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)7(cid:3)(cid:1)(cid:1)(cid:1)(cid:2)(cid:5)(cid:10)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:16)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:8)(cid:1)(cid:1)(cid:1)(cid:7)(cid:1)(cid:1)(cid:1)(cid:4)(cid:15)(cid:11)(cid:1)(cid:1)(cid:1)(cid:14)(cid:1)(cid:1)(cid:1)(cid:1)(cid:9)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1). 
(cid:18)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)7(cid:3)(cid:1)(cid:1)(cid:1)(cid:2)(cid:5)(cid:10)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:16)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:8)(cid:1)(cid:1)(cid:1)(cid:7)(cid:1)(cid:1)(cid:1)(cid:4)(cid:15)(cid:11)(cid:1)(cid:1)(cid:1)(cid:14)(cid:1)(cid:1)(cid:1)(cid:1)(cid:9)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1). 
 
 

scoring:  try to use phrase pairs that have been frequently observed. 
scoring:  try to use phrase pairs that have been frequently observed. 
                try to output a sentence with frequent english word sequences. 
                try to output a sentence with frequent english word sequences. 

phrase-based translation 

(cid:18)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)7(cid:3)(cid:1)(cid:1)(cid:1)(cid:2)(cid:5)(cid:10)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:16)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:8)(cid:1)(cid:1)(cid:1)(cid:7)(cid:1)(cid:1)(cid:1)(cid:4)(cid:15)(cid:11)(cid:1)(cid:1)(cid:1)(cid:14)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:9)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1). 
 

scoring:  try to use phrase pairs that have been frequently observed. 
                try to output a sentence with frequent english word sequences. 

scoring:

   basic approach, sum up phrase translation scores and a 

language model
   define y = p1p2   pl to be a translation with phrase pairs pi
   define e(y) be the output english sentence in y
   let h() be the log id203 under a tri-gram language model
   let g() be a phrase pair score (from last slide)
   then, the full translation score is:

f (y) = h(e(y)) +

g(pk)

   goal, compute the best translation

lxk=1

y (x) = arg max
y   y(x)

f (y)

the pharaoh decoder

   scores at each step include lm and tm

the pharaoh decoder

space of possible translations
   phrase table constrains possible translations
   output sentence is built left to right

   but source phrases can match any part of sentence
   each source word can only be translated once
   each source word must be translated

scoring:

   in practice, much like for alignment models, also include a 

distortion penalty
   define y = p1p2   pl to be a translation with phrase pairs pi
   let s(pi) be the start position of the foreign phrase  
   let t(pi) be the end position of the foreign phrase  
   define    to be the distortion score (usually negative!)
   then, we can define a score with distortion penalty:

f (y) = h(e(y)) +
f (y) = h(e(y)) +

g(pk) +
g(pk) +

      |t(pk) + 1   s(pk+1)|
      |t(pk) + 1   s(pk+1)|

lxk=1
lxk=1

l 1xk=1
l 1xk=1

   goal, compute the best translation
y (x) = arg max
y   y(x)

f (y)

hypothesis expansion

17
18
19
13

hypothesis expansion
hypothesis expansion
hypothesis expansion
hypothesis expansion

maria
maria
maria
maria

mary
mary
mary
mary

no
no
no
no

not
not
not
not

did not
did not
did not
did not

dio
dio

give
give
give
give

no
no
no
no
did not give
did not give
did not give
did not give

e: witch
e: witch
e: witch
f: -------*-
f: -------*-
f: -------*-
p: .182
p: .182
p: .182

e: mary
e: mary
e: mary
f: *--------
f: *--------
f: *--------
p: .534
p: .534
p: .534

e: 
e: 
e: 
e: 
f: ---------
f: ---------
f: ---------
f: ---------
p: 1
p: 1
p: 1
p: 1

dio una bofetada
dio una bofetada

una
una

bofetada
bofetada

slap
slap
slap
slap

a slap
a slap
a slap
a slap

a
a
a
a

slap
slap
slap
slap

slap
slap
slap
slap

e: ... slap
e: slap
f: *-***----
f: *-***----
p: .043
p: .043

bruja
bruja
bruja

bruja verde

verde
verde
verde

witch
witch
witch
witch

green
green
green
green

green witch
green witch
green witch
green witch

a la

la
la
la

the
the
the
the

a
a
a

to
to
to
to
by
by
by
by

to the
to the
to the
to the

to
to
to
to
the
the
the
the

the witch
the witch
the witch
the witch

e: did not
f: **-------
p: .154

e: slap
f: *****----
p: .015

e: the
f: *******--
p: .004283

e:green witch
f: *********
p: .000271

    start with empty hypothesis
    ... until all foreign words covered
    further hypothesis expansion
    add another hypothesis
    e: no english words
       nd best hypothesis that covers all foreign words
    f: no foreign words covered
    backtrack to read o  translation
    p: score 1

miles osborne
miles osborne
miles osborne
miles osborne

machine translation
machine translation
machine translation
machine translation

16 february 2012
16 february 2012
16 february 2012
16 february 2012

hypothesis explosion!20

hypothesis expansion

maria

mary

no
no

not

did not

dio

give

no
did not give

bofetada

slap

a slap

una

a

slap

a

to
by

slap

e: witch
f: -------*-
p: .182

e: slap
f: *-***----
p: .043

la

the

bruja

verde

witch

green

green witch

to the

to
the

the witch

e: 
f: ---------
p: 1

e: mary
f: *--------
p: .534

e: did not
f: **-------
p: .154

e: slap
f: *****----
p: .015

e: the
f: *******--
p: .004283

e:green witch
f: *********
p: .000271

   q: how much time to find the best translation?

    adding more hypothesis
    explosion of search space

   exponentially many translations, in length of source sentence
   np-hard, just like for word translation models
   so, we will use approximate search techniques!

miles osborne

machine translation

16 february 2012

hypothesis lattices

can recombine if:
    last two english words match
    foreign word coverage vectors match

decoder pseudocode

initialization: set beam q={q0} where q0 is initial state with 
no words translated
for i=0     n-1 
   for each state q   beam(q) and phrase p   ph(q)

[where n in input sentence length]

1. q   =next(q,p)
2. add(q,q   ,q,p)

[compute the new state]
[add the new state to the beam]

notes:
   ph(q): set of phrases that can be added to partial 
translation in state q
   next(q,p): updates the translation in q and records which 
words have been translated from input
   add(q,q   ,q,p): updates beam, q    is added to q if it is in 
the top-n overall highest scoring partial translations

decoder pseudocode

initialization: set beam q={q0} where q0 is initial state with 
no words translated
for i=0     n-1 
   for each state q   beam(q) and phrase p   ph(q)

[where n in input sentence length]

1. q   =next(q,p)
2. add(q,q   ,q,p)

[compute the new state]
[add the new state to the beam]

possible state representations:
   full: q = (e, b,   ), e.g. (   joe did not give,    11000000, 0.092) 

    e is the partial english sentence
    b is a bit vector recorded which source words are 

translated

       is score of translation so far  

decoder pseudocode

initialization: set beam q={q0} where q0 is initial state with 
no words translated
for i=0     n-1 
   for each state q   beam(q) and phrase p   ph(q)

[where n in input sentence length]

1. q   =next(q,p)
2. add(q,q   ,q,p)

[compute the new state]
[add the new state to the beam]

possible state representations:
   full: q = (e, b,   ), e.g. (   joe did not give,    11000000, 0.092) 
   compact: q = (e1, e2, b, r,   ) , 

    e.g. (   not,       give,    11000000, 4, 0.092) 
    e1 and e2 are the last two words of partial translation
   

r is the length of the partial translation

   compact representation is more efficient, but requires back 
pointers to get the final translation

