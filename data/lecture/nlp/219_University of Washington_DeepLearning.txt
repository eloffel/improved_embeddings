cse 447/547: natural language 

processing

deep learning

winter 2018

yejin choi

university of washington

next several slides are from carlos guestrin, luke zettlemoyer

human neurons

    switching time
    ~ 0.001 second

    number of neurons

    1010

    104-5

    connections per neuron

    scene recognition time

    0.1 seconds

    number of cycles per scene recognition?

    100    much parallel computation!

id88 as a neural network

g

this is one neuron:

    input edges x1 ... xn, along with basis
    the sum is represented graphically
    sum passed through an activation function g

sigmoid neuron

g

just change g!
    why would we want to do this?
    notice new output range [0,1]. what was it before?
    look familiar?

ln |h|e m           ln    
optimizing a neuron
ln|h|   m        ln    
ln|h| + ln 1
we train to minimize sum-squared error

 
 x

 

m    

   

g (x) = g(x)(1   g(x))

f (g(x)) = f (g(x))g (x)

ln|h| + ln 1

 

m

       

   l
   wi

=     j

[yj   g(w0 +   i

wixj

i )]

   
   wi

g(w0 +   i

wixj
i )

@
@wi

g(w0 +xi

7

wixj

i ) = xj

i g0(w0 +xi

wixj
i )

solution just depends on g   : derivative of activation function!

sigmoid units: have to differentiate 

 l
 wi

=   j
g(w0 + i

wixj

[yj   g(w0 + i

g
i ) = xj

i

 
 wi

g(w0 + i

wixj

i )]

 
 wi

g(w0 + i

wixj

i ) = xj

 
 wi

i g (w0 + i

g (x) = g(x)(1   g(x))

id88, linear classification, 

boolean functions: xi   {0,1} 

    -0.5 + x1 + x2

    can learn x1     x2?
    can learn x1     x2?
    can learn any conjunction or disjunction?

    -1.5 + x1 + x2

    -0.5 + x1 +     + xn
    (-n+0.5) + x1 +     + xn
    can learn majority?
    (-0.5*n) + x1 +     + xn
    what are we missing? the dreaded xor!, 

g

etc.

going beyond linear classification
solving the xor problem

y = x1 xor x2

= (x1       x2)     (x2      x1)

v1 = (x1       x2) 

= -1.5+2x1-x2

v2 = (x2       x1) 

= -1.5+2x2-x1

y = v1    v2

= -0.5+v1+v2

1

x1

x2

1

v1

-1
-1.5

v2

-1.5
2

-1
2

-0.5

1

1

y

hidden layer

    single unit:

    1-hidden layer:  

    no longer convex function!

example 
data for nn 
with hidden 
layer

learned 
weights for 
hidden layer

why    representation learning   ?
    maxent (multinomial id28):

    nns:

y = softmax(w    f (x, y))

you design the feature vector

y = softmax(w     (u x))
y = softmax(w     (u (n)(... (u (2) (u (1)x))))

feature representations 
are    learned    through 
hidden layers

very deep models in computer 

vision

learning: 

id26

next 10 slides on back propagation are adapted from andrew rosenberg

error id26

    model parameters:

for brevity:

ij , w(2)

jk , w(3)
~    = {w(1)
kl }
~    = {wij, wjk, wkl}

w(1)
ij

w(2)
jk

x0

x1

x2

xp

w(3)
kl

f(x, ~   )

error id26

    model parameters:
    let a and z be the input and output of each 

~    = {wij, wjk, wkl}

node
zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

x0

x1

x2

xp

f(x, ~   )

18

error id26
zj = g(aj)

wij

zj

wjk

zi

     

     

aj

aj =xi

wijzi

    let a and z be the input and output of each 

node

aj =xi

wijzi

wjkzj

ak =xj

al =xk

wklzk

zj = g(aj)

zk = g(ak)

zl = g(al)

zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

    let a and z be the input and output of each 

node

aj =xi

wijzi

wjkzj

ak =xj

al =xk

wklzk

zj = g(aj)

zk = g(ak)

zl = g(al)

zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

r(   ) =

=

=

1
n

1
n

empirical risk function

l(yn   f(xn))
1
2

training: minimize loss
nxn=0
nxn=0
nxn=0

20@yn   g0@xk

wklg0@xj

wjkg xi

(yn   f(xn))2

zk

aj

zj

1

ak

1
n

zi

2

wijxn,i!1a1a1a

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

r(   ) =

=

=

1
n

1
n

empirical risk function

l(yn   f(xn))
1
2

training: minimize loss
nxn=0
nxn=0
nxn=0

20@yn   g0@xk

wklg0@xj

wjkg xi

(yn   f(xn))2

zk

aj

zj

1

ak

1
n

zi

2

wijxn,i!1a1a1a

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

taking partial derivatives   

error id26

optimize last layer weights wkl

ln =

1
2

(yn   f(xn))2

calculus chain rule

@r
@wkl

=

1

n xn     @ln

@al,n     @al,n
@wkl 

zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

error id26

optimize last layer weights wkl

ln =

(yn   f(xn))2

1
2

@r
@wkl

=

@r
@wkl

=

1

@al,n     @al,n
@wkl 
2(yn   g(al,n))2

n xn     @ln
n xn     @ 1

@al,n

1

calculus chain rule

     @al,n
@wkl 

zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

error id26

optimize last layer weights wkl

ln =

(yn   f(xn))2

1
2

@r
@wkl

=

@r
@wkl

=

1

@al,n     @al,n
@wkl 
2(yn   g(al,n))2

n xn     @ln
n xn     @ 1

@al,n

1

calculus chain rule

     @zk,nwkl
@wkl  

zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

error id26

ln =

(yn   f(xn))2

1
2

optimize last layer weights wkl

@r
@wkl

=

1

n xn     @ln

@al,n     @al,n
@wkl 
     @zk,nwkl
@wkl   =

2(yn   g(al,n))2

@al,n

@r
@wkl

=

1

n xn     @ 1

calculus chain rule

1

n xn

[ (yn   zl,n)g0(al,n)] zk,n

aj

zj

wjk

ak

zk

al

zl

wkl

f(x, ~   )

zi

wij

x0

x1

x2

xp

error id26

optimize last layer weights wkl

ln =

(yn   f(xn))2

1
2

@r
@wkl

=

1

n xn     @ 1

@r
@wkl

=

1

n xn     @ln

@al,n     @al,n
@wkl 
@wkl   =
     @zk,nwkl

@al,n

2(yn   g(al,n))2

zi

aj

zj

wij

wjk

=
zk

ak

calculus chain rule

1

n xn
n xn

1

[ (yn   zl,n)g0(al,n)] zk,n

 l,nnzk,n
al
zl

wkl

f(x, ~   )

x0

x1

x2

xp

error id26

repeat for all previous layers

@r
@wkl
@r
@wjk

@r
@wij

=

=

=

1

1

n xn     @ln
n xn     @ln
n xn     @ln

@al,n     @al,n
@wkl  =
@ak,n     @ak,n
@wjk  =
@aj,n     @aj,n
@wij  =

1

aj

zj

zi

1

1

n xn
n xn "xl
n xn "xk

[ (yn   zl,n)g0(al,n)] zk,n =
 l,nwklg0(ak,n)# zj,n =
 k,nwjkg0(aj,n)# zi,n =

1

ak

zk

al

zl

wkl

wij

wjk

x0

x1

x2

xp

1

1

n xn
n xn
n xn

1

 l,nzk,n

 k,nzj,n

 j,nzi,n

f(x, ~   )

backprop recursion

 i

zi

     

wij

zj = g(aj)
zj

wjk

 j

 k zk
     

     

aj

wijzi

aj =xi
n xn     @ln
n xn     @ln

1

1

=

=

@r
@wjk

@r
@wij

@ak,n     @ak,n
@wjk  =
@aj,n     @aj,n
@wij  =

1

n xn "xl
n xn "xk

1

 l,nwklg0(ak,n)# zj,n =
 k,nwjkg0(aj,n)# zi,n =

1

n xn
n xn

1

 k,nzj,n

 j,nzi,n

learning: id119

wt+1

ij

wt+1
jk

wt+1

kl

= wt

ij      

= wt

= wt

jk      
kl      

@r
wij
@r
wkl
@r
wkl

zi

aj

zj

ak

zk

al

zl

wij

wjk

wkl

f(x, ~   )

x0

x1

x2

xp

id26

values

    starts with a forward sweep to compute all the intermediate function 
 j @r
@wij

    through backprop, computes the partial derivatives recursively
    a form of id145

zi

    instead of considering exponentially many paths between a weight w_ij and the final loss 

(risk), store and reuse intermediate results.

    a type of automatic differentiation. (there are other variants e.g., recursive 

differentiation only through forward propagation.
forward 

gradient

id26

primary interface language:
    tensorflow (https://www.tensorflow.org/) 
    python
    torch (http://torch.ch/)  
   
    theano (http://deeplearning.net/software/theano/) 
    python
    cntk (https://github.com/microsoft/cntk)
    c++
   
    c++
    caffe (http://caffe.berkeleyvision.org/) 
    c++

id98 (https://github.com/clab/id98)

lua

forward 

gradient

cross id178 loss (aka log loss, logistic 

    cross id178

    related quantities

p(y) log q(y)

predicted prob

true prob

    id178
    kl divergence (the distance between two distributions p and q)

p(y)log p(y)

loss)

h(p, q) =  xy
h(p) =xy
dkl(p||q) =xy

p(y) log

p(y)
q(y)

    use cross id178 for models that should have more probabilistic 

h(p, q) = ep[ log q] = h(p) + dkl(p||q)

flavor (e.g., language models)

    use mean squared error loss for models that focus on 

correct/incorrect predictions
1
2

mse =

(y   f (x))2

recurrent neural 

networks

recurrent neural networks (id56s)
    each id56 unit computes a new hidden state using the previous 

    each id56 unit (optionally) makes an output using the current hidden 

ht = f (xt, ht 1)
yt = softmax(v ht)

state and a new input 

state

    parameters are shared (tied) across all id56 units (unlike feedforward

ht 2 rd

    hidden states                  are continuous vectors

    can represent very rich information
    possibly the entire history from the beginning

   "

   #

   $

    $

   %

    %

    #

nns) 

    "

recurrent neural networks (id56s)
    generic id56s:

ht = f (xt, ht 1)
yt = softmax(v ht)

    vanilla id56:

ht = tanh(u xt + w ht 1 + b)
yt = softmax(v ht)

   "

   #

   $

    $

   %

    %

    #

    "

recurrent neural networks (id56s)
    generic id56s:
ht = f (xt, ht 1)
    vanilla id56s:
ht = tanh(u xt + w ht 1 + b)
    lstms (id137):

it =  (u (i)xt + w (i)ht 1 + b(i))
ft =  (u (f )xt + w (f )ht 1 + b(f ))
ot =  (u (o)xt + w (o)ht 1 + b(o))
  ct = tanh(u (c)xt + w (c)ht 1 + b(c))
ct = ft   ct 1 + it     ct
ht = ot   tanh(ct)

    #
   #

    $
   $

    $

    #

    %

    "
   "

    "

there are many 
known variations 
to this set of 
equations!

    ( : cell state
   ( : hidden state

    %
   %

many uses of id56s

1. classification (seq to one)

input: a sequence

   
    output: one label (classification)
    example: sentiment classification

ht = f (xt, ht 1)
y = softmax(v hn)

   "

    "

    #

   #

    $

   %
    %

   $

many uses of id56s

2. one to seq

input: one item

   
    output: a sequence
    example: image captioning

ht = f (xt, ht 1)
yt = softmax(v ht)

   $

   #

   "cat                sitting                on                    top                 of    .
    "

   %

   #

   $

   "

many uses of id56s
3. sequence tagging

input: a sequence

   
    output: a sequence (of the same length)
    example: id52, id39
    how about language models?
    yes! id56s can be used as lms!
    id56s make markov assumption: t/f?

ht = f (xt, ht 1)
yt = softmax(v ht)

   "
    "

   "

   #
    #

   #

   $
    $

   $

   %
    %

many uses of id56s
4. language models

input: a sequence of words

   
    output: one next word 
    output: or a sequence of next words
    during training, x_t is the actual word in the training sentence. 
    during testing, x_t is the word predicted from the previous time 

ht = f (xt, ht 1)
yt = softmax(v ht)

step.

    does id56 lms make markov assumption?

    i.e., the next word depends only on the previous n words

   "
    "

   "

   #
    #

   #

   $
    $

   $

   %
    %

many uses of id56s

5. id195 (aka    encoder-decoder   )

input: a sequence

   
    output: a sequence (of different length)
    examples?

ht = f (xt, ht 1)
yt = softmax(v ht)

   %

   )

   +

   *

   *

   )

   %

   "

   #

   $

    $

    #

    "

many uses of id56s

4. id195 (aka    encoder-decoder   )

    conversation and dialogue
    machine translation

figure from http://www.wildml.com/category/conversational-agents/

many uses of id56s

4. id195 (aka    encoder-decoder   )

parsing!
-    grammar as foreign language    (vinyals et al., 2015)

   )

   %

   %

   )

   *

   *

   +

   "

    "

   #

   $

    $

    #

john   has   a  dog 

recurrent neural networks (id56s)
    generic id56s:

ht = f (xt, ht 1)
yt = softmax(v ht)

    vanilla id56:

ht = tanh(u xt + w ht 1 + b)
yt = softmax(v ht)

   "

   #

   $

    $

   %

    %

    #

    "

recurrent neural networks (id56s)
    generic id56s:
ht = f (xt, ht 1)
    vanilla id56s:
ht = tanh(u xt + w ht 1 + b)
    lstms (id137):

it =  (u (i)xt + w (i)ht 1 + b(i))
ft =  (u (f )xt + w (f )ht 1 + b(f ))
ot =  (u (o)xt + w (o)ht 1 + b(o))
  ct = tanh(u (c)xt + w (c)ht 1 + b(c))
ct = ft   ct 1 + it     ct
ht = ot   tanh(ct)

    #
   #

    $
   $

    $

    #

    %

    "
   "

    "

there are many 
known variations 
to this set of 
equations!

    ( : cell state
   ( : hidden state

    %
   %

lstms (long short-term memory 

networks

    (,"
   (,"

    (
   (

figure by christopher olah (colah.github.io)

lstms (long short-term memory 

networks

forget gate: forget the past or not
ft =  (u (f )xt + w (f )ht 1 + b(f ))

sigmoid:
[0,1] 

figure by christopher olah (colah.github.io)

sigmoid:
[0,1] 

tanh:
[-1,1] 

lstms (long short-term memory 

networks

forget gate: forget the past or not
ft =  (u (f )xt + w (f )ht 1 + b(f ))

input gate: use the input or not
it =  (u (i)xt + w (i)ht 1 + b(i))

new cell content (temp):
  ct = tanh(u (c)xt + w (c)ht 1 + b(c))

figure by christopher olah (colah.github.io)

sigmoid:
[0,1] 

tanh:
[-1,1] 

lstms (long short-term memory 

networks

forget gate: forget the past or not
ft =  (u (f )xt + w (f )ht 1 + b(f ))

input gate: use the input or not
it =  (u (i)xt + w (i)ht 1 + b(i))

new cell content (temp):
  ct = tanh(u (c)xt + w (c)ht 1 + b(c))

new cell content: 

- mix old cell with the new temp cell
ct = ft   ct 1 + it     ct

figure by christopher olah (colah.github.io)

lstms (long short-term memory 

networks

output gate: output from the 
new cell or not
ot =  (u (o)xt + w (o)ht 1 + b(o))

hidden state:
ht = ot   tanh(ct)

figure by christopher olah (colah.github.io)

forget gate: forget the past or not
ft =  (u (f )xt + w (f )ht 1 + b(f ))

input gate: use the input or not
it =  (u (i)xt + w (i)ht 1 + b(i))

new cell content (temp):
  ct = tanh(u (c)xt + w (c)ht 1 + b(c))

new cell content: 

- mix old cell with the new temp cell
ct = ft   ct 1 + it     ct

lstms (long short-term memory 

networks

forget gate: forget the past or not
input gate: use the input or not
output gate: output from the new 
cell or not

ft =  (u (f )xt + w (f )ht 1 + b(f ))
it =  (u (i)xt + w (i)ht 1 + b(i))
ot =  (u (o)xt + w (o)ht 1 + b(o))

new cell content (temp):
new cell content: 

- mix old cell with the new temp cell

  ct = tanh(u (c)xt + w (c)ht 1 + b(c))
ct = ft   ct 1 + it     ct

hidden state:
ht = ot   tanh(ct)

    (,"
   (,"

    (
   (

vanishing gradient problem for 

id56s.

    the shading of the nodes in the unfolded network indicates their 

sensitivity to the inputs at time one (the darker the shade, the greater 
the sensitivity). 
    the sensitivity decays over time as new inputs overwrite the activations 

of the hidden layer, and the network    forgets    the first inputs. 

example from graves 2012

preservation of gradient information by 

lstm

output 
gate

forget gate

input gate

    for simplicity, all gates are either entirely open (   o   ) or closed (         ). 
    the memory cell    remembers    the first input as long as the forget gate is 
    the sensitivity of the output layer can be switched on and off by the output 

open and the input gate is closed. 

gate without affecting the cell. 

example from graves 2012

recurrent neural networks (id56s)
    generic id56s:
    vanilla id56s:
    grus (id149):

ht = f (xt, ht 1)
ht = tanh(u xt + w ht 1 + b)

zt =  (u (z)xt + w (z)ht 1 + b(z))
rt =  (u (r)xt + w (r)ht 1 + b(r))
  ht = tanh(u (h)xt + w (h)(rt   ht 1) + b(h))
ht = (1   zt)   ht 1 + zt     ht

   "

   #

   $

    $

    #

    "

    %

z: update gate
r: reset gate

less parameters 
than lstms. 
easier to train for 
comparable 
performance!

   %

id56 learning: backprop through time 

(bptt)

    similar to backprop with non-recurrent nns
    but unlike feedforward (non-recurrent) nns, each unit in 

the computation graph repeats the exact same 
parameters   

    backprop gradients of the parameters of each unit as if 

they are different parameters

    when updating the parameters using the gradients, use 

the average gradients throughout the entire chain of 
units.

   "

   #

   $

    $

   %

    %

    #

    "

gates

    gates contextually control information 

flow

    open/close with sigmoid
    in lstms and grus, they are used to 

(contextually) maintain longer term history

59

bi-directional id56s

    can incorporate context from both directions
    generally improves over uni-directional id56s

60

google id4 (oct 2016)

tree lstms

    are tree lstms more 

expressive than sequence 
lstms?
i.e., recursive vs recurrent

   

    when are tree structures 

necessary for deep 
learning of 
representations? 
jiwei li, minh-thang
luong, dan jurafsky and 
eduard hovy. emnlp, 
2015.

62

id56s

    sometimes, id136 over a tree structure makes more sense 

than sequential structure

    an example of compositionality in ideological bias detection 
(red     conservative, blue     liberal, gray     neutral) in which 
modifier phrases and punctuation cause polarity switches at 
higher levels of the parse tree

example from iyyer et al., 2014

id56s

    nns connected as a tree
    tree structure is fixed a priori
    parameters are shared, similarly as id56s

example from iyyer et al., 2014

neural probabilistic language model  (bengio 2003)

65

neural probabilistic language model  (bengio 2003)

review: a neural probabilistic language model

    each word prediction is 
a separate feed forward 
neural network
    feedforward nnlm is a 

markovian language 
model

    dashed lines show 

optional direct 
connections

optional, direct connection layers,

nndmlp1(x) = [tanh(xw1 + b1), x]w 2 + b2

i w1 2 rdin   dhid, b1 2 r1   dhid;    rst a ne transformation
i w2 2 r(dhid+din)   dout, b2 2 r1   dout; second a ne transformation

66

attention!

encoder     decoder architecture

sequence-to-sequence

the

red

dog

  y1

st
1

  x1

  y2

st
2

  x2

  y3

st
3

  x3

ss
1

x1

ss
2

x2

ss
3

x3

the

red

dog

<s>

diagram borrowed from alex rush

68

trial: hard attention
st
i

    at each step generating the target word 
    compute the best alignment to the source word
    and incorporate the source word to generate the target 

ss
j

word

yt
i = argmaxyo(y, st

i, ss
j)
    contextual hard alignment. how?

zj = tanh([st

i, ss

j]w + b)
j = argmaxjzj

    problem?

69

attention: soft alignments 

    at each step generating the target word 
    compute the attention
to the source sequence
    and incorporate the attention to generate the target 

c

st
i

word

yt
i = argmaxyo(y, st

i, ss
j)

    contextual attention as soft alignment. how?

zj = tanh([st

i, ss

j]w + b)
    = softmax(z)
   jss
j

c =xj

    step-1: compute the attention weights 
    step-2: compute the attention vector as interpolation

ss

70

attention

diagram borrowed from alex rush

71

seq-to-seq with attention

diagram from http://distill.pub/2016/augmented-id56s/

72

seq-to-seq with attention

diagram from http://distill.pub/2016/augmented-id56s/

73

attention function 
parameterization
zj = tanh([st
zj = tanh([st

    feedforward nns

i; ss
i; ss

j]w + b)
i   ss
j; st

j]w + b)

    dot product

    cosine similarity 

    bi-linear models

j

zj =

zj = st

i    ss
st
i    ss
||st
i||||ss
j||
t w ss
zj = st
j
i

j

74

learned attention!

diagram borrowed from alex rush

75

qualitative results

neural image id134 with visual attention

neural image id134 with visual attention

figure 2. attention over time. as the model generates each word, its attention changes to re   ect the relevant parts of the image.    soft   
(top row) vs    hard    (bottom row) attention. (note that both models generated the same captions in this example.)

figure 2. attention over time. as the model generates each word, its attention changes to re   ect the relevant parts of the image.    soft   
(top row) vs    hard    (bottom row) attention. (note that both models generated the same captions in this example.)

figure 3. examples of attending to the correct object (white indicates the attended regions, underlines indicated the corresponding word)

figure 3. examples of attending to the correct object (white indicates the attended regions, underlines indicated the corresponding word)

two variants: a    hard    attention mechanism and a    soft   
attention mechanism. we also show how one advantage of

two variants: a    hard    attention mechanism and a    soft   
attention mechanism. we also show how one advantage of

m. malinowski

2. related work
in this section we provide relevant background on previous

2. related work
in this section we provide relevant background on previous

76

27

bidaf

77

learning: training deep 

networks

vanishing / exploding gradients
    deep networks are hard to train
    gradients go through multiple layers
    the multiplicative effect tends to lead to 

exploding or vanishing gradients

    practical solutions w.r.t. 

    network architecture
    numerical operations

79

vanishing / exploding gradients
    practical solutions w.r.t. network 

architecture
    add skip connections to reduce distance

    residual networks, id199,    

    add gates (and memory cells) to allow longer 

term memory 
    lstms, grus, memory networks,    

80

deep networks

gradients of deep networks

nnlayer (x) = relu(xw1 + b1)

hn

hn 1
. . .

h2

h1

x

i can have similar issues with vanishing gradients.

   l

   hn 1,jn 1

=   

jn

1(hn,jn > 0)wjn 1,jn

   l
   hn,jn

diagram borrowed from alex rush

81

effects of skip connections on gradients

thought experiment: additive skip-connections

    thought experiment: additive skip-connections

exercise

nnsl1(x) =

1
2

relu(xw1 + b1) +

1
2

x

we now have the average of two terms. one with no saturation
hn
condition or multiplicative term.
hn 1
. . .

h3

h2

h1

x

   l

   hn 1,jn 1

= 1
2

(  

jn

1(hn,jn > 0)wjn 1,jn

   l
   hn,jn

) +

1
2

(hn 1,jn 1

   l

   hn,jn 1

)

diagram borrowed from alex rush

82

effects of skip connections on gradients
thought experiment 2: dynamic skip-connections
    thought experiment: dynamic skip-connections

nnsl2(x) = (1   t) relu(xw1 + b1) + tx

t = s(xwt + bt)

w1 2 rdhid   dhid
wt 2 rdhid   1

hn

hn 1
. . .

h3

h2

h1

x

diagram borrowed from alex rush

83

xl to produce its output yl. thus, x1 is the input to the
network and yl is the network   s output. omitting the layer
highway network (srivastava et al., 2015) 
index and biases for clarity,

y = h(x, wh).

y = h(x, wh).

    a plain feedforward neural network:

h is usually an af   ne transform followed by a non-linear
activation function, but in general it may take other forms.
for a highway network, we additionally de   ne two non-
linear transforms t (x, wt) and c(x, wc) such that
linear activation

(1)
    h is a typical affine transformation followed by a non-
h is usually an af   ne transform followed by a non-linear
activation function, but in general it may take other forms.
for a highway network, we additionally de   ne two non-
linear transforms t (x, wt) and c(x, wc) such that

y = h(x, wh)   t (x, wt) + x    c(x, wc).

    highway network:

    t is a    transform gate   
    c is a    carry gate   
    often c = 1     t for simplicity 

we refer to t as the transform gate and c as the carry gate,
since they express how much of the output is produced by
transforming the input and carrying it, respectively. for
simplicity, in this paper we set c = 1   t , giving

y = h(x, wh)   t (x, wt) + x    c(x, wc).

(2)

84

use a plain layer (without highways) to change dimension-
ality and then continue with stacking highway layers. this
is the alternative we use in this study.
convolutional highway layers are constructed similar to
fully connected layers. weight-sharing and local receptive
   elds are utilized for both h and t transforms. we use
zero-padding to ensure that the block state and transform
gate feature maps are the same size as the input.

2.2. training deep id199
for plain deep networks, training with sgd stalls at the
beginning unless a speci   c weight initialization scheme is
used such that the variance of the signals during forward
and backward propagation is preserved initially (glorot &

deep	residual	learning

deep	residual	learning

residual networks
a0 is	any	desired	mapping,
0
hope	the	2	weight	layers	fit	a(0)
b(0)
a0 =b0 +0

weight	layer
relu
weight	layer

    residual net

weight	layer
relu
weight	layer
relu

0
a(0)

relu

    plaint	net

any	two

stacked	layers

identity0

a0 is	any	desired	mapping,
hope	the	2	weight	layers	fit	a(0)
hope the	2	weight	layers	fit	b(0)

    resnet (he et al. 2015): first very deep (152 layers) 
network successfully trained for object recognition

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

85

a0 is	any	desired	mapping,
hope	the	2	weight	layers	fit	a(0)
hope the	2	weight	layers	fit	b(0)

deep	residual	learning

deep	residual	learning

residual networks
a0 is	any	desired	mapping,
0
hope	the	2	weight	layers	fit	a(0)
b(0)
a0 =b0 +0

weight	layer
relu
weight	layer

    residual net

weight	layer
relu
weight	layer
relu

0
a(0)

    plaint	net

any	two

stacked	layers

identity0

relu
    f(x) is a residual mapping with respect to identity
    direct input connection +x leads to a nice property w.r.t. back 
propagation --- more direct influence from the final loss to any 
deep layer
in contrast, lstms & id199 allow for long distance 
input connection only through    gates   .

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

   

86

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

residual networks

revolution	of	depth

vgg,	19	layers
(ilsvrc	2014)

alexnet,	8	layers
(ilsvrc	2012)

11x11	conv,	96,	/4,	pool/2

5x5	conv,	256,	pool/2

3x3	conv,	384

3x3	conv,	384

3x3	conv,	256,	pool/2

fc,	4096

fc,	4096

fc,	1000

googlenet,	22	layers

(ilsvrc	2014)

3x3	conv,	64

3x3	conv,	64,	pool/2

3x3	conv,	128

3x3	conv,	128,	pool/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

fc,	4096

fc,	4096

soft max2

soft maxact ivat ion

fc

averagepool 
7x7+ 1(v)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

soft max1

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

soft maxact ivat ion

maxpool 
3x3+ 2(s)

dept hconcat

fc

fc

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

averagepool 
5x5+ 3(v)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

soft max0

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

soft maxact ivat ion

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

fc

fc

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

averagepool 
5x5+ 3(v)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

maxpool 
3x3+ 2(s)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

maxpool 
3x3+ 2(s)

localrespnorm

conv

3x3+ 1(s)

conv

1x1+ 1(v)

localrespnorm

maxpool 
3x3+ 2(s)

conv

7x7+ 2(s)

87

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

input

fc,	1000

residual networks

revolution	of	depth

alexnet,	8	layers
(ilsvrc	2012)

11x11	conv,	96,	/4,	pool/2

5x5	conv,	256,	pool/2

3x3	conv,	384

3x3	conv,	384

3x3	conv,	256,	pool/2

fc,	4096

fc,	4096

fc,	1000

vgg,	19	layers
(ilsvrc	2014)

3x3	conv,	64

3x3	conv,	64,	pool/2

3x3	conv,	128

3x3	conv,	128,	pool/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

fc,	4096

fc,	4096

fc,	1000

resnet,	152	layers

(ilsvrc	2015)

7x7	conv,	64,	/2,	pool/2

1x1	conv,	64

3x3	conv,	64

1x1	conv,	256

1x1	conv,	64

3x3	conv,	64

1x1	conv,	256

1x1	conv,	64

3x3	conv,	64

1x1	conv,	256

1x1	conv,	128,	/2

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	256,	/2

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	512,	/2

3x3	conv,	512

1x1	conv,	2048

1x1	conv,	512

3x3	conv,	512

1x1	conv,	2048

1x1	conv,	512

3x3	conv,	512

1x1	conv,	2048

ave	pool,	fc	1000

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

88

residual networks

revolution	of	depth

152	layers

28.2

25.8

16.4

11.7

22	layers

19	layers

6.7

7.3

3.57

8	layers

8	layers

shallow

ilsvrc'15	
resnet

ilsvrc'14	
googlenet

ilsvrc'14

vgg

ilsvrc'13

ilsvrc'12	
alexnet

ilsvrc'11

ilsvrc'10

id163	classification	top-5	error	(%)
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

89

xl to produce its output yl. thus, x1 is the input to the
network and yl is the network   s output. omitting the layer
highway network (srivastava et al., 2015) 
index and biases for clarity,

y = h(x, wh).

y = h(x, wh).

    a plain feedforward neural network:

h is usually an af   ne transform followed by a non-linear
activation function, but in general it may take other forms.
for a highway network, we additionally de   ne two non-
linear transforms t (x, wt) and c(x, wc) such that
linear activation

(1)
    h is a typical affine transformation followed by a non-
h is usually an af   ne transform followed by a non-linear
activation function, but in general it may take other forms.
for a highway network, we additionally de   ne two non-
linear transforms t (x, wt) and c(x, wc) such that

y = h(x, wh)   t (x, wt) + x    c(x, wc).

    highway network:

    t is a    transform gate   
    c is a    carry gate   
    often c = 1     t for simplicity 

we refer to t as the transform gate and c as the carry gate,
since they express how much of the output is produced by
transforming the input and carrying it, respectively. for
simplicity, in this paper we set c = 1   t , giving

y = h(x, wh)   t (x, wt) + x    c(x, wc).

(2)

90

use a plain layer (without highways) to change dimension-
ality and then continue with stacking highway layers. this
is the alternative we use in this study.
convolutional highway layers are constructed similar to
fully connected layers. weight-sharing and local receptive
   elds are utilized for both h and t transforms. we use
zero-padding to ensure that the block state and transform
gate feature maps are the same size as the input.

2.2. training deep id199
for plain deep networks, training with sgd stalls at the
beginning unless a speci   c weight initialization scheme is
used such that the variance of the signals during forward
and backward propagation is preserved initially (glorot &

vanishing / exploding gradients

    practical solutions w.r.t. numerical operations

    gradient clipping: bound gradients by a max 

value

    gradient id172: renormalize gradients 

when they are above a fixed norm

    careful initialization, smaller learning rates
    avoid saturating nonlinearities (like tanh, sigmoid)

    relu or hard-tanh instead

91

sigmoid
non-linear functions: sigmoid

logistic sigmoid function:

1

 (x) =

1 + e x

s(t) =
 0(x) =  (x)(1    (x))

1

1 + exp( t)

    often used for gates
    pro: neuron-like, 

differentiable

    con: gradients saturate to 

zero almost everywhere 
except x near zero => 
vanishing gradients

    batch id172 helps

i s((xw1 + b1)i )

92

tanh

other non-linearities: tanh

hyperbolic tangeant:

tanh(x) =

ex   e x
ex + e x

    often used for 

hidden states & cells 
in id56s, lstms

    pro: differentiable, 

often converges 
faster than sigmoid

    con: gradients easily 
saturate to zero => 
vanishing gradients

tanh   (x) = 1   tanh2(x)
tanh(t) =
tanh(x) = 2 (2x)   1

exp(t)   exp( t)
exp(t) + exp( t)

93

other non-linearities: hard tanh

hyperbolic tangeant:
hard tanh

hyperbolic tangeant:

    pro: computationally 

cheaper

    con: saturates to 
zero easily, doesn   t 
differentiate at 1, -1

hardtanh(t) =8>>><>>>:

 1 t <  1
t

 1     t     1
t > 1

hardtanh(t) =8>>><>>>:

1

 1 t <  1
t

1

i intuition: similar to sigmoid, but range between 0 and -1.

94

(sub)gradient rule

i technically relu is non-di   erentiable.

relu

    pro: doesn   t saturate for 

i we informally use subgradients,

relu(x) = max(0, x)

i only an issue at 0, generally for    ties   .

d relu(x)

dx

1

0

x > 0

x < 0

1 or 0

o.w

=8>>><>>>:

x > 0, computationally 
cheaper, induces sparse 
nns

    con: non-differentiable 

at 0

    used widely in deep 

generally,

nn, but not as much in 
id56s

dx

d maxv0 (f (x, v0))

    we informally use 

subgradients:

= f 0(x,   v ) for any   v 2 arg max

v0

f (x, v0)

95

vanishing / exploding gradients

    practical solutions w.r.t. numerical operations

    gradient clipping: bound gradients by a max 

value

    gradient id172: renormalize gradients 

when they are above a fixed norm

    careful initialization, smaller learning rates
    avoid saturating nonlinearities (like tanh, sigmoid)

    relu or hard-tanh instead

    batch id172: add intermediate input 

id172 layers

96

batch id172

97

id173

objective with id173

    id173 by objective term

l(q) =

max{0, 1   (   yc     yc0 )} + l||q||2

n

  

i =1

    modify loss with l1 or l2 norms

    less depth, smaller hidden states, early stopping
    dropout

edges) dropped with a id203 p

    randomly delete parts of network during training
    each node (and its corresponding incoming and outgoing 
    p is higher for internal nodes, lower for input nodes
    the full network is used for testing
    faster training, better results
    vs. id112

10     max{0, 1   (y   x)} + 5     max{0, 1   (x   y )} + 5     ||q||2

98

convergence of backprop

    without non-linearity or hidden layers, learning is 

id76
    id119 reaches global minima

    multilayer neural nets (with nonlinearity) are not 

convex
    id119 gets stuck in local minima
    selecting number of hidden units and layers =  fuzzy 

process

    nns have made a huge comeback in the last few years

    neural nets are back with a new name

    id50
    huge error reduction when trained with lots of data on gpus

supplementary topics 

id193

id193! (vinyals et al. 2015)

    nns with attention: content-based attention to input
    id193: location-based attention to input
    applications: convex haul, delaunay triangulation, traveling 

salesman

102

id193

(a) sequence-to-sequence

(b) ptr-net

figure 1: (a) sequence-to-sequence - an id56 (blue) processes the input sequence to create a code
vector that is used to generate the output sequence (purple) using the id203 chain rule and
another id56. the output dimensionality is    xed by the dimensionality of the problem and it is the

103

id193

attention mechanism vs id193

attention mechanism

ptr-net

softmax normalizes the vector eij to be an output distribution over the dictionary of inputs

diagram borrowed from keon kim

104

copynet (gu et al. 2016)

    i: hello jack, my name is chandralekha
    r: nice to meet you, chandralekha
    i: this new guy doesn   t perform exactly as expected.
    r: what do you mean by    doesn   t perform exactly as 

    conversation

expected?   

    translation

105

copynet (gu et al. 2016)

(b) generate-mode & copy-mode

prob(   jebara   ) = prob(   jebara   , g) + prob(   jebara   , c)

hi     ,     tony  jebara

s1

s2

s3

s4

<eos>   hi     ,     tony

attentive	read

    ...

vocabulary

softmax

s4

h1

h2

h3

h4

h5

h6

h7

h8

hello    ,     my     name   is    tony  jebara   . 
(a) attention-based encoder-decoder (id56search)

(c) state update

dnn

   tony   

!

source

m

embedding 
for    tony   
selective read 
for    tony   

m

figure 1: the overall diagram of copynet. for simplicity, we omit some links for prediction (see
sections 3.2 for more details).

106

decoder: an id56 that reads m and predicts

given the decoder id56 state st at time t to-

figure 1: the overall diagram of copynet. for simplicity, we omit some links for prediction (see

figure 1: the overall diagram of copynet. for simplicity, we omit some links for prediction (see
sections 3.2 for more details).

	|	./=34

where g stands for the generate-mode, and c the
copy mode. the id203 of the two modes are
given respectively by

decoder: an id56 that reads m and predicts
the target sequence. it is similar with the canoni-
cal id56-decoder in (bahdanau et al., 2014), with
however the following important differences

given the decoder id56 state st at time t to-
gether with m, the id203 of generating any
target word yt, is given by the    mixture    of proba-
bilities as follows

given the decoder id56 state st at time t to-
gether with m, the id203 of generating any
target word yt, is given by the    mixture    of proba-
bilities as follows

copynet (gu et al. 2016)
	|	78=34 
'(exp,-./
$   %
%
    key idea: interpolation between generation model & 
unk
p(yt|st, yt 1, ct, m) = p(yt, g|st, yt 1, ct, m)
*z	is	the	id172	term.
exp,-./
	|	78=34,./=34
(4)
p(yt|st, yt 1, ct, m) = p(yt, g|st, yt 1, ct, m)
(4)

'(    exp,678 	
9:
$
'(     exp,678 +
9:

+ p(yt, c|st, yt 1, ct, m)

copy model
    prediction: copynet predicts words based
on a mixed probabilistic model of two modes,
namely the generate-mode and the copy-
mode, where the latter picks words from the
source sequence (see section 3.2);
    state update: the predicted word at time t 1
is used in updating the state at t, but copy-
net uses not only its word-embedding but
also its corresponding location-speci   c hid-
den state in m (if any) (see section 3.3 for

p(yt, g|  )=8>><>>:

e g(yt),

1
z

1
z

0,

+ p(yt, c|st, yt 1, ct, m)
figure 2: the illustration of the decoding proba-
where g stands for the generate-mode, and c the
bility p(yt|  ) as a 4-class classi   er.
copy mode. the id203 of the two modes are
generate-mode:
given respectively by
the same scoring function as
yt 2 v
in the generic id56 encoder-decoder (bahdanau et
yt 2 x \   v
al., 2014) is used, i.e.
yt 62 v [ x
yt 2 x
otherwise

e g(unk)

e g(yt),

1
z

(5)

0,

e g(unk)

zpj:xj =yt e c(xj ),

    reading m: in addition to the attentive read
to m, copynet also has   selective read   
to m, which leads to a powerful hybrid of
content-based addressing and location-based
addressing (see both sections 3.3 and 3.4 for
more discussion).

p(yt, c|  )=( 1
p(yt, c|  )=( 1
where  g(  ) and  c(  ) are score functions for
generate-mode and copy-mode, respectively, and
 c(yt = xj) =     h>j wc    st,
xj 2 x (8)
z is the id172 term shared by the two
modes, z =pv2v[{unk} e g(v) +px2x e c(x).
where wc 2 rdh   ds, and   is a non-linear ac-
where  g(  ) and  c(  ) are score functions for

yt 2 v
 g(yt = vi) = v>i wost,
vi 2 v [ unk (7)
yt 2 x \   v
(5)
where wo 2 r(n +1)   ds and vi is the one-hot in-
yt 62 v [ x
dicator vector for vi.
copy-mode:
the score for    copying    the word
yt 2 x
xj is calculated as
otherwise

zpj:xj =yt e c(xj ),

due to the shared id172 term, the two

1
z

(6)

(6)

107

0

0

p(yt, g|  )=8>><>>:

'(exp	[,-unk]

convolution neural 

network

next several slides borrowed from alex rush

all window for classi   cation

models with sliding windows

    classification/prediction with sliding windows

    e.g., neural language model
idea: use window at each location.

    feature representations with sliding window

    e.g., sequence tagging with crfs or structured id88

[w1 w2 w3 w4 w5] w6 w7 w8

w1 [w2 w3 w4 w5 w6] w7 w8

w1 w2 [w3 w4 w5 w6 w7] w8

...

each maps from window of embeddings to dhid

109

sliding windows w/ convolution
convolution formally
convolution formally
let our input be the embeddings of the full sentence, x 2 rn   d 0
let our input be the embeddings of the full sentence, x 2 rn   d 0

x = [v (w1), v (w2), v (w3), . . . , v (wn)]
x = [v (w1), v (w2), v (w3), . . . , v (wn)]

de   ne a window model as nnwindow : r1   (dwind 0) 7! r1   dhid,
de   ne a window model as nnwindow : r1   (dwind 0) 7! r1   dhid,

nnwindow (xwin) = xwinw1 + b1
nnwindow (xwin) = xwinw1 + b1

the convolution is de   ned as nnconv : rn   d 0 7! r(n dwin+1)   dhid,
the convolution is de   ned as nnconv : rn   d 0 7! r(n dwin+1)   dhid,

nnconv (x) = tanh
nnconv (x) = tanh

2666664
2666664

nnwindow (x1:dwin)
nnwindow (x1:dwin)
nnwindow (x2:dwin+1)
nnwindow (x2:dwin+1)

...

...

nnwindow (xn dwin:n)
nnwindow (xn dwin:n)

3777775
3777775

110

pooling

i unfortunately nnconv : rn   d 0 7! r(n dwin+1)   dhid.
pooling operations
i need to map down to dout for di   erent n

i recall pooling operations.

i pooling    over-time    operations f : rn   m 7! r1   m

1. fmax (x)1,j = maxi xi,j
2. fmin(x)1,j = mini xi,j
3. fmean(x)1,j =   i xi,j /n

f (x) =

2666664

+ + . . .
+ + . . .

...

+ + . . .

3777775

= [

. . .

]

111

putting it together

convolution + pooling

  y = softmax(fmax (nnconv (x))w2 + b2)

i w2 2 rdhid   dout, b2 2 r1   dout
i final linear layer w2 uses learned window features

112

multiple convolutions

multiple convolutions

  y = softmax([f (nn 1

conv (x)), f (nn 2

conv (x)), . . . , f (nn f

conv (x))]w2 + b2)

i concat several convolutions together.

i each nn 1, nn 2, etc uses a di   erent dwin

i allows for di   erent window-sizes (similar to multiple id165s)

113

convolution diagram (kim, 2014)

convolution diagram (kim 2014)

i n = 9, dhid = 4 , dout = 2

i red- dwin = 2, blue- dwin = 3, (ignore back channel)

114

classi   cation results

text classification (kim 2014)

115

alexnet (krizhevsky et al., 2012)

116

discussion points

    strength and challenges of deep learning?

    what do nns think about this?

117

hafez: neural sonnet writer 

(ghazvininejad et al. 2016)

118

neural sonnets

deep convolution network

outrageous channels on the wrong connections,
an empty space without an open layer,
a closet full of black and blue extensions,
connections by the closure operator.

theory

another way to reach the wrong conclusion!
a vision from a total transformation,
created by the great magnetic fusion,
lots of people need an explanation.

119

discussion points

    strength and challenges of deep learning?

    representation learning

    data

hyperparameter tuning!)

    less efforts on feature engineering (at the cost of more 
    in id161: nn learned representation is significantly 
    in nlp: often nn induced representation is concatenated with 

better than human engineered features

additional human engineered features.

    most success from massive amount of clean (expensive) data
    recent surge of data creation type papers (especially ai 
    which significantly limits the domains & applications
    need stronger models for unsupervised & distantly supervised 

challenge type tasks)

approaches

120

discussion points

    strength and challenges of deep learning?

    architecture 

    allows for flexible, expressive, and creative modeling

    easier entry to the field

    recent breakthrough from engineering 
advancements than theoretic advancements
    several nn platforms, code sharing culture

121

neural check list

neural checklist models

(kiddon et al., 2016)

    what can we do with gating & attention?

123

encoder--decoder architecture

chop

the

tomatoes

.

add

<s>

chop

the

tomatoes

.

doesn   t 
address 
changing 
ingredients

garlic tomato salsa

want to update 

ingredient 

information as 
ingredients are 

used

encode title - decode recipe

sausage sandwiches

cut each sandwich in halves.
sandwiches with sandwiches.
sandwiches, sandwiches, sandwiches, 
sandwiches, sandwiches
sandwiches, sandwiches, sandwiches, 
sandwiches, sandwiches, sandwiches, or 
sandwiches or triangles, a griddle, each 
sandwich.
top each with a slice of cheese, tomato, 
and cheese.
top with remaining cheese mixture.
top with remaining cheese.
broil until tops are bubbly and cheese is 
melted, about 5 minutes. 

recipe generation vs machine 

translation

decode

recipe

token

by

token

recipe title

ingredient 1
ingredient 2
ingredient 3
ingredient 4

two input sources

<s>

decode

recipe

token

by

   

    only ~6-10% words align 
between input and output.
the rest must be generated 
from context (and implicit 
knowledge about cooking)
    contextual switch between 
two different input sources

encoder--decoder with attention

chop

the

tomatoes

.

add

<s>

chop

the

tomatoes

.

doesn   t 
address 
changing 
ingredients

garlic tomato salsa

want to update 

ingredient 

information as 
ingredients are 

used

neural checklist model

let   s make salsa!

garlic tomato salsa

tomatoes
onions
garlic
salt

neural checklist model

chop

lm

<s>

hidden state classifier:
non-ingredient 
new ingredient
used ingredient

garlic

tomato salsa

new hidden state

which ingredients 
are still available

neural checklist model

chop

the

tomatoes

.

0.85
0.10
0.04
0.01

non-
ingredient

new
ingredient

<s>

chop

the

tomatoes

   

neural checklist model

dice

the

onions

.

0.00
0.94
0.03
0.01

.

dice

the

onions

   

   

   

neural checklist model

add

to

tomatoes

.

0.94
0.04
0.01
0.01

used
ingredient

.

add

to

tomatoes

   

   

   

   

checklist is probabilistic

add

to

tomatoes

= new ingredient prob. distribution

.

0.90
0.08
0.01
0.01

0.85
1.00
0.04
0.02

used
ingredient

.

add

to

tomatoes

0.85

1.00

0.04

0.02

0.85 1.00

0.04

0.02

hidden state classifier is soft
.
add

tomatoes

to

0.00
0.00
0.50
0.50

0.85
1.00
0.04
0.02

0.90
0.08
0.01
0.01

0.01

0.05

0.94

0.85
1.00
0.04
0.02

.

add

to

tomatoes

0.85

1.00

0.04

0.02

0.85 1.00

0.04

0.02

interpolation

id203 distribution over vocabulary

0.00
0.85
attention model 
0.00
1.00
over available 
0.50
0.04
ingredients
0.02
0.50

0.90
0.85
attention model 
0.08
1.00
over used 
0.01
0.04
ingredients
0.02
0.01

0.01

0.05

0.94

choose ingredient via attention

generates a id203 
distribution over a set of 
embeddings that 
corresponds to how close a 
content vector from
target embedding is to each
language model
available ingredient
embeddings

temperature term

available ingredient embeddings

id12 for other nlp tasks
mt (balasubramanian et al. 13,

bahdanau et al. 14)

sentence summarization (rush et 
al. 15)
machine reading (cheng et al. 
16)
image captioning (xu et al. 15)

attention-generated embeddings

ingredient embeddings

can generate an 
embedding from 

the attention 
probabilities

neural 
recipe 

example #1

title: oven eggplant

eggplant
cheese cottage 
lowfat
chopped onion
bay ground leaf
basil
oregano
tomato sauce
provolone

in a small bowl , combine the cheese , eggplant , 
basil , oregano , tomato sauce and onion . mix well . 
shape mixture into 6 patties , each about 3/4-inch 
thick.
place on baking sheet .
bake at 350 degrees for 30 minutes or until lightly 
browned .
southern living magazine , sometime in 1980 .
typed for you by nancy coleman .

cook eggplant in boiling water , covered , for 10 min .
drain and cut in half lengthwise . scoop out insides 
leaving 1/2 '' shell . mash insides with cottage cheese 
, onion , bay leaf , basil , oregano and tomato sauce .
preheat oven to 350 ^ stuff eggplant halves , place in 
casserole dish and bake covered for 15 min .
add a little water to bottom of pan to keep eggplant 
moist . top with provolone cheese .
bake 5 more min uncovered 1 serving =

