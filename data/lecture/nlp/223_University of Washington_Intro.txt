natural language processing

cse 447 

winter 2018

introduction
yejin choi

slides adapted from dan klein, luke zettlemoyer

what is nlp?

   fundamental goal: deep understand of broad language

   not just string processing or keyword matching

   end systems that we want to build:

   simple: id147, text categorization   
   complex: id103, machine translation, information 

extraction, id31, id53   

   unknown: human-level comprehension (is this just nlp?)

machine translation

   translate text from one language to another
   recombines fragments of example translations
   challenges:

   what fragments?  [learning to translate]
   how to make efficient?  [fast translation  search]
   fluency (second half of this class) vs fidelity (later)

2013 google translate: french

2013 google translate: russian

jeopardy! world champion

us cities: its largest airport 
is named for a world war ii 
hero; its second largest, for 
a world war ii battle.

id13:    things not strings   

information extraction

   from unstructured text to database entries

new york times co. named russell t. lewis, 45, president and general 
manager of its flagship new york times newspaper, responsible for all 
business-side activities. he was executive vice president and deputy 
general manager. he succeeds lance r. primis, who in september was 
named president and chief operating officer of the parent. 

person

russell t. lewis

russell t. lewis

company

post

new york times 
newspaper

new york times 
newspaper

president and general 
manager

executive  vice 
president

state

started

ended

lance r. primis

new york times co.

president and ceo

started

information extraction

new york times co. named russell t. lewis, 45, president and general manager of its 
flagship new york times newspaper, responsible for all business-side activities. he was 
executive vice president and deputy general manager. he succeeds lance r. primis, who in 
september was named president and chief operating officer of the parent. 

person
russell t. lewis

russell t. lewis

lance r. primis

company
new york times 
newspaper
new york times 
newspaper
new york times co.

post
president and general 
manager
executive vice president

state
started

ended

president and ceo

started

sub-problems:

1) id39: finding named entities x and their types t(x) 

persons:    russell t. lewis   ,    lance r. primis   
companies:    new york times newspaper   ,    new york times co.   
2) id36: the relation r(x,y) between named entities x, y

works_for(russell t. lewis, new york times newspaper)

3) coreference resolution: which text spans refer to the same named entity?

{russell t.lewis, he, he} are an equivalence set.

   is this easy or hard? 
   easier if the model exploits the redundancy of information!

id53

   id53:

   more than search
   can be really easy: 

   what   s the capital of 
wyoming?   

   can be harder:    how 

many us states    capitals 
are also their largest 
cities?   

   can be open ended: 
   what are the main 
issues in the global 
warming debate?   

   natural language 

interaction:
   understand  requests  and 

act on them

      make me a reservation 

for two at quinn   s tonight      

human-machine interactions

will this be part of all our home devices? 

    uw sounding board among 3 finalists! 
    final competition in las vegas in nov
    unclear if any team will make the 20 min goal
    how not to win:

    brute force more data, more depth 
    add rl and pray magic will arise

announced at aws re:invent

id103

   automatic id103 (asr)

   audio in, text out
   sota: 0.3% error for digit strings, 5% dictation, 50%+ tv

   speech 
lab   

   text to speech (tts)

   text in, audio out
   sota: totally intelligible (if sometimes unnatural)

analyzing public opinion, making political forecasts

    today: in 2012 election, automatic id31 actually being 

used to complement traditional methods (surveys, focus groups)

    past:    id31    research started in 2002 
    future: computational social science and nlp for digital humanities 

(psychology, communication, literature and more)

    challenge: need statistical models for deeper semantic 

understanding --- subtext, intent, nuanced messages

summarization

   condensing 

documents
   single or 

multiple docs
   extractive or 

synthetic

   aggregative or 

representative

   very context-

dependent!

   an example of 

analysis with 
generation

writer-bots for earthquake & financial reports

some of the formulaic 
news articles are now 
written by computers.

   definitely far from 
   op-ed    
   can we make the 
generation engine 
statistically learned 
rather than 
engineered?

bot or human?

despite an expected dip in profit, analysts are generally optimistic about 

steelcase as it prepares to reports its third-quarter earnings on monday, 
december 22, 2014. the consensus earnings per share estimate is 26 cents per 
share.

the consensus estimate remains unchanged over the past month, but it has 
decreased from three months ago when it was 27 cents. analysts are expecting 
earnings of 85 cents per share for the fiscal year. revenue is projected to be 5% 
above the year-earlier total of $784.8 million  at $826.1 million for the quarter. for 
the year, revenue is projected to come in at $3.11 billion.

the company has seen revenue grow for three quarters straight. the less than 

a percent revenue increase brought the figure up to $786.7 million  in the most 
recent quarter. looking back further, revenue increased 8% in the first quarter 
from the year earlier and 8% in the fourth quarter.

the majority of analysts (100%) rate steelcase as a buy. this compares 

favorably  to the analyst ratings of three similar companies, which average 57% 
buys. both analysts rate steelcase as a buy.

steelcase is a designer, marketer and manufacturer of office furniture. other 
companies in the furniture and fixtures industry with upcoming earnings release 
dates include: hni and knoll.

language and vision

   imagine, for example, a computer that could 
look at an arbitrary scene anything from a sunset 
over a fishing village to grand central station at 
rush hour and produce a verbal description. this is 
a problem of overwhelming difficulty, relying as it 
does on finding solutions to both vision and 
language and then integrating them. i suspect 
that scene analysis will be one of the last cognitive 
tasks to be performed well by computers    
-- david stork (hal   s legacy, 2001) on a. 
rosenfeld   s vision 

what begins to work (e.g., kuznetsova et al. 2014)

the flower was so 
vivid and attractive. 

we sometimes do well: 1 out of 4 times, machine 
captions were preferred over the original flickr captions:

blue flowers are running 
rampant in my garden. 

spring in a white dress. 
blue flowers have no scent. small white 
flowers have no idea what they are. 

scenes around the lake on my bike ride.
this horse walking along the road as we drove by. 

but many challenges remain 

(better examples of when things go awry)

the couch is definitely bigger than it 
looks in this photo. 

incorrect object recognition

yellow ball suspended in water. 

incorrect 
scene 
matching

incorrect 
composition

my cat laying in my duffel bag. 

a high chair in the trees. 

table of content

   definition of nlp
   historical account of nlp

nlp history: pre-statistics

(1) colorless green ideas sleep furiously.
(2) furiously sleep ideas green colorless.

   it is fair to assume that neither sentence (1) nor (2) (nor indeed 

any part of these sentences) had ever occurred in an english 
discourse. hence, in any statistical model for grammaticalness, 
these sentences will be ruled out on identical grounds as equally 
"remote" from english. yet (1), though nonsensical, is 
grammatical, while (2) is not.    (chomsky 1957)

  70s and 80s: more linguistic focus

   emphasis on deeper models, syntax and semantics
   toy domains / manually engineered systems
   weak empirical evaluation

nlp: machine learning and empiricism

   whenever i fire a linguist our system 
performance improves.       jelinek, 1988

   1990s: empirical revolution

   corpus-based methods produce the first widely used tools
   deep linguistic analysis often traded for robust 

approximations

   empirical evaluation is essential

   2000s: richer linguistic representations used in 

statistical approaches, scale to more data!

   2010s: you decide! 

what is nearby nlp?

   computational linguistics

   using computational methods to learn more 

about how language works

   we end up doing this and using it

   cognitive science

   figuring out how the human brain works
   includes the bits that do language
   humans: the only working nlp prototype!

   speech?

   mapping audio signals to text
   traditionally separate from nlp, converging?
   two components: acoustic models and 

language models

   language models in the domain of stat nlp

table of content

   definition of nlp
   historical account of nlp
   unique challenges of nlp

problem: ambiguities

   headlines:

   enraged cow injures farmer with ax
   ban on nude dancing on governor   s desk
   teacher strikes idle kids
   hospitals are sued by 7 foot doctors
   iraqi head seeks arms
   stolen painting found by tree
   kids make nutritious snacks
   local hs dropouts cut in half

   why are these funny?

syntactic analysis

hurricane emily howled toward mexico 's caribbean coast on sunday 

packing 135 mph winds and torrential rain and causing panic in cancun , 

where frightened tourists squeezed into musty shelters .

   sota: ~90% accurate for many languages when given many 

training examples, some progress in analyzing languages given few 
or no examples

semantic ambiguity

at last, a computer that understands  you like your mother.

   direct meanings:

   it understands you like your mother (does) [presumably well]
   it understands (that) you like your mother
   it understands you like (it understands) your mother

   but there are other possibilities, e.g. mother could 

mean:
   a woman who has given birth to a child
   a stringy slimy substance consisting of yeast cells and bacteria; is 

added to cider or wine to produce vinegar

   context matters, e.g. what if previous sentence was:

   wow, amazon predicted that you would need to order a big 

batch of new vinegar brewing ingredients. j

[example from l. lee]

dark ambiguities

   dark ambiguities: most structurally permitted 

analyses are so bad that you can   t get your mind to 
produce them

this analysis corresponds 

to the correct parse of 

   this will panic buyers !    

   unknown words and new usages
   solution: we need mechanisms to focus attention on 

the best ones, probabilistic techniques do this

problem: scale

   people did know that language was ambiguous!

      but they hoped that all interpretations would be    good    ones 

(or ruled out pragmatically)

      they didn   t realize how bad it would be

adj

det

noun

det

noun

plural noun

np

np

pp

np

conj

corpora

   a corpus is a collection of text

   often annotated in some way
   sometimes just lots of text
   balanced vs. uniform corpora

   examples

   newswire collections: 500m+ words
   brown corpus: 1m words of tagged 

   balanced    text

   id32: 1m words of parsed 

wsj

   canadian hansards: 10m+ words of 
aligned french / english sentences
   the web: billions of words of who 

knows what

problem: sparsity
   however: sparsity is always a problem
   new unigram (word), bigram (word pair)

n
e
e
s
n
o

 

i
t
c
a
r
f

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

unigrams

bigrams

0

200000

400000
600000
number of words

800000

1000000

table of content

   definition of nlp
   historical account of nlp
   unique challenges of nlp
   class administrivia

site & crew

   site: https://courses.cs.washington.edu/courses/cse447/18wi/
   canvas: https://canvas.uw.edu/courses/1208727
   crew:
   instructor: 

yejin choi (office hour: mon 4:30     4:30)

   ta: 

luheng he 
phoebe mulcaire
ari holtzman
nelson liu

textbooks and notes

   textbook (recommended but not required):

   jurafsky and martin, speech and language 

processing, 2nd edition 

   manning and schuetze, foundations of statistical 

nlp

   goodfellow, bengio, and courville, "deep 

learning" (free online book available at 
deeplearningbook.org ) 

   lecture slides & notes are required

   see the course website for details

   assumed technical background:

   data structure, algorithms, strong programming 

skills, probabilities, statistics

grading & policy

   grading:

   5 homework (50%)
   in-class quiz (15%)
   final exam (30%)
   course/discussion board participation (5%)

   policy:

   all homework will be completed individually.
   final projects can be done in groups.
   academic honest and plagiarism.

   participation and discussion:

   class participation is expected and 

appreciated!!!

   email is great, but please use the message board 

when possible (we monitor it closely) 

what is this class?

   three aspects to the course:

   linguistic issues

   what are the range of language phenomena?
   what are the knowledge sources that let us disambiguate?
   what representations are appropriate?
   how do you know what to model and what not to model?

   statistical modeling methods

   increasingly complex model structures
   learning and parameter estimation
   efficient id136: id145, search, sampling

   engineering methods

   issues of scale
   where the theory breaks down (and what to do about it)

   we   ll focus on what makes the problems hard, and what works in 

practice   

approximate schedule

i. introduction 
ii. words: language models (lms)
ii. words: unknown words (smoothing) 
iii. sequences: id48 (id48s) 
iii. sequences: id48 (id48s) 
v. trees: probabilistic id18s (pid18) 
v. trees: grammar refinement
v. trees: dependency grammars & mildly context-sensitive grammars

iii. sequences: sequence tagging 
iv. learning (feature-rich models): id148 
iv. learning (structural id114): id49 (crfs) 

1 

2 

3

4 

5 

6  vi. translation: alignment  models & phrase-based mt 

7 

vii. semantics: frame semantics 
vii. semantics: distributed semantics, embeddings

8  viii. deep learning: neural networks 
9  viii. deep learning: more nns 
10  viii. deep learning: yet more nns 

comparisons with other classes

   compared to ml 

   typically multivariate, id145 everywhere
   structural learning & id136
   insights into language matters (a lot!)
   dl: id56s, lstms, seq-to-seq, attention,    

   compared to compling classes

   more focus on core algorithm design, technically more 

demanding in terms of math, algorithms, and programming

   you can take this class either as 447 or as 547

   547 requires roughly 20-25% more work for homework 

assignments

add code / audit

   sorry, the class is currently overbooked (even 

after a major increase)
   447 section shows less number of the actual 

enrollment

   there are additional students enrolled under 547
   higher priority on cse students
   need to make an appeal to the ugrad advisors

   audit     ok if there are sits (still) not taken

class requirements and goals 
   class requirements

   uses a variety of skills / knowledge:

   id203 and statistics
   decent coding skills 
   data structure and algorithms (id145!)
   (optional) basic linguistics background 

   ml/ai helps if you   ve taken either before, but not 

necessary 
   class goals

   learn the fundamental concepts and techniques
   learn current engineering practices
   learn how to advance the field!

