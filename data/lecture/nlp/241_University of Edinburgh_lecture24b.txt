bayesian cognitive science

cognitive science views the brain as an information processor:

    information comes from the senses, language, memory etc.

    information is typically uncertain / noisy.

    we need to reason about the past to help with the present and future.

id203 and id205 is a natural way to think about cogsci.

1

playing tennis

suppose you are playing tennis:

    you know how quickly you can move.

    you have an idea how your partner will server.

    how can you anticipate the best action to take?

2

playing tennis

we can think of the world as being a state:

    a state encodes where the ball will bounce.

we can connect to the world using sensory input:

    sensory input means watching the opponent.

3

playing tennis

putting the two together:

p(state | sensory input) =

p(sensory input | state)p(state)

p(sensory input)

this allows us to combine together what we see with that we believe.

experimental results suggest that people learn the prior and combine with sensory
evidence is a similar manner.

4

eating puffer fish

puffer    sh is a delicacy. but (wikipedia):

(contains) a powerful neurotoxin that can cause death in nearly 60% of
the humans that ingest it. a human only has to ingest a few milligrams of
the toxin for a fatal reaction to occur. once consumed the toxin blocks
the sodium channels in the nervous tissues, ultimately paralyzing the
muscle tissue.

5

eating puffer fish

people like eating puffer fish, yet have to consider the possibility of being
poisoned.

    can we reason about the cost of eating puffer fish against the yummy taste?

    we wish to select some action which has the lowest average cost over all

possible states.

    decision theory allows us to reason about taking optimal actions.

6

eating puffer fish

decision theory connects actions with probabilities:

    l(x,y ) is a id168.

    a id168 characterises the cost of taking action x in state y .

    l(eat, poisoned): the cost of eating bad    sh.
    l(eat, safe): the cost of eating good    sh.

7

eating puffer fish

we need to consider all possible states:

e(action) = (cid:229)

state

l(action,state)p(state | action)

    suppose we believe that the cost of eating bad    sh is 5000

    and believe that the cost of eating safe    sh is 0.

    p(poisoned | eat) = 1

10,000

should we eat the    sh?

8

eating puffer fish

the expected loss of eating    sh is then:

e(eat) = l(eat, poisoned)p(poisoned | eat)+

l(eat, safe)p(safe | eat)
=    0.4999

    if we do nothing, then the loss is zero.

we should therefore eat the    sh.

9

decision theory

    decision theory allows us to reason about the relationship between perceived

costs and uncertainty.

    dt has been applied to neural processing.

10

occam   s razor

one theory of human learning is that we try to    nd simple descriptions:

    the world rests on a tortoise, which swims in an ocean . . .

    the world is a rock in space.

occam   s razor:

all things being equal, the simplest solution tends to be the best one.

how can we formalise simplicity?

11

occam   s razor

id205 considers compressing items:

    if an item x occurs with id203 p(x), then an optimal code will use

l(x) =    logp(x) units to represent it.

    l(x) is the description length of x.

    suppose the letter e has p(e) = 1/5 and z has p(z) = 1/100.
    l(e) = 1.6 units, l(z) = 4.6 units

    the complexity of a theory is then equivalent to the description length of that

theory.

12

occam   s razor

highly likely theories will receive short description lengths.

    an empty theory will have a minimally short description!

    we also need to consider how well the theory accounts for the data (. . . all

things being equal).

    the likelihood is a natural way to talk about the data in terms of a theory:

p(d | m).

    l(p(d | m)) gives us the length of the data encoded in the model.

high likelihoods compactly describe the data.

13

occam   s razor

putting it together:

    select a compact model, which describes the data simply:

l(m) + l(d | m)

    this shows the connection between id47 and simplicity.

    much cogsci can be seen in terms of simplicity:

process

data

code

language learning

linguistic input grammars

low-level preception

sensory input

filters in early vision

ants

paths to food

tactile contact between ants

14

summary

id203 is a rich language for cogsci:

    bayes allows us to talk about combining together existing knowledge with our

current state-of-affairs.

    decision theory allows us to reason about subjective costs and uncertainty.

    id205 lets us talk about simplicity in a formal manner.

final comment: do we actually think using probabilities, or is it just a metaphor?

15

