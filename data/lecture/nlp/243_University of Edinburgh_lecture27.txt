application: discovering collocations
codes

application: discovering collocations
codes

formal modeling in cognitive science

lecture 27: application of mutual information; codes

frank keller

school of informatics
university of edinburgh
keller@inf.ed.ac.uk

march 12, 2006

1 application: discovering collocations

what are collocations?
the naive approach
using mutual information

2 codes

source codes
properties of codes

frank keller

formal modeling in cognitive science

1

frank keller

formal modeling in cognitive science

2

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

discovering collocations

discovering collocations

remember collocations from informatics 1b?

collocations are sequences of words that occur together;

correspond to conventionalized, habitual ways of saying things;

are often highly frequent in the language;

collocations contrast with other expressions that are
near-synonyms, but not conventionalized (strong tea vs.
powerful tea; strong car vs. powerful car );

task: automatically identify collocations in a large corpus.

(1)

average
careless

he spoke english with a/n . . . french accent.
a.
b.
c. widespread
d.
pronounced
chronic
e.

frank keller

formal modeling in cognitive science

3

frank keller

formal modeling in cognitive science

4

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

discovering collocations

discovering collocations

(2)

he gave us a . . . account of all that you had achieved over
there.
a.
b.
c.
d.
e.

ready
yellow
careless
luxury
glowing

(3)

could you please give me a/n . . . account?
a.
b.
c.
d.
e.

itemized
dreadful
great
luxury
glowing

frank keller

formal modeling in cognitive science

5

frank keller

formal modeling in cognitive science

6

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

discovering collocations

discovering collocations

(4)

kim and sandy made . . . after the argument.
a. with
b.
c.
d.
e.

about
o(cid:11)
up
for

why do we care about collocations? in cognitive science:

speakers of a language have strong intuitions about
collocations (see previous slides).

where do these intuitions come from? can collocational
knowledge be learned from exposure? is simple co-occurrence
frequency enough to learn them?

engineering applications:

collocations are di(cid:11)erent for di(cid:11)erent text types: discover
them automatically to create dictionaries;

translation systems have to replace a collocation in the source
language with a valid collocation in the target language.

can we discover collocations in corpora (large collections of text)?

frank keller

formal modeling in cognitive science

7

frank keller

formal modeling in cognitive science

8

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

the naive approach

the naive approach

the simplest way of (cid:12)nding collocations is counting. if two words
occur together a lot, they form a collocation:

go to a corpus;

look for two word combinations (bigrams);

count their frequency;

select most frequent combinations;

assume these are collocations.

c(w1; w2) w1
of
89871
in
58841
26430
to
on
21842
for
21839
and
18568
16121
that
at
15630
15494
to

w2
the
the
the
the
the
the
the
the
be

: : :
11428

: : :

: : :
new york

frank keller

formal modeling in cognitive science

9

frank keller

formal modeling in cognitive science

10

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

pointwise mutual information

pointwise mutual information

as the previous example shows, if two words co-occur a lot in
a corpus, it does not mean that they are collocations;

if we have a set of candidate collocations (e.g., all
co-occurrences of tea), then we can use (cid:31)
informatics 1b);

2 to (cid:12)lter them (see

however, this doesn   t work so well for discovering collocations
from scratch;

instead: use pointwise mutual information;

intuitively, mi tells us how informative the occurrence of one
word is about the occurrence of another word;

words that are highly informative about each other form a
collocation.

i (w1; w2)
18.38
17.98
16.31
15.94
15.19
1.09
1.01
0.53
0.46
0.29

c(w1)
42
41
30
77
24
14907
13484
14734
14093
15019

c(w2)
20
27
117
59
320
9017
10570
13487
14776
15629

c(w1; w2) w1
20
20
20
20
20
20
20
20
20
20

ayatollah
bette
agatha
videocassette
unsalted
(cid:12)rst
over
into
like
time

w2
ruhollah
midler
christie
recorder
butter
made
many
them
people
last

frank keller

formal modeling in cognitive science

11

frank keller

formal modeling in cognitive science

12

application: discovering collocations
codes

what are collocations?
the naive approach
using mutual information

application: discovering collocations
codes

source codes
properties of codes

pointwise mutual information

source codes

example
take an example from the table:

i (x; y ) = log

f (x ; y )
f (x)f (y )

= log

c(x ;y )

n
c(x)

c(y )

n

n

i (unsalted; butter) = log

20

14307668

24

320

14307668

14307668

= 15:19

this means: the amount of information we have about unsalted at
position i increases by 15.19 bits if we are told that butter is at
position i + 1 (i.e., uncertainty is reduced by 15.19 bits).

de(cid:12)nition: source code
a source code c for a random variable x is a mapping from x 2 x
to f0; 1g(cid:3). let c (x) denote the code word for x and l(x) denote
the length of c (x).

here, f0; 1g(cid:3) is the set of all (cid:12)nite binary strings (we will only
consider binary codes).

de(cid:12)nition: expected length

the expected length l(c ) of a source code c (x) for a random
variable with the id203 distribution f (x) is:

l(c ) = x

f (x)l(x)

x 2x

frank keller

formal modeling in cognitive science

13

frank keller

formal modeling in cognitive science

14

application: discovering collocations
codes

source codes
properties of codes

application: discovering collocations
codes

source codes
properties of codes

source codes

properties of codes

example
let x be a random variable with the following distribution and
code word assignment:
x

a
1
2
0

b
1
4
10

c
1
8

d
1
8

110

111

f (x)
c (x)

the expected code length of x is:

l(c ) = x

f (x)l(x) =

x 2x

1
2

(cid:1) 1 +

1
4

(cid:1) 2 +

1
8

(cid:1) 3 +

1
8

(cid:1) 3 = 1:75

de(cid:12)nition: non-singular code
a code is called non-singular if every x 2 x maps into a di(cid:11)erent
string in f0; 1g(cid:3).

if a code is non-singular, then we can transmit a value of x
unambiguously.

however, what happens if we want to transmit several values
of x in a row?

we could use a special symbol to separate the code words.

however, this is not an e(cid:14)cient use of the special symbol;
instead use self-punctuating codes (pre(cid:12)x codes).

frank keller

formal modeling in cognitive science

15

frank keller

formal modeling in cognitive science

16

application: discovering collocations
codes

source codes
properties of codes

application: discovering collocations
codes

source codes
properties of codes

properties of codes

properties of codes

de(cid:12)nition: extension
the extension c (cid:3) of a code c is:

c (cid:3)(x1x2 : : : xn) = c (x1)c (x2) : : : c (xn)

where c (x1)c (x2) : : : c (xn) indicates the concatenation of the
corresponding code words.

de(cid:12)nition: uniquely decodable
a code is called uniquely decodable if its extension is non-singular.

if the code is uniquely decodable, then for each string there is only
one source string that produced it;

however, we have to look at the whole string to do the decoding.

de(cid:12)nition: pre(cid:12)x code
a code is called a pre(cid:12)x code (instantaneous code) if no code word
is a pre(cid:12)x of another code word.

we don   t have to wait for the whole string to be able to decode it;
the end of a code word can be recognized instantaneously.

example
the code in the previous example is a pre(cid:12)x code. take the
following sequence: 01011111010.

the (cid:12)rst symbol, 0, tells us we have an a; the next two symbols
10, have to correspond to b; the next three symbols have to
correspond to a d, etc. the decoded sequence is: abdcb.

frank keller

formal modeling in cognitive science

17

frank keller

formal modeling in cognitive science

18

application: discovering collocations
codes

source codes
properties of codes

application: discovering collocations
codes

source codes
properties of codes

properties of codes

summary

example
the following table illustrates the di(cid:11)erent classes of codes:

singular

non-singular, not uniq. decodable,
uniq. decodable

not instant.

instant.

0
0
0
0

0
010
01
10

10
00
11
110

0
10
110
111

x
a
b
c
d

collocations are sequences of words that occur together;

simple co-occurrence frequency in a corpus is not enough to
discover collocations

instead, use the pointwise mutual information of two words;

a code is uniquely decodable if there is only one possible
source sequence for every code sequence;

a code is instantaneous if each code word has a unique pre(cid:12)x.

frank keller

formal modeling in cognitive science

19

frank keller

formal modeling in cognitive science

20

