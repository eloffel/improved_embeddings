natural language processing

info 159/259   

lecture 2: text classi   cation 1 (aug 29, 2017) 

david bamman, uc berkeley

quizzes

    take place in the    rst 10 minutes of class: 

    start at 3:40, end at 3:50 

    we drop 3 lowest quizzes and homeworks total.  

for q quizzes and h homeworks, we keep (h+q)-3 
highest scores.

a mapping h from input data 
x (drawn from instance 

classi   cation
space     ) to a label (or 
enumerable output space     
     = set of all documents 
     = {english, mandarin, greek,    }

labels) y from some 

x = a single document 

y = ancient greek

classi   cation

h(x) = y 

h(                               ) = ancient grc

classi   cation

let h(x) be the    true    

mapping.  we never know it.  
how do we    nd the best   (x) 

to approximate it?

one option: rule based 

 if x has characters in    

unicode point range 0370-03ff:   

    (x) = greek 

classi   cation

supervised learning

given training data in the 
form of <x, y> pairs, learn 

  (x)

text categorization problems 

task

language id

spam classi   cation

authorship attribution

genre classi   cation

id31

    

    

text

email

text

novel

text

{english, mandarin, greek,    }

{spam, not spam}

{jk rowling, james joyce,    }

{detective, romance, gothic,    }

{postive, negative, neutral, mixed}

id31

    document-level sa: is the entire text positive or 

negative (or both/neither) with respect to an implicit 
target? 

    movie reviews [pang et al. 2002, turney 2002]

training data

positive

       is a    lm which still causes real, not    gurative, 
chills to run along my spine, and it is certainly the 
bravest and most ambitious fruit of coppola's genius   

       i hated this movie. hated hated hated 
hated hated this movie. hated it. hated 
every simpering stupid vacant 
audience-insulting moment of it. hated 
the sensibility that thought anyone 
would like it.   

roger ebert, north

roger ebert, apocalypse now

negative

    implicit signal: star ratings 

    either treat as ordinal 

regression problem ({1, 2, 
3, 4, 5} or binarize the 
labels into {pos, neg}

id31

    is the text positive or 

negative (or both/
neither) with respect 
to an explicit target 
within the text?

hu and liu (2004),    mining and summarizing 

customer reviews   

id31

    political/product 

opinion mining

twitter sentiment    

job approval polls    

o   connor et al (2010),    from tweets to polls: linking text 

sentiment to public opinion time series   

sentiment as tone

    no longer the speaker   s attitude with respect to some 
particular target, but rather the positive/negative tone 
that is evinced.

sentiment as tone

   once upon a time and a very good time it was there was a moocow coming 
down along the road and this moocow that was coming down along the road 

met a nicens little boy named baby tuckoo   "

http://www.matthewjockers.net/2014/06/05/a-novel-method-for-detecting-plot/

sentiment dictionaries

    mpqa subjectivity lexicon 

(wilson et al. 2005)   
http://mpqa.cs.pitt.edu/
lexicons/subj_lexicon/ 

    liwc (linguistic inquiry 

and word count, 
pennebaker 2015)

pos

unlimited
prudent
supurb
closeness
impeccably
fast-paced

treat

destined
blessing
steadfastly

neg
lag

contortions

fright
lonely

tenuously
plebeian
morti   cation

outrage
allegations
disoriented

why is sa hard?

    sentiment is a measure of a speaker   s private state, 

which is unobservable. 

    sometimes words are a good indicator of sentence 

(love, amazing, hate, terrible); many times it 
requires deep world + contextual knowledge

   valentine   s day is being marketed as a date movie. i think it   s more 
of a first-date movie. if your date likes it, do not date that person 
again. and if you like it, there may not be a second date.    

                                                                                                        roger ebert, valentine   s day

classi   cation

supervised learning

given training data in the 
form of <x, y> pairs, learn 

  (x)

x

loved it!

terrible movie
not too shabby

y

positive
negative
positive

  (x)

    the classi   cation function that we want to learn has 

two different components: 

    the formal structure of the learning method 
(what   s the relationship between the input and 
output?)     naive bayes, id28, 
convolutional neural network, etc. 

    the representation of the data

representation for sa

    only positive/negative words in mpqa 

    only words in isolation (bag of words) 

    conjunctions of words (sequential, skip ngrams, 

other non-linear combinations) 

    higher-order linguistic structure (e.g., syntax)

       is a    lm which still causes real, not    gurative, 
chills to run along my spine, and it is certainly the 
bravest and most ambitious fruit of coppola's genius   

roger ebert, apocalypse now

   i hated this movie. hated hated hated 
hated hated this movie. hated it. hated 
every simpering stupid vacant audience-
insulting moment of it. hated the 
sensibility that thought anyone would like 
it.   

roger ebert, north

bag of 
words

representation of text 
only as the counts of 
words that it contains

apocalypse    

now
1

0

0

1

1

0

0

north

1

0

9

0

0

1

1

the

of

hate

genius

bravest

stupid

like

   

naive bayes

    given access to <x,y> pairs in training data, we can 
train a model to estimate the class probabilities for a 
new review. 

    with a bag of words representation (in which each 

word is independent of the other), we can use naive 
bayes 

    probabilistic model; not as accurate as other models 

(see next two classes) but fast to train and the 
foundation for many other probabilistic techniques.

random variable

    a variable that can take values within a    xed set 

(discrete) or within some range (continuous).

x 2 {1, 2, 3, 4, 5, 6}

x 2 {the, a, dog, cat, runs, to, store}

p (x = x)

id203 that the random variable x takes 
the value x (e.g., 1)

x 2 {1, 2, 3, 4, 5, 6}

two conditions: 
1. between 0 and 1: 
2. sum of all probabilities = 1

0     p (x = x)     1
p (x = x) = 1

xx

fair dice

fair

x 2 {1, 2, 3, 4, 5, 6}

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0
.
0

1

2

3

4

5

6

weighted dice

not fair

x 2 {1, 2, 3, 4, 5, 6}

5
.
0

4
.
0

3
.
0

2
.
0

1

.

0

0

.

0

1

2

3

4

5

6

id136
x 2 {1, 2, 3, 4, 5, 6}

we want to infer the id203 distribution 

that generated the data we see.

5
.
0

4
.
0

3

.

0

2

.

0

1

.

0

0

.

0

fair

not fair

?

5
.
0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

2

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

6

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

2id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

6

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

26id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

266id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

6

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

2661id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

3

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

26616id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

6

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

266163id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

6

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

2661636id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

3

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

26616366id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

6

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

266163663id203

fair

not fair

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

?

5

.

0

4

.

0

3

.

0

2

.

0

1
.
0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

1

15,625

2661636636independence

    two random variables are independent if:

    in general:

p(a, b) = p(a)   p(b)

p(x1, . . . , xn) =

p(xi)

n i=1

    information about one random variable (b) gives no 

information about the value of another (a)

p(a) = p(a | b)

p(b) = p(b | a)

data likelihood

fair

5

.

0

4

.

0

3

.

0

p(               |                )

0

1

0

2

.

.

0

.

0

5
.

0

1

2

3

4

5

6

not fair

p(               |                )

2
.

.
0

4
.

0

3

0

1

.
0

0
.
0

1

2

3

4

5

6

=.17 x .17 x .17    
= 0.004913

= .1 x .5 x .5    
= 0.025

266266data likelihood

    the likelihood gives us a way of discriminating 

between possible alternative parameters, but also 
a strategy for picking a single best* parameter 
among all possibilities

word choice as weighted 

dice

0.04
0.03
0.02
0.01
0

the

of

hate

like

stupid

unigram id203

positive reviews

negative reviews

0.04
0.03
0.02
0.01
0

0.04
0.03
0.02
0.01
0

the

of

hate

like

stupid

the

of

hate

like

stupid

p (x = the) =

#the

#total words

maximum likelihood 

estimate

    this is a maximum likelihood estimate for p(x); the 
parameter values for which the data we observe (x) 
is most likely.

maximum likelihood 

estimate

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

1

2

3

4

5

6

2661636636  1

  2

  3

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

5
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

1

2

3

4

5

6

p(x |   1) = 0.0000311040 

p(x |   2) = 0.0000000992   

(313x less likely)

p(x |   3) = 0.0000031250   

(10x less likely)

2661636636id155

p (x = x|y = y)

    id203 that one random variable takes a 
particular value given the fact that a different 
variable takes another

p (xi = hate | y =  )

id31

   really really the worst movie ever   

independence assumption

really really the worst movie ever

x1

x2

x3

x4

x5

x6

p(really, really, the, worst, movie, ever) = 
p(really) x p(really) x p(the)     p(ever)

independence assumption

really really the worst movie ever

x1

x2

x3

x4

x5

x6

we will assume the features are independent:

p(x1, x2, x3, x4, x6, x7 | c) = p(x1 | c)p(x2 | c) . . . p(x7 | c)

p(xi...xn | c) =

p(xi | c)

n i=1

a simple classi   er

really really the worst movie ever

y=positive

p(x=really | y=   )

p(x=really | y=   )

p(x=the | y=   )

p(x=worst | y=   )

p(x=movie | y=   )

p(x=ever | y=   )

0.0010

0.0010

0.0551

0.0001

0.0032

0.0005

y=negative

p(x=really | y=   )

p(x=really | y=   )

p(x=the | y=   )

p(x=worst | y=   )

p(x=movie | y=   )

p(x=ever | y=   )

0.0012

0.0012

0.0518

0.0004

0.0045

0.0005

a simple classi   er

really really the worst movie ever

p(x =    really really the worst movie ever    | y =    ) 

p(x=really | y=   ) x p(x=really | y=   ) x p(x=the | y=   ) x p(x=worst | 
y=   ) x p(x=movie | y=   ) x p(x=ever | y=   )   
 = 6.00e-18
p(x =    really really the worst movie ever    | y =    ) 

p(x=really | y=   ) x p(x=really | y=   ) x p(x=the | y=   ) x p(x=worst | 
y=   ) x p(x=movie | y=   ) x p(x=ever | y=   )   
= 6.20e-17

aside: use logs

    multiplying lots of small probabilities (all are under 
1) can lead to numerical under   ow (converging to 
0)

log i

xi = i

log xi

a simple classi   er

    the classi   er we just speci   ed is a maximum likelihood 

classi   er, where we compare the likelihood of the data under 
each class and choose the class with the highest likelihood

likelihood: id203 of data 
(here, under class y)

prior id203 of class y

p(x = xi . . . xn | y = y)
p(y = y)

bayes    rule

prior belief that y = y   

(before you see any data)

likelihood of the data    

given that y=y

p (y = y|x = x) =

p (y = y)p (x = x|y = y)

py p (y = y)p (x = x|y = y)

posterior belief that y=y given that x=x

bayes    rule

prior belief that y = positive   
(before you see any data)

likelihood of    really really the 

worst movie ever      
given that y= positive

p (y = y|x = x) =

p (y = y)p (x = x|y = y)

py p (y = y)p (x = x|y = y)

posterior belief that y=positive given that   
x=   really really the worst movie ever   

this sum ranges over  
y=positive + y=negative   
(so that it sums to 1)

likelihood: id203 of data 
(here, under class y)

prior id203 of class y

p(x = xi . . . xn | y = y)
p(y = y)

posterior belief in the id203 
of class y after seeing data

p(y = y | x = xi . . . xn)

naive bayes classi   er

p (y =  )p (x =    really . . .     | y =  ) + p (y =  )p (x =    really . . .     | y =  )

p (y =  )p (x =    really . . .     | y =  )

let   s say p(y=   ) = p(y=   ) = 0.5 
(i.e., both are equally likely a priori)

0.5   (6.00   10 18)

0.5   (6.00   10 18) + 0.5   (6.2   10 17)

p (y =   | x =    really . . .    ) = 0.088
p (y =   | x =    really . . .    ) = 0.912

naive bayes classi   er

    to turn probabilities into a classi   cation decisions, 
we just select the label with the highest posterior 
id203

  y = arg max
y y

p (y | x)

p (y =   | x =    really . . .    ) = 0.088
p (y =   | x =    really . . .    ) = 0.912

taxicab problem

   a cab was involved in a hit and run accident at night. two cab companies, 
the green and the blue, operate in the city.  you are given the following 
data: 

    85% of the cabs in the city are green and 15% are blue. 

    a witness identi   ed the cab as blue. the court tested the reliability of 
the witness under the same circumstances that existed on the night of 
the accident and concluded that the witness correctly identi   ed each 
one of the two colors 80% of the time and failed 20% of the time. 

what is the id203 that the cab involved in the accident was blue rather 
than green knowing that this witness identi   ed it as blue?    

 

 

 

 

 

 

 

 

 

 

(tversky & kahneman 1981)

prior belief

    now let   s assume that there are 1000 times more positive reviews 

than negative reviews. 

    p(y= negative) = 0.000999 
    p(y = positive) = 0.999001

0.999001   (6.00   10 18)

0.999001   (6.00   10 18) + 0.000999   (6.2   10p ( 17)

p (y =   | x =    really . . .    ) = 0.990
p (y =   | x =    really . . .    ) = 0.010

priors

    priors can be informed (re   ecting expert 

knowledge) but in practice, but priors in naive 
bayes are often simply estimated from training data

p (y =  ) =

# 

#total texts

smoothing

    maximum likelihood estimates can fail miserably 

when features are never observed with a particular 
class.

6

.

0

5

.

0

4
.
0

3

.

0

2
.
0

1
.
0

0

.

0

1

2

3

4

5

6

what   s the id203 of:

246smoothing

    one solution: add a little id203 mass to every 

element.

maximum likelihood 

estimate

p(xi | y) =

ni,y
ny

smoothed estimates

p(xi | y) =

ni,y +   
ny + v  

same    for all xi

ni,y = count of word i in class y 

ny = number of words in y 

v = size of vocabulary

p(xi | y) =

ni,y +   i

ny + v

j=1   j

possibly different    for each xi

smoothing

id113

smoothing with    =1 

6
.
0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

6
.
0

5
.
0

4
.
0

3
.
0

2

.

0

1

.

0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

naive bayes training

training a naive bayes classi   er consists of estimating 
these two quantities from training data for all classes y

p (y = y|x = x) =

p (y = y)p (x = x|y = y)

py p (y = y)p (x = x|y = y)

at test time, use those estimated probabilities to 
calculate the posterior id203 of each class y 
and select the class with the highest id203

    naive bayes    
independence assumption 
can be killer 

    one instance of hate 

makes seeing others much 
more likely (each mention 
does contribute the same 
amount of information) 

    we can mitigate this by not 

reasoning over counts of 
tokens but by their 
presence absence

the

of

hate

genius

bravest

stupid

like

   

apocalypse    

now
1

0

0

1

1

0

0

north

1

0

9 1

0

0

1

1

multinomial naive bayes

discrete distribution for modeling count data (e.g., word 

counts; single parameter   

   =

the
3
531

4

.

0

2

.

0

0

.

0

a
1
209

the

a

dog

cat

runs

to

store

dog
0
13

cat
1
8

runs
0
2

to
2
331

store

0
1

multinomial naive bayes

maximum likelihood parameter estimate

    i =

ni
n

count n

  

the
531
0.48

a
209
0.19

dog
13
0.01

cat
8
0.01

runs
2
0.00

to
331
0.30

store

1
0.00

bernoulli naive bayes

    binary event (true or false; {0, 1}) 
    one parameter: p (id203 of 

an event occurring)

p(x = 1 | p) = p
p(x = 0 | p) = 1   p

examples: 
    id203 of a particular feature being true    

(e.g., review contains    hate   )
1
n

  pid113 =

xi

n i=1

bernoulli naive bayes

x1

x2

x3

x4

x5

x6

x7

x8

pid113

f1

f2

f3

f4

f5

1

0

1

1

0

0

0

1

0

0

0

0

1

0

0

0

0

1

1

0

1

0

1

1

0

1

0

0

0

0

0

1

0

0

0

0

0

1

1

0

0.375

0.125

0.750

0.500

0.000

bernoulli naive bayes

positive
x3
x2

negative
x7
x6

x4

x5

0

0
1

0

0

0

0
1

0

0

0

0
1

1

0

1

0
1

1

0

1

0
0

0

0

0

1
0

0

0

x1

1

0
1

1

0

f1

f2
f3

f4

f5

x8

pid113,p pid113,n

0

0
1

1

0

0.25

0.00
1.00

0.50

0.00

0.50

0.25
0.50

0.50

0.00

tricks for sa

    negation in bag of words: add negation marker to 

all words between negation and end of clause 
(e.g., comma, period) to create new vocab term [das 
and chen 2001] 
    i do not [like this movie] 

    i do not like_neg this_neg movie_neg

sentiment dictionaries

    mpqa subjectivity lexicon 

(wilson et al. 2005)   
http://mpqa.cs.pitt.edu/
lexicons/subj_lexicon/ 

    liwc (linguistic inquiry 

and word count, 
pennebaker 2015)

pos

unlimited
prudent
supurb
closeness
impeccably
fast-paced

treat

destined
blessing
steadfastly

neg
lag

contortions

fright
lonely

tenuously
plebeian
morti   cation

outrage
allegations
disoriented

homework 1: due 9/4

annotate the sentiment by the writer toward the 

people and organizations mentioned

