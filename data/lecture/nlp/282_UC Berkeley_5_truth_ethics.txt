natural language processing

info 159/259   

lecture 5: truth and ethics (sept 7, 2017) 

david bamman, uc berkeley

hw  t! w   g  rde   

na in g  ardagum,     odcyninga    

  rym gefr  non, h            elingas ellen 
fremedon. oft scyld sc     ng scea  ena 

natural language processing

info 159/259   

lecture 5: truth and ethics (sept 7, 2017) 

david bamman, uc berkeley

i

hated

it

i

really

hated

it

x1

x2

x3

x4

x5

x6

x7

convolutional 

networks

convolutional window size

x

w

1

1

1

x1 x2 x3

w1 w2 w3

size of vocab

size of vocab

h1=f(i, hated, it)

h1

h2

h2=f(it, i, really)

h3
h3=f(really, hated, it)

h1 =  (x1w1 + x2w2 + x3w3)

h2 =  (x3w1 + x4w2 + x5w3)
h3 =  (x5w1 + x6w2 + x7w3)

convolutional 

networks

10

this de   nes one    lter.

x1

x2

x3

x4

x5

x6

1

10

2

-1

5

x7
convolution

max pooling

modern nlp is driven by 

annotated data

    id32 (1993; 1995;1999); morphosyntactic 

annotations of wsj 

    ontonotes (2007   2013); syntax, predicate-argument 

structure, word sense, coreference 

    framenet (1998   ): frame-semantic lexica/annotations 

    mpqa (2005): opinion/sentiment 

    squad (2016): annotated questions + spans of answers in 

wikipedia

modern nlp is driven by 

annotated data

    in most cases, the data we have is the product of 

human judgments. 

    what   s the correct part of speech tag? 

    syntactic structure? 

    sentiment?

ambiguity

   one morning i shot    

an elephant in my pajamas   

animal crackers

dogmatism

fast and horvitz (2016), 
   identifying dogmatism in social 
media: signals and models   

sarcasm

https://www.nytimes.com/2016/08/12/opinion/an-even-stranger-donald-trump.html?ref=opinion

fake news

http://www.fakenewschallenge.org

annotation 
pipeline

pustejovsky and stubbs (2012),   

natural language annotation for machine learning

homework 1

mohammad 2016

annotation 
pipeline

pustejovsky and stubbs (2012),   

natural language annotation for machine learning

annotation guidelines

    our goal: given the constraints of our problem, how 
can we formalize our description of the annotation 
process to encourage multiple annotators to 
provide the same judgment?

annotation guidelines

    what is the goal of the project? 

    what is each tag called and how is it used? (be 

speci   c: provide examples, and discuss gray areas.) 

    what parts of the text do you want annotated, and 

what should be left alone? 

    how will the annotation be created? (for example, 
explain which tags or documents to annotate    rst, 
how to use the annotation tools, etc.)

pustejovsky and stubbs (2012), natural language annotation for machine learning

practicalities

    annotation takes time, concentration (can   t do it 8 

hours a day) 

    annotators get better as they annotate (earlier 

annotations not as good as later ones)

why not do it yourself?

    expensive/time-consuming 

    multiple people provide a measure of 

consistency: is the task well enough de   ned? 

    low agreement = not enough training, guidelines 

not well enough de   ned, task is bad

adjudication

    adjudication is the process of deciding on a single 

annotation for a piece of text, using information 
about the independent annotations. 

    can be as time-consuming (or more so) as a 

primary annotation. 

    does not need to be identical with a primary 
annotation (both annotators can be wrong by 
chance)

adjudicate!

http://bit.ly/sentimentannotation

    what   s your judgment for the correct entity + 

sentiment annotation? 

    how would you amend the annotation guidelines to 

solicit more consistent annotations?

interannotator agreement

b

t

t

 
r
o
a
o
n
n
a

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

6

2

3

5

observed agreement = 11/16 = 68.75%

https://twitter.com/teenybiscuit/status/705232709220769792/photo/1

cohen   s kappa

    if classes are imbalanced, we can get high inter 

annotator agreement simply by chance

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

7

8

4

81

b

 
r
o

t
a
t
o
n
n
a

cohen   s kappa

    if classes are imbalanced, we can get high inter 

annotator agreement simply by chance

  =

po   pe
1   pe

  =

0.88   pe
1   pe

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

7

8

4

81

b

 
r
o

t
a
t
o
n
n
a

cohen   s kappa

    expected id203 of agreement is how often we would 

expect two annotators to agree assuming independent 
annotations

pe = p (a = puppy, b = puppy) + p (a = chicken, b = chicken)

= p (a = puppy)p (b = puppy) + p (a = chicken)p (b = chicken)

cohen   s kappa

= p (a = puppy)p (b = puppy) + p (a = chicken)p (b = chicken)

p(a=puppy)
p(b=puppy)
p(a=chicken)
p(b=chicken)

15/100 = 0.15
11/100 = 0.11
85/100 = 0.85
89/100 = 0.89

= 0.15   0.11 + 0.85   0.89
= 0.773

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

7

8

4

81

b

 
r
o

t
a
t
o
n
n
a

cohen   s kappa

    if classes are imbalanced, we can get high inter 

annotator agreement simply by chance

  =

  =

  =

po   pe
1   pe
0.88   pe
1   pe
0.88   0.773
1   0.773

= 0.471

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

7

8

4

81

b

 
r
o

t
a
t
o
n
n
a

cohen   s kappa

       good    values are subject to interpretation, but rule of thumb:

0.80-1.00

0.60-0.80

0.40-0.60

0.20-0.40

< 0.20

very good agreement

good agreement

moderate agreement

fair agreement

poor agreement

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

0

0

0

100

b

t

t

 
r
o
a
o
n
n
a

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

50

0

0

50

b

t

t

 
r
o
a
o
n
n
a

annotator a

puppy

fried 
chicken

puppy

fried 
chicken

0

50

50

0

b

t

t

 
r
o
a
o
n
n
a

interannotator agreement

    cohen   s kappa can be used for any number of 

classes. 

    still requires two annotators who evaluate the same 

items. 

    fleiss    kappa generalizes to multiple annotators, 
each of whom may evaluate different items (e.g., 
id104)

fleiss    kappa

    same fundamental idea of 

measuring the observed 
agreement compared to 
the agreement we would 
expect by chance. 

    with n > 2, we calculate 
agreement among pairs 
of annotators

  =

po   pe
1   pe

fleiss    kappa

number of annotators who assign category j to item i

nij

for item i with n annotations, how 
many annotators agree, among all 

n(n-1) possible pairs 

pi =

1

n(n   1)

k j=1

nij(nij   1)

fleiss    kappa

for item i with n annotations, how 
many annotators agree, among all 

n(n-1) possible pairs 

pi =

1

n(n   1)

k j=1

nij(nij   1)

annotator
c
b
+
+

d
-

a
+

label

+
-

nij
3
1

agreeing pairs   
 of annotators    

a-b 
b-a 
a-c 
c-a 
b-c 
c-b

pi =

1

4(3)

(3(2) + 1(0))

fleiss    kappa

average agreement among all items

po =

1
n

id203 of category j

pj =

expected agreement by chance     
joint id203 two raters pick the 
same label is the product of their 
independent probabilities of picking 

that label

pe =

1
n n

pi

n i=1
n i=1
k j=1

nij

p2
j

annotator bias correction

    dawid, a. p. and skene, a. m. "id113 of observer error-rates 

using the em algorithm," journal of the royal statistical society, 28(1):20   28, 1979. 

    weibe et al. (1999), "development and use of a gold-standard data set for subjectivity 

classi   cations," acl (for sentiment) 

    carpenter (2010), "multilevel bayesian models of categorical data annotation" 

    rion snow, brendan o'connor, daniel jurafsky and andrew y. ng. cheap and fast - but 
is it good? evaluating non-expert annotations for natural language tasks. emnlp 2008 

    sheng et al. (2008), "get another label? improving data quality and data mining using 

multiple, noisy labelers", kdd. 

    raykar et al. (2009), "supervised learning from multiple experts: whom to trust when 

everyone lies a bit," icml 

    hovy et al. (2013), "learning whom to trust with mace," naacl

annotator bias correction

positive
negative
mixed
unknown

t

h
u
r
t

annotator label
negative mixed
0.03
0.10
0.50
0.10

0
0.80
0.05
0.10

positive
0.95
0
0.20
0.15

p (label | truth)

unknown

0.02
0.10
0.25
0.70

confusion matrix for a single annotator (david)

annotator bias 

correction

basic idea: the true label is 

unobserved; what we observe are 
noisy judgments by annotators

annotator confusion matrix 

p(label | truth)

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

annotator bias 

correction

dawid and skene 1979

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

truth

labels

l

i

ethics

why does a discussion about ethics need to be a 

part of nlp?

conversational agents

id53

http://searchengineland.com/according-google-barack-obama-king-united-states-209733

id38

vector semantics

    the decisions we make about our methods     

training data, algorithm, evaluation     are often tied 
up with its use and impact in the world.

scope

nsubj

dobj

det

prep

pobj

det

i saw the man with the telescope

prep

    nlp often operates on text divorced from the 

context in which it is uttered. 

    it   s now being used more and more to reason 

about human behavior.

privacy

interventions

exclusion

    focus on data from one domain/demographic  

    state-of-the-art models perform worse for young 

(hovy and s  gaard 2015) and minorities (blodgett 
et al. 2016)

exclusion

language identi   cation

id33

blodgett et al. (2016), "demographic dialectal variation in social media: a case study 

of african-american english" (emnlp)

overgeneralization

    managing and communicating the uncertainty of 

our predictions 

    is a false answer worse than no answer?  

dual use

    authorship attribution (author of federalist papers 

vs. author of ransom note vs. author of political 
dissent) 

    fake review detection vs. fake review generation 

    censorship evasion vs. enabling more robust 

censorship

homework 2

    derive the updates for a id98 and implement the 

functions for forward/backward pass 

    out tomorrow, due sept 21 

    be sure to check piazza for any updates

