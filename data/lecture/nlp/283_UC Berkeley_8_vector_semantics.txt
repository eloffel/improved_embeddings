natural language processing

david packard, a concordance to livy (1968)

info 159/259   

lecture 8: vector semantics (sept 19, 2017) 

david bamman, uc berkeley

announcements

    homework 2 party today 5-7pm: 202 south hall 

    db of   ce hours on monday 10/25 10-noon (no 

of   ce hours this friday) 

    no quiz 10/3 or 10/5

http://dlabctawg.github.io
356 barrows hall (d-lab)   

wed 3-5pm

recurrent neural network

    id56 allow arbitarily-sized conditioning contexts; 

condition on the entire sequence history.

from last time

recurrent neural network

goldberg 2017

from last time

recurrent neural network

    each time step has two inputs: 
    xi (the observation at time 

step i); one-hot vector, 
feature vector or 
distributed representation. 

    si-1 (the output of the 

previous state); base case: 
s0 = 0 vector

from last time

training id56s

    given this de   nition of an id56:

si = r(xi, si 1) = g(si 1w s + xiw x + b)
yi = o(si) = (cid:98)(cid:81)(cid:55)(cid:105)(cid:75)(cid:28)(cid:116)(siw o + bo)

    we have    ve sets of parameters to learn: 

w s, w x, w o, b, bo

from last time

lexical semantics

   you shall know a word by the company it keeps      

                                            [firth 1957]

everyone likes

______________

a bottle of

______________

is on the table

______________

makes you drunk

a cocktail with

______________

and seltzer

context

   you shall know a word by the company it keeps      

                                            [firth 1957]

    a few different ways we can encode the notion of 

   company    (or context).

context

everyone likes

______________

a bottle of

______________

is on the table

______________

makes you drunk

a cocktail with

______________

and seltzer

distributed representation

    vector representation that encodes information 

about the distribution of contexts a word appears in 

    words that appear in similar contexts have similar 

representations (and similar meanings, by the 
distributional hypothesis).

term-document matrix

haid113t

macbeth

romeo   
& juliet

richard iii

julius    
caesar

tempest

othello

king lear

knife

dog

sword

love

like

1

2

17

64

75

1

2

38

4

6

7

135

34

2

6

12

63

36

2

2

2

12

41

2

12

17

48

44

27

34

context = appearing in the same document.

vector

vector 
representation of 
the document; 
vector size = v

haid113t

king lear

1

2

17

64

75

2

12

17

48

44

vectors

knife

1

sword

17

1

2

4

7

2

12

2

2

2

17

vector representation of the 
term; vector size = number 
of documents

weighting dimensions

    not all dimensions are equally informative

tf-idf

    term frequency-inverse document frequency 

    a scaling to represent a feature as function of how 
frequently it appears in a data point but accounting 
for its frequency in the overall collection 

    idf for a given term = the number of documents in 
collection / number of documents that contain term

tf-idf

    term frequency (tft,d) = the number of times term t 

occurs in document d 

    inverse document frequency = inverse fraction of 
number of documents containing (dt) among total 
number of documents n

tf idf (t, d) = tft,d   log

n
dt

haid113t macbet

h
1

knife

dog

1

2

sword 17

2

love

like

64

75

38

idf

romeo   
& juliet

4

6

7

135

34

julius    
caesar

richard 

iii
2

tempes

t
2

othello

6

12

63

36

34

2

2

12

41

27

king 
lear
2

12

17

48

44

idf

0.12

0.20

0.12

0.20

0

idf for the informativeness of the terms when 

comparing documents

pmi

    mutual information provides a measure of how 

independent two variables (x and y) are. 

    pointwise mutual information measures the 

independence of two outcomes (x and y)

pmi

log2

p (x, y)

p (x)p (y)

log2

p (w, c)

p (w)p (c)

w = word, c = context

what   s this value for w and c 
that never occur together?

p p m i = max log2

p (w, c)

p (w)p (c)

, 0 

haid113t macbet

h
1

knife

dog

1

2

sword 17

2

love

like

64

75

total

159

38

41

romeo   
& juliet

4

6

7

135

34

richard 

iii
2

6

12

63

36

186

119

julius    
caesar tempest othello

2

2

2

12

41

59

34

34

27

27

king 
lear
2

12

17

48

44

123

total

12

28

57

322

329

748

p m i(love, r&j) =

135
748

186

748   322

748

term-term matrix

    rows and columns are both words; cell counts = 
the number of times word wi and wj show up in the 
same document. 

    more common to de   ne document = some smaller 

context (e.g., a window of 5 tokens)

term-document matrix

haid113t

macbeth

romeo   
& juliet

richard iii

julius    
caesar

tempest

othello

king lear

knife

dog

sword

love

like

1

2

17

64

75

1

2

38

4

6

7

135

34

2

6

12

63

36

2

2

2

12

41

2

12

17

48

44

27

34

term-term matrix

knife

dog

sword

love

like

knife

dog

sword

love

like

6

5

6

5

5

5

5

5

5

5

6

5

6

5

5

5

5

5

5

5

5

5

5

5

8

term-term matrix

jurafsky and martin 2017

write a book 
write a poem

    first-order co-occurrence (syntagmatic 

association): write co-occurs with book in the same 
sentence. 

    second-order co-occurrence (paradigmatic 

association): book co-occurs with poem (since 
each co-occur with write)

syntactic context

lin 1998; levy and goldberg 2014

cosine similarity

cos(x, y) =

i=1 xiyi

 f
i  f

i=1 x2

  f

i=1 y2
i

    we can calculate the cosine similarity of two vectors 
to judge the degree of their similarity [salton 1971] 

    euclidean distance measures the magnitude of 

distance between two points 

    cosine similarity measures their orientation

intrinsic evaluation

    relatedness: 

correlation 
(spearman/pearson) 
between vector 
similarity of pair of 
words and human 
judgments

word 1

midday

journey

car

   

word 2

noon

voyage

automobile

   

professor cucumber

king

cabbage

human 
score
9.29

9.29

8.94

   

0.31

0.23

wordsim-353 (finkelstein et al. 2002)

intrinsic evaluation
    analogical reasoning (mikolov et al. 2013).  for analogy    
germany : berlin :: france : ???,    nd closest vector to    
v(   berlin   ) - v(   germany   ) + v(   france   )

possibly
generating

think

baltimore
shrinking
rabat

impossibly
generated
thinking
maryland
shrank
morocco

certain
shrinking

look

oakland
slowing
astana

target
uncertain
shrank
looking
california
slowed

kazakhstan

sparse vectors

   aardvark   

v-dimensional vector, single 1 for 

the identity of the element

a
a
aa
aal
aalii
aam
aani

aardvark
aardwolf

...

zymotoxic
zymurgy
zyrenian
zyrian
zyryan
zythem
zythia
zythum
zyzomys
zyzzogeton

0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0

dense 
vectors

1

   

0.7

1.3

-4.5

singular value decomposition
    any n   p matrix x  can be decomposed into the 

product of three matrices (where m = the number of 
linearly independent rows)

9

4

3

1

2

   

   

7

9

8

1

n x m

m x m 
(diagonal)

m x p

singular value decomposition

    we can approximate the full matrix by only 

considering the leftmost k terms in the diagonal matrix 

9

4

0

0

0

   

   

0

0

0

0

n x m

m x m 
(diagonal)

m x p

singular value decomposition

    we can approximate the full matrix by only considering 
the leftmost k terms in the diagonal matrix  (the k largest 
singular values)

9

4

0

0

0

   

   

0

0

0

0

n x m

m x m

m x p

1

haid113t macbeth romeo   
& juliet
4
1
6
2
7
17
135
64
34
75

38

2

richard 

iii
2
6
12
63
36

julius    
caesar

tempest othello

2
2
2
12
41

34

27

king 
lear
2
12
17
48
44

haid113

t

macbet

h

romeo   
& juliet

richar
d iii

julius    
caesar

tempe

st

othello

king 
lear

knife
dog
sword
love
like

knife

dog

sword

love

like

low-dimensional 
representation for 
terms (here 2-dim)

low-dimensional 
representation for 

documents (here 2-dim)

knife

dog

sword

love

like

haid113

t

macbet

h

romeo   
& juliet

richar
d iii

julius    
caesar

tempe

st

othello

king 
lear

latent semantic analysis

    latent semantic analysis/indexing (deerwester et 

al. 1998) is this process of applying svd to the 
term-document co-occurence matrix 

    terms typically weighted by tf-idf 

    this is a form of id84 (for terms, 

from a d-dimensionsal sparse vector to a k-
dimensional dense one), k << d.

dense vectors from 

prediction

    learning low-dimensional representations of words 

by framing a predicting task: using context to predict 
words in a surrounding window 

    transform this into a supervised prediction problem; 

similar to id38 but we   re ignoring 
order within the context window

dense vectors from 

prediction

a cocktail with gin 

and seltzer

x

a

cocktail

with

and

seltzer

y

gin

gin

gin

gin

gin

window size = 3

id84

   
the
a
an
for
in
on
dog
cat
   

   
1
0
0
0
0
0
0
0
   

the

4.1

-0.9

the is a point in v-dimensional space

the is a point in 2-dimensional space

gin

cocktail

globe

gin

cocktail
globe

x1

x2

x3

x

0

1

0

w

v

h1

h2

y

y

y

gin

cocktail

globe

w

-0.5

1.3

0.4 0.08

1.7

3.1

v

0.7

1.3

4.1

-0.9

0.1

0.3

y

1

0

0

w

v

gin

cocktail

globe

x1

x2

x3

h1

h2

y

y

y

gin

cocktail

globe

only one of the 
inputs is nonzero.

= the inputs are 

really wtable

w

-0.5

1.3

0.4 0.08

1.7

3.1

v

0.7

1.3

4.1

-0.9

0.1

0.3

x

1

w

0.56
0.07
1.19
1.38
-1.46
-1.24
-0.26
-0.85
0.47
-1.21
0.00
-2.52
-0.14
0.01
-1.76
-0.56
-0.74
1.03
-0.84
-0.18

0.13
-1.75
0.80
-0.11
-0.62
-1.16
0.99
-1.46
0.79
0.06
-0.31
-1.01
-1.50
-0.14
-0.13
-1.08
-0.17
0.31
-0.24
-0.79

x w =

-1.01

-2.52

this is the 

embedding of the 

context

id27s

    can you predict the output word from a vector 

representation of the input word? 

    rather than seeing the input as a one-hot encoded 
vector specifying the word in the vocabulary we   re 
conditioning on, we can see it as indexing into the 
appropriate row in the weight matrix w

id27s

    similarly, v has one h-dimensional vector for each element 

in the vocabulary (for the words that are being predicted)

v

cocktail

0.7
1.3

cat
0.1
0.3

gin
4.1
-0.9

globe
1.3
-3.4

this is the 

embedding of the 

word

y

cat

dog

puppy

x

wrench

screwdriver

    why this behavior? dog, cat show up in similar 

positions

the
the
the
the
the

black
black
black
black
black

cat
dog
puppy
skunk
shoe

jumped
jumped
jumped
jumped
jumped

on
on
on
on
on

the
the
the
the
the

table
table
table
table
table

49

    why this behavior? dog, cat show up in similar 

positions

the
the
the
the
the

black
black
black
black
black

[0.4, 0.08]
[0.4, 0.07]

puppy
skunk
shoe

jumped
jumped
jumped
jumped
jumped

on
on
on
on
on

the
the
the
the
the

table
table
table
table
table

to make the same predictions, these numbers need to be close to each other.

id84

   
the
a
an
for
in
on
dog
cat
   

   
1
0
0
0
0
0
0
0
   

the

4.1

-0.9

the is a point in v-dimensional space; 

representations for all words are completely independent

the is a point in 2-dimensional space 
representations are now structured

analogical id136

    mikolov et al. 2013 show that vector representations 
have some potential for analogical reasoning through 
vector arithmetic.

apple - apples     car - cars
king - man + woman     queen

mikolov et al., (2013),    linguistic regularities in continuous space word representations    (naacl)

low-dimensional distributed 

representations

    low-dimensional, dense word representations are 

extraordinarily powerful (and are arguably 
responsible for much of gains that neural network 
models have in nlp). 

    lets your representation of the input share 

statistical strength with words that behave similarly 
in terms of their distributional properties (often 
synonyms or words that belong to the same class).

54

two kinds of    training    data

    the labeled data for a speci   c task (e.g., labeled 
sentiment for movie reviews): ~ 2k labels/reviews, 
~1.5m words     used to train a supervised model 
    general text (wikipedia, the web, books, etc.), ~ 
trillions of words     used to train word distributed 
representations

55

using dense vectors

    in neural models (id98s, id56s, lm), replace the v-
dimensional sparse vector with the much smaller k-
dimensional dense one. 

    can also take the derivative of the id168 

with respect to those representations to optimize for 
a particular task.

56

zhang and wallace 2016,    a sensitivity 
analysis of (and practitioners    guide to) 

convolutional neural networks for 

sentence classi   cation   

using dense vectors

    (short) document-level representation: coordinate-

wise max, min or average; use directly in neural 
network [joulin et al. 2016] 

    id116 id91 on vectors into distinct 

partitions (though beware of strange geometry [mimno 
and thompson 2017])

58

emoji2vec

eisner et al. (2016),    emoji2vec: learning emoji representations from their description   

59

node2vec

grover and leskovec (2016),    node2vec: scalable id171 for networks   

60

trained embeddings

    id97   

https://code.google.com/archive/p/id97/ 

    glove   

http://nlp.stanford.edu/projects/glove/ 

    levy/goldberg dependency embeddings   
https://levyomer.wordpress.com/2014/04/25/
dependency-based-word-embeddings/

61

