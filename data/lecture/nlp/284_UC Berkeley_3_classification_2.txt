natural language processing

info 159/259   

lecture 3: text classi   cation 2 (aug 31, 2017) 

david bamman, uc berkeley

generative vs. 

discriminative models

    generative models specify a joint distribution over the labels 

and the data.  with this you could generate new data

p(x, y) = p(y) p(x | y)

    discriminative models specify the conditional distribution of 
the label y given the data x.  these models focus on how to 
discriminate between the classes 

p(y | x)

generating

p(x | y =  )

a

amazing bad

best

good

like

love movie

not

of

sword

the

worst

p(x | y =  )

a

amazing bad

best

good

like

love movie

not

of

sword

the

worst

0.06

0.04

0.02

0.00

0.06

0.04

0.02

0.00

generation

taking allen pete visual an lust be in   nite corn physical here 
decidedly 1 for . never it against perfect the possible 
spanish of supporting this all this this pride turn that sure the 
a purpose in real . environment there's trek right . scattered 
wonder dvd three criticism his .
positive

us are i do tense kevin fall shoot to on want in ( . minutes not 
problems unusually his seems enjoy that : vu scenes rest 
half in outside famous was with lines chance survivors good 
to . but of modern-day a changed rent that to in attack lot 
minutes 

negative

generative models

    with generative models (e.g., naive bayes), we ultimately 
also care about p(y | x), but we get there by modeling more.

posterior

p(y = y | x) =

prior

likelihood

p(y = y)p(x | y = y)
 y y

p(y = y)p(x | y = y)

    discriminative models focus on modeling p(y | x)     and only 

p(y | x)     directly.

remember
f i=1
f i=1

xi = xi   x2   . . .   xf

xi  i = x1  1 + x2  2 + . . . + xf  f

exp(x) = ex   2.7x
log(x) = y   ey = x

exp(x + y) = exp(x) exp(y)

log(xy) = log(x) + log(y)

6

a mapping h from input data 
x (drawn from instance 

classi   cation
space     ) to a label (or 
enumerable output space     
     = set of all documents 
     = {english, mandarin, greek,    }

labels) y from some 

x = a single document 

y = ancient greek

training data

positive

       is a    lm which still causes real, not    gurative, 
chills to run along my spine, and it is certainly the 
bravest and most ambitious fruit of coppola's genius   

       i hated this movie. hated hated hated 
hated hated this movie. hated it. hated 
every simpering stupid vacant 
audience-insulting moment of it. hated 
the sensibility that thought anyone 
would like it.   

roger ebert, north

roger ebert, apocalypse now

negative

id28

p(y = 1 | x,   ) =

1

1 + exp   f

i=1 xi  i 

output space

y = {0, 1}

x = feature vector

   = coef   cients

feature

the
and
bravest
love
loved
genius
not
fruit
bias

value

0
0
0
0
0
0
0
1
1

feature

the
and
bravest
love
loved
genius
not
fruit
bias

  

0.01
0.03
1.4
3.1
1.2
0.5
-3.0
-0.8
-0.1

10

  

x1
x2
x3

bias
-0.1

love
3.1

loved
1.2

bias

love

loved

a=   xi  i

exp(-a) 1/(1+exp(-a))

1
1
1

1
1
0

0
1
0

3
4.2
-0.1

0.05
0.015
1.11

95.2%
98.5%
47.4%

11

features

    as a discriminative classi   er, logistic 
regression doesn   t assume features 
are independent like naive bayes 
does. 

    its power partly comes in the ability 
to create richly expressive features 
with out the burden of independence. 

    we can represent text through 

features that are not just the identities 
of individual words, but any feature 
that is scoped over the entirety of the 
input.

features

contains like

has word that shows up in 

positive sentiment 

dictionary

review begins with    i like   

at least 5 mentions of 
positive affectual verbs 

(like, love, etc.)

12

features

feature classes

unigrams (   like   )

bigrams (   not like   ), higher 

order ngrams

pre   xes (words that start with 

   un-   )

has word that shows up in 
positive sentiment dictionary

13

features

feature

the
and
bravest
love
loved
genius
not
fruit
bias

value

0
0
0
0
0
0
1
0
1

feature

like

not like

did not like

in_pos_dict_mpqa
in_neg_dict_mpqa
in_pos_dict_liwc
in_neg_dict_liwc

author=ebert
author=siskel

value

1
1
1
1
0
1
0
1
0

14

how do we get 
good values for   ?

   = coef   cients

feature

the
and
bravest
love
loved
genius
not
fruit
bias

  

0.01
0.03
1.4
3.1
1.2
0.5
-3.0
-0.8
-0.1

15

likelihood

remember the likelihood of data is its id203 
under some parameter values 

in id113, we pick the 
values of the parameters under which the data is 
most likely.

16

likelihood

fair

5

.

0

4

.

0

3

.

0

p(               |                )

0

1

0

2

.

.

0

.

0

5
.

0

1

2

3

4

5

6

not fair

p(               |                )

2
.

.
0

4
.

0

3

0

1

.
0

0
.
0

1

2

3

4

5

6

=.17 x .17 x .17    
= 0.004913

= .1 x .5 x .5    
= 0.025

266266conditional likelihood

n i

p(yi | xi,   )

for all training data, we want 
id203 of the true label y for 

each data point x to high

bias

love

loved

a=   xi  i

x1
x2
x3

1
1
1

1
1
0

0
1
0

3
4.2
-0.1

exp(-a) 1/(1+exp(-a)) true 
y
1
0.05
1
0.015
1.11
0

95.2%
98.5%
47.5%

18

conditional likelihood

n i

p(yi | xi,   )

for all training data, we want 
id203 of the true label y for 

each data point x to high

this principle gives us a way to pick the values of 
the parameters    that maximize the id203 of 

the training data <x, y>

19

the value of    that maximizes likelihood also 

maximizes the log likelihood

arg max

  

n i=1

p(yi | xi,   ) = arg max

  

log

n i=1

p(yi | xi,   )

the log likelihood is an easier form to work with:

log

n i=1

p(yi | xi,   ) =

n i=1

log p(yi | xi,   )

20

    we want to    nd the value of    that leads to the 

highest value of the log likelihood:

 (  ) =

n i=1

log p(yi | xi,   )

21

we want to    nd the values of    that make the value of this 

function the greatest

 <x,y=+1>

log p(0 | x,   )

log p(1 | x,   ) +  <x,y=0>
 (  ) =  <x,y>

 
   i

(y     p(x)) xi

22

id119

if y is 1 and p(x) = 0, then this still pushes the weights a lot

if y is 1 and p(x) = 0.99, then this still pushes the 

weights just a little bit

23

stochastic g.d.

    batch id119 reasons over every training data point 

for each update of   .  this can be slow to converge. 

    stochastic id119 updates    after each data point.

24

practicalities

    when calculating the p(y | x) or in calculating the 

gradient, you don   t need to loop through all 
features     only those with nonzero values 

    (which makes sparse, binary values useful)

p(y = 1 | x,   ) =

1

1 + exp   f

i=1 xi  i 

 
   i

 (  ) =  <x,y>

(y     p(x)) xi

 
   i

 (  ) =  <x,y>

(y     p(x)) xi

if a feature xi only shows up with the positive 
class (e.g., positive sentiment), what are the 

possible values of its corresponding   i?
 (  ) =  <x,y>

 (  ) =  <x,y>

(1   0)1

 
   i

(1   0.9999999)1

 
   i

always positive

many features that show up 

rarely may likely only appear (by 

chance) with one label 

more generally, may appear so 

few times that the noise of 
randomness dominates

   = coef   cients

feature

like

did not like

in_pos_dict_mpqa

in_neg_dict_mpqa

in_pos_dict_liwc

in_neg_dict_liwc

author=ebert

author=ebert     dog 
    starts with    in    

  

2.1

1.4

1.7

-2.1

1.4

-3.1

-1.7

30.1

27

feature selection

    we could threshold features by minimum count but that 

also throws away information 

    we can take a probabilistic approach and encode a prior 

belief that all    should be 0 unless we have strong 
evidence otherwise

28

l2 id173
f j=1
n i=1
 
      

log p(yi | xi,   )
 
  

we want this to be high

  2
j

 

  

 (  ) =

but we want this to be small

    we can do this by changing the function we   re trying to optimize by adding 

a penalty for having values of    that are high 

    this is equivalent to saying that each    element is drawn from a normal 

distribution centered on 0. 

       controls how much of a penalty to pay for coef   cients that are far from 0 

(optimize on development data)

29

no l2 

id173

some l2 

id173

high l2 

id173

33.83 won bin
29.91 alexander beyer
24.78 bloopers
23.01 daniel br  hl
22.11 ha jeong-woo
20.49 supernatural
18.91 kristine debell
18.61 eddie murphy
18.33 cher
18.18 michael douglas

eddie murphy
tom cruise
tyler perry

2.17
1.98
1.70
1.70 michael douglas
1.66 robert redford
1.66
1.64 dance
1.63
1.63
1.62 cher

schwarzenegger
lee tergesen

julia roberts

family film
0.41
thriller
0.41
fantasy
0.36
0.32 action
0.25 buddy    lm
0.24 adventure
0.20 comp animation
0.19 animation
0.18 science fiction
0.18 bruce willis

30

  

  2

  

y

x

     norm(  ,   2)

y   ber  

exp  f
1 + exp  f

i=1 xi  i 
i=1 xi  i 

  

31

l1 id173
f j=1 |  j|
n i=1
 
 
  

log p(yi | xi,   )
 
  

we want this to be high

 

 

  

but we want this to be small
    l1 id173 encourages coef   cients to be 

 (  ) =

exactly 0. 

       again controls how much of a penalty to pay for 

coef   cients that are far from 0 (optimize on 
development data)

32

what do the coef   cients 

mean?

p(y | x,   ) =

exp (x0  0 + x1  1)

1 + exp (x0  0 + x1  1)

p(y | x,   )(1 + exp (x0  0 + x1  1)) = exp (x0  0 + x1  1)

p(y | x,   ) + p(y | x,   ) exp (x0  0 + x1  1) = exp (x0  0 + x1  1)

p(y | x,   ) + p(y | x,   ) exp (x0  0 + x1  1) = exp (x0  0 + x1  1)

p(y | x,   ) = exp (x0  0 + x1  1)   p(y | x,   ) exp (x0  0 + x1  1)

p(y | x,   ) = exp (x0  0 + x1  1)(1   p(y | x,   ))

this is the odds of y 

occurring

p(y | x,   )
1   p(y | x,   )

= exp (x0  0 + x1  1)

odds

    ratio of an event occurring to its not taking place

p(x)
1   p(x)

green bay packers 

vs. sf 49ers

0.75
0.25 =

3
1 = 3 : 1

id203 of 
gb winning

odds for gb 

winning

p(y | x,   ) + p(y | x,   ) exp (x0  0 + x1  1) = exp (x0  0 + x1  1)

p(y | x,   ) = exp (x0  0 + x1  1)   p(y | x,   ) exp (x0  0 + x1  1)

p(y | x,   ) = exp (x0  0 + x1  1)(1   p(y | x,   ))

this is the odds of y 

occurring

= exp (x0  0 + x1  1)

p(y | x,   )
1   p(y | x,   )
p(y | x,   )
1   p(y | x,   )

= exp (x0  0) exp (x1  1)

p(y | x,   )
1   p(y | x,   )

= exp (x0  0) exp (x1  1)

let   s increase the value of x by 

1 (e.g., from 0     1)

exp(x0  0) exp((x1 + 1)  1)
exp(x0  0) exp(x1  1 +   1)
exp(x0  0) exp (x1  1) exp (  1)

exp(  ) represents the factor by 
which the odds change with a 

1-unit increase in x

p(y | x,   )
1   p(y | x,   )

exp (  1)

room change!

    starting next tuesday 9/5, we   ll be in 2060 valley 

life sciences building

