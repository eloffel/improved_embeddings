natural language processing

info 159/259   

lecture 4: text classi   cation 3 (sept 5, 2017) 

david bamman, uc berkeley

. 

https://www.forbes.com/sites/kevinmurnane/2016/04/01/what-is-deep-learning-and-how-is-it-useful

history of nlp

    foundational insights, 1940s/1950s 

    two camps (symbolic/stochastic), 1957-1970 

    four paradigms  (stochastic, logic-based, nlu, discourse 

modeling), 1970-1983 

    empiricism and id122 (1983-1993) 

    field comes together (1994-1999) 

    machine learning (2000   today) 

    neural networks (~2014   today)

j&m 2008, ch 1

neural networks in nlp

    id38 [mikolov et al. 2010] 
    text classi   cation [kim 2014; iyyer et al. 2015] 

    syntactic parsing [chen and manning 2014, dyer et al. 2015, andor et al. 2016] 
    id35 super tagging [lewis and steedman 2014] 

    machine translation [cho et al. 2014, sustkever et al. 2014] 
    dialogue agents [sordoni et al. 2015, vinyals and lee 2015, ji et al. 2016] 

    (for overview, see goldberg 2017, 1.3.1)

neural networks

    discrete, high-dimensional representation of inputs 
(one-hot vectors) -> low-dimensional    distributed    
representations. 

    non-linear interactions of input features 

    multiple    layers    to capture hierarchical structure

neural network libraries

id28

  y =

1

1 + exp   f

i=1 xi  i 

x

1

1

0

  

-0.5

-1.7

0.3

not

bad

movie

sgd

calculate the derivative of some id168 with respect 
to parameters we can change, update accordingly to make 

predictions on training data a little less wrong next time.

8

id28

  y =

y

x1

x2

x3

  1

  2

  3

1

1 + exp   f

i=1 xi  i 
not

bad

movie

x

1

1

0

  

-0.5

-1.7

0.3

neural networks

    two core ideas: 

    non-linear id180 

    multiple layers

*for simplicity, we   re 

leaving out the bias term, 
but assume most layers 

have them as well.

w

x1

x2

x3

w1,1

w1,2

w2,1

w2,2

w3,1

w3,2

v

v1

v2

y

h1

h2

input

   hidden    
layer

output

w

v

h1

h2

y

w

-0.5

1.3

0.4 0.08

1.7

3.1

y

1

v

4.1

-0.9

x1

x2

x3

x

1

1

0

not
bad
movie

w

v

h1

h2

y

x1

x2

x3

hj = f  f i=1

xiwi,j 

the hidden nodes are 

completely determined by the 

input and weights

w

v

h1

h2

y

x1

x2

x3

h1 = f  f i=1

xiwi,1 

id180

  (z) =

1

1 + exp( z)

1.00

0.75

y

0.50

0.25

0.00

-10

-5

0
x

5

10

id28

x1

x2

x3

  1

  2

  3

y

  y =

1

1 + exp   f
i=1 xi  i 
xi  i 
  y =     f i=1

we can think about id28 as a 

neural network with no hidden layers

id180

tanh(z) =

exp(z)   exp( z)
exp(z) + exp( z)

1.0

0.5

y

0.0

-0.5

-1.0

-10

-5

0
x

5

10

id180

rectifier(z) = max(0, z)

10.0

7.5

y

5.0

2.5

0.0

-10

-5

0
x

5

10

w

v

h1

h2

y

x1

x2

x3

h1 =     f i=1
h2 =     f i=1

xiwi,1 
xiwi,2 

  y =    [v1h1 + v2h2]

w

v

h1

h2

y

x1

x2

x3

  y =    v1     f i

xiwi,1   + v2     f i

xiwi,2   

we can express y as a function only of the input x and the weights w and v

  y =         

v1     f i
xiwi,1  
 
  
 

h1

+v2     f i
xiwi,2  
 
  
 

h2

this is hairy, but differentiable

      

id26: given training samples of 
<x,y> pairs, we can use stochastic gradient 
descent to    nd the values of w and v that 
minimize the loss.

w

v

h1

h2

y

x1

x2

x3

neural networks are a series of 

functions chained together

xw    (xw)

   (xw) v

   (   (xw) v)

the loss is another function 

chained on top

log (   (   (xw) v))

chain rule

 

 v log (   (   (xw) v))

let   s take the likelihood for a 
single training example with 
label y =1; we want this value 

to be as high as possible

  log (   (   (xw) v))

    (   (xw) v)

    (   (xw) v)

    (xw) v

=

    (xw) v

 v

  log (   (hv))

    (hv)

=

 

a

  

b

    (hv)

      

 hv

c

     hv

 v

 

chain rule

=

 
      1

a

=

   (hv)  

  log (   (hv))

    (hv)

a

b

c

b

 

 v

 hv

    (hv)

  
      
     hv
 
  
 
   (hv)   (1      (hv)) 
= (1      (hv))h
= (1     y)h

c

    h

neural networks

    tremendous    exibility on design choices 
(exchange feature engineering for model 
engineering) 

    articulate model structure and use the chain rule to 

derive parameter updates.

neural network structures

x1

x2

x3

h1

h2

y

1

output one real value

neural network structures

x1

x2

x3

h1

h2

y

y

y

0

1

0

multiclass: output 3 values, only 

one = 1 in training data

neural network structures

x1

x2

x3

h1

h2

y

y

y

1

1

0

output 3 values, several = 1 in 

training data

id173

    increasing the number of parameters = increasing 

the possibility for over   tting to training data

id173

    l2 id173: penalize w and v for being too 

large 

    dropout: when training on a <x,y> pair, randomly 

remove some node and weights. 

    early stopping: stop id26 before the 

training error is too small.

deeper networks

w1

w2

v

x1

x2

x3

x3

h1

h2

h2

h2

h2

y

w

densely 

connected layer

h1

h2

h2

x

w

h

h =  (xw )

x1

x2

x3

x4

x5

x6

x7

convolutional networks

    with convolution networks, the same operation is 

(i.e., the same set of parameters) is applied to 
different regions of the input

2d convolution

0
0
0
0
0

0 0
0 1
0 1
0 1
0 0

0 0
1 1
1 1
1 1
0 0
blurring

http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/   

https://docs.gimp.org/en/plug-in-convmatrix.html

1d convolution

convolution k

1/3 1/3 1/3

x

1

3

-1

4

0

x   1:4k

= moving average

i

hated

it

i

really

hated

it

x1

x2

x3

x4

x5

x6

x7

convolutional 

networks

h1=f(i, hated, it)

h1

h2

h2=f(it, i, really)

h3
h3=f(really, hated, it)

x
w
h

h1 =  (x1w1 + x2w2 + x3w3)

h2 =  (x3w1 + x4w2 + x5w3)
h3 =  (x5w1 + x6w2 + x7w3)

indicator 
vector
    every token is a v-
dimensional vector 
(size of the vocab) 
with a single 1 
identifying the word 

    we   ll get to distributed 

representations of 
words in on 9/19

vocab item

indicator

a

aa

aal

aalii

aam

aardvark

aardwolf

aba

0

0

0

0

0

1

0

0

convolutional 

networks

convolutional window size

x

w

1

1

1

x1 x2 x3

w1 w2 w3

size of vocab

size of vocab

h1 =  (x1w1 + x2w2 + x3w3)

h2 =  (x3w1 + x4w2 + x5w3)
h3 =  (x5w1 + x6w2 + x7w3)

h1

h2

h3

x1

x2

x3

x4

x5

x6

x7

x1

x2

x3

x4

x5

x6

x7

convolutional 

networks

convolutional window size

x

w

1

1

1

x1 x2 x3

w1 w2 w3

size of vocab

size of vocab

h1

h2

h3

for indicator vectors, we   re just adding 

these numbers together

h1 =  (w1,xid

1

+ w2,xid

2

+ w3,xid

3

)

(where xnid speci   es the location of the 1 
in the vector     i.e., the vocabulary id)

pooling

    down-samples a layer by 

selecting a single point 
from some set 

    max-pooling selects the 

largest value

7

3

1

9

2

1

0

5

3

7

9

5

convolutional 

networks

10

this de   nes one    lter.

x1

x2

x3

x4

x5

x6

1

10

2

-1

5

x7
convolution

max pooling

x1

x2

x3

x4

x5

x6

x7

x

1

1

1

x1 x2 x3

w

w1 w2 w3

h1

vectorize

vectorize

1

1

1

x1

x2

x3

w1

w2

w3

h1 =  (x w )

x1

x2

x3

x4

x5

x6

x7

we can specify multiple    lters; each 
   lter is a separate set of parameters to 

be learned

wa

wb

wc wd

1

1

1

x1

x2

x3

h1 =  (x w )   r4

x1

x2

x3

x4

x5

x6

x7

we can specify multiple    lters; each 
   lter is a separate set of parameters to 

be learned

wa

wb

wc wd

1

1

1

x1

x2

x3

h1 =  (x w )   r4

convolutional networks

    with max pooling, we select a single number for each 

   lter over all tokens 

    (e.g., with 100    lters, the output of max pooling stage = 

100-dimensional vector) 

    if we specify multiple    lters, we can also scope each    lter 

over different window sizes

zhang and wallace 2016,    a sensitivity 
analysis of (and practitioners    guide to) 

convolutional neural networks for 

sentence classi   cation   

wa

wb

wc wd

v

x1

x2

x3

x4

x5

x6

x7

y

5 tokens, 4    lters

tanh(x   w )

max

 (hv )

id98 as important ngram 

detector

higher-order ngrams are 
much more informative than 
just unigrams (e.g.,    i don   t 
like this movie    [   i   ,    don   t   , 
   like   ,    this   ,    movie   ]) 

we can think about a id98 
as providing a mechanism 
for detecting important 
(sequential) ngrams without 
having the burden of 
creating them as unique 
features

unique types

unigrams

50921

bigrams

451,220

trigrams

910,694

4-grams

1,074,921

unique ngrams (1-4) in cornell movie review dataset

id98 backprop (v)

l(w, v ) = y log o

+ (1   y) log(1   o)

      a

 

b

  

 

 l(w, v )

 v

=

 a + b

 v

=

 a
v

+

 b
v

id98 backprop (v)

id98 backprop (v)

id98 backprop (v)

    you   ll derive and implement updates for the rest of 

the parameters in homework 2

generative vs. 

discriminative models

    generative models specify a joint distribution over the labels 

and the data.  with this you could generate new data

p(x, y) = p(y) p(x | y)

    discriminative models specify the conditional distribution of 
the label y given the data x.  these models focus on how to 
discriminate between the classes 

p(y | x)

259 project proposal   

due 9/26

    final project involving 1 or 2 students involving natural language 
processing -- either focusing on core nlp methods or using nlp 
in support of an empirical research question.  

    proposal (2 pages): 

    outline the work you   re going to undertake 
    motivate its rationale as an interesting question worth asking 
    assess its potential to contribute new knowledge by 

situating it within related literature in the scienti   c 
community. (cite 5 relevant sources) 

    who is the team and what are each of your responsibilities 

(everyone gets the same grade)

thursday

    read hovy and spruit (2016) and come prepared 

to discuss!

