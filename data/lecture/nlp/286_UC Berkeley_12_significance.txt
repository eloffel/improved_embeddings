natural language processing

lecture 12: features and hypothesis tests (oct 3, 2017) 

info 159/259   

david bamman, uc berkeley

announcements

    no of   ce hours for db this friday (email if you   d like 

to chat)

midterm 10/17

    in-class, 80 minutes.  

    covers all of class material and readings through 

10/12; mix of multiple choice and short/long 
answer.  

    no laptops or other devices; bring a single 

8.5   x11    cheat sheet  

    we   ll send out practice questions 10/12.

midterm questions

    email us sample questions (and answers)!  we 

might put it in the exam.  completely voluntary/no 
credit, but if we pick it, you   ll know the answer. 

    submit by 11:59pm next tuesday 10/10.

memm training

n i=1

p (yi | yi 1, x,  )

locally normalized     at each time step,    
each conditional distribution sums to 1

from last time

conditional random    elds

p (y | x,  ) =

exp( (x, y)  )

 y  y exp( (x, y )  )

    in memms, we normalize over the set of 45 pos 

tags 

    crfs are globally normalized, but the 

id172 complexity is huge     every 
possible sequence of labels of length n.

from last time

goldberg 2017

from last time

training biid56s

    given this de   nition of an biid56:

b = rb(xi, si+1
si
f = rf (xi, si 1
si

b

f

) = g(si+1
) = g(si 1

b w s
f w s

b + xiw x
f + xiw x

b + bb)
f + bf )

yi = softmax [si

f ; si

b]w o + bo 

    we have 8 sets of parameters to learn (3 for each 

id56 + 2 for the    nal layer)

from last time

signi   cance in nlp

    you develop a new id52 algorithm; is it 

better than what comes before? 

    you   re developing a new model; should you include 

feature x? (when there is a cost to including it) 

    you're developing a new model; does feature x 

reliably predict outcome y?

evaluation

    a critical part of development new algorithms and 

methods and demonstrating that they work

a mapping h from input data 
x (drawn from instance 

classi   cation
space     ) to a label (or 
enumerable output space     
     = set of all documents 
     = {english, mandarin, greek,    }

labels) y from some 

x = a single document 

y = ancient greek

    

instance space

train

dev

test

experiment design

training

development

testing

size

80%

10%

10%

purpose

training models

model selection

evaluation; 

never look at it 
until the very 

end

metrics

    evaluations presuppose that you have some metric to evaluate 

the    tness of a model. 

    language model: perplexity 

    id52/ner: accuracy, precision, recall, f1 

    phrase-structure parsing: parseval (bracketing overlap) 

    id33: labeled/unlabeled attachment score 

    machine translation: id7, meteor 

    summarization: id8 

metrics

    downstream tasks that use nlp to predict the 

natural world also have metrics: 

    predicting presidential approval rates from 

tweets. 

    predicting the type of job applicants from a job 

description. 

    conversational agent

multiclass confusion matrix

predicted (  )

nn

100

0

30

vbz

2

104

40

jj

15

30

70

)
y
(
 

e
u
r
t

nn

vbz

jj

accuracy

1
n

i[  yi = yi]

n i=1
i[x] 1 if x is true

0 otherwise

predicted (  )

nn

100

0

30

vbz

2

104

40

jj

15

30

70

)
y
(
 

e
u
r
t

nn

vbz

jj

precision

precision(nn) =

i=1 i(yi =   yi = nn)

i=1 i(  yi = nn)

 n
 n

)
y
(
 

e
u
r
t

nn

vbz

jj

precision: proportion 
of predicted class 
that are actually that 
class. 

predicted (  )

nn

100

0

30

vbz

2

104

40

jj

15

30

70

recall

recall(nn) =

i=1 i(yi =   yi = nn)

i=1 i(yi = nn)

 n
 n

)
y
(
 

e
u
r
t

nn

vbz

jj

recall: proportion of 
true class that are 
predicted to be that 
class. 

predicted (  )

nn

100

0

30

vbz

2

104

40

jj

15

30

70

f score

f =

2   precision   recall
precision + recall

ablation test

    to test how important individual features are (or 

components of a model), conduct an ablation test 

    train the full model with all features included, 

conduct evaluation. 

    remove feature, train reduced model, conduct 

evaluation.

ablation test

gimpel et al. 2011,    part-of-speech tagging for twitter   

signi   cance

your work

current state of the art

58%

50%

    if we observe difference in performance, what   s the 

cause?  is it because one system is better than another, 
or is it a function of randomness in the data?  if we had 
tested it on other data, would we get the same result?

hypotheses

hypothesis

the average income in two sub-populations is different

web design a leads to higher ctr than web design b

self-reported location on twitter is predictive of political preference

your system x is better than state-of-the-art system y

null hypothesis

    a claim, assumed to be true, that we   d like to test 

(because we think it   s wrong)

hypothesis

h0

the average income in two sub-

populations is different

web design a leads to higher ctr 

than web design b

self-reported location on twitter is 
predictive of political preference

your system x is better than state-of-

the-art system y

the incomes are the same

the ctr are the same

location has no relationship with 

political preference

there is no difference in the two 

systems.

hypothesis testing

    if the null hypothesis were true, how likely is it that 

you   d see the data you see?

hypothesis testing

    hypothesis testing measures our con   dence in 

what we can say about a null from a sample.

hypothesis testing

    current state of the art = 50%; your model = 58%.  
both evaluated on the same test set of 1000 data 
points. 

    null hypothesis = there is no difference, so we 
would expect your model to get 500 of the 1000 
data points right.   

    if we make parametric assumptions, we can model 

this with a binomial distribution (number of 
successes in n trials)

example

0.025

0.020

0.015

0.010

0.005

0.000

400

450

500

# dem

550

600

binomial id203 distribution for number of correct predictions in n=1000 with p = 0.5

example

at what point is a sample statistic unusual enough to reject 

the null hypothesis?

0.025

0.020

0.015

0.010

0.005

0.000

510

400

450

500

# dem

550

600

580

example

    the form we assume for the null hypothesis lets us 

quantify that level of surprise. 

    we can do this for many parametric forms that 

allows us to measure p(x     x) for some sample of 
size n; for large n, we can often make a normal 
approximation.

z score

z =

x     
  / n

for normal distributions, transform into standard 

normal (mean = 0, standard deviation =1 )

z score

510 correct    
= z score 0.63

-6

-3

0
z

3

6

580 correct    
= z score 5.06

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

tests

    we will de   ne    unusual    to equal the most extreme 

areas in the tails

least likely 10%

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

-4

-2

0
z

2

4

least likely 5%

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

-4

-2

0
z

2

4

least likely 1%

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

-4

-2

0
z

2

4

tests

510 correct    
= z score 0.63

-6

-3

0
z

3

6

580 correct    
= z score 5.06

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

tests

    decide on the level of signi   cance   .  {0.05, 0.01} 

    testing is evaluating whether the sample statistic 

falls in the rejection region de   ned by   

    two-tailed tests measured whether 
the observed statistic is different (in 
either direction) 

    one-tailed tests measure difference 

in a speci   c direction 

    all differ in where the rejection 

region is located;    = 0.05 for all.

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

tails

0
z

two-tailed test

2

4

-4

-2

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

-4

-2

0
z

2

4

-4

-2

0
z

2

4

lower-tailed test

upper-tailed test

p values

a p value is the id203 of observing a statistic at 
least as extreme as the one we did if the null 
hypothesis were true.

    two-tailed test

    lower-tailed test

    upper-tailed test

p-value(z) = 2   p(z    |z|)
p-value(z) = p(z   z)
p-value(z) = 1   p(z   z)

errors

test results

keep null

reject null

type i error   

  

type ii error   

  

power

truth

keep null

reject null

errors

    type i error: we reject the null hypothesis but we 

shouldn   t have. 

    type ii error: we don   t reject the null, but we should 

have.

1
2
3
4
5
6
7
8
9
   
1,000

   jobs    is predictive of presidential approval rating
   job    is predictive of presidential approval rating
   war    is predictive of presidential approval rating
   car    is predictive of presidential approval rating
   the    is predictive of presidential approval rating
   star    is predictive of presidential approval rating
   book    is predictive of presidential approval rating
   still    is predictive of presidential approval rating
   glass    is predictive of presidential approval rating
   
   bottle    is predictive of presidential approval rating

errors

    for any signi   cance level    and n hypothesis tests, 

we can expect      n type i errors. 

      =0.01, n=1000 = 10    signi   cant    results simply by 

chance

multiple hypothesis 

corrections

    bonferroni correction: for 

family-wise signi   cance level 
  0 with n hypothesis tests: 

    

  0
n

    [very strict; controls the 

id203 of at least one type i 
error.] 

    false discovery rate

nonparametric tests

    many hypothesis tests rely on parametric 

assumptions (e.g., normality) 

    alternatives that don   t rely on those assumptions: 

    permutation test 
    the bootstrap

back to logistic 

regression

  
2.17
1.98
1.70
1.70
1.66
   
-0.94
-1.00
-1.05
-1.14
-1.23

change in odds

feature name

8.76
7.24
5.47
5.47
5.26
   
0.39
0.37
0.35
0.32
0.29

eddie murphy
tom cruise
tyler perry
michael douglas
robert redford

   

kevin conway
fisher stevens
b-movie
black-and-white
indie

signi   cance of coef   cients

    a   i value of 0 means that feature xi has no effect 

on the prediction of y 

    how great does a   i value have to be for us to say 
that its effect probably doesn   t arise by chance? 

    people often use parametric tests (coef   cients are 
drawn from a normal distribution) to assess this for 
id28, but we can use it to illustrate 
another more robust test.

hypothesis tests

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

-4

-2

0
z

2

4

hypothesis tests measure how (un)likely an observed 

statistic is under the null hypothesis

hypothesis tests

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

-4

-2

0
z

2

4

permutation test

    non-parametric way of creating a null distribution 

(parametric = normal etc.) for testing the difference 
in two populations a and b 

    for example, the median height of men (=a) and 

women (=b) 

    we shuf   e the labels of the data under the null 

assumption that the labels don   t matter (the null is 
that a = b)

true 
labels
woman 

woman 

woman 

woman 

perm 1

perm 2

perm 3

perm 4

perm 5

man 

man 

man 

man 

man 

man 

man 

woman 

woman 

man 

man

man 

woman  woman

woman 

man 

man

man 

man 

woman  woman

man 

man

woman  woman  man 

man  woman  woman  man  woman  woman

man 
man  woman  woman  woman  man 

man  woman  man  woman  man
man

man  woman  man  woman  man  woman

man  woman  woman  woman  woman  woman

x1 

x2 

x3 

x4 

x5 

x6 

x7 
x8 

x9 

x10 

62.8

66.2

65.1

68.0

61.0

73.1

67.0
71.2

68.4

70.9

observed true difference in medians: -5.5

true 
labels
woman 
woman 

   
man 
man 

62.8
66.2
   
68.4
70.9

x1 
x2 
   
x9 
x10 

perm 1
man 
man 
   

perm 2
man 
man 
   

perm 3
woman 
man 
   

perm 5
perm 4
man
man 
woman  woman

   
woman  man 

woman  man 
woman
woman  woman  woman  woman  woman

   

difference in medians:

4.7

5.8

1.4

2.9

3.3

how many times is the difference in medians between the 
permuted groups greater than the observed difference?

0.4

0.3

y
t
i
s
n
e
d

0.2

0.1

0.0

observed real difference:   

-5.5

-6

-3

difference in medians among permuted dataset

0

3

6

a=100 samples from norm(70,4)

b=100 samples from norm(65, 3.5)

permutation test

the p-value is the number of times the permuted test statistic 

tp is more extreme than the observed test statistic t:

  p =

1
b

b i=1

i[abs(t) < abs(tp)]

permutation test

    the permutation test is a robust test that can be used for 

many different kinds of test statistics, including 
coef   cients in id28. 

    how? 

    a = members of class 1 
    b = members of class 0 
       are calculated as the (e.g.) the values that 

maximize the id155 of the class 
labels we observe; its value is determined by the 
data points that belong to a or b

permutation test

    to test whether the coef   cients have a statistically 

signi   cant effect (i.e., they   re not 0), we can conduct a 
permutation test where, for b trials, we: 

1. shuf   e the class labels in the training data 

2. train id28 on the new permuted 

dataset 

3. tally whether the absolute value of    learned on 

permuted data is greater than the absolute value of 
   learned on the true data

permutation test

the p-value is the number of times the permuted   p is more 

extreme than the observed   t:

bootstrap

    the permutation test assesses signi   cance 
conditioned on the test data you have (we 
rearrange the labels to form the null distribution, 
but the data itself doesn   t change). 

    to also model the variability in the data we have, 
we can use the statistical bootstrap (efron 1979).

bootstrap

    core idea: the data we happen to have is a sample 
from from all data that could exist; let   s sample from 
our sample to estimate the variability.
    

insta

train

dev

test

bootstrap

    start with test data x of size n 

    draw b bootstrap samples x(i) of size n by sampling 

with replacement from x 

    for each x(i) 

    let m(i) = the metric of interest calculated from x(i)

bootstrap

    at the end of the process, you end up with a vector 

of values m = [m(1),    , m(b)] (for b bootstrap 
samples)

m

utility

pos accuracy for system a

estimate con   dence intervals

i[system a > system b]

calculate signi   cance of 

difference

issues

    evaluation performance may not hold across 

domains (e.g., wsj    literary texts) 

    covariates may explain performance (mt/parsing, 

sentences up to length n) 

    multiple metrics may offer competing results

s  gaard et al. 2014

100
87.5
75
62.5
50

100
87.5
75
62.5
50

90
80
70
60
50

english pos

german pos

english pos

100
87.5
75
62.5
50

wsj

shakespeare

modern

early modern

italian pos

english pos

100
87.5
75
62.5
50

wsj

twitter

news

dante

100
87.5
75
62.5
50

100
85
70
55
40

wsj

middle english

english ner

conll

twitter

phrase structure parsing

id33

id33

89
79.25
69.5
59.75
50

87
77.75
68.5
59.25
50

wsj

patent

wsj

genia

wsj

magazines

81.997.069.697.056.297.375.097.073.797.341.089.079.389.579.688.277.186.9takeaways

    at a minimum, always evaluate a method on the 

domain you   re using it on 

    when comparing the performance of models, 
quantify your uncertainty with signi   cant tests/
con   dence bounds 

    use ablation tests to identify the impact that a 

feature class has on performance.

