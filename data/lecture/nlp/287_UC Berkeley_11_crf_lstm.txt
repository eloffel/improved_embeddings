natural language processing

info 159/259   

lecture 11: sequence labeling 2 (sept 28, 2017) 

david bamman, uc berkeley

id52

labeling the tag that   s correct 
for the context.

nnp

fw

sym

ls
dt

in
jj
vb
vbp

vbz
nn

nn
fruit    ies like a banana

nn

in
jj
vb
vbp

vbz
nn

nn
nn
time    ies like an arrow

dt

(just tags in evidence within the id32     more are possible!)

id39

pers

pers
tim cook is the ceo of apple

org

3 or 4-class:

    person 
location 
   
    organization 
   

(misc)

7-class:

    person 
location 
   
    organization 
   
    money 
    percent 
    date

time 

bio notation

i-pers

b-org
b-pers
tim cook is the ceo of apple

bio notation

b-pers

b-pers

after he helped harry ron went downstairs

majority class

    pick the label each word is seen most often with in 

the training data 

fruit
nn 12

   ies
vbz 7
nns 1

banana
nn 3

like
vb 74
vbp 31
jj 28
in 533

a
fw 8
sym 13
ls 2
jj 2
in 1
dt 25820
nnp 2

naive bayes

    treat each prediction as independent of the others

p (y | x) =

p (vbz | f lies) =

p (y)p (x | y)

 y  y p (y )p (x | y )
 y  y p (y )p (f lies | y )

p (vbz)p (f lies | vbz)

reminder: how do we learn p(y) and p(x|y) from training data?

id28

    treat each prediction as independent of the others but 

condition on much more expressive set of features

p (y | x;  ) =

p (vbz | f lies) =

exp x  y 
 y  y exp(x  y )
exp x  vbz 
 y  y exp(x  y )

hidden markov model

prior id203 of label 

sequence

p (y) = p (y1, . . . , yn)

p (y1, . . . , yn)  

n+1 i=1

p (yi | yi 1)

    we   ll make a    rst-order markov assumption and calculate the 

joint id203 as the product the individual factors 
conditioned only on the previous tag.

memm

general maxent form

arg max

y

p (y | x,  )

maxent with    rst-order markov 
assumption: maximum id178 

markov model

arg max

y

n i=1

p (yi | yi 1, x)

features

f (ti, ti 1; x1, . . . , xn)

features are scoped over 
the previous predicted 

tag and the entire 
observed input

feature

example

xi = man

ti-1 = jj

i=n (last word of 

sentence)

xi ends in -ly

1

1

1

0

memm training

n i=1

p (yi | yi 1, x,  )

for all training data, we want id203 of the true label 
yi conditioned on the previous true label yi-1 to be high. 

this is simply multiclass id28

memm training

n i=1

p (yi | yi 1, x,  )

locally normalized     at each time step,    
each conditional distribution sums to 1

label bias
n i=1

p (yi | yi 1, x,  )

    for a given conditioning context, the id203 of 
the a tag (e.g., vbz) only competes against other 
tags with that same context (e.g., nn)

bottou 2001; lafferty et al. 2001

label bias

nn to vb
will to    ght

nn md
40
10
-1
7
-2
7

modals show up much more 
frequently at the start of the 
sentence than nouns do (e.g., 

questions)

toutanova et al. 2003

xi=will

yi-1=start

bias

label bias

nn to vb
will to    ght

but we know that md + to is very rare 

    *can to eat 
    *would to eat 
    *could to eat 
    *may to eat

toutanova et al. 2003

label bias

nn to vb
will to    ght

xi=to
yi-1=nn
yi-1=md

10000000

to

0
0

to is relatively deterministic 

(almost always to) so it doesn   t 

matter what tag precedes it.

toutanova et al. 2003

label bias

nn to vb
will to    ght

n i=1

p (yi | yi 1, x,  )

because of this local 

id172, p(to | context) will 

always be 1 if x=   to   

toutanova et al. 2003

label bias

nn to vb
will to    ght

that means our prediction for to can   t help us 
disambiguate will.  we lose the information that 

mb + to sequences rarely happen.

toutanova et al. 2003

conditional random    elds

    we can solve this problem using global 

id172 (over the entire sequences) rather 
than locally normalized factors.

memm

crf

p (y | x,  ) =

p (y | x,  ) =

p (yi | yi 1, x,  )

n i=1
 y  y exp( (x, y )  )

exp( (x, y)  )

conditional random    elds

p (y | x,  ) =

exp( (x, y)  )

 y  y exp( (x, y )  )

feature vector scoped over the 
entire input and label sequence

 (x, y) =

 (x, i, yi, yi 1)

n i=1
     is the same feature vector we 

used for local predictions using 

memms

features

 (x, i, yi, yi 1)

features are scoped over 
the previous predicted 

tag and the entire 
observed input

feature

example

xi = man

yi-1 = jj

i=n (last word of 

sentence)

xi ends in -ly

1

1

1

0

conditional random    elds

p (y | x,  ) =

exp( (x, y)  )

 y  y exp( (x, y )  )

    in memms, we normalize over the set of 45 pos 

tags 

    crfs are globally normalized, but the 

id172 complexity is huge     every 
possible sequence of labels of length n.

forward algorithm (crf)

p (y | x,  ) =

exp( (x, y)  )

 y  y exp( (x, y )  )

    calculating the denominator naively would involve a 

summation over kn terms 

    but we can do this ef   ciently in nk2 time using the 

forward algorithm

for details, see: collins,    the forward-backward algorithm   

forward algorithm (crf)

end
dt
nnp
vb
nn
md
start

^

janet

will

back

the

bill

$

 (1, y) = exp  (x, i, y, start)   

 (i   1, y )   exp  (x, i, y, y )   

 (i, y) =  y  s

forward algorithm (crf)

end
dt
nnp
vb
nn
md
start

^

janet

will

back

the

bill

$

z =  y  y

exp  (x, y )    = s s

 (n, s)

conditional random    elds

with a crf, we have exactly the same 
parameters as we do with an equivalent memm;  
but we learn the best values of those parameters 
that leads to the best id203 of the sequence 
overall  (in our training data)

memm

xi=to
yi-1=nn
yi-1=md

10000000

to

0
0

crf

to
7.8
1.4
-5.8

xi=to
yi-1=nn
yi-1=md

parameter estimation

    just like id28/memm, we can    nd the 
optimal values for the parameters using stochastic 
id119 for a given sequence x and true 
labels y.

 l
  k

=  k(x, y)    y  y

p (y  | x,  ) k(x, y )

features for the true 

label y

expected feature counts under the 
probabity for a sequence assigned by 

current   

parameter estimation

    just like id28/memm, we can    nd the 
optimal values for the parameters using stochastic 
id119 for a given sequence x and true 
labels y.

 l
  k

=  k(x, y)    y  y

p (y  | x,  ) k(x, y )

if current model assigns id203 of 1 to true sequence and 0 to all 

other kn sequences, then gradient = 0

parameter estimation

 y  y
p (y  | x,  ) k(x, y )
 k(x, i, a, b)  y  y:y i 1=a,y i=b

=

n i=1  a s,b s

p (y  | x,  )

this entire sum can be found in nk2 time 
using the forward-backward algorithm

for details, see: collins,    the forward-backward algorithm   

viterbi decoding
v1(y) = exp  (x, 1, st art, y)   

[vt 1(u)   exp  (x, t, y, u)   ]

vt(y) = max
u s

(equivalently)

v1(y) =  (x, 1, st art, y)  

vt(y) = max
u s

[vt 1(u) +  (x, t, y, u)  ]

in practice

    crf training is slow!  nk2 complexity for each 

sequence at each gradient step. 

    accuracy is typically better than memms

recurrent neural network

    id56 allow arbitarily-sized conditioning contexts; 

condition on the entire sequence history.

recurrent neural network

goldberg 2017

recurrent neural network

    each time step has two inputs: 
    xi (the observation at time 

step i); one-hot vector, 
feature vector or 
distributed representation. 

    si-1 (the output of the 

previous state); base case: 
s0 = 0 vector

training id56s

    given this de   nition of an id56:

si = r(xi, si 1) = g(si 1w s + xiw x + b)
yi = o(si) = (cid:98)(cid:81)(cid:55)(cid:105)(cid:75)(cid:28)(cid:116)(siw o + bo)

    we have    ve sets of parameters to learn: 

w s, w x, w o, b, bo

recurrent neural network
predict the word from      conditioned on the context

    id56s for id38 are already performing 

a kind of sequence labeling: at each time step, 

recurrent neural network
    for id52, predict the tag from      conditioned 

on the context

dt

nn

vbd

in

nn

the

dog

ran

into

town

id56s for pos

nn to vb
will to    ght

    to make a prediction for yt, id56s condition on all 

input seen through time t (x1,    , xt) 

    but knowing something about the future can help 

(xt+1,    , xn)

recurrent neural network

    the simplest thing we could do is condition on a 

window of inputs

dt

nn

vbd

in

nn

the

dog

ran

into

town

recurrent neural network

    e.g. 5 time steps, input = 

[xi-1, xi, xi+1] 

    quite similar to what we do 

with id28

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

0.7

1.3

0.7

1.3

-4.5

bidirectional id56

    a powerful alternative is make predictions 

conditioning both on the past and the future. 

    two id56s  

    one running left-to-right 
    one right-to-left 

    each produces an output vector at each time step, 

which we concatenate

goldberg 2017

training biid56s

    given this de   nition of an biid56:

b = rb(xi, si+1
si
f = rf (xi, si 1
si

b

f

) = g(si+1
) = g(si 1

b w s
f w s

b + xiw x
f + xiw x

b + bb)
f + bf )

yi = softmax [si

f ; si

b]w o + bo 

    we have 8 sets of parameters to learn (3 for each 

id56 + 2 for the    nal layer)

error

goldberg 2017

id56s for pos

amazon and spotify   s streaming services are going to 

devour apple and its music purchasing model

nn?

nnp?

id56s for pos

amazon and spotify   s streaming services are going to 

devour apple and its music purchasing model

prediction:

can the information from far away get to the time 

step that needs it?

training:

can error reach that far back during 

id26?

id56s

    recurrent networks are deep in that they involve 

one    layer    for each time step (e.g., words in a 
sentence) 

    vanishing gradient problem: as error is back 

propagated through the layers of a deep network, 
they tend toward 0.

long short-term memory  

network (lstm)

    designed to account for the vanishing gradient 

problem 

    basic idea: split the s vector propagated between 
time steps into a memory component and a hidden 
state component

lstms

http://colah.github.io/posts/2015-08-understanding-lstms/

lstm/id56

    is an id56 the same kind of sequence labeling 

model as an memm or crf? 

    it doesn   t use nearby labels in making predictions!  

(more like id28 in this respect)

recurrent neural network

    how could we incorporate nearby label information 

into a single-direction id56?

huang et al. 2015,    bidirectional lstm-crf models for sequence tagging"

ma and hovy (2016),    end-to-end sequence 
labeling via bi-directional lstm-id98s-crf   

ma and hovy (2016),    end-to-end sequence labeling via bi-directional lstm-id98s-crf   

