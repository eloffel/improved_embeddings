natural language processing

info 159/259   

lecture 6: language models 1 (sept 12, 2017) 

david bamman, uc berkeley

language model
    vocabulary      is a    nite set of discrete symbols 
(e.g., words, characters); v = |      | 
        + is the in   nite set of sequences of symbols from 
    ; each sequence ends with stop 
    x         +

language model

p (w) = p (w1, . . . , wn)

p(   call me ishmael   ) =    

p(w1 =    call   , w2 =    me   , w3 =    ishmael   ) x p(stop)

 w v +

over all sequence lengths!

p (w) = 1

0   p (w)   1

language model

    language models provide us with a way to quantify 

the likelihood of sequence     i.e., plausible 
sentences.

ocr

    to fee great pompey paffe the areets of rome: 
    to see great pompey passe the streets of rome:

machine translation

    fidelity (to source text) 
    fluency (of the translation)

id103

    'scuse me while i kiss the sky. 
    'scuse me while i kiss this guy 
    'scuse me while i kiss this    y. 
    'scuse me while my biscuits fry

dialogue generation

li et al. (2016), "deep id23 for dialogue generation" (emnlp)

information theoretic view

y

   one morning i shot an elephant in 

my pajamas   

encode(y)

decode(encode(y))

shannon 1948

noisy channel

asr
mt
ocr

x

speech signal

target text

pixel densities

y

transcription
source text
transcription

p (y | x)   p (x | y )

channel model

 

  

source model

p (y )

    

 

language model

    id38 is the task of estimating p(w) 

    why is this hard?

p(   it was the best of times, it was the worst of times   )

chain rule (of id203)

p (x1, x2, x3, x4, x5) = p (x1)

  p (x2 | x1)
  p (x3 | x1, x2)
  p (x4 | x1, x2, x3)
  p (x5 | x1, x2, x3, x4)

chain rule (of id203)

p(   it was the best of times, it was the worst of times   )

chain rule (of id203)

p(   it   )

p(   was    |    it    )

this is easy

p (w1)
p (w2 | w1)
p (w3 | w1, w2)
p (w4 | w1, w2, w3)

this is hard

p (wn | w1, . . . , wn 1)

p(   times    |    it was the best of times, it was the worst of    )

markov assumption

   rst-order

p (xi | x1, . . . xi 1)   p (xi | xi 1)

second-order

p (xi | x1, . . . xi 1)   p (xi | xi 2, xi 1)

markov assumption

bigram model 

(   rst-order markov)

trigram model 

(second-order markov)

p (wi | wi 1)   p (stop | wn)

n i
n i

p (wi | wi 2, wi 1)
 p (stop | wn 1, wn)

   it was the best of 
times, it was the 
worst of times   

p (it | start1, start2)
p (was | start2, it)
p (the | it, was)
   

p (times | worst, of )
p (stop | of, times)

estimation

unigram

bigram

trigram

p (wi)

n i

n i

n i

p (wi | wi 1)

p (wi | wi 2, wi 1)

 p (st op )

 p (st op | wn)

 p (st op | wn 1, wn)

maximum likelihood estimate

c(wi)

n

c(wi 1, wi)
c(wi 1)

c(wi 2, wi 1, wi)
c(wi 2, wi 1)

generating

0.06

0.04

0.02

0.00

a

amazing bad

best

good

like

love movie

not

of

sword

the

worst

    what we learn in estimating language models is p(word | 
context), where context     at least here     is the previous 
n-1 words (for ngram of order n) 

    we have one multinomial over the vocabulary (including 

stop) for each context

generating

    as we sample, 
the words we 
generate form 
the new context 
we condition on

generated   

word

the

dog

walked

context1

context2

start

start

the

dog

start

the

dog

walked

in

aside: sampling?

sampling from a multinomial

id203 
mass function 

(pmf)

p(z = x) 
exactly

)
x
 
=
 
z
(
p

6

.

0

5

.

0

4

.

0

3

.

0

2

.
0

1
.
0

0
.
0

1

2

3

x

4

5

sampling from a multinomial

cumulative 

density 

function (cdf)

p(z     x)

)
x
 
=
<
 
z
(
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

1

2

3

x

4

5

sampling from a multinomial

p=.78

sample p 
uniformly in 

[0,1] 

find the point 

cdf-1(p)

)
x
 
=
<
 
z
(
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

1

2

3

x

4

5

sampling from a multinomial

sample p 
uniformly in 

[0,1] 

find the point 

cdf-1(p)

)
x
 
=
<
 
z
(
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

p=.06

1

2

3

x

4

5

sampling from a multinomial

sample p 
uniformly in 

[0,1] 

find the point 

cdf-1(p)

)
x
 
=
<
 
z
(
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

   1.000

   0.703

   0.008    0.059

   0.071

1

2

3

x

4

5

unigram model

    the around, she they i blue talking    don   t to and little 

come of 

    on fallen used there. young people to l  zaro 

    of the 

    the of of never that ordered don't avoided to 

complaining.  

    words do had men    ung killed gift the one of but thing 

seen i plate bradley was by small kingmaker.

bigram model

       what the way to feel where we   re all those ancients 
called me one of the council member, and smelled 
tales of like a korps peaks.    

    tuna battle which sold or a monocle, i planned to help 

and distinctly. 

       i lay in the canoe     

    she started to be able to the blundering collapsed. 

       fine.   

trigram model

       i   ll worry about it.    
    avenue great-grandfather edgeworth hasn   t gotten there. 
       if you know what. it was a photograph of seventeenth-century 

   ourishin    to their right hands to the    sh who would not care at all. 
looking at the clock, ticking away like electronic warnings about 
wonderfully sat on fifth 

    democratic convention in rags soaked and my past life, i managed 

to wring your neck a boss won   t so david pritchet giggled. 

    he humped an argument but her bare he stood next to larry, these 
days it will have no trouble jay grayer continued to peer around the 
germans weren   t going to faint in the

4gram model

    our visitor in an idiot sister shall be blotted out in bars and 

   irting with curly black hair right marble, wallpapered on 
screen credit.    

    you are much instant coffee ranges of hills. 

    madison might be stored here and tell everyone about was 

tight in her pained face was an old enemy, trading-posts of the 
outdoors watching anyog extended on my lips moved feebly. 

    said. 

       i   m in my mind, threw dirt in an inch,    the director.

evaluation

    the best id74 are external     how 

does a better language model in   uence the 
application you care about? 

    id103 (word error rate), machine 

translation (id7 score), topic models 
(sensemaking)

evaluation

    a good language model should judge unseen real language to 

have high id203 

    perplexity = inverse id203 of test data, averaged by word. 

    to be reliable, the test data must be truly unseen (including 

knowledge of its vocabulary).

perplexity = 

n 

1

p (w1, . . . , wn)

experiment design

training

development

testing

size

80%

10%

10%

purpose

training models

model selection; 
hyperparameter 

tuning

evaluation; 

never look at it 
until the very 

end

evaluation

log p (w1, . . . , wn) =

1
n

exp  

1
n

perplexity = 

n i
n i
n i

log p (wi)

log p (wi)

log p (wi) 

perplexity

trigram model 

(second-order markov)

exp  

1
n

n i

log p (wi | wi 2, wi 1) 

perplexity

model

unigram

bigram

trigram

perplexity

962

170

109

slp3 4.3

smoothing

    when estimating a language model, we   re relying 
on the data we   ve observed in a training corpus. 

    training data is a small (and biased) sample of the 

creativity of language.

data sparsity

slp3 4.1

n i

p (wi | wi 1)   p (stop | wn)

    as in naive bayes, p(wi) = 0 causes p(w) = 0.  

(perplexity?)

smoothing in nb

    one solution: add a little id203 mass to every 

element.

maximum likelihood 

estimate

p(xi | y) =

ni,y
ny

smoothed estimates

p(xi | y) =

ni,y +   
ny + v  

same    for all xi

ni,y = count of word i in class y 

ny = number of words in y 

v = size of vocabulary

p(xi | y) =

ni,y +   i

ny + v

j=1   j

possibly different    for each xi

additive smoothing

laplace smoothing:   

   = 1

p (wi) =

c(wi) +  
n + v  

p (wi | wi 1) =

c(wi 1, wi) +  
c(wi 1) + v  

smoothing

id113

smoothing is the re-allocation 

of id203 mass

smoothing with    =1 

6
.
0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

6
.
0

5
.
0

4
.
0

3
.
0

2

.

0

1

.

0

0

.

0

1

2

3

4

5

6

1

2

3

4

5

6

smoothing

    how can best re-allocate id203 mass?

stanley f. chen and joshua goodman. an empirical study of smoothing techniques for id38. 
technical report tr-10-98, center for research in computing technology, harvard university, 1998.

interpolation

    as ngram order rises, we have the potential for 
higher precision but also higher variability in our 
estimates. 

    a linear interpolation of any two language models p 
and q (with        [0,1]) is also a valid language model.

 p + (1    )q

p = the web

q = political speeches

interpolation

    we can use this fact to make higher-order 

language models more robust.

p (wi | wi 2, wi 1) =  1p (wi | wi 2, wi 1)

+  2p (wi | wi 1)
+  3p (wi)

 1 +  2 +  3 = 1

interpolation

    how do we pick the best values of   ? 

    grid search over development corpus 

    expectation-maximization algorithm (treat as 

missing parameters to be estimated to 
maximize the id203 of the data we see).

kneser-ney smoothing

    intuition: when backing off to a lower-order ngram, 
maybe the overall ngram frequency is not our best 
guess.

i can   t see without my reading ____________

p(   francisco   ) > p(   glasses   )

    francisco is more frequent, but shows up in fewer 

unique bigrams (   san francisco   )     so we shouldn   t 
expect it in new contexts; glasses, however, does show 
up in many different bigrams

kneser-ney smoothing

    intuition: estimate how likely a word is to show up in 

a new continuation? 

    how many different bigram types does a word type 
w show up in (normalized by all bigram types that 
are seen)

continuation id203: of all 
bigram types in training data, 
how many is w the suf   x for?

|v   v : c(v, w) > 0|

|v , w    v : c(v , w ) > 0|

pkn (v) =

|v   v : c(v, w) > 0|

|v , w    v : c(v , w ) > 0|

pkn(v) is the continuation id203 for the 

unigram v (the frequency with which it 

appears as the suf   x in distinct bigram types)

kneser-ney smoothing

discounted mass

max{c(wi 1, wi)   d, 0}

c(wi 1)

+  (wi 1)pkn (wi)

discounted bigram id203

continuition id203

kneser-ney smoothing

max{c(wi 1, wi)   d, 0}

c(wi 1)

discounted bigram id203

d is a discount factor 

+  (wi 1)pkn (wi)

(usually between 0 and 1     
how much we discount the 

observed counts by

kneser-ney smoothing

pre   x types

 (wi 1) =

d   |v   v : c(wi 1v) > 0|

c(wi 1)

pre   x tokens

   here captures the discounted mass we   re 

reallocating from pre   x wi-1

kneser-ney smoothing

wi-1
red
red
red
sum

wi
hook
car
watch

c(wi-1, wi)

c(wi-1, wi) - d(1)

3
2
10
15

2
1
9
12

 (red) = 1  

3
15

12/15 of the id203 mass stays 

with the original counts;   

 3/15 is reallocated

pkn (v) =

|v   v : c(v, w) > 0|

|v , w    v : c(v , w ) > 0|

discounted mass

max{c(wi 1, wi)   d, 0}

c(wi 1)

+  (wi 1)pkn (wi)

discounted bigram id203

continuition id203

we   ll move all of the mass we subtracted 

here over to this side

max{c(wi 1, wi)   d, 0}

c(wi 1)

+  (wi 1)pkn (wi)

and distribute it according to the 

continuation id203

stanley f. chen and joshua goodman. an empirical study of smoothing techniques for id38. 
technical report tr-10-98, center for research in computing technology, harvard university, 1998.

   stupid backoff   

if full sequence observed

c(wi k+1, . . . , wi)
c(wi k+1, . . . , wi 1)

s(wi | wi k+1, . . . , wi 1) =

no discounting here, just back off 
to lower order ngram if the higher 

order is not observed. 

cheap to calculate; works almost 

as well as kn when there is    

a lot of data

otherwise

=  s(wi | wi k+2, . . . , wi 1)

brants et al. (2007),    large language models in machine translation   

you should feel comfortable:

    calculate the id203 of a sentence given a 

trained model 

    estimating (e.g., trigram) language model 

    evaluating perplexity on held-out data 

    sampling a sentence from a trained model

tools

    srilm   

http://www.speech.sri.com/projects/srilm/ 

    kenlm   

https://khea   eld.com/code/kenlm/ 

    berkeley lm   

https://code.google.com/archive/p/berkeleylm/

