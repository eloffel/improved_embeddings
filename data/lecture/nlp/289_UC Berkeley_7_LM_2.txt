natural language processing

info 159/259   

lecture 7: language models 2 (sept 14, 2017) 

david bamman, uc berkeley

language model
    vocabulary      is a    nite set of discrete symbols 
(e.g., words, characters); v = |      | 
        + is the in   nite set of sequences of symbols from 
    ; each sequence ends with stop 
    x         +

language model

id38 is the task of estimating p(w)

language model

arma virumque cano

    arms man and i sing 
    arms, and the man i sing [dryden] 
    i sing of arms and a man

    when we have choices to make about different ways 

to say something, a language models gives us a 
formal way of operationalizing their    uency   
 (in terms of how likely they are exist in the language)

markov assumption

bigram model 

(   rst-order markov)

trigram model 

(second-order markov)

p (wi | wi 1)   p (stop | wn)

n i
n i

p (wi | wi 2, wi 1)
 p (stop | wn 1, wn)

a mapping h from input data 
x (drawn from instance 

classi   cation
space     ) to a label (or 
enumerable output space     
     = set of all documents 
     = {english, mandarin, greek,    }

labels) y from some 

x = a single document 

y = ancient greek

a mapping h from input data 
x (drawn from instance 

classi   cation
space     ) to a label (or 
enumerable output space     
     = {the, of, a, dog, iphone,    }

labels) y from some 

x = (context) 

y = word

id28

p(y = 1 | x,   ) =

1

1 + exp   f

i=1 xi  i 

output space

y = {0, 1}

x = feature vector

   = coef   cients

feature

the
and
bravest
love
loved
genius
not
fruit
bias

value

0
0
0
0
0
0
0
1
1

feature

the
and
bravest
love
loved
genius
not
fruit
bias

  

0.01
0.03
1.4
3.1
1.2
0.5
-3.0
-0.8
-0.1

9

p(y = 1 | x = x;   ) =

exp(x   )

1 + exp(x   )

=

exp(x   )

exp(x 0) + exp(x   )

id28

p(y = y | x = x;   ) =

exp(x   y)

 y  y exp(x   y )

output space

y = {1, . . . , k}

x = feature vector

   = coef   cients

feature

value

feature

  1

  2

  3

  4

  5

the
and
bravest
love
loved
genius
not
fruit
bias

0
0
0
0
0
0
0
1
1

the
and

bravest
love
loved
genius

not
fruit
bias

1.33
1.21
0.96
1.49
-0.52
0.98
-0.96
0.59
-1.92

-0.80
-1.73
-0.05
0.53
-0.02
0.77
2.14
-0.76
-0.70

-0.54
-1.57
0.24
1.01
2.21
1.53
-0.71
0.93
0.94

0.87
-0.13
0.81
0.64
-2.53
-0.95
0.43
0.03
-0.63

0
0
0
0
0
0
0
0
0

12

language model

    we can use multi class id28 for 

id38 by treating the vocabulary as 
the output space

y = v

unigram lm

    a unigram language model here would have just 

one feature: a bias term.

feature

  the

  of

  a

  dog

  iphone

bias

-1.92

-0.70

0.94

-0.63

0

bigram lm

value
1
0
0
0
0
0
0
0
1

  the
1.33
1.21
0.96
1.49
-0.52
0.98
-0.96
0.59
-1.92

  of
-0.80
-1.73
-0.05
0.53
-0.02
0.77
2.14
-0.76
-0.70

  a

-0.54
-1.57
0.24
1.01
2.21
1.53
-0.71
0.93
0.94

  dog
0.87
-0.13
0.81
0.64
-2.53
-0.95
0.43
0.03
-0.63

  iphone
0
0
0
0
0
0
0
0
0

st

feature
wi-1=the
wi-1=and
wi-1=brave
wi-1=love
wi-1=loved
wi-1=geniu
wi-1=not
wi-1=fruit
bias

s

p(wi = dog | wi 1 = the)

value
1
0
0
0
0
0
0
0
1

  the
1.33
1.21
0.96
1.49
-0.52
0.98
-0.96
0.59
-1.92

  of
-0.80
-1.73
-0.05
0.53
-0.02
0.77
2.14
-0.76
-0.70

  a

-0.54
-1.57
0.24
1.01
2.21
1.53
-0.71
0.93
0.94

  dog
0.87
-0.13
0.81
0.64
-2.53
-0.95
0.43
0.03
-0.63

  iphone
0
0
0
0
0
0
0
0
0

st

feature
wi-1=the
wi-1=and
wi-1=brave
wi-1=love
wi-1=loved
wi-1=geniu
wi-1=not
wi-1=fruit
bias

s

trigram lm

p(wi = dog | wi 2 = and, wi 1 = the)

feature

wi-2=the ^ wi-1=the
wi-2=and ^ wi-1=the
wi-2=bravest ^ wi-1=the
wi-2=love ^ wi-1=the
wi-2=loved ^ wi-1=the
wi-2=genius ^ wi-1=the
wi-2=not ^ wi-1=the
wi-2=fruit ^ wi-1=the

bias

value
0
1
0
0
0
0
0
0
1

parameters

    how many parameters do we have with a log-linear 

trigram language model? 

    how many do we have with a generative trigram 

lm?

smoothing

p(wi = dog | wi 2 = and, wi 1 = the)

second-order 

features

   rst-order 
features

feature

wi-2=the ^ wi-1=the
wi-2=and ^ wi-1=the

wi-2=bravest ^ wi-1=the
wi-2=love ^ wi-1=the

wi-1=the
wi-1=and
wi-1=bravest
wi-1=love

bias

value
0
1
0
0
1
0
0
0
1

l2 id173
f j=1
n i=1
 
      

log p(yi | xi,   )
 
  

we want this to be high

  2
j

 

  

 (  ) =

but we want this to be small

    we can do this by changing the function we   re trying to optimize by adding 

a penalty for having values of    that are high 

    this is equivalent to saying that each    element is drawn from a normal 

distribution centered on 0. 

       controls how much of a penalty to pay for coef   cients that are far from 0 

(optimize on development data)

20

l1 id173
f j=1 |  j|
n i=1
 
 
  

log p(yi | xi,   )
 
  

we want this to be high

 

 

  

but we want this to be small
    l1 id173 encourages coef   cients to be 

 (  ) =

exactly 0. 

       again controls how much of a penalty to pay for 

coef   cients that are far from 0 (optimize on 
development data)

21

richer representations

    id148 give us the    exibility of encoding 

richer representations of the context we are 
conditioning on. 

    we can reason about any observations from the 

entire history and not just the local context.

   jacksonville, fla.     stressed and exhausted families 
across the southeast were assessing the damage from 
hurricane irma on tuesday, even as    ooding from the storm 
continued to plague some areas, like jacksonville, and the 
worst of its wallop was being revealed in others, like the 
florida keys. 

of   cials in florida, georgia and south carolina tried to 
prepare residents for the hardships of recovery from the 
______________   

https://www.nytimes.com/2017/09/12/us/irma-jacksonville-   orida.html

   hillary clinton seemed to add benghazi to her 
already-long list of culprits to blame for her upset loss 
to donald ________   

http://www.foxnews.com/politics/2017/09/13/clinton-laments-how-benghazi-tragedy-hurt-her-politically.html

   hillary clinton seemed to add benghazi to her 
already-long list of culprits to blame for her upset loss 
to donald ________   

feature classes

example

ngrams (wi-1, wi-2:wi-1, wi-3:wi-1)

wi-2=   to   , wi=   donald   

gappy ngrams

spelling, capitalizaiton

class/gazetteer membership

w1=   hillary    and wi-1=   donald   
wi-1 is capitalized and wi is 

wi-1 in list of names and wi in list of 

capitalized

names

26

classes

bigram

<color> car

count

137

bigram

black car

blue car

red car

count

100

37

0

goldberg 2017

tradeoffs

    richer representations = more parameters, higher 

likelihood of over   tting 

    much slower to train than estimating the 

parameters of a generative model

p(y = y | x = x;   ) =

exp(x   y)

 y  y exp(x   y )

neural lm

simple feed-forward multilayer id88 

(e.g., one hidden layer)

input x = vector concatenation of a conditioning context of 

   xed size k

1

x = [v(w1); . . . ; v(wk)]

bengio et al. 2003

x = [v(w1); . . . ; v(wk)]

w1 = tried    
w2 = to    
w3 = prepare    
w4 = residents

1

1

1

1

v(w1)

v(w2)

v(w3)

v(w4)

v(w1)

v(w2)

v(w3)

v(w4)

0.7

1.3

-4.5

0.7

1.3

-4.5

0.7

1.3

-4.5

0.7

1.3

-4.5

one-hot encoding

distributed 
representation

bengio et al. 2003

w1   rkd h
b1   rh

w2   rh v
b2   rv

h = g(xw1 + b1)

x = [v(w1); . . . ; v(wk)]

  y = softmax(hw2 + b2)

bengio et al. 2003

softmax

p(y = y | x = x;   ) =

exp(x   y)

 y  y exp(x   y )

neural lm

conditioning context

tried to prepare residents for the hardships of recovery from the

y

recurrent neural network

    id56 allow arbitarily-sized conditioning contexts; 

condition on the entire sequence history.

recurrent neural network

goldberg 2017

recurrent neural network

    each time step has two inputs: 
    xi (the observation at time 

step i); one-hot vector, 
feature vector or 
distributed representation. 

    si-1 (the output of the 

previous state); base case: 
s0 = 0 vector

recurrent neural network
si = r(xi, si 1)

r computes the output state as a 
function of the current input and 

previous state

yi = o(si)
o computes the output as a 
function of the current output 

state

continuous bag of words

si = r(xi, si 1) = si 1 + xi

each state is the sum of all 

previous inputs

yi = o(si) = si

output is the identity

simple id56

g = tanh or relu

si = r(xi, si 1) = g(si 1ws + xiwx + b)

different weight vectors w 

transform the previous state and 
current input before combining

ws   rh h
wx   rd h
b   rh
yi = o(si) = si

elman 1990, mikolov 2012

id56 lm

    the output state si is an h-

dimensional real vector; we can 
transfer that into a id203 
by passing it through an 
additional linear transformation 
followed by a softmax

yi = o(si) = (cid:98)(cid:81)(cid:55)(cid:105)(cid:75)(cid:28)(cid:116)(siw o + bo)

elman 1990, mikolov 2012

training id56s

    given this de   nition of an id56:

si = r(xi, si 1) = g(si 1w s + xiw x + b)
yi = o(si) = (cid:98)(cid:81)(cid:55)(cid:105)(cid:75)(cid:28)(cid:116)(siw o + bo)

    we have    ve sets of parameters to learn: 

w s, w x, w o, b, bo

training id56s

we

tried

to

prepare residents

    at each time step, we make a prediction and incur 
a loss; we know the true y (the word we see in that 
position)

 l( )y1

 l( )y2

 l( )y3

 l( )y4

 l( )y5

 w s

 w s

 w s

 w s

 w s

we

tried

to

prepare residents

    training here is standard id26, taking 

the derivative of the loss we incur at step t with 
respect to the parameters we want to update

generation

    as we sample, 
the words we 
generate form 
the new context 
we condition on

generated   

word

the

dog

walked

context1

context2

start

start

the

dog

start

the

dog

walked

in

generation

conditioned generation

    in a basic id56, the input at each timestep is a 

representation of the word at that position

si = r(xi, si 1) = g(si 1w s + xiw x + b)

    but we can also condition on any arbitrary context 

(topic, author, date, metadata, dialect, etc.)

si = r(xi, si 1) = g(si 1w s + [xi; c]w x + b)

we

tried

to

prepare residents

0

0

0

0

0

-2
5
-2
-4
8

8
-4
5
-2
-4

-8
2
-11
8
-1

-1
0
0
-3
0

2
2
5
-2
2

each state i encodes information seen until time i    
and its structure is optimized to predict the next word

character lm

    vocabulary      is a    nite set of discrete characters 

    when the output space is small, you   re putting a lot 

of the burden on the structure of the model 

    encode long-range dependencies (suf   xes 
depend on pre   xes, word boundaries etc.)

character lm

http://karpathy.github.io/2015/05/21/id56-effectiveness/

character lm

http://karpathy.github.io/2015/05/21/id56-effectiveness/

character lm

http://karpathy.github.io/2015/05/21/id56-effectiveness/

drawbacks

    very expensive to train (especially for large 

vocabulary, though tricks exists     cf. hierarchical 
softmax) 

    id26 through long histories leads to 
vanishing gradients (cf. lstms in a few weeks). 

    but they consistently have some of the strongest 

performances in perplexity evaluations.

mikolov 2010

melis, dyer and blunsom 2017

distributed representations

    some of the greatest power 

in neural network 
approaches is in the 
representation of words 
(and contexts) as low-
dimensional vectors 

    we   ll talk much more about 

that on tuesday.

1

   

0.7

1.3

-4.5

