natural language processing

info 159/259   

lecture 10: sequence labeling 1 (sept 26, 2017) 

david bamman, uc berkeley

id52

labeling the tag that   s correct 
for the context.

nnp

fw

sym

ls
dt

in
jj
vb
vbp

vbz
nn

nn
fruit    ies like a banana

nn

in
jj
vb
vbp

vbz
nn

nn
nn
time    ies like an arrow

dt

(just tags in evidence within the id32     more are possible!)

id39

pers pers
tim cook is the ceo of apple

org

3 or 4-class:

    person 
location 
   
    organization 
   

(misc)

7-class:

    person 
location 
   
    organization 
   
    money 
    percent 
    date

time 

supersense tagging

artifact

artifact

group
the station wagons arrived at noon, a long shining line  

motion

time

motion

location

location

that coursed through the west campus. 

noun supersenses (ciarmita and altun 2003)

book segmentation

sequence labeling

x = {x1, . . . , xn}
y = {y1, . . . , yn}

    for a set of inputs x with n sequential time steps, 

one corresponding label yi for each xi

majority class

    pick the label each word is seen most often with in 

the training data 

fruit
nn 12

   ies
vbz 7
nns 1

banana
nn 3

like
vb 74
vbp 31
jj 28
in 533

a
fw 8
sym 13
ls 2
jj 2
in 1
dt 25820
nnp 2

naive bayes

    treat each prediction as independent of the others

p (y | x) =

p (vbz | f lies) =

p (y)p (x | y)

 y  y p (y )p (x | y )
 y  y p (y )p (f lies | y )

p (vbz)p (f lies | vbz)

reminder: how do we learn p(y) and p(x|y) from training data?

id28

    treat each prediction as independent of the others but 

condition on much more expressive set of features

p (y | x;  ) =

p (vbz | f lies) =

exp x  y 
 y  y exp(x  y )
exp x  vbz 
 y  y exp(x  y )

discriminative features

features are scoped over 

entire observed input

fruit    ies like a banana

feature

xi =    ies

xi = car

xi-1 = fruit

xi+1 = like

example

1

0

1

1

sequences

    models that make independent predictions for 

elements in a sequence can reason over 
expressive representations of the input x (including 
correlations among inputs at different time steps xi 
and xj. 

    but they don   t capture another important source of 

information: correlations in the labels y.

sequences

in
jj
vb
vbp

vbz

nn
time    ies like an arrow

sequences

most common tag bigrams in 

id32 training

dt
nnp
nn
in
jj
dt
nn
nn
in
nn
jj
nns
to
nnp
in

nn
nnp
in
dt
nn
jj
nn
,

nnp

.

nns
in
vb
,
nn

41909
37696
35458
35006
29699
19166
17484
16352
15940
15548
15297
15146
13797
13683
11565

sequences

x

y

time

nn

   ies

vbz

like

in

an

dt

arrow

nn

p (y = nn vbz in dt nn | x = time    ies like an arrow)

generative vs. 

discriminative models

    generative models specify a joint distribution over the labels 

and the data.  with this you could generate new data

p(x, y) = p(y) p(x | y)

    discriminative models specify the conditional distribution of 
the label y given the data x.  these models focus on how to 
discriminate between the classes 

p(y | x)

generative

p (y | x) =

p (x | y)p (y)

 y  y p (x | y)p (y)

p (y | x)   p (x | y)p (y)

max

y

p (x | y)p (y)

how do we parameterize these probabilities when x and y are sequences?

hidden markov model

prior id203 of label 

sequence

p (y) = p (y1, . . . , yn)

p (y1, . . . , yn)  

n+1 i=1

p (yi | yi 1)

    we   ll make a    rst-order markov assumption and calculate the 

joint id203 as the product the individual factors 
conditioned only on the previous tag.

hidden markov model

p (yi, . . . , yn) = p (y1)

  p (y2 | y1)
  p (y3 | y1, y2)
. . .
  p (yn | y1, . . . , yn 1)
    remember: a markov assumption is an approximation to this 

exact decomposition (the chain rule of id203)

hidden markov model

p (x | y) = p (x1, . . . , xn | y1, . . . , yn)

p (x1, . . . , xn | y1, . . . , yn)  

n i=1

p (xi | yi)

    here again we   ll make a strong assumption: the id203 of 
the word we see at a given time step is only dependent on its 
label

nnp vbz

nn vbz

is
has
says
does
plans
expects

   s

wants
owns
makes
hopes
remains
claims
seems
estimates

1121
854
420
77
50
47
40
31
30
29
24
24
19
19
17

is
has
does
says
remains

   s

includes
continues
makes
seems
comes
re   ects
calls
expects
goes

2893
1004
128
109
56
51
44
43
40
34
33
31
30
29
27

p (xi | yi, yi 1)

id48

p (x1, . . . , xn, y1, . . . , yn)  

n+1 i=1

p (yi | yi 1)

n i=1

p (xi | yi)

id48

y1

x1

y2

x2

p (y3 | y2)

y3

x3

y4

x4

p (x3 | y3)

y5

x5

y6

x6

y7

x7

id48

p (v b | n n p )

nnp

nnp

vb

rb

dt

jj

nn

mr.

collins

was

not

a

sensible

man

p (was | v b)

parameter estimation

p (yt | yt 1)

id113 for both is just counting    

(as in naive bayes)

p (xt | yt)

c(y1, y2)

c(y1)

c(x, y)
c(y)

transition probabilities

emission probabilities

smoothing

    one solution: add a little id203 mass to every 

element.

maximum likelihood 

estimate

p(xi | y) =

ni,y
ny

smoothed estimates

p(xi | y) =

ni,y +   
ny + v  

same    for all xi

ni,y = count of word i in class y 

ny = number of words in y 

v = size of vocabulary

p(xi | y) =

ni,y +   i

ny + v

j=1   j

possibly different    for each xi

decoding

    greedy: proceed left to right, committing to the 
best tag for each time step (given the sequence 
seen so far)

fruit

nn

   ies

vb

like

in

a

dt

banana

nn

decoding

nn

vbd

dt
???
the horse raced past the barn fell

nn

dt

in

decoding

in

nn

vbd

dt
???
the horse raced past the barn fell
vbd
dt

vbn

nn

nn

nn

dt

dt

in

information later on in the sentence can in   uence the best 

tags earlier on.

all paths

end
dt
nnp
vb
nn
md
start

^

janet

will

back

the

bill

$

ideally, what we want is to calculate the joint 

id203 of each path and pick the one with the 
highest id203.  but for n time steps and k 

labels, number of possible paths = kn

5 word sentence with 45 id32 tags 

455 = 184,528,125 different paths 

4520 = 1.16e33 different paths

viterbi algorithm

    basic idea: if an optimal path through a sequence 

uses label l at time t, then it must have used an 
optimal path to get to label l at time t 

    we can discard all non-optimal paths up to label l 

at time t

end
dt
nnp
vb
nn
md
start

^

janet

will

back

the

bill

$

    at each time step t ending in label k, we    nd the 
max id203 of any path that led to that state 

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

janet

will

back

the

bill

what   s the id48 id203 of ending in janet = nnp?

p (yt | yt 1)p (xt | yt)

p (nnp | start)p (janet | nnp)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

vt(end)

best path through time step 1 
ending in tag y (trivially - best 

path for all is just start)

janet

will

back

the

bill

v1(y) = max
u y

[p (yt = y | yt 1 = u)p (xt | yt = y)]

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

janet

will

back

the

bill

what   s the max id48 id203 of ending in will = md?

first, what   s the id48 id203 of a single path 

ending in will = md?

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

janet

will

back

the

bill

p (y1 | st art )p (x1 | y1)   p (y2 = md | y1)p (x2 | y2 = md)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

vt(end)

best path through time step 2 

ending in tag md

janet

will

back

the

bill

p (dt | start)   p (janet | dt)   p (yt = md | p (yt 1 = dt)   p (will | yt = md)
p (nnp | start)   p (janet | nnp)   p (yt = md | p (yt 1 = nnp)   p (will | yt = md)
p (vb | start)   p (janet | vb)   p (yt = md | p (yt 1 = vb)   p (will | yt = md)
p (nn | start)   p (janet | nn)   p (yt = md | p (yt 1 = nn)   p (will | yt = md)
p (md | start)   p (janet | md)   p (yt = md | p (yt 1 = md)   p (will | yt = md)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

vt(end)

best path through time step 2 

ending in tag md

janet

will

back

the

bill

let   s say the best path ending will = md includes 

janet = nnp.  by de   nition, every other path has lower 

id203.

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

vt(end)

best path through time step 2 

ending in tag md

janet

will

back

the

bill

p (nnp | start)   p (janet | nnp)   p (yt = md | p (yt 1 = nnp)   p (will | yt = md)

p(dt|start) p(janet|dt) p(yt=md|p(yt 1=dt) p(will|yt=md)p(vb|start) p(janet|vb) p(yt=md|p(yt 1=vb) p(will|yt=md)p(nn|start) p(janet|nn) p(yt=md|p(yt 1=nn) p(will|yt=md)p(md|start) p(janet|md) p(yt=md|p(yt 1=md) p(will|yt=md)v1(y) = max
u y

[p (yt = y | yt 1 = u)p (xt | yt = y)]

p (dt | start)   p (janet | dt)   p (yt = md | p (yt 1 = dt)   p (will | yt = md)
p (nnp | start)   p (janet | nnp)   p (yt = md | p (yt 1 = nnp)   p (will | yt = md)
p (vb | start)   p (janet | vb)   p (yt = md | p (yt 1 = vb)   p (will | yt = md)
p (nn | start)   p (janet | nn)   p (yt = md | p (yt 1 = nn)   p (will | yt = md)
p (md | start)   p (janet | md)   p (yt = md | p (yt 1 = md)   p (will | yt = md)

v1(dt )   p (yt = md | p (yt 1 = dt)   p (will | yt = md)

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

janet

will

back

the

bill

vt(y) = max
u y

[vt 1(u)   p (yt = y | yt 1 = u)p (xt | yt = y)]

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

v3(dt)
v3(nnp)
v3(vb)
v3(nn)
v3(md)

janet

will

back

the

bill

25 paths ending in back = vb

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

v3(dt)
v3(nnp)
v3(vb)
v3(nn)
v3(md)

janet

will

back

the

bill

let   s say the best path ending in back = vb includes 

will = md.

vt(end)

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

v3(dt)
v3(nnp)
v3(vb)
v3(nn)
v3(md)

janet

will

back

the

bill

if the best path ending in will = md includes 

janet=nnp, we can forget all paths with janet != nnp 

for any path including will = md because we know 

they are less likely.

end
dt
nnp
vb
nn
md
start

vt(end)

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

v3(dt)
v3(nnp)
v3(vb)
v3(nn)
v3(md)

v4(dt)
v4(nnp)
v4(md)
v4(nn)
v4(md)

janet

will

back

the

bill

125 possible paths ending in the = dt, but we only need 
to consider 5 (best path ending in back = dt, back = 

nnp, back = vb, back = nn, back = md)

end
dt
nnp
vb
nn
md
start

vt(end)

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

v3(dt)
v3(nnp)
v3(vb)
v3(nn)
v3(md)

v4(dt)
v4(nnp)
v4(md)
v4(nn)
v4(md)

v5(dt)
v5(nnp)
v5(md)
v5(nn)
v5(md)

janet

will

back

the

bill

end
dt
nnp
vb
nn
md
start

vt(end)

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

v2(dt)
v2(nnp)
v2(vb)
v2(nn)
v2(md)

v3(dt)
v3(nnp)
v3(vb)
v3(nn)
v3(md)

v4(dt)
v4(nnp)
v4(md)
v4(nn)
v4(md)

v5(dt)
v5(nnp)
v5(md)
v5(nn)
v5(md)

janet

will

back

the

bill

vt(end) encodes the best path through the 

entire sequence

vt(end)

end
dt
nnp
vb
nn
md
start

janet

will

back

the

bill

for each timestep t + label, keep track of the 
max element from t-1 to reconstruct best path

end
dt
nnp
vb
nn
md
start

v1(dt)
v1(nnp)
v1(vb)
v1(nn)
v1(md)

vt(end)

can viterbi decoding help with 
independent preditions? (e.g., 

naive bayes or logreg)

janet

will

back

the

bill

v1(y) = max
u y

[p (yt = y | yt 1 = u)p (xt | yt = y)]

when making 

independent predictions:

p (yt = y | yt 1 = u) = p (yt = y)

generative vs. 

discriminative models

    generative models specify a joint distribution over the labels 

and the data.  with this you could generate new data

p(x, y) = p(y) p(x | y)

    discriminative models specify the conditional distribution of 
the label y given the data x.  these models focus on how to 
discriminate between the classes 

p(y | x)

memm

general maxent form

arg max

y

p (y | x,  )

maxent with    rst-order markov 
assumption: maximum id178 

markov model

arg max

y

n i=1

p (yi | yi 1, x)

memm

nnp

nnp

vb

rb

dt

jj

nn

mr.

collins

was

not

a

sensible

man

memm

nnp

nnp

vb

rb

dt

jj

nn

mr.

collins

was

not

a

sensible

man

memms condition on the entire input

memm

nnp

nnp

vb

rb

dt

jj

nn

mr.

collins

was

not

a

sensible

man

features

feature

example

xi = man

ti-1 = jj

i=n (last word of 

sentence)

xi ends in -ly

1

1

1

0

features are scoped over 
the previous predicted 

tag and the entire 
observed input

training
n i=1

p (yi | yi 1, x,  )

for all training data, we want id203 of the true label 
yi conditioned on the previous true label yi-1 to be high. 

this is simply multiclass id28

decoding

    with id28, our prediction is simply the  

argmax y:

p (y | x,  )

    with an memm, we know the true yi-1 during 

training but we never of course know it at test time

p (yi | yi 1, x,  )

greedy decoding

    a i=1, predict the argmax given start:

p (y1 | st art , x,  )

    for each subsequent time step, condition on the y 

just predicted during the step before

p (yi | yi 1, x,  )

viterbi decoding

viterbi for id48: max joint id203

p (y)p (x | y) = p (x, y)

vt(y) = max
u y

[vt 1(u)   p (yt = y | yt 1 = u)p (xt | yt = y)]

viterbi for memm: max id155

p (y | x)

vt(y) = max
u y

[vt 1(u)   p (yt = y | yt 1 = u, x,  )]

project propsals

    due today by 11:59pm!

