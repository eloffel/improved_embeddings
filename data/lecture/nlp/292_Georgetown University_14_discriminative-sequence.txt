lecture 14: 

discriminative sequence 

tagging

nathan schneider 
enlp | 3.14.2018

1

id48 + features

    there are variants of  the generative id48 that 

emit features instead of  just words. 

    however, these suffer from similar problems as 

features in na  ve bayes (too strong 
independence assumptions). 

    can we be discriminative instead? 

    yes! in fact, we can reuse the same machinery for 

discriminative learning with linear models.

2

recasting id48 as a linear model

    recall that a linear model is one that scores candidate 
outputs y with wt  (x,y). decoding = arg maxy    wt  (x,y   ). 

    not just classification: we can be predicting a 
structured output y. thus arg maxy    wt  (x,y   ). 

    how can we express an id48 in this framework? 

    transitions = features over tag id165s 
    emissions = tag + word features 
    weights = log probabilities 
    arg maxy    = viterbi decoding

3

viterbi for linear models

    essentially, the viterbi algorithm stays the 

same: 
   

transition probabilities replaced by linear score of  
transition (multi-tag) features 

    emission probabilities replaced by linear score of  

non-transition (single-tag) features

d

4

generative     discriminative

   

if  we want to estimate the weights without making 
independence assumptions about the features    

       we can use a discriminative learning algorithm! 

    however, the algorithm has to take the structure of  
the output into account. tag id165 features mean 
the prediction of  one tag influences what the model 
thinks about other tags. 

    machine learning with models where the outputs are 

interrelated is called id170.

5

review: id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run current classifier 
 
       arg maxy    w      (x, y   ) 
 
 
 
if        y then # mistake 
 
  w     w +   (x, y)       (x,   ) 
 
return w

l

w

6

review: id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run current classifier 
 
                        x    
 
 
 
if        y then # mistake 
 
  w     w +   (x, y)       (x,   ) 
 
return w

c

decoding is a 
subroutine of learning

l

w

7

structured id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run structured decoding 
 
                        x    
 
 
 
if        y then # mistake 
 
  w     w +   (x, y)       (x,   ) 
 
return w

d

decoding is a 
subroutine of learning

l

w

8

structured id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run structured decoding 
 
                        x    
 
 
 
if        y then # mistake: incorrect tag(s) 
 
  w     w +   (x, y)       (x,   ) 
 
return w
that    re for mistagged tokens

for sequence tagging, 
decoder = viterbi!

update affects weights of features 

d

l

w

9

structured id88

    what are the constraints on the kinds of  features we can use? (tag 

bigrams? trigrams? word bigrams? trigrams?) 
    remember that discriminative = we don   t care about modeling the 

id203 of  the language. thus, every model feature should involve at 
least one tag. 

    as a sequence model, markov order is still relevant: if  we want to use the 

bigram viterbi algorithm, which is o(t  n), we can have features over tag 
bigrams, but not trigrams. 

   

local feature = feature which respects the independence assumptions of  
the decoding algorithm (e.g., tag bigram viterbi). using nonlocal features 
would require fancier algorithms. 

    unlike the generative id48, no constraint on which words can be in a 

feature. e.g., there could be a feature that relates the first tag to the last 
token! (in id52, perhaps ending with    ?    correlates with certain 
kinds of  initial words.)

10

predicted (iteration 1):
<s>

det 

det 

old

man

det   
the

det 

the

det 

boats

det 

</s>

.

gold:
<s>

det   
the

adj 

verb 

det 

noun 

punct 

</s>

old

man

the

boats

.

11

   
   
   
   
   
   
   
   
   
   
det   
the

gold:
<s>

det   
the

predicted (iteration 1):
<s>

det 

det 

old

man

det,det

det 

the

det 

boats

det 

</s>

det,<lowercase> 

det,boats 
.
det,len=5 
det,suf   x=s

adj 

verb 

det 

noun 

punct 

</s>

old

man

the

boats

.

unlike the generative id48, each connection can 

involve multiple weighted features.

12

   
   
   
   
   
   
   
   
   
   
predicted (iteration 1):
<s>

det 

det 

old

man

det   
the

det 

the

det 

boats

det 

</s>

.

gold:
<s>

det   
the

adj 

verb 

det 

noun 

punct 

</s>

old

man

the

boats

.

update parameters!

correct tags: no change to weights

13

   
   
   
   
   
   
   
   
   
   
predicted (iteration 1):
<s>

det 

det 

old

man

det   
the

det 

the

det 

boats

det 

</s>

.

gold:
<s>

det   
the

adj 

verb 

det 

noun 

punct 

</s>

old

man

the

boats

.

update parameters!

weights for incorrect tags get more negative,    
weights for correct tags get more positive

14

   
   
   
   
   
   
   
   
   
   
discriminative classi   ers:    

non-probabilistic

    the structured counterpart of  the id88 
classifier is called   the structured id88. 
    also: structural id166 (max-margin).

15

discriminative classi   ers:    

probabilistic

    the structured counterpart of  the id28 classifier: 

conditional random field (crf). 
    most common: linear-chain structure, i.e., sequence 
    probabilistic   linear score is exponentiated & normalized 
    training requires forward-backward algorithm (expensive!) 
    generally state-of-the-art 
    downloadable implementations include crf++ 
   

if  you want the gory details: sutton & mccallum, http://
homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf  

    there is also the maximum id178 markov model (memm), which 

makes simplifying assumptions to reduce computation and is nearly 
as accurate in practice.

16

final projects

    see how nlp components fit together in a system 
    off-the-shelf  tools such as spacy, stanford corenlp 
    + new code 

    work in an interdisciplinary team of  3 people 

    each team should have at least 2 departments/programs represented 
    design the project to suit the team   s strengths! (programming, data collection, 

analysis) 

    build something cool! 

    artistic, scientific, or practical 
    using data (existing or new) & concepts from this course 
    start simple, then iterate 

   

instructor & tas will help you scope the project, find relevant literature, design 
evaluation, etc.

17

