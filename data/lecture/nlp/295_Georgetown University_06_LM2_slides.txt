empirical methods in natural language processing

lecture 6

language models: more smoothing

(most slides from sharon goldwater; some adapted from philipp koehn)

31 january 2017

nathan schneider

enlp lecture 6

31 january 2017

recap: smoothing for language models

    n -gram lms reduce sparsity by assuming each word only depends on a

   xed-length history.

    but even this assumption isn   t enough: we still encounter lots of unseen

n -grams in a test set or new corpus.

    if we use id113, we   ll assign 0 id203 to unseen items: useless as an lm.
    smoothing solves this problem: move id203 mass from seen items to

unseen items.

    add-   smoothing: (   = 1 or < 1) very simple, but no good when vocabulary

size is large.

nathan schneider

enlp lecture 6

1

good-turing smoothing: intuition

    good-turing smoothing:

    estimate the id203 of seeing (any) item with nc counts (e.g., 0 count)

as the proportion of items already seen with nc+1 counts (e.g., 1 count).
    divide that id203 evenly between all possible items with nc counts.

nathan schneider

enlp lecture 6

2

good-turing smoothing

    if n is count of history, then pgt = c   

n where

c    = (c + 1)

nc+1
nc

    nc number of n -grams that occur exactly c times in corpus

    n0 total number of unseen n -grams

    ex. for trigram id203 pgt (three|i spent), then n is count of i spent and

c is count of i spent three.

nathan schneider

enlp lecture 6

3

problems with good-turing

    assumes we know the vocabulary size (no unseen words)

[but again, use unk: see j&m 4.3.2]

    doesn   t allow    holes    in the counts (if ni > 0, ni   1 > 0)

[can estimate using id75: see j&m 4.5.3]

    applies discounts even to high-frequency items

[but see j&m 4.5.3]

    but there   s a more fundamental problem...

nathan schneider

enlp lecture 6

4

remaining problem

    in training corpus, suppose we see scottish beer but neither of

    scottish beer drinkers
    scottish beer eaters

    if we build a trigram model smoothed with add-   or g-t, which example has

higher id203?

nathan schneider

enlp lecture 6

5

remaining problem

    previous smoothing methods assign equal id203 to all unseen events.
    better: use information from lower order n -grams (shorter histories).

    beer drinkers
    beer eaters

    two ways: interpolation and backo   .

nathan schneider

enlp lecture 6

6

interpolation

    combine higher and lower order n -gram models, since they have di   erent

strengths and weaknesses:

    high-order n -grams are sensitive to more context, but have sparse counts
    low-order n -grams have limited context, but robust counts

    if pn is n -gram estimate (from id113, gt, etc.; n     {1, 2, 3}), use:

pint(w3|w1, w2) =   1 p1(w3) +   2 p2(w3|w2) +   3 p3(w3|w1, w2)

pint(three|i, spent) =   1 p1(three) +   2 p2(three|spent)
+  3 p3(three|i, spent)

nathan schneider

enlp lecture 6

7

interpolation

    note that   is must sum to 1:

pint(w3|w1, w2)

(cid:88)
(cid:88)

w3

1 =

=

(cid:88)

w3

=   1

[  1 p1(w3) +   2 p2(w3|w2) +   3 p3(w3|w1, w2)]

(cid:88)

(cid:88)

p1(w3) +   2

p2(w3|w2) +   3

p3(w3|w1, w2)

w3

w3

w3

=   1 +   2 +   3

nathan schneider

enlp lecture 6

8

fitting the interpolation parameters

    in general, any weighted combination of distributions is called a mixture

model.

    so   is are interpolation parameters or mixture weights.
    the values of the   is are chosen to optimize perplexity on a held-out data set.

nathan schneider

enlp lecture 6

9

back-o   

    trust the highest order language model that contains n -gram

pbo(wi|wi   n +1, ..., wi   1) =

p    (wi|wi   n +1, ..., wi   1)

if count(wi   n +1, ..., wi) > 0

=

  (wi   n +1, ..., wi   1) pbo(wi|wi   n +2, ..., wi   1)

else

                                 

nathan schneider

enlp lecture 6

10

back-o   

    requires

    adjusted prediction model p    (wi|wi   n +1, ..., wi   1)
    backo    weights   (w1, ..., wn   1)

    exact equations get complicated to make probabilities sum to 1.
    see textbook for details if interested.

nathan schneider

enlp lecture 6

11

do our smoothing methods work here?

example from mackay and peto (1995):

imagine, you see, that the language, you see, has, you see, a frequently
occurring couplet,    you see   , you see, in which the second word of the
couplet, see, follows the    rst word, you, with very high id203, you
see. then the marginal statistics, you see, are going to become hugely
dominated, you see, by the words you and see, with equal frequency, you
see.

    p (see) and p (you) both high, but see nearly always follows you.
    so p (see|novel) should be much lower than p (you|novel).

nathan schneider

enlp lecture 6

12

diversity of histories matters!

    a real example: the word york

    fairly frequent word in europarl corpus, occurs 477 times
    as frequent as foods, indicates and providers
    in unigram language model: a respectable id203
    however, it almost always directly follows new (473 times)
    so, in unseen bigram contexts, york should have low id203

    lower than predicted by unigram model as used in interpolation/backo   .

nathan schneider

enlp lecture 6

13

kneser-ney smoothing

    kneser-ney smoothing takes diversity of histories into account
    count of distinct histories for a word:

n1+(   wi) = |{wi   1 : c(wi   1, wi) > 0}|
    recall: maximum likelihood est. of unigram language model:

    in kn smoothing, replace raw counts with count of histories:

pm l(wi) =

pkn(wi) =

w c(w)

c(wi)(cid:80)
(cid:80)
n1+(   wi)
w n1+(   w)

nathan schneider

enlp lecture 6

14

kneser-ney in practice

    original version used backo   , later    modi   ed kneser-ney    introduced using

interpolation.

    fairly complex equations, but until recently the best smoothing method for

word id165s.

    see chen and goodman (1999) for extensive comparisons of kn and other

smoothing methods.

    kn (and other methods) implemented in language modelling toolkits like
srilm (classic), kenlm (good for really big models), opengrm ngram library
(uses    nite state transducers), etc.

nathan schneider

enlp lecture 6

15

kenneth hea   eldlm

kenlm is a highly e   cient id38 toolkit.

https://kheafield.com/code/kenlm/

nathan schneider

enlp lecture 6

16

are we done with smoothing yet?

we   ve considered methods that predict rare/unseen words using
    uniform probabilities (add-  , good-turing)
    probabilities from lower-order id165s (interpolation, backo   )
    id203 of appearing in new contexts (kneser-ney)

what   s left?

nathan schneider

enlp lecture 6

17

back to the big picture

    however we train our lm, we will want to use it in some application.
    now, a bit more detail about how that can work.
    we need another concept from id205: the id87.

nathan schneider

enlp lecture 6

18

id87

    we imagine that someone tries to communicate a sequence to us, but noise is

introduced. we only see the output sequence.

nathan schneider

enlp lecture 6

19

p(x)symbolsequencenoisy/errorfulencodingsequenceoutputp(y)p(x|y)id87

    we imagine that someone tries to communicate a sequence to us, but noise is

introduced. we only see the output sequence.

application
id103
machine translation words in l1
id147

y
spoken words

intended words

x
acoustic signal
words in l2
typed words

nathan schneider

enlp lecture 6

20

p(x)symbolsequencenoisy/errorfulencodingsequenceoutputp(y)p(x|y)example: id147

    p (y ): distribution over the words the user intended to type. a language

model!

    p (x|y ): distribution describing what user is likely to type, given what they
meant. could incorporate information about common spelling errors, key
positions, etc. call it a noise model.

    p (x): resulting distribution over what we actually see.
    given some particular observation x (say, e   ert), we want to recover the most

probable y that was intended.

nathan schneider

enlp lecture 6

21

summary

    di   erent smoothing methods account for di   erent aspects of sparse data and

word behaviour.

    interpolation/backo   :

leverage advantages of both higher and lower order

n -grams.

    kneser-ney smoothing: accounts for diversity of history.
    distributed representations: account for word similarity.

    id87 combines lm with noise model to de   ne the    best   

solution for many applications.

nathan schneider

enlp lecture 6

22

references

chen, s. f. and goodman, j. (1999). an empirical study of smoothing techniques for language

modeling. computer speech & language, 13(4):359   394.

mackay, d. j. c. and peto, l. c. b. (1995). a hierarchical dirichlet language model. natural

language engineering, 1(3):289   308.

nathan schneider

enlp lecture 6

23

