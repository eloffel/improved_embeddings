linear models for 
classi   cation:    
features & weights

nathan schneider   

(some slides borrowed from chris dyer) 

enlp | 7 february 2018

1

outline

    words, probabilities     features, weights 

    geometric view: decision boundary 

    id88 

    generative vs. discriminative 

this lecture

next lecture

    more discriminative models: id28/maxent; 

id166 

    id168s, optimization 

    id173; sparsity

2

id51 

(wsd)

    given a word in context, predict which sense is 

being used. 
    evaluated on corpora such as semcor, which is fully 

annotated for id138 synsets. 

    for example: consider joint pos & wsd 

classification for    interest   , with 3 senses: 
    n:financial (i repaid the loan with interest) 
    n:nonfinancial (i read the news with interest) 
    v:nonfinancial (can i interest you in a dessert?)

3

beyond bow

    neighboring words are relevant to this decision. 

    more generally, we can define features of  the input that may 

individual words 

help identify the correct class. 
   
    bigrams (pairs of  consecutive words: wall street) 
    capitalization (interest vs. interest vs. interest) 
    metadata: document genre, author,     

    these can be used in na  ve bayes:    bag of  features    

    with overlapping features, independence assumption is even more 

na  ve: p(y | x)     p(y)        p(wall | y) p(street | y) p(wall street | y)

4

choosing features

    supervision means that we don   t have to pre-specify the precise 

relationship between each feature and the classification 
outcomes. 

    but domain expertise helps in choosing which kinds of  features to 

include in the model. (words, subword units, metadata,    ) 
    and sometimes, highly task-specific features are helpful. 

    the decision about what features to include in a model is called 

feature engineering. 
   

(there are some algorithmic techniques, such as feature selection, that 
can assist in this process.) 

    more features = more flexibility, but also more expensive to train, more 

opportunity for overfitting.

5

feature extraction

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

6

x = wall street vets raise concerns 

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

about interest rates , politics
1
bias feature (   class prior): value of 
0
1 for every x so the learned weight 
3
8
will re   ect prevalence of the class
0.27
0
1
0
1
0
0
1
1
1
0
0
0

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

   

6

feature extraction

x = wall street vets raise concerns 

about interest rates , politics

spelling feature

6

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

1
0
3
8
0.27
0
1
0
1
0
0
1
1
1
0
0
0

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

   

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

7

feature extraction

x = wall street vets raise concerns 

about interest rates , politics

6

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

1
0
3
token positional features
8
0.27
0
1
0
1
0
0
1
1
1
0
0
0

   

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

8

feature extraction

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

x = wall street vets raise concerns 

about interest rates , politics

6

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

1
0
3
8
0.27
0
immediately neighboring words
1
0
1
0
0
1
1
1
0
0
0

   

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

9

feature extraction

x = wall street vets raise concerns 

about interest rates , politics

6

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

1
0
3
8
0.27
0
1
0
1
0
0
unigrams
1
1
1
0
0
0

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

   

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

10

feature extraction

x = wall street vets raise concerns 

about interest rates , politics

6

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

1
0
3
8
0.27
0
1
0
1
0
0
1
1
1
0
bigrams
0
0

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

   

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

11

feature extraction

6

  (x)   (x   )
1
0
6
3
0.66
1
0
1
0
1
1
1
0
0
1
1
1

1
0
3
8
0.27
0
1
0
1
0
0
1
1
1
0
0
0

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

   

x = wall street vets raise concerns 

about interest rates , politics

x    = pet 's best interest in mind , but 

vets must follow law

    turns the input into a table of  
features with real values (often 
binary: 0 or 1). 

   

in practice: define feature templates 
like    leftword=       from which specific 
features are instantiated

12

linear model

    for each input x (e.g., a document or word token), let   (x) be 

a function that extracts a vector of  its features. 
    features may be binary (e.g., capitalized?) or real-valued (e.g., 

#word=debt). 

    each feature receives a real-valued weight parameter w . each 

candidate label y    is scored for the token by summing the 
weights for the active features: 
t  (x)   

   wy   
=   j wy   ,j      j(x) 

    for binary classification, equivalent to: sign(wt  (x))     +1 or    1

13

bias

capitalized?
#wordsbefore
#wordsafter
relativeoffset
leftword=about
leftword=best
rightword=rates
rightword=in

wall
street
vets
best
in

wall street
street vets
vets raise

  (x)
1
0
6
3
0.6
66
1
0
1
0
1
1
1
0
0
1
1
1

   

w   (x   )

1
   3.00
.22
0
3
   .01
.01  8
1.00  0.2
7
.00  0
1
   2.00
0
5.00
   1.00
1
1.00  0
   1.00
0
1
   .05
   1.00
1
1
   .01
4.00
0
.00  0
.00  0

x = wall street vets raise concerns 

about interest rates , politics

x    = pet 's best interest in mind , but 

vets must follow law

    weights are learned from data 

    for the moment, assume 

binary classification: financial 
or nonfinancial 
    more positive weights more 

indicative of  financial. 

    wt  (x) = 6.59, wt  (x   ) =    6.74

14

more then 2 classes

    simply keep a separate weight vector for each 

class: wy 

    the class whose weight vector gives the highest 

score wins!

15

learning the weights

    weights depend on the choice of  model and learning algorithm. 

    na  ve bayes fits into this framework, under the following estimation procedure for w: 

    wbias = log p(y) 
        features f: wf = log p(f | y) 
      j wj      j(x) = wbias       +   f       (x) wf    
                    = log p(y) +   f       (x) log p(f | y)   
                    = log (p(y)      f       (x) p(f | y)) 

    however, the na  ve independence assumption   that all features are conditionally 

independent given the class   can be harmful. 

    could the weights shown on the previous slide be na  ve bayes estimates? 

    no, because some are positive (thus not log-probabilities). other kinds of  learning 

procedures can give arbitrary real-valued weights. 

    if  using log probabilities as weights, then the classification threshold should be 

equivalent to id203 of  .5, i.e. log .5.

16

linear classi   ers: geometric view

x

c

y

17

linear classi   ers: geometric view

d

e

cisio
n b

u

o

u

 : 
w   
 
=
 
0

n

d

u

a
r
y

x

w

c

y

18

linear classi   ers: geometric view

u

 
:
 

w

x

   

w
u
=

 

 

0

c

y

19

linear classi   ers: geometric view

u

 
:
 

x

   

w
u
=

 

 

0

c

y

20

linear classi   ers (> 2 classes)

return   
arg maxy wy      (x)

x

c

y

21

the term    feature   

    the term    feature    is overloaded in nlp/ml. here are 

three different concepts: 
    linguistic feature: in some formalisms, a symbolic property 

that applies to a unit to categorize it, e.g. [   voice] for a 
sound in phonology or [+past] for a verb in morphology. 
    percept (or input feature): captures some aspect of an  

input x; binary- or real-valued. [the term    percept    is 
ends in -ing
nonstandard but i think it is useful!] 

    parameter (or model feature): an association between some 
percept and an output class (or structure) y for which a real-
ends in -ing    y=verb
valued weight or score is learned.

22

