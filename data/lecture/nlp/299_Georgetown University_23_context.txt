context in language 

lecture 23 
processing
nathan schneider   
enlp | 25 april 2018

   
ai ambitions

semantic analysis

    we   ve seen tasks that analyze the meanings or topics of 

documents, words, and sentences 
    document classi   cation 
    topic models 
    word representations & similarity 
    id51 
    id14 

    these are challenging tasks. but even if we could 

automate them perfectly, we   d still be a long way from    
human-like automatic language processing.

understanding: 
beyond semantics

5

what is required to 

understand this conversation?

6

    semantics of the 

expressions themselves
    coffee refers to the drink, 
not the tree or bean (wsd) 
    4:00 and 3:00 are times 
(ner) 
       at 4:00   : semantic role 
marking the time of an event 
       ?    indicates question 
    but there   s a lot more to 

understanding than just the 
explicit language   .

7

    encyclopedic knowledge 

about the world
    nobody would think this 
means    does coffee exist at 
4:00?    we know about 
social activities associated 
with coffee. 
    likely 4:00pm, because 
people are normally asleep 
at 4:00am. (and people 
generally don   t go for coffee 
in their sleep.) unless   .

8

    knowledge of the situation/

conversational context/
common ground
    perhaps it   s 2:30am and 
we   re working to    nish 
something for a deadline. or 
we both are back from a 
conference and are severely 
jetlagged. 
    perhaps we have a habit of 
going to a certain place for 
coffee, so it can be left 
implicit.

9

    discourse coherence
    we normally assume that 
interlocutors are 
   cooperative    (h.p. grice): 
they respond with relevant 
information, say what they 
believe to be true, don   t 
change topics without 
suitable pause or warning, 
etc. 
    here, we interpret the 
second question as 
proposing an alternative 
time, and requesting 
con   rmation.

10

    relationship to action

    a truly intelligent app would offer 
information that would help my 
decision (e.g., when the caf   
closes) 

    and put the event on my calendar 

at the agreed-upon time 

    and remind me to leave in time to 

arrive at the agreed-upon 
meeting place at that time. 

    if it is unsure of details, it should 
con   rm with me rather than do 
the wrong thing. 

    industry is already moving in this 
direction with personal assistants.

11

understanding:    

it   s not just the words

    actually understanding such conversations 
requires a lot of id136s based on world 
knowledge and context (pragmatics). 

    but is that only true of conversations? what about 

unidirectional language use (books, articles)?

12

understanding:    

it   s not just the words

what is said

what is understood   

13

   
   
   
   
   
   
   
understanding:    

it   s not just the words

    actually understanding such conversations 
requires a lot of id136s based on world 
knowledge and context (pragmatics). 

    but is that only true of conversations? what about 

unidirectional language use (books, articles)?

14

   sherwood park had its third    re in less than a month on tuesday.  however, there 
were no injuries    (http://www.sherwoodparknews.com/2016/01/14/no-injuries-in-
park-   re) 
    semantics: sherwood park is a neighborhood (not a literal park); this    re is no 

longer active 

    discourse + world knowledge:  

    no humans injured in this    re (unknown whether any ants were harmed) 
       however    signals a contrast with an expectation raised by the    rst 

sentence: injuries might have been expected from an unintentional    re 
    harm to humans is highly newsworthy, so it   s important for the story to 

inform us of an event that didn   t occur 

    likely id136: there is a pattern of    res in sherwood park (why?) 
    were there injuries in previous    res? unspeci   ed. 
    what would have to change for the information to be presented in the 

opposite order?

15

understanding:    

it   s not just the words

    different aspects of meaning are required to be explicit in different 

languages. e.g., lexicalization patterns in hebrew vs. english:

16

eish     

srefa           

understanding:    

it   s not just the words

    different aspects of meaning are required to be explicit in different 

languages. e.g., lexicalization patterns in hebrew vs. english: 
    en       re    (cid:15511) he {eish    purposeful    re   , srefa    destructive    re   } 
    en {   color   ,    paint   } (cid:15511) he tseva 

    formality/social status: which 2nd person pronoun to use in 

german or french? 

    evidentiality: how does the speaker know the information? 

(directly observed, secondhand, etc.) 

    spatial systems: absolute (compass directions) or relative

19

understanding:    

it   s not just the words

    some information can be made    minimally explicit   , requiring 

discourse-level id136. 

    anaphora (pronouns): he sells the greatest soup you   ve ever 

eaten.  
    need to decide which pronouns are referential, and resolve 

their antecedents. 

    special case of coreference resolution (grouping referring 

expressions that indicate the same entity). 

    pro-drop: in many languages, pronominal subjects can be 

dropped (verb agreement helps disambiguate): quiero un taco.

20

perspective in language

    the choice of language can put a    spin    on the 

information being conveyed, emphasizing certain 
nuances or dimensions of meaning. sometimes 
called construal. 

    may indicate a social perspective (framing) 

    mistakes were made. 
       thrifty    vs.    stingy    
       terrorists    vs.    freedom    ghters    

    may be mundane and subtle: on the bus vs. in the bus

21

understanding:    
it   s complicated

    lots of implicit information, even in expository text. 

    how to even evaluate whether a system is comprehending 

the story? 
    give the system an exam   multiple choice or    ll-in-the-blank. 
challenge datasets based on actual exam questions (reading 
comprehension, mathematical reasoning, biology). 

    test the system   s decision-making skills, such as controlling a 
robot or making moves in a game based on language. requires 
link between comprehension and action/grounding. 

    multimodal: link text to image, video, or action.

22

automatic caption 

generation

figure 5. a selection of evaluation results, grouped by human rating.

http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/

4.3.7 analysis of embeddings
in order to represent the previous word st 1 as input to
the decoding lstm producing st, we use id27
vectors [22], which have the advantage of being indepen-
dent of the size of the dictionary (contrary to a simpler one-
hot-encoding approach). furthermore, these word embed-
dings can be jointly trained with the rest of the model. it

word
vinyals et al., cvpr 2015   
car
boy
street
horse
computer

vinyals_show_and_tell_2015_cvpr_paper.pdf 

neighbors
van, cab, suv, vehicule, jeep
toddler, gentleman, daughter, son
road, streets, highway, freeway
pony, donkey, pig, goat, mule
computers, pc, crt, chip, compute

table 6. nearest neighbors of a few example words

language-directed robot 

navigation

https://www.youtube.com/watch?
v=7nuq28utugm&list=pl6syoj2z5jwfbfhzqdxf_luq
-sgpdxao4&index=1

summary

    the techniques discussed in this course were aimed at classifying 

documents, or analyzing words and sentences. 

    but much of human language exploits our awareness of discourse, 

pragmatics, perspectives, other modalities, and the world. 

    different languages have different requirements for what needs to 

be explicit. 

    tasks like coreference resolution, automatic id134, 

and language-directed robot tasks are important for stimulating 
research along dimensions beyond local and explicit linguistic 
communication.

