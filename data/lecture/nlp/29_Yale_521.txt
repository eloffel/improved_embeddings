nlp
introduction to nlp
support vector machines
which boundary is best?

which boundary is best?

does a new data point change your answer?

which boundary is best?

what if the new data point was here instead?

support vector machines
find the decision boundary
as a hyperplane that maximize the so-called    margins    (space around the hyperplane with no examples in it)
main advantages
theoretically well founded (structural risk minimization)
certain guarantee of generalization ability
main problems
slow training (quadratic programming)
need many examples
kernel design 

feature mapping
in many cases, data are not linearly separable, so we map the original feature space to some higher-dimensional feature space where the training set is separable
slide from ray mooney
feature mapping example
(mapping to a higher-dimensional space)
kernels and id166s
some well-known kernels
polynomial kernel:
sigmoid kernel:
rbf kernel:
many other kernels are useful for ir:
e.g., string kernels, subsequence kernels, tree kernels, etc.
nlp
