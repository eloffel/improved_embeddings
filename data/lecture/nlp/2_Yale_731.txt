nlp
libraries for deep learning
deep learning
id127 in python
http://stackoverflow.com/questions/10508021/matrix-multiplication-in-python
id127 in numpy
libraries for deep learning
torch (lua): 
http://torch.ch/
pytorch (python)
http://pytorch.org/
tensorflow (python and c++):
https://www.tensorflow.org/
theano (python)
http://deeplearning.net/software/theano/ 
no longer maintained
keras, paddlepaddle, cntk

libraries for deep learning: tensorflow
(slides by jason chu)
deep learning
what is tensorflow?
open source software library for numerical computation using data flow graphs
developed by google brain team for machine learning and deep learning and made open-source
tensorflow provides an extensive suite of functions and classes that allow users to build various models from scratch


these slides are adapted from the following stanford lectures:
https://web.stanford.edu/class/cs20si/2017/lectures/slides_01.pdf
https://cs224d.stanford.edu/lectures/cs224d-lecture7.pdf

what   s a tensor?
formally, tensors are multilinear maps from vector spaces to the real numbers
think of them as n-dimensional array, with 0-d tensors being scalars, 1-d tensor vectors, 2-d tensor matrices, etc

some basic terminology
dataflow graphs: entire computation
data nodes: individual data or operations
edges: implicit dependencies between nodes
operations: any computation 
constants: single values (tensors)


   tensorflow programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph.    - tensorflow docs

 
all nodes return tensors, or higher-dimensional matrices
you are metaprogramming. no computation occurs yet!

data flow graphs
import tensorflow as tf

a = tf.add(2, 3)

tf automatically names nodes
if you do not

x = 2
y = 3

print a
>> tensor("add:0", shape=(), dtype=int32)

note: a is not 5


a
tensorflow session
session object encapsulates the environment in which operation objects are executed and tensor objects, like a in the previous slide, are evaluated

import tensorflow as tf
a = tf.add(2, 3)
with tf.session() as sess:
print sess.run(a)

tensorflow sessions
there are 3 arguments for a session, all of which are optional.
target         the execution engine to connect to.
graph         the graph to be launched.
config         a configproto protocol buffer with configuration options for the session


tensorflow variables
   when you train a model you use variables to hold and update parameters. variables are in-memory buffers containing tensors    - tensorflow docs.
tensorflow variables must be initialized before they have values
placeholders and feed dictionaries
you can input data from numpy using tf.convert_to _tensor, but not scalable
use tf.placeholder variables (dummy nodes that provide entry points for data to computational graph)
a feed_dict is a python dictionary mapping from tf. placeholder vars (or their names) to data (numpy arrays, lists, etc.)

input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
output = tf.mul(input1, input2)
with tf.session() as sess:
	print(sess.run([output], feed_dict={input1:[7.], input2:[2.]})))
variable scope
tf.variable_scope() provides simple name-spacing to avoid clashes of variables

with tf.variable_scope("foo"):
	with tf.variable_scope("bar"):
		v = tf.get_variable("v", [1])
assert v.name == "foo/bar/v:0   

tf.get_variable() creates/accesses variables from within a variable scope.
tf.get_variable_scope().reuse_variables()


id75 example
id75 example
id75 example
id75 example
computation graphs in tensorflow
homework 4
some useful functions:
tf.expand_dims(input, axis=none,name=none,dim=none)
inserts a dimension of 1 to a tensor   s shape
t is tensor of shape [2], tf.shape(tf.expand_dims(t, 0)) -> t becomes [1 , 2]
tf.gather(params,indices,validate_indices=none,name=none,axis=0)
gathers the elements at the passed-in indices of the given axis of params
x = [ 1,2,3,4,3,2,1 ] 
tf.gather(x, 3).eval()
more functions
tf.reduce_sum(input_tensor, axis=none, keepdims=none, name=none, reduction_indices=none, keep_dims=none)
computes the sum of elements across dimensions of a tensor
axis: the dimensions to reduce. if  none  (the default), reduces all dimensions. must be in the range  [-rank(input_tensor), rank(input_tensor))
ex: x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x)   # 6
tf.reduce_sum(x, axis=0)   # [2, 2, 2]
tf.reduce_sum(x, axis=1)   # [3, 3]
x = tf.constant([[1, 1, 1], [1, 1, 1]])
tf.reduce_sum(x) -> 6
tf.matmul(a, b,transpose_a=false,transpose_b=false,adjoint_a=false,
adjoint_b=false, a_is_sparse=false, b_is_sparse=false, name=none)
multiplies matrix a by b

tf.nn module
provides functions for neural network support
tf.nn.l2_loss(t, name=none): computes half the l2 norm of a tensor without the  sqrt
tf.nn.relu(features, name=none): computes rectified linear unit (relu) activation function; f(x)=max(0,x)
tf.nn.sparse_softmax_cross_id178_with_logits(_sentinel=none, labels=none, logits=none, name=none)
computes sparse softmax cross id178 between  logits  and  labels; measures the id203 error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class)

https://www.tensorflow.org/api_docs/python/tf/nn
tf.train module
module for training support; choose an optimizer to perform optimization; many different types of optimizer
class adamoptimizer
optimizer that implements the adam algorithm
adam alg can be found here: https://arxiv.org/pdf/1412.6980v8.pdf
class gradientdescentoptimizer
 implements the id119 algorithm
calling optimizer.minimize() will return an operation (computation) object
adam alg allows it to use a larger step size than gdoptimizer, so it will converge to that step size without a lot of tuning, but it requires more computation and more state/storage
tf.argmax()
tf.argmax(
      input,
      axis=none,
      name=none,
      dimension=none,
      output_type=tf.int64
)

returns the index with the largest value across axes of a tensor
libraries for deep learning: pytorch
(slides by rui zhang)
deep learning
pytorch tensor
import torch

mat1=torch.randn(2,3)
mat2=torch.randn(3,3)

print mat1
print mat2



id127 in pytorch
import torch

mat1=torch.randn(2,3)
mat2=torch.randn(3,3)
res=torch.mm(mat1,mat2)

print res.size()

output:
(2l, 3l)


batch id127 in pytorch
import torch

batch1=torch.randn(10,3,4)
batch2=torch.randn(10,4,5)
res=torch.bmm(batch1,batch2)

print res.size()

output:
(10l, 3l, 5l)

many tensor operations in pytorch      
torch.mm
id127
torch.bmm
batch id127
torch.cat
tensor concatenation
torch.sqeueeze/torch.unsqueeze
change tensor dimensions
   ..
   ..

check documentation at http://pytorch.org/docs/master/torch.html#tensors
pytorch variables
import torch
from torch.autograd import variable

#pytorch tensor
x = torch.ones(2,2)
y = torch.ones(2,1)
w = torch.randn(2,1)
b = torch.randn(1)

#pytorch variable
x = variable(x, requires_grad=false)
y = variable(y, requires_grad=false)
w = variable(w, requires_grad=true)
b = variable(b, requires_grad=true)

a pytorch variable is a wrapper around a pytorch tensor, and represents a node in a computational graph
computational graphs
 
# computational graph
p_1 = torch.sigmoid(torch.mm(x, w) + b) # prediction
xent = -y * torch.log(p_1) - (1-y) * torch.log(1-p_1) # cross-id178 loss
cost = xent.mean() # the cost to minimize

automatic gradient computation
 
# computational graph
p_1 = torch.sigmoid(torch.mm(x, w) + b) # prediction
xent = -y * torch.log(p_1) - (1-y) * torch.log(1-p_1) # cross-id178 loss
cost = xent.mean() # the cost to minimize

cost.backward()

print w.grad
print b.grad

build neural networks using pytorch
neural networks can be constructed using the  torch.nn  package.

forward
an  nn.module  contains layers, and a method  forward(input) that returns the  output
you can use any of the tensor operations in the  forward  function

backward
nn  depends on  autograd  to define models and differentiate them
you just have to define the  forward  function, and the  backward  function (where gradients are computed) is automatically defined for you using  autograd
define a network class
you don   t need to define a backward function!
id98 for mnist: a full example
example from http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html
define a id98 network class
compute loss
input is a random image
target is a dummy label
id26
use torch.optim package to do id26
links about deep learning
aan: our search engine for resources and papers
http://tangra.cs.yale.edu/newaan/ 
richard socher   s stanford class
http://cs224d.stanford.edu/
libraries for deep learning: theano
(slides by rui zhang)
	(for reference only)
deep learning
id127 in theano
import theano
import theano.tensor as t
import numpy as np

#    symbolic    variables
x = t.matrix('x')
y = t.matrix(   y   )
dot = t.dot(x, y)


id127 in theano
import theano
import theano.tensor as t
import numpy as np

#    symbolic    variables
x = t.matrix('x')
y = t.matrix(   y   )
dot = t.dot(x, y)



#this is the slow part
f = theano.function([x,y],[dot])

#now we can use this function
a = np.random.random((2,3))
b = np.random.random((3,4))
c = f(a, b) #now a 2 x 4 array
sigmoid in theano
in = t.vector(   in   )
sigmoid = 1 / (1 + t.exp(-in))
#same as t.nnet.sigmoid
sigmoid = t.nnet.sigmoid(x)


shared variables vs symbolic variables
# this is symbolic
x = t.matrix('x')

#shared means that it is not symbolic
w = theano.shared(np.random.randn(n))
b = theano.shared(0.)



computational graph
# this is symbolic
x = t.matrix('x')
#shared means that it is not symbolic
w = theano.shared(np.random.randn(n))
b = theano.shared(0.)

# computational graph
p_1 = sigmoid(t.dot(x, w) + b)
xent = -y * t.log(p_1) - (1-y) * t.log(1-p_1) # cross-id178
cost = xent.mean() # the cost to minimize

automatic gradient computation
p_1 = sigmoid(t.dot(x, w) + b)

xent = -y * t.log(p_1) - (1-y) * t.log(1-p_1) # cross-id178

cost = xent.mean() # the cost to minimize

gw, gb = t.grad(cost, [w, b]) 


compile a function
train = theano.function(
          inputs=[x,y],
          outputs=[prediction, xent],
          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))

computation graphs in theano
lstm id31 demo
if you   re new to deep learning and want to work with theano, do yourself a favor and work through http://deeplearning.net/tutorial/
a lstm demo is described here: http://deeplearning.net/tutorial/lstm.html
id31 model trained on imdb movie reviews

lstms: one time step
~
[slides from catherine finegan-dollak]
lstms: building a sequence
the
cat
sat
on
   
theano implementation of an lstm step
(lstm.py, l. 174) 
def _step(m_, x_, h_, c_):
        preact = tensor.dot(h_, tparams[_p(prefix, 'u')])
        preact += x_

        i = tensor.nnet.sigmoid(_slice(preact, 0, options['dim_proj']))
        f = tensor.nnet.sigmoid(_slice(preact, 1, options['dim_proj']))
        o = tensor.nnet.sigmoid(_slice(preact, 2, options['dim_proj']))
        c = tensor.tanh(_slice(preact, 3, options['dim_proj']))

        c = f * c_ + i * c
        c = m_[:, none] * c + (1. - m_)[:, none] * c_

        h = o * tensor.tanh(c)
        h = m_[:, none] * h + (1. - m_)[:, none] * h_

        return h, c

theano.scan iterates through a series of steps
rval, updates = theano.scan(_step,
   sequences=[mask, state_below],
   outputs_info=[tensor.alloc(numpy_floatx(0.),
                  n_samples, dim_proj),
                 tensor.alloc(numpy_floatx(0.),
                  n_samples, dim_proj)],
   name=_p(prefix, '_layers'),
   n_steps=nsteps)

(lstm.py, l. 195) 

nlp
