empirical methods in 
nlp 

coreference resolution 

amir zeldes 
amir.zeldes@georgetown.edu  

what is coreference? 

1 

 
 
 
 
 

    what spans are candidates? 
    what counts as coreference? 
    how to do this automatically? 

why coreference resolution? 

2 

general premise: 
    reduce ambiguity     every pronoun replaceable by 

lexical np 

    theoretical models of discourse comprehension     
how do humans know what we   re talking about? 

    in practice:  

    enable information extraction (ir, summarization) 
    better input data for entity sensitive tasks (e.g. mt) 
    id86 / referring expression generation (qa) 
 

what kind of task is this? 

3 

    step 1: 

    identify referring expressions 

    step 2     two variants: 

    a: perform id91 into entities 
    b: perform linking of anaphor-antecedent pairs 
 

    assumptions: 

    referentiality is binary (referring/non-referring) 
    id91: coreference is transitive? a <- b <- c |= a <-c 
    linking: coreference always    points backward   ? 

mention detection 

4 

    na  ve approach:  

    use a parser (which? errors now inevitable?) 
    take all nps  - recall oriented (what about    on the 

other hand   ?) 

    let   s try an easy one     how many mentions? 

where do they start and end? 

 

    new zealand begins process to consider 

changing national flag design 
 
 
 

mention detection 

5 

    na  ve approach:  

    use a parser (which? errors now inevitable?) 
    take all nps  - recall oriented (what about    on the 

other hand   ?) 

    harder example (ontonotes corpus)     which 

nps are referring expressions? 

 

    if [any part of [the matter]] were in [[my] hand], [no 

eye] would have read [it] and [no passerby] would 
have come across [it] 
 

mention detection 

6 

    are nps enough? where are the borders? 

mention detection 

7 

    parser input is still the most common approach 
    but recently, considering and ranking any span of 

tokens up to length k has been proposed 
(lee et al. 2017) 

    advantage: 

    possible to identify unusual spans from training data 
    potentially better at verb event coreference 

    disadvantage: 

    possible spurious spans (hard to rule out    blunders   ) 
    can   t capitalize on larger training data for parsing 

coreference 

8 

    what phenomena should be included? 
    easy! group cases of same entity in the world 

    pronominal np anaphora: kim says she   . 
    lexical coref: aamir khan     this indian actor     
    apposition: shinzo abe, the japanese premier 
    cataphora: in her speech the chairwoman said 
    event anaphora: ben visited rome     the visit 
    sense anaphora: don   t you like beer? yes, i   ll have one 
    bridging: looking at mexico, they said the economy ... 

annotation schemes and consequences 

9 

    guidelines and goals still debated 
    many discussions begin with the ace corpora 

(doddington et al. 2004) 

    current de facto standard:  

ontonotes (hovy et al. 2006) 

this is what 
you get from 

corenlp, spacy 

    but many in between (e.g. arrau, poesio & 

artstein 2008; gum, see discussion in zeldes & 
zhang 2016) 
 

ontonotes - indefinites 

10 

    biggest points of contention: 
no antecedents for indefinites (bbn 2007, 4) 
[parents]x should be involved with their children's education 
at home, not in school. [they]x should see to it that [their]x 
kids don't play truant; [they]x should make certain that the 
children spend enough time doing homework; [they]x should 
scrutinize the report card. [parents]y are too likely to blame 
schools for the educational limitations of [their]y children. if 
[parents]z are dissatisfied with a school, [they]z should have 
the option of switching to another. 

ontonotes - predication 

11 

    no predicatives, no    as    phrases: 

[george] was [the king] and was treated as [the monarch] 
    relations should be derivable from syntax but: 

    not all corpora have gold syntax 
       as    can be ambiguous 
    negation, modality    

    sometimes counter-intuitive: 

    it was a beetle! (no markup whatsoever in ontonotes) 
    milisanidis scored 9.687     it was the best result for greek 

gymnasts since they began taking part in gymnastic internationals. 
(markup, but only pronouns! cf. lee et al. 2013) 
    markup catches less interesting mention: what was best for greek gymnasts? 

ontonotes - coordination 

12 

    no coordination envelope without aggregate 

mention: 
   

[the us] and [japan] ... [the us] and [japan] 
 
[[the us] and [japan]] ... [they] 

 
 

   difficult for coreferencer to make local decision on 

coordinate mention 
 

ontonotes - apposition 

13 

    apposition envelope:  

    a peculiarity of ontonotes     appositions are a 

separate entity reference: 

ontonotes 

gum 

ontonotes - i within i 

14 

    ontonotes forbids nested mention coreference 

    he has in tow [his prescient girlfriend, whose sassy 

retorts mark [her]    ] (not annotated!) 

    but external reference to embedded mentions 

is possible: 
    [the american administration who planned carefully 

for this event through experts in media and public 
relations, and [its] tools]     have caught [them] by 
surprise (all three linked!) 

compound modifiers 

15 

    only proper noun modifiers are included: 

    [hong kong] government     [hong kong] (annotated) 

    no annotation for: 

    small investors seem to be adapting to greater [stock 

market] volatility     glenn britta     says he is 
   factoring    [the market   s] volatility    into investment 
decisions.    
 

      same entity in the world    is not so simple    

 

antecedent detection 

16 

    after mention detection, check for every referring 

expression: 
    given or new?  
    if new: singleton? (recasens et al. 2013) 
    if given: what is the antecedent? 

    id91 approach: (lee et al. 2013, clark & manning 2016) 
    add best guess to cluster, recalculate next best guess 

    mention pair/ranking approach: (durrett & klein 2013, lee 

et al. 2017) 

    and in between (wiseman et al. 2015, zeldes & zhang 2016) 

mention pair approach 

17 

    apply binary classification    anaphoricity ranking 

(durret & klein 2013, lee et al. 2017) 
 
 
 
 
 

    fast, simple (e.g. loglinear models) 
    but: global chain constraints missed 

durrett & klein 

id91 approach 

18 

    score all possible matches and make best decision 

first (e.g. clark & manning 2015) 

    share features for all clustered mentions 
    what will happen here? 

    mr. clinton     clinton     ms. clinton     she     clinton 

 

    and here? 

    georgetown is a university in dc. george washington 

university, the closest university to it in the city, is also the 
largest in the district. both universities offer 
undergraduate and graduate degrees. 

candidate selection 

19 

    classic approach    smash    (cf. kehler 2008): 

    search match and select using heuristic 

    basic idea, for each anaphor: 

    search through all previous mentions 
    perform feature matching (esp. morphological 

agreement: gender, number) 

    discard incompatible mentions 
    select best candidate (good baseline: most recent) 

sieve approach 

20 

    used e.g. in corenlp d-coref (lee et al. 2013) 

problems 

    precision oriented: 

21 

    notional agreement: [the new zealand  government] 

announced the start of a process to determine 
whether [their] citizens (zeldes, to appear) 

    verbal coreference/event anaphora 
    uphill semantics battle (durrett & klein 2013) 
    synonymy: [this novel idea] == [the new approach] 
    antonymy: [the good news] != [the bad news] 
    semantic compatibility: [the gold medalist] .. [this athlete] 
    world knowledge: [the woman in the window]     [the 

recent new york times bestseller] 

 

knowledge-base approaches 

22 

    huge knowledge bases exist (curated and 

scraped): dbpedia, freebase, yago, conceptnet, 
ppdb    
 
 
 
 

    what do we need to know for coref? 

pure machine learning approaches 

23 

end-to-end corpus training (lee et al. 2017) 
    top of the line because: 

    consider    all possible features   ? 
    simple (but slow to train) 
    best results on (homogeneous) test set 

    but: 

    large corpora unavailable for most languages 
    no way of integrating novel facts 
    risk of overfitting style, period, other irrelevant 

properties 

lee et al. 2017 

24 

    nb: all spans up to length k, within sentence 

are considered 

    lstm learns sentence-wise 

lee et al. 2017 

25 

    syntactic heads are not explicitly learned 
    attention mechanism learns something very 

similar 

hard to rule 

out    

evaluation 

26 

    reference scorer implemented by pradhan et al. 

(2014) 

    three main metrics: 

    muc (vilain et al. 1995) 
    b3 (bagga & baldwin 1998) 
    ceafe (luo 2005) 

example from pradhan et al. (2014) 

27 

 

scoring 

28 

    muc     precision and recall for links in gold entities 

    link based 

    b3     mention based     each mention in a gold 

entity gets credit based on ratio of correct 
mentions in its predicted entity 

    ceafe     entity based     calculate best scoring 

alignment of gold and predicted entities, then get 
proportion of correct and incorrect links in each 
entity 
    other metrics: ceafm, blanc (recasens & hovy 2011, 

luo et al. 2014) 

 

  

finding the culprit     p-link 

29 

    use partitioned version of link-based score 

(zeldes & simonson 2016; extension of martschat et al. 2015) 
    each segment type accumulates credit  

(or blame) 

    precision and recall in terms of 

correct link end points per partition 
 

    )

        

 

                           ,     =

 

        
    =1
 

        
    =1

             (        
         1
        

                           ,     =

 

        
    =1
 

                (        
        
         1
        

        
    =1

    )

 

 

a 

b 

c 

d 

g 

e 

decl 

wh 

f 

h 

i 

gold entity 

predicted 
entity 

implementation available: 
https://github.com/amir-zeldes/reference-coreference-scorers  

recent criticism 

30 

    moosavi & strube (2016) point out inconsistent 

behavior of metrics 

    it is possible to construct cases where one 

metric improves while another degrades 

       mean of three bad metrics does not make a 

good one    
 
 

    see http://www.aclweb.org/anthology/p16-1060.pdf  

