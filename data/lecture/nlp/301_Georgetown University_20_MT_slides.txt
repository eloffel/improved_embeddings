lecture 20 

machine translation

nathan schneider   

(with slides by philipp koehn, marine carpuat,    

chris dyer)   

enlp | 16 april 2018

   
a clear plan

interlingua

5

lexical transfer

source

target

philipp koehn

machine translation

28 january 2016

a clear plan

interlingua

6

syntactic transfer

lexical transfer

analysis
source

g

e

n

eratio

n

target

philipp koehn

machine translation

28 january 2016

a clear plan

interlingua

semantic transfer

syntactic transfer

lexical transfer

analysis

source

7

g

e

n

eratio

n

target

philipp koehn

machine translation

28 january 2016

a clear plan

interlingua

8

analysis

semantic transfer

syntactic transfer

lexical transfer

g

e

n

eratio

n

source

target

philipp koehn

machine translation

28 january 2016

evaluation

problem: no single right answer

32

israeli of   cials are responsible for airport security.
israel is in charge of the security at this airport.
the security work for this airport is the responsibility of the israel government.
israeli side was in charge of the security of this airport.
israel is responsible for the airport   s security.
israel is responsible for safety work at this airport.
israel presides over the security of the airport.
israel took charge of the airport security.
the safety of this airport is taken charge of by israel.
this airport   s security is the responsibility of the israeli security of   cials.

philipp koehn

machine translation

28 january 2016

human evaluation

    manually score or rank candidate translations 

    e.g., for    uency (target language grammaticality/

naturalness) and adequacy (respecting the meaning of the 
source sentence)

human evaluation

    manually score or rank candidate translations 

    e.g., for    uency (target language grammaticality/

naturalness) and adequacy (respecting the meaning of the 
source sentence)

    manually edit the system output until it is an acceptable 

reference translation (hter = human translation edit rate) 
    insertions, substitutions, deletions, shifts (moving a word or 
phrase) 

    then measure # edits / # words in reference (i.e., 1     recall)

automatic evaluation

9

    why automatic id74?

    manual evaluation is too slow
    evaluation on large test sets reveals minor improvements
    automatic tuning to improve machine translation performance

    history

    word error rate
    id7 since 2002

    id7 in short: overlap with reference translations

philipp koehn

emnlp lecture 14

21 february 2008

automatic evaluation

10

    the gunman was shot to death by the police .

    reference translation
    system translations

    the gunman was police kill .
    wounded police jaya of
    the gunman was shot dead by the police .
    the gunman arrested by police kill .
    the gunmen were killed .
    the gunman was shot to death by the police .
    gunmen were killed by police ?sub>0 ?sub>0
    al by the police .
    the ringer is killed by the police .
    police killed the gunman .

    matches

    green = 4 gram match (good!)
    red = word not matched (bad!)

philipp koehn

emnlp lecture 14

21 february 2008

automatic evaluation

11

    id7 correlates with human judgement

    multiple reference translations may be used

[from george doddington, nist]

philipp koehn

emnlp lecture 14

21 february 2008

29

what is it good for?

philipp koehn

machine translation

28 january 2016

30

what is it good enough for?

philipp koehn

machine translation

28 january 2016

quality

33

hter

assessment

publishable

editable

gistable

triagable

0%

10%

20%

30%

40%

50%

(scale developed in preparation of darpa gale programme)

philipp koehn

machine translation

28 january 2016

applications

34

hter

assessment

application examples

seaid113ss bridging of language divide

publishable automatic publication of of   cial announcements

editable

gistable

triagable

increased productivity of human translators
access to of   cial publications
multi-lingual communication (chat, social networks)
information gathering
trend spotting
identifying relevant documents

0%

10%

20%

30%

40%

50%

philipp koehn

machine translation

28 january 2016

current state of the art

35

hter

assessment

language pairs and domains

0%

10%

20%

30%

40%

50%

publishable

editable

french-english restricted domain
french-english technical document localization
french-english news stories

english-german news stories
english-czech open domain

gistable

triagable

(informal rough estimates by presenter)

philipp koehn

machine translation

28 january 2016

machine translation

cmsc 723 / ling 723 / inst 725

marine carpuat
marine@cs.umd.edu

today: an introduction
to machine translation

    the id87 decomposes machine 

translation into
    word alignment
    id38

    how can we automatically align words within 

sentence pairs? we   ll rely on:
    probabilistic modeling

    ibm1 and variants [brown et al. 1990]

    unsupervised learning 

    expectation maximization algorithm

machine translation as a 
id87

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

in particular, we are interested in the words spring and kl kl kl kl , each of which is ambiguous, 
and translate differently in different cases.  
the flowers bloom in the spring. 
kilya\ vsnt me' i%lti h  ' 3 
sita came yesterday. 
sita kl aayi qi 3 
the gymnast makes springing up to the bar look easy. 
ksrtbaj @'@e ke   pr se kudne ke kayr ko aasan bna deta hw 3 
it rained yesterday. 
kl bairx hu   qi 3 
school will commence tomorrow. 
iv  aly kl se aarm. hoga 3 
with a spring the cat reached the branch. 
vh iblli ek $hni pr kud gyi 3 
i will come tomorrow. 
m  ' kl aa  \ga 3 
the train stopped, and the child sprang for the door and in a twinkling was gone. 
relga;i ke   kte hi bcca drvaje se kudkr rfuc  r ho gya 3 

rosetta stone

egyptian
hieroglyphs

demotic

greek

warren weaver (1947)

when i look at an article in 

russian, i say to myself: this is 
really written in english, but it 
has been coded in some strange 
symbols.  i will now proceed to 

decode.

weaver   s intuition formalized 
as a id87

    translating a french sentence f is finding the 

english sentence e that maximizes p(e|f)

    the id87 breaks down p(e|f)

into two components

translation model 
& word alignments

    how can we define the translation model p(f|e) 

between a french sentence f and an english 
sentence e?

    problem: there are many possible sentences!

    solution: break sentences into words

    model mappings between word position to represent 

translation

    just like in the centauri/arcturian example

probabilistic models of 
word alignment

defining a probabilistic model

for word alignment

id203 lets us
1) formulate a model of pairs of sentences
2) learn an instance of the model from data
3) use it to infer alignments of new inputs

recall id38

id203 lets us
1) formulate a model of a sentence

e.g, bi-grams

2) learn an instance of the model from data

3) use it to score new sentences

how can we model p(f|e)?
    we   ll describe the word alignment models 

introduced in early 90s at ibm

    assumption: each french word f is aligned to 

exactly one english word e
    including null

word alignment 

vector representation 

    alignment vector a = [2,3,4,5,6,6,6]

    length of a = length of sentence f
    ai = j if french position i is aligned to english position j

word alignment 

vector representation 

    alignment vector a = [0,0,0,0,2,2,2]

how many possible alignments?
    how many possible alignments for (f,e) where

    f is french sentence with m words
    e is an english sentence with l words

    for each of m french words, we choose an 
alignment link among (l+1) english words

    answer: (    +1)    

formalizing the connection 

between word alignments & the 

translation model

    we define a conditional model

    projecting word translations
    through alignment links

ibm model 1: generative story

    input

    an english sentence of length l
    a length m

    for each french position     in1..m

    pick an english source index j

    choose a translation 

ibm model 1: generative story

    input

    an english sentence of length l
    a length m

    for each french position     in1..m

alignment is based on 
word positions, not 
are uniform

alignment  probabilities 
word identities

    pick an english source index j

    choose a translation 

words are translated 

independently

ibm model 1: parameters

    t(f|e) 

    word translation 
id203 table

    for all words in french 

& english vocab

ibm model 1: generative story

    input

    an english sentence of length l
    a length m

    for each french position     in1..m

    pick an english source index j

    choose a translation 

ibm model 1: example

    alignment vector a = [2,3,4,5,6,6,6]
    p(f,a|e)?

improving on ibm model 1: 

ibm model 2 

    input

    an english sentence of length l
    a length m

    for each french position     in1..m

    pick an english source index j

    choose a translation 

remove 

assumption that  q 

is uniform

ibm model 2: parameters

    q(j|i,l,m) 

    now a table
    not uniform as in ibm1

    how many 

parameters are there?

defining a probabilistic model

for word alignment

id203 lets us
1) formulate a model of pairs of sentences

=> ibm models 1 & 2

2) learn an instance of the model from data
3) use it to infer alignments of new inputs

2 remaining tasks

id136
    given

    a sentence pair (e,f)
    an alignment model 
with parameters t(e|f)
and q(j|i,l,m)

    what is the most 

probable alignment a?

parameter estimation
    given

    training data (lots of 

sentence pairs)

    a model definition

    how do we learn the 
parameters t(e|f) and 
q(j|i,l,m)?

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes 

p(e,a|f)?
    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes 

p(e,a|f)?
    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes 

p(e,a|f)?
    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes 

p(e,a|f)?
    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes 

p(e,a|f)?
    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes 

p(e,a|f)?
    hint: recall independence assumptions!

alignment error rates:

    given: 

how good is the prediction?
    precision:  |         |
recall:  |         |
|    |
|    |
    aer(a|s,p) =1             |+         |
    +|    |

predicted alignments a, sure links s, 
and possible links p

reference
alignments, 

with 

possible 
links and 
sure links

1 remaining task

id136
    given a sentence pair 
(e,f), what is the most 
probable alignment a?

parameter estimation
    how do we learn the 
parameters t(e|f) and 
q(j|i,l,m) from data?

parameter estimation (warm-up)
    inputs

    model definition ( t and q )
    a corpus of sentence pairs, with word alignment

    how do we build tables for t and q?

    use counts, just like for id165 models!

parameter estimation (for real)

    problem

    parallel corpus gives us (e,f) pairs only, a is hidden

    we know how to

    estimate t and q, given (e,a,f)
    compute p(e,a|f), given t and q

    solution: expectation-maximization algorithm (em)

    e-step: given hidden variable, estimate parameters
    m-step: given parameters, update hidden variable

parameter estimation: hard em

parameter estimation: soft em

use    soft    values 
instead of binary 

counts

parameter estimation: soft em

    soft em considers all possible alignment links
    each alignment link now has a weight

example: learning t table using em for ibm1

we have now fully specified our 
probabilistic alignment model!

id203 lets us
1) formulate a model of pairs of sentences

=> ibm models 1 & 2

2) learn an instance of the model from data

=> using em

3)  use it to infer alignments of new inputs

=> based on independent translation decisions

summary: id87 

for machine translation

    the id87 decomposes machine 

translation into two independent subproblems
    word alignment
    id38

summary: word alignment with 

ibm models 1, 2

    probabilistic models with strong independence 

assumptions
    results in linguistically na  ve models

    asymmetric, 1-to-many alignments

    but allows efficient parameter estimation and 

id136

    alignments are hidden variables 

    unlike words which are observed
    require unsupervised learning (em algorithm)

today

    walk through an example of em

    phrase-based models

    a slightly more recent translation model

    decoding

em for ibm1

ibm model 1: generative story

    input

    an english sentence of length l
    a length m

    for each french position     in1..m

    pick an english source index j

    choose a translation 

em for ibm model 1

    expectation (e)-step:

    compute expected counts for parameters (t) 

based on summing over hidden variable

    maximization (m)-step:

    compute the maximum likelihood estimate of 

t from the expected counts

em example: initialization

green house

the house

casa verde

la casa

for the rest of this talk, french = spanish

em example: e-step

(a) compute id203 of each alignment p(a|f,e)

note: we   re making many simplification assumptions 
in this example!!
    no null word
    we only consider alignments were each french 

and english word is aligned to something

    we ignore q

em example: e-step
(b) normalize to get p(a|f,e)

em example: e-step
(c) compute expected counts 
(weighting each count by p(a|e,f)

em example: m-step
normalizing expected counts

compute id203 estimate by 

em example: next iteration

em for ibm 1 in practice

    the previous example aims to illustrate the 

intuition of em algorithm

    but it is a little na  ve

    we had to enumerate all possible alignments
    very inefficient!!
    in practice, we don   t need to sum overall all 

possible alignments explicitly for ibm1

http://www.cs.columbia.edu/~mcollins/courses/nlp2011
/notes/ibm12.pdf

em

    procedure for optimizing generative models without 

supervision 
    randomly initialize parameters, then 
    e: predict hidden structure y (hard or soft)   

m: estimate new parameters p  (y | x) by id113

    likelihood function is non-convex. consider trying 

several random initializations to avoid getting stuck in 
local optima.

   
phrase-based models

phrase-based models

    most common way to model p(f|e) nowadays 

(instead of ibm models)

start position of 

f_i

end position of 

f_(i-1)

id203 of 
two consecutive 
english phrases 
being separated 
by a particular 
span in french

phrase alignments are derived 

from word alignments

this means that the 
ibm model represents 

p(spanish|english)

get high confidence 
alignment links by
intersecting ibm 
word alignments 
from both directions

phrase alignments are derived 

from word alignments

improve recall by adding 

some links from the 
union of alignments

phrase alignments are derived 

from word alignments

extract phrases that are consistent 

with word alignment

phrase translation probabilities

    given such phrases we can get the 

required statistics for the model from 

phrase-based machine translation

decoding

decoding for phrase-based mt
    basic idea

    search the space of possible english translations in an 

efficient manner.  

    according to our model

decoding as search

    starting point: null state.  no french 
content covered, no english included.

    we   ll drive the search by 

    choosing french word/phrases to    cover   , 
    choosing a way to cover them

    subsequent choices are pasted left-to-

right to previous choices. 

    stop: when all input words are covered.

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

12/8/2015

and martin       

speech and language processing - jurafsky 

28

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

green

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

green

witch

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

green

witch

phrase-based machine 
translation: the full picture

                                                                                                                                                                        
democracy
in this
the
that
democracies
a
democratic
the
at
of democracy
it
here

system
a system
systems
which
network

sense
meaning
terms
way
sense ,

actions
action
the
acts
steps

some
partially
part
in part
partly

discredit

some

such
similar
these
this
like
these actions

american
u.s.
us
america
america's
american democracy
america   s democracy

us democracy

this
in this sense
in that sense
in this respect

syntax-based translation

27

(cid:6)

s

pro

(cid:5)

vbz

|

wants

(cid:4)

  vp

vp

to

|
to

vp

vb

np

np

det

|
a

nn

|

cup

np

in
|
of

pp

nn

(cid:1)

pro
she

sie
pper

(cid:2)

(cid:3)

nn

coffee

kaffee

nn

vb

drink

trinken
vvinf

will
vafin

eine
art

tasse

nn

np

vp

s

philipp koehn

machine translation

28 january 2016

semantic translation
    id15 [knight et al., ongoing]
(w / want-01

28

:agent (b / boy)
:theme (l / love

:agent (g / girl)
:patient b))

    generalizes over equivalent syntactic constructs
(e.g., active and passive)

    de   nes semantic relationships

    semantic roles
    co-reference
    discourse relations

    in a very preliminary stage

philipp koehn

machine translation

28 january 2016

neural mt

    current research on neural network architectures, 
with state-of-the-art scores for some language pairs

mt: summary

    human-quality machine translation is an ai-complete problem. 

    all the challenges of nl: ambiguity,    exibility (dif   cult to evaluate!), 
vocabulary & grammar divergences between languages, context 

    state-of-the-art now good enough to be useful/commercially successful for 

some language pairs and purposes.

    tension: simplistic models + huge data, or linguistically savvy models + less 

data? mt systems can be word-level, phrase-based, syntax-based, semantics-
based/interlingua (vauquois triangle)

    statistical methods, enabled by large parallel corpora and automatic 

evaluations (such as id7), are essential for broad coverage 
    automatic word alignment on parallel data via em (ibm models) 
    id87: id165 language model for target language + 

translation model that uses probabilities from word alignments 

    open-source toolkits like moses make it relatively easy to build your own mt 

system from data

