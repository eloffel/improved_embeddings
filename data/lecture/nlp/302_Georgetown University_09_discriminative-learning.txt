linear models for classi   cation:    
discriminative learning 
(id88, id166s, maxent)

nathan schneider   

(some slides borrowed from chris dyer) 

enlp | 12 february 2018

23

outline
    words, probabilities     features, weights 

    geometric view: decision boundary 

    id88 

    generative vs. discriminative 

previous lecture

this lecture

    more discriminative models: id28/maxent; 

id166 

    id168s, optimization 

    id173; sparsity

24

id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run current classifier 
 
       arg maxy    wy         (x) 
 
 
 
if        y then # mistake 
 
  wy     wy +   (x)   
 
            w       w         (x) 
return w

w

(assumes all 
classes have the 
same percepts)

l

25

id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run current classifier 
 
                        x 
 
 
 
if        y then # mistake 
 
  wy     wy +   (x)   
 
            w       w         (x) 
return w

c

decoding is a 
subroutine of learning

w

(assumes all 
classes have the 
same percepts)

l

26

id88 learner

for binary classi   cation  
single weight vector such that 
>0     + class, <0         class

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run current classifier 
 
       sign(w      (x)) 
 
 
 
if        y then # mistake 
 
  w     w + sign(y)      (x) 
 
return w

w

(assumes all 
classes have the 
same percepts)

l

27

id88 learner

x
y

w     0 
for i = 1     i: 
   for t = 1     t: 
select (x, y)t 
 
# run current classifier 
 
       arg maxy    w      (x, y   ) 
 
 
 
if        y then # mistake 
 
  w     w +   (x, y)       (x,   ) 
 
return w

if different classes 

percepts

have different 
l

w

28

work through example on the board

x1 =    i thought it was great    

x2 =    not so great    

x3 =    good but not great   

y1 = + 

y2 =     

y3 = +

29

id88 learner

    the id88 doesn   t estimate probabilities. it just adjusts weights up 

or down until they classify the training data correctly. 
    no assumptions of  feature independence necessary! (cid:15482) better accuracy than nb 

    the id88 is an example of  an online learning algorithm because it 
potentially updates its parameters (weights) with each training datapoint. 

    classification, a.k.a. decoding, is called with the latest weight vector. 

mistakes lead to weight updates. 

    one hyperparameter: i, the number of  iterations (passes through the 

training data). 

    often desirable to make several passes over the training data. the number 

can be tuned. under certain assumptions, it can be proven that the 
learner will converge.

30

id88: avoiding over   tting

    like any learning algorithm, the id88 risks 
overfitting the training data. two main techniques 
to improve generalization: 
    averaging: keep a copy of  each weight vector as it 

changes, then average all of  them to produce the final 
weight vector. daum   chapter has a trick to make this 
efficient with large numbers of  features. 

    early stopping: tune i by checking held-out accuracy 

on dev data (or cross-val on train data) after each 
iteration. if  accuracy has ceased to improve, stop 
training and use the model from iteration i     1.

31

generative vs. discriminative

    na  ve bayes allows us to classify via the joint id203 of  x and y: 

    p(y | x)     p(y)   w     x p(w | y)   

               = p(y) p(x | y) (per the independence assumptions of  the model)   
                    = p(y, x) (chain rule) 

    this means the model accounts for both x and y. from the joint distribution 

p(x,y) it is possible to compute p(x) as well as p(y), p(x | y), and p(y | x). 

    nb is called a generative model because it assigns id203 to 

linguistic objects (x). it could be used to generate    likely    language 
corresponding to some y. (subject to its na  ve modeling assumptions!) 
    (not to be confused with the    generative    school of  linguistics.) 

    some other linear models, including the id88, are discriminative: 

they are trained directly to classify given x, and cannot be used to 
estimate the id203 of  x or generate x | y.

32

many possible decision boundaries

?

x

which one is best?

?

c

y

33

max-margin methods (e.g., id166)

{margin

x

choose decision 
boundary that is 
   halfway between 
nearest positive and 
negative examples

c

y

34

max-margin methods

    support vector machine (id166): most popular 

max-margin variant 

    closely related to the id88; can be 

optimized (learned) with a slight tweak to the 
id88 algorithm. 

    like id88, discriminative, non-

probabilistic.

35

maximum id178 (maxent) a.k.a. 
(multinomial) id28

    what if  we want a discriminative classifier with probabilities? 

    e.g., need confidence of  prediction, or want the full distribution over possible classes 

    wrap the linear score computation (w      (x, y   )) in the softmax function:  

score can be negative; exp(score) is always positive

   

    binary case:    

log

log p(y | x) = log   exp(w      (x, y))       = w      (x, y)     log   y    exp(w      (x, y   ))   
                                y    exp(w      (x, y   )) 
denominator = id172 (makes probabilities sum to 1).    
sum over all classes (cid:15482) same for all numerators (cid:15482) can be ignored at classi   cation time.
log

log p(y=1 | x) = log                exp(w      (x, y=1))   
                                   exp(w      (x, y=1)) + exp(w      (x, y=0))   
                        = log    exp(w      (x, y=1))           (fixing w      (x, y=0) = 0)   
                                   exp(w      (x, y=1)) + 1 

log

    maxent classifiers are a special case of  maxent a.k.a. id148. 

    why the term    maximum id178   ? see smith linguistic structure prediction, appendix c.

36

objectives

    for all linear models, the classification rule or decoding 
objective is: y     arg maxy    w      (x, y   ) 
    objective function = function for which we want to find the optimum 

(in this case, the max) 

    there is also a learning objective for which we want to find the 

optimal parameters. mathematically, nb, maxent, id166, and 
id88 all optimize different learning objectives. 
    when the learning objective is formulated as a minimization 

problem, it   s called a id168. 

    a id168 scores the    badness    of  the training data under any 

possible set of  parameters. learning = choosing the parameters 
that minimize the badness.

37

objectives

    na  ve bayes learning objective: joint data likelihood 

    p*     arg maxp ljoint(d; p)   

       = arg maxp   (x, y)     d log p(x,y) = arg maxp   (x, y)     d log (p(y)p(x | y)) 

   

it can be shown that relative frequency estimation (i.e., count and divide, no 
smoothing) is indeed the maximum likelihood estimate 

    maxent learning objective: conditional log likelihood 

    p*     arg maxp lcond(d; p)   

       = arg maxp   (x, y)     d log p(y|x)    
w      arg maxw   (x, y)     d w      (x, y)     log   y    exp(w      (x, y   )) [2 slides ago] 

    this has no closed-form solution. hence, we need an optimization algorithm 

to try different weight vectors and choose the best one. 

    with thousands or millions of  parameters   not uncommon in nlp   it may 

also overfit.

38

   max
w     (x, `0)      w     (x, `)
objectives

`02l

in the binary case:

visualizing different id168s for binary classi   cation

worse

5

4

3

s
s
o

l

2

0   1 loss (error)

1

0

better

   4

log loss (maxent)

hinge loss (id88)

4

0

score

   2
2
w      (x, y   )

in purple is the hinge loss, in blue is the log loss; in red is the
   zero-one    loss (error).

   gure from noah smith

39

objectives

    why not just penalize error directly if  that   s how we   re going to evaluate 

our classifier (accuracy)? 
    error is difficult to optimize! log loss and hinge loss are easier. why? 

    because they   re differentiable. 

    can use stochastic (sub)id119 (sgd) and other gradient-based 
optimization algorithms (l-bfgs, adagrad,    ). there are freely available 
software packages that implement these algorithms. 

    with supervised learning, these id168s are convex: local optimum = 
global optimum (so in principle the initialization of  weights doesn   t matter). 

    the id88 algorithm can be understood as a special case of  

subid119 on the hinge loss! 

    n.b. i haven   t explained the math for the hinge loss (id88) or the 
id166. or the derivation of  gradients. see further reading links if  you   re 
interested.

40

christopher	manning

visualizes the likelihood objective (vertical axis) as a function of 2 parameters. 

a likelihood surface
likelihood = maximization problem. flip upside down for the loss.    
a	likelihood	surface

gradient-based optimizers choose a point on the surface, look at its curvature, 

and then successively move to better points.

   gure from chris manning

41

   
id173

    better maxent learning objective: regularized conditional log likelihood 

    w*     arg maxw      r(w) +   (x, y)     d w      (x, y)     log   y    exp(w      (x, y   )) 

    to avoid overfitting, the id173 term (   regularizer   )      r(w) penalizes complex 

models (i.e., parameter vectors with many large weights). 

    close relationship to bayesian prior (a priori notion of  what a    good    model looks like if  there is 

not much training data). note that the regularizer is a function of  the weights only (not the 
data)! 

   

in nlp, most popular values of  r(w) are the (cid:361)1 norm (   lasso   ) and the (cid:361)2 norm (   ridge   ): 
    (cid:361)2  =    w   2 = (  i wi  )   1/2 encourages most weights to be small in magnitude
    (cid:361)1  =    w   1 =   i |wi| encourages most weights to be 0 
       determines the tradeoff  between id173 and data-fitting. can be tuned on dev data. 

 

    id166 objective also incorporates a id173 term. id88 does not (hence, 

averaging and early stopping).

42

sparsity

    (cid:361)1 id173 is a way to promote model sparsity: many weights are 

pushed to 0. 
    a vector is sparse if  (# nonzero parameters)     (total # parameters). 
   

intuition: if  we define very general feature templates   e.g. one feature per word 
in the vocabulary   we expect that most features should not matter for a 
particular classification task. 

   

in nlp, we typically have sparsity in our feature vectors as well. 
    e.g., in wsd, all words in the training data but not in context of  a particular 

token being classified are effectively 0-valued features. 

    exception: dense word representations popular in recent neural network 

models (we   ll get to this later in the course). 

    sometimes the word    sparsity    or    sparseness    just means    not very 

much data.   

43

summary: linear models

classifier: y     arg maxy    w      (x, y   )

kind of model

id168

learning 
algorithm

avoiding 
overfitting

na  ve bayes

probabilistic, 

generative

likelihood   

closed-form 
estimation

smoothing   

id28 

(maxent)

probabilistic, 
discriminative

conditional 
likelihood

optimization    id173 

penalty

id88

non-probabilistic, 

discriminative

hinge   

optimization   

averaging; 

early stopping

id166 (linear kernel)

non-probabilistic, 

discriminative

max-margin   

optimization    id173 

penalty

44

take-home points

    feature-based linear classifiers are important to nlp. 

    you define the features, an algorithm chooses the weights. some classifiers then exponentiate 

and normalize to give probabilities. 

    more features (cid:15482) more flexibility, also more risk of  overfitting. because we work with large 

vocabularies, not uncommon to have millions of  features. 

    learning objective/id168s formalize training as choosing parameters to optimize a 

function. 

    some model both the language and the class (generative); some directly model the class 

   

conditioned on the language (discriminative). 
in general: generative (cid:15482) training is cheaper, but lower accuracy.    
discriminative (cid:15482) higher accuracy with sufficient training data and computation (optimization). 

    some models, like na  ve bayes, have a closed-form solution for parameters. learning is 

cheap! 

    other models require fancier optimization algorithms that may iterate multiple times over 

the data, adjusting parameters until convergence (or some other stopping criterion). 

    the advantage: fewer modeling assumptions. weights can be interdependent.

45

which classi   er to use?

    fast and simple: na  ve bayes 

    very accurate, still simple: id88 

    very accurate, probabilistic, more complicated to 

implement: maxent 

    potentially best accuracy, more complicated to implement: 

id166 

    all of  these: watch out for overfitting! 
    check the web for free and fast implementations,    

e.g. id166light

46

further reading:    
basics & examples

    manning: features in linear classifiers   

http://www.stanford.edu/class/cs224n/handouts/maxenttutorial-16x9-
featureclassifiers.pdf  

    goldwater: na  ve bayes & maxent examples   

http://www.inf.ed.ac.uk/teaching/courses/fnlp/lectures/07_slides.pdf  

    o   connor: maxent   incl. step-by-step examples, comparison to na  ve 

bayes   
http://people.cs.umass.edu/~brenocon/inlp2015/04-logreg.pdf  

    daum  :    the id88    (a course in machine learning, ch. 3)   

http://www.ciml.info/dl/v0_8/ciml-v0_8-ch03.pdf   

    neubig:    the id88 algorithm      

http://www.phontron.com/slides/nlp-programming-en-05-id88.pdf  

47

further reading:    

advanced

    neubig:    advanced discriminative learning      maxent w/ derivatives, sgd, 

id166s, id173     
http://www.phontron.com/slides/nlp-programming-en-06-
discriminative.pdf  

    manning: generative vs. discriminative, maxent likelihood function and 

derivatives   
http://www.stanford.edu/class/cs224n/handouts/maxenttutorial-16x9-
memms-smoothing.pdf, slides 3   20 

    daum  : linear models    

http://www.ciml.info/dl/v0_8/ciml-v0_8-ch06.pdf   

    smith: a variety of  id168s for text classification   

http://courses.cs.washington.edu/courses/cse517/16wi/slides/tc-intro-
slides.pdf  & http://courses.cs.washington.edu/courses/cse517/16wi/
slides/tc-advanced-slides.pdf  

48

evaluating multiclass 

classi   ers    

and retrieval algorithms

49

accuracy

    assume we are disambiguating word senses such 

that every token has 1 gold sense label. 

    the classifier predicts 1 label for each token in the 

test set. 

    thus, every test set token has a predicted label 

(pred) and a gold label (gold). 

    the accuracy of  our classifier is just the % of  

tokens for which the predicted label matched the 
gold label: #pred=gold/#tokens

50

precision and recall

    to measure the classifier with respect to a certain 

label y, and there are >2, we distinguish precision and 
recall: 
    precision = proportion of  times the label was predicted 
and that prediction matched the gold: #pred=gold=y/#pred=y 

    recall = proportion of  times the label was in the gold 
standard and was recovered correctly by the classifier: 
#pred=gold=y/#gold=y 

    the harmonic mean of  precision and recall, called f1-

score, balances between the two.    
f1 = 2*precision*recall / (precision + recall)

51

evaluating retrieval systems

    precision/recall/f-score are also useful for 

evaluating retrieval systems. 

    e.g., consider a system which takes a word as input 

and is supposed to retrieve all rhymes. 

    now, for a single input (the query), there are often 

many correct outputs. 

    precision tells us whether most of  the given outputs 

were valid rhymes; recall tells us whether most of  
the valid rhymes in the gold standard were recovered.

52

rhymes for    hinge   

gold

system

klinge 
minge 
vinje

binge 
cringe 
fringe 
hinge 
impinge 
infringe 
syringe 
tinge 
twinge 
unhinge

ainge

53

rhymes for    hinge   

gold

system

klinge 
minge 
vinje

binge 
cringe 
fringe 
hinge 
impinge 
infringe 
syringe 
tinge 
twinge 
unhinge

false positive   
(type i error)

ainge

54

rhymes for    hinge   
e   
a ti v
g
e
e   n
e  ii  e rr o r)
p

system

gold

false positive   
(type i error)

f

a l s
( t

y

klinge 
minge 
vinje

binge 
cringe 
fringe 
hinge 
impinge 
infringe 
syringe 
tinge 
twinge 
unhinge

ainge

55

rhymes for    hinge   
e   
a ti v
g
e
e   n
e  ii  e rr o r)
p

system

gold

false positive   
(type i error)

f

a l s
( t

y

klinge 
minge 
vinje

binge 
cringe 
fringe 
hinge 
impinge 
infringe 
syringe 
tinge 
twinge 
unhinge

sys=y sys=n

gold=y

10

3

gold=n

1

(large)

ainge

correctly predicted =    

true positive 
all other words =   
true negative

56

precision & recall
e   
a ti v
g
e
e   n
e  ii  e rr o r)
p

system

gold

false positive   
(type i error)

f

a l s
( t

y

klinge 
minge 
vinje

binge 
cringe 
fringe 
hinge 
impinge 
infringe 
syringe 
tinge 
twinge 
unhinge

sys=y sys=n

gold=y

10

3

gold=n

1

(large)

ainge

precision = tp/(tp+fp)    
= 10/11 = 91%
recall = tp/(tp+fn)    
= 10/13 = 77%
f1 = 2  p  r/(p+r) = 83%

correctly predicted =    

true positive 
all other words =   
true negative

57

