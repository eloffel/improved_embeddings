empirical methods in natural language processing

lecture 2

text corpora

(some slides adapted from alex lascarides)

17 january 2018

nathan schneider

enlp lecture 2

17 january 2018

corpora in nlp

this lecture:
    what is a corpus?
    why do we need text corpora for nlp? (learning, evaluation)
    how can we access corpora with nltk?

illustrative application: id31
. . . and a bit about id121

nathan schneider

enlp lecture 2

1

corpora in nlp

corpus
noun, plural corpora or, sometimes, corpuses.

1. a large or complete collection of writings: the entire corpus of old english

poetry.

2. the body of a person or animal, especially when dead.

3. anatomy. a body, mass, or part having a special character or function.

4. linguistics. a body of utterances, as words or sentences, assumed to
be representative of and used for lexical, grammatical, or other linguistic
analysis.

5. a principal or capital sum, as opposed to interest or income.

dictionary.com

nathan schneider

enlp lecture 2

2

corpora in nlp

    to understand and model how language works, we need empirical evidence.
ideally, naturally-occurring corpora serve as realistic samples of a language.
    aside from linguistic utterances, corpus datasets include metadata   side
information about where the language comes from, such as author, date,
topic, publication.

    of particular interest for core nlp, and therefore this course, are corpora
with linguistic annotations   where humans have read the text and marked
categories or structures describing their syntax and/or meaning.

nathan schneider

enlp lecture 2

3

examples of corpora (in choronological order)

focusing on english; most released by the linguistic data consortium (ldc):

brown: 500 texts, 1m words in 15 genres. pos-tagged. semcor subset (234k

words) labelled with id138 word senses.

wsj: 6 years of wall street journal; subsequently used to create id32,
propbank, and more! translated into czech for the prague czech-english
dependency treebank. ontonotes bundles english wsj with broadcast
news and web data, as well as arabic and chinese corpora, with syntactic and
semantic annotations.

eci: european corpus initiative, multilingual.

bnc: british national corpus: balanced selection of written and spoken genres,

100m words.

gigaword: 1b words of news text.

ami: multimedia (video, audio, synchronised transcripts).

google books id165s: 5m books, 500b words (361b english).

nathan schneider

enlp lecture 2

4

markup

    there are several common markup formats for structuring linguistic data,
including xml, json, conll-style (one token per line, annotations in tab-
separated columns).

    some datasets, such as id138 and propbank, use custom    le formats.
nltk provides friendly python apis for reading many corpora so you don   t
have to worry about this.

nathan schneider

enlp lecture 2

5

id31

goal: predict the opinion expressed in a piece of text. e.g., + or    . (or a rating
on a scale.)

nathan schneider

enlp lecture 2

6

id31

rottentomatoes.com

nathan schneider

enlp lecture 2

7

je"rey lyles (/critic/je"rey-lyles/)lyles' movie filesview all critic reviews (212)  (/m/star_wars_episode_i_the_phantom_menace/reviews/audience reviews for star wars episode i - the phantom menacejay hutchinson (/user/id/904627900/) super reviewermatthew samuel mirliani (/user/id/896467979/) super reviewerkj proulx (/user/id/896976177/) super reviewerchris garman (/user/id/816762000/ super reviewerview all audience reviews (40031)(/m/star_wars_episode_i_the_phantom_menace/reviews/?type=user)star wars episode i - the phantom menace quotesfull review    (http://www.patheos.com/blogs/filmchat/1999/05/review-star-wars-episode-i-the-phantom-menace-dir-george-lucas-1999.html) | november 20,to the original trilogy that this new filmlacks.        this movie is terrible           phantom is a frustrating watch, however thereare elements worth admiring: its ambition plot,williams' score, the art direction, and the iconicduel with darth maul.         filled with horrific dialogue, laughablecharacters, a laughable plot, ad really nointeresting stakes during this film, "star warsepisode i: the phantom menace" is not at allwhat i wanted from a film that is supposed tobe the huge opening to the segue into thefantastic original trilogy. the positives includethe score, the sound e"ects, and most of the           i've had a saying that i've used for almost 20years now in relation to the phantom menace. icompare the film to waking up christmasmorning expecting some great present only toreceive socks. nothing against socks. they havea place and are quite needed, but there's noflash with it. the same goes for the phantommenace, a film that really doesn't live up to the      view all critic reviews (323)  (/m/star_wars_episode_vii_the_force_awakens/reviews/audience reviews for star wars: the force awakensmatthew samuel mirliani (/user/id/896467979/) super reviewerjim hunter super reviewersanjay rema (/user/id/905108980/) super reviewerross collins (/user/id/427418005/) super reviewerview all audience reviews (12628)(/m/star_wars_episode_vii_the_force_awakens/reviews/?type=userthe force awakens is an exciting,nostalgic, powerful and moving film, thatis capable of generating acceleratedfull review    (http://www.siete24.mx/resena-star-wars-star wars returns to be fun again on thebig screen. [full review in spanish]                  extraordinarily faithful to the tone and style ofthe originals, the force awakens brings backthe old trilogy's heart, humor, mystery, andfun. since it is only the first piece in a newthree-part journey it can't help but feelincomplete. but everything that's already there,from the stunning visuals, to the thrilling actionsequences, to the charismatic new characters,            rey, a young smuggler, is thrust into a battlebetween the first order and the resistancewhen she teams up with a storm trooper whosu!ered a crisis of conscience.the new entry into the star wars universe isprofoundly derivative, essentially an updatedretelling of a new hope, and while ignoring thebackstory about the first order largely mutes              jj abrams is very good and knowing what hisaudience wants and giving just that to them. heis not great, however, because he rarely showsus something we didn't know we wanted. thisfilm derives a lot from the first star wars, andjust goes along as you might expect, yet it is stillvery enjoyable because it's star wars. the oldfaces were cool to see, and the new ones do                 well, i always thought a new hope was the beststar wars movie... episode 7 has kept the styleof the original movie, its soundtrack, not justthe score - the entire sound scape, but it shouldhave le# it at that. what we have feels like a fanremake of the original; re-hashing every plotelement, every character, every scene, evenripping o! several lines of the dialogue directly.      id31

rottentomatoes.com

nathan schneider

enlp lecture 2

8

je"rey lyles (/critic/je"rey-lyles/)lyles' movie filesview all critic reviews (212)  (/m/star_wars_episode_i_the_phantom_menace/reviews/audience reviews for star wars episode i - the phantom menacejay hutchinson (/user/id/904627900/) super reviewermatthew samuel mirliani (/user/id/896467979/) super reviewerkj proulx (/user/id/896976177/) super reviewerchris garman (/user/id/816762000/ super reviewerview all audience reviews (40031)(/m/star_wars_episode_i_the_phantom_menace/reviews/?type=user)star wars episode i - the phantom menace quotesfull review    (http://www.patheos.com/blogs/filmchat/1999/05/review-star-wars-episode-i-the-phantom-menace-dir-george-lucas-1999.html) | november 20,to the original trilogy that this new filmlacks.        this movie is terrible           phantom is a frustrating watch, however thereare elements worth admiring: its ambition plot,williams' score, the art direction, and the iconicduel with darth maul.         filled with horrific dialogue, laughablecharacters, a laughable plot, ad really nointeresting stakes during this film, "star warsepisode i: the phantom menace" is not at allwhat i wanted from a film that is supposed tobe the huge opening to the segue into thefantastic original trilogy. the positives includethe score, the sound e"ects, and most of the           i've had a saying that i've used for almost 20years now in relation to the phantom menace. icompare the film to waking up christmasmorning expecting some great present only toreceive socks. nothing against socks. they havea place and are quite needed, but there's noflash with it. the same goes for the phantommenace, a film that really doesn't live up to the      view all critic reviews (323)  (/m/star_wars_episode_vii_the_force_awakens/reviews/audience reviews for star wars: the force awakensmatthew samuel mirliani (/user/id/896467979/) super reviewerjim hunter super reviewersanjay rema (/user/id/905108980/) super reviewerross collins (/user/id/427418005/) super reviewerview all audience reviews (12628)(/m/star_wars_episode_vii_the_force_awakens/reviews/?type=userthe force awakens is an exciting,nostalgic, powerful and moving film, thatis capable of generating acceleratedfull review    (http://www.siete24.mx/resena-star-wars-star wars returns to be fun again on thebig screen. [full review in spanish]                  extraordinarily faithful to the tone and style ofthe originals, the force awakens brings backthe old trilogy's heart, humor, mystery, andfun. since it is only the first piece in a newthree-part journey it can't help but feelincomplete. but everything that's already there,from the stunning visuals, to the thrilling actionsequences, to the charismatic new characters,            rey, a young smuggler, is thrust into a battlebetween the first order and the resistancewhen she teams up with a storm trooper whosu!ered a crisis of conscience.the new entry into the star wars universe isprofoundly derivative, essentially an updatedretelling of a new hope, and while ignoring thebackstory about the first order largely mutes              jj abrams is very good and knowing what hisaudience wants and giving just that to them. heis not great, however, because he rarely showsus something we didn't know we wanted. thisfilm derives a lot from the first star wars, andjust goes along as you might expect, yet it is stillvery enjoyable because it's star wars. the oldfaces were cool to see, and the new ones do                 well, i always thought a new hope was the beststar wars movie... episode 7 has kept the styleof the original movie, its soundtrack, not justthe score - the entire sound scape, but it shouldhave le# it at that. what we have feels like a fanremake of the original; re-hashing every plotelement, every character, every scene, evenripping o! several lines of the dialogue directly.      id31

rottentomatoes.com + intuitions about positive/negative cue words

nathan schneider

enlp lecture 2

9

kj proulx (/user/id/896976177/) super reviewer      filled with horrific dialogue, laughablecharacters, a laughable plot, ad really nointeresting stakes during this film, "star warsepisode i: the phantom menace" is not at allwhat i wanted from a film that is supposed tobe the huge opening to the segue into thefantastic original trilogy. the positives includethe score, the sound e"ects, and most of the   matthew samuel mirliani (/user/id/896467979/) super reviewer               extraordinarily faithful to the tone and style ofthe originals, the force awakens brings backthe old trilogy's heart, humor, mystery, andfun. since it is only the first piece in a newthree-part journey it can't help but feelincomplete. but everything that's already there,from the stunning visuals, to the thrilling actionsequences, to the charismatic new characters,   so, you want to build a sentiment analyzer

questions to ask yourself:

1. what is the input for each prediction?

(sentence?

full review text?

text+metadata?)

2. what are the possible outputs? (+ or    )

3. how will it decide?

4. how will you measure its e   ectiveness?

the last one, at least, requires data!

nathan schneider

enlp lecture 2

10

before you build a system, choose a dataset for

evaluation!

why is data-driven evaluation important?
    good science requires controlled experimentation.
    good engineering requires benchmarks.
    your intuitions about typical inputs are probably wrong.

sometimes you want multiple evaluation datasets: e.g., one for development as
you hack on your system, and one reserved for    nal testing.

nathan schneider

enlp lecture 2

11

where can you get a corpus?

    many corpora are prepared speci   cally for linguistic/nlp research with text
in fact, there is an entire sub   eld

from content providers (e.g., newspapers).
devoted to developing new language resources.

    you may instead want to collect a new one, e.g., by scraping websites. (there

are tools to help you do this.)

nathan schneider

enlp lecture 2

12

annotations

to evaluate and compare sentiment analyzers, we need reviews with gold labels
(+ or    ) attached. these can be
    derived automatically from the original data artifact (metadata such as star

ratings), or

    added by a human annotator who reads the text

    issue to consider/measure: how consistent are human annotators? if they

often have trouble deciding or agreeing, how can this be addressed?

more on these issues later in the course!

nathan schneider

enlp lecture 2

13

an evaluation measure

once we have a dataset with gold (correct) labels, we can give the text of each
review as input to our system and measure how often its output matches the gold
label.

simplest measure:

accuracy =

# correct
# total

more measures later in the course!

nathan schneider

enlp lecture 2

14

catching our breath

we now have:

  a de   nition of the id31 task (inputs and outputs)

  a way to measure a sentiment analyzer (accuracy on gold data)

so we need:
    an algorithm for predicting sentiment

nathan schneider

enlp lecture 2

15

a simple sentiment classi   cation algorithm

use a sentiment lexicon to count positive and negative words:

positive:
absolutely
adorable
accepted
acclaimed
accomplish
achieve
action
active
admire
adventure
a   rm
. . .

beaming
beautiful
believe
bene   cial
bliss
bountiful
bounty
brave
bravo
brilliant
bubbly

calm
celebrated
certain
champ
champion
charming
cheery
choice
classic
classical
clean
. . .

negative:

abysmal
adverse
alarming
angry
annoy
anxious
apathy
appalling
atrocious
awful

bad
banal
barbed
belligerent
bemoan
beneath
boring
broken

callous
can   t
clumsy
coarse
cold
collapse
confused
contradictory
contrary
corrosive
corrupt
. . .

from http://www.enchantedlearning.com/wordlist/

simplest rule: count positive and negative words in the text. predict whichever
is greater.

nathan schneider

enlp lecture 2

16

some possible problems with simple counting

1. hard to know whether words that seem positive or negative tend to actually

be used that way.
    sense ambiguity
    sarcasm/irony
    text could mention expectations or opposing viewpoints,

author   s actual opinon

in contrast to

2. opinion words may be describing (e.g.) a character   s attitude rather than an

evaluation of the    lm.

3. some words act as semantic modi   ers of other opinion-bearing words/phrases,

so interpreting the full meaning requires sophistication:

i can   t stand this movie

vs.

i can   t believe how great this movie is

nathan schneider

enlp lecture 2

17

what if we have more data?

perhaps corpora can help address the    rst objection:

1. hard to know whether words that seem positive or negative tend to actually

be used that way.

a data-driven method: use frequency counts to ascertain which words tend to
be positive or negative.

nathan schneider

enlp lecture 2

18

nltk

the natural language toolkit (http://nltk.org) is a python library for
nlp. nltk
    is open-source, community-built software
    was designed for

teaching nlp:

simple access

to datasets,

reference

implementations of important algorithms

    contains wrappers for using (some) state-of-the-art nlp tools in python

it will help if you familiarize yourself with python strings and methods/libraries
for manipulating them.

(if you are familiar with python 2.7, know that strings and unicode are handled di   erently in
python 3.)

nathan schneider

enlp lecture 2

19

using an nltk corpus
>>> from nltk.corpus import movie_reviews
>>> movie_reviews.words()
[u'plot', u':', u'two', u'teen', u'couples', u'go', ...]
>>> movie_reviews.sents()
[[u'plot', u':', u'two', u'teen', u'couples', u'go',

(cid:44)    u'to', u'a', u'church', u'party', u',', u'drink',
(cid:44)    u'and', u'then', u'drive', u'.'], [u'they',
(cid:44)    u'get', u'into', u'an', u'accident', u'.'], ...]
(cid:44)    movie_reviews.sents()[:5]))
(cid:44)    and then drive .

plot : two teen couples go to a church party , drink

>>> print('\n'.join(' '.join(sent) for sent in

they get into an accident .
one of the guys dies , but his girlfriend continues to

(cid:44)    see him in her life , and has nightmares .

what ' s the deal ?
watch the movie and " sorta " find out .

nathan schneider

enlp lecture 2

20

using an nltk corpus: word frequencies

>>> from nltk import freqdist
>>> f = freqdist(movie_reviews.words())
>>> f.most_common()[:20]
[(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a',

(cid:44)    38106), (u'and', 35576), (u'of', 34123), (u'to',
(cid:44)    31937), (u"'", 30585), (u'is', 25195), (u'in',
(cid:44)    21822), (u's', 18513), (u'"', 17612), (u'it',
(cid:44)    16107), (u'that', 15924), (u'-', 15595), (u')',
(cid:44)    11781), (u'(', 11664), (u'as', 11378), (u'with',
(cid:44)    10792), (u'for', 9961)]

nathan schneider

enlp lecture 2

21

using an nltk corpus: word frequencies
>>> f = freqdist(w for w in movie_reviews.words() if

(cid:44)    any(c.isalpha() for c in w))

>>> f.most_common()[:20]
[(u'the', 76529), (u'a', 38106), (u'and', 35576),

(cid:44)    (u'of', 34123), (u'to', 31937), (u'is', 25195),
(cid:44)    (u'in', 21822), (u's', 18513), (u'it', 16107),
(cid:44)    (u'that', 15924), (u'as', 11378), (u'with',
(cid:44)    10792), (u'for', 9961), (u'his', 9587), (u'this',
(cid:44)    9578), (u'film', 9517), (u'i', 8889), (u'he',
(cid:44)    8864), (u'but', 8634), (u'on', 7385)]

nathan schneider

enlp lecture 2

22

using an nltk corpus: categories

>>> movie_reviews.categories()
[u'neg', u'pos']
>>> fpos =

(cid:44)    freqdist(movie_reviews.words(categories='pos'))
(cid:44)    freqdist(movie_reviews.words(categories='neg'))

>>> fneg =

>>> fmoreneg = fneg - fpos
>>> fmoreneg.most_common()[:20]
[(u'movie', 721), (u't', 700), (u'i', 685), (u'bad',
(cid:44)    673), (u'?', 631), (u'"', 628), (u'have', 421),
(cid:44)    (u'!', 399), (u'no', 350), (u'plot', 321),
(cid:44)    (u'there', 318), (u'if', 301), (u'*', 286),
(cid:44)    (u'this', 282), (u'so', 267), (u'why', 250),
(cid:44)    (u'just', 221), (u'only', 219), (u'worst', 210),
(cid:44)    (u'even', 207)]

nathan schneider

enlp lecture 2

23

what if we have more data?

perhaps corpora can help address the    rst objection:

1. hard to know whether words that seem positive or negative tend to actually

be used that way.

a data-driven method: use frequency counts from a training corpus to ascertain
which words tend to be positive or negative.
    why separate the training and test data (held-out test set)? because otherwise,
it   s just data analysis; no way to estimate how well the system will do on new
data in the future.

nathan schneider

enlp lecture 2

24

id121

let   s take another look at the movie reviews corpus:

>>> print('\n'.join(' '.join(sent) for sent in

(cid:44)    movie_reviews.sents()[:5]))
(cid:44)    and then drive .

plot : two teen couples go to a church party , drink

they get into an accident .
one of the guys dies , but his girlfriend continues to

(cid:44)    see him in her life , and has nightmares .

what ' s the deal ?
watch the movie and " sorta " find out .

what do you notice about spelling conventions? spacing?

nathan schneider

enlp lecture 2

25

id121

normal written conventions sometimes do not re   ect the    logical    organization
of textual symbols. for example, some punctuation marks are written adjacent to
the previous or following word, even though they are not part of it. (the details
vary according to language and style guide!)

given a string of raw text, a tokenizer adds logical boundaries between separate
word/punctuation tokens (occurrences) not already separated by spaces:

daniels made several appearances as c-3po on numerous tv shows and
commercials, notably on a star wars-themed episode of the donny and

marie show in 1977, disneyland   s 35th anniversary.

   

daniels made several appearances as c-3po on numerous tv shows and
commercials , notably on a star wars - themed episode of the donny and

marie show in 1977 , disneyland    s 35th anniversary .

to a large extent, this can be automated by rules. but there are always di   cult
cases.

nathan schneider

enlp lecture 2

26

id121 in nltk

>>> nltk.word_tokenize("daniels made several

['daniels', 'made', 'several', 'appearances', 'as',

(cid:44)    appearances as c-3po on numerous tv shows and
(cid:44)    commercials, notably on a star wars-themed episode
(cid:44)    of the donny and marie show in 1977, disneyland's
(cid:44)    35th anniversary.")
(cid:44)    'c-3po', 'on', 'numerous', 'tv', 'shows', 'and',
(cid:44)    'commercials', ',', 'notably', 'on', 'a', 'star',
(cid:44)    'wars-themed', 'episode', 'of', 'the', 'donny',
(cid:44)    'and', 'marie', 'show', 'in', '1977', ',',
(cid:44)    'disneyland', "'s", '35th', 'anniversary', '.']

nathan schneider

enlp lecture 2

27

id121 in nltk

>>> nltk.word_tokenize("daniels made several

['daniels', 'made', 'several', 'appearances', 'as',

(cid:44)    appearances as c-3po on numerous tv shows and
(cid:44)    commercials, notably on a star wars-themed episode
(cid:44)    of the donny and marie show in 1977, disneyland's
(cid:44)    35th anniversary.")
(cid:44)    'c-3po', 'on', 'numerous', 'tv', 'shows', 'and',
(cid:44)    'commercials', ',', 'notably', 'on', 'a', 'star',
(cid:44)    'wars-themed', 'episode', 'of', 'the', 'donny',
(cid:44)    'and', 'marie', 'show', 'in', '1977', ',',
(cid:44)    'disneyland', "'s", '35th', 'anniversary', '.']

english id121 conventions vary somewhat   e.g., with respect to:
    clitics (contracted forms)    s, n   t,    re, etc.
    hyphens in compounds like president-elect (fun fact: this convention changed

between versions of the id32!)

nathan schneider

enlp lecture 2

28

sentence id121

there is also the problem of    nding sentence boundaries (sentence id121
or sentence splitting). you can use nltk.sent tokenize() for this, though
it won   t be perfect!

in april 1938, bernarr a. macfadden, publisher of liberty magazine stepped
in, o   ering a prize of $1,000 to the winning composer, stipulating that the
song must be of simple    harmonic structure   ,    within the limits of [an]
untrained voice   , and its beat in    march tempo of military pattern   .

the contest rules required the winner to submit his entry in written form,
and crawford immediately complied. however his original title, what do
you think of the air corps now?, was soon o   cially changed to the army
air corps.

https://en.wikipedia.org/wiki/the_u.s._air_force_%28song%29

nathan schneider

enlp lecture 2

29

choice of training and evaluation data

we know that the way people use language varies considerably depending on
context. factors include:
    mode of communication: speech (in person, telephone), writing (print, sms,

web)

    topic: chitchat, politics, sports, physics, . . .
    genre: news story, novel, wikipedia article, persuasive essay, political address,

tweet, . . .

    audience: formality, politeness, complexity (think: child-directed speech), . . .

in nlp, domain is a cover term for all these factors.

nathan schneider

enlp lecture 2

30

choice of training evaluation data

    statistical approaches typically assume that the training data and the test data

are sampled from the same distribution.

    i.e., if you saw an example data point, it would be hard to guess whether it

was from the training or test data.

    things can go awry if the test data is appreciably di   erent: e.g.,

    di   erent id121 conventions
    new vocabulary
    longer sentences
    more colloquial/less edited style
    di   erent distribution of labels

    id20 techniques attempt to correct for this assumption when
something about the source/characteristics of the test data is known to be
di   erent.

nathan schneider

enlp lecture 2

31

why do we need text corpora?

two main reasons:

1. to evaluate our systems on

    good science requires controlled experimentation.
    good engineering requires benchmarks.

2. to help our systems work well (data-driven methods/machine learning)

    when a system   s behavior is determined solely by manual rules or databases,
it is said to be rule-based, symbolic, or knowledge-driven (early days of
computational linguistics)
    learning: collecting statistics or patterns automatically from corpora to
govern the system   s behavior (dominant in most areas of contemporary
nlp)
    supervised learning: the data provides example input/output pairs (main

focus in this course)

    core behavior: training; re   ning behavior: tuning

nathan schneider

enlp lecture 2

32

