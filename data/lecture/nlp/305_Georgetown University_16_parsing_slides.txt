parsing with 
id18s

cmsc 723 / ling 723 / inst 725

marine carpuat
marine@cs.umd.edu

today   s agenda

    grammar-based parsing with id18s

    cky algorithm

    dealing with ambiguity

    probabilistic id18s

    strategies for improvement
    rule rewriting / lexicalization

sample grammar

grammar-based parsing: cky

grammar-based parsing

    problem setup

    input: string and a id18

    output: parse tree assigning proper structure to input 

string

       proper structure   

    tree that covers all and only words in the input

    tree is rooted at an s

    derivations obey rules of the grammar

    usually, more than one parse tree   

parsing algorithms

    two basic (= bad) algorithms:

    top-down search

    bottom-up search

    a    real    algorithm:

    cky parsing

top-down search

    observation

    trees must be rooted with an s node

    parsing strategy

    start at top with an s node

    apply rules to build out trees

    work down toward leaves

top-down search

top-down search

top-down search

bottom-up search

    observation

    trees must cover all input words

    parsing strategy

    start at the bottom with input words

    build structure based on grammar

    work up towards the root s

bottom-up search

bottom-up search

bottom-up search

bottom-up search

bottom-up search

top-down vs. bottom-up

    top-down search

    only searches valid trees

    but, considers trees that are not consistent 

with any of the words

    bottom-up search

    only builds trees consistent with the input

    but, considers trees that don   t lead anywhere

parsing as search

    search involves controlling choices in the 

search space
    which node to focus on in building structure

    which grammar rule to apply

    general strategy: backtracking

    make a choice, if it works out then fine

    if not, back up and make a different choice

shared sub-problems

    observation

    ambiguous parses still share sub-trees

    we don   t want to redo work that   s already 

been done

    unfortunately, na  ve backtracking leads to 

duplicate work

efficient parsing 

with the cky algorithm

    solution: id145

    intuition: store partial results in tables
    thus avoid repeated work on shared sub-

problems

    thus efficiently store ambiguous structures with 

shared sub-parts

    we   ll cover one example
    cky: roughly, bottom-up

cky parsing: cnf

    cky parsing requires that the grammar consist of 

binary rules in chomsky normal form
    all rules of the form:

a     b c 
d     w

    what does the tree look like?

cky parsing with arbitrary id18s

    what if my grammar has rules like             

vp     np pp pp
    problem: can   t apply cky!

    solution: rewrite grammar into cnf

    introduce new intermediate non-terminals into the 

grammar

a     b c d

a     x d
x     b c

(where x is a symbol that 
doesn   t occur anywhere else in 
the grammar)

sample grammar

cnf conversion

original grammar

cnf version

cky parsing: intuition

    consider the rule d     w

    terminal (word) forms a constituent

    trivial to apply

    consider the rule a     b c

       if there is an a somewhere in the input, then there must be a b 

followed by a c in the input   

    first, precisely define span [ i, j ] 

    if a spans from i to j in the input then there must be some k such 

that i<k<j

    easy to apply: we just need to try different values for k

i

j

a

b

c

k

cky parsing: table

    any constituent can conceivably span [ i, j ] for all 

0   i<j   n, where n = length of input string
    we need an n    n table to keep track of all spans   

    but we only need half of the table

    semantics of table: cell [ i, j ] contains a iff a spans i to j

in the input string
    of course, must be allowed by the grammar!

cky parsing: table-filling

   

in order for a to span [ i, j ]
    a     b c is a rule in the 

grammar, and

    there must be a b in [ i, k ] and 

a c in [ k, j ] for some i<k<j

    operationally 

    to apply rule a     b c, look for 

a b in [ i, k ] and a c in [ k, j ]

    in the table: look left in the row 

and down in the column

cky parsing: canonical ordering

    standard cky algorithm:

    fill the table a column at a time, from left to 

right, bottom to top 

    whenever we   re filling a cell, the parts needed 
are already in the table (to the left and below)

    nice property: processes input left to right, 

word at a time

cky parsing: ordering illustrated

cky algorithm

cky: example

?

?

?

?

filling column 5

cky: example

recall our cnf grammar:

?

?

?

?

cky: example

?

?

?

cky: example

?

?

cky: example

recall our cnf grammar:

?

cky: example

cky parsing: recognize or parse

    recognizer

    output is binary

    can the complete span of the sentence be 

covered by an s symbol?

    parser

    output is a parse tree

    from recognizer to parser: add backpointers!

ambiguity

    cky can return multiple parse trees

    plus: compact encoding with shared sub-trees

    plus: work deriving shared sub-trees is reused

    minus: algorithm doesn   t tell us which parse is 

correct!

ambiguity

probabilistic context-free 
grammars

simple id203 model

    a derivation (tree) consists of the bag of 

grammar rules that are in the tree
    the id203 of a tree is the product of the 

probabilities of the rules in the derivation.

rule probabilities

    what   s the id203 of a rule?

    start at the top...

    a tree should have an s at the top. so given 

that we know we need an s, we can ask about 
the id203 of each particular s rule in the 
grammar:  p(particular rule | s)

    in general we need

for each rule in the grammar  

      p(         |   )training the model

    we can get the estimates we need from a 

treebank

for example, to get the id203 for a particular vp rule:

1.

2.

count all the times the rule is used

divide by the number of vps overall.

parsing (decoding)

how can we get the best (most probable) 
parse for a given input?

1. enumerate all the trees for a sentence

2. assign a id203 to each using the model

3. return the argmax

example

    consider...

    book the dinner flight

examples

    these trees consist of the following rules.

id145

    of course, as with normal parsing we don   t 

really want to do it that way...

    instead, we need to exploit dynamic 

programming
    for the parsing (as with cky)

    and for computing the probabilities and 

returning the best parse (as with viterbi and 
id48s)

probabilistic cky

    store probabilities of constituents in the table

    table[i,j,a] = id203 of constituent a that spans 

positions i through j in input

    if a is derived from the rule a     b c :

    table[i,j,a] = p(a     b c | a) * table[i,k,b] * table[k,j,c]

    where

    p(a     b c | a) is the rule id203

    table[i,k,b] and table[k,j,c] are already in the table 

given the way that cky operates

    only store the max id203 over all the a rules.

probabilistic cky

problems with pid18s

   

the id203 model we   re using is just 
based on the bag of rules in the 
derivation   
1. doesn   t take the actual words into account 

in any useful way.

2. doesn   t take into account where in the 

derivation a rule is used

3. doesn   t work terribly well

improving our parser

improved approaches

there are two approaches to overcoming 
these shortcomings

1. rewrite the grammar to better capture the 

dependencies among rules 

2.

integrate lexical dependencies into the model

solution 2: 

lexicalized grammars

    lexicalize the grammars with heads

    compute the rule probabilities on these 

lexicalized rules

    run prob cky as before

lexicalized grammars: example

how can we learn probabilities for 

lexicalized rules?

    we used to have

    vp -> v np pp 
    p(rule|vp) = count of this rule divided by the 

number of vps in a treebank

    now we have fully lexicalized rules...

    vp(dumped)-> v(dumped) np(sacks)pp(into)
p(r|vp ^ dumped is the verb ^ sacks is the head 

of the np ^ into is the head of the pp)

   

   

we need to make 

independence assumptions

strategies: exploit independence and 
collect the statistics we can get
    many many ways to do this...

let   s consider one generative story: given 
a rule we   ll
1. generate the head

2. generate the stuff to the left of the head

3. generate the stuff to the right of the head

from the generative story 

to rule probabilities   

the rule id203 for

can be estimated as

framework

    that   s just one simple model

       collins model 1   

    other assumptions that might lead to better 

models
    make sure that you can get the counts you need

    make sure they can get exploited efficiently 

during decoding

today   s agenda

    grammar-based parsing with id18s

    cky algorithm

    dealing with ambiguity

    probabilistic id18s

    strategies for improvement

    lexicalization

today   s agenda

    grammar-based parsing with id18s

    cky algorithm

    dealing with ambiguity

    probabilistic id18s

    strategies for improvement

    lexicalization

    tools for parsing english, chinese, french,     with 
pid18s http://nlp.stanford.edu/software/lex-parser.shtml

