lecture 24 
wrapping up

nathan schneider   
enlp | 30 april 2018

1

   
in a nutshell

    we have seen representations, datasets, models, and algorithms for 

computationally reasoning about textual language. 
    persistent challenges: zipf   s law, ambiguity &    exibility, variation, 

context 

    core nlp tasks (judgments about the language itself): id121, pos 

tagging, syntactic parsing (constituency, dependency), word sense 
disambiguation, word similarity, id14, coreference 
resolution 

    nlp applications (solve some practical problem involving/using 

language): spam classi   cation, language/author identi   cation, sentiment 
analysis, id39, id53, machine translation 

    which of these are generally easy, and which are hard?

2

language complexity and 

diversity

    ambiguity and    exibility of expression often best addressed 

with corpora & statistics 
    treebanks and statistical parsing 

    grammatical forms help convey meaning, but the relationship is 

complicated, motivating semantic representations 
    proposed by linguists, or 
    induced from data 

    typological variation: languages vary extensively in 

phonology, morphology, and syntax

methods useful for more 

than one task

    annotation, id104 

    rule-based/   nite-state methods, e.g. id157 

    classi   cation (na  ve bayes, id88) 

    id38 (id165 or neural) 

    grammars & parsing 

    sequence modeling (id48s, structured id88) 

    id170   id145 (viterbi, cky)

4

models & learning

    because language is so complex, most nlp tasks bene   t from statistical 

learning. 

    in this course, mostly supervised learning with labeled data. exceptions: 

    unsupervised learning: the em algorithm (e.g. for word alignment, topic models) 
    language models, distributional similarity/embeddings: supervised learning, but 

no extra labels necessary   the context is the supervision 

    in nlp research, a tension between building a lot of linguistic insights into 

models vs. learning almost purely from the data. 
    current research on neural networks tries to bypass hand-designed features/

intermediate representations as much as possible. 

    we still don   t quite know how to capture    deep    understanding.

5

generative and 

discriminative models

    assign id203 to language and hidden variable? or just score 

hidden variable given language? 

    independence assumptions: how useful/harmful are they? 

       all models are wrong, but some are useful    
    bag-of-words; markov models 
    combining statistics from different sources, e.g. noisy channel 

model 

    avoiding over   tting (smoothing, id173) 

    evaluation: gold standard? sometimes dif   cult

id145 

algorithms

    allow us to search a combinatorial (exponential) space 

ef   ciently by reusing partial results. 

    in a sentence of length n, what is the asymptotic runtime 

complexity of: 
    ibm model 2 word alignment, where the other 

sentence has length m?

7

id145 

algorithms

    allow us to search a combinatorial (exponential) space 

ef   ciently by reusing partial results. 

    in a sentence of length n, what is the asymptotic runtime 

complexity of: 
    word id153, where the other sentence has 

length m? o(m  n) 

    viterbi (in a    rst-order id48), with l possible labels?   

8

id145 

algorithms

    allow us to search a combinatorial (exponential) space 

ef   ciently by reusing partial results. 

    in a sentence of length n, what is the asymptotic runtime 

complexity of: 
    word id153, where the other sentence has 

length m? o(m  n) 

    viterbi (in a    rst-order id48), with l possible labels?   

o(nl  ) 

    cky, with a grammar of size g?

9

id145 

algorithms

    allow us to search a combinatorial (exponential) space 

ef   ciently by reusing partial results. 

    in a sentence of length n, what is the asymptotic runtime 

complexity of: 
    word id153, where the other sentence has 

length m? o(m  n) 

    viterbi (in a    rst-order id48), with l possible labels?   

o(nl  ) 

    cky, with a grammar of size g? o(n  g)

10

applications

    id31, machine translation 
    your projects! 
    now that you know the tools in the toolbox, you can

the final exam

    thursday 5/10, 4:00-6:00 
    largely similar in style to the midterm & quizzes, but with content 

covering the entire course. 

       and more short answer questions. for each major concept or 
technique, be prepared to de   ne it, explain its relevance to nlp, 
discuss its strengths and weaknesses, and compare to alternatives. 
    e.g.:    why is smoothing used? for a model covered in class, 

describe two methods for smoothing and their pros/cons.    

    study guide will be posted. 
    review session: wednesday 1:00   2:00, icc 462

other administrivia

    projects due midnight tomorrow! 

    peer evaluations for the    nal project (watch for an 

announcement after tomorrow; we need these to determine 
your grade) 

    no more of   ce hours (unless you contact us) 
    related courses next semester include    

advanced semantic representation (cosc/ling-672) and 
dialogue systems (cosc-483/ling-463) 

    ta & course evaluations    

https://eval.georgetown.edu/ 

