enlp lecture 22   
deep learning & 
neural networks

austin blodgett, georgetown university 
april 23, 2018

a family of algorithms

example input

example output

nn task

binary 
classification
multiclass 
classification
sequence
sequence to 
sequence 
tree/graph 
parsing

nn task

binary 
classification
multiclass 
classification
sequence
sequence to 
sequence 
tree/graph 
parsing

example input

example output

features

+/-

features

decl, imper,     

sentence

pos tags

(english) sentence

(spanish) sentence

sentence

dependency tree or 

amr parsing

(slide from manning and socher)

(slide from manning and socher)

(slide from manning and socher)

id88

id88

ffnns

   feed forward neural net     multiple layers of neurons 
   can solve non-linearly separable problems 
   (all arrows face the same direction) 
   applications: 

   text classification     id31, language detection,     
   unsupervised learning     dimension reduction, id97

=

compact diagram

   how do i interpret an nn? 

faq
   an nn performs function approximation 
   connections in an nn posit relatedness 
   lack of connection posits independence

faq

   what do the weights mean? 

   functional perspective     these weights optimize nn   s task performance  
   representation perspective     weights represent unlabeled, distributed  
knowledge (useful but not generally interpretable)

faq

   can an nn learn anything? 

   no, but      

theorem:    one hidden layer is enough to represent (not learn) an approximation 

of any function to an arbitrary degree of accuracy   

   (given infinite training data, memory, etc.)

faq
   what happens if i make an nn deeper?

i

w
d
t
h

depth

width controls 
overfitting/underfitting 

depth allows complex 
functions, can reduce 
overfitting

(goodfellow 2017)

id180

   activation function        squishes    neuron inputs into an output 

   use in output layer     sigmoid (binary class), softmax (multiclass) 
   use in hidden layers     relu, leaky relu

sigmoid

relu (rectified linear unit)

training

   to train an nn, you need: 

   training set - ordered pairs each with an input and target output 
   id168 - a function to be optimized, e.g. cross id178 
   optimizer -  a method for adjusting the weights, e.g. id119

id119     use gradient 
to find lowest point in a function

id26

   id26 = chain rule + id145
id168     measures nn   s 
performance.  

adjust weights by gradient (using a learning 
weight) of the loss. save repeated partial 
computations along the way.

id168s

   id168     measures nn   s performance.  

   probabilistic interpretation 

    binary output - binary cross id178 and sigmoid 
    multiclass/sequence output - categorical cross id178 and softmax 
    either generative or discriminative 
    mean squared error or hinge loss (like in structured id88)

   geometric interpretation 

id56s

   recurrent neural net - model a sequence of any length 
   weight sharing, unlimited history 
   (also     lstm, gru, bidirectional) 
   applications: 

   language models 
   language generation 
   sequence classification - part-of-speech tagging 

   not just words (characters, structured data,    )

id56

id56 language 

model

weight sharing

id56 dimensions

m
u
l
t
i
-
l
a
y
e
r
e
d

depth

dimension

or

id56 part-of-speech tagger
how is an id56 different than id48?

id56 part-of-speech tagger
how is an id56 different than id48?  

unlimited history

compact diagram

=

encoder-decoder models

   encoder-decoder model (also id195)     take a sequence as 
input and predict a sequence as output 
   input and output may be different lengths 
   encoder (id56) models input, decoder (id56) models output 
   applications: 

   machine translation 
   morphological analysis

encoder

decoder

encoder (english)

decoder (french)

embeddings

   embeddings - dense vector representations of words, characters, 
documents, etc. 
   used as input features for most neural nlp models 
   prepackaged     id97, glove 
   use pre-trained id27s and train them yourself!

some references

   nn packages     tensorflow, pytorch, keras 
   some books 

   goldberg book (free from georgetown) 
   goodfellow book (chapters and videos)

