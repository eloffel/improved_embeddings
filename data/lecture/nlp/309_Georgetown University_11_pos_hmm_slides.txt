empirical methods in natural language processing

lecture 11

part-of-speech tagging and id48s

(based on slides by sharon goldwater and philipp koehn)

21 february 2018

nathan schneider

enlp lecture 11

21 february 2018

what is id52?

    given a string:

this is a simple sentence

    identify parts of speech (syntactic categories):

this/det is/vb a/det simple/adj sentence/noun

nathan schneider

enlp lecture 11

1

why do we care about id52?

    id52 is a    rst step towards syntactic analysis (which in turn, is often

useful for semantic analysis).

    simpler models and often faster than full parsing, but sometimes enough to

be useful.

    for example, pos tags can be useful features in text classi   cation (see

previous lecture) or id51 (see later in course).

    illustrates the use of id48 (id48s), which are also used

for many other tagging (sequence labelling) tasks.

nathan schneider

enlp lecture 11

2

examples of other tagging tasks

    id39:

e.g.,

label words as belonging to persons,

organizations, locations, or none of the above:

barack/per obama/per spoke/non from/non the/non white/loc
house/loc today/non ./non

    information    eld segmentation: given speci   c type of text (classi   ed
identify which words belong to which       elds   

advert, bibiography entry),
(price/size/location, author/title/year)

3br/size    at/type in/non brunts   eld/loc ,/non near/loc
main/loc roads/loc ./non bright/feat ,/non well/feat maintained/feat
...

nathan schneider

enlp lecture 11

3

sequence labelling: key features

in all of these tasks, deciding the correct label depends on
    the word to be labeled

    ner: smith is probably a person.
    id52: chair is probably a noun.

    the labels of surrounding words

    ner: if following word is an organization (say corp.), then this word is

more likely to be organization too.

    id52: if preceding word is a modal verb (say will) then this word is

more likely to be a verb.

id48 combines these sources of information probabilistically.

nathan schneider

enlp lecture 11

4

parts of speech: reminder

    open class words (or content words)

    nouns, verbs, adjectives, adverbs
    mostly content-bearing: they refer to objects, actions, and features in the

world

    open class, since there is no limit to what these words are, new ones are

added all the time (email, website).
    closed class words (or function words)

    pronouns, determiners, prepositions, connectives, ...
    there is a limited number of these
    mostly functional: to tie the concepts of a sentence together

nathan schneider

enlp lecture 11

5

how many parts of speech?

    both linguistic and practical considerations
    corpus annotators decide. distinguish between
    proper nouns (names) and common nouns?
    singular and plural nouns?
    past and present tense verbs?
    auxiliary and main verbs?
    etc

    commonly used tagsets for english usually have 40-100 tags. for example,

the id32 has 45 tags.

nathan schneider

enlp lecture 11

6

j&m fig 5.6: id32 pos tags pos tags in other languages

    morphologically rich languages often have compound morphosyntactic tags

noun+a3sg+p2sg+nom

(j&m, p.196)

    hundreds or thousands of possible combinations
    predicting these requires more complex methods than what we will discuss

(e.g., may combine an fst with a probabilistic disambiguation system)

nathan schneider

enlp lecture 11

8

why is id52 hard?

the usual reasons!
    ambiguity:

glass of water/noun vs. water/verb the plants
lie/verb down
wind/verb down

vs. tell a lie/noun
vs. a mighty wind/noun

(homographs)

how about time    ies like an arrow?

    sparse data:

    words we haven   t seen before (at all, or in this context)
    word-tag pairs we haven   t seen before (e.g., if we verb a noun)

nathan schneider

enlp lecture 11

9

relevant knowledge for id52

remember, we want a model that decides tags based on
    the word itself

    some words may only be nouns, e.g. arrow
    some words are ambiguous, e.g. like,    ies
    probabilities may help, if one tag is more likely than another

    tags of surrounding words

    two determiners rarely follow each other
    two base form verbs rarely follow each other
    determiner is almost always followed by adjective or noun

nathan schneider

enlp lecture 11

10

a probabilistic model for tagging

to incorporate these sources of information, we imagine that the sentences we
observe were generated probabilistically as follows.
    to generate sentence of length n:

let t0 =<s>
for i = 1 to n

choose a tag conditioned on previous tag: p (ti | ti   1)
choose a word conditioned on its tag: p (wi | ti)

    so, the model assumes:

    each tag depends only on previous tag: a bigram tag model.
    words are independent given tags

nathan schneider

enlp lecture 11

11

a probabilistic model for tagging

in math:

n(cid:89)

i=1

p (t, w ) =

p (ti | ti   1)    p (wi | ti)

nathan schneider

enlp lecture 11

12

a probabilistic model for tagging

in math:

n(cid:89)

p (t, w ) =

p (ti | ti   1)    p (wi | ti)

i=1

   p (</s> | tn)

where w0 = <s> and |w| = |t| = n

nathan schneider

enlp lecture 11

13

a probabilistic model for tagging

in math:

n(cid:89)

p (t, w ) =

p (ti | ti   1)    p (wi | ti)

i=1

   p (</s> | tn)

where w0 = <s> and |w| = |t| = n
    this can be thought of as a language model over words + tags. (kind of a

hybrid of a bigram language model and na    ve bayes.)

    but typically, we don   t know the tags   i.e. they   re hidden.

it is therefore a

bigram hidden markov model (id48).

nathan schneider

enlp lecture 11

14

probabilistic    nite-state machine

    one way to view the model:

sentences are generated by walking through

states in a graph. each state represents a tag.

    prob of moving from state s to s(cid:48) (transition id203): p (ti = s(cid:48) | ti   1 =

s)

nathan schneider

enlp lecture 11

15

example transition probabilities

ti   1\ti
<s>
nnp
md
vb
jj
. . .

nnp
0.2767
0.3777
0.0008
0.0322
0.0306

. . .

md

0.0006
0.0110
0.0002
0.0005
0.0004

. . .

vb

0.0031
0.0009
0.7968
0.0050
0.0001

. . .

jj

0.0453
0.0084
0.0005
0.0837
0.0733

. . .

nn

0.0449
0.0584
0.0008
0.0615
0.4509

. . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .

    probabilities estimated from tagged wsj corpus, showing, e.g.:

    proper nouns (nnp) often begin sentences: p (nnp|<s>)     0.28
    modal verbs (md) nearly always followed by bare verbs (vb).
    adjectives (jj) are often followed by nouns (nn).

table excerpted from j&m draft 3rd edition, fig 8.5

nathan schneider

enlp lecture 11

16

example transition probabilities

ti   1\ti
<s>
nnp
md
vb
jj
. . .

nnp
0.2767
0.3777
0.0008
0.0322
0.0306

. . .

md

0.0006
0.0110
0.0002
0.0005
0.0004

. . .

vb

0.0031
0.0009
0.7968
0.0050
0.0001

. . .

jj

0.0453
0.0084
0.0005
0.0837
0.0733

. . .

nn

0.0449
0.0584
0.0008
0.0615
0.4509

. . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .

    this table is incomplete!
    in the full table, every row must sum up to 1 because it is a distribution over

the next state (given previous).

table excerpted from j&m draft 3rd edition, fig 8.5

nathan schneider

enlp lecture 11

17

probabilistic    nite-state machine: outputs

    when passing through each state, emit a word.

    prob of emitting w from state s (emission or output id203):

p (wi = w | ti = s)

nathan schneider

enlp lecture 11

18

vblikefliesexample output probabilities

the

back

janet

will
0

0.000032

ti\wi
. . .
nnp
. . .
md
. . .
vb
. . .
dt
. . .
. . .
. . .
    id113 probabilities from tagged wsj corpus, showing, e.g.:

0.308431
0.000028

0
0
0
. . .

0.000672

0.000048

0.506099

0
. . .

0
. . .

. . .

0
0

0
0

    0.0032% of proper nouns are janet: p (janet|nnp) = 0.000032
    about half of determiners (dt) are the.
    the can also be a proper noun. (annotation error?)

    again, in full table, rows would sum to 1.

from j&m draft 3rd edition, fig 8.6

nathan schneider

enlp lecture 11

19

graphical model diagram

in graphical model notation, circles = random variables, and each arrow = a
id155 factor in the joint likelihood:

    = a lookup in the transition distribution,
    = a lookup in the emission distribution.

http://www.cs.virginia.edu/~hw5x/course/cs6501-text-mining/_site/mps/mp3.html

nathan schneider

enlp lecture 11

20

what can we do with this model?

    if we know the transition and output probabilities, we can compute the

id203 of a tagged sentence.

    that is,

    suppose we have sentence w = w1 . . . wn

and its tags t = t1 . . . tn.

    what is the id203 that our probabilistic id122 would generate exactly

that sequence of words and tags, if we stepped through at random?

nathan schneider

enlp lecture 11

21

what can we do with this model?

    if we know the transition and output probabilities, we can compute the

id203 of a tagged sentence.

    suppose we have sentence w = w1 . . . wn

and its tags t = t1 . . . tn.

    what is the id203 that our probabilistic id122 would generate exactly

that sequence of words and tags, if we stepped through at random?

    this is the joint id203

p (w, t ) =

n(cid:89)

p (ti | ti   1)p (wi | ti)

i=1

   p (</s> | tn)

nathan schneider

enlp lecture 11

22

example: computing joint prob. p (w, t )

what   s the id203 of this tagged sentence?

this/det is/vb a/det simple/jj sentence/nn

nathan schneider

enlp lecture 11

23

example: computing joint prob. p (w, t )

what   s the id203 of this tagged sentence?

this/det is/vb a/det simple/jj sentence/nn

    first, add begin- and end-of-sentence <s> and </s>. then:

(cid:34) n(cid:89)

i=1

(cid:35)

p (w, t ) =

p (ti|ti   1)p (wi|ti)

p (</s>|tn)

= p (det|<s>)p (vb|det)p (det|vb)p (jj|det)p (nn|jj)p (</s>|nn)

  p (this|det)p (is|vb)p (a|det)p (simple|jj)p (sentence|nn)

    then, plug in the probabilities we estimated from our corpus.

nathan schneider

enlp lecture 11

24

but... tagging?

normally, we want to use the model to    nd the best tag sequence for an untagged
sentence.
    thus, the name of the model: hidden markov model

    markov: because of markov independence assumption (each tag/state only

depends on    xed number of previous tags/states   here, just one).

    hidden:

because at test time we only see the words/emissions;

the

tags/states are hidden (or latent) variables.

    id122 view: given a sequence of words, what is the most probable state path

that generated them?

nathan schneider

enlp lecture 11

25

hidden markov model (id48)

id48 is actually a very general model for sequences. elements of an id48:
    a set of states (here: the tags)
    an output alphabet (here: words)
    intitial state (here: beginning of sentence)
    state transition probabilities (here: p (ti | ti   1))
    symbol emission probabilities (here: p (wi | ti))

nathan schneider

enlp lecture 11

26

relationship to previous models

    id165 model: a model for sequences that also makes a markov assumption,

but has no hidden variables.

    na    ve bayes: a model with hidden variables (the classes) but no sequential

dependencies.

    id48: a model for sequences with hidden variables.

like many other models with hidden variables, we will use bayes    rule to help us
infer the values of those variables.

(in nlp, we usually assume hidden variables are observed during training   though
there are unsupervised methods that do not.)

nathan schneider

enlp lecture 11

27

relationship to other models

side note for those interested:
    na    ve bayes: a generative model (use bayes    rule, strong independence

assumptions) for classi   cation.

    maxent: a discriminative model (model p (y | x) directly, use arbitrary

features) for classi   cation.

    id48: a generative model for sequences with hidden variables.
    memm: a discriminative model for sequences with hidden variables. other
sequence models can also use more features than id48: e.g., conditional
random field (crf) or structured id88.

nathan schneider

enlp lecture 11

28

formalizing the tagging problem

find the best tag sequence t for an untagged sentence w :

p (t | w )

arg max

t

nathan schneider

enlp lecture 11

29

formalizing the tagging problem

find the best tag sequence t for an untagged sentence w :

    bayes    rule gives us:

p (t | w )

arg max

t

p (t | w ) =

p (w | t ) p (t )

p (w )

    we can drop p (w ) if we are only interested in arg maxt :

arg max

t

p (t | w ) = arg max

t

p (w | t ) p (t )

nathan schneider

enlp lecture 11

30

decomposing the model

now we need to compute p (w | t ) and p (t ) (actually, their product p (w |
t )p (t ) = p (w, t )).
    we already de   ned how!
    p (t ) is the state transition sequence:

    p (w | t ) are the emission probabilities:

(cid:89)

i

(cid:89)

p (t ) =

p (ti | ti   1)

p (w | t ) =

p (wi | ti)

i

nathan schneider

enlp lecture 11

31

search for the best tag sequence

    we have de   ned a model, but how do we use it?

    given: word sequence w
    wanted: best tag sequence t    

    for any speci   c tag sequence t , it is easy to compute

p (w, t ) = p (w | t )p (t ).

(cid:89)

p (w | t ) p (t ) =

p (wi | ti) p (ti | ti   1)

i

    so, can   t we just enumerate all possible t , compute their probabilites, and

choose the best one?

nathan schneider

enlp lecture 11

32

enumeration won   t work

    suppose we have c possible tags for each of the n words in the sentence.
    how many possible tag sequences?

nathan schneider

enlp lecture 11

33

enumeration won   t work

    suppose we have c possible tags for each of the n words in the sentence.
    how many possible tag sequences?
    there are cn possible tag sequences: the number grows exponentially in the

length n.

    for all but small n, too many sequences to e   ciently enumerate.

nathan schneider

enlp lecture 11

34

the viterbi algorithm

    we   ll use a id145 algorithm to solve the problem.
    id145 algorithms order the computation e   ciently so partial

values can be computed once and reused.
    the viterbi algorithm    nds the best

enumerating all sequences.

tag sequence without explicitly

    partial results are stored in a chart to avoid recomputing them.
    details next time.

nathan schneider

enlp lecture 11

35

viterbi as a decoder

the problem of    nding the best tag sequence for a sentence is sometimes called
decoding.
    because, like id147 etc., id48 can also be viewed as a noisy

channel model.

    someone wants to send us a sequence of tags: p (t )
    during encoding,    noise    converts each tag to a word: p (w|t )
    we try to decode the observed words back to the original tags.

    in fact, decoding is a general term in nlp for inferring the hidden variables
in a test instance (so,    nding correct spelling of a misspelled word is also
decoding).

nathan schneider

enlp lecture 11

36

computing marginal prob. p (w )

recall that the id48 can be thought of as a language model over words and
tags. what about estimating probabilities of just the words of a sentence?

nathan schneider

enlp lecture 11

37

computing marginal prob. p (w )

recall that the id48 can be thought of as a language model over words and
tags. what about estimating probabilities of just the words of a sentence?

(cid:88)

p (w ) =

p (w, t )

t

nathan schneider

enlp lecture 11

38

computing marginal prob. p (w )

recall that the id48 can be thought of as a language model over words and
tags. what about estimating probabilities of just the words of a sentence?

(cid:88)

p (w ) =

p (w, t )

t

instead, use the forward
again, cannot enumerate all possible taggings t .
algorithm (id145 algorithm closely related to viterbi   see
textbook if interested).

could be used to measure perplexity of held-out data.

nathan schneider

enlp lecture 11

39

supervised learning

the id48 consists of
    transition probabilities
    emission probabilities

how can these be learned?

nathan schneider

enlp lecture 11

40

supervised learning

the id48 consists of
    transition probabilities
    emission probabilities

how can these be estimated? from counts in a treebank such as the penn
treebank!

nathan schneider

enlp lecture 11

41

supervised learning

the id48 consists of
    transition probabilities: given tag t, what is the id203 that t(cid:48) follows?
    emission probabilities: given tag t, what is the id203 that the word is w?

how can these be estimated? from counts in a treebank such as the penn
treebank!

nathan schneider

enlp lecture 11

42

do transition & emission probs. need smoothing?

nathan schneider

enlp lecture 11

43

do transition & emission probs. need smoothing?

    emissions: yes, because if there is any word w in the test data such that
p (wi = w | ti = t) = 0 for all tags t, the whole joint id203 will go to 0.
    transitions: not necessarily, but if any transition probabilities are estimated

as 0, that tag bigram will never be predicted.

    what are some transitions that should never occur in a bigram id48?

nathan schneider

enlp lecture 11

44

do transition & emission probs. need smoothing?

    emissions: yes, because if there is any word w in the test data such that
p (wi = w | ti = t) = 0 for all tags t, the whole joint id203 will go to 0.
    transitions: not necessarily, but if any transition probabilities are estimated

as 0, that tag bigram will never be predicted.

    what are some transitions that should never occur in a bigram id48?

        <s>
</s>        
<s>     </s>

nathan schneider

enlp lecture 11

45

unsupervised learning

    with the number of hidden tags speci   ed but no tagged training data, the

learning is unsupervised.

    the forward-backward algorithm, a.k.a. baum-welch em, clusters the data
into    tags    that will give the training data high id203 under the id48.
this is used in id103.

    see the textbook for details if interested.

nathan schneider

enlp lecture 11

46

higher-order id48s

    the    markov    part means we ignore history of more than a    xed number of

words.

    equations thus far have been for bigram id48: i.e., transitions are p (ti | ti   1).
    but as with language models, we can increase the n in the id165: trigram

id48 transition probabilities are p (ti | ti   2, ti   1), etc.

    as usual, smoothing the transition distributions becomes more important with

higher-order models.

nathan schneider

enlp lecture 11

47

summary

    part-of-speech tagging is a sequence labelling task.
    id48 uses two sources of information to help resolve ambiguity in a word   s

pos tag:

    the words itself
    the tags assigned to surrounding words

    can be viewed as a probabilistic id122.
    given a tagged sentence, easy to compute its id203. but    nding the best

tag sequence will need a clever algorithm.

nathan schneider

enlp lecture 11

48

