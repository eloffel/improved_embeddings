empirical methods in natural language processing

lecture 5

id165 language models

(most slides from sharon goldwater; some adapted from alex lascarides)

29 january 2017

nathan schneider

enlp lecture 5

29 january 2017

recap

    previously, we talked about corpus data and some of the information we can

get from it, like word frequencies.

    for some tasks, like id31, word frequencies alone can work pretty

well (though can certainly be improved on).

    for other tasks, we need more.
    today we consider sentence probabilities: what are they, why are they useful,

and how might we compute them?

nathan schneider

enlp lecture 5

1

review: word-based sentiment

    recall that we can predict sentiment for a document based on counting positive

and negative words.

    do you think the following words would be positive or negative in a movie

review?

    ok
    action
    star

nathan schneider

enlp lecture 5

2

id165s

    in some cases,

informative.

looking at more than one word at a time might be more

    action movie vs. action packed
    star wars vs. star studded

    an id165 is a word sequence of length n.

    1-gram or unigram: action
    2-gram or bigram: action packed
    3-gram or trigram: action packed adventure
    4-gram: action packed adventure    lm

nathan schneider

enlp lecture 5

3

id165s

the force awakens brings back the old trilogy    s heart , humor , mystery , and

fun .

how many:
    unigrams?
    bigrams?
    trigrams?

nathan schneider

enlp lecture 5

4

character id165s

    a character id165 applies the same idea to characters rather than words.
    e.g. unnatural has character bigrams un, nn, na, . . . , al
    why might this concept be useful for nlp?

nathan schneider

enlp lecture 5

5

towards sentence probabilities

       id203 of a sentence    = how likely is it to occur in natural language

    consider only a speci   c language (english)

p(the cat slept peacefully) > p(slept the peacefully cat)

p(she studies morphosyntax) > p(she studies more faux syntax)

nathan schneider

enlp lecture 5

6

language models in nlp

    it   s very di   cult to know the true id203 of an arbitrary sequence of words.
    but we can de   ne a language model that will give us good approximations.
    like all models, language models will be good at capturing some things and

less good for others.

    we might want di   erent models for di   erent tasks.
    today, one type of language model: an id165 model.

nathan schneider

enlp lecture 5

7

id147

sentence probabilities help decide correct spelling.

mis-spelled text

no much e   ert

   

(error model)

possible outputs

   

(language model)

no much e   ect
so much e   ort
no much e   ort
not much e   ort
...

best-guess output

not much e   ort

nathan schneider

enlp lecture 5

8

automatic id103

sentence probabilities help decide between similar-sounding options.

speech input

   

possible outputs

(acoustic model)

   

(language model)

she studies morphosyntax
she studies more faux syntax
she   s studies morph or syntax
...

best-guess output

she studies morphosyntax

nathan schneider

enlp lecture 5

9

machine translation

sentence probabilities help decide word choice and word order.

non-english input

   

(translation model)

possible outputs

   

(language model)

she is going home
she is going house
she is traveling to home
to home she is going
...

best-guess output

she is going home

nathan schneider

enlp lecture 5

10

lms for prediction

    lms can be used for prediction as well as correction.
    ex: predictive text correction/completion on your mobile phone.

    keyboard is tiny, easy to touch a spot slightly o    from the letter you meant.
    want to correct such errors as you go, and also provide possible completions.

predict as as you are typing: ine   ...

    in this case, lm may be de   ned over sequences of characters instead of (or in

addition to) sequences of words.

nathan schneider

enlp lecture 5

11

but how to estimate these probabilities?

    we want to know the id203 of word sequence (cid:126)w = w1 . . . wn occurring in

english.

    assume we have some training data: large corpus of general english text.
    we can use this data to estimate the id203 of (cid:126)w (even if we never see it

in the corpus!)

nathan schneider

enlp lecture 5

12

id203 theory vs estimation

    id203 theory can solve problems like:

    i have a jar with 6 blue marbles and 4 red ones.
    if i choose a marble uniformly at random, what   s the id203 it   s red?

nathan schneider

enlp lecture 5

13

id203 theory vs estimation

    id203 theory can solve problems like:

    i have a jar with 6 blue marbles and 4 red ones.
    if i choose a marble uniformly at random, what   s the id203 it   s red?

    but often we don   t know the true probabilities, only have data:

    i have a jar of marbles.
    i repeatedly choose a marble uniformly at random and then replace it before

choosing again.

    in ten draws, i get 6 blue marbles and 4 red ones.
    on the next draw, what   s the id203 i get a red marble?

    the latter also requires estimation theory.

nathan schneider

enlp lecture 5

14

notation

    i will often omit the random variable in writing probabilities, using p (x) to

mean p (x = x).

    when the distinction is important, i will use

    p (x) for true probabilities
      p (x) for estimated probabilities
    pe(x) for estimated probabilities using a particular estimation method e.
    but since we almost always mean estimated probabilities, may get lazy later

and use p (x) for those too.

nathan schneider

enlp lecture 5

15

example estimation: m&m colors

what is the proportion of each color of m&m?
    in 48 packages, i    nd1 2620 m&ms, as follows:
red orange yellow green blue brown
372

369

483

481

371

544

    how to estimate id203 of each color from this data?

1actually, data from: https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/

nathan schneider

enlp lecture 5

16

relative frequency estimation

    intuitive way to estimate discrete probabilities:

prf(x) =

c(x)

n

n =(cid:80)

where c(x) is the count of x in a large dataset, and

x(cid:48) c(x(cid:48)) is the total number of items in the dataset.

nathan schneider

enlp lecture 5

17

relative frequency estimation

    intuitive way to estimate discrete probabilities:

prf(x) =

c(x)

n

n =(cid:80)

where c(x) is the count of x in a large dataset, and

x(cid:48) c(x(cid:48)) is the total number of items in the dataset.

    m&m example: prf(red) = 372
    this method is also known as maximum-likelihood estimation (id113) for

2620 = .142

reasons we   ll get back to.

nathan schneider

enlp lecture 5

18

id113 for sentences?

can we use id113 to estimate the id203 of (cid:126)w as a sentence of english? that
is, the prob that some sentence s has words (cid:126)w?

pid113(s = (cid:126)w) =

c( (cid:126)w)

n

where c( (cid:126)w) is the count of (cid:126)w in a large dataset, and
n is the total number of sentences in the dataset.

nathan schneider

enlp lecture 5

19

sentences that have never occurred

the archaeopteryx soared jaggedly amidst foliage

vs

jaggedly trees the on    ew

    neither ever occurred in a corpus (until i wrote these slides).

    c( (cid:126)w) = 0 in both cases: id113 assigns both zero id203.

    but one is grammatical (and meaningful), the other not.
    using id113 on full sentences doesn   t work well
estimation.

for language model

nathan schneider

enlp lecture 5

20

the problem with id113

    id113 thinks anything that hasn   t occurred will never occur (p=0).
    clearly not true! such things can have di   erering, and non-zero, probabilities:

    my hair turns blue
    i injure myself in a skiing accident
    i travel to finland

    and similarly for word sequences that have never occurred.

nathan schneider

enlp lecture 5

21

sparse data

    in fact, even things that occur once or twice in our training data are a problem.

remember these words from europarl?

corn   akes, mathematicians, pseudo-rapporteur,
uncitral, policyfor, commissioneris, 145.95

lobby-ridden, lycketoft,

all occurred once. is it safe to assume all have equal id203?

    this is a sparse data problem:

not enough observations to estimate
(unlike the m&ms, where we had large counts for all

probabilities well.
colours!)

    for sentences, many (most!) will occur rarely if ever in our training data. so

we need to do something smarter.

nathan schneider

enlp lecture 5

22

towards better lm probabilities

    one way to try to    x the problem: estimate p ( (cid:126)w) by combining the probabilities

of smaller parts of the sentence, which will occur more frequently.

    this is the intuition behind id165 language models.

nathan schneider

enlp lecture 5

23

deriving an id165 model

    we want to estimate p (s = w1 . . . wn).

    ex: p (s = the cat slept quietly).

    this is really a joint id203 over the words in s:
p (w1 = the, w2 = cat, w3 = slept, . . . w4 = quietly).
    concisely, p (the, cat, slept, quietly) or p (w1, . . . wn).

nathan schneider

enlp lecture 5

24

deriving an id165 model

    we want to estimate p (s = w1 . . . wn).

    ex: p (s = the cat slept quietly).

    this is really a joint id203 over the words in s:
p (w1 = the, w2 = cat, w3 = slept, . . . w4 = quietly).
    concisely, p (the, cat, slept, quietly) or p (w1, . . . wn).
    recall that for a joint id203, p (x, y ) = p (y |x)p (x). so,

p (the, cat, slept, quietly) = p (quietly|the, cat, slept)p (the, cat, slept)
= p (quietly|the, cat, slept)p (slept|the, cat)p (the, cat)
= p (quietly|the, cat, slept)p (slept|the, cat)p (cat|the)p (the)

nathan schneider

enlp lecture 5

25

deriving an id165 model

    more generally, the chain rule gives us:

n(cid:89)

p (w1, . . . wn) =

p (wi|w1, w2, . . . wi   1)
    but many of these conditional probs are just as sparse!
    if we want p (i spent three years before the mast)...
    we still need p (mast|i spent three years before the).

i=1

example due to alex lascarides/henry thompson

nathan schneider

enlp lecture 5

26

deriving an id165 model

    so we make an independence assumption: the id203 of a word only

depends on a    xed number of previous words (history).
    trigram model: p (wi|w1, w2, . . . wi   1)     p (wi|wi   2, wi   1)
    bigram model: p (wi|w1, w2, . . . wi   1)     p (wi|wi   1)
    unigram model: p (wi|w1, w2, . . . wi   1)     p (wi)

    in our example, a trigram model says

    p (mast|i spent three years before the)     p (mast|before the)

nathan schneider

enlp lecture 5

27

trigram independence assumption

    put another way, trigram model assumes these are all equal:

    p (mast|i spent three years before the)
    p (mast|i went home before the)
    p (mast|i saw the sail before the)
    p (mast|i revised all week before the)
because all are estimated as p (mast|before the)

    also called a markov assumption

andrey markov    

    not always a good assumption! but it does reduce the sparse data problem.

nathan schneider

enlp lecture 5

28

estimating trigram conditional probs

    we still need to estimate p (mast|before, the): the id203 of mast given

the two-word history before, the.

    if we use relative frequencies (id113), we consider:

    out of all cases where we saw before, the as the    rst two words of a trigram,
    how many had mast as the third word?

nathan schneider

enlp lecture 5

29

estimating trigram conditional probs

    so, in our example, we   d estimate

pm le(mast|before, the) =

c(before, the, mast)

c(before, the)

where c(x) is the count of x in our training data.

    more generally, for any trigram we have

pm le(wi|wi   2, wi   1) =

c(wi   2, wi   1, wi)

c(wi   2, wi   1)

nathan schneider

enlp lecture 5

30

example from moby dick corpus

c(before, the) = 55

c(before, the, mast)

c(before, the, mast) = 4

c(before, the)

= 0.0727

    mast is the second most common word to come after before the in moby dick;

wind is the most frequent word.

    pm le(mast) is 0.00049, and pm le(mast|the) is 0.0029.
    so seeing before the vastly increases the id203 of seeing mast next.

nathan schneider

enlp lecture 5

31

trigram model: summary

    to estimate p ( (cid:126)w), use chain rule and make an indep. assumption:

p (w1, . . . wn) =

p (wi|w1, w2, . . . wi   1)

n(cid:89)

i=1

p (wi|wi   2, ww   1)
    then estimate each trigram prob from data (here, using id113):

    p (w1)p (w2|w1)

i=3

n(cid:89)

pm le(wi|wi   2, wi   1) =

c(wi   2, wi   1, wi)

c(wi   2, wi   1)

    on your own: work out the equations for other n -grams (e.g., bigram,

unigram).

nathan schneider

enlp lecture 5

32

practical details (1)

    trigram model assumes two word history:

n(cid:89)

p ( (cid:126)w) = p (w1)p (w2|w1)

p (wi|wi   2, ww   1)

    but consider these sentences:

i=3

w4
yellow
daily
    what   s wrong? does the model capture these problems?

w2
w3
saw the
cats
the

w1
he
feeds

(1)
(2)

nathan schneider

enlp lecture 5

33

beginning/end of sequence

    to capture behaviour at beginning/end of sequences, we can augment the

input:

w   1 w0

w1
(1) <s> <s> he
(2) <s> <s> feeds

w4
yellow </s>
daily
</s>
    that is, assume w   1 = w0 = <s> and wn+1 = </s> so:

w2
w3
saw the
the
cats

w5

n+1(cid:89)

p ( (cid:126)w) =

p (wi|wi   2, wi   1)

i=1

    now, p (</s>|the, yellow) is low, indicating this is not a good sentence.

nathan schneider

enlp lecture 5

34

beginning/end of sequence

    alternatively, we could model all sentences as one (very long) sequence,

including punctuation:

two cats live in sam    s barn . sam feeds the cats daily . yesterday , he

saw the yellow cat catch a mouse . [...]

    now, trigrams like p (.|cats daily) and p (,|. yesterday) tell us about

behavior at sentence edges.

    here, all tokens are lowercased. what are the pros/cons of not doing that?

nathan schneider

enlp lecture 5

35

practical details (2)

    word probabilities are typically very small.
    multiplying lots of small probabilities quickly gets so tiny we can   t represent

the numbers accurately, even with double precision    oating point.

    so in practice, we typically use negative log probabilities (sometimes called

costs):
    since probabilities range from 0 to 1, negative log probs range from 0 to    :

lower cost = higher id203.

    instead of multiplying probabilities, we add neg log probabilities.

nathan schneider

enlp lecture 5

36

interim summary: id165 probabilities

       id203 of a sentence   : how likely is it to occur in natural

language?

useful in many natural language applications.

    we can never know the true id203, but we may be able to estimate it

from corpus data.

    n -gram models are one way to do this:

    to alleviate sparse data, assume word probs depend only on short history.
    tradeo   : longer histories may capture more, but are also more sparse.
    so far, we estimated n -gram probabilites using id113.

nathan schneider

enlp lecture 5

37

interim summary: language models

    language models tell us p ( (cid:126)w) = p (w1 . . . wn): how likely to occur is this

sequence of words?

roughly: is this sequence of words a    good    one in my language?

    lms are used as a component in applications such as id103,

machine translation, and predictive text completion.

    to reduce sparse data, id165 lms assume words depend only on a    xed-

length history, even though we know this isn   t true.

coming up next:
    weaknesses of id113 and ways to address them (more issues with sparse data).
    how to evaluate a language model: are we estimating sentence probabilities

accurately?

nathan schneider

enlp lecture 5

38

evaluating a language model

    intuitively, a trigram model captures more context than a bigram model, so

should be a    better    model.

    that is, it should more accurately predict the probabilities of sentences.
    but how can we measure this?

nathan schneider

enlp lecture 5

39

two types of evaluation in nlp

    extrinsic: measure performance on a downstream application.
    for lm, plug it into a machine translation/asr/etc system.
    the most reliable evaluation, but can be time-consuming.
    and of course, we still need an evaluation measure for the downstream

system!

    intrinsic: design a measure that is inherent to the current task.

    can be much quicker/easier during development cycle.
    but not always easy to    gure out what the right measure is:

ideally, one

that correlates well with extrinsic measures.

let   s consider how to de   ne an intrinsic measure for lms.

nathan schneider

enlp lecture 5

40

id178

    de   nition of the id178 of a random variable x:

h(x) =(cid:80)

x    p (x) log2 p (x)

    intuitively: a measure of uncertainty/disorder
    also: the expected value of     log2 p (x)

nathan schneider

enlp lecture 5

41

id178 example

one event (outcome)

p (a) = 1

h(x) =     1 log2 1

= 0

nathan schneider

enlp lecture 5

42

id178 example

2 equally likely events:

p (a) = 0.5
p (b) = 0.5

h(x) =     0.5 log2 0.5     0.5 log2 0.5

=     log2 0.5
= 1

nathan schneider

enlp lecture 5

43

id178 example

p (a) = 0.25
p (b) = 0.25
p (c) = 0.25
p (d) = 0.25

4 equally likely events:
h(x) =     0.25 log2 0.25     0.25 log2 0.25
    0.25 log2 0.25     0.25 log2 0.25

=     log2 0.25
= 2

nathan schneider

enlp lecture 5

44

p (a) = 0.7
p (b) = 0.1
p (c) = 0.1
p (d) = 0.1

id178 example

3 equally likely events and one more
likely than the others:

h(x) =     0.7 log2 0.7     0.1 log2 0.1
    0.1 log2 0.1     0.1 log2 0.1
=     0.7 log2 0.7     0.3 log2 0.1
=     (0.7)(   0.5146)     (0.3)(   3.3219)
= 0.36020 + 0.99658

= 1.35678

nathan schneider

enlp lecture 5

45

p (a) = 0.97
p (b) = 0.01
p (c) = 0.01
p (d) = 0.01

id178 example

3 equally likely events and one much
more likely than the others:

h(x) =     0.97 log2 0.97     0.01 log2 0.01
    0.01 log2 0.01     0.01 log2 0.01
=     0.97 log2 0.97     0.03 log2 0.01
=     (0.97)(   0.04394)     (0.03)(   6.6439)
= 0.04262 + 0.19932

= 0.24194

nathan schneider

enlp lecture 5

46

h(x) = 0

h(x) = 1

h(x) = 2

h(x) = 3

h(x) = 1.35678

h(x) = 0.24194

nathan schneider

enlp lecture 5

47

id178 as y/n questions

how many yes-no questions (bits) do we need to    nd out the outcome?
    uniform distribution with 2n outcomes: n q   s.
    other cases: id178 is the average number of questions per outcome in
a (very) long sequence of outcomes, where questions can consider multiple
outcomes at once.

nathan schneider

enlp lecture 5

48

id178 as encoding sequences

    assume that we want to encode a sequence of events x.
    each event is encoded by a sequence of bits, we want to use as few bits as

possible.

    for example

    coin    ip: heads = 0, tails = 1
    4 equally likely events: a = 00, b = 01, c = 10, d = 11
    3 events, one more likely than others: a = 0, b = 10, c = 11
    morse code: e has shorter code than q

    average number of bits needed to encode x     id178 of x

nathan schneider

enlp lecture 5

49

the id178 of english

    given the start of a text, can we guess the next word?
    for humans, the measured id178 is only about 1.3.

    meaning: on average, given the preceding context, a human would need

only 1.3 y/n questions to determine the next word.

    this is an upper bound on the true id178, which we can never know

(because we don   t know the true id203 distribution).

    but what about n -gram models?

nathan schneider

enlp lecture 5

50

cross-id178

    our lm estimates the id203 of word sequences.
    a good model assigns high id203 to sequences that actually have high

id203 (and low id203 to others).

    put another way, our model should have low uncertainty (id178) about which

word comes next.

    we can measure this using cross-id178.
    note that cross-id178     id178: our model   s uncertainty can be no less

than the true uncertainty.

nathan schneider

enlp lecture 5

51

computing cross-id178

    for w1 . . . wn with large n, per-word cross-id178 is well approximated by:

hm(w1 . . . wn) =     1
n

log2 pm(w1 . . . wn)

    this is just the average negative log prob our model assigns to each word in

the sequence. (i.e., normalized for sequence length).

    lower cross-id178     model is better at predicting next word.

nathan schneider

enlp lecture 5

52

cross-id178 example

using a bigram model from moby dick, compute per-word cross-id178 of i
spent three years before the mast (here, without using end-of sentence padding):

   1
7(

lg2(p (i)) + lg2(p (spent|i)) + lg2(p (three|spent)) + lg2(p (years|three))
+ lg2(p (before|years)) + lg2(p (the|before)) + lg2(p (mast|the))

)

7(    6.9381     11.0546     3.1699     4.2362     5.0     2.4426     8.4246 )
=    1
=    1
7(
   
6

41.2660 )

    per-word cross-id178 of the unigram model is about 11.
    so, unigram model has about 5 bits more uncertainty per word then bigram

model. but, what does that mean?

nathan schneider

enlp lecture 5

53

data compression

    if we designed an optimal code based on our bigram model, we could encode

the entire sentence in about 42 bits.

    a code based on our unigram model would require about 77 bits.
    ascii uses an average of 24 bits per word (168 bits total)!
    so better language models can also give us better data compression: as

elaborated by the    eld of id205.

nathan schneider

enlp lecture 5

54

perplexity

    lm performance is often reported as perplexity rather than cross-id178.
    perplexity is simply 2cross-id178
    the average branching factor at each decision point, if our distribution were

uniform.

    so, 6 bits cross-id178 means our model perplexity is 26 = 64: equivalent

uncertainty to a uniform distribution over 64 outcomes.

nathan schneider

enlp lecture 5

55

interpreting these measures

i measure the cross-id178 of my lm on some corpus as 5.2.
is that good?

nathan schneider

enlp lecture 5

56

interpreting these measures

i measure the cross-id178 of my lm on some corpus as 5.2.
is that good?
    no way to tell! cross-id178 depends on both the model and the corpus.

    some language is simply more predictable (e.g. casual speech vs academic

writing).

    so lower cross-id178 could mean the corpus is    easy   , or the model is

good.

    we can only compare di   erent models on the same corpus.
    should we measure on training data or held-out data? why?

nathan schneider

enlp lecture 5

57

sparse data, again

suppose now we build a trigram model from moby dick and evaluate the same
sentence.
    but i spent three never occurs, so pm le(three | i spent) = 0
    which means the cross-id178 is in   nite.
    basically right: our model says i spent three should never occur, so our model

is in   nitely wrong/surprised when it does!

    even with a unigram model, we will run into words we never saw before. so
even with short n -grams, we need better ways to estimate probabilities from
sparse data.

nathan schneider

enlp lecture 5

58

smoothing

    the    aw of id113:

it estimates probabilities that make the training data
maximally probable, by making everything else (unseen data) minimally
probable.

    smoothing methods address the problem by stealing id203 mass from

seen events and reallocating it to unseen events.

    lots of di   erent methods, based on di   erent kinds of assumptions. we will

discuss just a few.

nathan schneider

enlp lecture 5

59

add-one (laplace) smoothing

    just pretend we saw everything one more time than we did.
c(wi   2, wi   1, wi)

pml(wi|wi   2, wi   1) =

c(wi   2, wi   1)

   

p+1(wi|wi   2, wi   1) =

c(wi   2, wi   1, wi) + 1

c(wi   2, wi   1)

?

nathan schneider

enlp lecture 5

60

add-one (laplace) smoothing

    just pretend we saw everything one more time than we did.
c(wi   2, wi   1, wi)

pml(wi|wi   2, wi   1) =

c(wi   2, wi   1)

   

p+1(wi|wi   2, wi   1) =

c(wi   2, wi   1, wi) + 1

c(wi   2, wi   1)
    no! sum over possible wi (in vocabulary v ) must equal 1:

?

(cid:88)

wi   v

p (wi|wi   2, wi   1) = 1

    if increasing the numerator, must change denominator too.

nathan schneider

enlp lecture 5

61

add-one smoothing: id172

(cid:88)

wi   v

c(wi   2, wi   1, wi) + 1

c(wi   2, wi   1) + x

= 1

    we want:

    solve for x: (cid:88)
(cid:88)

wi   v

(c(wi   2, wi   1, wi) + 1) = c(wi   2, wi   1) + x

wi   v
c(wi   2, wi   1, wi) +

(cid:88)

wi   v

1 = c(wi   2, wi   1) + x

c(wi   2, wi   1) + v = c(wi   2, wi   1) + x

v = x

where v = vocabulary size.

nathan schneider

enlp lecture 5

62

add-one example (1)

    moby dick has one trigram that begins i spent (it   s i spent in) and the

vocabulary size is 17231.

    comparison of id113 vs add-one id203 estimates:

  p (three | i spent)
  p (in | i spent)

id113
0
1

+1
0.00006
0.0001

      p (in|i spent) seems very low, especially since in is a very common word. but

can we    nd better evidence that this method is    awed?

nathan schneider

enlp lecture 5

63

add-one example (2)

    suppose we have a more common bigram w1, w2 that occurs 100 times, 10 of

which are followed by w3.

  p (w3|w1, w2)

id113
10
100

+1
11

17331

    0.0006

    shows that the very large vocabulary size makes add-one smoothing steal way

too much from seen events.

    in fact, id113 is pretty good for frequent events, so we shouldn   t want to

change these much.

nathan schneider

enlp lecture 5

64

add-   (lidstone) smoothing

    we can improve things by adding    < 1.

p+  (wi|wi   1) =

c(wi   1, wi) +   
c(wi   1) +   v

    like laplace, assumes we know the vocabulary size in advance.
    but if we don   t, can just add a single    unknown    (unk) item to the vocabulary,

and use this for all unknown words during testing.

    then: how to choose   ?

nathan schneider

enlp lecture 5

65

optimizing    (and other model choices)

    use a three-way data split: training set (80-90%), held-out (or development)

set (5-10%), and test set (5-10%)

    train model (estimate probabilities) on training set with di   erent values of

  

    choose the    that minimizes cross-id178 on development set
    report    nal results on test set.

    more generally, use dev set for evaluating di   erent models, debugging, and

optimizing choices. test set simulates deployment, use it only once!

    avoids over   tting to the training set and even to the test set.

nathan schneider

enlp lecture 5

66

summary

    we can measure the relative goodness of lms on the same corpus using

cross-id178: how well does the model predict the next word?

    we need smoothing to deal with unseen n -grams.
    add-1 and add-   are simple, but not very good.
    we   ll see even better smoothing methods next time.

nathan schneider

enlp lecture 5

67

