empirical methods in

natural language processing

lecture 17

id33

(transition-based slides from harry eldridge)

4 march 2018

nathan schneider

enlp lecture 17

0

dependency parse

saw

    

hhhh

kids birds

   sh

with

saw

           

hhhhhhhhhhh

kids

birds binoculars

with

linguists have long observed that the meanings of words within
a sentence depend on one another, mostly in asymmetric, binary
relations.

    though some constructions don   t cleanly    t this pattern: e.g.,

coordination, relative clauses.

nathan schneider

enlp lecture 17

1

dependency parse
saw
saw

    

hhhh

kids birds

   sh

with

           

hhhhhhhhhhh

kids

birds binoculars

with

equivalently, but showing word order (head ! modi   er):

kids

saw

birds

with

   sh

because it is a tree, every word has exactly one parent.

nathan schneider

enlp lecture 17

2

content vs. functional heads

some treebanks prefer content heads:

little

kids

were

always

watching

birds

with

   sh

others prefer functional heads:

little

kids

were

always

watching

birds

with

   sh

nathan schneider

enlp lecture 17

3

edge labels

it is often useful to distinguish di   erent kinds of head ! modi   er
relations, by labeling edges:
root

pobj

sbj

dobj

prep

kids

saw

birds

with

   sh

important relations for english include subject, direct object,
determiner, adjective modi   er, adverbial modi   er, etc.
treebanks use somewhat di   erent label sets.)

(di   erent

    how would you identify the subject in a constituency parse?

nathan schneider

enlp lecture 17

4

dependency paths

for information extraction tasks involving real-world relationships
between entities, chains of dependencies can provide good features:

nsubj

aux

pobj

poss

amod

prep pobj

aux

prep

amod

british o cials in tehran have been meeting with their iranian counterparts

(example from brendan o   connor)

nathan schneider

enlp lecture 17

5

projectivity

    a sentence   s dependency parse is said to be projective if every
subtree (node and all its descendants) occupies a contiguous span
of the sentence.

    = the dependency parse can be drawn on top of the sentence

without any crossing edges.

sbj

root

att

att

pc

att

vc

tmp

a

hearing

on

the

issue

is

scheduled

today

nathan schneider

enlp lecture 17

6

nonprojectivity

    other sentences are nonprojective:

root

att

att

sbj

vc

tmp

pc

att

a

hearing

is

scheduled

on

the

issue

today

    nonprojectivity is rare in english, but quite common in many

languages.

nathan schneider

enlp lecture 17

7

id33

some of the algorithms you have seen for pid18s can be adapted to
id33.

    cky can be adapted, though e ciency is a concern: obvious
approach is o(gn5); eisner algorithm brings it down to o(gn3)

    n. smith   s slides explaining the eisner algorithm: http://

courses.cs.washington.edu/courses/cse517/16wi/slides/
an-dep-slides.pdf

nathan schneider

enlp lecture 17

8

transition-based parsing

    adapts shift-reduce methods: stack and bu   er
    remember:

latent structure is just edges between words.
train a classi   er to predict next action (shift, reduce,
attach-left, or attach-right), and proceed left-to-right
through the sentence. o(n) time complexity!

    only    nds projective trees (without special extensions)
    pioneering system: nivre   s maltparser
    see http://spark-public.s3.amazonaws.com/nlp/slides/
parsing-dependency.pdf (jurafsky & manning coursera
slides) for details and examples

nathan schneider

enlp lecture 17

9

a quick dependency parse:

the dog bit the boy

root

dobj

det

det

nsubj

the dog bit the boy

why is this useful?

why is this useful?

- conveys some level of semantic meaning
- good for languages with freer word order

transition based id33

- high level idea

- process words from left to right

transition based id33

- high level idea

- process words from left to right
- at each stage, decide if two words should be attached

transition based id33

- similar to id132 for programming languages
- 3 components

input buffer (the words of the sentence)

-
- stack (where the words are moved to and manipulated)
- dependency relations (the list of relations between words that becomes the dependency 

parse)

- configuration: some state of the 3 components
- parsing consists of a sequence of transitions between configurations until all 

the words have been accounted for

-

the available transitions define the type of approach

the arc-standard approach

- leftarc: assert a head-dependent relation between the word at the top of 

the stack and the word directly beneath it; remove the lower word from the 
stack

- rightarc: assert a head-dependent relation between the second word on 

the stack and the word at the top; remove the word at the top of the stack

- shift: remove the word from the front of the input buffer and push it onto 

the stack

restrictions

- leftarc cannot be applied when the root is the second element of the stack 

(the root cannot be a dependent)

- leftarc & rightarc can only be applied if there are 2 or more elements 

on the stack. 

she gave me the book

root

dobj

nsubj

iobj

det

she gave me the book

stack
[root]

word list

[she, gave, me, the, book]

relations

word list

[she, gave, me, the, book]

[gave, me, the, book]

relations

stack
[root]

[root, she]

operation: shift

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]

relations

stack
[root]

[root, she]

[root, she, gave]

operation: shift

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

relations

(she     gave)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

operation: leftarc

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]
[me, the, book]

relations

(she     gave)
(root     gave)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root]

operation: rightarc?

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]

relations

(she     gave)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

operation: shift!

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]
[the, book]

relations

(she     gave)

(gave     me)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

[root, gave]

operation: rightarc

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]
[the, book]

[book]

relations

(she     gave)

(gave     me)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

[root, gave]

[root, gave, the]

operation: shift

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

[root, gave]

[root, gave, the]

[root, gave, the, book]

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]
[the, book]

[book]

[]

relations

(she     gave)

(gave     me)

operation: shift

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]
[the, book]

[book]

[]
[]

relations

(she     gave)

(gave     me)

(the     book)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

[root, gave]

[root, gave, the]

[root, gave, the, book]

[root, gave, book]

operation: leftarc

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]
[the, book]

[book]

[]
[]
[]

relations

(she     gave)

(gave     me)

(the     book)
(gave     book)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

[root, gave]

[root, gave, the]

[root, gave, the, book]

[root, gave, book]

[root, gave]

operation: rightarc

word list

[she, gave, me, the, book]

[gave, me, the, book]

[me, the, book]
[me, the, book]

[the, book]
[the, book]

[book]

[]
[]
[]
[]

relations

(she     gave)

(gave     me)

(the     book)
(gave     book)
(root     gave)

stack
[root]

[root, she]

[root, she, gave]

[root, gave]

[root, gave, me]

[root, gave]

[root, gave, the]

[root, gave, the, book]

[root, gave, book]

[root, gave]

[root]

operation: rightarc

run time

run time

- linear in the size of the sentence

run time

- linear in the size of the sentence
- a head decision for each word uniquely defines a tree

how to decide what to do at each step? 

how to decide what to do at each step? 

- build an oracle

how to decide what to do at each step? 

- build an oracle with machine learning!
- need something that maps configurations to transitions

how to decide what to do at each step? 

- build an oracle with machine learning!
- need something that maps configurations to transitions
- data comes from treebanks

how to decide what to do at each step? 

- build an oracle with machine learning!
- need something that maps configurations to transitions
- data comes from treebanks

- corpora annotated with gold trees
-

http://universaldependencies.org/

how to decide what to do at each step? 

- build an oracle with machine learning!
- need something that maps configurations to transitions
- data comes from treebanks

- corpora annotated with gold trees
-

http://universaldependencies.org/

- best results have historically come from multinomial id28 and 

id166 models. 

how to decide what to do at each step? 

- build an oracle with machine learning!
- need something that maps configurations to transitions
- data comes from treebanks

- corpora annotated with gold trees
-

http://universaldependencies.org/

- best results have historically come from multinomial id28 and 

id166 models. 

- recently, neural networks have been performing well

how to decide what to do at each step? 

- build an oracle with machine learning!
- need something that maps configurations to transitions
- data comes from treebanks

- corpora annotated with gold trees
-

http://universaldependencies.org/

- best results have historically come from multinomial id28 and 

id166 models. 

- recently, neural networks have been performing well.

- naturally lend themselves to the task

forms analysis before reading in the whole sentence

-
- neural networks model a sequence of decisions, which is exactly how the parsing 

operates

possible features?

possible features?

- some obvious ones, the word currently at the top of the stack, etc.

possible features?

- some obvious ones, the word currently at the top of the stack, etc.
- pos tags are also very useful

possible features?

- some obvious ones, the word currently at the top of the stack, etc.
- pos tags are also very useful

- usually a pos tagged is run and used as input to the dependency parser

edge labels

- the example only dealt with connections
- can modify the oracle to learn and output the transition, as well as the arc 

label at each step (if rightarc or leftarc is called)

possible weaknesses? 

possible weaknesses? 

- can only produce projective parses

weaknesses of dependency parses

weakness of dependency parses

- head-modifier relation doesn   t always work neatly
- coordination

-

   cats and dogs ran.   

- auxiliaries

-

   do you want coffee?   

- relative clauses

-

   i met the girl who started this year   

- prepositional phrases: 
   i saw a cow in the barn   

-

advanced methods

- arc eager transition system

 

advanced methods

- arc eager transition system

- we couldn   t add the arc between root and gave because gave still needed to point to other 

-

words
in general, the longer a word has to wait to get assigned its head the more opportunities there 
are for something to go awry

 

advanced methods

- arc eager transition system

- we couldn   t add the arc between root and gave because gave still needed to point to other 

-

words
in general, the longer a word has to wait to get assigned its head the more opportunities there 
are for something to go awry

- solution: change the set of operators

 

new operators

- leftarc: assert a head-dependent relation between the word at the front of 

the input buffer and the word at the top of the stack; pop the stack.

- rightarc: assert a head-dependent relation between the word on the top 
of the stack and the word at the front of the input buffer; shift the word at the 
front of the input buffer to the stack.

- shift: remove the word from the front of the input buffer and push it onto 

the stack (stays the same). 

- reduce: pop the stack.

advanced methods

- arc eager transition system

- we couldn   t add the arc between root and gave because gave still needed to point to other 

-

words
in general, the longer a word has to wait to get assigned its head the more opportunities there 
are for something to go awry

- graph based methods

- can think of dependency parses as a directed graph with arc labels
- other methods use graph based algorithms to find the best dependency parse

 

graph-based parsing

    global algorithm: from the fully connected directed graph of all

possible edges, choose the best ones that form a tree.

    edge-factored models: classi   er assigns a nonnegative score to
each possible edge; maximum spanning tree algorithm    nds the
spanning tree with highest total score in o(n2) time.
    edge-factored assumption can be relaxed (higher-order models

score larger units; more expensive).

    unlabeled parse ! edge-labeling classi   er (pipeline).
    pioneering work: mcdonald   s mstparser
    can be formulated as constraint-satisfaction with integer linear

programming (martins   s turboparser)

nathan schneider

enlp lecture 17

10

graph-based vs. transition-based

vs. conversion-based

    tb: features in scoring function can look at any part of the stack;
linear-time; (classically)

no optimality guarantees for search;
projective only

    gb: features

in scoring function limited by factorization;
optimal search within that model; quadratic-time; no projectivity
constraint

    cb: in terms of accuracy, sometimes best to    rst constituency-
parse, then convert to dependencies (e.g., stanford parser).
slower than direct methods.

nathan schneider

enlp lecture 17

11

id33 evaluation

training and evaluation, we

can automatically convert
for
constituency treebanks (like the id32) to dependencies   
see
like
universal dependencies, available in many languages (http:
//universaldependencies.org).

below   or we

dependency

treebanks

can

use

standard metrics for comparing against a gold standard are:

    uas (unlabeled attachment score): % of words attached correctly

(correct head)

    las (labeled attachment score): % of words attached to the

correct head with the correct relation label

nathan schneider

enlp lecture 17

12

choosing a parser: criteria

    target representation: constituency or dependency?
    e ciency? in practice, both runtime and memory use.
    incrementality: parse the whole sentence at once, or obtain partial

left-to-right analyses/expectations?

    retrainable system?

nathan schneider

enlp lecture 17

13

advanced topic: relationship between

constituency and dependency parses

constituency parses/grammars can be extended with a notion of
lexical head, which can

    improve constituency parsing, or
    help convert a constituency parse to a dependency parse

nathan schneider

enlp lecture 17

14

vanilla pid18s: no lexical dependencies

replacing one word with another with the same pos will never
result in a di   erent parsing decision, even though it should!

    kids saw birds with    sh vs.
kids saw birds with binoculars

    she stood by the door covered in tears vs.
she stood by the door covered in ivy

    stray cats and dogs vs.
siamese cats and dogs

nathan schneider

enlp lecture 17

15

a way to    x pid18s: lexicalization

create new categories, this time by adding the lexical head of the
phrase (note: n level under nps not shown for brevity):

np-kids

kids

             

s-saw

hhhhhhhhhhhhh
            
vp-saw
hhhhhh
      
np-birds

v-saw

vp-saw

hhhhhhhhhhhh
pp-binoculars
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

    now consider:
vp-saw! vp-saw pp-fish vs. vp-saw! vp-saw pp-binoculars

nathan schneider

enlp lecture 17

16

how to get lexical heads?

nathan schneider

enlp lecture 17

17

head rules

the standard solution is to use head rules:
for every non-unary
(p)id18 production, designate one rhs nonterminal as containing
the head. s ! np vp, vp ! vp pp, pp ! p np (content head), etc.

s
hhhhhhhhh
         
vp
np
hhhhhhhh
        
pp
vp
hhhhh
     
p

    
v

np

hhhh

kids

np

saw

birds

with

binoculars

    heuristics to scale this to large grammars: e.g., within an np, last

immediate n child is the head.

nathan schneider

enlp lecture 17

18

then, propagate heads up the tree:

head rules

np-kids

kids

s

             

vp

hhhhhhhhhhhhh
            
vp
hhhhhh
      
np-birds

v-saw

hhhhhhhhhhhh
pp
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

nathan schneider

enlp lecture 17

19

then, propagate heads up the tree:

head rules

np-kids

kids

s

             

vp

hhhhhhhhhhhhh
            
vp-saw
hhhhhh
      
np-birds

v-saw

hhhhhhhhhhhh
pp
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

nathan schneider

enlp lecture 17

20

then, propagate heads up the tree:

head rules

np-kids

kids

s

             

vp

hhhhhhhhhhhhh
            
vp-saw
hhhhhh
      
np-birds

v-saw

hhhhhhhhhhhh
pp-binoculars
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

nathan schneider

enlp lecture 17

21

then, propagate heads up the tree:

head rules

np-kids

kids

s

             

vp-saw

hhhhhhhhhhhhh
            
vp-saw
hhhhhh
      
np-birds

v-saw

hhhhhhhhhhhh
pp-binoculars
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

nathan schneider

enlp lecture 17

22

then, propagate heads up the tree:

head rules

np-kids

kids

             

s-saw

hhhhhhhhhhhhh
            
vp-saw
hhhhhh
      
np-birds

v-saw

vp-saw

hhhhhhhhhhhh
pp-binoculars
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

nathan schneider

enlp lecture 17

23

lexicalized constituency parse (reading 1)

np-kids

kids

             

s-saw

hhhhhhhhhhhhh
            
vp-saw
      
hhhhhh
np-birds

v-saw

vp-saw

hhhhhhhhhhhh
pp-binoculars
hhhhhhh
       
np-binoculars

p-with

saw

birds

with

binoculars

nathan schneider

enlp lecture 17

24

lexicalized constituency parse (reading 2)

s-saw

          

hhhhhhhhhh
         

vp-saw

np-kids

kids

hhhhhhhhh
        

np-birds

hhhhhhhh

v-saw

saw

np-birds

birds

pp-   sh
     
hhhhh
np-   sh
p-with

with

   sh

nathan schneider

enlp lecture 17

25

constituency tree ! dependency tree

the lexical heads can then be used to collapse down to an unlabeled
dependency tree.

nathan schneider

enlp lecture 17

26

lexicalized constituency parse

s-saw

          

hhhhhhhhhh
         

vp-saw

np-kids

kids

hhhhhhhhh
        

np-birds

hhhhhhhh

v-saw

saw

np-birds

birds

pp-   sh
     
hhhhh
np-   sh
p-with

with

   sh

nathan schneider

enlp lecture 17

27

. . . remove the phrasal categories. . .

saw
hhhhhhh
       
saw

kids

kids

      

hhhhhh

saw

saw

birds

      

hhhhhh

birds

birds

   sh

    
with

hhhh

   sh

with

   sh

nathan schneider

enlp lecture 17

28

. . . remove the (duplicated) terminals. . .

saw
hhhhhhh
       
saw

kids

      

hhhhhh

saw

birds

      

hhhhhh

birds

   sh

    

hhhh

with    sh

nathan schneider

enlp lecture 17

29

. . . and collapse chains of duplicates. . .

saw
hhhhhhh
       
saw

kids

      

hhhhhh

saw

birds

      

hhhhhh

birds

   sh

    

hhhh

with    sh

nathan schneider

enlp lecture 17

30

. . . and collapse chains of duplicates. . .

saw

      

hhhhhh

kids

saw

      

hhhhhh

saw

birds
hhhh
    
birds

   sh

with

nathan schneider

enlp lecture 17

31

. . . and collapse chains of duplicates. . .

saw

      

hhhhhh

kids

saw

      

hhhhhh

saw

birds
hhhh
    
birds

   sh

with

nathan schneider

enlp lecture 17

32

. . . and collapse chains of duplicates. . .

saw

     

hhhhh

kids

saw

    

hhhh

saw birds

   sh

with

nathan schneider

enlp lecture 17

33

. . . and collapse chains of duplicates. . .

saw

     

hhhhh

kids

saw

    

hhhh

saw birds

   sh

with

nathan schneider

enlp lecture 17

34

. . . and collapse chains of duplicates. . .

saw

    

hhhh

kids birds

   sh

with

nathan schneider

enlp lecture 17

35

practicalities of lexicalized id18

constituency parsing

    leads to huge grammar blowup and very sparse data (bad!)
    there are fancy techniques to address these issues. . . and they

can work pretty well.

    but: do we really need phrase structures in the    rst place?

not always!

    hence: sometimes we want to parse directly to dependencies, as

with transition-based or graph-based algorithms.

nathan schneider

enlp lecture 17

36

summary

    while constituency parses give hierarchically nested phrases,
dependency parses represent syntax with trees whose edges
connect words in the sentence.
(no abstract phrase categories
like np.) edges often labeled with relations like subject.

    head rules govern how a lexicalized constituency grammar can be
extracted from a treebank, and how a constituency parse can be
coverted to a dependency parse.

    for english,

it is often fastest and most convenient to parse
directly to dependencies. two main paradigms, graph-based
and transition-based, with di   erent kinds of models and search
algorithms.

nathan schneider

enlp lecture 17

37

