lecture 13: annotation

nathan schneider

(with material from henry thompson, alex lascarides)

enlp | 12 march 2018

1 / 14

annotation

why    gold    (cid:54)= perfect

quality control

2 / 14

factors in annotation

suppose you are tasked with building an annotated corpus. (e.g., with
part-of-speech tags.) in order to estimate cost in time and money, you
need to decide on:

(cid:73) source data (genre? size? licensing?)
(cid:73) annotation scheme (complexity? guidelines?)
(cid:73) annotators (expertise? training?)
(cid:73) annotation software (graphical interface?)
(cid:73) quality control procedures (multiple annotation, adjudication?)

3 / 14

annotation scheme

(cid:73) assuming a competent annotator, some kinds of annotation are

straightforward for most inputs.

(cid:73) others are not.

(cid:73) text may be ambiguous
(cid:73) there may be gray area between categories in the annotation

scheme

4 / 14

you play annotator

noun or adverb?

(cid:73) yesterday was my birthday .
(cid:73) yesterday i ate a cake .
(cid:73) he was    red yesterday for leaking the information .
(cid:73) i read it in yesterday    s news .
(cid:73) i had not heard of it until yesterday .

5 / 14

you play annotator

verb, noun, or adjective?

(cid:73) we had been walking quite briskly
(cid:73) walking was the remedy, they decided
(cid:73) in due time sandburg was a walking thesaurus of american folk

music.

(cid:73) we all lived within walking distance of the studio
(cid:73) a woman came along carrying a folded umbrella as a walking stick
(cid:73) the walking dead premiered in the u.s. on october 31, 2010, on

the cable television channel amc

6 / 14

annotation: not as easy as you might think

pretty much any annotation scheme for language will have some
di   cult cases where there is gray area, and multiple decisions are
plausible.

(cid:73) because human language needs to be    exible, it cuts corners and

is reshaped over time.

(cid:73) not just syntax: wait till we get to semantics!

7 / 14

annotation guidelines

however, we want a dataset   s annotations to be as clean as possible so
we can use them reliably in systems.

documenting conventions in an annotation manual/standard/guidelines
document is important to help annotators produce consistent data,
and to help end users interpret the annotations correctly.

8 / 14

annotation guidelines

(cid:73) id32: 36 pos tags (excluding punctuation).
(cid:73) tagging guidelines (3rd revision): 34 pages

(cid:73)    the temporal expressions yesterday, today and tomorrow should
be tagged as nouns (nn) rather than as adverbs (rb). note that
you can (marginally) pluralize them and that they allow a
possessive form, both of which true adverbs do not.    (p. 19)

(cid:73) an entire page on nouns vs. verbs.
(cid:73) 3 pages on adjectives vs. verbs.

(cid:73) id32 bracketing (tree) guidelines: >300 pages!

9 / 14

annotation quality

but even with extensive guidelines, human annotations won   t be
perfect:

(cid:73) simple error (hitting the wrong button)
(cid:73) not reading the full context
(cid:73) not noticing an erroneous pre-annotation
(cid:73) forgetting a detail from the guidelines
(cid:73) cases not anticipated by or not fully speci   ed in guidelines (room

for interpretation)

   gold    data will have some tarnish. how can we measure its quality?

10 / 14

inter-annotator agreement (iaa)

(cid:73) an important way to estimate the reliability of annotations is to
have multiple people independently annotate a common sample,
and measure inter-annotator/coder/rater agreement.

(cid:73) raw agreement rate: proportion of labels in agreement
(cid:73) if the annotation task is perfectly well-de   ned and the annotators
are well-trained and do not make mistakes, then (in theory) they
would agree 100%.

(cid:73) if agreement is well below what is desired (will di   er depending on
the kind of annotation), examine the sources of disagreement and
consider additional training or re   ning guidelines.

(cid:73) the agreement rate can be thought of as an upper bound (human

ceiling) on accuracy of a system evaluated on that dataset.

11 / 14

iaa: beyond raw agreement rate

(cid:73) raw agreement rate counts all annotation decisions equally.
(cid:73) some measures take knowledge about the annotation scheme into

account (e.g., counting singular vs. plural noun as a minor
disagreement compared to noun vs. preposition).

(cid:73) what if some decisions (e.g., pos tags) are far more frequent

than others?

(cid:73) if 2 annotators both tagged hell as a noun, what is the chance that

they agreed by accident? what if they agree that it is an
interjection (rare tag)   is that equally likely to be an accident?

(cid:73) chance-corrected measures such as cohen   s kappa (  ) adjust the

agreement score based on label probabilities. (cohen   s assumes 2
raters, categorical labels)

(cid:73) . . . but they make modeling assumptions about how    accidental   
agreement would arise; important that these match the reality of
the annotation process!

12 / 14

cohen   s   

(cid:73) 2 raters (annotators a and b), categorical labels (y1, y2, . . . )
(cid:73) from interannotator confusion matrix, compute:

(cid:73) observed id203 of agreement (i.e., raw agreement rate):

po =   p(a = b = y1) +   p(a = b = y2) +       

(cid:73) expected agreement by chance if annotators    decisions were

independent:
pe =   p(a = y1)   p(b = y1) +   p(a = y2)   p(b = y2) +       

(cid:73) agreement above chance:

   =

po     pe
1     pe

(cid:73) interpretation of    is subjective.

(cid:73) landis and koch (1977): 0   0.20 is    slight    agreement, 0.21   0.40 is

   fair   , 0.41   0.60 is    moderate   , 0.61   0.80 is    substantial   , and
0.81   1 is    almost perfect   

(cid:73) assumes that chance is random guessing according to one   s overall

preferences   not always realistic!

(cid:73) tends to underestimate agreement for rare labels.

13 / 14

id104

(cid:73) quality control is even more important when eliciting annotations

from    the crowd   .

(cid:73) e.g., amazon mechanical turk facilitates paying anonymous
web users small amounts of money for small amounts of work
(   human intelligence tasks   ).

(cid:73) need to take measures to ensure annotators are quali   ed and

taking the task seriously.

(cid:73) redundancy to combat noise: elicit 5+ annotations per data point.
(cid:73) embed data points with known answers, reject annotators who get

them wrong.

14 / 14

