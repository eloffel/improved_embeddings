empirical methods in natural language processing

lecture 1

introduction

(today   s slides based on those of sharon goldwater, philipp koehn, alex lascarides)

10 january 2018

nathan schneider

enlp (cosc/ling-572) lecture 1

10 january 2018

what is natural language processing?

nathan schneider

enlp (cosc/ling-572) lecture 1

1

what is natural language processing?

applications
    machine translation
    information retrieval
    id53
    dialogue systems
    information extraction
    summarization
    id31
    ...

core technologies
    language modelling
    part-of-speech tagging
    syntactic parsing
    named-entity recognition
    coreference resolution
    id51
    semantic role labelling
    ...

nlp lies at the intersection of computational linguistics and arti   cial intelligence. nlp is
(to various degrees) informed by linguistics, but with practical/engineering rather than purely
scienti   c aims. processing speech (i.e., the acoustic signal) is separate.

nathan schneider

enlp (cosc/ling-572) lecture 1

2

this course

nlp is a big    eld! we focus mainly on core ideas and methods needed for
technologies in the second column (and eventually for applications).
    linguistic facts and issues
    computational models and algorithms, especially using data (   empirical   )

nathan schneider

enlp (cosc/ling-572) lecture 1

3

what are your goals?

why are you here? perhaps you want to:
    work at a company that uses nlp (perhaps as the sole language expert among

engineers)

    use nlp tools for research in linguistics (or other domains where text data is

important: social sciences, humanities, medicine, . . . )

    conduct research in nlp (or ir, mt, etc.)

nathan schneider

enlp (cosc/ling-572) lecture 1

4

what does an nlp system need to    know   ?

    language consists of many levels of structure
    humans    uently integrate all of these in producing/understanding language
    ideally, so would a computer!

nathan schneider

enlp (cosc/ling-572) lecture 1

5

words

nathan schneider

enlp (cosc/ling-572) lecture 1

6

discoursethisisasimplesentencebe3sgpresentdtvbzdtjjnnnpvpsnpsentence1string of words satisfying the grammatical rules of a languaugesimple1having few partsbut it is an instructive one.contrastwordsmorphologysyntaxdiscoursepart of speechsemanticssharongoldwaterfnlplecture111of a languageof a languagemorphology

nathan schneider

enlp (cosc/ling-572) lecture 1

7

discoursethisisasimplesentencebe3sgpresentdtvbzdtjjnnnpvpsnpsentence1string of words satisfying the grammatical rules of a languaugesimple1having few partsbut it is an instructive one.contrastwordsmorphologysyntaxdiscoursepart of speechsemanticssharongoldwaterfnlplecture111of a languageof a languageparts of speech

nathan schneider

enlp (cosc/ling-572) lecture 1

8

discoursethisisasimplesentencebe3sgpresentdtvbzdtjjnnnpvpsnpsentence1string of words satisfying the grammatical rules of a languaugesimple1having few partsbut it is an instructive one.contrastwordsmorphologysyntaxdiscoursepart of speechsemanticssharongoldwaterfnlplecture111of a languageof a languagesyntax

nathan schneider

enlp (cosc/ling-572) lecture 1

9

discoursethisisasimplesentencebe3sgpresentdtvbzdtjjnnnpvpsnpsentence1string of words satisfying the grammatical rules of a languaugesimple1having few partsbut it is an instructive one.contrastwordsmorphologysyntaxdiscoursepart of speechsemanticssharongoldwaterfnlplecture111of a languageof a languagesemantics

nathan schneider

enlp (cosc/ling-572) lecture 1

10

discoursethisisasimplesentencebe3sgpresentdtvbzdtjjnnnpvpsnpsentence1string of words satisfying the grammatical rules of a languaugesimple1having few partsbut it is an instructive one.contrastwordsmorphologysyntaxdiscoursepart of speechsemanticssharongoldwaterfnlplecture111of a languageof a languagediscourse

nathan schneider

enlp (cosc/ling-572) lecture 1

11

discoursethisisasimplesentencebe3sgpresentdtvbzdtjjnnnpvpsnpsentence1string of words satisfying the grammatical rules of a languaugesimple1having few partsbut it is an instructive one.contrastwordsmorphologysyntaxdiscoursepart of speechsemanticssharongoldwaterfnlplecture111of a languageof a languagewhy is nlp hard?

1. ambiguity at many levels:
    word senses: bank (   nance or river?)
    part of speech: chair (noun or verb?)
    syntactic structure: i saw a man with a telescope
    quanti   er scope: every child loves some movie
    multiple: i saw her duck

how can we model ambiguity, and choose the correct analysis in context?

nathan schneider

enlp (cosc/ling-572) lecture 1

12

ambiguity

what can we do about ambiguity?
    non-probabilistic methods (id122s for morphology, cky parsers for syntax)

return all possible analyses.

    probabilistic models (id48s for id52, pid18s for syntax) and

algorithms (viterbi, probabilistic cky) return the best possible analysis.

but the    best    analysis is only good if our probabilities are accurate. where do
they come from?

nathan schneider

enlp (cosc/ling-572) lecture 1

13

statistical nlp

like most other parts of ai, nlp is dominated by statistical methods.
    typically more robust than earlier rule-based methods.
    relevant statistics/probabilities are learned from data.
    normally requires lots of data about any particular phenomenon.

nathan schneider

enlp (cosc/ling-572) lecture 1

14

why is nlp hard?

2. sparse data due to zipf   s law.
    to illustrate, let   s look at the frequencies of di   erent words in a large text

corpus.

    assume    word    is a string of

letters

separated by spaces

(a great

oversimpli   cation, we   ll return to this issue...)

nathan schneider

enlp (cosc/ling-572) lecture 1

15

word counts

most frequent words in the english europarl corpus (out of 24m word tokens)

any word

frequency token
1,698,599
849,256
793,731
640,257
508,560
407,638
400,467
394,778
263,040

the
of
to
and
in
that
is
a
i

nouns
frequency token

124,598 european
104,325 mr
92,195 commission
66,781 president
62,867 parliament
57,804 union
report
53,683
53,547 council
45,842

states

nathan schneider

enlp (cosc/ling-572) lecture 1

16

word counts

but also, out of 93,638 distinct words (word types), 36,231 occur only once.
examples:
    corn   akes, mathematicians, fuzziness, jumbling
    pseudo-rapporteur, lobby-ridden, perfunctorily,
    lycketoft, uncitral, h-0695
    policyfor, commissioneris, 145.95, 27a

nathan schneider

enlp (cosc/ling-572) lecture 1

17

plotting word frequencies

order words by frequency. what is the frequency of nth ranked word?

nathan schneider

enlp (cosc/ling-572) lecture 1

18

plottingwordfrequenciesorderwordsbyfrequency.whatisthefrequencyofnthrankedword?sharongoldwaterfnlplecture119plotting word frequencies

order words by frequency. what is the frequency of nth ranked word?

nathan schneider

enlp (cosc/ling-572) lecture 1

19

plottingwordfrequenciesorderwordsbyfrequency.whatisthefrequencyofnthrankedword?sharongoldwaterfnlplecture119plottingwordfrequenciesorderwordsbyfrequency.whatisthefrequencyofnthrankedword?sharongoldwaterfnlplecture119rescaling the axes

really

see
to
what   s going on,
use logarithmic
axes:

nathan schneider

enlp (cosc/ling-572) lecture 1

20

rescalingtheaxestoreallyseewhat   sgoingon,uselogarithmicaxes:sharongoldwaterfnlplecture120nathan schneider

enlp (cosc/ling-572) lecture 1

21

sharongoldwaterfnlplecture121zipf   s law

summarizes the behaviour we just saw:

f    r     k

    f = frequency of a word
    r = rank of a word (if sorted by frequency)
    k = a constant

nathan schneider

enlp (cosc/ling-572) lecture 1

22

zipf   s law

summarizes the behaviour we just saw:

f    r     k

    f = frequency of a word
    r = rank of a word (if sorted by frequency)
    k = a constant

why a line in log-scales?

f r = k     f = k

r     log f = log k     log r

nathan schneider

enlp (cosc/ling-572) lecture 1

23

implications of zipf   s law

    regardless of how large our corpus is, there will be a lot of infrequent (and

zero-frequency!) words.

    in fact, the same holds for many other levels of linguistic structure (e.g.,

syntactic rules in a id18).

    this means we need to    nd clever ways to estimate probabilities for things we

have rarely or never seen.

nathan schneider

enlp (cosc/ling-572) lecture 1

24

why is nlp hard?

3. variation
    suppose we train a part of speech tagger on the wall street journal:

mr./nnp vinken/nnp is/vbz chairman/nn of/in elsevier/nnp
n.v./nnp ,/, the/dt dutch/nnp publishing/vbg group/nn ./.

nathan schneider

enlp (cosc/ling-572) lecture 1

25

why is nlp hard?

3. variation
    suppose we train a part of speech tagger on the wall street journal:

mr./nnp vinken/nnp is/vbz chairman/nn of/in elsevier/nnp
n.v./nnp ,/, the/dt dutch/nnp publishing/vbg group/nn ./.

    what will happen if we try to use this tagger for social media??

ikr smh he asked    r yo last name

twitter example due to noah smith

nathan schneider

enlp (cosc/ling-572) lecture 1

26

why is nlp hard?

4. expressivity
    not only can one form have di   erent meanings (ambiguity) but the same

meaning can be expressed with di   erent forms:

she gave the book to tom vs. she gave tom the book

some kids popped by vs. a few children visited

is that window still open? vs please close the window

nathan schneider

enlp (cosc/ling-572) lecture 1

27

why is nlp hard?

5 and 6. context dependence and unknown representation
    last example also shows that correct interpretation is context-dependent and

often requires world knowledge.

    very di   cult to capture, since we don   t even know how to represent the
knowledge a human has/needs: what is the    meaning    of a word or sentence?
how to model context? other general knowledge?

nathan schneider

enlp (cosc/ling-572) lecture 1

28

organization of topics (1/2)

traditionally, nlp survey courses cover morphology, then syntax, then semantics
and applications. this re   ects the traditional form-focused orientation of the
   eld, but this course will be organized di   erently, with the following units:
    introduction (   4 lectures): getting everyone onto the same page with the

fundamentals of text processing (python 3/unix) and linguistics.

    id165s (   2 lectures): statistical modeling of words and word sequences.
    classi   cation (   3 lectures): approaches to classi   cation that ignore linguistic
structure within a sentence or document, focusing on the individual words/bags
of words.

    sequential prediction (   5 lectures): techniques that assign additional
in sentences by modeling sequential

lingusitic
relationships, including part-of-speech tagging and lexical semantic tagging.

information to words

nathan schneider

enlp (cosc/ling-572) lecture 1

29

organization of topics (2/2)

    hierarchical sentence structure (   5 lectures): tree-based models of
sentences that capture grammatical phrases and relationships (syntactic
structure), as well as structured representations of within-sentence semantic
relationships.

    other learning paradigms and applications (   4 lectures): models for
characterizing words and text collections based on unlabeled data, or nonlinear
models (neural networks) without hand-engineered features; and overviews
of language technologies for text such as machine translation and question
answering.

nathan schneider

enlp (cosc/ling-572) lecture 1

30

backgrounds

this course has enrollment from multiple programs!:
    linguistics
    computer science
    possibly: data analytics; communication, culture & technology; math &

statistics

this means that there will be a diversity of backgrounds and skills, which is a
fantastic opportunity for you to learn from fellow students. it also requires a bit
of care to make sure the course is valuable for everyone.

nathan schneider

enlp (cosc/ling-572) lecture 1

31

what   s not in this course

    formal language theory
    computational morphology
    logic-based id152
    speech/signal processing, id102, phonology

(but see next 2 slides!)

nathan schneider

enlp (cosc/ling-572) lecture 1

32

some related courses as of spring 2018 (1/2)

in linguistics:
    intro to nlp (amir zeldes, last semester)
    signal processing (corey miller, last semester)
    id151 (achim ruopp, this semester)
    computational corpus linguistics (zeldes, last semester)
    computational id59s (zeldes, fall 2016)

nathan schneider

enlp (cosc/ling-572) lecture 1

33

some related courses as of spring 2018 (2/2)

in computer science:
    id151 (achim ruopp, this semester)
    machine learning (mark maloof, spring 2017)
    automated reasoning (maloof, last semester)
    data analytics (lisa singh, last semester)
    information retrieval (nazli goharian, this semester)
    id111 & analysis (goharian, last semester)
    web search and sense-making (yang, this semester)

nathan schneider

enlp (cosc/ling-572) lecture 1

34

course organization

    instructor: nathan schneider
    tas: sean simpson, austin blodgett
    lectures: mw 3:30   4:45, icc 106
    web site:

syllabus,

for

schedule (lecture slides/readings/assignments):

http://tiny.cc/enlp

    make sure to read the syllabus!
    no hard-copy textbook; readings will be posted online.

    we will also use canvas for communication once enrollment is    nalized.

nathan schneider

enlp (cosc/ling-572) lecture 1

35

