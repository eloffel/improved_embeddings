id65

sean simpson
(some slides from marine carpuat, nathan schneider)

enlp lecture 21
april 18, 2018

topics

l lexical semantics

   word similarity
   distributional hypothesis
   vector representations
   evaluation

l document semantics

   id96

lexical semantics

semantic similarity: intuition

l identify word closest to target:

l accidental
   abominate
   meander
   inadvertent
   inhibit

semantic similarity: intuition

l identify word closest to target:

l accidental
   abominate
   meander
   inadvertent
   inhibit

semantic similarity: intuition

l identify word closest to target:

l fedex

   car
   ups
   rotate
   navy

semantic similarity: intuition

l identify word closest to target:

l fedex

   car
   ups
   rotate
   navy

semantic similarity: intuition

l identify word closest to target:

l millennial

   octopus
   fork
   water
   avocado

semantic similarity: intuition

l identify word closest to target:

l millennial

   octopus
   fork
   water
   avocado

semantic similarity
what drives semantic similarity?
l meaning

   the two concepts are close in terms of meaning 
   e.g.    inadvertent    and    accidental   

l world knowledge

   the two concepts have similar properties, often occur together, or occur in 

similar contexts

   e.g.    spinach    and    kale,    or    ups    and    fedex   

l psychology

   the two concepts fit together within an over-arching psychological schema or 

framework

   e.g.    money    and    bank   , or    millennial    and    avocado   

semantic similarity
what drives semantic similarity?
l meaning

   the two concepts are close in terms of meaning 
   e.g.    inadvertent    and    accidental   

l world knowledge

   the two concepts have similar properties, often occur together, or occur in 

similar contexts

   e.g.    spinach    and    kale,    or    ups    and    fedex   

l psychology

   the two concepts fit together within an over-arching psychological schema or 

framework

   e.g.    money    and    bank   , or    millennial    and    avocado   

automatic computation of semantic similarity

why would such a thing be useful?
   semantic similarity gives us a way to generalize beyond word 

identities

   lots of practical applications

   information retrieval
   machine translation
   ontological hierarchies
   etc.

distributional hypothesis

idea: similar linguistic objects have similar contents (for documents, 
paragraphs, sentences) or contexts (for words)

      differences of meaning correlates with differences of distribution   
(harris, 1970)

  

      you shall know a word by the company it keeps!    (firth, 1957)

example

   he handed her a glass of bardiwac
   beef dishes are made to complement the bardiwac
   nigel staggered to his feet, face flushed from too much bardiwac.
   malbec, one of the lesser-known bardiwac grapes, responds well to   

australia   s sunshine

   i dined off bread and cheese and this excellent bardiwac
   the drinks were delicious: bold bardiwac as well as light, sweet 

rhenish.

word vectors

l a word type may be represented as a vector of features indicating 

the contexts in which it occurs in a corpus.

context features

word co-occurrence within a window:

grammatical relations:

context features

feature values:

   boolean
  raw counts
   weighting scheme (e.g. tf-idf)
  association values

association value: pointwise mutual information

l measures how often a target word w and a feature f occur together 

compared to what we would expect if the two were independent 

   pmi ranges from -inf to +inf, but negative values are generally 

unreliable (jurafsky & martin, 2017:275). 
   use positive pmi and clip at zero.

computing similarity

semantic similarity boils down to computing some measure of 

spatial similarity between context vectors in vector space.

words in a vector space

l in 2 dimensions:

l v =    cat   
l w =    computer   

euclidean distance

l formula:

   can be oversensitive to extreme 

values

cosine similarity

l formula:

   typically better than euclidean 

distance for vector space semantics

vector sparseness

l co-occurrence based context vectors tend to very long and very 

sparse.
   len(word_vec) == len(vocab)

l short (dim. of around 50-300) and dense context vectors are usually 

preferable.
   easier to include as features in machine learning systems
   fewer parameters = better generalization & less over-fitting
   better at capturing synonymy

dense vectors

2 main methods of producing short, dense vectors:

(1) id84

(2) neural language models

id84

methods:

  principal component analysis 
(pca)
  t-distributed stochastic neighbor 
embedding (id167)
  latent semantic analysis (lsa)
  ...

neural network embeddings

idea: train a neural network to predict context words based on 
current current    target    word.

l similar input words     similar context word prediction
l similar input words     similar corresponding rows in the weight 

matrix of the trained network. 

we don   t actually care about context word prediction!
l rows in the trained weight matrix become our context vectors (aka 

word vectors, aka id27s)

neural network embeddings

most popular family of methods: id97 (mikolov et al. 2013, mikolov et al. 2013a)

1) skip-gram: predict context from word
2) continuous bag of words (cbow): predict word from context

neural lm architectures: which to use?

l cbow and skip-gram typically produce similar embeddings, but:

l cbow is several times faster to train, better accuracy for frequent words
l skip-gram works well with small amounts of training data, and does well 

with representing rare words

l mikolov:    best practice is to try a few experiments and see what 

works the best for you   

l https://groups.google.com/forum/#!searchin/id97-toolkit/c-bow/id97-toolkit/nlvyxu99cam/esld8lcdxlaj

properties of dense id27s

dense id27s encode:

   semantic relationships
   syntactic relationships

can probe relations between words using vector arithmetic:

   king     male + female = ?
   walked     walk + fly = ?

vector arithmetic demonstration 

data:

l twitter data from continental usa
l 2.4 billion tokens

embeddings:

l dimensionality: 200
l context window: 5
l algorithm: cbow

train your own id27s:

tensorflow: https://www.tensorflow.org/tutorials/id97

gensim: https://rare-technologies.com/id97-tutorial/

fasttext: https://github.com/facebookresearch/fasttext

pretrained id27s:

id97: https://code.google.com/archive/p/id97/

   trained on 100 billion tokens from google news corpus

glove: https://nlp.stanford.edu/projects/glove/

  6b wikipedia, 42-840b tokens common crawl, 27b tokens twitter

lexvec: https://github.com/alexandres/lexvec

   58b tokens common crawl, 7b tokens wikipedia + newscrawl

id27s: evaluation

how to judge the quality of embeddings?

l    relatedness    scores for given word pairs

   compare model   s relatedness scores to human relatedness scores

l analogy tests

   find x such that x : y best resembles a sample relationship a : b

lcategorization

   recover a known id91 of words into different categories.

document features

l so far: features in word-vectors can be: context counts, pmi 

scores, weights from neural lms    

l can also be features of the docs in which the words occur.

l document occurrence features are useful for topical/thematic

similarity

document-term matrix

d1

23

102

14

0

d2

17

0

57

0

d3

0

14

0

18

d4

0

24

2

38

w1

w2

w3

w4

term frequency     inverse document frequency 
(tf-idf)

l common in ir tasks
l popular method to weight term-document matrices in general

tf: relative frequency of term in document

   tf(t,d) = f(t,d)

idf: inverse of the proportion of docs containing the term

   n / nt  (n = total # of docs, nt = # of docs term t appeared in)

document-term matrix

d1

23

102

14

0

d2

17

0

57

0

d3

0

14

0

18

d4

0

24

2

38

w1

w2

w3

w4

tf-idf weighted document-term matrix

d1

.12

.21

.03

0

d2

.16

0

.22

0

d3

0

.13

0

.39

d4

0

.11

.01

.41

w1

w2

w3

w4

tf-idf weighted document-term matrix

d1

.12

.21

.03

0

d2

.16

0

.22

0

d3

0

.13

0

.39

d4

0

.11

.01

.41

w1

w2

w3

w4

word
vectors

tf-idf weighted document-term matrix

document 
vectors

d2

.16

0

.22

0

d3

0

.13

0

.39

d4

0

.11

.01

.41

d1

.12

.21

.03

0

w1

w2

w3

w4

topic models

id44 (lda) and variants known as topic 
models.

  learned on large document collection (unsupervised)

  latent probabilistic id91 of words that tend to occur in the same 
document. each    topic    cluster is a distribution over words.

  generative model: each document is a sparse mixture of topics. each word in 
the doc is chosen by sampling a topic from the doc-specific topic distribution, 
then sampling a word from that topic.

topic models

https://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext

visualizing topics

https://dhs.stanford.edu/algorithmic-literacy/using-word-clouds-for-topic-modeling-results/

questions?

