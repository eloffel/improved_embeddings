lecture 12:

algorithms for id48s

nathan schneider

(some slides from sharon goldwater; 
thanks to jonathan may for bug fixes)
enlp | 26 february 2018

recap: tagging

    id52 is a sequence labelling task.
    we can tackle it with a model (id48) that 

uses two sources of information:
    the word itself
    the tags assigned to surrounding words

    the second source of information means we 

can   t just tag each word independently.

local tagging

words:
possible tags: 
(ordered by 
frequency for 
each word)

<s>
<s>

one
cd
nn
prp

dog
nn
vb

bit
nn
vbd

</s>
</s>

    choosing the best tag for each word independently, 

i.e. not considering tag context, gives the wrong 
answer (<s> cd nn nn </s>).

    though nn is more frequent for    bit   , tagging it as 

vbd may yield a better sequence
(<s> cd nn vb </s>) 
    because p(vbd|nn) and p(</s>|vbd) are high.

recap: id48

    elements of id48:
    set of states (tags)
    output alphabet (word types)
    start state (beginning of sentence)
    state transition probabilities p(ti | ti-1)
    output probabilities from each state p(wi | ti)

recap: id48

    given a sentence w=w1   wn with tags t=t1   tn, 
compute p(w,t) as:
4 -5161-616173
-(.,/)=0123
argmax/-(/|.)
enumerating all possible tag sequencest

    but we want to find 

without 

    use a greedy approximation, or
    use viterbi algorithm to store partial computations.

greedy tagging

words:

possible tags: 
(ordered by 
frequency for 
each word)

<s>
<s>

one
cd
nn
prp

dog
nn
vb

bit
nn
vbd

</s>
</s>

    for i = 1 to n: choose the tag that maximizes 

    transition id203 !"#"#$%   
    emission id203 !'#"#

    this uses tag context but is still suboptimal. why?

    it commits to a tag before seeing subsequent tags.
    it could be the case that all possible next tags have low 

transition probabilities. e.g., if a tag is unlikely to occur at the 
end of the sentence, that is disregarded when going left to right.

greedy vs. id145
    the greedy algorithm is fast: we just have to 
make one decision per token, and we   re done.
    runtime complexity?

   !(#$) with # tags, length-$ sentence

    but subsequent words have no effect on each 

decision, so the result is likely to be suboptimal.
    id145 search gives an optimal
global solution, but requires some bookkeeping 
(= more computation). postpones decision about 
any tag until we can be sure it   s optimal.

viterbi tagging: intuition

words:
possible tags: 
(ordered by 
frequency for 
each word)

<s>
<s>

one
cd
nn
prp

dog
nn
vb

bit
nn
vbd

</s>
</s>

    suppose we have already computed

a) the best tag sequence for <s>     bit that ends in nn.
b) the best tag sequence for <s>     bit that ends in vbd.

    then, the best full sequence would be either

    sequence (a) extended to include </s>, or
    sequence (b) extended to include </s>.

viterbi tagging: intuition

words:
possible tags: 
(ordered by 
frequency for 
each word)

<s>
<s>

one
cd
nn
prp

dog
nn
vb

bit
nn
vbd

</s>
</s>

    but similarly, to get

a) the best tag sequence for <s>     bit that ends in nn.

    we could extend one of:

    the best tag sequence for <s>     dog that ends in nn.
    the best tag sequence for <s>     dog that ends in vb.

    and so on   

argmax*+(*|.)

viterbi: high-level picture

    want to find 

    intuition: the best path of length i ending in state t
must include the best path of length i-1 to the 

previous state. so,
    find the best path of length i-1 to each state.
    consider extending each of those by 1 step, to state t.
    take the best of those options as the best path to state t.

viterbi algorithm

    use a chart to store partial results as we go 

   t	  n	table, where %&,(
state sequence for w1   wi that ends in state t.

is the id203* of the best 

*specifically, v(t,i) stores the max of the joint id203 p(w1   wi,t1   ti-1,ti=t|  )

viterbi algorithm

is the id203* of the best 

    use a chart to store partial results as we go 

   t	  n	table, where %&,(
state sequence for w1   wi that ends in state t.
%&,( =max45%&   ,(   1 78(&|&   )78<=|&=
    the max is over each possible previous tag &.
at (   1 we came from.

    fill in columns from left to right, with

    store a backtrace to show, for each cell, which state 

*specifically, v(t,i) stores the max of the joint id203 p(w1   wi,t1   ti-1,ti=t|  )

det

verb

</s>

transition matrix: p(ti|	ti-1):
transition and output probabilities
adv
prep
noun
<s>
noun
verb
det
emission matrix: p(wi|	ti):
prep
adv
very
the
doctor
cat

.1
.4
.05
.01
.05
.5

.3
.01
.3
.01
.4
.1

.2
.3
.2
.01
.1
.1

.1
.04
.1
.07
.05
.1

0
.05
.05
0
0
.1

.3
.2
.3
.9
.4
.1

in

is

a

noun
verb
det
prep
adv

0
0
.3
0
0

.5
0
0
0
0

.4
.1
0
0
0

0
0
0
1.0
.1

.1
.9
0
0
0

0
0
.7
0
0

0
0
0
0
.9

example

suppose w=the	doctor	is	in. our initially empty 
table:vw1=the w2=doctor w3=is w4=in </s>
nounverbdetprepadv

table:

filling in the first column

suppose w=the	doctor	is	in. our initially empty 
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
.noun,the =2noun<s>2(the|noun)=.3(0)
.det,the =2det<s>)2(the|det =.3(.7)

0
0
.21
0
0

   

the second column

/noun,doctor
=max56/78,the :!(noun|7   ):!(doctor|noun)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
!noundet)!(doctor|noun =.3(.4)

0
0
.21
0
0

?

= max { 0, 0, .21(.36), 0, 0 } = .0756

the second column

/noun,doctor
=max56/78,the :!(noun|7   ):!(doctor|noun)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
!noundet)!(doctor|noun =.9(.4)

0
0
.21
0
0

.0756

= max { 0, 0, .21(.001), 0, 0 } = .00021

the second column

averb,doctor
=maxfgahi,the j=(verb|h   )j=(doctor|verb)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=verbdet)=(doctor|verb =.01(.1)

0
0
.21
0
0

.0756
.00021

= max { 0, 0, .21(.001), 0, 0 } = .00021

the second column

averb,doctor
=maxfgahi,the j=(verb|h   )j=(doctor|verb)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=verbdet)=(doctor|verb =.01(.1)

0
0
.21
0
0

.0756
.00021

0
0
0

the third column

= max { .0756(.02), .00021(.03), 0, 0, 0 } = .001512

anoun,is=maxfgahi,doctor j=(noun|h   )j=(is|noun)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=nounnoun)=(is|noun =.2(.1)=.02
=nounverb)=(is|noun =.3(.1)=.03

0
0
.21
0
0

.0756
.00021

.001512

0
0
0

= max { .0756(.36), .00021(.045), 0, 0, 0 } = .027216

the third column

averb,is=maxfgahi,doctor j=(verb|h   )j=(is|verb)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=verbnoun)=(is|verb =.4(.9)=.36
=verbverb)=(is|verb =.05(.9)=.045

0
0
.21
0
0

.001512
.027216

.0756
.00021

0
0
0

0
0
0

= max { .001512(.3), .027216(.2), 0, 0, 0 } = .005443

the fourth column

aprep,in=maxfgahi,is j=(prep|h   )j=(in|prep)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=prepnoun)=(in|prep =.3(1.0)
=prepverb)=(in|prep =.2(1.0)

0
0
.21
0
0

.001512
.027216

.0756
.00021

.005443

0
0
0

0
0
0

0
0
0

= max { .000504(.004), .027216(.01), 0, 0, 0 } = .000272

the fourth column

aprep,in=maxfgahi,is j=(prep|h   )j=(in|prep)
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=advnoun)=(in|adv =.04(.1)
=advverb)=(in|adv =.1(.1)

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

end of sentence

= max { 0, 0, 0, .005443(0), .000272(.1) } = .0000272

></s>=maxbc>de,in g=(</s>|d   )
vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv
=</s>prep =0
=</s>adv =.1

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

.000027

2

completed viterbi chart

vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

.000027

2

following the backtraces

vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

.000027

2

following the backtraces

vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

.000027

2

following the backtraces

vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

.000027

2

following the backtraces

vw1=the w2=doctor w3=is w4=in </s>
noun
verb
det
prep
adv

0
0
.21
0
0

.001512
.027216

.005443
.000272

.0756
.00021

0
0
0

0
0
0

0
0
0

.000027

2

verb

prep

noun

det

implementation and efficiency
    for sequence length n with t possible tags, 
    enumeration takes o(tn) time and o(n) space.
    bigram viterbi takes o(t2n) time and o(tn) space.
    viterbi is exhaustive: further speedups might be had 

using methods that prune the search space.

    as with id165 models, chart probs get really 

tiny really fast, causing underflow.
    so, we use costs (neg log probs) instead.
    take minimum over sum of costs, instead of maximum 

over product of probs.

higher-order viterbi

    for a tag trigram model with t possible tags, 

we effectively need t2 states
    id165 viterbi requires tn-1 states, takes o(tnn)

time and o(tn-1n) space.

noun verb

verb noun

verb prep

verb </s>

verb verb

    using viterbi, we can find the best tags for a 

id48s: what else?

sentence (decoding), and get !(#,%).
    compute the likelihood!(#), i.e., the id203 of a 

    we might also want to

sentence regardless of its tags (a language model!)

    learn the best set of parameters (transition & emission 
probs.) given only an unannotated corpus of sentences.

computing the likelihood

    from id203 theory, we know that

"($)='( "($,()
    there are an exponential number of ts.

    again, by computing and storing partial results, we 

can solve efficiently.

    (advanced slides show the algorithm for those who 

are interested!)

summary

    id48: a generative model of sentences using 

hidden state sequence

    greedy tagging: fast but suboptimal
    id145 algorithms to compute
    best tag sequence given words (viterbi algorithm)
    likelihood (forward algorithm   see advanced 

slides)

    best parameters from unannotated corpus 

(forward-backward algorithm, an instance of em   
see advanced slides)

advanced topics

(the following slides are just for people who are 

interested)

notation

    sequence of observations over time o1, o2,    , on

    here, words in sentence

    vocabulary size v of possible observations
    set of possible states q1, q2,    , qt (see note next slide)

    here, tags 

    a, an t  t matrix of transition probabilities

    aij: the prob of transitioning from state i to j. 
    b, an t  v matrix of output probabilities

    bi(ot): the prob of emitting ot from state i. 

note on notation

    j&m use q1, q2,    , qn for set of states, but also use 

q1, q2,    , qn for state sequence over time.
    so, just seeing q1 is ambiguous (though usually 

disambiguated from context).

    i   ll instead use qi for state names, and qn for state at time n.  
    so we could have qn = qi, meaning: the state we   re in at 

time n is qi.

id48 example w/ new notation

start

.3

.7

q1

q2

.5

.5

.6 .1 .3
x

    states {q1,	q2}	(or {<s>,	q1,	q2}): think nn, vb
    output symbols {x,	y,	z}: think chair, dog, help

.1 .7 .2
x

y

y

z

z

adapted from manning & schuetze, fig 9.2

id48 example w/ new notation
    a possible sequence of outputs for this id48:

z y y x y z x z z

    a possible sequence of states for this id48:

q1 q2 q2 q1 q1 q2 q1 q1 q1

    for these examples, n = 9, q3= q2 and o3= y

transition and output probabilities

    transition matrix a:
aij=	p(qj|	qi)
ex:p(qn=	q2	|	qn-1=	q1)	=	.3
    output matrix b:
bi(o)	=	p(o	|	qi)
ex:p(on=	y|	qn=	q1)	=	.1

q1
q2

q1
<s> 1
q1
q2
y
x

.7
.5

.6
.1

.1
.7

q2

0
.3
.5

z

.3
.2

forward algorithm

    use a table with cells   (j,t): the id203 of being in 
state j after seeing o1   ot (forward id203).
*(+,,)=;(91,92,   9,,=,=+|?)
2 *3,,   156/75879:
*+,, =./01

    fill in columns from left to right, with

    same as viterbi, but sum instead of max (and no backtrace).

note: because there   s a sum, we can   t use the trick that replaces probs with costs. for 
implementation info, see http://digital.cs.usu.edu/~cyan/cs7960/id48-tutorial.pdf and 
http://stackoverflow.com/questions/13391625/underflow-in-forward-algorithm-for-id48s .

example

    suppose o=xzy. our initially empty table:
o3=y
q1q2

o1=x

o2=z

o3=y

filling the first column

o1=x

o2=z
q1
q2
*1,1 =,-./01213)= 1(.6
*2,1 =,-./81223)= 0(.1

.6
0

.6
0

.126

o3=y

starting the second column

o1=x
o2=z
q1
q2
0 *1,1 23-/2415
*1,2 =,-./
=*1,1 23//24/5 +*2,1 23</241(5)
= .6 .7 .3 + 0 .5 .3
=.126

.6
0

o3=y

finishing the second column

o1=x
o2=z
q1
q2
0 *1,1 23-42526
*2,2 =,-./
=*1,1 23/42546 +*2,1 2344252(6)
= .6 .3 .2 + 0 .5 .2
=.036

.126
.036

third column and finish

o1=x

o2=z

o3=y

.6
0

.126
.036

.01062
.03906

q1
q2

    add up all probabilities in last column to get the 

id203 of the entire sequence:

2 34,6
*+|- =./01

learning

parameters   =	(a,	b).

    given only the output sequence, learn the best set of 

    assume    best    = maximum-likelihood.
    other definitions are possible, won   t discuss here.

unsupervised learning

    training an id48 from an annotated corpus is 

simple.
    supervised learning: we have examples labelled with the 
right    answers    (here, tags): no hidden variables in training.

    training from unannotated corpus is trickier.

    unsupervised learning: we have no examples labelled with 
the right    answers   : all we see are outputs, state sequence 
is hidden.

    if we know the state sequence, we can find the best   .
    if we know   , we can find the best state sequence.

circularity
#$%|$' =)(+,   +.)
)(+,)

    e.g., use id113:

    use viterbi

    but we don't know either!

expectation-maximization (em)
as in id147, we can use em to bootstrap, 
iteratively updating the parameters and hidden variables.

    initialize parameters   (0)
    at each iteration k,
    e-step: compute expected counts using   (k-1)
    m-step: set   (k) using id113 on the expected counts
    repeat until   	doesn't change (or other stopping 

criterion).

expected counts??

counting transitions from qi   qj:
    count 1 each time we see qi   qj in true tag sequence.
    with current   , compute probs of all possible tag sequences.
    if sequence q has id203 p, countp for each qi   qj in q.

    expected counts:

    real counts: 

    add up these fractional counts across all possible sequences.

example

    notionally, we compute expected counts as follows:

possible 
sequence

q1=
q2=
q3=
q4=

observs:

q1
q1
q1
q1
x

q1
q2
q1
q2
z

q1
q1
q2
q2
y

id203 of 

sequence

p1
p2
p3
p4

example

    notionally, we compute expected counts as follows:

possible 
sequence

q1=
q2=
q3=
q4=

observs:

id203 of 

sequence

p1
p2
p3
p4

q1
q2
q1
q2
z

q1
q1
q1
q1
q1
q2
q2
q1
y
x
!"#1   #1 =2(1+(3

forward-backward algorithm
    as usual, avoid enumerating all possible sequences.
    forward-backward (baum-welch) algorithm computes 

expected counts using forward probabilities and 
backward probabilities:

!(#,%)=(()%=#,*+,-,*+,.,   *0|2)

    details, see j&m 6.5

    em idea is much more general: can use for many latent 

variable models.

guarantees

    em is guaranteed to find a local maximum of the likelihood.

p(o|   )

values of   

    not guaranteed to find global maximum.
    practical issues: initialization, random restarts, early stopping. 

fact is, it doesn   t work well for learning pos taggers!

