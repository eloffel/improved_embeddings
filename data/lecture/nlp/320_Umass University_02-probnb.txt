lecture 2:

id203, naive bayes

cs 585, fall 2015

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

thursday, september 10, 15

today

    id203 review
       naive bayes    classi   cation
    python demo

thursday, september 10, 15

2

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

id203 theory review

1 =xa

p (a = a)

p (ab)
p (b)

p (a|b) =
p (ab) = p (a|b)p (b)

p (a, b = b)

p (a) =xb
p (a) =xb

p (a|b = b)p (b = b)
p (a _ b) = p (a) + p (b)   p (ab)

p (  a) = 1   p (a)

3

id155

chain rule

law of total id203

disjunction (union)

negation (complement)

thursday, september 10, 15

bayes rule

h: unknown

p(d|h)

model:  how hypothesis causes data

bayesian id136

p(h|d)

d: observed 
data / evidence

bayes rule tells you how to    ip the conditional.
useful if you assume a generative process for your data.

likelihood

prior

p (h|d) =

p (d|h)p (h)

p (d)

posterior

normalizer

4

thursday, september 10, 15

rev. thomas bayes
c. 1701-1761

bayes rule and its pesky denominator

likelihood

prior

p (h|d) =

p (d|h)p (h)

p (d)

=

p (h|d) / p (d|h)p (h)

unnormalized posterior
by itself does not sum to 1!

p (d|h)p (h)

ph0 p (d|h0)p (h0)
        proportional to   

constant
w.r.t.  h

implicitly for varying h.
this notation is very common, though 
slightly ambiguous.

thursday, september 10, 15

5

bayes rule for classi   cation id136

doc label

y

assume a generative process,  p(w | y)

id136 problem:  given w, what is y?

words

w

authorship problem: classify a new text.
1.3 the chain rule and marginal probabilities (1 pt)
is it y=anna or y=barry?
alice has been studying karate, and has a 50% chance of surviving an en-
counter with zombie bob. if she opens the door, what is the chance that she
will live, conditioned on bob saying graagh? assume there is a 100% chance
observe w:  look at random word in the new text.
that alice lives if bob is not a zombie.
it is abracadabra.
2 necromantic scrolls
the necromantic scroll a   cionados (nsa) would like to know the author of a
recently discovered ancient scroll. they have narrowed down the possibilities
to two candidate wizards: anna and barry. from painstaking corpus analy-
sis of texts known to be written by each of these wizards, they have collected
frequency statistics for the words abracadabra and gesundheit, shown in table 1

p(y | w) = p(w|y)  p(y)  /  p(w)

p(y=a | w=abracadabra)  ?

p(y):  assume 50% prior prob.
p(w | y):  
calculate from 
previous data

abracadabra
5 per 1000 words
10 per 1000 words

anna
barry

gesundheit
6 per 1000 words
1 per 1000 words

thursday, september 10, 15

table 1: word frequencies for wizards anna and barry

6

bayes rule as hypothesis vector scoring

a

b

c

8
.
0

4
.
0

0
.
0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

multiply

normalize

thursday, september 10, 15

sum to 1?

p (h = h)
prior

p (e|h = h)
likelihood

p (e|h = h)p (h = h)
unnorm. posterior

1
z

7

p (e|h = h)p (h = h)
posterior

bayes rule as hypothesis vector scoring

8
.
0

4
.
0

0
.
0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

multiply

normalize

thursday, september 10, 15

sum to 1?

yes

[0.2, 0.2, 0.6]

a

b

c

p (h = h)
prior

p (e|h = h)
likelihood

p (e|h = h)p (h = h)
unnorm. posterior

1
z

7

p (e|h = h)p (h = h)
posterior

bayes rule as hypothesis vector scoring

8
.
0

4
.
0

0
.
0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

multiply

normalize

thursday, september 10, 15

[0.2, 0.2, 0.6]

a

b

c

[0.2, 0.05, 0.05]

sum to 1?

yes

no

p (h = h)
prior

p (e|h = h)
likelihood

p (e|h = h)p (h = h)
unnorm. posterior

1
z

7

p (e|h = h)p (h = h)
posterior

bayes rule as hypothesis vector scoring

[0.2, 0.2, 0.6]

a

b

c

[0.2, 0.05, 0.05]

[0.04, 0.01, 0.03]

8
.
0

4
.
0

0
.
0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

multiply

normalize

thursday, september 10, 15

sum to 1?

yes

no

no

p (h = h)
prior

p (e|h = h)
likelihood

p (e|h = h)p (h = h)
unnorm. posterior

1
z

7

p (e|h = h)p (h = h)
posterior

bayes rule as hypothesis vector scoring

[0.2, 0.2, 0.6]

a

b

c

[0.2, 0.05, 0.05]

[0.04, 0.01, 0.03]

[0.500, 0.125, 0.375]

8
.
0

4
.
0

0
.
0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

8

.

0

4

.

0

0

.

0

sum to 1?

p (h = h)
prior

p (e|h = h)
likelihood

p (e|h = h)p (h = h)
unnorm. posterior

1
z

7

p (e|h = h)p (h = h)
posterior

yes

no

no

yes

multiply

normalize

thursday, september 10, 15

text classi   cation

with

naive bayes

thursday, september 10, 15

8

classi   cation problems

    given text d, want to predict label y
    is this restaurant review positive or negative?
    is this email spam or not?
    which author wrote this text?
    (is this word a noun or verb?)
    d: documents, sentences, etc.
    y: discrete/categorical variable

goal: from training set of (d,y) pairs, learn  

a probabilistic classi   er  f(d) = p(y|d) 

(   supervised learning   )

9

thursday, september 10, 15

to

features for model: bag-of-words

i love this movie! it's sweet, 
but with satirical humor. the 
dialogue is great and the 
adventure scenes are fun... 
it manages to be whimsical 
and romantic while laughing 
at the conventions of the 
fairy tale genre. i would 
recommend it to just about 
anyone. i've seen it several 
times, and i'm always happy 
to see it again whenever i 
have a friend who hasn't 
seen it yet!

ignored, keeping only their frequency in the document. in the example in the    gure,
instead of representing the word order in all the phrases like    i love this movie    and
   i would recommend it   , we simply note that the word i occurred 5 times in the
entire excerpt, the word it 6 times, the word love, recommend, and movie once, and
so on.

10draft6.1

fairy
it
love
always
it
whimsical
it
i
and
areanyone
seen
friend
dialogue
happy
recommend
adventure
satirical
sweet
of
who
movie
to
i
but
it
romantic
yet
several
again
the
it
the
seen
scenes
i
to
the
the
times
fun
and
about
whenever
with

naive bayes is a probabilistic classi   er, meaning that for a document d, out of
all classes c 2 c the classi   er returns the class   c which has the maximum posterior

figure 6.1
words is ignored (the bag of words assumption) and we make use of the frequency of each word.

it 
i
the
to
and
seen
yet
would
whimsical
times
sweet
satirical
adventure
genre
fairy
humor
have
great
   

intuition of the multinomial naive bayes classi   er applied to a movie review. the position of the

humor
would
manages
and

6 
5
4
3
3
2
1
1
1
1
1
1
1
1
1
1
1
1
   

conventions

while

have

it
i

i

thursday, september 10, 15

levels of linguistic structure

discourse

semantics

communicationevent(e)
agent(e, alice)
recipient(e, bob)

speakercontext(s)
temporalbefore(e, s)

syntax

np

s

vp

pp

words

morphology

characters

thursday, september 10, 15

verbpast
nounprp
alice talked

prep nounprp
bob
to
talk -ed

alice  talked  to  bob.

11

.

punct
.

levels of linguistic structure

words

alice talked

to

bob

.

characters

alice  talked  to  bob.

thursday, september 10, 15

12

levels of linguistic structure

words are fundamental units of meaning

words

alice talked

to

bob

.

characters

alice  talked  to  bob.

thursday, september 10, 15

12

levels of linguistic structure

words are fundamental units of meaning

and easily identi   able*

*in some languages

words

alice talked

to

bob

.

characters

alice  talked  to  bob.

thursday, september 10, 15

12

how to classify with words?
    approach #1: use a prede   ned dictionary

(or make one up)
human knowledge
    e.g. for sentiment....

    score += 1 for each    happy   ,    awesome   ,    cool   
    score -= 1  for each    sad   ,    awful   ,    bad   

thursday, september 10, 15

13

how to classify with words?
    approach #1: use a prede   ned dictionary

(or make one up)
human knowledge
    e.g. for sentiment....

    score += 1 for each    happy   ,    awesome   ,    cool   
    score -= 1  for each    sad   ,    awful   ,    bad   
    approach #2: use labeled documents
supervised learning
    learn which words correlate to positive vs. 
    use these correlations to classify new 

negative documents

documents

thursday, september 10, 15

13

supervised learning

many labeled 

examples

(d=    ...   , y=b)
(d=    ...   , y=b)
(d=    ...   , y=a)

training

model

parameters

classify
new texts

(d=    ...   , y=?)

(d=    ...   , y=?)

y=a

y=b

thursday, september 10, 15

14

supervised learning: generative model

many labeled 

examples

(d=    ...   , y=b)
(d=    ...   , y=b)
(d=    ...   , y=a)

training

model

parameters

p(y)

p(d | y)

classify
new texts

(d=    ...   , y=?)

(d=    ...   , y=?)

y=a

y=b

15

p(y | d)

thursday, september 10, 15

multinomial naive bayes
p (y | w1..wt ) / p (y) p (w1..wt | y)

tokens in doc

predictions:

predict class

arg max

y

p (y = y | w1..wt )

or, predict prob of classes...

thursday, september 10, 15

multinomial naive bayes
p (y | w1..wt ) / p (y) p (w1..wt | y)

tokens in doc

p (wt | y)

the    naive bayes    
assumption: 
conditional indep.

yt

parameters: p (w | y)

for each document category y and wordtype w
p (y) prior distribution over document categories y

learning:  estimate parameters as frequency ratios; e.g.

p (w | y,    ) =

#(w occurrences in docs with label y) +    

#(tokens total across docs with label y) + v    

predictions:

predict class

arg max

y

p (y = y | w1..wt )

or, predict prob of classes...

thursday, september 10, 15

