nlp evaluation:

id64 & sig tests

cs 585, fall 2015

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

college of information and computer sciences

university of massachusetts amherst

sunday, november 22, 15

    questions
    what metrics to use?
    how to deal with complex outputs like translations?
    are the human judgments ... 
    ... measuring something real?
    ... reliable?

    is the sample of texts suf   ciently 
    how reliable or certain are the results?

representative?

sunday, november 22, 15

2

are my results meaningful?

sunday, november 22, 15

3

are my results meaningful?
    system1=87% accuracy.
system2=89% accuracy.

sunday, november 22, 15

3

are my results meaningful?
    system1=87% accuracy.
system2=89% accuracy.
    does this difference mean anything?  key questions:

sunday, november 22, 15

3

are my results meaningful?
    system1=87% accuracy.
system2=89% accuracy.
    does this difference mean anything?  key questions:
    do you trust the human judgments?

    analyze agreement rates

sunday, november 22, 15

3

are my results meaningful?
    system1=87% accuracy.
system2=89% accuracy.
    does this difference mean anything?  key questions:
    do you trust the human judgments?
    is the data from the right distribution?

    analyze agreement rates

correct domain/genre?
    judgment call...?

sunday, november 22, 15

3

are my results meaningful?
    system1=87% accuracy.
system2=89% accuracy.
    does this difference mean anything?  key questions:
    do you trust the human judgments?
    is the data from the right distribution?

    analyze agreement rates

correct domain/genre?
    judgment call...?
    statistical question!  [today]

    are there enough examples that we can trust it?

sunday, november 22, 15

3

statistical    signi   cance   

    assume data was drawn from a greater population.
    if we were to take a new sample, how much would 
data differ?
    or: how much would a statistic of that data differ?
       con   dence interval   

(better name: uncertainty interval)

    how to test stat sig?
    1.  bootstrap simulation:  handles anything  (**)
    2.  off-the-shelf tests:  for speci   c situations
    3.  quick rule-of-thumb  (**)

sunday, november 22, 15

4

bootstrap test
    [blackboard]
    inputs

    original data size n
    test statistic:  stat(data).  e.g.
    accuracy (numeric)
    system1 better than system2?  (boolean)

    algorithm

    for each of 10,000 replications:

    draw samp:  a sample with replacement from the original data, 
    calculate  stat(samp)

again size n.   (many of the original examples will not be in sample)

    save all 10,000 stat(samp) values.  then analyze
    numeric:  histogram.  mean, standard deviation, ci
    boolean:  proportion that are true?

sunday, november 22, 15

5

bootstrap test

    two types  (many others...)
    1. binary null hypothesis  (7.3 jm 3ed)

    boolean statistic: is null hypo true?
    p-value:  proportion of replications where null hypo is true

(pvalue<.05 means a non-null hypothesis is ...
   signi   cant    ... worth considering)

    2. con   dence interval  (this lecture)
    numeric statistic: e.g. accuracy rate
    the    normal approx    bootstrap ci:

95% ci = [mean +/- 2*stdev]

sunday, november 22, 15

6

paired tests

    single dataset.  compare system 1 vs system 2
    good approach (   paired   ): bootstrap sample 

items, compare system performances

    bad approach (   unpaired   ):
    1. bootstrap sample items. calc system1   s acc ci
    2. bootstrap sample items. calc system2   s acc ci
    3. do the cis overlap?
    why bad?

sunday, november 22, 15

7

power analysis

    how much data do we have to collect?
    power analysis: given how big an effect you want 
to measure, that implies how big n should be
    how to implement
    make fake dataset size n, run the bootstrap.  look 

at whether differences can be detected
    [ipynb demo]

    off-the-shelf formulas, e.g. r power.t.test(), 
    rules of thumb

power.prop.test(), http://www.statmethods.net/stats/power.html

sunday, november 22, 15

8

rules of thumb:  cis
    binomial ci  (agresti-coull version)

k occurrences in n examples.
let k   =k+2, n   =n+4,  p   =k   /n   
95% ci = [p     +/-  2*sqrt( p   (1-p   ) / n    )]
... or more conservatively ...
95% ci = [p     +/-  1/sqrt(n   )]

    rule of three

k=0 occurrences in n examples.
prob of occurrence?
95% ci  =  [0 .. 3/n]

sunday, november 22, 15

9

rules of thumb:  power analysis

http://www.nrcse.washington.edu/research/struts/chapter2.pdf

10

sunday, november 22, 15

rules of thumb:  power analysis

    rule of three:

k=0  =>  3/n  95% upper bound

to be sure prob <= p, how many examples?

http://www.nrcse.washington.edu/research/struts/chapter2.pdf

10

sunday, november 22, 15

rules of thumb:  power analysis

    rule of three:

k=0  =>  3/n  95% upper bound

to be sure prob <= p, how many examples?

    3/p

http://www.nrcse.washington.edu/research/struts/chapter2.pdf

10

sunday, november 22, 15

are my results meaningful?

sunday, november 22, 15

11

are my results meaningful?
    statistical signi   cance is neither suf   cient nor 
necessary for a meaningful result!  remember 
there are three different factors:

sunday, november 22, 15

11

are my results meaningful?
    statistical signi   cance is neither suf   cient nor 
necessary for a meaningful result!  remember 
there are three different factors:

    do you trust the human judgments?
    analyze agreement rates

sunday, november 22, 15

11

are my results meaningful?
    statistical signi   cance is neither suf   cient nor 
necessary for a meaningful result!  remember 
there are three different factors:

    do you trust the human judgments?
    analyze agreement rates
    is the data from the right distribution?
correct domain/genre?
    judgment call...?

sunday, november 22, 15

11

are my results meaningful?
    statistical signi   cance is neither suf   cient nor 
necessary for a meaningful result!  remember 
there are three different factors:

    do you trust the human judgments?
    analyze agreement rates
    is the data from the right distribution?
correct domain/genre?
    judgment call...?
    are there enough examples that we can trust it?
    statistical question!  [today]

sunday, november 22, 15

11

