parts of speech

cs 585, fall 2015

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

friday, september 25, 15

where we are going

    bags vs. sequences
    classi   cation vs. sequence tagging

friday, september 25, 15

2

what   s a part-of-speech (pos)?

meaning-bearing units

    syntax = how words compose to form larger 
    pos = syntactic categories for words
    you could substitute words within a class and have a 
    give information how words can combine.

syntactically valid sentence.

    i saw the dog
    i saw the cat
    i saw the {table, sky, dream, school, anger, ...}

friday, september 25, 15

3

pos is an old idea

    dionysius thrax of alexandria (100 bce):
    common in grammar classes today:

8 parts of speech

noun, verb, adjective, preposition, conjunction, 
pronoun, interjection

    many other more    ne-grained possibilities

https://www.youtube.com/watch?

v=odga7ssl-6g&index=1&list=pl6795522ead6ce2f7

friday, september 25, 15

4

open vs closed classes

open class (lexical) words 
nouns 

verbs 

proper 
ibm 
italy 

common 
cat / cats 
snow 

main 
see 
registered 

closed class (functional) 

determiners 

the some 

conjunctions 

and or 

pronouns 

he its 

modals 
can 
had 

adjectives 

old   older   oldest 

adverbs 

slowly 

numbers 
122,312 
one 

    more 

prepositions 

to with 

particles 

off   up 

    more 

interjections  ow  eh 

slide credit: chris manning

friday, september 25, 15

5

many tagging standards

    id32 (45 tags) ... the most common one
    brown corpus (85 tags)
    coarse tagsets
    petrov et al.    universal    tagset (12 tags)

http://code.google.com/p/universal-pos-tags/

   
    motivation: cross-linguistic regularities
    for english, collapsing of ptb tags

    e.g. adposition: pre- and postpositions
    gimpel et al. tagset for twitter  (25 tags)
    motivation: easier for humans to annotate
    we collapsed ptb, added new things that were necessary 

for twitter

friday, september 25, 15

6

why do we want pos?

    useful for many syntactic and other nlp tasks.
    phrase identi   cation (   chunking   )
    id39
    full parsing
    sentiment

friday, september 25, 15

7

pos patterns: sentiment

    turney (2002): identify bigram phrases, from unlabeled 

corpus, useful for id31.

first word 

review (brill, 1994).3 two consecutive words are 
extracted from the review if their tags conform to 
any of the patterns in table 1. the jj tags indicate 
adjectives, the nn tags are nouns, the rb tags are 
adverbs, and the vb tags are verbs.4 the second 
pattern, for example, means that two consecutive 
words are extracted if the first word is an adverb 
and the second word is an adjective, but the third 
word  (which  is  not  extracted)  cannot  be  a  noun. 
nnp and nnps (singular and plural proper nouns) 
are avoided, so that the names of the items in the 
review cannot influence the classification. 
table 1. patterns of tags for extracting two-word 
phrases from reviews.  
 
1.  jj 
2.  rb, rbr, or 
3.  jj 
4.  nn or nns 
5.  rb, rbr, or 

third word  
(not extracted) 
anything 
not nn nor nns 
not nn nor nns 
not nn nor nns 
anything 

second word 
nn or nns 
jj 
jj 
jj 
vb, vbd, 
vbn, or vbg 

rbs 
the second step is to estimate the semantic ori-
entation of the extracted phrases, using the pmi-ir 
algorithm. this algorithm uses mutual information 
as a measure of the strength of semantic associa-
tion between two words (church & hanks, 1989). 
(plus sentiment labels stuff)
pmi-ir  has  been  empirically  evaluated  using  80 
synonym test questions from the test of english as 
a foreign language (toefl), obtaining a score of 
74% (turney,  2001).  for comparison,  latent  se-
friday, september 25, 15
mantic analysis (lsa), another statistical measure 

rbs 

8

first word 

corpus, useful for id31.

we acquire about the presence of one of the words 
review (brill, 1994).3 two consecutive words are 
when we observe the other.  
extracted from the review if their tags conform to 
the  semantic  orientation  (so)  of  a  phrase, 
pos patterns: sentiment
any of the patterns in table 1. the jj tags indicate 
phrase, is calculated here as follows: 
adjectives, the nn tags are nouns, the rb tags are 
adverbs, and the vb tags are verbs.4 the second 
     so(phrase) = pmi(phrase,    excellent   )  
(2) 
                          - pmi(phrase,    poor   ) 
pattern, for example, means that two consecutive 
words are extracted if the first word is an adverb 
the reference words    excellent    and    poor    were 
    turney (2002): identify bigram phrases, from unlabeled 
and the second word is an adjective, but the third 
chosen because, in the five star review rating sys-
word  (which  is  not  extracted)  cannot  be  a  noun. 
tem, it is common to define one star as    poor    and 
nnp and nnps (singular and plural proper nouns) 
five  stars  as     excellent   .  so  is  positive  when 
are avoided, so that the names of the items in the 
phrase is more strongly associated with    excellent    
review cannot influence the classification. 
and negative when phrase is more strongly associ-
ated with    poor   .   
table 1. patterns of tags for extracting two-word 
pmi-ir estimates pmi by issuing queries to a 
phrases from reviews.  
search engine (hence the ir in pmi-ir) and noting 
 
the number of hits (matching documents). the fol-
lowing  experiments  use  the  altavista  advanced 
1.  jj 
search engine5, which indexes approximately 350 
2.  rb, rbr, or 
million web pages (counting only those pages that 
are in english). i chose altavista because it has a 
3.  jj 
near  operator.  the  altavista  near  operator 
4.  nn or nns 
constrains the search to documents that contain the 
5.  rb, rbr, or 
words  within ten  words  of  one  another, in either 
rbs 
order. previous work has shown that near per-
the second step is to estimate the semantic ori-
forms  better  than  and  when  measuring  the 
entation of the extracted phrases, using the pmi-ir 
strength  of  semantic  association  between  words 
algorithm. this algorithm uses mutual information 
(turney, 2001). 
as a measure of the strength of semantic associa-
let hits(query) be the number of hits returned, 
tion between two words (church & hanks, 1989). 
(plus sentiment labels stuff)
given the query query. the following estimate of 
pmi-ir  has  been  empirically  evaluated  using  80 
so can be derived from equations (1) and (2) with 
synonym test questions from the test of english as 
a foreign language (toefl), obtaining a score of 
74% (turney,  2001).  for comparison,  latent  se-
friday, september 25, 15
mantic analysis (lsa), another statistical measure 

the third step is to calculate the average seman-
tic orientation of the phrases in the given review 
and classify the review as recommended if the av-
erage is positive and otherwise not recommended.  
table 2 shows an example for a recommended 
review  and  table  3  shows  an  example  for  a  not 
recommended  review.  both  are  reviews  of  the 
bank of america. both are in the collection of 410 
reviews from epinions that are used in the experi-
ments in section 4. 
table 2. an example of the processing of a review that 
the author has classified as recommended.6 
part-of-speech 
extracted phrase 
tags 
jj nn 
online experience  
jj nns 
low fees  
jj nn 
local branch  
jj nn 
small part  
jj nn 
online service  
jj nn 
printable version  
jj nn 
direct deposit  
rb jj 
well other  
rb vbn 
inconveniently  
located  
jj nn 
other bank  
jj nn 
true service  
average semantic orientation 

 
                                                           
6 the semantic orientation in the following tables is calculated 
using the natural logarithm (base e), rather than base 2. the 
natural log is more common in the literature on log-odds ratio. 
since all logs are equivalent up to a constant factor, it makes 
no difference for the algorithm. 

semantic 
orientation 
 2.253 
 0.333 
 0.421 
 0.053 
 2.780 
-0.705 
 1.288 
 0.237 
-1.541 
-0.850 
-0.732 
 0.322 

third word  
(not extracted) 
anything 
not nn nor nns 
not nn nor nns 
not nn nor nns 
anything 

second word 
nn or nns 
jj 
jj 
jj 
vb, vbd, 
vbn, or vbg 

                                                           
5 http://www.altavista.com/sites/search/adv 

rbs 

8

3.1  constraints

terms; almost always, this is a single preposition between two noun phrases.
pos patterns: simple noun phrases
the  proposed  algorithm  requires  satisfaction  of  two  constraints  applied  to  word
strings  in  text.  strings  satisfying  the  constraints  are  the  intended  output  of  the
algorithm.  various  parameters  that  can  be  used  to  influence  the  behavior  of  the
    quick and dirty noun phrase identi   cation
algorithm are introduced in section 3.2.
frequency: candidate strings must have frequency  2 or more in the text.
grammatical structure:  candidate  strings are those multi-word  noun phrases that
are specified  by the regular expression ((a | n)+ | ((a \ n)'{np)-)(a  \ n)')n,
where
a is an adjective, but not a determiner.5

5  determiners  include  articles,  demonstratives,  possessive  pronouns,  and  quantifiers.  some  common
determiners  (after  huddleston  1984:233), occupying three fixed positions relative to one another, are
as follows. pre-determiners: all, both; half, one-third, three-quarters,...; double, twice, three times; such,
what(exclamative). determiners proper: the; this, these, that, those; my, our, your; we, us, you; which,
what(relative),  what(interrogative); a,  another,  some,  any,  no,  either,  neither;  each,  enough,  much,
more,  less;  a  few(positive),  a  little(positive).  post-determiners:  every;  many,  several, few(negative),
little(negative); one, two, three...; (a) dozen.

friday, september 25, 15

9

how to get pos tags?

    classi   cation or sequence labeling problem

friday, september 25, 15

10

most words types 
are unambiguous ...

wsj

types:

brown

tokens:

(2+ tags)

(2+ tags)

45,799 (85%)
8,050 (15%)

44,432 (86%)
7,025 (14%)

unambiguous (1 tag)
ambiguous

unambiguous (1 tag)
ambiguous

577,421 (45%) 384,349 (33%)
711,780 (55%) 786,646 (67%)

can we just use a tag dictionary
(one tag per word type)?

id52: lexical ambiguity

figure 8.2 the amount of tag ambiguity for word types in the brown and wsj corpora,
from the treebank-3 (45-tag) tagging. these statistics include punctuation as words, and
assume words are kept in their original case.

gle tag (janet is always nnp, funniest jjs, and hesitantly rb). but the ambiguous
words, although accounting for only 14-15% of the vocablary, are some of the most
common words of english, and hence 55-67% of word tokens in running text are
ambiguous. note the large differences across the two genres, especially in token
frequency. tags in the wsj corpus are less ambiguous, presumably because this
newspaper   s speci   c focus on    nancial news leads to a more limited distribution of
word usages than the more general texts combined into the brown corpus.

draft8.3

    ambiguous wordtypes tend to be very common ones.
    i know that he is honest = in  (relativizer)
    yes, that play was nice = dt  (determiner)
    you can   t go that far = rb  (adverb)

earnings growth took a back/jj seat
a small building in the back/nn
a clear majority of senators back/vbp the bill
dave began to back/vb toward the door
enable the country to buy back/rp about debt
i was twenty-one back/rb then

some of the most ambiguous frequent words are that, back, down, put and set;

here are some examples of the 6 different parts-of-speech for the word back:

friday, september 25, 15

11

most words types 
are unambiguous ...

wsj
wsj

types:
types:

brown
brown

tokens:
tokens:

(2+ tags)
(2+ tags)

(2+ tags)
(2+ tags)

44,432 (86%)
44,432 (86%)
7,025 (14%)
7,025 (14%)

can we just use a tag dictionary
(one tag per word type)?

unambiguous (1 tag)
unambiguous (1 tag)
ambiguous
ambiguous
unambiguous (1 tag)
unambiguous (1 tag)
ambiguous
ambiguous

id52: lexical ambiguity

45,799 (85%)
45,799 (85%)
8,050 (15%)
8,050 (15%)
577,421 (45%) 384,349 (33%)
577,421 (45%) 384,349 (33%)
711,780 (55%) 786,646 (67%)
711,780 (55%) 786,646 (67%)

figure 8.2 the amount of tag ambiguity for word types in the brown and wsj corpora,
figure 8.2 the amount of tag ambiguity for word types in the brown and wsj corpora,
from the treebank-3 (45-tag) tagging. these statistics include punctuation as words, and
from the treebank-3 (45-tag) tagging. these statistics include punctuation as words, and
assume words are kept in their original case.
assume words are kept in their original case.

tagset. most word types (80-86%) are unambiguous; that is, they have only a sin-
gle tag (janet is always nnp, funniest jjs, and hesitantly rb). but the ambiguous
gle tag (janet is always nnp, funniest jjs, and hesitantly rb). but the ambiguous
words, although accounting for only 14-15% of the vocablary, are some of the most
words, although accounting for only 14-15% of the vocablary, are some of the most
common words of english, and hence 55-67% of word tokens in running text are
common words of english, and hence 55-67% of word tokens in running text are
ambiguous. note the large differences across the two genres, especially in token
ambiguous. note the large differences across the two genres, especially in token
frequency. tags in the wsj corpus are less ambiguous, presumably because this
frequency. tags in the wsj corpus are less ambiguous, presumably because this
newspaper   s speci   c focus on    nancial news leads to a more limited distribution of
newspaper   s speci   c focus on    nancial news leads to a more limited distribution of
word usages than the more general texts combined into the brown corpus.
word usages than the more general texts combined into the brown corpus.

draft8.3
draft8.3

    ambiguous wordtypes tend to be very common ones.
    i know that he is honest = in  (relativizer)
    yes, that play was nice = dt  (determiner)
    you can   t go that far = rb  (adverb)

earnings growth took a back/jj seat
earnings growth took a back/jj seat
a small building in the back/nn
a small building in the back/nn
a clear majority of senators back/vbp the bill
a clear majority of senators back/vbp the bill
dave began to back/vb toward the door
dave began to back/vb toward the door
enable the country to buy back/rp about debt
enable the country to buy back/rp about debt
i was twenty-one back/rb then
i was twenty-one back/rb then

some of the most ambiguous frequent words are that, back, down, put and set;
some of the most ambiguous frequent words are that, back, down, put and set;

here are some examples of the 6 different parts-of-speech for the word back:
here are some examples of the 6 different parts-of-speech for the word back:

but not so for 
tokens!

friday, september 25, 15

11

id52: baseline
    baseline: most frequent tag.  92.7% accuracy
    simple baselines are very important to run!

friday, september 25, 15

12

id52: baseline
    baseline: most frequent tag.  92.7% accuracy
    simple baselines are very important to run!
    why so high?
    many ambiguous words have a skewed distribution 
    credit for easy things like punctuation,    the   ,    a   , 

of tags

etc.

friday, september 25, 15

12

id52: baseline
    baseline: most frequent tag.  92.7% accuracy
    simple baselines are very important to run!
    why so high?
    many ambiguous words have a skewed distribution 
    credit for easy things like punctuation,    the   ,    a   , 
    is this actually that high?
    i get 0.918 accuracy for token tagging
    ...but, 0.186 whole-sentence accuracy (!)

of tags

etc.

friday, september 25, 15

12

id52 can be hard for 
humans, too

around/rp to/to joining/vbg 

    mrs/nnp shaefer/nnp never/rb got/vbd 
    all/dt we/prp gotta/vbn do/vb is/vbz go/
vb around/in the/dt corner/nn 
    chateau/nnp petrus/nnp costs/vbz 

around/rb $/$ 250/cd

friday, september 25, 15

13

when they are the first members of  the double conjunctions both . . . and, either . . . or and neither . . . nor, 
both,  either and neither are tagged as coordinating conjunctions (cc), not as determiners (dt). 

need careful guidelines (and do annotators always follow them?)

examples:  either/dt  child could sing. 

ptb pos guidelines,  santorini (1990)

but: 
either/cc  a boy could sing or/cc  a girl could dance. 
either/cc  a boy or/cc a girl could sing. 
either/cc  a boy or/cc girl could sing. 

4  confusing parts of speech 
4  confusing parts of speech 
be aware that  either or neither can sometimes function as determiners (dt) even in the presence of  or or 
this section discusses parts of speech that are easily confused and gives guidelines on how to tag such cases. 
nor. 
when they are the first members of  the double conjunctions both . . . and, either . . . or and neither . . . nor, 
both,  either and neither are tagged as coordinating conjunctions (cc), not as determiners (dt). 
cd or jj 
number-number combinations should be tagged as adjectives (jj) if they have the same distribution as 
adjectives . 

examples:  either/dt  child could sing. 

either/dt boy or/cc girl could sing. 

example: 

examples:  a 50-3/jj  victory (cf. a handy/jj victory) 

but: 
either/cc  a boy could sing or/cc  a girl could dance. 
either/cc  a boy or/cc a girl could sing. 
either/cc  a boy or/cc girl could sing. 

hyphenated fractions one-half, three-fourths, seven-eighths, one-and-a-half, seven-and-three-eighths should 
be tagged as adjectives (jj) when they are prenominal modifiers, but as adverbs (rb) if they could be 
replaced by  double or twice. 
be aware that  either or neither can sometimes function as determiners (dt) even in the presence of  or or 
nor. 

cf. a full/jj  cup 

one-half/rb  the amount;  cf. twice/rb  the amount; double/rb the amount 
either/dt boy or/cc girl could sing. 

examples:  one-half/j j cup; 
example: 

cd or jj 
sometimes, it is unclear whether  one is cardinal number or a noun.  in general, it should be tagged as a 
number-number combinations should be tagged as adjectives (jj) if they have the same distribution as 
adjectives . 
cardinal number (cd) even when its sense is not clearly that of  a numeral. 

example: 
examples:  a 50-3/jj  victory (cf. a handy/jj victory) 

one/cd of the best reasons 
14

friday, september 25, 15

hyphenated fractions one-half, three-fourths, seven-eighths, one-and-a-half, seven-and-three-eighths should 
but if it could be pluralized or modified by an adjective in a particular context, it is a common noun (nn). 
be tagged as adjectives (jj) when they are prenominal modifiers, but as adverbs (rb) if they could be 

some other lexical ambiguities

    prepositions versus verb particles
    turn into/p a monster
    take out/t the trash
    check it out/t,  what   s going on/t,  shout out/t

test:
turn slowly into a monster
*take slowly out the trash

careful annotator guidelines are necessary to de   ne what to do in 
many cases.
   http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports
   http://www.ark.cs.cmu.edu/tweetnlp/annot_guidelines.pdf

friday, september 25, 15

15

some other lexical ambiguities

test:
turn slowly into a monster
*take slowly out the trash

    prepositions versus verb particles
    turn into/p a monster
    take out/t the trash
    check it out/t,  what   s going on/t,  shout out/t
    this,that -- pronouns versus determiners
    i just orgasmed over this/o
    this/d wind is serious

careful annotator guidelines are necessary to de   ne what to do in 
many cases.
   http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports
   http://www.ark.cs.cmu.edu/tweetnlp/annot_guidelines.pdf

friday, september 25, 15

15

how to build a pos tagger?

    key sources of information:
    1.  the word itself
    2.  word-internal characters
    3.  pos tags of surrounding words:

syntactic context

friday, september 25, 15

16

