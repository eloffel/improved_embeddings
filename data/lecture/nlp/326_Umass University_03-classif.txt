lecture 3

classi   cation

cs 585, fall 2015

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

tuesday, september 15, 15

your ta:  ari kobren

    http://people.cs.umass.edu/~akobren/

tuesday, september 15, 15

2

    python demo
    over   tting, pseudocounts
    intro: id28

tuesday, september 15, 15

3

multinomial naive bayes
p (wt | y)

p (y | w1..wt ) / p (y) yt

tokens in doc

parameters: p (w | y)

for each document category y and wordtype w
p (y) prior distribution over document categories y

learning:  estimate parameters as pseudocounted frequency ratios

p (w | y,    ) =

#(w occurrences in docs with label y) +    

#(tokens total across docs with label y) + v    

predictions:

predict class

arg max

y

p (y = y | w1..wt )

or, predict prob of classes...

tuesday, september 15, 15

why pseudocounts?
p (y | w1..wt ) / p (y) yt
p (wt | y)

learning:  estimate parameters as pseudocounted frequency ratios

#(w occurrences in docs with label y) +    

#(tokens total across docs with label y) + v    

p (w | y,    ) =
    alpha = 0   =>   ?
    alpha = 0.000001   ==>  ?

tuesday, september 15, 15

5

why pseudocounts?
p (y | w1..wt ) / p (y) yt
p (wt | y)

learning:  estimate parameters as pseudocounted frequency ratios

#(w occurrences in docs with label y) +    

#(tokens total across docs with label y) + v    

p (w | y,    ) =
    alpha = 0   =>   ?
    alpha = 0.000001   ==>  ?
    question: as alpha gets higher, posterior 
probabilities tend to
    (a)  50%
    (b)  p(y)
    (c)  100%
    (d)  no common trend

5

tuesday, september 15, 15

over   tting

training data

    over   tting:  model cares too much about 
    to check: held-out data, e.g. train vs test
    training vs test accuracy:  which is higher?
    pseudocount parameter combats over   tting

tuesday, september 15, 15

6

how to set the pseudocount?

test.

    split data into train versus 
    try different pseudocounts. 
for each train the model and 
predict on the held-out data.  
choose lambda that does 
best on test set: e.g. 
maximizes accuracy or 
likelihood.

    what values to try?  often 
we use a grid search
    e.g. (2^-2, 2^-1 ... 2^4, 2^5)

tuesday, september 15, 15

7

use this one

test set
accuracy

alpha

hopefully looks 

like this

data splitting

    train vs test
    better: use 
    train:  for    tting model parameters
    dev:  for tuning hyperparameters
    test:  reserve for    nal evaluation
    cross-validation

tuesday, september 15, 15

8

feature engineering

    what   s your word/feature representation?
    id121 rules:  splitting on whitespace?
    lowercase same as uppercase?
    numbers?
    punctuation?
    phrases?

tuesday, september 15, 15

9

