id28

september 17, 2015

questions?

    from previous lecture?

    from hw?

naive bayes recap

    what do you remember about classification 

with naive bayes?

naive bayes recap

    what do you remember about classification 

with naive bayes?

    what statistics do you need to make a 

classification?

naive bayes: bag of words

    bow - order independent

    can we add more features to the model?

naive bayes: bag of words

    features statistically independent given class

    examples of non-independent features?

independence assumption

    correlated features -> double counting

    can hurt classifier accuracy & calibration

id28

    (log) linear model - similar to naive bayes

    doesn   t assume features are independent

    correlated features don   t    double count   

classification: logreg (i)

first, we   ll discuss how logreg works.

classification: logreg (i)

first, we   ll discuss how logreg works.

then, why it   s set up the way that it is.

application: spam filtering

classification: logreg (i)

    compute features (xs)

classification: logreg (i)

    compute features (xs)

= (count    nigerian   , count    prince   , count    nigerian prince   )

classification: logreg (i)

    compute features (xs)

= (count    nigerian   , count    prince   , count    nigerian prince   )

    given weights (betas)

classification: logreg (i)

    compute features (xs)

= (count    nigerian   , count    prince   , count    nigerian prince   )

    given weights (betas)

= (-1.0,    -1.0,     4.0)

classification: logreg (i)

    compute features (x   s)
    given weights (betas)
    compute the dot product

classification: logreg (i)

    compute features (x   s)
    given weights (betas)
    compute the dot product

classification: logreg (ii)

    compute the dot product

classification: logreg (ii)

    compute the dot product

    compute the logistic function

logreg exercise

features: (count    nigerian   , count    prince   , count    nigerian prince   )

= (1,     1,     1)

= (-1.0,    -1.0,     4.0)

logreg exercise

= (1,     1,     1)

= (-1.0,    -1.0,     4.0)

classification: logreg

ok, let   s take this step by step...
    why dot product?

classification: logreg

ok, let   s take this step by step...
    why dot product?

    why would we use the logistic function?

classification: dot product

intuition: weighted sum of features
all linear models have this form!

nb as log-linear model

recall that naive bayes is also a linear model...

nb as log-linear model
    what are the features in naive bayes?

    what are the weights in naive bayes?

nb as log-linear model

nb as log-linear model

nb as log-linear model

nb as log-linear model

in both nb and logreg 

we compute the dot product!

logistic function

what does this function look like?

what properties does it have?

logistic function

logistic function

    logistic function 

logistic function

    logistic function 

    decision boundary is dot product = 0 (2 class)

logistic function

    logistic function 

    decision boundary is dot product = 0 (2 class)

    comes from linear log odds

nb vs. logreg

    both compute the dot product

    nb: sum of log probs; logreg: logistic fun.

learning weights

    nb: learn conditional probabilities separately 

via counting

    logreg: learn weights jointly

learning weights

    given: a set of feature vectors and labels

    goal: learn the weights.

learning weights

n examples; xs - features; ys - class

learning weights

we know:

so let   s try to maximize id203 of the entire 

dataset - id113

learning weights

so let   s try to maximize id203 of the entire 

dataset - id113

learning weights

so let   s try to maximize id203 of the entire 

dataset - id113

logreg exercise

features: (count    nigerian   , count    prince   , count    nigerian prince   )

= (1.0,    -3.0,     2.0)

63% accuracy

logreg exercise

features: (count    nigerian   , count    prince   , count    nigerian prince   )

= (1.0,    -3.0,     2.0)

= (0.5,    -1.0,     3.0)

63% accuracy

75% accuracy

logreg exercise

features: (count    nigerian   , count    prince   , count    nigerian prince   )

= (1.0,    -3.0,     2.0)

= (0.5,    -1.0,     3.0)

= (-1.0,    -1.0,     4.0)

63% accuracy

75% accuracy

81% accuracy

pros & cons

    logreg doesn   t assume independence

    better calibrated probabilities

    nb is faster to train; less likely to overfit

nb & log reg

    both are linear models: 

    training is different:

    nb: weights trained independently
    logreg: weights trained jointly

logreg: important details!

    overfitting / id173
    visualizing decision boundary / bias term
    multiclass logreg

you can use scikit-learn (python) to test it out!

bias term

