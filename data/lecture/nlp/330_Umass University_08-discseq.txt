id148 and crfs

[unused crf+id88 slides in this slidedeck too]

cs 585, fall 2015 -- oct. 1

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

thursday, october 1, 15

today

1. motivation: we want features in our sequence 

model!

2. and how do we learn the parameters?

3. outline

1. id148
2. log-linear sequence models:

1. log-scale additive viterbi
2. id49
3. learning:  the id88

thursday, october 1, 15

2

these are all id148

sequence

naive bayes

conditional

id48s

conditional

id28

linear-chain crfs

sequence

general
graphs

general
graphs

thursday, october 1, 15

fig. 2.3 diagram of the relationship between naive bayes, id28, id48s, linear-

[from sutton&mccallum reading]

3

[old slide, old notation]

thursday, october 1, 15

4

[old slide, old notation]

(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:47)(cid:82)(cid:74)(cid:53)(cid:72)(cid:74)(cid:3)(cid:11)(cid:44)(cid:12)

(cid:404) (cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:3)(cid:73)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)(cid:3)(cid:11)(cid:91)(cid:182)(cid:86)(cid:12)
(cid:404) (cid:74)(cid:76)(cid:89)(cid:72)(cid:81)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)(cid:3)(cid:11)(cid:69)(cid:72)(cid:87)(cid:68)(cid:86)(cid:12)
(cid:404) (cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:71)(cid:82)(cid:87)(cid:3)(cid:83)(cid:85)(cid:82)(cid:71)(cid:88)(cid:70)(cid:87)

decision rule:

  z > 0  ->  decide y*=pos
z <= 0  ->  decide y*=neg

5

thursday, october 1, 15

id148
    the form will generalize to multiclass and sequences...

    x:  text data
    y:  proposed class  or proposed sequence
      :  feature weights (model parameters)
    f(x,y):  feature extractor, produces feature vector
goodness(y) =xi

   ifi(x, y)

dot product notation:

       tf (x, y)

p(y|x) / exp g(y) , log p(y|x) = c + g(y)

decision rule:

arg max

y   

g(y   )

nb and logreg can be expressed in this form...
id48s and crfs can be expressed in this form...

thursday, october 1, 15

6

log-linear notation
g(y)  =       f(x,y)

f(x,y) based on these feature templates:
   key: (class=y and word=w) 
   value: count of w

  

{"pos_the": +0.01,
 "neg_the": -0.01,
 "pos_awesome": +2.2,
 "neg_awesome": -2.2,
...}

     f(x,pos) = ....
     f(x,neg) = ....

7

thursday, october 1, 15

f(x,  pos)
{"pos_the": 3,
 "pos_awesome": 7,
 "pos_quizzical": 0,
...}

f(x,  neg)
{"neg_the": 3,
 "neg_awesome   : 7,
...}

id148
    the form will generalize to multiclass and sequences...

    x:  text data
    y:  proposed class  or proposed sequence
      :  feature weights (model parameters)
    f(x,y):  feature extractor, produces feature vector
goodness(y) =xi

   ifi(x, y)

dot product notation:

       tf (x, y)

p(y|x) / exp g(y) , log p(y|x) = c + g(y)

decision rule:

arg max

y   

g(y   )

nb and logreg can be expressed in this form...
id48s and crfs can be expressed in this form...

thursday, october 1, 15

8

id148
    the form will generalize to multiclass and sequences...

    x:  text data
    y:  proposed class  or proposed sequence
      :  feature weights (model parameters)
    f(x,y):  feature extractor, produces feature vector
goodness(y) =xi

   ifi(x, y)

dot product notation:

       tf (x, y)

p(y|x) / exp g(y) , log p(y|x) = c + g(y)

decision rule:

arg max

y   

g(y   )

nb and logreg can be expressed in this form...
id48s and crfs can be expressed in this form...

thursday, october 1, 15

9

today

1. motivation: we want features in our sequence 

model!

2. and how do we learn the parameters?

3. outline

1. id148
2. log-linear sequence models:

1. log-scale additive viterbi
2. id49
3. learning:  the id88

thursday, october 1, 15

10

crf motivation: best of both worlds

conditional

conditional

    want info from features

naive bayes

is this the    rst token in the 
sentence?
second? third? last? next to last?
conditional

   
   
    word to left?  right?
   
   

last 3 letters of this word?  last 3 
letters of word on left?  on right?
is this word capitalized?  does it 
contain punctuation?

sequence

id48s

id28

linear-chain crfs

sequence

conditional

fig. 2.3 diagram of the relationship between naive bayes, id28, id48s, linear-
chain crfs, generative models, and general crfs.

sequence

id28

linear-chain crfs

    want info from pos context

    what tags are left/right?
    need joint decoding (viterbi)
sequence

fig. 2.3 diagram of the relationship between naive bayes, id28, id48s, linear-
chain crfs, generative models, and general crfs.

one perspective for gaining insight into the di   erence between gen-
erative and discriminative modeling is due to minka [80]. suppose we
have a generative model pg with parameters    . by de   nition, this takes
the form
id48s

one perspective for gaining insight into the di   erence between gen-
erative and discriminative modeling is due to minka [80]. suppose we

pg(y, x;    ) = pg(y;    )pg(x|y;    ).

general
graphs

generative directed models

naive bayes

thursday, october 1, 15

from id48 to crf

1. an id48 is a type of log-linear model

with    transition    and    emission    features.

2. do discriminative learning:  instead of 

learning the weights as simple conditional 
probabilities .... learn them to make high-
accuracy sequence predictions
[the structured id88: predict the entire sequence 
(viterbi), then update weights where there are errors.]

3. throw in lots more features!

thursday, october 1, 15

12

a1

a2

y1

b1

y2

b2

y3

b3

id48 as factor graph
p(y, w) =yt
log p(y, w) =xt

p(yt+1|yt)p(wt|yt)

g(y)

goodness

log p(wt|yt) + log p(yt+1|yt)
bt(yt)
a(yt, yt+1)
emission
transition
factor score

factor score

you can do viterbi with these log-scale factor scores.

   additive viterbi    let   s call it?

-- see exercise! --

thursday, october 1, 15

13

    stopped here on 10/1

thursday, october 1, 15

14

a1

a2

y1

b1

y2

b2

y3

b3

id48 as log-linear
p(y, w) =yt
log p(y, w) =xt

p(yt+1|yt)p(wt|yt)

g(y)

goodness

24xk2k xw2v
xt
xt xi2allfeats
xi2allfeats

g(y) =

=
=

thursday, october 1, 15

   ift,i(yt, yt+1, wt)

   ifi(yt, yt+1, wt)

log p(yt|wt) + log p(yt+1|yt)
a(yt, yt+1)
bt(yt)
emission
transition
factor score

factor score

  w,k1{yt = k ^ wt = w} + xk,j2k

 j,k1{yt = j ^ yt+1 = k}35

15

[~ sm eq 1.13, 1.14]

crf

    prob dist over whole sequence

log p(~y|~x) = c +    t ~f (~x, ~y)

    linear chain crf:

~f (~x, ~y) =xt

~ft(~x, yt, yt+1)

    its feature functions decompose over functions of 
    advantages

neighboring tags.

    1. why just word identity features?  add many more!
    2. can train it to optimize accuracy of sequences 

(discriminative learning)

thursday, october 1, 15

16

we can write (1.13) more compactly by introducing the concept of feature functions,
just as we did for id28 in (1.7). each feature function has the
form fk(yt, yt 1, xt). in order to duplicate (1.13), there needs to be one feature
fij(y, y0, x) = 1{y=i}1{y0=j} for each transition (i, j) and one feature fio(y, y0, x) =
1{y=i}1{x=o} for each state-observation pair (i, o). then we can write an id48 as:

p(y, x) =

1
z

exp( kxk=1

 kfk(yt, yt 1, xt)) .

(1.14)

    is there a terrible bug in sutton&mccallum?  

again, equation (1.14) de   nes exactly the same family of distributions as (1.13),
and therefore as the original id48 equation (1.8).
the last step is to write the conditional distribution p(y|x) that results from the
there   s no sum over t in these equations!
id48 (1.14). this is
an introduction to id49 for relational learning

10

thursday, october 1, 15

=

1
z

(1.15)

p(y, x) =

k=1  kfk(yt, yt 1, xt)o
k=1  kfk(y0t, y0t 1, xt)o .

expnpk
py0 expnpk
exp( kxk=1

p(y|x) = p(y, x)
py0 p(y0, x)

we can write (1.13) more compactly by introducing the concept of feature functions,
just as we did for id28 in (1.7). each feature function has the
form fk(yt, yt 1, xt). in order to duplicate (1.13), there needs to be one feature
this conditional distribution (1.15) is a linear-chain crf, in particular one that
fij(y, y0, x) = 1{y=i}1{y0=j} for each transition (i, j) and one feature fio(y, y0, x) =
includes features only for the current word   s identity. but many other linear-chain
1{y=i}1{x=o} for each state-observation pair (i, o). then we can write an id48 as:
crfs use richer features of the input, such as pre   xes and su xes of the current
word, the identity of surrounding words, and so on. fortunately, this extension
(1.14)
requires little change to our existing notation. we simply allow the feature functions
fk(yt, yt 1, xt) to be more general than indicator functions. this leads to the general
again, equation (1.14) de   nes exactly the same family of distributions as (1.13),
de   nition of linear-chain crfs, which we present now.
and therefore as the original id48 equation (1.8).
de   nition 1.1
the last step is to write the conditional distribution p(y|x) that results from the
let y, x be random vectors,     = { k} 2 <k be a parameter vector, and
id48 (1.14). this is
{fk(y, y0, xt)}k
k=1 be a set of real-valued feature functions. then a linear-chain
k=1  kfk(yt, yt 1, xt)o
conditional random    eld is a distribution p(y|x) that takes the form
k=1  kfk(y0t, y0t 1, xt)o .
 kfk(yt, yt 1, xt)) ,

p(y|x) = p(y, x)
py0 p(y0, x)
p(y|x) =

 kfk(yt, yt 1, xt)) .

this conditional distribution (1.15) is a linear-chain crf, in particular one that
where z(x) is an instance-speci   c id172 function
includes features only for the current word   s identity. but many other linear-chain
crfs use richer features of the input, such as pre   xes and su xes of the current
word, the identity of surrounding words, and so on. fortunately, this extension
requires little change to our existing notation. we simply allow the feature functions

 kfk(yt, yt 1, xt)) .

expnpk
py0 expnpk
exp( kxk=1
exp( kxk=1

z(x) =xy

(1.15)

(1.17)

(1.16)

z(x)

17

=

1

today

1. motivation: we want features in our sequence 

model!

2. and how do we learn the parameters?

3. outline

1. id148
2. log-linear sequence models:

1. log-scale additive viterbi
2. id49

3. learning:  the id88

thursday, october 1, 15

18

id88 learning algorithm

    for ~10 iterations
    for each (x,y) in dataset

    predict  

y    = arg max

   tf (x, y0)

y0

    if y=y*, do nothing
    else update weights

    :=     + r[f (x, y)   f (x, y   )]

learning rate constant

e.g. r=0.01

features for
true label

features for

predicted label

thursday, october 1, 15

19

update rule

y=pos   
x=   this awesome movie ...   
make mistake: y*=neg

learning rate
e.g. r=0.01

features for
true label

features for

predicted label

    :=     + r[f (x, y)   f (x, y   )]

pos_aw
esome

pos_this pos_oof

.... neg_aw
esome

neg_this neg_oof

....

f(x,  pos) =

f(x,  neg) =

1

0

f(x,  pos)     f(x, neg) =

+1

1

0

+1

0

0

0

....

....

....

0

1

-1

0

1

-1

0

0

0

....

....

....

thursday, october 1, 15

20

update rule

learning rate
e.g. r=0.01

features for
true label

features for

predicted label

    :=     + r[f (x, y)   f (x, y   )]

for binary features...
for each feature j in true y but not predicted y*:

   j :=    j + (r)fj(x, y)

for each feature j not in true y, but in predicted y*:

   j :=    j   (r)fj(x, y)

thursday, october 1, 15

21

id88 issues -- for next time

    does it converge? (sometimes, but generally no)
    solution: the averaged id88

take weight vectors every once in a while and 
average them
    can you regularize it?  no...  just averaging...
    by the way ... there   s also likelihood training out 

there

thursday, october 1, 15

22

