crf and structured id88

cs 585, fall 2015 -- oct. 6

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

tuesday, october 6, 15

    viterbi exercise solution
    crf & structured id88s
    thursday: project discussion + midterm review

tuesday, october 6, 15

2

id148  (nb, logreg, id48, crf...)
    x:  text data
    y:  proposed class  or sequence
      :  feature weights (model parameters)
    f(x,y):  feature extractor, produces feature vector

p(y|x) =

1
z

   tf (x, y)

exp    tf (x, y) 
}

{z

|

g(y)

decision rule:

arg

max

y   2outputs(x)

g(y   )

how to we evaluate for id48/crf?  viterbi!

3

tuesday, october 6, 15

things to do with a log-linear model

p(y|x) =

1
z

   tf (x, y)

exp    tf (x, y) 
}

{z

|

f(x,y)

g(y)

x

text input

feature extractor 
(feature vector)

decoding/prediction
g(y   )
arg

max

y   2outputs(x)

parameter learning

given

given

y

output

  

feature 
weights

given

given
(just one)

obtain
(just one)

given

(many pairs)

given

(many pairs)

obtain

feature engineering
(human-in-the-loop)

   ddle with

during

experiments

given

(many pairs)

given

(many pairs)

obtain
in each 

experiment

tuesday, october 6, 15

4

[this is new slide after lecture]

id48 as factor graph

a1

a2

y2

b2

y3

b3

y1

p(y, w) =yt
log p(y, w) =xt

g(y)

goodness

b1

p(wy|yt) p(yt+1|yt)
log p(wt|yt) + log p(yt|yt 1)
a(yt, yt+1)
transition
factor score

bt(yt)
emission

factor score

(additive) viterbi:

arg

max

y   2outputs(x)

g(y   )

tuesday, october 6, 15

5

we can write (1.13) more compactly by introducing the concept of feature functions,
just as we did for id28 in (1.7). each feature function has the
form fk(yt, yt 1, xt). in order to duplicate (1.13), there needs to be one feature
fij(y, y0, x) = 1{y=i}1{y0=j} for each transition (i, j) and one feature fio(y, y0, x) =
1{y=i}1{x=o} for each state-observation pair (i, o). then we can write an id48 as:

p(y, x) =

1
z

exp( kxk=1

 kfk(yt, yt 1, xt)) .

(1.14)

    is there a terrible bug in sutton&mccallum?  

again, equation (1.14) de   nes exactly the same family of distributions as (1.13),
and therefore as the original id48 equation (1.8).
the last step is to write the conditional distribution p(y|x) that results from the
there   s no sum over t in these equations!
id48 (1.14). this is
an introduction to id49 for relational learning

10

tuesday, october 6, 15

=

1
z

(1.15)

p(y, x) =

k=1  kfk(yt, yt 1, xt)o
k=1  kfk(y0t, y0t 1, xt)o .

expnpk
py0 expnpk
exp( kxk=1

p(y|x) = p(y, x)
py0 p(y0, x)

we can write (1.13) more compactly by introducing the concept of feature functions,
just as we did for id28 in (1.7). each feature function has the
form fk(yt, yt 1, xt). in order to duplicate (1.13), there needs to be one feature
this conditional distribution (1.15) is a linear-chain crf, in particular one that
fij(y, y0, x) = 1{y=i}1{y0=j} for each transition (i, j) and one feature fio(y, y0, x) =
includes features only for the current word   s identity. but many other linear-chain
1{y=i}1{x=o} for each state-observation pair (i, o). then we can write an id48 as:
crfs use richer features of the input, such as pre   xes and su xes of the current
word, the identity of surrounding words, and so on. fortunately, this extension
(1.14)
requires little change to our existing notation. we simply allow the feature functions
fk(yt, yt 1, xt) to be more general than indicator functions. this leads to the general
again, equation (1.14) de   nes exactly the same family of distributions as (1.13),
de   nition of linear-chain crfs, which we present now.
and therefore as the original id48 equation (1.8).
de   nition 1.1
the last step is to write the conditional distribution p(y|x) that results from the
let y, x be random vectors,     = { k} 2 <k be a parameter vector, and
id48 (1.14). this is
{fk(y, y0, xt)}k
k=1 be a set of real-valued feature functions. then a linear-chain
k=1  kfk(yt, yt 1, xt)o
conditional random    eld is a distribution p(y|x) that takes the form
k=1  kfk(y0t, y0t 1, xt)o .
 kfk(yt, yt 1, xt)) ,

p(y|x) = p(y, x)
py0 p(y0, x)
p(y|x) =

 kfk(yt, yt 1, xt)) .

this conditional distribution (1.15) is a linear-chain crf, in particular one that
where z(x) is an instance-speci   c id172 function
includes features only for the current word   s identity. but many other linear-chain
crfs use richer features of the input, such as pre   xes and su xes of the current
word, the identity of surrounding words, and so on. fortunately, this extension
requires little change to our existing notation. we simply allow the feature functions

 kfk(yt, yt 1, xt)) .

expnpk
py0 expnpk
exp( kxk=1
exp( kxk=1

z(x) =xy

(1.15)

(1.17)

(1.16)

z(x)

=

1

6

a1

a2

y2

b2

y3

b3

id48 as log-linear

y1

p(y, w) =yt
log p(y, w) =xt

g(y)

goodness

24xk2k xw2v
xt
xt xi2allfeats
xi2allfeats

g(y) =

=
=

tuesday, october 6, 15

b1

p(wy|yt) p(yt+1|yt)
log p(wt|yt) + log p(yt|yt 1)
a(yt, yt+1)
transition
factor score

bt(yt)
emission

factor score

  w,k1{yt = k ^ wt = w} + xk,j2k

 j,k1{yt = j ^ yt+1 = k}35

   ift,i(yt, yt+1, wt)

   ifi(yt, yt+1, wt)

7

[~ sm eq 1.13, 1.14]

crf

ft(x, yt, yt+1)

log p(y|x) = c +    tf (x, y)
f (x, y) =xt
    advantages

prob. dist over whole sequence

linear-chain crf: whole-
sequence feature function 

decomposes into pairs

    1. why just word identity features?  add many more!
    2. can train it to optimize accuracy of sequences 

(discriminative learning)

    viterbi can be used for ef   cient prediction

tuesday, october 6, 15

8

   nna

gold  y =

v

get
v

good

a

two simple feature templates

   transition features   

   observation features   

ftrans:a,b(x, y) =xt
femit:a,w(x, y) =xt

1{yt 1 = a, yt = b}

1{yt = a, xt = w}

tuesday, october 6, 15

9

f(x,y) is...

v,v: 1
v,a: 1
v,n: 0
....
v,   nna: 1
v,get: 1
a,good: 1
n,good: 0
...

   nna

gold  y =

v

get
v

good

a

mathematical convention is 
numeric indexing, though 
sometimes convenient to 
implement as hash table.

   

f (x, y)

transition features

observation features

-0.6

-1.0

1.1

0.5

0.0

0.8

0.5

-1.3

-1.6

0.0

0.6

0.0

-0.2

-0.2

0.8

-1.0

0.1

-1.9

1.1

1.2

-0.1

-1.0

-0.1

-0.1

1 0 2 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 3 0 0 0 0

ftrans:v,a(x, y) =

nxt=2

1{yt 1 = v, yt = a}

fobs:v,   nna(x, y) =

nxt=1

1{yt = v, xt =    nna}

goodness(y) =

   tf (x, y)

tuesday, october 6, 15

implement features for seq(), which extracts the full feature vector f (x, y),
question 5.3. [2 points]
crf: prediction with viterbi
where x is a sentence and y is an entire tagging sequence for that sentence. this will add up the
feature vectors from each local emissions features for every position, as well as transition features
for every position (there are n   1 of them, of course). show the output on a very short example
sentence and example proposed tagging, that   s only 2 or 3 words long.
to de   ne f (x, y) a little more precisely: if f (b)(t, x, y) means the local emissions feature vector
linear-chain crf: whole-
at position t (i.e. the local emission features function), and f (a)(yt 1, yt, y) is the transition feature
sequence feature function 
function for positions (t 1, t) (which just returns a feature vector where everything is zero, except
a single element is 1), then the full sequence feature vector will be the vector-sum of all those

log p(y|x) = c +    tf (x, y)
f (x, y) =xt
    scoring function has local decomposition
f (x, y) =

prob. dist over whole sequence

decomposes into pairs

f (b)(t, x, y) +

ft(x, yt, yt+1)

txt
   tf (x, y) =xt

txt=2
f (a)(yt 1, yt)
txt=2

you implemented f (b) above. you probably don   t need to bother implementing f (a) as a stan-
dalone function. you will have to decide on a particular convention to encode the name of a tran-
sition feature. for example, one way to do it is with string concatenation like this, "trans %s %s"
% (prevtag, curtag), where prevtag and curtag are strings. or you could use a python tuple

+f (a)(yt 1, yt)

   tf (b)(t, x, y) +

tuesday, october 6, 15

11

1. motivation: we want features in our sequence 

model!

2. and how do we learn the parameters?

3. outline

1. id148
2. log-linear sequence models:

1. log-scale additive viterbi
2. id49

3. learning:  the id88

tuesday, october 6, 15

12

the id88 algorithm

    id88 is not a model:

it is a learning algorithm
    rosenblatt 1957

    insanely simple algorithm
    iterate through dataset.

predict.
update weights to    x prediction errors.

    can be used for classi   cation or 

id170
    structured id88

    discriminative learning algorithm for any 
log-linear model  (our view in this course)

the mark i id88 machine was the    rst 
implementation of the id88 algorithm. the machine 
was connected to a camera that used 20  20 cadmium 
sul   de photocells to produce a 400-pixel image. the 
main visible feature is a patchboard that allowed 
experimentation with different combinations of input 
features. to the right of that are arrays ofpotentiometers 
that implemented the adaptive weights.

tuesday, october 6, 15

13

binary id88

    for ~10 iterations
    for each (x,y) in dataset

    predict  

y    = p os if    tx   0
= n eg if    tx < 0

    if y=y*, do nothing
    else update weights

    :=     + r x if pos misclassi   ed as neg:
let   s make it more positive-y next time around
    :=       r x
if neg misclassi   ed as pos:
let   s make it more negative-y next time

learning rate constant

e.g. r=1

tuesday, october 6, 15

14

structured/multiclass id88

    for ~10 iterations
    for each (x,y) in dataset

    predict  

y    = arg max

   tf (x, y0)

y0

    if y=y*, do nothing
    else update weights

    :=     + r[f (x, y)   f (x, y   )]

learning rate constant

e.g. r=1

features for
true label

features for

predicted label

tuesday, october 6, 15

15

update rule

y=pos   
x=   this awesome movie ...   
make mistake: y*=neg

learning rate

e.g. r=1

features for
true label

features for

predicted label

    :=     + r[f (x, y)   f (x, y   )]

pos_aw
esome

pos_this pos_oof

.... neg_aw
esome

neg_this neg_oof

....

real  f(x,  pos) =

pred  f(x,  neg) =

1

0

f(x,  pos)     f(x, neg) =

+1

1

0

+1

0

0

0

....

....

....

0

1

-1

0

1

-1

0

0

0

....

....

....

tuesday, october 6, 15

16

update rule

learning rate

e.g. r=1

features for
true label

features for

predicted label

    :=     + r[f (x, y)   f (x, y   )]

for each feature j in true y but not predicted y*:

   j :=    j + (r)fj(x, y)

for each feature j not in true y, but in predicted y*:

   j :=    j   (r)fj(x, y)

tuesday, october 6, 15

17

   nna

gold  y =

v

get
v

good

a

two simple feature templates

   transition features   

   observation features   

ftrans:a,b(x, y) =xt
femit:a,w(x, y) =xt

1{yt 1 = a, yt = b}

1{yt = a, xt = w}

tuesday, october 6, 15

18

f(x,y) is...

v,v: 1
v,a: 1
v,n: 0
....
v,   nna: 1
v,get: 1
a,good: 1
n,good: 0
...

   nna

gold  y =

v

get
v

good

a

mathematical convention is 
numeric indexing, though 
sometimes convenient to 
implement as hash table.

   

f (x, y)

transition features

observation features

-0.6

-1.0

1.1

0.5

0.0

0.8

0.5

-1.3

-1.6

0.0

0.6

0.0

-0.2

-0.2

0.8

-1.0

0.1

-1.9

1.1

1.2

-0.1

-1.0

-0.1

-0.1

1 0 2 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 3 0 0 0 0

ftrans:v,a(x, y) =

nxt=2

1{yt 1 = v, yt = a}

fobs:v,   nna(x, y) =

nxt=1

1{yt = v, xt =    nna}

goodness(y) =

   tf (x, y)

tuesday, october 6, 15

   nna

gold  y =

v
pred  y* = n

get
v
v

good

a
a

learning idea:  want gold y to 
have high scores.
update weights so y would 
have a higher score, and y* 
would be lower, next time.

f(x, y)
v,v: 1
v,a: 1
v,   nna: 1
v,get: 1
a,good: 1

f(x, y*)
n,v: 1
v,a: 1
n,   nna: 1
v,get: 1
a,good: 1

f(x,y) - f(x, y*)

v,v: +1
n,v: -1
v,   nna: +1
n,   nna: -1

id88 update rule:

    :=     + r[f (x, y)   f (x, y   )]

tuesday, october 6, 15

    :=     + r[f (x, y)   f (x, y   )]

transition features

observation features

   

f (x, y)
f (x, y   )

-0.6

-1.0

1.1

0.5

0.0

0.8

0.5

-1.3

-1.6

0.0

0.6

0.0

-0.2

-0.2

0.8

-1.0

0.1

-1.9

1.1

1.2

-0.1

-1.0

-0.1

1 0 2 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 3 0 0 0

1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 3 0 0 0

the update vector:

+ r

(

+1

tuesday, october 6, 15

-1

)

id88 notes/issues

    issue: does it converge? (generally no)
    solution: the averaged id88
    can you regularize it?  no...  just averaging...
    by the way, there   s also likelihood training out 
there (gradient ascent on the log-likelihood 
function: the traditional way to train a crf)
    structperc is easier to implement/conceptualize 

and performs similarly in practice

tuesday, october 6, 15

22

f (xi, y   )

 

g =

f (xi, yi)

both in theory and in practice, the predictive accuracy of a model trained by the structured
id88 will be better if we use the average value of     over the course of training, rather than
the    nal value of    . this is because     wanders around and doesnt converge (typically), because it
over   ts to whatever data it saw most recently. after seeing t training examples, de   ne the averaged

voted perc or averaged perc

feats of true output

feats of predicted output

| {z }

averaged id88
{z
|
    to get stability for the id88:
    see hw2 writeup
    averaging: for t   th example... average together 

}

vectors from every timestep

     t =

1
t

txt0=1

   t0

    ef   ciency?
    lazy update algorithm in hw

where    t0 is the weight vector after t0 updates.
(we are counting t by the number of training
examples, not passes through the data. so if you had 1000 examples and made 10 passes through
the data in order, the    nal time you see the    nal example is t = 10000.) for training, you still use
the current     parameter for predictions. but at the very end, you return the      , not    , as your    nal
model parameters to use on test data.

tuesday, october 6, 15

23

