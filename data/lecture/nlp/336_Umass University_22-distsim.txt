id65

cs 585, fall 2015

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

college of information and computer sciences

university of massachusetts amherst

tuesday, december 1, 15

[many slides borrowed from david belanger]

%
ques<on:%%

how%can%we%use%unsupervised%data%
improve%accuracy%on%a%supervised%

task?%%

tuesday, december 1, 15

%
answer:%%

the distributional hypothesis

          you%shall%know%a%word%by%the%company%

it%keeps.   %(firth,%57)%

       words%with%similar%roles%in%text%have%

similar%meanings.%%

       this%is%why%unsupervised%learning%works%

in%nlp.%%

tuesday, december 1, 15

cooccurrence&count&data&

tuesday, december 1, 15

the%   context   %of%a%token%

target%word:%blue%
context%words:%red%

she%told%the%story,%however,%with%great%spirit%
among%her%friends;%for%she%had%a%lively,%playful%
disposi<on,%which%delighted%in%anything%
ridiculous.%
%
%
(source:%pride%and%prejudice)%%

tuesday, december 1, 15

the%   context   %of%a%token%

target%word:%blue%
context%words:%red%

she%told%the%story,%however,%with%great%spirit%
among%her%friends;%for%she%had%a%lively,%playful%
disposi<on,%which%delighted%in%anything%
ridiculous.%
%
%
(source:%pride%and%prejudice)%%

tuesday, december 1, 15

the%   context   %of%a%token%

target%word:%blue%
context%words:%red%

she%told%the%story,%however,%with%great%spirit%
among%her%friends;%for%she%had%a%lively,%playful%
disposi<on,%which%delighted%in%anything%
ridiculous.%
%
%
(source:%pride%and%prejudice)%%

tuesday, december 1, 15

contexts%in%terms%of%parses%

(nltk.org)%

(ryan%mcdonald%thesis)%

tuesday, december 1, 15

context%types%

%
each%possible%context%is%a%tuple.%%

      trigram%context:%(the,dog)%
      unigram%context:%(the)%or%(dog)%
      parse%context:%(red_amod,ran_nsubj)%%

%

tuesday, december 1, 15

context%count%vector%
       represent%word%type%i,%as%a%vector%vi%%

%%

vi = [0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 1]

       value%in%index%k%%=%#<mes%context%type%k%

occurred.%%

tuesday, december 1, 15

example%
       find%contexts%containing%   art   %

vi = [0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 1]

a%collec<on%of%_%

structure%of_%

_%is%a%crea<on%

an%exhibi<on%of%_%

vi%is%very%long,%but%very%sparse.%%

tuesday, december 1, 15

%
ques<on:%%

example%sentence:%

the%dog%caught%the%frisbee.%%

what%are%3%reasonable%ways%to%
de   ne%context,%and%what%are%the%

vectors%for%   caught   %in%each?%

tuesday, december 1, 15

%
ques<on:%%

what%do%   art   %and%   pharmaceu<cals   %
have%in%common?%
%
what%are%contexts%that%they%would%
both%have?%%
what%are%contexts%that%they%wouldn   t%
share?%

tuesday, december 1, 15

comparing%context%vectors%

tuesday, december 1, 15

comparing%vectors%

deuclidean(x, y) =sxi
dmanhattan(x, y) =xi
x>y =xi
px>xpy>y

cos(x, y) =

dot%product:%

x>y

(xi   yi)2

|xi   yi|
xiyi

cosine similarity: 
very commonly used
tuesday, december 1, 15

vectorbspace%interpreta<on%of%

distribu<onal%hypothesis%

%
two%words%are%similar%if%their%context%vectors%%
are%similar.%%

tuesday, december 1, 15

%
ques<on:%%
what%does%it%mean%for%two%words%to%
be%similar?%%
%
are%   dog   %and%   <ger   %similar?%%
how%about%   dog   %and%   fetch?   %

tuesday, december 1, 15

%
ques<on:%%
what%are%the%pros%and%cons%of%using%a%
wide%window%for%a%token   s%context?%
%
hint:%syntax%v.s.%topics.%%
%

tuesday, december 1, 15

%
ques<on:%%

we%now%have%a%func<on%sim(word1,word2).%
how%could%we%use%this%to%improve%accuracy%in%
the%tasks%we   ve%discussed%in%class?%

tuesday, december 1, 15

wordbcontext%matrix%

context%

word%
type%

%
%
%
distribu4onal&hypothesis:&&
      a%word%is%characterized%by%its%row%in%this%matrix.%
      similar%words%have%similar%rows%
&
&

tuesday, december 1, 15

topic%model%

document%

word%
type%

%

%

a%document%is%characterized%by%the%distribu<on%of%words%in%it.%%

documents%are%similar%if%their%columns%are%similar.%%

lda%topic%model:%this%distribu<on%is%a%mixture%of%   topics   %%

%

tuesday, december 1, 15

word&embeddings&

tuesday, december 1, 15

word%embeddings%

sparse%context%vector%(10%million+%dimensional):%
%
vi = [0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 1, . . .]
%
[this can be directly used, but maybe too slow, sparse]
instead%represent%every%word%type%as%a%lowb
dimensional%dense%vector%(about%100%dimensional%).%%
%
%
these%don   t%come%directly%from%the%data.%they%need%
to%be%learned.%%

ei = [.253, 458, 4.56, 78.5, 120, . . .]

tuesday, december 1, 15

tuesday, december 1, 15

nearest%neighbors%

       deals%bb>%checks%approvals%vents%s<ckers%cuts%
       warned%bb>%suggested%speculated%predicted%

stressed%argued%

       ability%bb>%willingness%inability%eagerness%

disinclina<on%desire%

       dark%bb>%comfy%wild%austere%cold%<nny%
       possibility%bb>%possiblity%possibilty%dangers%

no<on%likelihood%

tuesday, december 1, 15

nearest%neighbors%

       deals%bb>%checks%approvals%vents%s<ckers%cuts%
       warned%bb>%suggested%speculated%predicted%

stressed%argued%

       ability%bb>%willingness%inability%eagerness%

disinclina<on%desire%

       dark%bb>%comfy%wild%austere%cold%<nny%
       possibility%bb>%possiblity%possibilty%dangers%

no<on%likelihood%

tuesday, december 1, 15

%
ques<on:%%

what%are%the%pros%and%cons%of%

represen<ng%word%types%with%such%

small%vectors?%

tuesday, december 1, 15

answer:%%

pro:%
%it%requires%less%annotated%data%to%
train%an%ml%model%on%low%dimensional%
features.%%
%
con:%
%you%can   t%capture%all%of%the%subtlety%of%
language%in%100%dimensions.%%%(...can you?)

tuesday, december 1, 15

learning%embeddings%by%preserving%

similarity%

       given%long,%sparse%context%cooccurrence%

vectors%%%%%%%and%%

vi

vj

       goal:%choose%embeddings%%%%%%%and%%%%%%%%such%
that%similarity%is%approximately%preserved%

ej

ei

v >i vj     e>i ej

       di   culty:%need%to%do%this%for%all%words%jointly.%%
       solu<on:%use%an%eigenbdecomposi<on%

(implemented%in%every%language).%%

tuesday, december 1, 15

id105

   

v

(counts)

e

(embeddings)

b

s

s
d
r
o
w

   

contexts

latent dims

i

m
d
 
t
n
e
t
a

contexts l

reconstruct the co-occurrence matrix

vi,c    xk

ei,kbk,c

singular value decomposition learns e,b
(or other id105 techniques)

tuesday, december 1, 15

preserve pairwise distances

between words i, j
i ej

v t
i vj     et

eigen decomposition learns e

30

word id91

    alternative word representation:  associate 

words with (hierarchical) clusters, as opposed 
to embeddings (real-valued vectors)

       brown id91   :
unsupervised id48 with hierarchical id91
    word belongs to only one class 
    iteratively merge words with high contextual 

(bad assumption, but better than alternative;  blunsom et al. 2011)

similarity

tuesday, december 1, 15

31

word clusters as features

features!

    labeled data is small and sparse.  lexical 
generalization via induced word classes.
    both embeddings and clusters can be used as 
    examples from twitter, for id52
    big data vs. i make my own data
    unlabeled:  56 m tweets, 847 m tokens
    labeled:      2374 tweets, 34k tokens
    1000 clusters over 217k word types
    preprocessing: discard words that occur < 40 times

http://www.ark.cs.cmu.edu/tweetnlp/cluster_viewer.html

32

tuesday, december 1, 15

what does it learn?
    orthographic id172s

so s0 -so so- $o /so //so
soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo 
sooooooooooo soooooooooooo sooooooooooooo soso soooooooooooooo 
sooooooooooooooo soooooooooooooooo sososo superrr sooooooooooooooooo ssooo 
so0o superrrr so0 soooooooooooooooooo sosososo sooooooooooooooooooo ssoo 
sssooo soooooooooooooooooooo #too s0o ssoooo s00 sooooooooooooooooooooo 
so0o0o sososososo soooooooooooooooooooooo sssoooo ssooooo superrrrr very2 
s000 soooooooooooooooooooooooo sooooooooooooooooooooooooo 
sooooooooooooooooooooooo _so_ soooooooooooooooooooooooooo /so/ sssooooo 
sosososososo

    suggests joint model for morphology/spelling

33

tuesday, december 1, 15

   

emoticons etc.
(clusters/tagger useful for id31: nrc-canada semeval 2013, 2014)

tuesday, december 1, 15

(immediate?) future auxiliaries

gonna gunna gona gna guna gnna ganna qonna gonnna gana 
qunna gonne goona gonnaa g0nna goina gonnah goingto 
gunnah gonaa gonan gunnna going2 gonnnna gunnaa gonny 
gunaa quna goonna qona gonns goinna gonnae qnna gonnaaa 
gnaa

tryna gon    nna bouta trynna boutta gne    na gonn tryina 
fenna qone trynaa qon boutaa funna    nnah bouda boutah 
abouta fena bouttah boudda trinna qne    nnaa    tna aboutta 
goin2 bout2    nnna trynah    naa ginna bouttaa fna try'na g0n 
trynn tyrna trna bouto    nsta fnna tranna    nta tryinna    nnuh 
tryingto boutto

       nna ~       xing to   
    tryna ~    trying to   
    bouta ~    about to   

tuesday, december 1, 15

35

subject-auxverb constructs

[contraction 
splitting?]

[mixed]

i'd you'd we'd he'd they'd she'd who'd i   d u'd youd you   d iwould theyd 
icould we   d i`d #whydopeople he   d i  d #iusedto they   d i'ld she   d 
#iwantsomeonewhowill i'de imust a:i'd you`d yu'd icud l'd
ill ima imma i'ma i'mma ican iwanna umma imaa #imthetypeto iwill 
amma #menshouldnever igotta #whywouldyou #iwishicould 
#sometimesyouhaveto #thoushallnot #ihatewhenpeople illl 
#thingspeopleshouldnotdo #howdareyou #thingsgirlswantboystodo 
im'a #womenshouldnever #thingsblackgirlsdo immma iima 
#ireallyhatewhenpeople ishould #thingspeopleshouldntdo #irefuseto itl 
#howtospoilahoodrat iwont imight #thingsweusedtodoaskids ineeda 
#thingswhitepeopledo we'l #whycantyoujust #whydogirls 
#everymanshouldknowhowto #ushouldnt #howtopissyourgirloff 
#amanshouldnot #uwannaimpressme #realfriendsdont immaa 
#ilovewhenyou
you'll we'll it'll he'll they'll she'll it'd that'll u'll that'd youll ull you   ll itll 
there'll we   ll itd there'd theyll this'll thatd thatll they   ll didja he   ll it   ll 
yu'll she   ll youl you`ll you'l you  ll yull u'l it'l we  ll we`ll didya that   ll 
it   d he'l shit'll they'l theyl she'l everything'll he`ll things'll u   ll this'd
i'll i   ll i'l i`ll i  ll i'lll l'll i\'ll i''ll -i'll /must @pretweeting she`ll

36

tuesday, december 1, 15

   toyota technological institute at chicago, chicago, il 60637, usa

corresponding author: brenocon@cs.cmu.edu

word clusters as features

11101011011100

we consider the problem of part-of-speech
tagging for informal, online conversational
text. we systematically evaluate the use of
large-scale unsupervised word id91
and new lexical features to improve tagging
accuracy. with these features, our system
11100101111001
tagging results on
both twitter and irc id52 tasks;
twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). quali-
tative analysis of these word clusters yields
insights about nlp and linguistic phenomena
in this genre. additionally, we contribute the
   rst pos annotation guidelines for such text
and release a new dataset of english language
tweets annotated using these guidelines.

11101011001010
11100101111001
11101011001011

11101011001010
11100101111001
11101011001011

11101011011100
11100101111001

11101011011100

11101011011100

!

yo
d
u
o

ikr
!

asked

lololol

he
o
he
o

v
can
v

smh
g
so
p

top words (by frequency)

name
n
fb
^

   r
p
top words (by frequency)
add
v

last
a
on
lmao lmfao lmaoo lmaooo hahahahaha lool ctfu ro    loool lmfaoo lmfaooo lmaoooo lmbo lololol
p
haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
lmao lmfao lmaoo lmaooo hahahahaha lool ctfu ro    loool lmfaoo lmfaooo lmaoooo lmbo lololol
figure 1: automatically tagged tweet showing nonstan-
top words (by frequency)
u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo
dya youz yyou
haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
dard orthography, capitalization, and abbreviation. ignor-
lmao lmfao lmaoo lmaooo hahahahaha lool ctfu ro    loool lmfaoo lmfaooo lmaoooo lmbo lololol
yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
w fo fa fr fro ov fer    r whit abou aft serie fore fah fuh w/her w/that fron isn agains
ing the interjections and abbreviations, it glosses as he
haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
facebook fb itunes myspace skype ebay tumblr bbm    ickr aim msn net   ix pandora
asked for your last name so he can add you on facebook.
yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
tryna gon    nna bouta trynna boutta gne    na gonn tryina fenna qone trynaa qon
yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
the tagset is de   ned in appendix a. refer to fig. 2 for
u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo
dya youz yyou
top words (by frequency)
gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
word clusters corresponding to some of these words.
w fo fa fr fro ov fer    r whit abou aft serie fore fah fuh w/her w/that fron isn agains
lmao lmfao lmaoo lmaooo hahahahaha lool ctfu ro    loool lmfaoo lmfaooo lmaoooo lmbo lololol
soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
dya youz yyou
u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo
haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
facebook fb itunes myspace skype ebay tumblr bbm    ickr aim msn net   ix pandora
;) :p :-) xd ;-) ;d (;
:3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
w fo fa fr fro ov fer    r whit abou aft serie fore fah fuh w/her w/that fron isn agains
yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
tryna gon    nna bouta trynna boutta gne    na gonn tryina fenna qone trynaa qon
:) (: =) :)) :]
yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
facebook fb itunes myspace skype ebay tumblr bbm    ickr aim msn net   ix pandora
gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
:(( >_< =[
:( :/ -_- -.- :-( :   ( d:
smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
tryna gon    nna bouta trynna boutta gne    na gonn tryina fenna qone trynaa qon
<3
soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo
gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
;) :p :-) xd ;-) ;d (;
:3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
w fo fa fr fro ov fer    r whit abou aft serie fore fah fuh w/her w/that fron isn agains
soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
:) (: =) :)) :]

ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. there
:[ #fml
interjections   ??
#love s2 <url-twitition.com> #neversaynever <3333
is preliminary work on social media part-of-speech
dya youz yyou
(pos) tagging (gimpel et al., 2011), named entity

:| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /:

   non-standard 
prepositions   

   interjections   

xoxo <33 xo <333

:   ) =] ^_^ :))) ^.^ [:

((: ^__^ (= ^-^ :))))

   hashtag-y 

   online service 

names   

yue juu

yue juu

yue juu

yue juu

;))

figure 2: example word clusters (id48 classes): we list the most probable words, starting with the most probable, in
descending order. boldfaced words appear in the example tweet (figure 1). the binary strings are root-to-leaf paths

:   ) =] ^_^ :))) ^.^ [:

((: ^__^ (= ^-^ :))))

tuesday, december 1, 15

;))

highest-weighted pos   treenode features  
hierarchical structure generalizes nicely.

cluster prefix
11101010*

tag
! 

highest weighted clusters
types
8160

most common word in each cluster with prefix
lol lmao haha yes yea oh omg aww ah btw wow thanks 
sorry congrats welcome yay ha hey goodnight hi dear 
please huh wtf exactly idk bless whatever well ok
i'm im you're we're he's there's its it's
x <3 :d :p :) :o :/
young sexy hot slow dark low interesting easy important 
safe perfect special different random short quick bad crazy 
serious stupid weird lucky sad
the da my your ur our their his
do did kno know care mean hurts hurt say realize believe 
worry understand forget agree remember love miss hate 
think thought knew hope wish guess bet have
you yall u it mine everything nothing something anyone 
someone everyone nobody
or n & and

11000*
1110101100*
111110*

1101*
01*

11101*

100110*

l
e
a

d
v

o

&

428
2798
6510

378
29267

899

103

tuesday, december 1, 15

tagger, tokenizer, and data all released at:

38

85

86

87

88

89

90

91

92

93

94

clusters help id52

words+clusters+dicts
words+clusters
words+dicts
just3clusters
words

all handcrafted features 

(shape, regexes, char ngrams)

85

86

87

88

89

90

91

92

93

94

test set accuracy

tuesday, december 1, 15

39

85

86

87

88

89

90

91

92

93

94

clusters help id52

words+clusters+dicts
words+clusters
words+dicts
just3clusters
words

all handcrafted features 

(shape, regexes, char ngrams)

85

86

87

88

89

90

91

92

93

94

test set accuracy

tuesday, december 1, 15

39

85

86

87

88

89

90

91

92

93

94

clusters help id52

words+clusters+dicts
words+clusters
words+dicts
just3clusters
words

all handcrafted features 

(shape, regexes, char ngrams)

85

86

87

88

89

90

91

92

93

94

test set accuracy

tuesday, december 1, 15

39

85

86

87

88

89

90

91

92

93

94

clusters help id52

words+clusters+dicts
words+clusters
words+dicts
just3clusters
words

all handcrafted features 

(shape, regexes, char ngrams)

85

86

87

88

89

90

91

92

93

94

test set accuracy

1,827 26,594 app. a oct 27-28, 2010

7,707 app. a jan 2011   jun 2012

10,578 44,997 ptb-like oct   nov 2006

789 15,185 ptb-like unknown

table 1: annotated datasets: number of messages, to-
kens, tagset, and date range. more information in   5,

y
c
a
r
u
c
c
a
g
n
g
g
a
t

 

i

           

   

   

0
9

5
8

0
8

5
7

   

   

1e+03

1e+05

1e+07

number of unlabeled tweets

patterns that seem quite compatible with our ap-
proach. more complex downstream processing like

tuesday, december 1, 15

figure 3: oct27 development set accuracy using only
clusters as features.

39

dev set accuracy
using only clusters as features

using word representations

http://nlp.stanford.edu/projects/glove/

    pre-trained word representations you can download
    vectors: glove algorithm, trained on news, web, twitter 
    vectors: id97 algorithm, trained on web news 
    clusters: brown hierarchical id91, trained on twitter  
    or, train your own embeddings; open-source software for all the 

http://www.ark.cs.cmu.edu/tweetnlp/

https://code.google.com/p/id97/

above

    incorporate as features for supervised learning

    (hierarchical) cluster id
    embedding dimension value  (or cluster it...)
    similarity to seed term(s)

    manual dictionary building: go through similarity list, manually 
    compare:  manually curated lexical resources like

select ones you want

id138 and freebase

tuesday, december 1, 15

40

ongoing research
in word representations

    morphology
    phrases and multiwords:  compositionality
    vector-valued summaries of sentences, 
    neural networks / deep learning

paragraphs, documents?

tuesday, december 1, 15

41

tuesday, december 1, 15

42

%
ques<on:%%
what%do%the%words%   spinning   %and%
   repea<ng   %have%in%common?%
%%
how%could%we%use%this%to%learn%bemer%
word%embeddings?%

tuesday, december 1, 15

morphological%neural%language%model%
       represent%every%word%type%as%a%feature%

vector.%%

       learn%an%embedding%for%every%feature.%%
       the%embedding%for%a%word%is%the%sum%of%the%

embeddings%of%its%features.%%

tuesday, december 1, 15

tuesday, december 1, 15

word%pair%b%path%

i%ate$the$cake%
%he%ate$the%burger%
%michelle%ate$the$pizza%
%

i%ate$the$cake%
%he%ate$the%burger%
%michelle%ate$the$pizza%
%

%
%
word%pairs%that%appear%with%similar%pamerns%have%similar%seman<c%
rela<onships%(turney%et%al.,%2003)%
%
i,%he,%and%michelle%are%similar%
cake,%burger,%and%pizza%are%similar%

%

tuesday, december 1, 15

word%pair%b%path%

i%ate$the$cake,%he%ate$the%burger,%michelle%ate$the$pizza%
%

path%

(word%type,%word%type)%

%
%
word%pairs%that%appear%with%similar%pamerns%have%similar%seman<c%
rela<onships%(turney%et%al.,%2003)%
%
i,%he,%and%michelle%are%similar%
cake,%burger,%and%pizza%are%similar%

%

tuesday, december 1, 15

word%pair%b%path%

path%

(word%type,%word%type)%

%
pamerns%are%similar%if%they%have%similar%arguments.%

zuckerberg,%ceo%of%facebook,%zuckerberg,%head%of%facebook,%
zuckerberg,%head%honcho%at%facebook%%

tuesday, december 1, 15

