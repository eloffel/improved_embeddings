lecture 20

coreference and entity resolution

intro to nlp, cs585, fall 2015

brendan o   connor

friday, november 20, 15

1

    syntactic nlp news today --

new release of    universal dependencies    for 
multiple languages
http://universaldependencies.github.io/docs/

friday, november 20, 15

2

logistics

    two more homeworks
    tomorrow: hw4 out, on coref.  due in 2 weeks
    later: a short hw5

friday, november 20, 15

3

in-class quiz for grad-level nlp

october 9, 2013

do within-document coreference in the following document by assigning the mentions entity numbers:

[the government]    said [today]    [it]       s going to cut back on [[[the enormous number]   
of [people]   ]    who descended on [yemen]    to investigate [[the attack]    on [the     uss
cole]   ]   ]   .     [[[so many people]    from [several agencies]   ]   ]    wanting to par-
ticipate that [the yemenis]    are feeling somewhat overwhelmed in [[their]    own country]   .
[investigators]    have come up with [[another theory]    on how [the terrorists]    operated]   .
[[abc    s]    john miller]    on [[the house]    with [a view]   ]   . high on [[a hillside]   , in
[[a run - down section]    of [aden]   ]   ]   , [[the house]    with [the blue door]   ]    has
[[a perfect view]    of [the harbor]   ]   .
[american and yemeni investigators]    believe [that
view]    is what convinced [[a man]    who used [[the name]    [abdullah]   ]   ]    to rent
[the house]    [several weeks]    before [[the bombing]    of [the     uss cole]   ]   .     early
on [investigators]    theorized [it]    was [an inside job]    and [[much]    of [the focus]   ]   
was on [[employees]    of [[the mansoon shipping company]   , which was under [[contract]    by
[the navy]    to refuel [u.s. warships]   ]    and would have had [[advance information]    about
[[the     cole    s    ]    arrival]   ]   ]   ]   . now [the fbi]    believes [[all]    [the terrorists]   
needed to do]    was look out [the window]   , to go through [precisely the same drill]   , well before
[the     cole    ]    [arrived]   . [[the man]    in [this house]   ]    would have had [[plenty]    of
[[time]    to signal [[two bombers]    waiting with [the boat]    across [the bay]   ]   ]   ]   .
[investigators]    say [[clues]    collected over [the last few days]   ]    have already pointed
[them]    to [[locations]    both near and far outside [[the port city]    of [aden]   ]   ]   ,
but [they]    wo n   t say [there]       s [any indication that [[the plot]    here]    goes beyond
[[yemen    s]    boarders]   ]   . learning [[the true identities]    of [[those]    involved in [the
bombing]   ]   ]    would help answer [that question]   , but [the two suicide bombers]    died in

4

friday, november 20, 15

    1.  within-document coreference
    2.  cross-document coreference

friday, november 20, 15

5

kinds(of(reference(

       referring(expressions(

      john%smith%
      president%smith%
      the%president%
      the%company   s%new%executive%

       free(variables(

      smith(saw(his%pay%increase(

       bound(variables((

      the(dancer(hurt(herself.(

more(common(in(
newswire,(generally(
harder(in(practice(

more(interesting(
grammatical(
constraints,(
more(linguistic(
theory,(easier(in(
practice(

   anaphora(
resolution   (

friday, november 20, 15

    types of coref subtasks
    1.  pronoun resolution  (id2)
    2.  common nouns and names
    typical pipeline
    1.  identify candidate mentions
    2. cluster the candidate mentions

(ideally, referential mentions: exclude times, etc)

friday, november 20, 15

7

syntactic vs semantic cues

    state-of-the-art coref uses    rst two

friday, november 20, 15

syntactic vs semantic cues

    syntactic cues
    [john], a [lawyer], bought [himself] a book.
    [john], a [lawyer], bought [him] a book.

    state-of-the-art coref uses    rst two

friday, november 20, 15

syntactic vs semantic cues

    syntactic cues
    [john], a [lawyer], bought [himself] a book.
    [john], a [lawyer], bought [him] a book.
    shallow semantic cues
    john saw mary.  she was eating salad.
    john saw mary.  he was eating salad.

    state-of-the-art coref uses    rst two

friday, november 20, 15

syntactic vs semantic cues

    syntactic cues
    [john], a [lawyer], bought [himself] a book.
    [john], a [lawyer], bought [him] a book.
    shallow semantic cues
    john saw mary.  she was eating salad.
    john saw mary.  he was eating salad.
    deeper semantics (world knowledge)
    the city council denied the demonstrators a 
    the city council denied the demonstrators a 

permit because they feared violence.

permit because they advocated violence.

    state-of-the-art coref uses    rst two

friday, november 20, 15

mention pair model

hary potter was a wizard.  lord voldemort attempted to murder him.

mention pairs

    view gold standard as de   ning links between 
    think of as binary classi   cation problem: take 
    issues: many mention pairs.  also: have to resolve 

random pairs as negative examples

local decisions into entities

friday, november 20, 15

9

antecedent selection model

[null]
?

?

?

?

hary potter was a wizard.  lord voldemort attempted to murder him.
    view as antecedent selection problem: which previous mention 
do i corefer with?
    makes most sense for pronouns, though can use model for all 
    process mentions left to right. for the n   th mention,

expressions

n-way multi-class classi   cation problem:
antecedent is one of the n-1 mentions to the left, or null.
    features are asymmetric!
    use a limited window for antecedent candidates
    score each candidate by a linear function of features.

e.g. last 5 sentences (for news...)

predict antecedent to be the highest-ranking candidate.

friday, november 20, 15

10

antecedent selection model

?

?

?

hary potter was a wizard.  lord voldemort attempted to murder him.

[null]
?

    prediction: select the highest-scoring candidate as 
the antecedent.  (though multiple may be ok.)
    using for applications: take these links and form 

entity clusters from connected components  
[whiteboard] 

    training: simple way is to process the gold 
standard coref chains (entity clusters) into positive 
and negative links.  train binary classi   er.

friday, november 20, 15

11

features for pronoun resolution

friday, november 20, 15

12

features for pronoun resolution

    english pronouns grammar/semantic matching.  use as 

features against antecedent candidate properties.

friday, november 20, 15

12

features for pronoun resolution

    english pronouns grammar/semantic matching.  use as 
    number agreement

features against antecedent candidate properties.

    he/she/it vs. they/them
    match to: singular/plural nouns  (   person   ,    people   )

friday, november 20, 15

12

features for pronoun resolution

    english pronouns grammar/semantic matching.  use as 
    number agreement

features against antecedent candidate properties.

    animacy/human-ness agreement

    he/she/it vs. they/them
    match to: singular/plural nouns  (   person   ,    people   )
    it vs. he/she/him/her/his
    match to:  name-or-not vs.    person    vs.    car   

(need lexical semantic db: e.g. id138?)

friday, november 20, 15

12

features for pronoun resolution

    english pronouns grammar/semantic matching.  use as 
    number agreement

features against antecedent candidate properties.

    animacy/human-ness agreement

    he/she/it vs. they/them
    match to: singular/plural nouns  (   person   ,    people   )
    it vs. he/she/him/her/his
    match to:  name-or-not vs.    person    vs.    car   

(need lexical semantic db: e.g. id138?)

    gender agreement

    he/him/his vs. she/her vs. it  ---- match to: name gender?
    match to:   gender of names, common nouns

friday, november 20, 15

12

features for pronoun resolution

friday, november 20, 15

13

features for pronoun resolution

    grammatical person - interacts with dialogue/
discourse structure
       rst person:  i/me
    second person:  you/y   all
    third person:  he/she/it/they

friday, november 20, 15

13

features for pronoun resolution

    grammatical person - interacts with dialogue/
discourse structure
       rst person:  i/me
    second person:  you/y   all
    third person:  he/she/it/they
    re   exives: bind to close subject (usually 
forbidden)
    john knew that bob bought him a book.
    bob knew that john bought himself a book.

friday, november 20, 15

13

other syntactic constraints

    high-precision patterns
    predicate-nominatives:    x was a y       
    appositives:     x, a y,       
    role appositives:    [president] [lincoln]   

    maybe you   re happy with a high-precision, low-

recall system?

friday, november 20, 15

14

features for pronominal anaphora 
structural features for pronoun resolution

resolution 

       preferences:%

       recency:%more%recently%men2oned%en22es%are%more%

likely%to%be%referred%to%
       john%went%to%a%movie.%jack%went%as%well.%he%was%not%busy.%
       gramma2cal%role:%en22es%in%the%subject%posi2on%is%

more%likely%to%be%referred%to%than%en22es%in%the%object%
posi2on%
       john%went%to%a%movie%with%jack.%he%was%not%busy.%%

       parallelism:%%

       john%went%with%jack%to%a%movie.%joe%went%with%him%to%a%bar.%

%

friday, november 20, 15

features for pronominal anaphora 
structural features for pronoun resolution

resolution 

       preferences:%

       verb%seman2cs:%certain%verbs%seem%to%bias%whether%
the%subsequent%pronouns%should%be%referring%to%their%
subjects%or%objects%
       john%telephoned%bill.%he%lost%the%laptop.%
       john%cri2cized%bill.%he%lost%the%laptop.%

       %selec2onal%restric2ons:%restric2ons%because%of%

seman2cs%
       john%parked%his%car%in%the%garage%aber%driving%it%around%for%
hours.%%

       encode%all%these%and%maybe%more%as%features%

%

friday, november 20, 15

    how to combine information
    features in supervised ml -- 

easiest to do, if you have training data
[berkeley coref -- durrett and klein]

    rule-based approach.  [stanford dcoref, lee et al.]

typically, use a priority ordering:
    go through each high-precision rule. if it    res: take it. done.
    else:    lter out mentions based on semantic agreement and 
forbidden syntactic con   gurations.  choose syntactically 
closest mention.
    other multistage approaches  e.g. bamman et al   s book-nlp:

    1. cluster names based on string match / similarity
    2. resolve pronouns with antecedent model

friday, november 20, 15

17

features for non-pronoun resolution

    string match ... substring match ... id153

       abraham lincoln    ...    president lincoln   
       bill clinton    ...    hillary clinton    ...    clinton   
    special-case name parsing (   rstname vs surname)?
    i saw a green house.  the house was old.

    head string match

...    mr. clinton   

    many harder cases
       bill    ...    the boy   
       novartis    ...    the company   

friday, november 20, 15

18

within-doc coref performance

predicted clusters match gold-standard clusters?

    have to evaluate: how well do system   s 
    current systems get 70-80ish % accuracy 
depending on genre and how you view this

friday, november 20, 15

19

db/cross-doc coref

tasks

features

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

    discover the entities: like 

    cross-doc coref
within-doc coref.
(building your own entity db)
    id91 problem across all 

mentions in all docs!

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    name matching is really important

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

    discover the entities: like 

    cross-doc coref
within-doc coref.
(building your own entity db)
    id91 problem across all 

mentions in all docs!

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    name matching is really important
    fuzzy matching for

e.g. middle initials, multiple 
surnames (token level?)
e.g. id68s: qadda   , 
gadda   , el-qadda     (character 
level)

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

    discover the entities: like 

    cross-doc coref
within-doc coref.
(building your own entity db)
    id91 problem across all 

mentions in all docs!

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    name matching is really important
    fuzzy matching for

e.g. middle initials, multiple 
surnames (token level?)
e.g. id68s: qadda   , 
gadda   , el-qadda     (character 
level)

    jaro-winkler id153: 

especially customized for names 
(at least, names typical for the 
u.s. census)

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

    discover the entities: like 

    cross-doc coref
within-doc coref.
(building your own entity db)
    id91 problem across all 

mentions in all docs!

friday, november 20, 15

20

db/cross-doc coref

tasks

features

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

    discover the entities: like 

    cross-doc coref
within-doc coref.
(building your own entity db)
    id91 problem across all 

mentions in all docs!

friday, november 20, 15

20

    name matching is really important
    fuzzy matching for

e.g. middle initials, multiple 
surnames (token level?)
e.g. id68s: qadda   , 
gadda   , el-qadda     (character 
level)

    jaro-winkler id153: 

especially customized for names 
(at least, names typical for the 
u.s. census)

    tf-idf weighting

db/cross-doc coref

tasks

features

    record linkage

entities

    db1 of entities <=> db2 of 
    e.g. match voter records against 
facebook pro   les  (bond et al.)

    entity linking

    db entities  <=>  mentions in 

corpus

    discover the entities: like 

    cross-doc coref
within-doc coref.
(building your own entity db)
    id91 problem across all 

mentions in all docs!

    name matching is really important
    fuzzy matching for

e.g. middle initials, multiple 
surnames (token level?)
e.g. id68s: qadda   , 
gadda   , el-qadda     (character 
level)

    jaro-winkler id153: 

especially customized for names 
(at least, names typical for the 
u.s. census)

    tf-idf weighting
    context
e.g. bag-of-words near the mention

friday, november 20, 15

20

