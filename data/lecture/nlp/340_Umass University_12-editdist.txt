id153, id147,

and the noisy channel

cs 585, fall 2015

introduction to natural language processing

http://people.cs.umass.edu/~brenocon/inlp2015/

brendan o   connor

thursday, october 22, 15

    projects
    oh after class 
    some major topics in second half of course
    translation: spelling, machine translation
    syntactic parsing:  dependencies, hierarchical phrase 
structures
    coreference
    lexical semantics
    unsupervised language learning
    topic models, exploratory analysis?
    neural networks?

thursday, october 22, 15

2

bayes rule for doc classif.
noisy 
channel 
model

id136 problem

class

hypothesized transmission process

observed

text

previously:
naive bayes

thursday, october 22, 15

3

thursday, october 22, 15

4

id87

original

text

hypothesized transmission process

id136 problem

observed

text

codebreaking
p(plaintext | encrypted text) / p(encrypted text | plaintext) p(plaintext)

transmission:
enigma machine

id136:
bletchley park  (wwii)

thursday, october 22, 15

id87

original

text

hypothesized transmission process

id136 problem

observed

text

codebreaking
p(plaintext | encrypted text) / p(encrypted text | plaintext) p(plaintext)
id103
p(text | acoustic signal)

/ p(acoustic signal | text) p(text)

thursday, october 22, 15

id87

original

text

hypothesized transmission process

id136 problem

observed

text

codebreaking
p(plaintext | encrypted text) / p(encrypted text | plaintext) p(plaintext)
id103
p(text | acoustic signal)
id42
p(text | image)

/ p(acoustic signal | text) p(text)
/ p(image | text) p(text)

thursday, october 22, 15

id87

original

text

hypothesized transmission process

id136 problem

observed

text

codebreaking
p(plaintext | encrypted text) / p(encrypted text | plaintext) p(plaintext)
id103
p(text | acoustic signal)
id42
p(text | image)
machine translation
p(target text | source text) / p(source text | target text) p(target text)
id147
p(target text | source text) / p(source text | target text) p(target text)

/ p(acoustic signal | text) p(text)
/ p(image | text) p(text)

thursday, october 22, 15

id87

in this section we introduce the id87 and show how to apply it to
the task of detecting and correcting spelling errors. the id87 was
applied to the id147 task at about the same time by researchers at at&t
bell laboratories (kernighan et al. 1990, church and gale 1991) and ibm watson
research (mays et al., 1991).

noisy channel

original word

guessed word

word hyp1
word hyp2
...
word hyp3
 

decoder

noisy 1
noisy 2
noisy n

noisy word

figure 6.1
in the id87, we imagine that the surface form we see is actually
a    distorted    form of an original word passed through a noisy channel. the decoder passes
each hypothesis through a model of this channel and picks the word that best matches the
surface noisy word.

9

thursday, october 22, 15

noisy channel

the intuition of the id87 (see fig. 6.1) is to treat the misspelled

id147 as noisy channel

prior id203 of a hidden word is modeled by p(w). we can compute the most
probable word   w given that we   ve seen some observed misspelling x by multiply-
ing the prior p(w) and the likelihood p(x|w) and choosing the word for which this
product is greatest.
we apply the noisy channel approach to correcting non-word spelling errors by
taking any word not in our spell dictionary, generating a list of candidate words,
ranking them according to eq. 6.4, and picking the highest-ranked one. we can
modify eq. 6.4 to refer to this list of candidate words instead of the full vocabulary
v as follows:

i was too tired to go
i was to tired to go
i was zzz tired to go
...

input
i was ti tired to go

hypothetical model

id136 problem

  w = argmax

w2c

channel model

z }| {
p(x|w)

prior

z}|{p(w)

(6.5)

the noisy channel algorithm is shown in fig. 6.2.
to see the details of the computation of the likelihood and language model, let   s
walk through an example, applying the algorithm to the example misspelling acress.
the    rst stage of the algorithm proposes candidate corrections by    nding words that

language model

id153

1. how to score possible translations?
2. how to ef   ciently search over them?

thursday, october 22, 15

10

id153
    tasks
   
   

president barack obama
president barak obama

    calculate numerical similarity between pairs

    enumerate edits with distance=1
    model: assume possible changes.
    deletions:         actress => acress
    insertions:           cress => acress
    substitutions:     access => acress
    [transpositions:  caress => acress]

    probabilistic model: assume each has prob of occurrence

thursday, october 22, 15

11

dynamic
programming

recomputing all those paths, we could just remember the shortest path to a state
each time we saw it. we can do this by using id145. dynamic
programming is the name for a class of algorithms,    rst introduced by bellman
(1957), that apply a table-driven method to solve problems by combining solutions
to sub-problems. some of the most commonly used algorithms in natural language
processing make use of id145, such as the the viterbi and forward
algorithms (chapter 7) and the cky algorithm for parsing (chapter 12).

levenshtein algorithm
    goal: infer minimum id153, and argmin edit path,
the intuition of a id145 problem is that a large problem can
be solved by properly combining the solutions to various sub-problems. consider
the shortest path of transformed words that represents the minimum id153
    id157, text id172, id153
between the strings intention and execution shown in fig. 2.14.

for a pair of strings.  e.g.:  (intention, execution)

18 chapter 2

an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (this is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). using this version, the levenshtein distance between
intention and execution is 8.

i n t e n t i o n
n t e n t i o n
e t e n t i o n
e x e n t i o n
e x e n u t i o n
2.4.1 the minimum id153 algorithm
e x e c u t i o n
how do we    nd the minimum id153? we can think of this as a search task, in
figure 2.14 path from intention to execution.
which we are searching for the shortest path   a sequence of edits   from one string
to another.

delete i
substitute n by e
substitute t by x
insert u
substitute n by c

imagine some string (perhaps it is exention) that is in this optimal path (whatever
it is). the intuition of id145 is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intention to exention. why? if there were a shorter path from intention to exention,
then we could use it instead, resulting in a shorter overall path, and the optimal
i n x e n t i o n
sequence wouldn   t be optimal, thus leading to a contradiction.
the minimum id153 algorithm was named by wagner and fischer (1974)
figure 2.13 finding the id153 viewed as a search problem

i n t e c n t i o n

i n t e n t i o n

n t e n t i o n

subst

del

ins

minimum edit
distance

the space of all possible edits is enormous, so we can   t search naively. how-
ever, lots of distinct edit paths will end up in the same state (string), so rather than
recomputing all those paths, we could just remember the shortest path to a state
each time we saw it. we can do this by using id145. dynamic

12

thursday, october 22, 15

dynamic

[added after lecture]

    want to calculate:

minimum id153 between two strings x, y, lengths n,m

declaratively, id153 has a recursive substructure:
d(barak obama, barack obama) = d(barak obama, barack obam) + insertioncost

this allows for a id145 algorithm to quickly compute the lowest cost 
path -- speci   cally, the levenshtein algorithm.
(we   ll just do the version with ins/del/subst, no transpositions)

thursday, october 22, 15

13

levenshtein algorithm

substring of length j but an empty source going from 0 characters to j characters
and the    rst j characters of y . the id153 between x and y is thus d(n,m).
requires j inserts. having computed d(i, j) for small i, j we then compute larger
we   ll use id145 to compute d(n,m) bottom up, combining so-
d(i, j) based on previously computed smaller values. the value of d(i, j) is com-
lutions to subproblems. in the base case, with a source substring of length i but an
puted by taking the minimum of the three possible paths through the matrix which
empty target string, going from i characters to 0 requires i deletes. with a target
arrive there:
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. having computed d(i, j) for small i, j we then compute larger
d(i, j) based on previously computed smaller values. the value of d(i, j) is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:

    want to calculate:
    d(i,j): edit dist between x[1..i] and y[1..j].
if we assume the version of levenshtein distance in which the insertions and
deletions each have a cost of 1 (ins-cost(  ) = del-cost(  ) = 1), and substitutions have
a cost of 2 (except substitution of identical letters have zero cost), the computation
for d(i, j) becomes:

d[i  1, j] + del-cost(source[i])
d[i, j  1] + ins-cost(target[ j]))
d[i  1, j  1] + sub-cost(source[i],target[ j])

d[i  1, j] + del-cost(source[i])
d[i, j  1] + ins-cost(target[ j]))
d[i  1, j  1] + sub-cost(source[i],target[ j])

minimum id153 between two strings x, y, lengths n,m

d(n,m):  edit dist between x and y

if we assume the version of levenshtein distance in which the insertions and
deletions each have a cost of 1 (ins-cost(  ) = del-cost(  ) = 1), and substitutions have
ins,del=1
a cost of 2 (except substitution of identical letters have zero cost), the computation
sub=2
for d(i, j) becomes:

d[i  1, j] + 1
d[i, j  1] + 1
d[i  1, j  1] +    2;
d[i  1, j] + 1
to quickly calculate all d[i,j].
d[i, j  1] + 1
d[i  1, j  1] +    2;

    levenshtein algorithm:  id145 algorithm
the algorithm itself is summarized in fig. 2.15 and fig. 2.16 shows the results
of applying the algorithm to the distance between intention and execution with the
version of levenshtein in eq. 2.5.

knowing the minimum id153 is useful for algorithms like    nding poten-
tial spelling error corrections. but the id153 algorithm is important in another

if source[i] 6= target[ j]
if source[i] = target[ j]

if source[i] 6= target[ j]
if source[i] = target[ j]

0;

0;

14

d[i, j] = min8<:
d[i, j] = min8<:
d[i, j] = min8>><>>:
d[i, j] = min8>><>>:

thursday, october 22, 15

figure 2.15 the minimum id153 algorithm, an example of the class of dynamic
programming algorithms. the various costs can either be    xed (e.g., 8x,ins-cost(x) = 1)
or can be speci   c to the letter (to model the fact that some letters are more likely to be in-
serted than others). we assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x,x) = 0).

src\tar
#
i
n
t
e
n
t
i
o
n

#
0
1
2
3
4
5
6
7
8
9

e
1
2
3
4
3
4
5
6
7
8

x
2
3
4
5
4
5
6
7
8
9

e
3
4
5
6
5
6
7
8
9
10

c
4
5
6
7
6
7
8
9
10
11

u
5
6
7
8
7
8
9
10
11
12

t
6
7
8
7
8
9
8
9
10
11

i
7
6
7
8
9
10
9
8
9
10

o
8
7
8
9
10
11
10
9
8
9

n
9
8
7
8
9
10
11
10
9
8

figure 2.16 computation of minimum id153 between intention and execution with
the algorithm of fig. 2.15, using levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions. in italics are the initial values representing the distance from the
empty string.

we   ve shown a schematic of these backpointers in fig. 2.17, after a similar diagram
in gus   eld (1997). some cells have multiple backpointers because the minimum
extension could have come from multiple previous cells. in the second step, we

15

thursday, october 22, 15

backpointers

2.5

21

    summary
o
8

x
2

e
3

e
1

c
4

u
5

  6

#
i
n
t
e
n
t
i
o
n

  4 -  5

t
#
6
0
1 - " 2 - " 3 - " 4 - " 5 - " 6 - " 7
2 - " 3 - " 4 - " 5 - " 6 - " 7 - " 8
3 - " 4 - " 5 - " 6 - " 7 - " 8
- 7
4
5
6
7
8
9

n
i
9
7
  7   8
- 6
" 7 - " 8 - 7
" 8
 " 8 - " 9
" 9
  7  " 8 - " 9 - " 10
- 3
" 4 - " 5 - " 6 - " 7 - " 8 - " 9 - " 10 - " 11 -" 10
  10  " 11
" 5 - " 6 - " 7 - " 8 - " 9
" 6 - " 7 - " 8 - " 9 - " 10
  9   10
- 8   9
" 7 - " 8 - " 9 - " 10 - " 11
" 9 - 8
" 8 - " 9 - " 10 - " 11 - " 12
figure 2.17 when entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. after the table is full we compute an alignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
following the arrows back. the sequence of bold cells represents one possible minimum cost
alignment between the two strings.

- 8
" 9
" 10
" 11

  9
- 8
" 9
" 10

while we worked our example with simple levenshtein distance, the algorithm
in fig. 2.15 allows arbitrary weights on the operations. for id147, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. we   ll discuss how these weights can be estimated in

16

thursday, october 22, 15

et al. (2008). nltk is an essential tool that offers both useful python libraries
(http://www.nltk.org) and textbook descriptions (bird et al., 2009). of many
algorithms including text id172 and corpus interfaces.

id145

for more on herdan   s law and heaps    law, see herdan (1960, p. 28), heaps
(1978), egghe (2007) and baayen (2001); yasseri et al. (2012) discuss the relation-
ship with other measures of linguistic complexity. for more on id153, see the
excellent gus   eld (1997). our example measuring the id153 from    intention   
to    execution    was adapted from kruskal (1983). there are various publicly avail-
able packages to compute id153, including unix diff and the nist sclite
program (nist, 2005).

in his autobiography bellman (1984) explains how he originally came up with

the term id145:

i decided therefore to use the word,    programming   .

   ...the 1950s were not good years for mathematical research. [the]
secretary of defense ...had a pathological fear and hatred of the word,
research...
i
wanted to get across the idea that this was dynamic, this was multi-
stage... i thought, let   s ...
take a word that has an absolutely precise
meaning, namely dynamic... it   s impossible to use the word, dynamic,
in a pejorative sense. try thinking of some combination that will pos-
sibly give it a pejorative meaning.
it   s impossible. thus, i thought
id145 was a good name. it was something not even a
congressman could object to.   

exercises

2.1 write id157 for the following languages.

1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
17

thursday, october 22, 15

et al. (2008). nltk is an essential tool that offers both useful python libraries
(http://www.nltk.org) and textbook descriptions (bird et al., 2009). of many
algorithms including text id172 and corpus interfaces.

id145

for more on herdan   s law and heaps    law, see herdan (1960, p. 28), heaps
(1978), egghe (2007) and baayen (2001); yasseri et al. (2012) discuss the relation-
ship with other measures of linguistic complexity. for more on id153, see the
excellent gus   eld (1997). our example measuring the id153 from    intention   
to    execution    was adapted from kruskal (1983). there are various publicly avail-
able packages to compute id153, including unix diff and the nist sclite
program (nist, 2005).

in his autobiography bellman (1984) explains how he originally came up with

the term id145:

i decided therefore to use the word,    programming   .

   ...the 1950s were not good years for mathematical research. [the]
secretary of defense ...had a pathological fear and hatred of the word,
research...
i
wanted to get across the idea that this was dynamic, this was multi-
stage... i thought, let   s ...
take a word that has an absolutely precise
meaning, namely dynamic... it   s impossible to use the word, dynamic,
in a pejorative sense. try thinking of some combination that will pos-
sibly give it a pejorative meaning.
it   s impossible. thus, i thought
id145 was a good name. it was something not even a
congressman could object to.   

exercises

    levenshtein, viterbi are both examples

2.1 write id157 for the following languages.

1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
17

thursday, october 22, 15

del[x,y]: count(xy typed as x)
ins[x,y]: count(x typed as xy)
sub[x,y]: count(x typed as y)
trans[x,y]: count(xy typed as yx)

channel model
    norvig reading: just make up edit parameters:
    to estimate from data:  need access to a corpus of mistakes

note that we   ve conditioned the insertion and deletion probabilities on the previ-
deletions, edits, etc. all have logprob = -1
ous character; we could instead have chosen to condition on the following character.
where do we get these confusion matrices? one way is to extract them from

lists of misspellings like the following:

6 chapter 6

additional: addional, additonal
environments: enviornments, enviorments, enviroments
preceded: preceeded
...

    id147 and the noisy channel

, if deletion

del[xi 1,wi]
count[xi 1wi]
ins[xi 1,wi]
count[wi 1]
sub[xi,wi]
count[wi]
trans[wi,wi+1]
count[wiwi+1]

there are lists available on wikipedia and from roger mitton (http://www.
dcs.bbk.ac.uk/  roger/corpora.html) peter norvig (http://norvig.com/
ngrams/). norvig also gives the counts for each single-character edits that can be
used to directly create the error model probabilities.

p(x|w) =

, if insertion

, if substitution

an alternative approach used by kernighan et al. (1990) is to compute the ma-
trices by iteratively using this very spelling error correction algorithm itself. the
iterative algorithm    rst initializes the matrices with equal values; thus, any character
is equally likely to be deleted, equally likely to be substituted for any other char-
acter, etc. next, the spelling error correction algorithm is run on a set of spelling
errors. given the set of typos paired with their predicted corrections, the confusion
matrices can now be recomputed, the spelling algorithm run again, and so on. this

bilities for acress shown in fig. 6.4.

, if transposition

using the counts from kernighan et al. (1990) results in the error model proba-

candidate

correct

error

18

(6.6)

8>>>>>>>>>>>><>>>>>>>>>>>>:

thursday, october 22, 15

8>>>>>>>>>>>><>>>>>>>>>>>>:

p(x|w) =

ins[xi 1,wi]
count[wi 1]
sub[xi,wi]
count[wi]
trans[wi,wi+1]
  w = argmax
count[wiwi+1]

w2c

, if substitution

, if insertion
channel model
, if transposition

z }| {
p(x|w)

id153

(6.6)

prior

z}|{p(w)

modify eq. 6.4 to refer to this list of candidate words instead of the full vocabulary
v as follows:

using the counts from kernighan et al. (1990) results in the error model proba-

language model

bilities for acress shown in fig. 6.4.

the noisy channel algorithm is shown in fig. 6.2.
to see the details of the computation of the likelihood and language model, let   s
walk through an example, applying the algorithm to the example misspelling acress.
the    rst stage of the algorithm proposes candidate corrections by    nding words that

x|w

candidate
correction
actress
cress
caress
access
across
acres
acres

correct
letter
t
-
ca
c
o
-
-

error
letter
-
a
ac
r
e
s
s

c|ct
a|#
ac|ca
r|c
e|o
es|e
ss|s

p(x|w)
.000117
.00000144
.00000164
.000000209
.0000093
.0000321
.0000342

(6.5)

figure 6.4 channel model for acress; the probabilities are taken from the del[], ins[],
sub[], and trans[] confusion matrices as shown in kernighan et al. (1990).

figure 6.5 shows the    nal probabilities for each of the potential corrections;
the unigram prior is multiplied by the likelihood (computed with eq. 6.6 and the
confusion matrices). the    nal column shows the product, multiplied by 109 just for
readability.

candidate correct error
correction letter letter x|w

t
-
ca
c
o
-
thursday, october 22, 15
-

actress
cress
caress
access
across
acres
acres

109*p(x|w)p(w)
2.7

p(w)
p(x|w)
.0000231
c|ct .000117
.000000544 0.00078
.00000144
a|#
0.0028
.00000170
ac|ca .00000164
.000000209 .0000916
0.019
r|c
.000299
.0000093
2.8
e|o
19
1.0
.0000318
es|e .0000321
ss|s .0000342
.0000318
1.0

-
a
ac
r
e
s
s

8>>>>>>>>>>>><>>>>>>>>>>>>:

modify eq. 6.4 to refer to this list of candidate words instead of the full vocabulary
v as follows:

figure 6.3 candidate corrections for the misspelling acress and the transformations that
would have produced the error (after kernighan et al. (1990)).           represents a null letter.

p(x|w) =

ins[xi 1,wi]
count[wi 1]
sub[xi,wi]
count[wi]
trans[wi,wi+1]
  w = argmax
count[wiwi+1]

w2c

, if substitution

, if insertion
channel model
, if transposition

z }| {
p(x|w)

id153

(6.6)

prior

z}|{p(w)

using the counts from kernighan et al. (1990) results in the error model proba-

bilities for acress shown in fig. 6.4.

once we have a set of a candidates, to score each one using eq. 6.5 requires that

we compute the prior and the channel model.

the prior id203 of each correction p(w) is the language model id203
of the word w in context. we can use any language model from the previous chapter,
from unigram to trigram or 4-gram. for this example let   s start in the following table
by assuming a unigram language model. we computed the language model from the
404,253,213 words in the corpus of contemporary english (coca).

language model

(6.5)

the noisy channel algorithm is shown in fig. 6.2.
to see the details of the computation of the likelihood and language model, let   s
walk through an example, applying the algorithm to the example misspelling acress.
the    rst stage of the algorithm proposes candidate corrections by    nding words that

unigram lm:
count(w) p(w)

x|w

p(x|w)
.000117
.00000144
.00000164
.000000209
.0000093
.0000321
.0000342
how can we estimate the likelihood p(x|w), also called the channel model or

w
actress 9,321
220
cress
caress 686
access 37,038
across 120,844
acres
12,874

.0000231
.000000544
.00000170
.0000916
.000299
.0000318

figure 6.4 channel model for acress; the probabilities are taken from the del[], ins[],
sub[], and trans[] confusion matrices as shown in kernighan et al. (1990).

channel model

candidate
correction
actress
cress
caress
access
across
acres
acres

correct
letter
t
-
ca
c
o
-
-

error
letter
-
a
ac
r
e
s
s

c|ct
a|#
ac|ca
r|c
e|o
es|e
ss|s

figure 6.5 shows the    nal probabilities for each of the potential corrections;
the unigram prior is multiplied by the likelihood (computed with eq. 6.6 and the
confusion matrices). the    nal column shows the product, multiplied by 109 just for
readability.

candidate correct error
correction letter letter x|w

t
-
ca
c
o
-
thursday, october 22, 15
-

actress
cress
caress
access
across
acres
acres

109*p(x|w)p(w)
2.7

p(w)
p(x|w)
.0000231
c|ct .000117
.000000544 0.00078
.00000144
a|#
0.0028
.00000170
ac|ca .00000164
.000000209 .0000916
0.019
r|c
.000299
.0000093
2.8
e|o
19
1.0
.0000318
es|e .0000321
ss|s .0000342
.0000318
1.0

-
a
ac
r
e
s
s

8>>>>>>>>>>>><>>>>>>>>>>>>:
z }| {
p(x|w)

8>>>>>>>>>>>><>>>>>>>>>>>>:

(6.6)

p(x|w) =

, if substitution

we compute the prior and the channel model.

using the counts from kernighan et al. (1990) results in the error model proba-

once we have a set of a candidates, to score each one using eq. 6.5 requires that

figure 6.3 candidate corrections for the misspelling acress and the transformations that
would have produced the error (after kernighan et al. (1990)).           represents a null letter.

modify eq. 6.4 to refer to this list of candidate words instead of the full vocabulary
v as follows:

the noisy channel algorithm is shown in fig. 6.2.
to see the details of the computation of the likelihood and language model, let   s
walk through an example, applying the algorithm to the example misspelling acress.
the    rst stage of the algorithm proposes candidate corrections by    nding words that

the prior id203 of each correction p(w) is the language model id203
of the word w in context. we can use any language model from the previous chapter,
from unigram to trigram or 4-gram. for this example let   s start in the following table
by assuming a unigram language model. we computed the language model from the
404,253,213 words in the corpus of contemporary english (coca).

ins[xi 1,wi]
using the counts from kernighan et al. (1990) results in the error model proba-
count[wi 1]
bilities for acress shown in fig. 6.4.
sub[xi,wi]
count[wi]
candidate
trans[wi,wi+1]
correction
  w = argmax
count[wiwi+1]
actress
w2c
cress
caress
access
error
across
letter
acres
-
acres
a
ac
r
e
s
s

, if insertion
channel model
error
correct
letter
letter
, if transposition
-
t
-
a
ac
ca
c
r
e
o
x|w
p(x|w)
-
s
.000117
c|ct
s
-
.00000144
a|#
.00000164
ac|ca
.000000209
r|c
.0000093
e|o
.0000321
es|e
.0000342
ss|s
how can we estimate the likelihood p(x|w), also called the channel model or

p(x|w)
.000117
c|ct
.00000144
a|#
.00000164
ac|ca
.000000209
r|c
unigram lm:
.0000093
e|o
count(w) p(w)
w
.0000321
es|e
actress 9,321
.0000342
ss|s
220
cress
caress 686
access 37,038
across 120,844
acres
12,874

figure 6.5 shows the    nal probabilities for each of the potential corrections;
the unigram prior is multiplied by the likelihood (computed with eq. 6.6 and the
confusion matrices). the    nal column shows the product, multiplied by 109 just for
readability.

figure 6.4 channel model for acress; the probabilities are taken from the del[], ins[],
sub[], and trans[] confusion matrices as shown in kernighan et al. (1990).

figure 6.4 channel model for acress; the probabilities are taken from the del[], ins[],
sub[], and trans[] confusion matrices as shown in kernighan et al. (1990).

candidate
correction
actress
cress
caress
access
across
acres
acres

.0000231
.000000544
.00000170
.0000916
.000299
.0000318

correct
letter
t
-
ca
c
o
-
-

z}|{p(w)

prior
x|w

bilities for acress shown in fig. 6.4.

language model

id153

(6.5)

channel model

=>

candidate correct error
correction letter letter x|w

candidate correct error
figure 6.5 shows the    nal probabilities for each of the potential corrections;
p(w)
p(x|w)
correction letter letter x|w
the unigram prior is multiplied by the likelihood (computed with eq. 6.6 and the
confusion matrices). the    nal column shows the product, multiplied by 109 just for
.0000231
c|ct .000117
-
actress
t
readability.
.000000544 0.00078
.00000144
a
a|#
cress
-
0.0028
.00000170
ac|ca .00000164
ca
caress
ac
.000000209 .0000916
0.019
r
r|c
c
access
109*p(x|w)p(w)
p(w)
p(x|w)
2.8
.000299
.0000093
e|o
e
o
across
c|ct .000117
.0000231
2.7
es|e .0000321
.0000318
1.0
-
acres
s
.000000544 0.00078
.00000144
a|#
1.0
.0000318
ss|s .0000342
s
-
acres
ac|ca .00000164
0.0028
.00000170
figure 6.5 computation of the ranking for each candidate correction, using the language
.000000209 .0000916
0.019
r|c
model shown earlier and the error model from fig. 6.4. the    nal score is multiplied by 109
.000299
.0000093
2.8
e|o
19
for readability.
1.0
.0000318
es|e .0000321
ss|s .0000342
.0000318
1.0

t
-
ca
c
o
-
thursday, october 22, 15
-

actress
cress
caress
access
across
acres
acres

unnorm.
posterior:

109*p(x|w)p(w)
2.7

-
a
ac
r
e
s
s

6.2

    real-word spelling errors

id165 language models
    was called a    stellar and versatile acress whose combination of 
    real-word spelling errors
    more context helps!  assume a bigram markov model

for this reason, it is important to use larger language models than unigrams.
for example, if we use the corpus of contemporary american english to compute
sass and glamour has de   ned her
bigram probabilities for the words actress and across in their context using add-one
smoothing, we get the following probabilities:
for this reason, it is important to use larger language models than unigrams.
for example, if we use the corpus of contemporary american english to compute
bigram probabilities for the words actress and across in their context using add-one
smoothing, we get the following probabilities:

6.2

7

7

p(actress|versatile) = .000021
p(acress | versatile _ whose) = ?
p(across|versatile) = .000021
p(whose|actress) = .0010
p(actress|versatile) = .000021
p(whose|across) = .000006
p(across|versatile) = .000021
p(whose|actress) = .0010
p(whose|across) = .000006

dates in context:

multiplying these out gives us the language model estimate for the two candi-

dates in context:

multiplying these out gives us the language model estimate for the two candi-

p(   versatile actress whose   ) = .000021    .0010 = 210    10 10
p(   versatile across whose   ) = .000021    .000006 = 1    10 10
p(   versatile actress whose   ) = .000021    .0010 = 210    10 10
combining the language model with the error model in fig. 6.5, the bigram noisy
p(   versatile across whose   ) = .000021    .000006 = 1    10 10

channel model now chooses the correct word actress.
thursday, october 22, 15

20

the study was conducted mainly be john black.

    many misspellings are legitimate english words.  language model is key.

the noisy channel can deal with real-word errors as well. let   s begin with a
version of the id87    rst proposed by mays et al. (1991) to deal
with these real-word spelling errors. their algorithm takes the input sentence x =
{x1,x2, . . . ,xk, . . . ,xn}, generates a large set of candidate correction sentences c(x),
then picks the sentence with the highest language model id203.
to generate the candidate correction sentences, we start by generating a set of
candidate words for each input word xi. the candidates, c(xi). includes every en-
glish word with a small id153 from xi   (mays et al., 1991) chose id153
one. so c(graffe) = {giraffe,graff,gaffe}. we then make the simplifying assumption
that every sentence has only one error. thus the set of candidate sentences c(x) for
a sentence x = only two of thew apples would be:

    id147 and the noisy channel

even assuming only 1 error per sentence:

8 chapter 6

only two of thew apples
oily two of thew apples
only too of thew apples
only to of thew apples
only tao of the apples
only two on thew apples
only two off thew apples
only two of the apples
only two of threw apples
only two of thew applies
only two of thew dapples
...

each sentence is scored by the noisy channel:

thursday, october 22, 15

  w = argmax
w2c(w )

p(x|w )p(w )

21

(6.7)

for p(w ), we can use the trigram id203 of the sentence.

