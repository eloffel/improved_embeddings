cs224d

deep	learning	

for	natural	language	processing

lecture	3:	

more	word	vectors

richard	socher

refresher:	the	simple	id97	model
    main	cost	function	j:

    with	probabilities	defined	as:

    we	derived	the	gradient	for	the	internal	vectors	vc

lecture	1,	slide	2

richard	socher

4/5/16

calculating	all	gradients!
    we	went	through	gradients	for	each	center	vector	v	in	a	window
    we	also	need	gradients	for	outside	vectors	u	
    derive	at	home!	

    generally	in	each	window	we	will	compute	updates	for	all	

parameters	that	are	being	used	in	that	window.

    for	example	window	size	c	=	1,	sentence:	

   i	like	learning	.   
    first	window	computes	gradients	for:	

    internal	vector	vlike and	external	vectors	ui and	ulearning

    next	window	in	that	sentence?

lecture	1,	slide	3

richard	socher

4/5/16

compute	all	vector	gradients!
    we	often	define	the	set	of	all	parameters	in	a	model	in	terms	

   

of	one	long	vector	
in	our	case	with	
d-dimensional	vectors
and
v	many	words:

lecture	1,	slide	4

richard	socher

4/5/16

gradient	descent
    to	minimize														over	the	full	batch	(the	entire	training	data)	

would	require	us	to	compute	gradients	for	all	windows

    updates	would	be	for	each	element	of	   :

    with	step	size	  
   

in	matrix	notation	for	all	parameters:

lecture	1,	slide	5

richard	socher

4/5/16

vanilla	gradient	descent	code

lecture	1,	slide	6

richard	socher

4/5/16

intuition
    for	a	simple	convex	function	over	two	parameters.

    contour	lines	show	levels	of	objective	function
   

lecture	1,	slide	7

richard	socher

4/5/16

stochastic	gradient	descent
    but	corpus	may	have	40b	tokens	and	windows
    you	would	wait	a	very	long	time	before	making	a	single	update!

    very	bad	idea	for	pretty	much	all	neural	nets!
   

instead:	we	will	update	parameters	after	each	window	t	
   stochastic	gradient	descent	(sgd)

lecture	1,	slide	8

richard	socher

4/5/16

stochastic	gradients	with	word	vectors!
    but	in	each	window,	we	only	have	at	most	2c	-1	words,	

so																		is	very	sparse!

lecture	1,	slide	9

richard	socher

4/5/16

stochastic	gradients	with	word	vectors!
    we	may	as	well	only	update	the	word	vectors	that	actually	

appear!

    solution:	either	keep	around	hash	for	word	vectors	or	only	
update	certain	columns	of	full	embedding	matrix	u and	v

|v|

[												]

d

   

important	if	you	have	millions	of	word	vectors	and	do	
distributed	computing	to	not	have	to	send	gigantic	updates	
around.

lecture	1,	slide	10

richard	socher

4/5/16

approximations:	pset 1
    the	id172	factor	is	too	computationally	expensive

    hence,	in	pset1	you	will	implement	the	skip-gram	model	

    main	idea:	train	binary	logistic	regressions	for	a	true	pair	(center	
word	and	word	in	its	context	window)	and	a	couple	of	random	
pairs	(the	center	word	with	a	random	word)

lecture	1,	slide	11

richard	socher

4/5/16

pset 1:	the	skip-gram	model	and	negative	sampling
    from	paper:	   distributed	representations	of	words	and	phrases	

and	their	compositionality   	(mikolovet	al.	2013)

    overall	objective	function:	

    where	k	is	the	number	of	negative	samples	and	we	use,

    the	sigmoid	function!	

(we   ll	become	good	friends	soon)
    so	we	maximize	the	id203	

of	two	words	co-occurring	in	first	log
  

lecture	1,	slide	12

richard	socher

4/5/16

pset 1:	the	skip-gram	model	and	negative	sampling
    slightly	clearer	notation:

    max.	id203	that	real	outside	word	appears,	

minimize	prob.	that	random	words	appear	around	center	word

    p(w)=u(w)3/4/z,

the	unigram	distribution	u(w)	raised	to	the	3/4rd	power
(we	provide	this	function	in	the	starter	code).	

    the	power	makes	less	frequent	words	be	sampled	more	often

lecture	1,	slide	13

richard	socher

4/5/16

pset 1:	the	continuous	bag	of	words	model
    main	idea	for	continuous	bag	of	words	(cbow):	predict	center	

word	from	sum	of	surrounding	word	vectors	instead	of	
predicting	surrounding	single	words	from	center	word	as	in	skip-
gram	model

    to	make	pset slightly	easier:

the	implementation	for	the	cbow	model	is	not	required	and	for	
bonus	points!

lecture	1,	slide	14

richard	socher

4/5/16

count	based	vs direct	prediction

lsa, hal (lund & burgess), 
coals (rohde et al), 
hellinger-pca  (lebret & collobert)

    fast training
    efficient usage of statistics

    primarily used to capture word 
similarity
    disproportionate importance 
given to large counts

    nnlm, hlbl, id56, skip-
gram/cbow, (bengio et al; collobert
& weston; huang et al; mnih & hinton; 
mikolov et al; mnih & kavukcuoglu)

    scales with corpus size
    inefficient usage of statistics
    generate improved performance 
on other tasks

    can capture complex patterns 
beyond word similarity 

15

richard	socher

4/5/16

combining	the	best	of	both	worlds:	glove

   fast	training
   scalable	to	huge	corpora
   good	performance	even	with	small	corpus,	and	small	
vectors

16

richard	socher

4/5/16

glove	results

nearest	words	to
frog:

1.	frogs
2.	toad
3.	litoria
4.	leptodactylidae
5.	rana
6.	lizard
7.	eleutherodactylus

litoria

leptodactylidae

17

rana

richard	socher

eleutherodactylus
4/5/16

what	to	do	with	the	two	sets	of	vectors?
    we	end	up	with	u	and	v	from	all	the	vectors	u	and	v	(in	

columns)

    both	capture	similar	co-occurrence	information.	it	turns	out,	the	

best	solution	is	to	simply	sum	them	up:

xfinal =	u	+	v

    one	of	many	hyperparameters explored	in	glove:	global	
vectors	for	word	representation	 (pennington	et	al.	(2014)

lecture	1,	slide	18

richard	socher

4/5/16

how	to	evaluate	word	vectors?
    related	to	general	evaluation	in	nlp:	intrinsic	vs extrinsic
   

intrinsic:
    evaluation	on	a	specific/intermediate	subtask
    fast	to	compute
    helps	to	understand	that	system
    not	clear	if	really	helpful	unless	correlation	to	real	task	is	established
extrinsic:
    evaluation	on	a	real	task
    can	take	a	long	time	to	compute	accuracy
    unclear	if	the	subsystem	is	the	problem	or	its	interaction	or	other	

   

subsystems

    if	replacing	one	subsystem	with	another	improves	accuracy	   winning!

lecture	1,	slide	19

richard	socher

4/5/16

intrinsic	word	vector	evaluation
    word	vector	analogies

a:b	::	c:?

man:woman::	king:?

   

evaluate	word	vectors	by	how	well	
their	cosine	distance	after	addition	
captures	intuitive	semantic	and	
syntactic	analogy	questions

    discarding	the	input	words	from	the	

search!

    problem:	what	if	the	information	is	

there	but	not	linear?

king

woman

man

lecture	1,	slide	20

richard	socher

4/5/16

glove	visualizations

21

richard	socher

4/5/16

glove	visualizations:	company	- ceo

22

richard	socher

4/5/16

glove	visualizations:	superlatives

23

richard	socher

4/5/16

details	of	intrinsic	word	vector	evaluation
    word	vector	analogies:	syntactic	and	semantic examples	from	

http://code.google.com/p/id97/source/browse/trunk/questions-
words.txt

problem:	different	cities	
may	have	same	name

:	city-in-state
chicago	illinois	houston	texas
chicago	illinois	philadelphia	pennsylvania
chicago	illinois	phoenix	arizona
chicago	illinois	dallas	texas
chicago	illinois	jacksonville	florida
chicago	illinois	indianapolis	indiana
chicago	illinois	austin	texas
chicago	illinois	detroit	michigan
chicago	illinois	memphis	tennessee
chicago	illinois	boston	massachusetts

lecture	1,	slide	24

richard	socher

4/5/16

details	of	intrinsic	word	vector	evaluation
    word	vector	analogies:	syntactic	and	semantic examples	from

problem:	can	change

:	capital-world
abuja	nigeria	accra	ghana
abuja	nigeria	algiers	algeria
abuja	nigeria	amman	jordan
abuja	nigeria	ankara	turkey
abuja	nigeria	antananarivo	madagascar
abuja	nigeria	apia	samoa
abuja	nigeria	ashgabat	turkmenistan
abuja	nigeria	asmara	eritrea
abuja	nigeria	astana	kazakhstan

lecture	1,	slide	25

richard	socher

4/5/16

details	of	intrinsic	word	vector	evaluation
    word	vector	analogies:	syntactic and	semantic	examples	from

:	gram4-superlative
bad	worst	big	biggest
bad	worst	bright	brightest
bad	worst	cold	coldest
bad	worst	cool	coolest
bad	worst	dark	darkest
bad	worst	easy	easiest
bad	worst	fast	fastest
bad	worst	good	best
bad	worst	great	greatest

lecture	1,	slide	26

richard	socher

4/5/16

details	of	intrinsic	word	vector	evaluation
    word	vector	analogies:	syntactic and	semantic	examples	from

:	gram7-past-tense
dancing	danced	decreasing	decreased
dancing	danced	describing	described
dancing	danced	enhancing	enhanced
dancing	danced	falling	fell
dancing	danced	feeding	fed
dancing	danced	flying	flew
dancing	danced	generating	generated
dancing	danced	going	went
dancing	danced	hiding	hid
dancing	danced	hitting	hit

lecture	1,	slide	27

richard	socher

4/5/16

portional to the sum over all elements of the co-
occurrence matrix x,

xi j =

analogy	evaluation	and	hyperparameters
| x|xr =1
    very	careful	analysis:	glove	word	vectors	

k
r     = kh| x|,    ,

(18)

sg

model
ivlbl
hpca
glove

table 2: results on the word analogy task, given
as percent accuracy. underlined scores are best
within groups of similarly-sized models; bold
scores are best overall. hpca vectors are publicly
available2; (i)vlbl results are from (mnih et al.,
2013); skip-gram (sg) and cbow results are
from (mikolov et al., 2013a,b); we trained sg   
and cbow    using the id97 tool3. see text
for details and a description of the svd models.
sem. syn. tot.
53.2
10.8
60.3
61
36.1
60.0
64.0
70.3
7.3
42.1
60.1
65.7
69.1
71.7
63.7
65.6
49.2
75.0

dim.
100
100
100
300
cbow 300
vlbl
300
300
ivlbl
300
glove
svd
300
300
svd-s
300
svd-l
cbow   
300
sg   
300
glove
300
cbow 1000
1000
300
300

size
1.5b 55.9
1.6b
4.2
1.6b 67.5
1b
61
1.6b 16.1
1.5b 54.2
1.5b 65.2
1.6b 80.8
6b
6.3
36.7
6b
56.6
6b
6b
63.6
73.0
6b
77.4
6b
6b
57.3
66.1
6b
38.4
42b
81.9
42b

50.1
16.4
54.3
61
52.6
64.8
63.0
61.5
8.1
46.6
63.0
67.4
66.0
67.0
68.9
65.1
58.2
69.3

svd-l
glove

sg

dataset for ner (tjong kim sang and de meul-
der, 2003).

richard	socher

word analogies. the word analogy task con-

4/5/16

where we have rewritten the last sum in terms of
the generalized harmonic number hn, m. the up-
per limit of the sum,
|x|, is the maximum fre-
quency rank, which coincides with the number of
nonzero elements in the matrix x. this number is
also equal to the maximum value of r in eqn. (17)
such that xi j   1, i.e., |x| = k1/   . therefore we
can write eqn. (18) as,

|c|     |x|    h| x|,    .

(19)
we are interested in how |x| is related to |c| when
both numbers are large; therefore we are free to
expand the right hand side of the equation for large
|x|. for this purpose we use the expansion of gen-
eralized harmonic numbers (apostol, 1976),

+     (s) + o(x s)

if s > 0, s , 1 ,
(20)

|c|     |x|
1      

+     (   ) |x|    + o(1) ,
lecture	1,	slide	28

where     (s) is the riemann zeta function. in the
limit that x is large, only one of the two terms on
the right hand side of eqn. (21) will be relevant,

(21)

analogy	evaluation	and	hyperparameters
    asymmetric	context	(only	words	to	the	left)	are	not	as	good

80

70

60

50

40

30

]

%

[
 
y
c
a
r
u
c
c
a

 
20
0

100

]

%

[
 
y
c
a
r
u
c
c
a

70

65

60

55

50

45

 
 
40
2

semantic
syntactic
overall

500

600

200

300

vector dimension

400

(a) symmetric context

 

70

65

60

55

50

45

]

%

[
 
y
c
a
r
u
c
c
a

10

 
 
40
2

 

semantic
syntactic
overall

4

6

window size

8

10

(c) asymmetric context

semantic
syntactic
overall

4

6

window size

8

(b) symmetric context

figure 2: accuracy on the analogy task as function of vector size and window size/type. all models are
    best	dimensions	~300,	slight	drop-off	afterwards	
trained on the 6 billion token corpus. in (a), the window size is 10. in (b) and (c), the vector size is 100.
    but	this	might	be	different	for	downstream	tasks!
word similarity. while the analogy task is our
primary focus since it tests for interesting vector
    window	size	of	8	around	each	center	word	is	good	for	glove	vectors
space substructures, we also evaluate our model on
a variety of word similarity tasks in table 3. these
include wordsim-353 (finkelstein et al., 2001),
mc (miller and charles, 1991), rg (rubenstein
lecture	1,	slide	29
and goodenough, 1965), scws (huang et al.,
2012), and rw (luong et al., 2013).

has 6 billion tokens; and on 42 billion tokens of
web data, from common crawl5. we tokenize
and lowercase each corpus with the stanford to-
kenizer, build a vocabulary of the 400,000 most
frequent words6, and then construct a matrix of co-
occurrence counts x. in constructing x, we must
choose how large the context window should be
and whether to distinguish left context from right

richard	socher

4/5/16

analogy	evaluation	and	hyperparameters
    more	training	time	helps

5

6

glove
cbow

20

25

negative samples (cbow)
(a) glove vs cbow

40

50

lecture	1,	slide	30

]

%

[
 
y
c
a
r
u
c
c
a

72

70

68

66

64

62

60

3

6

training time (hrs)
15

12

9

18

21

24

glove
skip-gram

20

40

60

80

100

iterations (glove)
10 12

1 2 3 4 5 6

15
negative samples (skip-gram)

7

20

(b) glove vs skip-gram

richard	socher

4/5/16

figure 4: overall accuracy on the word analogy task as a function of training time, which is governed by

analogy	evaluation	and	hyperparameters
    more	data	helps,	wikipedia	is	better	than	news	text!

table 4: f1 score on ner task with 50d vectors.
discrete is the baseline without word vectors. we
use publicly-available vectors for hpca, hsmn,

]

%

[
 
y
c
a
r
u
c
c
a

85

80

75

70

65

60

55

50

semantic

syntactic

overall

wiki2010
1b tokens

wiki2014
1.6b tokens

gigaword5
4.3b tokens

gigaword5 + 

wiki2014
6b tokens

common crawl 

42b tokens

figure 3: accuracy on the analogy task for 300-
dimensional vectors trained on different corpora.
4/5/16

richard	socher

lecture	1,	slide	31

intrinsic	word	vector	evaluation
    word	vector	distances	and	their	correlation	with	human	judgments
   

example	dataset:	wordsim353	
http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/

cat
tiger
paper

7.35
10.00
7.46
internet 7.58
5.77
doctor 6.62

word	1 word	2 human	(mean)
tiger
tiger
book
computer
plane
professor
stock
stock
stock

phone 1.62
cd
1.31
jaguar 0.92

car

lecture	1,	slide	32

richard	socher

4/5/16

the sum w +   w as our word vectors. doing so typ-
ically gives a small boost in performance, with the
biggest increase in the semantic analogy task.

we compare with the published results of a va-
riety of state-of-the-art models, as well as with
our own results produced using the id97
tool and with several baselines using svds. with
id97, we train the skip-gram (sg   ) and
continuous bag-of-words (cbow   ) models on the
6 billion token corpus (wikipedia 2014 + giga-
word 5) with a vocabulary of the top 400,000 most
frequent words and a context window size of 10.
we used 10 negative samples, which we show in
section 4.6 to be a good choice for this corpus.

for the svd baselines, we generate a truncated
matrix xtrunc which retains the information of how
frequently each word occurs with only the top
10,000 most frequent words. this step is typi-
cal of many matrix-factorization-based methods as
the extra columns can contribute a disproportion-
ate number of zero entries and the methods are
lecture	1,	slide	33
otherwise computationally expensive.

   

correlation	evaluation
    word	vector	distances	and	their	correlation	with	human	judgments

table 3: spearman rank correlation on word simi-
larity tasks. all vectors are 300-dimensional. the
cbow    vectors are from the id97 website
and differ in that they contain phrase vectors.

model size ws353 mc rg scws rw
35.3
6b
svd
25.6
56.5
34.7
svd-s
6b
65.7
svd-l 6b
37.0
cbow    6b
57.2
32.5
sg   
62.8
6b
37.2
65.8
38.1
glove
6b
74.0
svd-l 42b
39.9
47.8
75.9
glove
42b
cbow    100b 68.4
45.5

35.1 42.5
71.5 71.0
72.7 75.1
65.6 68.2
65.2 69.7
72.7 77.8
76.4 74.1
83.6 82.9
79.6 75.4

38.3
53.6
56.5
57.0
58.1
53.9
58.3
59.6
59.4

some	ideas	from	glove	paper	have	been	shown	to	improve	skip-gram	(sg)	
model	also	(e.g.	sum	both	vectors)

l model on this larger corpus. the fact that this
basic svd model does not scale well to large cor-
pora lends further evidence to the necessity of the
type of weighting scheme proposed in our model.
table 3 shows results on    ve different word
similarity datasets. a similarity score is obtained
from the word vectors by    rst normalizing each

richard	socher

4/5/16

but	what	about	ambiguity?	
    you	may	hope	that	one	vector	captures	both	kinds	of	

information	(run	=	verb	and	noun)	but	then	vector	is	pulled	in	
different	directions

    alternative	described	in:	improving	word	representations	 via	

global	context	and	multiple	word	prototypes (huang	et	al.	
2012)

   

idea:	cluster	word	windows	around	words,	retrain	with	each	
word	assigned	to	multiple	different	clusters	bank1,	bank2,	etc

lecture	1,	slide	34

richard	socher

4/5/16

but	what	about	ambiguity?	
   

improving	word	representations	 via	global	context	and	
multiple	word	prototypes (huang	et	al.	2012)

lecture	1,	slide	35

richard	socher

4/5/16

extrinsic	word	vector	evaluation
   

extrinsic	evaluation	of	word	vectors:	all	subsequent	tasks	in	this	class

semantic

    one	example	where	good	word	vectors	should	help	directly:	named	entity	

80

table 4: f1 score on ner task with 50d vectors.
discrete is the baseline without word vectors. we
use publicly-available vectors for hpca, hsmn,
and cw. see text for details.

model
discrete

recognition:	finding	a	person,	organization	or	location
dev test ace muc7
73.4
91.0
svd
90.8
73.7
svd-s
74.3
91.0
71.5
svd-l
90.5
hpca
92.6
80.7
74.7
hsmn 90.5
92.2
80.2
81.1
cbow 93.1
93.2
82.2
glove

77.4
77.3
77.6
73.6
81.7
78.7
81.7
82.2
82.9

85.4
85.7
85.5
84.8
88.7
85.7
87.4
88.2
88.3

cw

]

%

[
 
y
c
a
r
u
c
c
a

85

75

70

65

60

55

50

shown for neural vectors in (turian et al., 2010).

    next:	how	to	use	word	vectors	in	neural	net	models!
4.4 model analysis: vector length and

lecture	1,	slide	36

context size

richard	socher

in fig. 2, we show the results of experiments that

wiki2010
1b tokens

wiki2014
1.6b tokens

figure 3: accuracy on the analogy task for 300-
dimensional vectors trained on different corpora.

entries are updated to assimilate new knowledge,
whereas gigaword is a    xed news repository with
outdated and possibly incorrect information.

4.6 model analysis: run-time
the total run-time is split between populating x
4/5/16
and training the model. the former depends on

simple	single	word	classification

    what	is	the	major	benefit	of	deep	learned	word	

vectors?
    ability	to	also	classify	words	accurately

   

   

   

countries	cluster	together	   classifying	location	words	
should	be	possible	with	word	vectors

incorporate	any	information	into	them	other	tasks

project	sentiment	into	words	to	find	most	
positive/negative	words	in	corpus

lecture	1,	slide	37

richard	socher

4/5/16

the	softmax

logistic	regression	=	softmax classification	on	word	
vector	x	to	obtain	id203	for	class	y:

a1

a2

where:
generalizes	>2	classes	
(for	just	binary	sigmoid	unit	would	suffice	as	in	skip-gram)

x1

x2																x3

lecture	1,	slide	38

richard	socher

4/5/16

the	softmax - details
    terminology:	loss	function	=	cost	function	=	objective	function
    loss	for	softmax:	cross	id178

    to	compute	p(y|x):	first	take	the	y   th row	of	w	and	multiply	that	

with	row	with	x:

    compute	all	fc for	c=1,   ,c
    normalize	to	obtain	id203	with	softmax function:	

lecture	1,	slide	39

richard	socher

4/5/16

the	softmax and	cross-id178	error
    the	loss	wants	to	maximize	the	id203	of	the	correct	class	y

    hence,	we	minimize	the	negative	log	id203	of	that	class:

    as	before:	we	sum	up	multiple	cross	id178	errors	if	we	have	

multiple	classifications	in	our	total	error	function	over	the	
corpus	(more	next	lecture)

lecture	1,	slide	40

richard	socher

4/5/16

background:	the	cross	id178	error
    assuming	a	ground	truth	(or	gold	or	target)	id203	

distribution	that	is	1	at	the	right	class	and	0	everywhere	else:
p	=	[0,   ,0,1,0,   0]	and	our	computed	id203	is	q,	then	the	
cross	id178	is:	

    because	of	one-hot	p,	the	only	term	left	is	the	negative	

id203	of	the	true	class

    cross-id178	can	be	re-written	in	terms	of	the	id178	and	
id181	between	the	two	distributions:

lecture	1,	slide	41

richard	socher

4/5/16

the	kl	divergence
    cross	id178:
    because	p	is	zero	in	our	case	(and	even	if	it	wasn   t	it	would	be	
fixed	and	have	no	contribution	to	gradient),	to	minimize	this	is	
equal	to	minimizing	the	kl	divergence

    the	kl	divergence	is	not	a	distance	but	a	non-symmetric	

measure	of	the	difference	between	two	id203	distributions	
p and	q

lecture	1,	slide	42

richard	socher

4/5/16

pset 1
    derive	the	gradient	of	the	cross	id178	error	with	respect	to	

the	input	word	vector	x	and	the	matrix	w

lecture	1,	slide	43

richard	socher

4/5/16

simple	single	word	classification
    example:	sentiment

    two	options:	train	only	softmax weights	w	and	fix	word	vectors	

or	also	train	word	vectors

    question:	what	are	the	advantages	and	disadvantages	of	

training	the	word	vectors?

    pro:	better	fit	on	training	data
    con:	worse	generalization	because	the	words	move	in	the	

vector	space

lecture	1,	slide	44

richard	socher

4/5/16

visualization	of	sentiment	trained	word	vectors

lecture	1,	slide	45

richard	socher

4/5/16

next	level	up:	window	classification
    single	word	classification	has	no	context!	

    let   s	add	context	by	taking	in	windows	and	classifying	the	center	

word	of	that	window!

    possible:	softmax and	cross	id178	error	or	max-margin	loss

    next	class!

lecture	1,	slide	46

richard	socher

4/5/16

references

47

richard	socher

4/5/16

