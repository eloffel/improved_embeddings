cs224d
deep	nlp

lecture	8:

recap,	projects	and

fancy	recurrent	neural	networks

for	machine	translation

richard	socher

richard@metamind.io

overview

projects
recap	of	most	important	concepts	&	equations

   
   
    wrap	up:	deep	id56s	and	f1	evaluation
    machine	translation
   

fancy	id56	models	tackling	mt:
   
   

gated	recurrent	units	by	cho	et	al.	(2014)
long-short-term-memories	
by	hochreiter and	schmidhuber (1997)

2

richard	socher

4/26/16

recap	of	most	important	concepts

id97

glove

nnet &	max-margin

3

richard	socher

4/26/16

recap	of	most	important	concepts
a(nl 2))

= ( (nl))t w (nl 1)

f (w (nl 2)

i  

= ( (nl))t w (nl 1)

)a(nl 2)

j

@w (nl 2)

ij
@

ij

@w (nl 2)
f0(z(nl 1)

i

(54)

(55)

i

  i

)a(nl 2)

=    ( (nl))t w (nl 1)
= 0@
sl+1xj=1
where the sigmoid derivative from eq. 14 gives f0(z(l)) = (1   a(l))a(l). using that de   nition, we get the
hidden layer backprop derivatives:
|

    f0(z(nl 1)
)1a f0(z(nl 1)
}

multilayer	nnet
&
backprop

=  (nl 1)

j  (l+1)

w (nl 1)

a(nl 2)
j

a(nl 2)
j

{z

 (nl)
j

(56)

(57)

@

ji

)

j

i

(58)
+  w (l)
ij
where we used in the    rst line that the top layer is linear. this is a very detailed account of essentially

er = a(l)

@w (l)
ij

i

i

so, we can write the   errors of all layers l (except the top layer) (in vector format, using the hadamard

which in one simpli   ed vector notation becomes:

 (l) =   (w (l))t  (l+1)      f0(z(l)),

@

@w (l) er =  (l+1)(a(l))t +  w (l).
(59)

in summary, the backprop procedure consists of four steps:

4

richard	socher

4/26/16

1. apply an input xn and forward propagate it through the network to get the hidden and output

activations using eq. 18.

  i

  i

  i

7

recap	of	most	important	concepts

recurrent	neural	networks

cross	id178	error

mini-batched	sgd

5

richard	socher

4/26/16

deep	bidirectional	id56s	by	irsoy and	cardie
going deep 
y

h(3)

h(2)

h(1)

!(i)
!"! (i)ht
t = f (w
h
(i   1) +v
!(i)
!"" (i)ht
t = f (w
h
(i   1) +v
!
!
(l);h
yt = g(u[h

!(i)
!"(i)h
!(i))
t   1 +b
!(i)
!"(i)h
!(i))
t+1 +b
(l)]+c)

t

t

x
each memory layer passes an intermediate sequential 
representation to the next. 

6

richard	socher

4/26/16

data

consists	of	535	news	articles	(11,111	sentences)	

    mpqa	1.2	corpus	(wiebe et	al.,	2005)	
   
    manually	labeled	at	the	phrase	level	
    evaluation:	f1

7

richard	socher

4/26/16

evaluation

 

f
p
o
r
p

63 
61 
59 
57 

 
1
f
 
n
b

i

74 
72 
70 
68 
66 
64 

53 
51 
49 
47 

68 
66 
64 
62 
60 
58 

1 

2 

3 

4 

5 

1 

2 

3 

4 

5 

# layers 

# layers 

results: deep vs shallow id56s 

67 
65 
63 
8
61 
59 

 

 
1
f
p
o
r
p

dse 

57 
55 
53 
51 
49 

richard	socher

ese 

4/26/16

24k 
200k 

machine	translation

    methods	are	statistical	
    use	parallel	corpora
   
european	parliament	
first	parallel	corpus:
   

rosetta	stone	  
    traditional	systems

   

are	very	complex

9

richard	socher

4/26/16

current	statistical	machine	translation	systems
    source	language	f,	e.g.	french
    target	language	e,	e.g.	english
    probabilistic	formulation	(using	bayes	rule)

    translation	model	p(f|e)	trained	on	parallel	corpus
    language	model	p(e)	trained	on	english	only	corpus	(lots,	free!)

french	  

translation model

p(f|e)

   pieces	of	english	   language	model

p(e)

decoder

argmax p(f|e)p(e)    proper	english

10

richard	socher

4/26/16

step	1:	alignment	

alignments 

goal:	know	which	word	or	phrases	in	source	language	
would	translate	to	what	words	or	phrases	in	target	
language?	   hard	already!

we can factor the translation model p(f | e ) 
by identifying alignments (correspondences) 
between words in f and words in e 

 
x
u
a
e
v
u
o
n

 
s
e
m
s
  
s

i

 

n
o
p
a
j

 

  
u
o
c
e
s

 

e
l

 
x
u
e
d

 
r
a
p

(cid:7)spurious(cid:8) 

word 

japan 
shaken 
by 
two 
new 
quakes 

le 
japon 
secou   
par 
deux 
nouveaux 
s  ismes 

japan 
shaken 
by 
two 
new 
quakes 

alignment	examples	from	chris	manning/cs224n
11

richard	socher

4/26/16

step	1:	alignment	

alignments: harder 

 

e
m
m
a
r
g
o
r
p

 

e
l

 

n
o

i
t

a
c

i
l

p
p
a

 

  

t

  

i

 
s
m

 

n
e

 

a

(cid:7)zero fertility(cid:8) word 

not translated 

and 
the 
program 
has 
been 
implemented 

one-to-many 
alignment 

le 
programme 
a 
  t   
mis 
en 
application 

and 
the 
program 
has 
been 
implemented 

12

richard	socher

4/26/16

step	1:	alignment	
really	hard	:/	

alignments: harder 

 
t
i

a
n
e
t
r
a
p
p
a

 

e
t
s
e
r

 
x
u
a

 

e
l

 

le 
reste 
 
appartenait 
 
aux 
 
autochtones 

the 
balance 
was 
the 
territory 
of 
the 
aboriginal 
people 

many-to-one 
alignments 

the 
balance 
was 
the 
territory 
of 
the 
aboriginal 
people 

 
s
e
n
o

t

h
c
o
u
a

t

13

richard	socher

4/26/16

step	1:	alignment	

alignments: hardest 

les 
pauvres 
sont 
d  munis 

the 
poor 
don   t 
have 
any 
money 

many-to-many 

alignment 

the 
poor 
don(cid:6)t 
have 
any 
money 

14

richard	socher

 
s
e
r
v
u
a
p

 
t

n
o
s

i

 
s
n
u
m
  
d

 
s
e
l

phrase 
alignment 

4/26/16

step	1:	alignment	

translation process

    we	could	spend	an	entire	lecture	on	alignment	models
    not	only	single	words	but	could	use	phrases,	syntax
    then	consider	reordering	of	translated	phrases

    task: translate this sentence from german into english

er

er

geht

geht

ja

nicht

nach

hause

ja nicht

nach hause

he

does not

go

home

    pick phrase in input, translate

example	from	philipp	koehn

15

richard	socher

4/26/16

after	many	steps
each	phrase	in	source	language	has	many	possible	
translations	resulting	in	large	search	space:

translation options

er

he
it
, it
, he

geht

is
are
goes

go

ja

yes
is

, of course

,

nicht

not

do not

does not

is not

nach

hause

after

to

according to

in

house
home

chamber
at home

it is

he will be

it goes
he goes

not

is not

does not

do not

home

under house
return home

do not

is
are

is after all

does

not

is not
are not
is not a

to

following
not after

not to

richard	socher
    many translation options to choose from
16

4/26/16

    in europarl phrase table: 2727 matching phrase pairs for this sentence

decode:	search	for	best	of	many	hypotheses

hard	search	problem	that	also	includes	language	model

decoding: find best path

er

geht

ja

nicht

nach

hause

he

are

it

yes

goes

home

does not

home

go

to

17

4/26/16
backtrack from highest scoring complete hypothesis

richard	socher

traditional	mt

skipped	hundreds	of	important	details

   
    a	lot	of	human	feature	engineering
    very	complex	systems

    many	different,	independent	machine	learning	

problems

18

richard	socher

4/26/16

deep	learning	to	the	rescue!	   	?

maybe,	we	could	translate	directly	with	an	id56?

encoder

h1

x1

decoder:

awesome	

y1

sauce
y2

h2

w

x2

h3

w

x3

echt

dicke

kiste

this	needs to	
capture	the	
entire	phrase!

19

richard	socher

4/26/16

mt	with	id56s	    simplest	model

encoder:
decoder:		

minimize	cross	id178	error	for	all	target	words	
conditioned	on	source	words

it   s	not	quite	that	simple	;)	

20

richard	socher

4/26/16

id56	translation	model	extensions

1.	train	different	id56	weights	for	encoding	and	decoding

awesome	

y1

sauce
y2

h1

x1

h2

w

x2

h3

w

x3

echt

dicke

kiste

21

richard	socher

4/26/16

id56	translation	model	extensions

notation:	each	input	of	   has	its	own	linear	
transformation	matrix.	simple:
2 id56 encoder   decoder
2. compute	every	hidden	state	in	
2.1 preliminary: recurrent neural networks
a recurrent neural network (id56) is a neural net-
work that consists of a hidden state h and an
optional output y which operates on a variable-
previous	hidden	state	(standard)
length sequence x = (x1, . . . , xt ). at each time
step t, the hidden state hhti of the id56 is updated
last	hidden	vector	of	encoder	c=ht
by
(1)
previous	predicted	output	word	yt-1

decoder	from
   
   
   

hhti = f hht 1i, xt  ,

is

a non-linear

activation func-
where f
tion.
f may be as simple as an element-
wise logistic sigmoid function and as com-
plex as a long short-term memory (lstm)
unit (hochreiter and schmidhuber, 1997).

an id56 can learn a id203 distribution
over a sequence by being trained to predict the
next symbol in a sequence. in that case, the output
at each timestep t is the conditional distribution
p(xt | xt 1, . . . , x1). for example, a multinomial

richard	socher

22

figure 1: an illustration of the proposed id56
encoder   decoder.

cho	et	al.	2014

should note that the input and output sequence
lengths t and t 0 may differ.

4/26/16

the encoder is an id56 that reads each symbol
of an input sequence x sequentially. as it reads

(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:4)(cid:5)(cid:4)(cid:6)(cid:5)(cid:3)(cid:5)(cid:2)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)(cid:13)(cid:14)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)different	picture,	same	idea

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

ui

ip

e
l
p
m
a
s
s
 
d
r
o
w

y
t
i
l
i
b
a
b
o
r
p
 
d
r
o
w

t
n
e
r
r
u
c
e
r

ezi

t
a
t
s

t
n
e
r
r
u
c
e
r

e hi

t
a
t
s

si

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

 

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
c

wi

g
n
i
d
o
c
 
k

-
f
o
-
1

d
e
c
o
d
e
r

e
n
c
o
d
e
r

kyunghyun cho	et	al.	2014

e = (economic, growth, has, slowed, down, in, recent, years, .)
richard	socher

23

4/26/16

(cid:85)(cid:54)(cid:81)(cid:96)(cid:43)(cid:28)(cid:47)(cid:28)(cid:29)(cid:417)(cid:50)(cid:43)(cid:81)(cid:45) (cid:82)(cid:78)(cid:78)(cid:100)(cid:45) (cid:69)(cid:28)(cid:72)(cid:43)(cid:63)(cid:35)(cid:96)(cid:50)(cid:77)(cid:77)(cid:50)(cid:96)(cid:29)(cid:34)(cid:72)(cid:109)(cid:77)(cid:98)(cid:81)(cid:75)(cid:45) (cid:107)(cid:121)(cid:82)(cid:106)(cid:99) (cid:42)(cid:63)(cid:81) (cid:50)(cid:105) (cid:28)(cid:72)(cid:88)(cid:45) (cid:107)(cid:121)(cid:82)(cid:57)(cid:99) (cid:97)(cid:109)(cid:105)(cid:98)(cid:70)(cid:50)(cid:112)(cid:50)(cid:96)

id56	translation	model	extensions

going deep 
y

3. train	stacked/deep	id56s	

with	multiple	layers

4. potentially	train	

bidirectional	encoder

h(3)

h(2)

h(1)

!"! (i)ht
!(i)
t = f (w
h
!"" (i)ht
!(i)
t = f (w
h
yt = g(u[h

x
each memory layer passes an intermediate sequential 
representation to the next. 

5. train	input	sequence	in	reverse	order	for	simple	

optimization	problem:	instead	of	a	b	c	   x	y,	
train	with	c	b	a	   x	y

24

richard	socher

4/26/16

6.	main	improvement:	better	units

    more	complex	hidden	unit	computation	in	

recurrence!

    gated	recurrent	units	(gru)

introduced	by	cho	et	al.	2014	(see	reading	list)

    main	ideas:	

   

   

keep	around	memories	to	capture	long	distance	
dependencies
allow	error	messages	to	flow	at	different	strengths	
depending	on	the	inputs

25

richard	socher

4/26/16

grus

   

standard	id56	computes	hidden	layer	at	next	time	step	
directly:

    gru	first	computes	an	update	gate (another	layer)	
based	on	current	input	word	vector	and	hidden	state

    compute	reset	gate	similarly	but	with	different	weights

26

richard	socher

4/26/16

grus

    update	gate	
    reset	gate
    new	memory	content:

if	reset	gate	unit	is	~0,	then	this	ignores	previous	
memory	and	only	stores	the	new	word	information	
final	memory	at	time	step	combines	current	and	
previous	time	steps:		

richard	socher

4/26/16

   

27

attempt	at	a	clean	illustration

final	memory

memory	(reset)

update	gate

reset	gate

input:

28

~ht-1

zt-1

rt-1

xt-1

ht-1

ht

~ht

zt

rt

xt

richard	socher

4/26/16

gru	intuition
   

if	reset	is	close	to	0,	
ignore	previous	hidden	state
   allows	model	to	drop	
information	that	is	irrelevant
in	the	future

    update	gate	z	controls	how	much	of	past	state	should	

matter	now.
   

if	z	close	to	1,	then	we	can	copy	information	in	that	unit	
through	many	time	steps!	less	vanishing	gradient!

    units	with	short-term	dependencies	often	have	reset	

gates	very	active

29

richard	socher

4/26/16

gru	intuition
    units	with	long	term	

dependencies	have	active
update	gates	z
   
illustration:	
where     is the set of the model parameters and
each (xn, yn) is an (input sequence, output se-
quence) pair from the training set.
in our case,
as the output of the decoder, starting from the in-
put, is differentiable, we can use a gradient-based
algorithm to estimate the model parameters.

once the id56 encoder   decoder is trained, the
model can be used in two ways. one way is to use
the model to generate a target sequence given an
input sequence. on the other hand, the model can
be used to score a given pair of input and output
sequences, where the score is simply a id203
p   (y | x) from eqs. (3) and (4).
2.3 hidden unit that adaptively remembers

30

    derivative	of																	?	   rest	is	same	chain	rule,	but

implement	with	modularization or	automatic	
differentiation

figure 2: an illustration of the proposed hidden
activation function. the update gate z selects
whether the hidden state is to be updated with
a new hidden state   h. the reset gate r decides
whether the previous hidden state is ignored. see
eqs. (5)   (8) for the detailed equations of r, z, h
and   h.

only. this effectively allows the hidden state to
drop any information that is found to be irrelevant

richard	socher

4/26/16

in addition to a novel model architecture, we also

(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)long-short-term-memories	(lstms)
    we	can	make	the	units	even	more	complex
    allow	each	time	step	to	modify	
input	gate	(current	cell	matters)
forget	(gate	0,	forget	past)

   
   
    output	(how	much	cell	is	exposed)
    new	memory	cell
final	memory	cell:
final	hidden	state:	

   
   

31

richard	socher

4/26/16

illustrations	all	a	bit	overwhelming	;)

net c j

w ic j

g

yinj
w i
inj

g+

=
sc j sc j
1.0

g yinj

yinj
h

yc j

h yout j

net

inj

yout j

w i
out j

wic j

net

out j

long	short-term	memory	by	hochreiter and	schmidhuber (1997)

http://people.idsia.ch/~juergen/lstm/sld017.htm

http://deeplearning.net/tutorial/lstm.html

intuition:	memory	cells	can	keep	information	 intact,	unless	inputs	makes	them
forget	it	or	overwrite	it	with	new	input.
cell	can	decide	to	output	this	information	or	just	store	it

32

richard	socher

4/26/16

lstms	are	currently	very	hip!

    en	vogue	default	model	for	most	sequence	labeling	

tasks

    very	powerful,	especially	when	stacked	and	made	

even	deeper	(each	hidden	layer	is	already	computed	
by	a	deep	internal	network)

    most	useful	if	you	have	lots	and	lots	of	data

33

richard	socher

4/26/16

deep	lstms	compared	to	traditional	systems	2015

method

bahdanau et al. [2]
baseline system [29]

single forward lstm, beam size 12
single reversed lstm, beam size 12

ensemble of 5 reversed lstms, beam size 1
ensemble of 2 reversed lstms, beam size 12
ensemble of 5 reversed lstms, beam size 2
ensemble of 5 reversed lstms, beam size 12

test id7 score (ntst14)

28.45
33.30
26.17
30.59
33.00
33.27
34.50
34.81

table 1: the performance of the lstm on wmt   14 english to french test set (ntst14). note that
an ensemble of 5 lstms with a beam of size 2 is cheaper than of a single lstm with a beam of
size 12.

method

baseline system [29]

cho et al. [5]

best wmt   14 result [9]

rescoring the baseline 1000-best with a single forward lstm
rescoring the baseline 1000-best with a single reversed lstm

rescoring the baseline 1000-best with an ensemble of 5 reversed lstms

oracle rescoring of the baseline 1000-best lists

test id7 score (ntst14)

33.30
34.54
37.0
35.61
35.85
36.5
   45

table 2: methods that use neural networks together with an smt system on the wmt   14 english
34
to french test set (ntst14).

sequence	to	sequence	learning	by	sutskever et	al.	2014	

deep	lstms	(with	a	lot	more	tweaks)	today
wmt	2016	competition	results	from	yesterday

35

richard	socher

4/26/16

we were surprised to discover that the lstm did well on long sentences, which is shown quantita-
tively in    gure 3. table 3 presents several examples of long sentences and their translations.

deep	lstm	for	machine	translation

3.8 model analysis

pca	of	vectors	from	last	time	step	hidden	layer

4

3

2

1

0

   1

   2

   3

   4

   5

   6
   8

mary admires john

mary is in love with john

mary respects john

john admires mary

john is in love with mary

john respects mary

   6

   4

   2

0

2

4

6

8

10

15

10

5

0

   5

   10

   15

   20

   15

i was given a card by her in the garden

in the garden , she gave me a card

she gave me a card in the garden

she was given a card by me in the garden

in the garden , i gave her a card

i gave her a card in the garden

   10

   5

0

5

10

15

20

figure 2: the    gure shows a 2-dimensional pca projection of the lstm hidden states that are obtained
after processing the phrases in the    gures. the phrases are clustered by meaning, which in these examples is
primarily a function of word order, which would be dif   cult to capture with a bag-of-words model. notice that
both clusters have similar internal structure.

sequence	to	sequence	learning	by	sutskever et	al.	2014	

36

one of the attractive features of our model is its ability to turn a sequence of words into a vector
of    xed dimensionality. figure 2 visualizes some of the learned representations. the    gure clearly

richard	socher

4/26/16

further	improvements:	more	gates!

gated	feedback	recurrent	neural	networks,	chung	et	al.	2015

gated feedback recurrent neural networks

(a) conventional stacked id56

(b) gated feedback id56

figure 1. illustrations of (a) conventional stacking approach and (b) gated-feedback approach to form a deep id56 architecture. bullets
in (b) correspond to global reset gates. skip connections are omitted to simplify the visualization of networks.

the global reset gate is computed as:

37

gi!j =     wi!j

hj 1

g h   t 1    ,

computed by

richard	socher

t = tanh wj 1!jhj 1

t +

hj

4/26/16

lxi=1

gi!jui!jhi

t 1! ,

summary

    recurrent	neural	networks	are	powerful
    a	lot	of	ongoing	work	right	now
    gated	recurrent	units	even	better
   
    this	was	an	advanced	lecture	   gain	intuition,	

lstms	maybe	even	better	(jury	still	out)

encourage	exploration

    next	up:	recursive	neural	networks

simpler	and	also	powerful	:)

38

richard	socher

4/26/16

