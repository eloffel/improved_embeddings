cs224d
deep	nlp

lecture	4:	

word	window	classification

and	neural	networks

richard	socher

overview	today:
    general	classification	background

    updating	word	vectors	for	classification

    window	classification	&	cross	id178	error	derivation	tips

    a	single	layer	neural	network!

   

(max-margin	loss	and	backprop)

lecture	1,	slide	2

richard	socher

4/7/16

classification	setup	and	notation
    generally	we	have	a	training	dataset	consisting	of	samples	

{xi,yi}n

i=1

    xi - inputs,	e.g.	words	(indices	or	vectors!),	context	windows,	

sentences,	documents,	etc.

    yi - labels	we	try	to	predict,	

    e.g.	other	words
    class:	sentiment,	named	entities,	buy/sell	decision,	
    later:	multi-word	sequences

lecture	1,	slide	3

richard	socher

4/7/16

classification	intuition
    training	data:	{xi,yi}n

i=1

    simple	illustration	case:	

    fixed	2d	word	vectors	to	classify
    using	logistic	regression
       linear	decision	boundary	  

    general	ml:	assume	x	is	fixed	and	

only	train	logistic	regression	weights	
w	and	only	modify	the	decision	boundary

visualizations	with	convnetjs by	karpathy!
http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html

lecture	1,	slide	4

richard	socher

4/7/16

classification	notation
    cross	id178	loss	function	over	

dataset	{xi,yi}n

i=1	

    where	for	each	data	pair	(xi,yi):	

    we	can	write	f in	matrix	notation and	index	elements	of	it	based	

on	class:

lecture	1,	slide	5

richard	socher

4/7/16

classification:	id173!
    really	full	loss	function	over	any	dataset	includes	id173

over	all	parameters	  :

    id173	will	prevent	overfitting

when	we	have	a	lot	of	features	(or	
later	a	very	powerful/deep	model)
    x-axis:	more	powerful	model	or	
more	training	iterations
    blue:	training	error,	red:	test	error

lecture	1,	slide	6

richard	socher

4/7/16

details:	general	ml	optimization
    for	general	machine	learning	   usually

only	consists	of	columns	of	w:

    so	we	only	update	the	decision	

boundary

visualizations	with	convnetjs by	karpathy

lecture	1,	slide	7

richard	socher

4/7/16

classification	difference	with	word	vectors
    common	in	deep	learning:

    learn	both	w	and	word	vectors	x

very	large!

overfittingdanger!

lecture	1,	slide	8

richard	socher

4/7/16

losing	generalization	by	re-training	word	vectors
    setting:	training	logistic	regression	for	movie	review	sentiment	

and	in	the	training	data	we	have	the	words		
       tv   	and	   telly   
in	the	testing	data	we	have	
       television   

   

    originally	they	were	all	similar	
(from	pre-training	word	vectors)

    what	happens	when	we	train

the	word	vectors?

telly

tv

television

lecture	1,	slide	9

richard	socher

4/7/16

losing	generalization	by	re-training	word	vectors
    what	happens	when	we	train	the	word	vectors?

    those	that	are	in	the	training	data	move	around	
    words	from	pre-training	that	do	not	appear	in	training	stay

    example:
   
   

in	training	data:	   tv   	and	   telly   
in	testing	data	only:	   television   

telly

tv

lecture	1,	slide	10

richard	socher

4/7/16

:(
television

losing	generalization	by	re-training	word	vectors
    take	home	message:

if	you	only	have	a	small	
training	data	set,	don   t	
train	the	word	vectors.	

if	you	have	have	a	very	
large	dataset,	it	may	
work	better	to	train	
word	vectors	to	the	task.

telly

tv

lecture	1,	slide	11

richard	socher

4/7/16

television

side	note	on	word	vectors	notation
    the	word	vector	matrix	l	is	also	called	lookup	table
    word	vectors	=	word	embeddings =	word	representations	(mostly)
    mostly	from	methods	like	id97	or	glove

l =									d

|v|

[												]

   									   

aardvark	a				   	meta					   			zebra
    these	are	the	word	features	xword from	now	on

    conceptually	you	get	a	word   s	vector	by	left	multiplying	a	one-hot	

vector	e by	l:					x =	le	2 d   v	   v	   1

12

window	classification
    classifying	single	words	is	rarely	done.

   

interesting	problems	like	ambiguity	arise	in	context!

    example:	auto-antonyms:

    "to	sanction"	can	mean	"to	permit"	or	"to	punish.   
    "to	seed"	can	mean	"to	place	seeds"	or	"to	remove	seeds."

    example:	ambiguous	named	entities:

    paris	   paris,	france	vs paris	hilton	
    hathaway	   berkshire	hathaway	vs anne	hathaway

lecture	1,	slide	13

richard	socher

4/7/16

window	classification
   

idea:	classify	a	word	in	its	context	window	of	neighboring	words.

    for	example	named	entity	recognition	into	4	classes:

    person,	location,	organization,	none

    many	possibilities	exist	for	classifying	one	word	in	context,	e.g.	

averaging	all	the	words	in	a	window	but	that	looses	position	
information

lecture	1,	slide	14

richard	socher

4/7/16

window	classification
    train	softmax classifier	by	assigning	a	label	to	a	center	word	and	

concatenating	all	word	vectors	surrounding	it

    example:	classify	paris	in	the	context	of	this	sentence	with	

window	length	2:	

   					museums						in									paris									are						amazing				   	.

xwindow =	[		xmuseums

xin

xparis

xare

xamazing ]t

    resulting	vector	xwindow =	x	2 r5d				,	a	column	vector!

lecture	1,	slide	15

richard	socher

4/7/16

simplest	window	classifier:	softmax
    with	x	=	xwindow we	can	use	the	same	softmax classifier	as	before

predicted	model	
output	id203
    with	cross	id178	error	as	before:	

same

    but	how	do	you	update	the	word	vectors?

lecture	1,	slide	16

richard	socher

4/7/16

updating	concatenated	word	vectors
    short	answer:	just	take	derivatives	as	before

    long	answer:	let   s	go	over	the	steps	together	(you   ll	have	to	fill	

in	the	details	in	pset 1!)

    define:	

   
   

   

:	softmax id203	output	vector	(see	previous	slide)						
:	target	id203	distribution	(all	0   s	except	at	ground	
truth	index	of	class	y,	where	it   s	1)

and	fc =	c   th element	of	the	f	vector

    hard,	the	first	time,	hence	some	tips	now	:)

lecture	1,	slide	17

richard	socher

4/7/16

updating	concatenated	word	vectors

   

   

   

tip	1:	carefully	define	your	variables	and
keep	track	of	their	dimensionality!

tip	2:	know	thy	chain	rule	and	don   t	forget	which	variables	
depend	on	what:

tip	3:	for	the	softmax part	of	the	derivative:	first	take	the	
derivative	wrt fc when	c=y	(the	correct	class),	then	take	
derivative	wrt fc when	c    y	(all	the	incorrect	classes)

lecture	1,	slide	18

richard	socher

4/7/16

updating	concatenated	word	vectors

   

   

tip	4:	when	you	take	derivative	wrt
one	element	of	f,	try	to	see	if	you	can
create	a	gradient	in	the	end	that	includes
all	partial	derivatives:

tip	5:	to	later	not	go	insane	&	implementation!	   results	in	
terms	of	vector	operations	and	define	single	index-able	vectors:

lecture	1,	slide	19

richard	socher

4/7/16

updating	concatenated	word	vectors

   

   

   

tip	6:	when	you	start	with	the	chain	rule,
first	use	explicit	sums	and	look	at	
partial	derivatives	of	e.g.	xi or	wij

tip	7:	to	clean	it	up	for	even	more	complex	functions	later:	
know	dimensionality	of	variables	&simplify	into	matrix	notation

tip	8:	write	this	out	in	full	sums	if	it   s	not	clear!

lecture	1,	slide	20

richard	socher

4/7/16

updating	concatenated	word	vectors

    what	is	the	dimensionality	of	the	window	vector	gradient?

   

x is	the	entire	window,	5	d-dimensional	word	vectors,	so	the	
derivative	wrt to	x	has	to	have	the	same	dimensionality:

lecture	1,	slide	21

richard	socher

4/7/16

updating	concatenated	word	vectors

   

the	gradient	that	arrives	at	and	updates	the	word	vectors	can	
simply	be	split	up	for	each	word	vector:
let	

   
    with	xwindow =	[		xmuseums

xin xparis xare xamazing ]

    we	have

lecture	1,	slide	22

richard	socher

4/7/16

updating	concatenated	word	vectors

   

   

this	will	push	word	vectors	into	areas	such	they	will	be	helpful	
in	determining	named	entities.	

for	example,	the	model	can	learn	that	seeing	xin as	the	word	
just	before	the	center	word	is	indicative	for	the	center	word	to	
be	a	location

lecture	1,	slide	23

richard	socher

4/7/16

what   s	missing	for	training	the	window	model?

   

   
   

the	gradient	of	j	wrt the	softmax weights	w!

similar	steps,	write	down	partial	wrt wij first!
then	we	have	full	

lecture	1,	slide	24

richard	socher

4/7/16

a	note	on	matrix	implementations

    there	are	two	expensive	operations	in	the	softmax:
    the	matrix	multiplication																				and	the	exp
    a	for	loop	is	never	as	efficient	when	you	implement	it	

compared	vs when	you	use	a	larger	matrix	
multiplication!

    example	code	  

25

richard	socher

4/7/16

a	note	on	matrix	implementations

   

looping	over	word	vectors	instead	of	concatenating	
them	all	into	one	large	matrix	and	then	multiplying	
the	softmax weights	with	that	matrix

    1000	loops,	best	of	3:	639	  s	per	loop

10000	loops,	best	of	3:	53.8	  s	per	loop

26

richard	socher

4/7/16

a	note	on	matrix	implementations

   

result	of	faster	method	is	a	c	x	n	matrix:
   

each	column	is	an	f(x)	in	our	notation	(unnormalized class	scores)

    matrices	are	awesome!	
   

you	should	speed	test	your	code	a	lot	too

27

richard	socher

4/7/16

softmax (=	logistic	regression)	is	not	very	powerful

   

softmax only	gives	linear	decision	boundaries	in	the	
original	space.	

    with	little	data	that	can	be	a	good	regularizer
    with	more	data	it	is	very	limiting!

28

richard	socher

4/7/16

softmax (=	logistic	regression)	is	not	very	powerful

   

   

   

29

softmax only	linear	decision	boundaries

   lame	when	problem

is	complex

wouldn   t	it	be	cool	to	
get	these	correct?

richard	socher

4/7/16

neural	nets	for	the	win!

    neural	networks	can	learn	much	more	complex	

functions	and	nonlinear	decision	boundaries!

30

richard	socher

4/7/16

from	logistic	regression	to	neural	nets

31

demystifying	neural	networks

neural	networks	come	with	
their	own	terminological	
baggage	

   	just	like	id166s

but	if	you	understand	how	
softmax models	work

then	you	already	understand the	
operation	of	a	basic	neural	
network	neuron!

32

a	single	neuron

a	computational	unit	with	n	(3)	inputs

and	1	output

and	parameters	w,	b

inputs

activation	
function

output

bias	unit	corresponds	to	intercept	term

a	neuron	is	essentially	a	binary	logistic	regression	unit

hw,b(x) = f (wtx +b)
f (z) =

1
1+e   z

b:	we	can	have	an	   always	on   	
feature,	which	gives	a	class	prior,	
or	separate	it	out,	as	a	bias	term

33

w,	b are	the	parameters	of	this	neuron

i.e.,	this	logistic	regression	model

a	neural	network	
=	running	several	logistic	regressions	at	the	same	time
if	we	feed	a	vector	of	inputs	through	a	bunch	of	logistic	regression	
functions,	then	we	get	a	vector	of	outputs	   

but	we	don   t	have	to	decide	
ahead	of	time	what	variables	
these	logistic	regressions	are	
trying	to	predict!

34

a	neural	network	
=	running	several	logistic	regressions	at	the	same	time
   	which	we	can	feed	into	another	logistic	regression	function

it	is	the	loss	function	
that	will	direct	what	
the	intermediate	
hidden	variables	should	
be,	so	as	to	do	a	good	
job	at	predicting	the	
targets	for	the	next	
layer,	etc.

35

a	neural	network	
=	running	several	logistic	regressions	at	the	same	time
before	we	know	it,	we	have	a	multilayer	neural	network   .

36

matrix	notation	for	a	layer

we	have	

a1 = f (w11x1 +w12x2 +w13x3 +b1)
a2 = f (w21x1 +w22x2 +w23x3 +b2)
etc.

in	matrix	notation

z =wx +b
a = f (z)

where	f is	applied	element-wise:

f ([z1,z2,z3]) =[ f (z1), f (z2), f (z3)]

37

a1

a2

a3

w12

b3

non-linearities (f):	why	they   re	needed

example:	function	approximation,	
e.g.,	regression	or	classification
    without	non-linearities,	deep	neural	networks	

can   t	do	anything	more	than	a	linear	
transform

    extra	layers	could	just	be	compiled	down	into	

a	single	linear	transform:	
w1	w2	x =	wx

    with	more	layers,	they	can	approximate	more	

complex	functions!

   

38

a	more	powerful	window	classifier
    revisiting	

    xwindow =	[		xmuseums

xin

xparis

xare

xamazing ]

lecture	1,	slide	39

richard	socher

4/7/16

a	single	layer	neural	network
    a	single	layer	is	a	combination	of	a	linear	layer	
and	a	nonlinearity:

    the	neural	activations	a	can	then
be	used	to	compute	some	function
    for	instance,	a	softmax id203		or	an	
unnormalized score:

40

summary:	feed-forward	computation

computing	a	window   s	score	with	a	3-layer	neural	
net:	s	=	score(museums	in	paris	are	amazing	)

41

xwindow =	[		xmuseums

xin

xparis

xare

xamazing]

next	lecture:

training	a	window-based	neural	network.

taking	more	deeper	derivatives	   backprop

then	we	have	all	the	basic	tools	in	place	to	learn	about	
more	complex	models	:)

42

richard	socher

4/7/16

probably	for	next	lecture   

43

richard	socher

4/7/16

another	output	layer	and	loss	function	combo!

so	far:	softmax and	cross-id178	error	(exp slow)

   
    we	don   t	always	need	probabilities,	often	

unnormalized scores	are	enough	to	classify	correctly.

    also:	max-margin!

    more	on	that	in	future

lectures!

44

neural	net	model	to	classify	grammatical	phrases

   

   
   

45

idea:	train	a	neural	network	to	produce	high	scores	
for	grammatical	phrases	of	specific	length	and	low	
scores	for	ungrammatical	phrases

s =	score(cat	chills	on	a	mat)
sc =	score(cat	chills	menlo	a	mat)

richard	socher

4/7/16

another	output	layer	and	loss	function	combo!

    idea	for	training	objective
    make	score	of	true	window	larger	and	corrupt	

window   s	score	lower	(until	they   re	good	enough):	
minimize

    this	is	continuous,	can	perform	sgd

46

training	with	id26

assuming	cost	j	is	>	0,	it	is	simple	to	see	that	we	
can	compute	the	derivatives	of	s and	sc wrt all	the	
involved	variables:	u,	w,	b,	x

47

training	with	id26
    let   s	consider	the	derivative	of	a	single	weight	wij

    this	only	appears	inside	ai
    for	example:	w23 is	only	

used	to	compute	a2

48

s		

u2

a1

a2

w23

x1

x2																	x3

+1

training	with	id26

derivative	of	weight	wij:

s		

u2

a1

a2

w23

x1

x2																	x3

+1

49

training	with	id26

derivative	of	single	weight	wij :

s		

u2

a1

a2

w23

local	error	

signal

local	input	

signal

where																																																		for	logistic	f

50

x1

x2																	x3

+1

training	with	id26

    from	single	weight	wij to	full	w:

    we	want	all	combinations	of

i =	1,	2 and j	=	1,	2,	3

    solution:	outer	product:
where																		is	the	
   responsibility   	coming	from	
each	activation	a

51

s		

u2

a1

a2

w23

x1

x2																	x3

+1

training	with	id26
    for	biases	b,	we	get:

s		

u2

a1

a2

w23

x1

x2																	x3

+1

52

training	with	id26

that   s	almost	id26

it   s	simply	taking	derivatives	and	using	the	chain	rule!

remaining	trick:	we	can	re-use	derivatives	computed	for	
higher	layers	in	computing	derivatives	for	lower	layers

example:	last	derivatives	of	model,	the	word	vectors	in	x

53

training	with	id26

    take	derivative	of	score	with	
respect	to	single	word	vector	
(for	simplicity	a	1d	vector,	
but	same	if	it	was	longer)
    now,	we	cannot	just	take	
into	consideration	one	ai
because	each	xj is	connected	
to	all	the	neurons	above	and	
hence	xj influences	the	
overall	score	through	all	of	
these,	hence:

54

re-used	part	of	previous	derivative

summary

55

richard	socher

4/7/16

