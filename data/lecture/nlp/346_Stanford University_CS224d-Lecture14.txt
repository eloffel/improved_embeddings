cs224d: deep learning for 
natural language processing 

andrew	
   maas	
   
spring	
   2016	
   	
   

	
   

neural	
   networks	
   in	
   speech	
   recogni5on	
   	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

outline	
   

       speech	
   recogni?on	
   systems	
   overview	
   
       id48-     dnn	
   (hybrid)	
   acous?c	
   modeling	
   
       what   s	
   di   erent	
   about	
   modern	
   id48-     dnns?	
   
       id48-     free	
   id56	
   recogni?on	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

cat	
   

clothes	
   

climbing	
   

deep	
   neural	
   network	
   

what	
   ac?on?	
   

is	
   the	
   user	
   annoyed?	
   

ask	
   for	
   clari   ca?on?	
   

are	
   there	
   any	
   good	
   robot	
   
movies	
   i	
   can	
   rent	
   tonight?	
   

understanding	
   

transcrip?on	
   

noise	
   reduc?on	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

conversa?onal	
   speech	
   data	
   

switchboard	
   

300	
   
hours	
   

4,870	
   

speakers	
   

but	
   it	
   was	
   really	
   nice	
   to	
   get	
   back	
   with	
   a	
   telephone	
   and	
   the	
   city	
   and	
   everything	
   and	
   you	
   
know	
   yeah	
   

well	
   (i-     )	
   the	
   only	
   way	
   i	
   could	
   bear	
   it	
   was	
   to	
   (pass)	
   (some)	
   to	
   be	
   asleep	
   i	
   was	
   like	
   well	
   it	
   is	
   
not	
   gonna	
   (be-     )	
   get	
   over	
   un?l	
   you	
   know	
   (w-     )	
   (w-     )	
   yeah	
   it	
   (re-     )	
   really	
   i	
   (th-     )	
   i	
   think	
   that	
   is	
   
what	
   ruined	
   it	
   for	
   us	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

outline	
   

       speech	
   recogni?on	
   systems	
   overview	
   
       id48-     dnn	
   (hybrid)	
   acous5c	
   modeling	
   
       what   s	
   di   erent	
   about	
   modern	
   id48-     dnns?	
   
       id48-     free	
   id56	
   recogni?on	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

acous?c	
   modeling	
   with	
   gmms	
   

transcrip5on:	
   
pronuncia5on:	
   
sub-     phones	
   :	
   
	
   
hidden	
   markov	
   
model	
   (id48):	
   
	
   
	
   
acous5c	
   model:	
   
	
   
	
   
audio	
   input:	
   

%&*

%&)

%&(

%&'

%&!

%&"

%&#

%&$

%
!!

samson	
   
s	
      	
   ae	
      	
   m	
      	
   s	
      ah	
      	
   n	
   
942	
      	
   6	
      	
   37	
      	
   8006	
      	
   4422	
      	
   

942	
   

942	
   

6	
   

%&*

%&)

%&(

%&'

%&!

%&"

%&#

%&$

%
!!

!"

!#

!$

%

$

#

"

!

!"

!#

!$

%

$

#

"

!

!

"

#

$

%

!$

!#

!"

%&*

%&)

%&(

%&'

%&!

%&"

%&#

%&$

%
!!

gmm	
   models:	
   
p(x|s)	
   
x:	
   input	
   features	
   
s:	
   id48	
   state	
   

features	
   

features	
   

features	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

dnn	
   hybrid	
   acous?c	
   models	
   

transcrip5on:	
   
pronuncia5on:	
   
sub-     phones	
   :	
   
	
   
hidden	
   markov	
   
model	
   (id48):	
   
	
   
	
   
	
   
	
   
acous5c	
   model:	
   
	
   
	
   
	
   
	
   
	
   
audio	
   input:	
   

samson	
   
s	
      	
   ae	
      	
   m	
      	
   s	
      ah	
      	
   n	
   
942	
      	
   6	
      	
   37	
      	
   8006	
      	
   4422	
      	
   

942	
   

942	
   

6	
   

p(s|x1)	
   

p(s|x2)	
   

p(s|x3)	
   

features	
   (x1)	
   

features	
   (x2)	
   

features	
   (x3)	
   

use	
   a	
   dnn	
   to	
   approximate:	
   
p(s|x)	
   
	
   
apply	
   bayes   	
   rule:	
   
p(x|s)	
   =	
   p(s|x)	
   *	
   p(x)	
   /	
   p(s)	
   
	
   
	
   	
   dnn	
   *	
   constant	
   /	
   state	
   prior	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

not	
   really	
   a	
   new	
   idea	
   

renals,	
   morgan,	
   bourland,	
   cohen,	
   &	
   franco.	
   1994.	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

modern	
   systems	
   use	
   dnns	
   and	
   senones	
   

dahl,	
   yu,	
   deng	
   &	
   acero.	
   2011.	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

hybrid	
   systems	
   now	
   dominate	
   asr	
   

hinton	
   et	
   al.	
   2012.	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

what   s	
   di   erent	
   in	
   modern	
   dnns?	
   

       fast	
   computers	
   =	
   run	
   many	
   experiments	
   
       deeper	
   nets	
   improve	
   on	
   shallow	
   nets	
   
       architecture	
   choices	
   (easiest	
   is	
   replacing	
   sigmoid)	
   
       pre-     training	
   ma#ers	
   very	
   li#le.	
   ini?ally	
   we	
   thought	
   

this	
   was	
   the	
   new	
   trick	
   that	
   made	
   things	
   work	
   

       many	
   more	
   parameters	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

depth	
   magers	
   (somewhat)	
   

warning!	
   depth	
   can	
   also	
   act	
   as	
   a	
   regularizer	
   because	
   it	
   makes	
   op?miza?on	
   
more	
   di   cult.	
   this	
   is	
   why	
   you	
   will	
   some?mes	
   see	
   very	
   deep	
   networks	
   perform	
   
well	
   on	
   timit	
   or	
   other	
   small	
   tasks.	
   

yu,	
   seltzer,	
   li,	
   huang,	
   seide.	
   2013.	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

replacing	
   sigmoid	
   hidden	
   units	
   

tanh	
   

rel	
   

-     4	
   

2	
   

1	
   

0	
   

-     1	
   

0	
   

4	
   

(glorot	
   &	
   bengio.	
   2011)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

comparing	
   nonlineari?es	
   

switchboard	
   wer	
   

tanh	
   
rel	
   

gmm	
   

2	
   layer	
   

3	
   lyaer	
   

4	
   layer	
   

msr	
   9	
   layer	
   

ibm	
   7	
   layer	
   

mmi	
   

25	
   

20	
   

15	
   

10	
   

5	
   

0	
   

(maas,	
   qi,	
   xie,	
   hannun,	
   lengerich,	
   jurafsky,	
   &	
   ng.	
   in	
   submission)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

scaling	
   up	
   nn	
   acous?c	
   models	
   in	
   1999	
   

0.7m	
   total	
   nn	
   parameters	
   

(ellis	
   &	
   morgan.	
   1999)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

adding	
   more	
   parameters	
   15	
   years	
   ago	
   
size	
   ma#ers:	
   an	
   empirical	
   study	
   of	
   neural	
   network	
   
training	
   for	
   lvcsr.	
   ellis	
   &	
   morgan.	
   icassp.	
   1999.	
   	
   
	
   
hybrid	
   nn.	
   1	
   hidden	
   layer.	
   54	
   id48	
   states.	
   	
   
74hr	
   broadcast	
   news	
   task	
   
	
   
      improvements	
   are	
   almost	
   always	
   obtained	
   by	
   increasing	
   either	
   or	
   
both	
   of	
   the	
   amount	
   of	
   training	
   data	
   or	
   the	
   number	
   of	
   network	
   
parameters	
      	
   we	
   are	
   now	
   planning	
   to	
   train	
   an	
   8000	
   hidden	
   unit	
   net	
   on	
   
150	
   hours	
   of	
   data	
      	
   this	
   training	
   will	
   require	
   over	
   three	
   weeks	
   of	
   
computa?on.   	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

adding	
   more	
   parameters	
   now	
   	
   

       comparing	
   total	
   number	
   of	
   parameters	
   (in	
   millions)	
   

of	
   previous	
   work	
   versus	
   our	
   new	
   experiments	
   

0	
   

50	
   

100	
   

total	
   dnn	
   parameters	
   (m)	
   
150	
   
300	
   

200	
   

250	
   

350	
   

400	
   

450	
   

(maas,	
   qi,	
   xie,	
   hannun,	
   lengerich,	
   jurafsky,	
   &	
   ng.	
   in	
   submission)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

combining	
   speech	
   corpora	
   

switchboard	
   

300	
   
hours	
   

fisher	
   

4,870	
   

speakers	
   

2,000	
   hours	
   

combined	
   corpus	
   baseline	
   system	
   now	
   available	
   in	
   kaldi	
   	
   

23,394	
   
speakers	
   

(maas,	
   qi,	
   xie,	
   hannun,	
   lengerich,	
   jurafsky,	
   &	
   ng.	
   in	
   submission)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

scaling	
   total	
   parameters	
   

50	
   

45	
   

40	
   

	
   

e
t
a
r
	
   
r
o
r
r
e
e
m
a
r
f

	
   

	
   

35	
   

30	
   

25	
   

20	
   

gmm	
   

36m	
   

100m	
   

200m	
   

400m	
   

(maas,	
   qi,	
   xie,	
   hannun,	
   lengerich,	
   jurafsky,	
   &	
   ng.	
   in	
   submission)	
   

model	
   size	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

40	
   
38	
   
36	
   
34	
   
32	
   
30	
   
28	
   
26	
   
24	
   
22	
   
20	
   
18	
   
16	
   
14	
   
12	
   
10	
   
8	
   
6	
   
4	
   
2	
   
0	
   

	
   

	
   

e
t
a
r
	
   
r
o
r
r
e
d
r
o
w
3
0
-     
t
r

	
   

	
   

e
t
a
r
	
   
r
o
r
r
e
e
m
a
r
f

	
   

	
   

35	
   

50	
   

45	
   

40	
   

30	
   

25	
   

20	
   

scaling	
   total	
   parameters	
   

frame	
   error	
   rate	
   
word	
   error	
   rate	
   

45	
   

40	
   

	
   

35	
   

30	
   

	
   

e
t
a
r
	
   
r
o
r
r
e
d
r
o
w
3
0
-     
t
r

	
   

gmm	
   

36m	
   

100m	
   

200m	
   

400m	
   

25	
   

20	
   

(maas,	
   qi,	
   xie,	
   hannun,	
   lengerich,	
   jurafsky,	
   &	
   ng.	
   in	
   submission)	
   

model	
   size	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

outline	
   

       speech	
   recogni?on	
   systems	
   overview	
   
       id48-     dnn	
   (hybrid)	
   acous?c	
   modeling	
   
       what   s	
   di   erent	
   about	
   modern	
   id48-     dnns?	
   
       id48-     free	
   id56	
   recogni5on	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

id48-     dnn	
   speech	
   recogni?on	
   

transcrip5on:	
   
pronuncia5on:	
   
sub-     phones	
   :	
   
	
   
hidden	
   markov	
   
model	
   (id48):	
   
	
   
	
   
	
   
	
   
acous5c	
   model:	
   
	
   
	
   
	
   
	
   
	
   
audio	
   input:	
   

samson	
   
s	
      	
   ae	
      	
   m	
      	
   s	
      ah	
      	
   n	
   
942	
      	
   6	
      	
   37	
      	
   8006	
      	
   4422	
      	
   

942	
   

942	
   

6	
   

p(s|x1)	
   

p(s|x2)	
   

p(s|x3)	
   

features	
   (x1)	
   

features	
   (x2)	
   

features	
   (x3)	
   

use	
   a	
   dnn	
   to	
   approximate:	
   
p(s|x)	
   
	
   
apply	
   bayes   	
   rule:	
   
p(x|s)	
   =	
   p(s|x)	
   *	
   p(x)	
   /	
   p(s)	
   
	
   
	
   	
   dnn	
   *	
   constant	
   /	
   state	
   prior	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

id48-     free	
   recogni?on	
   

samson	
   
s	
      	
   ae	
      	
   m	
      	
   s	
      ah	
      	
   n	
   
942	
      	
   6	
      	
   37	
      	
   8006	
      	
   4422	
      	
   

942	
   

942	
   

6	
   

p(s|x1)	
   

p(s|x2)	
   

p(s|x3)	
   

features	
   (x1)	
   

features	
   (x2)	
   

features	
   (x3)	
   

transcrip5on:	
   
pronuncia5on:	
   
sub-     phones	
   :	
   
	
   
hidden	
   markov	
   
model	
   (id48):	
   
	
   
	
   
	
   
	
   
acous5c	
   model:	
   
	
   
	
   
	
   
	
   
	
   
audio	
   input:	
   

(graves	
   &	
   jaitly.	
   2014)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

id48-     free	
   recogni?on	
   

transcrip5on:	
   
	
   
characters:	
   
	
   
collapsing	
   
func5on:	
   
	
   
	
   
	
   
acous5c	
   model:	
   
	
   
	
   
	
   
	
   
	
   
audio	
   input:	
   

samson	
   

samson	
   
	
   
ss___aa_m_s___o___nnnn	
   

s	
   

p(a|x1)	
   

s	
   

p(a|x2)	
   

_	
   

p(a|x3)	
   

features	
   (x1)	
   

features	
   (x2)	
   

features	
   (x3)	
   

use	
   a	
   dnn	
   to	
   approximate:	
   
p(a|x)	
   
	
   
the	
   distribu?on	
   over	
   
characters	
   

(graves	
   &	
   jaitly.	
   2014)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

ctc	
   objec?ve	
   func?on	
   
labels	
   at	
   each	
   ?me	
   index	
   are	
   condi?onally	
   
independent	
   (like	
   id48s)	
   
	
   
	
   
sum	
   over	
   all	
   ?me-     level	
   labelings	
   consistent	
   with	
   the	
   
output	
   label.	
   	
   
output	
   label:	
   ab	
   
time-     level	
   labelings:	
   ab,	
   _ab,	
   a_b,	
      	
   _a_b_	
   
	
   
final	
   objec?ve	
   maximizes	
   id203	
   of	
   true	
   labels:	
   

(graves	
   &	
   jaitly.	
   2014)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

collapsing	
   example	
   

per-     frame	
   argmax:	
   
____________________________________________________________________________________________________y
y__ee_________g_	
   	
   ____________________________________________a_____	
   	
   
_rr__e________hh__________b___ii_______lll__i_____g______aa______g_______iio__n___	
   	
   
___cc_____rrr_u_____________________	
   	
   ________ii___ss	
   	
   
______________o__________nn_____________hhh_a___________________nnddd	
   	
   ________________i__n___	
   
__thh_e_____	
   	
   __________________________________________bb_uuii_______lllldd____ii____nng_____	
   	
   
___________________________________l___o___o_g__g___ii____nng______	
   	
   
____b___rr_ii________ck__s__________________________________________p___ll__a________ssg_________eerr__	
   	
   
______a___nnd_	
   	
   ___b___lll_uu____ee__pp___r___i________nnss_	
   	
   
________________f______oou____________rrr________	
   _____________f_____oo__rrr__g_y____	
   	
   
_____t____www_oo__________	
   	
   	
   ____nn___ew___________________	
   	
   
______________________________________________________b___e_______t__________i____n___	
   	
   
____e________pp_____aa___rr___g____mm_ee___nnntss	
   	
   	
   
_____________________________________________________________________________________________________
________________________________	
   	
   
	
   

aper	
   collapsing:	
   
yet	
   a	
   rehbilita?on	
   cru	
   is	
   onhand	
   in	
   the	
   building	
   loogging	
   bricks	
   plaster	
   and	
   blueprins	
   four	
   forty	
   two	
   new	
   be?n	
   epartments	
   	
   

reference:	
   
yet	
   a	
   rehabilita?on	
   crew	
   is	
   on	
   hand	
   in	
   the	
   building	
   lugging	
   bricks	
   plaster	
   and	
   blueprints	
   for	
   forty	
   two	
   new	
   bedroom	
   
apartments	
   	
   

(hannun,	
   maas,	
   jurafsky,	
   &	
   ng.	
   2014)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

recurrence	
   magers!	
   

s	
   

p(a|x1)	
   

s	
   
p(a|x2)	
   

_	
   

p(a|x3)	
   

features	
   (x1)	
   

features	
   (x2)	
   

features	
   (x3)	
   

architecture	
   
dnn	
   
+	
   recurrence	
   
+	
   bi-     direc?onal	
   
recurrence	
   

cer	
   
22	
   
13	
   
10	
   

(hannun,	
   maas,	
   jurafsky,	
   &	
   ng.	
   2014)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

decoding	
   with	
   a	
   language	
   model	
   
character	
   error	
   rate	
   

12	
   

lexicon	
   

[a,	
      ,	
   zebra]	
   

language	
   
model	
   

p(   yeah   	
   |	
      oh   )	
   

character	
   
probabili5es	
   

__oo_h__y_e_aa_h	
   

8	
   

4	
   

0	
   

40	
   
30	
   
20	
   
10	
   
0	
   

none	
   

lexicon	
   

bigram	
   

word	
   error	
   rate	
   

none	
   

lexicon	
   

bigram	
   

(hannun,	
   maas,	
   jurafsky,	
   &	
   ng.	
   2014)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

rethinking	
   decoding	
   

lexicon	
   

[a,	
      ,	
   zebra]	
   

language	
   
model	
   

p(   yeah   	
   |	
      oh   )	
   

character	
   
probabili5es	
   

__oo_h__y_e_aa_h	
   

out	
   of	
   vocabulary	
   words	
   
syriza	
   
bae	
   
sof-     -     	
   

abo-     -     	
   
schmidhuber	
   

character	
   
language	
   
model	
   

character	
   
probabili5es	
   

p(h	
   |	
   o,h,	
   ,y,e,a,)	
   

__oo_h__y_e_aa_h	
   

(maas*,	
   xie*,	
   jurafsky,	
   &	
   ng.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

lexicon-     free	
   &	
   id48-     free	
   on	
   switchboard	
   

40	
   

35	
   

30	
   

25	
   

20	
   

15	
   

10	
   

5	
   

0	
   

id48-     gmm	
   

ctc	
   no	
   lm	
   

ctc	
   +	
   7-     gram	
   

ctc	
   +	
   nn	
   lm	
   

id48-     dnn	
   

(maas*,	
   xie*,	
   jurafsky,	
   &	
   ng.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

transcribing	
   out	
   of	
   vocabulary	
   words	
   

truth:	
   yeah	
   i	
   went	
   into	
   the	
   i	
   do	
   not	
   know	
   what	
   you	
   think	
   of	
      delity	
   but	
   
id48-     gmm:	
   yeah	
   when	
   the	
   i	
   don   t	
   know	
   what	
   you	
   think	
   of	
      del	
   it	
   even	
   them	
   
ctc-     clm:	
   yeah	
   i	
   went	
   to	
   i	
   don   t	
   know	
   what	
   you	
   think	
   of	
      delity	
   but	
   um	
   

truth:	
   no	
   no	
   speaking	
   of	
   weather	
   do	
   you	
   carry	
   a	
   al?meter	
   slash	
   barometer	
   
id48-     gmm:	
   no	
   i   m	
   not	
   all	
   being	
   the	
   weather	
   do	
   you	
   uh	
   carry	
   a	
   uh	
   helped	
   emiuers	
   last	
   brahms	
   
her	
   
ctc-     clm:	
   no	
   no	
   bea?ng	
   of	
   whether	
   do	
   you	
   uh	
   carry	
   a	
   uh	
   a	
   5me	
   or	
   less	
   barometer	
   

truth:	
   i	
   would	
   ima-     	
   well	
   yeah	
   it	
   is	
   i	
   know	
   you	
   are	
   able	
   to	
   stay	
   home	
   with	
   them	
   
id48-     gmm:	
   i	
   would	
   amount	
   well	
   yeah	
   it	
   is	
   i	
   know	
   um	
   you   re	
   able	
   to	
   stay	
   home	
   with	
   them	
   
ctc-     clm:	
   i	
   would	
   ima-     	
   well	
   yeah	
   it	
   is	
   i	
   know	
   uh	
   you   re	
   able	
   to	
   stay	
   home	
   with	
   them	
   

(maas*,	
   xie*,	
   jurafsky,	
   &	
   ng.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

comparing	
   alignments	
   

id48-     gmm	
   phone	
   probabili?es	
   

ctc	
   character	
   probabili?es	
   

(id48	
   slide	
   from	
   dan	
   ellis)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

learning	
   phonemes	
   and	
   timing	
   

(maas*,	
   xie*,	
   jurafsky,	
   &	
   ng.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

learning	
   phonemes	
   and	
   timing	
   

(maas*,	
   xie*,	
   jurafsky,	
   &	
   ng.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

pushing	
   performance	
   with	
   id48-     free	
   	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

ctc	
   now	
   powers	
   google	
   search	
   asr	
   
       context-     dependent	
   states	
   rather	
   than	
   characters	
   
       uni-     direc?onal	
   lstm	
   for	
   faster	
   streaming	
   
       ctc	
   +	
   sequence	
   discrimina?ve	
   loss	
   

hgp://googleresearch.blogspot.com/2015/09/google-     voice-     search-     faster-     and-     more.html	
   
(sak,	
   senior,	
   rao,	
   &	
   beaufays.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

deep	
   speech	
   2:	
   scaling	
   up	
   ctc	
   

       e   cient	
   gpu	
   training	
   
       some	
   recurrent	
   architecture	
   variants	
   
       data	
   augmenta?on	
   
       works	
   on	
   both	
   english	
   and	
   mandarin	
   

(amodei	
   	
   et	
   al.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

listen,	
   agend,	
   and	
   spell	
   

(chan,	
   jaitly,	
   le,	
   &	
   vinyals.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

listen,	
   agend,	
   and	
   spell	
   

(chan,	
   jaitly,	
   le,	
   &	
   vinyals.	
   2015)	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

conclusion	
   

       id48-     dnn	
   systems	
   are	
   now	
   the	
   default,	
   state-     of-     

the-     art	
   for	
   speech	
   recogni?on	
   

       we	
   roughly	
   understand	
   why	
   id48-     dnns	
   work	
   but	
   

older,	
   shallow	
   hybrid	
   models	
   didn   t	
   work	
   as	
   well	
   
       id48-     free	
   approaches	
   are	
   rapidly	
   improving	
   and	
   

making	
   their	
   way	
   to	
   produc?on	
   systems	
   

       it   s	
   a	
   very	
   excieng	
   eme	
   for	
   speech	
   recognieon	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

end	
   

       more	
   on	
   spoken	
   language	
   understanding:	
   

      cs224s.stanford.edu	
   

       open	
   source	
   speech	
   recogni?on	
   toolkit	
   (kaldi):	
   

       mul?ple	
   open	
   source	
   implementa?ons	
   of	
   ctc	
   

      kaldi.sf.net	
   

available	
   

andrew	
   maas.	
   stanford	
   cs224d.	
   2016	
   

