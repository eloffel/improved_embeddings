cs224d	

deep	learning		

for	natural	language	processing	

	
	

	
	

lecture	2:	word	vectors	

richard	socher	

how	do	we	represent	the	meaning	of	a	word?	

de   ni4on:	meaning	(webster	dic4onary)	
      
      

the	idea	that	is	represented	by	a	word,	phrase,	etc.	
the	idea	that	a	person	wants	to	express	by	using	
words,	signs,	etc.	
the	idea	that	is	expressed	in	a	work	of	wri4ng,	art,	etc.	

richard	socher	

3/31/16	

      

2	

how	to	represent	meaning	in	a	computer?	
common	answer:	use	a	taxonomy	like	id138	that	has	
hypernyms	(is-a)	rela4onships								and	

	

	

	

	

						synonym	sets	(good):	

[synset('procyonid.n.01'),		
synset('carnivore.n.01'),		
synset('placental.n.01'),		
synset('mammal.n.01'),		
synset('vertebrate.n.01'),		
synset('chordate.n.01'),		
synset('animal.n.01'),		
synset('organism.n.01'),		
synset('living_thing.n.01'),		
synset('whole.n.02'),		
synset('object.n.01'),		
synset('physical_en4ty.n.01'),		
synset('en4ty.n.01')]	
3	

s:	(adj)	full,	good		
s:	(adj)	es4mable,	good,	honorable,	respectable		
s:	(adj)	bene   cial,	good		
s:	(adj)	good,	just,	upright		
s:	(adj)	adept,	expert,	good,	prac4ced,		
pro   cient,	skillful	
s:	(adj)	dear,	good,	near		
s:	(adj)	good,	right,	ripe	
   	
s:	(adv)	well,	good		
s:	(adv)	thoroughly,	soundly,	good		
s:	(n)	good,	goodness		
s:	(n)	commodity,	trade	good,	good		

richard	socher	

3/31/16	

problems	with	this	discrete	representadon	

       great	as	resource	but	missing	nuances,	e.g.	

synonyms:		
adept,	expert,	good,	prac4ced,	pro   cient,	skillful?	

       missing	new	words	(impossible	to	keep	up	to	date):	

wicked,	badass,	nixy,	crack,	ace,	wizard,	genius,	ninjia	

       subjec4ve	
       requires	human	labor	to	create	and	adapt	
       hard	to	compute	accurate	word	similarity	  	

4	

richard	socher	

3/31/16	

problems	with	this	discrete	representadon	

the	vast	majority	of	rule-based	and	sta4s4cal	nlp	work	regards	
words	as	atomic	symbols:	hotel, conference, walk 
	

in	vector	space	terms,	this	is	a	vector	with	one	1	and	a	lot	of	zeroes	

[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] 

dimensionality:	20k	(speech)	   	50k	(ptb)	   	500k	(big	vocab)	   	13m	(google	1t)	

we	call	this	a	   one-hot   	representa4on.	its	problem:	
  motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]  and 
  hotel  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]  =  0	

5	

distribudonal	similarity	based	representadons	

you	can	get	a	lot	of	value	by	represen4ng	a	word	by	
means	of	its	neighbors	
   you	shall	know	a	word	by	the	company	it	keeps   	

(j.	r.	firth	1957:	11)	
one	of	the	most	successful	ideas	of	modern	sta4s4cal	nlp	

 
government debt problems turning into banking crises as has happened in 
         saying that europe needs unified banking regulation to replace the hodgepodge 

  	these	words	will	represent	banking	  	

	6	

how	to	make	neighbors	represent	words?	

answer:	with	a	cooccurrence	matrix	x	
       2	op4ons:	full	document	vs	windows	
       word	-	document	cooccurrence	matrix	will	give	
general	topics	(all	sports	terms	will	have	similar	
entries)	leading	to	   latent	seman4c	analysis   	

      

7	

instead:	window	around	each	word	  	captures	both	
syntac4c	(pos)	and	seman4c	informa4on	

richard	socher	

3/31/16	

window	based	cooccurence	matrix	
       window	length	1	(more	common:	5	-	10)	
       symmetric	(irrelevant	whether	lex	or	right	context)	
       example	corpus:		

      
      
      

i	like	deep	learning.		
i	like	nlp.		
i	enjoy	   ying.	

8	

richard	socher	

3/31/16	

window	based	cooccurence	matrix	
       example	corpus:		

      
      
      
i	
counts	
i	
0	
like	
2	
enjoy	
1	
deep	
0	
learning	 0	
nlp	
0	
   ying	
0	
.	
0	

i	like	deep	learning.		
i	like	nlp.		
i	enjoy	   ying.	
like	
2	
0	
0	
1	
0	
1	
0	
0	

enjoy	 deep	
1	
0	
0	
0	
0	
0	
1	
0	

learning	 nlp	    ying	
0	
0	
0	
1	
0	
0	
1	
0	
0	
1	
0	
0	
0	
0	
richard	socher	
0	
1	

.	
0	
0	
0	
0	
1	
1	
1	
0	
3/31/16	

0	
1	
0	
0	
0	
0	
0	
1	

0	
0	
1	
0	
0	
0	
0	
1	

9	

problems	with	simple	cooccurrence	vectors	

increase	in	size	with	vocabulary	
	
very	high	dimensional:	require	a	lot	of	storage	
	
subsequent	classi   ca4on	models	have	sparsity	issues	
	
  	models	are	less	robust	

10	

richard	socher	

3/31/16	

soludon:	low	dimensional	vectors	
      

idea:	store	   most   	of	the	important	informa4on	in	a	
   xed,	small	number	of	dimensions:	a	dense	vector	

       usually	around	25	   	1000	dimensions	

       how	to	reduce	the	dimensionality?	

11	

richard	socher	

3/31/16	

modeling word meaning using lexical co-occurrence

method	1:	dimensionality	reducdon	on	x	

rohde, gonnerman, plaut

singular	value	decomposi4on	of	cooccurrence	matrix	x.		
	

rohde, gonnerman, plaut

m

r

r

n

n

m

x
m

x
= n
m

= n

r

uuu1

2

3

.

..

=

u
k

n

n

1

2

s s s
0

3

. .

0
.

r

0
.

s

r

r

k

s
k

1

s s s2
0

.

3

0
.

r

..

3

2

1

3

2

. .

.
uuu1
r
s s s
u
k
0
s
..
uuu1
3
k
s s s2
u
0

k

.

3

1

2

n

r

k

m
s
r
v
1
vv
2
.
3
..
v t
sk
m
v
1
vv
2
..
3
v t

3

2

k

n

sk

..

s

x

0
.

uuu1

x
=
figure 1: the singular value decomposition of matrix x.
  x is the best rank k approximation to x, in terms of least
squares.
is	the	best	rank	k	approxima4on	to	x	,	in	terms	of	least	squares.		
	

figure 1: the singular value decomposition of matrix x.
  x is the best rank k approximation to x, in terms of least
squares.

tropy of the document distribution of row vector a. words
that are evenly distributed over documents will have high
id178 and thus a low weighting, re   ecting the intuition

richard	socher	

u

s

12	

table 3. it is unclear whether the vectors are    rst scaled
by the singular values, s, before computing the cosine,
as implied in deerwester, dumais, furnas, landauer, and
harshman (1990).

modeling word meaning using lexical co-occurrence
m
v
1
vv
2
.
3
..
table 3. it is unclear whether the vectors are    rst scaled
v t
by the singular values, s, before computing the cosine,
as implied in deerwester, dumais, furnas, landauer, and
m
harshman (1990).
v
1
vv
2
computing the svd itself is not trivial. for a dense
..
3
matrix with dimensions n < m, the svd computation
v t
requires time proportional to n2m. this is impractical
for matrices with more than a few thousand dimensions.
however, lsa co-occurrence matrices tend to be quite
sparse and the svd computation is much faster for sparse
matrices, allowing the model to handle hundreds of thou-
sands of words and documents. the lsa similarity rat-
ings tested here were generated using the term-to-term
pairwise comparison interface available on the lsa web
site (http://lsa.colorado.edu).2 the model was trained on

computing the svd itself is not trivial. for a dense
matrix with dimensions n < m, the svd computation
requires time proportional to n2m. this is impractical
for matrices with more than a few thousand dimensions.
however, lsa co-occurrence matrices tend to be quite
sparse and the svd computation is much faster for sparse
matrices, allowing the model to handle hundreds of thou-
sands of words and documents. the lsa similarity rat-
ings tested here were generated using the term-to-term
pairwise comparison interface available on the lsa web
site (http://lsa.colorado.edu).2 the model was trained on
the touchstoneapplied science associates (tasa)    gen-
eral reading up to    rst year college    data set, with the top
300 dimensions retained.

3/31/16	

simple	svd	word	vectors	in	python	

corpus:		
i	like	deep	learning.	i	like	nlp.	i	enjoy	   ying.	
	

13	

richard	socher	

3/31/16	

simple	svd	word	vectors	in	python	
corpus:	i	like	deep	learning.	i	like	nlp.	i	enjoy	   ying.	
prin4ng	   rst	two	columns	of	u	corresponding	to	the	2	biggest	singular	values	

14	

richard	socher	

3/31/16	

word	meaning	is	de   ned	in	terms	of	vectors	

      

in	all	subsequent	models,	including	deep	learning	models,	a	
word	is	represented	as	a	dense	vector	

	
	
	
	
linguis,cs		=	

0.286	
0.792	
   0.177	
   0.107	
0.109	
   0.542	
0.349	
0.271	

15	

hacks	to	x	

       problem:	func4on	words	(the,	he,	has)	are	too	

frequent	  	syntax	has	too	much	impact.	some	   xes:		
       min(x,t),	with	t~100	
      

ignore	them	all	

       ramped	windows	that	count	closer	words	more	
       use	pearson	correla4ons	instead	of	counts,	then	set	

nega4ve	values	to	0	

       +++	

16	

richard	socher	

3/31/16	

figure 8: multidimensional scaling for three noun classes.

interesdng	semandc	papers	emerge	in	the	vectors	

wrist
ankle
shoulder
arm
leg
hand
head

foot
nose
finger
toe
face
ear

eye

tooth

dog
cat

puppy
kitten
cow
turtle

mouse

oyster

lion
bull
chicago
atlanta

montreal
nashville
tokyo

china
russia
africa
asia
europe

america

brazil
france

moscow

hawaii

figure 9: hierarchical id91 for three noun classes using distances based on vector correlations.
an	improved	model	of	seman4c	similarity	based	on	lexical	co-occurrence		
rohde	et	al.	2005	
	

richard	socher	
20

3/31/16	

17	

interesdng	syntacdc	papers	emerge	in	the	vectors	

choosing

choose

chosen

chose

spoke

spoken

speak
speaking

shown

showed
showing

show

steal

stolen
stole
stealing

throw
thrownthrowing

threw

eaten
eat

ate
eating

taken

take
taking
took

grown

grow

grew

growing

figure 11: multidimensional scaling of present, past, progressive, and past participle forms for eight verb families.

an	improved	model	of	seman4c	similarity	based	on	lexical	co-occurrence		
rohde	et	al.	2005	
	

18	

richard	socher	

22

3/31/16	

rohde, gonnerman, plaut

interesdng	semandc	papers	emerge	in	the	vectors	

modeling word meaning using lexical co-occurrence

drive

clean

swim

driver

swimmer

janitor

student

teacher

doctor

bride

priest

learn

teach

marry

treat

pray

figure 13: multidimensional scaling for nouns and their associated verbs.

an	improved	model	of	seman4c	similarity	based	on	lexical	co-occurrence		
rohde	et	al.	2005	
	

table 10
the 10 nearest neighbors and their percent correlation similarities for a set of nouns, under the coals-14k model.
19	

3/31/16	

gun

1) 46.4 handgun

point
32.4 points

mind
33.5 minds

cardboard

47.4 plastic

lipstick
42.9 shimmery

leningrad

24.0 moscow

feet

59.5 inches

richard	socher	
monopoly
39.9 monopolies

problems	with	svd	

computa4onal	cost	scales	quadra4cally		for	n	x	m	matrix:	
o(mn2)	   ops	(when	n<m)		
  	bad	for	millions	of	words	or	documents	
	
hard	to	incorporate	new	words	or	documents	
di   erent	learning	regime	than	other	dl	models	
	
	

richard	socher	

3/31/16	

20	

idea:	directly	learn	low-dimensional	word	vectors	
       old	idea.	relevant	for	this	lecture	&	deep	learning:	
learning	representa4ons	by	back-propaga4ng	errors.	
(rumelhart	et	al.,	1986)	

      

       a	neural	probabilis4c	language	model	(bengio	et	al.,	2003)			
       nlp	(almost)	from	scratch	(collobert	&	weston,	2008)	
       a	recent,	even	simpler	and	faster	model:		

id97	(mikolov	et	al.	2013)	  	intro	now	

	

21	

richard	socher	

3/31/16	

instead	of	capturing	cooccurrence	counts	directly,	

main	idea	of	id97	
      
       predict	surrounding	words	of	every	word		
       both	are	quite	similar,	see	   glove:	global	vectors	for	
word	representa,on   	by	pennington	et	al.	(2014)	and	
levy	and	goldberg	(2014)	   	more	later	

       faster	and	can	easily	incorporate	a	new	sentence/

document	or	add	a	word	to	the	vocabulary	

22	

richard	socher	

3/31/16	

details	of	id97	
       predict	surrounding	words	in	a	window	of	length	m	of	

every	word.	

       objec4ve	func4on:	maximize	the	log	id203	of	

any	context	word	given	the	current	center	word:	

      

		

       where	  	represents	all	variables	we	op4mize	

23	

richard	socher	

3/31/16	

training time. the basic skip-gram formulation de   nes p(wt+j|wt) using the softmax function:

details	of	id97	

       predict	surrounding	words	in	a	window	of	length	m	of	every	

word	

for																						the	simplest	   rst	formula4on	is		

      
exp!v   
w=1 exp!v   
#w
       where	o	is	the	outside	(or	output)	word	id,	c	is	the	center	word	

   vwi"
   vwi"

(2)

wo

w

w are the    input    and    output    vector representations of w, and w is the num-
ber of words in the vocabulary. this formulation is impractical because the cost of computing
    log p(wo|wi ) is proportional to w, which is often large (105   107 terms).

id,	u	and	v	are	   center   	and	   outside   	vectors	of	o	and	c	

      
      

every	word	has	two	vectors!	

this	is	essen4ally	   dynamic   	logis4c	regression	

a computationally ef   cient approximation of the full softmax is the hierarchical softmax. in the
context of neural network language models, it was    rst introduced by morin and bengio [12]. the
main advantage is that instead of evaluating w output nodes in the neural network to obtain the

richard	socher	

24	

3/31/16	

cost/objecdve	funcdons	

we	will	op4mize	(maximize	or	minimize)		
our	objec4ve/cost	func4ons	
	
for	now:	minimize	  	gradient	descent	
	
refresher	with	trivial	example:	(from	wikipedia)	
find	a	local	minimum	of	the	func4on		
f(x)=x4   3x3+2,	with	deriva4ve	f'(x)=4x3   9x2.		
	
	
	

25	

richard	socher	

3/31/16	

derivadons	of	gradient	

       whiteboard	(see	video	if	you   re	not	in	class	;)	
       the	basic	lego	piece	
       useful	basics:	
      

if	in	doubt:	write	out	with	indices	

       chain	rule!	if	y	=	f(u)	and	u	=	g(x),	i.e.	y=f(g(x)),	then:	

26	

richard	socher	

3/31/16	

chain	rule	

       chain	rule!	if	y	=	f(u)	and	u	=	g(x),	i.e.	y=f(g(x)),	then:	

       simple	example:		

27	

richard	socher	

3/31/16	

interacdve	whiteboard	session!	

let   s	derive	gradient	together	
for	one	example	window	and	one	example	outside	word:	

28	

richard	socher	

3/31/16	

approximadons:	pset	1	
       with	large	vocabularies	this	objec4ve	func4on	is	not	

scalable	and	would	train	too	slowly!	  	why?	

idea:	approximate	the	normaliza4on	or		

      
       de   ne	nega4ve	predic4on	that	only	samples	a	few	

words	that	do	not	appear	in	the	context	

       similar	to	focusing	on	mostly	posi4ve	correla4ons	
       you	will	derive	and	implement	this	in	pset	1!	

29	

richard	socher	

3/31/16	

linear	reladonships	in	id97	

these	representa4ons	are	very	good	at	encoding	dimensions	of	
similarity!	
       analogies	tes4ng	dimensions	of	similarity	can	be	solved	quite	
well	just	by	doing	vector	subtrac4on	in	the	embedding	space	
syntac4cally	
       xapple	   	xapples	   	xcar	   	xcars	   	xfamily	   	xfamilies		

       similarly	for	verb	and	adjec4ve	morphological	forms	
seman4cally	(semeval	2012	task	2)	
       xshirt	   	xclothing	   	xchair	   	xfurniture		
       xking	   	xman	   	xqueen	   	xwoman		

30	

count	based	vs	direct	predicdon	

lsa, hal (lund & burgess), 
coals (rohde et al),    
hellinger-pca (lebret & collobert)	

       fast training	
       ef   cient usage of statistics	

       primarily used to capture word 
similarity	
       disproportionate importance 
given to large counts	

       nnlm, hlbl, id56, skip-
gram/cbow, (bengio et al; collobert 
& weston; huang et al; mnih & hinton; 
mikolov et al; mnih & kavukcuoglu)	

       scales with corpus size	
       inef   cient usage of statistics	
       generate improved performance    
on other tasks	

       can capture complex patterns    
beyond word similarity 	

31	

richard	socher	

3/31/16	

combining	the	best	of	both	worlds:	glove	

      fast	training	
      scalable	to	huge	corpora	
      good	performance	even	with	small	corpus,	and	small	
vectors	

	

32	

richard	socher	

3/31/16	

glove	results	

nearest	words	to		
frog:	

1.	frogs	
2.	toad	
3.	litoria	
4.	leptodactylidae	
5.	rana	
6.	lizard	
7.	eleutherodactylus	

litoria	

leptodactylidae	

33	

rana	

eleutherodactylus	

richard	socher	

3/31/16	

word	analogies	

test	for	linear	rela4onships,	examined	by	mikolov	et	al.	(2014)	

a:b	::	c:?	
a:b	::	c:?	

man:woman	::	king:?	

+	

-	

+	

king	

[	0.30	0.70	]	

man	
woman	

[	0.20	0.20	]	
[	0.60	0.30	]	

queen	

[	0.70	0.80	]	

queen	

king	

woman	

man	

glove	visualizadons	

35	

richard	socher	

3/31/16	

glove	visualizadons:	company	-	ceo	

36	

richard	socher	

3/31/16	

glove	visualizadons:	superladves	

37	

richard	socher	

3/31/16	

word	embedding	matrix	
      

ini4alize	most	word	vectors	of	future	models	with	our	   pre-
trained   	embedding	matrix	
	
	
																																														|v|	
	

[												]	

														   

										n		

	l		=									

	

	

	

				aardvark				a								at			   	

       also	called	a	look-up	table	

       conceptually	you	get	a	word   s	vector	by	lex	mul4plying	a	
one-hot	vector	e	(of	length	|v|)	by	l:					x	=	le	

38	

advantages	of	low	dimensional	word	vectors	

what	is	the	major	bene   t	of	deep	learned	word	vectors?	
ability	to	also	propagate	any	informa4on	into	them	
via	neural	networks	(next	lecture).	
	

p(c | d,  ) =

e  t f (c,d)
e  t f ( !c ,d)
!c   

39	39	

c1											c2												c3			
	

s	

a1	

								a2	

								x2																x3												+1	

x1	
	

advantages	of	low	dimensional	word	vectors	

       word	vectors	will	form	the	basis	for	all	subsequent	

lectures.	

       all	our	seman4c	representa4ons	will	be	vectors!	
       next	lecture:	

some	more	details	about	word	vectors	

      
       predict	labels	for	words	in	context	for	solving	lots	of	

di   erent	tasks	

40	

