cs224d
deep	nlp

lecture	8:

recurrent	neural	networks

richard	socher

richard@metamind.io

overview

   
   
   
   
   

   
   

2

feedback
traditional	language	models
id56s
id56	language	models
important	training	problems	and	tricks
   
vanishing	and	exploding	gradient	problems
id56s	for	other	sequence	tasks
bidirectional	and	deep	id56s

richard	socher

4/21/16

feedback

3

richard	socher

4/21/16

feedback	   super	useful	   thanks!

explain	the	intuition	behind	the	math	and	models	more	
   some	today	:)
give	more	examples,	more	toy	examples	and	recap	slides	can	help	us	
understand	faster
   some	toy	examples	today.	recap	of	main	concepts	next	week
consistency	issues	in	dimensionality,	row	vs column,	etc.
   all	vectors	should	be	column	vectors	   	unless	i	messed	up,	please	send	
errata
i	like	the	quality	of	the	problem	sets	and	especially	the	starter	code.	it	would	be	
nice	to	include	ballpark	values	we	should	expect
   will	add	in	future	psets and	on	piazza.	we   ll	also	add	dimensionality.

4

richard	socher

4/21/16

feedback	on	project

please	give	list	of	proposed	projects

  
    great	feedback,	i	asked	research	groups	at	stanford	

and	will	compile	a	list	for	next	tuesday.

    we   ll	move	project	proposal	deadline	to	next	week	

thursday.

    extra	credit	deadline	for	dataset	+	first	baseline	is	for	

project	milestone.

5

richard	socher

4/21/16

language	models

a	language	model	computes	a	id203	for	a	sequence	
of	words:
    useful	for	machine	translation

    word	ordering:

p(the	cat	is	small)	>	p(small	the	is	cat)

    word	choice:

p(walking	home	after	school)	>	p(walking	house	after	
school)

6

richard	socher

4/21/16

traditional	language	models

    id203	is	usually	conditioned	on	window	of	n	

previous	words

    an	incorrect	but	necessary	markov	assumption!

    to	estimate	probabilities,	compute	for	unigrams	and	
bigrams	(conditioning	on	one/two	previous	word(s):

7

richard	socher

4/21/16

traditional	language	models

    performance	improves	with	keeping	around	higher	n-

grams	counts	and	doing	smoothing	and	so-called	
backoff (e.g.	if	4-gram	not	found,	try	3-gram,	etc)

    there	are	a	lot	of	id165s!

   gigantic	ram	requirements!	

    recent	state	of	the	art:	scalable	modified	kneser-ney	

language	model	estimation by	heafield et	al.:	
   using	one	machine	with	140	gb	ram	for	2.8	days,	
we	built	an	unpruned model	on	126	billion	tokens   

8

richard	socher

4/21/16

recurrent	neural	networks!

    id56s	tie	the	weights	at	each	time	step
    condition	the	neural	network	on	all	previous	words
    ram	requirement	only	scales	with	number	of	words

yt   1

yt

yt+1

ht   1

xt   1

ht

w

xt

ht+1

w

xt+1

9

richard	socher

4/21/16

recurrent	neural	network	language	model

given	list	of	word	vectors:
at	a	single	time	step:

    

xt

ht

10

richard	socher

4/21/16

recurrent	neural	network	language	model

main	idea:	we	use	the	same	set	of	w	weights	at	all	time	
steps!
everything	else	is	the	same:

is	some	initialization	vector	for	the	hidden	layer	

at	time	step	0

is	the	column	vector	of	l	at	index	[t]	at	time	step	t

recurrent	neural	network	language	model

is	a	id203	distribution	over	the	vocabulary

same	cross	id178	loss	function	but	predicting	words	
instead	of	classes

12

richard	socher

4/21/16

recurrent	neural	network	language	model

evaluation	could	just	be	negative	of	average	log	
id203	over	dataset	of	size	(number	of	words)	t:

but	more	common:	perplexity:				2j
lower	is	better!

13

richard	socher

4/21/16

training	id56s	is	hard
    multiply	the	same	matrix	at	each	time	step	during	forward	prop

yt   1

yt

yt+1

ht   1

xt   1

ht

w

xt

ht+1

w

xt+1

   
ideally	inputs	from	many	time	steps	ago	can	modify	output	y
    take										for	an	example	id56	with	2	time	steps!	insightful!

lecture	1,	slide	14

richard	socher

4/21/16

the	vanishing/exploding	gradient	problem
    multiply	the	same	matrix	at	each	time	step	during	backprop

yt   1

yt

yt+1

ht   1

xt   1

ht

w

xt

ht+1

w

xt+1

lecture	1,	slide	15

richard	socher

4/21/16

the	vanishing	gradient	problem	- details
    similar	but	simpler	id56	formulation:

    total	error	is	the	sum	of	each	error	at	time	steps	t

    hardcore	chain	rule	application:

lecture	1,	slide	16

richard	socher

4/21/16

the	vanishing	gradient	problem	- details
    similar	to	backprop but	less	efficient	formulation
    useful	for	analysis	we   ll	look	at:

    remember:
    more	chain	rule,	remember:

    each	partial	is	a	jacobian:

lecture	1,	slide	17

richard	socher

4/21/16

the	vanishing	gradient	problem	- details
    from	previous	slide:	

ht   1

ht

    remember:

    to	compute	jacobian,	derive	each	element of	matrix:	

    where:

check	at	home	
that	you	understand
the	diag matrix	
formulation

lecture	1,	slide	18

richard	socher

4/21/16

the	vanishing	gradient	problem	- details
    analyzing	the	norms	of	the	jacobians,	yields:

    where	we	defined	     s	as	upper	bounds	of	the	norms
    the	gradient	is	a	product	of	jacobian matrices,	each	associated	

with	a	step	in	the	forward	computation.	

    this	can	become	very	small	or	very	large	quickly	[bengio et	al	
1994],	and	the	locality	assumption	of	gradient	descent	breaks	
down.	   vanishing	or	exploding	gradient

lecture	1,	slide	19

richard	socher

4/21/16

why	is	the	vanishing	gradient	a	problem?
    the	error	at	a	time	step	ideally	can	tell	a	previous	time	step	

from	many	steps	away	to	change	during	backprop

yt   1

yt

yt+1

ht   1

xt   1

ht

w

xt

ht+1

w

xt+1

lecture	1,	slide	20

richard	socher

4/21/16

the	vanishing	gradient	problem	for	language	models
   

in	the	case	of	language	modeling	or	question	answering	words	
from	time	steps	far	away	are	not	taken	into	consideration	when	
training	to	predict	the	next	word

    example:	

jane	walked	into	the	room.	john	walked	in	too.	it	was	late	in	the	
day.	jane	said	hi	to	____

lecture	1,	slide	21

richard	socher

4/21/16

ipython notebook	with	vanishing	gradient	example
    example	of	simple	and	clean	nnet implementation	

    comparison	of	sigmoid	and	relu units

    a	little	bit	of	vanishing	gradient

lecture	1,	slide	22

richard	socher

4/21/16

lecture	1,	slide	23

richard	socher

4/21/16

is learned). there are several variations of this basic
structure. this solution does not address explicitly the

trick	for	exploding	gradient:	clipping	trick

nents element-wise (clipping an entry when it exceeds
in absolute value a    xed threshold). clipping has been
shown to do well in practice and it forms the backbone
of our approach.

    the	solution	first	introduced	by	mikolov is	to	clip	gradients

3.2. scaling down the gradients
as suggested in section 2.3, one simple mechanism to
deal with a sudden increase in the norm of the gradi-
ents is to rescale them whenever they go over a thresh-
old (see algorithm 1).

to	a	maximum	value.	

sutskever et al. (2011) use the hessian-free opti-
mizer in conjunction with structural damping, a spe-
ci   c damping strategy of the hessian. this approach
seems to deal very well with the vanishing gradient,
though more detailed analysis is still missing. pre-
sumably this method works because in high dimen-
sional spaces there is a high id203 for long term
components to be orthogonal to short term ones. this
would allow the hessian to rescale these components
independently. in practice, one can not guarantee that
this property holds. as discussed in section 2.3, this
method is able to deal with the exploding gradient
as well. structural damping is an enhancement that
forces the change in the state to be small, when the pa-
rameter changes by some small value     . this asks for
@    to have small norm, hence
further helping with the exploding gradients problem.
the fact that it helps when training recurrent neural
models on long sequences suggests that while the cur-
vature might explode at the same time with the gradi-
ent, it might not grow at the same rate and hence not
be su cient to deal with the exploding gradient.

24

echo state networks (luko  sevi  cius and jaeger, 2009)

algorithm 1 pseudo-code for norm clipping the gra-
dients whenever they explode

  g   @e@   
if k  gk   threshold then
end if

  g   threshold

k  gk

  g

    makes	a	big	difference	in	id56s.

this algorithm is very similar to the one proposed by
tomas mikolov and we only diverged from the original
proposal in an attempt to provide a better theoretical
foundation (ensuring that we always move in a de-
scent direction with respect to the current mini-batch),
though in practice both variants behave similarly.

the proposed clipping is simple to implement and
computationally e cient, but it does however in-
troduce an additional hyper-parameter, namely the
threshold. one good heuristic for setting this thresh-

gradient	clipping	intuition

on the di culty of training recurrent neural networks

figure	from	paper:	
on	the	difficulty	of	
training	recurrent	neural	
networks,	pascanu et	al.	
2013

rithm should work even when the rate of growth of the
gradient is not the same as the one of the curvature
(a case for which a second order method would fail
as the ratio between the gradient and curvature could
still explode).

error	surface	of	a	single	hidden	unit	id56,	

   
figure 6. we plot the error surface of a single hidden unit
    high	curvature	walls
recurrent network, highlighting the existence of high cur-
vature walls. the solid lines depicts standard trajectories
   
that id119 might follow. using dashed arrow
the diagram shows what would happen if the gradients is
rescaled to a    xed size when its norm is above a threshold.
    dashed	lines	gradients	rescaled	to	fixed	size
25

solid	lines:	standard	gradient	descent	trajectories	

richard	socher

our hypothesis could also help to understand the re-
cent success of the hessian-free approach compared
to other second order methods. there are two key dif-
ferences between hessian-free and most other second-
order algorithms. first, it uses the full hessian matrix
and hence can deal with exploding directions that are
not necessarily axis-aligned. second, it computes a
new estimate of the hessian matrix before each up-
date step and can take into account abrupt changes in
curvature (such as the ones suggested by our hypothe-
sis) while most other approaches use a smoothness as-
sumption, i.e., averaging 2nd order signals over many
steps.

4/21/16

explode so does the curvature along v, leading to a

3. dealing with the exploding and

it reaches 1,000,000 iterations and report the results in    gure 3 (best hyperparameters are listed in
table 2).

for	vanishing	gradients:	initialization	+	relus!

pixel   by   pixel permuted mnist

lstm
id56 + tanh
id56 + relus
iid56

 

   

100

90

80

70

60

50

40

30

20

10

y
c
a
r
u
c
c
a

 
t
s
e
t

pixel   by   pixel mnist

 

100

lstm
id56 + tanh
id56 + relus
iid56

initialize	w(*)   s	to
identity	matrix	i
and
f(z)		=

rect(z) = max(z,0)

y
c
a
r
u
c
c
a

 
t
s
e
t

90

80

70

60

50

40

30

20

10

       huge	difference!

0

 
0

1

2

3

4

5

steps

6

7

8

9

10
x 105

0

 
0

1

2

3

4

5

steps

6

7

8

9

10
x 105

   

initialization	idea	first	introduced	in	parsing	with	compositional	
figure 3: the results of recurrent methods on the    pixel-by-pixel mnist    problem. we report the
vector	grammars,	socher	et	al.	2013
test set accuracy for all methods. left: normal mnist. right: permuted mnist.

    new	experiments	with	recurrent	neural	nets	2	weeks	ago	(!)	in	

id56 + tanh
a	simple	way	to	initialize	recurrent	networks	of	rectified	
linear	units,	le	et	al.	2015
lr = 10   8, gc = 10

id56 + relus
lr = 10   8, gc = 10

iid56
lr = 10   8, gc = 1

lr = 10   8, gc = 1

richard	socher

lr = 10   6, gc = 10

4/21/16
lr = 10   9, gc = 1

problem lstm
mnist
permuted
26
mnist

lr = 0.01, gc = 1
f b = 1.0
lr = 0.01, gc = 1
f b = 1.0

perplexity	results

kn5	=	count-based	language	model	with	kneser-ney	
smoothing	&	5-grams

table	from	paper	extensions	of	recurrent	neural	network	
language	model by	mikolov et	al	2011

27

richard	socher

4/21/16

problem:	softmax is	huge	and	slow

trick:	class-based	word	prediction
p(wt|history)	

=	p(ct|history)p(wt|ct)
=	p(ct|ht)p(wt|ct)

the	more	classes,
the	better	perplexity
but	also	worse	speed:

28

richard	socher

4/21/16

one	last	implementation	trick

    you	only	need	to	pass	backwards	through	your	

sequence	once	and	accumulate	all	the	deltas	from	
each	et

29

richard	socher

4/21/16

sequence	modeling	for	other	tasks

    classify	each	word	into:	

    ner
   
   

entity	level	sentiment	in	context	
opinionated	expressions

    example	application	and	slides	from	paper	opinion	
mining	with	deep	recurrent	nets	by	irsoy and	cardie
2014

30

richard	socher

4/21/16

opinion	mining	with	deep	recurrent	nets	

goal:	classify	each	word	as
direct	subjective	expressions	(dses)	and	
expressive	subjective	expressions	(eses).	
dse:	explicit	mentions	of	private	states	or	speech	events	
expressing	private	states	
ese:	expressions	that	indicate	sentiment,	emotion,	etc.	
without	explicitly	conveying	them.	

31

richard	socher

4/21/16

example	annotation

in	bio	notation	(tags	either	begin-of-entity	(b_x)	or	
continuation-of-entity	(i_x)):
the	committee,	[as	usual]ese,	[has	refused	to	make	any	
statements]dse.	

32

richard	socher

4/21/16

approach:	recurrent	neural	network

    notation	from	paper	(so	you	get	used	to	different	ones)

recurrent neural network 
y

h

ht = f (wxt +vht   1 +b)
yt = g(uht +c)

x
x
    represents a token (word) as a vector. 
y
    represents the output label (b, i or o). 
h
    is the memory, computed from the past memory and 
current word. it summarizes the sentence up to that time. 

    x	represents	a	token	(word)	as	a	vector.	
    y	represents	the	output	label	(b,	i	or	o)     g	=	softmax !
    h	is	the	memory,	computed	from	the	past	memory	and	current	

word.	it	summarizes	the	sentence	up	to	that	time.

33

richard	socher

4/21/16

bidirectional	id56s

problem:	for	classification	you	want	to	incorporate	
bidirectionality 
information	from	words	both	preceding	and	following
y
ideas?

h

!"!
!
!"
t = f (w
h
xt +v
!""
!
!"
t = f (w
h
xt +v
!
!
t;h
yt = g(u[h

!
h
!
h
t]+c)

!)
t   1 +b
!)
t+1 +b

x
!;h
!]
h =[h
                 now represents (summarizes) the past and future 
around a single token. 
34

richard	socher

4/21/16

deep	bidirectional	id56s
going deep 
y

h(3)

h(2)

h(1)

!(i)
!"! (i)ht
t = f (w
h
(i   1) +v
!(i)
!"" (i)ht
t = f (w
h
(i   1) +v
!
!
(l);h
yt = g(u[h

!(i)
!"(i)h
!(i))
t   1 +b
!(i)
!"(i)h
!(i))
t+1 +b
(l)]+c)

t

t

x
each memory layer passes an intermediate sequential 
representation to the next. 

35

richard	socher

4/21/16

data

    mpqa	1.2	corpus	(wiebe et	al.,	2005)	
   
consists	of	535	news	articles	(11,111	sentences)	
    manually	labeled	with	dse	and	eses	at	the	phrase	

level	

    evaluation:	f1

36

richard	socher

4/21/16

evaluation

 

f
p
o
r
p

63 
61 
59 
57 

 
1
f
 
n
b

i

74 
72 
70 
68 
66 
64 

53 
51 
49 
47 

68 
66 
64 
62 
60 
58 

1 

2 

3 

4 

5 

1 

2 

3 

4 

5 

# layers 

# layers 

results: deep vs shallow id56s 

67 
65 
63 
37
61 
59 

 

 
1
f
p
o
r
p

dse 

57 
55 
53 
51 
49 

richard	socher

ese 

4/21/16

24k 
200k 

recap

    recurrent	neural	network	is	one	of	the	best	deepnlp

model	families

    training	them	is	hard	because	of	vanishing	and	

exploding	gradient	problems

    they	can	be	extended	in	many	ways	and	their	training	

improved	with	many	tricks	(more	to	come)

    next	week:	most	important	and	powerful	id56	

extensions	with	lstms	and	grus

38

richard	socher

4/21/16

