cs224d:	deep	nlp

lecture	9:

wrap	up:	lstms

and

recursive	neural	networks

richard	socher

richard@metamind.io

overview

video	issues	and	fire	alarm
finish	lstms
recursive	neural	networks
    motivation:	compositionality
   

structure	prediction:	parsing

   

   

id26 through	structure

vision	example

next	lecture:
   

rvnn improvements

2

richard	socher

4/29/16

long-short-term-memories	(lstms)
    we	can	make	the	units	even	more	complex
    allow	each	time	step	to	modify	
input	gate	(current	cell	matters)
forget	(gate	0,	forget	past)

   
   
    output	(how	much	cell	is	exposed)
    new	memory	cell
final	memory	cell:
final	hidden	state:	

   
   

3

richard	socher

4/29/16

illustrations	all	a	bit	overwhelming	;)

net c j

w ic j

g

yinj
w i
inj

g+

=
sc j sc j
1.0

g yinj

yinj
h

yc j

h yout j

net

inj

yout j

w i
out j

wic j

net

out j

long	short-term	memory	by	hochreiter and	schmidhuber (1997)

http://people.idsia.ch/~juergen/lstm/sld017.htm

http://deeplearning.net/tutorial/lstm.html

intuition:	memory	cells	can	keep	information	 intact,	unless	inputs	makes	them
forget	it	or	overwrite	it	with	new	input.
cell	can	decide	to	output	this	information	or	just	store	it

4

richard	socher

4/29/16

lstms	are	currently	very	hip!

    en	vogue	default	model	for	most	sequence	labeling	

tasks

    very	powerful,	especially	when	stacked	and	made	

even	deeper	(each	hidden	layer	is	already	computed	
by	a	deep	internal	network)

    most	useful	if	you	have	lots	and	lots	of	data

5

richard	socher

4/29/16

deep	lstms	compared	to	traditional	systems	2015

method

bahdanau et al. [2]
baseline system [29]

single forward lstm, beam size 12
single reversed lstm, beam size 12

ensemble of 5 reversed lstms, beam size 1
ensemble of 2 reversed lstms, beam size 12
ensemble of 5 reversed lstms, beam size 2
ensemble of 5 reversed lstms, beam size 12

test id7 score (ntst14)

28.45
33.30
26.17
30.59
33.00
33.27
34.50
34.81

table 1: the performance of the lstm on wmt   14 english to french test set (ntst14). note that
an ensemble of 5 lstms with a beam of size 2 is cheaper than of a single lstm with a beam of
size 12.

method

baseline system [29]

cho et al. [5]

best wmt   14 result [9]

rescoring the baseline 1000-best with a single forward lstm
rescoring the baseline 1000-best with a single reversed lstm

rescoring the baseline 1000-best with an ensemble of 5 reversed lstms

oracle rescoring of the baseline 1000-best lists

test id7 score (ntst14)

33.30
34.54
37.0
35.61
35.85
36.5
   45

table 2: methods that use neural networks together with an smt system on the wmt   14 english
6
to french test set (ntst14).

sequence	to	sequence	learning	by	sutskever et	al.	2014	

deep	lstms	(with	a	lot	more	tweaks)	today
wmt	2016	competition	results	from	yesterday

7

richard	socher

4/29/16

we were surprised to discover that the lstm did well on long sentences, which is shown quantita-
tively in    gure 3. table 3 presents several examples of long sentences and their translations.

deep	lstm	for	machine	translation

3.8 model analysis

pca	of	vectors	from	last	time	step	hidden	layer

4

3

2

1

0

   1

   2

   3

   4

   5

   6
   8

mary admires john

mary is in love with john

mary respects john

john admires mary

john is in love with mary

john respects mary

   6

   4

   2

0

2

4

6

8

10

15

10

5

0

   5

   10

   15

   20

   15

i was given a card by her in the garden

in the garden , she gave me a card

she gave me a card in the garden

she was given a card by me in the garden

in the garden , i gave her a card

i gave her a card in the garden

   10

   5

0

5

10

15

20

figure 2: the    gure shows a 2-dimensional pca projection of the lstm hidden states that are obtained
after processing the phrases in the    gures. the phrases are clustered by meaning, which in these examples is
primarily a function of word order, which would be dif   cult to capture with a bag-of-words model. notice that
both clusters have similar internal structure.

sequence	to	sequence	learning	by	sutskever et	al.	2014	

8

one of the attractive features of our model is its ability to turn a sequence of words into a vector
of    xed dimensionality. figure 2 visualizes some of the learned representations. the    gure clearly

richard	socher

4/29/16

further	improvements:	more	gates!

gated	feedback	recurrent	neural	networks,	chung	et	al.	2015

gated feedback recurrent neural networks

(a) conventional stacked id56

(b) gated feedback id56

figure 1. illustrations of (a) conventional stacking approach and (b) gated-feedback approach to form a deep id56 architecture. bullets
in (b) correspond to global reset gates. skip connections are omitted to simplify the visualization of networks.

the global reset gate is computed as:

9

gi!j =     wi!j

hj 1

g h   t 1    ,

computed by

richard	socher

t = tanh wj 1!jhj 1

t +

hj

4/29/16

lxi=1

gi!jui!jhi

t 1! ,

summary

    recurrent	neural	networks	are	powerful
    a	lot	of	ongoing	work	right	now
    gated	recurrent	units	even	better
   
    this	was	an	advanced	lecture	   gain	intuition,	

lstms	maybe	even	better	(jury	still	out)

encourage	exploration

    next	up:	recursive	neural	networks

simpler	and	also	powerful	:)

10

richard	socher

4/29/16

building	on	word	vector	space	models

x2
5
4
3
2
1

germany

1
3
france 2
2.5

1
5

1.1
4

monday
tuesday

9
2

9.5
1.5

0								1						2					3					4						5					6					7					8						9					10

x1

the	country	of	my	birth
the	place	where	i	was	born

but	how	can	we	represent	the	meaning	of	longer	phrases?
by	mapping	them	into	the	same	vector	space!

11

semantic	vector	spaces

vectors	representing
phrases	and	sentences
that	do	not	ignore	word	order
and	capture	semantics	for	nlp	tasks

single	word	vectors
    distributional	techniques
    brown	clusters
    useful	as	features	inside	

models,	e.g.	crfs	for	ner,	etc.
    cannot	capture	longer	phrases

documents	vectors
    bag	of	words	models
    pca	(lsa, lda)
    great	for	ir,	document	

exploration,	etc.
ignore	word	order,	no	
detailed	understanding

   

how	should	we	map	phrases	into	a	vector	space?

use	principle	of	compositionality
the	meaning	(vector)	of	a	sentence	
is		determined	by	
(1) the	meanings	of	its	words	and
(2) the	rules	that	combine	them.

1
5

5.5
6.1

2.5
3.8

2.1
3.3

4
4.5
country							of							 my		

7
7

1
3.5

0.4
0.3
the		

the	country	of	my	birth

the	place	where	i	was	born

germany

france

x2
5

4

3

2

1

0								1						2							3						4						5						6						7							8						9					10

monday
tuesday

x1

models	in	this	section	
can	jointly	learn	parse	
trees	and	compositional	
vector	representations

13

2.3
3.6
birth

sentence	parsing:	what	we	want

s

vp

pp

np

np

9
1
the																cat														sat														on														the															mat.

5
3

8
5

9
1

7
1

4
3

14

learn	structure	and	representation

5
4

s

vp

7
3

8
3

pp

5
2

np

3
3

np

9
1
the															cat														sat															on														the															mat.

5
3

8
5

9
1

7
1

4
3

15

learn	structure	and	representation?

    do	we	really	need	to	learn	this	structure?

lecture	1,	slide	16

richard	socher

4/29/16

sidenote:	recursive	vs recurrent	neural	networks

1
5

5.5
6.1

2.5
3.8

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

1
5

5.5
6.1

4.5
3.8

2.5
3.8

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

4/29/16

1
3.5

0.4
0.3
the		

1
3.5

0.4
0.3
the		

sidenote:	are	languages	recursive?
    cognitively	debatable
    but:	recursion	helpful	in	describing	natural	language
    example:	   the	church	which	has	nice	windows   ,	a	noun	phrase	

containing	a	relative	clause	that	contains	a	noun	phrases

    arguments	for	now:	1)	helpful	in	disambiguation:

lecture	1,	slide	18

richard	socher

4/29/16

is	recursion	useful?

2)	helpful	for	some	tasks	to	refer	to	specific	phrases:

    john	and	jane	went	to	a	big	festival.	they	enjoyed	the	trip	
and	the	music	there.
       they   :	john	and	jane
       the	trip   :	went	to	a	big	festival
       there   :	big	festival

3)	labeling	less	clear	if	specific	to	only	subphrases

    i	liked	the	bright	screen	but	not	the	buggy	slow	keyboard of	
the	phone.	it	was	a	pain	to	type	with.	it	was	nice	to	look	at.
4)	works	better	for	some	tasks	to	use	grammatical	tree	structure

    this	is	still	up	for	debate.

lecture	1,	slide	19

richard	socher

4/29/16

recursive	neural	networks	for	structure	prediction
inputs:	two	candidate	children   s	representations
outputs:
1. the	semantic	representation	if	the	two	nodes	are	merged.
2. score	of	how	plausible	the	new	node	would	be.

1.3

8
3

neural 
network

8
5

3
3

20

8
3

3
3

9
1

8
5
on											the													mat.

4
3

recursive	neural	network	definition

score		=

1.3

8
3

=	parent

neural 
network

8
5
c1

3
3
c2

21

score		=		utp

p =		tanh(w +	b),

c1
c2

same w parameters	at	all	nodes	
of	the	tree

parsing	a	sentence	with	an	id56

5
2

3.1

0.3

0
1

0.1

2
0

0.4

1
0

3
3

2.3

neural 
network

neural 
network

neural 
network

neural 
network

neural 
network

9
1
the																cat													sat															on														the																mat.

8
5

9
1

7
1

5
3

4
3

22

parsing	a	sentence

2
1

1.1

neural 
network

5
2

2
0

0.4

1
0

0.1

3
3

2.3

neural 
network

neural 
network

neural 
network

9
1
the																cat													sat															on														the																mat.

8
5

9
1

7
1

5
5
3
3

4
3

23

parsing	a	sentence

2
1

1.1

neural 
network

5
2

2
0

0.1

neural 
network

8
3

3.6

neural 
network

3
3

9
1
the																cat													sat															on														the																mat.

8
5

9
1

7
1

5
5
3
3

4
3

24

parsing	a	sentence

5
4

7
3

8
3

5
2

3
3

9
1

5
5
3
3

7
1

8
5

9
1

4
3

the																cat													sat															on														the																mat.

25

max-margin	framework	- details

    the	score	of	a	tree	is	computed	by	

the	sum	of	the	parsing	decision
scores	at	each	node:

1.3

8
3

id56

8
5

3
3

26

max-margin	framework	- details
    similar	to	max-margin	parsing	(taskar et	al.	2004),	a	supervised	

max-margin	objective

    the	loss																penalizes	all	incorrect	decisions

    structure	search	for	a(x)	was	maximally	greedy

    instead:	beam	search	with	chart

27

(58)
+  w (l)
ij
where we used in the    rst line that the top layer is linear. this is a very detailed account of essentially

introduced	by	goller &	k  chler (1996)	

j  (l+1)

@w (l)
ij

i

id26	through	structure
er = a(l)

=  (nl 1)

a(nl 2)
j

@

{z

where the sigmoid derivative from eq. 14 gives f0(z(l)) = (1   a(l))a(l). using that de   nition, we get the
hidden layer backprop derivatives:

ji

i

(57)

w (nl 1)

 (nl)
j

a(nl 2)
j

= 0@
sl+1xj=1
|

i

)

)1a f0(z(nl 1)
}

so, we can write the   errors of all layers l (except the top layer) (in vector format, using the hadamard

principally	the	same	as	general	id26
which in one simpli   ed vector notation becomes:

 (l) =   (w (l))t  (l+1)      f0(z(l)),

@

@w (l) er =  (l+1)(a(l))t +  w (l).
(59)

in summary, the backprop procedure consists of four steps:

7

three	differences	resulting	from	the	recursion	and	tree	structure:

1. apply an input xn and forward propagate it through the network to get the hidden and output

activations using eq. 18.

2. evaluate  (nl) for output units using eq. 42.

1. sum	derivatives	of	w from	all	nodes
2. split	derivatives	at	each	node
3. add	error	messages	from	parent	+	node	itself

3. backpropagate the     s to obtain a  (l) for each hidden layer in the network using eq. 59.

28

4. evaluate the required derivatives with eq. 62 and update all the weights using an optimization
procedure such as conjugate gradient or l-bfgs. cg seems to be faster and work better when
using mini-batches of training data to estimate the derivatives.

bts:	1)	sum	derivatives	of	all	nodes
you	can	actually	assume	it   s	a	different	w at	each	node
intuition	via	example:

if	we	take	separate	derivatives	of	each	occurrence,	we	get	same:

29

bts:	2)	split	derivatives	at	each	node

during	forward	prop,	the	parent	is	computed	using	2	children

8
3

8
5

c1

3
3

c2

p		=		tanh(w							+	b)

c1
c2

hence,	the	errors	need	to	be	computed	wrt each	of	them:

8
3

8
5

c1

3
3

c2

30

where	each	child   s	error	is	n-dimensional

bts:	3)	add	error	messages
    at	each	node:	

    what	came	up	(fprop)	must	come	down	(bprop)
    total	error	messages	   =	error	messages	from	parent	+	error	
message	from	own	score

score

parent

8
3

8
5

c1

3
3

c2

lecture	1,	slide	31

richard	socher

4/29/16

bts	python	code:	forwardprop

lecture	1,	slide	32

richard	socher

4/29/16

|

=  (nl 1)

a(nl 2)
j

{z

bts	python	code:	backprop

where the sigmoid derivative from eq. 14 gives f0(z(l)) = (1   a(l))a(l). using that de   nition, we get the
hidden layer backprop derivatives:
where we used in the    rst line that the top layer is linear. this is a very detailed account of essentially
just the chain rule.

i

so, we can write the   errors of all layers l (except the top layer) (in vector format, using the hadamard

er = a(l)

j  (l+1)

i

+  w (l)
ij

@

@w (l)
ij

product  ):

which in one simpli   ed vector notation becomes:

 (l) =   (w (l))t  (l+1)      f0(z(l)),
@w (l) er =  (l+1)(a(l))t +  w (l).

@

7

in summary, the backprop procedure consists of four steps:

1. apply an input xn and forward propagate it through the network to get the hidden and output

activations using eq. 18.

2. evaluate  (nl) for output units using eq. 42.

3. backpropagate the     s to obtain a  (l) for each hidden layer in the network using eq. 59.

lecture	1,	slide	33

4. evaluate the required derivatives with eq. 62 and update all the weights using an optimization
procedure such as conjugate gradient or l-bfgs. cg seems to be faster and work better when
using mini-batches of training data to estimate the derivatives.

richard	socher

4/29/16

bts:	optimization

    as	before,	we	can	plug	the	gradients	into	a	
standard	off-the-shelf	l-bfgs	optimizer	or	sgd
    best	results	with	adagrad (duchi et	al,	2011):	

    for	non-continuous	objective	use	subgradient
method (ratliff	et	al.	2007)

34

discussion:	simple	id56
    good	results	with	single	matrix	id56	(more	later)
    single	weight	matrix	id56	could	capture	some	

phenomena	but	not	adequate	for	more	complex,	
higher	order	composition	and	parsing	long	sentences

wscore

w

s
p

c1

c2

    the	composition	function	is	the	same	

for	all	syntactic	categories,	punctuation,	etc

solution:	syntactically-untied	id56
    idea:	condition	the	composition	function	on	the	

syntactic	categories,	   untie	the	weights   

    allows	for	different	composition	functions	for	pairs	

of	syntactic	categories,	e.g.	adv	+	adjp,	vp	+	np

    combines	discrete	syntactic	categories	with	

continuous	semantic	information

solution:	compositional	vector	grammars
    problem:	speed.	every	candidate	score	in	beam	

search	needs	a	matrix-vector	product.

    solution:	compute	score	only	for	a	subset	of	trees	

coming	from	a	simpler,	faster	model	(pid18)
    prunes	very	unlikely	candidates	for	speed
    provides	coarse	syntactic	categories	of	the	
children	for	each	beam	candidate

    compositional	vector	grammars:	cvg	=	pid18	+	id56

details:	compositional	vector	grammar
    scores	at	each	node	computed	by	combination	of	

pid18	and	su-id56:

    interpretation:	factoring	discrete	and	continuous	

parsing	in	one	model:

    socher	et	al.	(2013)

related	work	for	recursive	neural	networks	

pollack	(1990):	recursive	auto-associative	memories

previous	recursive	neural	networks	work	by	
goller &	k  chler (1996),	costa	et	al.	(2003) assumed	
fixed	tree	structure	and	used	one	hot	vectors.

hinton	(1990)	and	bottou (2011):	related	ideas	about	
recursive	models	and	recursive	operators	as	smooth	
versions	of	logic	operations

39

related	work	for	parsing

    resulting	cvg	parser	is	related	to	previous	work	that	extends	pid18	

parsers

    klein	and	manning	(2003a)	:	manual	feature	engineering
    petrov et	al.	(2006)	:	learning	algorithm	that	splits	and	merges	

   

syntactic	categories	
lexicalized	parsers	(collins,	2003;	charniak,	2000):	describe	each	
category	with	a	lexical	item

    hall	and	klein	(2012)	combine	several	such	annotation	schemes	in	a	

    cvgs	extend	these	ideas	from	discrete	representations	to	richer	

factored	parser.	

continuous	ones

experiments
    standard	wsj	split,	labeled	f1
    based	on	simple	pid18	with	fewer	states
    fast	pruning	of	search	space,	few	matrix-vector	products
    3.8%	higher	f1,	20%	faster	than	stanford	factored	parser
parser
stanford	pid18, (klein	and	manning,	 2003a)
stanford factored	(klein	and	manning,	 2003b)
factored	pid18s	(hall and	klein,	2012)
collins	(collins, 1997)
ssn	(henderson, 2004)
berkeley parser	(petrov and	klein,	2007)
cvg	(id56)	(socher	et	al., acl	2013)
cvg	(su-id56)	(socher	et	al., acl	2013)
charniak - self	trained (mcclosky et	al.	2006)
charniak - self	trained-reranked (mcclosky et	al.	2006)

85.5
86.6
89.4
87.7
89.4
90.1
85.0
90.4
91.0
92.1

test, all	sentences

su-id56	analysis
    learns	notion	of	soft	head	words

dt-np	

vp-np

analysis	of	resulting	vector	representations

all	the	figures	are	adjusted	for	seasonal	variations
1.	all	the	numbers	are	adjusted	for	seasonal	fluctuations
2.	all	the	figures	are	adjusted	to	remove	usual	seasonal	patterns
knight-ridder	wouldn   t	comment	on	the	offer
1.	harsco	declined	to	say	what	country	placed	the	order
2.	coastal	wouldn   t	disclose	the	terms
sales	grew	almost	7%	to	$unk	m.	from	$unk	m.
1.	sales	rose	more	than	7%	to	$94.9	m.	from	$88.3	m.
2.	sales	surged	40%	to	unk	b.	yen	from	unk	b.

su-id56	analysis
    can	transfer	semantic	information	from	
single	related	example
    train	sentences:

    he	eats	spaghetti	with	a	fork.	
    she	eats	spaghetti	with	pork.	

    test	sentences	

    he	eats	spaghetti	with	a	spoon.	
    he	eats	spaghetti	with	meat.

su-id56	analysis

labeling	in	recursive	neural	networks

    we	can	use	each node   s	

representation	as	features	for	a	
softmax classifier:

    training	similar	to	model	in	part	1	with	

standard	cross-id178	error	+	scores

46

np

softmax
layer

8
3

neural 
network

scene	parsing

   

similar	principle	of	compositionality.
the	meaning	of	a	scene	image	is	
also	a	function	of	smaller	regions,	
    how	they	combine	as	parts	to	form	

larger	objects,
and	how	the	objects	interact.

   

47

algorithm	for	parsing	images

same	recursive	neural	network	as	for	natural	language	parsing!	

(socher	et	al.	icml	2011)

parsing	natural	scene	images
parsing	natural	scene	images

grass

people building

tree

48

semantic		
representations
features
segments

multi-class	segmentation

method
pixel	crf (gould	et	al.,	iccv	2009)
classifier on	superpixel features
region-based	energy (gould	et	al.,	iccv	2009)
local	labelling (tighe &	lazebnik,	eccv	2010)
superpixel mrf	(tighe &	lazebnik, eccv	2010)
simultaneous	mrf	(tighe &	lazebnik,	eccv	2010)
recursive	neural	network
stanford	background	dataset	(gould	et	al.	2009)

49

accuracy

74.3
75.9
76.4
76.9
77.5
77.5
78.1

lecture	1,	slide	50

richard	socher

4/29/16

