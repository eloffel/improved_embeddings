cs224d
deep	nlp

lecture	5:

project	information

+

neural	networks	&	backprop

richard	socher

richard@metamind.io

overview	today:
    organizational	stuff

    project	tips

    from	one-layer	to	multi	layer	neural	network!

    max-margin	loss	and	backprop!	

(this	is	the	hardest	lecture	of	the	quarter)

lecture	5,	slide	2

richard	socher

4/12/16

announcement:	
    1%	extra	credit	for	piazza	participation!
    hint	for	pset1:	understand	math	and	dimensionality,	then	add	

print	statements,	e.g.

    student	survey	sent	out	last	night,	please	give	us	feedback	to	

improve	the	class	:)

lecture	5,	slide	3

richard	socher

4/12/16

class	project
    most	important	(40%)	and	lasting	result	of	the	class
    pset 3	a	little	easier	to	have	more	time
    start	early	and	clearly	define	your	task	and	dataset

    project	types:

implement	a	complex	neural	architecture

1. apply	existing	neural	network	model	to	a	new	task
2.
3. come	up	with	a	new	neural	network	model
4. theory

lecture	5,	slide	4

richard	socher

4/12/16

class	project:	apply	existing	nnets to	tasks

1. define	task:	

   

example:	summarization

2. define	dataset

1. search	for	academic	datasets	

they	already	have	baselines
e.g.:	document	understanding	conference	(duc)	

2. define	your	own	(harder,	need	more	new	baselines)

if	you   re	a	graduate	student:	connect	to	your	research
summarization,	wikipedia:	intro	paragraph	and	rest	of	large	article
be	creative:	twitter,	blogs,	news

lecture	5,	slide	5

richard	socher

4/12/16

   
   

   
   
   

class	project:	apply	existing	nnets to	tasks

3. define	your	metric

   
   

search	online	for	well	established	metrics	on	this	task
summarization:	id8	(recall-oriented	understudy	for	
gisting evaluation)	which	defines	id165	overlap	to	human	
summaries

4. split	your	dataset!
train/dev/test

   
    academic	dataset	often	come	pre-split
    don   t	look	at	the	test	split	until	~1	week	before	deadline!

lecture	5,	slide	6

richard	socher

4/12/16

class	project:	apply	existing	nnets to	tasks

implement	the	simplest	model	(often	logistic	regression	on	
unigrams	and	bigrams)	first
compute	metrics	on	train	and	dev

5. establish	a	baseline

   

   
    analyze	errors
   

if	metrics	are	amazing	and	no	errors:	
done,	problem	was	too	easy,	restart	:)	

6.

implement	existing	neural	net	model	
compute	metric	on	train	and	dev

   
    analyze	output	and	errors
    minimum	bar	for	this	class

lecture	5,	slide	7

richard	socher

4/12/16

class	project:	apply	existing	nnets to	tasks

7. always	be	close	to	your	data!

visualize	the	dataset
collect	summary	statistics
look	at	errors

   
   
   
    analyze	how	different	hyperparameters affect	performance

8. try	out	different	model	variants

   

soon	you	will	have	more	options
    word	vector	averaging	model	(neural	bag	of	words)
   
   
   
   

fixed	window	neural	model
recurrent	neural	network
recursive	neural	network
convolutional	neural	network

lecture	5,	slide	8

richard	socher

4/12/16

class	project:	a	new	model	-- advanced	option

    do	all	other	steps	first	(start	early!)
    gain	intuition	of	why	existing	models	are	flawed

    talk	to	other	researchers,	come	to	my	office	hours	a	lot
    implement	new	models	and	iterate	quickly	over	ideas
    set	up	efficient	experimental	framework
    build	simpler	new	models	first
    example	summarization:

    average	word	vectors	per	paragraph,	then	greedy	search
    implement	language	model	or	autoencoder (introduced	later)
    stretch	goal	for	potential	paper:	generate	summary!

lecture	5,	slide	9

richard	socher

4/12/16

project	ideas
    summarization
    ner,	like	pset 2	but	with	larger	data

natural	language	processing	(almost)	from	scratch,	ronan	collobert,	 jason	weston,	 leon	bottou,	michael	
karlen,	koray kavukcuoglu,	 pavel kuksa,	http://arxiv.org/abs/1103.0398

    simple	question	answering,a neural	network	for	factoid	question	answering	over	

   

paragraphs,	mohit iyyer,	jordan	boyd-graber,	leonardo	claudino,	 richard	socher	and	hal	daum   iii	(emnlp	
2014)
image	to	text	mapping	or	generation,
grounded	compositional	 semantics	for	finding	and	describing	images	with	sentences,	richard	socher,	andrej	
karpathy,	quoc v.	le,	christopher	 d.	manning,	andrew	y.	ng.	(tacl	2014)
or
deep	visual-semantic	alignments	for	generating	image	descriptions,	 andrej	karpathy,	li	fei-fei

    entity	level	sentiment
    use	dl	to	solve	an	nlp	challenge	on	kaggle,

develop	a	scoring	algorithm	for	student-written	short-answer	responses,	 https://www.kaggle.com/c/asap-sas

lecture	5,	slide	10

richard	socher

4/12/16

default	project:	sentiment	classification
   
   

sentiment	on	movie	reviews:	http://nlp.stanford.edu/sentiment/
lots	of	deep	learning	baselines	and	methods	have	been	tried

lecture	1,	slide	11

richard	socher

4/12/16

a	more	powerful	window	classifier
    revisiting	

    xwindow =	[		xmuseums

xin

xparis

xare

xamazing ]

    assume	we	want	to	classify	whether	the	center	word	is	a	

location	or	not

lecture	5,	slide	12

richard	socher

4/12/16

a	single	layer	neural	network
    a	single	layer	is	a	combination	of	a	linear	layer	
and	a	nonlinearity:

    the	neural	activations	a	can	then
be	used	to	compute	some	function
    for	instance,	an	unnormalized score	or	a	
softmax id203	we	care	about:

13

summary:	feed-forward	computation

computing	a	window   s	score	with	a	3-layer	neural	
net:	s	=	score(museums	in	paris	are	amazing	)

14

xwindow =	[		xmuseums

xin

xparis

xare

xamazing]

main	intuition	for	extra	layer

the	layer	learns	non-linear
interactions	between	the	
input	word	vectors.

xwindow =	[		xmuseums

xin

xparis

xare

xamazing]

example:
only	if	   museums    is
first	vector	should
it	matter	that	   in    is
in	the	second	position

15

summary:	feed-forward	computation
    s =	score(museums	in	paris	are	amazing)
    sc =	score(not	all	museums	in	paris)
    idea	for	training	objective:	make	score	of	true	window	
larger	and	corrupt	window   s	score	lower	(until	they   re	
good	enough):	minimize

    this	is	continuous,	can	perform	sgd

16

max-margin	objective	function
    objective	for	a	single	window:

    each	window	with	a	location	at	its	center	should	have	
a	score	+1	higher	than	any	window	without	a	location	
at	its	center

   

xxx		|   1				  |			ooo

    for	full	objective	function:	sum	over	all	training	

windows

17

training	with	id26

assuming	cost	j	is	>	0,	
compute	the	derivatives	of	s and	sc wrt all	the	
involved	variables:	u,	w,	b,	x

18

training	with	id26
    let   s	consider	the	derivative	of	a	single	weight	wij

    this	only	appears	inside	ai
    for	example:	w23 is	only	

used	to	compute	a2

19

s		

u2

a1

a2

w23
b2

x1

x2																	x3

+1

training	with	id26

derivative	of	weight	wij:

s		

u2

a1

a2

w23

x1

x2																	x3

+1

20

training	with	id26

derivative	of	single	weight	wij :

local	error	

signal

local	input	

signal

where																																																		for	logistic	f

21

s		

u2

a1

a2

w23

x1

x2																	x3

+1

training	with	id26

    from	single	weight	wij to	full	w:

s

    we	want	all	combinations	of

i =	1,	2 and j	=	1,	2,	3	   ?
    solution:	outer	product:
where																		is	the	
   responsibility   	or	error	message
coming	from	each	activation	a

22

s		

u2

a1

a2

w23

x1

x2																	x3

+1

training	with	id26
    for	biases	b,	we	get:

s		

u2

a1

a2

w23

x1

x2																	x3

+1

23

training	with	id26

that   s	almost	id26

it   s	simply	taking	derivatives	and	using	the	chain	rule!

remaining	trick:	we	can	re-use	derivatives	computed	for	
higher	layers	in	computing	derivatives	for	lower	layers!

example:	last	derivatives	of	model,	the	word	vectors	in	x

24

training	with	id26

    take	derivative	of	score	with	
respect	to	single	element	of	
word	vector

    now,	we	cannot	just	take	
into	consideration	one	ai
because	each	xj is	connected	
to	all	the	neurons	above	and	
hence	xj influences	the	
overall	score	through	all	of	
these,	hence:

25

re-used	part	of	previous	derivative

training	with	id26
    with																								,what	is	the	full	gradient?	  

    observations:		the	error	message	   that	arrives	at	a	hidden	

layer	has	the	same	dimensionality	as	that	hidden	layer

26

putting	all	gradients	together:
    remember:	full	objective	function	for	each	window	was:	

    for	example:	gradient	for	u:

27

two	layer	neural	nets	and	full	backprop
    let   s	look	at	a	2	layer	neural	network
    same	window	definition	for	x
    same	scoring	function	
    2	hidden	layers	(carefully	not	superscripts	now!)

u

w(2)

w(1)

s

a(3)

a(2)

x

lecture	5,	slide	28

richard	socher

4/12/16

two	layer	neural	nets	and	full	backprop
    fully	written	out	as	one	function:

a(3)

a(2)

w(2)

w(1)

    same	derivation	as	before	for	w(2)	(now	sitting	on	a(1))

s

lecture	5,	slide	29

richard	socher

4/12/16

two	layer	neural	nets	and	full	backprop
    same	derivation	as	before	for	top	w(2)	:

   

in	matrix	notation:

where																																									and	   is	the	element-wise	product			

also	called	hadamard product

    last	missing	piece	for	understanding		general	backprop:

lecture	5,	slide	30

richard	socher

4/12/16

two	layer	neural	nets	and	full	backprop
    last	missing	piece:	

    what   s	the	bottom	layer   s	

error	message	  (2)?

    similar	derivation	to	single	layer	model

    main	difference,	we	already	have																				and	need	to	apply	

the	chain	rule	again	on	

lecture	5,	slide	31

richard	socher

4/12/16

two	layer	neural	nets	and	full	backprop
    chain	rule	for:

    get	intuition	by	deriving														as	if	it	was	a	scalar

   

intuitively,	we	have	to	sum	over	all	the	nodes	coming	into	layer	

    putting	it	all	together:

where we used in the    rst line that the top layer is linear. this is a very detailed account of essentially
just the chain rule.

so, we can write the   errors of all layers l (except the top layer) (in vector format, using the hadamard

product  ):

lecture	5,	slide	32

richard	socher

4/12/16

 (l) =   (w (l))t  (l+1)      f0(z(l)),

= ( (nl))t w (nl 1)

f0(z(nl 1)

)a(nl 2)

i

j

  i

i

)a(nl 2)

two	layer	neural	nets	and	full	backprop
  i
    last	missing	piece:	

=    ( (nl))t w (nl 1)
= 0@
sl+1xj=1
where the sigmoid derivative from eq. 14 gives f0(z(l)) = (1   a(l))a(l). using that de   nition, we get the
hidden layer backprop derivatives:
|

    f0(z(nl 1)
)1a f0(z(nl 1)
}

j  (l+1)

w (nl 1)

a(nl 2)
j

{z

 (nl)
j

(56)

(57)

   

ji

)

j

i

(55)

(58)
+  w (l)
ij
where we used in the    rst line that the top layer is linear. this is a very detailed account of essentially

er = a(l)

i

i

a(nl 2)
j

=  (nl 1)

in	general	for	any	matrix	w(l)	at	internal	
@
layer	l and	any	error	with	id173	er
@w (l)
all	backprop in	standard	multilayer	
ij
neural	networks	boils	down	to	2	equations:

so, we can write the   errors of all layers l (except the top layer) (in vector format, using the hadamard

which in one simpli   ed vector notation becomes:

 (l) =   (w (l))t  (l+1)      f0(z(l)),

@

@w (l) er =  (l+1)(a(l))t +  w (l).
(59)

in summary, the backprop procedure consists of four steps:

    top	and	bottom	layers	have	simpler	  

7

1. apply an input xn and forward propagate it through the network to get the hidden and output

activations using eq. 18.

lecture	5,	slide	33

2. evaluate  (nl) for output units using eq. 42.

richard	socher

4/12/16

3. backpropagate the     s to obtain a  (l) for each hidden layer in the network using eq. 59.

visualization	of	intuition
our first example:  
    let   s	say	we	want	
id26 using error vectors 

with	previous	layer	and	f	=	  

cs224d: deep learning for nlp 

31 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

  (3) 

lecture	1,	slide	34

gradient w.r.t w(2) =   (3)a(2)t 

richard	socher

4/12/16

our first example:  
visualization	of	intuition
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

w(2)t   (3) 

  (3) 

--reusing the   (3) for downstream updates. 
--moving error vector across affine transformation simply requires multiplication with 
the transpose of forward matrix 
--notice that the dimensions will line up perfectly too! 

lecture	5,	slide	35

richard	socher

4/12/16

our first example:  
visualization	of	intuition
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

     (z(2))!w(2)t   (3) 
 
=   (2) 

w(2)t   (3) 

 
--moving error vector across point-wise non-linearity requires point-wise 
multiplication with local gradient of the non-linearity 

lecture	5,	slide	36

richard	socher

4/12/16

our first example:  
visualization	of	intuition
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

w(1)t   (2) 

  (2) 

gradient w.r.t w(1) =   (2)a(1)t 

lecture	5,	slide	37

richard	socher

4/12/16

id26 (another	explanation)
    compute	gradient	of	example-wise	loss	wrt

parameters	

simply	applying	the	derivative	chain	rule	wisely

if	computing	the	loss(example,	parameters)	is	o(n)	
computation,	then	so	is	computing	the	gradient

   

   

38

simple chain rule

39

multiple paths chain rule

40

multiple	paths	chain	rule	- general

   

41

chain rule in flow graph

   

   

flow	graph:	any	directed	acyclic	graph

node	=	computation	result
arc	=	computation	dependency

=	successors	of	

   

42

back-prop in multi-layer net

h = sigmoid(vx)

   
   

43

back-prop in general flow graph

1. fprop:	visit	nodes	in	topo-sort	order	

- compute	value	of	node	given	predecessors

2. bprop:

- initialize	output	gradient	=	1	
- visit	nodes	in	reverse	order:

compute	gradient	wrt each	node	using	
gradient	wrt successors

=	successors	of	

single	scalar	output

   

   

   

44

automatic differentiation

    the	gradient	computation	can	
be	automatically	inferred	from	
the	symbolic	expression	of	the	
fprop.

    each	node	type	needs	to	know	
how	to	compute	its	output	and	
how	to	compute	the	gradient	
wrt its	inputs	given	the	
gradient	wrt its	output.

    easy	and	fast	prototyping

   

   

45

summary

    congrats!
    you	survived	the	hardest	part	of	this	class.
    everything	else	from	now	on	is	just	more	matrix	

multiplications	and	backprop :)

    next	up:	

   

recurrent	neural	networks

46

richard	socher

4/12/16

