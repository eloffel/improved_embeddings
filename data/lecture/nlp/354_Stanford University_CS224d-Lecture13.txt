cs224d:	deep	nlp

lecture	13:

convolutional	neural	networks	(for	nlp)

richard	socher

richard@metamind.io

overview	of	today
    from	id56s	to	id98s

    id98	variant	1:	simple	single	layer
    application:	sentence	classification
    more	details	and	tricks
    evaluation
    comparison	between	sentence	models:	bov,	id56s2,	id98s

    id98	variant	2:	complex	multi	layer

richard	socher

5/12/16

from	id56s	to	id98s	

1
5

5.5
6.1

2.5
3.8

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

1
5

5.5
6.1

4.5
3.8

2.5
3.8

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

richard	socher

5/12/16

1
3.5

0.4
0.3
the		

1
3.5

0.4
0.3
the		

from	id56s	to	id98s	
    recursive	neural	nets
require	a	parser	to	get
tree	structure

1
3.5

0.4
0.3
the		

1
5

5.5
6.1

2.5
3.8

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

    recurrent	neural	nets
cannot	capture	phrases
without	prefix	context
and	often	capture	too	much
of	last	words	in	final	vector

1
3.5

1
5

5.5
6.1

4.5
3.8

2.5
3.8

0.4
0.3
the		
richard	socher

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

5/12/16

from	id56s	to	id98s	
    id56:	get	compositional	vectors	for	grammatical	phrases	only	

    id98:	what	if	we	compute	vectors	for	every	possible	phrase?
    example:	   the	country	of	my	birth   	computes	vectors	for:
    the	country,	country	of,	of	my,	my	birth,	the	country	of,	
country	of	my,	of	my	birth,	the	country	of	my,	country	of	my	
birth

    regardless	of	whether	it	is	grammatical
    wouldn   t	need	parser
    not	very	linguistically	or	cognitively	plausible

richard	socher

5/12/16

what	is	convolution	anyway?
    1d	discrete	convolution	generally:

    convolution	is	great	to	extract	features	from	images

    2d	example	  
    yellow	shows	filter	weights
    green	shows	input

richard	socher

stanford	ufldl	wiki

5/12/16

from	id56s	to	id98s	
    first	layer:	compute	all	bigram	vectors

1
3.5

1
5

5.5
6.1

2.5
3.8

0.4
0.3
the		

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

    same	computation	as	in	id56	but	for	every	pair

    this	can	be	interpreted	as	a	convolution	over	the	word	vectors

richard	socher

5/12/16

from	id56s	to	id98s	
    now	multiple	options	to	compute	higher	layers.
    first	option	(simple	to	understand	but	not	necessarily	best)
   

just	repeat	with	different	weights:

2
3.5

4
5.5

1
3.5

1
3.5

1
5

5.5
6.1

2.5
3.8

0.4
0.3
the		

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

richard	socher

5/12/16

from	id56s	to	id98s	
    first	option	(simple	to	understand	but	not	necessarily	best)

2
3.5

1
5

2
3.5

1
3.5

4
5.5

4
5.5

1
3.5

5.5
6.1

2.5
3.8

0.4
0.3
the		

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

richard	socher

5/12/16

from	id56s	to	id98s	
    first	option	(simple	to	understand	but	not	necessarily	best)

3
3.5

4
5.5

4
5.5

1
3.5

5.5
6.1

2.5
3.8

2
3.5

1
5

2
3.5

1
3.5

0.4
0.3
the		

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

richard	socher

5/12/16

figure 1: model architecture with two channels for an example sentence.

necessary) is represented as

figure 1: model architecture with two channels for an example sentence.

figure 1: model architecture with two channels for an example sentence.

   convolutional	neural	networks	for	sentence	classification   
necessary) is represented as

ferent from the original task for which the feature
extractors were trained.
single	layer	id98
2 model
    a	simple	variant	using	one	convolutional	layer	and	pooling	
necessary) is represented as
    based	on	collobertand	weston	(2011)	and	kim	(2014)	
that is kept static throughout training and one that
the model architecture, shown in    gure 1, is a
is    ne-tuned via id26 (section 3.2).2
slight variant of the id98 architecture of collobert
that is kept static throughout training and one that
x1:n = x1   x2   . . .   xn,
(1)
in the multichannel architecture, illustrated in    g-
x1:n = x1   x2   . . .   xn,
    word	vectors:	
is    ne-tuned via id26 (section 3.2).2
et al. (2011). let xi 2 rk be the k-dimensional
ure 1, each    lter is applied to both channels and
where   is the concatenation operator.
in gen-
    sentence:
x1:n = x1   x2   . . .   xn,
where   is the concatenation operator.
in the multichannel architecture, illustrated in    g-
(1)
word vector corresponding to the i-th word in the
the results are added to calculate ci in equation
eral, let xi:i+j refer to the concatenation of words
    concatenation	of	words	in	range:	
(2). the model is otherwise equivalent to the sin-
ure 1, each    lter is applied to both channels and
sentence. a sentence of length n (padded where
eral, let xi:i+j refer to the concatenation of words
xi, xi+1, . . . , xi+j. a convolution operation in-
in gen-
where   is the concatenation operator.
gle channel architecture.
    convolutional	filter:	
the results are added to calculate ci in equation
xi, xi+1, . . . , xi+j. a convolution operation in-
volves a    lter w 2 rhk, which is applied to a
eral, let xi:i+j refer to the concatenation of words
1https://code.google.com/p/id97/
(2). the model is otherwise equivalent to the sin-
    could	be	2	(as	before)	higher,	e.g.	3:
window of h words to produce a new feature. for
volves a    lter w 2 rhk, which is applied to a
xi, xi+1, . . . , xi+j. a convolution operation in-
2.1 id173
gle channel architecture.
example, a feature ci is generated from a window
volves a    lter w 2 rhk, which is applied to a
window of h words to produce a new feature. for
for id173 we employ dropout on the
of words xi:i+h 1 by
window of h words to produce a new feature. for
example, a feature ci is generated from a window
penultimate layer with a constraint on l2-norms of
2.1 id173
example, a feature ci is generated from a window
(2)
the weight vectors (hinton et al., 2012). dropout
of words xi:i+h 1 by
proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1746   1751,
2.3
for id173 we employ dropout on the
of words xi:i+h 1 by
prevents co-adaptation of hidden units by ran-
3.6
here b 2 r is a bias term and f is a non-linear
penultimate layer with a constraint on l2-norms of
birth
domly dropping out   i.e., setting to zero   a pro-
function such as the hyperbolic tangent. this    lter
(2)
the weight vectors (hinton et al., 2012). dropout
portion p of the hidden units during foward-
is applied to each possible window of words in the
prevents co-adaptation of hidden units by ran-
id26. that is, given the penultimate

ci = f(w    xi:i+h 1 + b).
0.4
4
0.3
4.5
country							of							 my		
the		
ci = f(w    xi:i+h 1 + b).

october 25-29, 2014, doha, qatar. c 2014 association for computational linguistics

ci = f(w    xi:i+h 1 + b).

(goes	over	window	of	h	words)

(vectors	concatenated)

1746

richard	socher

5/12/16

2.1
3.3

1.1

7
7

single	layer	id98
    convolutional	filter:	
    note,	filter	is	vector!
    window	size	h	could	be	2	(as	before)	or	higher,	e.g.	3:
    to	compute	feature	for	id98	layer:

where   is the concatenation operator.
in gen-
eral, let xi:i+j refer to the concatenation of words
xi, xi+1, . . . , xi+j. a convolution operation in-
volves a    lter w 2 rhk, which is applied to a
window of h words to produce a new feature. for
example, a feature ci is generated from a window
of words xi:i+h 1 by

(goes	over	window	of	h	words)

ci = f(w    xi:i+h 1 + b).

(2)
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter
is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
a feature map

1.1

2.1
3.3

4
4.5
country							of							 my		

7
7

2.3
3.6
birth

c = [c1, c2, . . . , cn h+1],

(3)
with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)

richard	socher

5/12/16

0.4
0.3
the		

ure 1, each    lter is applied to both channels and
the results are added to calculate ci in equation
(2). the model is otherwise equivalent to the sin-
gle channel architecture.

2.1 id173
for id173 we employ dropout on the
penultimate layer with a constraint on l2-norms of
the weight vectors (hinton et al., 2012). dropout
prevents co-adaptation of hidden units by ran-
domly dropping out   i.e., setting to zero   a pro-
portion p of the hidden units during foward-
id26. that is, given the penultimate
layer z = [  c1, . . . ,   cm] (note that here we have m
   lters), instead of using

for output unit y in forward propagation, dropout

    sentence:

necessary) is represented as

ci = f(w    xi:i+h 1 + b).

    all	possible	windows	of	length	h:

of words xi:i+h 1 by
example, a feature ci is generated from a window
of words xi:i+h 1 by

example, a feature ci is generated from a window
single	layer	id98
of words xi:i+h 1 by
figure 1: model architecture with two channels for an example sentence.
ci = f(w    xi:i+h 1 + b).
(2)
here b 2 r is a bias term and f is a non-linear
    filter	w	is	applied	to	all	possible	windows	(concatenated	vectors)
ci = f(w    xi:i+h 1 + b).
function such as the hyperbolic tangent. this    lter
here b 2 r is a bias term and f is a non-linear
here b 2 r is a bias term and f is a non-linear
is applied to each possible window of words in the
function such as the hyperbolic tangent. this    lter
x1:n = x1   x2   . . .   xn,
(1)
function such as the hyperbolic tangent. this    lter
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
is applied to each possible window of words in the
is applied to each possible window of words in the
a feature map
in gen-
where   is the concatenation operator.
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
eral, let xi:i+j refer to the concatenation of words
a feature map
a feature map
xi, xi+1, . . . , xi+j. a convolution operation in-
    result	is	a	feature	map:	
volves a    lter w 2 rhk, which is applied to a
c = [c1, c2, . . . , cn h+1],
window of h words to produce a new feature. for
example, a feature ci is generated from a window
with c 2 rn h+1. we then apply a max-over-
of words xi:i+h 1 by
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
richard	socher
portant feature   one with the highest value   for

(3)
with c 2 rn h+1. we then apply a max-over-
1.1
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
4
0.4
ci = f(w    xi:i+h 1 + b).
4.5
0.3
  c = max{c} as the feature corresponding to this
the		
country							of							 my		
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally

with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally
deals with variable sentence lengths.

(2)
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter

2.1 id173
for id173 we employ dropout on the
penultimate layer with a constraint on l2-norms of
the weight vectors (hinton et al., 2012). dropout
prevents co-adaptation of hidden units by ran-
domly dropping out   i.e., setting to zero   a pro-
portion p of the hidden units during foward-

that is kept static throughout training and one that
is    ne-tuned via id26 (section 3.2).2
in the multichannel architecture, illustrated in    g-
ure 1, each    lter is applied to both channels and
the results are added to calculate ci in equation
c = [c1, c2, . . . , cn h+1],
(2). the model is otherwise equivalent to the sin-
gle channel architecture.

we have described the process by which one

c = [c1, c2, . . . , cn h+1],

2.3
3.6
birth

??????????

   

5/12/16

2.1
3.3

2.4

3.5

7
7

    sentence:

necessary) is represented as

ci = f(w    xi:i+h 1 + b).

    all	possible	windows	of	length	h:

of words xi:i+h 1 by
example, a feature ci is generated from a window
of words xi:i+h 1 by

example, a feature ci is generated from a window
single	layer	id98
of words xi:i+h 1 by
figure 1: model architecture with two channels for an example sentence.
ci = f(w    xi:i+h 1 + b).
(2)
here b 2 r is a bias term and f is a non-linear
    filter	w	is	applied	to	all	possible	windows	(concatenated	vectors)
ci = f(w    xi:i+h 1 + b).
function such as the hyperbolic tangent. this    lter
here b 2 r is a bias term and f is a non-linear
here b 2 r is a bias term and f is a non-linear
is applied to each possible window of words in the
function such as the hyperbolic tangent. this    lter
x1:n = x1   x2   . . .   xn,
(1)
function such as the hyperbolic tangent. this    lter
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
is applied to each possible window of words in the
is applied to each possible window of words in the
a feature map
in gen-
where   is the concatenation operator.
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
eral, let xi:i+j refer to the concatenation of words
a feature map
a feature map
xi, xi+1, . . . , xi+j. a convolution operation in-
    result	is	a	feature	map:	
volves a    lter w 2 rhk, which is applied to a
c = [c1, c2, . . . , cn h+1],
window of h words to produce a new feature. for
example, a feature ci is generated from a window
with c 2 rn h+1. we then apply a max-over-
of words xi:i+h 1 by
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
richard	socher
portant feature   one with the highest value   for

(3)
with c 2 rn h+1. we then apply a max-over-
1.1
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
4
0.4
ci = f(w    xi:i+h 1 + b).
4.5
0.3
  c = max{c} as the feature corresponding to this
the		
country							of							 my		
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally

with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally
deals with variable sentence lengths.

0
(2)
0
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter

2.1 id173
for id173 we employ dropout on the
penultimate layer with a constraint on l2-norms of
the weight vectors (hinton et al., 2012). dropout
prevents co-adaptation of hidden units by ran-
domly dropping out   i.e., setting to zero   a pro-
portion p of the hidden units during foward-

that is kept static throughout training and one that
is    ne-tuned via id26 (section 3.2).2
in the multichannel architecture, illustrated in    g-
ure 1, each    lter is applied to both channels and
the results are added to calculate ci in equation
c = [c1, c2, . . . , cn h+1],
(2). the model is otherwise equivalent to the sin-
gle channel architecture.

we have described the process by which one

c = [c1, c2, . . . , cn h+1],

2.3
3.6
birth

   

5/12/16

2.1
3.3

2.4

3.5

7
7

0
0

    from	feature	map

ci = f(w    xi:i+h 1 + b).

in	particular:	max-over-time	pooling	layer
idea:	capture	most	important	activation	(maximum	over	time)

is applied to each possible window of words in the
here b 2 r is a bias term and f is a non-linear
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
single	layer	id98:	pooling	layer
function such as the hyperbolic tangent. this    lter
a feature map
is applied to each possible window of words in the
    new	building	block:	pooling
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
   
c = [c1, c2, . . . , cn h+1],
a feature map
   

(2)
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter
is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
a feature map

the weight vectors (hinton et al., 2012). dropout
prevents co-adaptation of hidden units by ran-
domly dropping out   i.e., setting to zero   a pro-
portion p of the hidden units during foward-
id26. that is, given the penultimate
layer z = [  c1, . . . ,   cm] (note that here we have m
   lters), instead of using
with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
with c 2 rn h+1. we then apply a max-over-
over the feature map and take the maximum value
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally
each feature map. this pooling scheme naturally
deals with variable sentence lengths.
deals with variable sentence lengths.

(3)
with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally
deals with variable sentence lengths.

where   is the element-wise multiplication opera-
tor and r 2 rm is a    masking    vector of bernoulli
random variables with id203 p of being 1.
gradients are backpropagated only through the

we have described the process by which one
feature is extracted from one    lter. the model

we have described the process by which one
feature is extracted from one    lter. the model
uses multiple    lters (with varying window sizes)

for output unit y in forward propagation, dropout
uses

c = [c1, c2, . . . , cn h+1],

    but	we	want	more	features!

c = [c1, c2, . . . , cn h+1],

    pooled	single	number:

richard	socher

5/12/16

we have described the process by which one

c = [c1, c2, . . . , cn h+1],

of words xi:i+h 1 by

ci = f(w    xi:i+h 1 + b).

    because	of	max	pooling																								,	length	of	c irrelevant

solution:	multiple	filters
    use	multiple	filter	weights	w	

is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
ci = f(w    xi:i+h 1 + b).
a feature map

(2)
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter
is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
a feature map

here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter
is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
    useful	to	have	different	window	sizes	h
a feature map

for id173 we employ dropout on the
penultimate layer with a constraint on l2-norms of
the weight vectors (hinton et al., 2012). dropout
prevents co-adaptation of hidden units by ran-
domly dropping out   i.e., setting to zero   a pro-
portion p of the hidden units during foward-
id26. that is, given the penultimate
layer z = [  c1, . . . ,   cm] (note that here we have m
   lters), instead of using
(3)
with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
with c 2 rn h+1. we then apply a max-over-
over the feature map and take the maximum value
time pooling operation (collobert et al., 2011)
  c = max{c} as the feature corresponding to this
we have described the process by which one
over the feature map and take the maximum value
particular    lter. the idea is to capture the most im-
feature is extracted from one    lter. the model
  c = max{c} as the feature corresponding to this
portant feature   one with the highest value   for
uses multiple    lters (with varying window sizes)
each feature map. this pooling scheme naturally
particular    lter. the idea is to capture the most im-
to obtain multiple features. these features form
deals with variable sentence lengths.
portant feature   one with the highest value   for
the penultimate layer and are passed to a fully con-
we have described the process by which one
nected softmax layer whose output is the probabil-
richard	socher
each feature map. this pooling scheme naturally
feature is extracted from one    lter. the model
ity distribution over labels.
deals with variable sentence lengths.

(3)
with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally
deals with variable sentence lengths.

where   is the element-wise multiplication opera-
tor and r 2 rm is a    masking    vector of bernoulli
5/12/16
random variables with id203 p of being 1.

for output unit y in forward propagation, dropout
uses

    so	we	can	have	some	filters	that	look	at	unigrams,	bigrams,	tri-

c = [c1, c2, . . . , cn h+1],

c = [c1, c2, . . . , cn h+1],

y = w    (z   r) + b,

grams,	4-grams,	etc.

multi-channel	idea
   

initialize	with	pre-trained	word	vectors	(id97	or	glove)

    start	with	two	copies

    backprop into	only	one	set,	keep	other	   static   

    both	channels	are	added	to	ci before	max-pooling

richard	socher

5/12/16

classification	after	one	id98	layer
    first	one	convolution,	followed	by	one	max-pooling

penultimate layer with a constraint on l2-norms of
the weight vectors (hinton et al., 2012). dropout
prevents co-adaptation of hidden units by ran-
domly dropping out   i.e., setting to zero   a pro-
portion p of the hidden units during foward-
id26. that is, given the penultimate
layer z = [  c1, . . . ,   cm] (note that here we have m
   lters), instead of using

    to	obtain	final	feature	vector:

simple	final	softmax layer	

y = w    z + b

of words xi:i+h 1 by

ci = f(w    xi:i+h 1 + b).

(2)
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter
is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
(assuming	m	filters	w)
(3)
with c 2 rn h+1. we then apply a max-over-
time pooling operation (collobert et al., 2011)
over the feature map and take the maximum value
  c = max{c} as the feature corresponding to this
particular    lter. the idea is to capture the most im-
portant feature   one with the highest value   for
each feature map. this pooling scheme naturally
deals with variable sentence lengths.

c = [c1, c2, . . . , cn h+1],

   

we have described the process by which one
feature is extracted from one    lter. the model

for output unit y in forward propagation, dropout
uses

y = w    (z   r) + b,

where   is the element-wise multiplication opera-
tor and r 2 rm is a    masking    vector of bernoulli
random variables with id203 p of being 1.
gradients are backpropagated only through the
richard	socher
unmasked units. at test time, the learned weight
vectors are scaled by p such that   w = pw, and

5/12/16

figure	from	kim	(2014)

figure 1: model architecture with two channels for an example sentence.

necessary) is represented as

n	words	(possibly	 zero	padded)	 and	each	word	vector	has	k	dimensions
x1:n = x1   x2   . . .   xn,
(1)
in gen-
where   is the concatenation operator.
richard	socher
eral, let xi:i+j refer to the concatenation of words

that is kept static throughout training and one that
is    ne-tuned via id26 (section 3.2).2
in the multichannel architecture, illustrated in    g-
ure 1, each    lter is applied to both channels and
the results are added to calculate ci in equation
(2). the model is otherwise equivalent to the sin-

5/12/16

wait for the video and do n't rent it n x k representation of sentence with static and non-static channels convolutional layer with multiple filter widths and feature maps max-over-time pooling fully connected layer with dropout and  softmax output tricks	to	make	it	work	better:	dropout
   

idea:	randomly	mask/dropout/set	to	0	some	of	the	feature	
weights	z

    create	masking	vector	r	of	bernoulli	random	variables	with	

id203	p	(a	hyperparameter)	of	being	1

    delete	features	during	training:

    reasoning:	prevents	co-adaptation	(overfitting to	seeing	specific	

feature	constellations)

richard	socher

5/12/16

tricks	to	make	it	work	better:	dropout

    at	training	time,	gradients	are	backpropagated only	through	

those	elements	of	z	vector	for	which	ri =	1

    at	test	time,	there	is	no	dropout,	so	feature	vectors	z	are	larger.
    hence,	we	scale	final	vector	by	bernoulli	id203	p	

    kim	(2014)	reports	2	    4%	improved	accuracy	and	ability	to	use	

very	large	networks	without	overfitting

richard	socher

5/12/16

another	id173	trick
    somewhat	less	common

    constrain	l2 norms	of	weight	vectors	of	each	class	(row	in	

softmax weight	w(s))	to	fixed	number	s	(also	a	hyperparameter)

   

if	

,	then	rescale	it	so	that:	

richard	socher

5/12/16

all	hyperparameters in	kim	(2014)
    find	hyperparameters based	on	dev set
    nonlinearity:	relu
    window	filter	sizes	h	=	3,4,5
    each	filter	size	has	100	feature	maps
    dropout	p	=	0.5
    l2	constraint	s	for	rows	of	softmax s	=	3
    mini	batch	size	for	sgd	training:	50
    word	vectors:	pre-trained	with	id97,	k	=	300

    during	training,	keep	checking	performance	on	dev set	and	pick	

highest	accuracy	weights	for	final	evaluation

richard	socher

5/12/16

experiments

model
id98-rand
id98-static
id98-non-static
id98-multichannel
rae (socher et al., 2011)
mv-id56 (socher et al., 2012)
rntn (socher et al., 2013)
did98 (kalchbrenner et al., 2014)
paragraph-vec (le and mikolov, 2014)
ccae (hermann and blunsom, 2013)
sent-parser (dong et al., 2014)
nbid166 (wang and manning, 2012)
mnb (wang and manning, 2012)
g-dropout (wang and manning, 2013)
f-dropout (wang and manning, 2013)
tree-crf (nakagawa et al., 2010)
crf-pr (yang and cardie, 2014)
id166s (silva et al., 2011)

sst-1
mr
45.0
76.1
45.5
81.0
48.0
81.5
47.4
81.1
43.2
77.7
44.4
79.0
45.7
 
48.5
 
48.7
 
77.8
 
79.5
 
79.4
 
79.0
 
79.0
 
79.1
 
77.3
 
 
 
 
 
richard	socher

sst-2
82.7
86.8
87.2
88.1
82.4
82.9
85.4
86.8
87.8
 
 
 
 
 
 
 
 
 

subj
89.6
93.0
93.4
93.2
 
 
 
 
 
 
 
93.2
93.6
93.4
93.6
 
 
 

trec
91.2
92.8
93.6
92.2
 
 
 
93.0
 
 
 
 
 
 
 
 
 
95.0

cr mpqa
79.8
84.7
84.3
85.0
 
 
 
 
 
 
 
81.8
80.0
82.1
81.9
81.4
82.7
 

83.4
89.6
89.5
89.4
86.4
 
 
 
 
87.2
86.3
86.3
86.3
86.1
86.3
86.1
 
 

table 2: results of our id98 models against other methods. rae: recursive autoencoders with pre-trained word vectors from
wikipedia (socher et al., 2011). mv-id56: matrix-vector id56 with parse trees (socher et al., 2012).
rntn: recursive neural tensor network with tensor-based feature function and parse trees (socher et al., 2013). did98:
dynamic convolutional neural network with k-max pooling (kalchbrenner et al., 2014). paragraph-vec: logistic regres-

5/12/16

problem	with	comparison?
    dropout	gives	2	    4	%	accuracy	improvement
    several	baselines	didn   t	use	dropout	

    still	remarkable	results	and	simple	architecture!

    difference	to	window	and	id56	architectures	we	described	in	

previous	lectures:	pooling,	many	filters	and	dropout
ideas	can	be	used	in	id562s	too	

   
    tree-lstms	obtain	better	performance	on	sentence	datasets

richard	socher

5/12/16

    fixed	tree	id56s	explored	in	computer	vision:	

socher et	al	(2012):	   convolutional-recursive	deep	learning	for	
3d	object	classification   

lecture	1,	slide	26

richard	socher

5/12/16

relationship	between	id56s	and	id98s
   

id98

id56

richard	socher

5/12/16

relationship	between	id56s	and	id98s
   

id98

id56

richard	socher

5/12/16

relationship	between	id56s	and	id98s
   

id98

id56

    stride	size	flexible	in	id98s,	id56s	   weighted	average	pool   
    tying	(sharing)	weights	of	filters	inside	vs across	different	layers
    id98:	multiple	filters,	additional	layer	type:	max-pooling
    balanced	input	independent	structure	vs input	specific	tree

richard	socher

5/12/16

id98	alternatives
    narrow	vs wide	convolution

c5

tor wi 2 rd of a word in the sentence:

c1

c5

(2)

s =24w1

s1

. . . ws35

...

ss

ss

max(c1,:)

cmax =264

s1
figure 2: narrow and wide types of convolution.
to address the problem of varying sentence
the    lter m has size m = 5.
lengths, the max-tdnn takes the maximum of
each row in the resulting matrix c yielding a vector
(over	sequences)
of d values:
and	deeper	convolutional	layers
(3)

    complex	pooling	schemes
a sequence c 2 rs m+1 with j ranging from m
to s. the wide type of convolution does not have
requirements on s or m and yields a sequence c 2
rs+m 1 where the index j ranges from 1 to s +
m   1. out-of-range input values si where i < 1
    kalchbrenner et	al.	(2014)
or i > s are taken to be zero. the result of the
narrow convolution is a subsequence of the result
of the wide convolution. the two types of one-
dimensional convolution are illustrated in fig. 2.
the trained weights in the    lter m correspond
to a linguistic feature detector that learns to recog-
nise a speci   c class of id165s. these id165s
have size n     m, where m is the width of the
   lter. applying the weights m in a wide convo-
lution has some advantages over applying them in

the max-tdnn model has many desirable
properties. it is sensitive to the order of the words
in the sentence and it does not depend on external
language-speci   c features such as dependency or
constituency parse trees. it also gives largely uni-

the aim is to capture the most relevant feature, i.e.
the one with the highest value, for each of the d
rows of the resulting matrix c. the    xed-sized
vector cmax is then used as input to a fully con-
nected layer for classi   cation.

max(cd,:)

375

richard	socher

fully connected 

layer

k-max pooling

(k=3)

folding

convolution

wide
(m=2)

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

 the cat sat on the red mat

figure 3: a did98 for the seven word input sen-
tence. id27s have size d = 4. the

5/12/16

layer to the network, the tdnn can be adopted as
a sentence model (collobert and weston, 2008).

various neural sentence models have been de-
scribed. a general class of basic sentence models
is that of neural bag-of-words (nbow) models.
these generally consist of a projection layer that
maps words, sub-word units or id165s to high
dimensional embeddings; the latter are then com-
bined component-wise with an operation such as
summation. the resulting combined vector is clas-
si   ed through one or more fully connected layers.
a model that adopts a more general structure
provided by an external parse tree is the recursive
neural network (reid98) (pollack, 1990; k  uchler
and goller, 1996; socher et al., 2011; hermann
and blunsom, 2013). at every node in the tree the
contexts at the left and right children of the node
are combined by a classical layer. the weights of
the layer are shared across all nodes in the tree.
the layer computed at the top node gives a repre-

id98	application:	translation
    one	of	the	first	successful	neural	

machine	translation	efforts

    uses	id98	for	encoding	and	

id56	for	decoding

    kalchbrenner and	blunsom (2013)

   recurrent	continuous	translation	models   

p( f | e ) 

s 

e 

csm 

richard	socher

e 

5/12/16

rctm i

model	comparison
    bag	of	vectors:	surprisingly	good	baseline	for	simple	

classification	problems.	especially	if	followed	by	a	few	layers!

    window	model:	good	for	single	word	classification	for	

problems	that	do	not	need	wide	context

    id98s:	good	for	classification,	unclear	how	to	incorporate	phrase	

level	annotation	(can	only	take	a	single	label),	need	zero	
padding	for	shorter	phrases,	hard	to	interpret,	easy	to	
parallelize	on	gpus

richard	socher

5/12/16

model	comparison	
    recursive	neural	networks:	most	linguistically	plausible,	

interpretable,	provide	most	important	phrases	(for	
visualization),	need	parse	trees

    recurrent	neural	networks:	most	cognitively	plausible	(reading	

from	left	to	right),	not	usually	the	highest	classification	
performance	but	lots	of	improvements	right	now	with	gates	
(grus,	lstms,	etc).

    best	but	also	most	complex	models:	hierarchical	recurrent	
neural	networks	with	attention	mechanisms	and	additional	
memory	   last	week	of	class	:)	

richard	socher

5/12/16

next	week:
    guest	lectures	next	week:	

    speech	recognition	and	state	of	the	art	machine	translation

richard	socher

5/12/16

