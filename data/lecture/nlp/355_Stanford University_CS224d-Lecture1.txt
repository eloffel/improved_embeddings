cs224d	

deep	learning		

for	natural	language	processing	

	
	
	
	

richard	socher,	phd	

welcome	

1.    cs224d	logis7cs		

2.   
	

2	

introduc7on	to	nlp,	deep	learning	and	their	intersec7on	

richard	socher	

3/31/16	

course	logis>cs	
      

instructor:	richard	socher		
(stanford	phd,	2014;	now	founder/ceo	at	metamind)	

       tas:	james	hong,	bharath	ramsundar,	sameep	bagadia,	david	

dindi,	++	

       time:	tuesday,	thursday	3:00-4:20	
       loca7on:	gates	b1	

       there	will	be	3	problem	sets	(with	lots	of	programming),		

a	midterm	and	a	   nal	project		

       for	syllabus	and	o   ce	hours,	see	h\p://cs224d.stanford.edu/	
       slides	uploaded	before	each	lecture,	video	+	lecture	notes	a]er	

lecture	1,	slide	3	

richard	socher	

3/31/16	

pre-requisites	
       pro   ciency	in	python	

       all	class	assignments	will	be	in	python.	there	is	a	tutorial	here		

       college	calculus,	linear	algebra	(e.g.	math	19	or	41,	math	51)	

       basic	id203	and	sta7s7cs	(e.g.	cs	109	or	other	stats	course)	

	

       equivalent	knowledge	of	cs229	(machine	learning)	

       cost	func7ons,		
       taking	simple	deriva7ves		
       performing	op7miza7on	with	gradient	descent.	

lecture	1,	slide	4	

richard	socher	

3/31/16	

grading	policy	
       3	problem	sets:	15%	x	3	=	45%		
       midterm	exam:	15%		
       final	course	project:	40%	

      

       milestone:	5%	(2%	bonus	if	you	have	your	data	and	ran	an	experiment!)	
       a\end	at	least	1	project	advice	o   ce	hour:	2%	
       final	write-up,	project	and	presenta7on:	33%	
       bonus	points	for	excep7onal	poster	presenta7on		
late	policy	
       7	free	late	days	   	use	as	you	please	
       a]erwards,	25%	o   	per	day	late	
       psets	not	accepted	a]er	3	late	days	per	pset	
       does	not	apply	to	final	course	project		

       collabora7on	policy:	read	the	student	code	book	and	honor	code!	
       understand	what	is	   collabora7on   	and	what	is	   academic	infrac7on   		

lecture	1,	slide	5	

richard	socher	

3/31/16	

high	level	plan	for	problem	sets	
       the	   rst	half	of	the	course	and	the	   rst	2	psets	will	be	hard	

       pset	1	is	in	pure	python	code	(numpy	etc.)	to	really	understand	

the	basics	

       released	on	april	4th	

       new:	psets	2	&	3	will	be	in	tensorflow,	a	library	for	punng	

together	new	neural	network	models	quickly	(  	special	lecture)	

       pset	3	will	be	shorter	to	increase	7me	for	   nal	project	

       libraries	like	tensorflow	(or	torch)	are	becoming	standard	tools	
       but	s7ll	some	problems	

lecture	1,	slide	6	

richard	socher	

3/31/16	

what	is	natural	language	processing	(nlp)?	
       natural	language	processing	is	a	   eld	at	the	intersec7on	of		

       computer	science	
       ar7   cial	intelligence	
       and	linguis7cs.		

       goal:	for	computers	to	process	or	   understand   	natural	
language	in	order	to	perform	tasks	that	are	useful,	e.g.	
       ques7on	answering	

       fully	understanding	and	represen>ng	

the	meaning	of	language	(or	even		
de   ning	it)	is	an	illusive	goal.	

       perfect	language	understanding	is		

ai-complete		

lecture	1,	slide	7	

richard	socher	

3/31/16	

nlp	levels	

lecture	1,	slide	8	

richard	socher	

3/31/16	

(a	>ny	sample	of)	nlp	applica>ons		
       applica7ons	range	from	simple	to	complex:	

       spell	checking,	keyword	search,	   nding	synonyms	

       extrac7ng	informa7on	from	websites	such	as		

       product	price,	dates,	loca7on,	people	or	company	names	
       classifying,	reading	level	of	school	texts,	posi7ve/nega7ve	

sen7ment	of	longer	documents	

       machine	transla7on	
       spoken	dialog	systems	
       complex	ques7on	answering	

lecture	1,	slide	9	

richard	socher	

3/31/16	

nlp	in	industry	
       search	(wri\en	and	spoken)	

       online	adver7sement	

       automated/assisted	transla7on	

       sen7ment	analysis	for	marke7ng	or	   nance/trading	

       speech	recogni7on	

       automa7ng	customer	support	

lecture	1,	slide	10	

richard	socher	

3/31/16	

why	is	nlp	hard?	
       complexity	in	represen7ng,	learning	and	using	linguis7c/

situa7onal/world/visual	knowledge	

       jane	hit	june	and	then	she	[fell/ran].	

       ambiguity:	   i	made	her	duck   	

lecture	1,	slide	11	

richard	socher	

3/31/16	

what   s	deep	learning	(dl)?	
       deep	learning	is	a	sub   eld	of	machine	learning	

3.3. approach

       most	machine	learning	methods	work		

well	because	of	human-designed		
representa7ons	and	input	features	
       for	example:	features	for	   nding		
named	en77es	like	loca7ons	or		
organiza7on	names	(finkel,	2010):	

!

!

ner

feature
current word
previous word
next word
current word character id165
current pos tag
surrounding pos tag sequence
current word shape
surrounding word shape sequence
size 4
presence of word in left window
presence of word in right window size 4

all

!

!

!

!

!

length     6

size 9
size 9

       machine	learning	becomes	just	op7mizing	

weights	to	best	make	a	   nal	predic7on	

table 3.1: features used by the crf for the two tasks: id39 (ner)
and template    lling (tf).

lecture	1,	slide	12	

richard	socher	

can go beyond imposing just exact identity conditions). i illustrate this by modeling two
forms of non-local structure: label consistency in the id39 task, and
template consistency in the template    lling task. one could imagine many ways of de   ning
such models; for simplicity i use the form
3/31/16	

pm(y|x)        

#(  ,y,x)

  

machine	learning	vs	deep	learning	

machine learning in practice 

describing your data with 
features a computer can 
understand 

learning 
algorithm 

domain	speci   c,	requires	ph.d.	
level	talent	

op7mizing	the	
weights	on	features	

what   s	deep	learning	(dl)?	
       representa7on	learning	a\empts		

to	automa7cally	learn	good		
features	or	representa7ons	

       deep	learning	algorithms	a\empt	to	

learn	(mul7ple	levels	of)		
representa7on	and	an	output	

       from	   raw   	inputs	x	(e.g.	words)	

lecture	1,	slide	14	

richard	socher	

3/31/16	

on	the	history	and	term	of	   deep	learning   	
       we	will	focus	on	di   erent	kinds	of	neural	networks		
       the	dominant	model	family	inside	deep	learning	

       only	clever	terminology	for	stacked	logis7c	regression	units?	
       somewhat,	but	interes7ng	modeling	principles	(end-to-end)	
and	actual	connec7ons	to	neuroscience	in	some	cases	

       we	will	not	take	a	historical	approach	but	instead	focus	on	

methods	which	work	well	on	nlp	problems	now	

       for	history	of	deep	learning	models	(star7ng	~1960s),	see:		

deep	learning	in	neural	networks:	an	overview		
by	schmidhuber	

lecture	1,	slide	15	

richard	socher	

3/31/16	

reasons	for	exploring	deep	learning	
       manually	designed	features	are	o]en	over-speci   ed,	incomplete	

and	take	a	long	7me	to	design	and	validate	

       learned	features	are	easy	to	adapt,	fast	to	learn	

       deep	learning	provides	a	very	   exible,	(almost?)	universal,	

learnable	framework	for	represen>ng	world,	visual	and	
linguis7c	informa7on.	

       deep	learning	can	learn	unsupervised	(from	raw	text)	and	

supervised	(with	speci   c	labels	like	posi7ve/nega7ve)	

lecture	1,	slide	16	

richard	socher	

3/31/16	

reasons	for	exploring	deep	learning	
      

in	2006	deep	learning	techniques	started	outperforming	other	
machine	learning	techniques.	why	now?	

       dl	techniques	bene   t	more	from	a	lot	of	data	
       faster	machines	and	mul7core	cpu/gpu	help	dl		
       new	models,	algorithms,	ideas		

  	improved	performance	(   rst	in	speech	and	vision,	then	nlp)	

lecture	1,	slide	17	

richard	socher	

3/31/16	

phonemes/words	

deep	learning	for	speech	
       the	   rst	breakthrough	results	of	
   deep	learning   	on	large	datasets	
happened	in	speech	recogni7on	
       context-dependent	pre-trained	
deep	neural	networks	for	large	
vocabulary	speech	recogni7on		
dahl	et	al.	(2010)	

acous>c	model	

tradi7onal	
features	
deep	learning	

recog	
\	wer	
1-pass	
   adapt	
1-pass	
   adapt	

rt03s	
fsh	
27.4	

hub5	
swb	
23.6	

18.5	
(   33%)	

16.1	
(   32%)	

lecture	1,	slide	18	

richard	socher	

3/31/16	

deep	learning	for	computer	vision	
       most	deep	learning	groups	

(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:85)(cid:69)(cid:76)(cid:87)(cid:85)(cid:68)(cid:85)(cid:92)(cid:3)(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:86)(cid:3)(cid:68)(cid:79)(cid:82)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:68)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:82)(cid:83)(cid:17)(cid:17)(cid:17)

have	(un7l	2	years	ago)		
focused	on	computer	vision	

       break	through	paper:	

id163	classi   ca7on	with	
deep	convolu7onal	neural	
networks	by	krizhevsky	et	
al.	2012	

ilsvrc

(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:56)(cid:81)(cid:71)(cid:72)(cid:85)(cid:86)(cid:87)(cid:68)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:86)
(cid:61)(cid:72)(cid:76)(cid:79)(cid:72)(cid:85)(cid:3)(cid:9)(cid:3)(cid:41)(cid:72)(cid:85)(cid:74)(cid:88)(cid:86)(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:22)
(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:85)(cid:69)(cid:76)(cid:87)(cid:85)(cid:68)(cid:85)(cid:92)(cid:3)(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:86)(cid:3)(cid:68)(cid:79)(cid:82)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:68)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:82)(cid:83)(cid:17)(cid:17)(cid:17)

(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:56)(cid:81)(cid:71)(cid:72)(cid:85)(cid:86)(cid:87)(cid:68)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:86)
(cid:61)(cid:72)(cid:76)(cid:79)(cid:72)(cid:85)(cid:3)(cid:9)(cid:3)(cid:41)(cid:72)(cid:85)(cid:74)(cid:88)(cid:86)(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:22)
(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:85)(cid:69)(cid:76)(cid:87)(cid:85)(cid:68)(cid:85)(cid:92)(cid:3)(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:86)(cid:3)(cid:68)(cid:79)(cid:82)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:68)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:82)(cid:83)(cid:17)(cid:17)(cid:17)

(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)
(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)

olga russakovsky* et al.

(cid:55)(cid:75)(cid:72)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:76)(cid:86)(cid:3)(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:3)(cid:89)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:72)(cid:71)

(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)
(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)

(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:56)(cid:81)(cid:71)(cid:72)(cid:85)(cid:86)(cid:87)(cid:68)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:86)
(cid:61)(cid:72)(cid:76)(cid:79)(cid:72)(cid:85)(cid:3)(cid:9)(cid:3)(cid:41)(cid:72)(cid:85)(cid:74)(cid:88)(cid:86)(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:22)
(cid:21)(cid:3)(cid:41)(cid:72)(cid:69)(cid:3)(cid:21)(cid:19)(cid:20)(cid:24)
(cid:21)(cid:3)(cid:41)(cid:72)(cid:69)(cid:3)(cid:21)(cid:19)(cid:20)(cid:24)
(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:85)(cid:69)(cid:76)(cid:87)(cid:85)(cid:68)(cid:85)(cid:92)(cid:3)(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:86)(cid:3)(cid:68)(cid:79)(cid:82)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:68)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:82)(cid:83)(cid:17)(cid:17)(cid:17)

(cid:22)(cid:20)

(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)
(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)

(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)
(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)

(cid:22)(cid:19)

(cid:21)(cid:3)(cid:41)(cid:72)(cid:69)(cid:3)(cid:21)(cid:19)(cid:20)(cid:24)
(cid:21)(cid:3)(cid:41)(cid:72)(cid:69)(cid:3)(cid:21)(cid:19)(cid:20)(cid:24)

      

(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)
(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)

(cid:22)(cid:19)
(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:56)(cid:81)(cid:71)(cid:72)(cid:85)(cid:86)(cid:87)(cid:68)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:86)
(cid:61)(cid:72)(cid:76)(cid:79)(cid:72)(cid:85)(cid:3)(cid:9)(cid:3)(cid:41)(cid:72)(cid:85)(cid:74)(cid:88)(cid:86)(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:22)
(cid:57)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:85)(cid:69)(cid:76)(cid:87)(cid:85)(cid:68)(cid:85)(cid:92)(cid:3)(cid:81)(cid:72)(cid:88)(cid:85)(cid:82)(cid:81)(cid:86)(cid:3)(cid:68)(cid:79)(cid:82)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:68)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:82)(cid:83)(cid:17)(cid:17)(cid:17)

(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)
(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)

(cid:21)(cid:3)(cid:41)(cid:72)(cid:69)(cid:3)(cid:21)(cid:19)(cid:20)(cid:24)
(cid:21)(cid:3)(cid:41)(cid:72)(cid:69)(cid:3)(cid:21)(cid:19)(cid:20)(cid:24)

19	

      

zeiler	and	fergus	(2013)	

richard	socher	

(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)
(cid:41)(cid:72)(cid:76)(cid:16)(cid:41)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)(cid:9)(cid:3)(cid:36)(cid:81)(cid:71)(cid:85)(cid:72)(cid:77)(cid:3)(cid:46)(cid:68)(cid:85)(cid:83)(cid:68)(cid:87)(cid:75)(cid:92)

(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)
(cid:47)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)(cid:27)(cid:3)(cid:16)
lecture	1,	slide	19	

(cid:20)(cid:19)

deep	learning	+	nlp	=	deep	nlp	
       combine	ideas	and	goals	of	nlp	and	use	representa7on	learning	

and	deep	learning	methods	to	solve	them	

       several	big	improvements	in	recent	years	across	di   erent	nlp		

       levels:	speech,	morphology,	syntax,	seman7cs	
       applica>ons:	machine	transla7on,	sen7ment	analysis	and	
ques7on	answering	

lecture	1,	slide	20	

richard	socher	

3/31/16	

representa>ons	at	nlp	levels:	phonology	
       tradi7onal:	phonemes	

              the international phonetic alphabet (revised to 2005)

bilabial labiodental dental alveolar post alveolar retroflex
p  b
m
  

uvular
    (cid:152) c     k  g q  g
   
r

palatal

velar

(cid:143)

=

n

   2005 ipa
pharyngeal glottal

/

s  z

    (cid:159)     j x  v x         ? h  h

consonants (pulmonic)

plosive
nasal
trill
tap or flap
fricative
lateral
fricative
approximant
lateral
approximant

  

    v

t  d
n
r
|
f  b f   v t  d  s (cid:172)(cid:172)z
    l
  
l

(cid:165)

j
  

  
k

where symbols appear in pairs, the one to the right represents a voiced consonant. shaded areas denote articulations judged impossible.

consonants (non-pulmonic)

vowels

       dl:	trains	to	predict	phonemes	(or	words	directly)	from	sound	

clicks
voiced implosives
   
(cid:156) bilabial
> bilabial
   dental/alveolar p   
  
dental
t   
!
e (cid:154)
   palatal
features	and	represent	them	as	vectors	
k   
  
   palatoalveolar    velar
s   
e {    
alveolar lateral    uvular
(cid:148)

  front                         central                             back
i
   u

ejectives
examples:
bilabial

i y
pe

alveolar fricative

   o

dental/alveolar

     

close-mid

open-mid

(post)alveolar

close

velar

y

u

other symbols
(cid:153)    voiceless labial-velar fricative
w  (cid:172) voiced labial-velar approximant
       voiced labial-palatal approximant   
   voiceless epiglottal fricative

lecture	1,	slide	21	

      alveolo-palatal fricatives
     voiced alveolar lateral flap
simultaneous  s  and   x

affricates and double articulations

   o

  

  

  

a    

a   
where symbols appear in pairs, the one 
to the right represents a rounded vowel.

richard	socher	

suprasegmentals
 "
primary stress
   

3/31/16	

   

    
 (cid:210)

open

representa>ons	at	nlp	levels:	morphology	
       tradi7onal:	morphemes

				su   x	

	
	

	pre   x	 	stem	
	un	

	interest			ed	

	

	

	

       dl:		

       every	morpheme	is	a	vector	
       a	neural	network	combines		
two	vectors	into	one	vector	
       thang	et	al.	2013	

                          

^dd

  

   

     

   

                      

^dd

    

^h&

  

   

     

   

    

wz 

                  

^dd

lecture	1,	slide	22	

figure 1: morphological recursive neural net-
work. a vector representation for the word    un-

richard	socher	

3/31/16	

neural	word	vectors	-	visualiza>on	

23	

representa>ons	at	nlp	levels:	syntax	
       tradi7onal:	phrases	

discrete	categories	like	np,	vp	

       dl:		

       every	word	and	every	phrase	
is	a	vector	
       a	neural	network	combines		
two	vectors	into	one	vector	
       socher	et	al.	2011	

lecture	1,	slide	24	

richard	socher	

3/31/16	

representa>ons	at	nlp	levels:	seman>cs	
       tradi7onal:	lambda	calculus	

       carefully	engineered	func7ons	
       take	as	inputs	speci   c	other	
func7ons	
       no	no7on	of	similarity	or	
fuzziness	of	language	

       dl:		

       every	word	and	every	phrase	
much of the theoretical work on natural lan-
guage id136 (and some successful
imple-
and	every	logical	expression		
mented models; maccartney and manning 2009;
is	a	vector	
watanabe et al. 2012) involves natural logics,
       a	neural	network	combines		
which are formal systems that de   ne rules of in-
ference between natural language words, phrases,
two	vectors	into	one	vector	
and sentences without the need of intermediate
representations in an arti   cial logical language.
       bowman	et	al.	2014	
in our    rst three experiments, we test our mod-
els    ability to learn the foundations of natural lan-
guage id136 by training them to reproduce the

lecture	1,	slide	25	

softmax classi   er

p (@) = 0.8

comparison
n(t)n layer

all reptiles walk vs. some turtles move

composition
rn(t)n
layers

all reptiles walk

some turtles move

all reptiles

walk

some turtles move

all

reptiles

some

turtles

pre-trained or randomly initialized learned word vectors

richard	socher	

figure 1:
two separate tree-
structured networks build up vector representa-

in our model,

3/31/16	

nlp	applica>ons:	sen>ment	analysis	
       tradi7onal:	curated	sen7ment	dic7onaries	combined	with	either	

bag-of-words	representa7ons	(ignoring	word	order)	or	hand-
designed	nega7on	features	(ain   t	gonna	capture	everything)	

       same	deep	learning	model	that	was	used	for	morphology,	syntax	

and	logical	seman7cs	can	be	used!	  	recursivenn		

	

lecture	1,	slide	26	

richard	socher	

3/31/16	

dependency q: what can the splitting of water lead to?

temporal

ques>on	answering	
       common:	a	lot	of	feature	engineering	to	capture	world	and	

a: light absorption
b: transfer of ions
q: what is the correct order of events?
a: pdgf binds to tyrosine kinases, then cells divide, then wound healing
b: cells divide, then pdgf binds to tyrosine kinases, then wound healing
q: cdk associates with mpf to become cyclin
a: true
b: false

121 (20.68%)

57 (9.74%)

true-false

other	knowledge,		e.g.	regular	expressions,	berant	et	al.	(2014)	

table 3: examples and statistics for each of the three coarse types of questions.

is main verb trigger?

yes

no

condition
wh- word subjective?
wh- word object?

regular exp.
agent
theme

condition
default
direct
prevent

regular exp.
(enable|super)+
(enable|super)
(enable|super)   prevent(enable|super)   

       dl:	same	deep	learning	model	that	was	used	for	morphology,	

syntax,	logical	seman7cs	and	sen7ment	can	be	used!	

figure 3: rules for determining the id157 for queries concerning two triggers. in each table, the condition
column decides the regular expression to be chosen. in the left table, we make the choice based on the path from the root to
the wh- word in the question. in the right table, if the word directly modi   es the main trigger, the direct regular expression
is chosen. if the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the prevent regular
expression. otherwise, the default one is chosen. we omit the relation label same from the expressions, but allow going
through any number of edges labeled by same when matching expressions to the structure.

       facts	are	stored	in	vectors	

that we expand using id138.

5.3 answering questions

the    nal step in constructing the query is to
identify the regular expression for the path con-
necting the source and the target. due to paucity
of data, we do not map a question and an answer
to arbitrary id157. instead, we con-
struct a small set of id157, and build
a rule-based system that selects one. we used the
training set to construct the id157

lecture	1,	slide	27	

we match the query of an answer to the process
structure to identify the answer. in case of a match,
the corresponding answer is chosen. the matching
path can be thought of as a proof for the answer.

if neither query matches the graph (or both do),

machine	transla>on	
       many	levels	of	transla7on		
have	been	tried	in	the	past:	

       tradi7onal	mt	systems	are		
very	large	complex	systems		

       what	do	you	think	is	the	interlingua	for	the	dl	approach	to	

transla7on?	

lecture	1,	slide	28	

richard	socher	

3/31/16	

machine	transla>on	

lecture	1,	slide	29	

richard	socher	

3/31/16	

due to the considerable time lag between the inputs and their corresponding outputs (   g. 1).
there have been a number of related attempts to address the general sequence to sequence learning
machine	transla>on	
problem with neural networks. our approach is closely related to kalchbrenner and blunsom [18]
who were the    rst to map the entire input sentence to vector, and is related to cho et al. [5] although
the latter was used only for rescoring hypotheses produced by a phrase-based system. graves [10]
introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-
       source	sentence	mapped	to	vector,	then	output	sentence	
ferent parts of their input, and an elegant variant of this idea was successfully applied to machine
translation by bahdanau et al. [2]. the connectionist sequence classi   cation is another popular
generated.		
technique for mapping sequences to sequences with neural networks, but it assumes a monotonic
alignment between the inputs and the outputs [11].

figure 1: our model reads an input sentence    abc    and produces    wxyz    as the output sentence. the
       sequence	to	sequence	learning	with	neural	networks	by	
model stops making predictions after outputting the end-of-sentence token. note that the lstm reads the
input sentence in reverse, because doing so introduces many short term dependencies in the data that make the
sutskever	et	al.	2014;	luong	et	al.	2016	
optimization problem much easier.
the main result of this work is the following. on the wmt   14 english to french translation task,
we obtained a id7 score of 34.81 by directly extracting translations from an ensemble of 5 deep
       about	to	replace	very	complex	hand	engineered	architectures	
lstms (with 384m parameters and 8,000 dimensional state each) using a simple left-to-right beam-
search decoder. this is by far the best result achieved by direct translation with large neural net-
works. for comparison, the id7 score of an smt baseline on this dataset is 33.30 [29]. the 34.81
id7 score was achieved by an lstm with a vocabulary of 80k words, so the score was penalized
whenever the reference translation contained a word not covered by these 80k. this result shows
that a relatively unoptimized small-vocabulary neural network architecture which has much room
richard	socher	
for improvement outperforms a phrase-based smt system.

lecture	1,	slide	30	

3/31/16	

lecture	1,	slide	31	

richard	socher	

3/31/16	

representa>on	for	all	levels:	vectors	
       we	will	learn	in	the	next	lecture	how	we	can	learn	vector	

representa7ons	for	words	and	what	they	actually	represent.	

       next	week:	neural	networks	and	how	they	can	use	these	vectors	

for	all	nlp	levels	and	many	di   erent	applica7ons	

lecture	1,	slide	32	

richard	socher	

3/31/16	

