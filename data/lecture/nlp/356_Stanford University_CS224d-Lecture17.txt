dynamic	memory	networks	for	
question	answering	over	text	

and	images
richard	socher

joint	work	with	the	metamind	team

caiming xiong,	stephen	merity,	james	bradbury,	

ankit kumar,	ozan irsoy and	others

current	research

all	nlp/ai	tasks	can

be	reduced	to	

question	answering

qa	examples

i: mary walked to the bathroom.
i: sandra went to the garden.
i: daniel went back to the garden.
i: sandra took the milk there.
q: where is the milk?

i: mary walked to the bathroom.
i: sandra went to the garden.
i: daniel went back to the garden.
i: sandra took the milk there.
q: where is the milk?
a: garden
i: everybody is happy.
q: what   s the sentiment?
a: positive

i: everybody is happy.
q: what   s the sentiment?

i: jane has a baby in dresden.
q: what are the named entities?
a: jane - person, dresden - location
i: jane has a baby in dresden.
q: what are the pos tags?
a: nnp vbz dt nn in nnp .
i: i think this model is incredible
q: in french?
a: je pense que ce mod`ele est incroyable.

i: jane has a baby in dresden.
q: what are the named entities?
a: jane - person, dresden - location
i: jane has a baby in dresden.
q: what are the pos tags?
a: nnp vbz dt nn in nnp .
i: i think this model is incredible
q: in french?
a: je pense que ce mod`ele est incroyable.

figure 1: example inputs and questions together with answers all of which are generated by the
same dynamic memory network.

figure 1: example inputs and questions together with answers all of which are generated by the
same dynamic memory network.

2 the dynamic memory network

2 the dynamic memory network

goal

a	joint	model	for	

general	qa

first	major	obstacle

    for	nlp	no	single	model	architecture with	

consistent	state	of	the	art	results	across	tasks
task
question	answering	
(babi)
sentiment	analysis
(sst)
part	of	speech	tagging
(ptb-wsj)

state	of	the	art	model
strongly	supervised	memnn
(weston	et	al	2015)
tree-lstms	(tai	et	al.	2015)

bi-directional	lstm-crf	
(huang	et	al.	2015)	

second	major	obstacle

    fully	joint	multitask	learning*	is	hard:

    usually	restricted	to	lower	layers
    usually	helps	only	if	tasks	are	related
    often	hurts	performance	if	tasks	are	not	related

*	meaning:	same	decoder/classifier	

and	not	only	transfer	learning

tackling	first	obstacle

dynamic	memory	

networks	

an	architecture	for	any	qa	task

high	level	idea	for	harder	questions

    imagine	having	to	read	an	

article,	memorize it,	then	get	
asked	various	questions	  
hard!

    you	can't	store	everything	in	

working	memory

    optimal: give	you	the	input	
data,	give	you	the	question,	
allow	as	many	glances	as	
possible

basic	lego	block:	id56s

word vectors are stored in the semantic memory described in the next subsection.
furthermore, word vectors are given as inputs to a recurrent neural network (id56) sequence model
[5] to compute context-dependent representations at each time step: wt = seq model(vt, wt 1)
resulting in the full sequence w . other modules can access both the original word vectors as well as
the hidden states wt. in particular, we use a gated recurrent network (gru) [6, 7]. we also explored
the more complex lstm [8] but it performed similarly and is more complex. both work much
better than the standard single layer tanh id56 and we postulate that the main strength comes from
having gates that allow the model to suffer less from the vanishing gradient problem.
gru de   nition: we de   ne the gru subcomponent in general since it is used for various sequences
inside dmn modules. assume each time step has an input xt and a hidden node ht. we will
abbreviate the below computation with ht = gru (xt, ht 1):

    gated	recurrent	unit	(gru),	cho	et	al.	2014
    a	type	of	recurrent	neural	network	(id56),	similar	to	the	lstm	
    consumes	and/or	generates	sequences	(chars,	words,	...)
    the	gru	updates	an	internal	state	h according	to	the	existing	state	

h and	the	current	input	x:	

zt =     w (z)xt + u (z)ht 1 + b(z)   
rt =     w (r)xt + u (r)ht 1 + b(r)   
  ht = tanh   w xt + rt   u ht 1 + b(h)   
ht = zt   ht 1 + (1   zt)     ht,

where   is an element-wise product.
in some cases, there is a direct output in terms of a word
(equivalent to a standard supervised class label) which is computed via yt = softmax(w (s)ht).
depending on which module is being described, the gru notation changes but the internal states
and equations are the same. for the input module, we have wt = gru (vt, wt).

dmn	overview

semantic memory
module 

episodic memory
module

2

e 1

0.0

2

e 2

0.3

(glove vectors)

1

e 1

0.3

1

e 2

0.0

2

e 3

0.0

1

e 3

0.0

2

e 4

0.0

1

e 4

0.0

2

e 5

0.0

2

e 6

0.9

1

e 5

0.0

1

e 6

0.0

2

e7

1

e7

0.0

1.0

2

e 8

0.0

1

e 8

0.0

m2

m1

answer module

hallway

<eos>

input module

s 1

s 2

s 3

s 4

s 5

s 6

s7

s 8

question module

q

w 1

mary got the milk there.
john moved to the bedroom.
mary travelled to the hallway.
sandra went back to the kitchen.

john got the football there.

john went to the hallway.
john put down the football.

mary went to the garden.

wt

where is the fooball?

figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
each search over inputs. we do not draw connections for gates that are close to zero. see section
4.1 for details on the dataset that this example comes from.

the	modules:	input

semantic memory
semantic memory
module 
module 

episodic memory
episodic memory
module
module

2
2

e 1
e 1

0.0
0.0

(glove vectors)
(glove vectors)

1
1

e 1
e 1

0.3
0.3

2
2

e 2
e 2

0.3
0.3

1
1

e 2
e 2

0.0
0.0

2
2

e 3
e 3

0.0
0.0

1
1

e 3
e 3

0.0
0.0

2
2

e 4
e 4

0.0
0.0

1
1

e 4
e 4

0.0
0.0

2
2

e 5
e 5

0.0
0.0

1
1

e 5
e 5

0.0
0.0

2
2

e 6
e 6

0.9
0.9

1
1

e 6
e 6

0.0
0.0

2
2

e7
e7

1
1

e7
e7

0.0
0.0

1.0
1.0

2
2

e 8
e 8

0.0
0.0

1
1

e 8
e 8

0.0
0.0

m2
m2

m1
m1

answer module
answer module

hallway
hallway

<eos>
<eos>

input module
input module

s 1
s 1

s 2
s 2

s 3
s 3

s 4
s 4

s 5
s 5

s 6
s 6

s7
s7

s 8
s 8

question module
question module

q
q

w 1
w 1

mary got the milk there.
john moved to the bedroom.
john got the football there.
mary travelled to the hallway.
sandra went back to the kitchen.
mary got the milk there.
john moved to the bedroom.
john got the football there.
mary travelled to the hallway.
sandra went back to the kitchen.

john went to the hallway.
john put down the football.
john went to the hallway.
john put down the football.

mary went to the garden.
mary went to the garden.

wt
wt

where is the fooball?
where is the fooball?

figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
each search over inputs. we do not draw connections for gates that are close to zero. see section
each search over inputs. we do not draw connections for gates that are close to zero. see section
4.1 for details on the dataset that this example comes from.
4.1 for details on the dataset that this example comes from.

further	improvement:	bigru

dynamic memory networks for visual and textual id53

facts

input fusion

layer

textual input module
f1

f2

gru
gru

f1

gru
gru

f2

sentence
positional encoder
reader
 1
w 2

 1
w 3

 1
w 1

 1
w 4

 2
w 4

positional encoder

 2
w 2

 2
w 3

 2
w 1

positional encoder

 3
w 2

 3
w 3

 3
w 1

f3

gru
gru

f3

 3
w 4

facts

input 

fusion layer

feature 
embedding

visual feature 

extraction

figure 3. vqa input module to represent images for the dmn.

the	modules:	question

semantic memory
semantic memory
module 
module 

episodic memory
episodic memory
module
module

2
2

e 1
e 1

0.0
0.0

(glove vectors)
(glove vectors)

1
1

e 1
e 1

0.3
0.3

2
2

e 2
e 2

0.3
0.3

1
1

e 2
e 2

0.0
0.0

2
2

e 3
e 3

0.0
0.0

1
1

e 3
e 3

0.0
0.0

2
2

e 4
e 4

0.0
0.0

1
1

e 4
e 4

0.0
0.0

2
2

e 5
e 5

0.0
0.0

1
1

e 5
e 5

0.0
0.0

2
2

e 6
e 6

0.9
0.9

1
1

e 6
e 6

0.0
0.0

2
2

e7
e7

1
1

e7
e7

0.0
0.0

1.0
1.0

2
2

e 8
e 8

0.0
0.0

1
1

e 8
e 8

0.0
0.0

m2
m2

m1
m1

answer module
answer module

hallway
hallway

<eos>
<eos>

input module
input module

s 1
s 1

s 2
s 2

s 3
s 3

s 4
s 4

s 5
s 5

s 6
s 6

s7
s7

s 8
s 8

question module
question module

q
q

w 1
w 1

mary got the milk there.
john moved to the bedroom.
mary travelled to the hallway.
sandra went back to the kitchen.
mary got the milk there.
john moved to the bedroom.
mary travelled to the hallway.
sandra went back to the kitchen.

john got the football there.
john got the football there.

john went to the hallway.
john put down the football.
john went to the hallway.
john put down the football.

mary went to the garden.
mary went to the garden.

wt
wt

where is the fooball?
where is the fooball?

figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
each search over inputs. we do not draw connections for gates that are close to zero. see section
each search over inputs. we do not draw connections for gates that are close to zero. see section
4.1 for details on the dataset that this example comes from.
4.1 for details on the dataset that this example comes from.

standard	gru.	output:	last	hidden	state	   q

to take multiple passes over the facts, focusing attention on different facts at each pass. each pass
to take multiple passes over the facts, focusing attention on different facts at each pass. each pass

the	modules:	episodic	memory

episodic memory
episodic memory
module
module

semantic memory
semantic memory
module 
module 

we propose a modi   cation to the gru architecture by em-
bedding information from the attention mechanism. the
update gate ui in equation 1 decides how much of each di-
mension of the hidden state to retain and how much should
be updated with the transformed input xi from the current
timestep. as ui is computed using only the current input
and the hidden state from previous timesteps, it lacks any
knowledge from the question or previous episode memory.
by replacing the update gate ui in the gru (equation 1)
mary got the milk there.
john moved to the bedroom.
john got the football there.
mary travelled to the hallway.
sandra went back to the kitchen.
mary got the milk there.
john moved to the bedroom.
john got the football there.
mary travelled to the hallway.
sandra went back to the kitchen.
with the output of the attention gate gt
i (equation 10) in
equation 4, the gru can now use the attention gate for
updating its internal state. this change is depicted in fig 5.

john went to the hallway.
john put down the football.
john went to the hallway.
john put down the football.

mary went to the garden.
mary went to the garden.

input module
input module

(glove vectors)
(glove vectors)

wt
wt

w 1
w 1

0.0
0.0

0.3
0.3

0.0
0.0

0.0
0.0

e 1
e 1

0.0
0.0

e 2
e 2

0.9
0.9

e 3
e 3

0.0
0.0

0.0
0.0

e 4
e 4

e 5
e 5

e 6
e 6

e 8
e 8

e 1
e 1

0.3
0.3

e 2
e 2

0.0
0.0

e 3
e 3

0.0
0.0

e 4
e 4

0.0
0.0

0.0
0.0

e 5
e 5

0.0
0.0

e 6
e 6

1.0
1.0

0.0
0.0

e 8
e 8

e7
e7

s 1
s 1

s 2
s 2

s 3
s 3

s 4
s 4

s 5
s 5

s 6
s 6

s 8
s 8

e7
e7

s7
s7

2
2

2
2

2
2

2
2

2
2

2
2

2
2

2
2

1
1

1
1

1
1

1
1

1
1

1
1

1
1

1
1

dynamic memory networks for visual and textual id53

answer module
answer module

hallway
hallway

<eos>
<eos>

m2
m2

m1
m1

question module
question module

q
q

where is the fooball?
where is the fooball?

m2m2

attngru

episodic memory pass 2

attention mechanism
. . .
attngru

hi =gt

figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
t are shown above the corresponding vectors. the gates change with
speci   c question. gate values gi
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
each search over inputs. we do not draw connections for gates that are close to zero. see section
each search over inputs. we do not draw connections for gates that are close to zero. see section
hi
4.1 for details on the dataset that this example comes from.
4.1 for details on the dataset that this example comes from.

i     hi + (1   gt
i)   hi 1
an important consideration is that gt
i is a scalar, generated
using a softmax activation, as opposed to the vector ui 2

figure 5. (a) the traditional gru model, and (b) the proposed
to take multiple passes over the facts, focusing attention on different facts at each pass. each pass
to take multiple passes over the facts, focusing attention on different facts at each pass. each pass
attention-based gru model
produces an episode, and these episodes are then summarized into the memory. endowing our
produces an episode, and these episodes are then summarized into the memory. endowing our

(11)

attention mechanism
. . .
attngru

episodic memory pass 1

memory update

memory update

gate attention

attngru

attngru

attngru

gt
i

ui

out

out

 !f !f

  hi

  hi

ri

ri

hi

(b) 

m1m1

(a)

. . .

. . .

. . .

in

in

c1c1

c2c2

mt = gru (ct, mt 1)

the	modules:	episodic	memory

structed contextual vector ct, producing mt. in the dmn,
a gru with the initial hidden state set to the question vec-
tor q is used for this purpose. the episodic memory for
pass t is computed by

episode   s memory to simply be the question m0 = q. next we compute a series of gates, one for
each sentence in the input. the gate basically captures how relevant that sentence is for the current
question and takes into account what else the model has already stored in its memory.
for instance, in the    rst set of inputs of fig. 1, we may ask where is mary? and would hope that
the gate for the    rst sentence is close to 1, whereas all other gates of sentences that do not mention
mary would be close to 0.
(12)
the gating function g takes as input a sentence vector at time step t, the current memory vector
    gates	are	activated	if	relevant	to	the	question
and the question vector: g1
t = g(st, m0, q) and returns a single scalar g. we de   ne the function
g(s, m, q) as follows:
the work of sukhbaatar et al. (2015) suggests that using
different weights for each pass through the episodic mem-
ory may be advantageous. when the model contains only
one set of weights for all episodic passes over the input, it
is referred to as a tied model, as in the    mem weights    row
in table 1.
4
following the memory update component used in
sukhbaatar et al. (2015) and peng et al. (2015) we experi-
ment with using a relu layer for the memory update, cal-
culating the new episode memory state by
relevant	facts	are	summarized	in	another	gru	
or	simple	nnet
(13)

    when	the	end	of	the	input	is	reached,	the	
mt = relu w t[mt 1; ct; q] + b 

z(s, m, q) = [s   q, s   m,|s   q|,|s   m|, s, m, q, st w (b)q, st w (b)m]
g(s, m, q) =     w (2) tanh   w (1)z(s, m, q) + b(1)    + b(2)   

(5)
(6)

where ; is the concatenation operator, w t 2 rnh   nh , b 2
rnh , and nh is the hidden size. the untying of weights
and using this relu formulation for the memory update

low neural network models to use a question to selectively
pay attention to speci   c inputs. they can bene   t image
classi   cation (stollenga et al., 2014), generating captions
for images (xu et al., 2015), among others mentioned be-
low, and machine translation (cho et al., 2014; bahdanau
et al., 2015; luong et al., 2015). other recent neural ar-
chitectures with memory or attention which have proposed
include id63s (graves et al., 2014), neu-
ral gpus (kaiser & sutskever, 2015) and stack-augmented
id56s (joulin & mikolov, 2015).
id53 in nlp id53 involv-
ing natural language can be solved in a variety of ways to
which we cannot all do justice. if the potential input is a
large text corpus, qa becomes a combination of informa-
tion retrieval and extraction (yates et al., 2007). neural
approaches can include reasoning over knowledge bases,
(bordes et al., 2012; socher et al., 2013a) or directly via
sentences for trivia competitions (iyyer et al., 2014).
visual id53 (vqa) in comparison to qa
in nlp, vqa is still a relatively young task that is feasible
only now that objects can be identi   ed with high accuracy.
the    rst large scale database with unconstrained questions
about images was introduced by antol et al. (2015). while

the	modules:	episodic	memory
    if	summary	is	insufficient	to	answer	the	question,	

repeat	sequence	over	input

semantic memory
semantic memory
module 
module 

(glove vectors)
(glove vectors)

episodic memory
episodic memory
module
module

2
2

e 1
e 1

2
2

e 2
e 2

0.3
0.3

0.0
0.0

2
2

e 3
e 3

0.0
0.0

2
2

e 4
e 4

0.0
0.0

2
2

e 5
e 5

0.0
0.0

2
2

e 6
e 6

0.9
0.9

1
1

e 1
e 1

0.3
0.3

1
1

e 2
e 2

0.0
0.0

1
1

e 3
e 3

0.0
0.0

1
1

e 4
e 4

0.0
0.0

1
1

e 5
e 5

0.0
0.0

1
1

e 6
e 6

0.0
0.0

2
2

e7
e7

1
1

e7
e7

0.0
0.0

1.0
1.0

2
2

e 8
e 8

0.0
0.0

1
1

e 8
e 8

0.0
0.0

m2
m2

m1
m1

answer module
answer module

hallway
hallway

<eos>
<eos>

input module
input module

s 1
s 1

s 2
s 2

s 3
s 3

s 4
s 4

s 5
s 5

s 6
s 6

s7
s7

s 8
s 8

question module
question module

q
q

w 1
w 1

mary got the milk there.
john moved to the bedroom.
john got the football there.
mary travelled to the hallway.
sandra went back to the kitchen.
mary got the milk there.
john moved to the bedroom.
john got the football there.
mary travelled to the hallway.
sandra went back to the kitchen.

john went to the hallway.
john put down the football.
john went to the hallway.
john put down the football.

mary went to the garden.
mary went to the garden.

wt
wt

where is the fooball?
where is the fooball?

figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
each search over inputs. we do not draw connections for gates that are close to zero. see section
each search over inputs. we do not draw connections for gates that are close to zero. see section

inspiration	from	neuroscience

    episodic	memory is	the	memory of	

autobiographical	events	(times,	places,	etc).	a	
collection	of	past	personal	experiences	that	
occurred	at	a	particular	time	and	place.

    the	hippocampus,	the	seat	of	episodic	memory	in	

humans,	is	active	during	transitive	id136

    in	the	dmn	repeated	passes	over	the	input	are	

needed	for	transitive	id136

the	modules:	answer

input	to	answer	gru:	a	=	[m	q]

gru	for	sequences,	standard	softmax for	single	class

semantic memory
module 

episodic memory
module

2

e 1

0.0

2

e 2

0.3

(glove vectors)

1

e 1

0.3

1

e 2

0.0

2

e 3

0.0

1

e 3

0.0

2

e 4

0.0

1

e 4

0.0

2

e 5

0.0

2

e 6

0.9

1

e 5

0.0

1

e 6

0.0

2

e7

1

e7

0.0

1.0

2

e 8

0.0

1

e 8

0.0

m2

m1

answer module

hallway

<eos>

input module

s 1

s 2

s 3

s 4

s 5

s 6

s7

s 8

question module

q

w 1

mary got the milk there.
john moved to the bedroom.
mary travelled to the hallway.
sandra went back to the kitchen.

john got the football there.

john went to the hallway.
john put down the football.

mary went to the garden.

wt

where is the fooball?

figure 3: real example of an input sentence sequence and the attention gates that are triggered by a
speci   c question. gate values gi
t are shown above the corresponding vectors. the gates change with
each search over inputs. we do not draw connections for gates that are close to zero. see section

academic	papers	and	related	work

for	full	details:	

   
    ask	me	anything:	dynamic	memory	networks	for	natural	language	

processing (kumar	et	al.,	2015)

    dynamic	memory	networks	for	visual	and	textual	question	

answering (xiong et	al.,	2016)	

sequence	to	sequence	(sutskever et	al.	2014)

   
    neural	turing	machines	(graves	et	al.	2014)
   
   
   

teaching	machines	to	read	and	comprehend	(hermann	et	al.	2015)
learning	to	transduce	with	unbounded	memory	(grefenstette 2015)
structured	memory	for	neural	turing	machines	(wei	zhang	2015)

    memory	networks	(weston	et	al.	2015)
   

end	to	end	memory	networks	(sukhbaatar et	al.	2015)
  

comparison	to	memnets

similarities:
    memnets and	dmns	have	input,	scoring,	attention	and	response	

mechanisms

differences:
    for	input	representations	memnets use	bag	of	word,	nonlinear	or	

linear	embeddings	that	explicitly	encode	position	

    memnets iteratively	run	functions	for	attention	and	response

    dmns	shows	that	neural	sequence	models	can	be	used	for	
input	representation,	attention	and	response	mechanisms	
   naturally	captures	position	and	temporality

    enables	broader	range	of	applications

4.1 id53

experiments:	qa	on	babi (1k)

the facebook babi dataset is a synthetic dataset meant to test a model   s ability to retrieve facts
and reason over them. each task tests a different skill that a good id53 model ought
to have, such as coreference resolution, deduction, and induction. training on the babi dataset

task
1: single supporting fact
2: two supporting facts
3: three supporting facts
4: two argument relations
5: three argument relations
6: yes/no questions
7: counting
8: lists/sets
9: simple negation
10: inde   nite knowledge

memnn dmn task

100
100
100
100
98
100
85
91
100
98

100
98.2
95.2
100
99.3
100
96.9
96.5
100
97.5

11: basic coreference
12: conjunction
13: compound coreference
14: time reasoning
15: basic deduction
16: basic induction
17: positional reasoning
18: size reasoning
19: path finding
20: agent   s motivations
mean accuracy (%)

memnn dmn
99.9
100
99.8
100
100
99.4
59.6
95.3
34.5
100
93.6

100
100
100
99
100
100
65
95
36
100
93.3

table 1: test accuracies on the babi dataset. memnn numbers taken from weston et al. [18]. the
dmn passes (accuracy > 95%) 18 tasks, whereas the memnn passes 16.

uses the following objective function: j =    ece(gates) +  ece(answers), where ece is the
this	still	requires	that	relevant	facts	are	marked	during	training	to	train	the	gates.
standard cross-id178 cost and     and   are hyperparameters. in practice, we begin training with    
set to 1 and   set to 0, and then later switch   to 1 while keeping     at 1. we subsample the facts
from the input module by end-of-sentence tokens. the gate supervision aims to select one sentence
per pass; thus, we also experimented with modifying eq. 6 to a simple softmax instead of a gru.
t) =
t here is the value of the gate before the sigmoid. this setting achieves better

here, we compute the    nal episode vector via: ei = pt

t)ct, where softmax(gi

t=1 softmax(gi

exp(gi
t)

j ), and gi

live	demo

experiments:	sentiment	analysis

ask me anything: dynamic memory networks for natural language processing

    stanford	sentiment	treebank

task
1: single supporting fact
2: two supporting facts
3: three supporting facts
4: two argument relations
5: three argument relations
6: yes/no questions
7: counting
8: lists/sets
9: simple negation
10: inde   nite knowledge
11: basic coreference
12: conjunction
13: compound coreference
14: time reasoning
15: basic deduction
16: basic induction
17: positional reasoning
18: size reasoning
19: path finding

100
    test	accuracies:
100
    mv-id56	and	rntn:	socher	et	al.	(2013)
100
    did98:	kalchbrenner et	al.	(2014)
100
    pvec:	le	&	mikolov.	(2014)
98
    id98-mc:	kim	(2014)
100
    did56:	irsoy &	cardie (2015)
85
    ct-lstm:	tai	et	al.	(2015)	
91
100
98
100
100
100
99
100
100
65
95
36

memnn dmn
100
98.2
95.2
100
99.3
100
96.9
96.5
100
97.5
99.9
100
99.8
100
100
99.4
59.6
95.3
34.5

binary
task
82.9
mv-id56
85.4
rntn
86.8
did98
87.8
pvec
88.1
id98-mc
did56
86.6
ct-lstm 88.0
88.6
dmn

fine-grained

44.4
45.7
48.5
48.7
47.4
49.8
51.0
52.1

table 2. test accuracies for id31 on the stanford
sentiment treebank. mv-id56 and rntn: socher et al. (2013).
did98: kalchbrenner et al. (2014). pvec: le & mikolov. (2014).
id98-mc: kim (2014). did56: irsoy & cardie (2015), 2014.
ct-lstm: tai et al. (2015)

we list results in table 1. the dmn does worse than
the memory network, which we refer to from here on as
memnn, on tasks 2 and 3, both tasks with long input se-
quences. we suspect that this is due to the recurrent input

analysis	of	number	of	episodes

ask me anything: dynamic memory networks for natural language processing

    how	many	attention	+	memory	passes	are	needed	in	the	episodic	memory?

max
passes
0 pass
1 pass
2 pass
3 pass
5 pass

task 3
three-facts

0
0
16.7
64.7
95.2

task 7
count
48.8
48.8
49.1
83.4
96.9

task 8
lists/sets

sentiment
(   ne grain)

33.6
54.0
55.6
83.4
96.5

50.0
51.5
52.1
50.1
n/a

table 4. effectiveness of episodic memory module across tasks.
each row shows the    nal accuracy in term of percentages with
a different maximum limit for the number of passes the episodic
memory module can take. note that for the 0-pass dmn, the
network essential reduces to the output of the attention module.

[done reading]

analysis	of	attention	for	sentiment
table 5. an example of what the dmn focuses on during each episode on a real query in the babi task. darker colors mean that the
attention weight is higher.
    sharper	attention	when	2	passes	are	allowed.	
    examples	that	are	wrong	with	just	one	pass

analysis	of	attention	for	sentiment

figure 4. attention weights for sentiment examples that were

bill went back to the cinema yesterday.
mary went to the bedroom this morning.
julie went back to the bedroom this afternoon.

analysis	of	attention	for	sentiment

table 5. an example of what the dmn focuses on during each episode on a real query in the babi task. darker colors mean that the

    examples	where	full	sentence	context	from	first	pass	changes	

attention	to	words	more	relevant	for	final	prediction

analysis	of	attention	for	sentiment

    examples	where	full	sentence	context	from	first	pass	changes	

attention	to	words	more	relevant	for	final	prediction

live	demo

modeling very long inputs. the memnn does not suffer from this problem as it views each sentence
seperately. the power of the episodic memory module is evident in tasks 7 and 8, where the dmn
signi   cantly outperforms the memnn. both tasks require the model to iteratively retrieve facts and
store them in a representation that slowly incorporates more of the relevant information of the input
sequence. both models do poorly on tasks 17 and 19, though the memnn does better. we suspect
this is due to the memnn using id165 features as well as explicit sequence position features.

experiments:	pos	tagging

4.2 sequence tagging: id52

    ptb	wsj,	standard	splits
    episodic	memory	does	not	require	multiple	passes,	single	pass	enough

part-of-speech tagging is traditionally modeled as a sequence tagging problem: every word in a
sentence is to be classi   ed into its part-of-speech class (see fig. 1). we evaluate on the standard
wall street journal dataset included in penn-iii [26]. we use the standard splits of sections 0-18
for training, 19-21 for development and 22-24 for test sets [27]. since this is a word level tagging
task, dmn memories are produced at the word -rather than sentence- level. we compare the dmn

model
acc (%)

id166tool

97.15

sogaard
97.27

suzuki et al.

spoustova et al.

97.40

97.44

sid98 dmn
97.56
97.50

table 2: test accuracies on wsj-ptb

with the results in [27]. the dmn achieves state-of-the-art accuracy with a single model, reaching
a development set accuracy of 97.5. ensembling the top 4 development models, the dmn gets to
97.58 dev and 97.56 test accuracies, achieving a new state-of-the-art (table 2).

7

live	demo

{cmxiong,smerity,richard}metamind.io

modularization	allows	for	different	inputs

episodic memory

answer
kitchen

episodic memory

answer
palm

input module

 john moved to the 
garden.
 john got the apple there.
 john moved to the 
kitchen.
 sandra picked up the 
milk there.
 john dropped the apple.
 john moved to the 
of   ce.

input module

question
where is 
the 
apple?

question

what kind 
of tree is 
in the 
backgrou
nd?

(a) text question-answering

(b) visual question-answering

figure 1. id53 over text and images using a dy-

dynamic memory networks for visual and textual id53

input	module	for	images

input module

gru
gru

w

gru
gru

w

id98

14

14

gru
gru

w

512

 

i

n
o
s
u
f
 
t
u
p
n

r
e
y
a

l

i

 

e
r
u
t
a
e
f

i

g
n
d
d
e
b
m
e

 

e
r
u
t
a
e
f
 
l

a
u
s
v

i

n
o
i
t
c
a
r
t
x
e

figure 3. vqa input module to represent images for the dmn.

accuracy:	visual	question	answering	

dynamic memory networks for visual and textual id53

    dppnet - noh	et	al.	

dmn+
e2e nr
vqa	test-dev and	
0.3
-
0.3
test-standard:
-
2.1
1.1
    antol et	al.	(2015)
0.5
0.8
-
    ack	wu	et	al.	(2015);
-
0.1
0.0
    ibowimg - zhou	et	al.	
-
2.0
2.4
(2015);
0.0
0.9
-
0.0
-
0.3
(2015);	d-nmn	- andreas	
0.0
0.1
-
et	al.	(2016);	
0.2
-
0.1
45.3
51.8
-
0.9
18.6
4.2
2.1
5.3
-
1.6
2.3
0.0
-
4.2
2.8
1
3
-

    san	- yang	et	al.	(2015)	

5: 3 argument relations

17: positional reasoning

failed tasks (err >5%)

table 2. test error rates of various model architectures on tasks
from the the babi english 10k dataset. e2e = end-to-end mem-

all

method
vqa
64.0
28.1
image
75.7
48.1
question
75.6
52.6
q+i
78.9
53.7
lstm q+i
79.2
ack
55.7
76.5
ibowimg 55.7
80.7
57.2
dppnet
80.5
d-nmn
57.9
58.7
san
79.3
60.3 80.5
dmn+

test-dev

y/n other num

test-std
all

-
-
-
54.1
56.0
55.9
57.4
58.0
58.9
60.4

3.8
27.1
37.4
36.4
40.1
42.6
41.7
43.1
46.1
48.3

0.4
36.7
33.7
35.2
36.1
35.0
37.2
37.4
36.6
36.8

table 3. performance of various architectures and approaches on
vqa test-dev and test-standard data. vqa numbers are from
antol et al. (2015); ack wu et al. (2015);ibowimg -zhou
et al. (2015);dppnet - noh et al. (2015); d-nmn - andreas et al.
(2016); san -yang et al. (2015)

attention	visualization

dynamic memory networks for visual and textual id53

what is the main color on 

the bus ?

answer: blue

what type of trees are in 

the background ?

answer: pine

which man is dressed more

how many pink    ags

are there ?

answer: 2

is this in the wild ?

answer: no

what time of day was this 

dynamic memory networks for visual and textual id53
is this in the wild ?

answer: 2

answer: no

attention	visualization

what time of day was this 

picture taken ?

answer: night

answer: pine
answer: metal

which man is dressed more

what color are 
    amboyantly ?
the bananas ?

answer: right
answer: green

who is on both photos ?

answer: girl

answer: stripes

what time of day was this 

did the player hit
the ball ?
picture taken ?

answer: yes
answer: night

what is the boy holding ?

answer: surfboard

figure 4. examples of qualitative results of attention for vqa. each image (left) is shown with the attention that the episodic memory
module places on each region (right). answers are given by the dmn+.

attention	visualization

is this in the wild ?

answer: 2

how many pink    ags

are there ?

answer: no

what time of day was this 

what is this sculpture 

made out of ?

answer: metal

what color are 
the bananas ?

answer: green

who is on both photos ?

what is the pattern on the 

cat ' s fur on its tail ?

answer: stripes

did the player hit

the ball ?

answer: yes

what is the boy holding ?

figure 4. examples of qualitative results of attention for vqa. each image (left) is shown with the attention that the episodic memory
module places on each region (right). answers are given by the dmn+.

live	demo

summary

    most	nlp	tasks	can	be	reduced	to	qa
    dmn	accurately	solves	variety	of	qa	tasks
    next	goals:	one	joint	multitask	dmn

