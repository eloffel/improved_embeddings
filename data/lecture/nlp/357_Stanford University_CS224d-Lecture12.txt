cs224d:	deep	nlp

lecture	12:

midterm	review

richard	socher

richard@metamind.io

overview	today	    mostly	open	for	questions!
    linguistic	background:	levels	and	tasks

    word	vectors

    backprop

    id56s

lecture	1,	slide	2

richard	socher

5/5/16

overview	of	linguistic	levels

lecture	1,	slide	3

richard	socher

5/5/16

tasks:	ner

lecture	1,	slide	4

richard	socher

5/5/16

tasks:	pos

lecture	1,	slide	5

richard	socher

5/5/16

tasks:	sentiment	analysis

lecture	1,	slide	6

richard	socher

5/5/16

machine	translation

lecture	1,	slide	7

richard	socher

5/5/16

skip-gram

i task: given a center word, predict its

context words

i for each word, we have an    input vector   

vw and an    output vector    v0w

figure 1: new model architectures. the cbow architecture predicts the current word based on the
context, and the skip-gram predicts surrounding words given the current word.

r words from the future of the current word as correct labels. this will require us to do r   2
word classi   cations, with the current word as input, and each of the r + r words as output. in the
following experiments, we use c = 10.

w(t)          input         projection      outputw(t-2)w(t-1)w(t+1)w(t+2)skip-gram v.s. cbow

skip-gram

cbow

vwi
figure 1: new model architectures. the cbow architecture predicts the current word based on the
context, and the skip-gram predicts surrounding words given the current word.

task center word ! context
r

context ! center word

f (vwi c ,       , vwi 1, vwi+1,       , vwi+c )

figure 1: new model architectures. the cbow architecture predicts the current word based on the
context, and the skip-gram predicts surrounding words given the current word.

r words from the future of the current word as correct labels. this will require us to do r   2
word classi   cations, with the current word as input, and each of the r + r words as output. in the
following experiments, we use c = 10.

all id97    gures are from http://arxiv.org/pdf/1301.3781.pdf

r words from the future of the current word as correct labels. this will require us to do r   2
word classi   cations, with the current word as input, and each of the r + r words as output. in the
following experiments, we use c = 10.

4 results

w(t)          input         projection      outputw(t-2)w(t-1)w(t+1)w(t+2)w(t-2)w(t+1)w(t-1)w(t+2)w(t)sum       input         projection         outputid97 as id105 (conceptually)

i id105

24 m 35n   n

   24

.
a>
.

35n   k    . b .    k   n

mij     a>i bj

i imagine m is a matrix of counts for events co-occurring, but
we only get to observe the co-occurrences one at a time. e.g.

m =24

1 0 4
0 0 2
1 3 0

35

but we only see

(1,1), (2,3), (3,2), (2,3), (1,3), . . .

id97 as id105 (conceptually)

mij     a>i bj

i whenever we see a pair (i, j) co-occur, we try to increasing

a>i bj

i we also try to make all the other inner-products smaller to

account for pairs never observed (or unobserved yet), by
decreasing a>

  i bj and a>i b  j

i remember from the lecture that the word co-occurrence
matrix usually captures the semantic meaning of a word?
for id97 models, roughly speaking, m is the windowed
word co-occurrence matrix, a is the output vector matrix, and
b is the input vector matrix.

i why not just use one set of vectors? it   s equivalent to a = b

in our formulation here, but less constraints is usually easier
for optimization.

glove v.s. id97

fast
training

e cient
usage of
statistics

quality
a   ected
by size of
corpora

captures
complex
patterns

direct
prediction
(id97)

glove

no

no*

yes

scales
with size
of corpus

yes

yes

no

yes

* skip-gram and cbow are qualitatively di   erent when it comes to smaller corpora

cs224d: deep learning for nlp 

2 

overview 
       neural network example 
       terminology  
       example 1:  
       forward pass 
       id26 using chain rule 
       what is delta? from chain rule to modular error flow 

       example 2: 

       forward pass 
       id26  

cs224d: deep learning for nlp 

3 

neural networks 
       one of many different types of non-linear classifiers (i.e. 
leads to non-linear decision boundaries) 

 

       most common design involves the stacking of affine 
transformations followed by point-wise (element-wise) 
non-linearity 

cs224d: deep learning for nlp 

4 

an example of a neural network 

       this is a 4 layer neural network.  
       2 hidden-layer neural network. 
       2-10-10-3 neural network (complete architecture defn.)  

cs224d: deep learning for nlp 

5 

our first example 

layer 1 
(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

a4

x1 
x2 

x3 
x4 

layer 2 

layer 3 

(1) 

(2) 
z1
a1

(2) 

a1

(3)  a1
z1
1 

(3) s 

(2) 
z2
a2

(2) 

(1) 

       this is a 3 layer neural network  
       1 hidden-layer neural network 

our first example:  
terminology 

cs224d: deep learning for nlp 

6 

layer 2 

layer 3 

layer 1 
(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

a4

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(3)  a1
z1
1 

(3)  s 

(2) 
z2
a2

(2) 

(1) 

model input 

model output 

our first example:  
terminology 

cs224d: deep learning for nlp 

7 

layer 2 

layer 3 

layer 1 
(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

a4

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(3)  a1
z1
1 

(3)  s 

(2) 
z2
a2

(2) 

(1) 

model input 

model output 

activation units 

cs224d: deep learning for nlp 

8 

our first example:  
activation unit terminology 

(2)  a1
z1

  

(2) 

(2) 
z1

  

+ 

a1

(2) 

we draw this 

this is actually what   s going on 

(1) + w12

(2) = w11
(1)a3
z1
(2) is the 1st activation unit of layer 2 
a1
(2) =   (z1
a1

(1)a1
(2)) 

(1) + w13

(1)a2

(1) + w14

(1)a4

(1) 

our first example:  
forward pass 

cs224d: deep learning for nlp 

9 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

(1) = x1
z1
 
(1) = x2 
z2
(1) = x3 
z3
(1) = x4 
z4
 

our first example:  
forward pass 

cs224d: deep learning for nlp 

10 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

a1
a2
a3
a4

(1) = z1
(1) = z2
(1) = z3
(1) = z4

(1) 
(1) 
(1) 
(1) 

our first example:  
forward pass 

cs224d: deep learning for nlp 

11 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

(2) = w11
z1
(2) = w21
z2

(1)a1
(1)a1

(1) + w12
(1) + w22

(1)a2
(1)a2

(1) + w13
(1) + w23

(1)a3
(1)a3

(1) + w14
(1) + w24

(1)a4
(1)a4

(1) 
(1) 

our first example:  
forward pass 

cs224d: deep learning for nlp 

12 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

z(2) 
z1
(2)  
(2) = 
z2

w11
w21

w(1) 
(1) w13
(1) w23

(1) w12
(1) w22

(1) w14
(1) w24

(1) 
(1) 

a(1) 
a1
(1) 
a2
(1) 
a3
(1) 
a4
(1) 
 

our first example:  
forward pass 

cs224d: deep learning for nlp 

13 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

z(2) =w(1)a(1) 
 
affine transformation 

our first example:  
forward pass 

cs224d: deep learning for nlp 

14 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

a(2) =   (z(2)) 
point-wise/element-wise non-linearity 

our first example:  
forward pass 

cs224d: deep learning for nlp 

15 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

z(3) = w(2)a(2) 
affine transformation 

our first example:  
forward pass 

cs224d: deep learning for nlp 

16 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

a(3) = z(3) 
s = a(3)   

our first example:  
id26 
using chain rule 

cs224d: deep learning for nlp 

17 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

let us try to calculate the error gradient wrt w14
thus we want to find: 

(1) 

our first example:  
id26 
using chain rule 

x1 
x2 

cs224d: deep learning for nlp 

18 

a1

(1) 

(2) 
z1
a1

(2) 

(3)  a1
z1
1 

(3) s 

(2) 
z2
a2

(2) 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x3 
x4 

a4

(1) 

 
 
let us try to calculate the error gradient wrt w14
thus we want to find: 

(1) 

cs224d: deep learning for nlp 

19 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

our first example:  
id26 
using chain rule 

 
 
this is simply 1 

cs224d: deep learning for nlp 

20 

a1

(1) 

(2) 
z1
a1

(2) 

(3)  a1
z1
1 

(3) s 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

our first example:  
id26 
using chain rule 

x1 
x2 

 
 

(2) 

x3 
x4 

a4

(1) 

(2) 
z2
a2

!!!(!)!!!(!)
!!!(!)!!!(!)
!!!(!)
!!!"(!)!
!!!(!)!!!(!)
!(!!!(!)!!! +!!!"(!)!!(!))
!!!(!)
!!!"(!)!
!!!(!)

our first example:  
id26 
using chain rule 

 
 

cs224d: deep learning for nlp 

21 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

our first example:  
id26 
using chain rule 

cs224d: deep learning for nlp 

22 

a1

(1) 

(2) 
z1
a1

(2) 

(3)  a1
z1
1 

(3) s 

(2) 
z2
a2

(2) 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

 
 

!!!!!    !!!

a4

(1) 

!!!(!)
!!!"(!)!

our first example:  
id26 
using chain rule 

 
 

cs224d: deep learning for nlp 

23 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

our first example:  
id26 
using chain rule 

 
 

cs224d: deep learning for nlp 

24 

a1

(1) 

(2) 
z1
a1

(2) 

(3)  a1
z1
1 

(3) s 

(2) 
z2
a2

(2) 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a4

(1) 

!!!!!    !!! !!(!)!

  1

(2)  

cs224d: deep learning for nlp 

25 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

x1 
x2 

x3 
x4 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

(1) 

our first example:  
id26 
observations 
we got error  
gradient wrt w14
 
 
 
required:  
      
      
      
 

the signal forwarded by w14
(1) =  a4
(1)  
the error propagating backwards w11
(2) 
the local gradient      (z1
(2)) 

x1 
x2 

x3 
x4 

(1) 

our first example:  
id26 
observations 
we tried to get error  
gradient wrt w14
 
required:  
      
 
the signal forwarded by w14
(1) =  a4
(1)  
      
the error propagating backwards w11
(2) 
the local gradient      (z1
 
      
(2)) 
 
we can do this for 
all of w(1): 
 
(as outer product) 
 

  1
(2)a1
  2
(2)a1
  1
  2

(2) 
(2) 

cs224d: deep learning for nlp 

26 

(1) 
z1
1 
z2
(1) 
1 
(1) 
z3
1 
(1) 
z4
1 

a1

(1) 

(2) 
z1
a1

(2) 

(2) 
z2
a2

(2) 

a4

(1) 

(3)  a1
z1
1 

(3) s 

(1)    1
(1)    2
(1) a2
a1

(2)a2
(2)a2
(1 a3

(1)    1
(1)    2
(1) a4

(2)a3
(2)a3
(1)  

(1)    1
(1)    2

(2)a4
(2)a4

(1)  
(1)  

cs224d: deep learning for nlp 

27 

our first example:  
let us define    

(2) 
z1

  

+ 

a1

(2) 

  1

(2)    

+ 

 
 

 
 

recall that this is forward pass 

this is the id26 

(2) is the error flowing backwards at the same  

(2) passed forwards. thus it is simply the gradient 

  1
point where z1
of the error wrt z1

(2). 

cs224d: deep learning for nlp 

28 

our first example:  
id26 using error vectors 
the chain rule of differentiation just boils 
down very simple patterns in error 
id26: 
 
1.    an error x flowing backwards passes a 

orange = backprop.
green = fwd. pass  

neuron by getting amplified by the local 
gradient. 

 
2.    an error    that needs to go through an 
affine transformation distributes itself in 
the way signal combined in forward pass. 

 
 

  

+ 

x 

   
z 

   =      (z)x 
  w1 
a1w1 
  w2 
a2w2 
  w3 
a3w3 

cs224d: deep learning for nlp 

29 

our first example:  
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

cs224d: deep learning for nlp 

30 

our first example:  
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

  (3) 

this is             for softmax 

cs224d: deep learning for nlp 

31 

our first example:  
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

  (3) 

gradient w.r.t w(2) =   (3)a(2)t 

cs224d: deep learning for nlp 

32 

our first example:  
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

w(2)t   (3) 

  (3) 

--reusing the   (3) for downstream updates. 
--moving error vector across affine transformation simply requires multiplication with 
the transpose of forward matrix 
--notice that the dimensions will line up perfectly too! 

cs224d: deep learning for nlp 

33 

our first example:  
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

     (z(2))!w(2)t   (3) 
 
=   (2) 

w(2)t   (3) 

 
--moving error vector across point-wise non-linearity requires point-wise 
multiplication with local gradient of the non-linearity 

cs224d: deep learning for nlp 

34 

our first example:  
id26 using error vectors 

z(1) 

1 

a(1) 

 w(1) 

z(2) 

   

a(2) 

 w(2) 

z(3) 

1 

s 

w(1)t   (2) 

  (2) 

gradient w.r.t w(1) =   (2)a(1)t 

cs224d: deep learning for nlp 

35 

our second example (4-layer network):  
id26 using error vectors 

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

cs224d: deep learning for nlp 

36 

our second example (4-layer network):  
id26 using error vectors 

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

yp    y =   (4) 
  

cs224d: deep learning for nlp 

37 

our second example (4-layer network):  
id26 using error vectors 
grad w(3) =   (4)a(3)t 
  

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

w(3)t  (4) 
  

  (4) 
  

cs224d: deep learning for nlp 

38 

our second example (4-layer network):  
id26 using error vectors 

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

  (3)=      (z(3))!w(3)t  (4) 

 

w(3)t  (4) 

cs224d: deep learning for nlp 

39 

our second example (4-layer network):  
id26 using error vectors 

grad w(2) =   (3)a(2)t 
  

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

w(2)t  (3) 

 

  (3) 

cs224d: deep learning for nlp 

40 

our second example (4-layer network):  
id26 using error vectors 

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

  (2)=      (z(2))!w(2)t  (3) 

 

w(2)t  (3) 

cs224d: deep learning for nlp 

41 

our second example (4-layer network):  
id26 using error vectors 
grad w(1) =   (2)a(1)t 
  

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

w(1)t  (2) 

 

  (2) 

cs224d: deep learning for nlp 

42 

our second example (4-layer network):  
id26 using error vectors 

grad wrt input vector = w(1)t  (2)   
  

z(1) 

a(1)   w(1)  z(2) 

   

1 

a(2)   w(2) 

z(3) 

   

a(3) 

 w(3) 

z(4) 

soft
max 

yp 

w(1)t  (2) 

 

w(1)t  (2) 

 

cs224d midterm review

ian tenney

may 4, 2015

outline

id26 (continued)

id56 structure
id56 id26

backprop on a dag

example: id149 (grus)
gru id26

outline

id26 (continued)

id56 structure
id56 id26

backprop on a dag

example: id149 (grus)
gru id26

basic id56 structure

h(t 1)

  y(t)

h(t)

x(t)

...

i basic id56 ("elman network")
i you   ve seen this on assignment #2 (and also in lecture #5)

basic id56 structure

h(t 1)

  y(t)

h(t)

x(t)

...

i two layers between input and prediction, plus hidden state

h(t) = sigmoid   hh(t 1) + w x(t) + b1   
  y(t) = softmax   u h(t) + b2   

unrolled id56

  y(t 2)

  y(t 1)

h(t 3)

h(t 2)

h(t 1)

x(t 2)

x(t 1)

  y(t)

h(t)

x(t)

...

i helps to think about as    unrolled    network: distinct nodes

for each timestep

i just do backprop on this! then combine shared gradients.

backprop on id56

i usual cross-id178 loss (k-class):

  p (y(t) = j | x(t), . . . , x(1)) =   y(t)
kxj=1
j (t)(   ) =  

j

y(t)
j

log   y(t)
j

i just do backprop on this! first timestep (    = 1):

@j (t)
@u

@j (t)
@h(t)

@j (t)
@b2

@j (t)

@w     (t)

@j (t)

@h     (t)

@j (t)
@x(t)

backprop on id56

i first timestep (s = 0):

@j (t)
@u

@j (t)
@h(t)

@j (t)
@h(t s)

@j (t)
@b2

@j (t)

@w     (t)
@w     (t s)

@j (t)

@j (t)
@x(t)

@j (t)
@x(t s)

@j (t)

@h     (t)
@h     (t s)

@j (t)

i back in time (s = 1, 2, . . . ,       1)

backprop on id56

yuck, that   s a lot of math!

i actually, it   s not so bad.
i solution: error vectors ( )

making sense of the madness

i chain rule to the rescue!
i a(t) = u h(t) + b2
i   y(t) = softmax(a(t))
i gradient is transpose of jacobian:

raj =  @j (t)
@a(t)!t

=   y(t)   y(t) =  (2)(t) 2 rk   1

i now dimensions work out:

@j (t)
@a(t)   

@a(t)
@b2

= ( (2)(t))t i

2 r(1   k)  (k   k) = r1   k

making sense of the madness

i chain rule to the rescue!
i a(t) = u h(t) + b2
i   y(t) = softmax(a(t))
i gradient is transpose of jacobian:

raj =  @j (t)
@a(t)!t

=   y(t)   y(t) =  (2)(t) 2 rk   1

i now dimensions work out:

@j (t)
@a(t)   

@a(t)
@b2

= ( (2)(t))t i

2 r(1   k)  (k   k) = r1   k

making sense of the madness

i chain rule to the rescue!
i a(t) = u h(t) + b2
i   y(t) = softmax(a(t))
i gradient is transpose of jacobian:

raj =  @j (t)
@a(t)!t

=   y(t)   y(t) =  (2)(t) 2 rk   1

i now dimensions work out:

@j (t)
@a(t)   

@a(t)
@b2

= ( (2)(t))t i

2 r(1   k)  (k   k) = r1   k

making sense of the madness

i chain rule to the rescue!
i a(t) = u h(t) + b2
i   y(t) = softmax(a(t))
i matrix dimensions get weird:

@a(t)
@u

2 rk   (k   dh)

i but we don   t need fancy tensors:

ru j (t) =  @j (t)

@a(t)   

@a(t)

@u !t

=  (2)(t)(h(t))t

2 rk   dh

i numpy: self.grads.u += outer(d2, hs[t])

making sense of the madness

i chain rule to the rescue!
i a(t) = u h(t) + b2
i   y(t) = softmax(a(t))
i matrix dimensions get weird:

@a(t)
@u

2 rk   (k   dh)

i but we don   t need fancy tensors:

ru j (t) =  @j (t)

@a(t)   

@a(t)

@u !t

=  (2)(t)(h(t))t

2 rk   dh

i numpy: self.grads.u += outer(d2, hs[t])

going deeper

i really just need one simple pattern:
i z(t) = hh(t 1) + w x(t) + b1
i h(t) = f (z(t))
i compute error delta (s = 0, 1, 2, . . .):

i from top:  (t) =   h(t)   (1   h(t))      u t  (2)(t)
i deeper:  (t s) =   h(t s)   (1   h(t s))      h t  (t s+1)

i these are just chain-rule expansions!

@j (t)
@z(t) =

@j (t)
@a(t)   

@a(t)
@h(t)   

@h(t)
@z(t) = ( (t))t

going deeper

i really just need one simple pattern:
i z(t) = hh(t 1) + w x(t) + b1
i h(t) = f (z(t))
i compute error delta (s = 0, 1, 2, . . .):

i from top:  (t) =   h(t)   (1   h(t))      u t  (2)(t)
i deeper:  (t s) =   h(t s)   (1   h(t s))      h t  (t s+1)

i these are just chain-rule expansions!

@j (t)
@z(t) =

@j (t)
@a(t)   

@a(t)
@h(t)   

@h(t)
@z(t) = ( (t))t

going deeper

i these are just chain-rule expansions!

@a(t)   

@j (t)

@j (t)

=  @j (t)
@b1     (t)
=  @j (t)
@h     (t)
@z(t 1) =  @j (t)

@a(t)   

@j (t)

@a(t)   

@a(t)
@h(t)   

@a(t)
@h(t)   

@h(t)

@z(t)!   
@z(t)!   
@z(t)!   

@h(t)

@h(t)

@a(t)
@h(t)   

@z(t)
@b1

= ( (t))t @z(t)
@b1

@z(t)
@h

= ( (t))t @z(t)
@h

@z(t)

@h(t 1) = ( (t))t @z(t)

@z(t 1)

going deeper

i and there   s shortcuts for them too:

  @j (t)
@b1     (t)!t
@h     (t)!t
  @j (t)
  @j (t)
@z(t 1)!t

=  (t)

=  (t)    (h(t 1))t
= hh(t 1)   (1   h(t 1))i   h t  (t) =  (t 1)

outline

id26 (continued)

id56 structure
id56 id26

backprop on a dag

example: id149 (grus)
gru id26

motivation

i gated units with    reset    and    output    gates
i reduce problems with vanishing gradients

figure : you are likely to be eaten by a gru. (figure from chung, et
al. 2014)

intuition

i gates zi and ri for each hidden layer neuron
i zi, ri 2 [0, 1]
i   h as    candidate    hidden layer
i   h, z, r all depend on on x(t), h(t 1)
i h(t) depends on h(t 1) mixed with   h(t)

figure : you are likely to be eaten by a gru. (figure from chung, et
al. 2014)

equations

i z(t) =   wzx(t) + uzh(t 1) 
i r(t) =   wrx(t) + urh(t 1) 
i   h(t) = tanh w x(t) + r(t)   u h(t 1) 
i h(t) = z(t)   h(t 1) + (1   z(t))     h(t)
i optionally can have biases; omitted for clarity.

figure : you are likely to be eaten by a gru. (figure from chung, et
al. 2014)

same eqs. as lecture 8, subscripts/superscripts as in assignment #2.

id26

multi-path to compute @j
@x(t)

i start with  (t) =    @j
@h(t)   t
2 rd
i h(t) = z(t)   h(t 1) + (1   z(t))     h(t)
i expand chain rule into sum (a.k.a. product rule):

@j
@x(t) =

+

@j

@z(t)

@h(t)   "z(t)  
@h(t)   "(1   z(t))  

@h(t 1)
@x(t) +
@  h(t)
@x(t) +

@x(t)   h(t 1)#
@(1   z(t))

@x(t)

@j

    h(t)#

it gets (a little) better

multi-path to compute @j
@x(t)

i drop terms that don   t depend on x(t):

@j
@x(t) =

=

@h(t 1)
@x(t) +

@j

@j

@h(t)   "z(t)  
@h(t)   "(1   z(t))  
@h(t)   " @z(t)

@j

+

@z(t)

@x(t)   h(t 1)#
@(1   z(t))
@x(t)#

@  h(t)

@x(t)

@  h(t)
@x(t) +

@x(t)   h(t 1) + (1   z(t))  
@z(t)
@x(t)     h(t)

@j
@h(t)

 

    h(t)#

almost there!

multi-path to compute @j
@x(t)

i now we really just need to compute two things:
i output gate:

@z(t)
@x(t) = z(t)   (1   z(t))   wz

i candidate   h:

@  h(t)
@x(t) =

(1   (  h(t))2)   w

+ (1   (  h(t))2)  

@r(t)
@x(t)   u h(t 1)

i ok, i lied - there   s a third.
i don   t forget to check all paths!

almost there!

multi-path to compute @j
@x(t)

i now we really just need to compute two things:
i output gate:

@z(t)
@x(t) = z(t)   (1   z(t))   wz

i candidate   h:

@  h(t)
@x(t) =

(1   (  h(t))2)   w

+ (1   (  h(t))2)  

@r(t)
@x(t)   u h(t 1)

i ok, i lied - there   s a third.
i don   t forget to check all paths!

almost there!

multi-path to compute @j
@x(t)

i now we really just need to compute two things:
i output gate:

@z(t)
@x(t) = z(t)   (1   z(t))   wz

i candidate   h:

@  h(t)
@x(t) =

(1   (  h(t))2)   w

+ (1   (  h(t))2)  

@r(t)
@x(t)   u h(t 1)

i ok, i lied - there   s a third.
i don   t forget to check all paths!

almost there!

multi-path to compute @j
@x(t)

i last one:

@r(t)
@x(t) = r(t)   (1   r(t))   wr

i now we can just add things up!
i (i   ll spare you the pain...)

whew.

i why three derivatives?
i three arrows from x(t) to distinct nodes
i four paths total ( @z(t)

@x(t) appears twice)

whew.

i grus are complicated
i all the pieces are simple
i same matrix gradients that you   ve seen before

summary

i check your dimensions!
i write error vectors  ; just parentheses around chain rule
i combine simple operations to make complex network

i matrix-vector product
i id180 (tanh, sigmoid, softmax)

