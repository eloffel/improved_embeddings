cs224d:

tensorflow tutorial

bharath ramsundar

administrative announcements
    pset 1 due today 4/19 (3 late days maximum)
    pset 2 released tomorrow 4/20 (due 5/5)
    help us help you! fill out class survey to give us 

feedback.

    qiaojing will host tensorflow on aws setup session in 

office hours, sundar 4/24, 4-6 pm, gates b24

    will host special tensorflow help session in my office 

hours, tuesday 4/26, 1-3 pm, huang basement.

deep-learning package zoo

    torch
    caffe
    theano (keras, lasagne)
    cudnn
    tensorflow
    mxnet
    etc.

deep-learning package design choices
    model specification: configuration file (e.g. caffe, 

distbelief, cntk) versus programmatic generation (e.g. 
torch, theano, tensorflow)

    for programmatic models, choice of high-level language: 

lua (torch) vs. python (theano, tensorflow) vs others.

    we chose to work with python because of rich community 

and library infrastructure.

tensorflow vs. theano
    theano is another deep-learning library with python-

wrapper (was inspiration for tensorflow)

    theano and tensorflow are very similar systems. 

tensorflow has better support for distributed systems 
though, and has development funded by google, while 
theano is an academic project.

what is tensorflow?

    tensorflow is a deep learning library 

recently open-sourced by google.

    but what does it actually do? 

    tensorflow provides primitives for 
defining functions on tensors and 
automatically computing their derivatives.

but what   s a tensor? 
    formally, tensors are multilinear maps from vector spaces 

to the real numbers (   vector space, and     dual space) 

    a scalar is a tensor (                               )
    a vector is a tensor (                                )
    a matrix is a tensor (                                          )
    common to have fixed basis, so a tensor can be 

represented as a multidimensional array of numbers.

tensorflow vs. numpy
    few people make this comparison, but tensorflow and 

numpy are quite similar. (both are n-d array libraries!)

    numpy has ndarray support, but doesn   t offer methods to 

create tensor functions and automatically compute 
derivatives (+ no gpu support).

vs

simple numpy recap

in [23]: import numpy as np

in [24]: a = np.zeros((2,2)); b = np.ones((2,2))

in [25]: np.sum(b, axis=1)
out[25]: array([ 2.,  2.])

in [26]: a.shape
out[26]: (2, 2)

in [27]: np.reshape(a, (1,4))
out[27]: array([[ 0.,  0.,  0.,  0.]])

repeat in tensorflow

more on session 
soon

in [31]: import tensorflow as tf

in [32]: tf.interactivesession()

in [33]: a = tf.zeros((2,2)); b = tf.ones((2,2))

in [34]: tf.reduce_sum(b, reduction_indices=1).eval()
out[34]: array([ 2.,  2.], dtype=float32)

in [35]: a.get_shape()
out[35]: tensorshape([dimension(2), dimension(2)])

in [36]: tf.reshape(a, (1, 4)).eval()
out[36]: array([[ 0.,  0.,  0.,  0.]], dtype=float32)

more on .eval() 
in a few slides

tensorshape behaves 
like a python tuple.

numpy to tensorflow dictionary

numpy

tensorflow

a = np.zeros((2,2)); b = np.ones((2,2))

a = tf.zeros((2,2)), b = tf.ones((2,2))

np.sum(b, axis=1)

tf.reduce_sum(a,reduction_indices=[1])

a.shape

a.get_shape()

np.reshape(a, (1,4))

tf.reshape(a, (1,4))

b * 5 + 1

np.dot(a,b)

b * 5 + 1

tf.matmul(a, b)

a[0,0], a[:,0], a[0,:]

a[0,0], a[:,0], a[0,:]

tensorflow requires explicit evaluation!

in [37]: a = np.zeros((2,2))

in [38]: ta = tf.zeros((2,2))

in [39]: print(a)
[[ 0.  0.]
 [ 0.  0.]]

tensorflow computations define a 
computation graph that has no numerical 
value until evaluated!

in [40]: print(ta)
tensor("zeros_1:0", shape=(2, 2), dtype=float32)

in [41]: print(ta.eval())
[[ 0.  0.]
 [ 0.  0.]]

tensorflow session object (1)
       a session object encapsulates the environment in which 

tensor objects are evaluated    - tensorflow docs

in [20]: a = tf.constant(5.0)

in [21]: b = tf.constant(6.0)

in [22]: c = a * b

in [23]: with tf.session() as sess:
   ....:     print(sess.run(c))
   ....:     print(c.eval())
   ....:     
30.0
30.0

c.eval() is just syntactic sugar for 
sess.run(c) in the currently active 
session!

tensorflow session object (2)
    tf.interactivesession() is just convenient syntactic 

sugar for keeping a default session open in ipython.

    sess.run(c) is an example of a tensorflow fetch. will 

say more on this soon.

tensorflow computation graph
       tensorflow programs are usually structured into a 
construction phase, that assembles a graph, and an 
execution phase that uses a session to execute ops in the 
graph.    - tensorflow docs

    all computations add nodes to global default graph (docs)

tensorflow variables (1)
       when you train a model you use variables to hold and 

update parameters. variables are in-memory buffers 
containing tensors    - tensorflow docs.

    all tensors we   ve used previously have been constant 

tensors, not variables.

tensorflow variables (2)

in [32]: w1 = tf.ones((2,2))

in [33]: w2 = tf.variable(tf.zeros((2,2)), name="weights")

in [34]: with tf.session() as sess:
           print(sess.run(w1))
           sess.run(tf.initialize_all_variables())
           print(sess.run(w2))
   ....:     
[[ 1.  1.]
 [ 1.  1.]]
[[ 0.  0.]
 [ 0.  0.]]

note the initialization step tf.
initialize_all_variables()

tensorflow variables (3)
    tensorflow variables must be initialized before they have 

values! contrast with constant tensors.

in [38]: w = tf.variable(tf.zeros((2,2)), name="weights")

variable objects can be 
initialized from constants or 
random values

in [39]: r = tf.variable(tf.random_normal((2,2)), name="random_weights")

in [40]: with tf.session() as sess:
   ....:     sess.run(tf.initialize_all_variables())
   ....:     print(sess.run(w))
   ....:     print(sess.run(r))
   ....:     

initializes all variables with 
specified values.

updating variable state

in [63]: state = tf.variable(0, name="counter")

in [64]: new_value = tf.add(state, tf.constant(1))

in [65]: update = tf.assign(state, new_value)

in [66]: with tf.session() as sess:
   ....:     sess.run(tf.initialize_all_variables())
   ....:     print(sess.run(state))
   ....:     for _ in range(3):
   ....:         sess.run(update)
   ....:         print(sess.run(state))
   ....:         
0
1
2
3

roughly new_value = state + 1

roughly state = new_value

roughly
state = 0
print(state)
for _ in range(3):
  state = state + 1
  print(state)

fetching variable state (1)

in [82]: input1 = tf.constant(3.0)
in [83]: input2 = tf.constant(2.0)
in [84]: input3 = tf.constant(5.0)
in [85]: intermed = tf.add(input2, input3)
in [86]: mul = tf.mul(input1, intermed)
in [87]: with tf.session() as sess:
   ....:       result = sess.run([mul, intermed])
   ....:       print(result)
   ....:     
[21.0, 7.0]

calling sess.run(var) on a tf.session() object 
retrieves its value. can retrieve multiple variables 
simultaneously with sess.run([var1, var2]) 
(see fetches in tf docs)

fetching variable state (2)

inputting data
    all previous examples have manually defined tensors. 

how can we input external data into tensorflow?

    simple solution: import from numpy:

in [93]: a = np.zeros((3,3))
in [94]: ta = tf.convert_to_tensor(a)
in [95]: with tf.session() as sess:
   ....:     print(sess.run(ta))
   ....:     
[[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]

placeholders and feed dictionaries (1)
    inputting data with tf.convert_to_tensor() is 

convenient, but doesn   t scale.

    use tf.placeholder variables (dummy nodes that 
provide entry points for data to computational graph). 
    a feed_dict is a python dictionary mapping from tf.

placeholder vars (or their names) to data (numpy arrays, 
lists, etc.).

placeholders and feed dictionaries (2)

in [96]: input1 = tf.placeholder(tf.float32)

in [97]: input2 = tf.placeholder(tf.float32)

in [98]: output = tf.mul(input1, input2)

define tf.placeholder 
objects for data entry.

in [99]: with tf.session() as sess:
   ....:       print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))
   ....:     

[array([ 14.], dtype=float32)]

fetch value of output 
from computation graph.

feed data into 
computation graph.

placeholders and feed dictionaries (3)

variable scope (1)
    complicated tensorflow models can have hundreds of 

variables. 
    tf.variable_scope() provides simple name-spacing 

to avoid clashes.

    tf.get_variable() creates/accesses variables from 

within a variable scope.

variable scope (2)
    variable scope is a simple type of namespacing that adds 

prefixes to variable names within scope

with tf.variable_scope("foo"):
    with tf.variable_scope("bar"):
        v = tf.get_variable("v", [1])
assert v.name == "foo/bar/v:0"

variable scope (3)
    variable scopes control variable (re)use

with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])
    tf.get_variable_scope().reuse_variables()
    v1 = tf.get_variable("v", [1])
assert v1 == v

    you   ll need to use reuse_variables() to implement id56s 

in homework

understanding get_variable (1)
    behavior depends on whether variable reuse enabled
    case 1: reuse set to false

    create and return new variable

with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])
assert v.name == "foo/v:0"

understanding get_variable (2)
    case 2: variable reuse set to true

    search for existing variable with given name. raise 

valueerror if none found.

with tf.variable_scope("foo"):
    v = tf.get_variable("v", [1])
with tf.variable_scope("foo", reuse=true):
    v1 = tf.get_variable("v", [1])
assert v1 == v

ex: id75 in tensorflow (1)

import numpy as np
import seaborn

# define input data
x_data = np.arange(100, step=.1)
y_data = x_data + 20 * np.sin(x_data/10)

# plot input data
plt.scatter(x_data, y_data)

ex: id75 in tensorflow (2)

# define data size and batch size
n_samples = 1000
batch_size = 100 

# tensorflow is finicky about shapes, so resize
x_data = np.reshape(x_data, (n_samples,1)) 
y_data = np.reshape(y_data, (n_samples,1))

# define placeholders for input
x = tf.placeholder(tf.float32, shape=(batch_size, 1)) 
y = tf.placeholder(tf.float32, shape=(batch_size, 1)) 

ex: id75 in tensorflow (3)

# define variables to be learned
with tf.variable_scope("linear-regression"):
  w = tf.get_variable("weights", (1, 1), 
                      initializer=tf.random_normal_initializer())
  b = tf.get_variable("bias", (1,),
                      initializer=tf.constant_initializer(0.0))
  y_pred = tf.matmul(x, w) + b 
  loss = tf.reduce_sum((y - y_pred)**2/n_samples)

note reuse=false so 
these tensors are 
created anew

ex: id75 in tensorflow (4)

# sample code to run one step of id119
in [136]: opt = tf.train.adamoptimizer()

in [137]: opt_operation = opt.minimize(loss)

note tensorflow scope is 
not python scope! python 
variable loss is still visible.

in [138]: with tf.session() as sess:
   .....:     sess.run(tf.initialize_all_variables())
   .....:     sess.run([opt_operation], feed_dict={x: x_data, y: y_data})
   .....: 

but how does this actually work under the 
hood? will return to tensorflow 
computation graphs and explain.

ex: id75 in tensorflow (4)

# sample code to run full id119:
# define optimizer operation
opt_operation = tf.train.adamoptimizer().minimize(loss)

with tf.session() as sess:
  # initialize variables in graph
  sess.run(tf.initialize_all_variables())
  # id119 loop for 500 steps
  for _ in range(500):
    # select random minibatch
    indices = np.random.choice(n_samples, batch_size)
    x_batch, y_batch = x_data[indices], y_data[indices]
    # do id119 step
    _, loss_val = sess.run([opt_operation, loss], feed_dict={x: x_batch, y: y_batch})

let   s do a deeper. 
graphical dive into 
this operation

ex: id75 in tensorflow (5)

ex: id75 in tensorflow (6)

learned model offers nice 
fit to data.

concept: auto-differentiation
    id75 example computed l2 loss for a linear 

regression system. how can we fit model to data?
    tf.train.optimizer creates an optimizer.
    tf.train.optimizer.minimize(loss, var_list) 

adds optimization operation to computation graph. 

    automatic differentiation computes gradients without user 

input!

tensorflow gradient computation
    tensorflow nodes in computation graph have attached 

gradient operations.

    use id26 (using node-specific gradient ops) to 

compute required gradients for all variables in graph.

tensorflow gotchas/debugging (1)
    convert tensors to numpy array and print. 
    tensorflow is fastidious about types and shapes. check 

that types/shapes of all tensors match.

    tensorflow api is less mature than numpy api. many 

advanced numpy operations (e.g. complicated array 
slicing) not supported yet!

tensorflow gotchas/debugging (2)
    if you   re stuck, try making a pure numpy implementation 

of forward computation.

    then look for analog of each numpy function in 

tensorflow api

    use tf.interactivesession() to experiment in shell. 

trial and error works!

tensorboard
    tensorflow has some neat 

built-in visualization tools 
(tensorboard).

    we won   t use tensorboard for 

homework (tricky to set up 
when tensorflow is running 
remotely), but we encourage 
you to check it out for your 
projects.

tensorflow at stanford
    cpu-only version of tensorflow now available on a 

number of stanford clusters (corn, myth)

    gpu versions of tensorflow available only on limited 

clusters (sherlock, xstream). feel free to use if you 
already have access.

    cpu-only version sufficient for homework (but will be 

slower than gpu version)

hint for hw: defining embeddings in tensorflow

# define placeholders for inputs
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

# look up embeddings for inputs.
# you   ll use this for pset 2
embeddings = tf.variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
embed = tf.nn.embedding_lookup(embeddings, train_inputs)

