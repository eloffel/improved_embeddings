cs224d
deep	nlp

lecture	6:

neural	tips	and	tricks

+

recurrent	neural	networks

richard	socher

richard@metamind.io

overview	today:
    another	explanation	of	backprop

(from	a	tutorial	yoshua,	chris	and	i	did)

    practical	tips	and	tricks:
    multi-task	learning
    nonlinearities
    finite	difference	gradient	check
    momentum,	adagrad

    language	models
    first	intro	to	recurrent	neural	networks

lecture	1,	slide	2

richard	socher

4/17/16

id26 (another	explanation)
    compute	gradient	of	example-wise	loss	wrt

parameters	

simply	applying	the	derivative	chain	rule	wisely

if	computing	the	loss(example,	parameters)	is	o(n)	
computation,	then	so	is	computing	the	gradient

   

   

3

simple chain rule

4

multiple paths chain rule

5

multiple	paths	chain	rule	- general

   

6

chain rule in flow graph

   

   

flow	graph:	any	directed	acyclic	graph

node	=	computation	result
arc	=	computation	dependency

=	successors	of	

   

7

back-prop in multi-layer net

h = sigmoid(vx)

   
   

8

back-prop in general flow graph

1. fprop:	visit	nodes	in	topo-sort	order	

- compute	value	of	node	given	predecessors

2. bprop:

- initialize	output	gradient	=	1	
- visit	nodes	in	reverse	order:

compute	gradient	wrt each	node	using	
gradient	wrt successors

=	successors	of	

single	scalar	output

   

   

   

9

automatic differentiation

    the	gradient	computation	can	
be	automatically	inferred	from	
the	symbolic	expression	of	the	
fprop.

    each	node	type	needs	to	know	
how	to	compute	its	output	and	
how	to	compute	the	gradient	
wrt its	inputs	given	the	
gradient	wrt its	output.

    easy	and	fast	prototyping

   

   

10

neural	tips	and	tricks

11

multi-task	learning	/	weight	sharing

    base	model:	neural	network	
from	last	class	but	replaces	
the	single	scalar	score	with	a	
softmax classifier

    training	is	again	done	via	

id26

    nlp	(almost)	from	scratch,	

collobertet	al.	2011

12

c1

c2

c3

a1

a2

x1

x2																x3

+1

the	model
    we	already	know	the	softmax	classifier	and	how	to	optimize	it
    the	interesting	twist	in	deep	learning	is	that	the	input	features	x	

and	their	transformations	in	a	hidden	layer	are	also	learned.

    two	final	layers	are	possible:

s		

u2

a1

a2

w23

c1

c2

c3

a1

a2

s

x1

x2																x3

+1

x1

x2																x3

+1

13

multitask	learning
    main	idea:	we	can	share	both	the	word	vectors	and	the	hidden	

layer	weights.	only	the	softmax weights	are	different.

    cost	function	is	just	the	sum	of	two	cross	id178	errors

c1

c2

c3

c4

c5

c6

s1

a1

a2

s2

a1

a2

x1

x2																x3

+1

x1

x2																x3

+1

14

the	multitask	model	- training
    example:	predict	each	window   s	center	ner	tag	and	pos	tag:
(example	pos	tags:	dt,	nn,	nnp,	jj,	jjs	(superlative	adj),	vb,   )

    efficient	implementation:	same	forward	prop,	

compute	errors	on	hidden	vectors	and	add		

c1

c2

c3

c4

c5

c6

s1

a1

a2

s2

a1

a2

x1

x2																x3

+1

x1

x2																x3

+1

15

the	secret	sauce	(sometimes)	is	the	unsupervised	
word	vector	pre-training	on	a	large	text	collection

pos
wsj (acc.)
97.24
96.37
97.20

state-of-the-art*
supervised	nn
word	vector	pre-training	
followed	by	supervised	nn**
+	hand-crafted	features*** 97.29

ner
conll (f1)
89.31
81.47
88.87

89.59

*	representative	systems:	pos:	(toutanova	et	al.	2003),	ner: (ando	&	zhang	
2005)
**	130,000-word	embedding	trained	on	wikipedia	and	reuters	with	11	word	
window,	100	unit	hidden	layer	    then	supervised	task	training
***features	are	character	suffixes	for	pos	and	a	gazetteer	for	ner
16

supervised	refinement	of	the	unsupervised	word	
representation	helps

pos
wsj (acc.)
supervised	nn
96.37
nn	with	brown	clusters 96.92
97.10
fixed	embeddings*
c&w 2011**
97.29

ner
conll (f1)
81.47
87.15
88.87
89.59

*	same	architecture	as	c&w	2011,	but	word	embeddings	are	kept	constant	
during	the	supervised	training	phase
**	c&w	is	unsupervised	pre-train	+	supervised	nn	+	features	model	of	last	slide

17

general	strategy	for	successful	nnets

1. select	network	structure	appropriate	for	problem

1. structure:	single	words,	fixed	windows,	sentence	based,		

document	level;	bag	of	words,	recursive	vs.	recurrent,	id98	

2. nonlinearity

2. check	for	implementation	bugs	with	gradient	checks
3. parameter	initialization
4. optimization	tricks
5. check	if	the	model	is	powerful	enough	to	overfit

1.
2.

if	not,	change	model	structure	or	make	model	   larger   
if	you	can	overfit:	regularize

18

non-linearities:	what   s	used

logistic	(   sigmoid   )																															tanh

tanh is	just	a	rescaled	and	shifted	sigmoid

tanh(z) = 2logistic(2z)    1

19

for	many	models,		tanh is	the	best!

   

in	comparison	to	sigmoid:

    at	initialization:	values	close	to	0

faster	convergence	in	practice

like	sigmoid:	nice	derivative:	

   

   

20

richard	socher

4/17/16

non-linearities:	there	are	various	other	choices

hard	tanh

soft	sign

rectified	linear	(relu)

softsign(z) =

a
1+ a

rect(z) = max(z,0)

    hard	tanh similar	but	computationally	cheaper	than	tanh and	saturates	hard.
    glorot and	bengio,	aistats 2011	discuss	softsign and	rectifier

21

maxout network

a	recent	type	of	nonlinearity/network
goodfellow et	al.	(2013)
where	

this	function	also	becomes	a	universal	approximator
when	stacked	in	multiple	layers
state	of	the	art	on	several	image	datasets

22

gradient	checks	are	awesome!
    allow	you	to	know	that	there	are	no	bugs	in	your	neural	

network	implementation!

    steps:

1.
2.

implement	your	gradient
implement	a	finite	difference	computation	by	looping	
through	the	parameters	of	your	network,	adding	and	
subtracting	a	small	epsilon	(   10-4)	and	estimate	derivatives

3. compare	the	two	and	make	sure	they	are	almost	the	same

23

if	you	gradient	fails	and	you	don   t	know	why?

using	gradient	checks	and	model	simplification
   
    what	now?	create	a	very	tiny	synthetic	model	and	dataset
    simplify	your	model	until	you	have	no	bug!	
    example:	start	from	simplest	model	then	go	to	what	you	want:

    only	softmax on	fixed	input
    backprop into	word	vectors	and	softmax
    add	single	unit	single	hidden	layer
    add	multi	unit	single	layer
    add	bias
    add	second	layer	single	unit,	add	multiple	units,	bias
    add	one	softmax on	top,	then	two	softmax layers

24

general	strategy

1. select	appropriate	network	structure

1. structure:	single	words,	fixed	windows	vs recursive	

sentence	based	vs bag	of	words

2. nonlinearity

2. check	for	implementation	bugs	with	gradient	check
3. parameter	initialization
4. optimization	tricks
5. check	if	the	model	is	powerful	enough	to	overfit

1.
2.

if	not,	change	model	structure	or	make	model	   larger   
if	you	can	overfit:	regularize

25

parameter	initialization
   

initialize	hidden	layer	biases	to	0	and	output	(or	reconstruction)	
biases	to	optimal	value	if	weights	were	0	(e.g.,	mean	target	or	
inverse	sigmoid	of	mean	target).

initialize	weights	    uniform(   r,	r),	r inversely	proportional	to	
fan-in	(previous	layer	size)	and	fan-out	(next	layer	size):

for	tanh units,	and	4x	bigger	for	sigmoid	units[glorot aistats	2010]

   

26

stochastic	gradient	descent	(sgd)
    gradient	descent	uses	total	gradient	over	all	examples	per	

update,	sgd	updates	after	only	1	or	few	examples:

   

jt =	loss	function	at	current	example,	   = parameter	vector,
   = learning	rate.

    ordinary	gradient	descent	as	a	batch	method	is	very	slow,	should	

never	be	used.	use	2nd order	batch	method	such	as	l-bfgs.	

    on	large	datasets,	sgd	usually	wins	over	all	batch	methods.	on	
smaller	datasets	l-bfgs	or	conjugate	gradients	win.	large-batch	
l-bfgs	extends	the	reach	of	l-bfgs	[le	et	al.	icml	2011].

27

mini-batch	stochastic	gradient	descent	(sgd)
    gradient	descent	uses	total	gradient	over	all	examples	per	

update,	sgd	updates	after	only	1	example
    most	commonly	used	now:	mini	batches	

    size	of	each	mini	batch	b:	20	to	1000:

    helps	parallelizing	any	model	by	computing	gradients	for	multiple	

elements	of	the	batch	in	parallel

28

improvement	over	sgd:	momentum
   
    when	the	gradient	keeps	pointing	in	the	same	direction,	this	will	

idea:	add	a	fraction	v	of	previous	update	to	current	one

increase	the	size	of	the	steps	taken	towards	the	minimum

    reduce	global	learning	rate	   when	using	a	lot	of	momentum	

    update	rule:

    v	is	initialized	at	0
    common:	   =	0.9
    momentum	often	increased	after	some	epochs	(0.5	   0.99)

lecture	1,	slide	29

richard	socher

4/17/16

intuition	momentum
    adds	friction	(momentum	~	misnomer)

    parameters	build	up	velocity	in	direction	of	consistent	gradient

    simple	convex	function	optimization	dynamics
with	momentum:

without	momentum

lecture	1,	slide	30

richard	socher

4/17/16

figure	from	https://www.willamette.edu/~gorr/classes/cs449/momrate.html

learning	rates
    simplest	recipe:	keep	it	fixed	and	use	the	same	for	all	

parameters.	standard:	

    better	results	by	allowing	learning	rates	to	decrease	options:	

    reduce	by	0.5	when	validation	error	stops	improving
    reduction	by	o(1/t)	because	of	theoretical	convergence	
guarantees,	e.g.:
with	hyper-parameters	  0 and	   and	t	is	iteration	numbers
    better	yet:	no	hand-set	learning	of	rates	by	using	adagrad   

31

adagrad
    adaptive	learning	rates	for	each	parameter!	
    related	paper:	adaptive	subgradient methods	for	online	

minimize the objective, we use the diagonal vari-
ant of adagrad (duchi et al., 2011) with mini-
batches. for our parameter updates, we    rst de-
   ne g    2 rm   1 to be the subgradient at time step
    and gt =pt
    . the parameter update at
    =1 g    gt
time step t then becomes:

learning	and	stochastic	optimization,	duchi et	al.	2010
   t =    t 1       (diag(gt)) 1/2 gt,

(7)

    learning	rate	is	adapting	differently	for	each	parameter	and	rare	

parameters	get	larger	updates	than	frequently	occurring	
parameters.	word	vectors!	

where     is the learning rate. since we use the di-
agonal of gt, we only have to store m values and
the update becomes fast to compute: at time step
t, the update for the i   th parameter    t,i is:

    let																									,	then:

   t,i =    t 1,i  

gt,i.

(8)

   
    =1 g2
   ,i

qpt

hence,

the learning rate is adapting differ-
ently for each parameter and rare parameters get
larger updates than frequently occurring parame-
ters. this is helpful in our setting since some w

richard	socher

4/17/16

lecture	1,	slide	32

we used the    rst 20    les of wsj section 22
to cross-validate several model and optimization
choices. the base pid18 uses simpli   ed cate-
gories of the stanford pid18 parser (klein and
manning, 2003a). we decreased the state split-
ting of the pid18 grammar (which helps both by
making it less sparse and by reducing the num-
ber of parameters in the su-id56) by adding
the following options to training:    -norightrec -
dominatesv 0 -basenp 0   . this reduces the num-
ber of states from 15,276 to 12,061 states and 602
pos tags. these include split categories, such as
parent annotation categories like vp  s. further-
more, we ignore all category splits for the su-
id56 weights, resulting in 66 unary and 882 bi-
nary child pairs. hence, the su-id56 has 66+882
transformation matrices and scoring vectors. note
that any pid18, including latent annotation pid18s
(matsuzaki et al., 2005) could be used. however,
since the vectors will capture lexical and semantic

general	strategy

1.

2.
3.
4.
5.

structure:	single	words,	fixed	windows	vs recursive	sentence	based	vs bag	of	words
nonlinearity

select	appropriate	network	structure
1.
2.
check	for	implementation	bugs	with	gradient	check
parameter	initialization
optimization	tricks
check	if	the	model	is	powerful	enough	to	overfit
1.
2.

if	not,	change	model	structure	or	make	model	   larger   
if	you	can	overfit:	regularize

assuming	you	found	the	right	network	structure,	implemented	it	
correctly,	optimize	it	properly	and	you	can	make	your	model	
overfit on	your	training	data.

now,	it   s	time	to	regularize
33

prevent	overfitting:	model	size	and	id173
    simple	first	step:	reduce	model	size	by	lowering	number	of	

units	and	layers	and	other	parameters

    standard	l1	or	l2	id173	on	weights	

    early	stopping:	use	parameters	that	gave	best	validation	error

    sparsity constraints	on	hidden	activations,	e.g.,	add	to	cost:	

34

prevent	feature	co-adaptation

dropout	(hinton	et	al.	2012)

    training	time:	at	each	instance	of	evaluation	(in	online	sgd-
training),	randomly	set	50%	of	the	inputs	to	each	neuron	to	0
    test	time:	halve	the	model	weights	(now	twice	as	many)
    this	prevents	feature	co-adaptation:	a	feature	cannot	only	be	
useful	in	the	presence	of	particular	other	features
    in	a	single	layer:	a	kind	of	middle-ground	between	na  ve	
bayes	(where	all	feature	weights	are	set	independently)	and	
logistic	regression	models	(where	weights	are	set	in	the	
context	of	all	others)
    can	be	thought	of	as	a	form	of	model	id112
    it	also	acts	as	a	strong	regularizer

35

deep	learning	tricks	of	the	trade
    y.	bengio	(2012),	   practical	recommendations	for	gradient-

based	training	of	deep	architectures   	
    unsupervised	pre-training
    stochastic	gradient	descent	and	setting	learning	rates
    main	hyper-parameters

    learning	rate	schedule	&	early	stopping,	minibatches,	parameter	

initialization,	number	of	hidden	units,	id173	(=	weight	decay)
    how	to	efficiently	search	for	hyper-parameter	configurations

    short	answer:	random	hyperparameter search (!)

    some	more	advanced	and	recent	tricks	in	later	lectures

36

language	models

37

language	models

a	language	model	computes	a	id203	for	a	sequence	
of	words:
id203	is	usually	conditioned	on	window	of	n	
previous	words	:

very	useful	for	a	lot	of	tasks:
can	be	used	to	determine	whether	a	sequence	is	a	good	/	
grammatical	translation	or	speech	utterance
38

richard	socher

4/17/16

overall parameter set is    = (c,  ).

l =

log f (wt,wt 1,       ,wt n+1;  ) +r(  ),

training is achieved by looking for    that maximizes the training corpus penalized log-likelihood:
original	neural	language	model
1
t    t
where r(  ) is a id173 term. for example, in our experiments, r is a weight decay penalty
a	neural	probabilistic	language	model,	bengio et	al.	2003
applied only to the weights of the neural network and to the c matrix, not to the biases.3
in the above model, the number of free parameters only scales linearly with v, the number of
words in the vocabulary. it also only scales linearly with the order n : the scaling factor could
be reduced to sub-linear if more sharing structure were introduced, e.g. using a time-delay neural
network or a recurrent neural network (or a combination of both).
in most experiments below, the neural network has one hidden layer beyond the word features
mapping, and optionally, direct connections from the word features to the output. therefore there
are really two hidden layers: the shared word features layer c, which has no non-linearity (it would
not add anything useful), and the ordinary hyperbolic tangent hidden layer. more precisely, the
neural network computes the following function, with a softmax output layer, which guarantees
positive probabilities summing to 1:

the yi are the unnormalized log-probabilities for each output word i, computed as follows, with
parameters b,w,u,d and h:
(1)
where the hyperbolic tangent tanh is applied element by element, w is optionally zero (no direct
connections), and x is the word features layer activation vector, which is the concatenation of the
input word features from the matrix c:

original	equations:
y = b +wx +u tanh(d +hx)
eywt
  p(wt|wt 1,      wt n+1) =
   ieyi .

3. the biases are the additive parameters of the neural network, such as b and d in equation 1 below.

a neural probabilistic language model

bengio, ducharme, vincent and jauvin

i-th output = p(wt = i | context)

. . .

tanh

. . .

most  computation here

softmax

. . .

. . .

1142

x = (c(wt 1),c(wt 2),       ,c(wt n+1)).

. . .
let h be the number of hidden units, and m the number of features associated with each word. when
no direct connections from word features to outputs are desired, the matrix w is set to 0. the free
parameters of the model are the output biases b (with |v| elements), the hidden layer biases d (with
wt n+1
h elements), the hidden-to-output weights u (a |v|   h matrix), the word features to output weights
w (a |v|    (n 1)m matrix), the hidden layer weights h (a h    (n  1)m matrix), and the word
features c (a |v|   m matrix):

problem:	fixed	window
of	context	for	
conditioning	:(
39

c(wt n+1)
. . .
table
look   up
c
in

index for
richard	socher

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

4/17/16

index for

index for

wt 1

figure 1: neural architecture: f (i,wt 1,       ,wt n+1) = g(i,c(wt 1),       ,c(wt n+1)) where g is the

neural network and c(i) is the i-th word feature vector.

recurrent	neural	networks

40

recurrent	neural	networks!

solution:	condition	the	neural	network	on	all	previous	
words	and	tie	the	weights	at	each	time	step

ht   1

ht

ht+1

w

xt

w

xt+1

xt   1

41

richard	socher

4/17/16

recurrent	neural	network	language	model

given	list	of	word	vectors:
at	a	single	time	step:

    

xt

ht

42

richard	socher

4/17/16

recurrent	neural	network	language	model

main	idea:	we	use	the	same	set	of	w	weights	at	all	time	
steps!
everything	else	is	the	same:

is	some	initialization	vector	for	the	hidden	layer	

at	time	step	0

is	the	column	vector	of	l	at	index	[t]	at	time	step	t

recurrent	neural	network	language	model

is	a	id203	distribution	over	the	vocabulary

same	cross	id178	loss	function	but	predicting	words	
instead	of	classes

44

richard	socher

4/17/16

recurrent	neural	network	language	model

evaluation	could	just	be	negative	of	average	log	
id203	over	dataset	of	size	(number	of	words)	t:

but	more	commonly: perplexity:				2j
lower	is	better!

45

richard	socher

4/17/16

=    ( (nl))t w (nl 1)
= 0@
training	id56s	is	hard
sl+1xj=1
    the	gradient	is	a	product	of	jacobian matrices,	each	associated	
|

    f0(z(nl 1)
)1a f0(z(nl 1)
}

with	a	step	in	the	forward	computation.	

=  (nl 1)

w (nl 1)

a(nl 2)
j

a(nl 2)
j

)a(nl 2)

{z

 (nl)
j

  i

ji

)

j

i

i

i

where we used in the    rst line that the top layer is linear. this is a very detailed account of essentially
just the chain rule.

    multiply	the	same	matrix	at	each	time	step	during	backprop

so, we can write the   errors of all layers l (except the top layer) (in vector format, using the hadamard

product  ):

 (l) =   (w (l))t  (l+1)      f0(z(l)),

    this	can	become	very	small	or	very	large	quickly	[bengio et	al	
1994],	and	the	locality	assumption	of	gradient	descent	breaks	
down.	   vanishing	or	exploding	gradient

7

lecture	1,	slide	46

richard	socher

4/17/16

it reaches 1,000,000 iterations and report the results in    gure 3 (best hyperparameters are listed in
table 2).

initialization	trick	for	id56s!

pixel   by   pixel permuted mnist

lstm
id56 + tanh
id56 + relus
iid56

 

   

100

90

80

70

60

50

40

30

20

10

y
c
a
r
u
c
c
a

 
t
s
e
t

pixel   by   pixel mnist

 

100

lstm
id56 + tanh
id56 + relus
iid56

initialize	w(hh) to	be	
the	identity	matrix	i
and
f(z)		=

rect(z) = max(z,0)

y
c
a
r
u
c
c
a

 
t
s
e
t

90

80

70

60

50

40

30

20

10

       huge	difference!

0

 
0

1

2

3

4

5

steps

6

7

8

9

10
x 105

0

 
0

1

2

3

4

5

steps

6

7

8

9

10
x 105

   

initialization	idea	first	introduced	in	parsing	with	compositional	
figure 3: the results of recurrent methods on the    pixel-by-pixel mnist    problem. we report the
vector	grammars,	socher	et	al.	2013
test set accuracy for all methods. left: normal mnist. right: permuted mnist.

    new	experiments	with	recurrent	neural	nets	last	week	(!)	in	a	
iid56
simple	way	to	initialize	recurrent	networks	of	rectified	linear	
units,	le	et	al.	2015
lr = 10   8, gc = 1

id56 + tanh
lr = 10   8, gc = 10

id56 + relus
lr = 10   8, gc = 10

lr = 10   8, gc = 1

richard	socher

lr = 10   6, gc = 10

4/17/16
lr = 10   9, gc = 1

problem lstm
mnist
permuted
47
mnist

lr = 0.01, gc = 1
f b = 1.0
lr = 0.01, gc = 1
f b = 1.0

is learned). there are several variations of this basic
structure. this solution does not address explicitly the

long-term	dependencies	and	clipping	trick

nents element-wise (clipping an entry when it exceeds
in absolute value a    xed threshold). clipping has been
shown to do well in practice and it forms the backbone
of our approach.

    the	solution	first	introduced	by	mikolov is	to	clip	gradients

3.2. scaling down the gradients
as suggested in section 2.3, one simple mechanism to
deal with a sudden increase in the norm of the gradi-
ents is to rescale them whenever they go over a thresh-
old (see algorithm 1).

to	a	maximum	value.	

sutskever et al. (2011) use the hessian-free opti-
mizer in conjunction with structural damping, a spe-
ci   c damping strategy of the hessian. this approach
seems to deal very well with the vanishing gradient,
though more detailed analysis is still missing. pre-
sumably this method works because in high dimen-
sional spaces there is a high id203 for long term
components to be orthogonal to short term ones. this
would allow the hessian to rescale these components
independently. in practice, one can not guarantee that
this property holds. as discussed in section 2.3, this
method is able to deal with the exploding gradient
as well. structural damping is an enhancement that
forces the change in the state to be small, when the pa-
rameter changes by some small value     . this asks for
@    to have small norm, hence
further helping with the exploding gradients problem.
the fact that it helps when training recurrent neural
models on long sequences suggests that while the cur-
vature might explode at the same time with the gradi-
ent, it might not grow at the same rate and hence not
be su cient to deal with the exploding gradient.

48

echo state networks (luko  sevi  cius and jaeger, 2009)

algorithm 1 pseudo-code for norm clipping the gra-
dients whenever they explode

  g   @e@   
if k  gk   threshold then
end if

  g   threshold

k  gk

  g

    makes	a	big	difference	in	id56s.

this algorithm is very similar to the one proposed by
tomas mikolov and we only diverged from the original
proposal in an attempt to provide a better theoretical
foundation (ensuring that we always move in a de-
scent direction with respect to the current mini-batch),
though in practice both variants behave similarly.

the proposed clipping is simple to implement and
computationally e cient, but it does however in-
troduce an additional hyper-parameter, namely the
threshold. one good heuristic for setting this thresh-

gradient	clipping	intuition

on the di culty of training recurrent neural networks

on	the	difficulty	of	
training	recurrent	neural	
networks,	pascanu et	al.	
2013

rithm should work even when the rate of growth of the
gradient is not the same as the one of the curvature
(a case for which a second order method would fail
as the ratio between the gradient and curvature could
still explode).

error	surface	of	a	single	hidden	unit	id56,	

   
figure 6. we plot the error surface of a single hidden unit
    high	curvature	walls
recurrent network, highlighting the existence of high cur-
vature walls. the solid lines depicts standard trajectories
   
that id119 might follow. using dashed arrow
the diagram shows what would happen if the gradients is
rescaled to a    xed size when its norm is above a threshold.
    dashed	lines	gradients	rescaled	to	fixed	size
49

solid	lines:	standard	gradient	descent	trajectories	

richard	socher

our hypothesis could also help to understand the re-
cent success of the hessian-free approach compared
to other second order methods. there are two key dif-
ferences between hessian-free and most other second-
order algorithms. first, it uses the full hessian matrix
and hence can deal with exploding directions that are
not necessarily axis-aligned. second, it computes a
new estimate of the hessian matrix before each up-
date step and can take into account abrupt changes in
curvature (such as the ones suggested by our hypothe-
sis) while most other approaches use a smoothness as-
sumption, i.e., averaging 2nd order signals over many
steps.

4/17/16

explode so does the curvature along v, leading to a

3. dealing with the exploding and

summary

tips	and	tricks	to	become	a	deep	neural	net	ninjia
introduction	to	recurrent	neural	network	

next	week:	
   lecture	on	tensorflow for	practical	implementations,	

pset	and	project

   more	id56	details	and	variants	(lstms	and	grus)	
   exciting	times!
50

richard	socher

4/17/16

