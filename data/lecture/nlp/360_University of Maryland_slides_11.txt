loss-augmented	
structured	prediction
cmsc	723	/	ling	723	/	inst	725

marine	carpuat

figures,	algorithms	&	equations
from	ciml	chap	17

pos	tagging
sequence	labeling	with	the	id88

sequence	labeling	problem
    input:

    sequence	of	tokens	x	=	[x1     xl]
    variable	length	l

    output	(aka	label):	

    sequence	of	tags	y	=	[y1     yl]
    #	tags	=	k
    size	of	output	space?

structured	id88
    id88	algorithm	can	be	used	for	

sequence	labeling

    but	there	are	challenges

    how	to	compute	argmax efficiently?
    what	are	appropriate	features?

    approach:	leverage	structure	of	

output	space

solving	the	argmax problem	for	sequences	
with	dynamic	programming

    efficient	algorithms	possible	if	
the	feature	function	
decomposes	over	the	input

    this	holds	for	unary	and	markov
features	used	for	pos	tagging

feature	functions	for	sequence	labeling

    standard	features	of	pos	tagging

    unary	features: #	times	word	w	has	been	
labeled	with	tag	l	for	all	words	w	and	all	
tags	l

    markov	features: #	times	tag	l	is	adjacent	

to	tag	l   	in	output	for	all	tags	l	and	l   

    size	of	feature	representation	is	constant	wrt

input	length

solving	the	argmax problem	for	sequences

    trellis	sequence	labeling

    any	path	represents	a	labeling	of	

input	sentence	

    gold	standard	path	in	red

    each	edge	receives	a	weight	such	that	
adding	weights	along	the	path	
corresponds	to	score	for	input/ouput
configuration

    any	max-weight	max-weight	path	
algorithm	can	find	the	argmax

    e.g.	viterbi	algorithm	o(lk2)

defining	weights	of	edge	in	treillis

unary	features	at	position	l	

together	with	markov	features	that	

end	at	position	l

    weight	of	edge	that	goes	from	time	l-
1	to	time	l,	and	transitions	from	y	to	y   

dynamic	program

    define:	the	score	of	best	possible	output	prefix	up	
to	and	including	position	l	that	labels	the	l-th word	
with	label	k

    with	decomposable	features,	alphas	can	be	

computed	recursively

a	more	general	approach	for	argmax
integer	linear	programming
    ilp:	optimization	problem	of	the	form,	

for	a	fixed	vector	a

    with	integer	constraints

    pro:	can	leverage	well-engineered	

solvers	(e.g.,	gurobi)

    con:	not	always	most	efficient

pos	tagging	as	ilp

    markov	features	as	binary	indicator	variables

    enforcing	constraints	for	well	formed	

solutions

    output	sequence:	y(z)	obtained	by	reading	off	

variables	z

    define	a	such	that	a.z is	equal	to	score

sequence	labeling

    structured	id88

sequence	labeling

    a	general	algorithm	for	structured	prediction	problems	such	as	

    the	argmax problem

    efficient	argmax for	sequences	with	viterbi	algorithm,	given	some	

assumptions	on	feature	structure

    a	more	general	solution:	integer	linear	programming

    loss-augmented	structured	prediction

    training	algorithm
    loss-augmented	argmax

in	structured	id88,	all	errors	are	
equally	bad

all	bad	output	sequences	are	not	equally	bad

    hamming	loss

    gives	a	more	nuanced	evaluation	

of	output	than	0   1	loss

       "#=     ,    ,    ,    
          #=[    ,    ,    ,    ]

    consider	

loss	functions	for	structured	prediction

    recall	learning	as	optimization	for	classification

    e.g.,	

    let   s	define	a	structure-aware	optimization	objective

    e.g.,	

structured	hinge	loss
   
0	if		true	output	beats	
score	of	every	imposter	
output

    otherwise:	scales	linearly	

as	function	of	score	diff	
between	most	confusing	
imposter	and	true	output

optimization:	stochastic	subid119

    subgradients of	structured	hinge	
loss?

optimization:	stochastic	subid119

    subgradients of	structured	hinge	loss

optimization:	stochastic	subid119
resulting	training	algorithm

only	2	differences	compared	to	structured	id88!

loss-augmented	id136/search
recall	dynamic	programming	solution	without	hamming	loss

loss-augmented	id136/search	
dynamic	programming	with	hamming	loss

we	can	use	viterbi	
algorithm	as	before	as	long	
as	the	loss	function	
decomposes	over	the	input	
consistently	w	features!

sequence	labeling

    structured	id88

sequence	labeling

    a	general	algorithm	for	structured	prediction	problems	such	as	

    the	argmax problem

    efficient	argmax for	sequences	with	viterbi	algorithm,	given	some	

assumptions	on	feature	structure

    a	more	general	solution:	integer	linear	programming

    loss-augmented	structured	prediction

    training	algorithm
    loss-augmented	argmax

syntax	&	grammars

from	sequences	to	trees

syntax	&	grammar

    syntax

    from	greek	syntaxis,	meaning	   setting	out	together   
    refers	to	the	way	words	are	arranged	together.	

    grammar

    set	of	structural	rules	governing	composition	of		clauses,	phrases,	and	words	

in	any	given	natural	language
    descriptive,	not	prescriptive
    panini   s	grammar	of	sanskrit	~2000	years	ago

syntax	and	grammar

    goal	of	syntactic	theory

       explain	how	people	combine	words	to	form	sentences	and	how	children	

attain	knowledge	of	sentence	structure   

    implicit	knowledge	of	a	native	speaker
    acquired	without	explicit	instruction
    minimally	able	to	generate	all	and	only	the	possible	sentences	of	the	

    grammar	

language

[philips,	2003]

syntax	in	nlp

    syntactic	analysis	often	a	key	component	 in	applications

    grammar	checkers
    dialogue	systems
    question	answering	
    information	extraction
    machine	translation
       

two	views	of	syntactic	structure

    constituency	(phrase	structure)

    phrase	structure	organizes	words	in	nested	constituents

    dependency	structure

words

    shows	which	words	depend	on	(modify	or	are	arguments	of)	which	on	other	

constituency

    basic	idea:	groups	of	words	act	as	a	single	unit

    constituents	form	coherent	classes	that	behave	similarly

    with	respect	to	their	internal	structure:	e.g.,	at	the	core	of	a	noun	phrase	is	a	

    with	respect	to	other	constituents:	e.g.,	noun	phrases	generally	occur	before	

noun

verbs

constituency:	example

    the	following	are	all	noun	phrases	in	english...

    why?	

    they	can	all	precede	verbs
    they	can	all	be	preposed/postposed
       

grammars	and	constituency

    for	a	particular	language:

    what	are	the	   right   	set	of	constituents?
    what	rules	govern	how	they	combine?

    answer:	not	obvious	and	difficult

    that   s	why	there	are	many	different	theories	of	grammar	and	competing	

analyses	of	the	same	data!

    our	approach

    focus	primarily	on	the	   machinery   

context-free	grammars

    context-free	grammars	(id18s)
    aka	phrase	structure	grammars
    aka	backus-naur	form	(bnf)

    consist	of
    rules	
    terminals
    non-terminals

context-free	grammars

    terminals

    we   ll	take	these	to	be	words

    non-terminals

    the	constituents	in	a	language	(e.g.,	noun	phrase)

    rules

    consist	of	a	single	non-terminal	on	the	left	and	any	number	of	terminals	and	

non-terminals	on	the	right

an	example	grammar

parse	tree:	example

note:	equivalence	between	parse	trees	and	bracket	notation

dependency	grammars

    id18s	focus	on	constituents

    non-terminals	don   t	actually	appear	in	the	sentence

    in	dependency	grammar,	a	parse	is	a	graph	(usually	a	tree)	where:

    nodes	represent	words
    edges	represent	dependency	relations	between	words	

(typed	or	untyped,	directed	or	undirected)

dependency	grammars

    syntactic	structure	=	lexical	items	linked	by	binary	asymmetrical	
relations	called	dependencies	

dependency	relations

example	dependency	parse

they	hid	the	letter	on	the	shelf

compare	with	constituent	parse   	what   s	the	relation?

universal	dependencies	project

    set	of	dependency	relations	that	are

    linguistically	motivated
    computationally	useful
    cross-linguistically	applicable
    [nivre et	al.	2016]

    universaldependencies.org

summary

    syntax	&	grammar

    two	views	of	syntactic	structures

    context-free	grammars
    dependency	grammars
    can	be	used	to	capture	various	facts	about	the	structure	of	language	(but	not	

all!)

    treebanks as	an	important	resource	for	nlp

