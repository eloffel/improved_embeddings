vector'semantics

dense%vectors%

dan%jurafsky

sparse'versus'dense'vectors

    ppmi%vectors%are

    long (length%|v|=%20,000%to%50,000)
    sparse'(most%elements%are%zero)

    alternative:%learn%vectors%which%are

    short (length%200f1000)
    dense (most%elements%are%nonfzero)

2

dan%jurafsky

sparse'versus'dense'vectors

    why%dense%vectors?

    short%vectors%may%be%easier%to%use%as%features%in%machine%
learning%(less%weights%to%tune)
    dense%vectors%may%generalize%better%than%storing%explicit%counts
    they%may%do%better%at%capturing%synonymy:

    car and%automobile are%synonyms;%but%are%represented%as%
distinct%dimensions;%this%fails%to%capture%similarity%between%a%
word%with%car as%a%neighbor%and%a%word%with%automobile as%a%
neighbor

3

dan%jurafsky three'methods'for'getting'short'dense'

vectors

    singular%value%decomposition%(svd)

    a%special%case%of%this%is%called%lsa%    latent%semantic%analysis
       neural%language%model   finspired%predictive%models

    skipfgrams%and%cbow

    brown%id91

4

vector'semantics
dense%vectors%via%svd

dan%jurafsky

intuition

    approximate%an%nfdimensional%dataset%using%fewer%dimensions
    by%first%rotating%the%axes%into%a%new%space
   

in%which%the%highest%order%dimension%captures%the%most%
variance%in%the%original%dataset

    and%the%next%dimension%captures%the%next%most%variance,%etc.
    many%such%(related)%methods:

    pca%    principle%components%analysis
    factor%analysis
    svd

6

dan%jurafsky

6
6

dimensionality'reduction

5
5

pca dimension 1

pca dimension 2

4
4

3
3

2
2

1
1

7

1
1

2
2

3
3

4
4

5
5

6
6

dan%jurafsky

singular'value'decomposition

any/rectangular/w/x/c/matrix/x/equals/the/product/of/3/matrices:
w:%rows%corresponding%to%original%but%m%columns%represents%a%
dimension%in%a%new%latent%space,%such%that%

    m%column%vectors%are%orthogonal%to%each%other
    columns%are%ordered%by%the%amount%of%variance%in%the%dataset%each%new%

dimension%accounts%for

s:%%diagonal%m x%m matrix%of%singular'values'expressing%the%
importance%of%each%dimension.
c:%columns%corresponding%to%original%but%m%rows%corresponding%to%
singular%values
8

dan%jurafsky

here.  suffice  it  to  say  that  cookbook  versions  of  svd  adequate  for 
small (e.g.,  100      100)  matrices  are  available  in  several places  (e.g., 
mathematica,  1991 ), and a free software version (berry,  1992)  suitable 

singular'value'decomposition

contexts 

3= 

wxc 

w 

xm 

m x m  

m x c  

the matrix formed to represent this text is shown in figure a2.  (we 
discuss  the  highlighted  parts  of  the  tables  in  due  course.)  the  initial 
matrix  has  nine  columns,  one  for  each  title,  and  we  have  given  it  12 
rows, each corresponding to  a  content word that occurs in  at least two 
contexts. these are the words in italics. in lsa analyses of text, includ- 
ing some of those reported above, words that appear in only one context 
are often omitted in doing the svd. these contribute little to derivation 
of the space,  their vectors can be  constructed after the svd  with  little 
loss  as  a  weighted  average  of words  in  the  sample  in  which  they  oc- 
curred,  and their omission sometimes greatly reduces the computation. 
see  deerwester,  dumais, furnas, landauer, and harshman (1990)  and 
dumais (1994)  for more on such details. for simplicity of presentation, 

9

figure  a1.  schematic  diagram  of  the  singular  value  decomposition 
(svd)  of  a  rectangular  word  (w)  by  context  (c)  matrix  (x).  the 

landuaer and%dumais 1997

a~ inquiries  about  lsa  computer  programs  should  be  addressed  to 
susan t. dumais, bellcore,  600  south  street,  morristown,  new  jersey 

dan%jurafsky

svd'applied'to'term<document'matrix:
latent'semantic'analysis

reduce the number of dimensions systematically by, for example, remov- 
ing those with the smallest effect on the sum-squared error of the approx- 
imation simply by deleting those with the smallest singular values. 

the actual algorithms used to compute svds for large sparse matrices 
of the sort involved in lsa are rather sophisticated and are not described 
here.  suffice  it  to  say  that  cookbook  versions  of  svd  adequate  for 
small (e.g.,  100      100)  matrices  are  available  in  several places  (e.g., 
mathematica,  1991 ), and a free software version (berry,  1992)  suitable 

deerwester et%al%(1988)

if%instead%of%keeping%all%m%dimensions,%we%just%keep%the%top%k%
singular%values.%let   s%say%300.

    the%result%is%a%leastfsquares%approximation%to%the%original%x
    but%instead%of%multiplying,%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

contexts 

   

10

we   ll%just%make%use%of%w.

    each%row%of%w:

    a%kfdimensional%vector
    representing%word%w

3= 

wxc 

k
xm 
/

w 

/
/
m x m  
k
k

/
m x c  
k

dan%jurafsky

lsa'more'details

    300%dimensions%are%commonly%used
    the%cells%are%commonly%weighted%by%a%product%of%two%weights

    local%weight:%%log%term%frequency
    global%weight:%either%idf or%an%id178%measure

11

dan%jurafsky

let   s'return'to'ppmi'word<word'matrices

    can%we%apply%to%svd%to%them?

12

dan%jurafsky

original m. since the    rst dimensions encode the most variance, one way to view
the reconstruction is thus as modeling the most important information in the original
dataset.

svd'applied'to'term<term'matrix

svd applied to co-occurrence matrix x:

x

=

w

3777775

2666664

2666664

s1 0
0 . . . 0
0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sv
0
0
|v|   |v|

...

3777775

2666664

|v|   |v|

|v|   |v|
taking only the top k dimensions after svd applied to co-occurrence matrix x:

|v|   |v|

(i   m%simplifying%here%by%assuming%the%matrix%has%rank%|v|)

s1 0

0 . . . 0

c

3777775

c

i

3777775
2666664

3777775

3777775

2666664

2666664

13

2666664

h

3777775

2666664

dan%jurafsky

3777775

2666664

3777775

2666664

...
0

...

...
...
0 . . . sv

...
0
|v|   |v|

3777775

|v|   |v|

|v|   |v|
taking only the top k dimensions after svd applied to co-occurrence matrix x:

truncated'svd'on'term<term'matrix

|v|   |v|

2666664

x

|v|   |v|

3777775

=

2666664

w

|v|    k

s1 0
0 . . . 0
0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sk
0
0
k    k

...

i

c

k   |v|

h

3777775

figure 19.11 svd factors a matrix x into a product of three matrices, w, s, and c. taking
the    rst k dimensions gives a |v|    k matrix wk that has one k-dimensioned row per word that
can be used as an embedding.

14

2666664

3777775
2666664

3777775

2666664

dan%jurafsky

3777775

2666664

truncated'svd'produces'embeddings
embedding

    each%row%of%w%matrix%is%a%kfdimensional%

|v|   |v|

|v|   |v|

representation%of%each%word%w
    k%might%range%from%50%to%1000
    generally%we%keep%the%top%k%dimensions,%

x

but%some%experiments%suggest%that%
getting%rid%of%the%top%1%dimension%or%%even%
the%top%50%dimensions%is%helpful%(lapesa
and%evert%2014).

|v|   |v|

2666664

for 
word i
=

3777775

2666664

w

|v|    k

3777775
2666664

3777775

s1 0

taking only the top k dimensions after svd applied to co-occurrence matrix x:

15

figure 19.11 svd factors a matrix x into a product of three matrices, w, s, and c. taking
the    rst k dimensions gives a |v|    k matrix wk that has one k-dimensioned row per word that
can be used as an embedding.

dan%jurafsky

embeddings versus'sparse'vectors

    dense%svd%embeddings sometimes%work%better%than%

sparse%ppmi%matrices%at%tasks%like%word%similarity
    denoising:%lowforder%dimensions%may%represent%unimportant%
information
    truncation%may%help%the%models%generalize%better%to%unseen%data.
    having%a%smaller%number%of%dimensions%may%make%it%easier%for%
classifiers%to%properly%weight%the%dimensions%for%the%task.
    dense%models%may%do%better%at%capturing%higher%order%cof
occurrence.%

16

vector'semantics

embeddings inspired%by%
neural%language%models:%
skipfgrams%and%cbow

dan%jurafsky prediction<based'models:

an'alternative'way'to'get'dense'vectors

    skip<gram (mikolov et%al.%2013a)%%cbow (mikolov et%al.%2013b)
    learn%embeddings as%part%of%the%process%of%word%prediction.
    train%a%neural%network%to%predict%neighboring%words

   
   

inspired%by%neural'net'language'models.
in%so%doing,%learn%dense%embeddings for%the%words%in%the%training%corpus.

    advantages:

    fast,%easy%to%train%(much%faster%than%svd)
    available%online%in%the%id97 package
    including%sets%of%pretrained embeddings!

18

dan%jurafsky

skip<grams

skip-gram and cbow (continuous bag of words) (mikolov et al. 2013, mikolov
cbow et al. 2013a), draw inspiration from the neural methods for id38 intro-
duced in chapter 5. like the neural language models, these models train a network
to predict neighboring words, and while doing so learn dense embeddings for the
words in the training corpus. the advantage of these methods is that they are fast,
ef   cient to train, and easily available online in the id97 package; code and
pretrained embeddings are both available.
    so%for%c=2,%we%are%given%word%wt and%predicting%these%
we   ll begin with the skip-gram model. the skip-gram model predicts each
4%words:
neighboring word in a context window of 2c words from the current word. so
for a context window c = 2 the context is [wt 2,wt 1,wt+1,wt+2] and we are pre-
dicting each of these from word wt. fig. 17.12 sketches the architecture for a sample

    predict%each%neighboring%word%
    in%a%context%window%of%2c/words%
    from%the%current%word.%

19

dan%jurafsky

skip<grams'learn'2'embeddings
for'each'w

input'embedding'v,/in%the%input%matrix%w
    column%i of%the%input%matrix%w/is%the%1(cid:1)d/
embedding%vi for%word%i in%the%vocabulary.%

output'embedding'vl,%in%output%matrix%w   
    row%i of%the%output%matrix%wl%is%a%d/(cid:1) 1%

vector%embedding%vli for%word%i in%the%
vocabulary.

20

1
2
.
.
d

1
2
.
.
.
.
i
.
.
.
.
|v|

w
i

.
.

1 2

   

|v|

d x  |v|

w   
d   

1 2

 |v| x d

dan%jurafsky

setup

    walking%through%corpus%pointing%at%word%w(t),%whose%index%in%

the%vocabulary%is%j,%so%we   ll%call%it%wj (1%<%j/<%|v/|).%

    let   s%predict%w(t+1)%,%whose%index%in%the%vocabulary%is%k/(1%<%k/<%

|v/|).%hence%our%task%is%to%compute%p(wk|wj).%

21

dan%jurafsky

intuition:'similarity'as'dot<product
between'a'target'vector'and'context'vector

target embedding

for word j

similarity( j , k)

1
.
.
.
d

w

target embeddings

1.2      .j         |vw|

context embedding

for word k

c

context embeddings

1. ..          d

1
.
.
k
.
.
|vw|

22

dan%jurafsky

similarity'is'computed'from'dot'product

    remember:%two%vectors%are%similar%if%they%have%a%high%

dot%product
    cosine%is%just%a%normalized%dot%product

    similarity(j,k) (cid:1) ck o%vj

    we   ll%need%to%normalize%to%get%a%id203

    so:

23

dan%jurafsky

for word k

turning'dot'products'into'probabilities

of course, the dot product ck    v j is not a id203, it   s just a number ranging
from      to    . we can use the softmax function from chapter 7 to normalize the dot
product into probabilities. computing this denominator requires computing the dot
product between each other word w in the vocabulary with the target word wi:

    similarity(j,k) = ck    vj
    we%use%softmax to%turn%into%probabilities

p(wk|w j) =

exp(ck    v j)
pi2|v|

exp(ci    v j)

in summary, the skip-gram computes the id203 p(wk|w j) by taking the dot

24

dan%jurafsky

embeddings from'w'and'w   

    since%we%have%two%embeddings,%vj and%cj for%each%word%wj
    we%can%either:

    just%use%vj
    sum%them
    concatenate%them%to%make%a%doubleflength%embedding

25

dan%jurafsky

learning

    start%with%some%initial%embeddings (e.g.,%random)
   

iteratively%make%the%embeddings for%a%word%
    more%like%the%embeddings of%its%neighbors%
    less%like%the%embeddings of%other%words.%

26

dan%jurafsky visualizing'w'and'c'as'a'network'for'doing'

error'backprop

input layer
1-hot input vector
x1
x2

projection layer
embedding for wt

wt

xj

w

|v|   d

c  d     |v|

output layer
probabilities of
context words

y1
y2

yk

wt+1

x|v|
1   |v|

27

1   d

y|v|
1   |v|

dan%jurafsky

one<hot'vectors

    a%vector%of%length%|v|%
    1%for%the%target%word%and%0%for%other%words
    so%if%   popsicle   %is%vocabulary%word%5
    the%one<hot'vector'is
   
[0,0,0,0,1,0,0,0,0      .0]

w1

w0
w|v|
0 0 0 0 0     0 0 0 0 1 0 0 0 0 0     0 0 0 0

wj

28

dan%jurafsky

skip<gram

input layer
1-hot input vector
x1
x2

h%=%vj

projection layer
embedding for wt

wt

xj

w

|v|   d

c  d     |v|

o%=%ch
ok =%ckh
ok =%ckovj
output layer
probabilities of
context words

y1
y2

yk

wt+1

x|v|
1   |v|

29

1   d

y|v|
1   |v|

dan%jurafsky

of course, the dot product ck    v j is not a id203, it   s just a number ranging
from      to    . we can use the softmax function from chapter 7 to normalize the dot
product into probabilities. computing this denominator requires computing the dot
product between each other word w in the vocabulary with the target word wi:

    the%denominator:%have%to%compute%over%every%word%in%vocab

problem'with'the'softamx

p(wk|w j) =

exp(ci    v j)
instead:%just%sample%a%few%of%those%negative%words

exp(ck    v j)
pi2|v|

   

in summary, the skip-gram computes the id203 p(wk|w j) by taking the dot
product between the word vector for j (v j) and the context vector for k (ck), and
turning this dot product v j    ck into a id203 by passing it through a softmax

30

noise samples or negative samples: non-neighbor words. the goal will be to move
the embeddings toward the neighbor words and away from the noise words.

noise samples or negative samples: non-neighbor words. the goal will be to move
the embeddings toward the neighbor words and away from the noise words.

noise samples or negative samples: non-neighbor words. the goal will be to move
the embeddings toward the neighbor words and away from the noise words.
dan%jurafsky

this section offers a brief sketch of how this works. in the training phase, the
algorithm walks through the corpus, at each target word choosing the surrounding
for example, in walking through the example text below we come to the word
context words as positive examples, and for each positive example also choosing k
noise samples or negative samples: non-neighbor words. the goal will be to move
the embeddings toward the neighbor words and away from the noise words.

apricot, and let l = 2 so we have 4 context words c1 through c4:
lemon,

for example, in walking through the example text below we come to the word

for example, in walking through the example text below we come to the word

apricot, and let l = 2 so we have 4 context words c1 through c4:

goal'in'learning

negative
samples

samples

the embeddings toward the neighbor words and away from the noise words.

for example, in walking through the example text below we come to the word

c1

apricot, and let l = 2 so we have 4 context words c1 through c4:

c1

c4

c2

c1

c4

c2

w
c4

c1
w

lemon, a [tablespoon of apricot preserves or] jam

a [tablespoon of apricot preserves or] jam
c3

a [tablespoon of apricot preserves or] jam

for example, in walking through the example text below we come to the word

a [tablespoon of apricot preserves or] jam
c3

apricot, and let l = 2 so we have 4 context words c1 through c4:
w

the goal is to learn an embedding whose dot product with each context word
    make%the%word%like%the%context%words
is high. in practice skip-gram uses a sigmoid function s of the dot product, where
c1
apricot, and let l = 2 so we have 4 context words c1 through c4:
s(x) = 1
1+ex . so for the above example we want s(c1   w) +s(c2   w) +s(c3   w) +
the goal is to learn an embedding whose dot product with each context word
c3
the goal is to learn an embedding whose dot product with each context word
lemon,
the goal is to learn an embedding whose dot product with each context word
is high. in practice skip-gram uses a sigmoid function s of the dot product, where
s(c4   w) to be high.
is high. in practice skip-gram uses a sigmoid function s of the dot product, where
    we%want%this%to%be%high:
is high. in practice skip-gram uses a sigmoid function s of the dot product, where
s (x) = 1
1+ex . so for the above example we want s (c1   w) +s (c2   w) +s (c3   w) +
the goal is to learn an embedding whose dot product with each context word
s(x) = 1
1+ex . so for the above example we want s (c1   w) +s (c2   w) +s (c3   w) +
1+ex . so for the above example we want s(c1   w) +s(c2   w) +s(c3   w) +
s (c4   w) to be high.
in addition, for each context word the algorithm chooses k noise words according
is high. in practice skip-gram uses a sigmoid function s of the dot product, where
s (c4   w) to be high.
s(c4   w) to be high.
in addition, for each context word the algorithm chooses k noise words according
s (x) = 1
1+ex . so for the above example we want s (c1   w) +s (c2   w) +s (c3   w) +
to their unigram frequency. if we let k = 2, for each target/context pair, we   ll have 2
in addition, for each context word the algorithm chooses k noise words according
to their unigram frequency. if we let k = 2, for each target/context pair, we   ll have 2
s (c4   w) to be high.
    and%not%like%k randomly%selected%   noise%words   
in addition, for each context word the algorithm chooses k noise words according
to their unigram frequency. if we let k = 2, for each target/context pair, we   ll have 2
noise words for each of the 4 context words:
in addition, for each context word the algorithm chooses k noise words according
noise words for each of the 4 context words:
to their unigram frequency. if we let k = 2, for each target/context pair, we   ll have 2
noise words for each of the 4 context words:
to their unigram frequency. if we let k = 2, for each target/context pair, we   ll have 2
noise words for each of the 4 context words:
noise words for each of the 4 context words:
n7
[cement metaphysical dear coaxial

a [tablespoon of apricot preserves or] jam
the goal is to learn an embedding whose dot product with each context word
is high. in practice skip-gram uses a sigmoid function s of the dot product, where
1+ex . so for the above example we want s (c1   w) +s (c2   w) +s (c3   w) +
in addition, for each context word the algorithm chooses k noise words according
to their unigram frequency. if we let k = 2, for each target/context pair, we   ll have 2
n5
noise words for each of the 4 context words:

    we%want%this%to%be%low:
ding w; in other words we want s (n1   w) + s (n2   w) + ... + s (n8   w) to be low.
[cement metaphysical dear coaxial
31
we   d like these noise words n to have a low dot-product with our target embed-

[cement metaphysical dear coaxial
n8
n7
n1
n2
more formally, the learning objective for one word/context pair (w,c) is
ding w; in other words we want s (n1   w) + s (n2   w) + ... + s (n8   w) to be low.

n6
apricot attendant whence forever puddle]
ding w; in other words we want s (n1   w) + s (n2   w) + ... + s (n8   w) to be low.

n1
n8
more formally, the learning objective for one word/context pair (w,c) is

n5
[cement metaphysical dear coaxial
we   d like these noise words n to have a low dot-product with our target embed-
n1
n8

apricot attendant whence forever puddle]
n2

we   d like these noise words n to have a low dot-product with our target embed-

we   d like these noise words n to have a low dot-product with our target embed-

we   d like these noise words n to have a low dot-product with our target embed-

apricot attendant whence forever puddle]
n7

apricot attendant whence forever puddle]

[cement metaphysical dear coaxial

[cement metaphysical dear coaxial

n2
n3

c2

n3

n4

n5

n6

n7

n2

n4

n6

n3

n1

n4

n6

n8

c3

c2

n2

n3

n4

n5

c4

w

dan%jurafsky

we   d like these noise words n to have a low dot-product with our target embed-

ding w; in other words we want s (n1   w) + s (n2   w) + ... + s (n8   w) to be low.

more formally, the learning objective for one word/context pair (w,c) is

skipgram with'negative'sampling:
loss'function

logs (c   w) +

kxi=1

ewi   p(w) [logs ( wi    w)]

that is, we want to maximize the dot product of the word with the actual context
word, and minimize the dot products of the word with the k negative sampled non-
neighbor words. the noise words wi are sampled from the vocabulary v according

32

dan%jurafsky

relation'between'skipgrams and'pmi!

if%we%multiply%ww   t

   
    we%get%a%|v|x|v|%matrix%m ,%each%entry%mij corresponding%to%
some%association%between%input%word%i and%output%word%j/
    levy%and%goldberg%(2014b)%show%that%skipfgram%reaches%its%
optimum%just%when%this%matrix%is%a%shifted%version%of%pmi:

wwlt/=mpmi%   log%k/

    so%skipfgram%is%implicitly%factoring%a%shifted%version%of%the%pmi%

matrix%into%the%two%embedding%matrices.

33

19.5 properties of embeddings

dan%jurafsky

properties'of'embeddings
we   ll discuss in section 17.8 how to evaluate the quality of different embeddings.
but it is also sometimes helpful to visualize them. fig. 17.14 shows the words/phrases
that are most similar to some sample words using the phrase-based version of the
skip-gram algorithm (mikolov et al., 2013a).

    nearest%words%to%some%embeddings (mikolov et%al.%20131)

target:

34

redmond
redmond wash.
redmond washington
microsoft

havel
vaclav havel
president vaclav havel martial arts
velvet revolution

capitulate
capitulation
capitulated
capitulating
figure 19.14 examples of the closest tokens to some target words using a phrase-based
extension of the skip-gram algorithm (mikolov et al., 2013a).

graf   ti
spray paint
gra   tti
taggers

ninjutsu
ninja

swordsmanship

one semantic property of various kinds of embeddings that may play in their

usefulness is their ability to capture relational meanings

mikolov et al. (2013b) demonstrates that the offsets between vector embeddings
can capture some relations between words, for example that the result of the ex-
pression vector(   king   ) - vector(   man   ) + vector(   woman   ) is a vector close to vec-

dan%jurafsky

embeddings capture'relational'meaning!

vector(   king   )%f vector(   man   )%+%vector(   woman   )%   "vector(   queen   )
vector(   paris   )%f vector(   france   )%+%vector(   italy   )%    vector(   rome   )

35

cross-lingual embeddings
    skip-gram allows us learning embeddings for words in a single 
language

vectors in l1 

law 

world 

life 

market 

children 

money 

country 

war 

peace 

energy 

slides courtesy shyam upadhyay

cross-lingual embeddings
    skip-gram allows us learning embeddings for words in a single 
language

    but what if we want to work with multiple languages?

vectors in l1 

law 

loi 

monde 
world 

life 

vie 

market 

marche 

enfants 

children 

argent 
money 

country 
pays 

war guerre 

peace 
paix 

energy 

energie 

vectors in l2 

slides courtesy shyam upadhyay

general schema for cross-lingual embeddings

initial embedding (optional)!

w!

vectors in l1 

cross-lingual!
supervision!
l1 and l2!

cross-lingual 
word vector 

model!

initial embedding (optional)!

v!

vectors in l2 

slides courtesy shyam upadhyay

general schema for cross-lingual embeddings

initial embedding (optional)!

w!

vectors in l1 

cross-lingual!
supervision!
l1 and l2!

cross-lingual 
word vector 

model!

initial embedding (optional)!

v!

vectors in l2 

slides courtesy shyam upadhyay

sources of cross-lingual supervision

decreasing	cost		

je	

t   	

aime	

je	t   	aime	

i	

love	 you	

i	love	you	

word + sentence 

sentence 

(love, aime) 

(you, t   ) 

 

 

(i, je) 

 
word 

bonjour!	je	t   	aime	

hello!	how	are	
you?	i	love	you	

document 

biskip 

luong et al. 15 

 

bicvm 

hermann et al. 14 

bicca 

faruqui et al. 14 

bivcd 
vulic et al. 15 

slides courtesy shyam upadhyay

bisparse - sparse bilingual embeddings
    a method to learn embeddings, that are


bilingual

sparse

non-negative

    starting from 


monolingual embeddings in two languages

a    seed    dictionary

bisparse

    method based on id105

xe

   

ae

t
de

monolingual 
corpus statistics

s

cross-lingual
knowledge

xf

   

af

t
df

bisparse

    method based on id105

xe

   

ae

t
de

monolingual 
corpus statistics

s

cross-lingual
knowledge

xf

   

af

t
df

bisparse

    method based on id105

xe

   

ae

t
de

monolingual 
corpus statistics

s

cross-lingual
knowledge

xf

   

af

t
df

bisparse

    method based on id105

xe

   

ae

t
de

monolingual 
corpus statistics

s

cross-lingual
knowledge

xf

   

af

t
df

building the s matrix

       

    nuit    > night

    dog    > chien

    cake    > gateau

       

dog[                                       

]                                       

chien
0 
.. 
0 
1 
.. 
.. 
0 

0   0  ..  

0   0  ..  

interpreting embeddings

summary

    vector semantics with dense vectors

    singular value decomposition

    skip-gram embeddings

    cross-lingual embeddings

    bisparse model

