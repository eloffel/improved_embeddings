word	meaning:
distributional	representations	&	
word	sense	disambiguation

cmsc	723	/	ling	723	/	inst	725

marine	carpuat

slides	credit:	dan	jurafsky

reminders

    read	the	syllabus

    make	sure	you	have	access	to	piazza

    get	started	on	homework	1	    due	thursday	sep	7	by	12pm.

today:	word	meaning

2	core	issues	from	an	nlp	perspective

    semantic	similarity:	given	two	words,	how	similar	are	they	in	
meaning?

    word	sense	disambiguation:	given	a	word	that	has	more	than	one	
meaning,		which	one	is	used	in	a	specific	context?

word	similarity	
for	question	answering
   fast   	is	similar	to	   rapid   
   tall   	is	similar	to	   height   

question	answering:
q:	   how	tall is	mt.	everest?   
candidate	a:	   the	official	height of	mount	everest	is	
29029	feet   

word	similarity
for	plagiarism	detection

word	similarity	for	historical	linguistics:
semantic	change	over	time

kulkarni,	al-rfou,	perozzi,	skiena 2015

tesg  ino

a bottle of tesg  ino is on the table
everybody likes tesg  ino
tesg  ino makes you drunk
we make tesg  ino out of corn.

intuition:	two	words	are	similar	if	they	have	similar	word	contexts.

embedding	word	meaning	in	
vector	space
vector	semantics

distributional	models	of	meaning
=	vector-space	models	of	meaning	
=	vector	semantics

intuitions
zellig harris	(1954):

       oculist	and	eye-doctor	   	occur	in	almost	the	same	
environments   
       if	a	and	b	have	almost	identical	environments	we	
say	that	they	are	synonyms.   

firth	(1957):	

       you	shall	know	a	word	by	the	company	it	keeps!   

vector	semantics

    model	the	meaning	of	a	word	by	   embedding   	in	a	
vector	space.

    the	meaning	of	a	word	is	a	vector	of	numbers

    vector	models	are	also	called	   embeddings   .

    contrast:	word	meaning	is	represented	in	many	
nlp	applications	by	a	vocabulary	index	(   word	
number	545   )

many	varieties	of	vector	models

sparse	vector	representations

1. mutual-information	weighted	word	co-occurrence	matrices

dense	vector	representations:

2. singular	value	decomposition	(and	latent	semantic	analysis)
3. neural-network-inspired	models	(skip-grams,	cbow)

term-document	matrix

    each	cell:	count	of	term	t in	a	document	d:		tft,d

    each	document	is	a	count	vector	in	   v:	a	column	below	

as#you#like#it
1
2
37
6

twelfth#night
1
2
58
117

julius#caesar
8
12
1
0

battle
soldier
fool
clown

henry#v
15
36
5
0

term-document	matrix

    two	documents	are	similar	if	their	vectors	are	similar

as#you#like#it
1
2
37
6

twelfth#night
1
2
58
117

julius#caesar
8
12
1
0

battle
soldier
fool
clown

henry#v
15
36
5
0

the	words
in	a	term-document	matrix

    each	word	is	a	count	vector	in	   d:	a	row	below	
henry#v
15
36
5
0

twelfth#night
1
2
58
117

as#you#like#it
1
2
37
6

julius#caesar
8
12
1
0

battle
soldier
fool
clown

the	words	
in	a	term-document	matrix

    two	words are	similar	if	their	vectors	are	similar
henry#v
15
36
5
0

twelfth#night
1
2
58
117

as#you#like#it
1
2
37
6

julius#caesar
8
12
1
0

battle
soldier
fool
clown

the	word-word	
or	word-context	matrix
    instead	of	entire	documents,	use	smaller	contexts

    window	of	   4	words

    paragraph

    a	word	is	now	defined	by	a	vector	over	counts	of	
context	words
    instead	of	each	vector	being	of	length	d

    each	vector	is	now	of	length	|v|
    the	word-word	matrix	is	|v|x|v|

sample	contexts	   7	words

word co-occur in some context in some training corpus. the context could be the
document, in which case the cell represents the number of times the two words
appear in the same document. it is most common, however, to use smaller contexts,
such as a window around the word, for example of 4 words to the left and 4 words
to the right, in which case the cell represents the number of times (in some training
corpus) the column word occurs in such a   4 word window around the row word.
for example here are 7-word windows surrounding four sample words from the
brown corpus (just one example of each word):

word-word	matrix

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

preserve or jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

aardvark computer
0
0
2
0
1
0
2
1

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 17.2 shows a selection from the word-word
apricot
co-occurrence matrix computed from the brown corpus for these four words.
pineapple
digital
apricot
information
pineapple
   
information
figure 19.2 co-occurrence vectors for four words, computed from the brown corpus,
showing only six of the dimensions (hand-picked for pedagogical purposes). note that a
real vector would be vastly more sparse.

data pinch result
0
0
result
1
4

0
0
aardvark
0
0
   

sugar
1
1
0
0

1
1
pinch
0
0

computer

...
...
...
...
...

0
0
1
6

digital

sugar

0
0
1
6

1
1
0
0

1
1
0
0

0
0
1
4

data

0
0
0
0

   

...

word-word	matrix

    the	|v|x|v|	matrix	is	very	sparse	(most	values	are	0)

    the	size	of	windows	depends	on	representation	goals

    the	shorter	the	windows	,	the	more	syntactic the	representation

    the	longer	the	windows,	the	more	semantic the	representation

   1-3	very	syntacticy
   4-10	more	semanticy

positive	pointwise	mutual	
information	(ppmi)

vector	semantics

problem	with	raw	counts

    raw	word	frequency	is	not	a	great	measure	of	
association	between	words

    we   d	rather	have	a	measure	that	asks	whether	a	
context	word	is	particularly	informative	about	the	
target	word.

    positive	pointwise	mutual	information	(ppmi)

pointwise mutual	information

pointwise	mutual	information:	

do	events	x	and	y	co-occur	more	than	if	they	were	
independent?

pmi(x,y) = log2

p(x,y)
p(x)p(y)

pmi	between	two	words:		(church	&	hanks	1989)

do	words	x	and	y	co-occur	more	than	if	they	were	
independent?	

pmi                ),                + =log+ 0(23456,23457)
0234560(23457)

positive	pointwise mutual	information

    pmi	ranges	from	      		to	+   

    but	the	negative	values	are	problematic

things	are	co-occurring	less	than	we	expect	by	chance

   
    unreliable	without	enormous	corpora

    so	we	just	replace	negative	pmi	values	by	0
    positive	pmi	(ppmi)	between	word1	and	word2:

ppmi                ),                + =max log+     (                ),                +)
                    )    (                +),0

computing	ppmi	on	a	term-context	matrix

    matrix	f with	w rows	
(words)	and	c columns	
(contexts)
    fij is	#	of	times	wi occurs	in	
context	cj

pij =

fij
c
   
j=1

w
   
i=1

fij

pmiij = log2

pi* =

pij
pi*p* j

fij

c
   
j=1
w
c
   
   
j=1
i=1
ppmiij =

fij

!
#
"
$#

fij

w
   
i=1
w
c
   
   
j=1
i=1
if  pmiij > 0
pmiij
otherwise
0

fij

p*j =

pij =

fij
c
   
j=1

w
   
i=1

fij

p(w=information,c=data)	=	
p(w=information)	=
p(c=data)	=

7/19 =	.37

6/19
11/19 =	.58

=	.32

c
fij
   
j=1
n

p(wi) =

apricot
pineapple
digital
information

computer
0.00
0.00
0.11
0.05

p(w,context)

data pinch result
0.00
0.00
0.00
0.00
0.05
0.05
0.21
0.32

0.05
0.05
0.00
0.00

sugar
0.05
0.05
0.00
0.00

p(context)

0.16

0.37

0.11

0.26

0.11

w
fij
   
i=1
n

p(cj) =

p(w)

0.11
0.11
0.21
0.58

apricot
pineapple
digital
information
p(context)

apricot
pineapple
digital
information

computer
0.00
0.00
0.11
0.05

0.16

computer
1
1
1.66
0.00

pmiij = log2

pij
pi*p* j

p(w,context)

data pinch result
0.00
0.00
0.00
0.00
0.05
0.05
0.21
0.32

0.05
0.05
0.00
0.00

p(w)

0.11
0.11
0.21
0.58

sugar
0.05
0.05
0.00
0.00

0.37

0.11
ppmi(w,context)

0.26

0.11

data pinch result
1
1
0.00
0.47

1
1
0.00
0.57

2.25
2.25
1
1

sugar
2.25
2.25
1
1

weighting	pmi

    pmi	is	biased	toward	infrequent	events
    very	rare	words	have	very	high	pmi	values

    two	solutions:

    give	rare	words	slightly	higher	probabilities
    use	add-k smoothing	(which	has	a	similar	effect)

0

0

0

0.47

0.57

information
figure 19.4 the ppmi matrix showing the association between words and context words,
computed from the counts in fig. 17.2 again showing six dimensions.

weighting	pmi:	giving	rare	context	words	
slightly	higher	id203

pmi has the problem of being biased toward infrequent events; very rare words
tend to have very high pmi values. one way to reduce this bias toward low frequency
events is to slightly change the computation for p(c), using a different function pa (c)
that raises contexts to the power of a (levy et al., 2015):

    raise	the	context	probabilities	to	    =0.75:
.l).ijk.l).ij=.03
.hh.ijk.l).ij=.97    f     = .l).ij
    f     = .hh.ij
pc count(c)a

levy et al. (2015) found that a setting of a = 0.75 improved performance of
    consider	two	events,	p(a)	=	.99	and	p(b)=.01
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams (mikolov et al., 2013a) and glove (pennington et al., 2014)). this works
because raising the id203 to a = 0.75 increases the id203 assigned to rare
contexts, and hence lowers their pmi (pa (c) > p(c) when c is rare).

ppmia (w,c) = max(log2

(19.8)

(19.9)

p(w)pa (c)

count(c)a

pa (c) =

p(w,c)

,0)

another possible solution is laplace smoothing: before computing pmi, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. the larger the k, the more the non-zero counts
are discounted.

add-2	smoothing

apricot
pineapple
digital
information

computer
2
2
4
3

add#2%smoothed%count(w,context)
sugar
3
3
2
2

data pinch result
2
2
3
6

3
3
2
2

2
2
3
8

ppmi	vs	add-2	smoothed	ppmi

apricot
pineapple
digital
information

computer
1
1
1.66
0.00

ppmi(w,context)

1
1
0.00
0.57

data pinch result
1
1
0.00
0.47
ppmi(w,context).[add22]

2.25
2.25
1
1

apricot
pineapple
digital
information

computer
0.00
0.00
0.62
0.00

data pinch result
0.00
0.00
0.00
0.00
0.00
0.00
0.37
0.58

0.56
0.56
0.00
0.00

sugar
2.25
2.25
1
1

sugar
0.56
0.56
0.00
0.00

tf.idf:	an	alternative	to	ppmi	
for	measuring	association

    the	combination	of	two	factors

    tf:	term	frequency	(luhn 1957):	frequency	of	the	word
    idf:	inverse	document	frequency (sparck jones	1972)
$
idfi = log n
&
dfi
&
%

    n	is	the	total	number	of	documents
    dfi =	   document	frequency	of	word	i   

=	#	of	documents	with	word	i

!
#
#
"

    wij =	word	i in	document	j

wij=tfij idfi

measuring	similarity:	the	cosine
vector	semantics

cosine	for	computing	similarity

sec.	6.3

dot	product

unit	vectors

cos(   v,    w) =

   v    
   w
   v    w =

   v
   v    

   w
   w =

n
   
i=1
n
2
vi
i=1

viwi
n
   
i=1

   

2
wi

vi is	the	ppmi	value	for	word	v in	context	i
wi is	the	ppmi	value	for	word	w in	context	i.
cos(v,w)	is	the	cosine	similarity	of	v and	w

other	possible	similarity	measures

evaluating	similarity
vector	semantics

evaluating	similarity

    extrinsic	(task-based,	end-to-end)	evaluation:

    question	answering
    spell	checking
    essay	grading

    intrinsic	evaluation:

    correlation	between	algorithm	and	human	word	similarity	ratings

    wordsim353:	353	noun	pairs	rated	0-10.			sim(plane,car)=5.77

    taking	toefl	multiple-choice	vocabulary	tests

    levied is closest in meaning to:
imposed, believed, requested, correlated

today:	word	meaning

2	core	issues	from	an	nlp	perspective

    semantic	similarity:	given	two	words,	how	similar	are	they	in	
meaning?

    word	sense	disambiguation:	given	a	word	that	has	more	than	one	
meaning,		which	one	is	used	in	a	specific	context?

   big	rig	carrying	fruit	crashes	on	210	freeway,	
creates	jam   

http://articles.latimes.com/2013/may/20/local/la-me-ln-big-rig-crash-20130520

how	do	we	know	that	a	word	(lemma)	
has	distinct	senses?

    linguists	often	design	tests	for	
this	purpose

    e.g.,	zeugma combines	distinct	
senses	in	an	uncomfortable	way

which    ight serves breakfast?

which    ights serve bwi?

*which    ights serve breakfast 
and bwi? 

word	senses

       word	sense   	=	distinct	meaning	of	a	word
    same	word,	different	senses

    homonyms (homonymy):	unrelated	senses;	identical	orthographic	form	is	

coincidental

    e.g.,	financial	bank	vs.	river	bank

    polysemes (polysemy):	related,	but	distinct	senses

    e.g.,	financial	bank	vs.	blood	bank	vs.	tree	bank

    metonyms (metonymy):	   stand	in   ,	technically,	a	sub-case	of	polysemy

    e.g.,	use	   washington   	in	place	of	   the	us	government   

    different	word,	same	sense

    synonyms (synonymy)

    homophones:	same	pronunciation,	different	orthography,	different	
meaning
    examples:	would/wood,	to/too/two

    homographs:	distinct	senses,	same	orthographic	form,	different	
pronunciation
    examples:	bass	(fish)	vs.	bass	(instrument)

relationship	between	senses

    is-a	relationships

    from	specific	to	general	(up):	hypernym (hypernymy)
    from	general	to	specific	(down):	hyponym	(hyponymy)

    part-whole	relationships

    wheel	is	a	meronym of	car	(meronymy)
    car	is	a	holonym of	wheel	(holonymy)

id138:	
a	lexical	database	for	english
https://id138.princeton.edu/

    includes	most	english	nouns,	verbs,	adjectives,	adverbs
    electronic	format	makes	it	amenable	to	automatic	manipulation:	used	
in	many	nlp	applications
       id138s   	generically	refers	to	similar	resources	in	other	languages

synonymy	in	id138

    id138	is	organized	in	terms	of	   synsets   

    unordered	set	of	(roughly)	synonymous	   words   	(or	multi-word	phrases)

    each	synset expresses	a	distinct	meaning/concept	

id138:	example

noun
{pipe,	tobacco	pipe}	(a	tube	with	a	small	bowl	at	one	end;	used	for	smoking	

{pipe,	pipage,	piping}	(a	long	tube	made	of	metal	or	plastic	that	is	used	to	carry	

tobacco)	

water	or	oil	or	gas	etc.)	

{pipe,	tube}	(a	hollow	cylindrical	shape)	
{pipe}	(a	tubular	wind	instrument)	
{organ	pipe,	pipe,	pipework}	(the	flues	and	stops	on	a	pipe	organ)	

verb
{shriek,	shrill,	pipe	up,	pipe}	(utter	a	shrill	cry)	
{pipe}	(transport	by	pipeline)	   pipe	oil,	water,	and	gas	into	the	desert   
{pipe}	(play	on	a	pipe)	   pipe	a	tune   
{pipe}	(trim	with	piping)	   pipe	the	skirt   

id138	3.0:	size

part	of	speech
noun
verb
adjective
adverb
total

word	form
117,798
11,529
21,479
4,481
155,287

synsets
82,115
13,767
18,156
3,621
117,659

http://id138.princeton.edu/

word	sense	disambiguation

    task:	automatically	select	the	correct	sense	of	a	word

    input:	a	word	in	context
    output:	sense	of	the	word

    motivated	by	many	applications:

    information	retrieval
    machine	translation
       

how	big	is	the	problem?

    most	words	in	english	have	only	one	sense

    62%	in	longman   s	dictionary	of	contemporary	english
    79%	in	id138

    but	the	others	tend	to	have	several	senses

    average	of	3.83	in	ldoce
    average	of	2.96	in	id138

    ambiguous	words	are	more	frequently	used

    in	the	british	national	corpus,	84%	of	instances	have	more	than	one	sense

    some	senses	are	more	frequent	than	others

baseline	performance

    baseline:	most	frequent	sense

    equivalent	to	   take	first	sense   	in	id138
    does	surprisingly	well!

62%	accuracy	in	this	case!

upper	bound	performance

    upper	bound

    fine-grained	id138	sense:	75-80%	human	agreement

    coarser-grained	inventories:	90%	human	agreement	possible

simplest	wsd	algorithm:
lesk   s algorithm

    intuition:	note	word	overlap	between	context	and	dictionary	entries

    unsupervised, but	knowledge	rich

the	bank	can	guarantee	deposits	will	eventually	cover	future	tuition	costs	
because	it	invests	in	adjustable-rate	mortgage	securities.		
id138

lesk   s algorithm

    simplest	implementation:

    count	overlapping	content	words	between	glosses	and	context

    lots	of	variants:

    include	the	examples	in	dictionary	definitions
    include	hypernyms and	hyponyms
    give	more	weight	to	larger	overlaps	(e.g.,	bigrams)
    give	extra	weight	to	infrequent	words
       

alternative:	wsd	as	supervised classification

training

testing

training data

?

unlabeled 
document

label1

label2

label3

label4

feature functions

supervised machine 
learning algorithm

classifier

label1?

label2?

label3?

label4?

existing	corpora

    lexical	sample

    line-hard-serve corpus	(4k	sense-tagged	examples)
    interest	corpus (2,369	sense-tagged	examples)
       	

    all-words

    semcor (234k	words,	subset	of	brown	corpus)
    senseval/semeval (2081	tagged	content	words	from	5k	total	words)
       

word	meaning

2	core	issues	from	an	nlp	perspective

    semantic	similarity:	given	two	words,	how	similar	are	they	in	
meaning?
    key	concepts:	vector	semantics,	ppmi	and	its	variants,	cosine	similarity

    word	sense	disambiguation:	given	a	word	that	has	more	than	one	
meaning,		which	one	is	used	in	a	specific	context?
    key	concepts:	word	sense,	id138	and	sense	inventories,	
unsupervised	disambiguation	(lesk),	supervised	disambiguation

