phrase-based
machine translation

cmsc 723 / ling 723 / inst 725

marine carpuat
marine@cs.umd.edu

id87
for machine translation

    the id87 decomposes machine 

translation into two independent subproblems

    id38

    translation modeling / alignment

word alignment with 

ibm models 1, 2

    probabilistic models with strong independence 

assumptions

    alignments are hidden variables 

    unlike words which are observed

    require unsupervised learning (em algorithm)

    word alignments often used as building blocks 

for more complex translation models
    e.g., phrase-based machine translation

phrase-based models

phrase-based models

    most common way to model p(f|e) nowadays 

(instead of ibm models)

start position of 

f_i

end position of 

f_(i-1)

id203 of 

two consecutive 
english phrases 
being separated 
by a particular 
span in french

phrase alignments are derived 

this means that the 
ibm model represents 

from word alignments

p(spanish|english)

get high confidence 
alignment links by
intersecting ibm 
word alignments 

from both directions

phrase alignments are derived 

from word alignments

improve recall by adding 

some links from the 
union of alignments

phrase alignments are derived 

from word alignments

extract phrases that are consistent 

with word alignment

phrase translation probabilities

    given such phrases we can get the 

required statistics for the model from 

phrase-based machine translation

decoding

decoding for phrase-based mt

    basic idea

    search the space of possible english translations in an 

efficient manner.  

    according to our model

decoding as search

    starting point: null state.  no french content 

covered, no english included.

    we   ll drive the search by 

    choosing french word/phrases to    cover   , 

    choosing a way to cover them

    subsequent choices are pasted left-to-right to 

previous choices. 

    stop: when all input words are covered.

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

green

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

green

witch

decoding

maria

no

dio

una

bofetada

a

la

bruja

verde

mary

did not

slap

the

green

witch

decoding

    in practice: we need to incrementally 

pursue a large number of paths.

    solution: heuristic search algorithm called 

   multi-stack id125   

space of possible english translations 

given phrase-based model

stack decoding: a simplified view

note: here    stack     = priority queue

three stages of stack decoding

   multi-stack id125   

one stack per number of french 
words covered: so that we make 
apples-to-apples comparisons 

when pruning   

beam-search pruning for each stack: prune 
high cost states (those    outside the beam   )

   multi-stack id125   

cost = current cost + future cost 

    future cost = cost of translating remaining words in the 

french sentence

    exact future cost = minimum id203 of all remaining 

translations
    too expensive to compute!

    approximation

    find sequence of english phrases that has the minimum product 

of language model and translation model costs

recombination

    two distinct hypothesis paths might lead to the 

same translation hypotheses
    same number of source words translated

    same output words

    different scores

    recombination

    drop worse hypothesis

recombination

    two distinct hypothesis paths might lead to 

hypotheses that are indistinguishable in 
subsequent search
    same number of source words translated

    same last 2 output words (assuming 3-gram lm)

    different scores

    recombination

    drop worse hypothesis

complexity analysis

    time complexity of decoding as described so far

o(max stack size x sentence length^2)

    o( max stack size x number of ways to expand hyps. x sentence 

length)

reordering constraints

idea: limit reordering to maximum reordering distance

typically: 5 to 8 words

- depending on language pair

- empirically: larger limit hurts translation quality

resulting complexity: o(max stack size x sentence length)
    because we limit reordering distance, so that only a constant 

number of hypothesis expansions are considered

recap

id87
for machine translation

    the id87 decomposes machine 

translation into two independent subproblems

    id38

    translation modeling / alignment

phrase-based machine translation

    phrase-translation dictionary

phrase-based machine translation

    a simple model of translation

    phrase translation dictionary (   phrase-table   )

    extract all phrase pairs consistent with given 

alignment

    use relative frequency estimates for translation 

probabilities

    distortion model

    allows for reorderings

decoding in phrase-based 

machine translation

    approach: heuristic search 

    with several strategies to reduce the search 

space
    pruning

    recombination

    reordering constraints

what are the pros and cons of

phrase-based vs. neural mt?

