text	classification
&	linear	models

cmsc	723	/	ling	723	/	inst	725

marine	carpuat

slides	credit:	dan	jurafsky &	james	martin,	
jacob	eisenstein	

logistics/reminders

    homework	1	    due	thursday	sep	7	by	12pm.

    project	1	coming	up	

    thursday	lecture	time:	project	set-up	office	hour	in	csic	1121

recap:	word	meaning

2	core	issues	from	an	nlp	perspective

    semantic	similarity:	given	two	words,	how	similar	are	they	in	
meaning?
    key	concepts:	vector	semantics,	ppmi	and	its	variants,	cosine	similarity

    word	sense	disambiguation:	given	a	word	that	has	more	than	one	
meaning,		which	one	is	used	in	a	specific	context?
    key	concepts:	word	sense,	id138	and	sense	inventories,	
unsupervised	disambiguation	(lesk),	supervised	disambiguation

today

    text	classification	problems

    and	their	evaluation

    linear	classifiers

    features	&	weights
    bag	of	words
    na  ve	bayes

text	classification

is	this	spam?

from: "fabian starr    
<patrick_freeman@pamietaniepeerelu.pl>
subject: hey! sofware for the funny prices!

get the great discounts on popular software today 
for pc and macintosh
http://iiled.org/cj4lmx
70-90% discounts from retail price!!!
all sofware is instantly available to download - no 
need wait!

what	is	the	subject	of	this	article?

medline article

?

mesh subject	category	hierarchy

    antogonists and	
inhibitors
    blood	supply
    chemistry
    drug	therapy
    embryology
    epidemiology
       

text	classification

    assigning	subject	categories,	topics,	or	genres
    spam	detection
    authorship	identification
    age/gender	identification
    language	identification
    sentiment	analysis
       

text	classification:	definition

    input:

    a	document	d
    a	fixed	set	of	classes		y	= {y1,	y2,   ,	yj}

    output:	a	predicted	class	y    y

classification	methods:	
hand-coded	rules

    rules	based	on	combinations	of	words	or	other	
features
    spam:	black-list-address	or	(   dollars   	and	   have	been	

selected   )

    accuracy	can	be	high

    if	rules	carefully	refined	by	expert

    but	building	and	maintaining	these	rules	is	expensive

classification	methods:
supervised	machine	learning

    input

    a	document	d
    a	fixed	set	of	classes		y	= {y1,	y2,   ,	yj}
    a training	set	of	m hand-labeled	documents	(d1,y1),....,(dm,ym)

    output

    a	learned	classifier	d   y

aside:	getting	examples	for	supervised	
learning

    human	annotation

    by	experts	or	non-experts	(id104)
    found	data

    how	do	we	know	how	good	a	classifier	is?

    compare	classifier	predictions	with	human	annotation
    on	held	out test	examples
    evaluation	metrics:	accuracy,	precision,	recall

the	2-by-2	contingency	table

correct

not	correct

selected
not	selected

tp
fn

fp
tn

precision	and	recall

    precision:	%	of	selected	items	that	are	correct
recall:	%	of	correct	items	that	are	selected

correct

not	correct

selected
not	selected

tp
fn

fp
tn

a	combined	measure:	f

    a	combined	measure	that	assesses	the	p/r	tradeoff	is	f	measure	
(weighted	harmonic	mean):
1
1(
   +

pr
)1
+
rp
+

(
  
  

  

f

=

=

2

2

1
p

1)
  
r

    people	usually	use	balanced	f1	measure

   

i.e.,	with	b =	1	(that	is,	a =	  ):			

f =	2pr/(p+r)

linear	classifiers

bag	of	words

defining	features

defining	features

linear	classification

linear	models
for	classification

feature	
function	

representation

weights

how	can	we	learn	weights?

    by	hand

    id203

    e.g.,na  ve bayes

    discriminative	training

    e.g.,	id88,	support	vector	machines	

generative	story	
for	multinomial	na  ve	bayes

    a	hypothetical	stochastic	process	describing	how	training	examples	
are	generated

prediction	with	na  ve	bayes

score(x,y)

prediction	with	na  ve	bayes

score(x,y)

parameter	estimation	

       count	and	normalize   
    parameters	of	a	multinomial	distribution

    relative	frequency	estimator
    formally:	this	is	the	maximum	likelihood	estimate

    see	ciml	for	derivation

smoothing	(add	alpha	/	laplace)

na  ve	bayes	recap

today

    text	classification	problems

    and	their	evaluation

    linear	classifiers

    features	&	weights
    bag	of	words
    na  ve	bayes

