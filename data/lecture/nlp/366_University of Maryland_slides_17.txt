sequence to sequence models
for machine translation (2)

cmsc 723 / ling 723 / inst 725

marine carpuat

slides & figure credits: graham 
neubig

introduction to id4
    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    practical tricks

    sequence to sequence models for other nlp tasks

    attention mechanism

a recurrent language model

a recurrent language model

encoder-decoder model

encoder-decoder model

generating output

    we have a model p(e|f), how can we generate translations?

    2 methods

    sampling: generate a random sentence according to id203 distribution

    argmax: generate sentence with highest id203

training

    same as for id56 id38

    id168

    negative log-likelihood of training data
    total loss for one example (sentence) = sum of loss at each time step (word)

    id26 through time (bptt)

    gradient of loss at time step t is propagated through the network all the way 

back to 1st time step

note that training loss differs from 
evaluation metric (id7)

other encoder structures:
bidirectional encoder

   

motivation:
- help bootstrap learning
- by shortening length of 

dependencies

motivation: 
- take 2 hidden vectors from source 

encoder

- combine them into a vector of size 

required by decoder

a few more tricks: addressing length bias

    default models tend to generate short sentences
    solutions:

    prior id203 on sentence length

    normalize by sentence length

a few more tricks: ensembling

    combine predictions from 

multiple models

    methods

    linear or log-linear interpolation

    parameter averaging

introduction to id4
    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    practical tricks

    sequence to sequence models for other nlp tasks

    attention mechanism

beyond mt: encoder-decoder can be used as 
conditioned language models to generate text y 
according to some specification x

introduction to id4
    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    practical tricks

    sequence to sequence models for other nlp tasks

    attention mechanism

problem with previous encoder-decoder 
model

    long-distance dependencies remain a problem

    a single vector represents the entire source sentence

    no matter its length

    solution: attention mechanism

    an example of incorporating inductive bias in model architecture 

attention model intuition

    encode each word in source sentence into a vector

    when decoding, perform a linear combination of these vectors, 

weighted by    attention weights   

    use this combination when predicting next word

[bahdanau et al. 2015]

attention model
source word representations 
    we can use representations 

from bidirectional id56 encoder

    and concatenate them in a 

matrix

attention model
create a source context vector

    attention vector:

    entries between 0 and 1
    interpreted as weight given to 

each source word when 
generating output at time step t

context vector

attention vector

attention model
illustrating attention weights

attention model
how to calculate attention scores

attention model
various ways of calculating attention score
    dot product

    bilinear function

    multi-layer id88 (original 
formulation in bahdanau et al.)

advantages of attention

    helps illustrate/interpret translation decisions

    can help insert translations for oov

    by copying or look up in external dictionary

    can incorporate linguistically motivated priors in model

attention extensions
an active area of research
    attend to multiple sentences (zoph et al. 2015)

    attend to a sentence and an image (huang et al. 2016)

    incoprorate bias from alignment models

introduction to id4
    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    practical tricks

    sequence to sequence models for other nlp tasks

    attention mechanism

