dependency	parsing	2
cmsc	723	/	ling	723	/	inst	725

marine	carpuat

fig	credits:	joakim nivre,	dan	
jurafsky &	james	martin

dependency	parsing

    formalizing	dependency	trees

    transition-based	dependency	parsing

    shift-reduce	parsing
    transition	system
    oracle
    learning/predicting	parsing	actions

data-driven	dependency	parsing

goal: learn	a	good	predictor	of	dependency	graphs

input:	sentence	
output:	dependency	graph/tree	g	=	(v,a)

can	be	framed	as	a	structured	prediction	task

- very	large	output	space
- with	interdependent	labels	

2	dominant	approaches:	transition-based	parsing	and	graph-based	
parsing

transition-based	dependency	parsing

    builds	on	shift-reduce	parsing

[aho &	ullman,	1927]

    configuration

    stack
    input	buffer of	words
    set	of	dependency	relations

    goal	of	parsing

    find	a	final	configuration	where
    all	words	accounted	for
    relations	form	dependency	tree

transition	operators

    transitions:	produce	a	new	
configuration	given	current	
configuration

    parsing	is	the	task	of	

    finding	a	sequence	of	transitions
    that	leads	from	start	state	to	

desired	goal	state

    start	state

    stack	initialized	with	root	node
    input	buffer	initialized	with	words	

in	sentence

    dependency	relation	set	=	empty

    end	state

    stack	and	word	lists	are	empty
    set	of	dependency	relations	=	final	

parse

arc	standard	transition	system

    defines	3	transition	operators	[covington,	2001;	nivre 2003]	
    left-arc:

    create	head-dependent	rel.	between	word	at	top	of	stack	and	2nd word	

(under	top)

    remove	2nd word	from	stack

    create	head-dependent	rel.	between	word	on	2nd word	on	stack	and	word	on	

    right-arc:

top

    remove	word	at	top	of	stack	

    shift

    remove	word	at	head	of	input	buffer
    push	it	on	the	stack

arc	standard	transition	systems

    preconditions

    root	cannot	have	incoming	arcs
    left-arc	cannot	be	applied	when	root	is	the	2nd element	in	stack
    left-arc	and	right-arc	require	2	elements	in	stack	to	be	applied

transition-based	dependency	parser

    assume	an	oracle	

    parsing	complexity
    linear	in	sentence	

length!

    greedy	algorithm

    unlike	viterbi	for	pos	

tagging

transition-based	parsing	illustrated

where	to	we	get	an	oracle?

    multiclass	classification	problem

    input:	current	parsing	state	(e.g.,	current	and	previous	configurations)
    output:	one	transition	among	all	possible	transitions	
    q:	size	of	output	space?

    supervised	classifiers	can	be	used

    e.g.,	id88
    open	questions

    what	are	good	features	for	this	task?
    where	do	we	get	training	examples?

generating	training	examples

    what	we	have	in	a	treebank

    what	we	need	to	train	an	oracle

    pairs	of	configurations	and	

predicted	parsing	action

generating	training	examples

    approach:	simulate	parsing	to	generate	reference	tree

    given

    a	current	config with	stack	s,	dependency	relations	rc
    a	reference	parse	(v,rp)

    do

let   s	try	it	out

features

    configuration	consist	of	stack,	buffer,	current	set	of	relations

    typical	features

    features	focus	on	top	level	of	stack
    use	word	forms,	pos,	and	their	location	in	stack	and	buffer

features	example

    given	configuration

    example	of	useful	features

features	example

research	highlight:	
dependency	parsing	with	stack-lstms
    from	dyer	et	al.	2015:	http://www.aclweb.org/anthology/p15-1033

    idea

    instead	of	hand-crafted	feature
    predict	next	transition	using	recurrent	neural	networks	to	learn	

representation	of	stack,	buffer,	sequence	of	transitions

research	highlight:	
dependency	parsing	with	stack-lstms

research	highlight:	
dependency	parsing	with	stack-lstms

alternate	transition	systems

note:	a	different	way	of	writing	arc-standard	

transition	system

a	weakness	of		arc-standard	parsing

right	dependents	cannot	be	attached	to	their	head	
until	all	their	dependents	have	been	attached

arc	eager	parsing
    left-arc:

    right-arc:

    create	head-dependent	rel.	between	word	at	front	of	buffer	and		word	at	top	of	
    pop	the	stack

stack

    create	head-dependent	rel.	between	word	on	top	of	stack	and	word	at	front	of	
    shift	buffer	head	to	stack

buffer

    shift

    remove	word	at	head	of	input	buffer
    push	it	on	the	stack

    reduce

    pop	the	stack

arc	eager	parsing	example

trees	&	forests

    a	dependency	forest	(here)	is	a	dependency	graph	satisfying

    root
    single-head
    acyclicity
    but	not connectedness

properties	of	this	transition-based
parsing	algorithm

- correctness

- for	every	complete	transition	sequence,	the	resulting	graph	is	a	projective	

- for	every	projective	dependency	forest	g,	there	is	a	transition	sequence	that	

dependency	forest	(soundness)

generates	g	(completeness)

- trick:	forest	can	be	turned	into	tree	by	adding	links	to	root0

dealing	with	
non-projectivity

projectivity
    arc from	head	to	dependent	is	projective

    if	there	is	a	path	from	head	to	every	word	between	head	and	
dependent

    dependency	tree is	projective

    if	all	arcs	are	projective
    or	equivalently,	if	it	can	be	drawn	with	no	crossing	edges

    projective	trees	make	computation	easier
    but	most	theoretical	frameworks	do	not	assume	projectivity

    need	to	capture	long-distance	dependencies,	free	word	order

arc-standard	parsing	can   t	produce	non-
projective	trees

how	frequent	are	non-projective	structures?

    statistics	from	conll shared	task
    npd	=	non	projective	dependencies
    nps	=	non	projective	sentences

how	to	deal	with	non-projectivity?
(1)	change	the	transition	system

    add	new	transitions

    that	apply	to	2nd word	of	the	stack
    top	word	of	stack	is	treated	as	context

[attardi 2006]

how	to	deal	with	non-projectivity?
(2)	pseudo-projective	parsing

solution:	
       projectivize   	a	non-projective	tree	by	creating	
new	projective	arcs	
    that	can	be	transformed	back	into	non-projective	
arcs	in	a	post-processing	step

how	to	deal	with	non-projectivity?
(2)	pseudo-projective	parsing

solution:	
       projectivize   	a	non-projective	tree	by	creating	
new	projective	arcs	
    that	can	be	transformed	back	into	non-projective	
arcs	in	a	post-processing	step

graph-based	parsing

graph	concepts	refresher

directed	spanning	trees

maximum	spanning	tree

    assume	we	have	an	arc	factored	model

i.e.	weight	of	graph	can	be	factored	as	sum	or	product	of	weights	of	its	arcs

    chu-liu-edmonds	algorithm	can	find	the	maximum	spanning	tree	for	
us!
    greedy	recursive	algorithm
    na  ve	implementation:	o(n^3)

chu-liu-edmonds	illustrated

chu-liu-edmonds	illustrated

chu-liu-edmonds	illustrated

chu-liu-edmonds	illustrated

chu-liu-edmonds	illustrated

arc	weights	as	linear	classifiers

example	of	classifier	features

how	to	score	a	graph	g
using	features?

arc-factored	model	

assumption

by	definition	of		arc	weights	

as	linear	classifiers

how	can	we	learn	
the	classifier	from	data?

dependency	parsing:	what	you	should	know
    formalizing	dependency	trees

    transition-based	dependency	parsing
    shift-reduce	parsing
    transition	system:	arc	standard,	arc	eager
    oracle
    learning/predicting	parsing	actions

    graph-based	dependency	parsing
    a	flexible	framework	that	allows	many	extensions

    id56s	vs	feature	engineering,	non-projectivity

extension:	dynamic	oracle

problem	with	standard	classifier-based	oracle:
- it	is	   static   

- ie tied	to	optimal	config sequence	that	produces	gold	tree

- what	if	there	are	multiple	sequences	for	a	single	gold	tree?
- how	can	we	recover	if	the	parser	deviates	from	gold	sequence?

one	solution:	   dynamic	oracle   	[goldberg	&	nivre 2012]

see	also	locally	optimal	learning	to	search	[chang	et	al.	icml	2015]

extension:	dynamic	oracle

problem	with	standard

see	[goldberg	&	nivre 2012]	for	details

