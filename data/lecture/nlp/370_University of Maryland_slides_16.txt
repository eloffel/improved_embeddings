sequence to sequence models
for machine translation

cmsc 723 / ling 723 / inst 725

marine carpuat

slides & figure credits: graham 
neubig

machine translation

    translation system

    input: source sentence f
    output: target sentence e
    can be viewed as a function 

    3 problems

    modeling

    how to define p(.)?

    id151 systems

    training/learning

    how to estimate parameters from 

parallel corpora?

    search

    how to solve argmax efficiently?

introduction to id4

    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    practical tricks

    sequence to sequence models for other nlp tasks

a feedforward neural 3-gram model

a recurrent language model

a recurrent language model

examples of id56 variants

    lstms

    aim to address vanishing/exploding gradient issue

    stacked id56s

       

training in practice: online

training in practice: batch

training in practice: minibatch

    compromise between online and batch

    computational advantages

    can leverage vector processing instructions in modern hardware
    by processing multiple examples simultaneously 

problem with minibatches: in id38, 
examples don   t have the same length

    3 tricks

    padding

    add </s> symbol to make all 

sentences same length

    masking

    multiply id168 calculated 

over padded symbols by zero

    + sort sentences by length

introduction to id4

    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    training tricks

    sequence to sequence models for other nlp tasks

encoder-decoder model

encoder-decoder model

generating output

    we have a model p(e|f), how can we generate translations?

    2 methods

    sampling: generate a random sentence according to id203 distribution

    argmax: generate sentence with highest id203

ancestral sampling

    randomly generate words one 

by one

    until end of sentence symbol

    done!

greedy search

    one by one, pick single highest 

id203 word

    problems

    often generates easy words first
    often prefers multiple common 

words to rare words

greedy search

example

id125

example with beam size b = 2

we consider b top hypotheses at each time 
step

introduction to id4

    neural language models review

    sequence to sequence models for mt

    encoder-decoder
    sampling and search (greedy vs id125)
    practical tricks

    sequence to sequence models for other nlp tasks

