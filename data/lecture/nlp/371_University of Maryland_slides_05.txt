logistic	regression
&	neural	networks
cmsc	723	/	ling	723	/	inst	725

marine	carpuat

slides	credit:	graham	neubig,	jacob	
eisenstein	

logistic	regression

id88	&	probabilities

    what	if	we	want	a	id203	p(y|x)?

    the	id88	gives	us	a	prediction	y

    let   s	illustrate	this	with	binary	classification

illustrations:	graham	neubig

the	logistic	function

       softer   	function	than	in	id88
    can	account	for	uncertainty
    differentiable

logistic	regression:	how	to	train?

    " given	examples	    "

    train	based	on	conditional	likelihood
    find	parameters	w that	maximize	conditional	likelihood	of	all	answers	

stochastic	gradient	ascent	
(or	descent)
    online	training	algorithm	for	logistic	regression

    and	other	probabilistic	models

    update	weights	for	every	training	example
    move	in	direction	given	by	gradient
   

size	of	update	step	scaled	by	learning	rate	

gradient	of	the	logistic	function

example:	person/not-person	classification	problem

given	an	introductory	sentence	in	wikipedia
predict	whether	the	article	is	about	a	person

example:	initial	update

example:	second	update

how	to	set	the	learning	rate?

    various	strategies
    decay	over	time

    = 1    +    

parameter

number	of	
samples

    use	held-out	test	set,	increase	learning	rate	when	likelihood	increases

multiclass	version

some	models	are	
better	then	others   
    consider	these	2	examples

    which	of	the	2	models	below	is	better?

classifier	2	will	probably	
generalize	better!
it	does	not	include	irrelevant	
information
=>	smaller	model	is	better

id173

     +
     ,

    a	penalty	on	adding	extra	weights

    l2	id173:	

    big	penalty	on	large	weights
    small	penalty	on	small	weights

    l1	id173:

    uniform	increase	when	large	or	small
    will	cause	many	weights	to	become	zero

l1	id173	in	online	learning

what	you	should	know

    standard	supervised	learning	set-up	for	text	classification

    difference	between	train	vs.	test	data
    how	to	evaluate

    3	examples	of	supervised	linear	classifiers
    na  ve	bayes,	id88,	logistic	regression
    learning	as	optimization:	what	is	the	objective	function	optimized?
    difference	between	generative	vs.	discriminative	classifiers
    smoothing,	id173
    overfitting,	underfitting

neural	networks

person/not-person	classification	problem

given	an	introductory	sentence	in	wikipedia
predict	whether	the	article	is	about	a	person

formalizing	binary	prediction

the	id88:
a	   machine   	to	calculate	a	weighted	sum

     a   
= 1
     site   
= 1
     located    = 1
     maizuru   = 1
     ,   
= 2
= 1
     in   
     kyoto    = 1
     priest    = 0
     black    = 0

0
-3
0
0
0
0
0
2
0

sign -     "
."/,

     "    

-1

the	id88:	
geometric	interpretation

x

o

x

o

x

o

the	id88:
geometric	interpretation

x

o

x

o

x

o

limitation	of	id88

    can	only	find	linear	separations between	positive	and	
negative	examples

x

o

o

x

neural	networks

    connect	together	multiple	id88s

= 1
     a   
     site   
= 1
     located    = 1
     maizuru   = 1
     ,   
= 2
= 1
     in   
     kyoto    = 1
     priest    = 0
     black    = 0

-1

    motivation:	can	represent	non-linear	functions!

neural	networks:	key	terms

     a   
= 1
     site   
= 1
     located    = 1
     maizuru   = 1
     ,   
= 2
     in   
= 1
     kyoto    = 1
     priest    = 0
     black    = 0

    input	(aka	features)
    output	
    nodes
    layers
    hidden	layers
    activation	function	

(non-linear)

-1

    multi-layer	id88

example

    create	two	classifiers

  0(x1) = {-1, 1}

  0[1]

x

o

  0(x3) = {-1, -1}

  0(x2) = {1, 1}
o

  0[0]

x
  0(x4) = {1, -1}

  0[0]
  0[1]
1

  0[0]
  0[1]
1

w0,0
1
1

-1
b0,0
w0,1
-1
-1

-1
b0,1

sign

  1[0]

sign

  1[1]

example

    these	classifiers	map	to	a	new	space

  0(x1) = {-1, 1}
  2

x

  0(x2) = {1, 1}
o

  1(x3) = {-1, 1}

o

  1[1]

  1

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

  1[0]

o

  1(x2) = {1, -1}

x

  1(x1) = {-1, -1}
  1(x4) = {-1, -1}

  1[0]

  1[1]

1
1
-1

-1
-1
-1

example
    in	new	space,	the	examples	are	linearly	separable!

  0(x1) = {-1, 1}
  0[1]

x

  0(x2) = {1, 1}
o

  0[0]

o

  0(x3) = {-1, -1}

x
  0(x4) = {1, -1}

1
1
1

  2[0] = y

1
1
-1

-1
-1
-1

  1[0]

  1[1]

  1(x3) = {-1, 1}

o

  1[1]

  1(x1) = {-1, -1} x
  1(x4) = {-1, -1}

  1[0]

o   1(x2) = {1, -1}

example	wrap-up:
forward	propagation

    the	final	net

  0[0]
  0[1]
1

  0[0]
  0[1]
1

1
1

-1

-1
-1

-1

tanh

  1[0]

1

tanh

  2[0]

1

tanh

  1[1]

1 1

softmax function	
for	multiclass	classification

    sigmoid function for multiple classes

                =            67,9
               67,9;
 9;
    =exp             ,    
    =    -       
g
 e         

    can be expressed using matrix/vector ops

current class

sum of other classes

30

stochastic	gradient	descent

online	training	algorithm	for	probabilistic	models

w	=	0
for i iterations

for	each	labeled	pair	x,	y in	the	data

w +=	   *	dp(y|x)/dw

in	other	words
    for every training example, calculate the gradient
(the direction that will increase the id203 of y)
    move in that direction, multiplied by learning rate   

gradient	of	the	sigmoid	function

take	the	derivative	of	the	id203

=      

                    =1        =                         67
1+           67
           67
1+           67 +
                    =   1        =             1               67
1+           67
           67
1+           67 +

=         

learning:	we	don't	know	the	derivative	for	
hidden	units!

for	nns,	only	know	correct	tag	for	last	layer

        

      

w1

w2

w3

=        

                   7
1+                   7 +

            =1       
=?
            
            =1       
            
            =1       
=?
            
            =1       
=?
            

y=1

w4

answer:	back-propagation
                    
       ,            ,                
calculate	derivative	with	chain	rule
    ,,r

            =1       
=            =1       
            
                    
                   7
1+                   7 +

error of

next unit (  4)

weight gradient of

            =1       
        

this unit

=       "                 -  u
 u     ",u

in general
calculate i based
on next units j:

id26	

gradient	descent

=	

+	

chain	rule

feed	forward	neural	nets

all	connections	point	forward

      

y

it	is	a	directed	acyclic	graph	(dag)

neural	networks

    non-linear	classification

    prediction:	forward	propagation

    vector/matrix	operations	+	non-linearities

    training:	id26	+	stochastic	gradient	descent

for	more	details,	see	ciml	chap	7

