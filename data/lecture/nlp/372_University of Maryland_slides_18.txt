alignment in
machine translation

cmsc 723 / ling 723 / inst 725

marine carpuat
marine@cs.umd.edu

figures credit: matt post

centauri/arcturan [knight, 1997]

your assignment, translate this to arcturan:  farok crrrok hihok yorok clok kantok ok-yurp

1a. ok-voon ororok sprok .

1b. at-voon bichat dat .

2a. ok-drubel ok-voon anok plok
sprok .
2b. at-drubel at-voon pippat rrat dat .

7a. lalok farok ororok lalok sprok izok
enemok .
7b. wat jjat bichat wat dat vat eneat .

8a. lalok brok anok plok nok .
8b. iat lat pippat rrat nnat .

3a. erok sprok izok hihok ghirok .
3b. totat dat arrat vat hilat .

9a. wiwok nok izok kantok ok-yurp .
9b. totat nnat quat oloat at-yurp .

4a. ok-voon anok drok brok jok .
4b. at-voon krat pippat sat lat .

10a. lalok mok nok yorok ghirok clok .
10b. wat nnat gat mat bat hilat .

5a. wiwok farok izok stok .
5b. totat jjat quat cat .

11a. lalok nok crrrok hihok yorok zanzanok .
11b. wat nnat arrat mat zanzanat .

6a. lalok sprok izok jok stok .
6b. wat dat krat quat cat .

12a. lalok rarok nok izok hihok mok .
12b. wat nnat forat arrat vat gat .

centauri/arcturan [knight, 1997]

your assignment, put these words in order:    { jjat, arrat, mat, bat, oloat, at-yurp }

1a. ok-voon ororok sprok .

7a. lalok farok ororok lalok sprok izok enemok .

1b. at-voon bichat dat .

7b. wat jjat bichat wat dat vat eneat .

2a. ok-drubel ok-voon anok plok sprok .
2b. at-drubel at-voon pippat rrat dat .

8a. lalok brok anok plok nok .

8b. iat lat pippat rrat nnat .

3a. erok sprok izok hihok ghirok .

3b. totat dat arrat vat hilat .
4a. ok-voon anok drok brok jok .
4b. at-voon krat pippat sat lat .

5a. wiwok farok izok stok .

9a. wiwok nok izok kantok ok-yurp .
9b. totat nnat quat oloat at-yurp .

10a. lalok mok nok yorok ghirok clok .

10b. wat nnat gat mat bat hilat .
11a. lalok nok crrrok hihok yorok zanzanok .

5b. totat jjat quat cat .
6a. lalok sprok izok jok stok .

11b. wat nnat arrat mat zanzanat .
12a. lalok rarok nok izok hihok mok .

6b. wat dat krat quat cat .

12b. wat nnat forat arrat vat gat .

centauri/arcturian was actually spanish/english   

translate:  clients do not sell pharmaceuticals in europe.

1a. garcia and associates .
1b. garcia y asociados .

7a. the clients and the associates are enemies .
7b. los clients y los asociados son enemigos .

2a. carlos garcia has three associates .
2b. carlos garcia tiene tres asociados .

8a. the company has three groups .
8b. la empresa tiene tres grupos .

3a. his associates are not strong .
3b. sus asociados no son fuertes .

9a. its groups are in europe .
9b. sus grupos estan en europa .

4a. garcia has a company also .
4b. garcia tambien tiene una empresa .

10a. the modern groups sell strong pharmaceuticals
10b. los grupos modernos venden medicinas fuertes

5a. its clients are angry .
5b. sus clientes estan enfadados .

11a. the groups do not sell zenzanine .
11b. los grupos no venden zanzanina .

6a. the associates are also angry .
6b. los asociados tambien estan 
enfadados .

12a. the small groups are not modern .
12b. los grupos pequenos no son modernos .

1988

more about the ibm story: 20 years of bitext workshop

id87
for machine translation

    the id87 decomposes machine translation into two 

independent subproblems

    id38
    translation modeling / alignment

word  alignment

how can we model p(f|e)?

    we   ll describe the word alignment models introduced 

in early 90s at ibm

    assumption: each french word f is aligned to exactly 

one english word e

    including null

word alignment 
vector representation 

    alignment vector a = [2,3,4,5,6,6,6]

    length of a = length of sentence f
    ai = j if french position i is aligned to english position j

formalizing the connection between 
word alignments & the translation 
model

    we define a conditional model

    projecting word translations
    through alignment links

how many possible alignments in a?

    how many possible alignments for (f,e) where

    f is french sentence with m words
    e is an english sentence with l words

    for each of m french words, we choose an alignment link among (l+1) 

english words

    answer: (     + 1)    

ibm model 1: generative story

    input

    an english sentence of length l
    a length m

    for each french position      in 1..m

    pick an english source index j

    choose a translation 

ibm model 1: generative story

alignment is based on 

word positions, not 
are uniform

alignment  probabilities 
word identities

    input

    an english sentence of length l
    a length m

    for each french position      in 1..m

    pick an english source index j

    choose a translation 

words are translated 

independently

ibm model 1: parameters

    t(f|e) 

    word translation id203 table
    for all words in french & english 

vocab

ibm model 1: generative story

    input

    an english sentence of length l
    a length m

    for each french position      in 1..m

    pick an english source index j

    choose a translation 

ibm model 1: example

    alignment vector a = [2,3,4,5,6,6,6]
    p(f,a|e)?

improving on ibm model 1: 
ibm model 2 
    input

    an english sentence of length l
    a length m

    for each french position      in 1..m

    pick an english source index j

    choose a translation 

remove 

assumption that  q 

is uniform

ibm model 2: parameters

    q(j|i,l,m) 

    now a table
    not uniform as in ibm1

    how many parameters are 

there?

2 remaining tasks

id136
    given

parameter estimation
    given

    a sentence pair (e,f)
    an alignment model with 

    training data (lots of sentence 

pairs)

parameters t(f|e) and q(j|i,l,m)

    a model definition

    what is the most probable 

    how do we learn the parameters 

alignment a?

t(f|e) and q(j|i,l,m)?

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes p(f,a|e)?

    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes p(e,a|f)?

    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes p(e,a|f)?

    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes p(e,a|f)?

    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes p(e,a|f)?

    hint: recall independence assumptions!

id136

    inputs

    model parameter tables for t and q
    a sentence pair

    how do we find the alignment a that maximizes p(e,a|f)?

    hint: recall independence assumptions!

2 remaining tasks

id136
    given

parameter estimation
    given

    a sentence pair (e,f)
    an alignment model with 

    training data (lots of sentence 

pairs)

parameters t(f|e) and q(j|i,l,m)

    a model definition

    what is the most probable 

    how do we learn the parameters 

alignment a?

t(f|e) and q(j|i,l,m)?

parameter estimation (warm-up)

    inputs

    model definition ( t and q )
    a corpus of sentence pairs, with word alignment

    how do we build tables for t and q?

    use counts, just like for id165 models!

parameter estimation: hard em

parameter estimation

    problem

    parallel corpus gives us (e,f) pairs only, a is hidden

    we know how to

    estimate t and q, given (e,a,f)
    compute p(f,a|e), given t and q

    solution: expectation-maximization algorithm (em)

    e-step: given hidden variable, estimate parameters
    m-step: given parameters, update hidden variable

parameter estimation: em

use    soft    values 
instead of binary 

counts

parameter estimation: soft em

    soft em considers all possible alignment links
    each alignment link now has a weight

em for ibm model 1

    expectation (e)-step:

    compute expected counts for parameters (t) based on summing over hidden 

variable

    maximization (m)-step:

    compute the maximum likelihood estimate of t from the expected counts

em example: initialization

in this example:
source language f = spanish
target language e = english

green house

the house

casa verde

la casa

em example: e-step
(a) compute id203 of each alignment p(a,f|e)

note: we   re making simplification assumptions 
in this example
    no null word
    we only consider alignments were each 

french and english word is aligned to 
something

    we ignore q!

em example: e-step
(b) normalize to get p(a|f,e)

em example: e-step
(c) compute expected counts

em example: m-step
(d) normalize expected counts

em example: next iteration

parameter estimation with em

    em guarantees that data likelihood does not decrease across 

iterations

    em can get stuck in a local optimum

    initialization matters

em for ibm 1 in practice

    the previous example illustrates the em algorithm

    but it is a little na  ve

    we had to enumerate all possible alignments
    in practice, we don   t need to sum overall all possible alignments explicitly for 

ibm1

http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf

word alignment with 
ibm models 1, 2

    probabilistic models with strong independence assumptions

    results in linguistically na  ve models

    asymmetric, 1-to-many alignments

    but allows efficient parameter estimation and id136

    alignments are hidden variables 

    unlike words which are observed
    require unsupervised learning (em algorithm)

