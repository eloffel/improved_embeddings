linear	models	continued:
id88	&	logistic	regression
cmsc	723	/	ling	723	/	inst	725

marine	carpuat

slides	credit:	graham	neubig,	jacob	
eisenstein	

linear	models
for	classification

feature	
function	

representation

weights

na  ve	bayes	recap

the	id88

the	id88

    a	linear	model	for	classification

    an	algorithm	to	learn	feature	weights	given	labeled	data

    online	algorithm
    error-driven

multiclass	id88

understanding	the	id88

    what   s	the	impact	of	the	update	rule	on	parameters?

    the	id88	algorithm	will	converge	if	the	training	data	is	linearly	
separable

    proof:	see	   a	course	in	machine	learning   	ch.4

    practical	issues

    how	to	initalize?
    when	to	stop?
    how	to	order	training	examples?

when	to	stop?

    one	technique

    when	the	accuracy	on	held	out	data	starts	to	decrease
    early	stopping

requires	splitting	data	into	3	sets:	

training/development/test

ml	fundamentals	aside:	
overfitting/underfitting/generalization

training	error	is	not	sufficient

    we	care	about	generalization to	new	examples

    a	classifier	can	classify	training	data	perfectly,	yet	classify	new	
examples	incorrectly	
    because	training	examples	are	only	a	sample	of	data	distribution

    a	feature	might	correlate	with	class	by	coincidence

    because	training	examples	could	be	noisy

    e.g.,	accident	in	labeling

overfitting

    consider	a	model	     and	its:
    error	rate	over	training	data	                    %&   ()(    )
    true	error	rate	over	all	data	                    %&,-    
    we	say	    overfits the	training	data	if
                    %&   ()     <                    %&,-    

evaluating	on	test	data

    problem:	we	don   t	know	                    %&,-     !
    after	learning	a	classifier	    ,	we	calculate
                    %-0%    

    we	don   t	look	at	them	during	training!

    some	examples	that	will	be	used	for	evaluation

    we	set	aside	a	test	set

    solution:

overfitting

    another	way	of	putting	it

    a	classifier	     is	said	to	overfit the	training	data,	if	there	is	another	
hypothesis	       ,	such	that
        has	a	smaller	error	than	        on	the	training	data
    but	     has	larger	error	on	the	test	data	than	       .

underfitting/overfitting

    underfitting

didn   t

    learning	algorithm	had	the	opportunity	to	learn	more	from	training	data,	but	

    overfitting

    learning	algorithm	paid	too	much	attention	to	idiosyncracies of	the	training	

data;	the	resulting	classifier	doesn   t	generalize

back	to	the	id88

averaged	id88	improves	generalization

what	objective/loss	does	the	id88	
optimize?

    zero-one	loss	function

    what	are	the	pros	and	cons	compared	to	na  ve	bayes	loss?

logistic	regression

id88	&	probabilities

    what	if	we	want	a	id203	p(y|x)?

    the	id88	gives	us	a	prediction	y

    let   s	illustrate	this	with	binary	classification

illustrations:	graham	neubig

the	logistic	function

       softer   	function	than	in	id88
    can	account	for	uncertainty
    differentiable

logistic	regression:	how	to	train?

    ( given	examples	    (

    train	based	on	conditional	likelihood
    find	parameters	w that	maximize	conditional	likelihood	of	all	answers	

stochastic	gradient	ascent	
(or	descent)
    online	training	algorithm	for	logistic	regression

    and	other	probabilistic	models

    update	weights	for	every	training	example
    move	in	direction	given	by	gradient
   

size	of	update	step	scaled	by	learning	rate	

what	you	should	know

    standard	supervised	learning	set-up	for	text	classification

    difference	between	train	vs.	test	data
    how	to	evaluate

    3	examples	of	supervised	linear	classifiers
    na  ve	bayes,	id88,	logistic	regression
    learning	as	optimization:	what	is	the	objective	function	optimized?
    difference	between	generative	vs.	discriminative	classifiers
    smoothing,	id173
    overfitting,	underfitting

an	online learning	algorithm

id88	weight	update

    if	y	=	1,	increase	the	weights	for	features	in	

    if	y	=	-1,	decrease	the	weights	for	features	in	

