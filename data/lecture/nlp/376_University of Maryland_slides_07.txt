language	
modeling

introduction	to	id165s

dan	jurafsky

probabilistic	language	models

    today   s	goal:	assign	a	id203	to	a	sentence

why?

    machine	translation:

    p(high	winds	tonite)	>	p(large winds	tonite)

    spell	correction

    the	office	is	about	fifteen	minuets from	my	house

    p(about	fifteen	minutes from)	>	p(about	fifteen	minuets from)

    speech	recognition

    p(i	saw	a	van)	>>	p(eyes	awe	of	an)

    +	summarization,	question-answering,	etc.,	etc.!!

dan	jurafsky

probabilistic	language	modeling

    goal:	compute	the	id203	of	a	sentence	or	

sequence	of	words:

p(w)	=	p(w1,w2,w3,w4,w5   wn)

    related	task:	id203	of	an	upcoming	word:
    a	model	that	computes	either	of	these:

p(w5|w1,w2,w3,w4)

p(w)					or					p(wn|w1,w2   wn-1)									 is	called	a	language	model.
    better:	the	grammar							but	language	model	or	lm	is	standard

dan	jurafsky

how	to	compute	p(w)
    how	to	compute	this	joint	id203:

    p(its,	water,	is,	so,	transparent,	that)

   

intuition:	let   s	rely	on	the	chain	rule	of	id203

dan	jurafsky

reminder:	the	chain	rule

    recall	the	definition	of	conditional	probabilities
rewriting:			p(a,b)	=	p(a)p(b|a)

p(b|a)	=	p(a,b)/p(a)

    more	variables:

p(a,b,c,d)	=	p(a)p(b|a)p(c|a,b)p(d|a,b,c)

    the	chain	rule	in	general
p(x1,x2,x3,   ,xn)	=	p(x1)p(x2|x1)p(x3|x1,x2)   p(xn|x1,   ,xn-1)

dan	jurafsky the	chain	rule	applied	to	compute	

joint	id203	of	words	in	sentence

p(w1w2   wn) =
  

   
i

p(wi | w1w2   wi   1)

    

p(   its	water	is	so	transparent   )	=

p(its)	   p(water|its)	   p(is|its water)	

   p(so|its water	is)	   p(transparent|its water	is	

so)

dan	jurafsky

how	to	estimate	these	probabilities

    could	we	just	count	and	divide?

p(the |its water is so transparent that) =
count(its water is so transparent that the)
count(its water is so transparent that)

    no!		too	many	possible	sentences!
    we   ll	never	see	enough	data	for	estimating	these

dan	jurafsky

markov	assumption
    simplifying	assumption:
p(the |its water is so transparent that)     p(the |that)
    or	maybe
p(the |its water is so transparent that)     p(the |transparent that)

andrei	markov

dan	jurafsky

markov	assumption

p(w1w2   wn)    
  

   
i

p(wi | wi   k   wi   1)

    in	other	words,	we	approximate	each	
component	in	the	product
p(wi | w1w2   wi   1)     p(wi | wi   k   wi   1)
  

dan	jurafsky

simplest	case:	unigram	model
p(wi)
p(w1w2   wn)    
  

   
i

some	automatically	generated	sentences	from	a	unigram	model

fifth, an, of, futures, the, an, incorporated, a, 
a, the, inflation, most, dollars, quarter, in, is, 
mass

thrift, did, eighty, said, hard, 'm, july, bullish

    

that, or, limited, the

dan	jurafsky

bigram	model
condition	on	the	previous	word:

p(wi | w1w2   wi   1)     p(wi | wi   1)
  

texaco, rose, one, in, this, issue, is, pursuing, growth, in, 
a, boiler, house, said, mr., gurria, mexico, 's, motion, 
control, proposal, without, permission, from, five, hundred, 
fifty, five, yen

outside, new, car, parking, lot, of, the, agreement, reached

this, would, be, a, record, november

dan	jurafsky

id165	models

    we	can	extend	to	trigrams,	4-grams,	5-grams
    in	general	this	is	an	insufficient	model	of	language

    because	language	has	long-distance	dependencies:
   the	computer(s)	which	i	had	just	put	into	the	machine	room	
on	the	fifth	floor	is	(are)	crashing.   

    but	we	can	often	get	away	with	id165	models

language	
modeling

introduction	to	id165s

language	
modeling

estimating	id165	

probabilities

dan	jurafsky

estimating	bigram	probabilities

    the	maximum	likelihood	estimate

p(wi | wi   1) =

p(wi | wi   1) =

count(wi   1,wi)
count(wi   1)
c(wi   1,wi)
c(wi   1)

    

dan	jurafsky

an	example

p(wi | wi   1) =

c(wi   1,wi)
c(wi   1)

<s>	i	am	sam	</s>
<s>	sam	i	am	</s>
<s>	i	do	not	like	green	eggs	and	ham	</s>

dan	jurafsky more	examples:	

berkeley	restaurant	project	sentences

tell	me	about	chez	panisse

    can	you	tell	me	about	any	good	cantonese restaurants	close	by
    mid	priced	thai food	is	what	i   m looking	for
   
    can	you	give	me	a	listing	of	the	kinds	of	food	that	are	available
   
i   m looking	for	a	good	place	to	eat	breakfast
    when	is	caffe venezia open	during	the	day

dan	jurafsky

raw	bigram	counts

    out	of	9222	sentences

dan	jurafsky

raw	bigram	probabilities

    normalize	by	unigrams:

    result:

dan	jurafsky

bigram	estimates	of	sentence	probabilities

p(<s>	i	want	english food	</s>)	=

p(i|<s>)			
   p(want|i)		
   p(english|want)			
   p(food|english)			
   p(</s>|food)
=		.000031

dan	jurafsky

what	kinds	of	knowledge?

    p(english|want)		=	.0011
    p(chinese|want)	=		.0065
    p(to|want)	=	.66
    p(eat	|	to)	=	.28
    p(food	|	to)	=	0
    p(want	|	spend)	=	0
    p	(i |	<s>)	=	.25

dan	jurafsky

practical	issues

    we	do	everything	in	log	space

    avoid	underflow
    (also	adding	is	faster	than	multiplying)

log(p1    p2    p3    p4) = log p1 +log p2 +log p3 +log p4

dan	jurafsky

google	id165	release,	august	2006

   

dan	jurafsky

google	id165	release

    serve as the incoming 92
    serve as the incubator 99
    serve as the independent 794
    serve as the index 223
    serve as the indication 72
    serve as the indicator 120
    serve as the indicators 45
    serve as the indispensable 111
    serve as the indispensible 40
    serve as the individual 234

http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-you.html

dan	jurafsky

google	book	id165s
    https://books.google.com/ngrams

language	
modeling

estimating	id165	

probabilities

language	
modeling

evaluation	and	

perplexity

dan	jurafsky

evaluation:	how	good	is	our	model?

    does	our	language	model	prefer	good	sentences	to	bad	ones?
    assign	higher	id203	to	   real   	or	   frequently	observed   	sentences	

    than	   ungrammatical   	or	   rarely	observed   	sentences?
    we	train	parameters	of	our	model	on	a	training	set.
    we	test	the	model   s	performance	on	data	we	haven   t	seen.
    a	test	set	is	an	unseen	dataset	that	is	different	from	our	training	set,	

totally	unused.

    an	evaluation	metric	tells	us	how	well	our	model	does	on	the	test	set.

dan	jurafsky

training	on	the	test	set

    we	can   t	allow	test	sentences	into	the	training	set
    we	will	assign	it	an	artificially	high	id203	when	we	set	it	in	

the	test	set

       training	on	the	test	set   
    bad	science!
    and	violates	the	honor	code

29

dan	jurafsky

extrinsic	evaluation	of	id165	models

    best	evaluation	for	comparing	models	a	and	b

    put	each	model	in	a	task

    spelling	corrector,	speech	recognizer,	mt	system

    run	the	task,	get	an	accuracy	for	a	and	for	b

    how	many	misspelled	words	corrected	properly
    how	many	words	translated	correctly

    compare	accuracy	for	a	and	b

dan	jurafsky difficulty	of	extrinsic	(in-vivo)	evaluation	

of		id165	models

    extrinsic	evaluation
    so

    time-consuming;	can	take	days	or	weeks

    sometimes	use	intrinsic evaluation:	perplexity
    bad	approximation	

    unless	the	test	data	looks	just like	the	training	data
    so	generally	only	useful	in	pilot	experiments

    but	is	helpful	to	think	about.

dan	jurafsky

intuition	of	perplexity

    the	shannon	game:

    how	well	can	we	predict	the	next	word?
i	always	order	pizza	with	cheese	and	____
the	33rd president	of	the	us	was	____
i	saw	a	____

    unigrams	are	terrible	at	this	game.		(why?)

    a	better	model	of	a	text

mushrooms 0.1
pepperoni 0.1
anchovies 0.01
   .
fried rice 0.0001
   .
and 1e-100

   

is	one	which	assigns	a	higher	id203	to	the	word	that	actually	occurs

dan	jurafsky

perplexity

the	best	language	model	is	one	that	best	predicts	an	unseen	test	set

    gives	the	highest	p(sentence)
perplexity	is	the	inverse	id203	of	
the	test	set,	normalized	by	the	number	
of	words:

pp(w) = p(w1w2...wn )   

1
n

            =

n

1

p(w1w2...wn )

chain	rule:

for	bigrams:

minimizing	perplexity	is	the	same	as	maximizing	id203

dan	jurafsky

perplexity	as	branching	factor

    let   s	suppose	a	sentence	consisting	of	random	digits
    what	is	the	perplexity	of	this	sentence	according	to	a	model	

that	assign	p=1/10	to	each	digit?

dan	jurafsky

lower	perplexity	=	better	model

    training	38	million	words,	test	1.5	million	words,	wsj

id165	
order
perplexity 962

unigram bigram trigram

170

109

language	
modeling

evaluation	and	

perplexity

language	
modeling

generalization	and	

zeros

dan	jurafsky

the	shannon	visualization	method

   

choose	a	random	bigram	
(<s>,	w)	according	to	its	id203
    now	choose	a	random	bigram								
(w,	x)	according	to	its	id203
    and	so	on	until	we	choose	</s>
   
then	string	the	words	together

<s> i

i want

want to

to eat

eat chinese

chinese food

food  </s>

i want to eat chinese food

dan	jurafsky

bigrams by    rst generating a random bigram that starts with <s> (according to its
bigram id203), then choosing a random bigram to follow (again, according to
its bigram id203), and so on.

approximating	shakespeare

to give an intuition for the increasing power of higher-order id165s, fig. 4.3
shows random sentences generated from unigram, bigram, trigram, and 4-gram
models trained on shakespeare   s works.

gram

gram

rote life have
   hill he late speaks; or! a more to leg less    rst you enter

king. follow.
   what means, sir. i confess she? then all sorts, he is trim, captain.

1    to him swallowed confess hear both. which. of save on trail for are ay device and
2    why dost stand forth thy canopy, forsooth; he is this palpable hit the king henry. live
3    fly, and will rid me these news of price. therefore the sadness of parting, as they say,
4    king henry. what! i will go seek the traitor gloucester. exeunt some of the watch. a

   tis done.
   this shall forbid it should be branded, if renown made it empty.

great banquet serv   d in;
   it cannot be but so.

gram

gram

figure 4.3 eight sentences randomly generated from four id165s computed from shakespeare   s works. all
characters were mapped to lower-case and punctuation marks were treated as words. output is hand-corrected
for capitalization to improve readability.

dan	jurafsky

shakespeare	as	corpus
    n=884,647	tokens,	v=29,066
    shakespeare	produced	300,000	bigram	types	
out	of	v2=	844	million	possible	bigrams.
    so	99.96%	of	the	possible	bigrams	were	never	seen	
(have	zero	entries	in	the	table)
    quadrigrams worse:			what's	coming	out	looks	
like	shakespeare	because	it	is shakespeare

dan	jurafsky the	wall	street	journal	is	not	shakespeare	

(no	offense)

4.3

    generalization and zeros

11

gram

were recession exchange new endorsed a acquire to six executives

1 months the my and issue of year foreign new exchange   s september
2 last december through the way to preserve the hudson corporation n.

b. e. c. taylor would seem to complete the major central planners one
point    ve percent of u. s. e. has already old m. x. corporation of living
on information such as more frequently    shing to keep her

gram

3 they also point to ninety nine point six billion dollars from two hundred

four oh six three percent of the rates of interest stores as mexico and
brazil on market conditions

gram

figure 4.4 three sentences randomly generated from three id165 models computed from
40 million words of the wall street journal, lower-casing all characters and treating punctua-
tion as words. output was then hand-corrected for capitalization to improve readability.

dan	jurafsky

the	perils	of	overfitting

    id165s	only	work	well	for	word	prediction	if	the	test	

corpus	looks	like	the	training	corpus
    in	real	life,	it	often	doesn   t
    we	need	to	train	robust	models that	generalize!
    one	kind	of	generalization:	zeros!

    things	that	don   t	ever	occur	in	the	training	set

    but	occur	in	the	test	set

dan	jurafsky

zeros
    training	set:

   	denied	the	allegations
   	denied	the	reports
   	denied	the	claims
   	denied	the	request

    test	set

   	denied	the	offer
   	denied	the	loan

p(   offer   	|	denied	the)	=	0

dan	jurafsky

zero	id203	bigrams

    bigrams	with	zero	id203

    mean	that	we	will	assign	0	id203	to	the	test	set!

    and	hence	we	cannot	compute	perplexity	(can   t	divide	by	0)!

language	
modeling

generalization	and	

zeros

language	
modeling

smoothing:	add-one	
(laplace)	smoothing

dan	jurafsky

the intuition of smoothing (from dan klein)

    when	we	have	sparse	statistics:

   

p(w	|	denied	the)
3	allegations
2	reports
1	claims
1	request
7	total

steal	id203	mass	to	generalize	better

p(w	|	denied	the)
2.5	allegations
1.5	reports
0.5	claims
0.5	request
2	other
7	total

s
n
o
i
t
a
g
e
l
l
a

s
n
s
o
n
i
o
t
i
a
t
g
a
e
g
l
e
l
a
l
l
a

s
t
r
o
p
e
r

s
t
r
o
p
e
r

k
c
a

t
t

a

n
a
m

e
m
o
c
t

u
o

   

s
m
i
a
l
c

t
s
e
u
q
e
r

k
c
a

t
t

a

n
a
m

e
m
o
c
t

u
o

   

s
m
i
a
l
c

t
s
e
u
q
e
r

dan	jurafsky

add-one	estimation
    also	called	laplace	smoothing
    pretend	we	saw	each	word	one	more	time	than	we	did
    just	add	one	to	all	the	counts!

    id113	estimate:

    add-1	estimate:

pid113(wi | wi   1) =

padd   1(wi | wi   1) =

c(wi   1,wi)
c(wi   1)
c(wi   1,wi) +1
c(wi   1) +v

dan	jurafsky

maximum	likelihood	estimates

   

the	maximum	likelihood	estimate
    of	some	parameter	of	a	model	m	from	a	training	set	t
    maximizes	the	likelihood	of	the	training	set	t	given	the	model	m
   
suppose	the	word	   bagel   	occurs	400	times	in	a	corpus	of	a	million	words
    what	is	the	id203	that	a	random	word	from	some	other	text	will	be	

   bagel   ?

    id113	estimate	is	400/1,000,000	=	.0004
    this	may	be	a	bad	estimate	for	some	other	corpus

    but	it	is	the	estimate that	makes	it	most	likely that	   bagel   	will	occur	400	times	in	

a	million	word	corpus.

dan	jurafsky berkeley restaurant corpus: laplace 

smoothed bigram counts

dan	jurafsky

laplace-smoothed bigrams

dan	jurafsky

reconstituted counts

dan	jurafsky

compare with raw bigram counts

dan	jurafsky

add-1	estimation	is	a	blunt	instrument

    so	add-1	isn   t	used	for	id165s:	

    we   ll	see	better	methods

    but	add-1	is	used	to	smooth	other	nlp	models

    for	text	classification	
    in	domains	where	the	number	of	zeros	isn   t	so	huge.

language	
modeling

smoothing:	add-one	
(laplace)	smoothing

language	
modeling

interpolation,	backoff

dan	jurafsky

backoff and interpolation

    sometimes	it	helps	to	use	less context

    condition	on	less	context	for	contexts	you	haven   t	learned	much	about	

    backoff:	

    use	trigram	if	you	have	good	evidence,
    otherwise	bigram,	otherwise	unigram
interpolation:	
    mix	unigram,	bigram,	trigram

interpolation	works	better

   

   

dan	jurafsky

from all the id165 estimators, weighing and combining the trigram, bigram, and
unigram counts.

in simple linear interpolation, we combine different order id165s by linearly

linear	interpolation

interpolating all the models. thus, we estimate the trigram id203 p(wn|wn 2wn 1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
    simple	interpolation
by a l:

  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)

+l2p(wn|wn 1)
+l3p(wn)

such that the ls sum to 1:
  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)
+l2p(wn|wn 1)
+l3p(wn)
    lambdas	conditional	on	context:

xi

li = 1
(4.24)

li = 1

such that the ls sum to 1:

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in

xi

(4.25)

dan	jurafsky

how	to	set	the	lambdas?

    use	a	held-out corpus

training	data

held-out	

data

test	
data

    choose	  s to	maximize	the	id203	of	held-out	data:

    fix	the	id165	probabilities	(on	the	training	data)
    then	search	for	  s that	give	largest	id203	to	held-out	set:

logp(w1...wn | m(  1...  k)) =

logpm (  1...  k )(wi | wi   1)

   
i

dan	jurafsky unknown	words:	open	versus	closed	

vocabulary	tasks

if	we	know	all	the	words	in	advanced
    vocabulary	v	is	fixed
    closed	vocabulary	task
    often	we	don   t	know	this

   

   

    out	of	vocabulary =	oov	words
    open	vocabulary	task
instead:	create	an	unknown	word	token	<unk>
    training	of	<unk>	probabilities
    create	a	fixed	lexicon	l	of	size	v
    at	text	id172	phase,	any	training	word	not	in	l	changed	to		<unk>
    now	we	train	its	probabilities	like	a	normal	word

    at	decoding	time

   

if	text	input:	use	unk	probabilities	for	any	word	not	in	training

dan	jurafsky

id165	smoothing	summary

    add-1	smoothing:

    ok	for	text	categorization,	not	for	language	modeling

    the	most	commonly	used	method:

    extended	interpolated	kneser-ney

61

language	
modeling

interpolation,	backoff

