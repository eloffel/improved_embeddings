natural  language  processing

a  speech  example

language  modeling  i

dan  klein      uc  berkeley

the  noisy   channel  model

    we  want  to  predict  a  sentence  given  acoustics:

    the  noisy   channel  approach:

asr  components

language  model

source
p(w)

best
w

w

decoder

acoustic  model

channel
p(a|w)

a

observed          

a

acoustic  model:  id48s  over  
word  positions  with  mixtures  

of  gaussians  as  emissions

language  model:  distributions  

over  sequences  of  words  

(sentences)

acoustic  confusions

the  station  signs  are  in  deep  in  english
the  stations  signs  are  in  deep  in  english
the  station  signs  are  in  deep  into  english
the  station  's  signs  are  in  deep  in  english
the  station  signs  are  in  deep  in  the  english
the  station  signs  are  indeed  in  english
the  station  's  signs  are  indeed  in  english
the  station  signs  are  indians in  english
the  station  signs  are  indian in  english
the  stations  signs  are  indians in  english
the  stations  signs  are  indians and  english

   14732
   14735
   14739
   14740
   14741
   14757
   14760
   14790
   14799
   14807
   14815

argmax  p(w|a)  =  argmax  p(a|w)p(w)

w

w

    a  language  model  is  a  distribution  over  sequences  of  words  

(sentences)

language models

(cid:1842)(cid:1875) (cid:3404)(cid:1842)(cid:1875)(cid:2869)   (cid:1875)(cid:3041)

    what   s  w?    (closed  vs  open  vocabulary)
    what   s  n?    (must  sum  to  one  over  all  lengths)
    can  have  rich  structure  or  be  linguistically  naive

    why  language  models?

    usually  the  point  is  to  assign  high  weights  to  plausible  sentences  (cf

acoustic  confusions)

    this  is  not  the  same  as  modeling  grammaticality

1

translation:  codebreaking?

mt  system  components

   also knowing nothing official about, but having guessed
and inferred considerable about,
the powerful new
mechanized methods in cryptography   methods which i
believe succeed even when one does not know what
language has been coded   one naturally wonders if the
problem of translation could conceivably be treated as a
problem in cryptography. when i look at an article in
russian, i say:    this is really written in english, but it has
been coded in some strange symbols. i will now proceed to
decode.       

warren  weaver  (1947)

other  noisy  channel  models?

    we   re  not  doing  this  only  for  asr  (and  mt)

    grammar  /  spelling  correction
    handwriting  recognition,  ocr
    document  summarization
    dialog  generation
    linguistic  decipherment
       

n   gram models
    use  chain  rule  to  generate  words  left   to   right

    can   t  condition  on  the  entire  left  context

p(???  |  turn  to  page  134  and  look  at  the  picture  of  the)

    n   gram  models  make  a  markov  assumption

language model

translation model

source
p(e)

best
e

e

channel
p(f|e)

f

decoder

observed          

f

argmax  p(e|f)  =  argmax  p(f|e)p(e)

e

e

n   gram  models

empirical n   grams

    how  do  we  know  p(w  |  history)?

    use  statistics  from  data  (examples  using  google  n   grams)
    e.g.  what  is  p(door  |  the)?

s
t

 

n
u
o
c
g
n
n
a
r
t

i

i

198015222 the first
194623024 the same
168504105 the following
158562063 the world
   
14112454 the door
-----------------
23135851162 the *

    this  is  the  maximum  likelihood  estimate

2

increasing  n   gram order

increasing  n   gram  order

    higher  orders  capture  more  dependencies

bigram model
198015222 the first
194623024 the same
168504105 the following
158562063 the world
   
14112454 the door
-----------------
23135851162 the *

trigram model

197302 close the window 
191125 close the door 
152500 close the gap 
116451 close the thread 
87298 close the deal

-----------------
3785230 close the *

p(door | the) = 0.0006

p(door | close the) = 0.05

sparsity

sparsity

please  close  the  first  door  on  the  left.

3380 please close the door
1601 please close the window
1164 please close the new
1159 please close the gate
   
0 please close the first
-----------------
13951 please close the *

    problems  with  n   gram  models:

    new  words  (open  vocabulary)

    synaptitute
    132,701.03
    multidisciplinarization

    old  words  in  new  contexts

1
0.8
0.6
0.4
0.2
0

n
e
e
s
n
o

 

i
t
c
a
r
f

unigrams

bigrams

0

500000

1000000

number of words

    aside:  zipf   s law

    types  (words)  vs.  tokens  (word  occurences)
    broadly:  most  word  types  are  rare  ones
    specifically:  

    rank  word  types  by  token  frequency
    frequency  inversely  proportional  to  rank

    not  special  to  language:  randomly  generated  character  strings  have  

this  property  (try  it!)

    this  law  qualitatively  (but  rarely  quantitatively)  informs  nlp

smoothing

    we  often  want  to  make  estimates  from  sparse  statistics:

n   gram  estimation

p(w | denied the)
3 allegations
2 reports
1 claims
1 request
7 total

s
n
o
i
t
a
g
e

l
l

a

s
t
r
o
p
e
r

s
e
g
r
a
h
c

n
o

i
t

o
m

s
t
i
f

e
n
e
b

   

t
s
e
u
q
e
r

s
m
a
c

l

i

    smoothing  flattens  spiky  distributions  so  they  generalize  better:

p(w | denied the)
2.5 allegations
1.5 reports
0.5 claims
0.5 request
2 other
7 total

s
n
s
o
n
i
o
t
a
i
t
g
a
e
g
e
a
a

l
l

l
l

s
t
r
o
p
e
r

s
e
g
r
a
h
c

n
o

i
t

o
m

s
t
i
f

e
n
e
b

   

t
s
e
u
q
e
r

s
m
a
c

l

i

    very  important  all  over  nlp,  but  easy  to  do  badly

3

likelihood  and  perplexity

train,  held   out,  test

    how  do  we  measure  lm     goodness   ?
    shannon   s  game:  predict  the  next  word

when  i  eat  pizza,  i  wipe  off  the  _________

    formally:  define  test  set  (log)  likelihood

(cid:3050)   (cid:3025)

log	px(cid:2016) (cid:3404)(cid:3533)log	(cid:4666)(cid:1842)(cid:1875)(cid:2016)(cid:4667)
perpx,(cid:2016) (cid:3404)	exp (cid:3398)log	(cid:1842)(cid:4666)(cid:1850)|(cid:2016)(cid:4667)
|(cid:1850)|

    perplexity:     average  per  word  branching  

factor     (not  per   step)

grease 0.5
sauce 0.4
dust 0.05
   .
mice 0.0001
   .
the     1e-100

3516 wipe off the excess 
1034 wipe off the dust
547 wipe off the sweat
518 wipe off the mouthpiece
   
120 wipe off the grease
0 wipe off the sauce
0 wipe off the mice
-----------------
28048 wipe off the *

    want  to  maximize  likelihood  on  test,  not  training  data

    empirical  n   grams  won   t  generalize  well
    models  derived  from  counts  /  sufficient  statistics  require  

generalization  parameters  to  be  tuned  on  held   out  data  to  simulate  
test  generalization

training data

held-out

data

test
data

counts / parameters from 

here

hyperparameters

from here

evaluate here

    set  hyperparameters to  maximize  the  likelihood  of  the  held   out  data  

(usually  with  grid  search  or  em)

measuring  model  quality  (speech)

    we  really  want  better  asr  (or  whatever),  not  better  perplexities

    for  speech,  we  care  about  word  error  rate  (wer)

correct  answer:

andy  saw  a part of  the  movie

recognizer  output: and he saw  apart of  the  movie

wer:  

insertions +  deletions +  substitutions

true  sentence  size

=            4/7    =    57%

    common  issue:  intrinsic  measures  like  perplexity  are  easier  to  

use,  but  extrinsic  ones  are  more  credible

idea  1:  interpolation

please  close  the  first  door  on  the  left.

4   gram

3380 please close the door
1601 please close the window
1164 please close the new
1159 please close the gate
   
0 
-----------------
13951 please close the *

please close the first

3   gram

197302 close the window 
191125 close the door 
152500 close the gap 
116451 close the thread
   
8662     close the first
-----------------
3785230 close the *

2   gram
198015222 the first
194623024 the same
168504105 the following
158562063 the world
   
   
-----------------
23135851162 the *

0.0

0.002

0.009

specific  but  sparse

dense  but  general

(linear)  interpolation

    simplest  way  to  mix  different  orders:  linear  interpolation

    how  to  choose  lambdas?
    should  lambda  depend  on  the  counts  of  the  histories?

    choosing  weights:  either  grid  search  or  em  using  held   out  

data

    better  methods  have  interpolation  weights  connected  to  
context  counts,  so  you  smooth  more  when  you  know  less

idea  2:  discounting

    observation:  n   grams  occur  more  in  training  data  than  they  

will  later

empirical  bigram  counts  (church  and  gale,  91)

count  in  22m  words

future  c*  (next  22m)

1
2
3
4
5

0.45
1.25
2.24
3.23
4.21

4

absolute discounting

idea  3:  fertility

    absolute  discounting

    reduce  numerator  counts  by  a  constant  d  (e.g.  0.75)
    maybe  have  a  special  discount  for  small  counts
    redistribute  the     shaved     mass  to  a  model  of  new  events

    example  formulation

    shannon  game:     there  was  an  unexpected  _____   

       delay   ?
       francisco   ?

    context  fertility:  number  of  distinct  context  types  

that  a  word  occurs  in
    what  is  the  fertility  of     delay   ?
    what  is  the  fertility  of     francisco   ?
    which  is  more  likely  in  an  arbitrary  new  context?

kneser   ney smoothing
    kneser   ney  smoothing  combines  two  ideas
    discount  and  reallocate  like  absolute  discounting
    in  the  backoff model,  word  probabilities  are  proportional  

to  context  fertility,  not  frequency

(cid:1842)(cid:1875)    |(cid:1875)   :(cid:1855)(cid:1875)   ,(cid:1875) (cid:3408)0|

    theory  and  practice

    practice:  kn  smoothing  has  been  repeatedly  proven  both  

effective  and  efficient

    theory:  kn  smoothing  as  approximate  id136  in  a  

hierarchical  pitman   yor process  [teh,  2006]

(cid:3397)(cid:2009)(cid:1868)(cid:1870)(cid:1857)(cid:1874)	(cid:1863)(cid:3398)1(cid:1842)(cid:3038)(cid:2879)(cid:2869)(cid:4666)(cid:1875)|(cid:1868)(cid:1870)(cid:1857)(cid:1874)	(cid:1863)	(cid:3398)2(cid:4667)

    all  orders  recursively  discount  and  back   off:

kneser   ney  details*
(cid:1842)(cid:3038)(cid:1875)(cid:1868)(cid:1870)(cid:1857)(cid:1874)	(cid:1863)(cid:3398)1 (cid:3404)(cid:1855)   (cid:1875),(cid:1868)(cid:1870)(cid:1857)(cid:1874)	(cid:1863)(cid:3398)1 (cid:3398)(cid:1856)
    (cid:1855)   (cid:4666)(cid:1874),(cid:1868)(cid:1870)(cid:1857)(cid:1874)	(cid:1863)(cid:3398)1(cid:4667)
(cid:3049)
(cid:1855)   (cid:1876) (cid:3404)|(cid:1873):(cid:1855)(cid:1873),(cid:1876) (cid:3408)0|

all  others  it  is  the  context  fertility  of  the  n   gram:

you  can  figure  out  an  expression).

    alpha  is  computed  to  make  the  id203  normalize  (see  if  

    for  the  highest  order,  c     is  the  token  count  of  the  n   gram.    for  

    the  unigram  base  case  does  not  need  to  discount.
    variants  are  possible  (e.g.  different  d  for  low  counts)

idea  4:  big  data

what  actually  works?

there   s  no  data  like  more  data.

   

trigrams  and  beyond:
    unigrams,  bigrams  generally  

useless

    trigrams  much  better
    4   ,  5   grams  and  more  are  

really  useful  in  mt,  but  gains  
are  more  limited  for  speech

    discounting

    absolute  discounting,  good   
turing,  held   out  estimation,  
witten   bell,  etc   

    context  counting

    kneser   ney  construction  of  

lower   order  models

   

see  [chen+goodman]  reading  
for  tons  of  graphs   

[graph from

joshua goodman]

5

data  >>  method?

    having  more  data  is  better   

y
p
o
r
t
n
e

10
9.5
9
8.5
8
7.5
7
6.5
6
5.5

100,000 katz
100,000 kn
1,000,000 katz
1,000,000 kn
10,000,000 katz
10,000,000 kn
all katz
all kn

1

2

3

4

5

6

7

8

9 10 20

id165 order

         but  so  is  using  a  better  estimator
    another  issue:  n  >  3  has  huge  costs  in  speech  recognizers

what about   

tons  of  data?

[brants et  al,  2007]

unknown words?

    what  about  totally  unseen  words?

    most  lm  applications  are  closed  vocabulary

    asr  systems  will  only  propose  words  that  are  in  their  pronunciation  

dictionary

    mt  systems  will  only  propose  words  that  are  in  their  phrase  tables  

(modulo  special  models  for  numbers,  etc)

    in  principle,  one  can  build  open  vocabulary  lms
    e.g.  models  over  character  strings  rather  than  words
    back   off  needs  to  go  down  into  a     generate  new  word     model
    typically  if  you  need  this,  a  high   order  character  model  is  almost  as  

good

grammar?

what  gets  captured?

    the  n   gram  assumption  hurts  one   s  inner  linguist!

    many  linguistic  arguments  that  language  isn   t  regular

    long   distance  effects:     the  computer which  i  had  just  put  into  the  

machine  room  on  the  fifth  floor  ___.   

    recursive  structure

    answers

    n   grams  only  model  local  correlations,  but  they  get  them  all
    as  n  increases,  they  catch  even  more  correlations
    n   gram  models  scale  much  more  easily  than  structured  lms

    not  convinced?

    can  build  lms  out  of  our  grammar  models  (later  in  the  course)
    take  any  generative  model  with  words  at  the  bottom  and  marginalize  

out  the  other  variables

    bigram  model:

    [texaco,  rose,  one,  in,  this,  issue,  is,  pursuing,  growth,  in,  a,  boiler,  

house,  said,  mr.,  gurria,  mexico,  's,  motion,  control,  proposal,  without,  
permission,  from,  five,  hundred,  fifty,  five,  yen]

    [outside,  new,  car,  parking,  lot,  of,  the,  agreement,  reached]
    [this,  would,  be,  a,  record,  november]

    pid18  model:

    [this,  quarter,     s,  surprisingly,  independent,  attack,  paid,  off,  the,  risk,  

involving,  irs,  leaders,  and,  transportation,  prices,  .]

    [it,  could,  be,  announced,  sometime,  .]
    [mr.,  toseland,  believes,  the,  average,  defense,  economy,  is,  drafted,  

from,  slightly,  more,  than,  12,  stocks,  .]

6

scaling  up?

other  techniques?

    there   s  a  lot  of  training  data  out  there   

    lots  of  other  techniques

    maximum  id178  lms  (soon)

    neural  network  lms  (soon)

    syntactic  /  grammar   structured  lms  (much  later)

     next  class  we   ll  talk  about  how  to  make  it  fit.

7

