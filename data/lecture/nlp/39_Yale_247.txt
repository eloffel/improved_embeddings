nlp
introduction to nlp
parsing evaluation
parsing model
gen/eval framework
gen maps the input to a set of candidate parses
eval ranks the candidate parses
y* = argmax eval (x,y)
           
  y     gen(x)
evaluation methodology (1/2)
classification tasks
document retrieval
id52
parsing
data split
training
dev-test
test
evaluation methodology (2/2)
baselines
dumb baseline
intelligent baseline
human performance (ceiling)
new method
evaluation methods
accuracy
precision and recall
multiple references
interjudge agreement
kappa
agreement vs. expected agreement
p(a) is the level of agreement of the judges
p(e) is the expected id203 of agreement by chance
when k > .7     agreement is considered high
question
judge agreement on a binary classification task is 60%, is this high?

answer
data
p(a) = .6
p(e) = .5
kappa
k = .1/.5 = .2
not high

parsing evaluation
parseval: precision and recall
get the proper constituents
labeled precision and recall
also get the correct non-terminal labels
f1
harmonic mean of precision and recall
crossing brackets
(a (b c)) vs ((a b) c)
ptb corpus
training 02-21, development 22, test 23


evaluation example
gold = (s (np (dt the) (jj japanese) (jj industrial) (nns companies))
          (vp (md should) (vp (vb know) (advp (jjr better)))) (. .)
bracketing recall         =  80.00
bracketing precision      =  66.67
bracketing fmeasure       =  72.73
complete match            =   0.00
no crossing               = 100.00
tagging accuracy          =  87.50
char = (s (np (dt the) (jj japanese) (jj industrial) (nns companies)) 
           (vp (md should) (vp (vb know)) ((advp (rbr better)))) (. .))
nlp
