natural  language  processing

classification  i
dan  klein      uc  berkeley

1

classification

2

classification
    automatically  make  a  decision  about  inputs

    example:  document      category
    example:  image  of  digit      digit
    example:  image  of  object      object  type
    example:  query  +  webpages      best  match
    example:  symptoms      diagnosis
       

    three  main  ideas

    representation  as  feature  vectors  /  kernel  functions
    scoring  by  linear  functions
    learning  by  optimization

3

some  definitions

close  the  ____

{door,  table,     }

table

door

inputs

candidate 
set

candidates

true 
outputs

feature 
vectors

x   1=   the         y=   door   

   close     in  x     y=   door   

x   1=   the         y=   table   

y  occurs  in  x

4

features

5

feature  vectors

    example:  web  page  ranking  (not  actually  classification)
xi =    apple computers   

6

block  feature  vectors

    sometimes,  we  think  of  the  input  as  having  features,  which  

are  multiplied  by  outputs  to  form  the  candidates

    win the election    

   win   

   election   

    win the election    

    win the election    

    win the election    

7

non   block  feature  vectors

    sometimes  the  features  of  candidates  cannot  be  

decomposed  in  this  regular  way

    example:  a  parse  tree   s  features  may  be  the  productions  
np

s

vp

present  in  the  tree

s

np
n    n

vp
v

s

vp
np
n v    n

    different  candidates  will  thus  often  share  features
    we   ll  return  to  the  non   block  case  later

np
n    n

vp
v

np
n
vp
v    n

8

linear  models

9

linear  models:  scoring

    in  a  linear  model,  each  feature  gets  a  weight  w

    win the election    

    win the election    

    we  score  hypotheses  by  multiplying  features  and  weights:

    win the election    

    win the election    

10

linear  models:  decision  rule

    the  linear  decision  rule:

    win the election    

    win the election    

    win the election    

    win the election    

    win the election    

    win the election    

    we   ve  said  nothing  about  where  weights  come  from

11

binary  classification

    important  special  case:  binary  classification

    classes  are  y=+1/   1

    decision  boundary  is

a  hyperplane

bias  : -3
free  :  4
money :  2

+1 = spam

1

free

y
e
n
o
m

-1 = ham

2

1

0

0

12

multiclass  decision  rule

    if  more  than  two  classes:

    highest  score  wins
    boundaries  are  more  

complex

    harder  to  visualize

    there are other ways: e.g. reconcile pairwise decisions

13

learning

14

learning  classifier  weights

    two  broad  approaches  to  learning  weights

    generative:  work  with  a  probabilistic  model  of  the  data,  

weights  are  (log)  local  conditional  probabilities
    advantages:  learning  weights  is  easy,  smoothing  is  well   understood,  

backed  by  understanding  of  modeling

    discriminative:  set  weights  based  on  some  error   related  

criterion
    advantages:  error   driven,  often  weights  which  are  good  for  

classification  aren   t  the  ones  which  best  describe  the  data

    we   ll  mainly  talk  about  the  latter  for  now

15

how  to  pick  weights?

    goal:  choose     best     vector  w  given  training  data

    for  now,  we  mean     best  for  classification   

    the  ideal:  the  weights  which  have  greatest  test  set  

accuracy  /  f1  /  whatever
    but,  don   t  have  the  test  set
    must  compute  weights  from  training  set

    maybe  we  want  weights  which  give  best  training  set  

accuracy?
    hard  discontinuous  optimization  problem
    may  not  (does  not)  generalize  to  test  set
    easy  to  overfit

though, min-error 
training for mt 
does exactly this.

16

minimize  training  error?

    a  loss  function  declares  how  costly  each  mistake  is

    e.g.  0  loss  for  correct  label,  1  loss  for  wrong  label
    can  weight  mistakes  differently  (e.g.  false  positives  worse  than  false  

negatives  or  hamming  distance  over  structured  labels)

    we  could,  in  principle,  minimize  training  loss:

    this  is  a  hard,  discontinuous  optimization  problem

17

linear  models:  id88

    the  id88  algorithm

    iteratively  processes  the  training  set,  reacting  to  training  errors
    can  be  thought  of  as  trying  to  drive  down  training  error

    the  (online)  id88  algorithm:

    start  with  zero  weights  w
    visit  training  instances  one  by  one

    try  to  classify

    if  correct,  no  change!
    if  wrong:  adjust  weights

18

example:     best     web  page

xi =    apple computers   

19

examples:  id88

    separable  case

20

20

id88s  and  separability

    a  data  set  is  separable  if  some  
parameters  classify  it  perfectly

    convergence:  if  training  data  

separable,  id88  will  separate  
(binary  case)

    mistake  bound:  the  maximum  

number  of  mistakes  (binary  case)  
related  to  the  margin or  degree  of  
separability

separable

non-separable

21

examples:  id88

    non   separable  case

22

22

issues  with  id88s

    overtraining:  test  /  held   out  accuracy  

usually  rises,  then  falls
    overtraining  isn   t  the  typically  discussed  

source  of  overfitting,  but  it  can  be  
important

    id173:  if  the  data  isn   t  

separable,  weights  often  thrash  around
    averaging  weight  vectors  over  time  can  

help  (averaged  id88)

    [freund    &  schapire 99,  collins  02]

    mediocre  generalization:  finds  a     barely     

separating  solution

23

problems  with  id88s

    id88     goal   :  separate  the  training  data

1. this may be an entire 
feasible space

2. or it may be impossible

24

margin

25

objective  functions

    what  do  we  want  from  our  weights?

    depends!
    so  far:  minimize  (training)  errors:

    this  is  the     zero   one  loss   

    discontinuous,  minimizing  is  np   complete
    not  really  what  we  want  anyway

    maximum  id178  and  id166s  have  other  

objectives  related  to  zero   one  loss

26

linear  separators

    which  of  these  linear  separators  is  optimal?  

27

27

classification  margin  (binary)

    distance  of  xi to  separator  is  its  margin,  mi
    examples  closest  to  the  hyperplane  are  support  vectors
    margin     of  the  separator  is  the  minimum  m

   

m

28

classification  margin

    for  each  example  xi and  possible  mistaken  candidate  y,  we  avoid  

that  mistake  by  a  margin  mi(y) (with  zero   one  loss)

    margin      of  the  entire  separator  is  the  minimum  m

    it  is  also  the  largest      for  which  the  following  constraints  hold

29

maximum  margin
    separable  id166s:  find  the  max   margin  w

    can  stick  this  into  matlab and  (slowly)  get  an  id166
    won   t  work  (well)  if  non   separable

30

why  max  margin?

    why  do  this?    various  arguments:

    solution  depends  only  on  the  boundary  cases,  or  support  vectors (but  

remember  how  this  diagram  is  broken!)

    solution  robust  to  movement  of  support  vectors
    sparse  solutions  (features  not  in  support  vectors  get  zero  weight)
    generalization  bound  arguments
    works  well  in  practice  for  many  problems

support vectors

31

max  margin  /  small  norm

    reformulation:  find  the  smallest  w  which  separates  data

remember this 
condition?

        scales  linearly  in  w,  so  if  ||w||  isn   t  constrained,  we  can  

take  any  separating  w  and  scale  up  our  margin

    instead  of  fixing  the  scale  of  w,  we  can  fix      =  1

32

soft  margin  classification    

    what  if  the  training  set  is  not  linearly  separable?
    slack  variables   i can  be  added  to  allow  misclassification  of  difficult  or  

noisy  examples,  resulting  in  a  soft  margin classifier

  i

  i

33

maximum  margin

note: exist other 
choices of how to 
penalize slacks!

    non   separable  id166s

    add  slack  to  the  constraints
    make  objective  pay  (linearly)  for  slack:

    c  is  called  the  capacity of  the  id166      the  smoothing  

knob

    learning:

    can  still  stick  this  into  matlab  if  you  want
    constrained  optimization  is  hard;  better  methods!
    we   ll  come  back  to  this  later

34

maximum  margin

35

likelihood

36

linear  models:  maximum  id178

    maximum  id178  (logistic  regression)

    use  the  scores  as  probabilities:

    maximize  the  (log)  conditional  likelihood  of  training  data

make 
positive
normalize

37

maximum  id178  ii

    motivation  for  maximum  id178:

    connection  to  maximum  id178  principle  (sort  of)
    might  want  to  do  a  good  job  of  being  uncertain  on  noisy  

cases   

         in  practice,  though,  posteriors  are  pretty  peaked

    id173  (smoothing)

38

maximum  id178

39

loss  comparison

40

log   loss

    if  we  view  maxent  as  a  minimization  problem:

    this  minimizes  the     log  loss     on  each  example

    one  view:  log  loss  is  an  upper  bound on  zero   one  loss

41

remember  id166s   

    we  had  a  constrained minimization

       but  we  can  solve  for     i

    giving

42

hinge  loss

plot really only right 
in binary case

    consider the per-instance objective:

    this  is  called  the     hinge  loss   

    unlike  maxent /  log  loss,  you  stop  

gaining  objective  once  the  true  label  
wins  by  enough

    you  can  start  from  here  and  derive  the  

id166  objective

    can  solve  directly  with  sub   gradient  

decent  (e.g.  pegasos:  shalev   shwartz et  
al  07)

43

max  vs     soft   max     margin

    id166s:

    maxent:

you can make this zero

    but not this one

    very  similar!    both  try  to  make  the  true  score  better  

than  a  function  of  the  other  scores
    the  id166  tries  to  beat  the  augmented  runner   up
    the  maxent  classifier  tries  to  beat  the     soft   max   

44

loss  functions:  comparison

    zero   one  loss

    hinge

    log

45

separators:  comparison

46

conditional  vs
joint  likelihood

47

example:  sensors

reality

raining

sunny

p(+,+,r) = 3/8

p(-,-,r) = 1/8

p(+,+,s) = 1/8

p(-,-,s) = 3/8

nb model

raining?

m1

m2

nb  factors:
    p(s) =  1/2  
    p(+|s)  =  1/4  
    p(+|r)  =  3/4

predictions:
    p(r,+,+) = (  )(  )(  )
    p(s,+,+) = (  )(  )(  )
    p(r|+,+) = 9/10
    p(s|+,+) = 1/10

48

example:  stoplights

reality

lights working

lights broken

p(g,r,w) = 3/7

p(r,g,w) = 3/7

p(r,r,b) = 1/7

nb model

working?

ns

ew

nb  factors:
    p(w)  =  6/7  
    p(r|w)  =  1/2  
    p(g|w)  =  1/2

    p(b) = 1/7 
    p(r|b) = 1 
    p(g|b) = 0

49

example:  stoplights

    what  does  the  model  say  when  both  lights  are  red?

    p(b,r,r) =  (1/7)(1)(1)  
    p(w,r,r) =  (6/7)(1/2)(1/2)  
    p(w|r,r)  =  6/10!

=  1/7  
=  6/28

=  4/28
=  6/28

    we   ll  guess  that  (r,r)  indicates  lights  are  working!

    imagine  if  p(b)  were  boosted  higher,  to  1/2:

    p(b,r,r) =  (1/2)(1)(1)  
    p(w,r,r) =  (1/2)(1/2)(1/2)  
    p(w|r,r)  =  1/5!

=  1/2
=  1/8

=  4/8
=  1/8

    changing  the  parameters  bought  accuracy  at  the  

expense  of  data  likelihood

50

