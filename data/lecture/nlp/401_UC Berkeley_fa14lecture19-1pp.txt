natural  language  processing

machine  translation  iii

dan  klein      uc  berkeley

1

phrase   based  mt

2

3

phrase   based  decoding

    7                                             

      

   

.

decoder design is important: [koehn et al. 03]

4

phrase   based  decoding

5

monotonic  word  translation

    cost  is  lm  *  tm
    it   s  an  id48?
    p(e|e   1,e   2)
    p(f|e)

    state  includes

    exposed  english
    position  in  foreign

    dynamic  program  loop?

[   . a slap, 5]
0.00001

[   . slap to, 6]
0.00000016

[   . slap by, 6]
0.00000001

for (fposition in 1   |f|)

for (econtext in allecontexts)

for (eoption in translations[fposition])

score = scores[fposition-1][econtext] * lm(econtext+eoption) * tm(eoption, fword[fposition])
scores[fposition][econtext[2]+eoption] =max score

6

beam  decoding

    for  real  mt  models,  this  kind  of  dynamic  program  is  a  disaster  (why?)
    standard  solution  is  beam  search:  for  each  position,  keep  track  of  only  the  

best  k  hypotheses

for (fposition in 1   |f|)

for (econtext in bestecontexts[fposition])

for (eoption in translations[fposition])

score = scores[fposition-1][econtext] * lm(econtext+eoption) * tm(eoption, fword[fposition])
bestecontexts.maybeadd(econtext[2]+eoption, score)

    still  pretty  slow     why?
    useful  trick:  cube  pruning  (chiang  2005)

example from david chiang

7

phrase  translation

    if  monotonic,  almost  an  id48;  technically  a  semi   id48

for (fposition in 1   |f|)

for (lastposition < fposition)
for (econtext in econtexts)

for (eoption in translations[fposition])

    combine hypothesis for (lastposition ending in econtext) with eoption

    if  distortion     now  what?

8

non   monotonic  phrasal  mt

9

pruning:  beams  +  forward  costs

    problem:  easy  partial  analyses  are  cheaper

    solution  1:  use  beams  per  foreign  subset
    solution  2:  estimate  forward  costs  (a*   like)

10

the  pharaoh  decoder

11

hypotheis  lattices

12

parameter  tuning

13

14

15

16

17

phrase  scoring

    learning  weights  has  been  

tried,  several  times:
   
   
         and  others

[marcu and  wong,  02]
[denero et  al,  06]

    seems  not  to  work  well,  for  a  

variety  of  partially  
understood  reasons

    main  issue:  big  chunks  get  all  

the  weight,  obvious  priors  
don   t  help
   

though,  [denero et  al  08]

aiment

poisson

les chats

le

frais

.

cats
like
fresh

fish
.

.

18

phrase  size

    phrases  do  help

    but  they  don   t  need  

to  be  long

    why  should  this  be?

19

lexical  weighting

20

tuning  for  mt

    features  encapsulate  lots  of  information

    basic  mt  systems  have  around  6  features
    p(e|f),  p(f|e),  lexical  weighting,  language  model

    how  to  tune  feature  weights?

    idea  1:  use  your  favorite  classifier

21

why  tuning  is  hard

    problem  1:  there  are  latent  variables

    alignments  and  segementations
    possibility:  forced  decoding  (but  it  can  go  badly)

22

why  tuning  is  hard

    problem  3:  computational  constraints

    discriminative  training  involves  repeated  decoding
    very  slow!    so  people  tune  on  sets  much  smaller  than  those  used  to  

build  phrase  tables

23

minimum  error  rate  training

    standard  method:  minimize  id7  directly  (och 03)

    mert  is  a  discontinuous  objective
    only  works  for  max  ~10  features,  but  works  very  well  then
    here:  k   best  lists,  but  forest  methods  exist  (machery et  al  08)
    recently,  lots  of  alternatives  being  explored  for  more  features

e
r
o
c
s

 
l
e
d
o
m

  

24

mert

e
r
o
c
s

 
l

e
d
o
m

e
r
o
c
s
u
e
l
b

 

  

  

25

mert

26

27

28

syntactic  models

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

syntactic  decoding

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

flexible  syntax

78

soft syntactic mt: from chiang 2010

79

80

hiero  rules

from [chiang et al, 2005]

81

82

83

84

85

86

87

88

89

90

91

92

93

