natural  language  processing

language  modeling  ii

dan  klein      uc  berkeley

    n   gram  models  are  built  from  local  conditional  probabilities

language models
    language  models  are  distributions  over  sentences

(cid:1842)(cid:1875) (cid:3404)(cid:1842)(cid:1875)(cid:2869)   (cid:1875)(cid:3041)
(cid:1842)(cid:1875)(cid:2869)   (cid:1875)(cid:3041) (cid:3404)(cid:3537)(cid:1842)(cid:1875)(cid:3036)(cid:1875)(cid:3036)(cid:2879)(cid:2869),(cid:1875)(cid:3036)(cid:2879)(cid:2870)(cid:4667)
(cid:1842)(cid:1875)(cid:3036)(cid:1875)(cid:3036)(cid:2879)(cid:2869),(cid:1875)(cid:3036)(cid:2879)(cid:2870)(cid:4667)(cid:3404)(cid:1855)(cid:4666)(cid:1875)(cid:3036)(cid:2879)(cid:2870),(cid:1875)(cid:3036)(cid:2879)(cid:2869),(cid:1875)(cid:3036)(cid:4667)
(cid:1855)(cid:4666)(cid:1875)(cid:3036)(cid:2879)(cid:2870),(cid:1875)(cid:3036)(cid:2879)(cid:2869)(cid:4667)

(cid:3036)

    the  methods  we   ve  seen  are  backed  by  corpus  n   gram  counts

n   gram  demo

tons  of  data
    good  lms  need  lots  of  n   grams!

[brants et al, 2007]

storing  counts

example:  google  n   grams

    key  function:  map  from  n   grams  to  counts

   

searching for the best
searching for the right
searching for the cheapest
searching for the perfect
searching for the truth
searching for the    
searching for the most
searching for the latest
searching for the next
searching for the lowest
searching for the name
searching for the finest

   

192593
45805
44965
43959
23165
19086
15512
12670
10120
10080
8402
8171

1

efficient  storage

a  simple  java  hashmap?

per 3-gram:
1 pointer = 8 bytes
1 map.entry = 8 bytes (obj)

+3x8 bytes (pointers)

1 double = 8 bytes (obj) 

+ 8 bytes (double)
1 string[] = 8 bytes (obj) + 

+ 3x8 bytes (pointers)

    at best strings are canonicalized

total: > 88 bytes

obvious alternatives:
- sorted arrays
- open addressing

integer  encodings

bit  packing

got  3  numbers  under  220 to  store?

20  bits          20  bits        20  bits

fits  in  a  primitive  64   bit  long

efficient  hashing

rank  values

    closed  address  hashing

    resolve  collisions  with  chains
    easier  to  understand  but  bigger

    open  address  hashing

    resolve  collisions  with  probe  sequences
    smaller  but  easy  to  mess  up

    direct   address  hashing
    no  collision  resolution
    just  eject  previous  entries
    not  suitable  for  core  lm  storage  

2

tries

context  tries

context  encodings

context  encodings

[many  details  from  pauls and  klein,  2011]

n   gram  lookup

compression

3

idea:  differential compression

variable  length  encodings

encoding     9   
000  1001

length  

in  
unary

number  

in

binary

5.9
3.2

[elias, 75]

rolling queries

speed   ups

idea:  fast  caching

approximate  lms

    simplest  option:  hash   and   hope

    array  of  size  k  ~  n
    (optional)  store  hash  of  keys
    store  values  in  direct   address
    collisions:  store  the  max
    what  kind  of  errors  can  there  be?

    more  complex  options,  like  bloom  filters  (originally  for  membership,  but  

see  talbot  and  osborne  07),  perfect  hashing,  etc

lm  can  be  more  than  
10x  faster  w/  direct   

address  caching

4

maximum  id178  models

    want  a  model  over  completions  y  given  a  context  x:

maximum  id178  lms

(cid:1842)(cid:1877)(cid:1876) (cid:3404)(cid:1842)(cid:4666)																																					(cid:4667)

close  the  door  |  close  the

    want  to  characterize  the  important  aspects  of  y  =  

(v,x)  using  a  feature  function  f

    f  might  include

    indicator  of  v  (unigram)
    indicator  of  v,  previous  word  (bigram)
    indicator  whether  v  occurs  in  x  (cache)
    indicator  of  v  and  each  non   adjacent  previous  word
       

improving  on  n   grams?

    n   grams  don   t  combine  multiple  sources  of  evidence  well

p(construction  |  after  the  demolition  was  completed,  the)

    here:

       the     gives  syntactic  constraint
       demolition     gives  semantic  constraint
    unlikely  the  interaction  between  these  two  has  been  densely  

observed

    we   d  like  a  model  that  can  be  more  statistically  efficient

inputs

candidate 
set

candidates

true 
outputs

feature 
vectors

some  definitions

close the ____

{close the door, close the table,    }

close the table

close the door

   close    in x     v=   door   

v-1=   the        v=   door   

   door    in x and v

linear  models:  maximum  id178

maximum  id178  ii

    maximum  id178  (logistic  regression)

    use  the  scores  as  probabilities:

    maximize  the  (log)  conditional  likelihood  of  training  data

make  positive
normalize

    motivation  for  maximum  id178:

    connection  to  maximum  id178  principle  (sort  of)
    might  want  to  do  a  good  job  of  being  uncertain  on  noisy  

cases   

         in  practice,  though,  posteriors  are  pretty  peaked

    id173  (smoothing)

5

derivative  for  maximum  id178

convexity

    the  maxent objective  is  nicely  behaved:

    differentiable (so  many  ways  to  optimize)
    convex (so  no  local  optima*)

big  weights  are  bad

expected  feature  vector  
over  possible  candidates

total  count  of  feature  n  in  
correct  candidates

convex

non-convex

convexity guarantees a single, global maximum value because any 
higher points are greedily reachable

unconstrained  optimization

    once  we  have  a  function  f,  we  can  find  a  local  optimum  by  

iteratively  following  the  gradient

    for  convex  functions,  a  local  optimum  will  be  global
    basic  gradient  ascent  isn   t  very  efficient,  but  there  are  simple  

enhancements  which  take  into  account  previous  gradients:  
conjugate  gradient,  l   bfgs;  adagrad now  popular

    there  are  special   purpose  optimization  techniques  for  maxent,  

like  iterative  scaling,  but  they  aren   t  better

6

