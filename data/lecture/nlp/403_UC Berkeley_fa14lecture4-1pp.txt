natural language processing

id38 iii

dan klein     uc berkeley

improving on id165s?

  id165s don   t combine multiple sources of evidence well

p(construction | after the demolition was completed, the)

  here:

     the    gives syntactic constraint

     demolition    gives semantic constraint

  unlikely the interaction between these two has been densely 

observed in this specific id165

  we   d like a model that can be more statistically efficient

maximum id178 models

some definitions

close the ____

{door, table,    }

table

door

inputs

candidate 

set

candidates

true 

outputs

feature 

vectors

x-1=   the        y=   door   

   close    in x     y=   door   

x-1=   the        y=   table   

y occurs in x

more features, less interaction

x = closing the ____, y = doors

  id165s

x-1=   the        y=   doors   

  skips

x-2=   closing        y=   doors   

  lemmas

x-2=   close        y=   door   

  caching

y occurs in x

data: feature impact

features

train perplexity

test perplexity

3 gram indicators

1-3 grams

1-3 grams + skips

241

126

101

350

172

164

exponential form

  weights

features

  linear score

  unnormalized id203

  id203

likelihood objective

  model form:

  likelihood of training data

training

history of training

  1990   s: specialized methods (e.g. iterative 

scaling)

  2000   s: general-purpose methods (e.g. 

conjugate gradient)

  2010   s: online methods (e.g. stochastic 

gradient)

what does ll look like?

  example

  data: xxxy

  two outcomes, x and y

  one indicator for each

  likelihood

id76

  the maxent objective is an unconstrained convex problem

  one optimal value*, gradients point the way

gradients

count of features under 
target labels

expected count of features 
under model predicted label 
distribution

gradient ascent

  the maxent objective is an unconstrained optimization 

problem

  gradient ascent

  basic idea: move uphill from current guess
  gradient ascent / descent follows the gradient incrementally
  at local optimum, derivative vector is zero
  will converge if step sizes are small enough, but not efficient
  all we need is to be able to evaluate the function and its derivative

(quasi)-id77s

  2nd-order methods: repeatedly create a quadratic 

approximation and solve it

  e.g. lbfgs, which tracks derivative to approximate (inverse) 

hessian

id173

id173 methods

  early stopping

2
  l2: ll(w)-|w|2

  l1: ll(w)-|w|

id173 effects

  early stopping: don   t do this

  l2: weights stay small but non-zero

  l1: many weights driven to zero

  good for sparsity

  usually bad for accuracy for nlp

scaling

why is scaling hard?

  big id172 terms

  lots of data points

hierarchical prediction

  hierarchical prediction / softmax [mikolov et al 2013]

  noise-contrastive estimation [mnih, 2013]

  self-id172 [devlin, 2014]

image: ayende.com

stochastic gradient

  view the gradient as an average over data points

  stochastic gradient: take a step each example (or mini-batch)

  substantial improvements exist, e.g. adagrad (duchi, 11)

other methods

neural net lms

image: (bengio et al, 03)

neural vs maxent

  maxent lm

  neural net lm

nonlinear, e.g. tanh

mixed interpolation

  but can   t we just interpolate:

  p(w|most recent words)

  p(w|skip contexts)

  p(w|caching)

     

  yes, and people do (well, did)

  but additive combination tends to flatten 

distributions, not zero out candidates

id90 / forests

prev word?

   

last verb?

  id90?

  good for non-linear decision problems

  id79s can improve further [xu and jelinek, 2004]

  paths to leaves basically learn conjunctions

  general contrast between dts and linear models

