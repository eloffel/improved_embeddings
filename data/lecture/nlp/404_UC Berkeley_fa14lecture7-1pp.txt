natural  language  processing

speech  id136
dan  klein      uc  berkeley

1

grading

    class  is  now  big  enough  for  big   class  policies

    late  days:  7  total,  use  whenever

    grading:  projects  out  of  10

    6  points:  successfully  implemented  what  we  asked
    2  point:  submitted  a  reasonable  write   up
    1  point:  write   up  is  written  clearly
    1  point:  substantially  exceeded  minimum  metrics
    extra  credit:  did  non   trivial  extension  to  project

    letter  grades:

    10=a,  9=a   ,  8=b+,  7=b,  6=b   ,  5=c+,  lower  handled  case   by   case
    cutoffs  at  9.5,  8.5,  etc.,  a+  by  discretion

2

state  model

3

fsa  for  lexicon  +  bigram  lm

figure  from  huang  et  al  page  618

4

state  space

    full  state  space

(lm  context,  lexicon  index,  subphone)

    details:

    lm  context  is  the  past  n   1  words
    lexicon  index  is  a  phone  position  within  a  word  (or  a  trie of  the  

lexicon)

    subphone is  begin,  middle,  or  end
    e.g.  (after  the,  lec[t   mid]ure)

    acoustic  model  depends  on  clustered  phone  context

    but  this  doesn   t  grow  the  state  space

5

decoding

6

state  trellis

figure:  enrique  benimeli

7

na  ve  viterbi

8

beam  search

    at  each  time  step

    start:  beam  (collection)  vt of  hypotheses  s  at  time  t
    for  each  s  in  vt

    compute  all  extensions  s     at  time  t+1
    score  s     from  s
    put  s     in  vt+1 replacing  existing  s     if  better

    advance  to  t+1

    beams  are  priority  queues  of  fixed  size*  k  (e.g.  30)  

and  retain  only  the  top  k  hypotheses

9

prefix  trie encodings

    problem:  many  partial   word  states  are  indistinguishable
    solution:  encode  word  production  as  a  prefix  trie (with  

pushed  weights)

0.04

0.02

0.01

n

n

n

i

i

o

d

t

t

0.04

1

n
0.25

d

t

t

1

0.5

1

i

o

    a  specific  instance  of  minimizing  weighted  fsas  [mohri,  94]

example:  aubert,  02

10

lm score  integration

    imagine  you  have  a  unigram  language  model
    when  does  a  hypothesis  get     charged     for  cost  of  a  word?

    in  na  ve  lexicon  fsa,  can  charge  when  word  is  begun
    in  na  ve  prefix  trie,  don   t  know  word  until  the  end
         but  you  can  charge  partially  as  you  complete  it

0.04

0.02

0.01

n

n

n

i

i

o

d

t

t

0.04

1

n
0.25

d

t

t

1

0.5

1

i

o

11

lm  factoring

    problem:  higher   order  n   grams  explode  the  state  space
    (one)  solution:

    factor  state  space  into  (lexicon  index,  lm  history)
    score  unigram  prefix  costs  while  inside  a  word
    subtract  unigram  cost  and  add  trigram  cost  once  word  is  complete

the

0.04

1

n
0.25

d

t

t

1

0.5

1

i

o

    note  that  you  might  have  two  hypotheses  on  the  beam  that  differ  

only  in  lm  context,  but  are  doing  the  same  within   word  work

12

lm  reweighting

    noisy  channel  suggests

    in  practice,  want  to  boost  lm

    also,  good  to  have  a     word  bonus     to  offset  lm  costs

    the  needs  for  these  tweaks  are  both  consequences  of  broken  
independence  assumptions  in  the  model,  so  won   t  easily  get  
fixed  within  the  probabilistic  framework

13

other  good  ideas

    when  computing  emission  scores,  p(x|s)  depends  on  only  a  

projection     (s),  so  use  caching

    beam  search  is  still  dynamic  programming,  so  make  sure  you  
check  for  hypotheses  that  reach  the  same  id48  state  (so  you  
can  delete  the  suboptimal  one).

    beams  require  priority  queues,  and  beam  search  

implementations  can  get  object   heavy.    remember  to  intern  /  
canonicalize objects  when  appropriate.

14

training

15

what  needs  to  be  learned?

s

x

s

x

s

x

    emissions:  p(x  |  phone  class)

    x  is  mfcc   valued

    transitions:  p(state  |  prev state)

    if  between  words,  this  is  p(word  |  history)
    if  inside  words,  this  is  p(advance  |  phone  class)
    (really  a  hierarchical  model)

16

estimation from  aligned  data

    what  if  each  time  step  was  labeled  with  its  (context   

dependent  sub)  phone?

/k/

x

/ae/

/ae/

/ae/

x

x

x

/t/

x

    can  estimate  p(x|/ae/)  as  empirical  mean  and  (co   )variance  of  

x   s  with  label  /ae/

    problem:  don   t  know  alignment  at  the  frame  and  phone  level

17

forced alignment

    what  if  the  acoustic  model  p(x|phone)  was  known?

         and  also  the  correct  sequences  of  words  /  phones

    can  predict  the  best  alignment  of  frames  to  phones

   speech lab   

ssssssssppppeeeeeeetshshshshllllaeaeaebbbbb

    called     forced  alignment   

18

forced  alignment

    create  a  new  state  space  that  forces  the  hidden  variables  to  transition  

through  phones  in  the  (known)  order

/s/

/p/

/ee/

/ch/

/l/

/ae/

/b/

    still  have  uncertainty  about  durations

    in  this  id48,  all  the  parameters  are  known
    transitions  determined  by  known  utterance
    emissions  assumed  to  be  known
    minor  detail:  self   loop  probabilities

    just  run  viterbi  (or  approximations)  to  get  the  best  alignment

19

em  for  alignment

    input:  acoustic  sequences  with  word   level  transcriptions

    we  don   t  know  either  the  emission  model  or  the  frame  

alignments

    expectation  maximization  (hard  em  for  now)

    alternating  optimization
    impute  completions  for  unlabeled  variables  (here,  the  states  at  each  

time  step)

    re   estimate  model  parameters  (here,  gaussian  means,  variances,  

mixture  ids)

    repeat
    one  of  the  earliest  uses  of  em!

20

soft  em
    hard  em  uses  the  best  single  completion

    here,  single  best  alignment
    not  always  representative
    certainly  bad  when  your  parameters  are  initialized  and  the  alignments  

are  all  tied

    uses  the  count  of  various  configurations  (e.g.  how  many  tokens  of  

/ae/  have  self   loops)

    what  we   d  really  like  is  to  know  the  fraction  of  paths  that  

include  a  given  completion
    e.g.  0.32  of  the  paths  align  this  frame  to  /p/,  0.21  align  it  to  /ee/,  etc.
    formally  want  to  know  the  expected  count  of  configurations
    key  quantity:  p(st |  x)

21

computing marginals

= sum of all paths through s at t

sum of all paths

22

forward  scores

23

backward scores

24

total  scores

25

fractional  counts

    computing  fractional  (expected)  counts
    compute  forward  /  backward  probabilities
    for  each  position,  compute  marginal  posteriors
    accumulate  expectations
    re   estimate  parameters  (e.g.  means,  variances,  self   loop  

probabilities)  from  ratios  of  these  expected  counts

26

staged  training  and  state  tying

    creating  cd  phones:

    start  with  monophone,  do  em  

training

    clone  gaussians  into  triphones
    build  decision  tree  and  cluster  

gaussians

    clone  and  train  mixtures  

(gmms)

    general  idea:

    introduce  complexity  gradually
    interleave  constraint  with  

flexibility

27

