natural language processing

course information

http://www.cs.berkeley.edu/~klein/cs288/fa14/

lecture 1: introduction

dan klein     uc berkeley

https://piazza.com/berkeley/fall2014/cs288/

course requirements

other announcements

  prerequisites:

  cs 188 (cs 281a) and preferably cs170 (a-level mastery)
  strong skills in java or equivalent
  deep interest in language
  successful completion of the first project
  there will be a lot of math and programming

  work and grading:

  six assignments (individual, jars + write-ups)
  this course is a major time-commitment!

  books:

  primary text: jurafsky and martin, speech and language 

processing, 2nd edition (not 1st)

  also: manning and schuetze, foundations of statistical nlp

  course contacts:

  webpage: materials and announcements
  piazza: discussion forum

  enrollment: we   ll try to take everyone who meets the 

requirements

  computing resources

  you will want more compute power than the instructional labs
  experiments can take up to hours, even with efficient code
  recommendation: start assignments early

  questions?

ai: where do we stand?

language technologies

goal: deep understanding

reality: shallow matching

  requires context, linguistic 

  requires robustness and scale

structure, meanings   

  amazing successes, but 
fundamental limitations 

source: slav petrov

1

speech systems

  automatic id103 (asr)

  audio in, text out

 

sota: 0.3% error for digit strings, 5% dictation, 50%+ tv

   speech lab   

  text to speech (tts)

  text in, audio out

 

sota: totally intelligible (if sometimes unnatural)

example: siri

  siri contains

  id103

  language analysis

  dialog processing

  text to speech

image: wikipedia

text data is superficial

    but language is complex

an iceberg is a large piece of freshwater ice that has
broken off from a snow-formed glacier or ice shelf and
is floating in open water.

an iceberg is a large piece of
freshwater ice that has broken off
from a snow-formed glacier or ice
shelf and is floating in open water.

 

 

 

semantic structures

references and entities

discourse-level connectives

  meanings and implicatures

 

 

 

contextual factors

perceptual grounding 

    

deeper linguistic analysis

learning hidden syntax

personal pronouns (prp)

hurricane emily howled toward mexico 's caribbean coast on sunday 
packing 135 mph winds and torrential rain and causing panic in cancun, 

where frightened tourists squeezed into musty shelters .

accuracy: 90+

prp-1

prp-2

prp-3

nnp-14

nnp-12

nnp-2

nnp-1

nnp-15

nnp-3

it

it

it

them

he

he

proper nouns (nnp)

oct.

john

j.

bush

new

york

nov.

robert

e.

noriega

san

him

they

i

sept.

james

l.

peters

wall

francisco

street

2

search, facts, and questions

example: watson

language comprehension?

summarization

  condensing 
documents

 

single or multiple 
docs

  extractive or 

synthetic

  aggregative or 
representative

  very context-

dependent!

  an example of 

analysis with 
generation

machine translation

  translate text from one language to another

  recombines fragments of example translations

  challenges:

  what fragments?  [learning to translate]

  how to make efficient?  [fast translation search]

  fluency (next class) vs fidelity (later)

3

more data: machine translation

data by itself isn   t enough!

source

cela constituerait une solution transitoire qui permettrait de 
conduire    terme    une charte    valeur contraignante.

human

that would be an interim solution which would make it possible to 
work towards a binding charter in the long term .

1x data

[this] [constituerait] [assistance] [transitoire] [who] [permettrait] 
[licences] [to] [terme] [to] [a] [charter] [to] [value] [contraignante] [.]

10x data

[it]  [would] [a solution] [transitional] [which] [would] [of] [lead] 
[to] [term] [to a] [charter] [to] [value] [binding] [.]

100x data

[this] [would be] [a transitional solution] [which would] [lead to] [a 
charter] [legally binding] [.]

1000x data

[that would be] [a transitional solution] [which would] [eventually 
lead to] [a binding charter] [.]

data and knowledge

  classic id99 worry: how will a 

machine ever know that   

  ice is frozen water?

  beige looks like this:

  chairs are solid?

  answers:

  1980: write it all down

  2000: get by without it

  2020: learn it from data

deeper understanding: reference

names vs. entities

4

example errors

discovering knowledge

grounded language

grounding with natural data

    on the beige loveseat.

what is nearby nlp?

example: nlp meets cl

  computational linguistics

  using computational methods to learn more 

about how language works

  we end up doing this and using it

  cognitive science

  figuring out how the human brain works
  includes the bits that do language
  humans: the only working nlp prototype!

  speech processing

  mapping audio signals to text
  traditionally separate from nlp, converging?
  two components: acoustic models and language 

models

  language models in the domain of stat nlp

  example: language change, reconstructing ancient forms, phylogenies

    just one example of the kinds of linguistic models we can build

5

what is this class?

class requirements and goals 

  three aspects to the course:

  linguistic issues

  what are the range of language phenomena?
  what are the knowledge sources that let us disambiguate?
  what representations are appropriate?
  how do you know what to model and what not to model?

  statistical modeling methods

  increasingly complex model structures
  learning and parameter estimation
  efficient id136: id145, search, sampling

  engineering methods

  issues of scale
  where the theory breaks down (and what to do about it)

  we   ll focus on what makes the problems hard, and what 

works in practice   

  class requirements

  uses a variety of skills / knowledge:

  id203 and statistics, id114 (parts of cs281a)
  basic linguistics background (ling100)
  strong coding skills (java), well beyond cs61b

  most people are probably missing one of the above
  you will often have to work on your own to fill the gaps

  class goals

  learn the issues and techniques of statistical nlp
  build realistic nlp tools 
  be able to read current research papers in the field
  see where the holes in the field still are!

  this semester: new projects (speech, translation, analysis)

some big disclaimers

some early nlp history

  the purpose of this class is to train nlp researchers

  some people will put in a lot of time     this course is more work than 

most classes (grad or undergrad)

  there will be a lot of reading, some required, some not     you will 

have to be strategic about what reading enables your goals

  there will be a lot of coding and running systems on substantial 

amounts of real data

  there will be a lot of machine learning / math
  there will be discussion and questions in class that will push past what 

i present in lecture, and i   ll answer them

  not everything will be spelled out for you in the projects
  especially this term: new projects will have hiccups

  don   t say i didn   t warn you!

 

1950   s:

 

 

foundational work: automata, id205, etc.
first speech systems

  machine translation (mt) hugely funded by military

  toy models: mt using basically word-substitution

  optimism!

 

1960   s and 1970   s: nlp winter

  bar-hillel (fahqt) and alpac reports kills mt
  work shifts to deeper models, syntax
      but toy domains / grammars (shrdlu, lunar)

 

1980   s and 1990   s: the empirical revolution

  expectations get reset
  corpus-based methods become central
  deep analysis often traded for robust and simple approximations
  evaluate everything

 

2000+: richer statistical methods

  models increasingly merge linguistically sophisticated representations with statistical 

methods, confluence and clean-up

  begin to get both breadth and depth

problem: structure

problem: scale

  headlines:

  enraged cow injures farmer with ax
  teacher strikes idle kids
  hospitals are sued by 7 foot doctors
  ban on nude dancing on governor   s desk
  iraqi head seeks arms
  stolen painting found by tree
  kids make nutritious snacks
  local hs dropouts cut in half

  why are these funny?

  people did know that language was ambiguous!

     but they hoped that all interpretations would be    good    ones (or 

ruled out pragmatically)

     they didn   t realize how bad it would be

adj

det

noun

det

noun

plural noun

np

pp

np

np

conj

6

problem: sparsity

outline of topics

  however: sparsity is always a problem

  new unigram (word), bigram (word pair), and rule rates in 

newswire

 

n
e
e
s
n
o
i
t
c
a
r
f

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

unigrams

bigrams

0

200000

400000

600000

800000

1000000

number of words

  words and sequences
  id103
  id165 models
  working with a lot of data

  structured classification

  trees

  syntax and semantics
  syntactic mt
  id53

  machine translation

  other topics

  reference resolution
  summarization
  diachronics
     

a puzzle

  you have already seen n words of text, containing a bunch of 

different word types (some once, some twice   )

  what is the chance that the n+1st word is a new one?

7

