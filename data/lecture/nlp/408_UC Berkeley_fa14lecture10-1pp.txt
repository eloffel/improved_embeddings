natural  language  processing

parsing  ii

dan  klein      uc  berkeley

1

learning  pid18s

2

treebank  pid18s

[charniak 96]

    use  pid18s  for  broad  coverage  parsing
    can  take  a  grammar  right  off  the  trees  (doesn   t  work  well):

root     s
s     np vp .
np     prp
vp     vbd adjp

   ..

1
1
1
1

model
baseline

f1
72.0

3

conditional  independence?

    not  every  np  expansion  can  fill  every  np  slot

    a  grammar  with  symbols  like     np     won   t  be  context   free
    statistically,  conditional  independence  too  strong

4

non   independence
    independence  assumptions  are  often  too  strong.

all nps

nps under s
21%

nps under vp
23%

11%

9%

6%

9%

9%

7%

4%

np pp dt nn

prp

np pp dt nn

prp

np pp dt nn

prp

    example:  the  expansion  of  an  np  is  highly  dependent  on  the  

parent  of  the  np  (i.e.,  subjects  vs.  objects).

    also:  the  subject  and  object  expansions  are  correlated!

5

grammar  refinement

    example:  pp  attachment

6

grammar  refinement

    structure  annotation  [johnson     98,  klein&manning    03]
    lexicalization  [collins     99,  charniak    00]
    latent  variables  [matsuzaki et  al.  05,  petrov et  al.     06]

7

structural  annotation

8

the  game  of  designing  a  grammar

    annotation refines base treebank symbols to 

improve statistical fit of the grammar
    structural annotation

9

typical  experimental  setup

    corpus:  penn  treebank,  wsj

training:
development:
test:

sections
section
section

02-21
22 (here, first 20 files)
23

    accuracy      f1:  harmonic  mean  of  per   node  labeled  

precision  and  recall.

    here:  also  size      number  of  symbols  in  grammar.

10

vertical  markovization

    vertical  markov  
order:  rewrites  
depend  on  past  k
ancestor  nodes.
(cf.  parent  
annotation)

79%
78%
77%
76%
75%
74%
73%
72%

2

2v

1
3
vertical markov order

3v

order 1

order 2

l

s
o
b
m
y
s

25000
20000
15000
10000
5000
0

1

2v

2

3v

3

vertical markov order

11

horizontal  markovization

order 1

order    

74%
73%

72%
71%
70%

1

2v

0
inf
horizontal markov order

2

l

s
o
b
m
y
s

12000

9000
6000

3000
0

1

0
inf
horizontal markov order

2v

2

12

unary  splits

    problem:  unary  
rewrites  used  to  
transmute  
categories  so  a  
high   id203  
rule  can  be  used.

    solution: mark 

unary rewrite 
sites with -u

annotation
base
unary

f1
77.8
78.3

size
7.5k
8.0k

13

tag  splits

    problem:  treebank  tags  

are  too  coarse.

    example:  sentential,  pp,  
and  other  prepositions  
are  all  marked  in.

    partial  solution:

    subdivide  the  in  tag.

annotation
previous
split-in

f1
78.3
80.3

size
8.0k
8.1k

14

a  fully  annotated  (unlex)  tree

15

some  test  set  results

lp

parser
magerman 95 84.9
86.3
collins 96
86.9
unlexicalized
87.4
charniak 97
collins 99
88.7

lr
84.6
85.8
85.7
87.5
88.6

f1
84.7
86.0
86.3
87.4
88.6

cb
1.26
1.14
1.10
1.00
0.90

0 cb
56.6
59.9
60.3
62.1
67.1

    beats     first  generation     lexicalized  parsers.
    lots  of  room  to  improve      more  complex  models  next.

16

efficient  parsing  for
structural  annotation

17

grammar  projections

coarse grammar

fine grammar

np       dt  n   

np^s       dt^np  n   [   dt]^np

note:  x   bar  grammars  are  projec(cid:415)ons  with  rules  like  xp       y  x     or  xp       x     y  or  x          x

18

coarse   to   fine  pruning

for  each  coarse  chart  item  x[i,j],  compute  posterior  id203:

<   threshold

e.g.  consider  the  span  5  to  12:

coarse:

    qp np vp    

refined:

19

computing  (max   )marginals

20

inside  and  outside  scores

21

pruning  with  a*

    you  can  also  speed  up  the  
search  without  sacrificing  
optimality

    for  agenda   based  parsers:
    can  select  which  items  to  

process  first

    can  do  with  any     figure  of  

merit     [charniak  98]

    if  your  figure   of   merit  is  a  

valid  a*  heuristic,  no  loss  of  
optimiality  [klein  and  
manning  03]

x

0

i

j

n

22

a*  parsing

23

lexicalization

24

the  game  of  designing  a  grammar

    annotation refines base treebank symbols to improve 

statistical fit of the grammar
    structural annotation [johnson    98, klein and manning 03]
    head lexicalization [collins    99, charniak    00]

25

problems  with  pid18s

    if  we  do  no  annotation,  these  trees  differ  only  in  one  rule:

    vp      vp  pp
    np      np  pp

    parse  will  go  one  way  or  the  other,  regardless  of  words
    we  addressed  this  in  one  way  with  unlexicalized  grammars  (how?)
    lexicalization  allows  us  to  be  sensitive  to  specific  words

26

problems  with  pid18s

    what   s  different  between  basic  pid18  scores  here?
    what  (lexical)  correlations  need  to  be  scored?

27

lexicalized  trees

    add     head  words     to  

each  phrasal  node
    syntactic  vs.  semantic  

heads

    headship  not  in  (most)  

treebanks

    usually  use  head  rules,  

e.g.:

    np:

    vp:

    take  leftmost  np
    take  rightmost  n*
    take  rightmost  jj
    take  right  child

    take  leftmost  vb*
    take  leftmost  vp
    take  left  child

28

lexicalized  pid18s?

    problem:  we  now  have  to  estimate  probabilities  like

    never  going  to  get  these  atomically  off  of  a  treebank

    solution:  break  up  derivation  into  smaller  steps

29

lexical  derivation  steps

    a  derivation  of  a  local  tree  [collins  99]

choose a head tag and word

choose a complement bag

generate children (incl. adjuncts)

recursively derive children

30

lexicalized  cky

(vp->vbd...np    )[saw]

(vp->vbd    )[saw]

np[her]

x[h]

y[h] z[h   ]

i           h          k         h             j

bestscore(x,i,j,h)

if (j = i+1)
else

return tagscore(x,s[i])
return 

max max score(x[h]->y[h] z[h   ]) *

k,h   ,x->yz

max score(x[h]->y[h   ] z[h]) *

bestscore(y,i,k,h) *
bestscore(z,k,j,h   )
bestscore(y,i,k,h   ) *
bestscore(z,k,j,h)

k,h   ,x->yz

31

efficient  parsing  for
lexical  grammars

32

quartic  parsing

    turns  out,  you  can  do  (a  little)  better  [eisner  99]

x[h]

y[h] z[h   ]

x[h]

y[h]

z

i           h          k         h             j

i           h          k                     j

    gives  an  o(n4)  algorithm
    still  prohibitive  in  practice  if  not  pruned

33

pruning  with  beams

    the  collins  parser  prunes  with  per   

cell  beams  [collins  99]
    essentially,  run  the  o(n5) cky
    remember  only  a  few  hypotheses  for  

each  span  <i,j>.

    if  we  keep  k  hypotheses  at  each  span,  

then  we  do  at  most  o(nk2)  work  per  
span  (why?)

    keeps  things  more  or  less  cubic  (and  in  

practice  is  more  like  linear!)

    also:  certain  spans  are  forbidden  

entirely  on  the  basis  of  punctuation  
(crucial  for  speed)

x[h]

y[h] z[h   ]

i           h          k         h             j

34

pruning  with  a  pid18

    the  charniak parser  prunes  using  a  two   pass,  coarse   

to   fine  approach  [charniak 97+]
    first,  parse  with  the  base  grammar
    for  each  x:[i,j]  calculate  p(x|i,j,s)

    this  isn   t  trivial,  and  there  are  clever  speed  ups

    second,  do  the  full  o(n5) cky

    skip  any  x  :[i,j]  which  had  low  (say,  <  0.0001)  posterior

    avoids  almost  all  work  in  the  second  phase!

    charniak et  al  06:  can  use  more  passes
    petrov et  al  07:  can  use  many  more  passes

35

results

    some  results

    collins  99      88.6  f1  (generative  lexical)
    charniak  and  johnson  05      89.7  /  91.3  f1  (generative  

lexical  /  reranked)

    petrov  et  al  06      90.7  f1  (generative  unlexical)
    mcclosky  et  al  06      92.1  f1  (gen  +  rerank  +  self   train)

    however

    bilexical  counts  rarely  make  a  difference  (why?)
    gildea  01      removing  bilexical  counts  costs  <  0.5  f1

36

latent  variable  pid18s

37

the  game  of  designing  a  grammar

    annotation  refines  base  treebank symbols  to  improve  

statistical  fit  of  the  grammar
    parent  annotation  [johnson     98]
    head  lexicalization [collins     99,  charniak    00]
    automatic  id91?

38

latent  variable  grammars

...

parse tree 
sentence

derivations

parameters 

39

learning  latent  annotations

em  algorithm:

    brackets are known
    base categories are known
    only induce subcategories

just  like  forward   backward  for  id48s.

forward

x1

x4

x5

x6

right
was
backward

x7

.

x2
x3

he

40

refinement  of  the  dt  tag

dt

dt-1

dt-2

dt-3

dt-4

41

hierarchical  refinement

42

hierarchical  estimation  results

)
1
f
(
 
y
c
a
r
u
c
c
a
 
g
n

i

s
r
a
p

90

88

86

84

82

80

78

76

74

100

300

500

900

700

f1
model
1300
1500
87.3
flat training
total number of grammar symbols
hierarchical training 88.4

1100

1700

43

refinement  of  the  ,  tag

    splitting  all  categories  equally  is  wasteful:

44

adaptive splitting

    want to split complex categories more
    idea: split everything, roll back splits which 

were least useful

45

adaptive  splitting  results

f1
model
88.4
previous
with 50% merging 89.5

46

number  of  phrasal  subcategories

40

35

30

25

20

15

10

5

0

p
n

p
v

p
p

p s
v
d
a

p
j
d
a

r
a
b
s

p
q

p
n
h
w

n
r
p

x
n

v
n
s

i

t
r
p

q
s

p
p
h
w

p
j
n
o
c

g
a
r
f

c
a
n

p
c
u

p
v
d
a
h
w

j
t
n

i

c
r
r

q
r
a
b
s

p x
j
d
a
h
w

t
o
o
r

t
s
l

47

number  of  lexical  subcategories

70

60

50

40

30

20

10

0

j
pj
n
n

s
n
n

n
n

n
b
v

b
r

b
v

g
b
v

d
b
v

n

i

d
c

z
b
v

p
b
v

t
d

c
c

r
j
j

s:
j
j

s
p
n
n

p
r
p

$
p
r
p

d
m

r
b
r

p
w

s
o
p

t
d
p

b
r
w

b
r
l
-

x
e

$
p
w

t
d
w

b
r
r

-

-.

'
-'

w
f

s
b
r

o$
t

h,
u

`
`

p
r

s#
l

m
y
s

48

learned  splits

    proper nouns (nnp):
oct.
john

nnp-14
nnp-12
nnp-2
nnp-1
nnp-15
nnp-3

nov.
robert

e.

noriega

san

francisco

j.

bush
new
york

    personal pronouns (prp):

prp-0
prp-1
prp-2

it
it
it

he
he
them

sept.
james

l.

peters
wall
street

i

they
him

49

learned  splits

rbr-0
rbr-1
rbr-2

    relative  adverbs  (rbr):
further
more
earlier
    cardinal  numbers  (cd):
one
1989
million

cd-7
cd-4
cd-11
cd-0
cd-3
cd-9

1
1
78

lower
less
earlier

two
1990
billion

50
30
58

higher
more
later

three
1988
trillion
100
31
34

50

final  results  (accuracy)

e
n
g

g
e
r

c
h
n

charniak&johnson    05 (generative)

split / merge

dubey    05
split / merge

chiang et al.    02
split / merge

    40 words

f1
90.1
90.6

76.3
80.8

80.0
86.3

all 
f1
89.6
90.1

-

80.1

76.6
83.4

still higher numbers from reranking / self-training methods

51

efficient  parsing  for

hierarchical  grammars

52

coarse   to   fine  id136

    example:  pp  attachment

?????????

53

hierarchical  pruning

coarse:

    qp np vp    

split in two:

    qp1 qp2 np1 np2 vp1 vp2    

split in four:

    qp1 qp1 qp3 qp4 np1 np2 np3 np4 vp1 vp2 vp3 vp4    

split in eight:                                                                    

54

bracket  posteriors

55

1621  min
111  min
35  min
15  min

(no  search  error)

56

