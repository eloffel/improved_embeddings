natural  language  processing

classification  ii
dan  klein      uc  berkeley

1

classification

2

linear  models:  id88

    the  id88  algorithm

    iteratively  processes  the  training  set,  reacting  to  training  errors
    can  be  thought  of  as  trying  to  drive  down  training  error

    the  (online)  id88  algorithm:

    start  with  zero  weights  w
    visit  training  instances  one  by  one

    try  to  classify

    if  correct,  no  change!
    if  wrong:  adjust  weights

3

issues  with  id88s

    overtraining:  test  /  held   out  accuracy  

usually  rises,  then  falls
    overtraining  isn   t  the  typically  discussed  

source  of  overfitting,  but  it  can  be  
important

    id173:  if  the  data  isn   t  

separable,  weights  often  thrash  around
    averaging  weight  vectors  over  time  can  

help  (averaged  id88)

    [freund    &  schapire 99,  collins  02]

    mediocre  generalization:  finds  a     barely     

separating  solution

4

problems  with  id88s

    id88     goal   :  separate  the  training  data

1. this may be an entire 
feasible space

2. or it may be impossible

5

margin

6

objective  functions

    what  do  we  want  from  our  weights?

    depends!
    so  far:  minimize  (training)  errors:

    this  is  the     zero   one  loss   

    discontinuous,  minimizing  is  np   complete
    not  really  what  we  want  anyway

    maximum  id178  and  id166s  have  other  

objectives  related  to  zero   one  loss

7

linear  separators

    which  of  these  linear  separators  is  optimal?  

8

8

classification  margin  (binary)

    distance  of  xi to  separator  is  its  margin,  mi
    examples  closest  to  the  hyperplane  are  support  vectors
    margin     of  the  separator  is  the  minimum  m

   

m

9

classification  margin

    for  each  example  xi and  possible  mistaken  candidate  y,  we  avoid  

that  mistake  by  a  margin  mi(y) (with  zero   one  loss)

    margin      of  the  entire  separator  is  the  minimum  m

    it  is  also  the  largest      for  which  the  following  constraints  hold

10

maximum  margin
    separable  id166s:  find  the  max   margin  w

    can  stick  this  into  matlab and  (slowly)  get  an  id166
    won   t  work  (well)  if  non   separable

11

why  max  margin?

    why  do  this?    various  arguments:

    solution  depends  only  on  the  boundary  cases,  or  support  vectors (but  

remember  how  this  diagram  is  broken!)

    solution  robust  to  movement  of  support  vectors
    sparse  solutions  (features  not  in  support  vectors  get  zero  weight)
    generalization  bound  arguments
    works  well  in  practice  for  many  problems

support vectors

12

max  margin  /  small  norm

    reformulation:  find  the  smallest  w  which  separates  data

remember this 
condition?

        scales  linearly  in  w,  so  if  ||w||  isn   t  constrained,  we  can  

take  any  separating  w  and  scale  up  our  margin

    instead  of  fixing  the  scale  of  w,  we  can  fix      =  1

13

soft  margin  classification    

    what  if  the  training  set  is  not  linearly  separable?
    slack  variables   i can  be  added  to  allow  misclassification  of  difficult  or  

noisy  examples,  resulting  in  a  soft  margin classifier

  i

  i

14

maximum  margin

note: exist other 
choices of how to 
penalize slacks!

    non   separable  id166s

    add  slack  to  the  constraints
    make  objective  pay  (linearly)  for  slack:

    c  is  called  the  capacity of  the  id166      the  smoothing  

knob

    learning:

    can  still  stick  this  into  matlab  if  you  want
    constrained  optimization  is  hard;  better  methods!
    we   ll  come  back  to  this  later

15

maximum  margin

16

likelihood

17

linear  models:  maximum  id178

    maximum  id178  (logistic  regression)

    use  the  scores  as  probabilities:

    maximize  the  (log)  conditional  likelihood  of  training  data

make 
positive
normalize

18

maximum  id178  ii

    motivation  for  maximum  id178:

    connection  to  maximum  id178  principle  (sort  of)
    might  want  to  do  a  good  job  of  being  uncertain  on  noisy  

cases   

         in  practice,  though,  posteriors  are  pretty  peaked

    id173  (smoothing)

19

maximum  id178

20

loss  comparison

21

log   loss

    if  we  view  maxent  as  a  minimization  problem:

    this  minimizes  the     log  loss     on  each  example

    one  view:  log  loss  is  an  upper  bound on  zero   one  loss

22

remember  id166s   

    we  had  a  constrained minimization

       but  we  can  solve  for     i

    giving

23

hinge  loss

plot really only right 
in binary case

    consider the per-instance objective:

    this  is  called  the     hinge  loss   

    unlike  maxent /  log  loss,  you  stop  

gaining  objective  once  the  true  label  
wins  by  enough

    you  can  start  from  here  and  derive  the  

id166  objective

    can  solve  directly  with  sub   gradient  

decent  (e.g.  pegasos:  shalev   shwartz et  
al  07)

24

max  vs     soft   max     margin

    id166s:

    maxent:

you can make this zero

    but not this one

    very  similar!    both  try  to  make  the  true  score  better  

than  a  function  of  the  other  scores
    the  id166  tries  to  beat  the  augmented  runner   up
    the  maxent  classifier  tries  to  beat  the     soft   max   

25

loss  functions:  comparison

    zero   one  loss

    hinge

    log

26

separators:  comparison

27

conditional  vs
joint  likelihood

28

example:  sensors

reality

raining

sunny

p(+,+,r) = 3/8

p(-,-,r) = 1/8

p(+,+,s) = 1/8

p(-,-,s) = 3/8

nb model

raining?

m1

m2

nb  factors:
    p(s) =  1/2  
    p(+|s)  =  1/4  
    p(+|r)  =  3/4

predictions:
    p(r,+,+) = (  )(  )(  )
    p(s,+,+) = (  )(  )(  )
    p(r|+,+) = 9/10
    p(s|+,+) = 1/10

29

example:  stoplights

reality

lights working

lights broken

p(g,r,w) = 3/7

p(r,g,w) = 3/7

p(r,r,b) = 1/7

nb model

working?

ns

ew

nb  factors:
    p(w)  =  6/7  
    p(r|w)  =  1/2  
    p(g|w)  =  1/2

    p(b) = 1/7 
    p(r|b) = 1 
    p(g|b) = 0

30

example:  stoplights

    what  does  the  model  say  when  both  lights  are  red?

    p(b,r,r) =  (1/7)(1)(1)  
    p(w,r,r) =  (6/7)(1/2)(1/2)  
    p(w|r,r)  =  6/10!

=  1/7  
=  6/28

=  4/28
=  6/28

    we   ll  guess  that  (r,r)  indicates  lights  are  working!

    imagine  if  p(b)  were  boosted  higher,  to  1/2:

    p(b,r,r) =  (1/2)(1)(1)  
    p(w,r,r) =  (1/2)(1/2)(1/2)  
    p(w|r,r)  =  1/5!

=  1/2
=  1/8

=  4/8
=  1/8

    changing  the  parameters  bought  accuracy  at  the  

expense  of  data  likelihood

31

duals  and  kernels

32

nearest   neighbor  classification

    nearest  neighbor,  e.g.  for  digits:

    take  new  example
    compare  to  all  training  examples
    assign  based  on  closest  example

    encoding:  image  is  vector  of  intensities:

    similarity  function:

    e.g.  dot  product  of  two  images     vectors

33

non   parametric  classification

    non   parametric:  more  examples  means  

(potentially)  more  complex  classifiers

    how  about  k   nearest  neighbor?

    we  can  be  a  little  more  sophisticated,  averaging  

several  neighbors

    but,  it   s  still  not  really  error   driven  learning
    the  magic  is  in  the  distance  function

    overall:  we  can  exploit  rich  similarity  

functions,  but  not  objective   driven  learning

34

a  tale  of  two  approaches   

    nearest  neighbor   like  approaches

    work  with  data  through  similarity  functions
    no  explicit     learning   

    linear  approaches

    explicit  training  to  reduce  empirical  error
    represent  data  through  features

    kernelized  linear  models

    explicit  training,  but  driven  by  similarity!
    flexible,  powerful,  very  very  slow

35

the  id88,  again

    start  with  zero  weights
    visit  training  instances  one  by  one

    try  to  classify

    if  correct,  no  change!
    if  wrong:  adjust  weights

mistake vectors

36

id88  weights

    what  is  the  final  value  of  w?

    can  it  be  an  arbitrary  real  vector?
    no!    it   s  built  by  adding  up  feature  vectors  (mistake  vectors).

mistake counts

    can  reconstruct  weight  vectors  (the  primal  representation)  from  

update  counts  (the  dual  representation)  for  each  i

37

dual  id88

    track  mistake  counts  rather  than  weights

    start  with  zero  counts  (   )
    for  each  instance  x

    try  to  classify

    if  correct,  no  change!
    if  wrong:  raise  the  mistake  count  for  this  example  and  prediction

38

dual  /  kernelized  id88

    how  to  classify  an  example  x?

    if  someone  tells  us  the  value  of  k  for  each  pair  of  candidates,  

never  need  to  build  the  weight  vectors

39

issues  with  dual  id88

    problem:  to  score  each  candidate,  we  may  have  to  compare  

to  all training  candidates

    very,  very  slow  compared  to  primal  dot  product!
    one  bright  spot:  for  id88,  only  need  to  consider  candidates  we  

made  mistakes  on  during  training

    slightly  better  for  id166s  where  the  alphas  are  (in  theory)  sparse

    this  problem  is  serious:  fully  dual  methods  (including  kernel  

methods)  tend  to  be  extraordinarily  slow

    of  course,  we  can  (so  far)  also  accumulate  our  weights  as  we  

go...

40

kernels:  who  cares?

    so  far:  a  very  strange  way  of  doing  a  very  simple  

calculation

       kernel  trick   :  we  can  substitute  any* similarity  

function  in  place  of  the  dot  product

    lets  us  learn  new  kinds  of  hypotheses

* fine print: if your kernel doesn   t satisfy certain 
technical requirements, lots of proofs break.  
e.g. convergence, mistake bounds.  in practice, 
illegal kernels sometimes work (but not always).

41

some  kernels

    kernels  implicitly map  original  vectors  to  higher  dimensional  
spaces,  take  the  dot  product  there,  and  hand  the  result  back

    linear  kernel:

    quadratic  kernel:

    rbf:  infinite  dimensional  representation

    discrete  kernels:  e.g.  string  kernels,  tree  kernels

42

tree  kernels

[collins and 
duffy 01]

    want  to  compute  number  of  common  subtrees  between  t,  t   
    add  up  counts  of  all  pairs  of  nodes  n,  n   

    base:  if  n,  n     have  different  root  productions,  or  are  depth  0:

    base:  if  n,  n     are  share  the  same  root  production:

44

43

dual  formulation  for  id166s

    we  want  to  optimize:  (separable  case  for  now)

    this  is  hard  because  of  the  constraints
    solution:  method  of  lagrange  multipliers
    the  lagrangian representation  of  this  problem  is:

    all  we   ve  done  is  express  the  constraints  as  an  adversary  which  leaves  our  

objective  alone  if  we  obey  the  constraints  but  ruins  our  objective  if  we  
violate  any  of  them

44

lagrange  duality

    we  start  out  with  a  constrained  optimization  problem:

    we  form  the  lagrangian:

    this  is  useful  because  the  constrained  solution  is  a  saddle  

point  of      (this  is  a  general  property):

primal problem in w

dual problem in    

45

dual  formulation  ii

    duality  tells  us  that

has  the  same  value  as

    this  is  useful  because  if  we  think  of  the        s  as  constants,  we  have  an  

unconstrained  min  in  w  that  we  can  solve  analytically.

    then  we  end  up  with  an  optimization  over      instead  of  w (easier).

46

dual  formulation  iii

    minimize  the  lagrangian  for  fixed        s:

    so  we  have  the  lagrangian  as  a  function  of  only        s:

47

back  to  learning  id166s

    we  want  to  find      which  minimize

    this  is  a  quadratic  program:

    can  be  solved  with  general  qp  or  convex  optimizers
    but  they  don   t  scale  well  to  large  problems
    cf.  maxent  models  work  fine  with  general  optimizers  (e.g.  

cg,  l   bfgs)

    how  would  a  special  purpose  optimizer  work?

48

coordinate  descent  i

    despite  all  the  mess,  z is  just  a  quadratic  in  each     i(y)
    coordinate  descent:  optimize  one  variable  at  a  time

0

0

    if  the  unconstrained  argmin  on  a  coordinate  is  negative,  just  

clip  to  zero   

49

coordinate  descent  ii

    ordinarily,  treating  coordinates  independently  is  a  bad  idea,  but  here  the  

update  is  very  fast  and  simple

    so  we  visit  each  axis  many  times,  but  each  visit  is  quick

    this  approach  works  fine  for  the  separable  case
    for  the  non   separable  case,  we  just  gain  a  simplex  constraint  and  so  we  

need  slightly  more  complex  methods  (smo,  exponentiated  gradient)

50

what  are  the  alphas?

    each  candidate  corresponds  to  a  primal  

constraint

    in  the  solution,  an     i(y) will  be:
    zero  if  that  constraint  is  inactive
    positive  if  that  constrain  is  active
    i.e.  positive  on  the  support  vectors

    support  vectors  contribute  to  weights:

support vectors

51

structure

52

handwriting  recognition

x

y

brace

sequential structure

[slides:  taskar and  klein  05]

53

id18  parsing

x

y

the screen was 

a sea of red

recursive structure

54

bilingual  word  alignment

x

y

what is the anticipated 
cost of collecting fees 
under the new proposal?

en vertu de nouvelle 
propositions, quel est le 
c  ut pr  vu de perception 
de les droits?

what
is 
the
anticipated
cost
of
collecting 
fees 
under 
the 
new 
proposal
?
combinatorial structure

en 
vertu 
de
les
nouvelle 
propositions
, 
quel 
est 
le 
c  ut 
pr  vu 
de 
perception 
de 
le 
droits
?

55

structured  models

space of feasible outputs

assumption:

score  is  a  sum  of  local     part     scores
parts  =  nodes,  edges,  productions

56

id18  parsing

#(np     dt nn)

   

#(pp     in np)

   

#(nn        sea   )

57

bilingual  word  alignment

what
is 
the
anticipated
cost
of
collecting 
fees 
under 
the 
new 
proposal
?

j

k

en 
vertu 
de
les
nouvelle 
propositions
, 
quel 
est 
le 
c  ut 
pr  vu 
de 
perception 
de 
le 
droits
?

    association
    position
    orthography

58

option  0:  reranking

input

n-best list
(e.g. n=100)

[e.g. 
charniak and 
johnson 05]

output

x = 

   the screen was a sea of red.   

baseline 
parser

non-structured 
classification

   

60

59

reranking

    advantages:

    directly  reduce  to  non   structured  case
    no  locality  restriction  on  features

    disadvantages:

    stuck  with  errors  of  baseline  parser
    baseline  system  must  produce  n   best  lists
    but,  feedback  is  possible  [mccloskey,  charniak,  johnson  2006]

60

efficient  primal  decoding

    common  case:  you  have  a  black  box  which  computes

at  least  approximately,  and  you  want  to  learn  w

    many  learning  methods  require  more  (expectations,  dual  representations,  

k   best  lists),  but  the  most  commonly  used  options  do  not

    easiest  option  is  the  structured  id88  [collins  01]

    structure  enters  here  in  that  the  search  for  the  best  y  is  typically  a  

combinatorial  algorithm  (dynamic  programming,  matchings,  ilps,  a*   )

    prediction  is  structured,  learning  update  is  not

62

61

structured  margin

    remember  the  margin  objective:

    this  is  still  defined,  but  lots  of  constraints

62

full  margin:  ocr

    we  want:

    equivalently:

   brace   

   brace   

   brace   

   

   brace   

   aaaaa   

   aaaab   

   zzzzz   

a lot!

63

parsing  example

   it was red   

s
a b

c d

    we  want:

    equivalently:

   it was red   

s
a b

c d

   it was red   

s
a b

c d

   

   it was red   

s
a b

c d

   it was red   

   it was red   

   it was red   

s
a b
d f

s
a b
c d

s
e f
g h

a lot!

64

alignment  example

    we  want:

    equivalently:

   what is the   
   quel est le   

1
2
3

1
2
3

   what is the   
   quel est le   

   what is the   
   quel est le   

   what is the   
   quel est le   

1
2
3
1
2
3

1
2
3

1
2
3
1
2
3

   

1
2
3

   what is the   
   quel est le   

   what is the   
   quel est le   

   what is the   
   quel est le   

1
2
3

1
2
3

1
2
3

1
2
3

1
2
3

1
2
3

a lot!

65

cutting  plane

    a  constraint  induction  method  [joachims  et  al  09]

    exploits  that  the  number  of  constraints  you  actually  need  per  instance  

is  typically  very  small

    requires  (loss   augmented)  primal   decode  only

    repeat:

    find  the  most  violated  constraint  for  an  instance:

    add  this  constraint  and  resolve  the  (non   structured)  qp  (e.g.  with  

smo  or  other  qp  solver)

66

cutting  plane

    some  issues:

    can  easily  spend  too  much  time  solving  qps
    doesn   t  exploit  shared  constraint  structure
    in  practice,  works  pretty  well;  fast  like  mira,  more  stable,  

no  averaging

67

m3ns

    another  option:  express  all  constraints  in  a  packed  form

    maximum  margin  markov  networks  [taskar  et  al  03]
    integrates  solution  structure  deeply  into  the  problem  structure

    steps

    express  id136  over  constraints  as  an  lp
    use  duality  to  transform  minimax  formulation  into  min   min
    constraints  factor  in  the  dual  along  the  same  structure  as  the  primal;  

alphas  essentially  act  as  a  dual     distribution   
    various  optimization  possibilities  in  the  dual

68

likelihood,  structured

    structure  needed  to  compute:

    log   normalizer
    expected  feature  counts

    e.g.  if  a  feature  is  an  indicator  of  dt   nn  then  we  need  to  compute  posterior  

marginals  p(dt   nn|sentence)  for  each  position  and  sum  

    also  works  with  latent  variables  (more  later)

69

