natural  language  processing

classification  iii
dan  klein      uc  berkeley

1

classification

2

linear  models:  id88

    the  id88  algorithm

    iteratively  processes  the  training  set,  reacting  to  training  errors
    can  be  thought  of  as  trying  to  drive  down  training  error

    the  (online)  id88  algorithm:

    start  with  zero  weights  w
    visit  training  instances  one  by  one

    try  to  classify

    if  correct,  no  change!
    if  wrong:  adjust  weights

3

duals  and  kernels

4

nearest   neighbor  classification

    nearest  neighbor,  e.g.  for  digits:

    take  new  example
    compare  to  all  training  examples
    assign  based  on  closest  example

    encoding:  image  is  vector  of  intensities:

    similarity  function:

    e.g.  dot  product  of  two  images     vectors

5

non   parametric  classification

    non   parametric:  more  examples  means  

(potentially)  more  complex  classifiers

    how  about  k   nearest  neighbor?

    we  can  be  a  little  more  sophisticated,  averaging  

several  neighbors

    but,  it   s  still  not  really  error   driven  learning
    the  magic  is  in  the  distance  function

    overall:  we  can  exploit  rich  similarity  

functions,  but  not  objective   driven  learning

6

a  tale  of  two  approaches   

    nearest  neighbor   like  approaches

    work  with  data  through  similarity  functions
    no  explicit     learning   

    linear  approaches

    explicit  training  to  reduce  empirical  error
    represent  data  through  features

    kernelized  linear  models

    explicit  training,  but  driven  by  similarity!
    flexible,  powerful,  very  very  slow

7

the  id88,  again

    start  with  zero  weights
    visit  training  instances  one  by  one

    try  to  classify

    if  correct,  no  change!
    if  wrong:  adjust  weights

mistake vectors

8

id88  weights

    what  is  the  final  value  of  w?

    can  it  be  an  arbitrary  real  vector?
    no!    it   s  built  by  adding  up  feature  vectors  (mistake  vectors).

mistake counts

    can  reconstruct  weight  vectors  (the  primal  representation)  from  

update  counts  (the  dual  representation)  for  each  i

9

dual  id88

    track  mistake  counts  rather  than  weights

    start  with  zero  counts  (   )
    for  each  instance  x

    try  to  classify

    if  correct,  no  change!
    if  wrong:  raise  the  mistake  count  for  this  example  and  prediction

10

dual  /  kernelized  id88

    how  to  classify  an  example  x?

    if  someone  tells  us  the  value  of  k  for  each  pair  of  candidates,  

never  need  to  build  the  weight  vectors

11

issues  with  dual  id88

    problem:  to  score  each  candidate,  we  may  have  to  compare  

to  all training  candidates

    very,  very  slow  compared  to  primal  dot  product!
    one  bright  spot:  for  id88,  only  need  to  consider  candidates  we  

made  mistakes  on  during  training

    slightly  better  for  id166s  where  the  alphas  are  (in  theory)  sparse

    this  problem  is  serious:  fully  dual  methods  (including  kernel  

methods)  tend  to  be  extraordinarily  slow

    of  course,  we  can  (so  far)  also  accumulate  our  weights  as  we  

go...

12

kernels:  who  cares?

    so  far:  a  very  strange  way  of  doing  a  very  simple  

calculation

       kernel  trick   :  we  can  substitute  any* similarity  

function  in  place  of  the  dot  product

    lets  us  learn  new  kinds  of  hypotheses

* fine print: if your kernel doesn   t satisfy certain 
technical requirements, lots of proofs break.  
e.g. convergence, mistake bounds.  in practice, 
illegal kernels sometimes work (but not always).

13

some  kernels

    kernels  implicitly map  original  vectors  to  higher  dimensional  
spaces,  take  the  dot  product  there,  and  hand  the  result  back

    linear  kernel:

    quadratic  kernel:

    rbf:  infinite  dimensional  representation

    discrete  kernels:  e.g.  string  kernels,  tree  kernels

14

tree  kernels

[collins and 
duffy 01]

    want  to  compute  number  of  common  subtrees  between  t,  t   
    add  up  counts  of  all  pairs  of  nodes  n,  n   

    base:  if  n,  n     have  different  root  productions,  or  are  depth  0:

    base:  if  n,  n     are  share  the  same  root  production:

15

dual  formulation  for  id166s

    we  want  to  optimize:  (separable  case  for  now)

    this  is  hard  because  of  the  constraints
    solution:  method  of  lagrange  multipliers
    the  lagrangian representation  of  this  problem  is:

    all  we   ve  done  is  express  the  constraints  as  an  adversary  which  leaves  our  

objective  alone  if  we  obey  the  constraints  but  ruins  our  objective  if  we  
violate  any  of  them

16

lagrange  duality

    we  start  out  with  a  constrained  optimization  problem:

    we  form  the  lagrangian:

    this  is  useful  because  the  constrained  solution  is  a  saddle  

point  of      (this  is  a  general  property):

primal problem in w

dual problem in    

17

dual  formulation  ii

    duality  tells  us  that

has  the  same  value  as

    this  is  useful  because  if  we  think  of  the        s  as  constants,  we  have  an  

unconstrained  min  in  w  that  we  can  solve  analytically.

    then  we  end  up  with  an  optimization  over      instead  of  w (easier).

18

dual  formulation  iii

    minimize  the  lagrangian  for  fixed        s:

    so  we  have  the  lagrangian  as  a  function  of  only        s:

19

back  to  learning  id166s

    we  want  to  find      which  minimize

    this  is  a  quadratic  program:

    can  be  solved  with  general  qp  or  convex  optimizers
    but  they  don   t  scale  well  to  large  problems
    cf.  maxent  models  work  fine  with  general  optimizers  (e.g.  

cg,  l   bfgs)

    how  would  a  special  purpose  optimizer  work?

20

coordinate  descent  i

    despite  all  the  mess,  z is  just  a  quadratic  in  each     i(y)
    coordinate  descent:  optimize  one  variable  at  a  time

0

0

    if  the  unconstrained  argmin  on  a  coordinate  is  negative,  just  

clip  to  zero   

21

coordinate  descent  ii

    ordinarily,  treating  coordinates  independently  is  a  bad  idea,  but  here  the  

update  is  very  fast  and  simple

    so  we  visit  each  axis  many  times,  but  each  visit  is  quick

    this  approach  works  fine  for  the  separable  case
    for  the  non   separable  case,  we  just  gain  a  simplex  constraint  and  so  we  

need  slightly  more  complex  methods  (smo,  exponentiated  gradient)

22

what  are  the  alphas?

    each  candidate  corresponds  to  a  primal  

constraint

    in  the  solution,  an     i(y) will  be:
    zero  if  that  constraint  is  inactive
    positive  if  that  constrain  is  active
    i.e.  positive  on  the  support  vectors

    support  vectors  contribute  to  weights:

support vectors

23

structure

24

handwriting  recognition

x

y

brace

sequential structure

[slides:  taskar and  klein  05]

25

id18  parsing

x

y

the screen was 

a sea of red

recursive structure

26

bilingual  word  alignment

x

y

what is the anticipated 
cost of collecting fees 
under the new proposal?

en vertu de nouvelle 
propositions, quel est le 
c  ut pr  vu de perception 
de les droits?

what
is 
the
anticipated
cost
of
collecting 
fees 
under 
the 
new 
proposal
?
combinatorial structure

en 
vertu 
de
les
nouvelle 
propositions
, 
quel 
est 
le 
c  ut 
pr  vu 
de 
perception 
de 
le 
droits
?

27

structured  models

space of feasible outputs

assumption:

score  is  a  sum  of  local     part     scores
parts  =  nodes,  edges,  productions

28

id18  parsing

#(np     dt nn)

   

#(pp     in np)

   

#(nn        sea   )

29

bilingual  word  alignment

what
is 
the
anticipated
cost
of
collecting 
fees 
under 
the 
new 
proposal
?

j

k

en 
vertu 
de
les
nouvelle 
propositions
, 
quel 
est 
le 
c  ut 
pr  vu 
de 
perception 
de 
le 
droits
?

    association
    position
    orthography

30

option  0:  reranking

input

n-best list
(e.g. n=100)

[e.g. 
charniak and 
johnson 05]

output

x = 

   the screen was a sea of red.   

baseline 
parser

non-structured 
classification

   

31

reranking

    advantages:

    directly  reduce  to  non   structured  case
    no  locality  restriction  on  features

    disadvantages:

    stuck  with  errors  of  baseline  parser
    baseline  system  must  produce  n   best  lists
    but,  feedback  is  possible  [mccloskey,  charniak,  johnson  2006]

32

efficient  primal  decoding

    common  case:  you  have  a  black  box  which  computes

at  least  approximately,  and  you  want  to  learn  w

    many  learning  methods  require  more  (expectations,  dual  representations,  

k   best  lists),  but  the  most  commonly  used  options  do  not

    easiest  option  is  the  structured  id88  [collins  01]

    structure  enters  here  in  that  the  search  for  the  best  y  is  typically  a  

combinatorial  algorithm  (dynamic  programming,  matchings,  ilps,  a*   )

    prediction  is  structured,  learning  update  is  not

33

structured  margin

    remember  the  margin  objective:

    this  is  still  defined,  but  lots  of  constraints

34

full  margin:  ocr

    we  want:

    equivalently:

   brace   

   brace   

   brace   

   

   brace   

   aaaaa   

   aaaab   

   zzzzz   

a lot!

35

parsing  example

   it was red   

s
a b

c d

    we  want:

    equivalently:

   it was red   

s
a b

c d

   it was red   

s
a b

c d

   

   it was red   

s
a b

c d

   it was red   

   it was red   

   it was red   

s
a b
d f

s
a b
c d

s
e f
g h

a lot!

36

alignment  example

    we  want:

    equivalently:

   what is the   
   quel est le   

1
2
3

1
2
3

   what is the   
   quel est le   

   what is the   
   quel est le   

   what is the   
   quel est le   

1
2
3
1
2
3

1
2
3

1
2
3
1
2
3

   

1
2
3

   what is the   
   quel est le   

   what is the   
   quel est le   

   what is the   
   quel est le   

1
2
3

1
2
3

1
2
3

1
2
3

1
2
3

1
2
3

a lot!

37

cutting  plane

    a  constraint  induction  method  [joachims  et  al  09]

    exploits  that  the  number  of  constraints  you  actually  need  per  instance  

is  typically  very  small

    requires  (loss   augmented)  primal   decode  only

    repeat:

    find  the  most  violated  constraint  for  an  instance:

    add  this  constraint  and  resolve  the  (non   structured)  qp  (e.g.  with  

smo  or  other  qp  solver)

38

cutting  plane

    some  issues:

    can  easily  spend  too  much  time  solving  qps
    doesn   t  exploit  shared  constraint  structure
    in  practice,  works  pretty  well;  fast  like  id88/mira,  

more  stable,  no  averaging

39

m3ns

    another  option:  express  all  constraints  in  a  packed  form

    maximum  margin  markov  networks  [taskar et  al  03]
    integrates  solution  structure  deeply  into  the  problem  structure

    steps

    express  id136  over  constraints  as  an  lp
    use  duality  to  transform  minimax formulation  into  min   min
    constraints  factor  in  the  dual  along  the  same  structure  as  the  primal;  

alphas  essentially  act  as  a  dual     distribution   
    various  optimization  possibilities  in  the  dual

40

likelihood,  structured

    structure  needed  to  compute:

    log   normalizer
    expected  feature  counts

    e.g.  if  a  feature  is  an  indicator  of  dt   nn  then  we  need  to  compute  posterior  

marginals  p(dt   nn|sentence)  for  each  position  and  sum  

    also  works  with  latent  variables  (more  later)

41

