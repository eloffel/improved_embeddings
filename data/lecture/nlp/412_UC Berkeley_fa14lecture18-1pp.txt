natural  language  processing

machine  translation

dan  klein      uc  berkeley

1

machine  translation

2

machine  translation:  examples

3

levels  of  transfer

4

word   level  mt:  examples

la  politique  de  la  haine  .  

   
    politics  of  hate  .  
   

the  policy  of  the  hatred  .  

nous  avons  sign    le  protocole  .  

   
    we  did  sign  the  memorandum  of  agreement  .  
    we  have  signed  the  protocol  .  

o      tait  le  plan  solide  ?  

   
    but  where  was  the  solid  plan  ?  
    where  was  the  economic  base  ?  

(foreign original)
(reference  translation)
(ibm4+n   grams+stack)

(foreign original)
(reference  translation)
(ibm4+n   grams+stack)

(foreign original)
(reference  translation)
(ibm4+n   grams+stack)

5

phrasal  mt:  examples

6

metrics

7

mt:  evaluation

    human  evaluations:  subject  measures,  

fluency/adequacy

    automatic  measures:  n   gram  match  to  references

    nist  measure:  n   gram  recall  (worked  poorly)
    id7:  n   gram  precision  (no  one  really  likes  it,  but  

everyone  uses  it)
lots  more:  ter,  hter,  meteor,       

   

    id7:

    p1  =  unigram  precision
    p2,  p3,  p4  =  bi   ,  tri   ,  4   gram  precision
    weighted  geometric  mean  of  p1   4
    brevity  penalty  (why?)
   
    magnitude  only  meaningful  on  same  language,  corpus,  

somewhat  hard  to  game   

number  of  references,  probably  only  within  system  
types   

8

automatic  metrics  work  (?)

9

systems  overview

10

corpus   based  mt

modeling correspondences between languages

sentence-aligned parallel corpus:

yo lo har   ma  ana
i will do it tomorrow

hasta pronto
see you soon

hasta pronto
see you around

machine translation system:

yo lo har   pronto

model of 
translation

i will do it soon

i will do it around

see you tomorrow

11

phrase   based  system  overview

sentence-aligned 

corpus

word alignments

cat ||| chat ||| 0.9 
the cat ||| le chat ||| 0.8
dog ||| chien ||| 0.8 
house ||| maison ||| 0.6 
my house ||| ma maison ||| 0.9
language ||| langue ||| 0.9 
   

phrase table

(translation model)

many slides and examples from philipp koehn or john denero

12

word  alignment

13

word  alignment

14

word  alignment

x

what is the anticipated 
cost of collecting fees 
under the new proposal?

en vertu des nouvelles 
propositions, quel est le 
co  t pr  vu de perception 
des droits?

z

what
is 
the
anticipated
cost
of
collecting 
fees 
under 
the 
new 
proposal
?

en 
vertu 
de
les
nouvelles 
propositions
, 
quel 
est 
le 
co  t 
pr  vu 
de 
perception 
de 
les 
droits
?

15

unsupervised  word  alignment

    input:  a  bitext:  pairs  of  translated  sentences

nous acceptons votre opinion .
we accept your view .

    output:  alignments:  pairs  of

translated  words
    when  words  have  unique
sources,  can  represent  as
a  (forward)  alignment
function  a from  french  to
english  positions

16

1   to   many  alignments

17

evaluating  models

    how  do  we  measure  quality  of  a  word   to   word  model?

    method  1:  use  in  an  end   to   end  translation  system

    hard  to  measure  translation  quality
    option:  human  judges
    option:  reference  translations  (nist,  id7)
    option:  combinations  (hter)
    actually,  no  one  uses  word   to   word  models  alone  as  tms

    method  2:  measure  quality  of  the  alignments  produced

    easy  to  measure
    hard  to  know  what  the  gold  alignments  should  be
    often  does  not  correlate  well  with  translation  quality  (like  perplexity  in  

lms)

18

alignment  error  rate

    alignment  error  rate

=
=
=  

sure align.
possible align.
predicted align.

19

ibm  model  1:  allocation

20

ibm  model  1  (brown  93)

    alignments:  a  hidden  vector  called  an  alignment specifies  which  english  

source  is  responsible  for  each  french  target  word.

21

ibm  models  1/2

1

2
thank you

3
,

4
i

5

6
shall do

8

7
so gladly

9
.

1

3

7

6

8

8

8

8

9

gracias ,

lo

har   de muy buen grado .

emissions:  p( f1 = gracias | ea1 = thank )

transitions:  p( a2 = 3)

model parameters

e:

a:

f:

22

problems  with  model  1

    there   s  a  reason  they  designed  

models  2   5!

    problems:  alignments  jump  

around,  align  everything  to  rare  
words

    experimental  setup:

    training  data:  1.1m  sentences  

of  french   english  text,  canadian  
hansards

    evaluation  metric:  alignment  

error  rate  (aer)

    evaluation  data:  447  hand   

aligned  sentences

23

intersected  model  1

    post   intersection:  standard  
practice  to  train  models  in  
each  direction  then  intersect  
their  predictions  [och  and  
ney,  03]

    second  model  is  basically  a  

filter  on  the  first
    precision  jumps,  recall  drops
    end  up  not  guessing  hard  

alignments

model
model 1 e   f
82/58
model 1 f   e 85/58
model 1 and
96/46

p/r aer
30.6
28.7
34.8

24

joint  training?

    overall:

    similar  high  precision  to  post   intersection
    but  recall  is  much  higher
    more  confident  about  positing  non   null  alignments

model
model 1 e   f
model 1 f   e
model 1 and
model 1 int

p/r
82/58
85/58
96/46
93/69

aer
30.6
28.7
34.8
19.5

25

ibm  model  2:  global  

monotonicity

26

monotonic  translation

japan shaken by two new quakes

le japon secou   par deux nouveaux s  ismes 

27

local  order  change

japan is at the junction of four tectonic plates

le japon est au confluent de quatre plaques tectoniques

28

ibm  model  2

    alignments  tend  to  the  diagonal  (broadly  at  least)

    other  schemes  for  biasing  alignments  towards  the  diagonal:

    relative  vs  absolute  alignment
    asymmetric  distances
    learning  a  full  multinomial  over  distances

29

em  for  models  1/2

    model  1  parameters:

translation  probabilities  (1+2)
distortion  parameters  (2  only)

    start  with  
    for  each  sentence:

uniform,  including

    for  each  french  position  j

    calculate  posterior  over  english  positions

    (or  just  use  best  single  alignment)
    increment  count  of  word  fj with  word  ei by  these  amounts
    also  re   estimate  distortion  probabilities  for  model  2

    iterate  until  convergence

30

example

31

id48  model:  local  

monotonicity

32

phrase  movement

on tuesday nov. 4, earthquakes rocked japan once again

des tremblements de terre ont    nouveau touch   le japon jeudi 4 novembre. 

33

the  id48  model

1

2
thank you

3
,

4
i

5

6
shall do

8

7
so gladly

9
.

1

3

7

6

8

8

8

8

9

gracias ,

lo

har   de muy buen grado .

emissions:  p( f1 = gracias | ea1 = thank )

transitions:  p( a2 = 3 | a1 = 1)

model parameters

e:

a:

f:

34

the  id48  model

    model  2  preferred  global  monotonicity
    we  want  local  monotonicity:

    most  jumps  are  small

    id48  model  (vogel  96)

    re   estimate  using  the  forward   backward  algorithm
    handling  nulls  requires  some  care

    what  are  we  still  missing?

-2 -1  0  1  2  3

35

id48  examples

36

aer  for  id48s

model
model 1 int
id48 e   f
id48 f   e
id48 and
id48 int
giza m4 and

aer
19.5
11.4
10.8
7.1
4.7
6.9

37

models  3,  4,  and  5:  fertility

38

ibm  models  3/4/5

mary did not slap the green witch

mary not slap slap slap the green witch 

n(3|slap)

mary not slap slap slap null the green witch

mary no daba una botefada a la verde bruja

mary no daba una botefada a la bruja verde

p(null)

t(la|the)

d(j|i)

[from al-onaizan and knight, 1998]

39

examples:  translation  and  fertility

40

example:  idioms

he is nodding

il hoche la t  te

41

example:  morphology

42

some  results

    [och  and  ney  03]

43

