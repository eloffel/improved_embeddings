natural  language  processing

parsing  i

dan  klein      uc  berkeley

1

syntax

2

parse  trees

the move followed a round of similar increases by other lenders, 

reflecting a continuing decline in that market

3

phrase  structure  parsing

    phrase  structure  parsing  

organizes  syntax  into  
constituents  or  brackets

    in  general,  this  involves  

nested  trees

    linguists  can,  and  do,  

argue  about  details

    lots  of  ambiguity

    not  the  only  kind  of  

syntax   

s

vp

np

np

n   

pp

np

new art critics write reviews with computers

4

constituency  tests

    how  do  we  know  what  nodes  go  in  the  tree?

    classic  constituency  tests:

    substitution  by  proform
    question  answers
    semantic  gounds

    coherence
    reference
    idioms

    dislocation
    conjunction

    cross   linguistic  arguments,  too

5

conflicting  tests

    constituency  isn   t  always  clear

    units  of  transfer:

    think  about  ~  penser    
    talk  about  ~  hablar  de

    phonological  reduction:

    i  will  go      i   ll  go
    i  want  to  go      i  wanna  go
    a  le  centre      au  centre

    coordination

    he  went  to  and  came  from  the  store.

la   v  locit    des ondes sismiques

6

classical  nlp:  parsing

    write  symbolic  or  logical  rules:

grammar (id18)

lexicon

root     s
s     np vp
np     dt nn
np     nn nns

np     np pp
vp     vbp np
vp     vbp np pp
pp     in np

nn     interest
nns     raises
vbp     interest
vbz     raises
   

    use  deduction  systems  to  prove  parses  from  words

    minimal  grammar  on     fed  raises     sentence:  36  parses
    simple  10   rule  grammar:  592  parses
    real   size  grammar:  many  millions  of  parses

    this  scaled  very  badly,  didn   t  yield  broad   coverage  tools

7

ambiguities

8

ambiguities:  pp  attachment

9

attachments

    i  cleaned  the  dishes  from  dinner

    i  cleaned  the  dishes  with  detergent

    i  cleaned  the  dishes  in  my  pajamas

    i  cleaned  the  dishes  in  the  sink

10

syntactic  ambiguities  i

    prepositional  phrases:

they  cooked  the  beans  in  the  pot  on  the  stove  with  handles.

    particle  vs.  preposition:

the  puppy  tore  up  the  staircase.

    complement  structures

the  tourists  objected  to  the  guide  that  they  couldn   t  hear.
she  knows  you  like  the  back  of  her  hand.

    gerund  vs.  participial  adjective
visiting  relatives  can  be  boring.
changing  schedules  frequently  confused  passengers.

11

syntactic  ambiguities  ii

    modifier  scope  within  nps

impractical  design  requirements
plastic  cup  holder

    multiple  gap  constructions
the  chicken  is  ready  to  eat.
the  contractors  are  rich  enough  to  sue.

    coordination  scope:

small  rats  and  mice  can  squeeze  into  holes  or  cracks  in  the  
wall.

12

dark  ambiguities

    dark  ambiguities: most  analyses  are  shockingly  bad  

(meaning,  they  don   t  have  an  interpretation  you  can  get  
your  mind  around)

this analysis corresponds 

to the correct parse of 
   this will panic buyers !    

    unknown  words  and  new  usages
    solution:  we  need  mechanisms  to  focus  attention  on  the  

best  ones,  probabilistic  techniques  do  this

13

pid18s

14

probabilistic  context   free  grammars

    a  context   free  grammar  is  a  tuple  <n,  t,  s,  r>

    n :  the  set  of  non   terminals

    phrasal  categories:  s,  np,  vp,  adjp,  etc.
    parts   of   speech  (pre   terminals):  nn,  jj,  dt,  vb

    t :  the  set  of  terminals  (the  words)
    s :  the  start  symbol

    often  written  as  root  or  top
    not  usually  the  sentence  non   terminal  s

    r :  the  set  of  rules

    of  the  form  x      y1 y2      yk,  with  x,  yi     n
    examples:  s      np  vp,      vp      vp  cc  vp
    also  called  rewrites,  productions,  or  local  trees

    a  pid18  adds:

    a  top   down  production  id203  per  rule  p(y1 y2      yk  |  x)

15

treebank  sentences

16

treebank  grammars

    need  a  pid18  for  broad  coverage  parsing.
    can  take  a  grammar  right  off  the  trees  (doesn   t  work  well):

root     s
s     np vp .
np     prp
vp     vbd adjp
   ..

1

1

1

1

    better  results  by  enriching  the  grammar  (e.g.,  lexicalization).
    can  also  get  reasonable  parsers  without  lexicalization.

17

treebank  grammar  scale

    treebank  grammars  can  be  enormous

    as  fsas,  the  raw  grammar  has  ~10k  states,  excluding  the  lexicon
    better  parsers  usually  make  the  grammars  larger,  not  smaller

np

det

adj

noun

det

noun

plural noun

np

np

pp

np

conj

18

chomsky  normal  form

    chomsky  normal  form:

    all  rules  of  the  form  x      y  z  or  x      w
    in  principle,  this  is  no  limitation  on  the  space  of  (p)id18s

    n   ary  rules  introduce  new  non   terminals

vp

vbd   np   pp   pp

[vp     vbd np pp    ]

vp

[vp     vbd np    ]
vbd            np

pp

pp

    unaries  /  empties  are     promoted   

    in  practice  it   s  kind  of  a  pain:

    reconstructing  n   aries  is  easy
    reconstructing  unaries  is  trickier
    the  straightforward  transformations  don   t  preserve  tree  scores

    makes  parsing  algorithms  simpler!

19

cky  parsing

20

a  recursive  parser

bestscore(x,i,j,s)

if (j = i+1)
else

return tagscore(x,s[i])
return max score(x->yz) *

bestscore(y,i,k) *
bestscore(z,k,j)

    will  this  parser  work?
    why  or  why  not?
    memory  requirements?

21

a  memoized  parser

    one  small  change:

if (scores[x][i][j] == null)

bestscore(x,i,j,s)
if (j = i+1)
else

score = tagscore(x,s[i])
score = max score(x->yz) *

bestscore(y,i,k) *
bestscore(z,k,j)

scores[x][i][j] = score

return scores[x][i][j]

22

a  bottom   up  parser  (cky)

    can  also  organize  things  bottom   up

bestscore(s)

for (i : [0,n-1])

for (diff : [2,n])

for (x : tags[s[i]])
score[x][i][i+1] = 
tagscore(x,s[i])
for (i : [0,n-diff])
j = i + diff
for (x->yz : rule)

x

y

z

i                       k                      j

for (k : [i+1, j-1])

score[x][i][j] = max score[x][i][j],
score(x->yz) *
score[y][i][k] *
score[z][k][j]

23

unary  rules

    unary  rules?

bestscore(x,i,j,s)

if (j = i+1)
else

return tagscore(x,s[i])
return max max score(x->yz) *

bestscore(y,i,k) *
bestscore(z,k,j)
bestscore(y,i,j) 

max score(x->y) *

24

cnf  +  unary  closure

    we  need  unaries  to  be  non   cyclic

    can  address  by  pre   calculating  the  unary  closure
    rather  than  having  zero  or  more  unaries,  always  have  

exactly  one

vp

vbd

np

dt

nn

vp

vbd

np

np

dt

nn

sbar

s

vp

sbar

vp

    alternate  unary  and  binary  layers
    reconstruct  unary  chains  afterwards

25

alternating  layers

bestscoreb(x,i,j,s)

return max max score(x->yz) *

bestscoreu(y,i,k) *
bestscoreu(z,k,j)

bestscoreu(x,i,j,s)

if (j = i+1)
else

return tagscore(x,s[i])
return max max score(x->y) *

bestscoreb(y,i,j)

26

analysis

27

memory

    how  much  memory  does  this  require?

    have  to  store  the  score  cache
    cache  size:  |symbols|*n2 doubles
    for  the  plain  treebank  grammar:

    x  ~  20k,  n  =  40,  double  ~  8  bytes  =  ~  256mb
    big,  but  workable.

    pruning:  beams

    score[x][i][j]  can  get  too  large  (when?)
    can  keep  beams  (truncated  maps  score[i][j])  which  only  store  the  best  few  

scores  for  the  span  [i,j]

    pruning:  coarse   to   fine

    use  a  smaller  grammar  to  rule  out  most  x[i,j]
    much  more  on  this  later   

28

time:  theory

    how  much  time  will  it  take  to  parse?

    for  each  diff  (<=  n)

    for  each  i  (<=  n)

    for  each  rule  x      y  z  

    for  each  split  point  k

do  constant  work

x

y

z

    total  time:  |rules|*n3
    something  like  5  sec  for  an  unoptimized  parse  of  a  

i                       k                      j

20   word  sentences

29

time:  practice

    parsing  with  the  vanilla  treebank  grammar:

~ 20k rules

(not an 
optimized 
parser!)
observed 
exponent: 
3.6

    why   s  it  worse  in  practice?

    longer  sentences     unlock     more  of  the  grammar
    all  kinds  of  systems  issues  don   t  scale

30

same   span  reachability

top

nx

sq

x

rrc

adjp advp
frag intj np
pp prn qp s
sbar ucp vp

whnp

sbarq

whadvp

prt

whpp

sinv

whadjp

lst
conjp
nac

31

rule  state  reachability

example: np cc    
np

0

example: np cc np    

n-1

np

cc

np

0

n-k-1

n-k

cc

1 alignment

n

n

n alignments

    many  states  are  more  likely  to  match  larger  spans!

32

efficient  cky

    lots  of  tricks  to  make  cky  efficient

    some  of  them  are  little  engineering  details:

    e.g.,  first  choose  k,  then  enumerate  through  the  y:[i,k]  which  are  

non   zero,  then  loop  through  rules  by  left  child.

    optimal  layout  of  the  dynamic  program  depends  on  grammar,  

input,  even  system  details.

    another  kind  is  more  important  (and  interesting):

    many  x:[i,j]  can  be  suppressed  on  the  basis  of  the  input  string
    we   ll  see  this  next  class  as  figures   of   merit,  a*  heuristics,  coarse   

to   fine,  etc

33

agenda   based  parsing

34

agenda   based  parsing
    agenda   based  parsing  is  like  graph  search  (but  over  a  

hypergraph)

    concepts:

    numbering:  we  number  fenceposts  between  words
       edges     or  items:  spans  with  labels,  e.g.  pp[3,5],  represent  the  sets  of  

trees  over  those  words  rooted  at  that  label  (cf.  search  states)

    a  chart:  records  edges  we   ve  expanded  (cf.  closed  set)
    an  agenda:  a  queue  which  holds  edges  (cf.  a  fringe  or  open  set)

pp

critics

0

write

2

1

reviews

with

4

3

computers

5

35

word  items

    building  an  item  for  the  first  time  is  called  discovery.    items  go  

into  the  agenda  on  discovery.

    to  initialize,  we  discover  all  word  items  (with  score  1.0).

agenda
critics[0,1], write[1,2], reviews[2,3], with[3,4], computers[4,5]

chart [empty]

0

1

2

3

4

5

critics         write         reviews         with         computers

36

unary  projection

    when  we  pop  a  word  item,  the  lexicon  tells  us  the  tag  item  

successors  (and  scores)  which  go  on  the  agenda

critics[0,1]
nns[0,1]

write[1,2]
vbp[1,2] nns[2,3]

reviews[2,3]

with[3,4]
in[3,4]

computers[4,5]

nns[4,5]

critics

0

write

2

1

reviews

with

4

3

computers

5

critics         write         reviews         with         computers

37

item  successors

    when  we  pop  items  off  of  the  agenda:

    graph  successors:  unary  projections  (nns      critics,  np      nns)

y[i,j] with x     y forms  x[i,j]

    hypergraph  successors:  combine  with  items  already  in  our  chart

y[i,j] and z[j,k] with x     y z form  x[i,k]

    enqueue  /  promote  resulting  items  (if  not  in  chart  already)
    record  backtraces  as  appropriate
    stick  the  popped  edge  in  the  chart  (closed  set)

x

    queries  a  chart  must  support:

    is  edge  x:[i,j]  in  the  chart?    (what  score?)
    what  edges  with  label  y  end  at  position  j?
    what  edges  with  label  z  start  at  position  i?  

y

z

38

an  example

nns[0,1] vbp[1,2] nns[2,3] in[3,4] nns[3,4] np[0,1]
np[2,5]

root[0,2]

pp[3,5]

vp[1,3]

s[0,3] vp[1,5]
root
s

np[2,3] np[4,5]

s[0,2]
vp[1,2]
root[0,3] s[0,5] root[0,5]

root

s

root
s

vp

np
nns

critics

vp
vbp

write

1

0

np
nns

reviews

3

2

vp

np

in

with

pp

np
nns

computers

4

5

39

empty  elements

    sometimes  we  want  to  posit  nodes  in  a  parse  tree  that  don   t  

contain  any  pronounced  words:

i want you to parse this sentence
i want [    ] to parse this sentence

    these  are  easy  to  add  to  a  chart  parser!

    for  each  position  i,  add  the     word     edge     :[i,i]
    add  rules  like  np          to  the  grammar
    that   s  it!

   

0

i

   

1

like

np

   

2

vp

   

4

parse

   

3

to

   

5

empties

40

ucs  /  a*

    with  weighted  edges,  order  matters

    must  expand  optimal  parse  from  

bottom  up  (subparses  first)

    cky  does  this  by  processing  smaller  

spans  before  larger  ones

    ucs  pops  items  off  the  agenda  in  
order  of  decreasing  viterbi  score

    a*  search  also  well  defined

    you  can  also  speed  up  the  search  

without  sacrificing  optimality
    can  select  which  items  to  process  first
    can  do  with  any     figure  of  merit     

[charniak  98]

    if  your  figure   of   merit  is  a  valid  a*  

heuristic,  no  loss  of  optimiality  [klein  
and  manning  03]

x

0

i

j

n

41

(speech)  lattices

    there  was  nothing  magical  about  words  spanning  exactly  

one  position.

    when  working  with  speech,  we  generally  don   t  know  

how  many  words  there  are,  or  where  they  break.

    we  can  represent  the  possibilities  as  a  lattice  and  parse  

these  just  as  easily.

eyes

i

awe

saw

ivan

an

van

of

   ve

a

42

