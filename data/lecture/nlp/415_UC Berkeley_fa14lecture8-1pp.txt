natural  language  processing

part   of   speech  tagging

dan  klein      uc  berkeley

1

parts  of  speech

2

parts   of   speech  (english)

    one  basic  kind  of  linguistic  structure:  syntactic  word  classes

open class (lexical) words
nouns
proper
ibm
italy

common
cat / cats
snow

verbs
main
see
registered

closed class (functional)

determiners

the some

conjunctions

and or

pronouns

he its

auxiliary
can
had

adjectives

yellow

adverbs

slowly

    more

numbers
122,312
one

prepositions

to with

particles

off up

    more

3

cc 
cd 
dt 
ex 
fw 
in 
jj 
jjr 
jjs 
md 
nn 
nnp 
nnps 
nns 
pos 
prp 
prp$ 
rb 
rbr 
rbs 
rp 
to 
uh 
vb 
vbd 
vbg 
vbn 
vbp 
vbz 
wdt 
wp 
wp$ 
wrb 

conjunction, coordinating 

numeral, cardinal 

determiner 

existential there 

foreign word 

preposition or conjunction, subordinating 

adjective or numeral, ordinal 

adjective, comparative 
adjective, superlative 

modal auxiliary 

noun, common, singular or mass 

noun, proper, singular 
noun, proper, plural 
noun, common, plural 

genitive marker 
pronoun, personal 
pronoun, possessive 

adverb 

adverb, comparative 
adverb, superlative 

particle 

"to" as preposition or infinitive marker 

interjection 

verb, base form 
verb, past tense 

verb, present participle or gerund 

verb, past participle 

verb, present tense, not 3rd person singular 

verb, present tense, 3rd person singular 

wh-determiner 
wh-pronoun 

wh-pronoun, possessive 

wh-adverb 

and both but either or

mid-1890 nine-thirty 0.5 one

a all an every no that the

there 

gemeinschaft hund ich jeux
among whether out on by if
third ill-mannered regrettable

braver cheaper taller
bravest cheapest tallest
can may might will would 

cabbage thermostat investment subhumanity

motown cougar yvette liverpool

americans materials states

undergraduates bric-a-brac averages

' 's 

hers himself it we them

her his mine my our ours their thy your 
occasionally maddeningly adventurously
further gloomier heavier less-perfectly

best biggest nearest worst 

aboard away back by on open through

to 

huh howdy uh whammo shucks heck

ask bring fire see take

pleaded swiped registered saw

stirring focusing approaching erasing
dilapidated imitated reunifed unsettled
twist appear comprise mold postpone

bases reconstructs marks uses

that what whatever which whichever 
that what whatever which who whom

whose 

however whenever where why 

4

part   of   speech  ambiguity

    words  can  have  multiple  parts  of  speech

vbd                    vb            
vbn    vbz        vbp        vbz
nnp    nns        nn         nns    cd      nn
fed raises interest rates 0.5 percent

    two  basic  sources  of  constraint:

    grammatical  environment
    identity  of  the  current  word

    many  more  possible  features:

    suffixes,  capitalization,  name  databases  (gazetteers),  etc   

5

why  pos  tagging?
    useful  in  and  of  itself  (more  than  you   d  think)

    text   to   speech:  record,  lead
    lemmatization:  saw[v]      see,  saw[n]      saw
    quick   and   dirty  np   chunk  detection:  grep  {jj  |  nn}*  {nn  |  nns}

    useful  as  a  pre   processing  step  for  parsing

    less  tag  ambiguity  means  fewer  parses
    however,  some  tag  choices  are  better  decided  by  parsers

dt    nnp      nn   vbd vbn  rp nn        nns
the georgia branch had taken on loan commitments    

in

dt     nn     in     nn        vbd nns      vbd
the average of interbank offered rates plummeted    

vdn

6

part   of   speech  tagging

7

classic  solution:  id48s

    we  want  a  model  of  sequences  s  and  observations  w

s0

s1

w1

s2

w2

sn

wn

    assumptions:

states  are  tag  n   grams

   
    usually  a  dedicated  start  and  end  state  /  word
    tag/state  sequence  is  generated  by  a  markov model
    words  are  chosen  independently,  conditioned  only  on  the  tag/state
    these  are  totally  broken  assumptions:  why?

8

states

    states  encode  what  is  relevant  about  the  past
    transitions  p(s|s   )  encode  well   formed  tag  sequences

    in  a  bigram  tagger,  states  =  tags

<   >

s0

< t1>
s1

< t2>
s2

w1

w2

< tn>
sn

wn

    in  a  trigram  tagger,  states  =  tag  pairs

<   ,   >
s0

<    , t1>

< t1, t2>

< tn-1, tn>

s1

w1

s2

w2

sn

wn

9

estimating  transitions

    use  standard  smoothing  methods  to  estimate  transitions:

(
tp
i

|

t
i

,

t
i

1
   

   

2

)

   

   
2

(  
tp
i

|

t
i

,

t
i

1
   

   

2

)

   

(  
tp
i

   
1

|

t
i

1
   

)

1(
      

      
2
1

   

)(  )
tp
i

    can  get  a  lot  fancier  (e.g.  kn  smoothing)  or  use  higher  orders,  but  in  this  

case  it  doesn   t  buy  much

    one  option:  encode  more  into  the  state,  e.g.  whether  the  previous  word  

was  capitalized  (brants 00)

    big  idea:  the  basic  approach  of  state   splitting  /  refinement  turns  out  to  

be  very  important  in  a  range  of  tasks

10

estimating  emissions

    emissions  are  trickier:

    words  we   ve  never  seen  before
    words  which  occur  with  tags  we   ve  never  seen  them  with
    one  option:  break  out  the  fancy  smoothing  (e.g.  kn,  good   turing)
    issue:  unknown  words  aren   t  black  boxes:

343,127.23            11-year         minteria         reintroducibly

    basic  solution:  unknown  words  classes  (affixes  or  shapes)
x+-   ly   

d+,d+.d+

d+-x+

xx+

    common  approach:  estimate  p(t|w)  and  invert
    [brants 00]  used  a  suffix  trie as  its  (inverted)  emission  model

11

disambiguation  (id136)

    problem:  find  the  most  likely  (viterbi)  sequence  under  the  model

    given  model  parameters,  we  can  score  any  tag  sequence
<vbz, nn> <nn, nns> <nns, cd>
<   ,   >

<nnp, vbz>

<   ,nnp>

<cd, nn>

<stop>

nnp        vbz               nn          nns           cd             nn           .
fed    raises     interest   rates      0.5      percent    .

p(nnp|<   ,   >) p(fed|nnp) p(vbz|<nnp,   >) p(raises|vbz) p(nn|vbz,nnp)   ..

    in  principle,  we   re  done      list  all  possible  tag  sequences,  score  each  one,  

pick  the  best  one  (the  viterbi  state  sequence)  

nnp  vbz   nn  nns  cd  nn
nnp  nns  nn  nns  cd  nn
nnp  vbz  vb   nns  cd  nn

logp = -23
logp = -29
logp = -27

12

the  state  lattice  /  trellis

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

start       fed           raises       interest         rates         end

13

the  state  lattice  /  trellis

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

start       fed           raises       interest         rates         end

14

so  how  well  does  it  work?

    choose  the  most  common  tag

    90.3%  with  a  bad  unknown  word  model
    93.7%  with  a  good  one

    tnt  (brants,  2000):

    a  carefully  smoothed  trigram  tagger
    suffix  trees  for  emissions
    96.7%  on  wsj  text  (soa  is  ~97.5%)

    noise  in  the  data

    many  errors  in  the  training  and  test  corpora

dt     nn     in     nn
the average of interbank offered rates plummeted    

vbd nns      vbd

    probably  about  2%  guaranteed  error

from  noise  (on  this  data)

jj       jj         nn
chief executive officer
nn       jj         nn
chief executive officer
jj       nn         nn
chief executive officer
nn       nn         nn
chief executive officer

15

overview:  accuracies
    roadmap  of  (known  /  unknown)  accuracies:

    most  freq tag:  

~90%  /  ~50%

    trigram  id48:  

~95%  /  ~55%

    tnt (id48++):  

96.2%  /  86.0%

    maxent p(t|w):  
    memm  tagger:  
    state   of   the   art:  
    upper  bound:  

93.7%  /  82.6%
96.9%  /  86.9%
97+%  /  89+%
~98%

most errors 
on unknown 

words

16

common  errors

    common  errors  [from  toutanova  &  manning  00]

nn/jj nn
official knowledge

vbd rp/in dt nn
made  up   the story

rb   vbd/vbn nns
recently   sold   shares

17

richer  features

18

better  features

    can  do  surprisingly  well  just  looking  at  a  word  by  itself:

    word
    lowercased  word
    prefixes
    suffixes
    capitalization
    word  shapes

the:  the      dt
importantly:  importantly      rb
unfathomable:  un        jj
surprisingly:     ly      rb
meridian:  cap      nnp
35   year:  d   x      jj

    then  build  a  maxent  (or  whatever)  model  to  predict  tag
    maxent  p(t|w):  

93.7%  /  82.6%

s3

w3

19

why  linear  context  is  useful

    lots  of  rich  local  information!
rb

prp  vbd   in rb  in  prp    vbd   .
they  left     as soon as   he    arrived .

    we  could  fix  this  with  a  feature  that  looked  at  the  next  word

jj
nnp nns    vbd          vbn        .
intrinsic flaws remained undetected  .

    we  could  fix  this  by  linking  capitalized  words  to  their  lowercase  versions

    solution:  discriminative  sequence  models  (memms,  crfs)

    reality  check:

    taggers  are  already  pretty  good  on  wsj  journal  text   
    what  the  world  needs  is  taggers  that  work  on  other  text!
    though:  other  tasks  like  ie  have  used  the  same  methods  to  good  effect

20

sequence   free  tagging?

    what  about  looking  at  a  word  and  its  

environment,  but  no  sequence  information?

    add  in  previous  /  next  word the  __
    previous  /  next  word  shapes
    occurrence  pattern  features
    crude  entity  detection
    phrasal  verb  in  sentence?
    conjunctions  of  these  things

put          __

x  __  x
[x:  x  x  occurs]
__     ..  (inc.|co.)

    all  features  except  sequence:  96.6%  /  86.8%
    uses  lots  of  features:  >  200k
    why  isn   t  this  the  standard  approach?

t3

w3

w2

w4

21

feature   rich  sequence  models

    problem:  id48s  make  it  hard  to  work  with  arbitrary  features  

of  a  sentence

    example:  name  entity  recognition  (ner)

per per o        o o o

o

o

org        o      o o o o loc  loc o

tim boon has signed a contract extension with leicestershire which will keep him at grace road .

local context

prev
state other
word
tag
sig

at
in
x

next
???

cur
???
grace road
nnp
nnp
xx
xx

22

memm  taggers

    idea:  left   to   right  local  decisions,  condition  on  previous  tags  

and  also  entire  input

    train  up  p(ti|w,ti   1,ti   2)  as  a  normal  maxent  model,  then  use  to  score  

sequences

    this  is  referred  to  as  an  memm  tagger  [ratnaparkhi  96]
    beam  search  effective!    (why?)
    what  about  beam  size  1?

23

ner  features

because of id173 
term, the more common 
prefixes have larger 
weights even though 
entire-word features are 
more specific.

local context

prev
state other
word
tag
sig

at
in
x

next
???

cur
???
grace road
nnp
nnp
xx
xx

at

grace

feature

feature weights
pers
-0.73
0.03
0.45
0.47
-0.10
-0.70
0.80
0.68
-0.69
-0.20

<g
nnp
in nnp
other
xx
o-xx
x-xx-xx
o-x-xx

loc
0.94
0.00
-0.04
0.45
0.14
-0.92
0.46
0.37
0.37
0.82

feature type
previous word
current word
beginning bigram
current pos tag
prev and cur tags
previous state
current signature
prev state, cur sig
prev-cur-next sig
p. state - p-cur sig
   
total:

-0.58

2.68

24

conditional  random  fields  

(and  friends)

25

id88  taggers

[collins 01]

    linear  models:

         that  decompose  along  the  sequence

         allow  us  to  predict  with  the  viterbi  algorithm

         which  means  we  can  train  with  the  id88  algorithm  

(or  related  updates,  like  mira)

26

conditional  random  fields

    make  a  maxent  model  over  entire  taggings

    memm

    crf

27

crfs

    like  any  maxent model,  derivative  is:

    so  all  we  need  is  to  be  able  to  compute  the  expectation  of  each  feature  

(for  example  the  number  of  times  the  label  pair  dt   nn  occurs,  or  the  
number  of  times  nn   interest  occurs)  under  the  model  distribution

    critical  quantity:  counts  of  posterior  marginals:    

28

computing  posterior  marginals

    how  many  (expected)  times  is  word  w  tagged  with  s?

    how  to  compute  that  marginal?

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

^

n

v

j

d

$

start       fed           raises       interest         rates         end

29

transformation   based  learning

    [brill  95]  presents  a  transformation   based  tagger

    label  the  training  set  with  most  frequent  tags

dt      md    vbd      vbd    .
the    can    was    rusted  .

    add  transformation  rules  which  reduce  training  mistakes

    md      nn  :  dt  __
    vbd      vbn  :  vbd  __  .

    stop  when  no  transformations  do  sufficient  good
    does  this  remind  anyone  of  anything?

    probably  the  most  widely  used  tagger  (esp.  outside  nlp)
         but  definitely  not  the  most  accurate:  96.6%  /  82.0  %

30

learned  transformations

    what  gets  learned?  [from  brill  95]

31

domain  effects
    accuracies  degrade  outside  of  domain

    up  to  triple  error  rate
    usually  make  the  most  errors  on  the  things  you  care  about  

in  the  domain  (e.g.  protein  names)

    open  questions

    how  to  effectively  exploit  unlabeled  data  from  a  new  

domain  (what  could  we  gain?)

    how  to  best  incorporate  domain  lexica  in  a  principled  way  

(e.g.  umls  specialist  lexicon,  ontologies)

32

unsupervised  tagging

33

unsupervised  tagging?

    aka  part   of   speech  induction
    task:

    raw  sentences  in
    tagged  sentences  out
    obvious  thing  to  do:

    start  with  a  (mostly)  uniform  id48
    run  em
    inspect  results

34

em  for  id48s:  process

    alternate  between  recomputing  distributions  over  hidden  variables  (the  

tags)  and  reestimating  parameters

    crucial  step:  we  want  to  tally  up  how  many  (fractional)  counts  of  each  

kind  of  transition  and  emission  we  have  under  current  params:

    same  quantities  we  needed  to  train  a  crf!

35

merialdo:  setup

    some  (discouraging)  experiments  [merialdo  94]

    setup:

    you  know  the  set  of  allowable  tags  for  each  word
    fix  k  training  examples  to  their  true  labels

    learn  p(w|t)  on  these  examples
    learn  p(t|t   1,t   2)  on  these  examples

    on  n  examples,  re   estimate  with  em

    note:  we  know  allowed  tags  but  not  frequencies

36

merialdo:  results

37

distributional  id91

    the president said that the downturn was over    

president
president
governor
governor
said
said
reported

the __ of
the __ said
the __ of
the __ appointed
sources __    
president __ that
sources __    

president

governor

said
reported

the
a

[finch and chater 92, shuetze 93, many others]

38

distributional  id91

    three  main  variants  on  the  same  idea:

    pairwise  similarities  and  heuristic  id91

    e.g.  [finch  and  chater  92]
    produces  dendrograms
    vector  space  methods

    e.g.  [shuetze  93]
    models  of  ambiguity
    probabilistic  methods

    various  formulations,  e.g.  [lee  and  pereira  99]

39

nearest  neighbors

40

dendrograms                                              _

41

a  probabilistic  version?

csp

(

,

)

   

   

i

wwpcwpcp
i

1
   

(

)

(

)

(

,

|

i

i

i

i

|

c

i

)

1
   

c1

c2

c3

c4

c5

c6

c7

c8

    the president said that the downturn was over    

,(
csp

)

c1

c2

   

   
c3

i

(
|
ccpcwp
i

(

)

|

i

i

i

)

1
   

c4

c5

c6

c7

c8

    the president said that the downturn was over    

42

