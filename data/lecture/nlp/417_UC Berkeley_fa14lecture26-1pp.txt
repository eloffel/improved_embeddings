natural language processing

historical document transcription

dan klein     uc berkeley

joint work with taylor berg-kirkpatrick and greg durrett [acl 2013]

historical document 

historical document 

old bailey court proceedings 1775

transcription 

document image

document image

transcription 

transcription 

(google tesseract)

and ch   : priftmer anhc bar. jacob lazarus and his
ihp1  uh:  prifoner.  were  both  together  when!
rcccivcd  lhczn.  i  fold  eiev  n  pair  of  than
for xiirce guincas, and dclivcrcd the rcll'l:.in-
d:r  hack  lo  :11: prifuner. 1 fold ftvcn pairof
   lk  to mark simpcr :   nncpuir  of  mixcd. and.
mo  pair  of  i   rcad  to  lhz:  foolnun,  and on:
pair of zhrzad to lh: barber. '
  q: what is the foolmarfs name?
  fraum mgfzr.   i dun   : know.
  hairy hzrvir. l was    andingar the camp
icr  waizin  far  the  thcrrilfs  u   iceruo employ
in: :  mo 3   :  daughter  came  for  me  to  0 am!
take  the  prifoncr.  1  wm! to  |hc  old  aailcy

pipelined approach

pipelined approach

pipelined approach

pipelined approach

pipelined approach

pipelined approach

m

pipelined approach

m o

pipelined approach

m o d

historical document 

unknown fonts 

unknown fonts 

po

unknown fonts 

po

unknown fonts 

po

unknown fonts 
long s glyph

university of california at berkeley

{tberg,gdurrett,klein}@cs.berkeley.edu

wandering baseline

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

university of california at berkeley

{tberg,gdurrett,klein}@cs.berkeley.edu

wandering baseline

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

wandering baseline

university of california at berkeley

{tberg,gdurrett,klein}@cs.berkeley.edu

wandering baseline

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

uneven inking

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

learns the font in an unsupervised fashion. font
shape and character segmentation are tightly cou-

uneven inking

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

learns the font in an unsupervised fashion. font
shape and character segmentation are tightly cou-

uneven inking

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

learns the font in an unsupervised fashion. font
shape and character segmentation are tightly cou-

uneven inking

(a)

old bailey, 1725:

1725
(a)
(b)

old bailey, 1725:
old bailey, 1875:

(b)
1823
(c)

old bailey, 1875:
trove, 1823:

(c)

trove, 1823:

(d)

trove, 1883:

various historical documents 

(b)

1875
(b)

trove, 1823:

old bailey, 1875:

documents into text. we will use these manual
transcriptions to evaluate the output of our system.
from the old bailey proceedings, we extracted a
set of 20 images, each consisting of 30 lines of
text to use as our    rst test set. we picked 20 doc-
uments, printed in consecutive decades. the    rst
document is from 1715 and the last is from 1905.
old bailey, 1875:
we choose the    rst document in each of the corre-
sponding years, choose a random page in the doc-
ument, and extracted an image of the    rst 30 con-
secutive lines of text consisting of full sentences.5
the ten documents in the old bailey dataset that
were printed before 1810 use the long s glyph,
while the remaining ten do not.

documents into text. we will use these manual
(c)
transcriptions to evaluate the output of our system.
from the old bailey proceedings, we extracted a
set of 20 images, each consisting of 30 lines of
text to use as our    rst test set. we picked 20 doc-
uments, printed in consecutive decades. the    rst
document is from 1715 and the last is from 1905.
we choose the    rst document in each of the corre-
sponding years, choose a random page in the doc-
1883:
(d)
ument, and extracted an image of the    rst 30 con-
secutive lines of text consisting of full sentences.5
the ten documents in the old bailey dataset that
were printed before 1810 use the long s glyph,
while the remaining ten do not.

5.2 trove
trove, 1823:
trove, 1883:
our second dataset is taken from a collection of
digitized australian newspapers that were printed
between the years of 1803 and 1954. this col-
lection is called trove, and is maintained by the
the national library of australia (holley, 2010).
we extracted ten images from this collection in the
same way that we extracted images from old bai-
trove, 1883:
ley, but starting from the year 1803. we manually
produced our own gold annotations for these ten
images. only the    rst document of trove uses the
long s glyph.

5.2 trove
our second dataset is taken from a collection of
figure 6: portions of several documents from our test set rep-
(d)
digitized australian newspapers that were printed
resenting a range of dif   culties are displayed. on document
between the years of 1803 and 1954. this col-
(a), which exhibits noisy typesetting, our system achieves a
word error rate (wer) of 25.2. document (b) is cleaner in
lection is called trove, and is maintained by the
comparison, and on it we achieve a wer of 15.4. on doc-
the national library of australia (holley, 2010).

(c)

our approach 

our approach 
po

{tberg,gdurrett,klein}@cs.berkeley.edu

our approach 
po

(a)
(b)
(c)

figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.

learns the font in an unsupervised fashion. font

{tberg,gdurrett,klein}@cs.berkeley.edu
{tberg,gdurrett,klein}@cs.berkeley.edu

our approach 
po

(a)
(a)
(b)
(b)
(c)
(c)

figure 1: portions of historical documents with (a) unknown
figure 1: portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.
font, (b) uneven baseline, and (c) over-inking.

learns the font in an unsupervised fashion. font

generative model

p r i s o n e r

generative model

p r i s o n e r

generative model

p r i s o n e r

generative model

p r i s o n e r

generative model

p r i s o n e r

language 
model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

typesetting 

model

generative model

p r i s o n e r

rendering 

model

generative model

p r i s o n e r

rendering 

model

generative model

p r i s o n e r

rendering 

model

generative model

p r i s o n e r

ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (ho
and nagy, 2000; huang et al., 2006), which is not
feasible for noisy historical documents. kae and
learned-miller (2009) jointly learn the font and
image segmentation but do not outperform mod-

work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (vamvakas et al., 2008; kluzner et al.,
2009; kae et al., 2010; kluzner et al., 2011).
the most comparable work is that of kopec and
lomelin (1996) and kopec et al. (2001). they
integrated typesetting models with language mod-
in the nlp com-
munity, generative models have been developed
speci   cally for correcting outputs of ocr systems

of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

generative model

p (e, t, r, x) =
language model

e

p (e)
   p (t|e)
   p (r)
   p (x|e, t, r)

p r i s o n e r

[language model]
[typesetting model]
[inking model]
[noise model]

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 language model p (e)

ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (ho
and nagy, 2000; huang et al., 2006), which is not
feasible for noisy historical documents. kae and
learned-miller (2009) jointly learn the font and
image segmentation but do not outperform mod-

work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (vamvakas et al., 2008; kluzner et al.,
2009; kae et al., 2010; kluzner et al., 2011).
the most comparable work is that of kopec and
lomelin (1996) and kopec et al. (2001). they
integrated typesetting models with language mod-
in the nlp com-
munity, generative models have been developed
speci   cally for correcting outputs of ocr systems

p r i s o n e r

e

p (e, t, r, x) =
language model

of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

pixels in an image of the line). additionally, we
have a random variable t that speci   es the layout
generative model
of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

p (e)
p (e, t, r, x) =
   p (t|e)
[language model]
p (e)
typesetting model
   p (r)
t
[typesetting model]
   p (t|e)
   p (x|e, t, r)
[inking model]
   p (r)
[noise model]
   p (x|e, t, r)

[language model]
[typesetting model]
[inking model]
[noise model]

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 language model p (e)

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.

ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (ho
and nagy, 2000; huang et al., 2006), which is not
feasible for noisy historical documents. kae and
learned-miller (2009) jointly learn the font and
image segmentation but do not outperform mod-

work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (vamvakas et al., 2008; kluzner et al.,
2009; kae et al., 2010; kluzner et al., 2011).
the most comparable work is that of kopec and
lomelin (1996) and kopec et al. (2001). they
integrated typesetting models with language mod-
in the nlp com-
munity, generative models have been developed
speci   cally for correcting outputs of ocr systems

p r i s o n e r

e

p (e, t, r, x) =
language model

of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

pixels in an image of the line). additionally, we
have a random variable t that speci   es the layout
generative model
of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

[language model]
[typesetting model]
[inking model]
[noise model]

p (e)
p (e, t, r, x) =
   p (t|e)
[language model]
p (e)
typesetting model
   p (r)
t
[typesetting model]
   p (t|e)
   p (x|e, t, r)
[inking model]
   p (r)
rendering model
[noise model]
x
   p (x|e, t, r)
p (x|e, t )

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 language model p (e)

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.

ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (ho
and nagy, 2000; huang et al., 2006), which is not
feasible for noisy historical documents. kae and
learned-miller (2009) jointly learn the font and
image segmentation but do not outperform mod-

work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (vamvakas et al., 2008; kluzner et al.,
2009; kae et al., 2010; kluzner et al., 2011).
the most comparable work is that of kopec and
lomelin (1996) and kopec et al. (2001). they
integrated typesetting models with language mod-
in the nlp com-
munity, generative models have been developed
speci   cally for correcting outputs of ocr systems

e

p (e, t, r, x) =
language model

of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

pixels in an image of the line). additionally, we
have a random variable t that speci   es the layout
generative model
of the bounding boxes of the glyphs in the image,
and a random variable r that speci   es aspects of
the inking and rendering process. the joint distri-
bution is:

[language model]
[typesetting model]
[inking model]
[noise model]

p (e)
p (e, t, r, x) =
   p (t|e)
[language model]
p (e)
typesetting model
   p (r)
t
[typesetting model]
   p (t|e)
   p (x|e, t, r)
[inking model]
   p (r)
rendering model
[noise model]
x
   p (x|e, t, r)
p (x|e, t )

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 language model p (e)

we let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
for example, e represents the entire sequence of
text, while ei represents ith character in the se-
quence.

language model

e

language model

e

language model

r

ei 1

e

a

ei

t

ei+1

language model

r

ei 1

e

a

ei

t

ei+1

kneser-ney smoothed character 6-gram

typesetting model

a

ei

typesetting model

a

ei

t

a

ei

t

li

typesetting model

5

1

left pad 
width

a

ei

gi

t

li

typesetting model

5

1

left pad 
width

1
30
glyph box width

a

ei

gi

t

li

typesetting model

5

1

left pad 
width

ri

1

5

right pad 

width

1
30
glyph box width

a

ei

gi

t

li

typesetting model

5

1

left pad 
width

ri

1

5

right pad 

width

1
30
glyph box width

a

a

ei

gi

t

li

typesetting model

5

1

left pad 
width

ri

1

5

right pad 

width

1
30
glyph box width

a

a

ei

gi

t

li

typesetting model

5

1

left pad 
width

ri

1

5

right pad 

width

1
30
glyph box width

a

vi

a a a

vertical offset

a

ei

gi

t

li

typesetting model

ri

di

vi

5

1

left pad 
width

1

5

right pad 

width

1
30
glyph box width

a

a a a

vertical offset

a aaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
inking level

rendering model

rendering model

rendering model

glyph box

rendering model

glyph box

width
gi

glyph box

rendering model

glyph box

width
gi

vertical
offset
vi

glyph box

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph box

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph box

x

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

rendering model

glyph box

width
gi

vertical
offset
vi

inking 
level
di

glyph shape parameters

glyph box

x

sample
pixels

bernoulli
pixel probs

log-linear interpolation

log-linear interpolation

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

interpolation

weights    

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

dot product    > 

interpolation

weights    

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

dot product    > 

interpolation

weights    

glyph shape 
parameters  

apply logistic

bernoulli pixel probs    

log-linear interpolation

interpolation

weights    

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

interpolation

weights    

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

interpolation

weights    

glyph shape 
parameters  

bernoulli pixel probs    

log-linear interpolation

interpolation

weights    
j
glyph shape 
parameters  

bernoulli pixel probs    

j

log-linear interpolation

   j /

interpolation

weights    
j
glyph shape 
parameters  

bernoulli pixel probs    

j

log-linear interpolation
   j / exp[   >j  ]

interpolation

weights    
j
glyph shape 
parameters  

bernoulli pixel probs    

j

learning and id136

e

t

x

learning and id136

    learn font parameters using em

e

t

x

learning and id136

    learn font parameters using em

5

1

1

5

1

30

e

t

x

learning and id136

    learn font parameters using em

5

1

1

5

1

30

e

t

x

learning and id136

    learn font parameters using em

5

1

1

5

1

30

    initialize font parameters with 

mixtures of modern fonts

e

t

x

learning and id136

    learn font parameters using em

5

1

1

5

1

30

    initialize font parameters with 

mixtures of modern fonts

    semi-markov dp to compute 

expectations

e

t

x

learning and id136

    learn font parameters using em

5

1

1

5

1

30

    initialize font parameters with 

mixtures of modern fonts

    semi-markov dp to compute 

expectations

    ef   cient id136 using a coarse-

to-   ne approach

e

t

x

system output example

system output example

5

1

1

5

1

30

system output example

5

1

1

5

1

30

how the murderers came to

system output example

5

1

1

5

1

30

how the murderers came to

system output example

5

1

1

5

1

30

how the murderers came to

system output example

5

1

1

5

1

30

how the murderers came to

system output example

5

1

1

5

1

30

how the murderers came to

system output example

system output example

5

1

1

5

1

30

system output example

5

1

1

5

1

30

taken ill and taken  away -- i remember

system output example

5

1

1

5

1

30

taken ill and taken  away -- i remember

system output example

5

1

1

5

1

30

taken ill and taken  away -- i remember

system output example

5

1

1

5

1

30

taken ill and taken  away -- i remember

experiments

experiments

test data

experiments

test data

    old bailey (1715-1905)

experiments

test data

    old bailey (1715-1905)

20 images, 30 lines each

experiments

test data

    old bailey (1715-1905)

20 images, 30 lines each

    trove (1803-1893)

experiments

test data

    old bailey (1715-1905)

20 images, 30 lines each

    trove (1803-1893)

10 images, 30 lines each

experiments

test data

    old bailey (1715-1905)

20 images, 30 lines each

    trove (1803-1893)

10 images, 30 lines each

baselines

experiments

test data

    old bailey (1715-1905)

20 images, 30 lines each

    trove (1803-1893)

10 images, 30 lines each

baselines

    google tesseract

experiments

test data

    old bailey (1715-1905)

20 images, 30 lines each

    trove (1803-1893)

10 images, 30 lines each

baselines

    google tesseract
    abbyy finereader 11

experiments

test data

language models

    old bailey (1715-1905)

20 images, 30 lines each

    trove (1803-1893)

10 images, 30 lines each

baselines

    google tesseract
    abbyy finereader 11

experiments

test data

language models

    old bailey (1715-1905)

    new york times

20 images, 30 lines each

    trove (1803-1893)

10 images, 30 lines each

baselines

    google tesseract
    abbyy finereader 11

experiments

test data

language models

    old bailey (1715-1905)

    new york times

20 images, 30 lines each

34m words nyt gigaword

    trove (1803-1893)

10 images, 30 lines each

baselines

    google tesseract
    abbyy finereader 11

experiments

test data

language models

    old bailey (1715-1905)

    new york times

20 images, 30 lines each

34m words nyt gigaword

    trove (1803-1893)

    old bailey

10 images, 30 lines each

baselines

    google tesseract
    abbyy finereader 11

experiments

test data

language models

    old bailey (1715-1905)

    new york times

20 images, 30 lines each

34m words nyt gigaword

    trove (1803-1893)

    old bailey

10 images, 30 lines each

32m words manually 
transcribed

baselines

    google tesseract
    abbyy finereader 11

results

old bailey court proceedings

(1715-1905)

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

results

old bailey court proceedings

(1715-1905)

google
tesseract

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

54.8results

old bailey court proceedings

(1715-1905)

google
tesseract

abbyy

finereader

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

40.054.860
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

results

old bailey court proceedings

(1715-1905)

google
tesseract

abbyy

finereader

ocular
w/ nyt

[berg-kirkpatrick et al. 2013]

28.140.054.860
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

results

old bailey court proceedings

(1715-1905)

google
tesseract

abbyy

finereader

ocular
w/ nyt

ocular
w/ ob

[berg-kirkpatrick et al. 2013]

24.128.140.054.8results 

trove historical newspapers

(1803-1893)

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

results 

trove historical newspapers

(1803-1893)

google
tesseract

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

59.3results 

trove historical newspapers

(1803-1893)

google
tesseract

abbyy

finereader

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

49.259.3results 

trove historical newspapers

(1803-1893)

google
tesseract

abbyy

finereader

ocular
w/ nyt

60
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

33.049.259.360
50
40
30
20
10
0

e
t
a
r
 
r
o
r
r
e
 
d
r
o
w

results 

trove historical newspapers

(1803-1893)

google
tesseract

abbyy

finereader

ocular
w/ nyt

[berg-kirkpatrick et al. 2014]

25.649.259.3transcription 

google tesseract

and ch   : priftmer anhc bar. jacob lazarus and his
ihp1  uh:  prifoner.  were  both  together  when!
rcccivcd  lhczn.  i  fold  eiev  n  pair  of  than
for xiirce guincas, and dclivcrcd the rcll'l:.in-
d:r  hack  lo  :11: prifuner. 1 fold ftvcn pairof
   lk  to mark simpcr :   nncpuir  of  mixcd. and.
mo  pair  of  i   rcad  to  lhz:  foolnun,  and on:
pair of zhrzad to lh: barber. '
  q: what is the foolmarfs name?
  fraum mgfzr.   i dun   : know.
  hairy hzrvir. l was    andingar the camp
icr  waizin  far  the  thcrrilfs  u   iceruo employ
in: :  mo 3   :  daughter  came  for  me  to  0 am!
take  the  prifoncr.  1  wm! to  |hc  old  aailcy

transcription 

google tesseract

ocular

and ch   : priftmer anhc bar. jacob lazarus and his
ihp1  uh:  prifoner.  were  both  together  when!
rcccivcd  lhczn.  i  fold  eiev  n  pair  of  than
for xiirce guincas, and dclivcrcd the rcll'l:.in-
d:r  hack  lo  :11: prifuner. 1 fold ftvcn pairof
   lk  to mark simpcr :   nncpuir  of  mixcd. and.
mo  pair  of  i   rcad  to  lhz:  foolnun,  and on:
pair of zhrzad to lh: barber. '
  q: what is the foolmarfs name?
  fraum mgfzr.   i dun   : know.
  hairy hzrvir. l was    andingar the camp
icr  waizin  far  the  thcrrilfs  u   iceruo employ
in: :  mo 3   :  daughter  came  for  me  to  0 am!
take  the  prifoncr.  1  wm! to  |hc  old  aailcy

the prisoner at the bar. jacob lazarus and his
wife, the prisoners were both together when i
received them. i sold eleven pair of them
for three guineas, and delivered the remain-
der back to the prisoner. i sold, seven pair of
silk to mark simpert one pair of mixed, and
two pair of thread to the footman, and one
pair of thread to the barber,
  ms. what in the footman's name?
  franco asyut,  i don't know-
  nearly norris. i was standing at the comp-
ter waiting for the sherrill's officers to employ
me a moses's daughter came for me to go and
take the prisoner. i went to the old bailey

learned fonts

initializer

learned fonts

g

initializer

learned fonts

g

initializer

learned fonts

1780

1820

1740

1860

initializer

1700

1900

unobserved pixels

unobserved pixels

unobserved pixels

unobserved pixels

conclusion

conclusion

    unsupervised font learning yields state-of-the-art 

results on documents where font is unknown

conclusion

    unsupervised font learning yields state-of-the-art 

results on documents where font is unknown

    generatively modeling sources of noise speci   c to 

printing-press era documents is effective

conclusion

    unsupervised font learning yields state-of-the-art 

results on documents where font is unknown

    generatively modeling sources of noise speci   c to 

printing-press era documents is effective

    ocular available as a downloadable tool: 

nlp.cs.berkeley.edu/ocular.shtml

conclusion

thanks!

