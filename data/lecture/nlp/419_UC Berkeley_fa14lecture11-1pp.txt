natural  language  processing

parsing  iii

dan  klein      uc  berkeley

1

unsupervised  tagging

2

unsupervised  tagging?

    aka  part   of   speech  induction
    task:

    raw  sentences  in
    tagged  sentences  out
    obvious  thing  to  do:

    start  with  a  (mostly)  uniform  id48
    run  em
    inspect  results

3

em  for  id48s:  process

    alternate  between  recomputing  distributions  over  hidden  variables  (the  

tags)  and  reestimating  parameters

    crucial  step:  we  want  to  tally  up  how  many  (fractional)  counts  of  each  

kind  of  transition  and  emission  we  have  under  current  params:

    same  quantities  we  needed  to  train  a  crf!

4

merialdo:  setup

    some  (discouraging)  experiments  [merialdo  94]

    setup:

    you  know  the  set  of  allowable  tags  for  each  word
    fix  k  training  examples  to  their  true  labels

    learn  p(w|t)  on  these  examples
    learn  p(t|t   1,t   2)  on  these  examples

    on  n  examples,  re   estimate  with  em

    note:  we  know  allowed  tags  but  not  frequencies

5

merialdo:  results

6

latent  variable  pid18s

7

the  game  of  designing  a  grammar

    annotation  refines  base  treebank symbols  to  improve  

statistical  fit  of  the  grammar
    parent  annotation  [johnson     98]

8

the  game  of  designing  a  grammar

    annotation  refines  base  treebank symbols  to  improve  

statistical  fit  of  the  grammar
    parent  annotation  [johnson     98]
    head  lexicalization [collins     99,  charniak    00]

9

the  game  of  designing  a  grammar

    annotation  refines  base  treebank symbols  to  improve  

statistical  fit  of  the  grammar
    parent  annotation  [johnson     98]
    head  lexicalization [collins     99,  charniak    00]
    automatic  id91?

10

latent  variable  grammars

...

parse tree 
sentence

derivations

parameters 

11

learning  latent  annotations

em  algorithm:

    brackets are known
    base categories are known
    only induce subcategories

just  like  forward   backward  for  id48s.

forward

x1

x4

x5

x6

right
was
backward

x7

.

x2
x3

he

12

refinement  of  the  dt  tag

dt

dt-1

dt-2

dt-3

dt-4

13

hierarchical  refinement

14

hierarchical  estimation  results

)
1
f
(
 
y
c
a
r
u
c
c
a
 
g
n

i

s
r
a
p

90

88

86

84

82

80

78

76

74

100

300

500

900

700

f1
model
1300
1500
87.3
flat training
total number of grammar symbols
hierarchical training 88.4

1100

1700

15

refinement  of  the  ,  tag

    splitting  all  categories  equally  is  wasteful:

16

adaptive splitting

    want to split complex categories more
    idea: split everything, roll back splits which 

were least useful

17

adaptive  splitting  results

f1
model
88.4
previous
with 50% merging 89.5

18

number  of  phrasal  subcategories

40

35

30

25

20

15

10

5

0

p
n

p
v

p
p

p s
v
d
a

p
j
d
a

r
a
b
s

p
q

p
n
h
w

n
r
p

x
n

v
n
s

i

t
r
p

q
s

p
p
h
w

p
j
n
o
c

g
a
r
f

c
a
n

p
c
u

p
v
d
a
h
w

j
t
n

i

c
r
r

q
r
a
b
s

p x
j
d
a
h
w

t
o
o
r

t
s
l

19

number  of  lexical  subcategories

70

60

50

40

30

20

10

0

j
pj
n
n

s
n
n

n
n

n
b
v

b
r

b
v

g
b
v

d
b
v

n

i

d
c

z
b
v

p
b
v

t
d

c
c

r
j
j

s:
j
j

s
p
n
n

p
r
p

$
p
r
p

d
m

r
b
r

p
w

s
o
p

t
d
p

b
r
w

b
r
l
-

x
e

$
p
w

t
d
w

b
r
r

-

-.

'
-'

w
f

s
b
r

o$
t

h,
u

`
`

p
r

s#
l

m
y
s

20

learned  splits

    proper nouns (nnp):
oct.
john

nnp-14
nnp-12
nnp-2
nnp-1
nnp-15
nnp-3

nov.
robert

e.

noriega

san

francisco

j.

bush
new
york

    personal pronouns (prp):

prp-0
prp-1
prp-2

it
it
it

he
he
them

sept.
james

l.

peters
wall
street

i

they
him

21

learned  splits

rbr-0
rbr-1
rbr-2

    relative  adverbs  (rbr):
further
more
earlier
    cardinal  numbers  (cd):
one
1989
million

cd-7
cd-4
cd-11
cd-0
cd-3
cd-9

1
1
78

lower
less
earlier

two
1990
billion

50
30
58

higher
more
later

three
1988
trillion
100
31
34

22

final  results  (accuracy)

e
n
g

g
e
r

c
h
n

charniak&johnson    05 (generative)

split / merge

dubey    05
split / merge

chiang et al.    02
split / merge

    40 words

f1
90.1
90.6

76.3
80.8

80.0
86.3

all 
f1
89.6
90.1

-

80.1

76.6
83.4

still higher numbers from reranking / self-training methods

23

efficient  parsing  for

hierarchical  grammars

24

coarse   to   fine  id136

    example:  pp  attachment

?????????

25

hierarchical  pruning

coarse:

    qp np vp    

split in two:

    qp1 qp2 np1 np2 vp1 vp2    

split in four:

    qp1 qp1 qp3 qp4 np1 np2 np3 np4 vp1 vp2 vp3 vp4    

split in eight:                                                                    

26

bracket  posteriors

27

1621  min
111  min
35  min
15  min

(no  search  error)

28

other  syntactic  models

29

parse  reranking

    assume  the  number  of  parses  is  very  small
    we  can  represent  each  parse  t  as  an  arbitrary  feature  vector     (t)

    typically,  all  local  rules  are  features
    also  non   local  features,  like  how  right   branching  the  overall  tree  is
    [charniak  and  johnson  05]  gives  a  rich  set  of  features

30

k   best  parsing [huang and chiang 05, 

pauls, klein, quirk 10]

31

dependency  parsing

    lexicalized  parsers  can  be  seen  as  producing  dependency  trees

questioned

lawyer

the

witness

the

    each  local  binary  tree  corresponds  to  an  attachment  in  the  dependency  

graph

32

dependency  parsing

    pure  dependency  parsing  is  only  cubic  [eisner  99]

x[h]

y[h] z[h   ]

h

h

h   

i           h          k         h             j

h          k         h              

    some  work  on  non   projective dependencies

    common  in,  e.g.  czech  parsing
    can  do  with  mst  algorithms  [mcdonald  and  pereira  05]

33

shift   reduce  parsers

    another  way  to  derive  a  tree:

    parsing

    no  useful  dynamic  programming  search
    can  still  use  beam  search  [ratnaparkhi  97]

34

data   oriented  parsing:

    rewrite  large  (possibly  lexicalized)  subtrees  in  a  single  step

    formally,  a  tree   insertion  grammar
    derivational  ambiguity  whether  subtrees  were  generated  atomically  

or  compositionally

    most  probable  parse  is  np   complete

35

tig:  insertion

36

tree   adjoining  grammars

    start  with  local  trees
    can  insert  structure  

with  adjunction  
operators

    mildly  context   

sensitive

    models  long   distance  

dependencies  
naturally

         as  well  as  other  

weird  stuff  that  id18s  
don   t  capture  well  
(e.g.  cross   serial  
dependencies)

37

tag:  long  distance

38

id35  parsing

    combinatory  

categorial  grammar
    fully  (mono   )  

lexicalized  grammar
    categories  encode  

argument  sequences
    very  closely  related  

to  the  lambda  
calculus  (more  later)

    can  have  spurious  
ambiguities  (why?)

39

