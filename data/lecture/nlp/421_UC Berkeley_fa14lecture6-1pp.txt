natural  language  processing

acoustic  models
dan  klein      uc  berkeley

1

the  noisy  channel  model

acoustic model: id48s over 
word positions with mixtures 
of gaussians as emissions

language model: 

distributions over sequences 

of words (sentences)

figure:  j  &  m

2

speech  recognition  architecture

figure:  j  &  m

3

feature  extraction

4

digitizing  speech

figure:  bryan  pellom

5

frame extraction

    a frame (25 ms wide) extracted every 10 ms

25 ms

10ms

a1      a2      a3

. . .

figure:  simon  arnfield

6

mel  freq.  cepstral coefficients

    do  fft  to  get  spectral  information

    like  the  spectrogram  we  saw  earlier

    apply  mel  scaling

    models  human  ear;  more  sensitivity  

in  lower  freqs

    approx linear  below  1khz,  log  above,  
equal  samples  above  and  below  1khz

    plus  discrete  cosine  transform

[graph:  wikipedia]

7

final  feature  vector

    39  (real)  features  per  10  ms  frame:

    12  mfcc  features
    12  delta  mfcc  features
    12  delta   delta  mfcc  features
    1  (log)  frame  energy
    1  delta  (log)  frame  energy
    1  delta   delta  (log  frame  energy)

    so  each  frame  is  represented  by  a  39d  vector

8

emission  model

9

id48s  for  continuous  observations

    before:  discrete  set  of  observations

    now:  feature  vectors  are  real   valued

    solution  1:  discretization
    solution  2:  continuous  emissions

    gaussians
    multivariate  gaussians
    mixtures  of  multivariate  gaussians

    a  state  is  progressively

    context  independent  subphone (~3  per  

phone)

    context  dependent  phone  (triphones)
    state  tying  of  cd  phone

10

vector  quantization

    idea:  discretization

    map  mfcc  vectors  onto  

discrete  symbols  

    compute  probabilities  

just  by  counting

    this  is  called  vector  
quantization  or  vq

    not  used  for  asr  any  

more

    but:  useful  to  consider  as  

a  starting  point

11

gaussian  emissions

    vq  is  insufficient  for  top   

quality  asr
    hard  to  cover  high   

dimensional  space  with  
codebook

    moves  ambiguity  from  the  
model  to  the  preprocessing

    instead:  assume  the  
possible  values  of  the  
observation  vectors  are  
normally  distributed.
    represent  the  observation  

likelihood  function  as  a  
gaussian?

from bartus.org/akustyk

12

gaussians  for  acoustic  modeling

a gaussian is parameterized by a mean and a variance:

    p(x):

p(x)

p(x) is highest here at mean

p(x) is low here, far from mean

x

13

multivariate  gaussians

    instead  of  a  single  mean      and  variance     2:

    vector  of  means      and  covariance  matrix     

    usually  assume  diagonal  covariance  (!)

    this  isn   t  very  true  for  fft  features,  but  is  less  bad  for  mfcc  features

14

gaussians:  size  of     

        =  [0  0]                           =  [0  0]                    
        =  i  
    as      becomes  larger,  gaussian  becomes  more  spread  

    =  [0  0]  
    =  2i

    =  0.6i

out;  as      becomes  smaller,  gaussian  more  
compressed

text  and  figures  from  andrew  ng

15

gaussians:  shape  of     

    as  we  increase  the  off  diagonal  entries,  more  correlation  between  

value  of  x  and  value  of  y

text  and  figures  from  andrew  ng

16

but  we   re  not  there  yet

    single  gaussians  may  do  a  

bad  job  of  modeling  a  
complex  distribution  in  any  
dimension

    even  worse  for  diagonal  

covariances

    solution:  mixtures  of  

gaussians

from openlearn.open.ac.uk

17

mixtures  of  gaussians

    mixtures  of  gaussians:

from  robots.ox.ac.uk

http://www.itee.uq.edu.au/~comp4702

18

gmms

    summary:  each  state  has  an  emission  
distribution  p(x|s)  (likelihood  function)  
parameterized  by:
    m  mixture  weights
    m  mean  vectors  of  dimensionality  d
    either  m covariance  matrices  of  dxd or  m  

dx1  diagonal  variance  vectors

    like  soft  vector  quantization  after  all
    think  of  the  mixture  means  as  being  

learned  codebook  entries

    think  of  the  gaussian  densities  as  a  
learned  codebook  distance  function

    think  of  the  mixture  of  gaussians  like  a  

multinomial  over  codes

    (even  more  true  given  shared  gaussian  

inventories,  cf next  week)

19

state  model

20

state  transition  diagrams

    bayes  net:  id48  as  a  graphical  model

w

x

w

x

w

x

    state  transition  diagram:  markov  model  as  a  weighted  fsa

the

cat

chased

has

dog

21

asr  lexicon

figure:  j  &  m

22

lexical  state  structure

figure:  j  &  m

23

adding  an  lm

figure  from  huang  et  al  page  618

24

state  space

    state  space  must  include

    current  word  (|v|  on  order  of  20k+)
    index  within  current  word  (|l|  on  order  of  5)

    acoustic  probabilities  only  depend  on  phone  type

    e.g.  p(x|lec[t]ure)  =  p(x|t)

    from  a  state  sequence,  can  read  a  word  sequence

25

state  refinement

26

phones  aren   t  homogeneous

5000

)
z
h

(
 
y
c
n
e
u
q
e
r
f

0

0.48152

ay

time (s)

k

0.937203

27

need  to  use  subphones

figure:  j  &  m

28

a  word  with  subphones

figure:  j  &  m

29

modeling  phonetic  context

w  iy                    r  iy                          m  iy                        n  iy

30

   need     with  triphone  models

figure:  j  &  m

31

lots  of  triphones

    possible  triphones:  50x50x50=125,000

    how  many  triphone  types  actually  occur?

    20k  word  wsj  task  (from  bryan  pellom)

    word  internal  models:    need  14,300  triphones
    cross  word  models:  need  54,400  triphones

    need  to  generalize  models,  tie  triphones

32

state  tying  /  id91

    [young,  odell,  woodland  

1994]

    how  do  we  decide  which  

triphones  to  cluster  
together?

    use  phonetic  features (or  
   broad  phonetic  classes   )
    stop
    nasal
    fricative
    sibilant
    vowel
    lateral

figure:  j  &  m

33

state  space

    state  space  now  includes

    current  word:  |w|  is  order  20k
    index  in  current  word:  |l|  is  order  5
    subphone position:  3

    acoustic  model  depends  on  clustered  phone  context

    but  this  doesn   t  grow  the  state  space

34

decoding

35

id136  tasks

most  likely  word  sequence:

d                 

ae                           

d

most  likely  state  sequence:    

d1   d6   d6   d4   ae5   ae2   ae3   ae0   d2   d2   d3   d7   d5

36

viterbi  decoding

figure:  enrique  benimeli

37

viterbi  decoding

figure:  enrique  benimeli

38

emission  caching
    problem:  scoring  all  the  p(x|s)  values  is  too  slow
    idea:  many  states  share  tied  emission  models,  so  cache  them

39

prefix  trie encodings

    problem:  many  partial   word  states  are  indistinguishable
    solution:  encode  word  production  as  a  prefix  trie (with  

pushed  weights)

0.04

0.02

0.01

n

n

n

i

i

o

d

t

t

0.04

1

n
0.25

d

t

t

1

0.5

1

i

o

    a  specific  instance  of  minimizing  weighted  fsas  [mohri,  94]

figure:  aubert,  02

40

beam  search

    problem:  trellis  is  too  big  to  compute  v(s)  vectors
    idea:  most  states  are  terrible,  keep  v(s)  only  for  top  states  at  

each  time

the  b.
the  m.
and  then.
at  then.

the  ba.
the  be.
the  bi.

the  ma.
the  me.
the  mi.

then  a.
then  e.
then  i.

the  ba.
the  be.
the  ma.
then  a.

    important:  still  dynamic  programming;  collapse  equiv states

41

lm  factoring

    problem:  higher   order  n   grams  explode  the  state  space
    (one)  solution:

    factor  state  space  into  (word  index,  lm  history)
    score  unigram  prefix  costs  while  inside  a  word
    subtract  unigram  cost  and  add  trigram  cost  once  word  is  complete

the

0.04

1

n
0.25

d

t

t

1

0.5

1

i

o

42

lm  reweighting

    noisy  channel  suggests

    in  practice,  want  to  boost  lm

    also,  good  to  have  a     word  bonus     to  offset  lm  costs

    these  are  both  consequences  of  broken  independence  

assumptions  in  the  model

43

