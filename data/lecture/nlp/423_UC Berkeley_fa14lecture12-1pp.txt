natural  language  processing

parsing  iv

dan  klein      uc  berkeley

1

other  syntactic  models

2

dependency  parsing

    lexicalized  parsers  can  be  seen  as  producing  dependency  trees

questioned

lawyer

the

witness

the

    each  local  binary  tree  corresponds  to  an  attachment  in  the  dependency  

graph

3

dependency  parsing

    pure  dependency  parsing  is  only  cubic  [eisner  99]

x[h]

y[h] z[h   ]

h

h

h   

i           h          k         h             j

h          k         h              

    some  work  on  non   projective dependencies

    common  in,  e.g.  czech  parsing
    can  do  with  mst  algorithms  [mcdonald  and  pereira  05]

4

shift   reduce  parsers

    another  way  to  derive  a  tree:

    parsing

    no  useful  dynamic  programming  search
    can  still  use  beam  search  [ratnaparkhi  97]

5

tree  insertion  grammars

    rewrite  large  (possibly  lexicalized)  subtrees in  a  single  step

    formally,  a  tree   insertion  grammar
    derivational  ambiguity  whether  subtrees were  generated  atomically  

or  compositionally

    most  probable  parse  is  np   complete

6

tig:  insertion

7

tree   adjoining  grammars

    start  with  local  trees
    can  insert  structure  

with  adjunction  
operators

    mildly  context   

sensitive

    models  long   distance  

dependencies  
naturally

         as  well  as  other  

weird  stuff  that  id18s  
don   t  capture  well  
(e.g.  cross   serial  
dependencies)

8

tag:  long  distance

9

id35  parsing

    combinatory  

categorial  grammar
    fully  (mono   )  

lexicalized  grammar
    categories  encode  

argument  sequences
    very  closely  related  

to  the  lambda  
calculus  (more  later)

    can  have  spurious  
ambiguities  (why?)

10

empty  elements

11

empty  elements

    in  the  ptb,  three  kinds  of  empty  elements:

    null  items  (usually  complementizers)
    dislocation  (wh   traces,  topicalization,  relative  clause  and  

heavy  np  extraposition)

    control  (raising,  passives,  control,  shared  argumentation)

    need  to  reconstruct  these  (and  resolve  any  

indexation)

12

example:  english

13

example:  german

14

types  of  empties

15

a  pattern   matching  approach

    [johnson  02]

16

pattern   matching  details
    something  like  transformation   based  learning
    extract  patterns

    details:  transitive  verb  marking,  auxiliaries
    details:  legal  subtrees

    rank  patterns

    pruning  ranking:  by  correct  /  match  rate
    application  priority:  by  depth

    pre   order  traversal
    greedy  match

17

top  patterns  extracted

18

results

19

semantic  roles

20

semantic  role  labeling  (srl)

    characterize  clauses  as  relations with  roles:

    says  more  than  which  np  is  the  subject  (but  not  much  more):
    relations  like  subject are  syntactic,  relations  like  agent or  message are  

semantic

    typical  pipeline:

    parse,  then  label  roles
    almost  all  errors  locked  in  by  parser
    really,  srl  is  quite  a  lot  easier  than  parsing

21

srl  example

22

propbank  /  framenet

    framenet:  roles  shared  between  verbs
    propbank:  each  verb  has  its  own  roles
    propbank more  used,  because  it   s  layered  over  the  treebank (and  so  has  

greater  coverage,  plus  parses)

    note:  some  linguistic  theories  postulate  fewer  roles  than  framenet (e.g.  

5   20  total:  agent,  patient,  instrument,  etc.)

23

propbank  example

24

propbank  example

25

propbank  example

26

shared  arguments

27

path  features

28

results

    features:

    path  from  target  to  filler
    filler   s  syntactic  type,  headword,  case
    target   s  identity
    sentence  voice,  etc.
    lots  of  other  second   order  features

    gold  vs  parsed  source  trees

    srl  is  fairly  easy  on  gold  trees

    harder  on  automatic  parses

29

empties    and  srl

30

