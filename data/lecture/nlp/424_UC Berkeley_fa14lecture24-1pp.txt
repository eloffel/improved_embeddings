natural  language  processing

question  answering

dan  klein      uc  berkeley

the  following  slides  are  largely  from  chris  manning,  includeing many  slides  

originally  from  sanda harabagiu,  isi,  and  nicholas  kushmerick.

1

watson

2

large   scale  nlp:  watson

3

qa  vs  search

4

people  want to  ask  questions?

examples of search queries
who invented surf music?
how to make stink bombs
where are the snowdens of yesteryear?
which english translation of the bible is used in official catholic 

liturgies?
how to do clayart
how to copy psx
how tall is the sears tower?
how can i find someone in texas
where can i find information on puritan religion?
what are the 7 wonders of the world
how can i eliminate stress
what vacuum cleaner does consumers guide recommend

around 10   15% of query logs

5

a  brief  (academic)  history

    question  answering  is  not  a  new  research  area  
    question  answering  systems  can  be  found  in  many  

areas  of  nlp  research,  including:
    natural  language  database  systems

    a  lot  of  early  nlp  work  on  these

    spoken  dialog  systems

    currently  very  active  and  commercially  relevant

    the  focus  on  open   domain  qa  is  (relatively)  new

    murax  (kupiec 1993):  encyclopedia answers
    hirschman:  reading  comprehension  tests
    trec  qa  competition:  1999   

6

trec

7

question  answering  at  trec

    question  answering  competition  at  trec  consists  of  

answering  a  set  of  500  fact   based  questions,  e.g.,

   when  was  mozart  born?   .

    for  the  first  three  years  systems  were  allowed  to  return  5  
ranked  answer  snippets  (50/250  bytes)  to  each  question.
    ir  think
    mean  reciprocal  rank  (mrr)  scoring:

    1,  0.5,  0.33,  0.25,  0.2,  0  for  1,  2,  3,  4,  5,  6+  doc

    mainly  named  entity  answers  (person,  place,  date,     )

    from  2002+  the  systems  are  only  allowed  to  return  a  single  

exact answer  and  a  notion  of  confidence  has  been  
introduced.

8

sample  trec  questions

1. who is the author of the book, "the iron lady:  a 

biography of margaret thatcher"?

2. what was the monetary value of the nobel peace

prize in 1989?

3. what does the peugeot company manufacture?
4. how much did mercury spend on advertising in 1993?
5. what is the name of the managing director of apricot

computer?

6. why did david koresh ask the fbi for a word processor?
7. what debts did qintex group leave?
8. what is the name of the rare neurological disease with 

symptoms such as:  involuntary movements (tics), swearing,
and incoherent vocalizations (grunts, shouts, etc.)?

9

top  performing  systems

    currently  the  best  performing  systems  at  trec  can  

answer  approximately  70%  of  the  questions

    approaches  and  successes  have  varied  a  fair  deal

    knowledge   rich  approaches,  using  a  vast  array  of  nlp  
techniques  stole  the  show  in  2000,  2001,  still  do  well
    notably  harabagiu,  moldovan  et  al.      smu/utd/lcc

    askmsr system  stressed  how  much  could  be  achieved  by  
very  simple  methods  with  enough  text  (and  now  various  
copycats)

    middle  ground  is  to  use  large  collection  of  surface  

matching  patterns  (isi)

    emerging  standard:  analysis,  soft   matching,  abduction

10

pattern  induction:  isi

11

webclopedia  architecture

12

13

14

15

ravichandran  and  hovy  2002

learning  surface  patterns

    use  of  characteristic  phrases
    "when  was  <person>  born   

    typical  answers

    "mozart  was  born  in  1756.   
    "gandhi  (1869   1948)...   

    suggests  phrases  like

    "<name>  was  born  in  <birthdate>   
    "<name>  (  <birthdate>      

    regular  expressions

16

use  pattern  learning

    example:  start  with     mozart  1756   

    results:

young  age   

       the  great  composer  mozart  (1756   1791)  achieved  fame  at  a  

       mozart  (1756   1791)  was  a  genius   
       the  whole  world  would  always  be  indebted  to  the  great  

music  of  mozart  (1756   1791)   

    longest  matching  substring  for  all  3  sentences  is  

    suffix  tree  would  extract  "mozart  (1756   1791)"  as  an  

"mozart  (1756   1791)   

output,  with  score  of  3

    reminiscent  of  ie  pattern  learning

17

pattern  learning  (cont.)

    repeat  with  different  examples  of  same  question  type

       gandhi  1869   ,     newton  1642   ,  etc.

    some  patterns  learned  for  birthdate

    a.  born  in  <answer>,  <name>
    b.  <name>  was  born  on  <answer>  ,  
    c.  <name>  (  <answer>     
    d.  <name>  (  <answer>      )

18

pattern  precision

    birthdate  table:

    1.0
    0.85
    0.6
    0.59
    0.53
    0.50
    0.36

<name>  (  <answer>      )
<name>  was  born  on  <answer>,
<name>  was  born  in  <answer>
<name>  was  born  <answer>
<answer>  <name>  was  born
    <name>  (  <answer>
<name>  (  <answer>     

    inventor

    1.0
    1.0
    1.0

<answer>  invents  <name>
the  <name>  was  invented  by  <answer>
<answer>  invented  the  <name>  in

19

pattern  precision

    why   famous

    1.0
    1.0
    0.71

<answer>  <name>  called
laureate  <answer>  <name>
<name>  is  the  <answer>  of

    location
    1.0
    1.0
    0.92

<answer>'s  <name>
regional  :  <answer>  :  <name>
near  <name>  in  <answer>

    depending  on  question  type,  get  high  mrr  (0.6   0.9),  with  

higher  results  from  use  of  web  than  trec  qa  collection

20

shortcomings  &  extensions

    need  for  pos  &/or  semantic  types

    "where  are  the  rocky  mountains?   
    "denver's  new  airport,  topped  with  white  fiberglass  cones  in  

imitation  of  the  rocky  mountains  in  the  background ,  continues  to  
lie  empty   

    <name>  in  <answer>

    long  distance  dependencies

    "where  is  london?   
    "london,  which  has  one  of  the  busiest  airports  in  the  world,  lies  on  

the  banks  of  the  river  thames   

    would  require  pattern  like:

<question>,  (<any_word>)*,  lies  on  <answer>

    but:  abundance  of  web  data  compensates

21

aggregation:  askmsr

22

askmsr

    web  question  answering:  is  more  always  better?

    dumais,  banko,  brill,  lin,  ng  (microsoft,  mit,  berkeley)

    q:     where  is

the  louvre
located?   

    want     paris   
or     france   
or     75058
paris  cedex  01   
or  a  map
    don   t just
want  urls

23

askmsr:  shallow  approach

    in  what  year  did  abraham  lincoln  die?
    ignore  hard  documents  and  find  easy  ones

24

askmsr:  details

1

5

2

3

4

25

step  1:  rewrite  queries

    intuition:  the  user   s  question  is  often  syntactically  quite  close  

to  sentences  that  contain  the  answer

    where  is the louvre museum located?

    the louvre museum is located in  paris

    who  created the character of scrooge?

    charles  dickens created the character of scrooge.

26

query  rewriting:  variations

   

   
   
   

classify  question  into  seven  categories

who is/was/are/were   ?
when is/did/will/are/were     ?
where is/are/were     ?

a.  category   specific  transformation  rules

eg    for  where  questions,  move     is     to  all  possible  locations   
   where  is the  louvre  museum  located   
       is the  louvre  museum  located   
       the  is louvre  museum  located   
       the  louvre  is museum  located   
       the  louvre  museum  is located   
       the  louvre  museum  located  is   

b.  expected  answer     datatype     (eg,  date,  person,  location,     )

when was  the  french  revolution?        date

nonsense,
but who
cares?  it   s
only a few
more queries

   

hand   crafted  classification/rewrite/datatype rules
(could  they  be  automatically  learned?)

27

query  rewriting:  weights

    one  wrinkle:  some  query  rewrites  are  more  reliable  

than  others

where is the louvre museum located?

weight  5
if we get a match,

it   s probably right

weight  1
lots of non-answers
could come back too

+   the louvre museum is located   

+louvre +museum +located

28

step  2:  query  search  engine

    send  all  rewrites  to  a  search  engine
    retrieve  top  n  answers  (100?)
    for  speed,  rely  just  on  search  engine   s     snippets   ,  

not  the  full  text  of  the  actual  document

29

step  3:  mining  n   grams

    simple:  enumerate  all  n   grams  (n=1,2,3  say)  in  all  retrieved  

snippets

    weight  of  an  n   gram:  occurrence  count,  each  weighted  by  
   reliability     (weight)  of  rewrite  that  fetched  the  document

    example:     who  created  the  character  of  scrooge?   

    dickens      117
    christmas  carol      78
    charles  dickens      75
    disney      72
    carl  banks      54
    a  christmas      41
    christmas  carol      45
    uncle      31

30

step  4:  filtering  n   grams

    each  question  type  is  associated  with  one  or  more  

   data   type  filters     =  regular  expression

    when   
    where   
    what     
    who     

date
location

person

    boost  score  of    n   grams  that  do  match  regexp
    lower  score  of  n   grams  that  don   t  match  regexp
    details  omitted  from  paper   .

31

step  5:  tiling  the  answers

scores

20

15

10

charles    dickens 
dickens

mr charles

merged, discard

old id165s

score 45

mr charles  dickens

tile highest-scoring id165

id165s

id165s

repeat, until no more overlap

32

results

    standard  trec  contest  test   bed:

~1m  documents;  900  questions

    technique  doesn   t  do  too  well  (though  would  have  placed  in  

top  9  of  ~30  participants!)
    mrr  =  0.262  (ie,  right  answered  ranked  about  #4   #5  on  average)
    why?    because  it  relies  on  the  redundancy  of  the  web

    using  the  web  as  a  whole,  not  just  trec   s  1m  documents       
mrr  =  0.42  (ie,  on  average,  right  answer  is  ranked  about  #2   
#3)

33

issues

    in  many  scenarios  (e.g.,  an  individual   s  email   )  we  only  have  

a  limited  set  of  documents

    works  best/only  for     trivial  pursuit      style  fact   based  

questions

    limited/brittle  repertoire  of

    question  categories
    answer  data  types/filters
    query  rewriting  rules

34

abduction:  lcc

35

lcc:  harabagiu,  moldovan  et  al.

36

value  from  sophisticated  nlp
pasca  and  harabagiu  (2001)

    good  ir  is  needed:  smart  paragraph  retrieval  
    large  taxonomy  of  question  types  and  expected  answer  types  is  

crucial

    statistical  parser  used  to  parse  questions  and  relevant  text  for  

answers,  and  to  build  kb

    query  expansion  loops  (morphological,  lexical  synonyms,  and  

semantic  relations)  important  

    answer  ranking  by  simple  ml  method

37

abductive  id136

    system  attempts  id136  to  justify  an  answer  

(often  following  lexical  chains)

    their  id136  is  a  kind  of  funny  middle  ground  

between  logic  and  pattern  matching

    but  quite  effective:  30%  improvement
    q:  when  was  the  internal  combustion  engine  

invented?

    a:  the  first  internal   combustion  engine  was  built  in  

1867.

    invent     >  create_mentally    >  create     >  build

38

question  answering  example

    how  hot  does  the  inside  of  an  active  volcano  get?  
       lava  fragments  belched  out  of  the  mountain  were  as  

hot  as  300  degrees  fahrenheit     
    volcano  isa  mountain  
    lava  ispartof  volcano         lava  in  volcano  
    fragments  of  lava  havepropertiesof  lava  

    the  needed  semantic  information  is  in  id138  

definitions,  and  was  successfully  translated  into  a  form  
that  was  used  for  rough     proofs     

39

watson

slides from ferrucci et al, ai magazine, 2010

40

jeopardy   

41

architecture

42

watson  on  trec

43

human  p/r

44

metric  climbing

45

complex  qa

46

example  of  complex  questions

how have thefts impacted on the safety of russia   s nuclear navy, 
and has the theft problem been increased or reduced over time?

need of domain knowledge

to what degree do different thefts put nuclear
or radioactive materials at risk?

question decomposition

definition questions:
    what is meant by nuclear navy?
    what does    impact    mean?
    how does one define the increase or decrease of a problem?

factoid questions:
    what is the number of thefts that are likely to be reported?
    what sort of items have been stolen?

alternative questions:
    what is meant by russia? only russia, or also former soviet 

facilities in non-russian republics?

47

