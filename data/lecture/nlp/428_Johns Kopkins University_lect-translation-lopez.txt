machine translation

80 years of 

machine translation

in 1 hour

80 years of 

machine translation

in 1 hour

for more hours: mt-class.org

                          ,                                          

                          ,                                          

however , the sky remained clear under the strong north wind .

the tower of babel

pieter brueghel the elder (1563)

georges artsrouni   s    mechanical brain   , 

patented 1933 (france)

eniac (1946)

when i look at an article 
in russian, i say:    this 

is really written in 

english, but it has been 
coded in some strange 
symbols. i will now 
proceed to decode.   

warren weaver (1949)

popular view of mt in 2003

popular view of mt in 2013

statistical machine 
translation live

4/28/2006 03:40:00 pm

franz och

because we want to provide everyone with access to 
all the world's information, including information 
written in every language, one of the exciting projects 
at google research is machine translation... now you 
can see the results for yourself. we recently launched 
an online version of our system for arabic-english 

and english-arabic. try it out!

7 years later

how did they do this?

...but the id203 that an event has 
happened is the same as the id203 i 

have to guess right if i guess it has 
happened.  wherefore the following 
proposition is evident: if there be two 

subsequent events, the id203 of the 2d 
b/n and the id203 both together p/n, 

and it being 1st discovered that the 2d 
event has also happened, the id203 i 

am right is p/b.

thomas bayes

...but the id203 that an event has 
happened is the same as the id203 i 

have to guess right if i guess it has 
happened.  wherefore the following 
proposition is evident: if there be two 

subsequent events, the id203 of the 2d 
b/n and the id203 both together p/n, 

and it being 1st discovered that the 2d 
event has also happened, the id203 i 

am right is p/b.

thomas bayes

(image by 
chris dyer)

bayes    rule

p(english|chinese) =

p(english)    p(chinese|english)

p(chinese)

prior

likelihood

id172 term (ensures we   re 
working with valid probabilities).

when i look at an article 
in russian, i say:    this 

is really written in 

english, but it has been 
coded in some strange 
symbols. i will now 
proceed to decode.   

warren weaver (1949)

claude shannon

bayes    rule

p(english|chinese) =

p(english)    p(chinese|english)

p(chinese)

prior

likelihood

id172 term (ensures we   re 
working with valid probabilities).

noisy channel

p(english|chinese) =

p(english)    p(chinese|english)

p(chinese)

signal model

channel model

id172 term (ensures we   re 
working with valid probabilities).

machine translation

p(english|chinese) =

p(english)    p(chinese|english)

p(chinese)

language model

translation model

id172 term (ensures we   re 
working with valid probabilities).

p(chinese|english)

english

p(chinese|english)

   p(english)

english

p(chinese|english)

   p(english)

    p(english|chinese)

english

machine translation

p(english|chinese) =

p(english)    p(chinese|english)

p(chinese)

language model

translation model

id172 term (ensures we   re 
working with valid probabilities).

machine translation

p(english|chinese)    

p(english)    p(chinese|english)

machine translation

p(english|chinese)    

p(english)    p(chinese|english)

what is the id203 of an english sentence? 

machine translation

p(english|chinese)    

p(english)    p(chinese|english)

what is the id203 of an english sentence? 

what is the id203 of a chinese sentence, given a 

particular english sentence? 

language models

our language model must assign a id203 

to every possible english sentence.

language models

our language model must assign a id203 

to every possible english sentence.

q: what should this model look like?

language models

our language model must assign a id203 

to every possible english sentence.

q: what should this model look like?

q2: what is the dumbest thing you can think of?

language models

our language model must assign a id203 

to every possible english sentence.

q: what should this model look like?

q2: what is the dumbest thing you can think of?

a: an id165 model.

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

p(english length|chinese length)

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

p(english length|chinese length)

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

p(chinese word position)

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however 

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however 

p(english word|chinese word)

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however 

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however 

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however  , 

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however  , 

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however  ,  the 

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however  ,  the  sky remained clear under the strong north wind .

ibm model 1

p(despite|
p(however|
p(although|

)      

      

      

)

)

p(northern|
p(north|

)
   
)   

ibm model 1

p(despite|
p(however|
p(although|

)      

      

      

)

)

p(northern|
p(north|

)
   
)   

???

???

???

???
???

  {

ibm model 1

)      

p(despite|
p(however|
p(although|

p(northern|
p(north|

      

      

)

)

)
   
)   

???

???

???

???
???

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

however , the sky remained clear under the strong north wind .

p(north|

)   

=

???

suppose that we only ever see     aligned to

north or northern. 

        north

        northern

suppose that we only ever see     aligned to

north or northern. 

        north
p(north|

)   

        northern

p(northern|

   

)

suppose that we only ever see     aligned to

north or northern. 

        north
p(north|

)   

        northern
p(north|

1-

)   

        north
        northern
        north
        northern
        north

        north
        north
        north
        north
        northern

        north
        northern
        north
        northern
        north

        north
        north
        north
        north
        northern

p(north|

)    ?

        north
        northern
        north
        northern
        north

        north
        north
        north
        north
        northern

p(north|

)    ?

p(data) = p(north|

   

)7 + p(northern|

   

)3

        north
        northern
        north
        northern
        north

        north
        north
        north
        north
        northern

p(north|

)    ?

p(data) = p(north|

   

)7 + [1     p(north|

   

)]3

p(data) = p(north|

   

)7 + [1     p(north|

   

)]3

p(data)

0

p(north|

)   

1

p(data) = p(north|

   

)7 + [1     p(north|

   

)]3

p(data)

0

p(north|

)   

1

p(data) = p(north|

   

)7 + [1     p(north|

   

)]3

p(data)

0

.7

1

p(north|

)   

ibm model 1

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

however , the sky remained clear under the strong north wind .

p(north|

)   

=

# of times     aligns to north

# of times     occurs

    optimizationid113 for ibm model 1 (observed)

     = arg max

  

n   n=1

      p(i (n)|j (n))

i(n)   i=1

p(a(n)

i

|j (n))    p(f (n)

i

|e(n)

ai )      

id113 for ibm model 1 (observed)
number of
sentences

alignment of french 
word at position i

     = arg max

  

n   n=1

      p(i (n)|j (n))

i(n)   i=1

p(a(n)

i

|j (n))    p(f (n)

i

|e(n)

ai )      

french, english
sentence lengths

french, english

word pair

id113 for ibm model 1 (observed)

     = arg max

  

n   n=1

      p(i (n)|j (n))

i(n)   i=1

constant!{

p(a(n)

i

|j (n))    p(f (n)

i

|e(n)

ai )      

id113 for ibm model 1 (observed)

     = arg max

c

  

n   n=1

i(n)   i=1

p(f (n)

i

ai )
|e(n)

id113 for ibm model 1 (observed)

     = arg max

  

log      c

n   n=1

i(n)   i=1

p(f (n)

i

|e(n)

ai )      

log(a) < log(b)        a < b

id113 for ibm model 1 (observed)

     = arg max

  

log      c      f,e

p(f|e)count(   f,e   )      

id113 for ibm model 1 (observed)

     = arg max

  

log c +   f,e

count(   f, e   ) log p(f|e)

log of product = sum of logs

id113 for ibm model 1 (observed)

  (  ,   ) = log c +   f,e
count(   f, e   ) log p(f|e)
      e

p(f|e)     1      
{

  e         f

lagrange multiplier expresses id172 constraint

id113 for ibm model 1 (observed)

  (  ,   ) = log c +   f,e
count(   f, e   ) log p(f|e)
      e
= count(   f, e   )

p(f|e)     1      

  e         f

p(f|e)

      e

     (  ,   )
   p(f|e)

derivative

id113 for ibm model 1 (observed)
although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

however , the sky remained clear under the strong north wind .

p(north|

)   

=

# of times     aligns to north

# of times     occurs

id113 for ibm model 1 (unobserved)
although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

however , the sky remained clear under the strong north wind .

p(north|

)   

=

???

id113 for ibm model 1 (observed)

     = arg max

  

log      c

n   n=1

i(n)   i=1

p(f (n)

i

|e(n)

ai )      

id113 for ibm model 1 (unobserved)

     = arg max

  

p(f (n)

i

i(n)   i=1

|e(n)

ai )      

n   n=1   a

log      c
p(f|e) =   a

p(f, a|e)

marginalize over alignments:

id113 for ibm model 1 (unobserved)

     = arg max

  

log      c      f,e

p(f|e)e[count(   f,e   )]      

id113 for ibm model 1 (unobserved)

     = arg max

  

log      c      f,e

p(f|e)e[count(   f,e   )]      

not constant! depends on parameters, 

no analytic solution.

id113 for ibm model 1 (unobserved)

     = arg max

  

log      c      f,e

p(f|e)e[count(   f,e   )]      

not constant! depends on parameters, 

no analytic solution.

but it does strongly imply an iterative solution.

likelihood estimation for model 1
although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

parameters and alignments are both unknown.

,  the  sky remained clear under the strong north wind .

however 
p(english word|chinese word)

unobserved!

likelihood estimation for model 1
although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

parameters and alignments are both unknown.
if we knew the alignments, we could 
calculate the values of the  parameters.

,  the  sky remained clear under the strong north wind .

however 
p(english word|chinese word)

unobserved!

likelihood estimation for model 1
although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

parameters and alignments are both unknown.
if we knew the alignments, we could 
calculate the values of the  parameters.

if we knew the parameters, we could calculate 

the likelihood of the data.

,  the  sky remained clear under the strong north wind .

however 
p(english word|chinese word)

unobserved!

likelihood estimation for model 1
although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

parameters and alignments are both unknown.
if we knew the alignments, we could 
calculate the values of the  parameters.

if we knew the parameters, we could calculate 

the likelihood of the data.

,  the  sky remained clear under the strong north wind .

however 
p(english word|chinese word)

unobserved!

the plan: id64

   arbitrarily select a set of parameters (say, uniform).
   calculate expected counts of the unseen events.
   choose new parameters to maximize likelihood, 
using expected counts as proxy for observed counts.
   iterate.
   guarantee: likelihood will be monotonically 
nondecreasing.

the plan: id64

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  

however  ,  the  sky remained clear under the strong north wind .

the plan: id64

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  
if we had observed the 
alignment, this line would 
either be here (count 1) or it 

wouldn   t (count 0).

however  ,  the  sky remained clear under the strong north wind .

the plan: id64

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          
  
if we had observed the 
alignment, this line would 
either be here (count 1) or it 

wouldn   t (count 0).

since we didn   t observe the 
alignment, we calculate the 
id203 that it   s there.

however  ,  the  sky remained clear under the strong north wind .

marginalize: sum all alignments containing the link

                          ,                                          

p(

p(

p(

however , the sky remained clear under the strong north wind .

                          ,                                          

) +

) +

however , the sky remained clear under the strong north wind .

                          ,                                          

)

however , the sky remained clear under the strong north wind .

divide by sum of all possible alignments

                          ,                                          

however , the sky remained clear under the strong north wind .

                          ,                                          

) +

) +

however , the sky remained clear under the strong north wind .

                          ,                                          

)

however , the sky remained clear under the strong north wind .

p(

p(

p(

divide by sum of all possible alignments

                          ,                                          

however , the sky remained clear under the strong north wind .

                          ,                                          

) +

) +

however , the sky remained clear under the strong north wind .

                          ,                                          

)

p(

p(

p(

however , the sky remained clear under the strong north wind .

is this hard? how many alignments are there?

expectation maximization

id203 of an alignment.

p(f, a|e) = p(i|j)   ai

p(ai = j)p(fi|ej)

expectation maximization

id203 of an alignment.

p(f, a|e) = p(i|j)   ai

p(ai = j)p(fi|ej)

observed

uniform

expectation maximization

id203 of an alignment.

p(f, a|e) = p(i|j)   ai

factors across words.

p(ai = j)p(fi|ej)

observed

uniform

expectation maximization

marginal id203 of 
alignments containing link

.   a   a:    north

   

p(north|

   

)    p(rest of a)

expectation maximization

marginal id203 of 
alignments containing link

p(north|

   

)

.   a   a:    north

   

p(rest of a)

expectation maximization

marginal id203 of 
alignments containing link

p(north|

   

)

   c   chinese words

p(north|c)

   

.   a   a:    north
.   a   a:    north

c

p(rest of a)

p(rest of a)

marginal id203 of all 

alignments

expectation maximization

marginal id203 of 
alignments containing link

p(north|

   

)

   c   chinese words

p(north|c)

   

.   a   a:    north
.   a   a:    north

c

p(rest of a)

p(rest of a)

marginal id203 of all 

alignments

expectation maximization

marginal id203 of 
alignments containing link

p(north|

   

)

   c   chinese words

p(north|c)

   

.   a   a:    north
.   a   a:    north

c

p(rest of a)

p(rest of a)

identical!

marginal id203 of all 

alignments

expectation maximization

p(north|

   

)

.   c   chinese words p(north|c)

expectation maximization

marginal id203 (expected count) of an 

alignment containing the link

p(north|

   

)

.   c   chinese words p(north|c)

expectation maximization

marginal id203 (expected count) of an 

alignment containing the link

p(north|

   

)

.   c   chinese words p(north|c)

for each sentence, use this 
quantity instead of 0 or 1

translation models

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

however , the sky remained clear under the strong north wind .

p(however|

      

) =

# of times        aligns to however

# of times        occurs

translation models

although north wind howls  ,    but     sky      still      very     clear     .

                          ,                                          

however , the sky remained clear under the strong north wind .
expected # of times        

p(however|

      

) =

aligns to however

# of times        occurs

expectation maximization

why does this even work?

p(north|

   

)

.   c   chinese words p(north|c)

expectation maximization

observation 1: we are still solving a 

id113 problem.

expectation maximization

observation 1: we are still solving a 

id113 problem.

p(chinese|english) =    alignments

p(chinese, alignment|english)

expectation maximization

observation 1: we are still solving a 

id113 problem.

p(chinese|english) =    alignments
id113: choose parameters that maximize this 

p(chinese, alignment|english)

expression.

expectation maximization

observation 1: we are still solving a 

id113 problem.

p(chinese|english) =    alignments
id113: choose parameters that maximize this 

p(chinese, alignment|english)

expression.

minor problem: there is no analytic solution.

(from minka    98)

decoding

id203 models enable us to make predictions:
given a particular chinese sentence, what is the most 

probable english sentence corresponding to it?

decoding

id203 models enable us to make predictions:
given a particular chinese sentence, what is the most 

probable english sentence corresponding to it?

in math, we want to solve:

argmaxenglish p(english|chinese)

decoding

id203 models enable us to make predictions:
given a particular chinese sentence, what is the most 

probable english sentence corresponding to it?

in math, we want to solve:

argmaxenglish p(english|chinese)

problem: there are a lot of english sentences to 

choose from!

    optimization                     

                     

substitutions
permutations

                     

substitutions
permutations

o(5n)

                     

substitutions
permutations

o(5n)
o(n!)

                     

substitutions
permutations

o(5n)
o(n!)

15,000 possibilities!

                     

can we do this without enumerating                pairs?

o(5nn!)

                     

the strong north wind .

can we do this without enumerating                pairs?

o(5nn!)

                     

the strong north wind .

can we do this without enumerating                pairs?

o(5nn!)

                     

the strong north wind .

given a sentence pair and an 

alignment, we can easily calculate      

p(english, alignment|chinese)

can we do this without enumerating                pairs?

o(5nn!)

key idea

                     

the strong north wind .

there are                 target sentences.

o(5nn!)

but there are only               ways to start them.

o(5n)

key idea

                     

key idea

                     

key idea

coverage vector

                     

key idea

north

coverage vector

                     

key idea

p(north|st art )    p(

north

   

|north)

coverage vector

                     

key idea

p(north|st art )    p(

north

   

|north)

coverage vector

northern

                     

key idea

p(north|st art )    p(

north

   

|north)

p(northern|st art )    p(

northern

   

|northern)

                     

coverage vector

key idea

strong

p(north|st art )    p(

north

   

|north)

p(northern|st art )    p(

northern

   

|northern)

                     

coverage vector

key idea
p(strong|st art )    p(

strong

      

|strong)

p(north|st art )    p(

north

   

|north)

p(northern|st art )    p(

northern

   

|northern)

                     

coverage vector

key idea

p(north|st art )    p(

north

   

|north)

coverage vector

                     

key idea

p(north|st art )    p(

north

   

|north)

wind

coverage vector

                     

key idea

p(north|st art )    p(

north

   

|north)

wind

p(wind|north)    p(

   

|wind)

coverage vector

                     

key idea

p(north|st art )    p(

north

   

|north)

wind

p(wind|north)    p(

strong

   

|wind)

                     

coverage vector

key idea

p(north|st art )    p(

north

   

|north)

wind

p(wind|north)    p(

strong

   

|wind)

p(strong|north)    p(

                     

      

|strong)

coverage vector

key idea

work done at sentence beginnings is shared across 

many possible output sentences!

p(north|st art )    p(

north

   

|north)

wind

p(wind|north)    p(

strong

   

|wind)

p(strong|north)    p(

                     

      

|strong)

coverage vector

key idea

key idea

key idea

key idea

key idea

key idea

id145

key idea

amount of work:

o(5n2n)

id145

amount of work:

o(5n2n)

key idea

bad, but much 
better than
o(5nn!)

id145

amount of work:

o(5n2n)

key idea

bad, but much 
better than
o(5nn!)

id145

amount of work:

o(5n2n)

key idea

bad, but much 
better than
o(5nn!)

each edge labelled 
with a weight and a 
word (or words)

id145

amount of work:

o(5n2n)

key idea

bad, but much 
better than
o(5nn!)

north, 0.014

each edge labelled 
with a weight and a 
word (or words)

id145

amount of work:

o(5n2n)

key idea

bad, but much 
better than
o(5nn!)

weighted    nite-state automata

north, 0.014

each edge labelled 
with a weight and a 
word (or words)

id145

weighted languages
   the lattice describing the set of all possible 
translations is a weighted    nite state automaton.
   so is the language model.
   since regular languages are closed under 
intersection, we can intersect the devices and run 
shortest path graph algorithms.
   taking their intersection is equivalent to computing 
the id203 under bayes    rule.

practical issues

o(5n2n)

is still far too much work.

practical issues

o(5n2n)

is still far too much work.

can we do better?

can we do better?

                     

can we do better?

                     

north wind the strong .

can we do better?

                     

north wind the strong .

can we do better?

                     

north wind the strong .

each arc weighted by 
translation id203 + 

bigram id203

can we do better?

                     

north wind the strong .

each arc weighted by 
translation id203 + 

bigram id203

objective:    nd shortest path that visits each word once.

can we do better?

                     

london paris ny tokyo .

each arc weighted by 
translation id203 + 

bigram id203

objective:    nd shortest path that visits each word once.

can we do better?

probably not: this is the traveling salesman problem.

                     

london paris ny tokyo .

each arc weighted by 
translation id203 + 

bigram id203

objective:    nd shortest path that visits each word once.

two problems

   exact decoding requires exponential time.
   this is a consequence of arbitrary permutation.
   but in translation reordering is not arbitrary!
   parameterization of reordering is weak.
   no generalization!

la empresa tiene enemigos fuertes en europa .
the company has strong enemies in europe .
garcia and associates .

the clients and the associates are enemies .

garcia y asociados .

carlos garcia has three associates .

los clientes y los asociados son enemigos .

the company has three groups .

carlos garcia tiene tres asociados .

his associates are not strong .

la empresa tiene tres grupos .

its groups are in europe .

sus asociados no son fuertes .
garcia has a company also .

sus grupos estan en europa .

the modern groups sell strong pharmaceuticals .

garcia tambien tiene una empresa .

its clients are angry .

los grupos modernos venden medicinas fuertes .

the groups do not sell zanzanine .

sus clientes estan enfadados .
the associates are also angry .

los grupos no venden zanzanina .
the small groups are not modern .

los asociados tambien estan enfadados .

los grupos pequenos no son modernos .

la empresa tiene enemigos fuertes en europa .
the company has strong enemies in europe .
garcia and associates .

the clients and the associates are enemies .

garcia y asociados .

carlos garcia has three associates .

los clientes y los asociados son enemigos .

the company has three groups .

carlos garcia tiene tres asociados .

his associates are not strong .

la empresa tiene tres grupos .

its groups are in europe .

sus asociados no son fuertes .
garcia has a company also .

sus grupos estan en europa .

the modern groups sell strong pharmaceuticals .

garcia tambien tiene una empresa .

its clients are angry .

los grupos modernos venden medicinas fuertes .

the groups do not sell zanzanine .

sus clientes estan enfadados .
the associates are also angry .

los grupos no venden zanzanina .
the small groups are not modern .

los asociados tambien estan enfadados .

los grupos pequenos no son modernos .

la empresa tiene enemigos fuertes en europa .
the company has strong enemies in europe .
garcia and associates .

the clients and the associates are enemies .

garcia y asociados .

carlos garcia has three associates .

los clientes y los asociados son enemigos .

the company has three groups .

same pattern:
nn jj     jj nn

carlos garcia tiene tres asociados .

his associates are not strong .

la empresa tiene tres grupos .

its groups are in europe .

sus asociados no son fuertes .
garcia has a company also .

sus grupos estan en europa .

the modern groups sell strong pharmaceuticals .

garcia tambien tiene una empresa .

its clients are angry .

los grupos modernos venden medicinas fuertes .

the groups do not sell zanzanine .

sus clientes estan enfadados .
the associates are also angry .

los grupos no venden zanzanina .
the small groups are not modern .

los asociados tambien estan enfadados .

los grupos pequenos no son modernos .

la empresa tiene enemigos fuertes en europa .
the company has strong enemies in europe .
garcia and associates .

the clients and the associates are enemies .

garcia y asociados .

carlos garcia has three associates .

los clientes y los asociados son enemigos .

the company has three groups .

same pattern:
nn jj     jj nn

carlos garcia tiene tres asociados .

finite-state models do not capture 

la empresa tiene tres grupos .

his associates are not strong .

its groups are in europe .

this generalization.

sus asociados no son fuertes .
garcia has a company also .

sus grupos estan en europa .

the modern groups sell strong pharmaceuticals .

garcia tambien tiene una empresa .

its clients are angry .

los grupos modernos venden medicinas fuertes .

the groups do not sell zanzanine .

sus clientes estan enfadados .
the associates are also angry .

los grupos no venden zanzanina .
the small groups are not modern .

los asociados tambien estan enfadados .

los grupos pequenos no son modernos .

context-free grammar

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

context-free grammar

s

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

context-free grammar

s

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

context-free grammar

s

np

vp

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

context-free grammar

s

np

vp

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

hako  wo

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

hako  wo

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

hako  wo

akemasu

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

hako  wo

akemasu

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

s

np

vp

watashi  wa

np

v

hako  wo

akemasu
watashi  wa  hako  wo  akemasu

context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

{

note: this particular grammar 

is    nite, hence regular.

watashi  wa  watashi  wa  akemasu
watashi  wa  hako  wo  akemasu
hako  wo  hako  wo  akemasu
hako  wo watashi  wa  akemasu

}

context-free grammar

   s     a b
   s     a s b
  a     a
  b     b

context-free grammar

   s     a b
   s     a s b
  a     a
  b     b

s

a
a

b
b

context-free grammar

   s     a b
   s     a s b
  a     a
  b     b

s

s
s

a
a

a
a

a

a

b
b

b
b

b

b

context-free grammar

   s     a b
   s     a s b
  a     a
  b     b

s

s
s

a
a

a
a

a

a

b
b

b
b

b

b

s
s
s

b

b

b

b

b
b

a

a

a
a

a

a

context-free grammar

   s     a b
   s     a s b
  a     a
  b     b

s

s
s

a
a

a
a

a

a

b
b

b
b

b

b

s
s
s

b

b

b

b

b
b

a

a

a
a

a

a

l4 = {ab, aabb, aaabbb, ...} =    n   [1,inf)anbn

context-free vs. regular

context-free vs. regular
   regular languages     context-free languages

context-free vs. regular
   regular languages     context-free languages
   composition of languages:

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular
   regular     context-free = context-free

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular
   regular     context-free = context-free

a     bc     gcf l

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular
   regular     context-free = context-free

a     bc     gcf l

s, r, t     states(grl)

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular
   regular     context-free = context-free

a     bc     gcf l

s, r, t     states(grl)

sat     sbrrct     gcf l     grl

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular
   regular     context-free = context-free

a     bc     gcf l

s, r, t     states(grl)

sat     sbrrct     gcf l     grl

bar-hillel 1964

context-free vs. regular
   regular languages     context-free languages
   composition of languages:
   regular     regular = regular
   regular     context-free = context-free
   context-free     context-free = undecidable
a     bc     gcf l

sat     sbrrct     gcf l     grl

bar-hillel 1964

s, r, t     states(grl)

synchronous context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

synchronous context-free grammar

   s     np vp
np     watashi wa
np     hako wo
vp     np v
  v     akemasu

   s     np vp
np     i
np     the box
vp     v np
  v     open

synchronous context-free grammar

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

watashi  wa

vp

np

i

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

watashi  wa

vp

np

i

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

v

np

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

v

np

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

v

np

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

v

hako  wo

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

np

the  box

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

v

hako  wo

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

np

the  box

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu

v
open

np

the  box

   s     np1 vp2 / np1 vp2
np     watashi wa / i 
np     hako wo / the box
vp     np1 v2 / v2 np1
  v     akemasu / open

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu

v
open

np

the  box

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu
watashi  wa  hako  wo  akemasu

v
open

np

the  box

synchronous context-free grammar

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu
watashi  wa  hako  wo  akemasu

np

v
open
i  open  the  box

the  box

translation is parsing

watashi  wa  hako  wo  akemasu

translation is parsing

s

np

vp

watashi  wa

np

v

hako  wo

akemasu

watashi  wa  hako  wo  akemasu

translation is parsing

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu

v
open

np

the  box

watashi  wa  hako  wo  akemasu

translation is parsing

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu

v
open

np

the  box

watashi  wa  hako  wo  akemasu

i  open  the  box

translation is parsing

translation is parsing

   how many parses of a sentence are there?

translation is parsing

   how many parses of a sentence are there?
   for binary grammar: catalan number.

translation is parsing

   how many parses of a sentence are there?
   for binary grammar: catalan number. o(

(2n)!

(n + 1)!n!

)

translation is parsing

   how many parses of a sentence are there?
   for binary grammar: catalan number.
   id145 to the rescue!

o(

(2n)!

(n + 1)!n!

)

parsing

parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
prp 0,1     (w1 = i)     (prp     i)

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
prp 0,1     (w1 = i)     (prp     i)

p rp0,1

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

p rp0,1

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

vbd1,2

p rp0,1

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

vbd1,2

p rp0,1

prp$2,3

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

vbd1,2

prp2,3

p rp0,1

prp$2,3

i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

np2,4     prp$2,3     nn3,4     (np     prp$ nn)

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

np2,4     prp$2,3     nn3,4     (np     prp$ nn)

np2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

np2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

np2,4 sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

vp1,4

np2,4 sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

vp1,4

np2,4 sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

parsing
xi,i+1     (wi+1 = w)     (x     w)
xi,j     yi,k     zk,j     (x     y z)

s0,4

vp1,4

np2,4 sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3

nn3,4
i1    saw2    her3    duck4

parsing

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

parsing

s0,4

vp1,4

np2,4

vbd1,2

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

prp2,3vb3,4sbar2,4parsing

s

vp

np

prp vbd prp$ nn
i    saw    her    duck

s0,4

vp1,4

np2,4

vbd1,2

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

prp2,3vb3,4sbar2,4parsing

s

vp

sbar

prp vbd prp vb
i    saw    her    duck

s0,4

vp1,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

i1    saw2    her3    duck4

prp$2,3nn3,4np2,4parsing

analysis

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

analysis
nodes
o(n n2)
o(gn3) edges

parsing

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1
0.7

prp$2,3 nn3,4
i1    saw2    her3    duck4

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

prp2,3
0.3

vb3,4
1.0

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

np2,4
1.0
prp2,3
0.3

sbar2,4

0.3
vb3,4
1.0

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

sbar2,4

0.3

vbd1,2
1.0

1.00.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4np2,4prp2,3vb3,4s0,4probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.06

sbar2,4

0.3

vbd1,2
1.0

1.00.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4np2,4prp2,3vb3,4s0,4probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.06

sbar2,4

0.3

vbd1,2
1.0

1.00.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4np2,4prp2,3vb3,4s0,4probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.06

np2,4
1.0

vbd1,2
1.0

0.30.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4prp2,3vb3,4sbar2,4s0,4probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.8

np2,4
1.0

vbd1,2
1.0

0.30.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4prp2,3vb3,4sbar2,4s0,4probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

0.8

np2,4
1.0
prp2,3
0.3

sbar2,4

0.3
vb3,4
1.0

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

probabilistic parsing

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

0.56

vp1,4

0.8

np2,4
1.0
prp2,3
0.3

sbar2,4

0.3
vb3,4
1.0

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

probabilistic parsing

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

s0,4

0.56

vp1,4

0.8

np2,4
1.0
prp2,3
0.3

sbar2,4

0.3
vb3,4
1.0

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

np2,4
1.0
prp2,3
0.3

sbar2,4

0.3
vb3,4
1.0

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

sbar2,4

0.3

vbd1,2
1.0

1.00.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4np2,4prp2,3vb3,4s0,4computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.06

sbar2,4

0.3

vbd1,2
1.0

1.00.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4np2,4prp2,3vb3,4s0,4computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.06

np2,4
1.0

vbd1,2
1.0

0.30.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4prp2,3vb3,4sbar2,4s0,4computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

vp1,4

0.86

np2,4
1.0

vbd1,2
1.0

0.30.31.01.01.00.7i1    saw2    her3    duck4prp0,1prp$2,3nn3,4prp2,3vb3,4sbar2,4s0,4computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

vp1,4

0.86

np2,4
1.0
prp2,3
0.3

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

vb3,4
1.0

0.3sbar2,4computing expectations

nn     duck
np     prp$ nn
prp     her
prp     i
prp$     her
s     prp vp
sbar     prp vb
vb     duck
vp     vbd np
vp     vbd sbar
vbd     saw

(1.0)
(1.0)
(0.3)
(0.7)
(1.0)
(1.0)
(1.0)
(1.0)
(0.8)
(0.2)
(1.0)

s0,4

0.602

vp1,4

0.86

np2,4
1.0
prp2,3
0.3

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

vb3,4
1.0

0.3sbar2,4computing expectations

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

s0,4

0.602

vp1,4

0.86

np2,4
1.0
prp2,3
0.3

vbd1,2
1.0
p rp0,1
0.7

prp$2,3 nn3,4
1.0
i1    saw2    her3    duck4

1.0

vb3,4
1.0

0.3sbar2,4semiring parsing
xi,j     yi,k     zk,j     (x     y z)

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

semiring parsing
xi,j = xi,j     (yi,k     zk,j     (x     y z))

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

semiring parsing
xi,j = xi,j     (yi,k     zk,j     (x     y z))

   {t, f},   ,      

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

semiring parsing
xi,j = xi,j     (yi,k     zk,j     (x     y z))

   {t, f},   ,      

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

   r, max,     

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

semiring parsing
xi,j = xi,j     (yi,k     zk,j     (x     y z))

   {t, f},   ,      

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

   r, max,     

   r, +,     

semiring parsing
xi,j = xi,j     (yi,k     zk,j     (x     y z))

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

   {t, f},   ,      

   r, max,     

   r, +,     

xi,j = xi,j     (yi,k     zk,j     r(x     y z))

semiring parsing
xi,j = xi,j     (yi,k     zk,j     (x     y z))
boolean

   {t, f},   ,      

xi,j = max(xi,j, yi,k    zk,j    p(x     y z))

xi,j = xi,j + (yi,k    zk,j    p(x     y z))

viterbi

inside

   r, max,     

   r, +,     

xi,j = xi,j     (yi,k     zk,j     r(x     y z))

parsing
is intersection!

nn3,4     duck
np2,4     prp$2,3 nn3,4
prp2,3     her
prp0,1     i
prp$2,3     her
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     duck
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     saw

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

parsing
is intersection!

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

parsing
is intersection!

s0,4

vp1,4

p rp0,1

i1    saw2    her3    duck4vbd1,2prp$2,3nn3,4np2,4prp2,3vb3,4sbar2,4parsing
is intersection!

s0,4     prp0,1 vp1,4

s0,4

vp1,4

p rp0,1

i1    saw2    her3    duck4vbd1,2prp$2,3nn3,4np2,4prp2,3vb3,4sbar2,4parsing
is intersection!

s0,4     prp0,1 vp1,4

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

parsing
is intersection!

nn3,4     duck
np2,4     prp$2,3 nn3,4
prp2,3     her
prp0,1     i
prp$2,3     her
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     duck
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     saw

s0,4

vp1,4

np2,4

sbar2,4

vbd1,2

prp2,3

vb3,4

p rp0,1

prp$2,3 nn3,4
i1    saw2    her3    duck4

parsing
is intersection!
nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

nn3,4     duck
np2,4     prp$2,3 nn3,4
prp2,3     her
prp0,1     i
prp$2,3     her
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     duck
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     saw

parsing
is intersection!
nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

yo vi ella agacharse

yo vi su pato

nn3,4     duck
np2,4     prp$2,3 nn3,4
prp2,3     her
prp0,1     i
prp$2,3     her
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     duck
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     saw

synchronous parsing as intersection

synchronous parsing as intersection
   parse the english sentence (intersection).

synchronous parsing as intersection
   parse the english sentence (intersection).
   project grammar into french.

synchronous parsing as intersection
   parse the english sentence (intersection).
   project grammar into french.
   parse the french sentence (intersection).

synchronous parsing as intersection
   parse the english sentence (intersection).
   project grammar into french.
   parse the french sentence (intersection).

dyer, naacl 2010

translation as intersection?

nn3,4     duck
np2,4     prp$2,3 nn3,4
prp2,3     her
prp0,1     i
prp$2,3     her
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     duck
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     saw

yo vi ella agacharse

yo vi su pato

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

observation: target grammar generates a    nite language
nn3,4     duck
np2,4     prp$2,3 nn3,4
prp2,3     her
prp0,1     i
prp$2,3     her
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     duck
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     saw

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

yo vi su pato

yo vi ella agacharse

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

prp0,1

vp1,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

prp0,1

vp1,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

prp0,1

vp1,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

yo

vp1,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

yo

vp1,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

yo

vp1,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

vbd1,2

np2,4

yo

vbd1,2

sbar2,4

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

vbd1,2

np2,4

yo

vbd1,2

sbar2,4

translation as intersection?

yo

vbd1,2

np2,4

sbar2,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

yo

vi

np2,4

sbar2,4

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

su pato

yo

vi

ella agacharse

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

su pato

yo

vi

ella agacharse

better: lazy algorithm

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

su pato

yo

vi

ella agacharse

better: lazy algorithm

even better: convert to pda

translation as intersection?

nn3,4     pato
np2,4     prp$2,3 nn3,4
prp2,3     su
prp0,1     yo
prp$2,3     ella
s0,4     prp0,1 vp1,4
sbar2,4     prp2,3 vb3,4
vb3,4     agacharse
vp1,4     vbd1,2 np2,4
vp1,4     vbd1,2 sbar2,4
vbd1,2     vi

su pato

yo

vi

ella agacharse

better: lazy algorithm

even better: convert to pda

cambridge: best nist 2009 arabic system

translation as intersection?

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu

v
open

np

the  box

watashi  wa  hako  wo  akemasu

i  open  the  box

translation as intersection?

s

s

np

vp

np

vp

watashi  wa

np

v

i

hako  wo

akemasu
intersection of weighted 

cfl and rl

v
open

np

the  box

watashi  wa  hako  wo  akemasu

i  open  the  box

translation as intersection?

s

np

vp

np

vp

watashi  wa

np

v

i

np

hako  wo

akemasu
intersection of weighted 

cfl and rl

watashi  wa  hako  wo  akemasu

the  box
intersection of weighted 

cfl and rl
i  open  the  box

s

v
open

translation as intersection?

s

np

????

vp

np

s

v
open

vp

np

watashi  wa

np

v

i

hako  wo

akemasu
intersection of weighted 

cfl and rl

watashi  wa  hako  wo  akemasu

the  box
intersection of weighted 

cfl and rl
i  open  the  box

translation as intersection?

s

np

????

vp

np

s

v
open

vp

np

watashi  wa

np

v

i

hako  wo

akemasu
intersection of weighted 

the  box
intersection of weighted 

cfl and rl

cfl and rl
i  open  the  box
watashi  wa  hako  wo  akemasu
weighted tree languages, automata, and transducers.

bayes    rule

p(english|chinese)    

p(english)    p(chinese|english)

language model

translation model

p(chinese|english)

english

p(chinese|english)

   p(english)

english

p(chinese|english)

   p(english)

    p(english|chinese)

english

p(chinese|english)1

   p(english)1

    p(english|chinese)

english

p(chinese|english)2

   p(english)1

    p(english|chinese)

english

p(chinese|english)1/2

   p(english)1

    p(english|chinese)

english

p(chinese|english)0

   p(english)1

    p(english|chinese)

english

0    log p(chinese|english)

+1    log p(english)

    log p(english|chinese)

english

log(a) < log(b)        a < b

0    log p(chinese|english)

+1    log p(english)

    log p(english|chinese)

english

0    log p(chinese|english)

+1    log p(english)

= score(english|chinese)

english

score(english|chinese) =

  1 log p(chinese|english) +   2 log p(english)

score(english|chinese) =
exp(  1 log p(chinese|english) +   2 log p(english))

p(english|chinese) =

exp(  1 log p(chinese|english) +   2 log p(english))

exp(  1 log p(chinese|english) +   2 log p(english))

   english

p(english|chinese) =

exp(  1 log p(chinese|english) +   2 log p(english))

exp(  1 log p(chinese|english) +   2 log p(english))

   english

log-linear model

maximum id178 model

conditional model
undirected model

p(english|chinese) =

p(english)    p(chinese|english)

note: original model is a special case of this model!

log-linear model

maximum id178 model

conditional model
undirected model

p(english|chinese) =

exp(  1 log p(chinese|english) +   2 log p(english))

exp(  1 log p(chinese|english) +   2 log p(english))

   english

log-linear model

maximum id178 model

conditional model
undirected model

p(english|chinese) =

exp      k
exp      k

  khk(english, chinese)   
  khk(english   , chinese)   

   english   

log-linear model

maximum id178 model

conditional model
undirected model

spring 2014
mt-class.org

