finite-state methods

600.465 - intro to nlp - j. eisner

1

finite state acceptors (fsas)

c

a

defines the

language a? c*

= {a, ac, acc, accc,    ,
c, cc,   ccc,    }

   ,

    things you may

know about fsas:
    equivalence to

regexps

    union, kleene *,
concat, intersect,
complement,
reversal

    determinization,

minimization

    pumping,

myhill-nerode

600.465 - intro to nlp - j. eisner

2

e
id165 models not good enough
    want to model grammaticality
    a    training    sentence known to be grammatical:

bos mouse traps catch mouse traps eos

trigram model must allow these trigrams

    resulting trigram model has to overgeneralize:

    allows sentences with 0 verbs

bos mouse traps eos

    allows sentences with 2 or more verbs

bos mouse traps catch mouse traps

catch mouse traps catch mouse traps eos

    can   t remember whether it   s in subject or object

(i.e., whether it   s gotten to the verb yet)

600.465 - intro to nlp - j. eisner

3

finite-state models can    get it   

    want to model grammaticality

bos mouse traps catch mouse traps eos

    finite-state can capture the generalization here:

noun+ verb noun+
noun

noun

noun

verb

noun

preverbal states
(still need a verb
to reach final state)

postverbal states
(verbs no longer

allowed)

600.465 - intro to nlp - j. eisner

allows arbitrarily long
nps (just keep looping
around for another
noun modifier).

still, never forgets
whether it   s preverbal
or postverbal!

(unlike 50-gram model)

4

how powerful are regexps / fsas?

    more powerful than id165 models

    the hidden state may    remember    arbitrary past context
    with k states, can remember which of k    types    of context it   s in

    equivalent to id48s

    in both cases, you observe a sequence and it is    explained    by a

hidden path of states.  the fsa states are like id48 tags.

    appropriate for phonology and morphology

word   =   syllable+

=   (onset nucleus coda?)+
=   (c+ v+ c*)+
=   ( (b|d|f|   )+ (a|e|i|o|u)+ (b|d|f|   )* )+

600.465 - intro to nlp - j. eisner

5

how powerful are regexps / fsas?

    but less powerful than id18s / pushdown automata

    can   t do recursive center-embedding
    id48, humans have trouble processing those constructions too    

    this is the rat that ate the malt.
    this is the malt that the rat ate.

finite-state can

handle this pattern
(can you write the

    this is the cat that bit the rat that ate the malt.
    this is the malt that the rat that the cat bit ate.

regexp?)
    this is the dog that chased the cat that bit the rat that ate the malt.
    this is the malt that [the rat that [the cat that [the dog chased] bit] ate].

but not this pattern,
which requires a id18

600.465 - intro to nlp - j. eisner

6

how powerful are regexps / fsas?

    but less powerful than id18s / pushdown automata
    more important: less explanatory than id18s

    an id18 without recursive center-embedding can be converted
into an equivalent fsa     but the fsa will usually be far larger

    because fsas can   t reuse the same phrase type in different places

s =

noun

noun

noun

verb

noun

duplicated
structure

duplicated
structure

600.465 - intro to nlp - j. eisner

s =

more elegant     using
nonterminals like this
is equivalent to a id18

np =

np

verb

noun

noun

np
7

we   ve already used fsas this way    

    id18 with regular expression on the right-hand side:

x     (a | b) g h (p | q)
np     (det |    ) adj* n

    so each nonterminal has a finite-state automaton,

giving a    recursive transition network (rtn)   

a

x    

p

g

h
b
automaton state
replaces dotted

rule (x     a g . h p)

600.465 - intro to nlp - j. eisner
600.465 - intro to nlp - j. eisner

q
np    

adj

n

det

adj

n

8
8

we   ve already used fsas once ..

np     rules from the wsj grammar become a single dfa

np     adjp adjp jj jj nn nns

regular

expression

| adjp dt nn
| adjp jj nn
| adjp jj nn nns
| adjp jj nns
| adjp nn
| adjp nn nn
| adjp nn nns
| adjp nns
| adjp npr
| adjp nprs
| dt
| dt adjp
| dt adjp , jj nn
| dt adjp adjp nn
| dt adjp jj jj nn
| dt adjp jj nn
| dt adjp jj nn nn
etc.

600.465 - intro to nlp - j. eisner
600.465 - intro to nlp - j. eisner

np    
dfa

adjp
dt
np

adjp    

adj
p
adjp

9
9

but where can we put our weights?

    id18 / rtn

np    

adj

n

det

adj

n

    bigram model

of words or tags
(first-order
markov model)

start

det

adj

    hidden markov model of

words and tags together??

verb

prep

noun

stop

600.465 - intro to nlp - j. eisner

10

another useful fsa    

slide courtesy of l. karttunen (modified)

network
l
e

c

a

r

compile

f

e

a

t

v

h

e

wordlist

clear
clever

ear
ever
fat

father

/usr/dict/words

0.6 sec

25k words
206k chars

id122

17728 states,
37100 arcs

600.465 - intro to nlp - j. eisner

11

weights are useful here too!

slide courtesy of l. karttunen (modified)

network
l/0

e/0

c/0

a/0

r/0

compile

f/4

e/2

a/0

t/0

v/1

h/1

e/0

wordlist
clear 0
clever 1
ear
2
3
ever
fat
4
father 5

computes a perfect hash!

sum the weights along a word   s accepting path.

600.465 - intro to nlp - j. eisner

12

example: weighted acceptor

slide courtesy of l. karttunen (modified)

network
e
l
l/0
e/0

2

c
c/0

6

2

a
a/0

1

2

r
r/0

1

compile

f
f/4

e
e/2

a
a/0

2

t
t/0

2

2

e
e/0

v
v/1

h
h/1

1

wordlist
clear 0
clever 1
ear
2
3
ever
fat
4
father 5

    compute number of paths from each state (q: how?)
    successor states partition the path set
    use offsets of successor states as arc weights
    q: would this work for an arbitrary numbering of the words?

a: recursively, like dfs

600.465 - intro to nlp - j. eisner

13

example: unweighted transducer

vp [head=vouloir,...]

v[head=vouloir,
...
tense=present,
num=sg, person=p3]

veut

600.465 - intro to nlp - j. eisner

14

example: unweighted transducer

slide courtesy of l. karttunen (modified)

vouloir +pres +sing + p3

vp [head=vouloir,...]

finite-state
transducer

veut

v

o

canonical form
o

l

u

v

e

u

the relevant path

600.465 - intro to nlp - j. eisner

v[head=vouloir,
...
tense=present,
num=sg, person=p3]

veut

i

r

inflection codes
+p3

+sing

+pres

inflected form

t

15

example: unweighted transducer

slide courtesy of l. karttunen

vouloir +pres +sing + p3

finite-state
transducer

veut

    bidirectional: generation or analysis
    compact and fast
    xerox sells for about 20 languges
including english, german, dutch,
french, italian, spanish,
portuguese, finnish, russian,
turkish, japanese, ...

    research systems for many other
languages, including arabic, malay

v

o

canonical form
o

l

u

i

r

inflection codes
+p3

+sing

+pres

v

e

u

the relevant path

600.465 - intro to nlp - j. eisner

inflected form

t

16

what is a function?

    formally, a set of <input, output> pairs

where each input    domain, output    co-domain.
    moreover,   x    domain,    exactly one y
such that <x,y>    the function.

co-domain

domain

    square: int     int

= { <0,0>, <1,1>, <-1,1>, <2,4>, <-2,4>,

<3,9>, <-3,9>, ... }

    domain(square) = {0, 1, -1, 2, -2, 3, -3,    }
    range(square) = {0, 1, 4, 9, ...}

600.465 - intro to nlp - j. eisner

17

what is a relation?

    square: int     int

= { <0,0>, <1,1>, <-1,1>, <2,4>, <-2,4>,

<3,9>, <-3,9>, ... }

    inverse(square): int     int

= { <0,0>, <1,1>, <1,-1>, <4,2>, <4,-2>,

<9,3>, <9,-3>, ... }

    0    {0}

    is inverse(square) a function?

9    {3,-3}

7    {}

-1    {}

    no - we need a more general notion!

    a relation is any set of <input, output> pairs

600.465 - intro to nlp - j. eisner

18

regular relation (of strings)

    relation: like a function, but multiple outputs ok
    regular: finite-state
    transducer: automaton w/ outputs

a:

    b     ?
{b}
    aaaaa     ?

a     ?
{}

{ac, aca, acab,

acabc}

    invertible?
    closed under composition?

600.465 - intro to nlp - j. eisner

a:a

a:c

b:b

b:

b:b

?:c

?:b

?:a

19

regular relation (of strings)

    can weight the arcs:     vs.    
    b     {b}
    aaaaa     {ac, aca, acab,

a     {}

acabc}

a:a

a:c

    how to find best outputs?

    for aaaaa?
    for all inputs at once?

600.465 - intro to nlp - j. eisner

b:b

b:

a:

b:b

?:c

?:b

?:a

20

function from strings to ...

acceptors (fsas)
c

{false, true}

unweighted

a

transducers (fsts)
c:z

strings
a:x

e :y

weighted

numbers

c/.7

a/.5

e /.5

.3

(string, num) pairs

c:z/.7

a:x/.5

e :y/.5

.3

600.465 - intro to nlp - j. eisner

21

e
sample functions

acceptors (fsas)

transducers (fsts)

unweighted

{false, true}
grammatical?

strings
markup
correction
translation

weighted

numbers
how grammatical?
better, how probable?

(string, num) pairs
good markups
good corrections
good translations

600.465 - intro to nlp - j. eisner

22

terminology (acceptors)

regular language

defines

regexp

recognizes

compiles into
implements

fsa

600.465 - intro to nlp - j. eisner

string

23

terminology (transducers)

regular relation

defines

regexp

recognizes

compiles into
implements

fst

600.465 - intro to nlp - j. eisner

string pair

?

24

perspectives on a transducer
    remember these id18 perspectives:

3 views of a context-free rule

    generation (production):
s     np vp
    parsing (comprehension): s     np vp
s = np vp
   

verification (checking):

(randsent)
(parse)

    similarly, 3 views of a transducer:

    given 0 strings, generate a new string pair (by picking a path)
    given one string (upper or lower), transduce it to the other kind
    given two strings (upper & lower), decide whether to accept the pair
+p3

+pres

+sing

u

l

o

i

r

v

o

v

e

u

t

fst just defines the regular relation (mathematical object: set of pairs).
what   s    input    and    output    depends on what one asks about the relation.
the 0, 1, or 2 given string(s) constrain which paths you can use.
25

600.465 - intro to nlp - j. eisner

functions

ab?d

abcd

f

g

600.465 - intro to nlp - j. eisner

26

a
b
c
d
functions

ab?d

function composition: f     g

[first f, then g     intuitive notation, but opposite of the traditional math notation]

like the unix pipe: cat x | f | g > y
example: pass the input through a sequence of ciphers
600.465 - intro to nlp - j. eisner

27

a
b
c
d
from functions to relations

f

ab?d

3

2

6

abcd

abed

abjd

g

4

2

8 a

b    d

...

600.465 - intro to nlp - j. eisner

28

a
b
e
d
a
b
c
d
from functions to relations

ab?d

3

22

relation composition: f     g

6

4

2

8

b    d

...

600.465 - intro to nlp - j. eisner

29

a
b
c
d
a
b
e
d
a
from functions to relations

ab?d

relation composition: f     g

3+4

2+2

2+8

b    d

...

600.465 - intro to nlp - j. eisner

30

a
b
c
d
a
b
e
d
a
from functions to relations

ab?d

pick min-cost or max-prob output

2+2

often in nlp, all of the functions or relations involved
can be described as id122, and
manipulated using standard algorithms.

600.465 - intro to nlp - j. eisner

31

a
b
e
d
inverting relations

f

ab?d

3
abcd
2
abed
6
abjd

g

4

2

8a
b    d

...

600.465 - intro to nlp - j. eisner

32

a
b
e
d
a
b
c
d
inverting relations

f-1

ab?d

3
abcd
2
abed
6
abjd

g-1

4

2

8a
b    d

...

600.465 - intro to nlp - j. eisner

33

a
b
e
d
a
b
c
d
inverting relations

ab?d

(f     g)-1 = g-1     f-1

3+4

2+2

2+8

b    d

...

600.465 - intro to nlp - j. eisner

34

a
b
c
d
a
b
e
d
a
building a lexical transducer

slide courtesy of l. karttunen (modified)

big | clear | clever | ear | fat | ...

regular expression

lexicon

e

a

r

v
h

e

c

f

l

e

a

t

lexicon

fsa

compiler

composition

lexical transducer

(a single fst)

id157

for rules

composed
rule fsts

b

i

600.465 - intro to nlp - j. eisner

b

i

one path

g

g

+adj

+comp

g

e

r

35

building a lexical transducer

slide courtesy of l. karttunen (modified)

big | clear | clever | ear | fat | ...

regular expression

lexicon

e

a

r

v
h

e

c

f

l

e

a

t

lexicon

fsa

    actually, the lexicon must contain elements like

big +adj +comp

(big | clear | clever | fat | ...) +adj (    | +comp | +sup)    adjectives

    so write it as a more complicated expression:
   nouns
   ...

| (ear | father | ...) +noun (+sing | +pl)
| ...

    q: why do we need a lexicon at all?

600.465 - intro to nlp - j. eisner

36

weighted version of transducer:
assigns a weight to each string pair

slide courtesy of l. karttunen (modified)

4
19
20

50

  tre+indp +sg + p1
suivre+indp+sg+p1
suivre+indp+sg+p2
suivre+imp+sg + p2

   upper language   

payer+indp+sg+p1

weighted french transducer

12

3

suis

   lower language   

paie
paye

600.465 - intro to nlp - j. eisner

37

