id170 with
id88s and crfs

s

vp

pp

np

n

v

p d

n

time flies like an arrow

s

vp

np

np

n

n v d

n

time flies like an arrow
600.465 - intro to nlp - j. eisner

?

s
vp

vp

pp

np

v

n p d

n

time flies like an arrow

s    

s

v

v

np

v

v

v d

n

time flies like an arrow

1

id170 with
id88s and crfs

back to conditional
log-linear modeling    

but now, model

structures!

600.465 - intro to nlp - j. eisner

2

p(fill | shape)

600.465 - intro to nlp - j. eisner

3

p(fill | shape)

600.465 - intro to nlp - j. eisner

4

p(category | message)

goodmail

spam

reply today to claim your    

reply today to claim your    

goodmail

spam

wanna get pizza tonight?

wanna get pizza tonight?

goodmail

spam

thx; consider enlarging the    

thx; consider enlarging the    

goodmail

spam

enlarge your hidden    

enlarge your hidden    

p(rhs | lhs)

s    

np  vp

s    

n  vp

s    

s    

vp  np

s    

det n

s    

pp  p

np[+wh] v  s/v/np

   

600.465 - intro to nlp - j. eisner

6

p(rhs | lhs)

s    

np  vp

s    

n  vp

s    

np[+wh] v  s/v/np

s    

vp  np

s    

det n

s    

pp  p

np    

np  vp

np    

n  vp

np    

np  cp/np

np    

vp  np

np    

det  n

np    

np  pp

600.465 - intro to nlp - j. eisner

   

   

   

7

p(parse | sentence)

time flies like an arrow

time flies like an arrow

time flies like an arrow
600.465 - intro to nlp - j. eisner

time flies like an arrow

   

8

p(tag sequence | word sequence)

time flies like an arrow

time flies like an arrow

time flies like an arrow
600.465 - intro to nlp - j. eisner

time flies like an arrow

   

9

today   s general problem

    given some input x

    occasionally empty, e.g., no input needed for a generative n-

gram or model of strings (randsent)

    consider a set of candidate outputs y

    classifications for x
    taggings of x
    parses of x
    translations of x
       

(small number: often just 2)
(exponentially many)
(exponential, even infinite)
(exponential, even infinite)

id170

    want to find the    best    y, given x

600.465 - intro to nlp - j. eisner

10

remember weighted cky    
(find the minimum-weight parse)

time 1 flies 2
0 np 3
vst 3

np 10
s
8
np 4
vp 4

1

2
3
4

like 3

an 4 arrow 5

p 2
v 5

det 1

np 24
s
22
np 18
21
s
vp 18
pp 12
vp 16
np 10
n 8

1  s     np vp
6  s     vst np
2  s     s pp
1  vp     v np
2  vp     vp pp
1  np     det n
2  np     np pp
3  np     np np
0  pp     p np
11

but is weighted cky good for anything else??

so far, we used weighted cky only to
implement probabilistic cky for pid18s

time 1 flies 2
0 np 3
vst 3

np 10
s
8
np 4
vp 4

1

2
3
4

like 3

an 4 arrow 5

multiply to get 2-22

2-8

p 2
v 5

2-12

det 1

np 24
s
22
np 18
21
s
vp 18
pp 12
vp 16
np 10
n 8

2-2

1  s     np vp
6  s     vst np
2  s     s pp
1  vp     v np
2  vp     vp pp
1  np     det n
2  np     np pp
3  np     np np
0  pp     p np
12

but is weighted cky good for anything else??

do the weights have to be probabilities?

we set the weights to log probs

s

| s) = w(s     np vp)     + w(np     time)

w(

vp

np
time
vp
flies

pp
p
like

np

det
an

n
arrow

+ w(vp     vp np)

+ w(vp     flies) +    

just let w(x     y z) = -log p(x     y z | x)
then lightest tree has highest prob

13

summary of half of the course (statistics)

id203 is useful
    we love id203 distributions!

    we   ve learned how to define & use p(   ) functions.

    pick best output text t from a set of candidates

    id103 (hw2); machine translation; ocr; spell correction...
    maximize p1(t) for some appropriate distribution p1

    pick best annotation t for a fixed input i

    text categorization; parsing; part-of-speech tagging    
    maximize p(t | i); equivalently maximize joint id203 p(i,t)

    often define p(i,t) by noisy channel: p(i,t) = p(t) * p(i | t)

    id103 & other tasks above are cases of this too:

    we   re maximizing an appropriate p1(t) defined by p(t | i)
    pick best id203 distribution (a meta-problem!)

    really, pick best parameters    : train id48, pid18, id165s, clusters    
    maximum likelihood; smoothing; em if unsupervised (incomplete data)
    bayesian smoothing: max p(   |data) = max p(   , data) =p(   )p(data|   )
600.465 - intro to nlp - j. eisner

14

summary of other half of the course (linguistics)

id203 is flexible

    we love id203 distributions!

    we   ve learned how to define & use p(   ) functions.

    we want p(   ) to define id203 of linguistic objects

    trees of (non)terminals (pid18s; cky, earley, pruning, inside-outside)
    sequences of words, tags, morphemes, phonemes (id165s, fsas,

fsts; regex compilation, best-paths, forward-backward, collocations)

    vectors (decis.lists, gaussians, na  ve bayes; yarowsky, id91/id92)

    we   ve also seen some not-so-probabilistic stuff

    syntactic features, semantics, morph., gold.  could be stochasticized?
    methods can be quantitative & data-driven but not fully probabilistic:
transf.-based learning, bottom-up id91, lsa, competitive linking

    but probabilities have wormed their way into most things
    p(   ) has to capture our intuitions about the ling. data

600.465 - intro to nlp - j. eisner

15

an alternative tradition

    old ai hacking technique:

    possible parses (or whatever) have scores.
    pick the one with the best score.
    how do you define the score?

    completely ad hoc!
    throw anything you want into the stew
    add a bonus for this, a penalty for that, etc.

600.465 - intro to nlp - j. eisner

16

scoring by linear models
    given some input x
    consider a set of candidate outputs y
    define a scoring function score(x,y)

linear function: a sum of feature weights (you pick the features!)

weight of feature k

(learned or set by hand)

ranges over all features,

e.g., k=5   (numbered features)

whether (x,y) has feature k(0 or 1)
or how many times it fires (    0)
or how strongly it fires

(real #)

or k=   see det noun    (named features)
    choose y that maximizes score(x,y)

600.465 - intro to nlp - j. eisner

17

scoring by linear models
    given some input x
    consider a set of candidate outputs y
    define a scoring function score(x,y)

linear function: a sum of feature weights (you pick the features!)

(learned or set by hand)

this linear decision rule is sometimes called a    id88.   
it   s a    structured id88    if it does id170
(number of y candidates is unbounded, e.g., grows with |x|).

    choose y that maximizes score(x,y)

600.465 - intro to nlp - j. eisner

18

related older ideas

    id75 predicts a real number y:

    binary classification:

    predict    spam    if        f(x) > 0

    our multi-class classification uses f(x,y), not f(x):

    predict y that maximizes score(x,y)
    if only 2 possible values of y, equivalent to binary case
19

600.465 - intro to nlp - j. eisner

an alternative tradition
    old ai hacking technique:

    possible parses (or whatever) have scores.
    pick the one with the best score.
    how do you define the score?

    completely ad hoc!
    throw anything you want into the stew
    add a bonus for this, a penalty for that, etc.

       learns    over time     as you adjust bonuses and

penalties by hand to improve performance.    

    total kludge, but totally flexible too    

    can throw in any intuitions you might have

    could we make it learn automatically?

600.465 - intro to nlp - j. eisner

20

id88 training algorithm

    initialize    (usually to the zero vector)
    repeat:

    pick a training example (x,y)
    model predicts y   that maximizes score(x,y  )
    update weights by a step of size    > 0:

   =    +        (f(x,y)     f(x,y  ))

if model prediction was correct (y=y  ),    doesn   t change.
so once model predicts all training examples correctly, stop.
if some    can do the job, this eventually happens!

(if not,    will oscillate, but the average    from all steps
will settle down.  so return that eventual average.)

id88 training algorithm

    initialize    (usually to the zero vector)
    repeat:

    pick a training example (x,y)
    model predicts y   that maximizes score(x,y  )
    update weights by a step of size    > 0:

   =    +        (f(x,y)     f(x,y  ))

call this    

if model prediction was wrong (y   y  ), then we must have

score(x,y)     score(x,y  ) instead of > as we want.

equivalently,     f(x,y)         f(x,y  ) but we want it >
equivalently,     (f(x,y) - f(x,y  )) =             0 but we want it > 0.
so update increases        , to (  +        )      =         +      ||   ||2

    0

p(parse | sentence)

score(sentence, parse)

time flies like an arrow

time flies like an arrow

time flies like an arrow
600.465 - intro to nlp - j. eisner

time flies like an arrow

   

23

finding the best y given x
    at both training & test time, given input x,
id88 picks y that maximizes score(x,y)

    how do we compute that crucial prediction??

    easy when only a few candidates y (text classification, wsd,    )

    just try each y in turn.

    harder for id170: but you now know how!

    find the best string, path, or tree    
    that   s what viterbi-style or dijkstra-style algorithms are for.

    that is, use id145 to find the score of the best y.
    then follow backpointers to recover the y that achieves that score.

600.465 - intro to nlp - j. eisner

24

really so alternative?
an alternative tradition
    old ai hacking technique:

expos   at 9

    possible parses (or whatever) have scores.
    pick the one with the best score.
    how do you define the score?
critics say

probabilistic revolution
not really a revolution,
    completely ad hoc!
    throw anything you want into the stew
    add a bonus for this, a penalty for that, etc.
log-probabilities no more
than scores in disguise

       learns    over time     as you adjust bonuses and

penalties by hand to improve performance.    

   we   re just adding stuff up like
    total kludge, but totally flexible too    
    can throw in any intuitions you might have
the old corrupt regime did,   

admits spokesperson

600.465 - intro to nlp - j. eisner

25

nuthin    but adding weights

    id165s:     + log p(w7 | w5, w6) + log p(w8 | w6, w7) +    
    pid18: log p(np vp | s) + log p(papa | np) + log p(vp pp | vp)    
    id48 tagging:     + log p(t7 | t5, t6) + log p(w7 | t7) +    
    noisy channel: [log p(source)] + [log p(data | source)]
    cascade of composed fsts:

[log p(a)] + [log p(b | a)] + [log p(c | b)] +    

    na  ve bayes:
    note: here we   re using +logprob not    logprob:

log p(class) + log p(feature1 | class) + log p(feature2 | class)    

i.e., bigger weights are better.

600.465 - intro to nlp - j. eisner

26

nuthin    but adding weights

    id165s:     + log p(w7 | w5, w6) + log p(w8 | w6, w7) +    
    pid18: log p(np vp | s) + log p(papa | np) + log p(vp pp | vp)    

    score of a parse is its total weight
    the weights we add up have always been log-probs (    0)

    but what if we changed that?

    id48 tagging:     + log p(t7 | t5, t6) + log p(w7 | t7) +    
    noisy channel: [log p(source)] + [log p(data | source)]
    cascade of fsts:

[log p(a)] + [log p(b | a)] + [log p(c | b)] +    

    na  ve bayes:

log(class) + log(feature1 | class) + log(feature2 | class) +    
27

600.465 - intro to nlp - j. eisner

what if our weights were arbitrary real numbers?
change log p(this | that) to    (this ; that)

    id165s:     + log p(w7 | w5, w6) + log p(w8 | w6, w7) +    
    pid18: log p(np vp | s) + log p(papa | np) + log p(vp pp | vp)    
    id48 tagging:     + log p(t7 | t5, t6) + log p(w7 | t7) +    
    noisy channel: [log p(source)] + [log p(data | source)]
    cascade of fsts:

[log p(a)] + [log p(b | a)] + [log p(c | b)] +    

    na  ve bayes:

log p(class) + log p(feature1 | class) + log p(feature2 | class)    

600.465 - intro to nlp - j. eisner

28

what if our weights were arbitrary real numbers?
change log p(this | that) to    (this ; that)

    id165s:     +
    pid18:
   (np vp ; s) +
    id48 tagging:     +
    noisy channel: [
    cascade of fsts:

[

   (a)] + [
    na  ve bayes:
   (class) +

   (w7 ; w5, w6) +

   (w8 ; w6, w7) +    

   (papa ; np) +
   (t7 ; t5, t6) +

   (source)] + [

   (vp pp ; vp)    
   (w7 ; t7) +    
   (data ; source)]

   (b ; a)] + [

   (c ; b)] +    

   (feature1 ; class) +

   (feature2 ; class)    

in practice,     is a hash table
maps from feature name (a string or object) to feature weight (a float)
e.g.,    (np vp ; s)

= weight of the s     np vp rule, say -0.1 or +1.3

600.465 - intro to nlp - j. eisner

29

what if our weights were arbitrary real numbers?
change log p(this | that) to    (this ; that)    (that & this)
[prettier
name]

wid18

    id165s:     +
    pid18:
    id48 tagging:     +
    noisy channel: [
    cascade of fsts:

   (w5 w6 w7)    +

   (w6 w7 w8)   +    

   (s     np vp) +    (np     papa) +

   (t5 t6 t7) +
   (source)] + [

   (vp     vp pp)    
   (t7     w7) +    

   (source, data)]

[

   (a)] + [
    na  ve bayes:
   (class) +

   (a, b) ] + [
   (b, c)] +    
(multi-class) id28
   (class, feature 1) +

   (class, feature2)    

in practice,     is a hash table
maps from feature name (a string or object) to feature weight (a float)
e.g.,    (s     np vp)

= weight of the s     np vp rule, say -0.1 or +1.3
30

600.465 - intro to nlp - j. eisner

what if our weights were arbitrary real numbers?
change log p(this | that) to    (that & this)

wid18

   (w5 w6 w7)    +

   (s     np vp) +    (np     papa) +

    best parse is one whose rules have highest total weight

   (w6 w7 w8)   +    
    best string is the one whose trigrams have the highest total weight

    id165s:     +
    pid18:
    id48 tagging:     +
    noisy channel: [
    na  ve bayes:    (class) +    (class, feature 1) +    (class, feature 2)    
    best class maximizes prior weight + weight of compatibility with features

   (t7     w7) +    
    best tagging has highest total weight of all transitions and emissions

    to guess source: max (weight of source + weight of source-data match)

   (source, data)]

   (source)] + [

   (vp     vp pp)    

   (t5 t6 t7) +

(multi-class) id28

600.465 - intro to nlp - j. eisner

31

what if our weights were arbitrary real numbers?
change log p(this | that) to    (that & this)

wid18

   (w5 w6 w7)    +

   (s     np vp) +    (np     papa) +

we   ll just add up arbitrary

feature weights    that might not be

    best parse is one whose rules have highest total weight (use cky/earley)

all our algorithms still work!

   (w6 w7 w8)   +    
    best string is the one whose trigrams have the highest total weight

    id165s:     +
    pid18:
    id48 tagging:     +
    noisy channel: [
    na  ve bayes:    (class) +    (class, feature 1) +    (class, feature 2)    
    best class maximizes prior weight + weight of compatibility with features

   (t7     w7) +    
    best tagging has highest total weight of all transitions and emissions

log conditional probabilities
(they might even be positive!)

total score(x,y) can   t be interpreted

    to guess source: max (weight of source + weight of source-data match)

anymore as log p(x,y)

but we can still find the highest-scoring y

   (source, data)]

   (source)] + [

   (vp     vp pp)    

   (t5 t6 t7) +

(multi-class) id28

(using a viterbi algorithm)

600.465 - intro to nlp - j. eisner

32

given sentence x
you know how to find max-score parse y (or min-cost parse as shown)

    provided that the score of a parse = total score of its rules

flies

2

like 3

an 4

arrow

5

time 1
np
vst

3
3

np
s
s

10
8
13

0

1

2

3
4

np
vp

4
4

s

vp

pp

np

p 2

v 5

det

1

n

v

p d

n

time flies like an arrow

np
s
s
np
s
s
s

np
s
vp
pp
vp

np
n

24
22
27
24
27
22
27

18
21
18
12
16

10
8

1 s     np vp
6 s     vst np
2 s     s pp
1 vp     v np
2 vp     vp pp
1 np     det n
2 np     np pp
3 np     np np
0 pp     p np

given word sequence x
you know how to find max-score tag sequence y

    provided that the score of a tagged sentence

= total score of its emissions and transitions

    these don   t have to be log-probabilities!

    emission scores assess tag-word compatibility
    transition scores assess goodness of tag bigrams

   ?
prep
adj
verb   verb   noun verb
pn      adj     det   noun  prep noun    prep    det  noun
bill  directed   a    cortege  of  autos  through  the  dunes

given upper string x
you know how to find max-score path that accepts x (or min-cost path)

    provided that the score of a path = total score of its arcs

    then the best lower string y is the one along that best path

    (so in effect, score(x,y) is score of best path that transduces x to y)

    q: how do you make sure that the path accepts x, such as aaaaaba?

    a: compose with straight-line automaton for x, then find best path.

running example: predict a tagging

given word sequence x
find max-score tag sequence y
score(                             ) = ?

bos

eos

n
v
time flies

so what are the features?
let   s start with the usual emission and

transition features    

running example: predict a tagging

given word sequence x
find max-score tag sequence y
score(                             ) =    k   k fk(x,y)
=   bos,n +   n,time +   n,v +   v,flies +   v,eos

n
v
time flies

bos

eos

so what are the features?
let   s start with the usual emission and

transition features    

running example: predict a tagging

given word sequence x
find max-score tag sequence y
score(                             ) =    k   k fk(x,y)
=   bos,n +   n,time +   n,v +   v,flies +   v,eos

n
v
time flies

bos

eos

for each t     tags, w     words:

score includes fn,v(x,y) copies of this feature weight = one copy per n v token
|tags| |words|
emission
features

define ft,w(x,y) = count of emission t w
= | {i: 1     i     |x|, yi = t, xi = w} |

for each t, t        tags:

define ft,t   (x,y) = count of transition t t   

= | {i: 0     i     |x|, yi = t, yi+1 = t   } |

define |tags|2
transition
features

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
viterbi algorithm can find the highest-scoring tagging

bos

a
n

v

a
n

v

a
n

v

a
n

v

a
n

v

eos

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
viterbi algorithm can find the highest-scoring tagging

bos

a
n

v

a
n

v

eos

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
viterbi algorithm can find the highest-scoring tagging
set arc weights so that path weight = tagging score

bos

n

time

a
n

v
  bos,n+  n,time

a
n

v

eos

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
viterbi algorithm can find the highest-scoring tagging
set arc weights so that path weight = tagging score

bos

a
n

v

a
n

v

eos

  
  
  n,eos

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
viterbi algorithm can find the highest-scoring tagging
set arc weights so that path weight = tagging score

bos

a
n

v

a
n

v

eos

  
  
  n,eos

in structured id88,
weights    are no longer log-
probs.  they   re tuned by
structured id88 to make
the correct path outscore others.

why would we switch from
probabilities to scores?

1.    discriminative    training (e.g., id88) might work better.

    it tries to optimize weights to actually predict the right y for each x.
    more important than maximizing log p(x,y) = log p(y|x) + log p(x),
as we   ve been doing in id48s and pid18s.
    satisfied once the right y wins.  the example puts no more pressure on
the weights to raise log p(y|x).  and never pressures us to raise log p(x).

2. having more freedom in the weights might help?

    now weights can be positive or negative.
    exponentiated weights no longer have to sum to 1.
    but turns out new    vectors can   t do more than the old restricted ones.

    roughly, for every wid18 there   s an equivalent pid18.
    though it   s true a regularizer might favor one of the new ones.

3. we can throw lots more features into the stewpot.

    allows model to capture more of the useful predictive patterns!
    so, what features can we throw in efficiently?

when can you efficiently choose best y?
       provided that the score of a tagged sentence

= total score of its transitions and emissions   
       provided that the score of a path = total score of its arcs   
       provided that the score of a parse = total score of its rules   

this implies certain kinds of features in linear model    

e.g,   3 = score of n v tag bigram

f3(x,y) = # times n v
appears in y

600.465 - intro to nlp - j. eisner

45

when can you efficiently choose best y?
       provided that the score of a tagged sentence

= total score of its transitions and emissions   
       provided that the score of a path = total score of its arcs   
       provided that the score of a parse = total score of its rules   

this implies certain kinds of features in linear model    

e.g,   3 = score of vp     vp pp

f3(x,y) = # times vp     vp pp
appears in y

600.465 - intro to nlp - j. eisner

46

when can you efficiently choose best y?
       provided that the score of a tagged sentence

= total score of its transitions and emissions   
       provided that the score of a path = total score of its arcs   
       provided that the score of a parse = total score of its rules   

this implies certain kinds of features in linear model    

more generally: make a list of interesting substructures.
the feature fk(x,y) counts tokens of kth substructure in (x,y).
so far, the substructures = transitions, emissions, arcs, rules.
but model could use any features     what ones are efficient?
600.465 - intro to nlp - j. eisner

47

1. single-rule substructures

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp     vp pp

1. single-rule substructures

s

vp

pp

np

np

vp

these features
are efficient
for cky to
consider.

n

v

p d

n

time flies like an arrow

    count of vp     vp pp
    count of v     flies

(looks at y only)
(looks at both x and y)

2. within-rule substructures

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp with a pp child

2. within-rule substructures

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp with a pp child
    count of any node with a pp right child

2. within-rule substructures

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp with a pp child
    count of any node with a pp right child
    count of any node with a pp right child and

whose label matches left child   s label

2. within-rule substructures

s

vp

pp

np

vp

np

n

v

p d

n

time flies like an arrow

efficient?

yes: the weight that cky
uses for vp     vp pp is the
total weight of all of its

within-rule features.

some of these
features fire on
both vp     vp pp
and np     np pp.
so they   re really
backoff features.

    count of vp with a pp child
    count of any node with a pp right child
    count of any node with a pp right child and
whose nonterm matches left child   s nonterm

3. cross-rule substructures

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of    flies    as a verb with subject    time   

3. cross-rule substructures

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of    flies    as a verb with subject    time   
    count of np     d n when the np is the

object of a preposition

3. cross-rule substructures
two such vps, so
feature fires twice
on this (x,y) pair

s

vp

np

vp

pp

np

n

v

p d

n

time flies like an arrow

    count of    flies    as a verb with subject    time   
    count of np     d n when the np is the

object of a preposition

    count of vp constituents that contain a v

3. cross-rule substructures

s

vp

[hasv=true]
pp

np
[head=time]

n

vp
[hasv=true]
v

np

[role=prepobj]
n

p d

time flies like an arrow

efficient?  sort of.
for cky to work,

must add attributes
to the nonterminals

so that these

features can now be
detected within-rule.

that enlarges the

grammar.    

    count of    flies    as a verb with subject    time   
    count of np     d n when the np is the

object of a preposition

    count of vps that contain a v

what   s the analogue in

id122s?  splitting states to
remember more history.

4. global features

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of    np and np    when the two nps have very
different size or structure [this feature has weight < 0]
    the number of pps is even
    the depth of the tree is prime    
    count of the tag bigram v p in the preterminal seq

4. global features

or stop relying
only on dynamic
programming.
start using

approximate or
exact general
methods for
combinatorial
optimization.

s

[depth=5]

vp
[depth=4]
pp
[depth=3]

np

vp

[depth=2] [depth=2]

np
[depth=2]
n
[depth=1][depth=1][depth=1] [depth=1]
time flies like an arrow

p d

n

v

efficient?  depends
on whether you can
do it with attributes.
if you have infinitely
many nonterminals,
it   s not technically a
pid18 anymore, but
cky might still apply.

hot area!
    count of    np and np    when the two nps have very
different size or structure [this feature has weight < 0]
    the number of pps is even
    the depth of the tree is prime    
    count of the tag bigram v p in the preterminal seq

5. context-specific features
take any

s

efficient feature

that counts a
substructure.
modify it to

count only tokens

appearing in
a particular
red context.

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

5. context-specific features
take any

s

efficient feature

that counts a
substructure.
modify it to

count only tokens

appearing in
a particular
red context.

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp     vp pp whose first word is    flies   

5. context-specific features
take any

s

efficient feature

that counts a
substructure.
modify it to

count only tokens

appearing in
a particular
red context.

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp     vp pp whose first word is    flies   
    count of vp     vp pp whose right child has width 3

5. context-specific features
take any

s

efficient feature

that counts a
substructure.
modify it to

count only tokens

appearing in
a particular
red context.

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

3

4

1

2

5

0

    count of vp     vp pp whose first word is    flies   
    count of vp     vp pp whose right child has width 3
    count of vp     vp pp at the end of the input

5. context-specific features
take any

s

efficient feature

that counts a
substructure.
modify it to

count only tokens

appearing in
a particular
red context.

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

    count of vp     vp pp whose first word is    flies   
    count of vp     vp pp whose right child has width 3
    count of vp     vp pp at the end of the input
    count of vp     vp pp right after a capitalized word

5. context-specific features
take any

s

efficient feature

that counts a
substructure.
modify it to

count only tokens

appearing in
a particular
red context.

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

still efficient?
amazingly, yes!

features like these
have played a big role

in improving real-
world accuracy of

nlp systems.

    count of vp     vp pp whose first word is    flies   
    count of vp     vp pp whose right child has width 3
    count of vp     vp pp at the end of the input
    count of vp     vp pp right after a capitalized word

weight of vp     vp pp in this context =
+weight of vp     vp pp
+weight of vp     ? pp
+weight of vp     vp pp whose right child has width 3
+weight of vp     vp pp right after a capitalized word
+weight of vp     vp pp at the end of the input
weight of vp     vp pp whose first word is    flies   
5. context-specific features

like 3

an 4 arrow 5

time 1 flies 2
0 np 3
vst 3

np 10
s
8
np 4
vp 4

1

p 2
v 5

2
3
4
when cky combines [1,2] with [2,5] using the rule vp     vp pp,

det 1

it is using that rule in a particular context.

the weight of the rule in that context can sum over features that look at

the context (i.e., the red information).  doesn   t change cky runtime!

no longer do we
look up a constant

rule weight!
1  s     np vp
6  s     vst np
2  s     s pp
1  vp     v np
2  vp     vp pp
1  np     det n
2  np     np pp
3  np     np np
0  pp     p np

np 24
s
22
np 18
s
21
vp 18
pp 12
vp 16
np 10
n 8

same approach for tagging    
    previous slides used parsing as an example.

    given a sentence of length n,

reconstructing the best tree takes time o(n3).
    specifically, o(gn3) where g = # of grammar rules.

    as we   ll see, many nlp tasks only need to tag the

words (not necessarily with parts of speech).
    don   t need training trees, only training tags.
    reconstructing the best tagging takes only time o(n).

    specifically, o(gn) where g = # of legal tag bigrams.

    it   s just the viterbi tagging algorithm again.

    but now score is a sum of many feature weights    

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
set arc weights so that path weight = tagging score

bos

n

time

a
n

v
  bos,n+  n,time

a
n

v

eos

let   s add lots of other weights to the arc score!
does v:flies score highly?  depends on features of

n v and v flies in context (at word 2 of sentence x).

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
set arc weights so that path weight = tagging score

bos

n

time

a
n

v
  bos,n+  n,time

a
n

v

eos

context-specific tagging features

n

v

p d

n

time flies like an arrow

    count of tag p as the tag for    like   

in an id48, the weight of this
feature would be the log of an

emission id203

but in general, it doesn   t have to

be a log id203

context-specific tagging features

n

v

p d

n

time flies like an arrow

    count of tag p as the tag for    like   
    count of tag p

context-specific tagging features

n

v

time flies like an arrow

1

2

5

0

n

p d
4

3

    count of tag p as the tag for    like   
    count of tag p
    count of tag p in the middle third of the sentence

context-specific tagging features

n

v

p d

n

time flies like an arrow

    count of tag p as the tag for    like   
    count of tag p
    count of tag p in the middle third of the sentence
    count of tag bigram v p

in an id48, the weight of this
feature would be the log of an

emission id203

but in general, it doesn   t have

to be a log id203

context-specific tagging features

n

v

p d

n

time flies like an arrow

    count of tag p as the tag for    like   
    count of tag p
    count of tag p in the middle third of the sentence
    count of tag bigram v p
    count of tag bigram v p followed by    an   

context-specific tagging features

n

v

p d

n

time flies like an arrow

    count of tag p as the tag for    like   
    count of tag p
    count of tag p in the middle third of the sentence
    count of tag bigram v p
    count of tag bigram v p followed by    an   
    count of tag bigram v p where p is the tag for    like   

context-specific tagging features

n

v

p d

n

time flies like an arrow

    count of tag p as the tag for    like   
    count of tag p
    count of tag p in the middle third of the sentence
    count of tag bigram v p
    count of tag bigram v p followed by    an   
    count of tag bigram v p where p is the tag for    like   
    count of tag bigram v p where both words are lowercase

more expensive tagging features

n

v

p d

n

time flies like an arrow

    count of tag trigram n v p?

    a bigram tagger can only consider within-bigram features:

only look at 2 adjacent blue tags (plus arbitrary red context).

    so here we need a trigram tagger, which is slower.
    as an fst, its state would remember two previous tags.

n v

p

v p

we take this arc once per n v p triple,

so its weight is the total weight of
the features that fire on that triple.

more expensive tagging features

n

v

p d

n

time flies like an arrow

    count of tag trigram n v p?

    a bigram tagger can only consider within-bigram features:

only look at 2 adjacent blue tags (plus arbitrary red context).

    so here we need a trigram tagger, which is slower.

    count of    post-verbal    nouns? (   discontinuous bigram    v n)

    an id165 tagger can only look at a narrow window.
    so here we need an id122 whose states remember whether there

was a verb in the left context.
v

p

v

n

v     p

p

d

n

v     d

d

post-verbal
p d bigram

post-verbal
d n bigram

v     n

n

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

full name of tag i
first letter of tag i (will be    n    for both    nn    and    nns   )
full name of tag i-1 (possibly bos); similarly tag i+1 (possibly eos)
full name of word i
last 2 chars of word i (will be    ed    for most past-tense verbs)
first 4 chars of word i (why would this help?)
   shape    of word i (lowercase/capitalized/all caps/numeric/   )

for position i in a tagging, these might include:
   
   
   
   
   
   
   
    whether word i is part of a known city name listed in a    gazetteer   
    whether word i appears in thesaurus entry e (one attribute per e)
    whether i is in the middle third of the sentence

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

nonterminal at n
nonterminal at first child of n, or    null    if child is a word
nonterminal at second child of n, or    null    if only one child
constituent width j-i

for a node n in a parse tree that covers the substring (i,j):
   
   
   
   
    whether j-i     3 (true/false)
    whether j-i     10 (true/false)
    words i+1 and j (first and last words of constituent)
    words i and j+1 (words immediately before and after constituent)
suffixes, prefixes, shapes, and categories of all of these words
   

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).

at each position of (x,y), exactly one of the many
template7 features will fire:

n

v

p d

n

time flies like an arrow

at i=0, we see an instance of    template7=(bos,n,-es)   
so we add one copy of that feature   s weight to score(x,y)

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).

at each position of (x,y), exactly one of the many
template7 features will fire:

n

v

p d

n

time flies like an arrow

at i=1, we see an instance of    template7=(n,v,-ke)   
so we add one copy of that feature   s weight to score(x,y)

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).

at each position of (x,y), exactly one of the many
template7 features will fire:

n

v

p d

n

time flies like an arrow

at i=2, we see an instance of    template7=(n,v,-an)   
so we add one copy of that feature   s weight to score(x,y)

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).

at each position of (x,y), exactly one of the many
template7 features will fire:

n

v

p d

n

time flies like an arrow

at i=3, we see an instance of    template7=(p,d,-ow)   
so we add one copy of that feature   s weight to score(x,y)

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).

at each position of (x,y), exactly one of the many
template7 features will fire:

n

v

p d

n

time flies like an arrow

at i=4, we see an instance of    template7=(d,n,-)   
so we add one copy of that feature   s weight to score(x,y)

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).
this template gives rise to many features, e.g.:

score(x,y) =    
+   [   template7=(p,d,-ow)   ] * count(   template7=(p,d,-ow)   )
+   [   template7=(d,d,-xx)   ] * count(   template7=(d,d,-xx)   )
+    

with a handful of feature templates and a large vocabulary, you
can easily end up with millions of features.

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   

e.g., template 7 might be (tag(i), tag(i+1), suffix2(i+2)).

note: every template should mention at least some blue.
given an input x, a feature that only looks at red will contribute
the same weight to score(x,y1) and score(x,y2).
so it can   t help you choose between outputs y1, y2.

   

   

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   
3. train your system!

    what if you had too many features?

   
   

   

that   s what id173 is for.  prevents overfitting.
an l1 regularizer will do    feature selection    for you.  keeps a feature   s
weight at 0 if it didn   t help enough on the training data.
fancier extensions of l1 will even do feature template selection.
   
   

if training throws out a template, you get a test-time speedup.
(ordinarily at test time, at every position, you   d have to construct
a feature from that template & look up its weight in a hash table.)

group lasso, graphical lasso,
feature induction in random

fields, meta-features    

how might you come up with the features
that you will use to score (x,y)?

1. think of some attributes (   basic features   ) that you can

compute at each position in (x,y).

2. now conjoin them into various    feature templates.   
3. train your system!

   
   
   
   

    what if you had too many features?
    what if you didn   t have enough features?
then your system will have some errors.
study errors and come up with features that might help fix them.
maybe try to learn features automatically (e.g.,    deep learning   ).
alternatively, the    kernel trick    lets you expand to mind-bogglingly big
(even infinite) feature sets.  e.g., all 5-way conjunctions of existing
features, including conjunctions that don   t stay within an id165!
    runtime no longer scales up with the # of features that fire on a
sentence.  but now it scales up with the # of training examples.

check out
   kernelized
id88.   
but the trick
started with
kernel id166s.

83% of

probabilists   rally   behind    paradigm
   .2, .4, .6, .8!  we   re not gonna take your bait!   
1. maybe we like our training criterion better than id88

^

    modeling the true id203 distribution may generalize better

2. our model offers a whole distribution, not just one output:

    how sure are we that y is the correct parse? (confidence)
    what   s the expected error of parse y? (bayes risk)
    what parse y has minimum expected error? (posterior decoding)
    marginal prob that [time flies] is np? (soft feature for another system)

3. our results can be meaningfully combined     modularity!
    train several systems and multiply their conditional probabilities
    p(english text) * p(english phonemes | english text) * p(jap.

phonemes | english phonemes) * p(jap. text | jap. phonemes)

    p(semantics) * p(syntax | semantics) * p(morphology | syntax) *

p(phonology | morphology) * p(sounds | phonology)

600.465 - intro to nlp - j. eisner

90

probabilists regret being bound by principle
1. those context-specific features sure seem helpful!
2. and even with context-free features, discriminative

training generally gets better accuracy.

   

   

   

   

fortunately, both of these deficiencies can be fixed within
a probabilistic framework.    

id88 only learns how to score structures.  the scores may
use rich features, but don   t have a probabilistic interpretation.
let   s keep the same scoring functions (linear functions on the
same features).  but now interpret score(x,y) as log p(y | x).
as usual for such id148, train the weights
so that the events in training data have high conditional
id203 p(y | x).  slightly different from id88 training.
but like id88, we   re training the weights to discriminate
among y values, rather than to predict x.

91

600.465 - intro to nlp - j. eisner

p(parse | sentence)

score(sentence, parse) back to p(parse | sentence)

time flies like an arrow

time flies like an arrow

time flies like an arrow
600.465 - intro to nlp - j. eisner

time flies like an arrow

   

92

generative processes

1. those context-specific features sure seem helpful!
even with the same features, discriminative training
2.
generally gets better accuracy.

   

   

   
   
   

fortunately, both of these deficiencies can be fixed within
a probabilistic framework.    

our pid18, id48, and probabilistic fst frameworks relied on
modeling the probabilities of individual context-free moves:
   

p(rule | nonterminal), p(word | tag), p(tag | previous tag),
p(transition | state)

perhaps each of these was a log-linear id155.
our models multiplied them all to get a joint id203 p(x,y).
instead, let   s model p(y | x) as a single log-linear distribution    

600.465 - intro to nlp - j. eisner

93

generative processes

random fields

1. those context-specific features sure seem helpful!
even with the same features, discriminative training
2.
generally gets better accuracy.

   

fortunately, both of these deficiencies can be fixed within
a probabilistic framework.    

markov random field (mrf)

conditional random field (crf)

p(x,y) = 1/z exp      f(x,y)

p(y|x) = (1/z(x)) exp      f(x,y)

train to maximize log p(x,y)
generates x,y    all at once.   
scores result as a whole, not
individual generative steps.

train to maximize log p(y|x)

generates y    all at once    given x.
discriminative like id88    
and efficient for same features.

finding the best y given x

    how do you make predictions given input x?
    can just use the same viterbi algorithms again!
    id88 picks y that maximizes score(x,y).
    crf defines p(y | x) = (1/z(x)) exp score(x,y).

    for a single output, could pick y that maximizes p(y | x).

    this    1-best    prediction is the single y that is most likely to be

completely right (according to your trained model).

    but that   s exactly the y that maximizes score(x,y).

    why? exp is an increasing function, and 1/z(x) is constant.

    the only difference is in how    is trained.

600.465 - intro to nlp - j. eisner

95

id88 training algorithm

    initialize    (usually to the zero vector)
    repeat:

    pick a training example (x,y)

    current    predicts y   maximizing score(x,y  )
    update weights by a step of size    > 0:

   =    +        (f(x,y)     f(x,y  ))

crf

id88 training algorithm

    initialize    (usually to the zero vector)
    repeat:

    pick a training example (x,y)

defines a distribution p(y   | x)

    current    predicts y   maximizing score(x,y  )
    update weights by a step of size    > 0:

   =    +        (f(x,y)     f(x,y  ))

expected features of a random y  
chosen from the distribution:

   y   p(y   | x) f(x,y  )

crf training algorithm

    initialize    (usually to the zero vector)
    repeat:

    pick a training example (x,y)
    current    defines a distribution p(y   | x)
    update weights by a step of size    > 0:

    update   .

   =    +        (f(x,y)        y   p(y   | x) f(x,y  ))

observed     expected features

must get
smaller
but not
too fast;
can use
   =
1/(t+1000)
on iter t

that is, we   re training a conditional log-linear model p(y | x)

by stochastic gradient ascent as usual.
(should add a regularizer, hence a step to update weights toward 0.)

but now y is a big structure like trees or taggings.

crf training algorithm

    initialize    (usually to the zero vector)
    repeat:

    pick a training example (x,y)
    current    defines a distribution p(y   | x)
    update weights by a step of size    > 0:

   =    +        (f(x,y)        y   p(y   | x) f(x,y  ))

observed     expected features

how do we compute the expected features?
forward-backward or inside-algorithm tells us expected

count of each substructure (transition / emission / arc /
rule).  so we can iterate over that substructure   s features.

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
p(path | x) = (1/z(x)) exp(sum of    values on path)

bos

n

time

a
n

v
  bos,n+  n,time

a
n

v

eos

id88: (   ,   ) = (max,+)
crf: (   ,   ) = (log+,+)
treat scores as log-probs

run forward algorithm in this semiring to get

log z(x) =    total    (log+) weight of all paths

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
p(path | x) = (1/z(x)) exp(sum of    values on path)
= (1/z(x)) (product of     values on path)

where we define each    k = exp   k

bos

n

time

a
n

v

a
n

v

eos

   bos,n*   n,time
id88: (   ,   ) = (max,+)
crf: (   ,   ) = (log+,+)
if you prefer simpler semiring    

(+,*) forward algorithm in this semiring to get

log z(x) = total weight of all paths

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
p(path | x) = (1/z(x)) exp(sum of    values on path)
= (1/z(x)) (product of     values on path)

where we define each    k = exp   k

bos

n

time

a
n

v

a
n

v

eos

   bos,n*   n,time
id88: (   ,   ) = (max,+)
crf: (   ,   ) = (log+,+)
if you prefer simpler semiring    

(+,*) forward-backward algorithm to get

expected count (given x) of every arc

running example: predict a tagging

lattice of exponentially many taggings (fst paths)
p(path | x) = (1/z(x)) exp(sum of    values on path)
= (1/z(x)) (product of     values on path)

where we define each    k = exp   k

a
n

v

bos

n

time

   bos,n*   n,time

crf replaces id48   s
probabilities by arbitrary
   potentials       k = exp   k > 0,
chosen to maximize log p(y | x)

a
n

v

eos

summary of probabilistic
methods in this course!
    each observed sentence x has some unknown structure y.
    we like to define p(x,y) as a product of quantities associated with

different substructures of (x,y).
    these could be probabilities, or just potentials (= exponentiated weights).
    if they   re potentials, we   ll need to multiply 1/z into the product too.

    thus, p(x) =    y p(x,y) is a sum of products.

    among other uses, we need it to get p(y | x) = p(x,y) / p(x).
    lots of summands, corresponding to different structures y, so

we hope to do this sum by id145.  (forward, inside algs)

    to increase log p(yi | xi) by stochastic gradient ascent or em, must
find substructures of (xi,y) that are expected under current p(y | xi).
    more id145.  (forward-backward, inside-outside algs)

    simplest way to predict yi is as argmaxy p(xi,y).

    more id145. (viterbi inside, viterbi forward)
    this is actually all that   s needed to train a id88 (not probabilistic).
    posterior decoding might get better results, depending on id168.

600.465 - intro to nlp - j. eisner

104

from log-linear to deep learning:
   energy-based    models

    define
    then

    log-linear case:

105

from log-linear to deep learning:
   energy-based    models

    define

    why    energy-based   ?  these distributions show up in physics.

    let   s predict the 3-dimensional structure y

of a complex object x, such as a protein molecule

    energy(x,y) = a linear function: sums up local energies in the structure
    structure varies randomly according to boltzmann distribution:

p(y | x) = (1/z(x)) exp (   energy(x,y) / t) where t > 0 is temperature

    so at high temperature, many different structures are found; but
as t     0, the protein is usually found in its lowest-energy shape.
    in machine learning, we can define energy(x,y) however we like.

106

deep scoring functions

    linear model:

where f is a hand-designed feature vector

    what if we don   t want to design f by hand?
    could we learn automatic features?
    just define f using some additional parameters that

have to be learned along with   .

    log p(y | x) may now have local maxima    
    we can still locally optimize it by stochastic gradient:

deep scoring functions
    linear model:

where f gets a hand-designed feature vector

    multi-layer id88:

where g gives a (simple) hand-designed vector, maybe just
(or define g with same trick!)
basic attributes and embeddings.

fk fires iff g(x,y) has enough of the attributes picked out by wk.
so fk is a kind of conjunctive feature.  but by learning wk and bk, we   re
learning which attributes to conjoin and how strongly.

deep scoring functions
    linear model:

where f gets a hand-designed feature vector

    multi-layer id88:

replace by something differentiable
so we can learn by gradient ascent

deep scoring functions
    linear model:

where f gets a hand-designed feature vector

    neural network:

something differentiable

so we can learn by gradient ascent

let

where

deep scoring functions
    linear model:

where f gets a hand-designed feature vector

    neural network:

    to define g(x,y), we might also learn    recurrent neural

networks    that turn strings within (x,y) into vectors.
    e.g., the vector for    the tiny flea jumped    is the output of a neural

network whose input concatenates the vector for    the tiny flea   
(recursion!) with the embedding vector for    jumped   .

why is discriminative training good?

    id88s and crfs and deep crfs can

efficiently make use of richer features.

s

vp

pp

np

np

vp

n

v

p d

n

time flies like an arrow

n

v

p d

n

time flies like an arrow

why is discriminative training good?

    and even with the same features, discriminative

usually wins!

    joint training tries to predict both x and y.
    discriminative training only tries to predict y

(given x), so it does a better job of that:
    predict the correct y (id88)
    predict the distribution over y (crf)

    in fact, predicting x and y together may be too

much to expect    

why is discriminative training good?

    predicting x and y together may be too much to expect of a

   weak model    like a pid18 or id48.

    if you generate (x,y) from a pid18 or id48, it looks awful!

    you get silly or ungrammatical sentences x.
    suggests that pid18 and id48 aren   t really such great models of

p(x,y), at least not at their current size (    50 nonterminals or states).

    but generating y given x might still give good results.

    pid18 and id48 can provide good conditional distributions p(y | x).

    so just model p(y | x).  twisting the weights to also predict

sentences x will distort our estimate of p(y | x).

why is discriminative training good?

    predicting x and y together may be too much to expect of a    weak

model    like a pid18 or id48.

    so just model p(y | x).  twisting the weights to also predict sentences x

will distort our estimate of p(y | x).

    let p   denote the pid18 with weight parameters   .
    joint training: adjust    so that p  (x,y) matches joint distribution of data.
    discrim. training: adjust    so that p  (y | x) matches conditional distribution.

or equivalently, so that phybrid(x,y) matches joint distribution:

phybrid(x,y) = pempirical(x)     p  (y | x).

where pempirical(x) = 1/n for each of the n training sentences, and is not
sensitive to   .  so we   re letting the data (not the pid18!) tell us the
distribution of sentences x.

when do you want joint training?

    predicting x and y together may be too much to expect of a    weak

model    like a pid18 or id48.  so just model p(y | x).  twisting the weights
to also predict sentences x will distort our estimate of p(y | x).

    on the other hand, not trying to predict x means we   re not
learning from the distribution of x.    throwing away data.   
    use joint training if we trust our model.  discriminative training

throws away x data only because we doubt we can model it well.

    also use joint in unsupervised/semisupervised learning.  here x is all

we have for some sentences, so we can   t afford to throw it away   
    how can we know y then?  id48/pid18 assumes y latently influenced x.
    em algorithm fills in y to locally maximize log p  (x) = log    y p  (x,y).
    requires joint model p  (x,y).  (q: why not max log    y p  (y | x) instead?)
    em can work since the same    is used to define both p  (x) and p  (y | x).
both come from p  (x,y) (p  (x) =    y p  (x,y) and p  (y | x) = p  (x,y)/p  (x)).
by observing x, we get information about   , which helps us predict y.

na  ve bayes vs. id28

    dramatic example of training p(y | x) versus p(x,y).

    let   s go back to text categorization.
(a feature vector)

    x = (x1, x2, x3,    )
    y = {spam, gen}

       na  ve bayes    is a popular, very simple joint model:

    p(x,y) = p(y)     p(x1 | y)     p(x2 | y)     p(x3 | y)              
    q: how would you train this from supervised (x,y) data?
    q: given document x, how do we predict category y?
    q: what are the conditional independence assumptions?
    q: when are those    na  ve    assumptions reasonable?

na  ve bayes   s conditional independence
assumptions break easily

    pick y maximizing p(y)     p(x1 | y)     p(x2 | y)              
    x = buy this supercalifragilistic ginsu
    some features xk that fire on this example    

knife set for only $39 today    

   contains buy
   contains supercalifragilistic
   contains a dollar amount under $100
   contains an imperative sentence
   reading level = 7th grade
   mentions money (use word classes and/or regexp to detect this)
      

600.465 - intro to nlp - j. eisner

118

na  ve bayes   s conditional independence
assumptions break easily

    pick y maximizing p(y)     p(x1 | y)     p(x2 | y)              
    x = buy this supercalifragilistic ginsu
knife set for only $39 today    
    some features xk that fire on this example,
and their prob of firing when y=spam versus y=gen:

.5  .02

50% of spam has this     25x more likely than in gen

    contains a dollar amount under $100

.9  .1

90% of spam has this     9x more likely than in gen

    mentions money

na  ve bayes
claims .5*.9=45%
of spam has both
features    
25*9=225x more
likely than in gen.

but here are the emails with both features     only 25x!
first feature implies second feature.  na  ve bayes is
overconfident because it thinks they   re independent.

600.465 - intro to nlp - j. eisner

119

na  ve bayes vs. id28

    we have here a lousy model of p(x,y), namely p(y)     p(x1 | y)     p(x2 | y)              
if we used it to generate (x,y), we   d get incoherent feature vectors that could not
come from any actual document x (   mentions < $100   =1,    mentions money   =0).

   

its conditional distribution p(y | x) is nonetheless serviceable.

   
    training options:

    supervised: maximize log p(x,y). (   na  ve bayes   )
    unsupervised: maximize log p(x) = log    y p(x,y) via em. (   document id91   )
    supervised: maximize log p(y | x). (   id28   )

directly train conditional distribution we need.  how?

reinterpret na  ve bayes conditional distrib as log-linear (   nuthin    but adding weights   ):

p(y | x) = p(x,y) / p(x)

= (1/p(x)) p(y)     p(x1 | y)     p(x2 | y)              
= (1/z(x)) exp (  (y) +   (x1, y) +   (x2, y)              )
where    z(x) = p(x)

  (y) = log p(y)

so just do ordinary gradient ascent training of a conditional log-linear model.

whose features are as shown: conjoin features of x with the identity of y.

  (xk, y) = log p(xk | y)

id28 doesn   t model x, so doesn   t
model x   s features as independent given y!

na  ve bayes

p(xk | y)

initial   (xk,y)
= log p(xk | y)

final   (xk,y) after
gradient ascent

.5 .02

    contains a dollar amount under $100

-1 -5.6

-.85 -2.3

.9 .1

    mentions money

-.15 -3.3

-.15 -3.3

changed to compensate for the fact that whenever
this feature fires, so will    mentions money    feature.

id28 trains weights to work together (needs gradient ascent).
na  ve bayes trains weights independently for each k (easier: count & divide).

600.465 - intro to nlp - j. eisner

121

id28 doesn   t model x, so doesn   t
model x   s features as independent given y!

na  ve bayes

p(xk | y)

initial   (xk,y)
= log p(xk | y)

final   (xk,y) after
gradient ascent

.5 .02

    contains a dollar amount under $100

-1 -5.6

-.85 -2.3

-.15 -3.3

-.15 -3.3

    mentions money

.9 .1
q: is this truly just conditional training of the parameters of our original model?
the old parameters were probabilities that had to sum to 1.  but now it seems
we   re granting ourselves the freedom to use any old weights that can no longer be
interpreted as log p(y) and log p(xk | y).  is this extra power why we do better?
a: no extra power! challenge: show how to adjust the weights after training,

without disturbing p(y|x), to restore    y exp   (y) = 1 and   y   k    xkexp   (xk,y) = 1.

summary

given x, always compute best y by viterbi algorithm.
what   s different is the meaning of the resulting score.

    joint model, p(x,y):

    classical models: pid18, id48, na  ve bayes

    product of many simple conditional distributions over generative moves.
       locally normalized   : each distribution must sum to 1: divide by some z.

    or markov random field: p(x,y) = (1/z) exp        f(x,y)

       globally normalized   : one huge distribution normalized by a single z.
    z is hard to compute since it sums over all parses of all sentences.

    conditional model, p(y | x):

    conditional random field: p(y|x) = (1/z(x)) exp        f(x,y)

    globally normalized, but z(x) only sums over all parses of sentence x.
    z(x) is efficient to compute via inside algorithm.
    features can efficiently conjoin any properties of x and a    local   

property of y.  train by gradient ascent:

    doesn   t try to model p(x), i.e.,    throws away    x data: good riddance?

    discriminative model, score(x,y):

    e.g., id88: no probabilistic interpretation of the score.
    train    to make the single correct y beat the others (for each x).
    (variants: train to make    better    y values beat    worse    ones.)

summary: when to build a generative
p(x,y) vs. discriminative p(y|x) model?

    unsupervised learning?     generative

   
   

    observing only x gives evidence of p(x) only.
    generative model says p(x) carries info about y.
    discriminative model doesn   t care about p(x).

    it only tries to model p(y|x), treating p(x) as

    so it will ignore our only training data as    irrelevant

   someone else   s job.   

to my job.   

    (intermediate option: contrastive estimation.)

600.465 - intro to nlp - j. eisner

124

summary: when to build a generative
p(x,y) vs. discriminative p(y|x) model?

    unsupervised learning?     generative
    rich features?     discriminative
   

    discriminative p(y | x) can efficiently use

features that consider arbitrary properties of x.
    see earlier slides on    context-specific features.   
    also works for non-probabilistic discriminative
models, e.g., trained by structured id88.
    generative p(x,y) with the same features is

usually too computationally hard to train.
    since z =    x,y      f(x,y) would involve extracting

features from every possible input x.

125

600.465 - intro to nlp - j. eisner

   

summary: when to build a generative
p(x,y) vs. discriminative p(y|x) model?

    unsupervised learning?     generative
    rich features?     discriminative
    neither case?     let dev data tell you!

    use a generative model, but choose    to max

log p  (y|x) +    log p  (x) + c regularizer(  )

    tune   , c on dev data

       = 0     discriminative training (best to ignore distrib of x)
       = 1     generative training (distrib of x gives useful info
about   : true if the data were truly generated from your model!)

       = 0.3     in between (distrib of x gives you some useful info

but trying too hard to match it would harm predictive accuracy)

600.465 - intro to nlp - j. eisner

126

summary: when to build a generative
p(x,y) vs. discriminative p(y|x) model?

    unsupervised learning?     generative
    rich features?     discriminative
    neither case?     let dev data tell you!
    use a generative model, but train    to max
log p  (y|x) +    log p  (x) + c regularizer(  )

    what you really want is high log p  (y|x) on future data.
    but you only have a finite training sample to estimate

that. both blue terms provide useful bias that can help
compensate for the variance of your estimate.

    same idea as multi-task or multi-domain learning: to find params

that are good at your real task (predicting y from x), slightly prefer
params that are also good at something related (predicting x).

600.465 - intro to nlp - j. eisner

127

summary: when to build a generative
p(x,y) vs. discriminative p(y|x) model?

    unsupervised learning?     generative
    rich features?     discriminative
    neither case?     let dev data tell you!
    use a generative model, but train    to max
log p  (y|x) +    log p  (x) + c regularizer(  )

    note: equivalent to

(1-  ) log p  (y|x) +    log p  (y|x) +    log p  (x) + c r(  )
+ c r(  )

+    log p  (x,y)

= (1-  ) log p  (y|x)

    so on each example, stochastic gradient ascent can

stochastically follow the gradient of discriminative log
p  (y|x) with prob 1-   or generative log p  (x,y) with prob   

600.465 - intro to nlp - j. eisner

128

summary: when to build a generative
p(x,y) vs. discriminative p(y|x) model?

    use a generative model, but train    to max
log p  (y|x) +    log p  (x) + c regularizer(  )

    what you really want is high log p  (y|x) on future data.
ok, maybe not quite.  suppose you will act at test time (and dev time) using a decision rule
    (x) that tells you what action to take on input x, using the learned parameters   .
and the id168 l(a | x,y) tells you how bad action a would be in a situation with a
particular input x if the unobserved output were y.
then what you really want is low loss on future data: l(    (x) | x,y) should be low on average.
so replace log p  (y|x) in training with -l(    (x) | x,y), or perhaps -l(    (x) | x,y) +       log p  (y|x).

decision rules don   t have to be probabilistic or differentiable: e.g., the id88 uses a linear
scoring function as its decision rule.  (at training time it simply tries to drive training loss to 0.)
but if you have a good id203 model p  (y | x), then the ideal decision rule is minimum
bayes risk (mbr):     (x) = argmina    y p  (y | x) l(a | x,y).  (risid116 expected loss.)
mbr reduces to the viterbi decision rule,     (x) = argmaxy p   (y | x), in the special case where
actions are predictions of y and we use    0-1    loss, that is, l(a | x,y)=(if (a==y) then 0 else 1).
posterior decoding is the mbr rule for a different id168: it chooses a tag sequence a
that minimizes the expected number of incorrect tags (possibly p(a | x)=0, unlike viterbi!).

