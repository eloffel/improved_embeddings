id48

and the forward-backward 
algorithm

600.465 - intro to nlp - j. eisner

1

please see the spreadsheet

    i like to teach this material using an 

interactive spreadsheet:
    http://cs.jhu.edu/~jason/papers/#tnlp02
    has the spreadsheet and the lesson plan

    i   ll also show the following slides at 

appropriate points.

600.465 - intro to nlp - j. eisner

2

marginalization

sales jan feb mar apr

widgets

grommets

gadgets

   

5

7

0

   

0

3

0

   

3

10

1

   

2

8

0

   

   

   

   

   

   

600.465 - intro to nlp - j. eisner

3

marginalization

write the totals in the margins

sales jan feb mar apr

    total

widgets

grommets

gadgets

   

total

5

7

0

   

99

0

3

0

   

3

10

1

   

25 126

2

8

0

   

90

   

   

   

   

30

80

2

1000

600.465 - intro to nlp - j. eisner

grand total

4

marginalization

given a random sale, what & when was it?

prob.

jan feb mar apr

    total

widgets .005

0 .003 .002

grommets .007 .003 .010 .008

gadgets

   

0

   

0 .001

   

   

0

   

   

   

   

   

.030

.080

.002

total .099 .025 .126 .090

1.000

600.465 - intro to nlp - j. eisner

grand total

5

given a random sale, what & when was it?

marginalization

joint prob: p(jan,widget)

prob.

jan

feb mar

apr

    total

widgets .005

0 .003 .002

grommets .007 .003 .010 .008

gadgets

   

0

   

0 .001

   

   

0

   

   

   

   

   

marginal 
prob: 
p(widget)

.030

.080

.002

total .099 .025 .126 .090

1.000

marginal prob: p(jan)

marginal prob:
p(anything in table)

600.465 - intro to nlp - j. eisner

6

given a random sale in jan., what was it?

conditionalization

divide column 

through by z=0.99 

so it sums to 1
p(    | jan)

joint prob: p(jan,widget)

prob.

jan

feb mar

apr

    total

widgets .005

0 .003 .002

grommets .007 .003 .010 .008

gadgets

   

0

   

0 .001

   

   

0

   

   

   

   

   

.030

.080

.002

.005/.099

.007/.099

0

   

total .099 .025 .126 .090

1.000

.099/.099

marginal prob: p(jan)
conditional prob: p(widget|jan)=.005/.099

600.465 - intro to nlp - j. eisner

7

marginalization & conditionalization
in the weather example

    instead of a 2-dimensional table, 

now we have a 66-dimensional table:
    33 of the dimensions have 2 choices: {c,h}
    33 of the dimensions have 3 choices: {1,2,3}

    cross-section showing just 3 of the dimensions:

icecream2=1
icecream2=2
icecream2=3
600.465 - intro to nlp - j. eisner

weather2=c weather2=h

  0.000   

  0.000   

  0.000   

  0.000   

  0.000   

  0.000   

8

interesting probabilities in 
the weather example

    prior id203 of weather:

p(weather=chh   )

    posterior id203 of weather (after observing evidence):

p(weather=chh    | icecream=233   )

    posterior marginal id203 that day 3 is hot:

p(weather3=h | icecream=233   )
=    w such that w3=h p(weather=w | 
icecream=233   )
    posterior id155 

that day 3 is hot if day 2 is:
p(weather3=h | weather2=h, icecream=233   )
600.465 - intro to nlp - j. eisner

9

the id48 trellis

the id145 computation of       works forward from start.

day 1: 2 cones

day 2: 3 cones

day 3: 3 cones

   =1

start

)

t

* p ( 2 | c )
p ( c | s t a r
0 . 5 * 0 . 2 = 0 . 1
p(h|start)*p(2|h)
0.5*0.2=0.1

   =0.1*0.08+0.1*0.01

   =0.009*0.08+0.063*0.01

   =0.1

c

p(c|c)*p(3|c)
0.8*0.1=0.08

=0.009
c

=0.00135

p(c|c)*p(3|c)
0.8*0.1=0.08

p(h|c)*p(3|h)
0.1*0.7=0.07

p ( c | h )* p ( 3| c )
0 .1 * 0 .1
p(h|h)*p(3|h)
0.8*0.7=0.56

0 .0

=

1

p(h|c)*p(3|h)
0.1*0.7=0.07

p ( c | h )* p ( 3| c )
0 .1 * 0 .1
p(h|h)*p(3|h)
0.8*0.7=0.56

0 .0

=

h

c

1

h

h

   =0.1

   =0.1*0.07+0.1*0.56

=0.063

   =0.009*0.07+0.063*0.56

=0.03591

    this    trellis    graph has 233 paths.

    these represent all possible weather sequences that could explain the 

observed ice cream sequence 2, 3, 3,     

    what is the product of all the edge weights on one path h, h, h,    ?

    edge weights are chosen to get p(weather=h,h,h,    & icecream=2,3,3,   )

    what is the     id203 at each state?

    it   s the total id203 of all paths from start to that state.
    how can we compute it fast when there are many paths?

600.465 - intro to nlp - j. eisner

10

computing     values

all paths to state:
    = (ap1 + bp1 + cp1)
+ (dp2 + ep2 + fp2)
p1
p2

c

=    1   p1 +    2   p2

   

a
b

d

c

e
f

   1

   2

c

h

600.465 - intro to nlp - j. eisner

thanks, distributive law!

11

the id48 trellis

the id145 computation of     works back from stop.
day 34: lose diary
day 32: 2 cones

day 33: 2 cones

   =0.16*0.018+0.02*0.018

   =0.16*0.1+0.02*0.1

=0.00324

p(c|c)*p(2|c)
0.8*0.2=0.16

=0.018
c

p(c|c)*p(2|c)
0.8*0.2=0.16

   =0.1

0 .0

p ( h | c )* p ( 2| h )
p(c|h)*p(2|c)
2      
   0.1*0.2=0.02
0 .1 * 0 .2

=
p(h|h)*p(2|h)
0.8*0.2=0.16

0 .0

p ( h | c )* p ( 2| h )
2      
0 .1 * 0 .2
h

p ( c | h )* p ( 2| c )
0 .1 * 0 .2
=
p(h|h)*p(2|h)
0.8*0.2=0.16

0 .0

=

c

h

c

2

h

p(stop|c)
0.1
p ( s t o p | h )
0 . 1

stop

   =0.16*0.018+0.02*0.018

=0.00324

   =0.16*0.1+0.02*0.1

=0.018

   =0.1

    what is the     id203 at each state?

    it   s the total id203 of all paths from that state to stop
    how can we compute it fast when there are many paths?

600.465 - intro to nlp - j. eisner

12

computing     values

all paths from state:
    = (p1u + p1v + p1w)
+ (p2x + p2y + p2z)

c

= p1      1 + p2      2

p1
p2

c

h

u
v

w

x

y
z

   1

   2

   

600.465 - intro to nlp - j. eisner

13

computing state probabilities

all paths through state:
ax + ay + az
+ bx + by + bz
+ cx + cy + cz
   

a
b

c

x
y

z

   

c

= (a+b+c)   (x+y+z)

=       (c)        (c)

600.465 - intro to nlp - j. eisner

thanks, distributive law!

14

computing arc probabilities

all paths through the p arc:
apx + apy + apz
+ bpx + bpy + bpz
+ cpx + cpy + cpz

x
y

z

   

c

p

   

a
b

c

h

600.465 - intro to nlp - j. eisner

= (a+b+c)   p   (x+y+z)

=       (h)     p        (c)

thanks, distributive law!

15

posterior tagging

    give each word its highest-prob tag according to 

forward-backward.
    do this independently of other words.
    det adj
    det n
    n    v

0.35
0.2
0.45

    exp # correct tags = 0.55+0.35 = 0.9
    exp # correct tags = 0.55+0.2 = 0.75
    exp # correct tags = 0.45+0.45 = 0.9

    output is 

    det v

0

    exp # correct tags = 0.55+0.45 = 1.0

    defensible: maximizes expected # of correct tags.
    but not a coherent sequence.  may screw up 

subsequent processing (e.g., can   t find any parse).

600.465 - intro to nlp - j. eisner
600.465 - intro to nlp - j. eisner

16
16

alternative: viterbi tagging

    posterior tagging: give each word its highest-

prob tag according to forward-backward.
    det adj
    det n
    n    v

0.35
0.2
0.45

    viterbi tagging: pick the single best tag sequence 

(best path):
    n    v

0.45

    same algorithm as forward-backward, but uses a 
semiring that maximizes over paths instead of 
summing over paths.

600.465 - intro to nlp - j. eisner
600.465 - intro to nlp - j. eisner

17
17

max-product instead of sum-product

use a semiring that maximizes over paths instead of summing.  
we write    ,    instead of    ,    for these    viterbi forward    and    viterbi 
backward    probabilities.   

the id145 computation of    .  (    is similar but works back from stop.)

day 1: 2 cones

day 2: 3 cones

day 3: 3 cones

start

)

t

* p ( 2 | c )
p ( c | s t a r
0 . 5 * 0 . 2 = 0 . 1
p(h|start)*p(2|h)
0.5*0.2=0.1

   =max(0.1*0.08,0.1*0.01)

             =max(0.008*0.08,0.056*0.01)

   =0.1

c

h

   =0.1

p(c|c)*p(3|c)
0.8*0.1=0.08

p(h|c)*p(3|h)
0.1*0.7=0.07

p ( c | h )* p ( 3| c )
0 .1 * 0 .1
p(h|h)*p(3|h)
0.8*0.7=0.56

0 .0

=

1

=0.008
c

p(c|c)*p(3|c)
0.8*0.1=0.08

=0.00064

c

1

p(h|c)*p(3|h)
0.1*0.7=0.07

0 .0

p ( c | h )* p ( 3| c )
0 .1 * 0 .1
p(h|h)*p(3|h)
0.8*0.7=0.56
                =max(0.008*0.07,0.056*0.56)

h

=

h

=0.03136

   =max(0.1*0.07,0.1*0.56)

=0.056

   *    at a state = total prob of all paths through that state
   *    at a state = max prob of any path through that state
suppose max prob path has prob p: how to print it?

    print the state at each time step with highest    *    (= p); works if no ties
    or, compute     values from left to right to find p, then follow backpointers

600.465 - intro to nlp - j. eisner

18

