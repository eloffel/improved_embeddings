observe some values of a function 

learning in the limit 

gold   s theorem 

600.465 - intro to nlp - j. eisner 

1 

600.465 - intro to nlp - j. eisner 

guess the whole function 

another guess: just as good? 

600.465 - intro to nlp - j. eisner 

3 

600.465 - intro to nlp - j. eisner 

more data needed to decide 

poverty of the stimulus 

(cid:1)(cid:1) never enough input data to completely 

determine the polynomial     
(cid:1)(cid:1) always have infinitely many possibilities 

(cid:1)(cid:1)     unless you know the order of the polynomial 

ahead of time. 
(cid:1)(cid:1) 2 points determine a line 
(cid:1)(cid:1) 3 points determine a quadratic 
(cid:1)(cid:1) etc. 

(cid:1)(cid:1) in language learning, is it enough to know that 

the target language is generated by a id18? 
(cid:1)(cid:1) without knowing the size of the id18? 

600.465 - intro to nlp - j. eisner 

5 

600.465 - intro to nlp - j. eisner 

2 

4 

6 

language learning: 
what kind of evidence? 

(cid:1)(cid:1) children listen to language  [unsupervised] 
(cid:1)(cid:1) children are corrected??  [supervised] 
(cid:1)(cid:1) children observe language in context 
(cid:1)(cid:1) children observe frequencies of language 

remember: language = set of strings 

poverty of the stimulus (1957) 

chomsky: just like polynomials: never enough data 
unless you know something in advance.  so kids must be 
born knowing what to expect in language. 

(cid:1)(cid:1) children listen to language 
(cid:1)(cid:1) children are corrected?? 
(cid:1)(cid:1) children observe language in context 
(cid:1)(cid:1) children observe frequencies of language 

600.465 - intro to nlp - j. eisner 

7 

600.465 - intro to nlp - j. eisner 

8 

gold   s theorem (1967) 

a simple negative result along these lines: 
kids (or computers) can   t learn much 
without supervision, inborn knowledge, or statistics 

(cid:1)(cid:1) children listen to language 
(cid:1)(cid:1) children are corrected?? 
(cid:1)(cid:1) children observe language in context 
(cid:1)(cid:1) children observe frequencies of language 

the idealized situation 
(cid:1)(cid:1) mom talks 
(cid:1)(cid:1) baby listens 

(cid:1)(cid:1) 1. mom outputs a sentence 
(cid:1)(cid:1) 2. baby hypothesizes what the language is 

 

 (given all sentences so far) 

(cid:1)(cid:1) 3. goto step 1 

(cid:1)(cid:1) guarantee: mom   s language is in the set of hypotheses 

that baby is choosing among 

(cid:1)(cid:1) guarantee: any sentence of mom   s language is 

eventually uttered by mom (even if infinitely many) 
(cid:1)(cid:1) assumption: vocabulary (or alphabet) is finite. 

600.465 - intro to nlp - j. eisner 

9 

600.465 - intro to nlp - j. eisner 

10 

can baby learn under these 
conditions? 

(cid:1)(cid:1) learning in the limit:  

(cid:1)(cid:1) there is some point at which baby   s hypothesis is correct and 

never changes again.  baby has converged! 

(cid:1)(cid:1) baby doesn   t have to know that it   s reached this point     it can 

keep an open mind about new evidence     but if its hypothesis is 
right, no such new evidence will ever come along. 

(cid:1)(cid:1) a class c of languages is learnable in the limit if one 

could construct a perfect c-baby that can learn any 
language l (cid:1) c in the limit from a mom who speaks l. 

(cid:1)(cid:1) baby knows the class c of possibilities, but not l.  
(cid:1)(cid:1) is there a perfect finite-state baby?   
(cid:1)(cid:1) is there a perfect context-free baby? 

languages vs. grammars 

(cid:1)(cid:1) does baby have to get the right grammar?  
(cid:1)(cid:1) (e.g., does vp have to be called vp?) 

(cid:1)(cid:1) assumption: finite vocabulary. 

600.465 - intro to nlp - j. eisner 

11 

600.465 - intro to nlp - j. eisner 

12 

conservative strategy 

conservative strategy 

(cid:1)(cid:1) baby   s hypothesis should always be smallest 

language consistent with the data 

(cid:1)(cid:1) baby   s hypothesis should always be smallest 

language consistent with the data 

(cid:1)(cid:1) works for finite languages?  let   s try it     

(cid:1)(cid:1) works for finite languages?  let   s try it     

(cid:1)(cid:1) language 1: {aa,ab,ac} 
(cid:1)(cid:1) language 2: {aa,ab,ac,ad,ae} 
(cid:1)(cid:1) language 3: {aa,ac} 
(cid:1)(cid:1) language 4: {ab} 

(cid:1)(cid:1) language 1: {aa,ab,ac} 
(cid:1)(cid:1) language 2: {aa,ab,ac,ad,ae} 
(cid:1)(cid:1) language 3: {aa,ac} 
(cid:1)(cid:1) language 4: {ab} 

aa 
ab 

ac 

ae 

ad 

mom 
baby 

aa 
l3 

ab 
l1 

ac 
l1 

ab 
l1 

aa 
l1 

    

mom 
baby 

aa 
l3 

ab 
l1 

ac 
l1 

ab 
l1 

aa 
l1 

    

600.465 - intro to nlp - j. eisner 

13 

600.465 - intro to nlp - j. eisner 

14 

evil mom 

an unlearnable class 

(cid:1)(cid:1) to find out whether baby is perfect, we have to 

(cid:1)(cid:1) class of languages: 

see whether it gets 100% even in the most 
adversarial conditions 

(cid:1)(cid:1) assume mom is trying to fool baby 

(cid:1)(cid:1) although she must speak only sentences from l 
(cid:1)(cid:1) and she must eventually speak each such sentence 

(cid:1)(cid:1) does baby   s strategy work? 

(cid:1)(cid:1) let ln = set of all strings of length < n 
(cid:1)(cid:1) what is l0?   
(cid:1)(cid:1) what is l1? 
(cid:1)(cid:1) what is l(cid:1)? 

(cid:1)(cid:1) if the true language is l(cid:1), can mom really follow rules? 
(cid:1)(cid:1) must eventually speak every sentence of l(cid:1). possible? 
(cid:1)(cid:1) yes: (cid:2); a, b; aa, ab, ba, bb; aaa, aab, aba, abb, baa,     

(cid:1)(cid:1) our class is c = {l0, l1,     l(cid:1)} 

600.465 - intro to nlp - j. eisner 

15 

600.465 - intro to nlp - j. eisner 

16 

an unlearnable class 

an unlearnable class 

(cid:1)(cid:1) let ln = set of all strings of length < n 

(cid:1)(cid:1) what is l0?   
(cid:1)(cid:1) what is l1? 
(cid:1)(cid:1) what is l(cid:1)? 

(cid:1)(cid:1) our class is c = {l0, l1,     l(cid:1)} 
(cid:1)(cid:1) a perfect c-baby will distinguish among all 

of these depending on the input. 
(cid:1)(cid:1) but there is no perfect c-baby     

(cid:1)(cid:1) our class is c = {l0, l1,     l(cid:1)} 
(cid:1)(cid:1) suppose baby adopts conservative 

strategy, always picking smallest possible 
language in c. 

(cid:1)(cid:1) so if mom   s longest sentence so far has 

75 words, baby   s hypothesis is l76. 

(cid:1)(cid:1) this won   t always work: what language 

can   t a conservative baby learn? 

600.465 - intro to nlp - j. eisner 

17 

600.465 - intro to nlp - j. eisner 

18 

an unlearnable class 

mom   s revenge 

(cid:1)(cid:1) our class is c = {l0, l1,     l(cid:1)} 
(cid:1)(cid:1) could a non-conservative baby be a perfect c-

baby, and eventually converge to any of these? 

(cid:1)(cid:1) claim: any perfect c-baby must be    quasi-

conservative   : 
(cid:1)(cid:1) if true language is l76, and baby posits something 

else, baby must still eventually come back and guess 
l76 (since it   s perfect). 

(cid:1)(cid:1) so if longest sentence so far is 75 words, and mom 

keeps talking from l76, then eventually baby must 
actually return to the conservative guess l76. 

(cid:1)(cid:1) agreed? 

 if longest sentence so far is 75 words, and mom keeps talking 
from l76, then eventually a perfect c-baby must actually return 
to the conservative guess l76. 
(cid:1)(cid:1) suppose true language is l(cid:1). 
(cid:1)(cid:1) evil mom can prevent our supposedly perfect c-baby 

from converging to it. 

(cid:1)(cid:1) if baby ever guesses l(cid:1), say when the longest sentence 

is 75 words:  
(cid:1)(cid:1) then evil mom keeps talking from l76 until baby capitulates and 

revises her guess to l76     as any perfect c-baby must.   

(cid:1)(cid:1) so baby has not stayed at l(cid:1) as required. 

(cid:1)(cid:1) then mom can go ahead with longer sentences.  if baby 
ever guesses l(cid:1) again, she plays the same trick again. 

600.465 - intro to nlp - j. eisner 

19 

600.465 - intro to nlp - j. eisner 

20 

mom   s revenge 

implications 

 if longest sentence so far is 75 words, and mom keeps talking 
from l76, then eventually a perfect c-baby must actually return 
to the conservative guess l76. 
(cid:1)(cid:1) suppose true language is l(cid:1). 
(cid:1)(cid:1) evil mom can prevent our supposedly perfect c-baby 

from converging to it. 

(cid:1)(cid:1) if baby ever guesses l(cid:1), say when the longest sentence 

is 75 words:  
(cid:1)(cid:1) then evil mom keeps talking from l76 until baby capitulates and 

revises her guess to l76     as any perfect c-baby must.   

(cid:1)(cid:1) so baby has not stayed at l(cid:1) as required. 

(cid:1)(cid:1) conclusion: there   s no perfect baby that is guaranteed 

to converge to l0, l1,     or l(cid:1) as appropriate.  if it 
always succeeds on finite languages, evil mom can trick 
it on infinite language. 

(cid:1)(cid:1) we found that c = {l0, l1,     l(cid:1)} isn   t 

learnable in the limit. 

(cid:1)(cid:1) how about class of finite-state languages? 
(cid:1)(cid:1) not unless you limit it further (e.g., # of states) 
(cid:1)(cid:1) after all, it includes all languages in c, and 

more, so learner has harder choice 

(cid:1)(cid:1) how about class of context-free languages? 

(cid:1)(cid:1) not unless you limit it further (e.g., # of rules) 

600.465 - intro to nlp - j. eisner 

21 

600.465 - intro to nlp - j. eisner 

22 

perfect fit to perfect, incomplete data 

punchline 

(cid:1)(cid:1) but class of probabilistic context-free languages 

is learnable in the limit!! 

(cid:1)(cid:1) if mom has to output sentences randomly with 

the appropriate probabilities,  
(cid:1)(cid:1) she   s unable to be too evil  
(cid:1)(cid:1) there are then perfect babies that are guaranteed to 

converge to an appropriate probabilistic id18 

(cid:1)(cid:1) i.e., from hearing a finite number of sentences, 
baby can correctly converge on a grammar that 
predicts an infinite number of sentences.   
(cid:1)(cid:1) baby is generalizing!  just like real babies! 

600.465 - intro to nlp - j. eisner 

23 

600.465 - intro to nlp - j. eisner 

24 

imperfect fit to noisy data 

will an ungrammatical sentence ruin baby forever? 

(yes, under the conservative strategy ...) 

or can baby figure out which data to (partly) ignore? 

600.465 - intro to nlp - j. eisner 

statistics can help again ... how? 

25 

