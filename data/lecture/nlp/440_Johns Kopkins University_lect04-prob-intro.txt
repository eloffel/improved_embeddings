how to use probabilities

the crash course

1

goals of this lecture

    id203 notation like p(y | x):

    what does this expression mean?
    how can i manipulate it?
    how can i estimate its value in practice?

    id203 models:

    what is one?
    can we build one for language id?
    how do i know if my model is any good?

600.465     intro to nlp     j. eisner

2

3 kinds of statistics

    descriptive: mean hopkins sat (or median)

    confirmatory: statistically significant?

    predictive: wanna bet?

this course     why?

600.465     intro to nlp     j. eisner

3

fugue for tinhorns

    opening number from guys and dolls

    1950 broadway musical about gamblers
    words & music by frank loesser

    video:

https://www.youtube.com/results?search_query=fugue+for+tinhorns

    lyrics:

http://www.lyricsmania.com/fugue_for_tinhorns_lyrics_guys_and_dolls
.html

600.465     intro to nlp     j. eisner

4

notation for greenhorns

   paul
revere   

id203

model

p(paul revere wins | weather   s clear) = 0.9

600.465     intro to nlp     j. eisner

5

what does that really mean?

p(paul revere wins | weather   s clear) = 0.9

    past performance?

    revere   s won 90% of races with clear weather

    hypothetical performance?

    if he ran the race in many parallel universes    

    subjective strength of belief?

    would pay up to 90 cents for chance to win $1

    output of some computable formula?

    ok, but then which formulas should we trust?

p(y | x) versus q(y | x)

600.465     intro to nlp     j. eisner

6

p is a function on sets of    outcomes   

p(win | clear)     p(win, clear) / p(clear)

weather   s
clear

paul revere

wins

all outcomes (races)
600.465     intro to nlp     j. eisner

7

p is a function on sets of    outcomes   

p(win | clear)     p(win, clear) / p(clear)

syntactic sugar

logical conjunction

predicate selecting

of predicates

races where
weather   s clear

weather   s
clear

paul revere

wins

all outcomes (races)
600.465     intro to nlp     j. eisner

p measures total
id203 of a
set of outcomes
(an    event   ).

8

most of the
required properties of p (axioms)

    p(   ) = 0          p(all outcomes) = 1
    p(x)     p(y) for any x     y
    p(x) + p(y) = p(x     y) provided x     y=   

e.g., p(win & clear) + p(win &    clear) = p(win)

weather   s
clear

paul revere

wins

all outcomes (races)
600.465     intro to nlp     j. eisner

p measures total
id203 of a
set of outcomes
(an    event   ).

9

commas denote conjunction

p(paul revere wins, valentine places, epitaph

shows | weather   s clear)
what happens as we add conjuncts to left of bar ?

    id203 can only decrease
    numerator of historical estimate likely to go to zero:
# times revere wins and val places    and weather   s clear

# times weather   s clear

600.465     intro to nlp     j. eisner

10

commas denote conjunction

p(paul revere wins, valentine places, epitaph

shows | weather   s clear)

p(paul revere wins | weather   s clear, ground is
dry, jockey getting over sprain, epitaph also in race, epitaph
was recently bought by gonzalez, race is on may 17,     )
what happens as we add conjuncts to right of bar ?

    id203 could increase or decrease
    id203 gets more relevant to our case (less bias)
    id203 estimate gets less reliable (more variance)
# times revere wins and weather clear and     it   s may 17

# times weather clear and     it   s may 17

600.465     intro to nlp     j. eisner

11

simplifying right side: backing off

p(paul revere wins | weather   s clear, ground is
dry, jockey getting over sprain, epitaph also in race, epitaph
was recently bought by gonzalez, race is on may 17,     )

not exactly what we want but at least we can get a

reasonable estimate of it!

(i.e., more bias but less variance)
try to keep the conditions that we suspect will have the

most influence on whether paul revere wins

600.465     intro to nlp     j. eisner

12

simplifying left side: backing off

p(paul revere wins, valentine places, epitaph

shows | weather   s clear)

not allowed!
but we can do something similar to help    

600.465     intro to nlp     j. eisner

13

factoring left side: the chain rule

p(revere, valentine, epitaph | weather   s clear)
= p(revere | valentine, epitaph, weather   s clear)
* p(valentine | epitaph, weather   s clear)
* p(epitaph | weather   s clear)

rvew/w
= rvew/vew

* vew/ew
* ew/w

true because numerators cancel against denominators
makes perfect sense when read from bottom to top

epitaph?

valentine?

valentine?

revere?

revere?

revere?

revere?

epitaph, valentine, revere? 1/3 * 1/5 * 1/4
600.465     intro to nlp     j. eisner

14

factoring left side: the chain rule

p(revere, valentine, epitaph | weather   s clear)
= p(revere | valentine, epitaph, weather   s clear)
* p(valentine | epitaph, weather   s clear)
* p(epitaph | weather   s clear)

rvew/w
= rvew/vew

* vew/ew
* ew/w

true because numerators cancel against denominators
makes perfect sense when read from bottom to top
moves material to right of bar so it can be ignored

if this prob is unchanged by backoff, we say revere was
conditionally independent of valentine and epitaph
(conditioned on the weather   s being clear). often we just
assume conditional independence to get the nice product above.

600.465     intro to nlp     j. eisner

15

factoring left side: the chain rule

p(revere | valentine, epitaph, weather   s clear)

conditional independence lets us use backed-off data from
all four of these cases to estimate their shared probabilities

if this prob is unchanged by backoff, we say revere was
conditionally independent of valentine and epitaph
(conditioned on the weather   s being clear).

valentine?

epitaph?

yes

clear?

valentine?

irrelevant

revere?

revere?

revere?

revere?

no 3/4
yes 1/4
no 3/4
yes 1/4
no 3/4
yes 1/4
no 3/4
yes 1/4

600.465     intro to nlp     j. eisner

16

   courses in handicapping [i.e., estimating a horse's odds]

should be required, like composition and western
civilization, in our universities.  for sheer
complexity, there's nothing like a horse race,
excepting life itself ...  to weigh and evaluate a
vast grid of information, much of it meaningless,
and to arrive at sensible, if erroneous, conclusions,
is a skill not to be sneezed at.   

- richard russo, the risk pool (a novel)

remember language id?

       horses and lukasiewicz are on the curriculum.   

is this english or polish or what?

   
    we had some notion of using id165 models    

   
   

is it    good    (= likely) english?
is it    good    (= likely) polish?

    space of outcomes will be not races but character

sequences (x1, x2, x3,    ) where xn = eos

600.465     intro to nlp     j. eisner

18

remember language id?

    let p(x) = id203 of text x in english
    let q(x) = id203 of text x in polish
    which id203 is higher?

    (we   d also like bias toward english since it   s

more likely a priori     ignore that for now)

   horses and lukasiewicz are on the curriculum.   
p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )

600.465     intro to nlp     j. eisner

19

apply the chain rule

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
= p(x1=h)
* p(x2=o | x1=h)
* p(x3=r | x1=h, x2=o)
* p(x4=s | x1=h, x2=o, x3=r)
* p(x5=e | x1=h, x2=o, x3=r, x4=s)
* p(x6=s | x1=h, x2=o, x3=r, x4=s, x5=e)
*    

= 0

4470/ 52108

395/ 4470

5/

3/

3/

0/

395

5

3

3

counts from
brown corpus

600.465     intro to nlp     j. eisner

20

back off on right side

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    p(x1=h)
* p(x2=o | x1=h)
* p(x3=r | x1=h, x2=o)
* p(x4=s |
* p(x5=e |
* p(x6=s |
*      = 7.3e-10 *    

x4=s, x5=e)

x2=o, x3=r)

x3=r, x4=s)

5/

12/

12/

3/

4470/ 52108

395/ 4470

395

919

126

485

counts from
brown corpus

600.465     intro to nlp     j. eisner

21

change the notation

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    p(x1=h)
* p(x2=o | x1=h)
* p(xi=r | xi-2=h, xi-1=o, i=3)
* p(xi=s |
* p(xi=e |
* p(xi=s |
*     = 7.3e-10 *    

xi-2=o, xi-1=r, i=4)

xi-2=r, xi-1=s, i=5)

4470/ 52108

395/ 4470

5/

12/

12/

395

919

126

xi-2=s, xi-1=e, i=6)

3/

485

counts from
brown corpus

600.465     intro to nlp     j. eisner

22

another independence assumption

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    p(x1=h)
* p(x2=o | x1=h)
* p(xi=r | xi-2=h, xi-1=o)
* p(xi=s |
* p(xi=e |
* p(xi=s |
*     = 5.4e-7 *    

xi-2=o, xi-1=r)

xi-2=r, xi-1=s)

xi-2=s, xi-1=e)

4470/ 52108

395/ 4470

1417/ 14765

1573/ 26412

1610/ 12253

2044/ 21250

counts from
brown corpus

600.465     intro to nlp     j. eisner

23

simplify the notation

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    p(x1=h)
* p(x2=o | x1=h)
* p(r | h, o)
* p(s | o, r)
* p(e | r, s)
* p(s | s, e)
*    

4470/ 52108

395/ 4470

1417/ 14765

1573/ 26412

1610/ 12253

2044/ 21250

counts from
brown corpus

600.465     intro to nlp     j. eisner

24

simplify the notation

the parameters
of our old
trigram generator!
same assumptions
about language.

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    p(h | bos, bos)
* p(o | bos, h)
* p(r | h, o)
* p(s | o, r)
* p(e | r, s)
* p(s | s, e)
*    

values of
those
parameters,
as naively
estimated
from brown
corpus.

these basic probabilities
are used to define p(horses)

4470/ 52108

395/ 4470

1417/ 14765

1573/ 26412

1610/ 12253

2044/ 21250

counts from
brown corpus

600.465     intro to nlp     j. eisner

25

simplify the notation

the parameters
of our old
trigram generator!
same assumptions
about language.

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    t bos, bos, h
* t bos, h, o
* t h, o, r
* t o, r, s
* t r, s, e
* t s, e,s
*    

values of
those
parameters,
as naively
estimated
from brown
corpus.

this notation emphasizes that
they   re just real variables
whose value must be estimated

4470/ 52108

395/ 4470

1417/ 14765

1573/ 26412

1610/ 12253

2044/ 21250

counts from
brown corpus

600.465     intro to nlp     j. eisner

26

definition: id203 model

param
values

trigram model
(defined in terms
of parameters like
t h, o, r and t o, r, s )

definition

of p

generate
random
text

find event
probabilities

600.465     intro to nlp     j. eisner

27

english vs. polish

english
param
values

polish
param
values

trigram model
(defined in terms
of parameters like
t h, o, r and t o, r, s )

definition

of p

definition

of q

600.465     intro to nlp     j. eisner

compute

q(x)

compute

p(x)

28

what is    x    in p(x)?

    element (or subset) of some implicit    outcome space   

    e.g., race
    e.g., sentence

    what if outcome is a whole text?

    p(text)

= p(sentence 1, sentence 2,    )
= p(sentence 1)
* p(sentence 2 | sentence 1)
*    

600.465     intro to nlp     j. eisner

definition

of p

definition

of q

compute

q(x)

compute

p(x)

29

what is    x    in    p(x)   ?
    element (or subset) of some implicit    outcome space   

    e.g., race, sentence, text    

    suppose an outcome is a sequence of letters:

p(horses)

    but we rewrote p(horses)  as

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    p(x1=h) * p(x2=o | x1=h) *    

    what does this variable=value notation mean?

600.465     intro to nlp     j. eisner

30

random variables:
what is    variable    in    p(variable=value)   ?

answer: variable is really a function of outcome

    p(x1=h) * p(x2=o | x1=h) *    
    outcome is a sequence of letters
    x2 is the second letter in the sequence

    p(number of heads=2) or just p(h=2) or p(2)

    outcome is a sequence of 3 coin flips
    h is the number of heads

    p(weather   s clear=true) or just p(weather   s clear)

    outcome is a race
    weather   s clear is true or false

600.465     intro to nlp     j. eisner

31

random variables:
what is    variable    in    p(variable=value)   ?

answer: variable is really a function of outcome

    p(x1=h) * p(x2=o | x1=h) *    
    outcome is a sequence of letters
    x2(outcome) is the second letter in the sequence

    p(number of heads=2) or just p(h=2) or p(2)

    outcome is a sequence of 3 coin flips
    h(outcome) is the number of heads

    p(weather   s clear=true) or just p(weather   s clear)

    outcome is a race
    weather   s clear (outcome) is true or false

600.465     intro to nlp     j. eisner

32

random variables:
what is    variable    in    p(variable=value)   ?

    p(number of heads=2) or just p(h=2)
    outcome is a sequence of 3 coin flips
    h is the number of heads in the outcome

    so p(h=2)

= p(h(outcome)=2) picks out set of outcomes w/2 heads
= p({hht,hth,thh})
= p(hht)+p(hth)+p(thh)

ttt tth htt hth

600.465     intro to nlp     j. eisner

tht thh hht hhh

all outcomes

33

random variables:
what is    variable    in    p(variable=value)   ?

    p(weather   s clear)

    outcome is a race
    weather   s clear is true or false of the outcome

    so p(weather   s clear)

= p(weather   s clear(outcome)=true)

picks out the set of outcomes
with clear weather

weather   s
clear

p(win | clear)     p(win, clear) / p(clear)

600.465     intro to nlp     j. eisner

paul revere

wins

all outcomes (races)

34

random variables:
what is    variable    in    p(variable=value)   ?

    p(x1=h) * p(x2=o | x1=h) *    
    outcome is a sequence of letters
    x2 is the second letter in the sequence
    so p(x2=o)

= p(x2(outcome)=o) picks out set of outcomes with    
=     p(outcome) over all outcomes whose second
letter    
= p(horses) + p(boffo) + p(xoyzkklp) +    

600.465     intro to nlp     j. eisner

35

back to trigram model of p(horses)

the parameters
of our old
trigram generator!
same assumptions
about language.

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )
    t bos, bos, h
* t bos, h, o
* t h, o, r
* t o, r, s
* t r, s, e
* t s, e,s
*    

values of
those
parameters,
as naively
estimated
from brown
corpus.

this notation emphasizes that
they   re just real variables
whose value must be estimated

4470/ 52108

395/ 4470

1417/ 14765

1573/ 26412

1610/ 12253

2044/ 21250

counts from
brown corpus

600.465     intro to nlp     j. eisner

36

a different model

    exploit fact that horses is a common word

p(w1 = horses)

where word vector w is a function of the outcome (the

sentence) just as character vector x is.

= p(wi = horses | i=1)
    p(wi = horses) = 7.2e-5

independence assumption says that sentence-initial words w1
are just like all other words wi (gives us more data to use)

much larger than previous estimate of 5.4e-7     why?
advantages, disadvantages?
600.465     intro to nlp     j. eisner

37

improving the new model:
weaken the indep. assumption

    don   t totally cross off  i=1 since it   s not irrelevant:
    yes, horses is common, but less so at start of sentence

since most sentences start with determiners.

p(w1 = horses)   =    t p(w1=horses, t1 = t)
=    t p(w1=horses|t1 = t) * p(t1 = t)
=    t p(wi=horses|ti = t, i=1) * p(t1 = t)
       t p(wi=horses|ti = t) * p(t1 = t)
=     p(wi=horses|ti = plnoun) * p(t1 = plnoun)

+ p(wi=horses|ti = verb) * p(t1 = verb) +    

= (72 / 55912) * (977 / 52108) + (0 / 15258) * (146 / 52108) +    
= 2.4e-5 + 0 +     + 0

= 2.4e-5

600.465     intro to nlp     j. eisner

38

which model is better?

    model 1     predict each letter xi from

previous 2 letters xi-2, xi-1

    model 2     predict each word wi by its part

of speech ti, having predicted ti from i

    models make different independence

assumptions that reflect different intuitions

    which intuition is better???

600.465     intro to nlp     j. eisner

39

measure performance!

    which model does better on language id?

    administer test where you know the right answers
    seal up test data until the test happens

    simulates real-world conditions where new data comes along that

you didn   t have access to when choosing or training model

    in practice, split off a test set as soon as you obtain the

data, and never look at it

    need enough test data to get statistical significance
    report all results on test data

    for a different task (e.g., speech transcription instead
of language id), use that task to evaluate the models

600.465     intro to nlp     j. eisner

40

cross-id178 (   xent   )

    another common measure of model quality

    task-independent
    continuous     so slight improvements show up here
even if they don   t change # of right answers on task

    just measure id203 of (enough) test data
    higher prob means model better predicts the future
    there   s a limit to how well you can predict random stuff
    limit depends on    how random    the dataset is (easier to

predict weather than headlines, especially in arizona)

600.465     intro to nlp     j. eisner

41

cross-id178 (   xent   )

    want prob of test data to be high:

p(h | bos, bos) * p(o | bos, h) * p(r | h, o) * p(s | o, r)    
1/8              *          1/8          *    1/8         *     1/16         

average?
geometric average
of  1/23,1/23, 1/23, 1/24
= 1/23.25     1/9.5

    high prob     low xent by 3 cosmetic improvements:

    take logarithm (base 2) to prevent underflow:

log (1/8 * 1/8 * 1/8 * 1/16    )
= log 1/8 + log 1/8 + log 1/8 + log 1/16     = (-3) + (-3) + (-3) + (-4) +    
3+3+3+4+   

    negate to get a positive value in bits
    divide by length of text     3.25 bits per letter (or per word)

    want this to be small (equivalent to wanting good compression!)
    lower limit is called id178     obtained in principle as cross-id178

of the true model measured on an infinite amount of data

    or use perplexity = 2 to the xent (   9.5 choices instead of 3.25 bits)
600.465     intro to nlp     j. eisner

42

