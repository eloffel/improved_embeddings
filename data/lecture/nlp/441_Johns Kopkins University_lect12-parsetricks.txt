parsing tricks

left-corner parsing

    technique for 1 word of lookahead in 

algorithms like earley   s

    (can also do multi-word lookahead but it   s 

harder)

600.465 - intro to nlp - j. eisner

1

600.465 - intro to nlp - j. eisner

2

basic earley   s algorithm
0        papa      1

attach

0 np papa .
0 s np . vp
0 np np . pp

0 root . s
0 s . np vp
0 np . det n
0 np . np pp
0 np . papa
0 det . the
0 det . a

predict

0        papa      1

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
0 np . papa
1 vp . vp pp
0 det . the
0 det . a

0        papa      1

0 root . s
0 np papa .
0 s . np vp
0 s np . vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
1 vp . vp pp
0 np . papa
0 det . the
1 pp . p np
0 det . a

predict

0        papa      1

0 root . s
0 np papa .
0 s . np vp
0 s np . vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
1 vp . vp pp
0 np . papa
1 pp . p np
0 det . the
1 v . ate
0 det . a
1 v . drank
1 v . snorted

predict

    .v makes us add all the verbs in the 

vocabulary! 

    slow     we   d like a shortcut.

1

0        papa      1

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
1 vp . vp pp
0 np . papa
1 pp . p np
0 det . the
0 det . a
1 v . ate
1 v . drank
1 v . snorted

predict
    every .vp adds all vp         rules again.
    before adding a rule, check it   s not a 

duplicate.

    slow if there are > 700 vp         rules, 

so what will you do in homework 4?

0        papa      1 

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
1 vp . vp pp
0 np . papa
1 pp . p np
0 det . the
0 det . a
1 v . ate
1 v . drank
1 v . snorted
1 p . with

predict
    .p makes us add all the prepositions    

1-word lookahead would help
0        papa      1 

ate

1-word lookahead would help
0        papa      1 

ate

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
0 np . papa
1 vp . vp pp
1 pp . p np
0 det . the
1 v . ate
0 det . a
1 v . drank
1 v . snorted
1 p . with

no point in adding words other than ate

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
0 np . papa
1 vp . vp pp
1 pp . p np
0 det . the
1 v . ate
0 det . a
1 v . drank
1 v . snorted
1 p . with

in fact, no point in adding any constituent 

that can   t start with ate

don   t bother adding pp, p, etc.

no point in adding words other than ate

with left-corner filter
0        papa      1

ate

0 root . s
0 s . np vp
0 np . det n
0 np . np pp
0 np . papa
0 det . the
0 det . a

0 np papa .
0 s np . vp
0 np np . pp

attach

pp can   t start with ate

birth control     now we won   t predict

1 pp . p np
1 p . with

either!

predict

0        papa      1

ate

0 root . s
0 np papa .
0 s . np vp
0 s np . vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
0 np . papa
1 vp . vp pp
0 det . the
0 det . a

need to know that ate can   t start pp
take closure of all categories that it 

does start    

2

0        papa      1

ate

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
1 vp . vp pp
0 np . papa
1 v . ate
0 det . the
0 det . a
1 v . drank
1 v . snorted

predict

0        papa      1

ate

0 np papa .
0 root . s
0 s np . vp
0 s . np vp
0 np . det n
0 np np . pp
0 np . np pp 1 vp . v np
1 vp . vp pp
0 np . papa
1 v . ate
0 det . the
0 det . a
1 v . drank
1 v . snorted

predict

merging right-hand sides

merging right-hand sides

    grammar might have rules

x     a g h p
x     b g h p

    could end up with both of these in chart:

(2, x     a . g h p) in column 5
(2, x     b . g h p) in column 5

    but these are now interchangeable: if one 

produces x then so will the other

    to avoid this redundancy, can always use 

dotted rules of this form:  x     ... g h p

    similarly, grammar might have rules

x     a g h p
x     a g h q

    could end up with both of these in chart:

(2, x     a . g h p) in column 5
(2, x     a . g h q) in column 5

    not interchangeable, but we   ll be processing 

them in parallel for a while    

    solution: write grammar as x     a g h (p|q)

600.465 - intro to nlp - j. eisner

15

600.465 - intro to nlp - j. eisner

16

merging right-hand sides

merging right-hand sides

    combining the two previous cases:

x     a g h p
x     a g h q
x     b g h p
x     b g h q

becomes

x     (a | b) g h (p | q)

    and often nice to write stuff like

np     (det |    ) adj* n

x     (a | b) g h (p | q)
np     (det |    ) adj* n

    these are id157!
    build their minimal dfas:

a

p

x     

g

h

b

q

    automaton states 

replace dotted 
rules (x     a g . h p)

np     

adj

n

det

adj

n

600.465 - intro to nlp - j. eisner

17

600.465 - intro to nlp - j. eisner

18

3

merging right-hand sides
indeed, allnp     rules can be unioned into a single dfa!

merging right-hand sides
indeed, allnp     rules can be unioned into a single dfa!

np     adjp adjp jj jj nn nns
np     adjp dt nn
np     adjp jj nn
np     adjp jj nn nns
np     adjp jj nns
np     adjp nn
np     adjp nn nn
np     adjp nn nns
np     adjp nns
np     adjp npr
np     adjp nprs
np     dt
np     dt adjp
np     dt adjp , jj nn
np     dt adjp adjp nn
np     dt adjp jj jj nn
np     dt adjp jj nn
np     dt adjp jj nn nn

etc.

600.465 - intro to nlp - j. eisner

19

regular

expression

np     adjp adjp jj jj nn nns

| adjp dt nn
| adjp jj nn
| adjp jj nn nns
| adjp jj nns
| adjp nn
| adjp nn nn
| adjp nn nns
| adjp nns
| adjp npr
| adjp nprs
| dt
| dt adjp
| dt adjp , jj nn
| dt adjp adjp nn
| dt adjp jj jj nn
| dt adjp jj nn
| dt adjp jj nn nn
etc.

600.465 - intro to nlp - j. eisner

np     
dfa

adjp     

adjp
dt

np

adj
p

adjp

20

earley   s algorithm on dfas
    what does earley   s algorithm now look like?

earley   s algorithm on dfas
    what does earley   s algorithm now look like?

vp        

pp

   

np

column 4

   

(2,   )

predict

np     

det

adj

adj

pp

n

n

pp

   

np
   

vp        

pp    

column 4

predict

   

(2,   )
(4,   )
(4,   )

600.465 - intro to nlp - j. eisner

21

600.465 - intro to nlp - j. eisner

22

earley   s algorithm on dfas
    what does earley   s algorithm now look like?

vp        

pp    

pp

   

np
   

np     

det

adj

adj

pp

n

n

earley   s algorithm on dfas
    what does earley   s algorithm now look like?

vp        

pp    

pp

   

np
   

np     

det

adj

adj

pp

n

n

column 4 column 5

   

column 7

column 4 column 5

   

column 7

   

(2,   )
(4,   )
(4,   )

   

(4,    )

(4,   )

predict 
or attach?

   

(2,   )
(4,   )
(4,   )

   

(4,    )

600.465 - intro to nlp - j. eisner

23

600.465 - intro to nlp - j. eisner

predict 
or attach?

(4,   )
(7,   ) 
(2,   )

both!

24

4

pruning and prioritization

    heuristically throw away constituents that 

probably won   t make it into best complete parse.  

    use probabilities to decide which ones.  

    so probs are useful for speed as well as accuracy!

    both safe and unsafe methods exist

    iterative deepening: throw x away if p(x) < 10-200
(and lower this threshold if we don   t get a parse)

    heuristic pruning: throw x away if p(x) < 0.01 * p(y) 
for some y that spans the same set of words (for example)

    prioritization: if p(x) is low, don   t throw x away; just 
postpone using it until you need it (hopefully you won   t).

prioritization continued:
agenda-based parsing

    prioritization: if p(x) is low, don   t throw x away; 

just postpone using it until you need it.
    in other words, explore best options first.
    should get some good parses early on; then stop!

time 1  flies 2    like 3  an 4 arrow 5
0 np 3
vst 3

np 10
s
8
np 4
vp 4

np 24
s
22
np 18
s
21
vp 18
pp 12
vp 16
det  1 np 10
n 8

p 2
v 5

1

2

3
4

600.465 - intro to nlp - j. eisner

25

600.465 - intro to nlp - j. eisner

prioritization continued:
agenda-based parsing
    until we pop a parse 0s5 or fail with empty agenda

    pop top element iyj from agenda into chart
    for each right neighbor jzk

    for each rule x     y z in grammar

    for each left neighbor hzi
    for each rule x     z y

prioritization continued:
agenda-based parsing
    until we pop a parse 0s5 or fail with empty agenda

    pop top element iyj from agenda into chart
    for each right neighbor jzk

    for each rule x     y z in grammar

    for each left neighbor hzi
    for each rule x     z y

    put ixk onto the agenda
chart of good constituents
time 1  flies 2    like 3  an 4 arrow 5
0 np 3

vst 3 s

8
np 4
vp 4

1

2

3
4

p 2
v 5

det  1

n 8

    put hxj onto the agenda

prioritized agenda of
pending constituents
(ordered by p(x), say)

3np5 10

0np2 10

    put ixk onto the agenda
chart of good constituents
time 1  flies 2    like 3  an 4 arrow 5
0 np 3

vst 3 s

8
np 4
vp 4

1

2

3
4

p 2
v 5

det  1 np 10
n 8

600.465 - intro to nlp - j. eisner

27

600.465 - intro to nlp - j. eisner

    put hxj onto the agenda

prioritized agenda of
pending constituents
(ordered by p(x), say)

0np2 10

2vp5 16

28

prioritization continued:
agenda-based parsing
    until we pop a parse 0s5 or fail with empty agenda

    pop top element iyj from agenda into chart
    for each right neighbor jzk

    for each rule x     y z in grammar

    for each left neighbor hzi
    for each rule x     z y

    put ixk onto the agenda
chart of good constituents
time 1  flies 2    like 3  an 4 arrow 5
0 np 3

vst 3 s

8
np 4
vp 4

1

2

3
4

p 2
v 5

det  1 np 10
n 8

600.465 - intro to nlp - j. eisner

    put hxj onto the agenda

prioritized agenda of
pending constituents
(ordered by p(x), say)

0np2 10

2pp5 12

2vp5 16

29

always finds best parse!
analogous to dijkstra   s 
shortest-path algorithm

outside estimates
for better pruning and prioritization

    iterative deepening: throw x away if p(x)*q(x) < 10-200

(lower this threshold if we don   t get a parse)

    heuristic pruning: throw x away if p(x)*q(x) < 0.01*p(y)*q(y) 

for some y that spans the same set of words 

    prioritized agenda: priority of x on agenda is p(x)*q(x); stop at first parse

    in general, the    inside prob    p(x) will be higher for smaller constituents

    not many rule probabilities inside them

    the    outside prob    q(x) is intended to correct for this 

    estimates the prob of all the rest of the rules needed to build x into full parse
    so p(x)*q(x) estimates prob of the best parse that contains x
    if we take q(x) to be the best estimate we can get

    methods may no longer be safe (but may be fast!)
    prioritized agenda is then called a    best-first algorithm   

    but if we take q(x)=1, that   s just the methods from previous slides

    and iterative deepening and prioritization were safe there

    if we take q(x) to be an    optimistic estimate    (always     true prob)
    still safe!  prioritized agenda is then an example of an    a* algorithm   

600.465 - intro to nlp - j. eisner

30

5

outside estimates
for better pruning and prioritization

preprocessing

    iterative deepening: throw x away if p(x)*q(x) < 10-200

(lower this threshold if we don   t get a parse)

    heuristic pruning: throw x away if p(x)*q(x) < 0.01*p(y)*q(y) 

for some y that spans the same set of words 

    prioritized agenda: priority of x on agenda is p(x)*q(x); stop at first parse

    in general, the    inside prob    p(x) will be higher for smaller constituents

    not many rule probabilities inside them

    the    outside prob    q(x) is intended to correct for this 

    estimates the prob of all the rest of the rules needed to build x into full parse
    so p(x)*q(x) estimates prob of the best parse that contains x
    if we take q(x) to be the best estimate we can get

    methods may no longer be safe (but may be fast!)
    prioritized agenda is then called a    best-first algorithm   

terminology warning: here    inside    and    outside    mean
id203 of the best partial parse inside or outside x

    but if we take q(x)=1, that   s just the methods from previous slides

    and iterative deepening and prioritization were safe there
but traditionally, they mean total prob of all such partial parses
    if we take q(x) to be an    optimistic estimate    (always     true prob)
(as in the    inside algorithm    that we saw in the previous lecture)
    still safe!  prioritized agenda is then an example of an    a* algorithm   

600.465 - intro to nlp - j. eisner

31

    first    tag    the input with parts of speech:

    guess the correct preterminal for each word, using 

faster methods we   ll learn later

    now only allow one part of speech per word
    this eliminates a lot of crazy constituents!
    but if you tagged wrong you could be hosed

    raise the stakes: 

    what if tag says not just    verb    but    transitive verb   ?  

or    verb with a direct object and 2 pps attached   ?   
(   id55   )

    safer to allow a few possible tags per word, not 

just one    
600.465 - intro to nlp - j. eisner

32

center-embedding

if x
then 
if y
then 
if a
then b
endif

else b
endif

else b
endif

statement     if expr then 
statement endif

statement     if expr then 
statement else statement endif

but not:
statement     if expr then 

statement

center-embedding

    this is the rat that ate the malt.
    this is the malt that the rat ate.

    this is the cat that bit the rat that ate the malt.
    this is the malt that the rat that the cat bit ate.

    this is the dog that chased the cat that bit the 

rat that ate the malt.

    this is the malt that [the rat that [the cat that 

[the dog chased] bit] ate].

600.465 - intro to nlp - j. eisner

33

600.465 - intro to nlp - j. eisner

34

more center-embedding

center recursion vs. tail recursion

[what did you disguise 
[those handshakes that 

you greeted 
[the people we bought 

[the bench 

[billy was read to] 

on] 

with] 

with] 

for]?

[which mantelpiece did you 

put 
[the idol i sacrificed 
[the fellow we sold 

[the bridge you threw 

[the bench 

[billy was read to]

on] 

off] 

to] 

to] 

on]?

[what did you disguise 
[those handshakes that 

you greeted 
[the people we bought 

[for what did you disguise 

[those handshakes with which 

you greeted 
[the people with which we bought 

[the bench 

[billy was read to] 

[the bench on which 

[billy was read to]?

on] 

with] 

with] 

for]?

   pied piping       

np moves leftward,

preposition follows along

600.465 - intro to nlp - j. eisner

35

600.465 - intro to nlp - j. eisner

36

6

disallow center-embedding?

parsing algs for non-id18

    center-embedding seems to be in the grammar, but 

people have trouble processing more than 1 level of it.

    you can limit # levels of center-embedding via features:  

e.g., s[s_depth=n+1]     a s[s_depth=n] b

    if you   re going to make up a new kind of 

grammar, you should also describe how to 
parse it.

    if a id18 limits # levels of embedding, then it can be 

compiled into a finite-state machine     we don   t need a 
stack at all!
    finite-state recognizers run in linear time. 
    however, it   s tricky to turn them into parsers for the original 

id18 from which the recognizer was compiled.

    and compiling a small grammar into a much larger fsa may 
be a net loss     structure sharing in the parse chart is expanded 
out to duplicate structure all over the fsa.

    such algorithms exist, e.g., 

    for tag (where the grammar specifies not just rules but larger 

tree fragments, which can be combined by    substitution    and 
   adjunction    operations)

    for id35 (where the grammar only specifies preterminal rules, 

and there are generic operations to combine slashed 
nonterminals like x/y or (x/z)/(y\w))

600.465 - intro to nlp - j. eisner

37

600.465 - intro to nlp - j. eisner

38

7

