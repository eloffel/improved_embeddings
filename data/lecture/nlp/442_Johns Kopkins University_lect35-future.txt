current & future  
nlp research 

a few random remarks 

600.465 - intro to nlp - j. eisner 

1 

computational linguistics 

(cid:1)(cid:1) we can study anything about language ... 

(cid:1)(cid:1) 1. formalize some insights 
(cid:1)(cid:1) 2. study the formalism mathematically 
(cid:1)(cid:1) 3. develop & implement algorithms 
(cid:1)(cid:1) 4. test on real data 

600.465 - intro to nlp - j. eisner 

2 

reprise from lecture 1: 
what   s hard about this story? 

john stopped at the donut store on his way home from 

work.  he thought a coffee was good every few 
hours.  but it turned out to be too expensive there. 

(cid:1)(cid:1) these ambiguities now look familiar 
(cid:1)(cid:1) you now know how to solve some (e.g., conditional id148): 

(cid:1)(cid:1) pp attachment 
(cid:1)(cid:1) coreference resolution (which np does    it    refer to?) 
(cid:1)(cid:1) id51 

(cid:1)(cid:1) hardest part: how many senses?  what are they? 

(cid:1)(cid:1) others still seem beyond the state of the art (except in limited settings): 

(cid:1)(cid:1) anything that requires much semantics or reasoning 

(cid:1)(cid:1) quantifier scope 
(cid:1)(cid:1) reasoning about john   s beliefs and actions 
(cid:1)(cid:1)    deep    meaning of words and relations 

600.465 - intro to nlp - j. eisner 

3 

examples mostly from terry winograd in the 1970   s, 
via doug lenat 

deep nlp requires world knowledge 

(cid:1)(cid:1) the pen is in the box. 
the box is in the pen. 

(cid:1)(cid:1) the police watched the demonstrators because they feared violence. 

the police watched the demonstrators because they advocated violence. 

(cid:1)(cid:1) mary and sue are sisters. 

mary and sue are mothers. 

(cid:1)(cid:1) every american has a mother. 

every american has a president. 

(cid:1)(cid:1)

john saw his brother skiing on tv.  the fool 
    didn   t have a coat on! 
    didn   t recognize him! 

(cid:1)(cid:1) george burns: my aunt is in the hospital.   

 

  i went to see her today, and took her flowers. 

gracie allen: george, that   s terrible!   

600.465 - intro to nlp - j. eisner 

4 

big questions of cl 

(cid:1)(cid:1) what formalisms can encode various kinds of linguistic knowledge? 

(cid:1)(cid:1) discrete knowledge: what is possible? 
(cid:1)(cid:1) continuous knowledge: what is likely? 
(cid:1)(cid:1) what kind of p(   ) to use (e.g., a pid18)? 
(cid:1)(cid:1) what is the prior over the structure (set of rules) and parameters (rule weights)? 
(cid:1)(cid:1) how to combine different kinds of knowledge, including world knowledge? 

(cid:1)(cid:1) how can we compute efficiently within these formalisms? 

(cid:1)(cid:1) or find approximations that work pretty well? 
(cid:1)(cid:1) problem 1: prediction in a given model. problem 2: learning the model. 

(cid:1)(cid:1) how should we learn within a given formalism? 

(cid:1)(cid:1) hard with unsupervised, semi-supervised, heterogeneous data     
(cid:1)(cid:1) maximize p(data | (cid:1)) (cid:2) pprior(theta)?   
(cid:1)(cid:1) pick (cid:1) to directly minimize error rate of our predictions? 
(cid:1)(cid:1) online methods?  (adapt (cid:1) gradually in response to data, then forget) 
(cid:1)(cid:1) don   t pick a single (cid:1) at all, but consider all values even at test time?  
(cid:1)(cid:1) learn just the feature weights (cid:1), or also which features to have? 
(cid:1)(cid:1) what if the formalism is wrong, so no (cid:1) works well?  

600.465 - intro to nlp - j. eisner 

5 

some of the active research 

(cid:1)(cid:1) syntax:  

(cid:1)(cid:1) non-local features for scoring parses; discriminative models 
(cid:1)(cid:1) efficient approximate parsing (e.g., coarse to fine) 
(cid:1)(cid:1) unsupervised or partially supervised learning  

(learn a theory more detailed than one   s treebank) 

(cid:1)(cid:1) other formalisms besides id18 (dependency grammar, id35,    ) 
(cid:1)(cid:1) using syntax in applied nlp tasks 

(cid:1)(cid:1) machine translation: 

(cid:1)(cid:1) best-funded area of nlp, right now 
(cid:1)(cid:1) models and algorithms 
(cid:1)(cid:1) how to incorporate syntactic structure? 
(cid:1)(cid:1)    low-resource    and morphologically complex languages? 

600.465 - intro to nlp - j. eisner 

6 

some of the active research 

(cid:1)(cid:1) semantic tasks (how would you reduce these to prediction problems?) 

(cid:1)(cid:1) id31 
(cid:1)(cid:1) summarization 
(cid:1)(cid:1) information extraction, slot-filling 
(cid:1)(cid:1) discourse analysis 
(cid:1)(cid:1) id123 

(cid:1)(cid:1) speech:  

(cid:1)(cid:1) better id38 (predict next word)     syntax, semantics 
(cid:1)(cid:1) better models of acoustics, pronunciation 

(cid:1)(cid:1) fewer speaker-specific parameters  

(cid:1)(cid:1) to enable rapid adaptation to new speakers 

(cid:1)(cid:1) more robust recognition 

(cid:1)(cid:1) emotional speech, informal conversation, meetings 
(cid:1)(cid:1) juvenile/elderly voices, bad audio, background noise 

(cid:1)(cid:1) some techniques to solve these: 

(cid:1)(cid:1) non-local features 
(cid:1)(cid:1) physiologically informed models 
(cid:1)(cid:1) id84 

600.465 - intro to nlp - j. eisner 

7 

some of the active research 

(cid:1)(cid:1) all of these areas have learning problems 

attached. 

(cid:1)(cid:1) we   re really interested in unsupervised learning. 

(cid:1)(cid:1) how to learn fsts and their probabilities? 
(cid:1)(cid:1) how to learn id18s?  deep structure? 
(cid:1)(cid:1) how to learn good  word classes? 
(cid:1)(cid:1) how to learn translation models? 

600.465 - intro to nlp - j. eisner 

8 

semantics still tough  

(cid:1)(cid:1)    the perilously underestimated appeal of 
ross perot has been quietly going up this 
time.    

(cid:1)(cid:1) underestimated by whom? 
(cid:1)(cid:1) perilous to whom, according to whom? 
(cid:1)(cid:1)    quiet    = unnoticed; by whom? 
(cid:1)(cid:1)    appeal of perot     (cid:1)    perot appeals        

(cid:1)(cid:1) a court decision? 
(cid:1)(cid:1) to someone/something?  (actively or passively?) 

(cid:1)(cid:1)    the    appeal 
(cid:1)(cid:1)    go up    as idiom; and refers to amount of subject 
(cid:1)(cid:1)    this time    : meaning?  implied contrast? 

600.465 - intro to nlp - j. eisner 

9 

deploying nlp 

(cid:1)(cid:1) id103 and ir have finally gone commercial. 
(cid:1)(cid:1) and there is a ton of text and speech on the internet, cellphones, etc. 
(cid:1)(cid:1) but not much nlp is out in the real world. 
(cid:1)(cid:1) what killer apps should we be working toward? 

(cid:1)(cid:1) resources (see linguistic data consortium, lrec conference) 

(cid:1)(cid:1) treebanks (parsed corpora) 
(cid:1)(cid:1) other corpora, sometimes annotated 

(cid:1)(cid:1) corpora mailing list 
(cid:1)(cid:1) mechanical turk, annotation games 

(cid:1)(cid:1) id138; morphologies; maybe a few grammars 
(cid:1)(cid:1) research tools: 

(cid:1)(cid:1) published systems (write to the authors & ask for the code!) 
(cid:1)(cid:1) toolkits: finite-state, machine learning, machine translation, info extraction 
(cid:1)(cid:1) dyna     a new programming language being built at jhu 
(cid:1)(cid:1) annotation tools  
(cid:1)(cid:1) emerging standards like voicexml 

(cid:1)(cid:1) still out of the reach of j. random programmer 

600.465 - intro to nlp - j. eisner 

10 

deploying nlp 
(cid:1)(cid:1) sneaking nlp in through the back door: 

(cid:1)(cid:1) add features to existing interfaces 

(cid:1)(cid:1)    click to translate    
(cid:1)(cid:1) spell correction of queries 
(cid:1)(cid:1) allow multiple types of queries (phone number lookup, etc.) 
(cid:1)(cid:1) ir should return document clusters and summaries 
(cid:1)(cid:1) from ir to qa (id53) 
(cid:1)(cid:1) machines gradually replace humans @ phone/email helpdesks 

(cid:1)(cid:1) back-end processing 

(cid:1)(cid:1) information extraction and id172 to build databases:  

cd now, new york times,     

(cid:1)(cid:1) assemble good text from boilerplate 

(cid:1)(cid:1) hand-held devices 

(cid:1)(cid:1) translator 
(cid:1)(cid:1) personal conversation recorder, with topical search 

600.465 - intro to nlp - j. eisner 

11 

ie for the masses? 

   in most presidential elections, al gore   s detour to california 
today would be a sure sign of a campaign in trouble.  california is 
solid democratic territory, but a slip in the polls sent gore rushing 
back to the coast.    

600.465 - intro to nlp - j. eisner 

12 

ie for the masses? 

   in most presidential elections, al gore   s detour to california 
today would be a sure sign of a campaign in trouble.  california is 
solid democratic territory, but a slip in the polls sent gore rushing 
back to the coast.    

   al gore    

ag 

pll 

   polls    

location 

   territory    

ca 

   democratic    

   california    

   coast    

600.465 - intro to nlp - j. eisner 

13 

ie for the masses? 

(cid:1)(cid:1)    where did al gore go?    
(cid:1)(cid:1)    what are some democratic locations?    
(cid:1)(cid:1)    how have different polls moved in october?    

   al gore    

ag 

pll 

   polls    

location 

   territory    

ca 

   democratic    

   california    

   coast    

600.465 - intro to nlp - j. eisner 

14 

ie for the masses? 

(cid:1)(cid:1) allow queries over meanings, not sentences 
(cid:1)(cid:1) big semantic network extracted from the web 
(cid:1)(cid:1) simple entities and relationships among them 
(cid:1)(cid:1) not complete, but linked to original text 
(cid:1)(cid:1) allow inexact queries  

(cid:1)(cid:1) learn generalizations from a few tagged examples 
(cid:1)(cid:1) redundant; collapse for browsability or space 

600.465 - intro to nlp - j. eisner 

15 

dialogue systems 

(cid:1)(cid:1) games 
(cid:1)(cid:1) command-and-control applications 
(cid:1)(cid:1)    practical dialogue    (computer as assistant) 
(cid:1)(cid:1) the turing test 

600.465 - intro to nlp - j. eisner 

16 

turing test 

 q: please write me a sonnet on the subject of the forth 

bridge. 

a [either a human or a computer]: count me out on this 

one.  i never could write poetry. 

q: add 34957 to 70764. 
a:  (pause  about  30  seconds  and  then  give  an  answer)  

105621. 

q: do you play chess? 
a: yes. 
q: i have my k at my k1, and no other pieces.  you have 
only k at k6 and r at r1.   it is your move.   what do 
you play? 

a: (after a pause of 15 seconds) r-r8 mate.  

600.465 - intro to nlp - j. eisner 

17 

turing test 

q:  in  the  first  line  of  your  sonnet  which  reads     shall  i  compare 
thee to a summer   s day,    would not    a spring day    do as well or 
better? 

a: it wouldn   t scan. 
q: how about    a winter   s day   ?  that would scan all right. 
a: yes, but nobody wants to be compared to a winter   s day. 
q: would you say mr. pickwick reminded you of christmas? 
a: in a way. 
q:  yet  christmas  is  a  winter   s  day,  and  i  do  not  think  mr. 

pickwick would mind the comparison.   

a: i don   t think you   re serious.   by a winter   s day one means a 

typical winter   s day, rather than a special one like christmas. 

600.465 - intro to nlp - j. eisner 

18 

trips system 

600.465 - intro to nlp - j. eisner 

19 

trips system 

600.465 - intro to nlp - j. eisner 

20 

dialogue links (click!) 

(cid:1)(cid:1) turing's article (1950) 
(cid:1)(cid:1) eliza (the original chatterbot) 

(cid:1)(cid:1) weizenbaum's article (1966) 
(cid:1)(cid:1) eliza on the web - try it! 

(cid:1)(cid:1) loebner prize (1991-2001), with transcripts 
(cid:1)(cid:1) shieber:    one aspect of progress in research on nlp is appreciation 

for its complexity, which led to the dearth of entrants from the artificial 
intelligence community - the realization that time spent on winning the 
loebner prize is not time spent furthering the field.    

(cid:1)(cid:1) trips demo movies (1998) 

600.465 - intro to nlp - j. eisner 

21 

jhu   s center for language & speech processing 
(one of the biggest centers for nlp/speech research) 

electrical & 
computer 
engineering 

computer 
science 

clsp 

cognitive  
science 

(linguistics, brains) 

600.465 - intro to nlp - j. eisner 

22 

clsp vision statement 

   (cid:1) understand how human language is used  

to communicate ideas/thoughts/information. 

   (cid:1) develop technology for machine  

analysis, translation, and transformation  
of multilingual speech and text. 

600.465 - intro to nlp - j. eisner 

23 

the form of linguistic knowledge:  
mathematical formalisms for writing grammars 

paul 

smolensky 

colin 
wilson 

kyle 

rawlins 

+ others 

computer 
science 

clsp 

electrical & 
computer 
engineering 

cognitive  
science 

(linguistics, brains) 

600.465 - intro to nlp - j. eisner 

24 

recovering meaning in a noisy, ambiguous world: 

statistical modeling of speech & language    

fred 

sanjeev 

jelinek 

khudanpur 

damianos 
karakos 

clsp 

computer 
science 
mounya 
elhilali 

andreas 
andreou 

hynek 

hermansky 

electrical & 
computer 
engineering 

cognitive  
science 

(linguistics, brains) 

600.465 - intro to nlp - j. eisner 

25 

natural language processing lab: 
all of the above, plus algorithms 

david 

yarowsky 

keith 
jason 
hall 
eisner 

chris  
callison-burch 

computer 
science 

clsp 

electrical & 
computer 
engineering 

cognitive  
science 

(linguistics, brains) 

bunch of great students! 

600.465 - intro to nlp - j. eisner 

26 

human language 
center for language & speech processing 
technology center 

of excellence  
(hlt-coe) 

ken 

church 

mark 
dredze 

christine 

(+ several 

piatko 

others) 

electrical & 
computer 
engineering 

computer 
science 

clsp 

cognitive  
science 

(linguistics, brains) 

600.465 - intro to nlp - j. eisner 

27 

human language 
center for language & speech processing 
technology center 

of excellence  
(hlt-coe) 

electrical & 
computer 
engineering 

computer 
science 

clsp 

cognitive  
science 

(linguistics, brains) 

600.465 - intro to nlp - j. eisner 

28 

center for language & speech processing 

invited speakers: tuesdays 4:30 
student talks: fridays lunch 
reading groups: tu/th lunch 
summer school & workshop 
<admin@clsp.jhu.edu> 

electrical & 
computer 
engineering 

computer 
science 

clsp 

cognitive  
science 

(linguistics, brains) 

600.465 - intro to nlp - j. eisner 

29 

why language? 

y0 ? 

well, at least you can use it to make jokes with     

600.465 - intro to nlp - j. eisner 

30 

why language? 

   (cid:1) selfish reasons 

   (cid:1) really interesting data 
   (cid:1) use both sides of your brain 
   (cid:1) great problems => lifetime employment? 

   (cid:1) $elfish reason$ 

   (cid:1) space telescope:    all    cosmological data 
   (cid:1) genome:    all    biological data 
   (cid:1) online text/speech:    all    human thought and culture 

   (cid:1) suddenly pcs can see lots of speech & text      

but they can   t help you with it until they understand it! 

   (cid:1) sound fun?  600.465 natural language processing 

   (cid:1) techniques are transferable (comp bio, stocks) 

600.465 - intro to nlp - j. eisner 

31 

typical problems & solution 

(cid:1)(cid:1) map input to output: 

(cid:1)(cid:1) speech (cid:3) text 
(cid:1)(cid:1) text (cid:3) speech 
(cid:1)(cid:1) arabic (cid:3) english 
(cid:1)(cid:1) sentence (cid:3) meaning 
(cid:1)(cid:1) unedited (cid:3) edited 
(cid:1)(cid:1) document (cid:3) summary 
(cid:1)(cid:1) document (cid:3) database record 
(cid:1)(cid:1) query (cid:3) relevant documents 
(cid:1)(cid:1) question (cid:3) answer 
(cid:1)(cid:1) email (cid:3) is it spam? 

1.(cid:1) dream up a model 
of p(output | input) 

2.(cid:1) fit the model   s 
parameters from 
whatever data you 
can get 

3.(cid:1) invent an 

algorithm to 
maximize  
p(output | input) 
on new inputs 

600.465 - intro to nlp - j. eisner 

32 

one of two language-learning 
devices i recently helped build 
(this is model 1, from 2003) 

s t a t s 

2005 (fairly fluent) 

2004 (pre-babbling) 

