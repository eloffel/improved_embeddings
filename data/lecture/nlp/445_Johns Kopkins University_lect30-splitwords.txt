slide courtesy of d. yarowsky 

splitting words 

a.k.a.    word sense 
disambiguation    

600.465 - intro to nlp - j. eisner 

1 

slide courtesy of d. yarowsky 

slide courtesy of d. yarowsky 

slide courtesy of d. yarowsky 

slide courtesy of d. yarowsky 

slide courtesy of d. yarowsky 

slide courtesy of d. yarowsky 

representing word as vector 

each word type has a different vector 

(cid:1)(cid:1) could average over many occurrences of the 

word ... 

(cid:1)(cid:1) each word type has a different vector? 
(cid:1)(cid:1) each word token has a different vector? 
(cid:1)(cid:1) each word sense has a different vector? 

  (for this one, we need sense-tagged training data) 
  (is this more like a type vector or a token vector?) 

(cid:1)(cid:1) what is each of these good for? 

(cid:1)(cid:1) we saw this yesterday 
(cid:1)(cid:1) it   s good for grouping words 

(cid:1)(cid:1) similar semantics? 
(cid:1)(cid:1) similar syntax? 
(cid:1)(cid:1) depends on how you build the vector 

600.465 - intro to nlp - j. eisner 

9 

600.465 - intro to nlp - j. eisner 

10 

each word token has a different vector 

each word sense has a different vector 

(cid:1)(cid:1) good for splitting words - unsupervised wsd 
(cid:1)(cid:1) cluster the tokens: each cluster is a sense! 

(cid:1)(cid:1) have turned it into the hot dinner-party topic. the comedy is the 
(cid:1)(cid:1) selection for the world cup party, which will be announced on may 1  
(cid:1)(cid:1)
the by-pass there will be a street party. "then," he says, "we are going   

in the 1983 general election for a party which, when it could not bear to  
to attack the scottish national party , who look set to seize perth and  

(cid:1)(cid:1)
(cid:1)(cid:1)
(cid:1)(cid:1) number-crunchers within the labour party, there now seems little doubt 

that had been passed to a second party who made a financial decision 

(cid:1)(cid:1)
(cid:1)(cid:1) a future obliges each party to the contract to fulfil it by 
600.465 - intro to nlp - j. eisner 

(cid:1)(cid:1) represent each new word token as vector, too  
(cid:1)(cid:1) now assign each token the closest sense 

(cid:1)(cid:1) (could lump together all tokens of the word in the 

same document: assume they all have same sense) 

(cid:1)(cid:1) have turned it into the hot dinner-party topic. the comedy is the 
(cid:1)(cid:1) selection for the world cup party, which will be announced on may 1  
(cid:1)(cid:1)
the by-pass there will be a street party. "then," he says, "we are going   

let you know that there   s a party at my house tonight.  directions: drive 

? 

(cid:1)(cid:1)

in the 1983 general election for a party which, when it could not bear to  
to attack the scottish national party , who look set to seize perth and  

(cid:1)(cid:1)
(cid:1)(cid:1)
(cid:1)(cid:1) number-crunchers within the labour party, there now seems little doubt 

? 

11 

600.465 - intro to nlp - j. eisner 

12 

where can we get sense-
labeled training data? 

where can we get sense-
labeled training data? 

(cid:1)(cid:1) to do supervised wsd, need many examples of 

each sense in context 

(cid:1)(cid:1) to do supervised wsd, need many examples of 

each sense in context 

(cid:1)(cid:1) have turned it into the hot dinner-party topic. the comedy is the 
(cid:1)(cid:1) selection for the world cup party, which will be announced on may 1  
(cid:1)(cid:1)
the by-pass there will be a street party. "then," he says, "we are going   

let you know that there   s a party at my house tonight.  directions: drive 

? 

(cid:1)(cid:1)

in the 1983 general election for a party which, when it could not bear to  
to attack the scottish national party , who look set to seize perth and  

(cid:1)(cid:1)
(cid:1)(cid:1)
(cid:1)(cid:1) number-crunchers within the labour party, there now seems little doubt 

? 

600.465 - intro to nlp - j. eisner 

13 

600.465 - intro to nlp - j. eisner 

(cid:1)(cid:1) sources of sense-labeled training text:  

(cid:1)(cid:1) human-annotated text - expensive 
(cid:1)(cid:1) bilingual text (rosetta stone)     can figure out which 

sense of    plant    is meant by how it translates 

(cid:1)(cid:1) dictionary definition of a sense is one sample context 
(cid:1)(cid:1) roget   s thesaurus entry of a sense is one sample 

context 

hardly any data per sense     but we   ll use it later to 

get unsupervised training started 

14 

slide courtesy of d. yarowsky (modified) 

a problem with the vector model 

just one cue is sometimes enough ... 

(cid:1)(cid:1) bad idea to treat all context positions equally: 

(cid:1)(cid:1) possible solutions: 

(cid:1)(cid:1) faraway words don   t count as strongly? 
(cid:1)(cid:1) words in different positions relative to plant are 

different elements of the vector?  
(cid:1)(cid:1) (i.e., (pesticide, -1) and (pesticide,+1) are 

different features) 

(cid:1)(cid:1) words in different syntactic relationships to plant are 

different elements of the vector? 

600.465 - intro to nlp - j. eisner 

15 

600.465 - intro to nlp - j. eisner 

16 

an assortment of possible cues ... 

an assortment of possible cues ... 

slide courtesy of d. yarowsky (modified) 

slide courtesy of d. yarowsky (modified) 

only a  

weak cue ... 

but we   ll trust 

it if there   s 

nothing better 

generates a whole bunch 

of potential cues     use 
data to find out which 

ones work best 

600.465 - intro to nlp - j. eisner 

merged ranking 

of all cues  

of all these types 

17 

600.465 - intro to nlp - j. eisner 

18 

slide courtesy of d. yarowsky (modified) 

unsupervised learning! 

slide courtesy of d. yarowsky (modified) 

final decision list for lead   (abbreviated) 

to disambiguate a token of lead : 

(cid:1)(cid:1) scan down the sorted list 
(cid:1)(cid:1) the first cue that is found 

gets to make the decision all 
by itself 

(cid:1)(cid:1) not as subtle as combining 
cues, but works well for wsd 
cue   s score is its log-likelihood ratio:   
log [ p(cue | sense a)  [smoothed] 
       / p(cue | sense b) ] 

600.465 - intro to nlp - j. eisner 

19 

very readable paper at http://cs.jhu.edu/~yarowsky/acl95.ps 

sketched on the following slides ... 

unsupervised learning! 

make that    minimally supervised    
first, find a pair of    seed words    
that correlate well with the 2 senses 

unsupervised learning! 

table taken from yarowsky (1995) 

yarowsky   s id64 algorithm 

(cid:1)(cid:1) if    plant    really has 2 senses, it should appear in 

(cid:1)(cid:1) 2 dictionary entries: pick content words from those 
(cid:1)(cid:1) 2 thesaurus entries: pick synonyms from those 
(cid:1)(cid:1) 2 different clusters of documents: pick representative 

words from those 

(cid:1)(cid:1) 2 translations in parallel text: use the translations as 

seed words 

(cid:1)(cid:1) or just have a human name the seed words 

(maybe from a list of words that occur unusually 
often near    plant   ) 

600.465 - intro to nlp - j. eisner 

21 

target word: 

plant 

life 

(1%) 

98% 

manufacturing 

(1%) 

(life, manufacturing) 

600.465 - intro to nlp - j. eisner 

22 

unsupervised learning! 

figure taken from yarowsky (1995) 

unsupervised learning! 

table taken from yarowsky (1995) 

yarowsky   s id64 algorithm 

yarowsky   s id64 algorithm 

learn a classifier that 
distinguishes a from b.   
it will notice features like 

   animal    (cid:2) a,    automate    (cid:2) b. 

learn a classifier that 
distinguishes a from b.   
it will notice features like 

   animal    (cid:2) a,    automate    (cid:2) b. 

no surprise 
what the top 

cues are 

but other cues 
also good for 
discriminating 

these seed 
examples 

(life, manufacturing) 

(life, manufacturing) 

600.465 - intro to nlp - j. eisner 

23 

600.465 - intro to nlp - j. eisner 

24 

unsupervised learning! 

slide courtesy of d. yarowsky (modified) 

unsupervised learning! 

figure taken from yarowsky (1995) 

the strongest of 
the new cues 
help us classify 
more examples ... 

(life, manufacturing) 

600.465 - intro to nlp - j. eisner 

from which 
we can 
extract and 
rank even 
more cues 
that 
discriminate 
them ... 
25 

should be a good 

classifier, unless we 

accidentally learned some 

bad cues along the way 
that polluted the original 

sense distinction. 

(life, manufacturing) 

600.465 - intro to nlp - j. eisner 

26 

unsupervised learning! 

slide courtesy of d. yarowsky (modified) 

unsupervised learning! 

slide courtesy of d. yarowsky (modified) 

life and manufacturing are no longer even in the top cues! 
many unexpected cues were extracted, without supervised training 

this test 
example 

now use the final decision list to classify test examples: 

top ranked cue 
appearing in 

   one sense per discourse    

(cid:1)(cid:1) a final trick: 

(cid:1)(cid:1) all tokens of plant in the same document 

probably have the same sense. 

3 tokens in same document gang up on the 4th 

600.465 - intro to nlp - j. eisner 

28 

unsupervised learning! 

slide courtesy of d. yarowsky (modified) 

slide courtesy of d. yarowsky (modified) 

   one sense per discourse    

a note on combining cues 

(cid:1)(cid:1) a final trick: 

(cid:1)(cid:1) all tokens of plant in the same document 

probably have the same sense. 

these stats  

come from term 
papers of known 

authorship 

(i.e., supervised 

training) 

600.465 - intro to nlp - j. eisner 

29 

600.465 - intro to nlp - j. eisner 

30 

a note on combining cues 

a note on combining cues 

slide courtesy of d. yarowsky (modified) 

slide courtesy of d. yarowsky (modified) 

planta 

planta 

plantb 

planta 

plantb 

plantb 

planta 

planta 

planta 

1  

1  

2  

2  

plantb 

plantb 

plantb 

1  

1  

2  

2  

   naive bayes    model for classifying text 

(note the naive independence assumptions!) 

we   ll look at it again in a later lecture 

would this kind of 
sentence be more 

typical of a student a 
paper or a student b 

paper?  

600.465 - intro to nlp - j. eisner 

31 

600.465 - intro to nlp - j. eisner 

   naive bayes    model for classifying text 

used here for id51 

would this kind of 

sentence be more typical 
of a plant a context or a 

plant b context?  
32 

