finite-state
and the noisy channel

600.465 - intro to nlp - j. eisner

1

id40

x =

theprophetsaidtothecity

    what does this say?

    and what other words are substrings?

    could segment with parsing (how?), but slow.
    given l = a    lexicon    fsa that matches all english words.
    what do these regexps do?

(what does this fsa look like?  how many paths?)

    l*
    l* & x
    [ l         ]*
    [ l        :0 ]* .o. x
    [ l         ]* .o. [ [       :0] | \        ]* .o. x
(equivalent)

600.465 - intro to nlp - j. eisner

id40

x =

theprophetsaidtothecity

    what does this say?

    and what other words are substrings?

    could segment with parsing (how?), but slow.

    given l = a    lexicon    fsa that matches all english words.
    let   s try to recover the spaces with a single regexp    

[ [ l        :0 ]* .o. x ].u

word sequence
transduced to

spaceless version

600.465 - intro to nlp - j. eisner

restrict to paths
that produce

observed string x

language of
upper strings
of those paths

3

id40

x =

theprophetsaidtothecity

    given l = a    lexicon    fsa that matches all english words.

point of
the lecture:
this solution
generalizes!

[ [ l        :0 ]* .o. x ].u

word sequence
transduced to

restrict to paths
that produce

observed string x

spaceless version
    what if lexicon is weighted?
    from unigrams to bigrams?
    smooth l to include unseen words?
    make upper strings be over    alphabet    of words, not letters?
600.465 - intro to nlp - j. eisner

4

language of
upper strings
of those paths

id147

    id147 also needs a lexicon l
    but there is distortion beyond deleting spaces    
    let t be a transducer that models common typos and

other spelling errors
    ance     ence
    e     e
    e     e // cons _ cons
    rr     r
    ge     dge
    etc.

(deliverance, ...)
(deliverance, ...)
(athlete, ...)
(embarras   occurrence,    )
(privilege,    )

    what does l .o. t represent?  how   s that useful?

    should l and t have probabilities?
    we   ll want t to include    all possible    errors    

600.465 - intro to nlp - j. eisner

5

id87

real language   y

noisy channel   y     x

observed string   x

want to recover y from x

600.465 - intro to nlp - j. eisner

6

id87

real language   y

correct spelling

noisy channel   y     x

typos

observed string   x

misspelling

want to recover y from x

600.465 - intro to nlp - j. eisner

7

id87

real language   y

(lexicon space)*

noisy channel   y     x

delete spaces

observed string   x

text w/o spaces

want to recover y from x

600.465 - intro to nlp - j. eisner

8

id87

real language   y

language model

(lexicon space)*

noisy channel   y     x

acoustic model

pronunciation

observed string   x

speech

want to recover y from x

600.465 - intro to nlp - j. eisner

9

id87

real language   y

probabilistic id18

tree

noisy channel   y     x

delete everything

but terminals

observed string   x

text

want to recover y from x

600.465 - intro to nlp - j. eisner

10

id87

real language   y

noisy channel   y     x

observed string   x

p(y)

*

p(x | y)

=

p(x,y)

want to recover y   y from x   x
choose y that maximizes p(y | x) or equivalently p(x,y)

600.465 - intro to nlp - j. eisner

11

remember:
noisy channel and bayes    theorem

   noisy channel   

y

p(y=y)

mess up
y into x

p(x=x | y=y)

600.465 - intro to nlp - j. eisner

   decoder   

x

most likely
reconstruction of y
maximize p(y=y | x=x)
= p(y=y) p(x=x | y=y) / (x=x)
= p(y=y) p(x=x | y=y)

/    y    p(y=y   ) p(x=x | y=y   )
12

id87

.o.

=

p(y)

*

p(x | y)

=

p(x,y)

note p(x,y) sums to 1.
suppose x=   c   ; what is best    y   ?

13

600.465 - intro to nlp - j. eisner

id87

.o.

=

p(y)

*

p(x | y)

=

p(x,y)

600.465 - intro to nlp - j. eisner

suppose x=   c   ; what is best    y   ?

14

id87

.o.

.o.

=

restrict just to
paths compatible
with output    c   

p(y)

*

p(x | y)

*

(x==x)? 0 or 1

=

p(x,y)

600.465 - intro to nlp - j. eisner

15

id87

(i.e., paths transducing y    x

have total prob p(x | y))
noisy channel fst:
accepts pair (x,y)
with weight p(x | y)

[ y .o. t .o. x ].u

source model fsa:
accepts string y
with weight p(y)
(i.e., paths accepting y
have total prob p(y))

restrict to paths
that produce

observed string x

language of
upper strings
of those paths

600.465 - intro to nlp - j. eisner

16

just bayes    theorem again

.o.

.o.

=

restrict just to
paths compatible
with output    c   

600.465 - intro to nlp - j. eisner

p(y) = prior on y

*

p(x | y) = likelihood

of y, for each x

*

evidence x=x

=

p(x,y): divide by
constant p(x) to get
posterior p(y | x)

17

morpheme segmentation
    let lexicon be a machine that matches all turkish

words
    same problem as id40
    just at a lower level: morpheme segmentation
    turkish word: uygarla  t  ramad  klar  m  zdanm    s  n  zcas  na

= uygar+la  +t  r+ma+d  k+lar  +m  z+dan+m    +s  n  z+ca+s  +na
(behaving) as if you are among those whom we could not cause to
become civilized

    some constraints on morpheme sequence: bigram probs
    generative model     concatenate then fix up joints

    stop + -ing = stopping,     fly + -s = flies,     vowel harmony
    use a cascade of transducers to handle all the fixups

    but this is just morphology!
    can use probabilities here too (but people often don   t)
600.465 - intro to nlp - j. eisner
18

an fst: baby think & baby talk

:b/.2

:m/.4

.2

observe
talk

recover
think, by
composition

600.465 - intro to nlp - j. eisner

iwant:u/.8

   :m/.05 mama:m

iwant:    /.1

.1

fst mapping think to talk
(

= just babbling, no thought)

m

m

   :m/.05 mama:m

iwant:    /.1

:m/.4

.1

mama/.05mama/.005
mama iwant/.0005
mama iwant iwant/.00005

:m/.4

.2

/.032

19

joint prob. by double composition

think

   

:b/.2

:m/.4

.2

m

iwant:u/.8

   :m/.05 mama:m

iwant:    /.1

.1

m

   :m/.05 mama:m

iwant:    /.1

:m/.4

:m/.4

.1

.2

talk

compose

600.465 - intro to nlp - j. eisner

p(   * : mm) = .0375555 = sum of all paths

20

cute method for summing all paths

think

   

:b/.2

:m/.4

.2

m

iwant:u/.8

   :m/.05 mama:m

iwant:    /.1

.1

m

   :m/.05 mama:m

iwant:    /.1

:m/.4

:m/.4

.1

.2

talk

compose

600.465 - intro to nlp - j. eisner

p(   * : mm) = .0375555 = sum of all paths

21

cute method for summing all paths

think

   :   

:b/.2

:m/.4

.2

iwant:u/.8

   :m/.05 mama:m

iwant:    /.1

.1

m:   

m:   

   :   /.05

   :   

   :   /.1

   :   /.4

   :   /.4

.1

.2

talk

compose

600.465 - intro to nlp - j. eisner

p(   * : mm) = .0375555 = sum of all paths

22

cute method for summing all paths

think

   :   

:b/.2

:m/.4

.2

iwant:u/.8

   :m/.05 mama:m

iwant:    /.1

.1

talk

m:   

m:   

compose
+ minimize

0.0375555

600.465 - intro to nlp - j. eisner

p(   * : mm) = .0375555 = sum of all paths

23

p(mama     | mm)

think

   :mama

   :   

:b/.2

:m/.4

.2

iwant:u/.8

   :m/.05 mama:m

iwant:    /.1

.1

talk

m:   

m:   

compose
+ minimize

p(mama    * | mm) = .005555/.0375555

0.005555

= .148

p(mama    * : mm) = .005555 = sum of mama     paths
p(   * : mm) = .0375555 = sum of all paths

24

600.465 - intro to nlp - j. eisner

id153

    older baby said caca
    was probably thinking clara (?)

    do these match up well?  how well?

clara
caca

clar a
c a ca

clara
c aca

clara

caca

3 substitutions

+ 1 deletion
= total cost 4

2 deletions
+ 1 insertion
= total cost 3

1 deletion

+ 1 substitution
= total cost 2

5 deletions
+ 4 insertions
= total cost 9

600.465 - intro to nlp - j. eisner

minimum id153

(best alignment)

25

id153 as min-cost path
0 1 2 3 4 5
c l a r a

position in upper string

0      1       2      3       4       5

?

c a c a
0 1 2 3 4

minimum-cost path

shows the best

alignment,

and its cost is the

id153

0

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

1

2

3

4

600.465 - intro to nlp - j. eisner

26

id153 as min-cost path
0 10 1 2
0 1 2 3
0 1 2 3 4
0 1 2 3 4 5
0
c lc l a
c l a r
c l a r a
c

position in upper string

0      1       2      3       4       5

0

c a c a
c ac a c
c
0 10 1 2
0 1 2 3
0 1 2 3 4
0

minimum-cost path

shows the best

alignment,

and its cost is the

id153

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

1

2

3

4

600.465 - intro to nlp - j. eisner

27

id153 as min-cost path

position in upper string

0      1       2      3       4       5

a deletion edge

has cost 1

it advances in the
upper string only, so

it   s horizontal

it pairs the next
letter of the upper
string with (empty)
in the lower string

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

28

id153 as min-cost path

position in upper string

0      1       2      3       4       5

an insertion edge

has cost 1

it advances in the
lower string only, so

it   s vertical

it pairs (empty) in
the upper string with

the next letter of
the lower string

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

29

id153 as min-cost path
a substitution edge

position in upper string

0      1       2      3       4       5

has cost 0 or 1

it advances in the

upper and lower strings

simultaneously,
so it   s diagonal

it pairs the next letter
of the upper string with
the next letter of the

lower string

cost is 0 or 1 depending

on whether those

letters are identical!

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

30

id153 as min-cost path

position in upper string

0      1       2      3       4       5

we   re looking for a
path from upper left

to lower right

(so as to get through

both strings)

solid edges have

cost 0, dashed edges

have cost 1

so we want the path

with the fewest
dashed edges

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

31

id153 as min-cost path

position in upper string

0      1       2      3       4       5

clara
caca

3 substitutions

+ 1 deletion
= total cost 4

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

32

id153 as min-cost path

position in upper string

0      1       2      3       4       5

clar a
c a ca

2 deletions
+ 1 insertion
= total cost 3

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

33

id153 as min-cost path

position in upper string

0      1       2      3       4       5

clara
c aca

1 deletion

+ 1 substitution
= total cost 2

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

34

id153 as min-cost path

position in upper string

0      1       2      3       4       5

clara

caca

5 deletions
+ 4 insertions
= total cost 9

g
n
i
r
t
s
 
r
e
w
o
l
 
n
i
 
n
o
i
t
i
s
o
p

0

1

2

3

4

600.465 - intro to nlp - j. eisner

35

id153 transducer

o(k) deletion arcs

o(k) insertion

arcs

o(k2) substitution

arcs

o(k) no-change arcs

600.465 - intro to nlp - j. eisner

36

stochastic

id153 transducer

o(k) deletion arcs

o(k2) substitution

arcs

o(k) insertion

arcs

o(k) identity arcs

likely edits = high-id203 arcs
defines p(x | y), e.g., p(caca | clara)

600.465 - intro to nlp - j. eisner

37

most likely path that maps clara to caca?

best path (by dijkstra   s algorithm)

=

clara

.o.

.o.
caca

600.465 - intro to nlp - j. eisner

38

what string produced caca? (posterior distribution)

lexicon*

.o.

.o.
caca

=

more complicated fst,
each path is a possible input

with cycles.

string aligned to caca.

a path   s weight is proportional
to the posterior id203 of
that path as an explanation for
caca.  could find best path.

600.465 - intro to nlp - j. eisner

39

id103 by fst composition (pereira & riley 1996)

trigram language model

p(word seq)

.o.

pronunciation model

p(phone seq | word seq)

.o.

acoustic model

p(acoustics | phone seq)

.o.

observed acoustics

600.465 - intro to nlp - j. eisner

40

id103 by fst composition (pereira & riley 1996)

trigram language model

p(word seq)

.o.

cat:k    t

p(phone seq | word seq)

.o.
   :
.o.

phone
context

p(acoustics | phone seq)

phone
context

600.465 - intro to nlp - j. eisner

41

id48 tagging

    can think of this as a finite-state noisy

channel, too!

    see slides in the    tagging    lecture.

600.465 - intro to nlp - j. eisner

42

