bayes    theorem

600.465 - intro to nlp - j. eisner

1

remember language id?

    let p(x) = id203 of text x in english
    let q(x) = id203 of text x in polish
    which id203 is higher?

    (we   d also like bias toward english since it   s

more likely a priori     ignore that for now)

let   s revisit this
   horses and lukasiewicz are on the curriculum.   
p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )

600.465     intro to nlp     j. eisner

2

bayes    theorem

    p(a | b) = p(b | a) * p(a) / p(b)

    easy to check by removing syntactic sugar
    use 1: converts p(b | a) to p(a | b)
    use 2: updates p(a) to p(a | b)

    stare at it so you   ll recognize it later

600.465 - intro to nlp - j. eisner

3

language id

    given a sentence x, i suggested comparing its

prob in different languages:
    p(sent=x | lang=english)
    p(sent=x | lang=polish)
    p(sent=x | lang=xhosa)

(i.e., penglish(sent=x))
(i.e., ppolish(sent=x))
(i.e., pxhosa(sent=x))
    but surely for language id we should compare

    p(lang=english | sent=x)
    p(lang=polish | sent=x)
    p(lang=xhosa | sent=x)

600.465 - intro to nlp - j. eisner

4

language id
    for language id we should compare

    p(lang=english | sent=x)
    p(lang=polish | sent=x)
    p(lang=xhosa | sent=x)

a posteriori

    for ease, multiply by p(sent=x) and compare

    p(lang=english, sent=x)
    p(lang=polish, sent=x)
    p(lang=xhosa, sent=x)

sum of these is a way to find
p(sent=x); can divide back
by that to get posterior probs

    must know prior probabilities; then rewrite as

    p(lang=english) *  p(sent=x | lang=english)
    p(lang=polish)
*  p(sent=x | lang=polish)
*  p(sent=x | lang=xhosa)
    p(lang=xhosa)

a priori

likelihood (what we had before)

600.465 - intro to nlp - j. eisner

5

best

let   s try it!
p(lang=english) *
p(lang=polish)
*
*
p(lang=xhosa)
prior prob

0.7
0.2
0.1

from a very simple
model: a single die
whose sides are the
languages of the world

   first we pick a random lang,
then we roll a random sent
with the lang dice.   

p(sent=x | lang=english)
p(sent=x | lang=polish)
p(sent=x | lang=xhosa)
likelihood

0.00001
0.00004
0.00005

best

from a set of
trigram dice
(actually 3 sets,
one per language)

=
=
=

p(lang=english, sent=x)
p(lang=polish,  sent=x)
p(lang=xhosa,  sent=x)
joint id203

0.000007
0.000008
0.000005

best compromise

p(sent=x)

id203 of evidence

600.465 - intro to nlp - j. eisner

0.000020 total over all ways
of getting sent=x
6

let   s try it!

   first we pick a random lang,
then we roll a random sent
with the lang dice.   

   

=
=
=

p(lang=english, sent=x)
p(lang=polish,  sent=x)
p(lang=xhosa,  sent=x)
joint id203

0.000007
0.000008
0.000005

best compromise

add up

p(sent=x)

id203 of evidence

0.000020 total id203 of
getting sent=x
one way or another!

normalize
(divide by
a constant
so they   ll
sum to 1)

p(lang=english | sent=x)
p(lang=polish  | sent=x)
p(lang=xhosa  | sent=x)
posterior id203

600.465 - intro to nlp - j. eisner

0.000007/0.000020 = 7/20
0.000008/0.000020 = 8/20
0.000005/0.000020 = 5/20
given the evidence sent=x,
the possible languages sum to 1

best

7

general case (   noisy channel   )

   noisy channel   

a

p(a=a)

mess up
a into b

p(b=b | a=a)

language     text
text
spelled     misspelled
english     french

    speech

600.465 - intro to nlp - j. eisner

   decoder   

b

most likely
reconstruction of a
maximize p(a=a | b=b)
= p(a=a) p(b=b | a=a) / (b=b)
= p(a=a) p(b=b | a=a)

/    a    p(a=a   ) p(b=b | a=a   )
8

language id
    for language id we should compare

    for ease, multiply by p(sent=x) and compare

a posteriori

    p(lang=english | sent=x)
    p(lang=polish | sent=x)
    p(lang=xhosa | sent=x)

    p(lang=english, sent=x)
    p(lang=polish, sent=x)
    p(lang=xhosa, sent=x)

    which we find as follows (we need prior probs!):

    p(lang=english)
    p(lang=polish)
    p(lang=xhosa)

a priori

*  p(sent=x | lang=english)
*  p(sent=x | lang=polish)
*  p(sent=x | lang=xhosa)

likelihood

600.465 - intro to nlp - j. eisner

9

general case (   noisy channel   )

    want most likely a to have generated evidence b

    for ease, multiply by p(b=b) and compare

a posteriori

    p(a = a1 | b = b)
    p(a = a2 | b = b)
    p(a = a3 | b = b)

    p(a = a1, b = b)
    p(a = a2, b = b)
    p(a = a3, b = b)

    which we find as follows (we need prior probs!):

    p(a = a1)
    p(a = a2)
    p(a = a3)

a priori

*  p(b = b | a = a1)
*  p(b = b | a = a2)
*  p(b = b | a = a3)

likelihood

600.465 - intro to nlp - j. eisner

10

id103
    for baby id103 we should compare

    p(meaning=gimme | sound=uhh)
    p(meaning=changeme | sound=uhh)
    p(meaning=loveme | sound=uhh)

a posteriori

    for ease, multiply by p(sound=uhh) & compare

    p(meaning=gimme, sound=uhh)
    p(meaning=changeme, sound=uhh)
    p(meaning=loveme, sound=uhh)

    which we find as follows (we need prior probs!):

    p(mean=gimme)
    p(mean=changeme) *  p(sound=uhh | mean=changeme)
    p(mean=loveme)

*  p(sound=uhh | mean=loveme)

*  p(sound=uhh | mean=gimme)

a priori

likelihood

600.465 - intro to nlp - j. eisner

11

a simpler view?  odds ratios

    what a values are probable, given that b=b?
    bayes    theorem says:

    p(a=a1 | b=b) = p(a=a1)   p(b=b | a=a1) / p(b=b)
    p(a=a2 | b=b) = p(a=a2)   p(b=b | a=a2) / p(b=b)

    therefore

   

p(a=a1 | b=b)    p(a=a1)
p(a=a2 | b=b)    p(a=a2)

=

   

posterior
odds ratio

=

prior

   

odds ratio

p(b=b | a=a1)
p(b=b | a=a2)
likelihood

ratio

600.465 - intro to nlp - j. eisner

12

a simpler view?  odds ratios

   

p(a=a1 | b=b)    p(a=a1)
p(a=a2 | b=b)    p(a=a2)

=

   

posterior
odds ratio

prior

odds ratio

p(b=b | a=a1)
p(b=b | a=a2)
likelihood
odds ratio

0.7
0.2
0.1

p(lang=english)
p(lang=polish)
p(lang=xhosa)
prior

p(sent=x | lang=english)
p(sent=x | lang=polish)
p(sent=x | lang=xhosa)
likelihood

0.00001
0.00004
0.00005

    a priori, english is 7 times as probable as xhosa (7:1 odds)
    but likelihood of english is only 1/5 as large (1:5 odds)
    so a posteriori, english now 7    1/5 = 1.2 times as probable (7:5 odds)

    that is: p(english) = 7/12, p(xhosa) = 5/12 if no other options
600.465 - intro to nlp - j. eisner

13

growing evidence eventually
overwhelms the prior

    we were expecting polish text but actually it   s english
    what happens as we read more & more words?

    the prior odds ratio stays the same
    but the likelihood odds ratio becomes extreme (much bigger or

much smaller than 1, depending on which hypothesis is correct)
    suppose each trigram is 1.001 times more probable under the

english model than the polish model

    then after 700 trigrams, the likelihood ratio is > 2 in favor of

    and after 7000 trigrams, the likelihood ratio is > 210 in favor of

english (1.001700 > 2)

english!

    as long as the prior p(english) > 0, eventually we come to believe

it   s english a posteriori.  we get surer and surer with more evidence.

600.465 - intro to nlp - j. eisner

14

life or death!

does epitaph have hoof-
and-mouth disease?
he tested positive     oh no!
false positive rate only 5%

    p(hoof) = 0.001  so p(   hoof) = 0.999

    p(positive test |    hoof) = 0.05    false pos   
    p(negative test | hoof) =        0    false neg   

so p(positive test | hoof) = 1-  

    what is p(hoof | positive test)?

    consider the hoof:    hoof odds ratio
    prior odds ratio 1:999 (improbable!)
    likelihood ratio at most 1:0.05, or equivalently 20:1
    so posterior odds ratio at most 20:999, or about 1:50

    that is, p(hoof | positive test) at most about 1/51

600.465 - intro to nlp - j. eisner

15

