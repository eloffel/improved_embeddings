machine translation: word-based models

and the em algorithm

chris callison-burch

(slides borrowed from philipp koehn)

december 3, 2007

chris callison-burch

word-based translation models and em

december 3, 2007

machine translation

1

    task: make sense of foreign text like

    one of the oldest problems in arti   cial intelligence
    solutions may many encompass many other nlp applications:

generation, word
id68, pronoun resolution, etc.

sense

disambiguation,

named

entity

parsing,
recognition,

chris callison-burch

word-based translation models and em

december 3, 2007

the rosetta stone

2

    egyptian language was a mystery for centuries
    1799 a stone with egyptian text and its translation into greek was found
    allowed people to learn how to translate egyptian

chris callison-burch

word-based translation models and em

december 3, 2007

modern day rosetta stone

3

chris callison-burch

word-based translation models and em

december 3, 2007

sooner or later we will have to be suf   ciently progressive in terms of own resources as a basis for this fair tax system . we plan to submit the    rst accession partnership in the autumn of this year .  it is a question of equality and solidarity .   the recommendation for the year 1999 has been formulated at a time of favourable developments and optimistic prospects for the european economy .that does not , however , detract from the deep appreciation which we have for this report . what is more , the relevant cost dynamic is completely under control.  fr  her oder sp  ter m  ssen wir die notwendige progressivit  t der eigenmittel als grundlage dieses gerechten steuersystems zur sprache bringen . wir planen , die erste beitrittspartnerschaft im herbst dieses jahres vorzulegen .hier geht es um gleichberechtigung und solidarit  t .die empfehlung f  r das jahr 1999 wurde vor dem hintergrund g  nstiger entwicklungen und einer f  r den kurs der europ  ischen wirtschaft positiven perspektive abgegeben .  im   brigen tut das unserer hohen wertsch  tzung f  r den vorliegenden bericht keinen abbruch . im   brigen ist die diesbez  gliche kostenentwicklung v  llig unter kontrolle .  parallel data

4

    lots of translated text available: 100s of million words of translated text for

some language pairs
    a book has a few 100,000s words
    an educated person may read 10,000 words a day
    3.5 million words a year
    300 million a lifetime
    soon computers will be able to see more translated text than humans read

in a lifetime

    machines can learn how to translated foreign languages

chris callison-burch

word-based translation models and em

december 3, 2007

id151

    components: translation model, language model, decoder

5

chris callison-burch

word-based translation models and em

december 3, 2007

statistical analysisstatistical analysisforeign/englishparallel textenglishtexttranslationmodellanguagemodeldecoding algorithm6

    how to translate a word     look up in dictionary

lexical translation

haus     house, building, home, household, shell.

    multiple translations

    some more frequent than others
    for instance: house, and building most common
    special cases: haus of a snail is its shell

chris callison-burch

word-based translation models and em

december 3, 2007

collect statistics

7

    look at a parallel corpus (german text along with english translation)

translation of haus count
house
8,000
1,600
building
home
200
150
household
shell
50

chris callison-burch

word-based translation models and em

december 3, 2007

estimate translation probabilities

8

    id113

                                             

pf(e) =

if e = house,
if e = building,
if e = home,

0.8
0.16
0.02
0.015 if e = household,
0.005 if e = shell.

chris callison-burch

word-based translation models and em

december 3, 2007

9

alignment

    in a parallel text (or when we translate), we align words in one language with

the words in the other

    word positions are numbered 1   4

chris callison-burch

word-based translation models and em

december 3, 2007

dashausistkleinthehouseissmall1234123410

alignment function
    formalizing alignment with an alignment function
    mapping an english target word at position i to a german source word at

position j with a function a : i     j

    example

a : {1     1, 2     2, 3     3, 4     4}

chris callison-burch

word-based translation models and em

december 3, 2007

    words may be reordered during translation

reordering

11

a : {1     3, 2     4, 3     2, 4     1}

chris callison-burch

word-based translation models and em

december 3, 2007

dashausistkleinthehouseissmall12341234one-to-many translation
    a source word may translate into multiple target words

12

a : {1     1, 2     2, 3     3, 4     4, 4     5}

chris callison-burch

word-based translation models and em

december 3, 2007

dashausistklitzekleinthehouseisverysmall123412345dropping words

13

    words may be dropped when translated

    the german article das is dropped

a : {1     2, 2     3, 3     4}

chris callison-burch

word-based translation models and em

december 3, 2007

dashausistkleinhouseissmall1231234inserting words

14

    words may be added during translation

    the english just does not have an equivalent in german
    we still need to map it to something: special null token

a : {1     1, 2     2, 3     3, 4     0, 5     4}

chris callison-burch

word-based translation models and em

december 3, 2007

dashausistkleinthehouseisjustsmallnull1234123450ibm model 1

15

    generative model: break up translation process into smaller steps
    translation id203

    ibm model 1 only uses lexical translation

    for a foreign sentence f = (f1, ..., flf) of length lf
    to an english sentence e = (e1, ..., ele) of length le
    with an alignment of each english word ej to a foreign word fi according to

the alignment function a : j     i

p(e, a|f) =

ley

j=1

t(ej|fa(j))

chris callison-burch

word-based translation models and em

december 3, 2007

16

example

haus

e
house
building
home
household
shell

t(e|f)
0.8
0.16
0.02
0.015
0.005

ist

t(e|f)
0.8
0.16
0.02
0.015
0.005

e
is
   s
exists
has
are

klein

e
small
little
short
minor
petty

t(e|f)
0.4
0.4
0.1
0.06
0.04

das

e
the
that
which
who
this

t(e|f)
0.7
0.15
0.075
0.05
0.025

p(e, a|f) = t(the|das)    t(house|haus)    t(is|ist)    t(small|klein)

= 0.7    0.8    0.8    0.4
= 0.0028

chris callison-burch

word-based translation models and em

december 3, 2007

learning lexical translation models

    we would like to estimate the lexical translation probabilities t(e|f) from a

17

parallel corpus

    ... but we do not have the alignments
    chicken and egg problem
    if we had the alignments,

    we could estimate the parameters of our generative model
    we could estimate the alignments

    if we had the parameters,

chris callison-burch

word-based translation models and em

december 3, 2007

em algorithm

18

    incomplete data

    if we had complete data, would could estimate model
    if we had model, we could    ll in the gaps in the data

    expectation maximization (em) in a nutshell

    initialize model parameters (e.g. uniform)
    assign probabilities to the missing data
    estimate model parameters from completed data
    iterate

chris callison-burch

word-based translation models and em

december 3, 2007

19

ibm model 1 and em

    em algorithm consists of two steps
    expectation-step: apply model to the data

    parts of the model are hidden (here: alignments)
    using the model, assign probabilities to possible values

    maximization-step: estimate model from data

    take assign values as fact
    collect counts (weighted by probabilities)
    estimate model from counts

    iterate these steps until convergence

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em

20

step 1. set parameter values uniformly.
t(id7|house) = 1
t(maison|house) = 1
t(id7|blue) = 1
t(maison|blue) = 1

2

2

2

2

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em

21

compute p (a, f|e) for all alignments.

p (a, f|e) = 1

2     1

2 = 1

4

p (a, f|e) = 1

2     1

2 = 1

4

p (a, f|e) = 1

2

chris callison-burch

word-based translation models and em

december 3, 2007

housebluemaisonid7housebluemaisonid7housemaisonibm model 1 and em

22

step 3. normalize p (a, f|e) values to yield p (a|e, f) values.

p (a|e, f) = 1

4/2

4 = 1

2

p (a|e, f) = 1

4/2

4 = 1

2

p (a|e, f) = 1

2/1

2 = 1 there   s only one alignment, so

p (a|e, f) will be 1 always.

chris callison-burch

word-based translation models and em

december 3, 2007

housebluemaisonid7housebluemaisonid7housemaisonibm model 1 and em

23

step 4. collect fractional counts.
tc(id7|house) = 1
tc(maison|house) = 1
tc(id7|blue) = 1
tc(maison|blue) = 1

2 + 1 = 3

2

2

2

2

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em

24

step 5. normalize fractional counts to get revised parameter values.
t(id7|house) = 1
2 = 1
2/4
t(maison|house) = 3
2/4
t(id7|blue) = 1
2/1 = 1
t(maison|blue) = 1

2 = 3

4

2

4

2/1 = 1

2

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em

25

repeat step 2. compute p (a, f|e) for all alignments.

p (a, f|e) = 1

4     1

2 = 1

8

p (a, f|e) = 3

4     1

2 = 3

8

p (a, f|e) = 3

4

chris callison-burch

word-based translation models and em

december 3, 2007

housebluemaisonid7housebluemaisonid7housemaisonibm model 1 and em

26

repeat step 3. normalize p (a, f|e) values to yield p (a|e, f) values.

p (a|e, f) = 1

4

p (a|e, f) = 1

4/2

4 = 3

4

p (a|e, f) = 1

chris callison-burch

word-based translation models and em

december 3, 2007

housebluemaisonid7housebluemaisonid7housemaisonibm model 1 and em

27

repeat step 4. collect fractional counts.
tc(id7|house) = 1
tc(maison|house) = 3
tc(id7|blue) = 3
tc(maison|blue) = 1

4 + 1 = 7

4

4

4

4

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em

28

repeat step 5. normalize fractional counts to get revised parameter values.
t(id7|house) = 1
t(maison|house) = 7
t(id7|blue) = 3
t(maison|blue) = 1

8

4

8

4

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em

29

repeating steps 2-5 many times yields:
t(id7|house) = 0.0001
t(maison|house) = 0.9999
t(id7|blue) = 0.9999
t(maison|blue) = 0.0001

chris callison-burch

word-based translation models and em

december 3, 2007

ibm model 1 and em: pseudocode

30

initialize t(e|f) uniformly
do

set count(e|f) to 0 for all e,f
set total(f) to 0 for all f
for all sentence pairs (e_s,f_s)

for all words e in e_s

total_s = 0
for all words f in f_s

total_s += t(e|f)

for all words e in e_s

for all words f in f_s

count(e|f) += t(e|f) / total_s
total(f)
+= t(e|f) / total_s

for all f in domain( total(.) )

for all e in domain( count(.|f) )

t(e|f) = count(e|f) / total(f)

until convergence

chris callison-burch

word-based translation models and em

december 3, 2007

higher ibm models

31

ibm model 1
ibm model 2
ibm model 3
ibm model 4

lexical translation
adds absolute reordering model
adds fertility model
relative reordering model

    only ibm model 1 has global maximum
    training of a higher ibm model builds on previous model
    compuationally biggest change in model 3
    exhaustive count collection becomes computationally too expensive
    sampling over high id203 alignments is used instead

chris callison-burch

word-based translation models and em

december 3, 2007

32

next up: decoding

chris callison-burch

word-based translation models and em

december 3, 2007

derivations for the expectation step

33

    we need to compute p(a|e, f)
    applying the chain rule:

p(a|e, f) = p(e, a|f)
p(e|f)

    we already have the formula for p(e, a|f) (de   nition of model 1)
    we need to compute p(e|f)

chris callison-burch

word-based translation models and em

december 3, 2007

derivations for the expectation step

34

p(e, a|f)

a

p(e|f) =x
lfx
lfx
lfx
ley

a(1)=0

a(1)=0

=

=

=

...

...

lfx
lfx

a(le)=0

ley

a(le)=0

j=1

t(ej|fi)

p(e, a|f)

t(ej|fa(j))

trick in the last line removes the need for an exponential number of products.
this makes ibm model 1 estimation tractable

j=1

i=0

chris callison-burch

word-based translation models and em

december 3, 2007

derivations for the expectation step

35

    combine what we have:

p(a|e, f) = p(e, a|f)/p(e|f)

qle
qle
plf
j=1 t(ej|fa(j))
i=0 t(ej|fi)
ley
plf
t(ej|fa(j))
i=0 t(ej|fi)

j=1

j=1

=

=

chris callison-burch

word-based translation models and em

december 3, 2007

derivations for the maximization step

36

    now we have to collect counts
    evidence from a sentence pair e,f that word e is a translation of word f :

c(e|f; e, f) =x
p(a|e, f)
    with the same simplication as before:
ple
t(e|f)
j=1 t(e|fa(j))

c(e|f; e, f) =

a

lex
lex

j=1

j=1

  (e, ej)  (f, fa(j))

lfx

i=0

  (e, ej)

  (f, fi)

chris callison-burch

word-based translation models and em

december 3, 2007

37

derivations for the maximization step

    after collecting these counts over a corpus, we can estimate the model:

p
p
p
(e,f) c(e|f; e, f))
(e,f) c(e|f; e, f))

f

t(e|f; e, f) =

chris callison-burch

word-based translation models and em

december 3, 2007

