the expectation
maximization (em) algorithm

    continued!

600.465 - intro to nlp - j. eisner

1

general idea

    start by devising a noisy channel

    any model that predicts the corpus observations via

some hidden structure (tags, parses,    )

    initially guess the parameters of the model!

    educated guess is best, but random can work

    expectation step: use current parameters (and

observations) to reconstruct hidden structure

    maximization step: use that hidden structure

(and observations) to reestimate parameters

600.465 - intro to nlp - j. eisner

repeat until convergence!

2

general idea

initial
guess

e step

guess of
unknown
parameters
(probabilities)

guess of unknown
hidden structure
(tags, parses, weather)
observed structure

(words, ice cream)

m step

600.465 - intro to nlp - j. eisner

3

for id48

initial
guess

e step

guess of
unknown
parameters
(probabilities)

guess of unknown
hidden structure
(tags, parses, weather)
observed structure

(words, ice cream)

m step

600.465 - intro to nlp - j. eisner

4

for id48

initial
guess

e step

guess of
unknown
parameters
(probabilities)

guess of unknown
hidden structure
(tags, parses, weather)
observed structure

(words, ice cream)

m step

600.465 - intro to nlp - j. eisner

5

for id48

initial
guess

e step

guess of
unknown
parameters
(probabilities)

guess of unknown
hidden structure
(tags, parses, weather)
observed structure

(words, ice cream)

m step

600.465 - intro to nlp - j. eisner

6

grammar reestimation

e step

p
a
r
s
e
r

correct test trees

s
c
o
r
e
r

accuracy

expensive and/or
wrong sublanguage

test
sentences

cheap, plentiful
and appropriate

grammar

learner

m step

600.465 - intro to nlp - j. eisner

training
trees

7

em by id145:

two versions

    the viterbi approximation

    expectation: pick the best parse of each sentence
    maximization: retrain on this best-parsed corpus
    advantage: speed!

    real em

    expectation: find all parses of each sentence
    maximization: retrain on all parses in proportion to
their id203 (as if we observed fractional count)

    advantage: p(training corpus) guaranteed to increase
    exponentially many parses, so don   t extract them

from chart     need some kind of clever counting

600.465 - intro to nlp - j. eisner

8

examples of em
    finite-state case: id48
       forward-backward    or    baum-welch    algorithm
    applications:

    explain ice cream in terms of underlying weather sequence
    explain words in terms of underlying tag sequence
    explain phoneme sequence in terms of underlying word
    explain sound sequence in terms of underlying phoneme

compose
these?
    context-free case: probabilistic id18s

       inside-outside    algorithm: unsupervised grammar learning!
    explain raw text in terms of underlying cx-free parse

    in practice, local maximum problem gets in the way
    but can improve a good starting grammar via raw text
    id91 case: explain points via clusters

600.465 - intro to nlp - j. eisner

9

our old friend pid18

s

p(

vp

np
time
v
flies

pp
p
like

np

det
an

n
arrow

| s) = p(s     np vp | s) * p(np     time | np)

* p(vp     v pp | vp)
* p(v     flies | v) *    

600.465 - intro to nlp - j. eisner

10

viterbi reestimation for parsing

    start with a    pretty good    grammar

    e.g., it was trained on supervised data (a treebank) that is
small, imperfectly annotated, or has sentences in a different
style from what you want to parse.

s

    parse a corpus of unparsed sentences:

# copies of
this sentence
in the corpus

12 today stocks were up

   
   

    reestimate:

advp

s

12

today

np

vp

stocks

v

prt

were up

    collect counts:    ; c(s     np vp) += 12; c(s) += 2*12;    
    divide: p(s     np vp | s) = c(s     np vp) / c(s)
    may be wise to smooth

600.465 - intro to nlp - j. eisner

11

true em for parsing

    similar, but now we consider all parses of each sentence
    parse our corpus of unparsed sentences:

s

# copies of
this sentence
in the corpus

12 today stocks were up

   
   

    collect counts fractionally:

       ; c(s     np vp) += 10.8; c(s) += 2*10.8;    
       ; c(s     np vp) += 1.2;   c(s) += 1*1.2;    

advp

s

10.8

today

np

vp

stocks

v

prt

1.2

were up
s

np

vp

600.465 - intro to nlp - j. eisner

np

np

v

prt

today

stocks

were

up

where are the constituents?

p=0.5

coal

energy

expert witness

13

where are the constituents?

p=0.1

coal

energy

expert witness

14

where are the constituents?

p=0.1

coal

energy

expert witness

15

where are the constituents?

p=0.1

coal

energy

expert witness

16

where are the constituents?

p=0.2

coal

energy

expert witness

17

where are the constituents?

coal

energy

expert witness

0.5+0.1+0.1+0.1+0.2 = 1

18

where are nps, vps,     ?

np locations

vp locations

s

vp

pp

np

np

v

p

det

n

time
time

flies
flies

like
like

an
an

arrow
arrow

19

where are nps, vps,     ?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (np time) (vp flies (pp like (np an arrow))))

p=0.5

20

where are nps, vps,     ?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (np time flies) (vp like (np an arrow)))

p=0.3

21

where are nps, vps,     ?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (vp time (np (np flies) (pp like (np an arrow)))))

p=0.1

22

where are nps, vps,     ?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (vp (vp time (np flies)) (pp like (np an arrow))))

p=0.1

23

where are nps, vps,     ?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

arrow

0.5+0.3+0.1+0.1 = 1

24

how many nps, vps,     in all?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

arrow

0.5+0.3+0.1+0.1 = 1

25

how many nps, vps,     in all?

np locations

vp locations

time

flies

like

an

arrow

time

flies

like

an

2.1 nps
(expected)

1.1 vps
(expected)

arrow

26

where did the rules apply?

s     np vp locations np     det n locations

time

flies

like

an

arrow

time

flies

like

an

arrow

27

where did the rules apply?

s     np vp locations np     det n locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (np time) (vp flies (pp like (np an arrow))))

p=0.5

28

where is s     np vp substructure?

s     np vp locations np     det n locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (np time flies) (vp like (np an arrow)))

p=0.3

29

where is s     np vp substructure?

s     np vp locations np     det n locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (vp time (np (np flies) (pp like (np an arrow)))))

p=0.1

30

where is s     np vp substructure?

s     np vp locations np     det n locations

time

flies

like

an

arrow

time

flies

like

an

arrow

(s (vp (vp time (np flies)) (pp like (np an arrow))))

p=0.1

31

why do we want this info?

    grammar reestimation by em method

    e step collects those expected counts
    m step sets

    minimum bayes risk decoding

    find a tree that maximizes expected reward,
e.g., expected total # of correct constituents

    cky-like id145 algorithm

    the input specifies the id203 of correctness

for each possible constituent (e.g., vp from 1 to 5)
32

why do we want this info?

    soft features of a sentence for other tasks

    ner system asks:    is there an np from 0 to 2?   

    true answer is 1 (true) or 0 (false)
    but we return 0.3, averaging over all parses
    that   s a perfectly good feature value     can be fed as

to a crf or a neural network as an input feature

    writing tutor system asks:    how many times did

the student use s     np[singular] vp[plural]?   
    true answer is in {0, 1, 2,    }
    but we return 1.8, averaging over all parses

33

true em for parsing

    similar, but now we consider all parses of each sentence
    parse our corpus of unparsed sentences:

s

# copies of
this sentence
in the corpus

12 today stocks were up

   
   

    collect counts fractionally:

       ; c(s     np vp) += 10.8; c(s) += 2*10.8;    
       ; c(s     np vp) += 1.2;   c(s) += 1*1.2;    

    but there may be exponentially

many parses of a length-n sentence!
    how can we stay fast?  similar to taggings   

600.465 - intro to nlp - j. eisner

advp

s

10.8

today

np

vp

stocks

v

prt

1.2

were up
s

np

vp

np

np

v

prt

today

stocks

were

up

analogies to a , b

in pid18?

the id145 computation of a
day 1: 2 cones
day 2: 3 cones

.  (b

is similar but works back from stop.)

day 3: 3 cones

start

=0.1

c

h
=0.1

=0.1*0.08+0.1*0.01

=0.009

=0.009*0.08+0.063*0.01

=0.00135

p(c|c)*p(3|c)
0.8*0.1=0.08

p(c|c)*p(3|c)
0.8*0.1=0.08

c

h

c

h

p(h|h)*p(3|h)
0.8*0.7=0.56

p(h|h)*p(3|h)
0.8*0.7=0.56

=0.1*0.07+0.1*0.56

=0.063

=0.009*0.07+0.063*0.56

=0.03591

call these a

h(2) and b

h(2)

h(3) and b

600.465 - intro to nlp - j. eisner

h(3)
35

a
   inside probabilities   

s

p(

vp

np
time
v
flies

pp
p
like

np

det
an

n
arrow

| s) = p(s     np vp | s) * p(np     time | np)

* p(vp     v pp | vp)
* p(v     flies | v) *    

    sum over all vp parses of    flies like an arrow   :
   vp(1,5) = p(flies like an arrow | vp)

    sum over all s parses of    time flies like an arrow   :

   s(0,5) = p(time flies like an arrow | s)

600.465 - intro to nlp - j. eisner

36

compute     bottom-up by cky

s

| s) = p(s     np vp | s) * p(np     time | np)

p(

vp

np
time
v
flies

pp
p
like

np

det
an

n
arrow

* p(vp     v pp | vp)
* p(v     flies | v) *    

   vp(1,5) = p(flies like an arrow | vp) =    
   s(0,5) = p(time flies like an arrow | s)

=    np(0,1) *    vp(1,5) * p(s     np vp|s) +    

600.465 - intro to nlp - j. eisner

37

compute     bottom-up by cky
time 1 flies 2

an 4 arrow 5

like 3

np 3
vst 3

np 10
8
s
13
s

np 4
vp 4

0

1

2
3
4

p 2
v 5

det 1

np 24
np 24
22
s
s
27
27
s

np 18
21
s
vp 18
pp 12
vp 16
np 10
n 8

1   s     np vp
6   s     vst np
2   s     s pp
1   vp     v np
2   vp     vp pp
1   np     det n
2   np     np pp
3   np     np np
0   pp     p np

compute     bottom-up by cky
time 1 flies 2

an 4 arrow 5

like 3

np 2-3
vst 2-3

np  2-10
s     2-8
s     2-13

np  2-24
np  2-24
s     2-22
s     2-27
s     2-27
s     2-22

np 2-4
vp   2-4

p 2-2
v 2-5

np  2-18
s     2-21
vp  2-18
pp  2-12
vp  2-16
det 2-1 np  2-10
n 2-8

0

2
3
4

2-1 s     np vp
2-6 s     vst np
2-2 s     s pp
2-1 vp     v np
2-2 vp     vp pp
2-1 np     det n
2-2 np     np pp
2-3 np     np np
2-0 pp     p np

compute     bottom-up by cky
time 1 flies 2

an 4 arrow 5

like 3

np 2-3
vst 2-3

np  2-10
s     2-8
s     2-13

np 2-4
vp   2-4

p 2-2
v 2-5

0

2
3
4

np  2-24
np  2-24
s     2-22
s     2-27
s     2-27
s     2-22
s     2-27
np  2-18
s     2-21
vp  2-18
pp  2-12
vp  2-16
det 2-1 np  2-10
n 2-8

2-1 s     np vp
2-6 s     vst np
2-2 s     s pp
2-1 vp     v np
2-2 vp     vp pp
2-1 np     det n
2-2 np     np pp
2-3 np     np np
2-0 pp     p np

the efficient version: add as we go
time 1 flies 2

an 4 arrow 5

like 3

np 2-3
vst 2-3

np  2-10
s     2-8
s     2-13

np  2-24
np  2-24
s     2-22
s     2-27
s     2-27

np 2-4
vp   2-4

p 2-2
v 2-5

np  2-18
s     2-21
vp  2-18
pp  2-12
vp  2-16
det 2-1 np  2-10
n 2-8

0

2
3
4

2-1 s     np vp
2-6 s     vst np
2-2 s     s pp
2-1 vp     v np
2-2 vp     vp pp
2-1 np     det n
2-2 np     np pp
2-3 np     np np
2-0 pp     p np

the efficient version: add as we go
time 1 flies 2

an 4 arrow 5

like 3

np 2-3
vst 2-3

np  2-10
s     2-8
+2-13

   s(0,2)

0
   s(0,2)*    pp(2,5)*p(s     s pp | s)

np 2-4
vp   2-4

p 2-2
v 2-5

2
3
4

np  2-24
+2-24
s     2-22
+2-27
+2-27
+2-22
+2-27
np  2-18
s     2-21
vp  2-18
pp  2-12
vp  2-16
det 2-1 np  2-10
n 2-8

   pp(2,5)

   s(0,5)

2-1 s     np vp
2-6 s     vst np
2-2 s     s pp
2-1 vp     v np
2-2 vp     vp pp
2-1 np     det n
2-2 np     np pp
2-3 np     np np
2-0 pp     p np

compute     probs bottom-up (cky)
need some initialization up here for the width-1 case
for width := 2 to n

(* build smallest first *)

for i := 0 to n-width
let k := i + width
for j := i+1 to k-1

(* start *)

(* end *)

(* middle *)

for all grammar rules x     y z

   x(i,k) += p(x     y z | x) *    y(i,j) *    z(j,k)

x

y z

what if you
changed + to
max?
600.465 - intro to nlp - j. eisner

k

j

i

what if you
replaced all rule
probabilities by 1?

43

inside & outside probabilities

s

vp

np
time

v
flies

vp np
today
pp
p
like

np

det
an

n
arrow

   vp(1,5) = p(time vp today | s)

   vp(1,5) = p(flies like an arrow | vp)

   vp(1,5) *    vp(1,5)
= p(time [vp flies like an arrow] today | s)

600.465 - intro to nlp - j. eisner

44

inside & outside probabilities

s

vp

np
time

v
flies

vp np
today
pp
p
like

np

   vp(1,5) = p(time vp today | s)

   vp(1,5) = p(flies like an arrow | vp)

det
an

n
arrow

   vp(1,5) *    vp(1,5)
= p(time flies like an arrow today & vp(1,5) | s)

/    s(0,6)

p(time flies like an arrow today | s)

= p(vp(1,5) | time flies like an arrow today, s)

600.465 - intro to nlp - j. eisner

45

inside & outside probabilities

s

vp

np
time

v
flies

vp np
today
pp
p
like

np

det
an

n
arrow

   vp(1,5) = p(time vp today | s)

   vp(1,5) = p(flies like an arrow | vp)

strictly analogous

to forward-backward
in the finite-state case!

so    vp(1,5) *    vp(1,5) /    s(0,6)
is id203 that there is a vp here,
given all of the observed data (words)
600.465 - intro to nlp - j. eisner

start

c

h

c

h

c

h

46

inside & outside probabilities

s

vp

np
time

v
flies

vp np
today
pp
p
like

np

det
an

n
arrow

   vp(1,5) = p(time vp today | s)

   v(1,2) = p(flies | v)
   pp(2,5) = p(like an arrow | pp)

so    vp(1,5) *    v(1,2) *    pp(2,5) /    s(0,6)
is id203 that there is vp     v pp here,
given all of the observed data (words)
600.465 - intro to nlp - j. eisner

    or is it?

47

inside & outside probabilities

s

vp

np
time

v
flies

vp np
today
pp
p
like

np

det
an

n
arrow

   vp(1,5) = p(time vp today | s)

strictly analogous

to forward-backward
in the finite-state case!

start

c

h

c

h

   v(1,2) = p(flies | v)
   pp(2,5) = p(like an arrow | pp)
sum prob over all position triples like (1,2,5) to
get expected c(vp     v pp); reestimate pid18!

so    vp(1,5) * p(vp     v pp) *    v(1,2) *    pp(2,5) /    s(0,6)
is id203 that there is vp     v pp here (at 1-2-5),
given all of the observed data (words)
600.465 - intro to nlp - j. eisner

48

compute     probs bottom-up
(gradually build up larger blue    inside    regions)

summary:    vp(1,5) += p(v pp | vp) *    v(1,2) *    pp(2,5)

inside 1,5

inside 1,2

inside 2,5

p(flies like an arrow | vp)
p(v pp | vp)

+=
*

p(flies | v)

*

p(like an arrow | pp)

600.465 - intro to nlp - j. eisner

   vp(1,5)

vp

   v(1,2)

v
flies

pp

   pp(2,5)

p
like

np

det
an

n
arrow
49

compute     probs top-down

summary:    v(1,2) += p(v pp | vp) *    vp(1,5) *    pp(2,5)

(uses     probs as well)

outside 1,2

outside 1,5

inside 2,5

s

(gradually build up larger
pink    outside    regions)

p(time v like an arrow today | s)

np
time

vp

   vp(1,5)

vp

np
today

   v(1,2)

+= p(time vp today | s)
* p(v pp | vp)
* p(like an arrow | pp)

600.465 - intro to nlp - j. eisner

v
flies

pp

   pp(2,5)

p
like

np

det
an

n
arrow
50

compute     probs top-down

summary:    pp(2,5) += p(v pp | vp) *    vp(1,5) *    v(1,2)

(uses     probs as well)

outside 2,5

s

outside 1,5

inside 1,2

np
time

vp

   vp(1,5)

vp

np
today

v
flies

   v(1,2)

pp

   pp(2,5)

p
like

np

p(time flies pp today | s)

+= p(time vp today | s)
* p(v pp | vp)

det
an
600.465 - intro to nlp - j. eisner

n
arrow

* p(flies| v)

51

details:
compute     probs bottom-up

when you build vp(1,5),
from vp(1,2) and vp(2,5)
during cky,
increment    vp(1,5) by
p(vp     vp pp) *

   vp(1,2) *    pp(2,5)

why?    vp(1,5) is total
id203 of all derivations
p(flies like an arrow | vp)
and we just found another.
(see earlier slide of cky chart.)

600.465 - intro to nlp - j. eisner

   vp(1,5)

vp

   vp(1,2)

vp
flies

pp

   pp(2,5)

p
like

np

det
an

n
arrow
52

details:
compute     probs bottom-up (cky)

for width := 2 to n

for i := 0 to n-width
let k := i + width
for j := i+1 to k-1

(* build smallest first *)

(* start *)

(* end *)

(* middle *)

for all grammar rules x     y z

   x(i,k) += p(x     y z) *    y(i,j) *    z(j,k)

x

y z

i

j

k

600.465 - intro to nlp - j. eisner

53

details:
compute     probs top-down (reverse cky)

n downto 2

   unbuild    biggest first
(* build smallest first *)

for width := 2 to n

for i := 0 to n-width
let k := i + width
for j := i+1 to k-1

   y(i,j) += ???
   z(j,k) += ???

x

y z

for all grammar rules x     y z

(* start *)

(* end *)

(* middle *)

i

j

k

600.465 - intro to nlp - j. eisner

54

details:
compute     probs top-down (reverse cky)

after computing     during cky,
revisit constits in reverse order (i.e.,
bigger constituents first).
when you    unbuild    vp(1,5)
from vp(1,2) and vp(2,5),
increment    vp(1,2) by
   vp(1,5) * p(vp     vp pp) *
   pp(2,5)
and increment    pp(2,5) by
   vp(1,5) * p(vp     vp pp) *
   vp(1,2)

s

np
time

vp

   vp(1,2)

vp

   vp(1,5)

vp

np
today
pp    pp(2,5)

   vp(1,2)

   pp(2,5)
already computed on

bottom-up pass

   vp(1,2) is total prob of all ways to gen vp(1,2) and all outside words.
600.465 - intro to nlp - j. eisner
55

details:
compute     probs top-down (reverse cky)

n downto 2

   unbuild    biggest first
(* build smallest first *)

for width := 2 to n

for i := 0 to n-width
let k := i + width
for j := i+1 to k-1

(* start *)

(* end *)

(* middle *)

for all grammar rules x     y z

   y(i,j) +=    x(i,k) * p(x     y z) *    z(j,k)
   z(j,k) +=    x(i,k) * p(x     y z) *    y(i,j)

x

y z

i

j

k

600.465 - intro to nlp - j. eisner

56

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where
3. viterbi version as an a* or pruning heuristic
4. as a subroutine within non-context-free models

what inside-outside is good for

1. as the e step in the em training algorithm

s

   

that   s why we just did it

12 today stocks were up

   
   

c(s) +=       i,j    s(i,j)      s(i,j)/z

advp

s

10.8

today

np

vp

stocks

v

prt

1.2

were up

s

c(s     np vp) +=    i,j,k    s(i,k)   p(s     np vp)

      np(i,j)       vp(j,k)/z

where
z = total prob of all parses =    s(0,n)

np

vp

np

np

v

prt

today

stocks

were

up

does unsupervised
learning work?

    merialdo (1994)

       the paper that freaked me out       

- kevin knight

    em always improves likelihood
    but it sometimes hurts accuracy
    why?#@!?

600.465 - intro to nlp - j. eisner

59

does unsupervised
learning work?

600.465 - intro to nlp - j. eisner

60

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where

   

posterior decoding of a single sentence
   
   

like using           to pick the most probable tag for each word
but can   t just pick most probable nonterminal for each span    
    wouldn   t get a tree!   (not all spans are constituents.)
so, find the tree that maximizes expected # correct nonterms.
   
for each nonterminal (or rule), at each position:
              tells you the id203 that it   s correct.
   

for a given tree, sum these probabilities over all positions to get
that tree   s expected # of correct nonterminals (or rules).

alternatively, expected # of correct rules.

   

   

    how can we find the tree that maximizes this sum?

    id145     just weighted cky all over again.
   

but now the weights come from           (run inside-outside first).

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where

   
   

posterior decoding of a single sentence
as soft features in a predictive classifier
   
   
   

you want to predict whether the substring from i to j is a name
feature 17 asks whether your parser thinks it   s an np
if you   re sure it   s an np, the feature fires
   
if you   re sure it   s not an np, the feature doesn   t fire
   
but you   re not sure!
    the chance there   s an np there is p =    np(i,j)      np(i,j)/z
    so add p        17 to the log-id203

add 0        17 to the log-id203

add 1          17 to the log-id203

   

   

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where

   
   
   

posterior decoding of a single sentence
as soft features in a predictive classifier
pruning the parse forest of a sentence
   
   

to build a packed forest of all parse trees, keep all backpointer pairs
can be useful for subsequent processing
   

provides a set of possible parse trees to consider for machine
translation, semantic interpretation, or finer-grained parsing
but a packed forest has size o(n3); single parse has size o(n)

   
to speed up subsequent processing, prune forest to manageable size
keep only constits with prob          /z     0.01 of being in true parse
   
    or keep only constits for which          /z     (0.01     prob of best parse)

   

i.e., do viterbi inside-outside, and keep only constits from parses
that are competitive with the best parse (1% as probable)

   

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where
3. viterbi version as an a* or pruning heuristic

   

   

   

call the resulting quantities    ,    instead of    ,       (as for id48)

viterbi inside-outside uses a semiring with max in place of +
   
prob of best parse that contains a constituent x is    (x)      (x)
   

suppose the best overall parse has prob p.  then all its constituents
have    (x)      (x)=p, and all other constituents have    (x)      (x) < p.
so if we only knew    (x)      (x) < p, we could skip working on x.

   
in the parsing tricks lecture, we wanted to prioritize or prune x
according to    p(x)   q(x).     we now see better what q(x) was:
   
   

p(x) was just the viterbi inside id203: p(x) =    (x)
q(x) was just an estimate of the viterbi outside prob: q(x)        (x).

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where
3. viterbi version as an a* or pruning heuristic

   
   

[continued]
q(x) was just an estimate of the viterbi outside prob: q(x)        (x).
if we could define q(x) =    (x) exactly, prioritization would first
   
process the constituents with maximum          , which are just the
correct ones!  so we would do no unnecessary work.
   

but to compute     (outside pass), we   d first have to finish
parsing (since     depends on     from the inside pass).  so this isn   t
really a    speedup   : it tries everything to find out what   s necessary.

   

but if we can guarantee q(x)        (x), get a safe a* algorithm.
    we can find such q(x) values by first running viterbi inside-outside
on the sentence using a simpler, faster, approximate grammar    

what inside-outside is good for

1. as the e step in the em training algorithm
2. predicting which nonterminals are probably where
3. viterbi version as an a* or pruning heuristic

   
   

[continued]
if we can guarantee q(x)        (x), get a safe a* algorithm.
    we can find such q(x) values by first running viterbi inside-

outside on the sentence using a faster approximate grammar.

0.6 s     np[sing] vp[sing]
0.3 s     np[plur] vp[plur]
0   s     np[sing] vp[plur]
0   s     np[plur] vp[sing]
0.1 s     vp[stem]

   

max

0.6 s     np[?] vp[?]

this    coarse    grammar ignores features and
makes optimistic assumptions about how they
will turn out.  few nonterminals, so fast.

now define qnp[sing](i,j) = qnp[plur](i,j) =    np[?](i,j)

what inside-outside is good for

predicting which nonterminals are probably where

1. as the e step in the em training algorithm
2.
3. viterbi version as an a* or pruning heuristic
4. as a subroutine within non-context-free models

   

   

    we   ve always defined the weight of a parse tree as the sum of its

rules    weights.
advanced topic: can do better by considering additional features
of the tree (   non-local features   ), e.g., within a log-linear model.
cky no longer works for finding the best parse.    
   

approximate    reranking    algorithm: using a simplified model that uses
only local features, use cky to find a parse forest.  extract the best
1000 parses.  then re-score these 1000 parses using the full model.
better approximate and exact algorithms: beyond scope of this
course.  but they usually call inside-outside or viterbi inside-outside as
a subroutine, often several times (on multiple variants of the
grammar, where again each variant can only use local features).

   

