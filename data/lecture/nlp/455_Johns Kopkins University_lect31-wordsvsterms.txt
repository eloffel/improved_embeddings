words vs. terms

words vs. terms

    information retrieval cares about    terms   
    you search for    em, google indexes    em
    query:

    what kind of monkeys live in costa rica?

600.465 - intro to nlp - j. eisner

1

600.465 - intro to nlp - j. eisner

words vs. terms

    what kind of monkeys live in costa rica?

    words?  
    content words?
    word stems?
    word clusters?
    multi-word phrases?
    thematic content?  (e.g., it   s a    habitat question   )

finding phrases (   collocations   )

    kick the bucket
    directed graph
    iambic pentameter
    osama bin laden
    united nations
    real estate
    quality control
    international best practice
        have their own meanings, translations, etc.

600.465 - intro to nlp - j. eisner

3

600.465 - intro to nlp - j. eisner

2

4

finding phrases (   collocations   )

finding phrases (   collocations   )

    just use common bigrams?
    doesn   t work:

    just use common bigrams?
    better correction - filter by tags: a n, n n, n p n    

    80871
    58841
    26430
       
    15494
       
    12622
    11428
    10007

of the
in the
to the

to be

from the
new york
he said

    11487
    7261
    5412
    3301
       
    1074
    1073
       

new york
united states
los angeles
last year

chief executive
real estate

    possible correction     just drop function words!

600.465 - intro to nlp - j. eisner

5

600.465 - intro to nlp - j. eisner

6

1

finding phrases (   collocations   )

    still want to filter out    new companies   
    these words occur together reasonably often but 

only because both are frequent

    do they occur more often 

[among a n pairs?]

than you would expect by chance?
    expect by chance: p(new) p(companies) 
    actually observed: p(new companies)
    mutual information
    binomial significance test

= p(new) p(companies | new)

data from manning & sch  tze textbook (14 million words of ny times)

(pointwise) mutual information

___ companies

new ___
8

   new ___
4,667
(   old companies   )

total
4,675

___    companies

15,820

total

15,828

14,303,001

14,287,181
(   old machines   )
14,291,848 14,307,676

    p(new companies) = p(new) p(companies) ?
    mi = log2 p(new companies) /  p(new)p(companies)

= log2

(8/n)         /((15828/n)(4675/n)) = log2 1.55 = 0.63

    mi > 0   if and only if   p(co   s | new) > p(co   s) > p(co   s |    new)
    here mi is positive but small.  would be larger for stronger collocations.

n

8

600.465 - intro to nlp - j. eisner

7

600.465 - intro to nlp - j. eisner

data from manning & sch  tze textbook (14 million words of ny times)

data from manning & sch  tze textbook (14 million words of ny times)

significance tests

___ companies

new ___
1

___    companies

1978

total

1979

   new ___
583
(   old companies   )

1,785,898
(   old machines   )
1,786,481

total
584

1,787,876

1,788,460

    sparse data.  in fact, suppose we divided all counts by 8:

    would mi change?
    no, yet we should be less confident it   s a real collocation.
    extreme case: what happens if 2 novel words next to each other?
    so do a significance test!  takes sample size into account.
600.465 - intro to nlp - j. eisner
9

binomial significance (   coin flips   )

___ companies
___    companies
total

new ___
8
15,820
15,828

   new ___
4,667

total
4,675
14,287,181 14,303,001
14,291,848 14,307,676

    assume we have 2 coins that were used when generating the text.
    following new, we flip coin a to decide whether companies is next.
    following    new, we flip coin b to decide whether companies is next.
    we can see that a was flipped 15828 times and got 8 heads.

    id203 of this: p8 (1-p)15820   *  15828!/8! 15820!

    we can see that b was flipped 14291848 times and got 4667 heads.
    our question: do the two coins have different weights?

(equivalently, are there really two separate coins or just one?)

600.465 - intro to nlp - j. eisner

10

data from manning & sch  tze textbook (14 million words of ny times)

data from manning & sch  tze textbook (14 million words of ny times)

binomial significance (   coin flips   )

binomial significance (   coin flips   )

___ companies
___    companies
total

new ___
8
15,820
15,828

   new ___
4,667

total
4,675
14,287,181 14,303,001
14,291,848 14,307,676

    null hypothesis: same coin

    assume pnull(co   s | new) = pnull(co   s |    new) = pnull(co   s) = 4675/14307676
    pnull(data) = pnull(8 out of 15828)*pnull(4667 out of 14291848) = .00042

    collocation hypothesis: different coins

    assume pcoll(co   s | new) = 8/15828, pcoll(co   s |    new) = 4667/14291848
    pcoll(data) = pcoll(8 out of 15828)*pcoll(4667 out of 14291848) = .00081

    so collocation hypothesis doubles p(data).

    we can sort bigrams by the log-likelihood ratio: log pcoll(data)/pnull(data)
    i.e., how sureare we that    companies    is more likely after    new   ?

600.465 - intro to nlp - j. eisner

11

___ companies
___    companies
total

new ___
1
1978
1979

   new ___
583
1,785,898
1,786,481

total
584
1,787,876
1,788,460

    null hypothesis: same coin

    assume pnull(co   s | new) = pnull(co   s |    new) = pnull(co   s) = 584/1788460
    pnull(data) = pnull(1 out of 1979)*pnull(583 out of 1786481) = .0056

    collocation hypothesis: different coins

    assume pcoll(co   s | new) = 1/1979, pcoll(co   s |    new) = 583/1786481
    pcoll(data) = pcoll(1 out of 1979)*pcoll(583 out of 1786418) = .0061
    collocation hypothesis still increases p(data), but only slightly now.

    if we don   t have much data, 2-coin model can   t be much better at explaining it.
    pointwise mutual information as strong as before, but based on much less data.

so it   s now reasonable to believe the null hypothesis that it   s a coincidence.
12

600.465 - intro to nlp - j. eisner

2

data from manning & sch  tze textbook (14 million words of ny times)

binomial significance (   coin flips   )

function vs. content words

___ companies
___    companies
total

new ___
8
15,820
15,828

   new ___
4,667

total
4,675
14,287,181 14,303,001
14,291,848 14,307,676

    null hypothesis: same coin

    assume pnull(co   s | new) = pnull(co   s |    new) = pnull(co   s) = 4675/14307676
    pnull(data) = pnull(8 out of 15828)*pnull(4667 out of 14291848) = .00042

    collocation hypothesis: different coins

    assume pcoll(co   s | new) = 8/15828, pcoll(co   s |    new) = 4667/14291848
    pcoll(data) = pcoll(8 out of 15828)*pcoll(4667 out of 14291848) = .00081

    does this mean that collocation hypothesis is twice as likely?

    no, as it   s far less probable a priori! (most bigrams ain   t collocations)
    bayes: p(coll | data) = p(coll) * p(data | coll) / p(data)   isn   t twice p(null | data)

    might want to eliminate function words, or reduce 

their influence on a search

    tests for content word:

    if it appears rarely?   

    no: c(beneath) < c(kennedy)     c(aside)    c(oil) in wsj

    if it appears in only a few documents?

    better: kennedy tokens are concentrated in a few docs
    this is traditional solution in ir

    if its frequency varies a lot among documents?

    best: content words come in bursts (when it rains, it pours?)
    id203 of kennedy is increased if kennedy appeared in 
preceding text     it is a    self-trigger    whereas beneath isn   t

600.465 - intro to nlp - j. eisner

13

600.465 - intro to nlp - j. eisner

14

latent semantic analysis
    a trick from information retrieval

    each document in corpus is a length-k vector

    or each paragraph, or whatever

(0,    3,

3,

1,    0,

7,

. . .

1,    0) 

latent semantic analysis
    a trick from information retrieval

    each document in corpus is a length-k vector
    plot all documents in corpus

reduced-dimensionality plot

true plot in k dimensions

a single document

pretty sparse, so pretty noisy!

wish we could smooth it:

what would the document look like if it rambled on forever?
600.465 - intro to nlp - j. eisner

15

600.465 - intro to nlp - j. eisner

16

latent semantic analysis
    reduced plot is a perspective drawing of true plot
    it projects true plot onto a few axes
        a best choice of axes     shows most variation in the data.

    found by id202:    principal components analysis    (pca)

latent semantic analysis
    svd plot allows best possible reconstruction of true plot

(i.e., can recover 3-d coordinates with minimal distortion)

    ignores variation in the axes that it didn   t pick
    hope that variation   s just noise and we want to ignore it

reduced-dimensionality plot

true plot in k dimensions

reduced-dimensionality plot

true plot in k dimensions

600.465 - intro to nlp - j. eisner

17

theme a
600.465 - intro to nlp - j. eisner

18

b
 
e
m
e
h
t

3

latent semantic analysis
    svd finds a small number of theme vectors
    approximates each doc as linear combination of themes
    coordinates in reduced plot = linear coefficients

    how much of theme a in this document?   how much of theme b?
    each theme is a collection of words that tend to appear together

reduced-dimensionality plot

true plot in k dimensions

latent semantic analysis
    new coordinates might actually be useful for info retrieval
    to compare 2 documents, or a query and a document:

    project both into reduced space: do they have themes in common?  
    even if they have no words in common!

reduced-dimensionality plot

true plot in k dimensions

b
 
e
m
e
h
t

b
 
e
m
e
h
t

theme a
600.465 - intro to nlp - j. eisner

19

theme a
600.465 - intro to nlp - j. eisner

20

latent semantic analysis
    themes extracted for ir might help sense disambiguation

    each word is like a tiny document:  (0,0,0,1,0,0,   )
    express word as a linear combination of themes
    each theme corresponds to a sense? 

    e.g.,    jordan    has mideast and sports themes

(plus advertising theme, alas, which is same sense as sports)
    word   s sense in a document: which of its themes are strongest in 

the document?

    groups senses as well as splitting them

    one word has several themes and many words have same theme

latent semantic analysis
    a perspective on principal components analysis (pca)
    imagine an electrical circuit that connects terms to docs    

terms

1   2   3   4   5   6   7   8   9

matrix of strengths
(how strong is each

term in each document?)

1   2   3   4   5   6   7

documents

each connection has a

weight given by the matrix.

600.465 - intro to nlp - j. eisner

21

600.465 - intro to nlp - j. eisner

22

latent semantic analysis
    which documents is term 5 strong in?

latent semantic analysis
    which documents are terms 5 and 8 strong in?

terms

1   2   3   4   5   6   7   8   9

terms

1   2   3   4   5   6   7   8   9

docs 2, 5, 6 

light up strongest.

this answers a query

consisting of terms 5 and 8!

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

really just id127:

term vector (query) x strength matrix = doc vector  

600.465 - intro to nlp - j. eisner

23

600.465 - intro to nlp - j. eisner

24

4

latent semantic analysis
    conversely, what terms are strong in document 5?

terms

1   2   3   4   5   6   7   8   9

latent semantic analysis
    svd approximates by smaller 3-layer network

    forces sparse data through a bottleneck, smoothing it

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

gives doc 5   s coordinates!

themes

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

25

600.465 - intro to nlp - j. eisner

26

latent semantic analysis
    i.e., smooth sparse data by matrix approx: m     a b
    a encodes camera angle, b gives each doc   s new coords

terms

terms

latent semantic analysis
completely symmetric!  regard a, b as projecting terms and docs 
into a low-dimensional    theme space    where their similarity can be 
judged.

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

b

themes

matrix

m

a

b

themes

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

27

600.465 - intro to nlp - j. eisner

28

latent semantic analysis
    completely symmetric.  regard a, b as projecting terms and docs 

into a low-dimensional    theme space    where their similarity can 
be judged.

    cluster documents (helps sparsity problem!)
    cluster words
    compare a word with a doc
    identify a word   s themes with its senses

    sense disambiguation by looking at document   s senses

    identify a document   s themes with its topics

    topic categorization

if you   ve seen svd before    
    svd actually decomposes m = a d b    exactly
    a = camera angle (orthonormal); d diagonal; b    orthonormal

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

d

b   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

29

600.465 - intro to nlp - j. eisner

30

5

if you   ve seen svd before    
    keep only the largest j < k diagonal elements of d
    this gives best possible approximation to m using only j blue units

if you   ve seen svd before    
    keep only the largest j < k diagonal elements of d
    this gives best possible approximation to m using only j blue units

terms

terms

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

d

b   

matrix

m

a

d

b   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

31

600.465 - intro to nlp - j. eisner

32

political dimensions

if you   ve seen svd before    
    to simplify picture, can write m     a (db   ) = ab

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

b = 
db   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

    how should you pick j (number of blue units)?
    just like picking number of clusters:

    how well does system work with each j (on held-out data)?

600.465 - intro to nlp - j. eisner

33

600.465 - intro to nlp - j. eisner

34

dimensions of personality

600.465 - intro to nlp - j. eisner

35

6

