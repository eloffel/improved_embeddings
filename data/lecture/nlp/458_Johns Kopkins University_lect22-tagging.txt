part-of-speech tagging

a canonical finite-state task

600.465 - intro to nlp - j. eisner

1

the tagging task

the lead paint is unsafe

input:
output: the/det lead/n paint/n is/v unsafe/adj

    uses:

    text-to-speech (how do we pronounce    lead   ?)
    can write regexps like (det) adj* n+ over the output
    preprocessing to speed up parser (but a little dangerous)
    if you know the tag, you can back off to it in other tasks

600.465 - intro to nlp - j. eisner

2

why do we care?

the lead paint is unsafe

input:
output: the/det lead/n paint/n is/v unsafe/adj

    the first statistical nlp task
    been done to death by different methods
    easy to evaluate (how many tags are correct?)
    canonical finite-state task

    can be done well with methods that look at local context
    though should    really    do it by parsing!

600.465 - intro to nlp - j. eisner

3

degree of supervision

    supervised: training corpus is tagged by humans
    unsupervised: training corpus isn   t tagged
    partly supervised: training corpus isn   t tagged,

but you have a dictionary giving possible tags for
each word

    we   ll start with the supervised case and move to

decreasing levels of supervision.

600.465 - intro to nlp - j. eisner

4

current performance

the lead paint is unsafe

input:
output: the/det lead/n paint/n is/v unsafe/adj

    how many tags are correct?

    about 97% currently (for english)
    but baseline is already 90%

    baseline is performance of stupidest possible method
    tag every word with its most frequent tag
    tag unknown words as nouns

600.465 - intro to nlp - j. eisner

5

what should we look at?

correct tags

pn     verb    det   noun  prep noun    prep    det  noun
bill  directed   a    cortege  of  autos  through  the  dunes
pn      adj     det   noun  prep noun    prep    det  noun
verb   verb   noun verb
adj
prep
   ?

some possible tags for
each word (maybe more)

each unknown tag is constrained by its word
and by the tags to its immediate left and right.
but those tags are unknown too    

600.465 - intro to nlp - j. eisner

6

what should we look at?

correct tags

pn     verb    det   noun  prep noun    prep    det  noun
bill  directed   a    cortege  of  autos  through  the  dunes
pn      adj     det   noun  prep noun    prep    det  noun
verb   verb   noun verb
adj
prep
   ?

some possible tags for
each word (maybe more)

each unknown tag is constrained by its word
and by the tags to its immediate left and right.
but those tags are unknown too    

600.465 - intro to nlp - j. eisner

7

what should we look at?

correct tags

pn     verb    det   noun  prep noun    prep    det  noun
bill  directed   a    cortege  of  autos  through  the  dunes
pn      adj     det   noun  prep noun    prep    det  noun
verb   verb   noun verb
adj
prep
   ?

some possible tags for
each word (maybe more)

each unknown tag is constrained by its word
and by the tags to its immediate left and right.
but those tags are unknown too    

600.465 - intro to nlp - j. eisner

8

three finite-state approaches

    id87 (statistical)

real language   y

part-of-speech tags

(id165 model)

noisy channel   y     x

observed string   x

want to recover y from x

replace tags
with words

text

600.465 - intro to nlp - j. eisner

9

three finite-state approaches

1. id87 (statistical)

2. deterministic baseline tagger composed

with a cascade of fixup transducers

3. nondeterministic tagger composed with
a cascade of finite-state automata that
act as filters

600.465 - intro to nlp - j. eisner

10

review: noisy channel

real language   y

noisy channel   y     x

obseved string  x

p(y)

*

p(x | y)

=

p(x,y)

want to recover y   y from x   x
choose y that maximizes p(y | x) or equivalently p(x,y)

600.465 - intro to nlp - j. eisner

11

review: noisy channel

.o.

=

p(y)

*

p(x | y)

=

p(x,y)

note p(x,y) sums to 1.
suppose x=   c   ; what is best    y   ?

12

600.465 - intro to nlp - j. eisner

review: noisy channel

.o.

=

p(y)

*

p(x | y)

=

p(x,y)

600.465 - intro to nlp - j. eisner

suppose x=   c   ; what is best    y   ?

13

review: noisy channel

.o.

.o.

=

restrict just to
paths compatible
with output    c   

p(y)

*

p(x | y)

*

(x = x)?

=

p(x, y)

600.465 - intro to nlp - j. eisner

14

noisy channel for tagging

acceptor: p(tag sequence)
   markov model   

.o.

transducer: tags     words

   unigram replacement   

.o.

acceptor: the observed words

   straight line   
transducer: scores candidate tag seqs

on their joint id203 with obs words;

=

pick best path

600.465 - intro to nlp - j. eisner

p(y)

*

p(x | y)

*

(x = x)?

=

p(x, y)

15

markov model (bigrams)

start

det

adj

verb

prep

noun

stop

600.465 - intro to nlp - j. eisner

16

markov model

det

0.3

0.7

start

adj

0.4

0.5

0.1

verb

prep

noun

stop

600.465 - intro to nlp - j. eisner

17

markov model

0.8

start

det

0.3

0.7

adj

0.4

0.5

0.1

verb

prep

noun

0.2

stop

600.465 - intro to nlp - j. eisner

18

markov model
p(tag seq)

0.8

start

det

0.3

0.7

verb

prep

adj

0.4

0.5

0.1

noun

0.2

stop

start det adj adj noun stop = 0.8 * 0.3 * 0.4 * 0.5 * 0.2
600.465 - intro to nlp - j. eisner
19

markov model as an fsa
p(tag seq)

verb

0.8

start

det

0.3

0.7

adj

0.4

0.5

0.1

prep

noun

0.2

stop

start det adj adj noun stop = 0.8 * 0.3 * 0.4 * 0.5 * 0.2
600.465 - intro to nlp - j. eisner
20

markov model as an fsa
p(tag seq)

verb

det 0.8

det

adj 0.3

start

noun
0.7

adj

adj 0.4

noun
0.5

    0.1

prep

noun

    0.2

stop

start det adj adj noun stop = 0.8 * 0.3 * 0.4 * 0.5 * 0.2
600.465 - intro to nlp - j. eisner
21

markov model (tag bigrams)
p(tag seq)

det 0.8

det

adj 0.3

start

adj

noun
0.5

adj 0.4

noun

    0.2

stop

start det adj adj noun stop = 0.8 * 0.3 * 0.4 * 0.5 * 0.2
600.465 - intro to nlp - j. eisner
22

noisy channel for tagging

automaton: p(tag sequence)
   markov model   

.o.

transducer: tags     words

   unigram replacement   

.o.

automaton: the observed words

   straight line   
transducer: scores candidate tag seqs

on their joint id203 with obs words;

=

pick best path

600.465 - intro to nlp - j. eisner

p(y)

*

p(x | y)

*

p(x | x)

=

p(x, y)

23

noisy channel for tagging

det

det 0.8

adj 0.3

start

noun
0.7

verb

prep

adj 0.4

adj

noun
0.5

    0.1

noun

    0.2

stop

.o.

   

noun:cortege/0.000001

noun:autos/0.001
noun:bill/0.002

det:a/0.6
det:the/0.4

.o.

adj:cool/0.003

adj:directed/0.0005
adj:cortege/0.000001

   

the

cool

directed

autos

=

transducer: scores candidate tag seqs

on their joint id203 with obs words;

we should pick best path

600.465 - intro to nlp - j. eisner

p(y)

*

p(x | y)

*

p(x | x)

=

p(x, y)

24

unigram replacement model

p(word seq | tag seq)

   

noun:cortege/0.000001

noun:autos/0.001
noun:bill/0.002

sums to 1

det:a/0.6

det:the/0.4

adj:cool/0.003

adj:directed/0.0005
adj:cortege/0.000001

   

600.465 - intro to nlp - j. eisner

sums to 1

25

det

det 0.8

start

adj 0.3

noun
0.7

verb

adj 0.4

adj

noun
0.5

    0.1

noun

    0.2

prep

stop

   

noun:cortege/0.000001

noun:autos/0.001
noun:bill/0.002

det:a/0.6
det:the/0.4

adj:cool/0.003

adj:directed/0.0005
adj:cortege/0.000001

   

compose

p(tag seq)

det 0.8

det

adj 0.3

start

adj

noun
0.5

adj 0.4

verb

prep

noun

    0.2

stop

600.465 - intro to nlp - j. eisner

26

compose

det

det 0.8

start

adj 0.3

noun
0.7

verb

adj 0.4

adj

noun
0.5

    0.1

noun

    0.2

prep

stop

   

noun:cortege/0.000001

noun:autos/0.001
noun:bill/0.002

det:a/0.6
det:the/0.4

adj:cool/0.003

adj:directed/0.0005
adj:cortege/0.000001

   

p(word seq, tag seq) = p(tag seq) * p(word seq | tag seq)

det:a 0.48
det:the 0.32
start

verb

adj:cool 0.0009
adj:directed 0.00015
adj:cortege 0.000003

prep

det

adj

adj:cool 0.0012
adj:directed 0.00020
adj:cortege 0.000004
600.465 - intro to nlp - j. eisner

noun

n:cortege
n:autos

   

stop

27

observed words as straight-line fsa

word seq

the

cool

directed

autos

600.465 - intro to nlp - j. eisner

28

compose with the

cool

directed

autos

p(word seq, tag seq) = p(tag seq) * p(word seq | tag seq)

det:a 0.48
det:the 0.32
start

verb

adj:cool 0.0009
adj:directed 0.00015
adj:cortege 0.000003

prep

det

adj

adj:cool 0.0012
adj:directed 0.00020
adj:cortege 0.000004
600.465 - intro to nlp - j. eisner

noun

n:cortege
n:autos

   

stop

29

compose with

the

cool

directed

autos

p(word seq, tag seq) = p(tag seq) * p(word seq | tag seq)

verb

det

adj:cool 0.0009

det:the 0.32
start

prep

adj
why did this
loop go away?

adj:directed 0.00020

adj

600.465 - intro to nlp - j. eisner

noun

   

stop

n:autos

30

the best path:
start det  adj   adj         noun stop = 0.32 * 0.0009    

the  cool  directed  autos

p(word seq, tag seq) = p(tag seq) * p(word seq | tag seq)

det:the 0.32
start

det

adj

verb

adj:cool 0.0009

prep

adj:directed 0.00020

adj

600.465 - intro to nlp - j. eisner

noun

   

stop

n:autos

31

in fact, paths form a    trellis   

p(word seq, tag seq)

det
adj

det
adj

det
adj:directed   
adj

det
adj

start

stop

noun

noun

noun

noun

the best path:
start det  adj   adj         noun stop = 0.32 * 0.0009    

the  cool  directed  autos

600.465 - intro to nlp - j. eisner

32

the trellis shape emerges from the cross-product
construction for finite-state composition

0

0

0,0

1

1

1,1

2,1

3,1

3

2

   
   
   
.o.

2

=

1,2

2,2

3,2

4

3

4

all paths here are 4 words

1,3

2,3

3,3

   
   
   

1,4

2,4

3,4

4,4

so all paths here must have 4 words on output side

600.465 - intro to nlp - j. eisner

33

actually, trellis isn   t complete

p(word seq, tag seq)
trellis has no det     det or det    stop arcs; why?

det
adj

det
adj

det
adj:directed   
adj

det
adj

start

stop

noun

noun

noun

noun

the best path:
start det  adj   adj         noun stop = 0.32 * 0.0009    

the  cool  directed  autos

600.465 - intro to nlp - j. eisner

34

actually, trellis isn   t complete

p(word seq, tag seq)
lattice is missing some other arcs; why?

det
adj

det
adj

det
adj:directed   
adj

det
adj

start

stop

noun

noun

noun

noun

the best path:
start det  adj   adj         noun stop = 0.32 * 0.0009    

the  cool  directed  autos

600.465 - intro to nlp - j. eisner

35

actually, trellis isn   t complete

p(word seq, tag seq)
lattice is missing some states; why?

det

start

adj

adj:directed   
adj

stop

noun

noun

noun

the best path:
start det  adj   adj         noun stop = 0.32 * 0.0009    

the  cool  directed  autos

600.465 - intro to nlp - j. eisner

36

find best path from start to stop

det
adj

det
adj

det
adj:directed   
adj

det
adj

start

stop

noun

noun

noun

noun

    use id145     like prob. parsing:

    what is best path from start to each node?
    work from left to right
    each node stores its best path from start (as

id203 plus one backpointer)

    special acyclic case of dijkstra   s shortest-path alg.
    faster if some arcs/states are absent

600.465 - intro to nlp - j. eisner

37

in summary
    we are modeling p(word seq, tag seq)
    the tags are hidden, but we see the words
    is tag sequence x likely with these words?
    id87 is a    hidden markov model   :

probs
from tag
bigram
model

0.4

0.6

start pn   verb    det     noun  prep noun   prep     det  noun

0.001

probs from
unigram
replacement
    find x that maximizes id203 product

600.465 - intro to nlp - j. eisner

38

bill  directed   a    cortege  of   autos  through  the  dunes

another viewpoint
    we are modeling p(word seq, tag seq)
    why not use chain rule + some kind of backoff?
    actually, we are!

det    
   

start pn verb

bill directed a

p(
= p(start) * p(pn | start) * p(verb | start pn) * p(det | start pn verb) *    
* p(a | bill directed, start pn verb det    ) *    

* p(bill | start pn verb    ) * p(directed | bill, start pn verb det    )

)

600.465 - intro to nlp - j. eisner

39

another viewpoint
    we are modeling p(word seq, tag seq)
    why not use chain rule + some kind of backoff?
    actually, we are!

det    
   

start pn verb

bill directed a

p(
= p(start) * p(pn | start) * p(verb | start pn) * p(det | start pn verb) *    
* p(a | bill directed, start pn verb det    ) *    

* p(bill | start pn verb    ) * p(directed | bill, start pn verb det    )

)

start pn   verb       det    noun  prep    noun     prep     det     noun stop

bill  directed    a      cortege  of   autos   through    the    dunes

600.465 - intro to nlp - j. eisner

40

three finite-state approaches

1. id87 (statistical)

2. deterministic baseline tagger composed

with a cascade of fixup transducers

3. nondeterministic tagger composed with
a cascade of finite-state automata that
act as filters

600.465 - intro to nlp - j. eisner

41

another fst paradigm:
successive fixups
    like successive markups but alter
    morphology
    phonology
    part-of-speech tagging
       

input

600.465 - intro to nlp - j. eisner

output

42

transformation-based tagging
(brill 1995)

figure from brill   s thesis

600.465 - intro to nlp - j. eisner

43

transformations learned

figure from brill   s thesis

baselinetag*
nn @    vb // to _
vbp @    vb // ... _

etc.

compose this
cascade of fsts.

gets a big fst that

does the initial
tagging and the
sequence of fixups

   all at once.   
44

600.465 - intro to nlp - j. eisner

initial tagging of oov words

figure from brill   s thesis

600.465 - intro to nlp - j. eisner

45

three finite-state approaches

1. id87 (statistical)

2. deterministic baseline tagger composed

with a cascade of fixup transducers

3. nondeterministic tagger composed with
a cascade of finite-state automata that
act as filters

600.465 - intro to nlp - j. eisner

46

variations

    multiple tags per word

    transformations to knock some of them out

    how to encode multiple tags and knockouts?

    use the above for partly supervised learning
    supervised: you have a tagged training corpus
    unsupervised: you have an untagged training corpus
    here: you have an untagged training corpus and a

dictionary giving possible tags for each word

600.465 - intro to nlp - j. eisner

47

