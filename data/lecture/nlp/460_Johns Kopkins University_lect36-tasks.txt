nlp tasks and applications

600.465 - intro to nlp - j. eisner

1

the nlp research community
    papers
    acl anthology has nearly everything, free!

    over 36,000 papers!
    free-text searchable

    great way to learn about current research on a topic
    new search interfaces currently available in beta
    find recent or highly cited work; follow citations

    used as a dataset by various projects

    analyzing the text of the papers (e.g., parsing it)
    extracting a graph of papers, authors, and institutions
(who wrote what? who works where? what cites what?)

the nlp research community
    conferences
    most work in nlp is published as 8-page conference 
papers with 3 double-blind reviewers.
    main annual conferences: acl, emnlp, naacl
    also eacl, ijcnlp, coling
    + various specialized conferences and workshops
    big events, and growing fast!  acl 2015:
    about 1500 attendees
    692 full-length papers submitted (173 accepted)
    648 short papers submitted (145 accepted)
    14 workshops on various topics

the nlp research community
    institutions
    universities: many have 2+ nlp faculty
    several    big players    with many faculty
    some of them also have good linguistics, 
cognitive science, machine learning, ai
    old days: at&t bell labs, ibm
    now: google, microsoft, ibm, many startups    
    speech: nuance,    
    machine translation: language weaver, systran,    
    many niche markets     online reviews, medical transcription, 
news summarization, legal search and discovery    

    companies:

the nlp research community
    standard tasks
    if you want people to work on your problem, 
make it easy for them to get started and to 
measure their progress.  provide:
    test data, for evaluating the final systems
    development data, for measuring whether a change 
to the system helps, and for tuning parameters
    an evaluation metric (formula for measuring how 
well a system does on the dev or test data)
    a program for computing the evaluation metric
    labeled training data and other data resources
    a prize?     with clear rules on what data can be used

the nlp research community
    software
    lots of people distribute code for these tasks
    some lists of software, but no central site    
    some end-to-end pipelines for text analysis

    or you can email a paper   s authors to ask for their code

       one-stop shopping   
    cleanup/id121 + morphology + tagging + parsing +    
    nltk is easy for beginners and has a free book (intersession?)
    gate has been around for a long time and has a bunch of 
modules 

the nlp research community
    software
    to find good or popular tools:
    still, often hard to identify the best tool for your job:

    search current papers, ask around, use the web
    produces appropriate, sufficiently detailed output?
    accurate?  (on the measure you care about)
    robust?  (accurate on your data, not just theirs)
    fast?
    easy and flexible to use?  nice file formats, command line 
options, visualization?
    trainable for new data and languages?  how slow is training?
    open-source and easy to extend?

the nlp research community
    datasets

    raw text or speech corpora

    text or speech with manual or automatic annotations

    or just their id165 counts, for super-big corpora
    various languages and genres
    usually there   s some metadata (each document   s date, author, etc.)
    sometimes     licensing restrictions (proprietary or copyright data)
    what kind of annotations?  that   s the rest of this lecture    
    may include translations into other languages
    morphological, semantic, translational, evolutionary

    words and their relationships
    grammars
    world atlas of linguistic structures
    parameters of statistical models (e.g., grammar weights)

the nlp research community
    datasets

    read papers to find out what datasets others are using
    linguistic data consortium (searchable) hosts many large datasets
    many projects and competitions post data on their websites
    but sometimes you have to email the author for a copy
    corpora mailing list is also good place to ask around
    lrec conference publishes papers about new datasets & metrics
    amazon mechanical turk     pay humans (very cheaply) to annotate 
your data or to correct automatic annotations 
    old task, new domain: annotate parses etc. on yourkind of data
    new task: annotate something new that you want your system to find
    auxiliary task: annotate something new that your system may benefit 
from finding (e.g., annotate subjunctive mood to improve translation)

    can you make annotation so much fun or so worthwhile
that they   ll do it for free?

the nlp research community
    standard data formats
    often just simple ad hoctext-file formats
    some standards:

    documented in a readme; easily read with scripts
    unicode     strings in any language (see icu toolkit)
    pcm (.wav, .aiff)     uncompressed audio
    bwf and aup extend w/metadata; also many compressed formats
    xml     documents with embedded annotations
    text encoding initiative     faithful digital representations of 
printed text
    protocol buffers, json     structured data
    uima        unstructured information management   ; watson uses it
    standoff markup: raw text in one file, annotations in 
other files (       noun phrase from byte 378   392   )
    annotations can be independently contributed & distributed

the nlp research community
    survey articles
    may help you get oriented in a new area 
    synthesis lectures on human language technologies
    handbook of natural language processing
    oxford handbook of computational linguistics
    foundations & trends in machine learning
    survey articles in journals     jair, cl, jmlr
    acm computing surveys?
    online tutorial papers
    slides from tutorials at conferences
    textbooks

to write a typical paper
    need some of these ingredients:
    a domain of inquiry
    a task
    resources
    a method for training & testing
    an algorithm
    analysis of results

input & output representations, evaluation metric
derived from a model?
comparison to baselines & other systems,
significance testing, learning curves, 
ablation analysis, error analysis
    there are other kinds of papers too: theoretical papers on formal 
grammars and their properties, new error metrics, new tasks or 
resources, etc.

scientific or engineering question
corpora, annotations, dictionaries,    

text annotation tasks
1. classify the entire document

(   text categorization   )

600.465 - intro to nlp - j. eisner

13

sentiment classification
?

what features of the text could help predict # of stars?
(e.g., using a log-linear model)   how to identify more?
are the features hard to compute?  (syntax? sarcasm?)

600.465 - intro to nlp - j. eisner

example from amazon.com, thanks to delip rao

14

other text categorization tasks
    is it spam?  (see features)
    what medical billing code for this visit?
    what grade, as an answer to this essay question?
    is it interesting to this user?
    news filtering; helpdesk routing
    is it interesting to this nlp program?
    if it   s spanish, translate it from spanish
    if it   s subjective, run the sentiment classifier
    if it   s an appointment, run information extraction
    which mail folder?  (work, friends, junk, urgent ...)
    yahoo! / open directory / digital libraries
600.465 - intro to nlp - j. eisner

    where should it be filed?

15

measuring performance
    classification accuracy: what % of 
messages were classified correctly?
    is this what we care about?
accuracy accuracy 
overall 
on spam accuracy 
on gen
90%
99.99%
95%
99.99%
90%
95%
    which system do you prefer?

system 1
system 2

600.465 - intro to nlp - j. eisner

16

precision vs. recall of 
good (non-spam) email

measuring performance
    precision = 
good messages kept
all messages kept
    recall =
good messages kept
all good messages

100%
75%
50%
25%
0%

100%

75%

25%

0%

i

i

n
o
s
c
e
r
p

50%
recall

move from high precision to high recall by 

deleting fewer messages (delete only if spamminess > high threshold)

600.465 - intro to nlp - j. eisner

17

measuring performance

precision vs. recall of 
good (non-spam) email

ok for search engines 
(users only want top 10)

would prefer 
to be here!

n
o
s

i

i

c
e
r
p

high threshold:
100%
all we keep is good,
but we don   t keep much
75%
50%
25%
0%

point where
precision=recall
(occasionally 
reported)
25%

0%

600.465 - intro to nlp - j. eisner

low threshold:

keep all the good stuff,
but a lot of the bad too

75%

100%

50%
recall

ok for spam 
filtering and 
legal search
18

i

i

n
o
s
c
e
r
p

100%
75%
50%
25%
0%

precision vs. recall of 
good (non-spam) email

measuring performance
    precision = 
good messages kept
all messages kept
    recall =
good messages kept
all good messages
(             )-1
    f-measure =

move from high precision to high recall by 
deleting fewer messages (raise threshold)
conventional to tune system and threshold to optimize f-measure on dev data
but it   s more informative to report the whole curve
since in real life, the user should be able to pick a tradeoff point they like
600.465 - intro to nlp - j. eisner

another system: better 
for some users, worse for 
others (can   t tell just by 
comparing f-measures)
25%
75%

precision-1 + recall-1

50%
recall

2

100%

19

0%

supervised learning methods
    conditional id148 are a good hammer

    feature engineering: throw in enough features to fix most errors
    training: learn weights     such that in training data, the true 
answer tends to have a high id203
    test: output the highest-id203 answer
if the evaluation metric allows for partial credit, 
can do fancier things (   minimum-risk    training and decoding)
    the most popular alternatives are roughly similar
    id88, id166, mira, neural network,    
    these also learn a (usually linear) scoring function
    however, the score is not interpreted as a log-id203
    learner just seeks weights     such that in training data,
the desired answer has a higher score than the wrong answers

fancier perfomance metrics
    for multi-way classifiers:
    average accuracy (or precision or recall) of 2-way 
distinctions: sports or not, news or not, etc.
    better, estimate the cost of different kindsof errors
    e.g., how bad is each of the following?

    putting sports articles in the news section
    putting fashion articles in the news section
    putting news articles in the fashion section
    now tune system to minimize total cost

    for ranking systems:

which articles are most sports-like?
which articles / webpages most relevant?

    correlate with human rankings?
    get active feedback from user?
    measure user   s wasted time by tracking clicks?
600.465 - intro to nlp - j. eisner

21

supervised learning methods

    easy to build a    yes    or    no    predictor from supervised training data

    plenty of software packages to do the learning & prediction
    lots of people in nlp never go beyond this    

    similarly, easy to build a system that chooses from a small finite set
    basically the same deal
    but runtime goes up linearly with the size of the set, unless you   re clever 

(hw3)

text annotation tasks
1. classify the entire document
2. classify individual word tokens

600.465 - intro to nlp - j. eisner

23

p(class | token in context) 

(wsd)

build a special classifier just for tokens of    plant   

slide courtesy of d. yarowsky

p(class | token in context) 

wsd for

build a special classifier just for tokens of    sentence   

slide courtesy of d. yarowsky

p(class | token in context) 

slide courtesy of d. yarowsky

p(class | token in context) 

slide courtesy of d. yarowsky

p(class | token in context) 

slide courtesy of d. yarowsky

p(class | token in context) 

slide courtesy of d. yarowsky

p(class | token in context) 

slide courtesy of d. yarowsky

what features?  example:    word to left   

slide courtesy of d. yarowsky (modified)

id147 using an 
id165 language model 
(n     2) would use words to 
left and right to help 
predict the true word.
similarly, an id48 would 
predict a word   s class using 
classes to left and right.
but we   d like to throw in all 
kinds of other features, 
too    

600.465 - intro to nlp - j. eisner

31

an assortment of possible cues ...

slide courtesy of d. yarowsky (modified)

generates a whole bunch 
of potential cues     use 
data to find out which 
ones work best
600.465 - intro to nlp - j. eisner

32

an assortment of possible cues ...

slide courtesy of d. yarowsky (modified)

this feature is 
relatively 
weak, but weak 
features are 
still useful, 
especially since 
very few 
features will 
fire in a given 
context.

33

merged ranking
of all cues 
of all these types
600.465 - intro to nlp - j. eisner

final decision list for lead(abbreviated)

slide courtesy of d. yarowsky (modified)

list of all features,
ranked by their weight.
(these weights are for a simple 
   decision list    model where the 
single highest-weighted feature 
that fires gets to make the 
however, a log-linear model, 
which adds up the weights of all 
features that fire, would be 

decision all by itself.

roughly similar.)
600.465 - intro to nlp - j. eisner

34

id52
    we could treat tagging as a token classification problem

    tag each word independently given features of context
    and features of the word   s spelling (suffixes, capitalization)

600.465 - intro to nlp - j. eisner

35

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier

nnp

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

dt

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

nn

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

cc

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

to

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vb

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

prp

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

in

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

dt

slide from ray mooney

sequence labeling as 

classification

    classify each token independently but use 
as input features, information about the 
surrounding tokens (sliding window).
john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier

nn

slide from ray mooney

id52
    or we could use an id48: 

det
adj
noun
0.4

det
det
adj
adj
adj:directed   
noun
noun
0.6

det
adj
noun

stop

start

probs
from tag
bigram
model

start pn   verb    det     noun  prep noun   prep     det  noun 
bill  directed   a    cortege  of   autos  through  the  dunes

0.001

probs from
unigram
replacement
600.465 - intro to nlp - j. eisner

48

id52
    we could treat tagging as a token classification problem

    tag each word independently given features of context
    and features of the word   s spelling (suffixes, capitalization)

    or we could use an id48: 

    the point of the id48 is basically that the tag of one word might 
depend on the tags of adjacent words.

    combine these two ideas??

    we   d like rich features (e.g., in a log-linear model), but we   d also like 
our feature functions to depend on adjacent tags.
    so, the problem is to predict all tags together.

600.465 - intro to nlp - j. eisner

49

supervised learning methods

    easy to build a    yes    or    no    predictor from supervised training data

    plenty of software packages to do the learning & prediction
    lots of people in nlp never go beyond this    

    similarly, easy to build a system that chooses from a small finite set

    basically the same deal
    but runtime goes up linearly with the size of the set, unless you   re clever (hw3)
    harder to predict the best string or tree (set is exponentially large or infinite)

id52
    idea #1

    classify tags one at a time from left to right
    each feature function can look at the context of the word being 
tagged, including the tags of all previous words

600.465 - intro to nlp - j. eisner

51

forward classification

john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier

nnp

slide from ray mooney

forward classification

nnp
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

forward classification

nnp  vbd
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

dt

slide from ray mooney

forward classification

nnp vbd dt
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

nn

slide from ray mooney

forward classification

nnp vbd dt  nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

cc

slide from ray mooney

forward classification

nnp vbd dt nn  cc
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

forward classification

nnp vbd dt nn  cc    vbd
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

to

slide from ray mooney

forward classification

nnp vbd dt nn  cc    vbd   to
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vb

slide from ray mooney

forward classification

nnp vbd dt nn  cc    vbd   to  vb
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

prp

slide from ray mooney

forward classification

nnp vbd dt nn  cc    vbd   to  vb prp
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

in

slide from ray mooney

forward classification

nnp vbd dt nn  cc    vbd   to  vb prp  in
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

dt

slide from ray mooney

forward classification

nnp vbd dt nn  cc    vbd   to  vb prp  in  dt
john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier

nn

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier

nn

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

dt

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

dt   nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

in

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

in   dt     nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

prp

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

prp in  dt   nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vb

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

vb  prp in  dt   nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

to

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

to  vb  prp in  dt   nn 
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

vbd   to  vb  prp in  dt   nn 
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

cc

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

cc    vbd   to  vb  prp in  dt   nn 
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

vbd  cc   vbd   to  vb  prp in  dt   nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

dt

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

dt vbd  cc  vbd   to  vb  prp in  dt   nn
john  saw the  saw  and  decided  to  take  it     to   the   table.

classifier

vbd

slide from ray mooney

backward classification
    disambiguating    to    in this case would be 
even easier backward.

vbd dt vbd cc   vbd   to  vb  prp in  dt   nn 
john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier

nnp

slide from ray mooney

id52
    idea #1

    classify tags one at a time from left to right
    p(tag | wordseq, prevtags) = (1/z) exp score(tag, wordseq, prevtags)
    where z sums up exp score(tag   , wordseq, prevtags) over all possible tags
    each feature function can look at the context of the word being 
tagged, including the tags of all previous words
    asymmetric: can   t look at following tags, only preceding ones
    idea #2 (   maximum id178 markov model (memm)   )
    same model, but don   t commit to a tag before we predict 
the next tag.  instead, consider probabilities of all tag sequences.

600.465 - intro to nlp - j. eisner

76

maximum id178 markov model

is this a probable tag sequence for this sentence?
nnp vbd dt nn  cc    vbd   to  vb prp  in  dt   nn
john  saw the  saw  and  decided  to  take  it     to   the   table.
classifier
classifier

classifier

classifier

classifier

classifier

classifier

classifier

classifier

classifier

classifier

classifier

to

dt

cc

nn

nnp

vbd

prp
vbd
does each of these classifiers assign a high 
id203 to the desired tag?
is this the most likely sequence to get by rolling dice?
(does it maximize product of probabilities?)

vb

dt

in

nn

slide adapted from 
ray mooney

id52
    idea #1

    classify tags one at a time from left to right
    p(tag | wordseq, prevtags) = (1/z) exp score(tag, wordseq, prevtags)
    where z sums up exp score(tag   , wordseq, prevtags) over all possible tags
    each feature function can look at the context of the word being 
tagged, including the tags of all previous words
    asymmetric: can   t look at following tags, only preceding ones
    idea #2 (   maximum id178 markov model (memm)   )
    same model, but don   t commit to a tag before we predict 
the next tag.  instead, consider probabilities of all tag sequences.
    use id145 to find the most probable sequence
    for id145 to work, features can only consider 
the (n-1) previous tags, just as in an id48
    same algorithms as in an id48, but now transition id203 
is p(tag | previous n-1 tags and all words)
    still asymmetric: can   t look at following tags

600.465 - intro to nlp - j. eisner

78

id52
    idea #1

    classify tags one at a time from left to right

    idea #2 (   maximum id178 markov model (memm)   )
    idea #3 (   linear-chain conditional random field (crf)   )

    p(tag | wordseq, prevtags) = (1/z) exp score(tag, wordseq, prevtags)
    where z sums up exp score(tag   , wordseq, prevtags) over all possible tags
    same model, but don   t commit to a tag before we predict 
the next tag.  instead, evaluate id203 of every tag sequence.
    this version is symmetric, and very popular.
    score each tag sequence as a whole, using arbitrary features
    p(tagseq | wordseq) = (1/z) exp score(tagseq, wordseq)
    where z sums up exp score(tagseq   , wordseq) over competing tagseqs
    can still compute z and best path using id145
    id145 works if, for example, each feature f(tagseq,wordseq) 
considers at most an id165 of tags. 
    then you can score a (tagseq,wordseq) pair with a wfst whose state 
remembers the previous (n-1) tags.
    as in #2, arc weight can consider the current tag id165 and all words.
    but unlike #2, arc weight isn   t a id203 (only normalize at the end).

600.465 - intro to nlp - j. eisner

79

supervised learning methods

    easy to build a    yes    or    no    predictor from supervised training data

    plenty of software packages to do the learning & prediction
    lots of people in nlp never go beyond this    

    similarly, easy to build a system that chooses from a small finite set

    basically the same deal
    but runtime goes up linearly with the size of the set, unless you   re clever (hw3)
    harder to predict the best string or tree (set is exponentially large or infinite)

    requires id145; you might have to write your own code
    but finite-state or crf toolkits will find the best string for you
    and you could modify someone else   s parser to pick the best tree
    an algorithm for picking the best can usually be turned into a learning algorithm

text annotation tasks
1. classify the entire document
2. classify individual word tokens
3. identify phrases (   chunking   )

600.465 - intro to nlp - j. eisner

81

id39
chicago (ap)     citing high fuel prices, united airlines said friday 
it has increased fares by $6 per round trip on flights to some cities 
also served by lower-cost carriers. american airlines, a unit amr, 
immediately matched the move, spokesman tim wagner said. 
united, a unit of ual, said the increase took effect thursday night 
and applies to most routes where it competes against discount 
carriers, such as chicago to dallas and atlanta and denver to san 
francisco, los angeles and new york.

5/3/2016

slide from jim martin

82

ne types

slide from jim martin

83

information extraction

as a task:

filling slots in a database from sub-segments of text.

october 14, 2002, 4:00 a.m. pt
for years, microsoft corporation ceo bill 
gates railed against the economic philosophy 
of open-source software with orwellian fervor, 
denouncing its communal licensing as a 
"cancer" that stifled technological innovation.
today, microsoft claims to "love" the open-
source concept, by which software code is 
made public to encourage improvement and 
development by outside programmers. gates 
himself says microsoft will gladly disclose its 
crown jewels--the coveted code behind the 
windows operating system--to select 
customers.
"we can be open source. we love the concept 
of shared source," said bill veghte, a 
microsoft vp. "that's a super-important shift 
for us in terms of code access.   
richard stallman, founder of the free 
software foundation, countered saying   

slide from chris brew, adapted from slide by william cohen

ie

name              title   organization
bill gates
bill veghte
richard stallman

microsoft
microsoft
free soft..

ceo
vp
founder

the semantic web
    a simple scheme for representing factual 
knowledge as a labeled graph
    [draw example with courses, students, their names 
and locations, etc.]
    many information extraction tasks aim to 
produce something like this
    is a labeled graph (triples) really enough?
        can transform k-tuples to triples
(cf. davidsonian event variable)
        supports facts about individuals, but no 
direct support for quantifiers or reasoning

phrase types to identify for ie

closed set

u.s. states
he was born in alabama   
the big wyoming sky   

complex pattern
u.s. postal addresses
university of arkansas
p.o. box 140
hope, ar  71802
headquarters:
1128 main street, 4th floor
cincinnati, ohio 45210

slide from chris brew, adapted from slide by william cohen

regular set
u.s. phone numbers
phone: (413) 545-1323
the cald main office can be 
reached at 412-268-1299
ambiguous patterns,
needing context and
many sources of evidence
person names
   was among the six houses 
sold by hope feldman that year.
pawel opalinski, software
engineer at whizbang labs.

identifying phrases
    a key step in ie is to identify relevant phrases
    named entities
    as on previous slides
    relationship phrases
       said   ,    according to   ,    
       was born in   ,    hails from   ,    
       bought   ,    hopes to acquire   ,    formed a joint agreement with   ,    
       syntactic chunking    sometimes done before (or instead of) parsing
    also,    segmentation   : divide chinese text into words (no spaces) 
    earlier, we built an fst to mark dates by inserting brackets
    but, it   s common to set this up as a tagging problem    

    so, how do we learn to mark phrases?

    simple syntactic chunks (e.g., non-recursive nps)

reduce to a tagging problem    
    the iob encoding (ramshaw & marcus 1995):
    b_x =    beginning    (first word of an x)
    i_x =    inside    (non-first word of an x)
    o =    outside    (not in any phrase)
    does not allow overlapping or recursive phrases
   united airlines said friday it has increased    
b_org   i_org        o        o         o   o            o
    the move  ,  spokesman tim wagner said     
o      o     o         o        b_per  i_per     o    
what if this were tagged as b_org instead?
slide adapted from chris brew

88

some simple ner features

pos tags and chunks 
from earlier processing

now predict ner tagseq

a feature of this 
tagseq might give a 
positive or negative 
weight to this 
b_org in 
conjunction with 
some subset of the 
nearby properties  

89

or even faraway properties: 
b_org is more likely in a 
sentence with a spokesman!  

slide adapted from jim martin

example applications for ie
    classified ads
    restaurant reviews
    bibliographic citations
    appointment emails
    legal opinions
    papers describing clinical medical studies
       

text annotation tasks
1. classify the entire document
2. classify individual word tokens
3. identify phrases (   chunking   )
4. syntactic annotation (parsing)

600.465 - intro to nlp - j. eisner

91

parser id74
    runtime
    exact match
    labeled precision, recall, f-measure of constituents
    easier versions:

    is the parse 100% correct?
    precision: you predicted (np,5,8); was it right?
    recall: (np,5,8) was right; did you predict it?
    unlabeled: don   t worry about getting (np,5,8) right, only (5,8)
    short sentences: only test on sentences of     15,     40,     100 words
    id33: labeled and unlabeled attachment accuracy
    you predicted (   ,5,8), but there was really a constituent (   ,6,10)

    crossing brackets

labeled id33
raw sentence
he reckons the current account deficit will narrow to only 1.8 billion in september.
pos-tagged sentence
he reckons the current account deficit will narrow to only 1.8 billion in september.
prp     vbz       dt       jj            nn          nn    md       vb     to    rb    cd     cd    in        nnp      .
word dependency parsed sentence
he reckons the current account deficit will narrow to only 1.8 billion in september .
subj

word id33

part-of-speech tagging

comp

mod

mod
spec

mod
comp
mod

subj
s-comp

root

slide adapted from yuji matsumoto

dependency trees

s

[head=thrill]

1. assign heads

vp

np

[head=plan]
n
detthe
[head=plan]
vp
nplan
[head=plan]

[head=thrill]
vhas
vp
[head=thrill]
vbeen
vp
[head=swallow]
[head=thrill]
to
vthrilling
npotto
vp
[head=thrill]
[head=swallow]
[head=otto]
npwanda
vswallow
[head=swallow] [head=wanda]

dependency trees

np

[head=plan]
n
detthe
[head=plan]
vp
nplan
[head=plan]

s

vp

[head=thrill]

2. each word is 
the head of a 
whole 
connected 
subgraph
[head=thrill]
vhas
vp
[head=thrill]
vbeen
vp
[head=swallow]
[head=thrill]
to
vthrilling
npotto
vp
[head=thrill]
[head=swallow]
[head=otto]
npwanda
vswallow
[head=swallow] [head=wanda]

dependency trees

s

detthe

np

nplan

n

vp

vhas

to

vp
vswallow

npwanda

2. each word is 
the head of a 
whole 
connected 
subgraph
vp

vp

vbeen

vp
vthrilling

npotto

dependency trees

the

plan

swallow
to

3. just look at 
which words are 

related

thrilling

has
been

wanda

otto

dependency trees
4. optionally 
flatten the 
drawing
    shows which words modify (   depend on   ) another word
    each subtree of the dependency tree is still a constituent
    but not all of the original constituents are subtrees (e.g., vp)

the plan to swallow wanda has been thrilling otto.
    easy to spot semantic relations (   who did what to whom?   )
    easy to annotate (high agreement)
    easy to evaluate (what % of words have correct parent?)

    good source of syntactic features for other tasks

supervised learning methods

    easy to build a    yes    or    no    predictor from supervised training data

    plenty of software packages to do the learning & prediction
    lots of people in nlp never go beyond this    

    similarly, easy to build a system that chooses from a small finite set

    basically the same deal
    but runtime goes up linearly with the size of the set, unless you   re clever (hw3)
    harder to predict the best string or tree (set is exponentially large or infinite)

    requires id145; you might have to write your own code
    but finite-state or crf toolkits will find the best string for you
    and you could modify someone else   s parser to pick the best tree
    an algorithm for picking the best can usually be turned into a learning algorithm
    hardest if your features look at    non-local    properties of the string or tree
    now id145 won   t work (or will be something awful like o(n9))
    you need some kind of approximate search
    can be harder to turn approximate search into a learning algorithm
    still, this is a standard preoccupation of machine learning 
(   id170,       id114   )

text annotation tasks
1. classify the entire document
2. classify individual word tokens
3. identify phrases (   chunking   )
4. syntactic annotation (parsing)
5. semantic annotation

600.465 - intro to nlp - j. eisner

100

id14 (srl)

    for each predicate (e.g., verb)
1. find its arguments (e.g., nps) 
2. determine their semantic roles

john drove mary from austin to dallas in his toyota prius.

the hammer broke the window.

    agent: actor of an action
    patient: entity affected by the action
    source: origin of the affected entity
    destination: destination of the affected entity
    instrument: tool used in performing action.
    beneficiary: entity for whom action is performed

101
slide thanks to ray mooney (modified)

as usual, can solve as classification     
    consider one verb at a time:    bit   
    classify the role (if any) of each of the 3 nps
color code:
not-a-role
agent 
patient
source
destination
instrument
beneficiary

vp
v        np
det  a  n
bit
a
girl
det  a  n
boy
the

det  a  n
adj a
  
big

prep   np
with

np            pp

np

the

dog

s

  

  

102
slide thanks to ray mooney (modified)

parse tree paths as classification features 
path feature is
v     vp     s     np
which tends to 
be associated 
with agentrole

prep   np
with

np            pp

the

dog

vp

np

np

s

det  a  n
a
girl

  

v
bit
det  a  n
boy
the

  

det  a  n
adj a
  
big

103
slide thanks to ray mooney (modified)

parse tree paths as classification features 
path feature is
v     vp     s     np     pp     np
which tends to 
be associated 
with norole

prep   np
with

np            pp

the

dog

vp

np

np

s

det  a  n
a
girl

  

v
bit
det  a  n
boy
the

  

det  a  n
adj a
  
big

104
slide thanks to ray mooney (modified)

head words as features 

    some roles prefer to be filled by certain kinds of nps.
    this can give us useful features for classifying accurately:

       john ate the spaghetti with chopsticks.     (instrument)
   john ate the spaghetti with meatballs.      (patient)
   john ate the spaghetti with mary.   
    instruments should be tools
    patient of    eat    should be edible

       john boughtthe car for $21k.     (instrument)
   john boughtthe car for mary.     (beneficiary)
    instrument of    buy    should be money
    beneficiaries should be animate (things with desires)

       john drove mary to school in the van    
   john drove the van to work with mary.   
    what do you think?

105
slide thanks to ray mooney (modified)

uses of semantic roles

    find the answer to a user   s question
       who    questions usually want agents
       what    question usually want patients
       how    and    with what    questions usually want instruments
       where    questions frequently want sources/destinations.
       for whom    questions usually want beneficiaries
       to whom    questions usually want destinations
    generate text
    many languages have specific syntactic constructions that must or should 
be used for specific semantic roles.
    id51, using selectional restrictions 
    the bat ate the bug.   (what kind of bat?  what kind of bug?)
    agents (particularly of    eat   ) should be animate     animal bat, not baseball bat
    patients of    eat    should be edible     animal bug, not software bug
    john fired the secretary.
john fired the rifle.
patients of fire1 are different than patients of fire2

slide thanks to ray mooney (modified)

106

other current semantic 
annotation tasks (similar to srl)
    propbank     coarse-grained roles of verbs
    nombank     similar, but for nouns
    framenet     fine-grained roles of any word
    timebank     temporal expressions

framenet example 

revenge frame

avenger
offender (unexpressed in this sentence)
injury
injured party (unexpressed in this sentence)
punishment

we avenged the insult by setting fire to his village.

a word/phrase that triggers the revenge frame

slide thanks to cj fillmore (modified)

framenet example 
triggering words and phrases 

revenge frame
(not limited to verbs)

avenge, revenge, retaliate, get back at, pay back, get even,    
revenge, vengeance, retaliation, retribution, reprisal,    
vengeful, retaliatory, retributive; in revenge, in retaliation,    
take revenge, wreak vengeance, exact retribution,    

slide thanks to cj fillmore (modified)

generating new text
1. id103 (transcribe as text)
2. machine translation
3. text generation from semantics
4. inflect, analyze, or transliterate words
5. single- or multi-doc summarization

600.465 - intro to nlp - j. eisner

111

deeper information extraction
1. coreference resolution (within a document)
2. entity linking  (across documents)
3. event extraction and linking
4. knowledge base population (kbp)
5. recognizing texual entailment (rte)

600.465 - intro to nlp - j. eisner

112

user interfaces
1. dialogue systems
    personal assistance
    human-computer collaboration
interactive teaching
   
2. language teaching; writing help
3. id53
4. information retrieval

600.465 - intro to nlp - j. eisner

113

multimodal interfaces or 
modeling
1. sign languages
2. speech + gestures
3. images + captions
4. brain recordings, human reaction 

times

600.465 - intro to nlp - j. eisner

114

nlp automates things that humans do well, so that they can be done 
automatically on more sentences.  but this slide is about language analysis 
that   s hard even for humans.  computational linguistics(like comp bio, etc.) 
can discover underlying patterns in large datasets: things we didn   t know!

discovering linguistic structure

1. decipherment
2. grammar induction
3. id96
4. deep learning of word meanings
5. language evolution (historical linguistics)
6. grounded semantics

600.465 - intro to nlp - j. eisner

115

