smoothing

this dark art is why
nlp is taught in the
engineering school.

there are more principled

smoothing methods, too.  we   ll
look next at id148,
which are a good and popular
general technique.

but the traditional methods are
easy to implement, run fast, and
will give you intuitions about
what you want from a smoothing
method.

1

600.465 - intro to nlp - j. eisner

never trust a sample under 30

20

200

2000

2000000

never trust a sample under 30

smooth out
the bumpy
histograms
to look more
like the truth
(we hope!)

smoothing reduces variance

20

20

different samples of size 20 vary considerably
(though on average, they give the correct bell curve!)

20

20

smoothing reduces variance

truth

truth

average

unsmoothed estimates
from different samples

smoothed estimates
from different samples

estimates are correct on average:

such an estimation method

is called unbiased

estimates are incorrect on average:

such an estimation method

is called biased

estimates are typically far from truth
(high variance = mean squared error)

600.465 - intro to nlp - j. eisner

but estimates are typically close to average
(high variance = mean squared distance)
and so may tend to be closer to truth, too

5

parameter estimation

p(x1=h, x2=o, x3=r, x4=s, x5=e, x6=s,    )

    p(h | bos, bos)
* p(o | bos, h)
* p(r | h, o)
* p(s | o, r)
* p(e | r, s)
* p(s | s, e)
*    

600.465 - intro to nlp - j. eisner

trigram model   s
parameters

values of
those
parameters,
as naively
estimated
from brown
corpus.

4470/ 52108

395/ 4470

1417/ 14765

1573/ 26412

1610/ 12253

2044/ 21250

6

terminology: types vs. tokens
    word type = distinct vocabulary item
    a dictionary is a list of types (once each)
    word token = occurrence of that type

    a corpus is a list of tokens (each type has many tokens)

26 types 300 tokens
100
0
0
200
0

a
b
c
d
e
   
z
total

0
300

100 tokens of this type
0 tokens of this type

200 tokens of this type

    we   ll estimate probabilities of the dictionary types

by counting the corpus tokens

(in context)

7

how to estimate?

    p(z | xy) = ?
    suppose our training data includes

    xya ..
    xyd    
    xyd    
but never xyz

    should we conclude
p(a | xy) = 1/3?
p(d | xy) = 2/3?
p(z | xy) = 0/3?

    no!  absence of xyz might just be bad luck.

600.465 - intro to nlp - j. eisner

8

smoothing the estimates

    should we conclude

p(a | xy) = 1/3? reduce this
p(d | xy) = 2/3? reduce this
p(z | xy) = 0/3? increase this

    discount the positive counts somewhat
    reallocate that id203 to the zeroes
    especially if the denominator is small    

    1/3 probably too high, 100/300 probably about right

    especially if numerator is small    

    1/300 probably too high, 100/300 probably about right

600.465 - intro to nlp - j. eisner

9

add-one smoothing

xya
xyb
xyc
xyd
xye
   
xyz
total xy

1
0
0
2
0

0
3

1/3
0/3
0/3
2/3
0/3

0/3
3/3

2
1
1
3
1

2/29
1/29
1/29
3/29
1/29

1
29

1/29
29/29

600.465 - intro to nlp - j. eisner

10

add-one smoothing
300 observations instead of 3     better data, less smoothing

xya
xyb
xyc
xyd
xye
   
xyz
total xy

0
0

100 100/300
0/300
0/300
200 200/300
0/300

0

1
1

101 101/326
1/326
1/326
201 201/326
1/326

1

0

0/300
300 300/300

1

1/326
326 326/326

600.465 - intro to nlp - j. eisner

11

problem with add-one smoothing
we   ve been considering just 26 letter types    
2
1
1
3
1

2/29
1/29
1/29
3/29
1/29

1/3
0/3
0/3
2/3
0/3

1
0
0
2
0

0
3

0/3
3/3

1
29

1/29
29/29

600.465 - intro to nlp - j. eisner

12

xya
xyb
xyc
xyd
xye
   
xyz
total xy

problem with add-one smoothing
suppose we   re considering 20000 word types, not 26 letters
see the abacus
see the abbot
see the abduct
see the above
see the abram
   
see the zygote
total

2 2/20003
1 1/20003
1 1/20003
3 3/20003
1 1/20003

1 1/20003
20003 20003/20003

1/3
0/3
0/3
2/3
0/3

0/3
3/3

1
0
0
2
0

0
3

600.465 - intro to nlp - j. eisner

13

problem with add-one smoothing
suppose we   re considering 20000 word types, not 26 letters
see the abacus
see the abbot
see the abduct
see the above
   novel event    = 0-count event (never happened in training data).
here: 19998 novel events, with total estimated id203 19998/20003.
see the abram
so add-one smoothing thinks we are extremely likely to see novel events,
   
it thinks this only because we have a big dictionary: 20000 possible events.
see the zygote
total

2 2/20003
1 1/20003
1 1/20003
3 3/20003
1 1/20003

rather than words we   ve seen in training data.

1 1/20003
20003 20003/20003

1/3
0/3
0/3
2/3
0/3

is this a good reason?

0/3
3/3

1
0
0
2
0

0
3

600.465 - intro to nlp - j. eisner

14

infinite dictionary?
in fact, aren   t there infinitely many possible word types?
2 2/(   +3)
see the aaaaa
1 1/(   +3)
see the aaaab
1 1/(   +3)
see the aaaac
3 3/(   +3)
see the aaaad
1 1/(   +3)
see the aaaae
   
see the zzzzz
total

1 1/(   +3)
(   +3) (   +3)/(   +3)

1/3
0/3
0/3
2/3
0/3

1
0
0
2
0

0/3
3/3

0
3

600.465 - intro to nlp - j. eisner

15

add-lambda smoothing
    a large dictionary makes novel events too probable.

    to fix: instead of adding 1 to all counts, add     = 0.01?

    this gives much less id203 to novel events.

    but how to pick best value for    ?

    that is, how much should we smooth?

600.465 - intro to nlp - j. eisner

16

add-0.001 smoothing
doesn   t smooth much (estimated distribution has high variance)
0.331
0.0003
0.0003
0.661
0.0003

1.001
0.001
0.001
2.001
0.001

1/3
0/3
0/3
2/3
0/3

1
0
0
2
0

0
3

0/3
3/3

0.001
3.026

0.0003
1

600.465 - intro to nlp - j. eisner

17

xya
xyb
xyc
xyd
xye
   
xyz
total xy

add-1000 smoothing
smooths too much (estimated distribution has high bias)

xya
xyb
xyc
xyd
xye
   
xyz
total xy

1
0
0
2
0

0
3

1/3
0/3
0/3
2/3
0/3

0/3
3/3

1001
1000
1000
1002
1000

1000
26003

1/26
1/26
1/26
1/26
1/26

1/26
1

600.465 - intro to nlp - j. eisner

18

add-lambda smoothing
    a large dictionary makes novel events too probable.

    to fix: instead of adding 1 to all counts, add     = 0.01?

    this gives much less id203 to novel events.

    but how to pick best value for    ?

    that is, how much should we smooth?
    e.g., how much id203 to    set aside    for novel events?

    depends on how likely novel events really are!
    which may depend on the type of text, size of training corpus,    

    can we figure it out from the data?

    we   ll look at a few methods for deciding how much to smooth.

600.465 - intro to nlp - j. eisner

19

setting smoothing parameters
    how to pick best value for    ? (in add-       smoothing)
    try many     values & report the one that gets best results?

training

test

    how to measure whether a particular     gets good results?
    is it fair to measure that on test data (for setting    )?

    story: stock scam    
    moral: selective reporting on test data can make a method look artificially

also, tenure letters    

good.  so it is unethical.

    rule: test data cannot influence system development.  no peeking!  use it

only to evaluate the final system(s). report all results on it.

general rule of experimental ethics:
never skew anything in your favor.
applies to experimental design, reporting, analysis, discussion.
feynman   s advice:    the first principle is that you must not
fool yourself, and you are the easiest person to fool.   

600.465 - intro to nlp - j. eisner

20

setting smoothing parameters
    how to pick best value for    ?
    try many     values & report the one that gets best results?

training

test

    how to fairly measure whether a     gets good results?
    hold out some    development data    for this purpose
dev.

training

pick     that
gets best
results on
this 20%    

    when we collect counts
from this 80% and smooth
them using add-    smoothing.

600.465 - intro to nlp - j. eisner

now use
that     to get
smoothed
counts from
all 100%    

    and
report
results of
that final
model on
test data.

21

setting smoothing parameters
here we held out 20% of our training set (yellow) for development.
would like to use > 20% yellow:
would like to use > 80% blue:

    20% not enough to reliably assess    
    best     for smoothing 80%    

    best     for smoothing 100%

could we let the yellow and blue sets overlap?

    ethical, but foolish

training

test

dev.

training

pick     that
gets best
results on
this 20%    

    when we collect counts
from this 80% and smooth
them using add-    smoothing.

600.465 - intro to nlp - j. eisner

now use
that     to get
smoothed
counts from
all 100%    

    and
report
results of
that final
model on
test data.

22

5-fold cross-validation (   jackknifing   )

would like to use > 20% yellow:
would like to use > 80% blue:
    old version: train on 80%, test on 20%

    20% not enough to reliably assess    
    best     for smoothing 80%    

    best     for smoothing 100%

dev.
    if 20% yellow too little: try 5 training/dev splits as below

training

    pick     that gets best average performance

dev.

dev.

dev.

dev.

animation
test

dev.

        tests on all 100% as yellow, so we can more reliably assess    
        still picks a     that   s good at smoothing the 80% size, not 100%.

    but now we can grow that 80% without trouble    

600.465 - intro to nlp - j. eisner

23

cross-validation pseudocode

    for     in {0.01, 0.02, 0.03,     9.99}

    for each of the 5 blue/yellow splits

    train on the 80% blue data, using     to smooth the counts
    test on the 20% yellow data, and measure performance
    goodness of this     = average performance over the 5 splits
dev.

dev.

dev.

dev.

dev.

    using best     we found above:

    train on 100% of the training data, using     to smooth the counts
    test on the red test data, measure performance & report it
test

training

600.465 - intro to nlp - j. eisner

24

n-fold cross-validation (   leave one out   )

   

(more extreme
version of strategy
from last slide)

test

    to evaluate a particular     during dev, test on all the training data:
test each sentence with smoothed model from other n-1 sentences

        still tests on all 100% as yellow, so we can reliably assess    
        trains on nearly 100% blue data ((n-1)/n) to measure whether     is
good for smoothing that much data: nearly matches true test conditions

        surprisingly fast: why?

    usually easy to retrain on blue by adding/subtracting 1 sentence   s counts

600.465 - intro to nlp - j. eisner

25

smoothing reduces variance
remember: so does backoff
(by increasing size of sample). use both?

20

20

20

200

20

use the backoff, luke!

    why are we treating all novel events as the same?
    p(zygote | see the) vs. p(baby | see the)

    unsmoothed probs: count(see the zygote) / count(see the)
    smoothed probs: (count(see the zygote) + 1) / (count(see the) + v)
    what if count(see the zygote) = count(see the baby) = 0?

    baby beats zygote as a unigram
    the baby beats the zygote as a bigram
        see the baby beats see the zygote ?

(even if both have the same count, such as 0)

    backoff introduces bias, as usual:

    lower-order probabilities (unigram, bigram) aren   t quite what we want
    but we do have enuf data to estimate them & they   re better than nothing.

600.465 - intro to nlp - j. eisner

27

early idea: model averaging

    jelinek-mercer smoothing (   deleted interpolation   ):
    use a weighted average of backed-off na  ve models:
paverage(z | xy) =    3 p(z | xy) +    2 p(z | y) +    1 p(z)

where    3 +    2 +    1 = 1 and all are     0

    the weights     can depend on the context xy

    if we have    enough data    in context xy, can make    3 large.  e.g.:

    if count(xy) is high
    if the id178 of z is low in the context xy

    learn the weights on held-out data w/ jackknifing

    different    3 when xy is observed 1 time, 2 times, 3-5 times,    

    we   ll see some better approaches shortly

600.465 - intro to nlp - j. eisner

28

more ideas for smoothing

    cross-validation is a general-purpose wrench for tweaking

any constants in any system.
    here, the system will train the counts from blue data, but we use
yellow data to tweak how much the system smooths them (   ) and
how much it backs off for different kinds of contexts (   3 etc.)

    is there anything more specific to try in this case?
    remember, we   re trying to decide how much to smooth.

    e.g., how much id203 to    set aside    for novel events?

    depends on how likely novel events really are    
    which may depend on the type of text, size of training corpus,    

    can we figure this out from the data?

is there any theoretically nice way to pick      ?
how likely are novel events?

20000 types 300 tokens
150
18
0
0
50
0
0
38
0

a
both
candy
donuts
every
farina
grapes
his
ice cream
   

300 tokens
0
0
1
2
0
0
1
0
7

versus

0/300

0/300

which zero would you expect is really rare?

600.465 - intro to nlp - j. eisner

30

how likely are novel events?

20000 types 300 tokens
150
18
0
0
50
0
0
38
0

a
both
candy
donuts
every
farina
grapes
his
ice cream
   

determiners:
a closed class

600.465 - intro to nlp - j. eisner

300 tokens
0
0
1
2
0
0
1
0
7

versus

31

how likely are novel events?

20000 types 300 tokens
150
18
0
0
50
0
0
38
0

a
both
candy
donuts
every
farina
grapes
his
ice cream
   

600.465 - intro to nlp - j. eisner

300 tokens
0
0
1
2
0
0
1
0
7

versus

(food) nouns:
an open class

32

how common are novel events?

counts from brown corpus (n     1 million tokens)

n6 *

n5 *

n4 *

n3 *

n2 *
n1 *

n0 *

6

5

4

3

2

1

0

doubletons (occur twice)
singletons (occur once)
novel words (in dictionary but never occur)

0
= # doubleton types
n2
n2 * 2 = # doubleton tokens

5000

10000
   r nr
   r (nr * r) = total # tokens = n (all bars)

25000
15000
= total # types   = t (purple bars)

20000

how common are novel events?

1*
1*

69836
52108
n6 *
6
n5 *
5
n4 *
4
n3 *
3
n2 *
2
n1 *
1
n0 *
0

the

eos

abdomen, bachelor, caesar    
aberrant, backlog, cabinets    
abdominal, bach, cabana    
abbas, babel, cabot    
aback, babbitt, cabanas    
abaringe, babatinde, cabaret    

0

10000

20000

30000

40000

50000

60000

70000

80000

witten-bell smoothing idea

n6 *

n5 *

n4 *

n3 *

n2 *
n1 *

n0 *

6

5

4

3

2

1

0

if t/n is large, we   ve seen lots of novel types
in the past, so we expect lots more.

    imagine scanning the corpus in order.
    each type   s first token was novel.
    so we saw t novel types (purple).

doubletons
singletons
novel words

unsmoothed     smoothed
2/n     2/(n+t)

1/n     1/(n+t)
0/n     (t/(n+t)) / n0

0

25000
intuition: when we see a new type w in training, ++count(w); ++count(novel)
so p(novel) is estimated as t/(n+t), divided among n0 specific novel types

10000

20000

15000

5000

good-turing smoothing idea

n6*

6/n

n5*

5/n

n4*

4/n

n3*

3/n

n2*

2/n

n1*

1/n

n0*

0/n

partition the type vocabulary into classes

(novel, singletons, doubletons,    )
by how often they occurred in training data

use observed total id203 of class r+1

to estimate total id203 of class r

obs. (tripleton)
1.2%
obs. p(doubleton)
1.5%
obs. p(singleton)
2%

unsmoothed     smoothed

est. p(doubleton)
est. p(singleton)
est. p(novel)

2/n     (n3*3/n)/n2
(n3*3/n)/n2

1/n     (n2*2/n)/n1
(n2*2/n)/n1
0/n     (n1*1/n)/n0
(n1*1/n)/n0

0

0.005

0.01

0.015

0.02

0.025

r/n = (nr*r/n)/nr     (nr+1*(r+1)/n)/nr

justification of good-turing

   

6/n

5/n

4/n

3/n

2/n

1/n

0/n

obs. (tripleton)
1.2%
obs. p(doubleton)
1.5%
obs. p(singleton)
2%

est. p(doubleton)
est. p(singleton)
est. p(novel)

0

0.005

0.01

0.015

0.02

0.025

    justified by leave-one-out training!  (leave out 1 word at a time.)
    instead of just tuning    , we will tune

better variant: leave out 1 document at a time?

[= frac. of yellow dev. words that were novel in blue training]

    p(novel)=0.02
    p(singleton)=0.015 [= frac. of yellow dev. words that were singletons in blue training]
    p(doubleton)=0.012 [= frac. of yellow dev. words that were doubletons in blue training]
i.e.,
    p(novel) = fraction of singletons in full training
    p(singleton) = fraction of doubletons in full training, etc.

    example: c(aback)=2.   on the 2 folds where yellow=aback, aback was a
singleton in blue data, so we   d be rewarded for assigning a high prob to
training singletons.  overall, we   ll get such a reward on 1.5% of the folds.

witten-bell vs. good-turing

    estimate p(z | xy) using just the tokens we   ve

seen in context xy.  might be a small set    
    witten-bell intuition: if those tokens were

distributed over many different types, then novel
types are likely in future.
    formerly covered on homework 3

    good-turing intuition: if many of those tokens

came from singleton types , then novel types are
likely in future.
    very nice idea (but a bit tricky in practice)
    see the paper    good-turing smoothing without tears   
38

600.465 - intro to nlp - j. eisner

good-turing (old slides)

    intuition: can judge rate of novel events

(in a context) by rate of singletons (in that
context)

    let nr = # of word types with r training

tokens
    e.g., n0 = number of unobserved words
    e.g., n1 = number of singletons

    let n =     r nr = total # of training tokens

600.465 - intro to nlp - j. eisner

39

good-turing (old slides)

    let nr = # of word types with r training tokens
    let n =     r nr = total # of training tokens
    na  ve estimate: if x has r tokens, p(x) = ?

    answer: r/n

    answer: nr r / n.

    total na  ve id203 of all word types with r tokens?

    good-turing estimate of this total id203:

    defined as: nr+1 (r+1) / n
    so proportion of novel words in test data is estimated by

proportion of singletons in training data.

    proportion in test data of the n1 singletons is estimated by

proportion of the n2 doubletons in training data.   etc.

    so what is good-turing estimate of p(x)?
600.465 - intro to nlp - j. eisner

40

smoothing + backoff
    basic smoothing (e.g., add-   , good-turing, witten-bell):

    holds out some id203 mass for novel events
    e.g., good-turing gives them total mass of n1/n
    divided up evenly among the novel events

    backoff smoothing

    holds out same amount of id203 mass for novel events
    but divide up unevenly in proportion to backoff prob.
    when defining p(z | xy), the backoff prob for novel z is p(z | y)

    novel events are types z that were never observed after xy.
    when defining p(z | y), the backoff prob for novel z is p(z)

    here novel events are types z that were never observed after y.
    even if z was never observed after xy, it may have been observed
after the shorter, more frequent context y.   then p(z | y) can be
estimated without further backoff.  if not, we back off further to p(z).

    when defining p(z), do we need a backoff prob for novel z?

    what are novel z in this case?  what could the backoff prob be?  what
if the vocabulary is known and finite?  what if it   s potentially infinite?
41

600.465 - intro to nlp - j. eisner

smoothing + backoff

    note: the best known backoff smoothing methods:

    modified kneser-ney (smart engineering)
    witten-bell + one small improvement (carpenter 2005)
    hierarchical pitman-yor (clean bayesian statistics)
    all are about equally good.

    note:

    a given context like xy may be quite rare     perhaps we   ve only
    then it may be hard for good-turing, witten-bell, etc. to
    we could try to make a better guess by aggregating xy with other
    this is another form of backoff.  by contrast, basic good-turing,
    id148 accomplish this very naturally.

observed it a few times.
accurately guess that context   s novel-event rate as required
contexts (all contexts? similar contexts?).
witten-bell, etc. were limited to a single implicit context.

600.465 - intro to nlp - j. eisner

42

smoothing

this dark art is why
nlp is taught in the
engineering school.

there are more principled

smoothing methods, too.  we   ll
look next at id148,
which are a good and popular
general technique.

600.465 - intro to nlp - j. eisner

43

smoothing as optimization

there are more principled

smoothing methods, too.  we   ll
look next at id148,
which are a good and popular
general technique.

600.465 - intro to nlp - j. eisner

44

conditional modeling
    given a context x
    which outcomes y are likely in that context?
    we need a conditional distribution p(y | x)

    a black-box function that we call on x, y
    p(nextword=y | precedingwords=x)

    y is a unigram
    x is an (n-1)-gram

    p(category=y | text=x)

    y     {personal email, work email, spam email}
    x        *   (it   s a string: the text of the email)

    remember: p can be any function over (x,y)!

    provided that p(y | x)     0, and    y p(y | x) = 1

600.465 - intro to nlp - j. eisner

45

linear scoring
    we need a conditional distribution p(y | x)
    convert our linear scoring function to this distribution p

    require that p(y | x)     0, and    y p(y | x) = 1;  not true of score(x,y)

how well does y go with x?

simplest option: a linear function of (x,y).  but (x,y) isn   t a number.

so describe it by one or more numbers:   numeric features    that you pick.

then just use a linear function of those numbers.

weight of feature k

to be learned    

ranges over all features,

e.g., k=5   (numbered features)

or k=   see det noun    (named features)

whether (x,y) has feature k(0 or 1)
or how many times it fires (    0)
or how strongly it fires

(real #)

what features should we use?

weight of feature k

to be learned    

ranges over all features,

e.g., k=5   (numbered features)

or k=   see det noun    (named features)

whether (x,y) has feature k (0 or 1)
or how many times it fires (    0)
or how strongly it fires

(real #)

    p(nextword=y | precedingwords=x)

    y is a unigram
    x is an (n-1)-gram

    p(category=y | text=x)

    y     {personal email, work email, spam email}
    x        *   (it   s a string: the text of the email)

log-linear id155
(interpret score as a log-prob, up to a constant)

unnormalized
prob (at least
it   s positive!)

where we choose z(x) to ensure that

thus,

600.465 - intro to nlp - j. eisner

sum of unnormalized
probabilities of all the
output candidates y   

sometimes just written as z

48

training    

this version is    discriminative training   :

to learn to predict y from x, maximize p(y|x).

whereas    joint training    learns

to model x, too, by maximizing p(x,y).

    n training examples
    feature functions f1, f2,    
    want to maximize p(training data|   )

    easier to maximize the log of that:

alas, some weights    i may be optimal at -    or +   .
when would this happen?  what   s going    wrong   ?

training    

this version is    discriminative training   :

to learn to predict y from x, maximize p(y|x).

whereas    joint training    learns

to model x, too, by maximizing p(x,y).

    n training examples
    feature functions f1, f2,    
    want to maximize p(training data|   )     pprior(   )

    easier to maximize the log of that:

encourages weights close to 0:    l2 id173    (other choices possible)

corresponds to a gaussian prior,  since gaussian bell curve is just exp(quadratic).

gradient-based training

    gradually adjust     in a direction that increases this

    for this, use your favorite function maximization algorithm.

    id119, conjugate gradient, variable metric,etc.

    (go take an optimization course: 550.{361,661,662}.)
    (or just download some software!)

nasty non-differentiable cost

function with local minima

nice smooth and convex cost
function: pick one of these

gradient-based training

    gradually adjust     in a direction that improves this

gradient ascent to gradually increase f(   ):

while (   f(   )     0)       // not at a local max or min

    =     +          f(   )   // for some small     > 0

remember:    f(   ) = (   f(   )/      1,    f(   )/      2,    )
so update just means:    k +=    f(   )/      k
this takes a little step    uphill   

(direction of steepest increase).
this is why you took calculus.    

gradient-based training

    gradually adjust     in a direction that improves this

    the key part of the gradient works out as    

maximum id178

    suppose there are 10 classes, a through j.
    i don   t give you any other information.
    question: given message m: what is your guess for p(c | m)?

    suppose i tell you that 55% of all messages are in class a.
    question: now what is your guess for p(c | m)?

    suppose i also tell you that 10% of all messages contain buy

and 80% of these are in class a or c.

    question: now what is your guess for p(c | m),

if m contains buy?

    ouch!

600.465 - intro to nlp - j. eisner

54

maximum id178

a
.051

.499

buy
other

c

b
.0025 .029

e

d
.0025 .0025 .0025 .0025 .0025 .0025 .0025

h

g

f

j

i

.0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446

    column a sums to 0.55 (   55% of all messages are in class a   )

600.465 - intro to nlp - j. eisner

55

maximum id178

a
.051

c

b
.0025 .029

e

d
.0025 .0025 .0025 .0025 .0025 .0025 .0025

h

g

f

j

i

.499

buy
other
    column a sums to 0.55
    row buy sums to 0.1 (   10% of all messages contain buy   )

.0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446

600.465 - intro to nlp - j. eisner

56

maximum id178

a
.051

c

b
.0025 .029

e

d
.0025 .0025 .0025 .0025 .0025 .0025 .0025

h

g

f

j

i

.499

.0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446

buy
other
    column a sums to 0.55
    row buy sums to 0.1
    (buy, a) and (buy, c) cells sum to 0.08 (   80% of the 10%   )
    given these constraints, fill in cells    as equally as possible   :

maximize the id178 (related to cross-id178, perplexity)

id178 = -.051 log .051 - .0025 log .0025 - .029 log .029 -    
largest if probabilities are evenly distributed

600.465 - intro to nlp - j. eisner

57

maximum id178

a
.051

c

b
.0025 .029

e

d
.0025 .0025 .0025 .0025 .0025 .0025 .0025

h

g

f

j

i

.499

.0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446

buy
other
    column a sums to 0.55
    row buy sums to 0.1
    (buy, a) and (buy, c) cells sum to 0.08 (   80% of the 10%   )
    given these constraints, fill in cells    as equally as possible   :

maximize the id178

    now p(buy, c) = .029  and  p(c | buy) = .29
    we got a compromise: p(c | buy) < p(a | buy) < .55

600.465 - intro to nlp - j. eisner

58

maximum id178

a
.051

c

b
.0025 .029

e

d
.0025 .0025 .0025 .0025 .0025 .0025 .0025

h

g

f

j

i

buy
other
    given these constraints, fill in cells    as equally as possible   :

.0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446 .0446

.499

maximize the id178

    now p(buy, c) = .029  and  p(c | buy) = .29
    we got a compromise: p(c | buy) < p(a | buy) < .55
    punchline: this is exactly the maximum-likelihood log-

linear distribution p(y) that uses 3 binary feature
functions that ask: is y in column a?  is y in row buy?  is
y one of the yellow cells? so, find it by gradient ascent.
600.465 - intro to nlp - j. eisner
59

