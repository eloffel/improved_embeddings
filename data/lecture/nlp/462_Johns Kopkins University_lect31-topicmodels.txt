id96

600.465 - intro to nlp - j. eisner

1

embedding documents
    a trick from information retrieval

    each document in corpus is a length-k vector

    or each paragraph, or whatever

(0,    3,

3,

1,    0,

7,

. . .

1,    0)

a single document

pretty sparse, so pretty noisy!  hard to tell which docs are similar.

wish we could smooth the vector:

what would the document look like if it rambled on forever?
600.465 - intro to nlp - j. eisner

2

latent semantic analysis
    a trick from information retrieval

    each document in corpus is a length-k vector
    plot all documents in corpus

reduced-dimensionality plot

true plot in k dimensions

a id91 algorithm
might discover this
cluster of    similar   
documents     similar
thanks to smoothing!

600.465 - intro to nlp - j. eisner

3

latent semantic analysis
    reduced plot is a perspective drawing of true plot
    it projects true plot onto a few axes
        a best choice of axes     shows most variation in the data.

    found by id202:    principal components analysis    (pca)

reduced-dimensionality plot

true plot in k dimensions

600.465 - intro to nlp - j. eisner

4

latent semantic analysis
    svd plot allows best possible reconstruction of true plot

(i.e., can recover 3-d coordinates with minimal distortion)

    ignores variation in the axes that it didn   t pick
    hope that variation   s just noise and we want to ignore it

reduced-dimensionality plot

true plot in k dimensions

i

b
 
c
p
o
t

topic a
600.465 - intro to nlp - j. eisner

5

latent semantic analysis
    svd finds a small number of topic vectors
    approximates each doc as linear combination of topics
    coordinates in reduced plot = linear coefficients

    how much of topic a in this document?   how much of topic b?
    each topic is a collection of words that tend to appear together

reduced-dimensionality plot

true plot in k dimensions

i

b
 
c
p
o
t

topic a
600.465 - intro to nlp - j. eisner

6

latent semantic analysis
    new coordinates might actually be useful for info retrieval
    to compare 2 documents, or a query and a document:

    project both into reduced space: do they have topics in common?
    even if they have no words in common!

reduced-dimensionality plot

true plot in k dimensions

i

b
 
c
p
o
t

topic a
600.465 - intro to nlp - j. eisner

7

latent semantic analysis
    topics extracted for ir might help sense disambiguation

    each word is like a tiny document:  (0,0,0,1,0,0,   )
    express word as a linear combination of topics
    each topic corresponds to a sense?

    e.g.,    jordan    has mideast and sports topics

(plus advertising topic, alas, which is same sense as sports)

    word   s sense in a document: which of its topics are strongest in the

document?

    groups senses as well as splitting them

    one word has several topics and many words have same topic

600.465 - intro to nlp - j. eisner

8

latent semantic analysis
    a perspective on principal components analysis (pca)
    imagine an electrical circuit that connects terms to docs    

terms

1   2   3   4   5   6   7   8   9

matrix of strengths
(how strong is each

term in each document?)

1   2   3   4   5   6   7

documents

each connection has a

weight given by the matrix.

600.465 - intro to nlp - j. eisner

9

latent semantic analysis
    which documents is term 5 strong in?

terms

1   2   3   4   5   6   7   8   9

docs 2, 5, 6

light up strongest.

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

10

latent semantic analysis
    which documents are terms 5 and 8 strong in?

terms

1   2   3   4   5   6   7   8   9

this answers a query

consisting of terms 5 and 8!

1   2   3   4   5   6   7

documents

really just id127:

term vector (query) x strength matrix = doc vector       .

600.465 - intro to nlp - j. eisner

11

latent semantic analysis
    conversely, what terms are strong in document 5?

terms

1   2   3   4   5   6   7   8   9

gives doc 5   s coordinates!

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

12

latent semantic analysis
    svd approximates by smaller 3-layer network

    forces sparse data through a bottleneck, smoothing it

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

topics

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

13

latent semantic analysis
    i.e., smooth sparse data by matrix approx: m     a b
    a encodes camera angle, b gives each doc   s new coords

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

b

topics

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

14

latent semantic analysis

completely symmetric!  regard a, b as projecting terms and docs
into a low-dimensional    topic space    where their similarity can be
judged.

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

b

topics

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

15

latent semantic analysis

    completely symmetric.  regard a, b as projecting terms and docs
into a low-dimensional    topic space    where their similarity can be
judged.

    cluster documents (helps sparsity problem!)
    cluster words
    compare a word with a doc
    identify a word   s topics with its senses

    sense disambiguation by looking at document   s senses

    identify a document   s topics with its topics

    topic categorization

600.465 - intro to nlp - j. eisner

16

if you   ve seen svd before    
    svd actually decomposes m = a d b    exactly
    a = camera angle (orthonormal); d diagonal; b    orthonormal

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a
d
b   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

17

if you   ve seen svd before    
    keep only the largest j < k diagonal elements of d
    this gives best possible approximation to m using only j blue units

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a
d
b   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

18

if you   ve seen svd before    
    keep only the largest j < k diagonal elements of d
    this gives best possible approximation to m using only j blue units

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a
d
b   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

600.465 - intro to nlp - j. eisner

19

if you   ve seen svd before    
    to simplify picture, can write m     a (db   ) = ab

terms

terms

1   2   3   4   5   6   7   8   9

1   2   3   4   5   6   7   8   9

matrix

m

a

b =
db   

1   2   3   4   5   6   7

documents

1   2   3   4   5   6   7

documents

    how should you pick j (number of blue units)?
    just like picking number of clusters:

    how well does system work with each j (on held-out data)?
600.465 - intro to nlp - j. eisner

20

directed id114 (bayes nets)

under any model: p(a, b, c, d, e) = p(a)p(b|a)p(c|a,b)p(d|a,b,c)p(e|a,b,c,d)
model above says:

slide thanks to zoubin ghahramani (modified)

21

unigram model for generating text

w1

w2

w3

   

p(w1)     p(w2)     p(w3)    

22

explicitly show model   s parameters    

       is a vector that says
which unigrams are likely   

   

w1

w2

w3

   

p(   )     p(w1 |    )     p(w2 |    )     p(w3 |    )    

23

   plate notation    simplifies diagram

       is a vector that says
which unigrams are likely   

   

w

n1

p(   )     p(w1 |    )     p(w2 |    )     p(w3 |    )    

24

learn     from observed words
(rather than vice-versa)

   

w

n1

p(   )     p(w1 |    )     p(w2 |    )     p(w3 |    )    

25

explicitly show prior over     (e.g., dirichlet)

   even if we didn   t observe
word 5, the prior says that
   5 = 0 is a terrible guess   

    given
           dirichlet(   )
wi        

   

   

w

n1

p(   )     p(    |    )     p(w1 |    )     p(w2 |    )     p(w3 |    )    

26

dirichlet distribution

each point on a k dimensional simplex is a multinomial id203
distribution:

2   
1

0

1

3   

1

1   

   

   

   
   
   
   
   

2.0
5.0
3.0

   
   
   
   
   

   

   

   
   
   
   
   

1
0
0

   
   
   
   
   

i   

1      
i

i   

1      
i

dog the cat

dog the cat

slide thanks to nigel crook

27

dirichlet distribution

a dirichlet distribution is a distribution over multinomial

distributions     in the simplex.

2   
1

2   
1
1

0

1

1   

0

1

1   

1

3   

1

3   

2   
1

1

3   

1

1   

slide thanks to nigel crook

28

slide thanks to percy liang and dan klein

29

dirichlet distribution

example draws from a dirichlet distribution over the 3-simplex:

2   

dirichlet(5,5,5)

0

1   

3   

2   

1

0

2   

3   

3   

dirichlet(0.2, 5, 0.2)

1   

dirichlet(0.5,0.5,0.5)

1   

slide thanks to nigel crook

30

explicitly show prior over     (e.g., dirichlet)
posterior distribution

p(    |    , w)

is also a dirichlet

just like the prior p(    |    ).

   even if we didn   t observe
word 5, the prior says that
   5 = 0 is a terrible guess   

prior = dirichlet(   )     posterior = dirichlet(   +counts(w))
mean of posterior is like the max-likelihood estimate of    ,
but smooth the corpus counts by adding    pseudocounts       .

(but better to use whole posterior, not just the mean.)

   

   

w

n

p(   )     p(    |    )     p(w1 |    )     p(w2 |    )     p(w3 |    )    

31

training and test documents

   learn     from document 1,
use it to predict document 2   

test
w

n2
train
w

n1

   

   

what do good

configurations look
like if n1 is large?

what if n1 is small?

32

many documents

   3

   2

   1

   

w

w

w

n3

n2

n1

   each document has its

own unigram model   

now does observing
docs 1 and 3 help still

predict doc 2?

only if     learns that
all the       s are similar

(low variance).

and in that case,

why even have
separate       s?

33

many documents

or tuned to maximize

training or dev set likelihood

   each document has its

own unigram model   

    given
      d     dirichlet(   )
wdi        d

   

   

w

nd
d

34

bayesian text categorization

   each document chooses

one of only k topics
(unigram models)   

    given
      k     dirichlet(   )
wdi        k but which k?

   

   

k

w

nd
d

35

bayesian text categorization
    given
        dirichlet(   )
zd        
    given
      k     dirichlet(   )
wdi        zd

a topic
in 1   k

   

z

   

a distribution
over topics 1   k

   each document chooses

one of only k topics
(unigram models)   

allows documents to differ

considerably while some
still share     parameters.

   

   

k

w

nd
d

and, we can infer the
id203 that two
documents have the

same topic z.

might observe some topics.

36

   each document
chooses a mixture

of all k topics;

each word gets its

own topic   

id44
(blei, ng & jordan 2003)

   

   

z

w

nd
d

   

   

k

37

(part of) one assignment to lda   s variables

slide thanks to dave blei

38

(part of) one assignment to lda   s variables

slide thanks to dave blei

39

id44: id136?

   

   

z1

w
w1

z2

w2

z3

w3

   

   

d

   

   

k
k

40

finite-state dirichlet allocation
(cui & eisner 2006)

   a different id48 for

each document   

   

   

z1

w1

   

   

k

z2

w2

z3

w3

   

   

d

41

variants of id44

    syntactic topic model: a word or its topic is

influenced by its syntactic position.

    correlated topic model, hierarchical topic model,    :

some topics resemble other topics.

    polylingual topic model: all versions of the same

document use the same topic mixture, even if
they   re in different languages.  (why useful?)

    relational topic model: documents on the same
topic are generated separately but tend to link to
one another.  (why useful?)

    dynamic topic model: we also observe a year for
each document.  the k topics     used in 2011 have
evolved slightly from their counterparts in 2010.

42

dynamic topic model

slide thanks to dave blei

43

dynamic topic model

slide thanks to dave blei

44

dynamic topic model

slide thanks to dave blei

45

dynamic topic model

slide thanks to dave blei

46

remember: finite-state dirichlet allocation
(cui & eisner 2006)
   a different id48 for

each document   

   

   

z1

w1

   

   

k

z2

w2

z3

w3

   

   

d

47

bayesian id48

   

   

z1

w1

   

   

k

   shared id48 for

all documents   

(or just have 1 document)

z2

w2

z3

w3

   

d

we have to estimate
transition parameters    
and emission parameters    .

48

