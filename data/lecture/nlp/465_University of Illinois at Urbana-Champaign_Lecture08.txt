cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 8:
variational id136 

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

the em algorithm again 

c. bishop (2006) pattern recognition and machine learning

bayesian methods in nlp

2

the em algorithm
assume a model p(x, z |   ) over x and z in which 
maximizing the complete log-likelihood ln p( x, z |   ) 
of a complete data set (x, z) is easy.

however, if we are only given an incomplete data set x, 
maximizing the marginal log-likelihood ln p(x |   ) is dif   cult
because it contains a summation inside the logarithm:

ln p(x|   ) = ln   xz

p(x, z|   ) 

we instead maximize the expected complete log-likelihood q(  ,      ) 
according to z   s posterior p(z |x,      ) under the old parameters      

q(   ,    0) = ez|x,   0[ ln p(x, z |    ) ]

p(z | x,    0) ln p(x, z |    )

=xz

bayesian methods in nlp

3

the em algorithm
1. initialization: choose initial parameters   old
2. expectation step: evaluate p(z | x,   old)
  (posterior of z under current parameters   old)

3. maximization step: find   new  = argmax   q (  ,   old)
maximize expected complete log-likelihood under p(z|x,   old)
arg max

q(   ,    old) = arg max

ez|x,   old[ ln p(x, z |    ) ]

   

   

= arg max

p(z | x,    old) ln p(x, z |    )

    xz

4. stop, or set   old :=   new and go to 2.

bayesian methods in nlp

4

another view of ln p(x|   )
by the product rule:   ln p(x, z|   ) = ln p(z | x,   )  + ln p(x |   )
de   ne a functional (a function from functions to scalars) l(q,   ) of q(z) 
that depends on the joint likelihood p(x, z|   )
 

l(q,    ) = xz
kl(q||p) =  xz

q(z) ln p(z|x,    )

 the kl-divergence of q(z) and p(z | x,   ), the posterior of z:
 
thus for any distribution q(z) over the latent variables z:
ln p(x |   )  = l(q,   ) + kl(q || p) 
                  = l(q(z), p(x, z |   )) + kl(q(z) || p(z | x,   ))

q(z) ln p(x, z|   )

q(z)

q(z)

bayesian methods in nlp

5

 ln p(x |   )  = l(q,  )  + kl(q || p)
 

l(q,  ) is a lower bound on the marginal likelihood ln p(x |   ):

l(q,  ) = ln p(x |   )     kl(q || p)

thus, maximizing l(q,  )  will also increase ln p(x |   )

kl(q || p)

l(q,  )

ln p(x |   )

bayesian methods in nlp

6

e-step: qnew(z) = p(z | x,   old)

qnew(z) = argmaxq l(q(z),  old)
             = argmaxq [ ln p(x |   old)      kl( q(z) || p(z | x,   old) ) ]
             = argminq  kl( q(z) || p(z | x,   old) )
             = p(z | x,   old)
l(qnew(z),  old) = ln p(x |   old)

kl(q || p)

l(q,  old)

ln p(x |   old)

bayesian methods in nlp

7

m-step:   new = argmax   l(q(z),   )

  new = argmax   l(q(z),   ) 
       = argmax   l(p(z | x,   old),   )
       = argmax      z p(z | x,   old)ln p(x, z |   ) 
                               z p(z |x,   old)ln p(z | x,   old)
      = argmax      z q(z)ln p(x, z |   )        z q(z)ln q(z)
      = argmax      z q(z)ln p(x, z |   )        z h(q(z))
      = argmax      z q(z)ln p(x, z |   )
      = argmax      z p(z | x,   old) ln p(x, z |   ) 
     = argmax   q(  ,   old)
  

bayesian methods in nlp

8

m-step:   new = argmax   l(q(z),   )

if l(qnew,   new) > l(qnew,   old):
kl(qnew(z) || p(z | x,   new)) > 0  
and qnew(z) <  p(z | x,   new)

l(q,  new)

ln p(x |   new)

l(q,  old)

ln p(x |   old)

bayesian methods in nlp

9

em again...
l(q,   ) is a lower bound on the 
incomplete log-likelihood ln p(x |  )

e-step: 
with   old     xed, return qnew  
that maximizes l(q,   old) wrt. q(z), 
now kl(qnew || pold ) = 0.

m-step: 
with qnew    xed, return   new 
that maximizes l(qnew,   ) wrt.   .
if l( qnew,   new ) > l( qnew,   old ):
ln p( x |   new) > ln p( x |   old ), 
and hence kl( qnew || pnew ) > 0
 

bayesian methods in nlp

kl(q||p) = 0

l(q,    old)

ln p(x|   old)

kl(q||p)

l(q,    new)

ln p(x|   new)

10

variational id136
variational id136 is applicable when you have to compute 
an intractable posterior over latent variables p( z |x) 

basic idea:
replace the exact, but intractable posterior p( z |x)
with a tractable approximate posterior q( z |x, v)

q( z |x, v) is from a family of simpler distributions over the latent 
variables z  that is de   ned by a set of free variational 
parameters v

unlike in em, kl(q || p) > 0  for any q, since q only approximates p

bayesian methods in nlp

11

variational em 
initialization: 
de   ne initial model   old  and variational distribution q( z | x ,v )

e-step: 
find v that maximize the variational distribution q( z | x ,v )
compute the expectation of true posterior p(z | x,   old) 
under the new variational distribution q( z | x ,v )

m-step: 
find model parameters   new that maximize the expectation of 
the p(z, x|    ) under the variational posterior q( z | x ,v )

set   old :=   new

bayesian methods in nlp

12

variational id136
for lda

bayesian methods in nlp

13

blei, ng, jordan (2003)   s lda

  : k-dimensional dirichlet prior over topic distributions  
  d: k-dimensional multinomial over topics
  k: v-dimensional multinomial over words for topic k
  ij: id203 of word j in topic i
zd,n: topic label for word n in document d 
(indicator variable: k-dimensional unit vector)
wd,n: word n in document d 
(indicator variable: v-dimensional unit vector)
bayesian methods in nlp

14

posterior id136 for lda

given: 
- data (m documents w1, ..., wm)
- dirichlet priors    and word multinomials   
compute the posterior of the hidden variables: 
- document-speci   c topic multinomials   1, ...,   m  
- word-speci   c topic labels z1,1, ..., zm,n 
p(   1:d   z1,1:m,n, | w1:m,    ,   1:k)

bayesian methods in nlp

15

the posterior is intractable

p(   , z  | w,    ,   )  =  p(   , z, w |   ,   ) / p( w |   ,   )

p( w |   ,   ) requires marginalizing over the hidden 
variables   , z:

p( w |    ,   ) =          z   p(   , z,   ,  w |   ,    ) d   dz

this is intractable, because    and    are coupled when 
we sum over the latent topics.
n   d   

p(w|   ,  ) =

(   i ij)wj

      i 1
i

 (pi    i)
qi  (   i)z     kyi=1

       nyn=1

kxi=1

vyj=1

 = 
   n    i p(wn |   i ) p(  i |   i )

bayesian methods in nlp

16

an approximate lda model
q(   , z |   ,   ) 
= q(    |    )    n q( zn |   n )

this introduces two sets of variational parameters:
  : document-speci   c topic dirichlets 
  : word-speci   c topic multinomials
id203 of topic z given document d: q(   d |   d ) 
each document has its own dirichlet prior over topics   d
id203 of topic assignment to word wd,n: q( zd,n |   d,n )
each word word[d][n] has its own multinomial over topics   d,n

bayesian methods in nlp

17

mn  z    variational em
rewrite the marginal log-likelihood log p(w |   ,   ) as
     log p(w |   ,   ) = l(  ,   ;   ,   ) 
                            + kl( q(  , z |   ,   ) || p(   , z | w,   ,   ) )
             
with
l(  ,   ;   ,   ) = eq[log p(    |   )] + eq[log p( z |    )] + eq[log p( w | z,   )] 
                         eq[log q(    |   )]     eq[log q(z |    )]
                     = eq[ log p(   , z, w |   ,    )]      eq[log q(   , z |   ,   )]

recall standard em 

l(p(z | x,   old),   )  =    z p(z | x,   old)log p(x, z |   ) 
                                               z p(z |x,   old)log p(z | x,   old)
                                =    z q(z)log p(x, z |   )        z q(z)log q(z)
                                 =    eq[ log p(x, z |   ) ]      eq[ log q(z) ] 

bayesian methods in nlp

18

variational em for lda
goal: find   ,    which maximize the marginal log-likelihood 
          l(  ,   ) =    d  log p( wd |   ,   )

e-step: compute approximate variational posterior q(  ,z |   *,   *)

find optimal (data-speci   c) variational parameters (  d*,   d*) 
(  *,   *) = arg min (  *,   *) kl( q(  , z |   ,   ) || p(  , z | w,   ,   ) )
to compute q(  , z |   *,   *)
we will do this in two stages:    rst update   *, then   *  

m-step: update   ,    to maximize lower bound on l(  ,   )

(find id113 estimates of   ,   , using the expected suf   cient statistics 
for each document under the approximate posterior q):
updates:  id203 of word j in topic i:    ij        d   n  dni wdnj    
                         (  ij      sum of   dni for all words wdn that are equal to word j )
                            : computed numerically 

bayesian methods in nlp

19

inside the e-step (1): compute   d* 
   are word-speci   c multinomials over topics:
for each document d:    d*  = (  1, ...,   n ) 
           with each   n  = (  n1, ...,   nk ) a multinomial over topics for wn

  ni* = q(zn= i | wn), the variational posterior of topic i for word wn:    
    ni*       iwnexp( eq[ log(  i) |   ] ) 
           =   iwnexp(   (  i)      (   j   j))     (digamma):    rst derivative of   
      
                 
this update is like using bayes rule
               p(zn = i | wi)     p(wn | zn) p(zn = i)  
                                     =           iwn     i     
except p(zn = i) is approximated by exp( eq(  |  )[ log p(zn = i) ] ), which, 
without the eq(  |  ))[...], would just be exp(log p(zn = i)) = p(zn = i)
bayesian methods in nlp

20

inside e-step (2): compute   * 
  * = (  1, ...,   m)  with   d = (  d1, ...,   dk) document-speci   c 
posterior dirichlet over the topics

  i: posterior id203 of   i, the id203 of topic i in d = w1 ...wn, 
given the updated variational parameters   1*...  n*
(d index omitted):
  i =   i +    n   ni   = q(   i  |   1*...  n* ,   )

since   n is a multinomial,   ni  = e[zn = i |   ni]  is the expected 
frequency i of topic for word wn under the variational distribution

relation between true and variational parameters:

true posterior =  dir(hyperparameter + observed frequencies)
variational posterior = dir(hyperparameter + expectation of observed frequencies)

bayesian methods in nlp

21

summary: variational em

de   ne a tractable model q with variational parameters   ,    that 
de   nes a lower bound on the log-likelihood of the actual model

(e) update   ,    to get new approx. posterior of latent vars q(  , z |   ,   )
for each document,    nd optimal values of its variational parameters   * (dirichlet 
prior for document-speci   c topic multinomial) and   * (multinomials for topic 
assignment of each word)

variational id136 inside e-step: 
find   *,   * that minimize the kl-divergence of q to the actual model p:
(  *,   *) = arg min (  *,   *) d( q(  , z |   ,   )  || p(  , z | w,   ,   ) )

(m) update   ,    to get new p(  , z, w |   ,   ) 
= increase the lower bound on log-likelihood 
use expected suf   cient statistics (=counts) under the new approximate 
posterior q(  , z |   *,   *) to compute    id113    estimates of   ,     (this maximizes the 
lower bound on log-likelihood, or the approximate expected complete log-
likelihood)
bayesian methods in nlp

22

