cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 1:
introduction

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

class overview

bayesian methods in nlp

2

this class
seminar on (bayesian) statistical models in nlp:
-mathematical and algorithmic foundations
-applications to nlp
difference to cs498 (introduction to nlp):
-focus on current research and state-of-the-art techniques
-some of the material will be signi   cantly more advanced
-no exams, but a research project
-lectures (by me) and paper presentations (by you)

bayesian methods in nlp

3

class topics (i)
modeling text as a bag of words:
-applications: text classi   cation, id96
-methods: naive bayes, probabilistic latent semantic 
analysis, id44

modeling text as a sequence of words:
-applications: id38, pos-tagging
-methods: id165 models, id48, 
id49

bayesian methods in nlp

4

class topics (ii)
modeling the structure of sentences:
-applications: syntactic parsing, grammar induction
-methods: probabilistic grammars, loglinear models
modeling correspondences:
-applications: image annotation/retrieval, machine translation
-methods: correspondence lda, alignment models
understanding probabilistic models:
-bayesian vs. frequentist approaches
-generative vs. discriminative models
-exact vs. approximate id136
-parametric vs. nonparametric models

bayesian methods in nlp

5

tentative class outline
topics

week

1-4

5-6

7-8

9

10-11

11-15

lectures: background and topic models

papers: topic models

lectures: nonparametric models

papers: nonparametric models

lectures: sequences and trees

papers: sequences and trees

bayesian methods in nlp

6

introduction

1.
2. conjugate priors
3. text classi   cation: frequentist vs bayesian approaches
4. the em algorithm
5. sampling
6. probabilistic latent semantic analysis
7. id44
8. variational id136 for lda
9. papers: correlated topic models
10. papers: dynamic topic models
11. papers: supervised lda
12. papers: correspondence lda
13. dirichlet processes
14. hierarchical dirichlet processes
15. hierarchical dirichlet processes
16. project proposals
17. papers: unsupervised coreference resolution with hdps
18. papers: nonparametric id38

                        spring break                         

19. id48
20. probabilistic id18s
21. conditional random    elds
22. papers: the in   nite id48
23. papers: nonparametric pid18s
24. project updates
25. papers: grammar induction
26. papers: grammar induction
27. papers: language evolution
28. papers: multilingual id52
29. papers: synchronous grammar induction
bayesian methods in nlp

7

paper presentations
about half the lectures, you will present research 
papers in class 
goals: 
-get familiar with current work
-read and learn to present and critique research papers

bayesian methods in nlp

8

paper presentations: procedure
presenter:
-meet with me at least two days before your presentation
we want to make sure you understand the paper
-slides are recommended, but: please make your own, even 
when the authors make theirs available 
you don   t actually learn much by regurgitating somebody else   s slides.
-send me a pdf of your slides before class
-bring your laptop (or let me know in advance if you need to use mine)
everybody else:
-before class: submit a one-page summary of the paper
i won   t grade what you write, but i want you to engage with the material
-during/after class: critique the presentation
this is merely for everybody   s bene   t, and not part of the grade. 
in fact, i won   t even see what you write. 

bayesian methods in nlp

9

research projects
goal: write a research paper of publishable quality 
on a topic that is related to this class

requires literature review and implementation 
previous projects have been published in good conferences
 

bayesian methods in nlp

10

research projects: milestones
week 4: initial project proposal due (1-2 pages)
what project are you going to work on? what resources do 
you need? why is this interesting/novel? list related work 
week 8: fleshed out proposal due (3-4 pages)
              first in-class spotlight presentation
add initial literature review, and present preliminary results
week 12: status update report due; 
                second in-class spotlight presentation
make sure things are moving along
finals week: final report (8-10 pages); poster + talk
include detailed literature review, describe your results

bayesian methods in nlp

11

grading policies
50% research project
30% paper presentations
20% in-class participation and paper summaries

bayesian methods in nlp

12

a quick review of 
id203 theory

bayesian methods in nlp

13

id203 theory: terminology
trial: 
picking a shape, predicting a word
sample space   : 
the set of all possible outcomes 
(all shapes; all words in alice in wonderland)
event          : 
an actual outcome  (a subset of   )
(predicting    the   , picking a triangle)

bayesian methods in nlp

14

the id203 of events
kolmogorov axioms:
1) each event has a id203 between 0 and 1.
2) the null event has id203 0.
     the id203 that any event happens is 1.
3) the id203 of all disjoint events sums to 1.

0     p (     )     1
p (   ) = 0 and p ( ) = 1
  i  

p ( i) = 1 if    j  = i :  i      j =    
and  i  i =  

bayesian methods in nlp

15

random variables
a random variable x is a function from the sample 
space to a set of outcomes.
in nlp, the sample space is often the set of all 
possible words or sentences
random variables may be:
- categorical (discrete): the word; its part of speech
- boolean: is the word capitalized?
- integer-valued: how many letters are in the word?
- continuous/real-valued
- vectors (e.g. a id203 distribution)

bayesian methods in nlp

16

joint and id155
the id155 of x given y, p(x|y), 
is de   ned in terms of the id203 of y, p(y), and 
the joint id203 of x and y, p(x,y):
p (x|y ) = p (x, y )

p (y )

p(blue |    ) = 2/5

bayesian methods in nlp

17

the chain rule
the joint id203 p(x,y) can also be expressed in 
terms of the id155 p(x|y)

p (x, y ) = p (x|y )p (y )

this leads to the so-called chain rule: 

p (x1, x2, . . . , xn) = p (x1)p (x2|x1)p (x3|x2, x1)....p (xn|x1, ...xn 1)

= p (x1)

p (xi|x1 . . . xi 1)

n i=2

bayesian methods in nlp

18

independence
two random variables x and y are independent if

p (x, y ) = p (x)p (y )

if x and y are independent, then p(x|y) = p(x):

p (y )

p (x|y ) = p (x, y )
= p (x)p (y )
= p (x)

p (y )

(x , y independent)

bayesian methods in nlp

19

id203 models
building a id203 model consists of two steps:
- de   ning the model
- estimating the model   s parameters
using a id203 model requires id136
models (almost) always make 
independence assumptions.
that is, even though x and y are not actually independent, our 
model may treats them as independent.
this reduces the number of model parameters we need to 
estimate (e.g. from n2 to 2n)

bayesian methods in nlp

20

id114
id114 are a notation for id203 models.
nodes represent distributions over random variables:
 p(x) = 
arrows represent dependencies:
p(y) p(x | y) = 

p(y) p(z) p(x | y, z) =

shaded nodes represent observed variables.
white nodes represent hidden variables
p(y) p(x | y) with y hidden and x observed = 

bayesian methods in nlp

21

xxyxyzxydiscrete id203 distributions:
throwing a coin
bernoulli distribution:
id203 of success (=head,yes) in single yes/no trial 
-the id203 of head is p. 
-the id203 of tail is 1   p.
binomial distribution: 
prob. of the number of heads in a sequence of yes/no trials
the id203 of getting exactly k heads in n independent
yes/no trials is:

p (k heads, n   k tails) = n

k   pk(1   p)n k

bayesian methods in nlp

22

discrete id203 distributions:
rolling a die
categorical distribution:
id203 of getting one of n outcomes in a single trial.
the id203 of category/outcome ci is pi   (   pi = 1)
multinomial distribution:
id203 of observing each possible outcome ci 
exactly xi times in a sequence of n trials

p (x1 = xi, . . . , xn = xn) =

n!

x1!       xn! px1

1        pxn

n

if

n i=1

xi = n

bayesian methods in nlp

23

multinomial variables
-in nlp, x is often a discrete random variable 
that can take one of k states.

-we can represent such xs as k-dimensional vectors
in which one xk =1 and all other elements are 0
 x = (0,0,1,0,0)t

-denote id203 of xk =1 as   k with 0       k     1 and    k   k =1
then the id203 of x is:

p (x|  ) =

  xk
k

kyk=1

bayesian methods in nlp

24

probabilistic models 
for natural language:
id38

bayesian methods in nlp

25

unigram (bag of word) language models

alice was beginning to get very tired of 
alice was beginning to get very tired of 
sitting by her sister on the bank, and of 
sitting by her sister on the bank, and of 
having nothing to do: once or twice she 
having nothing to do: once or twice she 
had peeped into the book her sister was 
had peeped into the book her sister was 
reading, but it had no pictures or 
reading, but it had no pictures or 
conversations in it, 'and what is the use 
conversations in it, 'and what is the use 
of a book,' thought alice 'without 
of a book,' thought alice 'without 
pictures or conversation?'
pictures or conversation?'

p(of) = 3/66
p(alice) = 2/66
p(was) = 2/66
p(to) = 2/66

p(her) = 2/66
p(sister) = 2/66
p(,) = 4/66
p(') = 4/66

bayesian methods in nlp

26

id165 models

unigram model
bigram model
trigram model
id165 model

p (w1)p (w2)...p (wi)
p (w1)p (w2|w1)...p (wi|wi 1)
p (w1)p (w2|w1)...p (wi|wi 2wi 1)
p (w1)p (w2|w1)...p (wi|wi n 1...wi 1)

id165 models assume each word (event) 
depends only on the previous n-1 words (events). 
such independence assumptions are called 
markov assumptions (of order n-1).
) :  p (wi|wi 1
i n)

p (wi|wi 1

1

bayesian methods in nlp

27

estimating id165 models

1. bracket each sentence by special start and end symbols: 

<s> alice was beginning to get very tired    </s>
(we only assign probabilities to strings  <s>...</s>)

2. count the frequency of each id165   .
     c(<s> alice) = 1, c(alice was) = 1,   .
3. .... and normalize to get the id203:

p (wn|wn 1) = c(wn 1wn)
c(wn 1)

this is the relative frequency estimate

bayesian methods in nlp

28

generating from a distribution
how do you generate text from an id165 model? 

that is, how do you sample from a distribution p(x |y=y)?
-assume x has n possible outcomes (values): {x1,....,xn}
and p(xi | y=y) = pi 
-divide the interval [0,1] into n smaller intervals according to 
the probabilities of the outcomes
-generate a random number r between 0 and 1.
-return the x1 whose interval the number is in.

r

 0               p1                        p1+p2            p1+p2+p3            p1+p2+p3+p4     1

bayesian methods in nlp

29

x1x2x3x4x5generating shakespeare 

bayesian methods in nlp

30

shakespeare as corpus
the shakespeare corpus consists of n=884,647 
word tokens and a vocabulary of v=29,066 word 
types

shakespeare produced 300,000 bigram types 
out of v2= 844 million possible bigrams   
99.96% of the possible bigrams were never seen 

quadrigrams look like shakespeare because they 
are shakespeare

bayesian methods in nlp

31

unseen events matter
we estimated a model on 440k word tokens, but:
only 30,000 word types occurred.
any word that does not occur in the training data
has zero id203!
only 0.04% of all possible bigrams occurred.
any bigram that does not occur in the training data
has zero id203! 

bayesian methods in nlp

32

zipf   s law: the long tail
how many words occur once, twice, 100 times, 1000 times? 

how many words occur n times?

 100000

 10000

 1000

 100

 10

)
g
o
l
(
 
y
c
n
e
u
q
e
r
f

 1

 1

 10

 100

number of words (log) 

 1000

 10000

 100000

in natural language:
-a small number of events (e.g. words) occur with high frequency
-a large number of events occur with very low frequency

bayesian methods in nlp

33

a few words are very frequentmost words are very rarethe r-th most common word wr  has p(wr)     1/rdealing with unseen events
relative frequency estimation assigns all id203 
mass to events in the training corpus
but we need to reserve some id203 mass to 
events that don   t occur in the training data
unseen events = new words, new bigrams
important questions:
what possible events are there? 
how much id203 mass should they get?

bayesian methods in nlp

34

probabilistic models 
for natural language:
text classi   cation

bayesian methods in nlp

35

naive bayes for text classi   cation
the task:
assign (sentiment) label li     { +,   } to a document wi.
w1=    this is an amazing product: great battery life, amazing features and it   s cheap.   
w2=    how awful. it   s buggy, saps power and is way too expensive.   
the model:
-use bayes    rule:
li  = argmax l p( l  | wi ) = argmax l p( wi  | l )p( l) 
-assume wi is a    bag of words   :
w1= {an:1, and: 1, amazing: 2, battery: 1, cheap: 1, features: 1, great: 1,   }
w2= {awful: 1, and: 1, buggy: 1, expensive: 1,   }
- p( wi  | l ) is a multinomial distribution:  wi      multinomial(  l)
we have a vocabulary of v words. thus:   l = (  1,   .,   v)
- p( l ) is a bernoulli distribution: l      bernoulli(  )

bayesian methods in nlp

36

probabilistic models 
for natural language:
sequence labeling

bayesian methods in nlp

37

id52

bayesian methods in nlp

38

pierre vinken , 61 years old , will join the board as a nonexecutive director nov. 29 .pierre_nnp vinken_nnp ,_, 61_cd years_nns old_jj ,_, will_md join_vb the_dt board_nn as_in a_dt nonexecutive_jj director_nn nov._nnp 29_cd ._.raw texttagged texttagset:nnp: proper nouncd: numeral,jj: adjective,...pos taggerstatistical id52
what is the most likely sequence of tags t 
estimate argmaxt p(t|w) directly (in a conditional model)
estimate argmaxt p(t|w) directly (in a conditional model)
estimate argmaxt p(t|w) directly (in a conditional model)
estimate argmaxt p(t|w) directly (in a conditional model)
for the given sequence of words w ?
or use bayes    rule (and a generative model):
or use bayes    rule (and a generative model):
or use bayes    rule (and a generative model):
or use bayes    rule (and a generative model):
p(t,w)
p(t,w)
p(t,w)
p(t,w)
p(w)
p(w)
p(w)
p(w)
p(t,w)
p(t,w)
p(t,w)
p(t,w)
p(t)p(w|t)
p(t)p(w|t)
p(t)p(w|t)
p(t)p(w|t)

p(t|w) = argmax
p(t|w) = argmax
p(t|w) = argmax
p(t|w) = argmax
t
t
t
t
= argmax
= argmax
= argmax
= argmax
= argmax
= argmax
= argmax
= argmax

argmax
argmax
argmax
argmax

t
t
t
t

t
t
t
t

t
t
t
t

p(t,w) is a generative (joint) model.
id48 are generative models which 
decompose p(t,w) as p(t)p(w|t)

bayesian methods in nlp

39

id48

argmax
argmax
argmax

t
t
t

p(t|w)
p(t|w)
p(t|w)

=
=
=

:=def
:=def
:=def

argmax
argmax
argmax

t
t
t

argmax
argmax
argmax

t
t
t

id48 models are generative models of p(w,t)
(because they model p(w|t) rather than p(t |w))
they make two independence assumptions: 
a) approximate p(t) with an id165 model
b) assume that each word depends only on its pos tag

n
n
n

p(t)p(w|t)
p(t)p(w|t)
p(t)p(w|t)
 
 
 
   
   
   

.ti-1
p(ti|ti n..i  1)
p(ti|ti n..i  1)
p(ti|ti n..i  1)
   
   
   
prior p(t)
prior p(t)
prior p(t)

    
    
    

i=1
i=1
i=1

i
i
i

 
 
 
   
   
   

p(wi|ti)
p(wi|ti)
p(wi|ti)
   
    
   
   
    
    
likelihood p(w|t)
likelihood p(w|t)
likelihood p(w|t)

bayesian methods in nlp

40

probabilistic models 
for natural language:
grammars

bayesian methods in nlp

41

v
eat

np
sushi
vp

np
p
with tuna

eat sushi with tuna

grammars are ambiguous

vp
np
sushi  

v
eat

pp

p
with chopsticks

np

a grammar might generate multiple trees for a sentence:

eat sushi with chopsticks

incorrect analysis

vp

correct analysis
np
np
p
with tuna

pp

v
eat

vp

np
sushi
vp

vp
np
sushi  

v
eat

pp

p
with chopsticks

np

vp

pp
v
eat sushi with tuna
eat

np
p
with tuna

np
sushi
vp

np
p

pp

np

v
np
eat sushi with chopsticks
eat sushi  with chopsticks

eat sushi with tuna

eat sushi with chopsticks

what   s the most likely parse    for sentence s ?
we need a model of p(   | s)
np
p
with tuna

incorrect analysis

eat sushi with tuna

v
eat

pp

vp

vp

np
sushi
vp

bayesian methods in nlp

np
p

pp

np

42

v

np

id140
for every nonterminal  x, de   ne a id203 distribution 
p(x        | x) over all rules with the same lhs symbol x:

s   np vp
s   s conj s
np   noun
np   det noun
np   np pp
np   np conj np
vp   verb
vp   verb np
vp   verb np np
vp   vp pp
pp   p np

0.8
0.2
0.2
0.4
0.2
0.2
0.4
0.3
0.1
0.2
1.0

bayesian methods in nlp

43

