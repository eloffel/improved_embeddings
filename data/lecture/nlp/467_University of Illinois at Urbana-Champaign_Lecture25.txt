covariance	
   in	
   unsupervised	
   

learning	
   of	
   probabilis6c	
   grammars	
   

cohen	
   and	
   smith	
   (2010)	
   

presenter:	
   alice	
   lai	
   

introduc6on	
   

       a	
   framework	
   for	
   modeling	
   covariance	
   in	
   

probabilis6c	
   grammars	
   

       express	
   priors	
   using	
   logis6c	
   normal	
   

distribu6ons	
   

       experiments	
   on	
   dependency	
   grammar	
   

induc6on	
   with	
   parameter	
   tying	
   within	
   and	
   
across	
   grammars	
   	
   

grammar	
   induc6on	
   

       grammar	
   induc6on:	
   unsupervised	
   discovery	
   

of	
   gramma6cal	
   structure	
   

       bayesian	
   models	
   used	
   to	
   specify	
   priors	
   of	
   

probabilis6c	
   grammars	
   

       many	
   models	
   use	
   dirichlet	
   distribu6ons	
   

because	
   of	
   conjugate	
   prior	
   property	
   

dependency	
   grammars	
   

       syntax	
   is	
   a	
   directed	
   tree,	
   words	
   are	
   ver6ces,	
   

edges	
   are	
   dependency	
   rela6ons	
   
       two	
   words	
   have	
   a	
   dependency	
   rela6on	
   if	
   one	
   is	
   
an	
   argument	
   or	
   modi   er	
   of	
   the	
   other	
   

figure	
   from	
   nivre	
   (2005),	
   dependency	
   grammar	
   and	
   dependency	
   parsing.	
   

dependency	
   model	
   with	
   valence	
   
       proposed	
   by	
   klein	
   and	
   manning	
   (2004)	
   
       each	
   word	
   has:	
   

       binomial	
   distribu6on	
   over	
   whether	
   it	
   has	
   any	
   lew/
right	
   children	
   
       geometric	
   distribu6on	
   over	
   the	
   number	
   of	
   lew/
right	
   children	
   

       id136	
   is	
   cubic	
   in	
   the	
   length	
   of	
   the	
   

sentence	
   

       maximum	
   likelihood	
   via	
   em	
   algorithm	
   

y	
   =	
   

x	
   =	
   	
   	
   	
   	
   	
   	
      $   dt   jj   nn   vbz   	
   

	
   $	
   	
   	
   dt	
   	
   	
   	
   	
   jj	
   	
   	
   	
   	
   	
   	
   nn	
   	
   	
   	
   	
   vbz	
   	
   

           ,              =                    vbz   $,r               y   (1)      vbz,       
dmv	
   example	
   
                 (1)      vbz,       =                      stop   vbz,l,   f                         nn   vbz,   l         (   
       (2)   |nn,    )                   (stop|vbz,l,t)                   (stop|vbz,r,f)

   
                 (2)      nn,       =                      stop   nn,l,f                         jj   nn,l                         
stop   jj,r,f                         stop   jj,l,f                              stop   nn,l,t                      dt   nn,l   
                      stop   dt,r,f                         stop   dt,l,f                         stop   nn,l,t                         
stop   nn,r,f   

modeling	
   covariance	
   

       we	
   expect	
   to	
   see	
   covariance	
   in	
   probabilis6c	
   

grammars	
   
       words	
   and	
   word	
   classes	
   (e.g.	
   parts	
   of	
   speech)	
   
follow	
   pa^erns	
   
       example:	
   the	
   id203	
   that	
   a	
   word	
   class	
   has	
   
singular	
   noun	
   arguments	
   is	
   related	
   to	
   the	
   
id203	
   that	
   it	
   has	
   plural	
   noun	
   arguments	
   
       use	
   logis6c	
   normal	
   distribu6on	
   to	
   model	
   

covariance	
   

logis6c	
   normal	
   distribu6on	
   

       logis6c	
   transforma6on	
   of	
   mul6variate	
   normal	
   
distribu6on	
   to	
   points	
   on	
   probabilis6c	
   simplex	
   
       used	
   by	
   blei	
   and	
   la   erty	
   (2006)	
   for	
   correlated	
   

topic	
   models	
   

limita6ons	
   of	
   ln	
   distribu6on	
   

       covariance	
   only	
   modeled	
   within	
   a	
   

mul6nomial,	
   not	
   across	
   mul6nomials	
   

       probabilis6c	
   grammar	
   models	
   involve	
   mul6ple	
   

mul6nomials	
   
       we	
   want	
   to	
   model	
   the	
   correla6on	
   between	
   
di   erent	
   verb	
   types	
   (vbd,	
   vbz)	
   both	
   taking	
   nouns	
   
as	
   arguments	
   

par66oned	
   ln	
   distribu6on	
   

       de   ne	
   a	
   gaussian	
   over	
       =       =1                                 
variables	
   with	
   one	
             	
   covariance	
   matrix	
   

       covariance	
   matrix	
   models	
   correla6ons	
   
between	
   all	
   pairs	
   of	
   events	
   across	
   all	
   
mul6nomials	
   

       apply	
   the	
   logis6c	
   transforma6on	
   to	
   

subvectors	
   to	
   get	
   individual	
   mul6nomials	
   

create	
   

shared	
   ln	
   distribu6on	
   

                	
   size	
   covariance	
   matrix	
   is	
   expensive	
   to	
   
       par66on	
   normal	
   vectors,	
   use	
       	
   normal	
   
       result:	
       ~sln(    ,  ,    )	
   

       instead	
   of	
   a	
   single	
   normal	
   vector	
   for	
   all	
   
mul6nomials,	
   use	
   several	
   normal	
   vectors	
   

experts	
   to	
   sample	
   from	
   mul6nomials,	
   
recombine	
   parts	
   of	
   vectors	
   and	
   take	
   average	
   

sln	
   example	
   

	
   
	
   

bayesian	
   models	
   over	
   grammars	
   
       use	
   maximum	
   a	
   posteriori	
   framework	
   for	
   learning	
   

with	
   symmetric	
   dirichlet	
   priors	
   (smith	
   2006):	
   

       this	
   model:	
   treat	
       	
   as	
   a	
   hidden	
   variable:	
   integrate	
   
out	
       	
   in	
   the	
   id203	
   of	
   the	
   data	
   
       es6mate	
       ,	
   the	
   distribu6on	
   over	
   grammar	
   parameters	
   

	
   

two	
   model	
   varia6ons	
   

model	
   1:	
   grammar	
   parameters	
       	
   drawn	
   once	
   
model	
   2:	
   grammar	
   parameters	
       	
   drawn	
   once	
   for	
   

per	
   sentence	
   

all	
   sentences	
   in	
   corpus	
   

choosing	
   the	
   prior	
   distribu6on	
   

       rai   a	
   and	
   schaifer	
   (1961)	
   establish	
   3	
   

necessary	
   quali6es	
   for	
   prior	
   distribu6ons	
   
1)    analy6cal	
   tractability	
   
2)    richness	
   
3)    interpretability	
   

       most	
   literature	
   has	
   focused	
   on	
   (1),	
   using	
   a	
   
dirichlet	
   prior	
   because	
   it	
   is	
   conjugate	
   to	
   the	
   
mul6nomial	
   family	
   

       what	
   about	
   (2)	
   and	
   (3)?	
   

dirichlet	
   priors	
   

       computa6onally,	
   a	
   good	
   choice	
   for	
   prior	
   

because	
   of	
   analy6c	
   tractability	
   

unnecessary	
   grammar	
   rules)	
   

       may	
   encourage	
   sparse	
   solu6ons	
   (elimina6ng	
   

when	
   drawing	
       	
   from	
   a	
   dirichlet	
   distribu6on	
   

       however,	
   no	
   explicit	
   covariance	
   structure	
   

ln	
   priors	
   

       de   ne	
   one	
   ln	
   distribu6on	
   for	
   each	
   mul6nomial	
   
       sln	
   covariance:	
   de   ne	
   one	
   normal	
   expert	
   for	
   each	
   
single	
   mul6nomial	
   and	
   other	
   experts	
   across	
   related	
   
mul6nomials	
   

       prior	
   over	
                    	
   that	
   allows	
   covariance	
   among	
         
       {    ,1}   ,   ,          {    ,                 }      	
   
       for	
   sln,	
   covariance	
   among	
             {    ,    }   	
   not	
   directly	
   
       normal	
   experts	
             {    ,    }   	
   de   ne	
   this	
   rela6onship.	
   think	
   
of	
             {    ,    }   	
   as	
   weights	
   associated	
   with	
   event	
   

de   ned	
   

probabili6es.	
   

decoding	
   

       how	
   to	
   choose	
   an	
   analysis	
   (gramma6cal	
   
       viterbi	
   decoding:	
   the	
   most	
   likely	
   analysis	
   

structure	
   y)	
   given	
   the	
   input	
   

       minimum	
   bayes	
   risk	
   decoding:	
   the	
   analysis	
   that	
   

minimizes	
   risk	
   
	
   
	
   

      cost(    ,                   )	
   is	
   the	
   cost	
   of	
   choosing	
       	
   when	
   the	
   
correct	
   analysis	
   is	
                   	
   

3	
   decoding	
   techniques	
   

1)    viterbi	
   decoding	
   applied	
   to	
   point	
   es6mate	
   of	
       	
   
2)    mbr	
   decoding	
   applied	
   to	
   point	
   es6mate	
   of	
       	
   
       viterbi	
   and	
   mbr	
   ignore	
   covariance	
   matrix	
     	
   

grammar	
   weights,	
   apply	
   decoding,	
   average	
   
results	
   

3)    commi^ee	
   decoding:	
   randomly	
   sample	
   

       loss	
   func6on	
   is	
   dependency	
   a^achment	
   error.	
   

       this	
   method	
   has	
   generaliza6on	
   error	
   guarantees	
   

varia6onal	
   id136	
   

       bound	
   the	
   log-     likelihood	
   and	
   op6mize	
   with	
   

respect	
   to	
   approximate	
   posterior	
       (    ,    )	
   
       mean-        eld	
   approxima6on:	
       (    ,    )	
   is	
   factorized	
   
and	
   has	
   form	
       (    ,    )=    (    )    (    )	
   

       ln	
   prior	
   requires	
   addi6onal	
   approxima6on	
   

because	
   of	
   lack	
   of	
   conjugacy	
   
       first-     order	
   taylor	
   approxima6on	
   to	
   log	
   of	
   
       use	
   inside-     outside	
   algorithm	
   with	
   weighted	
   grammar	
   

normaliza6on	
   of	
   ln	
   distribu6on	
   

for	
   id136	
   

varia6onal	
   em	
   

varia6onal	
   id136	
   algorithm	
   assumes	
   that	
       	
   
and	
     	
   are	
      xed.	
   to	
   es6mate	
   these	
   parameters,	
   
update	
   values	
   of	
       	
   and	
     	
   from	
   varia6onal	
   

use	
   varia6onal	
   em.	
   
       e-     step:	
   maximize	
   bound	
   with	
   respect	
   to	
   
varia6onal	
   parameters	
   using	
   coordinate	
   
ascent.	
   op6mize	
   each	
   parameter	
   separately.	
   
       m-     step:	
   use	
   maximum	
   likelihood	
   es6ma6on	
   to	
   

parameters.	
   

grammar	
   induc6on	
   experiments	
   
1)    ln	
   distribu6on	
   on	
   english	
   text	
   
2)    ln	
   distribu6on	
   on	
   addi6onal	
   languages	
   
(chinese,	
   portuguese,	
   turkish,	
   czech,	
   
japanese)	
   

3)    sln	
   distribu6on	
   tying	
   parameters	
   for	
   coarse	
   

pos	
   tags	
   (english,	
   portuguese,	
   turkish)	
   
4)    sln	
   distribu6on	
   with	
   bilingual	
   sejngs	
   

(english,	
   portuguese,	
   turkish)	
   

experiment:	
   english	
   text	
   

       input:	
   gold	
   standard	
   pos	
   tagged	
   text	
   from	
   

penn	
   treebank	
   

       training	
   restricted	
   to	
   sentences	
   of	
      10	
   words	
   
       a^achment	
   accuracy:	
   for	
   what	
   frac6on	
   of	
   
words	
   did	
   the	
   predicted	
   parent	
   match	
   the	
   
gold	
   annota6on?	
   

experiment:	
   english	
   text	
   

1)    	
                                       	
   iden6ty	
   matrix	
   

covariance	
   matrix	
   ini6aliza6on	
   

2)    use	
   prior	
   knowledge	
   of	
   pos	
   tags	
   
       12	
   disjoint	
   tag	
   families	
   (coarse	
   tags)	
   
       covariance	
   matrix	
   has	
   1	
   on	
   diagonal,	
   0.5	
   
between	
   tags	
   in	
   same	
   family,	
   0	
   elsewhere	
   

results:	
   english	
   text	
   

results:	
   addi6onal	
   languages	
   

green:	
   id113,	
   yellow:	
   dirichlet-     i,	
   blue:	
   ln-     i,	
             (0)   =       ,	
   red:	
   ln-     i,	
   families	
   ini6alizer	
   

experiment:	
   sln	
   priors	
   

add	
   normal	
   experts	
   to	
   6e	
   probabili6es	
   of	
   
related	
   parents	
   (de   ned	
   by	
   coarse	
   tags)	
   for	
   each	
   
direc6on	
   
1)    verbal	
   parents	
   
2)    nominal	
   parents	
   
3)    verbs	
   and	
   nouns	
   
4)    adjec6val	
   parents	
   

results:	
   sln	
   priors	
   and	
   bilingual	
   data	
   

experiment:	
   bilingual	
   data	
   

       merge	
   models	
   for	
   2	
   languages	
   
       normal	
   experts	
   
      for	
   each	
   pos	
   tag	
   
      for	
   each	
   language	
   combining	
   mul6nomials	
   in	
   
coarse	
   pos	
   classes	
   
      for	
   2	
   languages	
   together	
   combining	
   mul6nomials	
   
in	
   coarse	
   pos	
   classes	
   
       non-     parallel	
   corpora	
   

results:	
   sln	
   priors	
   and	
   bilingual	
   data	
   

discussion	
   

       modeling	
   covariance	
   within	
   and	
   across	
   the	
   
mul6nomials	
   in	
   a	
   probabilis6c	
   grammar	
   can	
   
improve	
   performance	
   in	
   dependency	
   
grammar	
   induc6on	
   

       some	
   gains	
   from	
   training	
   joint	
   models	
   on	
   non-     
parallel	
   corpera	
   for	
   mul6ple	
   languages	
   
       is	
   there	
   a	
   be^er	
   way	
   to	
   represent	
   prior	
   
linguis6c	
   knowledge	
   besides	
   covariance	
   
structure?	
   

conclusions	
   

       used	
   logis6c	
   normal	
   distribu6on	
   to	
   model	
   

covariance	
   between	
   parameters	
   of	
   
probabilis6c	
   grammar	
   

       extended	
   ln	
   distribu6on	
   to	
   model	
   covariance	
   
across	
   mul6nomials	
   in	
   probabilis6c	
   grammar	
   
       introduced	
   varia6onal	
   id136	
   algorithm	
   for	
   
       experiments	
   in	
   grammar	
   induc6on	
   on	
   mul6ple	
   

probabilis6c	
   grammars	
   that	
   use	
   ln	
   priors	
   

languages	
   

