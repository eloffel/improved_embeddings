the in   nite hidden markov 

model	


matthew j. beal, zoubin ghahramani, carl 

edward rasmussen	


university college london	


nips   02   

	


presented by phuong nguyen 

motivation: modeling time series 	


       given a sequence of observations    

{y1, y2,    , yn}, for example:	

      sequence of images, or words	

      speech signals	

      stock prices	

      etc.	


- something that can predict    
p(yt|yt-1, yt-2, yt-3, ...) 	


       goal: to build a probabilistic model of the data 

3 

hidden markov model: causal 
structure and    hidden variables    	


       nlp (e.g., id52):	

       s: part of speech of word	

       y: word	


       vision:	


       s: object identities, poses, illumination	

       y: image pixel values 	


4 

hidden markov model	


       core: hidden k-state markov chain    

st     {1, 2,    , k} 	

       sequence of hidden states has markov dynamics	

       observations are independent of all other states	


       parameters	


       transition matrix: p(st | st-1)	

       emission matrix: p(yt | st)	


5 

choosing the number of hidden states	


       how do we choose k, the number of hidden 

states, in an id48? 	


       can we de   ne a model with an unbounded 

number of hidden states, and a suitable 
id136 algorithm? 	


idea: using dirichlet process to model 

transition and emission mechanisms 

6 

dirichlet process: k    nite states	

       drawing n samples {c1, c2,   , cn} that take on 
values {1, 2,   , k} with proportion given by   	


       put    under a conjugate prior	


7 

dirichlet process: k    nite states	


       joint & id155:	


8 

dirichlet process: in   nite states	


       what if the number of states is in   nite?	

       id155 when taking the limit:	


+ where k is the number of presented states,	

+    control the tendency to populate a 
previously unrepresented state	


9 

   
dirichlet process: in   nite states	


       take-away results	


      we can integrate out the in   nite number of 
transitions parameters	

      under dp, there is a natural tendency to use 
existing transitions => typical trajectories	


       problem:	


      state trajectories under the prior would never 
visit the same state twice	


solution: hierarchical dp a model for 

transition and emission for an infinite id48 

10 

hdp: hidden state transition 

mechanism	


       nij is the number of previous transitions from i to j	

         ,   , and    are hyperparameters	

       id203 of transition from i to j proportional to nij	

       with prob. proportional to      jump to a new state 	


11 

hdp   s hidden state transition 

mechanism: effects of parameters	


(a) 
(b) 
(c) 
(d) 

   
0.1 
0 
8 
1 

   
1000 
0.1 
2 
1 

   
100 
100 
2 

10000 

12 

hdp: emission mechanism	


       identical to transition mechanism, except that: 

there is no self-transition	


state emission generative mechanism 

word occurrence for entire alice novel  
13 

id136 in in   nite id48	


       what are the unknowns in iid48?	


       hidden state sequence s = {s1, s2,    , st}	

       five hyperparameters {  ,   ,   ,   e,   e}
id136 procedure: 
1.   
2.    for t = 1,    , t	


      

instantiate a random hidden state sequence {s1, s2,    , st}	


1.   
2.   

-   gibbs sample st given hyperparameter settings, count matrices, 
and observations. 	

-   update count matrices to re   ect new st; this may change k, 
the number of represented hidden states. 	


3.    end 	

4.    update hyperparameters {  ,   ,   ,   e,   e}	

5.    go to step 2. 	


14 

hyperparameter optimization	


       hyperparameter approximation:	


       optimize hyperparameters using maximum a 

posteriori (map)	


15 

estimating likelihood of observable 

sequence	


       issues:	


      estimating likelihood involves intractable sums 
over state trajectories	

      the number of distinct states grows with the 
sequence length	


       how to estimate the likelihood effectively?	


16 

estimating likelihood of observable 

sequence	


       solution: id143ing method	


17 

experiments	


18 

synthetic experiments: number of 

hidden states	


discovering the number of hidden states 

19 

synthetic experiments: expansive and 

compressive	


expansive (top row     4 states, 8 symbol) and compressive (bottom row     4 states, 3 symbols) 

20 

further reading	


       teh, jordan, beal and blei (2005) (hdp paper)	


      showed that iid48s can be derived from 
hierarchical dirichlet; processes, and provided a 
more ef   cient gibbs sampler 	


       van gael, saatci, teh, and ghahramani, 2008 

(beam sampling paper)	

      derived a much more ef   cient sampler based on 
id145 	


21 

questions?	


22 

