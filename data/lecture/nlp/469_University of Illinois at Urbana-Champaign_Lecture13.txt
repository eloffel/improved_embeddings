cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 13:
dirichlet processes

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

finite mixture model
mixing proportions:
the prior id203 of each component (assuming uniform    )

   |   ~ dirichlet(  /k, ...,   /k)

mixture components:
the distribution over observations for each component

  *k          |h ~ h  (h  is typically a dirichlet distribution)

indicator variables: 
which component is observation i drawn from?  

zi|   ~ multinomial(  )
the observations: 
the id203 of observation i under component zi 

xi|zi,{   *k           } ~ f(   *z     i ) (f  is typically a categorical distribution)

bayesian methods in nlp

2

dirichlet process dp(  , h )
the dirichlet process dp(  , h) de   nes a distribution 
over distributions over a id203 space   .

draws g ~ dp(  , h) from this dp 
are random distributions over   

dp(  , h) has two parameters:

base distribution h: 
a distribution over the id203 space   
concentration parameter   : 
a positive real number

if g ~ dp(  , h), then for any    nite measurable partition 
a1...ar of   :

(g(a1), ..., g(ar)) ~ dirichlet(  h(a1), ...,   h(ar))

bayesian methods in nlp

3

the base distribution h

id203 space   

a2

a1

a3

since  a1, a2, a3 partition   , we can use the base distribution h 
to de   ne a categorical distribution over a1, a2, a3:  
h(a1) + h(a2) + h(a3) = 1
note that we can use h to de   ne a categorical distribution 
over any    nite partition a1...ar  of   , even if h is smooth

bayesian methods in nlp

4

draws from the dp: g ~ dp(  , h )

id203 space   

a2

a1

a3

every individual draw g from dp(  , h) is also a distribution over    
g also de   nes a categorical distribution over any partition of   

for any    nite partition a1...ar of   , this categorical distribution is drawn 
from a dirichlet prior de   ned by    and h:
(g(a1),  g(a2),  g(a3)) ~ dir(  h(a1),   h(a2),    h(a3)) 

bayesian methods in nlp

5

the role of h and   
the base distribution h de   nes the mean (expectation) of g:

for any measurable set a       ,  e[g(a)] = h(a)

the concentration parameter    is inversely related 
to the variance of g :

v[g(a)] = h(a)(1     h(a))/(   +1)

   speci   es how much mass is around the mean
the larger   , the smaller the variance
   is also called the strength parameter: if we use dp(  , h) 
as a prior,    tells us how much we can deviate from the prior:

as           , g(a)     h(a)

 

bayesian methods in nlp

6

the posterior of g: g|  1, ...   n
assume the distribution g is drawn from a dp: g ~ dp(  , h )
the prior of g:  

(g(a1),..., g(ak)) ~ dirichlet(  h(a1), ...,   h(ak))
given a sequence of observations   1...   n  from   
that are drawn from this g:       i|g ~ g
what is the posterior of g given the observed   1...   n ?

for any    nite partition a1....ak of   , 
de   ne the number of observations in ak : nk = #{ i:   i     ak } 
the posterior of g given observations   1...   n

(g(a1),..., g(ak))|  1, ...   n ~ dirichlet(  h(a1) + n1, ...,   h(ak) + nk)

bayesian methods in nlp

7

the posterior of g: g|  1, ...   n
the observations   1...   n de   ne an empirical distribution over   :
 
the posterior of g given observations   1...   n

    this is just a fancy way of saying  p(ak) = nk/n

i=1     i
n

pn

(g(a1),..., g(ak))|  1, ...   n ~ dirichlet(  h(a1) + n1, ...,   h(ak) + nk)

the posterior is a dp with:

- concentration parameter    + n
- a base distribution that is a weighted average of h 
  and the empirical distribution.  
g|   1, ...   n     dp (    + n,
the weight of the empirical distribution is proportional to the 
amount of data. the weight of h is proportional to   
bayesian methods in nlp

    + npn

i=1     i
n

    + n

h +

   

n

)

8

the blackwell macqueen urn
assume each value in     has a unique color.

  1...   n is a sequence of colored balls. 

with id203    /(  +n), the n+1th ball is drawn
from h 

with id203 n/(  +n) the n+1th ball is drawn 
from an urn that contains all previously drawn balls.
note that this implies that g is a discrete distribution, 
even if h is not.

bayesian methods in nlp

9

the id91 property of dps
  1...   n induces a partition of the set 1...n into k unique 
values.
this means that the dp de   nes a distribution over 
such partitions.
the expected number of clusters k increases with    
but grows only logarithmically in n:  
     e[ k | n]        log(1 +  n/  )

bayesian methods in nlp

10

nlp 101: id38
task: given a stream of words w1...wn, predict the 
next word wn+1 with a unigram model p(w)
answer:  
if wn+1  is a word w we   ve seen before:
p(wn+1 = w)     freq(w)
but what if wn+1 has never been seen before? 
we need to reserve some mass for new events
p(wn+1 is a new word)       

p(wn+1 = w) =  freq(w)/(n+  )  if  freq(w) > 0
                    =     /(n+  )            if  freq(w) = 0
bayesian methods in nlp

11

the chinese restaurant processs

the (i+1)th customer ci+1 sits:
-at an existing table tk that already has nk customers 
with id203 nk/(i+  ) 
-at new table with id203   /(i+  ) 

bayesian methods in nlp

12

the predictive distribution 
  n+1|  1, ...,   n

the predictive distribution of   n+1 given a sequence of i.i.d. draws 
  1, ...,   n ~ g, with g ~ dp(  , h) and g marginalized out is given by
the posterior base distribution given   1, ...,   n

p (   n+1 2 a) = e[g(a)|   1, ...,    n]

=

   

    + n

h(a) +pn

i=1     i(a)
    + n

bayesian methods in nlp

13

the stick-breaking representation

0

  1
  1

  2   3

1       1

1

(1     1)  2

g ~ dp(  , h) if: 
- the component parameters are drawn from the  
base distribution:     *k ~ h
- the weights of each cluster are de   ned by 
a stick-breaking process: 
            k ~ beta(1,   )           k ~   k    l =1...k    1 (1     l )
  also written as    ~ gem(  ) (grif   ths/engen/mccloskey)

g =    k =1...      k   

  *k

bayesian methods in nlp

14

dirichlet process mixture models
each observation xi is associated with a latent parameter   i
each   i is drawn i.i.d. from g; each xi is drawn from f(  i)

 g|  ,h ~ dp(  , h)              i|g ~ g           xi|  i ~ f(  i)  

since g is discrete,   i can be equal to   j 
all xi, xj with   i =   j belong to the same mixture component
there are a countably in   nite number of mixture components.
stick-breaking representation: 
mixing proportions:   |   ~ gem(  )   
indicator variables: zi|   ~ mult(  )
component parameters:   *k |h ~ h
observations: xi|zi,{  *k} ~ f(  *zi)

bayesian methods in nlp

15

hierarchical dirichlet processes
since both h and g are distributions over the same space   , 
the base distribution of a dp can be a draw from another dp.
this allows us to specify hierarchical dirichlet processes, 
where each group of data is generated by its own dp.
assume a global measure g0 drawn from a dp:

g0 ~ dp(  , h)

for each group j, de   ne another dp gj  with base measure g0: 

gj ~ dp(  0, g0)  
(or gj ~ dp(  j, g0), but it is common to assume all   j are the same)
  0 speci   es the amount of variability around the prior g0
since all groups share the same base g0, all gj use the same 
atoms (balls of the same colors)
bayesian methods in nlp

16

