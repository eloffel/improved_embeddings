nlp
introduction to nlp
id51
introduction
polysemy
words have multiple senses (about 2 on average in id138)
example
let   s have a drink in the bar
i have to study for the bar
bring me a chocolate bar
homonymy
may i come in?
let   s meet again in may
part of speech ambiguity
joe won the first round
joe has a round toy
senses of the word    bar   
s: (n) barroom, bar, saloon, ginmill, taproom (a room or establishment where alcoholic drinks are served over a counter) "he drowned his sorrows in whiskey at the bar" 
s: (n) bar (a counter where you can obtain food or drink) "he bought a hot dog and a coke at the bar" 
s: (n) bar (a rigid piece of metal or wood; usually used as a fastening or obstruction or weapon) "there were bars in the windows to prevent escape" 
s: (n) measure, bar (musical notation for a repeating pattern of musical beats) "the orchestra omitted the last twelve bars of the song" 
s: (n) bar (an obstruction (usually metal) placed at the top of a goal) "it was an excellent kick but the ball hit the bar" 
s: (n) prevention, bar (the act of preventing) "there was no bar against leaving"; "money was allocated to study the cause and prevention of influenza" 
s: (n) bar ((meteorology) a unit of pressure equal to a million dynes per square centimeter) "unfortunately some writers have used bar for one dyne per square centimeter" 
s: (n) bar (a submerged (or partly submerged) ridge in a river or along a shore) "the boat ran aground on a submerged bar in the river" 
s: (n) legal profession, bar, legal community (the body of individuals qualified to practice law in a particular jurisdiction) "he was admitted to the bar in new jersey" 
s: (n) stripe, streak, bar (a narrow marking of a different color or texture from the background) "a green toad with small black stripes or bars"; "may the stars and stripes forever wave" 
s: (n) cake, bar (a block of solid substance (such as soap or wax)) "a bar of chocolate" 
s: (n) browning automatic rifle, bar (a portable .30 caliber automatic rifle operated by gas pressure and fed by cartridges from a magazine; used by united states troops in world war i and in world war ii and in the korean war) 
s: (n) bar (a horizontal rod that serves as a support for gymnasts as they perform exercises) 
s: (n) bar (a heating element in an electric fire) "an electric fire with three bars" 
s: (n) bar ((law) a railing that encloses the part of the courtroom where the judges and lawyers sit and the case is tried) "spectators were not allowed past the bar" 
id51
task
given a word
and its context
determine which sense it is
useful for machine translation
e.g., translate    play    into spanish
play the violin = tocar el viol  n 
play tennis = jugar al tenis
other uses
document retrieval (jaguar)
accent restoration (cote)
text to speech generation (lead)
id147 (aid/aide)
capitalization restoration (turkey)
dictionary method (lesk)
match sentences to dictionary definitions
examples of plant (m-w.com):
plant1 = a living thing that grows in the ground, usually has leaves or flowers, and needs sun and water to survive
plant2 = a building or factory where something is made
examples of leaf
leaf1 = a lateral outgrowth from a plant stem that is typically a flattened expanded variably shaped greenish organ, constitutes a unit of the foliage, and functions primarily in food manufacture by photosynthesis
leaf2 = a part of a book or folded sheet containing a page on each side 
find the pair of meanings that have the most overlapping definitions
   the leaf is the food making factory of green plants.    
decision lists (yarowsky)
method introduced by yarowsky (1994)
two senses per word
ordered rules: 
collocation -> sense
formula
decision lists (yarowsky)
fish within window -> bass1
striped bass -> bass1
guitar within window -> bass2
bass player -> bass2
play/v bass -> bass2
na  ve bayes
e.g., work by bill gale
correct sense = argmaxi p(sensei|context)
p(context|sensei)     pj p(wordj|sensei)
example: 
bar-legal near {lawyer, trial, judge, exam}
bar-restaurant near {drink, tab, beer}
applying naive bayes to wsd
p(c) is the prior id203 of that sense
counting in a labeled training set.
p(w|c)  id155 of a word given a particular sense
p(w|c) = count(w,c)/count(c)
we get both of these from a tagged corpus like semcor
can also generalize to look at other features besides words.
then it would be p(f|c) 
id155 of a feature given a sense
[slide from dan jurafsky]
choosing a class:
p(f|d5) 



p(g|d5) 



 1/4 * 2/9 * (2/9)2 * 2/9  
	    0.0006
11
conditional probabilities:
p(line|f) =
p(guitar|f)    =
p(jazz|f)     =
p(line|g) =
p(guitar|g)     =
p(jazz|g)      = 
priors:
p(f)= 

p(g)= 
3

4
1

4
(1+1) / (8+6) = 2/14
(0+1) / (8+6) = 1/14
(1+1) / (3+6) = 2/9 
(0+1) / (8+6) = 1/14
(1+1) / (3+6) = 2/9 
(1+1) / (3+6) = 2/9 
 3/4 * 2/14 * (1/14)2 * 1/14 
	    0.00003
v = {fish, smoked, line, haul, guitar, jazz}

[slide from dan jurafsky]
classification features
adjacent words (collocations)
e.g., chocolate bar, bar exam, bar stool, bar fight, foreign aid, presidential aide
position
e.g., plant pesticide vs. pesticide plant
adjacent parts of speech
nearby words
e.g., within 10 words
syntactic information
e.g., object of the verb    play   
topic of the text
classification methods
k-nearest neighbor (memory-based)
using euclidean distance
find the k most similar examples and return the majority class for them
see lecture on classification


id64
start with two senses and seeds for each sense 
e.g., plant1:leaf, plant2:factory
use these seeds to label the data using a supervised classifier (decision list)
add some of the newly labeled examples to the training data
repeat until no more examples can be labeled
id64
two principles:
one sense per collocation
one sense per discourse (e.g., document)

the yarowsky method
training data for wsd
senseval/semcor
234,000 words tagged with id138 senses
http://www.senseval.org/senseval3
two tasks: lexical sample and all words
available for many languages
pseudo-words
e.g., banana/door
multilingual corpora
aligned at the sentence level
use the translations as an indication of sense
wsd evaluation
extrinsic (task-based)
e.g., using machine translation or information retrieval
intrinsic
sense accuracy
baseline
most frequent sense

senseval-1 evaluation
metric
a = number of assigned senses
c = number of words assigned correct senses
t = total number of test words
precision = c/a; recall = c/t
results
best recall around 77p/77r
human lexicographer 97p/96r (for binary classification)
but only 75% for humans on id138 senses
most common sense 57p/50r (decent but depends on domain)
notes
there hasn   t been enough evidence that wsd helps downstream applications such as mt.
id165s (e.g.,    bar exam   ) are enough.
nlp
