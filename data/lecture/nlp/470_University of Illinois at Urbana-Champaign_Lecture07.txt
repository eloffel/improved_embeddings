cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 7:
topic models
(id44)

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

plsa as a graphical model

  d

zd,n

wd,n

  k

d documents d with n (nd) words wd,n  (from a vocabulary of size v ) are given
we assume k topics k 
each topic k defines a multinomial   k over words.
word wd,n is generated by topic zd,n     {1, ..., k, ..., k }.  (zd,n is an indicator variable)
the topic zd,n is generated by the document   s topic multinomial   d

d. blei and j. lafferty.    topic models.    in a. srivastava and m. sahami, editors, text 
mining: theory and applications. taylor and francis, 2009.

bayesian methods in nlp

2

kdnblei et al.   s criticisms of plsa
-where do the mixing weights for each document come from?
 
-each training document is a point on the topic simplex
plsa de   nes an empirical distribution over this simplex 

-plsa does not tell us how to de   ne mixing weights 
for unseen documents

-plsa risks over   tting:  the number of parameters grow 
linearly with the number of documents in the corpus

solution: place bayesian priors over the simplex to estimate a 
smooth distribution over the topic simplex

bayesian methods in nlp

3

lda as a graphical model

  

  d

zd,n

wd,n

  k

  

d documents d with n (nd) words wd,n  (from a vocabulary of size v ) are given
we assume k topics k 
each topic k defines a multinomial   k over words.
word wd,n is generated by topic zd,n     {1, ..., k, ..., k }.  (zd,n is an indicator variable)
the topic zd,n is generated by the document   s topic multinomial   d
there is a global k-dimensional dirichlet prior    over topic multinomials.
there is a global v-dimensional dirichlet prior    over word multinomials   

d. blei and j. lafferty.    topic models.    in a. srivastava and m. sahami, editors, text 
mining: theory and applications. taylor and francis, 2009.

bayesian methods in nlp

4

kdnthe generative process
for each topic 1...k:
-draw a multinomial over words   k     dir(  )

for each document 1...d:
- draw a multinomial over topics   d     dir(  )
- for each word wd,n:
        - draw a topic zd,n     mult(  d) with zd,n     [1..k]
         - draw a word wd,n     mult(  zd,n)

bayesian methods in nlp

5

$!3)*2!*)*2%)/+-%0)'#)-%.'"%-7)*2%&%)'#)"+)"+*'+")+.)/6*6!0)%?506#'8'*3)*2!*)&%#*&'5*#)$+&-#)*+)4%);!&*)+.)+"%)*+;'5)
+"039)c2'#)!00+$#)*+;'5)/+-%0#)*+)5!;*6&%);+03#%/37)$2%&%)*2%)#!/%)$+&-)2!#)/60*';0%)/%!"'"(#9)@+&)%?!/;0%7)4+*2)
*2%) /+"%3) !"-) &'8%&) *+;'5) 5!") ('8%) 2'(2) ;&+4!4'0'*3) *+) *2%) $+&-) i<hj7) $2'52) '#) #%"#'40%) ('8%") *2%) ;+03#%/+6#)
"!*6&%)+.)*2%)$+&-9)
)

the generative process

probabilistic generative process

statistical id136

money
b a n k
loan

k
n
a
b

m oney

l

o

a

n

k

n

a

b

loan

topic 1

r

i

v

e

r

river
bank
r e a m
s t
r
e
bank
stream
topic 2

ri v

1.0

doc1: money1 bank1 loan1
bank1  money1 money1
bank1 loan1

.5

.5

doc2:  money1
bank1
bank2 river2 loan1 stream2
bank1 money1

river2

doc3: 
bank2
stream2 bank2 river2 river2
stream2 bank2

1.0

?

topic 1

?

?

topic 2

steyvers, m. & grif   ths, t. (2006). probabilistic topic models. in t. landauer, d mcnamara, s. 
dennis, and w. kintsch (eds), latent semantic analysis: a road to meaning. laurence 
erlbaum

figure  2.) k006#*&!*'+") +.) *2%) (%"%&!*'8%) ;&+5%##) !"-) *2%) ;&+40%/) +.) #*!*'#*'5!0) '".%&%"5%) 6"-%&03'"() *+;'5)
/+-%0#))

bayesian methods in nlp

)
c2%)(%"%&!*'8%);&+5%##)-%#5&'4%-)2%&%)-+%#)"+*)/!,%)!"3)!##6/;*'+"#)!4+6*)*2%)+&-%&)+.)$+&-#)!#)*2%3)!;;%!&)'")

6

$!3)*2!*)*2%)/+-%0)'#)-%.'"%-7)*2%&%)'#)"+)"+*'+")+.)/6*6!0)%?506#'8'*3)*2!*)&%#*&'5*#)$+&-#)*+)4%);!&*)+.)+"%)*+;'5)
+"039)c2'#)!00+$#)*+;'5)/+-%0#)*+)5!;*6&%);+03#%/37)$2%&%)*2%)#!/%)$+&-)2!#)/60*';0%)/%!"'"(#9)@+&)%?!/;0%7)4+*2)
*2%) /+"%3) !"-) &'8%&) *+;'5) 5!") ('8%) 2'(2) ;&+4!4'0'*3) *+) *2%) $+&-) i<hj7) $2'52) '#) #%"#'40%) ('8%") *2%) ;+03#%/+6#)

the id136 problem

probabilistic generative process

statistical id136

doc1: money1 bank1 loan1
bank1  money1 money1
bank1 loan1

?

doc1: money? bank? loan?
bank?  money? money?
bank? loan?

doc2:  money1
bank1
bank2 river2 loan1 stream2
bank1 money1

topic 1

?

doc2:  money?
bank?
bank? river? loan? stream?
bank? money?

river2

doc3: 
bank2
stream2 bank2 river2 river2
stream2 bank2

?

river?

doc3: 
bank?
stream? bank? river? river?
stream? bank?

)
figure  2.) k006#*&!*'+") +.) *2%) (%"%&!*'8%) ;&+5%##) !"-) *2%) ;&+40%/) +.) #*!*'#*'5!0) '".%&%"5%) 6"-%&03'"() *+;'5)

steyvers, m. & grif   ths, t. (2006). probabilistic topic models. in t. landauer, d mcnamara, s. 
dennis, and w. kintsch (eds), latent semantic analysis: a road to meaning. laurence 
erlbaum

topic 2

bayesian methods in nlp

7

c2%)(%"%&!*'8%);&+5%##)-%#5&'4%-)2%&%)-+%#)"+*)/!,%)!"3)!##6/;*'+"#)!4+6*)*2%)+&-%&)+.)$+&-#)!#)*2%3)!;;%!&)'")

of the properties of the dirichlet, and replace these modeling choices with
an alternative distribution over the simplex.

2.2. exploring a corpus with the posterior distribution. lda provides
a joint distribution over the observed and hidden random variables. the hid-
den topic decomposition of a particular corpus arises from the correspond-
ing posterior distribution of the hidden variables given the d observed doc-
uments  w1:d,
(2)

the id136 problem
what is the posterior of the hidden variables given the 
observed variables (and hyperparameters)?
p(    1:d, z1:d,1:n ,     1:k | w1:d,1:n ,  ,    ) =

p(    1:d, z1:d,     1:k |  w1:d,  ,    )

       1:k       1:d  z p(    1:d, z1:d,     1:k |  w1:d,  ,    )

;

;

.

problem: the integral in the denominator is intractable!

loosely, this posterior can be thought of the    reversal    of the generative
process described above. given the observed corpus, the posterior is a dis-
tribution of the hidden variables which generated it.

solutions: approximate id136
-id150 (grif   ths & steyvers)
-variational id136 (blei et al.)

as discussed in blei et al. (2003), this distribution is intractable to com-
pute because of the integral in the denominator. before discussing approxi-
mation methods, however, we illustrate how the posterior distribution gives
a decomposition of the corpus that can be used to better understand and
organize its contents.
the quantities needed for exploring a corpus are the posterior expecta-
bayesian methods in nlp
tions of the hidden variables. these are the topic id203 of a term

      k,v = e[   k,v | w1:d,1:n ], the topic proportions of a document       d,k =

8

id150
represent corpus as an array of words w[i], 
document indices d[i] and topics z[i].
words w[i] and document indices d[i] are    xed. 
only the topics z[i] change.

states of the markov chain =  topic assignments to words.

   macrosteps   : assign a new topic to all of the words
   microsteps   : assign a new topic z[i] to word w[i].

bayesian methods in nlp

9

assigning a new topic to wi
the id203 p(zi = j | z-i, w, d) is proportional to the 
(global) id203 of wi under topic j 
times the id203 of topic j given document di

p(zi = j | z-i, w, d)     p(wi | zi = j) p(zi = j | di )

de   ne n-i,j(wi) as the frequency of wi labeled as topic j.
de   ne n-i,j(di) as number of words in di labeled as topic j.

p (zi = j|z i, w, d)  

n i,j(wi) +    

v=1 n i,j(wv)

v     +pv
|

{z

}

n i,j(di) +  

k=1 n i,j(di)

k pk
|

{z

}

prob of wi under topic zi

prob of topic zi in document di

bayesian methods in nlp

10

what other quantities do we need?
in a bayesian model, we need to compute the expected value 
of the parameters given the observed data.
our data is the set of words w1:d,1:n.
hence, we need to compute e[... |w1:d,1:n]

the expected topic id203 of the word v in topic k:
     k,v    = e[   k,v |w1:d,1:n]

the expected topic proportions of each document d:
     d,k    = e[  d,k |w1:d,1:n]

the expected topic assignment of each word wd,n:
   zd,n,k    = e[zd,n = k |w1:d,1:n]

bayesian methods in nlp

11

what other quantities do we need?

from any single sample, we can estimate:

the expected topic id203 of the word v in to topic k:
     k,v    = e[  k,v |w1:d,1:n ]

the topic proportions of each document d:
     d,k    = e[   d,k |w1:d,1:n]

h k,ii =

h   d,ki =

nj(wi) +    

v=1 nj(wv)

nj(di) +    

k=1 nj(di)

v     +pv
k    +pk

bayesian methods in nlp

12

running the gibbs sampler
a toy example from  grif   ths, t., & steyvers, m. (2004):

- 25 words.
- 10 prede   ned topics
- 2000 documents generated according to known distributions.
- each document =  5x5 image. 
pixel intensity = frequency of word.

bayesian methods in nlp

13

(a) graphical representation of 10 topics, combined to produce    documents    like those shown in b, 

where each image is the result of 100 samples from a unique mixture of these topics

griffiths t l, steyvers m pnas 2004;101:5228-5235

bayesian methods in nlp

  2004 by national academy of sciences

14

results of running the id150 algorithm

griffiths t l, steyvers m pnas 2004;101:5228-5235

bayesian methods in nlp

  2004 by national academy of sciences

15

topic model applications

-what are the topics that a document is about?

-given one document, can we    nd other documents
about the same topics?

-how do topics in a    eld change over time?

bayesian methods in nlp

16

things you can do with lda

topic models

7

top words from the top topics (by term score)

expected topic proportions

sequence

region

pcr

identi   ed
fragments 

two

genes
three
cdna

analysis  

measured
average

range
values
different

size
three

calculated

two
low

residues
binding
domains

helix
cys

regions
structure
terminus
terminal

site

computer
methods
number

two

principle
design
access

processing
advantage
important

0
2
.
0

0
1
.
0

0
0
.
0

abstract with the most likely topic assignments

top ten similar documents

exhaustive matching of the entire protein sequence database
how big is the universe of exons?
counting and discounting the universe of exons
detecting subtle sequence signals: a id150 strategy for multiple alignment
ancient conserved regions in new gene sequences and the protein databases
a method to identify protein sequences that fold into a known three- dimensional structure
testing the exon theory of genes: the evidence from protein structure
predicting coiled coils from protein sequences
genome sequence of the nematode c. elegans: a platform for investigating biology

d. blei and j. lafferty.    topic models.    in a. srivastava and m. sahami, editors, id111: theory 
and applications. taylor and francis, 2009.

figure 4. the analysis of a document from science. doc-
ument similarity was computed using eq. (4); topic words
were computed using eq. (3).

bayesian methods in nlp

the assignment of words to topics in the abstract of the article, and the top
ten most similar articles.

17

3. posterior id136 for lda

the central computational problem for id96 with lda is ap-
proximating the posterior in eq. (2). this distribution is the key to using

(3)

6

d. m. blei and j. d. lafferty

visualizing a topic
visualizing a topic. exploring a corpus through a topic model typi-
what is the distribution of words de   ned by a topic?
cally begins with visualizing the posterior topics through their per-topic
instead of using term probabilities directly, downweight 
term probabilities  . the simplest way to visualize a topic is to order the
them by how likely they are to be generated by any topic:
terms by their id203. however, we prefer the following score,

term-scorek,v =  k,v log          

topic models

             .

  k,v
      k
j=1   j,v  1

k

5

sec

gain

promises

local
jobs

expectations

criminal
discretion

female
men
women

contractual
expectation

employment
industrial

markets
earnings
investors

see
sexual
note

breach
enforcing

employees
relations
unfair

this is inspired by the popular tfidf term score of vocabulary terms used
in information retrieval baeza-yates and ribeiro-neto (1999). the    rst
justice
expression is akin to the term frequency; the second expression is akin to
civil
the document frequency, down-weighting terms that have high id203
process
under all the topics. other methods of determining the difference between
federal
a topic and others can be found in (tang and maclennan, 2005).
see
of   cer
parole
inmates

visualizing a document. we use the posterior topic proportions    d,k
and posterior topic assignments zd,n,k to visualize the underlying topic de-

composition of a document. plotting the posterior topic proportions gives a
sense of which topics the document is    about.    these vectors can also be
used to group articles that exhibit certain topics with high proportions. note
that, in contrast to traditional id91 models (fraley and raftery, 2002),
articles contain multiple topics and thus can belong to multiple groups. fi-
nally, examining the most likely topic assigned to each word gives a sense

of the properties of the dirichlet, and replace these modeling choices with
an alternative distribution over the simplex.

figure 3. five topics from a 50-topic model    t to the yale
law journal from 1980   2003.

research
structure
managers

agreement
economic

discrimination

   rm
risk
large

harassment

supra
note

employer

perform

gender

case

bayesian methods in nlp

18

visualizing a topic

top words from the top topics (by term score)

expected topic proportions

sequence

region

pcr

identi   ed
fragments 

two

genes
three
cdna

analysis  

measured
average

range
values
different

size
three

calculated

two
low

residues
binding
domains

helix
cys

regions
structure
terminus
terminal

site

computer
methods
number

two

principle
design
access

processing
advantage
important

0
2

.

0

0
1

.

0

0
0

.

0

d. blei and j. lafferty.    topic models.    in a. srivastava and m. sahami, editors, id111: theory 
and applications. taylor and francis, 2009.

abstract with the most likely topic assignments

bayesian methods in nlp

19

domains

number

two

region

pcr

identi   ed
fragments 

genes
three
cdna

analysis  

range
values
different

two
low

helix
cys

regions
structure
terminus
terminal

site

visualizing a document

principle
design
access

size
three

two

calculated

processing
advantage
important

0
2
.
0

0
1
.
0

0
0
0

.

abstract with the most likely topic assignments

top ten similar documents

use the posterior topic probabilities of each document and 
the posterior topic assignments to each word

exhaustive matching of the entire protein sequence database
how big is the universe of exons?
counting and discounting the universe of exons
detecting subtle sequence signals: a id150 strategy for multiple alignment
ancient conserved regions in new gene sequences and the protein databases
a method to identify protein sequences that fold into a known three- dimensional structure
testing the exon theory of genes: the evidence from protein structure
predicting coiled coils from protein sequences
genome sequence of the nematode c. elegans: a platform for investigating biology

d. blei and j. lafferty.    topic models.    in a. srivastava and m. sahami, editors, id111: theory 
and applications. taylor and francis, 2009.

bayesian methods in nlp

20

region

sequence

measured
average

top words from the top topics (by term score)

articles contain multiple topics and thus can belong to multiple groups. fi-
nally, examining the most likely topic assigned to each word gives a sense
of how the topics are divided up within the document.

document similarity
finding similar documents. we can further use the posterior topic pro-
portions to de   ne a topic-based similarity measure between documents.
these vectors provide a low dimensional simplicial representation of each
two documents are similar if they assign similar 
document, reducing their representation from the (v   1)-simplex to the
probabilities to topics:
(k   1)-simplex. one can use the hellinger distance between documents
as a similarity measure,

regions
structure
terminus
terminal

processing
advantage
important

computer
methods
number

principle
design
access

residues
binding
domains

range
values
different

identi   ed
fragments 

genes
three
cdna

size
three

helix
cys

calculated

analysis  

two
low

site

two

two

pcr

0
0

0
1

0
2

0

0

0

.

.

.

expected topic proportions

(4)

document-similarityd, f =

topic models

7

abstract with the most likely topic assignments

k k=1        d,k          f,k 2

.

sequence

identi   ed
fragments 

region

pcr

measured
average

expected topic proportions

range
values
different

residues
binding
domains

computer
methods
number

top words from the top topics (by term score)

to illustrate the above three notions, we examined an approximation to
the posterior distribution derived from the jstor archive of science from
1980   2002. the corpus contains 21,434 documents comprising 16m words
when we use the 10,000 terms chosen by tfidf (see section 3.2). the
model was    xed to have 50 topics.

we illustrate the analysis of a single article in figure 4. the    gure depicts
the topic proportions, the top scoring words from the most prevalent topics,

exhaustive matching of the entire protein sequence database
how big is the universe of exons?
counting and discounting the universe of exons
detecting subtle sequence signals: a id150 strategy for multiple alignment
ancient conserved regions in new gene sequences and the protein databases
a method to identify protein sequences that fold into a known three- dimensional structure
testing the exon theory of genes: the evidence from protein structure
predicting coiled coils from protein sequences
genome sequence of the nematode c. elegans: a platform for investigating biology

top ten similar documents

abstract with the most likely topic assignments

regions
structure
terminus
terminal

processing
advantage
important

principle
design
access

size
three

helix
cys

calculated

genes
three
cdna

two
low

site

two

two

0
0
0

0
1
0

0
2
0

.

.

.

analysis  

top ten similar documents

bayesian methods in nlp

exhaustive matching of the entire protein sequence database
how big is the universe of exons?
counting and discounting the universe of exons
detecting subtle sequence signals: a id150 strategy for multiple alignment
ancient conserved regions in new gene sequences and the protein databases
a method to identify protein sequences that fold into a known three- dimensional structure
testing the exon theory of genes: the evidence from protein structure
predicting coiled coils from protein sequences
genome sequence of the nematode c. elegans: a platform for investigating biology

21
figure 4. the analysis of a document from science. doc-
ument similarity was computed using eq. (4); topic words
were computed using eq. (3).

collaborative    ltering
given a data set of movies and users that indicate 
which movies they like.  a new user likes n movies. 
task: recommend (n+1)th movie 
mapping this to lda:
vocabulary = movies
documents = users 
id203 of unseen movie m for a user 
with known movies mobs  
p(m | mobs) =         z p( m | z) p(z |   )p(   | mobs) d  

(exchange integral and sum: linear combination of k dirichlet 
expectations)  
bayesian methods in nlp

22

references

d.  blei, a.  ng, and m.  jordan. id44. journal of machine 
learning research, 3:993   1022, january 2003.
d. blei and j. lafferty.    topic models.    in a. srivastava and m. sahami, editors, 
id111: theory and applications. taylor and francis, 2009.
grif   ths, t., & steyvers, m. (2004). finding scienti   c topics. proceedings of the 
national academy of sciences, 101 (suppl. 1), 5228-5235.

steyvers, m. & grif   ths, t. (2006). probabilistic topic models. in t. landauer, d 
mcnamara, s. dennis, and w. kintsch (eds), latent semantic analysis: a road 
to meaning. laurence erlbaum

bayesian methods in nlp

23

