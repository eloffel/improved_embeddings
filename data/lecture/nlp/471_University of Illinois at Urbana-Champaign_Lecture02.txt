cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 2:
statistical id136s

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
office hours: by appointment

statistical id136s 
in nlp

bayesian methods in nlp

2

authorship attribution
given two data sets d1 and d2
(e.g. the known works of shakespeare and of marlowe)
where does the new data set d    come from?
(e.g. a disputed piece)
assume d1 ~   1 and d2 ~   2 
each set is generated by a different underlying distribution
if p(d    |   1 ) > p(d    |   2 ), assume d    is more like d1

this requires us to estimate the parameters    
from the data d

bayesian methods in nlp

3

computing p(d |    ) 
we are given a data set d with n items  d = (x1, ...., xn)

we assume d is generated from a distribution with 
parameters   

what is the id203 of d?

we assume the items xi are independent and 
identically distributed (i.i.d.):
 xi   ~ p(d |   )  =  p(x1, ...., xn |    ) =    i=1..n  p(xi |    )
= we assume the xi are exchangeable

bayesian methods in nlp

4

statistical id136s (i)
we are given a data set d with n items  d = (x1, ...., xn)

we assume d is generated (sampled) from an 
(unknown) distribution with parameters   : xi  ~    
  : the parameters of a id203 distribution
what is the id203 of the next item?
xn+1 =  x p(x | x1...xn)
what is the most likely next item? 
this requires the predictive distribution p(xn+1|x1...xn)
nlp applications: id38 
bayesian methods in nlp

x*n+1 = argmax x p(x | x1...xn)

5

statistical id136s (ii)
we may also be given a data set d with n items  

d = ((x1, y1) , ...., (xn, yn))

and need to know the most likely hidden value yn+1 
for a previously unseen item xn+1

yn+1  = argmax y p(y |  xn+1; d )

(supervised learning)

nlp applications: 
pos-tagging, parsing, id31, etc.. 

bayesian methods in nlp

6

statistical id136s (iii)
or, we may be given in incomplete data set

d = ((x1, _) , ...., (xn, _))

and need to know the most likely hidden value yn+1 
for a previously unseen item xn+1 

yn+1  = argmax y p(y |  xn+1; d )

(= unsupervised learning)
common notation: xi is observed, yi is hidden

bayesian methods in nlp

7

statistical id136 (|v)
or, we may be given in incomplete data set

d = ((x1, _) , ...., (xn,_))

where there are latent variables zi:

(xi, zi) ~   

we need to assign probabilities to xn+1, 
or    nd the most likely xn+1

p(x |  xn+1; d)

xn+1  = argmaxx p(x |  xn+1; d)

= (one kind of) partially supervised learning

bayesian methods in nlp

8

statistical id136 (v)
or, we may be given in incomplete data set

d = ((x1, y1_) , ...., (xn, yn,_))

where there are latent variables zi:
(xi, yi, zi) ~   

we need to know the most likely yn+1 for xn+1

yn+1  = argmaxy p(y |  xn+1; d)

= (one kind of) partially supervised learning

bayesian methods in nlp

9

bayesian statistics

bayesian methods in nlp

10

bayesian statistics
  : the parameters of a id203 distribution
probabilities represent degrees of belief
data d provide evidence for/against our beliefs.

we update our belief    based on evidence we see:

p ( |d) =

p ( )p (d| )
r p ( )p (d| )d 

for    xed data d, p(d|  ) is the likelihood of   

bayesian methods in nlp

11

bayesian statistics

p(   | d) 
posterior
id203 

of   

p ( |d) =

p(  ): prior
id203

p(d |   ): 
likelihood 

of d
of   
p ( )p (d| )
r p ( )p (d| )d 

p(d) = marginal likelihood of d

bayesian methods in nlp

bayesian statistics
the posterior p(   | d) is proportional 
to the prior p(  ) times the likelihood p(d |   ):

p(q|d)    p(q )p(d | q )

bayesian methods in nlp

13

discrete id203 distributions:
throwing a coin
bernoulli distribution:
id203 of success (=head,yes) in single yes/no trial 
-the id203 of head is p. 
-the id203 of tail is 1   p.
binomial distribution: 
prob. of the number of heads in a sequence of yes/no trials
the id203 of getting exactly k heads in n independent
yes/no trials is:

p (k heads, n   k tails) = n

k   pk(1   p)n k

bayesian methods in nlp

14

looking at the binomial 
distribution again

bayesian methods in nlp

15

the binomial distribution 
if p is the id203 of heads, the id203 of getting exactly 
k heads in n independent yes/no trials is given by the binomial 
distribution bin(n,p): 

p (k heads) =    n

k   pk(1   p)n k
k!(n   k)! pk(1   p)n k

n!

=

expectation e(bin(n,p)) = np
variance var(bin(n,p)) = np(1-p)

bayesian methods in nlp

16

parameter estimation
given data d=htthtt, what is the id203    of heads?

   

p (d|   )

id113 (id113):
use the    which has the highest likelihood p(d|   ).
   m le = arg max
maximum a posterior estimation (map):
use the    which has the highest posterior id203 p(   |d).
   m ap = arg max
bayesian estimation:
integrate over all     => compute the expectation of    given d:

p (   |d) = arg max

p (   )p (d|   )

   

   

p (x = h|d) =z 1

0

p (x = h|   )p (   |d)d    = e[   |d]

bayesian methods in nlp

17

binomial likelihood
what distribution does p (id203 of heads) have,
given that the data d consists of #h heads and #t tails?

 0.007

 0.006

 0.005

 0.004

 0.003

 0.002

 0.001

 0

 0

likelihood l((cid:101);d=(#heads,#tails)) for binomial distribution

l((cid:101),(5,5))
l((cid:101),(3,7))
l((cid:101),(2,8))

 0.2

 0.4

 0.6

 0.8

 1

bayesian methods in nlp

(cid:101)

18

maximum likelihood 
estimation for the coin    ip

p (d| )
 h(1    )t

     = arg max

 

= arg max

 

=

h

h + t

bayesian methods in nlp

19

bayesian estimation: 
what prior?

the posterior p(   |d)  is proportional to prior x likelihood:

the likelihood p(d|  ) of a binomial is p(d|  ) =   h(1-  )t

p(   |d)    p(  )p(d|  )

assume the prior p(  ) is proportional to powers 
of    and (1-  ):  p(  )        a(1-  )b
then the posterior p(   |d) will also be proportional to 
powers of    and (1-  ):
      p(   |d)    p(  ) p(d|  ) 
                 =    a(1-  )b   h(1-  )t 
                 =    a+h(1-  ) b+t 

bayesian methods in nlp

20

in search of a prior for coin    ips...
we would like something of the form:

but -- this looks just like the binomial:

p ( ) /  a(1    )b
p (k heads) =    n

k   pk(1   p)n k
k!(n   k)! pk(1   p)n k

n!

=

   . except that k is an integer and    is a real with 0 <    < 1.

bayesian methods in nlp

21

the gamma function
the gamma function   (x) is the generalization of the factorial 
x! (or rather (x-1)!) to the reals: 

 ( ) =z 1

0

x  1e xdx for   > 0

for x >1,   (x) = (x-1)  (x-1).

for positive integers,   (x) = (x-1)!

bayesian methods in nlp

22

the gamma function

 25

 20

 15

 10

 5

 0

 0

(cid:75)(x) function

 1

 2

 3

 4

 5

bayesian methods in nlp

23

the beta distribution
a random variable x (0 < x < 1) has a beta distribution with 
(hyper)parameters    (   > 0) and    (   > 0) if x has a continuous 
distribution with id203 density function

 (    +  )
 (   ) ( ) x    1(1   x)  1

p (x|   ,  ) =
the    rst term is a id172 factor (to obtain a distribution)

z 1

0

x    1(1   x)  1dx =

 (    +  )
 (   ) ( )

expectation: 

   

   + 

bayesian methods in nlp

24

beta as prior for binomial
given a prior p(   |  ,  ) = beta(  ,  ),  and data d=(h,t),
what is our posterior?
p (   |   ,  , h, t ) / p (h, t|   )p (   |   ,  )

/    h(1      )t        1(1      )  1

=    h+    1(1      )t +  1

with id172
p (   |   ,  , h, t ) =

 (h +     + t +  )
 (h +    ) (t +  )    h+    1(1      )t +  1

= beta(    + h,   + t )

bayesian methods in nlp

25

so, what do we predict?
our bayesian estimate for the next coin    ip p(x=1 | d):

0

p (x = h|d) = z 1
= z 1
= e[   |d]
= e[beta(h +    , t +  )]

p (x = h|   )p (   |d)d   
   p (   |d)d   

0

=

h +    

h +     + t +  

bayesian methods in nlp

26

beta(  ,  ) with    >1,    >1: 
unimodal 

 7

 6

 5

 4

 3

 2

 1

 0

beta distribution beta((cid:95), (cid:96))

beta(1.5,1.5)
beta(3,1.5)
beta(3,3)
beta(20,20)
beta(3,20)

 0

 0.2

 0.4

 0.6

 0.8

 1

  

bayesian methods in nlp

27

beta(  ,  ) with    <1,    <1:
u-shaped

 6

 5

 4

 3

 2

 1

 0

beta distribution beta((cid:95), (cid:96))

beta(0.1,0.1)
beta(0.1,0.5)
beta(0.5,0.5)

 0

 0.2

 0.4

 0.6

 0.8

 1

  

bayesian methods in nlp

28

beta(  ,  ) with    =  : symmetric 
(  =  =1: uniform) 

 3.5

 3

 2.5

 2

 1.5

 1

 0.5

 0

 0

beta distribution beta((cid:95), (cid:96))

beta(0.1,0.1)
beta(1,1)
beta(2,2)

 0.2

 0.4

 0.6

 0.8

 1

  

bayesian methods in nlp

29

beta(  ,  ) with   <1,    >1:
strictly decreasing

 8
 7
 6
 5
 4
 3
 2
 1
 0

beta distribution beta((cid:95), (cid:96))

beta(0.1,1.5)
beta(0.5,1.5)
beta(0.5,2)

 0

 0.2

 0.4

 0.6

 0.8

 1

  

bayesian methods in nlp

30

beta(  ,  ) with    = 1,    >1
   = 1, 1<    < 2: strictly concave.
   = 1,    = 2: straight line
   = 1,    > 2: strictly convex

beta distribution beta((cid:95), (cid:96))

 3.5

 3

 2.5

 2

 1.5

 1

 0.5

 0

 0

bayesian methods in nlp

beta(1,1.5)
beta(1,2)
beta(1,3)

 0.2

 0.4

 0.6

 0.8

 1

  

31

conjugate priors
the beta distribution is a conjugate prior to the 
binomial: the resulting posterior is also a beta 
distribution.

all members of the exponential family of 
distributions have conjugate priors.

examples:
-multinomial: conjugate prior = dirichlet
-gaussian: conjugate prior = gaussian

bayesian methods in nlp

32

conjugate priors
the posterior is proportional to prior x likelihood:
p(   |d)    p(  ) p(d|  )
conjugate priors:
posterior is the same kind of distribution as prior.
for binomial likelihood:
conjugate prior  = beta distribution

bayesian methods in nlp

33

discrete id203 distributions:
rolling a die
categorical distribution:
id203 of getting one of n outcomes in a single trial.
the id203 of category/outcome ci is pi   (   pi = 1)
multinomial distribution:
id203 of observing each possible outcome ci 
exactly xi times in a sequence of n trials

p (x1 = xi, . . . , xn = xn) =

n!

x1!       xn! px1

1        pxn

n

if

n i=1

xi = n

bayesian methods in nlp

34

moving on to multinomials

bayesian methods in nlp

35

multinomials have a dirichlet prior
multinomial distribution:
id203 of observing each possible outcome ci exactly xi 
times in a sequence of n trials:
n!

p (x1 = xi, . . . , xk = xk) =

if

xi = n

x1!       xk!    x1

1           xk

k

nxi=1

dirichlet prior:

dir(   |   1, ...   k) =

 (   1 + ... +    k)

 (   1)... (   k) yk=1

      k 1
k

bayesian methods in nlp

36

multinomial variables
-in nlp, x is often a discrete random variable 
that can take one of k states.

-we can represent such xs as k-dimensional vectors
in which one xk =1 and all other elements are 0
 x = (0,0,1,0,0)t

-denote id203 of xk =1 as   k with 0       k     1 and    k   k =1
then the id203 of x is:

p (x|  ) =

  xk
k

kyk=1

bayesian methods in nlp

37

multinomial likelihood
what is the likelihood of d = x1   . xi ... xn?

de   ne
mk

:=

xnk

nxn=1

(= #observations with xk=1)

p (d|  ) =

=

=

:=

nyi=1
nyi=1
kyk=1
kyk=1

p (xi|  )
kyk=1
  (pn xnk)

  xnk
k

k

  mk
k

bayesian methods in nlp

38

the likelihood depends only on the mk.mk are suf   cient statisticsmultinomials: dirichlet prior
the joint distribution of (m1,   ,mk) conditioned on    
and n is a multinomial distribution:
 
m1!       mk!    m1
kxi=1
xk = n

p (m1, . . . , mk = xk) =

          xk

n!

if

k

1

multinomials have a dirichlet prior with 
hyperparameters   :
dir(   |   1, ...   k) =

 (   1 + ... +    k)

 (   1)... (   k) yk=1

      k 1
k

bayesian methods in nlp

39

the dirichlet
a dirichlet is con   ned to a simplex (here   =(  1,  2,  3)) 

  2

  3

  1

(figure from chris bishop   s prml book & website)

bayesian methods in nlp

40

examples of the dirichlet

{  k} = 0.1

{  k} = 1

{  k} = 10

(all    gures from chris bishop   s prml book & website)

bayesian methods in nlp

41

dirichlet as conjugate prior
given a prior  dir(  |  ) and data d with suf   cient statistics 
m=(m1,   ,mk), the posterior is

p(  |d,  )   p (d|  )p (  )
   k 1+mk
k

 

kyk=1

the normalized posterior is: 

p(  |d,    ) = dir(  |    + m)

=

 (   1 + . . . +    k + n )

 (   1 + m1)     . . .      (   k + mk)

bayesian methods in nlp

     k 1+mk
k

kyk=1

42

likelihood, prior and posterior for  
the dirichlet/multinomial

likelihood p(y|q ) =

prior p(q|a)   

posterior p(q|y,a)   

k

   

k=1
k

   

k=1
k

   

k=1

q mk
k

q ak 1
k

q mk+ak 1
k

bayesian methods in nlp

43

id113 vs bayesian estimate
maximum likelihood estimate:
maximize ln p(d|  ) wrt.   k under the constraint that       k = 1
(...use lagrange multipliers   )

  m le
k

= mk
n

bayesian estimate:

  be
k =

mk +    k

n +pk

k0=1    k0

bayesian methods in nlp

44

more about conjugate priors"
-we can interpret the hyperparameters as    pseudocounts   

-sequential estimation (updating counts after each 
observation) gives same results as batch estimation

-add-one smoothing (laplace smoothing) = uniform prior

-on average, more data leads to a sharper posterior
(sharper = lower variance)

bayesian methods in nlp

45

today   s reading
-bishop, pattern recognition and machine learning, ch. 2

bayesian methods in nlp

46

