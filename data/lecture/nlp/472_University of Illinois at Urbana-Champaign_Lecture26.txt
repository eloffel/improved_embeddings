discovering	
   morphological	
   paradigms	
   from	
   
plain	
   text	
   using	
   a	
   dirichlet	
   process	
   mixture	
   

model	
   

	
   

dreyer	
   et	
   al.	
   (2011)	
   

amey	
   chaugule	
   

achaugu2@illinois.edu	
   

introduceon	
   

       staesecal	
   nlp	
   is	
   oien	
   very	
   di   cult	
   for	
   

morphologically	
   rich	
   languages.	
   

       one	
   must	
   learn	
   lexical	
   features	
   individually	
   for	
   each	
   
word	
   form	
   as	
   it	
   is	
   not	
   possible	
   to	
   generalise	
   across	
   
in   eceons.	
   

       this	
   paper	
   proposes	
   a	
   mostly	
   unsupervised	
   
generaeve	
   probabilisec	
   model	
   to	
   capture	
   
morphological	
   relaeonships.	
   

introduceon	
   

       the	
   id136	
   algorithm	
   reconstructs	
   token,	
   type	
   &	
   

grammar	
   about	
   a	
   language   s	
   morphology.	
   

       tokens:	
   each	
   word	
   in	
   the	
   corpus	
   has	
   3	
   tags.	
   ex.	
   broken	
   

(1)	
   pos	
      	
   verb	
   (2)	
   in   eceon	
      	
   past	
   pareciple	
   and	
   (3)	
   
lexeme	
      	
   break.	
   

       types:	
   this	
   is	
   a	
   morphological	
   paradigm,	
   which	
   in	
   our	
   

case	
   is	
   a	
   grid	
   of	
   all	
   the	
   in   ected	
   forms	
   of	
   a	
   some	
   lexeme.	
   
       grammar:	
   parameter	
     	
   describes	
   the	
   general	
   pawerns	
   of	
   

the	
   language.	
   mote	
   carlo	
   em	
   is	
   used	
   to	
   esemate	
   this.	
   

overview	
   of	
   the	
   model	
   
modeling	
   morphological	
   alterna8ons	
   

       given	
   a	
   lemma	
   x	
   we	
   could	
   predict	
   its	
   in   ected	
   form	
   

y.	
   

       this	
   joint	
   distribueon	
   is	
   a	
   family	
   which	
   can	
   be	
   

described	
   by	
   this	
   log-     linear	
   model	
   :	
   

       f	
   is	
   local	
   feature	
   vector	
   and	
   parameter	
     	
   could	
   

penalise	
   or	
   reward	
   speci   c	
   features.	
   

overview	
   of	
   the	
   model	
   
modeling	
   morphological	
   paradigm	
   

       the	
   underlying	
   presumpeon	
   here	
   is	
   that	
   some	
   

language	
   speci   c	
   distribueon	
   p(  )	
   de   nes	
   whether	
   a	
   
paradigm	
     	
   is	
   a	
   grammaecal	
   way	
   for	
   a	
   lexeme	
   to	
   
express	
   itself.	
   

       learning	
   p(  )	
   helps	
   us	
   reconstruct	
   paradigms.	
   
       p(  )	
   is	
   modeled	
   as	
   a	
   renormalised	
   product	
   of	
   many	
   

pairwise	
   distribueons	
   prs(xr,xs)	
   each	
   having	
   log	
   
linear	
   form.	
   

overview	
   of	
   the	
   model	
   
modeling	
   morphological	
   paradigm	
   

this	
   is	
   an	
   undirected	
   graphical	
   model	
   (mrf)	
   over	
   
string-     valued	
   random	
   variables	
   xs.	
   
	
   

overview	
   of	
   the	
   model	
   
	
   

	
   modeling	
   the	
   lexicon	
   

	
   

	
   

	
   

1.    choose	
   parameter	
     	
   of	
   the	
   mrf	
   which	
   de   nes	
   
p(  ):which	
   paradigms	
   are	
   a	
   priori.	
     	
   is	
   sampled	
   
from	
   a	
   gaussian	
   prior.	
   

is	
   sampled	
   from	
   a	
   dirichlet	
   process.	
   

2.    choose	
   a	
   distribueon	
   over	
   abstract	
   lexemes	
   which	
   
3.    for	
   each	
   lexeme	
   choose	
   a	
   distribueon	
   over	
   its	
   
in   eceons.	
   this	
   is	
   again	
   sampled	
   from	
   a	
   dirichlet.	
   
4.    for	
   each	
   lexeme	
   choose	
   a	
   paradigm	
   that	
   can	
   be	
   

used	
   to	
   express	
   the	
   lexeme	
   orthographically.	
   

id136	
   and	
   learning	
   
gibbs	
   sampling	
   over	
   the	
   corpus	
   

	
   

       the	
   id136	
   task	
   is	
   to	
   extract	
   the	
   the	
   lexeme	
   and	
   

in   ecbon	
   per	
   token.	
   

       using	
   a	
   collapsed	
   gibbs	
   sampler,	
   reanalysis	
   of	
   of	
   
each	
   token	
   is	
   repeatedly	
   guessed	
   in	
   context	
   of	
   all	
   
other	
   tokens.	
   

       eventually	
   similar	
   tokens	
   get	
   clustered	
   together.	
   

id136	
   and	
   learning	
   

a	
   state	
   of	
   the	
   gibbs	
   sampler.	
   note	
   that	
   each	
   of	
   the	
   tokens	
   i	
   has	
   been	
   tagged	
   
with	
   pos	
   ti,	
   lexeme	
   li	
   and	
   in   eceon	
   si.	
   

id136	
   and	
   learning	
   

key	
   intuieons	
      	
   
1.    current	
   analyses	
   of	
   other	
   tokens	
   tagged	
   with	
   same	
   
part	
   of	
   speech	
   implies	
   a	
   posterior	
   distribueon	
   over	
   
that	
   pos	
   lexicon.	
   

2.    belief	
   propagaeon	
   gives	
   us	
   which	
   other	
   in   eceon	
   of	
   
a	
   given	
   lexeme	
   maps	
   to	
   a	
   token	
   with	
   same	
   spelling.	
   

3.    the	
   number	
   of	
   tokens	
   associated	
   with	
   a	
   lexeme	
   

suggests	
   popularity.	
   (e.g.	
   chinese	
   restaurant	
   
process	
      rich	
   get	
   richer   )	
   

id136	
   and	
   learning	
   

monte	
   carlo	
   em	
   training	
   of	
     	
   

	
   

       for	
   a	
   given	
     	
   gibbs	
   sampler	
   converges	
   to	
   posterior	
   

distribueon	
   over	
   analyses	
   of	
   the	
   enere	
   corpus.	
   

       to	
   improve	
   the	
   esemate,	
     	
   is	
   periodically	
   adjusted	
   to	
   

maximise	
   the	
   id203	
   of	
   most	
   recent	
   samples.	
   

id136	
   and	
   learning	
   

      

collapsed	
   representa8on	
   of	
   the	
   lexicon	
   

       lexicon	
   is	
   collapsed	
   out	
   of	
   the	
   sampler.	
   
      

if	
   (l,s)	
   points	
   to	
   at	
   least	
   one	
   token	
   i	
   then	
   we	
   know	
   that	
   
(l,s)	
   is	
   spelt	
   as	
   wi.	
   
if	
   the	
   spelling	
   of	
   (l,s)	
   isn   t	
   known	
   but	
   some	
   other	
   
spellings	
   in	
   l   s	
   paradigm	
   are	
   known	
   then	
   store	
   a	
   
truncated	
   distribueon	
   that	
   gives	
   25	
   most	
   likely	
   spellings	
   
of	
   (l,s).	
   
       last	
   case	
   is	
   where	
   we	
   know	
   nothing	
   about	
   l	
   thus	
   all	
   such	
   l	
   

share	
   the	
   same	
   marginal	
   distribueon	
   over	
   (l,s).	
   
probabilisec	
      nite	
   state	
   automata	
   is	
   used	
   to	
   approximate	
   
this	
   marginal.	
   

mixture	
   model	
   

       this	
   id136	
   model	
   clusters	
   words	
   together	
   

by	
   tagging	
   them	
   with	
   the	
   same	
   lexeme.	
   

       thus	
   the	
   base	
   distribueon	
   p(  )	
   predicts	
   word	
   

co-     occurrence	
   within	
   a	
   paradigm.	
   

       thus	
   the	
   model	
   assigns	
   words	
   to	
   a	
   parecular	
   

in   eceon	
   slot	
   in	
   the	
   paradigm.	
   

dirichlet	
   process	
   mixture	
   model	
   

       natural	
   languages	
   have	
   an	
   in   nite	
   lexicon	
   

although	
   most	
   lexemes	
   have	
   a	
   very	
   low	
   
id203.	
   

       thus	
   the	
   mixture	
   model	
   uses	
   in   nite	
   number	
   of	
   

mixture	
   components.	
   

       dpmm	
      rst	
   generates	
   a	
   distribueon	
   over	
   

countably	
   many	
   lexemes	
   and	
   then	
   generated	
   a	
   
weighted	
   paradigm	
   per	
   lexeme.	
   

formal	
   generaeve	
   model	
   

1.	
   	
    first	
   grammar	
   variables	
   need	
   to	
   be	
   selected	
   from	
   the	
   prior.	
   

2.    let	
   dt(  )	
   be	
   a	
   distribueon	
   over	
   paradigms	
   of	
   pos	
   t.	
   for	
   each	
   
discovered	
   lexeme	
   (t,	
   l)	
   paradigm	
     t,l	
   can	
   be	
   drawn	
   from	
   dt.	
   
3.    for	
   each	
   pos	
   t	
   langauges	
   has	
   a	
   distribueon	
   gt(l)	
   over	
   

4.   

lexemeswhere	
   gt	
   is	
   drawn	
   from	
   a	
   dirichlet	
   process	
   
dp(gt,  t)	
   where	
   g	
   is	
   the	
   base	
   distribueon	
   over	
   lexemes	
   l.	
   
in   eceonal	
   distribueon	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   for	
   each	
   tagged	
   
lexeme	
   (t,l)	
   the	
   language	
   speci   es	
   some	
   distribueon	
   ht.	
   ht	
   is	
   
a	
   log	
   linear	
   distribueon	
   with	
   parameters	
   that	
   refer	
   to	
   
features	
   of	
   in   eceon.	
   ht,l	
   is	
   an	
   independent	
   draw	
   from	
   a	
   
   nite	
   dimensional	
   dirichlet	
   distribueon	
   with	
   mean	
   ht	
   and	
   
concentraeon	
   parameter	
     .	
   

formal	
   generaeve	
   model	
   

5.

	
   
6.
	
   
7.

	
   the	
   pos	
   tag	
   sequence	
   for	
   the	
   experimental	
   model	
   is	
   given	
   but	
   in	
   a	
   general	
   
	
   use	
   case	
   to	
   discover	
   tags,	
   we	
   can	
   model	
   the	
   tag	
   sequence	
   by	
   a	
   markov	
   model.	
   

	
   a	
   lexeme	
   token	
   depends	
   on	
   its	
   tag.	
   draw	
   li	
   from	
   ge.	
   thus,	
   	
   

	
   in   eceon	
   slot	
   depends	
   on	
   the	
   tagged	
   lexeme.	
   we	
   draw	
   si	
   from	
   	
   

8.    given	
   a	
   tag,	
   lexeme	
   and	
   in   eceon	
   at	
   posieon	
   i,	
   word	
   wi	
   is	
   generated	
   by	
   

	
   simply	
   looking	
   up	
   its	
   spelling	
   in	
   an	
   appropriate	
   paradigm.	
   

	
   
9.    gt	
   is	
   unspeci   ed	
   given	
   the	
   sampler	
   state	
   but	
   it	
   only	
   appears	
   in	
   3	
   &	
   6	
   and	
   
can	
   be	
   integrated	
   out	
   of	
   their	
   product	
   to	
   get	
   a	
   collapsed	
   sub-     model	
   which	
   
generates	
   p(l	
   |	
   t,	
     )	
   directly.	
   this	
   is	
   akin	
   to	
   chinese	
   restaurant	
   whole	
   tables	
   
are	
   labeled	
   with	
   lexemes	
   each	
   customer	
   i	
   enters	
   restaurant	
   e,	
   in	
   turn	
   and	
   li	
   
denotes	
   the	
   table	
   he	
   joins.	
   

	
   

formal	
   generaeve	
   model	
   

	
   10.	
    	
   similarly	
   in   nitely	
   many	
   lexeme-     speci   c	
   distribueons	
   ht,l	
   can	
   be	
   integrated	
   
	
   
	
   

	
   out	
   of	
   product	
   of	
   4	
   &	
   7	
   and	
   replaced	
   with	
   a	
   collapsed	
   distribueon	
   
	
   

	
   	
   

	
   

experiments	
   

       as	
   corpus	
   they	
   used	
      rst	
   1	
   million	
   and	
   10	
   

million	
   words	
   from	
   wacky.	
   

	
   
       verbal	
   in   eceonal	
   paradigms	
   from	
   celex	
   
morphological	
   database	
   were	
   used	
   to	
   seed	
   
the	
   paradigms.	
   	
   

type	
   based	
   evaluaeon	
   

token	
   based	
   evaluaeon	
   

conclusion	
   

       the	
   authors	
   formulated	
   a	
   a	
   framework	
   for	
   

obtaining	
   both	
   morphological	
   annotaeons	
   and	
   
the	
   unbounded	
   lexicon	
   that	
   completed	
   the	
   
morphological	
   paradigms.	
   
       they	
   were	
   able	
   to	
   run	
   the	
   sampler	
   over	
   a	
   
corpus	
   of	
   10	
   million	
   words	
   and	
   by	
   inferring	
   
everything	
   jointly,	
   they	
   were	
   able	
   to	
   reduce	
   
the	
   prediceon	
   error	
   for	
   in   eceons	
   by	
   upto	
   
10%.	
   

