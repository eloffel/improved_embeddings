unsupervised coreference 

resolution in a nonparametric 

bayesian model 

aria haghighi and dan klein 

presented by brandon norick 

overview 

    introduction 

    preliminaries 

    coreference resolution models 

    experiments 

    conclusion 

introduction 

    when speaking or writing natural language 

there are two processes which govern 
references to entities 

    new entities are introduced, generally with proper 

or nominal expressions 

    references are made back to entities which have 
already been introduced, generally with pronouns 

    problem: how can a computer determine 

which entity references actually refer to the 
same entity (i.e., are coreferent)?  

introduction 

an example 

the weir group, whose headquarters is in the us, 
is a large, specialized corporation investing in the 
area of electricity generation. this power plant, 
which will be situated in rudong, jiangsu, has an 
annual generation capacity of 2.4 million kilowatts. 

introduction 

an example 

the weir group, whose headquarters is in the us, 
is a large, specialized corporation investing in the 
area of electricity generation. this power plant, 
which will be situated in rudong, jiangsu, has an 
annual generation capacity of 2.4 million kilowatts. 

introduction 

an example 

the weir group, whose headquarters is in the us, 
                                                                  
is a large, specialized corporation investing in the 
                                                                  
area of electricity generation. this power plant, 
                                                                          
which will be situated in rudong, jiangsu, has an 
                                                                                 
annual generation capacity of 2.4 million kilowatts. 
                                    

for the problem of coreference resolution, 
we are only interested in entity references 
and the rest of the text is ignored. 

background 

related work 

    primary approach is to treat the problem as a 

set of pairwise coreference decisions 

    use discriminative learning with features encoding 

properties such as distance and environment 

    however, there are several problems with this 

approach 

    in order to have rich features, a large amount of 

data is required, which is typically unavailable 

    in order to partition, a greedy approach is generally 

taken which relies solely on the pairwise model 

preliminaries 

    each document consists of a set of mentions 

(usually noun phrases) 

    a mention is a reference to some entity 

    there are three types of mentions: 

    proper (names) 

    nominal (descriptions) 

    pronominal (pronouns) 

    therefore, the coreference resolution problem 
is to partition the mentions according to their 
referents 

preliminaries 

    during the design process for the final model, 

the authors used data from the automatic 
context extraction (ace) 2004 task 

    this data was used to test performance, as well as 

for hyperparameter selection 

    used english translations of the arabic and 

chinese treebanks 

    95 documents, 3905 mentions 

 

preliminaries 

some assumptions 

    the system assumes that the following data is 

provided as input: 

    the true mention boundaries 

    the head words for mentions (i.e., the    main    word 

of a mention, such as    a big sheep dog) 

    the mention types 

    unlike related work, id39 

labels and part of speech tags are not required 

coreference resolution models 

generative story 

                                                                  
 
                                                                  
 
                                 
                                          
                                                                             
     
                                    

coreference resolution models 

generative story 

first, generate entities 

weir group 

weir group 

weir hq 

united states 

weir group 

                                                                  
 
                                                                  
 
                                 
                                          
weir plant 
                                                                             
     
                                    

weir plant 

rudong 

jiangsu 

coreference resolution models 

generative story 

then, generate mentions according to these entities 

weir group 

weir group 

weir hq 

united states 

weir group 

                                                                  
the weir group  whose headquarters         the us 
 
 
                                                                  
                                    corporation 
 
 
                                 
                                                        power plant 
                                          
 
weir plant 
                                                                             
which                              rudong  jiangsu 
     
                                    

weir plant 

rudong 

jiangsu 

coreference resolution models 

finite mixture model 

    documents are independent, with the 

exception of some global hyperparameters 

    each document is a mixture of a fixed number 

of components, k 

    the distribution over entities is drawn from a 

symmetric dirichlet distribution 

 

    the entity for each mention is drawn from beta 

 

 

coreference resolution models 

finite mixture model 

    each entity is associated with a multinomial 
distribution over head words, these are also 
drawn from a symmetric dirichlet distribution 

 

    the head word for each mention is drawn from 

the associated multinomial 

    the graphical model for this  

approach, where shaded nodes 
represent observed variables 

coreference resolution models 

finite mixture model 

    id150 to obtain samples from p(z|x) 

where x represents the variables associated 
with mentions, in this case only the head 
words 

 

 

 

coreference resolution models 

finite mixture model 

    a big problem with this model is that the 

number of entities, k, must be fixed a priori 

    what we want is for the model to be able to 

select k itself, in a manner which fits the data 

    in order to accomplish this in a principled 
manner, the authors suggest the use of a 
dirichlet process (dp), which allows for a 
countably infinite number of entities 

coreference resolution models 

infinite mixture model 

    the new graphical model, where the dirichlet 

priors have been replaced 

    now: 

 

 

 

 

coreference resolution models 

infinite mixture model 

    this approach is still rather crude, and has 

trouble with pronominal mentions 

 

    the entity specific multinomials in this 

approach are effective for proper and some 
nominal mentions, but do not make sense for 
pronominal mentions 

    all entities can be referred to with pronouns, and 

the choice depends on entity properties rather than 
the specific entity 

coreference resolution models 

pronoun head model 

    now, when generating a head word for a 

mention we consider more than the entity 
specific multinomial distribution over head 
words 

    also consider entity specific distributions 

over the properties 

    entity type (person, location, organization, misc.) 

    gener (male, female, neuter) 

    number (single, plural) 

coreference resolution models 

pronoun head model 

    each of these property distributions is 
assumed to be a draw from symmetric 
dirichlet distributions with small 
concentration parameters, encouraging 
peakedness 

coreference resolution models 

pronoun head model 

    the generative story for mentions is now 

slightly different 

    draw an entity type t, a gender g, and a number n 

from the appropriate distributions 

    draw a mention type m from a global multinomial 

(sym. dir. with   m) 

    a head word is then generated conditioned on 

these properties and the mention type 

    if m is not pronoun, the head word is drawn directly from 

the entity head word multinomial as before 

    otherwise, the head word is drawn based on the global 

pronoun head distribution, conditioning on the properties 

coreference resolution models 

pronoun head model 

    more specifically, 

 

 

 

    use the prior on theta, the parameters for the 

global pronoun head distribution, to encode 
compatible entity types for a pronoun (e.g., 
   he    with    person   ) 

coreference resolution models 

pronoun head model 

an example of the parameters 
associated with an entity 

the graphical model for this 
approach 

coreference resolution models 

pronoun head model 

    substantial improvement, achieving a muc f1 

of 64.1 

 

    however, there is no local preference for 
pronominal mentions exists in this model 

    introduce salience to address this issue 

 

coreference resolution models 

adding salience 

    the new graphical model is as follows: 

coreference resolution models 

adding salience 

    as the mentions in a document are generated, 

a list of active entities and their salience 
scores is maintained 

    when an entity is mentioned, its score is 

incremented by 1 

    when moving to generate the next mention, all 

scores decay by a factor of 0.5 

    based on the list of scores, l, each entity z 

has a rank on this list which can be in one of 
five buckets: top (1), high (2-3), mid (4-6), low 
(7+), or none 

coreference resolution models 

adding salience 

    this changes the sampling equation, which 
now has to account for how future salience 
values change when sampling an entity 
 
 

    this approach fixes the final error exhibited by 

the previous models, and gives an f1 of 71.5 

 

coreference resolution models 

adding salience 

    the posterior distribution of mention type m 

given salience s is described in the following 
table 

 

 

 

    pronoun type is preferred for the entities with 

top or high salience, whereas proper and 
nominal types are preferred otherwise 

figure from slides by aria haghighi 

coreference resolution models 

cross document coreference 

    sharing data across documents is desirable, 
allowing for information about the properties 
of entities to be pooled across all documents 

    this can easily be accomplished with a 
hierarchical dirichlet process for entity 
selection 

    assume the pool of entities is global, with global 

mixing weights   0 drawn from a dp prior with 
parameter  

    each document draws its own distribution   i from a 

dp centered on   0  

 

coreference resolution models 

cross document coreference 

    the graphical model for this approach: 

 

 

 

 

 

 

    results improved to an f1 score of 72.5 

 

experiments 

muc-6 

 

 

 

    as this is an unsupervised method, it is able 

to make use of unannotated data (with respect 
to coreferences) 

    the result labeled +dryrun-train displays this 

by including 191 unannotated documents from the 
muc-6 dryrun training set 

experiments 

muc-6 

 

 

 

    including data from a different corpora can 

even improve results 

    the result labeled +english-nwire includes data 

from the ace dataset, a different corpora from a 
different time period, and results still improve 

experiments 

muc-6 

 

 

 

    recent supervised results gave an f1 score of 

73.4 on the muc-6 test 

    relatively close the best unsupervised result of 

70.3 

experiments 

ace 2004 

 

 

 

    recent supervised results are 67.1 f1 and 69.2 

f1 for the english nwire and bnews 
respectively 

discussion 

errors 

    the largest source of error is from coreferent 

proper and nominal mentions 

    george w. bush, president of the us, visited idaho 

    this is unmodeled in the proposed system 

conclusion 

    a nonparametric bayesian approach is 

proposed for entity coreference 

    the proposed model accounts for the 

tendency to favor pronominal head words for 
coreferences in close proximity 

    a hierarchical dirichlet process is used to 

share data across documents 

    results comparable to supervised methods 

are achieved 

