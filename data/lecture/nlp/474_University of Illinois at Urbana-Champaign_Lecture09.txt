correlated topic models

authors: blei and lafferty, 2006

reviewer: casey hanson

recap id44

             set of documents.
         = set of topics.
         = set of all words. |    | words in each doc.
                 multi over topics for a document d         .          ~             (    )
                 multi over words in a topic,              .         ~            (    )
            ,         topic selected for word      in document     .         ,    ~multi(        )
            ,                     word in document     .         ,    ~ multi(            ,    )

id44

    need to calculate posterior:     (    1:    ,     1:    ,1:     ,     1:    |    1:    ,1:    ,     ,     )

            (    1:    ,     1:    ,1:    ,     1:    ,     1:    ,1:    ,     ,     )
    id172 factor,            
    need to use approximate id136. 

          (. . ), is intractable

    id150

    drawback 

    no intuitive relationship between topics.

    challenge 

    develop method similar to lda with relationships between topics.

normal or gaussian distribution

          =

            2
2    2

       

1

     2    

    continuous distribution

    symmetrical and defined for        <      <    

    parameters:          ,     2

             mean
        2     variance
             standard deviation

    estimation from data:      =     1             

   

      =

1
    
         2 =

             

     =1
1
    

     =1

    

                  2

multivariate gaussian distribution:      dimensions

          =              1              =

1

2         /2 det   

1
2

       

                     1(           )

         =     1             

    ~    (    ,   )

                  x 1 vector of means for each dimension
                  x      covariance matrix.

example: 2d case
         =           =     [    1]

    [    2] =     1

    2

       =

         1         1

2

         1         1     2         2

         1         1     2         2

         2         2

2

2d multivariate gaussian:

       =

2
        1

        1,    2        1        2

        1,    2        1        2

2
        2

    topic correlations on off diagonal

            1,    2        1        2=          1         1     2         2 =      =1

    

        ,1       1         ,2       2

    

    covariance matrix is diagonal!

matlab demo

   back to topic models

    how can we adapt lda to have correlations between topics.

    in lda, we assume two things:

    assumption 1: topics in a document are independent.         ~            (    )
    assumption 2: distribution of words in a topic is stationary.         ~(    )

    to sample topic distributions for topics that are correlated, we need 

to correct assumption 1.

exponential family of distributions

    family of distributions that can be placed in the following form:

               =                                                     

    ex: binomial distribution:      =     

         |     =

    
    

        (1         )           ,          0,1,2,     ,     

        (    ) = log

    

1       

         =     

     ,                 =      log 1         ,                               =     

natural parameterization

          =

       log

    

    

1       

    
    

+       log 1       

categorical distribution

    multinomial n=1: 

             1 =     1;          1 =                  1
    where     1 = 1 0 0. . 0      (iverson bracket or indicator vector)
             = 1

    parameters:     

         =     1     2     3 , where                = 1

            =

    1
        

    2
        

1

    log         = log

    1
        

log

    2
        

1

exponential family multinomial with n=1

                            :                    =                      
    we want:                =                                                     

                       =                        log      =1              =
    note: k-1 independent dimensions in multinomial

                       
     =1             

            = [log

    1
        

                          =   

        
        

log

    .0],             = log

    2
        
                          
       1         

1+     =1

           

verify: classroom participation

    given:      = [log
    show:                     =                       =                        log      =1             

    0]

log

    1
        

    2
        

intuition and demo

    can sample      from any number of places.

    choose normal (allows for correlation between topic dimensions)

    get a topic distribution for each document by sampling: 

     ~            1     ,     
    what is the     

    expected deviation from last topic: log

        
        

    negative means push density towards last topic (         < 0,          >         )

    what about the covariance 

    shows variability in deviation from last topic between topics.

     = 0 0     ,      = [1 0; 0 1]

  0.00.250.50.751.0favoring topic 3

     =    0.9,    0.9 ,

   = [1 0; 0 1]

     =    0.9,    0.9 ,

   = [1     0.9;    0.9 1]

favoring topic 3: 

     =    0.9,    0.9 ,

   = [1 0.4; 0.4 1]

exercises

correlated topic model

    algorithm: 
                    

    draw         |     ,    ~     (    ,   )
                 1          :

    draw topic assignment

            ,    |         ~ categorical              

    draw word

            ,    |         ,    ,     1:     ~ categorical             

    parameter estimation:

    intractable
    user variational id136 (later)

evaluation i: ctm on test data

evaluation ii: 10-fold cross validation lda vs ctm

    ~1500 documents in corpus.
    ~5600 unique words

    after pruning
    methodology:

    partition data into 10 sets
    10 fold cross validation
    calculate the log likelihood of a 

set, given you trained on the 
previous 9 sets, for both lda 
and ctm.

    right(l(ctm) - l(lda))
    left(l(ctm)     l(lda))

ctm shows a much higher log likelihood as the number of 
topics increases.

evaluation ii: predictive perplexity

    perplexity measure     expected 
number of equally likely words
    lower perplexity means higher 

word resolution.

    suppose you see a percentage of 
words in a document, how likely 
is the rest of the words in the 
document according to your 
model?

    ctm does better with lower #   s of 

observed words.

    able to infer certain words given 

topic probabilities.

conclusions

    ctm changes the distribution from which hyper parameters are 

drawn, from a dirichlet to a logistic normal function.

    very similar to lda

    able to model correlations between topics.
    for larger topic sizes, ctm performs better than lda.
    with known topics, ctm is able to infer words associations better 

than lda.

