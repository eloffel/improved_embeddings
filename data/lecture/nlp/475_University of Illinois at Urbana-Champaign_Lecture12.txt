modeling 
annotated data 

reviewer: saurabh singh (ss1@uiuc.edu) 

problem 
       modeling of associated document items 

       images & annotations 
       papers & bibliographies 
       genes & functions 

       documents are considered as pairs of data streams. 
       one type provides annotation for the other type. 

uses 
       retrieval, id91, classification  
       automatic annotation 
       retrieval of un-annotated data. 

this paper 
models images (r) and annotations (w) 
 
three primary tasks 
       joint distribution of an image and its caption (id91, 
organization) 
       conditional distribution of words given an image. 
(automatic annotation, text based retrieval) 
       conditional distribution of words given a region of an 
image. (automatic labeling of regions) 

modeling 
k factors or topics 
       each a distribution over words 
       each a distribution over image regions 

latent variables 
       topic assignments 
       distribution parameters (for components) 
 
features 
document: (r, w), n regions, m words 
 
distributions 
p(r, w), p(w | r), p(w | r, rn) 

text annotations 
vocabulary: 168 terms (v) 
captions: 2-4 words per image 
 
multinomials on v conditioned on topics 
 
 
 

images 
composed of 6-10 regions via n-cuts 
each region summarized as a feature 
vector ~40 
       size: percentage of image 
       position: center of mass [0, 1] 
       color:   ,    of r,g,b, l, a, b etc. 
       texture:   ,    of filter responses 
       shape: area/perimeter2, moment of 
inertia etc. 

 
multivariate gaussian over 
features:   ,    
 
 

models 
three hierarchical probabilistic models 
1.    gaussian multinomial mixture 
2.    gaussian multinomial lda 
3.    correspondence lda 

gaussian multinomial mixture 

  

z

r

w

n

m

4

   

  

  

  

d. m. blei and j. d. lafferty

d

figure 1: the gm-mixture model of images and
k

d

   d

zd,n wd,n n

 k

  

sampled once per image/caption, and is held    xed during
the process of generating its components. the joint distri-
bution of the hidden factor z and the image/caption (r, w)

variable is a    plate,    a notational device to denote
replication. the box around r denotes n replicates
of r (this gives the    rst product in eq. 1).

distributions 

assumed to have been generated conditional on the same fac-
tor, the resulting multinomial and gaussian parameters will
correspond. an image with high id203 under a certain
factor will likely contain a caption with high id203 in
the same factor.

p(z, r, w) = p(z |   ) n
n=1 p(rn | z,   ,   )
   m
m=1 p(wm | z,   ).

let us consider the three tasks under this model. first,
the joint id203 of an image/caption can be computed
by simply marginalizing out the hidden factor z from eq. (1).
second, we can obtain the conditional distribution of words
given an image by invoking bayes    rule to    nd p(z | r) and
marginalizing out the hidden factor:

       p(r, w) 
       p(w | r) = 

but no 
       p(w | r, rn) 

p(w | r) = z p(z | r)p(w | z).

given a    xed number of factors k and a corpus of im-
ages/captions, the parameters of a gm-mixture model can
be estimated by the em algorithm1. this yields k gaus-
sian distributions over features and k multinomial distri-
butions over words which together describe a id91 of
the images/captions. since each image and its caption are

finally, we would like to compute a region-speci   c distribu-
tion over words. this task, however, is beyond the scope of
the gm-mixture model. conditional on the latent factor
variable z, regions and words are generated independently,
and the correspondence between speci   c regions and speci   c
words is necessarily ignored.

3.2 gaussian-multinomial lda

the id44 (lda) model is a latent
variable model that allows factors to be allocated repeatedly
within a given document or image [3]. thus, di   erent words

gaussian multinomial lda 

  

  

z

v

n

m

4

r

w

d. m. blei and j. d. lafferty

d

  

  

  

figure 2: the gm-lda model of images and cap-
tions. unlike gm-mixture (figure 1), each word

k

d

   d

zd,n wd,n n

 k

  

   

level representation of the ensemble of image/caption pairs
in terms of a id203 distribution over factors that each
image/caption can be assembled from.
distributions 
the resulting joint distribution on image regions, caption
words, and latent variables is given as follows:

n

  

m

p(r, w,   , z, v) = p(   |   )

n=1 p(zn |   )p(rn | zn,   ,   )
m=1 p(vm |   )p(wm | vm,   ) .
as in the simpler lda model, it is intractable to compute
the conditional distributions of latent variables given ob-
served data under this joint distribution, but e   cient vari-
ational id136 methods are available to compute approx-
imations to these conditionals (see [2] for details). fur-
thermore, we can use variational id136 methods to    nd
the id155 p(w | r) needed for image anno-
tation/retrieval and the id155 p(w | r, rn)
needed for region labeling.
lda provides signi   cant improvements in predictive per-

all 
       p(r, w) 
       p(w | r) 
       p(w | r, rn) 
 

correspondence lda 

  

  

z

y

n

m

4

r

w

d. m. blei and j. d. lafferty

d

  

  

  

figure 3: the graphical model representation of the
corr-lda model. note that the variables ym are

k

d

   d

zd,n wd,n n

 k

  

   

(a) sample ym     unif(1, . . . , n)
(b) sample wm     p(w | ym, z,   ) from a multinomial

distribution conditioned on the zym factor.

distributions 

corr-lda thus speci   es the following joint distribution

on image regions, caption words, and latent variables:

n

p(r, w,   , z, y) = p(   |   )

n=1 p(zn |   )p(rn | zn,   ,   )
m=1 p(ym | n)p(wm | ym, z,   ) .
the independence assumptions of the corr-lda model
are a compromise between the extreme correspondence en-
forced by the gm-mixture model, where the entire image

m

  

all 
       p(r, w) 
       p(w | r) 
       p(w | r, rn) 
 

id136 & estimation 
       variational id136 

       exact intractable 
       approximate assuming factorizable distribution 
       minimize kl-divergence via iterative updates to parameters 

       parameter estimation 

       em algorithm 
       e: compute variational posterior. 
       m: id113 estimate of the model parameters. 

evaluation 
       7000 images and their captions 
       75% training & 25% testing 

       test set likelihood 
       automatic annotation 
       text based retrieval 

eval: test set likelihood 

0
5
6

0
0
6

0
5
5

0
0
5

0
5
4

0
0
4

0
5
3

y
t
i
l
i

b
a
b
o
r
p

 

g
o

l
 

e
v
i
t

a
g
e
n

 

e
g
a
r
e
v
a

corr   lda
gm   mixture
gm   lda
ml

0

50

100

150

200

number of factors

md

d=1

eval: automatic annotation 

maximum likelihood

empirical bayes smoothed

m=1 log p(wm | rd)/ d

puted the perplexity of the given captions under p(w | r) for
each image in the test set. perplexity, which is used in the
id38 community, is equivalent algebraically
to the inverse of the geometric mean per-word likelihood
(again, lower numbers are better):
perplexity = exp{    d
d=1 md}.
figure 5 (left) shows the perplexity of the held-out cap-
empirical bayes smoothed
maximum likelihood
tions under the maximum likelihood estimates of each model
for di   erent values of k. we see that over   tting is a seri-
ous problem in the gm-mixture model, and its perplexity
immediately grows o    the graph (e.g., when k = 200, the
perplexity is 2922). note that in related work [2], many
of the models considered are variants of gm-mixture and
rely heavily on an ad-hoc smoothing procedure to correct
for over   tting.
corr   lda
corr   lda
figure 5 (right) illustrates the caption perplexity under
gm   mixture
gm   mixture
gm   lda
gm   lda
the smoothed estimates of each model using the empirical
ml
ml
bayes procedure from section 4.2.1. the over   tting of gm-
150
50
mixture has been corrected. once smoothed, it performs
number of factors
better than gm-lda despite the gm-lda model   s supe-
rior performance in joint likelihood.

150
number of factors

150
number of factors

150
number of factors

corr   lda
corr   lda
gm   mixture
gm   mixture
gm   lda
gm   lda
ml
ml

y
t
i
x
e
p
r
e
p
n
o

y
t
0
i
8
x
e
p
0
r
7
e
p

p
a
c

p
a
0
5
c

n
0
6
o

0
0
1

100

100

200

0
0
1

100

200

100

0
9

50

0
3

0
4

0
8

0
9

0
3

0
4

0
5

0
6

0
7

50

50

i
t

i
t

 

 

0

0

l

l

0
0
1

0
0
1

0
9

0
9

y
t
0
i
8
x
e
p
0
r
7
e
p

l

 

n
0
6
o

i
t

p
a
0
5
c

0
8

0
7

0
6

0
5

0
4

0
4

0
3

0
3

figure 4: the per-image average negative log prob-
ability of the held-out test set as a function of the
number of hidden factors (lower numbers are bet-

0

0

200

200

number of factors

number of factors
eval: automatic annotation (qual.) 

true caption
bridge sky water
corr   lda
sky water buildings people mountain
figure 5: (left) caption perplexity on the test set for the ml estimates of the models (lower numbers are
gm   lda
better). note the serious over   tting problem in gm-mixture (values for k greater than    ve are o    the graph)
sky water people tree buildings
and the slight over   tting problem in corr-lda. (right) caption perplexity for the empirical bayes smoothed
gm   mixture
estimates of the models. the over   tting problems in gm-mixture and corr-lda have been corrected.
sky plane jet water snow

true caption
true caption
market people
sky tree water
corr   lda
corr   lda
people market pattern textile display
tree water sky people buildings
gm   lda
gm   lda
sky tree fish water people
people tree light sky water
gm   mixture
gm   mixture
tree vegetables pumpkins water gardens
people market street costume temple

true caption
bridge sky water
corr   lda
sky water buildings people mountain
gm   lda
sky water people tree buildings
gm   mixture
sky plane jet water snow

true caption
scotland water
corr   lda
scotland water flowers hills tree
gm   lda
tree water people mountain sky
gm   mixture
water sky clouds sunset scotland

true caption
true caption
mountain sky tree water
scotland water
corr   lda
corr   lda
sky water tree mountain people
scotland water flowers hills tree
gm   lda
gm   lda
sky tree water people buildings
tree water people mountain sky
gm   mixture
gm   mixture
buildings sky water tree people
water sky clouds sunset scotland

true caption
true caption
true caption
birds tree
clouds jet plane
bridge sky water
corr   lda
corr   lda
corr   lda
birds nest leaves branch tree
sky plane jet mountain clouds
sky water buildings people mountain
gm   lda
gm   lda
gm   lda
water birds nest tree sky
sky water people tree clouds
sky water people tree buildings
gm   mixture
gm   mixture
gm   mixture
tree ocean fungus mushrooms coral
sky plane jet clouds pattern
sky plane jet water snow

true caption
true caption
fish reefs water
sky tree water
corr   lda
corr   lda
fish water ocean tree coral
tree water sky people buildings
gm   lda
gm   lda
water sky vegetables tree people
sky tree fish water people
gm   mixture
gm   mixture
fungus mushrooms tree flowers leaves
tree vegetables pumpkins water gardens

true caption
mountain sky tree water
corr   lda
sky water tree mountain people
gm   lda
sky tree water people buildings
gm   mixture
buildings sky water tree people

figure 6: example images from the test set and their automatic annotations under di   erent models.

figure 6: example images from the test set and their automatic annotations under di   erent models.

eval: automatic annotation (qual.) 

3

4

2

1

6

5

corr   lda:
1. people, tree
2. sky, jet
3. sky, clouds
4. sky, mountain
5. plane, jet
6. plane, jet

gm   lda:
1. hotel, water
2. plane, jet
3. tundra, penguin
4. plane, jet
5. water, sky
6. boats, water

figure 7: an example of automatic region labeling.

the gm-lda model, as shown quantitatively in the pre-

text based retrieval 

candy

candy

candy

sunset

sunset

sunset

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

i

i

n
o
s
c
e
r
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0.2
0.0

corr   lda
gm   mixture
gm   lda

corr   lda
gm   mixture
gm   lda

corr   lda
gm   mixture
gm   lda

0

.

1

8

.

0

i

i

n
o
s
c
e
r
p

6

.

0

4

.

0

i

i

n
o
s
c
e
r
p

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

i

i

n
o
s
c
e
r
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0.0

0.2
0.0

2

.

0

0

.

0

1.0

1.0
0.8
0.6

1.0
0.8

0.4
0.2
0.0
recall

0.6
0.4
0.2
recall

0.8
0.6
0.4

recall

2

.

0

0

.

0

1.0

1.0
0.8
0.6

1.0
0.8

0.4
0.2
0.0
recall

0.6
0.4
0.2
recall

0.8
0.6
0.4

recall

corr   lda
gm   mixture
gm   lda

corr   lda
gm   mixture
gm   lda

corr   lda
gm   mixture
gm   lda

0

.

1

8

.

0

i

i

n
o
s
c
e
r
p

6

.

0

4

.

0

i

i

n
o
s
c
e
r
p

people & fish

people & fish

people & fish

corr   lda
gm   mixture
gm   lda

corr   lda
gm   mixture
gm   lda

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

i

i

n
o
s
c
e
r
p

0.0

0.2
0.0

0.4
0.2
0.0
recall

0.6
0.4
0.2
recall

0.8
0.6
0.4

recall

1.0
0.8
0.6

i

n
c
e
o
r
i
s
p
c
e
r
p

6
.
0
4
0
2
0

.

.

4

.

.
0
2
0
0
0

.

2
.
0
0
0

.

i

n
c
e
o
r
i
s
p
c
e
r
p

6
.
0
4
0
2
0

.

.

4

.

.
0
2
0
0
0

.

2
.
0
0
0

.

0.0
0.0
0.0

0
.
0

text based retrieval (qual.) 

0
.
0

0.8
0.8
0.8

1.0
1.0
1.0

0.6
0.6
0.6

0.4
0.4
recall
0.4
recall
recall

0.6
0.6
0.6

0.4
0.4
recall
0.4
recall
recall

0.0
0.0
0.0

0.2
0.2
0.2

0.2
0.2
0.2

0.8
0.8
0.8

1.0
1.0
1.0

0.0
0.0
0.0

0.2
0.2
0.2

0.6
0.6
0.6

0.4
0.4
recall
0.4
recall
recall

0.8
0.8
0.8

1.0
1.0
1.0

i

n
c
e
o
r
i
s
p
c
e
r
p

6
.
0
4
0
2
0

.

.

4

.

.
0
2
0
0
0

.

2
.
0
0
0

.

0
.
0

candy
candy
candy

sunset
sunset
sunset

people
& fish
people
& fish
people
& fish

figure 8: three examples of text-based id162. (top) precision/recall curves for three queries on a
200-factor corr-lda model. the horizontal lines are the mean precision for each model. (bottom) the top
figure 8: three examples of text-based id162. (top) precision/recall curves for three queries on a
figure 8: three examples of text-based id162. (top) precision/recall curves for three queries on a
   ve returned images for the same three queries.
200-factor corr-lda model. the horizontal lines are the mean precision for each model. (bottom) the top
200-factor corr-lda model. the horizontal lines are the mean precision for each model. (bottom) the top
   ve returned images for the same three queries.
   ve returned images for the same three queries.

processing systems 13, 2001.
processing systems 13, 2001.

[5] a. goodrum. image information retrieval: an overview of

acknowledgments
acknowledgments
acknowledgments

conclusion 

 
 

if conditionals are needed, then model them explicitly 

