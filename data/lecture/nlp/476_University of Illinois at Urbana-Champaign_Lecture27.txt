multilingual part-of-speech tagging: 
two unsupervised approaches 

naseem, t.; snyder, b.; eisenstein, j. & barzilay, r. (2009) 

presenter: chris cervantes 

outline 
    conceptual background 
    formal descriptions 
    experiments 

conceptual background | formal descriptions | experiments 

outline 
    conceptual background 

    monolingual id52 
    the role of additional languages 
    overview of approaches 
    merged node model 
    latent variable model 

    formal descriptions 
    experiments 

conceptual background | formal descriptions | experiments 

monolingual id52 
    unsupervised monolingual part-of-speech (pos) tagging assigns tags 

to words, where tags are learned from unlabeled text 

    tags are treated as a linear sequence of hidden variables and 

words as emitted observations 

    often represented as a hidden markov model (id48) 

    necessary components for id48 pos tagger 

initial and final states 

   
    transition probabilities 
    emission probabilities 
   

initial state distributions 

    these probabilities can also be expressed as transition 

probabilities from a start-of-sentence tag to all the other tags 

conceptual background     monolingual id52 | formal descriptions | experiments 

monolingual id52 
    unsupervised monolingual part-of-speech (pos) tagging assigns tags 

to words, where tags are learned from unlabeled text 

    tags are treated as a linear sequence of hidden variables and 

words as emitted observations 

    often represented as a hidden markov model (id48) 

    necessary components for id48 pos tagger 

initial and final states 

   
    transition probabilities 
    emission probabilities 
   

initial state distributions 

in the bayesian model, these 
distributions are drawn from 
priors 

    these probabilities can also be expressed as transition 

probabilities from a start-of-sentence tag to all the other tags 

conceptual background     monolingual id52 | formal descriptions | experiments 

monolingual id52 
    id48 pos tagger example 

i 

.8 

1 

q0 

they 

have 

love 

cats 

.2 

.5 

.5 

dogs 

.4 

fish 

.4 

.2 

q1 

1 

q2 

.67 

.33 

q3 

q4 

.1 

us 

.9 

it 

conceptual background     monolingual id52 | formal descriptions | experiments 

monolingual id52 
    id48 pos tagger example 

i 

.8 

1 

q0 

they 

have 

love 

cats 

.2 

.5 

.5 

dogs 

.4 

fish 

.4 

.2 

q1 

1 

q2 

.67 

.33 

q3 

q4 

 
      transition probabilities 
 
       emission probabilities 

.1 

us 

.9 

it 

conceptual background     monolingual id52 | formal descriptions | experiments 

role of additional languages 
    languages have different patterns of ambiguity 

    words with pos ambiguity 

   

   can    in english might be a standard verb, auxiliary verb, or 
noun 

    structural ambiguity 

    articles in english reduce next-pos possibilities 

    different ambiguity patterns are very likely to occur in different 

places / for different reasons across languages 

    unannotated multilingual data serves as a learning signal in an 

unsupervised system 

    key idea: combining information from multiple languages 

creates a clearer picture of each 

conceptual background     role of additional languages | formal descriptions | experiments 

overview of approaches 
    observed data 

    corpus of parallel sentences in multiple languages 
    word alignments between parallel sentence pairs are given via a 

black box mechanism and so are treated as observed 

    tags are drawn from tag dictionaries 

    not completely unsupervised 

    two approaches 

    merged node model 
    latent variable model 

 

conceptual background     overview of approaches | formal descriptions | experiments 

merged node model 
    model relies on language pairs 
    id48 nodes are created by merging tag nodes from different 

languages 

    nodes represent a pair of tags, one per language 

    each node emits two words, one per language 

 

conceptual background     overview of approaches | formal descriptions | experiments 

merged node model 
    model relies on language pairs 
    id48 nodes are created by merging tag nodes from different 

languages 

    nodes represent a pair of tags, one per language 

    each node emits two words, one per language 

 

i 

q0 

love 

q1 

j    

adore 

fish 

q3 

poisson 

q2 

les 

conceptual background     overview of approaches | formal descriptions | experiments 

latent variable model 
    operates over any number of languages with parallel text 
    like in the monolingual model, id48 nodes represent single tags and 

emit single words 

    assumes an additional layer of superlingual tags that inform which 

node to transition to 

conceptual background     overview of approaches | formal descriptions | experiments 

latent variable model 
    operates over any number of languages with parallel text 
    like in the monolingual model, id48 nodes represent single tags and 

emit single words 

    assumes an additional layer of superlingual tags that inform which 

node to transition to 

i 

ql
0 

love 

fish 

j    

adore 

les 

poisson 

ql
1 

ql
3 

ql
0 

ql
1 

ql
3 

ql
4 

conceptual background     overview of approaches | formal descriptions | experiments 

latent variable model 
    operates over any number of languages with parallel text 
    like in the monolingual model, id48 nodes represent single tags and 

emit single words 

    assumes an additional layer of superlingual tags that inform which 

node to transition to 

i 

ql
0 

love 

fish 

j    

adore 

les 

poisson 

ql
1 

ql
3 

ql
0 

ql
1 

ql
3 

ql
4 

ql
0 

ql
1 

ql
3 

ql
0 

ql
1 

ql
3 

ql
4 

ani 

ohev 

dagim 

mujhe  machchli 

pasand  hai 

conceptual background     overview of approaches | formal descriptions | experiments 

outline 
    conceptual background 
    formal descriptions 

    merged node model 
    latent variable model 

    experiments 

conceptual background | formal descriptions | experiments 

merged node model 
    terms 

    t / t           : tag set for respective languages 
   
    < t, t    >     : a tag pair (one tag from each language). tag pairs   

t  / t            : individual tag for respective languages 

 

                  are the nodes in this id48 

    <t, t   >    t x t    

                     : coupling distribution, which informs how the tags  

 

                  are merged into pairs 

    < yi, yj    >  : aligned tag pair. where <t,t   > is a tag pair from the 
 set of any two tags (one per language), <yi, yj   > is 
 aligned between the two languages 
    <yi, yj   > is conditioned on yi-1, y   j-1, and the coupling 

 
 

 
 

    w / w       : vocabulary for respective languages 

parameter   (yi, yj   ) 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 
    data 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 

    for each t    t 

    draw a transition distribution   t over tags t 
    draw an emission distribution   t over words w 

    for each t       t    

    draw a transition distribution   t    over tags t    
    draw an emission distribution   t    over words w    

    coupling parameter 
    data 

conceptual background | formal descriptions     merged node model  | experiments 

merged node approach 
    generative story 

    transition / emission parameters 

multinomials, 
each drawn 
from a 
symmetric 
dirichlet prior 

    for each t    t 

    draw a transition distribution   t over tags t 
    draw an emission distribution   t over words w 

    for each t       t    

    draw a transition distribution   t    over tags t    
    draw an emission distribution   t    over words w    

    coupling parameter 
    data 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 

    draw a bilingual coupling distribution,   , over tag pairs txt    

    data 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 

    draw a bilingual coupling distribution,   , over tag pairs txt    

    data 

also multinomial, drawn from 
symmetric dirichlet prior   0 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 
    data 

    for each parallel sentence 

    draw alignment a, a set of integer pairs (i,j) indicating 

aligned indices in parallel sentences. 
    draw a bilingual pos tag sequence,  

(y1,    , ym), (y1   ,    , yn   ) 

 
    for each pos tag yi, emit a word xi ~   yi
    for each pos tag yj   , emit a word xj    ~   y   j

  

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 
    data 

a ~ a0, a prior 
distribution 
over 
alignments 
provided by 
their black box 
mechanism  

    for each parallel sentence 

    draw alignment a, a set of integer pairs (i,j) indicating 

aligned indices in parallel sentences. 
    draw a bilingual pos tag sequence,  

(y1,    , ym), (y1   ,    , yn   ) 

 
    for each pos tag yi, emit a word xi ~   yi
    for each pos tag yj   , emit a word xj    ~   y   j

  

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 
    data 

a ~ a0, a prior 
distribution 
over 
alignments 
provided by 
their black box 
mechanism  

    for each parallel sentence 

    draw alignment a, a set of integer pairs (i,j) indicating 

aligned indices in parallel sentences. 
    draw a bilingual pos tag sequence,  

(y1,    , ym), (y1   ,    , yn   ) 

 
    for each pos tag yi, emit a word xi ~   yi
    for each pos tag yj   , emit a word xj    ~   y   j
                

(        )  

  

             1
unaligned i 
 

                   1

(y       ) 

unaligned j 

pos tag sequences are drawn according to: 
 
p((y1,    , ym), (y1   ,    , yn   ) | a,   t ,   t   ,   ) =  

    (                                                                ) 

(y       )   (    i,        j) 
(y   )   (    ,        ) 

(        )                    1

(    )                    1

             1
   (              1
y,y    

(i,j)    a 

 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
    generative story 

    transition / emission parameters 
    coupling parameter 
    data 

a ~ a0, a prior 
distribution 
over 
alignments 
provided by 
their black box 
mechanism  

    for each parallel sentence 

    draw alignment a, a set of integer pairs (i,j) indicating 

aligned indices in parallel sentences. 
    draw a bilingual pos tag sequence,  

(y1,    , ym), (y1   ,    , yn   ) 

 
    for each pos tag yi, emit a word xi ~   yi
    for each pos tag yj   , emit a word xj    ~   y   j
                

(        )  

  

xi and xj    are 
words from 
w and w   , 
respectively 

pos tag sequences are drawn according to: 
 
p((y1,    , ym), (y1   ,    , yn   ) | a,   t ,   t   ,   ) =  

             1
unaligned i 
 

                   1

(y       ) 

unaligned j 

    (                                                                ) 

(y       )   (    i,        j) 
(y   )   (    ,        ) 

(        )                    1

(    )                    1

             1
   (              1
y,y    

(i,j)    a 

 

conceptual background | formal descriptions     merged node model  | experiments 

merged node model 
   

id136 

    process occurs in a monolingual setting (and thus must be 

   

performed for each language in the pair) 
ideal transition and emission parameters 
 
 

  ,    = argmax     p(  ,   , y,    | x, a,   0,   0,   0) dy d   

^     ^ 

  ,   

    actual parameters are found with id150 

      ,   , and    are all marginalized out 
    only pos tags and priors are sampled 

    after sampling, parameters    and    are the maximum  

a posteriori estimates 

conceptual background | formal descriptions     merged node model  | experiments 

latent variable model 
    assumes an additional layer of superlingual tags 
    operates over any number of languages with parallel text 
    offers both a conceptual and a computational benefit over using the 

merged node model with more languages 

    multilingual information can reduce linguistic ambiguity during 
training; combining bilingually trained models (like the merged 
node model) doesn   t take advantage of this 

    state space in the merged node model grows exponentially with 

the number of languages, l 

    since nodes are tag pairs, the size of the state space is|t|l 
       has the same dimension. 

 

 

latent variable model 
    parameter generation 

    draw an infinite sequence of distribution sets 

      1,   2,     ~ g0 
      i    : a set of distributions over tags,  
one distribution per language l  
(  i

l    ,    )   

l ,   i

 
 

    draw an infinite sequence of mixture weights 

      1,   2,     ~ gem(  ) 
    these mixture weights weight the sets of distributions, 

above 

 
 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    parameter generation 

    draw an infinite sequence of distribution sets 

      1,   2,     ~ g0 
      i    : a set of distributions over tags,  
one distribution per language l  
(  i

l    ,    )   

l ,   i

 
 

g0 is some base 
distribution 

    draw an infinite sequence of mixture weights 

      1,   2,     ~ gem(  ) 
    these mixture weights weight the sets of distributions, 

above 

 
 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    parameter generation 

    draw an infinite sequence of distribution sets 

      1,   2,     ~ g0 
      i    : a set of distributions over tags,  
one distribution per language l  
(  i

l    ,    )   

l ,   i

 
 

    draw an infinite sequence of mixture weights 

      1,   2,     ~ gem(  ) 
    these mixture weights weight the sets of distributions, 

gem(  ) is a stick-
breaking process  

g0 is some base 
distribution 

above 

 
 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    given these parameters    

    superlingual tag z is drawn such that 

    z is drawn with id203   z 
    z is an index of the infinite sequence of sets of multinomials 

(  z

l,   z

l   ,    ) 

    pos tag yi is drawn according to 
(yi) 

(yi)             l

 

  yi-1

m  
m=1 

zm

 

z 

i               : tag position 
l               : language 

   

  
yi ~ 

   
   
      yi-1

 

      this tag 

(yi) : transition distribution from the previous tag to 

    zm           : value of the mth connected superlingual tag 
      l
(yi) : tag distribution for language l given by   zm 
    z             : sum of the product in the numerator over all 

zm

 

     values for yi 

    m          : all superlingual tag indices with which position l 

 

    is associated  

latent variable model 
    given these parameters    

    superlingual tag z is drawn such that 

    z is drawn with id203   z 
    z is an index of the infinite sequence of sets of multinomials 

(  z

l,   z

l   ,    ) 

    pos tag yi is drawn according to 
(yi) 

(yi)             l

 

  yi-1

m  
m=1 

zm

 

z 

i               : tag position 
l               : language 

   

  
yi ~ 

   
   
      yi-1

 

      this tag 

a beneficial consequence of 
drawing tags in this way is that a 
high id203 tag at a given 
position must be allowed for by 
each incoming distribution 

(yi) : transition distribution from the previous tag to 

    zm           : value of the mth connected superlingual tag 
      l
(yi) : tag distribution for language l given by   zm 
    z             : sum of the product in the numerator over all 

zm

 

     values for yi 

    m          : all superlingual tag indices with which position l 

 

    is associated  

latent variable model 
    generative story 

    transition / emission parameters 
    superlingual parameters 
    data 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    generative story 

    transition / emission parameters 

    for each language l = 1,    , n and for each tag t    tl 

    draw a transition distribution,   l

t , over tags tl 

 

    draw an emission distribution   l

t , over words wl 

 

    superlingual parameters 
    data 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    generative story 

    transition / emission parameters 

multinomials, 
each drawn 
from a 
symmetric 
dirichlet prior 

    for each language l = 1,    , n and for each tag t    tl 

    draw a transition distribution,   l

t , over tags tl 

 

    draw an emission distribution   l

t , over words wl 

 

    superlingual parameters 
    data 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    generative story 

    transition / emission parameters 
    superlingual parameters 

    draw an infinite sequence of distribution sets 

      1,   2,     ~ g0 
      i    : a set of distributions over tags,  

 

l) 
    draw an infinite sequence of mixture weights 

       one distribution per language l (  i

      1,   2,     ~ gem(  ) 
    these mixture weights weight the sets of distributions, 

above 

    data 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
    generative story 

    transition / emission parameters 
    superlingual parameters 
    data 

    for each multilingual parallel sentence 

    draw alignment a from am 

    a is a set of aligned indices across languages (i1, i2, 

   , in) 

    for each set of indices in a 

    draw superlingual tag z 

    for each language, l, and for each position i 

    draw yi such that 

 

 

   

  

yi ~ 

  yi-1

(yi)             l

m  
m=1 

zm

(yi) 

z 

    draw word wi    wl according to   yi 

conceptual background | formal descriptions     latent variable model  | experiments 

latent variable model 
   

id136 

    like in the merged node model, a sampling technique is used for 

id136 

      ,   ,   l
    only pos tags and superlingual tags need to be sampled 

i, and    are all marginalized out 

   

in order to integrate over    during superlingual tag sampling, the 
chinese restaurant process is used 
 

conceptual background | formal descriptions     latent variable model  | experiments 

outline 
    conceptual background 
    formal descriptions 
    experiments 

    full lexicon experiment 
    reduced lexicon experiment 
    analysis 

conceptual background | formal descriptions  | experiments 

experiments 
    george orwell   s 1984 is used as the experiment data 

   

 parallel text in english, bulgarian, czech, estonian, hungarian, 
slovene, serbian, and romanian 

    provided as part of the multext-east corpus, which is annoted 

with pos tags and provides a lexicon for each language 

    word alignments are provided with a black box mechanism (giza++) 
    for the sake of comparison, two other systems are implemented 

    a monolingual bayesian id48 
    a supervised id48 (trained with annotated data) 

    merged node model results (which are constrained by pairings) are 

combined in three ways 

    average across pairings 
    best-pair using an oracle 
    voting scheme 

conceptual background | formal descriptions  | experiments 

full lexicon experiment 
    assume the full tag lexicon     set of possible pos tags     is known in 

advance 

    possible tags per word is 1.39 
    tagging accuracy 

conceptual background | formal descriptions  | experiments     full lexicon experiment 

reduced lexicon experiment 
    three types of reduced lexicons are used.  

    all words with less than 5 instances are removed 
    all words with less than 10 instances are removed 
    only the top 100 words are retained in the lexicon 

    possible tags per word is 7.54 in the    top 100    model 
    tagging accuracy 

analysis 
    performance would be helped if the optimal language partners could 

be predicted 

    language relatedness isn   t necessarily helpful 

    slovene and serbian are related and optimal partners 
    bulgarian and english are optimal, but not closely related 

    tag / word ambiguity is correlated negatively with a language   s 

helpfulness as a partner 

conceptual background | formal descriptions  | experiments     analysis 

analysis 

average performance as the 
number of languages increases 

average performance of the latent 
variable model of languages as the 
number of language increases 

conceptual background | formal descriptions  | experiments     analysis 

analysis 
   

if the full lexicon is available, the two models proposed significantly 
improve on previous unsupervised methods 

    for most languages, performance is gained as more languages are 

added 

   

if only a reduced lexicon is available, the merged model is likely the 
better choice 

    performance varies greatly depending on which languages are chosen, 

but it   s difficult to determine what language is going to be helpful 

    this question is irrelevant in the latent variable model, since all 

languages are used 

conceptual background | formal descriptions  | experiments     analysis 

