cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 14: 
id136 in dirichlet 
processes

(blei & jordan, variational id136 for dirichlet 
process mixture models, bayesian analysis 2006)
julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

dirichlet process mixture models
a mixture model with a dp as nonparametric prior:

   mixing weights    (prior):  g |{  , g0} ~ dp(  , g0)
the base distribution g0 and g are distributions over the same 
id203 space.    
  
   cluster    parameters:   n | g ~ g
for each data point n = 1,..., n, draw a distribution   n 
with value   c* over observations from g
(we can interpret this as id91 because g is discrete with id203 1; 
hence different   n take on identical values   c* with nonzero id203.
data points are partitioned into |c| clusters: c = c1...cn) 

observed data:  xn|  n ~ p(xn |   n)
for each data point n = 1,...,n, draw observation xn  from   n                

bayesian methods in nlp

2

stick-breaking representation of dpms

  2

0

  1
v1

1     v1

1

(1   v1)v2

the component parameters   *:     *i ~ g0
the mixing proportions   i (v) are de   ned by 
a stick-breaking process: 
          vi ~ beta(1,   )           i (v) = vi     j =1...i    1 (1   vj )
  also written as   (v) ~ gem(  ) (grif   ths/engen/mccloskey)
hence, if g ~ dp(   , g0): 
   g =    i =1...      i (v)     i*  with   *i ~ g0
bayesian methods in nlp

3

dp mixture models with dp(  , g0)
1. de   ne stick-breaking weights by 
    drawing  vi |     ~ beta(1,   )

2. draw cluster   i* | g0 ~ g0  i = {1, 2, ...}

3. for the nth data point:
draw cluster id zn | {v1,v2...} ~ mult(  (v))
draw observation xn | zn  ~ p(x |   zn*) 

p(x |   *) is from an exponential family of distributions
g0 is from the corresponding conjugate prior
e.g. p(x |   *) multinomial, g0 dirichlet
bayesian methods in nlp

4

stick-breaking construction of dpms

stick lengths  vi ~ beta(1,   ), 
yielding mixing weights    i(v) = vi    j<i ( 1     vj )
component parameters:   i* ~ g0  
(assume g0 is conjugate prior with hyperparameter   )
assignment of data to components: zn |{v1, .... } ~ mult(  (v))
generating the observations: xn | zn ~ p( xn |   zn*)
bayesian methods in nlp

5

n   vk    k*  xnznid136 for dp mixture models
given observed data x1, ...., xn , compute the predictive density:

p(x | x1, ...., xn,   , g0)  
 =      p(x | w) p(w| x1, ...., xn,   , g0) dw

problem: the posterior of the latent variables p(w | x1, ....,xn,  , g0) 
can   t be computed in closed form
approximate id136:

- id150: 
sample from a markov chain with equilibrium distribution 
p(w | x1, ...., xn,   , g0) 
- variational id136:
construct a tractable variational approximation q of p 
with free variational parameters   

bayesian methods in nlp

6

id150

bayesian methods in nlp

7

id150 for dpms
two variants that differ in their de   nition 
of the markov chain
collapsed gibbs sampler: 
integrates out g and the distinct parameter values 
{  1*....   |c|*} associated with the clusters 
blocked gibbs sampler:
based on the stick-breaking construction.
this requires a truncated variant of the dp.

bayesian methods in nlp

8

collapsed gibbs sampler for dpms
integrate out the random measure g and 
the distinct parameter values {  1*....   |c|*} 
associated with each cluster 
given data x = x1...xn, each state of the markov chain 
is a cluster assignment c = c1...cn  to each data point
each sample is also a cluster assignment c = c1...cn

given a cluster assignment cb = c1...cn with c distinct 
clusters, the predictive density is 
p(xn+1 | cb, x,   ,   ) 
=    k     c+1  p(cn+1 = k | cb,   ) p(xn+1 | cb, cn+1 = k,   )

bayesian methods in nlp

9

collapsed gibbs sampler for dpms
   macro-sample step   :
assign a new cluster to all data points. 
   micro-sample step   :
sample assignment variables cn for each data point conditioned 
on the assignment of the remaining points,  c-n
cn is either one of the values in c-n or a new value:

p(cn = k | x, c-n)      p( xn | x-n, c-n, cn=k,   ) p( cn = k | c-n,    )
with p( xn | x-n, c-n, cn=k,   ) = p( xn, c-n, cn=k,   ) / p( x-n, c-n, cn=k,   )
and p( cn = k | c-n,    ) given by the polya (blackwell/mcqueen) urn

id136: 
after burn-in, collect b sample assignments cb 
and average across their predictive densities.
bayesian methods in nlp

10

blocked id150
based on the stick-breaking construction.
states of the markov chain consist of (v,   *, z)

problem: in the actual dpm model v,   * are in   nite.
instead, the blocked gibbs sampler uses a truncated 
dp (tdp), which samples only a    nite collection of t 
stick lengths (and hence clusters)
 
by setting vt-1= 1,   i = 0 for i     t: 
             i(v) = vi    j<i ( 1     vj )

bayesian methods in nlp

11

blocked id150
the states of the markov chain consist of 

-the beta variables v = {v1...vt-1}, 
-the mixture component parameters   * =  {  1*...  t*} 
-the indicator variables z = {z1...zn}
sampling:
-for n=1...n, sample zn from p(zn = k | v,   *,x) =   k(v)p(xn |   k*)
-for k=1...k, sample vk from  beta(  k2,   k1    + nk+1...k) 
  k1  = 1+ nk  with nk : number of data points in cluster k
  k2  =    + nk+1...k:  with nk+1...k  the data points in clusters k+1...k
-for k=1...k, sample   k* from its posterior p(  k* |   k)
  k = (  1 + n-ik(xi)  ,   2 + n-ik)
predictive density for each sample:
p(xn+1  | x, z,   ,   ) =    k e[  k(v) |   1....  k] p(xn+1 |   k)

bayesian methods in nlp

12

variational id136 
(recap)

bayesian methods in nlp

13

standard em

l(q,   ) = ln p(x |   )     kl(q || p) 
is a lower bound on the 
incomplete log-likelihood ln p(x |    )

e-step: 
with   old     xed, return qnew  
that maximizes l(q,   old) wrt. q(z), 
now kl(qnew || pold ) = 0.

m-step: 
with qnew    xed, return   new 
that maximizes l(qnew,   ) wrt.   .
if l( qnew,   new ) > l( qnew,   old ):
ln p( x |   new) > ln p( x |   old ), 
and hence kl( qnew || pnew ) > 0 
bayesian methods in nlp

kl(q || p)

l(q,  old)

ln p(x |   old)

14

variational id136
variational id136 is applicable when you have to compute 
an intractable posterior over latent variables p( w |x) 

basic idea:
replace the exact, but intractable posterior p( w |x)
with a tractable approximate posterior q( w |x, v)

q( w |x, v) is from a family of simpler distributions over the latent 
variables w  that is de   ned by a set of free variational 
parameters v

unlike in em, kl(q || p) > 0  for any q, since q only approximates p

bayesian methods in nlp

15

variational em 
initialization: 
de   ne initial model   old  and variational distribution q( w | x ,v )

e-step: 
find v that maximize the variational distribution q( w | x ,v )
compute the expectation of true posterior p(w | x,   old) 
under the new variational distribution q( w | x ,v )

m-step: 
find model parameters   new that maximize the expectation of 
the p(w, x|    ) under the variational posterior q( w | x ,v )

set   old :=   new

bayesian methods in nlp

16

blei and jordan   s 
mean-   eld variational 
id136 for dp

bayesian methods in nlp

17

variational id136
de   ne a family of variational distributions q  (w) with variational 
parameters    =  1....  m  that are speci   c to each observation xi
set    to minimze the kl-divergence between q  (w) and p( w | x,   ):

d( q  (w) || p(w |x,  ) ) 
          = eq [log q  (w)]     eq [log p(w, x |  )]  + log p(x|   )
(here, log p(x|   ) can be ignored when    nding q)

this is equivalent to maximizing a lower bound on log p(x |   ):

log p(x |   )  =  eq [log p(w, x |  )]     eq [log q  (w)] + d(q  (w)||p(w |x,  ))
log p(x |   )       eq [log p(w, x |  )]     eq [log q  (w)]  

bayesian methods in nlp

18

q  (w) for dpms
blei and jordan use again the stick-breaking construction.
hence, the latent variables are w = (v,   *, z)

v: t    1  truncated stick lengths
  *: t component parameters
z: cluster assignments of the n data points

bayesian methods in nlp

19

variational id136 for dpms
in general: 

log p(x |   )       eq [log p(w, x |  )]     eq [log q  (w)]  

for dpms:     = (  ,   );  w = (v,   *, z)

log p(x |   ,   )       eq [log p(v |   )] +  eq [log p(  * |   )] 
                            +    n[ eq[log p(zn | v)]  +  eq[log p(xn | zn)] ]
                                 eq [log q  (v,   *, z)]

problem: v = {v1, v2,...},   * = {  1*,   2*, ...} are in   nite.
solution: use a truncated representation 

bayesian methods in nlp

20

variational approximations q  (v,  *, z)

the variational parameters    = (  1..t-1,   1..t,    1...n)

q  (v,   *, z) =    t<t  q  t (vt)     t<t  q  t(  t*)    n   n  q  n(zn)

q  t(vt): beta distributions with variational parameter   t
q  t(  t*): conjugate priors for   , with parameter   t
q  n(zn): multinomials with variational parameters   n

bayesian methods in nlp

21

ntvt  t  k*  t  nzn