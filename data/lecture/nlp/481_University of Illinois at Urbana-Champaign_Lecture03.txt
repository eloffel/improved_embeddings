cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 3:
comparing frequentist 
and bayesian 
estimation techniques

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

text classi   cation
the task: binary classi   cation (e.g. id31)
assign (sentiment) label li     { +,   } to a document wi=(wi1...win).
w1=    this is an amazing product: great battery life, amazing features and it   s cheap.   
w2=    how awful. it   s buggy, saps power and is way too expensive.   
the data: a set d of n documents with (or without) labels
the model:  naive bayes
we will use a frequentist model and a bayesian model 
and compare supervised and unsupervised estimation 
techniques for them.

bayesian methods in nlp

2

a naive bayes model
the task: 
assign (sentiment) label li     {+,   } to document wi.
w1=    this is an amazing product: great battery life, amazing features and it   s cheap.   
w2=    how awful. it   s buggy, saps power and is way too expensive.   
the model:
li  = argmax l p( l  | wi ) = argmax l p( wi  | l )p( l) 

assume wi is a    bag of words   :
w1 = {an:1, and: 1, amazing: 2, battery: 1, cheap: 1, features: 1, great: 1,   }
w2 = {awful: 1, and: 1, buggy: 1, expensive: 1,   }
p( wi  | l ) is a multinomial distribution:  wi      multinomial(  l)
with a vocabulary of v words,   l = (  1,   .,   v)
p( l ) is a bernoulli distribution: l      bernoulli(  )

bayesian methods in nlp

3

the frequentist 
(maximum-likelihood) 
model

bayesian methods in nlp

4

the frequentist model
the frequentist model has speci   c parameters   l and   

li   = argmax l p( wi  |   l )p( l |   ) 

p( wi  |   l ) is a multinomial over v words 
with parameter   l = (  1,   .,   v):

wi      multinomial(  l)

p( l |   ) is a bernoulli distribution with parameter   : 

l      bernoulli(  )

bayesian methods in nlp

5

the frequentist model

bayesian methods in nlp

6

nniwij2  lli  supervised id113
the data is labeled:
we have a set d of d documents w1...wd with n words
each document wi  has ni words
d+ documents (subset d+) have a positive label and n+ words
d    documents (subset d-) have a negative label and n- words 
each word wi appears n+(wi) times in d+,  n   (wi) times in d- 
each word wi appears nj(wi) times in d j
id113: relative frequency estimation
-  labels: l      bernoulli(  )  with    = d+ /d
-  words: wi |+      multinomial(  +) with   i+ = n+(wi)/n+
- words: wi |         multinomial(     ) with   i    = n-(wi)/n-

bayesian methods in nlp

7

id136 with id113
the id136 task:
given a new document wi+1, what is its label li+1?

recall: the word wj occurs ni+1(wj) times in  wi+1.                      

p (l = +|wi+1) / p (+)p (wi+1|+)

=    

   ni+1(wj )
+j

vyj=1

bayesian methods in nlp

8

unsupervised id113
the data is unlabeled:
we have a set d of d documents w1...wd with n words
each document wi  has ni words 
each word w1...wi...wv appears nj(wi) times in wj
em algorithm:     expected relative frequency estimation   
initialization: pick initial   (0),   +(0),      (0)
iterate:
-labels: l      bernoulli(  )  with    (t) =    n+   (t-1)/    n    (t-1)
-words: wi |+      multinomial(  +) with   i+ (t) =    n+(wi)   (t-1)  /    w+   (t-1)
-words: wi |         multinomial(     ) with   i    (t) =    n   (wi)    (i-1) /    w      (i-1)

bayesian methods in nlp

9

id113
with complete (= labeled) data d = {     xi , zi     },
maximize the complete likelihood p(x, z |   ):

    * = argmax       i p(xi , zi  |   )

or   * = argmax       i ln(p(xi , zi  |   ))

bayesian methods in nlp

10

id113
with incomplete (= unlabeled) data, d  = {     xi , ?     }
maximize the incomplete (marginal) likelihood p(x |   ):
                 * = argmax       i ln(p(xi  |   ))
                      = argmax       i ln(    z p(xi , z  |   ) p( z  | xi,     ) )
                      = argmax       i ln( e z|x   ,     [ p(xi , z  |   )] )

p(z | x,   ): the posterior id203 of z  (x = our data)
e z|x   ,  [ p(xi, z  |   )]:  the expectation of p(x, z |   ) wrt. p(z | x,   )
find parameters    new  that maximize the expected log-
likelihood of the joint p(z,x |   new) under p(z | x,    old)
this requires an iterative approach
bayesian methods in nlp

11

the em algorithm
1. initialization: choose initial parameters   old
2. expectation step: compute p(z | x,   old) 
(= posterior of the latent variables z )
3. maximization step: compute   new
  new maximizes the expected log-likelihood of the 
joint p(z,x |    new) under p(z | x,    old):

 new = arg max

p(z|x,  old) ln p(x, z| )

4. check for convergence. 
  stop, or set   old :=   new and go to 2.

    xz

bayesian methods in nlp

12

the em algorithm
the classes we    nd may not correspond to the 
classes we would be interested in.
seed knowledge (e.g. a few positive and negative words) 
may help
we are not guaranteed to    nd a global optimum, 
and may get stuck in a local optimum.
initialization matters

bayesian methods in nlp

13

in our example...
initialization: pick (random)   a,    b = (1-  a),   a ,   b 
e-step: 
set na,nb, na(w1),...,na(wv), nb(w1), ... nb(wv) := 0
for each document wi, 
set li = a with p(li = a | wi,   a,    b,   a ,   b)       a     j p(wij |   a)
set li = b with p(li = b | wi,   a,    b,   a ,   b)       b     j p(wij |   b)
update na += p(li = a | wi,   a,    b,   a ,   b)
              nb += p(li =b | wi,   a,    b,   a ,   b)
 for all words wij in wi :
              na(wij)  += p(li = a | wi,   a,    b,   a ,   b)
              nb (wij) += p(li = b | wi,   a,    b,   a ,   b)
m-step: 
  a := na/(na + nb)                         b := nb/(na + nb)
  a(wi ) := na(wi) /    j (na (wj))        b(wi ) := nb(wi) /    j (nb (wj)) 

bayesian methods in nlp

14

the bayesian model

bayesian methods in nlp

15

the bayesian model
the bayesian model has priors dir(  ) and beta(  ,  )
with hyperparameters   =(  1, ...,   v) and   ,   

it does not have speci   c   l and   , but integrates them out:
li  = argmax l        p(wi  |   l )p(  l ;   l, d) p( l |   )p(  ;   ,  ,d)d  ld  
    = argmax l    p(wi  |   l )p(  l ;   l, d)d  l    p( l |   )p(  ;   ,  ,d)d  
    = argmax l p(wi  |   l, d) p( l |   ,  ,d)

p( wi  |   l ) is a multinomial with parameter   l = (  1,   .,   v),
p(   l ;   l) is a dirichlet with hyperparameter   l = (  1,   .,   v)
  l    dirichlet(  l)                       wi      multinomial(  l)

p( l |   ) is a bernoulli with parameter   , drawn from a beta prior

        beta(  ,   )        l      bernoulli(  )

bayesian methods in nlp

16

the bayesian model

bayesian methods in nlp

17

nniwij2  lli  ,      bayesian: supervised
the data is labeled:
we have a set d of d documents w1...wd with n words
each document wi  has ni words
d+ documents (subset d+) have a positive label and n+ words
d    documents (subset d-) have a negative label and n- words 
each word wi appears n+(wi) times in d+,  n   (wi) times in d- 
each word wj  appears ni(wj) times in wi
bayesian estimation
 p(l = + | d) =  (d+ +   )/(d +    +   )
 p(wi  |+, d)  = (n+(wi)  +   i)/(n+(wi)  +   0)
 p(wi | +, d) =    j p(wj | +)ni(wj)
  p(li =  + | wi, d) = [(d+ +   )/(d +    +   )]   j p(wj | +)ni(wj)

bayesian methods in nlp

18

bayesian: unsupervised
we need to approximate an integral/expectation:

p(li =+ | wi  ) 
            p(wi |+,   +) p(  +;   , d) p( l=+|   ) p(  ;   ,  , d)d  + d  
        p(wi | +,   +) p(  +;   , d) d  +   p( l=+|   ) p(  ;   ,  , d)d  
      p(wi |   , +,  d)  p(li =+ |    ,  , d) 

bayesian methods in nlp

19

approximating expectations

e[f(x)] = z 1

0

f(x)p(x)dx

= lim
n!1

1
n

nxi=1

f(x(i))

for x(1)...x(i)...x(n ) drawn from p(x)

1
t

   

txi=1

f(x(i))

for x(1)...x(i)...x(t ) drawn from p(x)

we can approximate the expectation of f(x),    f(x)    =    f(x)p(x)dx,  
by sampling a    nite number of points x(1), ..., x(t) 
according to p(x), evaluating f(x(i)) for each of them, 
and computing the average.

bayesian methods in nlp

20

id115
a multivariate distribution p(x)= p(x1,   ,xk) with discrete 
xi has only a    nite number of possible outcomes.

id115 methods construct a 
markov chain whose states are the outcomes of p(x).

the id203 of visiting state xj is  p(xj)

we sample from p(x) by visiting a sequence of states
from this markov chain.

bayesian methods in nlp

21

id150
our states: 
one label assignment l1,   ,ln  to each of our n documents
x = (l1,   ,ln)
our transitions:
we go from one label assignment x = (+,+,-,+,-...+) 
to another y = (-,+,+,+,   ,+)

our intermediate steps:
we generate label yi conditioned on y1...yi-1 and xi+1...xn
call label assignment y1...yi-1, xi+1...xn  l(-i)
we need to compute p(yi | d, l(-i),    ,   ,   )

bayesian methods in nlp

22

id150
we visit states according to transition probabilities p(y| x)

we go from state x = (x1,   ,xk) to state y = (y1,   ,yk)

we get from x = (x1,   ,xk) to y = (y1,   ,yk) in k steps:

(x1, x2,   , xi,    ., xk-1 , xk) = x = x(t)
(y1, x2,   , xi,    ., xk-1 , xk)
(y1, y2,   , xi,    ., xk-1 , xk)
(y1, y2,   , xi,    ., xk-1 , xk)
(y1, y2,   , yi,    ., xk-1 , xk)
(y1, y2,   , yi,    ., xk-1 , xk)
(y1, y2,   , yi,    ., yk-1 , xk)
(y1, y2,   , yi,    ., yk-1 , yk) = y = x(t+1)

bayesian methods in nlp

23

id150
we will visit a sequence of states according to the 
transition probabilities p(y | x)

that is, we will go from state x = (x1,    , xk)
to state y = (y1,   ,yk) with id203 p(y | x)

for i = 1...k: 
pick a value for yi  by sampling 
from p(yi | y1,   , yi-1, xi+1,   , xk)

p(yi = yi | y1,   ,yi-1, xi+1,   ,xk) = 
     p(y1,   ,yi-1, yi, xi+1,   ,xk)/(y1,   ,yi-1, xi+1,   , xk)
bayesian methods in nlp

24

id150
for us p(x) = p(d, l,   ,   +,   -;   ,  ,  )

  ,   +,   - are real-valued, but they disappear
because we integrate them out:

p (lj = + | l( j);  ,    ) =

  + n ( j)
  +     + n   1

+

p (wk = y|d( j)

+ ;  ) =

(y) +  y
nd( j)
 0 + nd( j)

x

x

bayesian methods in nlp

25

id150

{z

prob. that dj is pos. review

+ ;  )
pos. review generates dj

p (lj = +|d, l( j);    ,  ,  )
}
|
/ p (wj|+, d( j)
p (lj = +|l( j);    ,  )
{z
}
|
|
  + n ( j)
p (lj = + | l( j);  ,    ) =
  +     + n   1

prob. of pos. review

{z

}

+

p (wk = y|d( j)

+ ;  ) =

(y) +  y
nd( j)
 0 + nd( j)

x

x

bayesian methods in nlp

26

the gibbs sampler
initialize: 

de   ne priors   ,  ,   . 
assign initial labels l(0) to documents

iterate:

for each iteration t  = 1...t :
   for every document wi (with current label x=li(t-1))
     (temporarily) remove its word counts ni(wj) from its class x:
        nx\i(t-1)(wj) =  nx(t-1)(wj) - ni(t-1)(wj)
     (temporarily) remove wi  from the documents in its class x:
         dx\i(t-1) = dx(t-1) - 1
     assign a new label x    = li(t-1) to wi  with 
           p( l | wi , l0(t)...li-1(t) li+1(t-1)...ld(t-1);   ,   ,   )
     add wi  to the documents in class x   
     add its word counts ni(wj) to the word counts for class x   

final estimate: 

use (some of the) snapshots l(1)...l(t) to estimate  p(+), p(wi | +), p(wi | -)

bayesian methods in nlp

27

estimation

- labels: l      bernoulli(  )   words: wi |l      multinomial(  l)

supervised

unsupervised

freq.

relative frequency estimation
- labels:    = d+ /d
- words:   i+ = n+(wi)/n+

bayes

with priors:
- labels:    = (d+ +   )/(d +  +  )
- words: 
  i+ = (n+(wi)  +   i)/(n+(w) +   0)

expectation maximization:
at each iteration t: 
- labels:   (t) = e[d](t-1)/d
- words:   i+ = e[n+(wi)](t-1)/e[n+(wi)](t-1)

id150:
for each ministep i at each iteration t: 
- labels:   i = (d+(-i) +  )/(d-1 +    +   )
- words:
   i+ = (n+ (-i)(wi)  +   i)/(n+ (-i)(w) +   0) 

bayesian methods in nlp

28

