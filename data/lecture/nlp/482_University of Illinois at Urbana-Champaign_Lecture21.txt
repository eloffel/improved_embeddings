unsupervised id48 pos 
tagger comparison 
  (gao, johnson) 

john wieting 
cs 598 

unsupervised id52 
    predict the tags for each word in a sentence 
    2 approaches used in this paper 

o maximum likelihood 

o bayesian 

 

    notice the prior which can bias the model 

    use a dirichlet prior to incorporate knowledge that words 

tend to only have few pos 

    authors tend to not use map as they tend to prefer the full 

posterior as it incorporates the uncertainty of the 
parameters 

    no known closed form of posterior in most cases so mc 

and id58 approaches are used. 

what is this paper about? 
    authors found that recent papers produced 
contradictory results about these bayesian 
methods 

    they study 6 algorithms 

o em 

o variational em 

o 4 mcmc approaches 

    compare results on unsupervised pos 

tagging  

id48 id136 

    the parameters of an id48 are a pair of multinomials for each 
state t. the first specifies the distribution over states t' following 
state t and the second, the distribution over words w given t. 

    since this is a bayesian model, priors are put on these 

multinomials. the authors use fixed and uniform dirichlets for their 
simplification of id136. 

o these control the sparsity of the transition and emission 

id203 distributions. 
    as they approach zero, the model strongly prefers sparsity 

(i.e. few words per tag) 

expectation maximization 

    goal is to maximize the marginal log-

likelihood 

ml em in id48 

1. first compute forward and backward parameters which will be 

needed in m step 

 

 

 

2. then differentiate the q function and maximize it subject to 
the constraint the probabilities sum to 1. set to 0 and solve: 

 

 

 

 

3. then you are done! 

variational em 
    in variational em, we cannot represent our 
desired posterior in closed form. thus we 
need to approximate it by minimizing the kl 
divergence between it and the posterior. 

    this procedure works well for id48s since 
the modifications to the e and m step turn 
out to be very minor. the updates in the m 
step are: 

mcmc  

    samplers are either pointwise or blocked 

o pointwise = sample a single state ti corresponding to 

a particular word wi at each step (o(nm)). 

o blocked = resample all words in a sentence in a 

single step (o(nm^2)) using forward-backward 
algorithm varient. 

    they are also either explicit or collapsed 

o explicit = sample id48 parameters (both theta and 

phi) as well as the states 

o collapsed = integrate out the id48 parameters and 

only sample the states 

    in this paper all 4 possible variations are implemented 

and compared. 

pointwise and explicit 
    sample from the following distributions 

where nt is the state-to-state transition count 
and nt' is the state-to-word emission count. 
    first sample the id48 parameters and then 
sample each state ti given the current word 
wi and the neighboring states ti and ti+1 

collapsed and explicit 
    just sample from the following distribution: 

pointwise and blocked 
    here we are resampling an entire sentence 
    how?  

o first resample id48 parameters (using equations 

from pointwise and explicit sampler), then use 
forward-backward algorithm to sample a structure. 

 
 

o once done, we can update the counts to be used for 

the sampling step in the next iteration. 

collapsed and blocked 
    in this model, we again iterate through the sentences 

resampling the states for each sentence conditioned on n 
(state-to-state) and n' (state-to-word). 

o need to first compute parameters of a proposal id48 

 

    then a structure is sampled using the dynamic algorithm 
    the motivation for the proposal distribution is that we want to 

mentioned on the slide. 

sample from  

 

 

 
 

 

collapsed and blocked 
    however that denominator is tough to 

compute. so a hasting's sampler is used to 
sample from the desired distribution. the 
sample distribution chosen was to use the 
distribution whose parameters are  

  

evaluation 
    how to evaluate? 

o we need to somehow map a system's states to the 

gold standard states 

o variation of information 

   

information theoretic measure that measures the 
difference in information between two clusters 

    unfortunately this approach allows a tagger that 
assigns each word the same tag to perform well. 

o mapping approaches 

    map each id48 state to the most common pos 

tag occurring in it. 
   

issue with this approach is that it rewards id48s with large 
amounts of states 

evaluation 
    more mapping approaches 

o split gold data set and do the state mapping on one 

half and use the other half for evaluation (cross 
validation approach) 

o insist that at most one id48 state can be mapped to 

a particular pos tag 

    used greedy algorithm to match states to tags 

until it runs out of states/tags. unassigned 
states/tags are left unassigned. 

results 

    in their experiments, the authors vary the number of tags and 

the size of the corpus. 

    for each model they optimize the two hyperparameters over 

a range of values ranging from 0.0001 to 1 and report the 
results for the best set for that model. 

    as expected, on small data sets, the prior seems to play a 

more important role and so the mcmc approaches do better 
than em and vb (which has a worse approximation with 
smaller amounts of data). 

    on larger data sets the results evened out though. 
    in terms of convergence time, blocked samplers were faster 

than pointwise and explicit were faster than collapsed. 

results 

results 

results 

results 

summary 

    this paper compared the performance of 5 different 

bayesian approaches and 1 ml approach to 
unsupervised id52 using id48s. 

    the comparison spanned different numbers of hidden 

states and different amounts of training data 

    id150 approaches seemed to perform the 
best however their advantage decreased as the data 
sets increased in size 

    vb was the fastest bayesian model 

