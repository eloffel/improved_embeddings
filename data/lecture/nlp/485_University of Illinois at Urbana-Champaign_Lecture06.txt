cs598jhm: advanced nlp  (spring 2013)
http://courses.engr.illinois.edu/cs598jhm/

lecture 6:
(probabilistic) latent 
semantic analysis

julia hockenmaier
juliahmr@illinois.edu
3324 siebel center
of   ce hours: by appointment

indexing by latent 
semantic analysis
(deerwester et al., 1990)

bayesian methods in nlp

2

latent semantic analysis
the task: 
return relevant documents for text queries
the problem: relevance is conceptual/semantic
-the index of relevant documents may not contain all query 
terms (synonymy and missing information) 
-the query terms may be ambiguous (polysemy)
indexing by latent semantic analysis
-map queries and documents into a new vector space 
whose k dimensions correspond to independent concepts 
-in this space, queries will be near semantically close 
documents

bayesian methods in nlp

3

2

 

i

n
o
s
n
e
m
d

i

: documents
: terms
: query
: region closest to query

(e.g. cosine > .9)

dimension 1

bayesian methods in nlp

4

??latent semantic analysis
low-rank approximation of singular value decomposition (svd):
documents

concepts

s
    
m
r
e
t

   

s documents
   

t

p
e
c
n
o
c

=

x                               t0            s0            d0                                  =    
x: term-document matrix (=data): xij  = freq of wi in dj
    = t0 s0d0        (k-rank approximation of x)
t0: columns are orthogonal and unit-length t0   t0 = i 
s0: diagonal matrix of the k largest singular values
d0: columns are orthogonal and unit-length d0   d0 = i 

bayesian methods in nlp

5

s
m
r
e
t

^

this should really be xlsa: term similarity

t0

                                                  =  t0 s0 s0 t0

term wi

dot product of wi, wj 
in the new space

t0 s0
           =  t0 s0 s0 t0  
(d cancels out because s is diagonal and d orthonormal)
similarity of terms wi, wj in the new space:  (         )ij

bayesian methods in nlp

lsa: document similarity

d0
20

doc. dj

                                                    = d0 s0 s0 d0

         

dot product of di, dj 
in the new space

d0 s0

           =  d0 s0 s0 d0  
(t cancels out because s is diagonal and t orthonormal)
similarity of documents di, dj in the new space:  (         )ij

bayesian methods in nlp

7

lsa: term-document similarity
the elements of      give the similarity of terms and 
documents. 
now, terms are projected to ts1/2 , documents to ds1/2

bayesian methods in nlp

8

lsa: query-document similarity
queries q  are    pseudo-documents   : 
they don   t appear in x

construct their term vector xq 
de   ne their document vector dq = x   q ts-1

bayesian methods in nlp

9

probabilistic latent 
semantic indexing
(hofmann 1999)

bayesian methods in nlp

10

the aspect model
observations are document-word pairs (d, w)

assume there are k aspects z1...zk
each observation is associated with a hidden aspect z

p(d, w) = p(d)p(w | d)

with      p(w | d) =    z   z p(w | z)p(z | d)

or, equivalently: 

p(d, w) =    z   z p(z)p(d | z)p(w | z) 

bayesian methods in nlp

11

a geometric interpretation

w3

1.0

1.0

w1

1.0

w2

word simplex  
any point in this simplex de   nes 
a multinomial over words
documents    p(w |d)
each document corresponds 
to one multinomial over words 
topics            p(w | z)
each topic is 
a multinomial over words
topic simplex
the topics de   ne the corners 
of a (sub)simplex.
all training documents lie inside 
this topic simplex.

p(w | d)  =           1 p(w | z1 ) +           2 p(w | z2 ) +               3 p(w | z3 )
              = p(z1 | d)p(w | z1)  + p(z2 | d)p(w | z2)  + p(z3 | d)p(w | z3)

bayesian methods in nlp

12

plsa is a mixture model
mixture models:
-k mixture components and n observations x1... xn
-mixing weights (  1...   k): p( k ) =    k
-each observation xn is generated by mixture component zn
p( xn ) = p( zn ) p( xn | zn )

plsi:
-mixture components = topics
-mixing weights are speci   c to each document   d = (  d1...  dk)
-each observation (word) wd,n is a sample 
from the document-speci   c mixture model. 
it is drawn from one of the components zd,n
p( wd,n ) = p( zd,n  |   d ) p( wd,n | zd,n )

bayesian methods in nlp

13

estimation: em algorithm
e-step: recompute
p(z | d, w) = p(z, d, w) /    z    p(z   , d, w)
with      p(z, d, w) = p(z)p(d | z)p(w | z)
m-step:  recompute
p(w | z)        d freq(d, w) p( z | d, w)
p(d | z)         w freq(d, w) p( z | d, w)
p(z)              d    w freq(d, w) p( z | d, w)

bayesian methods in nlp

14

