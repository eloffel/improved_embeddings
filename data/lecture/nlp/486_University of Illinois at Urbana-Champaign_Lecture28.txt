indian buffet process
indian buffet process
xiaolong wang and daniel khashabi

uiuc, 2013

contents
    mixture model
    finite mixture
    infinite mixture

    matrix feature model

    finite features 
    finite features 
    infinite features(indian buffet process)

bayesian nonparametrics
    models with undefined number of elements, 

    dirichlet process for infinite mixture models 
    with various applications 

  hierarchies 
  topics and syntactic classes 
  objects appearing in one image 
  objects appearing in one image 

 

 

 

bayesian nonparametrics
    models with undefined number of elements, 

    dirichlet process for infinite mixture models 
    with various applications 

  hierarchies 
  topics and syntactic classes 
  objects appearing in one image 
  objects appearing in one image 

    cons

  the models are limited to the case that could be modeled using dp. 
  i.e. set of observations are generated by only one latent component 

bayesian nonparametrics contd. 

    in practice there might be more complicated interaction 

between latent variables and observations 

 

 
 
 

bayesian nonparametrics contd. 

    in practice there might be more complicated interaction 

between latent variables and observations 

    solution

    looking for more flexible nonparametric models 
 
 

bayesian nonparametrics contd. 

    in practice there might be more complicated interaction 

between latent variables and observations 

    solution

    looking for more flexible nonparametric models 
    such interaction could be captured via a binary matrix 
    infinite features means infinite number of columns 

bayesian nonparametrics contd. 

    in practice there might be more complicated interaction 

between latent variables and observations 

    solution

    looking for more flexible nonparametric models 
    such interaction could be captured via a binary matrix 
    infinite features means infinite number of columns 

finite mixture model

{ }n
x
i=
1

i

    set of observation: 
 
 
 
 

 

 

finite mixture model

{ }n
x
i=
1

i

    set of observation: 
    constant clusters, 
 
 
 

k

 

 

finite mixture model

    set of observation: 
    constant clusters, 
    cluster assignment for      is 
 
 

k

x

i

{ }n
x
i=
1

i

c    

i

{
1,...,

}

k

 

 

finite mixture model

    set of observation: 
    constant clusters, 
    cluster assignment for      is 
i
    cluster assignments vector :  
 

k

x

i

{ }n
x
i=
1

i

c    

{
1,...,

}

k

 

 

finite mixture model

    set of observation: 
    constant clusters, 
    cluster assignment for      is 
i
    cluster assignments vector :  
 

k

x

i

{ }n
x
i=
1

i

{
}
c    
1,...,
k
=c
,...,
2,
[
c c
1

c

n

t
]

 

 

finite mixture model

{ }n
x
i=
1

i

    set of observation: 
    constant clusters, 
    cluster assignment for      is 
i
t
    cluster assignments vector :  
]
    the id203 of each sample under the model: 

{
}
c    
1,...,
k
=c
,...,
2,
[
c c
1

k

c

x

n

i

p
p

(
(

x
x

i

   =
=
  
)
)

|
|

p
p

(
(

x
x

i

|
|

c
c
i

=
=

k p
k p

)
)

(
(

c
c
i

=
=

k
k

)
)

k

   
   

=
1

k

    the likelihood of samples: 
=

  
)

p

(

n

k

      x

|

=
1

i

=
1

k

 

p

(

x

i

|

c
i

=

k

)

p c
(
i

=

k

)

finite mixture model

{ }n
x
i=
1

i

    set of observation: 
    constant clusters, 
    cluster assignment for      is 
i
t
    cluster assignments vector :  
]
    the id203 of each sample under the model: 

{
}
c    
1,...,
k
=c
,...,
2,
[
c c
1

k

c

x

n

i

p
p

(
(

x
x

i

   =
=
  
)
)

|
|

p
p

(
(

x
x

i

|
|

c
c
i

=
=

k p
k p

)
)

(
(

c
c
i

=
=

k
k

)
)

k

   
   

=
1

k

    the likelihood of samples: 
=

  
)

p

(

n

k

      x

|

=
1

i

=
1

k

p

(

x

i

|

c
i

=

k

)

p c
(
i

=

k

)

    the prior on the component probabilities (symmetric dirichlet dits.)

     

|

~ dirichlet(

     
).
k
k

   

,

,

finite mixture model

    since we want the mixture model to be valid for any general 

component
assignments to be the goal of learning this mixture model ! 

we only assume the number of cluster 

p x

c

(

)

|

j

j

i=

    cluster assignments:  
,...,
c
    the model can be summarized as : 

c c
2,
1

[

=c

t
]

n

   
   
   
     
   
      

| ~ dirichlet(

     
     
).
k
k
| ~ discrete( )
  
  

   

c
i

,

,

 
 

finite mixture model

    since we want the mixture model to be valid for any general 

component
assignments to be the goal of learning this mixture model ! 

we only assume the number of cluster 

p x

c

(

)

|

j

j

i=

    cluster assignments:  
,...,
c
    the model can be summarized as : 

c c
2,
1

[

=c

t
]

n

   
   
   
     
   
      

| ~ dirichlet(

     
     
).
k
k
| ~ discrete( )
  
  

   

c
i

,

,

    to have a valid model, all of the distributions 

must be valid ! 

p

(
   c

|

)

=

p

(
c   

|

). ( )
  

p

p

( )
c

finite mixture model

    since we want the mixture model to be valid for any general 

component
assignments to be the goal of learning this mixture model ! 

we only assume the number of cluster 

p x

c

(

)

|

j

j

i=

    cluster assignments:  
,...,
c
    the model can be summarized as : 

c c
2,
1

[

=c

t
]

n

   
   
   
     
   
      

| ~ dirichlet(

     
     
).
k
k
| ~ discrete( )
  
  

   

c
i

,

,

    to have a valid model, all of the distributions 

must be valid ! 

p

(
   c

|

)

=

p

(
c   

|

). ( )
  

p

prior

p

( )
c

finite mixture model

    since we want the mixture model to be valid for any general 

component
assignments to be the goal of learning this mixture model ! 

we only assume the number of cluster 

p x

c

(

)

|

j

j

i=

    cluster assignments:  
,...,
c
    the model can be summarized as : 

c c
2,
1

[

=c

t
]

n

   
   
   
     
   
      

| ~ dirichlet(

     
     
).
k
k
| ~ discrete( )
  
  

   

c
i

,

,

    to have a valid model, all of the distributions 

must be valid ! 

likelihood

p

(
   c

|

)

=

p

(
c   

|

). ( )
  

p

prior

p

( )
c

finite mixture model

    since we want the mixture model to be valid for any general 

component
assignments to be the goal of learning this mixture model ! 

we only assume the number of cluster 

p x

c

(

)

|

j

j

i=

    cluster assignments:  
,...,
c
    the model can be summarized as : 

c c
2,
1

[

=c

t
]

n

   
   
   
     
   
      

| ~ dirichlet(

     
     
).
k
k
| ~ discrete( )
  
  

   

c
i

,

,

    to have a valid model, all of the distributions 

must be valid ! 

likelihood

posterior

p

(
   c

|

)

=

p

(
c   

|

). ( )
  

p

prior

p

( )
c

finite mixture model

    since we want the mixture model to be valid for any general 

component
assignments to be the goal of learning this mixture model ! 

we only assume the number of cluster 

p x

c

(

)

|

j

j

i=

    cluster assignments:  
,...,
c
    the model can be summarized as : 

c c
2,
1

[

=c

t
]

n

   
   
   
     
   
      

| ~ dirichlet(

     
     
).
k
k
| ~ discrete( )
  
  

   

c
i

,

,

    to have a valid model, all of the distributions 

must be valid ! 

likelihood

posterior

p

(
   c

|

)

=

p

(
c   

|

). ( )
  

p

prior

p

( )
c

marginal likelihood
(evidence)

finite mixture model contd.

p

( )
c

=    

   

k

p

( |
c   

)

p

( )
     

d

p

( )
  

   
=    
   

d

(

     
)
k
k

,...,

   
1

k

   
       
  
  
   
k
k

=
1

k

   
1

finite mixture model contd.

p

( )
c

m
k

p

( |
c   

)

p

( )
     

d

p

( )
  

   
=    
   

d

(

     
)
k
k

,...,

   
1

k

   
       
  
  
   
k
k

=
1

k

   
1

=    

   

k

n

=   (cid:1)

  
(

c
i

=
1

i

k

)

finite mixture model contd.

p

( )
c

m
k

p

( |
c   

)

p

( )
     

d

p

( )
  

   
=    
   

d

(

     
)
k
k

,...,

   
1

k

   
       
  
  
   
k
k

=
1

k

   
1

=    

   

k

n

=   (cid:1)

  
(

c
i

=
1

i

k

)

   

p

k

=    c   
(
|

)

  

m
k
k

=
1

k

finite mixture model contd.

p

( )
c

m
k

p
p

( )
( )
c
c

p

( |
c   

)

p

( )
     

d

p

( )
  

   
=    
   

d

(

     
)
k
k

,...,

   
1

k

   
       
  
  
   
k
k

=
1

k

   
1

=    

   

k

n

=   (cid:1)

  
(

c
i

=
1

i

k

)

   

p

k

=    c   
(
|

)

  

m
k
k

=
1

k

p
p

( |
( |
c   
c   

)
)

p
p

( )
( )
     
     

d
d

=    
   
=

   

k

=

   

   

k

1

d

(

     
)
k
k

.
..,

,

+

  
   
1
k

  

d

m
k

  

k

k

   

=
1

k

finite mixture model contd.

p

( )
c

m
k

p
p

( )
( )
c
c

p

( |
c   

)

p

( )
     

d

p

( )
  

   
=    
   

d

(

     
)
k
k

,...,

   
1

k

   
       
  
  
   
k
k

=
1

k

   
1

=    

   

k

k

=    c   
(
|

)

  

m
k
k

=
1

k

n

=   (cid:1)

  
(

c
i

=
1

i

k

)

   

p

p
p

( |
( |
c   
c   

)
)

p
p

( )
( )
     
     

d
d

=    
   
=

   

k

+

  
   
1
k

  

d

m
k

  

k

k

   

=
1

k

=

=

   

   

k

d

(

  

  

k

k

=
1

   
   
   
   

   
   
   
   
   
   

1

,

.
..,

     
)
k
k
  
k
k

m
k

   
   
   

+

  
k

   
   
   

   
   
   

  

(
)
  
  
+
)
n

(

,

s.t.

m
k

=

  
(

n

   

=
1

i

=

k

)

c
i

  

infinite mixture model 

 

 

  

 

 

 
 

 
 

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

 

 
 

 

 
 

 
 

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

    since we always have limited samples in reality, we will have limited 

number of clusters used; so we define two set of clusters:
 
 

 
 

 

 
 

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

    since we always have limited samples in reality, we will have limited 

number of clusters used; so we define two set of clusters:
 
 

number  of classes for which 

km >

k+

0

 
 

 

 
 

k+

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

    since we always have limited samples in reality, we will have limited 

number of clusters used; so we define two set of clusters:
 
 
 
 

number  of classes for which 
number  of classes for which

km >
km =

k+
0k

0
0

k+

0k

 

 
 

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

    since we always have limited samples in reality, we will have limited 

number of clusters used; so we define two set of clusters:
 
 

number  of classes for which 
number  of classes for which
    >
    >

km >
0
km =
0
   
   
k
m
k
m
k

    assume a reordering, such that 
    assume a reordering, such that 

=
=

0;
0;

and
and

k+
0k

k
k

+

           
           

k
k

k
k

+

m
m
k

>
>

0
0

k+

0k

 

 
 

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

    since we always have limited samples in reality, we will have limited 

number of clusters used; so we define two set of clusters:
 
 

number  of classes for which 
number  of classes for which
    >
    >

km >
0
km =
0
   
   
k
m
k
m
k

    assume a reordering, such that 
    assume a reordering, such that 

=
=

0;
0;

and
and

k+
0k

k
k

+

           
           

k
k

k
k

+

m
m
k

>
>

0
0

k+

0k

k

 

 
 

infinite mixture model 

    infinite clusters likelihood 
    it is like saying that we have : 

k      

    since we always have limited samples in reality, we will have limited 

number of clusters used; so we define two set of clusters:
 
 

number  of classes for which 
number  of classes for which
    >
    >

km >
0
km =
0
   
   
k
m
k
m
k

    assume a reordering, such that 
    assume a reordering, such that 

=
=

0;
0;

and
and

k+
0k

k
k

+

           
           

k
k

k
k

+

m
m
k

>
>

0
0

k+

0k

k

    infinite clusters likelihood 
    it is like saying that we have : 
    infinite dimensional multinomial cluster distribution. 

k      

infinite mixture model 
    now we return to the previous slides and set            in formulas 

k      

p

( )
c

=

k

k

=
1

   
   
   
   

   
   
   

  
k
k

  

  

   
   
   
   
   
   

m
k

+

  
k

   
   
   

   
   
   

  

)
(
  
  
+
n
)

  

(

,

s.t.

m
k

=

  
(

n

   

=
1

i

=

k

)

c
i

infinite mixture model 
    now we return to the previous slides and set            in formulas 

k      

p

( )
c

=

k

k

=
1

   
   
   
   

   
   
   

  
k
k

  

  

   
   
   
   
   
   

m
k

+

  
k

   
   
   

   
   
   

  

)
(
  
  
+
n
)

  

(

,

s.t.

m
k

=

  
(

n

   

=
1

i

=

k

)

c
i

   + =   

1)

x

x

(

( )
x

   

  

(

m
k

+

  
)
k

=

(

m
k

+

  
k

   

)...(
1

     
)
k
k

  
) (

infinite mixture model 
    now we return to the previous slides and set            in formulas 

k      

p

( )
c

=

k

k

=
1

   
   
   
   

   
   
   

  
k
k

  

  

   
   
   
   
   
   

m
k

+

  
k

   
   
   

   
   
   

  

)
(
  
  
+
n
)

  

(

,

s.t.

m
k

=

  
(

n

   

=
1

i

=

k

)

c
i

   + =   

1)

x

x

(

( )
x

   

  

(

m
k

+

  
)
k

=

(

m
k

+

  
k

   

)...(
1

     
)
k
k

  
) (

   

  
  

(
(

m
km

+
+

  
)
)
k

  

(

  
)
k

=

(

  
  
)
k

m
km

       
   
1
1
   
   
   
   

=
1

j

+

j

  
  
k

   
   
   
   

infinite mixture model 
    now we return to the previous slides and set            in formulas 

k      

p

( )
c

=

k

k

=
1

   
   
   
   

   
   
   

  
k
k

  

  

   
   
   
   
   
   

m
k

+

  
k

   
   
   

   
   
   

  

)
(
  
  
+
n
)

  

(

,

s.t.

m
k

=

  
(

n

   

=
1

i

=

k

)

c
i

   + =   

1)

x

x

(

( )
x

   

  

(

m
k

+

  
)
k

=

(

m
k

+

  
k

   

)...(
1

     
)
k
k

  
) (

   

   

p

( )
c

=

  
)
k
k

  
n
(

  
)
(
+
  
)

  

   
=    
   

  
k

   
   
   

k

+

k

k

=
1

  

   
   
     
   

(

km

+

(

  
)
k

   
   
   

   
1

+

mk
k

      

=
1

k

=
1

j

   
   
   

+

j

  
k

   
   
   

  
n
(

  
)
(
+
  
)

  

  
  

(
(

m
km

+
+

  
)
)
k

  

(

  
)
k

=

(

  
  
)
k

m
km

       
   
1
1
   
   
   
   

=
1

j

+

j

  
  
k

   
   
   
   

infinite mixture model 
    now we return to the previous slides and set            in formulas 

k      

p

( )
c

=

k

k

=
1

   
   
   
   

   
   
   

  
k
k

  

  

   
   
   
   
   
   

m
k

+

  
k

   
   
   

   
   
   

  

)
(
  
  
+
n
)

  

(

,

s.t.

m
k

=

  
(

n

   

=
1

i

=

k

)

c
i

   + =   

1)

x

x

(

( )
x

   

  

(

m
k

+

  
)
k

=

(

m
k

+

  
k

   

)...(
1

     
)
k
k

  
) (

   

  
  

(
(

m
km

+
+

  
)
)
k

  

(

  
)
k

=

(

  
  
)
k

m
km

       
   
1
1
   
   
   
   

=
1

j

+

j

  
  
k

   
   
   
   

   

p

( )
c

=

  
)
k
k

  
n
(

  
)
(
+
  
)

  

   
=    
   

  
k

   
   
   

k

+

k

k

=
1

  

   
   
     
   

(

km

+

(

  
)
k

   
   
   

   
1

+

mk
k

      

=
1

k

=
1

j

   
   
   

+

j

  
k

   
   
   

  
n
(

  
)
(
+
  
)

  

if we set                 the marginal likelihood will be                      .               

p    c

( )

0

k      

instead we can model this problem, by defining probabilities on 

partitions of samples, instead of class labels for each sample. 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
 

k+

n

 
 

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

 
 

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

   
1

+

mk
k

      

=
1

k

=
1

j

   
   
   

+

j

  
k

   
   
      

  
(
n

  
(
)
  
+
)

   

   
c

[ ]
c

p

( )
c

=

k

+

k

!

k

0

!

   
   
   

  
k

   
   
   

p

([ ])

c

=

 
 

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

k

!

k

0

k

+

k

+

  
k

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   

=
1

=
1

k

+
+

j

   
1

j

+

k

mk
k

=
1

      
=
1
  
  
k

+

j

j

   
   
   
   
   
.
   
     

  
(
n

  
(
)
  
+
)

+

  
   
   
      
k
  
  
)
(
)
(
+
  
)

  
  
n
(

p

([ ])

c

=

p

( )
c

=

   

   
c

[ ]
c

k
k

!
!

k k

!

0

   

p

([ ])

c

=

k

  

+

.

 
 

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

k

!

k

0

k

+

k

+

  
k

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   

=
1

=
1

k

+
+

j

   
1

j

+

k

mk
k

=
1

      
=
1
  
  
k

+

j

j

   
   
   
   
   
.
   
     

  
(
n

  
(
)
  
+
)

+

  
   
   
      
k
  
  
)
(
)
(
+
  
)

  
  
n
(

p

([ ])

c

=

p

( )
c

=

   

   
c

[ ]
c

k
k

!
!

k k

!

0

   

p

([ ])

c

=

k

  

+

.

 
 

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

p

([ ])

c

=

p

( )
c

=

   

   
c

[ ]
c

   

p

([ ])

c

=

k

  

+

.

k
k

!
!

k k

!

0

k

!

k

0

k

+

k

+
+

+

  
k

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   
   
(

=
1
k

m
k

=
1

k

+

j

=
1

k

  
(
n

  
(
)
  
+
)

   
1

j

+

k

mk
k

=
1

      
=
1
  
  
k
)
!1

+

   

j

j

+

  
   
   
   
   
      
   
k
  
  
  
  
   
   
)
(
)
(
.
   
     
+
  
n
(
)
  
  
(
)
  
+
)
(
n

  

   .

.
    

   

lim

k

   

   

p

([ ])

c

=

k

  

+

.
            

1

 
 

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

k

   
1

+

+

0

j

!

c

[ ]
c

   
c

p

p

p

k

k

+

=

=

( )
c

mk
k

   

([ ])

   

  
k

  
   
   
   
   
      
   
k
  
  
  
  
   
   
)
(
)
(
.
   
     
+
  
n
(
)
  
  
(
)
  
+
)
(
n
    valid id203 distribution for an infinite mixture model
 

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   
   
(

      
=
1
  
  
k
)
!1

.
            

k k

([ ])

([ ])

  

lim

  

   

=
1
k

.
    

m
k

   .

=

+

  

k
k

=

   

p

1

=
1

=
1

=
1

   

c

!
!

!

c

j

   

.

k

k

k

k

0

k

k

k

+
+

+

+

+

+

j

j

  
(
n

  
(
)
  
+
)

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

k

   
1

+

+

0

j

!

c

[ ]
c

   
c

p

p

p

k

k

+

=

=

( )
c

mk
k

   

([ ])

   

  
k

  
   
   
   
   
      
   
k
  
  
  
  
   
   
)
(
)
(
.
   
     
+
  
n
(
)
  
  
(
)
  
+
)
(
n
    valid id203 distribution for an infinite mixture model
    exchangeable with respect to clusters  assignments ! 

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   
   
(

      
=
1
  
  
k
)
!1

.
            

k k

([ ])

([ ])

  

lim

  

   

=
1
k

.
    

m
k

   .

=

+

  

k
k

=

   

p

1

=
1

=
1

=
1

   

c

!
!

!

c

j

   

.

k

k

k

k

0

k

k

k

+
+

+

+

+

+

j

j

  
(
n

  
(
)
  
+
)

 
 

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

k

   
1

+

+

0

j

!

c

[ ]
c

   
c

p

p

p

k

k

+

=

=

( )
c

mk
k

   

([ ])

   

  
k

  
   
   
   
   
      
   
k
  
  
  
  
   
   
)
(
)
(
.
   
     
+
  
n
(
)
  
  
(
)
  
+
)
(
n
    valid id203 distribution for an infinite mixture model
    exchangeable with respect to clusters  assignments ! 

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   
   
(

      
=
1
  
  
k
)
!1

.
            

k k

([ ])

([ ])

  

lim

  

   

=
1
k

.
    

m
k

   .

=

+

  

k
k

=

   

p

1

=
1

=
1

=
1

   

c

!
!

!

c

j

   

.

k

k

k

k

0

k

k

k

+
+

+

+

+

+

j

j

    important for id150 (and chinese restaurant process)
 

  
(
n

  
(
)
  
+
)

infinite mixture model contd.
    define a partition of objects; 
    want to partition        objects into         classes 
{
    equivalence class of object partitions:  

k+

[ ]
c

=

n

|i
c c

   

c

}

i

k

   
1

+

+

0

j

!

c

[ ]
c

   
c

p

p

p

k

k

+

=

=

( )
c

mk
k

   

([ ])

   

  
k

  
   
   
   
   
      
   
k
  
  
  
  
   
   
)
(
)
(
.
   
     
+
  
n
(
)
  
  
(
)
  
+
)
(
n
    valid id203 distribution for an infinite mixture model
    exchangeable with respect to clusters  assignments ! 

   
   
   
   
   
   
!
       
   
1
1
kmk
kmk
   
      
         
.
   
   
(

      
=
1
  
  
k
)
!1

.
            

k k

([ ])

([ ])

  

lim

  

   

=
1
k

.
    

m
k

   .

=

+

  

k
k

=

   

p

1

=
1

=
1

=
1

   

c

!
!

!

c

j

   

.

k

k

k

k

0

k

k

k

+
+

+

+

+

+

j

j

  
(
n

  
(
)
  
+
)

    important for id150 (and chinese restaurant process)
    di finetti   s theorem: explains why exchangeable observations are 

conditionally independent given some id203 distribution

chinese restaurant process (crp)

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

0m =
0m =

1

p c =
(

1

1
)

=

1

1

          

   

     

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m

1,             
1 1,             

=
=

m
m
2

=
=

0
0

p c
2

(

c=

1|

1

=

)

1
+

1

1

                  

p c
2

(

=

2 |

c
1

)

=

1
+

1

1

                         

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

1,             
1,             

m
m
2

=
=

1,             
1,             

m
m
3

=
=

0
0

(
p c
3

=

1|

c
1:2

)

=

1
+

12

            

      

p c
3

(

(
p c
3

=

2 |

c
1:2

)

=

1
+

1

2

=

3

|

c
1
:2

)

=

1
+

1

2

    

                         

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

2,             
2,             

m
m
2

=
=

1,             
1,             

m
m
3

=
=

0
0

p c
4

(

=

1|

c
1:3

)

=

2
+

1

3

                   
 

p c
4

(

=

3

|

c
1
:3

)

=

1
+

1

3

    

p c
4

(

=

2 |

c
1:3

)

=

1
+

1

3

                         

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

2,             
2,             

m
m
2

=
=

2,             
2,             

m
m
3

=
=

0
0

(
p c
5

=

1|

c
1:4

)

=

2
+

1

4

              

    

p c
5

(

=

3 |

c
1
:4

)

=

1
+

1

4

       
 

p c
5

(

=

2 |

c
1:4

)

=

2
+

1

4

                         

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

2,             
2,             

m
m
2

=
=

2,             
2,             

m
m
3

=
=

1,            
1,            

m
m
4

=
=

0 
0 

(
p c
6

=

1|

c
1:5

)

=

2
+

1

5

              

    

p c
6

(

=

3 |

c
1:
5

)

=

1
+

1

5

       
 

p c
6

(

=

2 |

c
1:5

)

=

2
+

1

5

 
                
    

p c
6

(

=

4 |

c
1:5

)

=

1
+

15

          

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

2,             
2,             

m
m
2

=
=

3,             
3,             

m
m
3

=
=

1,            
1,            

m
m
4

=
=

0 
0 

p c
7

(

=

1|

c
1:6

)

=

2
+

1

6

              

    

p c
7

(

=

3 |

c
1
:6

)

=

1
+

1

6

       
 

(
p c
7

=

2 |

c
1:6

)

=

3
+

1

6

 
                
    

p c
7

(

=

4 |

c
1:6

)

=

1
+

16

          

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

2,             
2,             

m
m
2

=
=

3,             
3,             

m
m
3

=
=

2,            
2,            

m
m
4

=
=

0 
0 

p c
8

(

=

1|

c
1:3

)

=

2
+

1

7

              

    

p c
8

(

=

3 |

c
1
:3

)

=

2
+

1

7

       
 

(
p c
8

=

2 |

c
1:3

)

=

3
+

1

7

                

 
    

p c
8

(

=

4 |

c
1:3

)

=

1
+

17

          

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

3,             
3,             

m
m
2

=
=

3,             
3,             

m
m
3

=
=

2,            
2,            

m
m
4

=
=

0 
0 

p c
9

(

=

1|

c
1:3

)

=

3
+

1

8

              

    

p c
9

(

=

3 |

c
1:

3

)

=

2
+

1

8

       
 

(
p c
9

=

2 |

c
1:3

)

=

3
+

1

8

 
                
    

p c
9

(

=

4 |

c
1:3

)

=

1
+

18

          

chinese restaurant process (crp)

(
p c
i

=

k c
1

|

,...,

c
i

   
1

)

=

parameter 

1  =

i

   
      
   
   
          
i

m
k
    +
1
  
+
1

k

   

k

+

k

=

k

+

1

  

  

m
m
1

=
=

3,             
3,             

m
m
2

=
=

3,             
3,             

m
m
3

=
=

2,            
2,            

m
m
4

=
=

1           
1           

m
m
5

=
=

5
5

(
p c

10

=

1|

c
1:4

)

=

3
+

1

9

                  

p c

(

10

=

3 |

c
1:4

)

=

2
+

9

p c
(

10

=

2 |

c
1:4

)

=

3
+

1

9

             

  

p c

(

10

1

=

         

p c

(

10

=

5 |

c
1:4

)

=

1
+

1

9

         

4 |

c
1:4

)

=

1
+

19

          

crp: id150
    gibbs sampler requires full conditional 

p

(

c
i

=

k

|

c x

,

   

i

   

)

p

(

x c

| ). (
p

=

c
i

k

|

c

)

   

i

    finite mixture model: 

p c
p c
i

(
(

=
=

k
k

|
|

c
c

   

i

=
=

)
)

    infinite mixture model: 

p

(

c
i

=

k

|

c

   

i

=

)

m
   
,
i k
    +
1
  
    +
1

  

  

   
   
   
   
   
   
   
   
   

n

n
0

i km

,

    +
+
   

1

n

  
k
k
  

,
i k

m
   
=

k

>

k

   

i

0
+

1

otherw
i

se

beyond the limit of single label

    in latent class models:

    each object (word) has only one latent label (topic)
    finite number of latent labels: lda
    infinite number of latent labels: dpm

    in latent feature (latent structure) models:
    each object (graph) has multiple latent features (entities)
    each object (graph) has multiple latent features (entities)
    finite number of latent features: finite feature model (ffm)
    infinite number of latent features: indian buffet process (ibp)

    rows are data points 
    columns are latent features 

    movie preference example:

    rows are movies: rise of the planet of the apes
    columns are latent features:

    made in u.s.
is science fiction
   
    has apes in it     

latent feature model

z
z

continuous
continuous

f
f

discrete
discrete

f
f

    f : latent feature matrix
    z : binary matrix
    v : value matrix

   =f z v

    with p(f)=p(z). p(v)

finite feature model
    generating z : (n*k) binary matrix
    for each column k, draw        from beta distribution
    for each object, flip a coin by 

  k

zik

   
  k |  ~ beta(
   
   
      
zik |  k ~ bernoulli(  k )
  

  
k

,1)

(  k ,1     k ) ~ dir(

  
k

,1)

 

finite feature model
    generating z : (n*k) binary matrix
    for each column k, draw        from beta distribution
    for each object, flip a coin by 

  k

zik

   
  k |  ~ beta(
   
   
      
zik |  k ~ bernoulli(  k )
  

  
k

,1)

    distribution of z : 

(  k ,1     k ) ~ dir(

  
k

,1)

finite feature model
    generating z : (n*k) binary matrix
    for each column k, draw        from beta distribution
    for each object, flip a coin by 

  k

zik

   
  k |  ~ beta(
   
   
      
zik |  k ~ bernoulli(  k )
  

  
k

,1)

(  k ,1     k ) ~ dir(

  
k

,1)

    distribution of z : 
      

z   

=

p

(

)

k

n

|

=
1

k

=
1

i

p z   
(

|

ik

k

=

)

k

   

=
1

k

m
k

  

k

(1

   

      

)

n

k

m
k

finite feature model
    generating z : (n*k) binary matrix
    for each column k, draw        from beta distribution
    for each object, flip a coin by 

  k

zik

   
  k |  ~ beta(
   
   
      
zik |  k ~ bernoulli(  k )
  

  
k

,1)

(  k ,1     k ) ~ dir(

  
k

,1)

    distribution of z : 
      

z   

=

p

(

)

k

n

|

=
1

k

=
1

i

p z   
(

|

ik

k

=

)

k

   

=
1

k

m
k

  

k

(1

   

      

)

n

k

m
k

p(z |   ) =

   

    (  k |  ) p(z   k |  k )d  k

p

  k   

k

k

finite feature model
    generating z : (n*k) binary matrix
    for each column k, draw        from beta distribution
    for each object, flip a coin by 

  k

zik

   
  k |  ~ beta(
   
   
      
zik |  k ~ bernoulli(  k )
  

  
k

,1)

(  k ,1     k ) ~ dir(

  
k

,1)

    distribution of z : 
      

z   

=

p

(

)

k

n

|

=
1

k

=
1

i

p z   
(

|

ik

k

=

)

k

   

=
1

k

m
k

  

k

(1

   

      

)

n

k

m
k

p(z |   ) =

   

k

k

p

    (  k |  ) p(z   k |  k )d  k
)  ( n     mk + 1)

  k   
  
  (mk +
k

  
k

k

   

k=1

=

  

  ( n + 1+

  
)
k

finite feature model
    generating z : (n*k) binary matrix
    for each column k, draw        from beta distribution
    for each object, flip a coin by 

  k

zik

   
  k |  ~ beta(
   
   
      
zik |  k ~ bernoulli(  k )
  

  
k

,1)

(  k ,1     k ) ~ dir(

  
k

,1)

    z is sparse:

    even  k        

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

    compute history h of feature (column) k
 

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

    compute history h of feature (column) k
 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

    compute history h of feature (column) k
 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

    compute history h of feature (column) k
 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

h4 = 2 3 + 2 7 + 29

    compute history h of feature (column) k
 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

h4 = 2 3 + 2 7 + 29

    compute history h of feature (column) k

    order features by h decreasingly 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

h4 = 2 3 + 2 7 + 29

    compute history h of feature (column) k

    order features by h decreasingly 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

h4 = 2 3 + 2 7 + 29

    compute history h of feature (column) k

    order features by h decreasingly 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

h4 = 2 3 + 2 7 + 29
z1 z2 are lof equivalent
iff lof(z1)=lof(z2) = [z]
iff lof(z1)=lof(z2) = [z]

    compute history h of feature (column) k

    order features by h decreasingly 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

    compute history h of feature (column) k

    order features by h decreasingly 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

h4 = 2 3 + 2 7 + 29
z1 z2 are lof equivalent
iff lof(z1)=lof(z2) = [z]
iff lof(z1)=lof(z2) = [z]

describing [z]:

k h = #{i;hi = h}
k+ =

k h

   

i>0

k = k+ + k 0

indian buffet process
1st representation:  k        

    difficulty:
    p(z) -> 0
    solution: define equivalence classes on random binary feature matrices.

    left-ordered form function of binary matrices, lof(z):

    compute history h of feature (column) k

    order features by h decreasingly 

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

h4 = 2 3 + 2 7 + 29
z1 z2 are lof equivalent
iff lof(z1)=lof(z2) = [z]
iff lof(z1)=lof(z2) = [z]

describing [z]:

k h = #{i;hi = h}
k+ =

k h

   

i>0

k = k+ + k 0

cardinality of [z]:

       k

k0...k

2n    1

   
   
   

  

   
    = k !
   
   
k h !

2n    1

h=0

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
  (mk +

k

  
k

  
k

)  ( n     mk + 1)

   

k=1

= k !
   

2n    1

h=0

  

k h !

  ( n + 1+

  
)
k

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
  (mk +

k

  
k

  
k

)  ( n     mk + 1)

   

k=1

= k !
   

2n    1

h=0

  

k h !

  ( n + 1+

   
   
   
   
   

  
k

  (

  
k

)  ( n + 1)

  ( n + 1+

  
)
k

= k !
   

2n    1

h=0

  

k h !

  
)
k
   
   
   
   
   

k0   
   
k

k

k=1

  (mk +

)  ( n     mk + 1)

  
k

  ( n + 1+

  
)
k

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k0   
   
k

k

k=1

   
   
   
   
   

  (mk +

)  ( n     mk + 1)

  
k

  ( n + 1+

  
)
k

= k !
   

2n    1

h=0

  

k h !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

  
k

  
k

  (

)  ( n + 1)

  (mk +

)  ( n     mk + 1)

  
k

k0   
   
k

k

k=1

   
   
   
   
   
   
   
   
   
   

= k !
   

2n    1

h=0

  
= k !
   

2n    1

h=0

  

k h !

  ( n + 1+

  
k

  (

  
k

)  ( n + 1)

k h !

  ( n + 1+

  
)
k

  
)
k

   
   
   
   
   
   
   
   
   
   

  ( n + 1+

  
)
k

k

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

mk    1

    j +

(

j=0

  
)
k

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

(mk     1)!   

  
k

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

= k !
   

2n    1

h=0

  

k h !

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

   
   
   
   
   
  

  
k

  (

  
k

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k

k+   

k=1

k+   

   k=1

   
   
   
   
   

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

= k !
   

2n    1

h=0

  

k h !

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

   
   
   
   
   
  

  (

+ 1)  ( n + 1)

  
k
  ( n + 1+

  
)
k

k

   
   
   
   
   

k

k+   

k=1

k+   

   k=1

   
   
   
   
   

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

= k !
   

2n    1

h=0

  

k h !

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

   
   
   
   
   
   
  

n

n !
   ( j +

j=1

k

  
)
k
   
   
   
   
   
   

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

= k !
   

2n    1

h=0

  

k h !

n

    1
(1+

j=1

  / j

k

   
   
   
   
   

)

   
   
   
   
   
  

k

   
   
   
   
   

k

k+   

k=1

k+   

   k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

  
)
k

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

= k !
   

2n    1

h=0

  

k h !

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

= k !
   

2n    1

h=0

  

k h !

  
k

  
k

  (

)  ( n + 1)

  
)
k
  / j
k

  ( n + 1+

n

   (1+
   j=1

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

2n    1

= k !
   

k h !
  
= k !
   

2n    1

h=0

k h !

  

h=0

  
k

  
k

  (

)  ( n + 1)

  
)
k
  / j
k

  ( n + 1+

n

   (1+
   j=1

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

= k !
   

2n    1

h=0

  

k h !

  
k

  
k

  (

)  ( n + 1)

  
)
k
  / j
k

  ( n + 1+

n

   (1+
   j=1

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

   
   
   
   
   

= k !
   

2n    1

h=0

k h !

  
k

  
k

  (

)  ( n + 1)

  ( n + 1+

k

   
   
   
   
   

k+   

k=1

  (mk +

  (

  
k
  
k

)  ( n     mk + 1)

)  ( n + 1)

  
)
k
  / j
k

  
=

  

k !
2n    1
   

h=1

k0 !

n

   (1+
   j=1

k h !

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

k+    (mk     1)!

  k+
k k+    k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

k !
2n    1
   

h=1

k0 !

  

k+    (mk     1)!

  k+
k k+    k=1

k h !

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

k !

k0 !k k+

  

  k+
2 n    1
   

h=1

kh !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

  k+
2n    1
   
  

h=1

k h !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

   

  / j
k

n

   e
   j=1

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

  k+
2n    1
   
  

h=1

k h !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

  k+
2n    1
   
  

h=1

k h !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])
   (1+
   j=1

=

n

k0 !

kh !

k !
2 n    1
   

h=1

  

     h n

 = e

  k+
2n    1
   
  
j harmonic sequence sum

h=1

k h !

h n =

n

    1

j=1

  / j
k

)    k

k+   

   k=1

(mk     1)!   

  
k

(n     mk )!

n !

k+    (mk     1)!

   k=1

(n     mk )!

n !

indian buffet process
1st representation:  k        

given:

pr(z |  ) =

k

   

k=1

  
k

  (mk +

  
k

)  ( n     mk + 1)

   
card([z]) =

   
   
   

       k

k0...k

2n    1

  ( n + 1+

  
)
k

2n    1

   
    = k !
   
   
k h !
k+ <    
k+ <    

  

h=0
derive when                   :
derive when                   :

k         (               almost surely)
k         (               almost surely)

pr([z ] |  ) = pr(z |  )    card([z ])

     h n

 = e

  k+
2n    1
   
  

h=1

k h !

k+    (mk     1)!(n     mk )!

   k=1

n !

note:

   

   

pr([z ] |  )
pr([z ] |  )

is well defined because                    a.s.
depends on kh :

k+ <    

    the number of features (columns) with history h
    permute the rows (data points) does not change kh (exchangeability) 

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

 

 

 

 

 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

    the first customer tastes first k1
 

(1) dishes, sample

(1) ~ poission(  )

k1

 

 

 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

    the first customer tastes first k1
    the i-th customer:

(1) ~ poission(  )
mk
i + 1
    mk : number of previously customers taking dish k

    taste a previously sampled dish with id203 

(1) dishes, sample

k1

 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

    the first customer tastes first k1
    the i-th customer:

(1) ~ poission(  )
mk
i + 1
    mk : number of previously customers taking dish k

    taste a previously sampled dish with id203 

(1) dishes, sample

k1

(i ) ~ poission(

k1

  
)
i

    taste following k1

(i) new dishes, sample 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

    the first customer tastes first k1
    the i-th customer:

(1) ~ poission(  )
mk
i + 1
    mk : number of previously customers taking dish k

    taste a previously sampled dish with id203 

(1) dishes, sample

k1

(i ) ~ poission(

k1

  
)
i

    taste following k1

(i) new dishes, sample 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

    the first customer tastes first k1
    the i-th customer:

(1) ~ poission(  )
mk
i + 1
    mk : number of previously customers taking dish k

    taste a previously sampled dish with id203 

(1) dishes, sample

k1

(i ) ~ poission(

k1

  
)
i

    taste following k1

(i) new dishes, sample 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

   

indian restaurant with in   nitely many in   nite dishes (columns)

    the first customer tastes first k1
    the i-th customer:

(1) ~ poission(  )
mk
i + 1
    mk : number of previously customers taking dish k

    taste a previously sampled dish with id203 

(1) dishes, sample

k1

(i ) ~ poission(

k1

  
)
i

    taste following k1

(i) new dishes, sample 

poission distribution: p(k |  ) =

  ke     

k!

indian buffet process
2st representation: customers & dishes

indian buffet process
2st representation: customers & dishes

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

  k+ e     h n
   k1

(i )

n

i

n

   (

i

1

i

( i )

)k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

  k+ e     h n
   k1

(i )

n

i

n

   (

i

1

i

( i )

)k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

1/1
1/2
1/3
2/4
2/5
3/6
4/7
4/7
3/8
5/9
6/10

n

   (

i

1

i

( i )

)k1

=

  k+ e     h n
   k1

(i )

n

i

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

1/1
1/2
1/3
2/4
2/5
3/6
4/7
4/7
3/8
5/9
6/10

3!6!

10!

n

   (

i

1

i

( i )

)k1

=

  k+ e     h n
   k1

(i )

n

i

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

1/1
1/2
1/3
2/4
2/5
3/6
4/7
4/7
3/8
5/9
6/10

3!6!

10!

n

   (

i

1

i

( i )

)k1

=

  k+ e     h n
   k1

(i )

n

i

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

1/1
2/2
3/3
1/4
4/5
5/6
6/7
6/7
1/8
7/9
8/10

1/1
1/2
1/3
2/4
2/5
3/6
4/7
4/7
3/8
5/9
6/10

3!6!

10!

n

   (

i

1

i

( i )

)k1

=

  k+ e     h n
   k1

(i )

n

i

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

1/1
1/2
1/3
2/4
2/5
3/6
4/7
4/7
3/8
5/9
6/10

1/1
2/2
3/3
1/4
4/5
5/6
6/7
6/7
1/8
7/9
8/10

3!6!

10!

8!1!

10!

n

   (

i

1

i

( i )

)k1

=

  k+ e     h n
   k1

(i )

n

i

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

indian buffet process
2st representation: customers & dishes

(i )

k1

k2

(i ) k 3

(i )

1/1
1/2
1/3
2/4
2/5
3/6
4/7
4/7
3/8
5/9
6/10

1/1
2/2
3/3
1/4
4/5
5/6
6/7
6/7
1/8
7/9
8/10

3!6!

10!

8!1!

10!

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

( i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

i

we have:
we have:

pibp (z |  ) =

 

 

 

 

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

i

we have:
we have:

pibp (z |  ) =

note:

 

 

 

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

i
(2), next k1
(1), next k1

(3),    next k1

(n) dishes (columns) does not change p(z)

note:

    permute k1
 

 

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

note:

i
(2), next k1
(1), next k1

    permute k1
    the permuted matrices are all of same lof equivalent class [z]
 

(3),    next k1

(n) dishes (columns) does not change p(z)

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

note:

i
(2), next k1
(1), next k1

    permute k1
    the permuted matrices are all of same lof equivalent class [z]
    number of permutation:

(3),    next k1

(n) dishes (columns) does not change p(z)

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

note:

i
(2), next k1
(1), next k1

    permute k1
    the permuted matrices are all of same lof equivalent class [z]
    number of permutation:

(3),    next k1

n

(i )

(n) dishes (columns) does not change p(z)

   k1

i

2 n    1

   

h=1

k h !

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

note:

i
(2), next k1
(1), next k1

    permute k1
    the permuted matrices are all of same lof equivalent class [z]
    number of permutation:

(3),    next k1

n

(i )

(n) dishes (columns) does not change p(z)

   k1

i

2 n    1

   

h=1

k h !

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

note:

i
(2), next k1
(1), next k1

    permute k1
    the permuted matrices are all of same lof equivalent class [z]
    number of permutation:

(3),    next k1

n

(i )

(n) dishes (columns) does not change p(z)

pibp ([z ] |  ) =

k+    (n     mk )!(mk     1)!

n !

k

  k+ e     h n
2n    1
   

k h !

h=1

   k1

i

2 n    1

   

h=1

k h !

indian buffet process
2st representation: customers & dishes

from:

pnew _ dish =

n

   

i

  
(
i

( i )

)k1

  
   
i

e

(i )!

k1

=

n

   (

i

1

i

(i )

)k1

pold _ dishes =

( i )

n

n

i

(i )

1

)k1

   (

  k+ e     h n
   k1
k+    (n     mk )!(mk     1)!

i

i

n !

k

we have:
we have:

pibp (z |  ) =

  k+ e     h n k+    (n     m )!(m     1)!
  k+ e     h n
k+    (n     mk )!(mk     1)!
   k1

n !

(i )

n

k

note:

i
(2), next k1
(1), next k1

    permute k1
    the permuted matrices are all of same lof equivalent class [z]
    number of permutation:

(3),    next k1

n

(i )

2nd representation

is equivalent to 
1st representation

   k1

i

2 n    1

   

h=1

k h !

(n) dishes (columns) does not change p(z)

pibp ([z ] |  ) =

k+    (n     mk )!(mk     1)!

n !

k

  k+ e     h n
2n    1
   

k h !

h=1

indian buffet process
3rd representation:  distribution over 
    directly generating the left ordered form (lof) matrix z
    for each history h:

collections of histories

    mh: number of non-zero elements in h
    generate kh columns of history h

kh ~ poission(  (mh     1)!(n     mh )!

)

n !

    the distribution over collections of histories
    the distribution over collections of histories

    note:

    permute digits in h does not change mh (nor p(k) )
    permute rows means customers are exchangeable

indian buffet process

    effective dimension of the model k+:

    follow poission distribution:
    derives from 2nd representation by summing poission

k+ ~ poission(  h n )

components

    number of features possessed by each object:

    follow possion distribution: 
    follow possion distribution: 
    derives from 2nd representation:

poission(  )
poission(  )

  the first customer chooses                        dishes
  the customers are exchangeable and thus can be purmuted

poission(  )

    z is sparse:

    non-zero element 
    derives from the 2nd (or 1st) representation:

  
n  
  expected number of non-zeros for each row is       
  expected entries in z is 

ibp: gibbs  sampling

    need to have the full conditional 

p(zik = 1| z

   ( ik ), x)     p(x | z) p(zik = 1| z

   (ik ) )

   

z

   

   
    p(x|z) depends on the model chosen for the observed data. 

denotes the entries of z other than           .

( , )n k

nkz

    by exchangeability, consider generating row as the last customer:

   
   

by ibp, in which
by ibp, in which

p(z = 1| z
p(zik = 1| z

   

m    i,k
    ik ) =     i,k
n

) =

   
    at the end, draw new dishes from                     with considering

if sample zik=0, and mk=0: delete the row
  
)
n

pois(

   p(x | z)

  approximated by truncation, computing probabilities for a range of values of new dishes up to a 

upper bound  

more talk on applications

    applications 

    as prior distribution in models with infinite number of features. 
    modeling  protein interactions
    models of bipartite graph consisting of one side with undefined 

number of elements. 

    binary id105 for modeling dyadic data
    binary id105 for modeling dyadic data
    extracting features from similarity judgments
    latent features in link prediction
    independent components analysis and sparse factor analysis

    more on id136

    stick-breaking representation (yee whye teh et al., 2007) 
    variational id136 (finale doshi-velez et al., 2009)
    accelerated id136 (finale doshi-velez et al., 2009)

references

    griffiths, t. l., & ghahramani, z. (2011). the indian buffet process: an 

introduction and review. journal of machine learning research, 12, 1185-1224.

    slides: tom griffiths,    the indian buffet process   .

