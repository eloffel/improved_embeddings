cs5740: natural language processing

spring 2018

language models

instructor: yoav artzi

slides adapted from dan klein, dan jurafsky, chris manning, 

michael collins, luke zettlemoyer, and yejin choi

overview

    the id38 problem
    id165 language models
    evaluation: perplexity
    smoothing

    add-n
    linear interpolation

the id38 problem
    setup: assume a (finite) vocabulary of words

    we can construct an (infinite) set of strings
v    = {the, a, the a, the fan, the man, the man with the telescope, ...}
    data: given a training set of example sentences             
    problem: estimate a id203 distribution
p(the) = 10 12
p(a) = 10 13
p(the fan) = 10 12
p(the fan saw beckham) = 2     10 8
p(the fan saw saw) = 10 15
. . .

and p(x)   0 for all x     v   
    question: why would we ever want to do this?

xx v   

p(x) = 1

speech 
recognition

    automatic speech 
recognition (asr)
    audio in, text out
    sota: 0.3% error for digit 

strings, 5% dictation, 50%+ 
tv

   

   

   wreck a nice beach?   
       recognize speech   
   eye eight uh jerry?   
       i ate a cherry   

the id87
    goal: predict sentence given acoustics

p (x | a)
    the noisy channel approach:

w    = arg max

x

acoustic model: 
distributions over 

acoustic waves given 

a sentence 

w    = arg max

x

= arg max

x

= arg max

x

p (x | a)
p (a | x)p (x)/p (a)
p (a | x)p (x)

language model: 
distributions over 
sequences of words 

(sentences)

acoustically scored hypotheses

the station signs are in deep in english
the stations signs are in deep in english
the station signs are in deep into english
the station 's signs are in deep in english
the station signs are in deep in the english
the station signs are indeed in english
the station 's signs are indeed in english
the station signs are indians in english
the station signs are indian in english
the stations signs are indians in english
the stations signs are indians and english

-14732
-14735
-14739
-14740
-14741
-14757
-14760
-14790
-14799
-14807
-14815

language model

asr system components
!(#)
#

!(%|#)

acoustic model
channel

decoder

#

%

observed     

best

source

%

arg max

x

p (x | a) = arg max

x

p (a | x)p (x)

translation as codebreaking
   also knowing nothing official about, but having 
guessed and inferred considerable about, the 
powerful new mechanized methods in 
cryptography   methods which i believe succeed 
even when one does not know what language has 
been coded   one naturally wonders if the 
problem of translation could conceivably be 
treated as a problem in cryptography. when i 
look at an article in russian, i say:    this is really 
written in english, but it has been coded in some 
strange symbols. i will now proceed to decode.        

warren weaver 
(1955:18, quoting a letter he wrote in 1947)

language model

mt system components
!(#)
#

translation model
channel

!(%|#)

decoder

%

#

best

source

observed     

%

arg max

e

p (e | f ) = arg max

e

p (f | e)p (e)

id134 system 

components

language model

source

!(#)
#

best

#

%

image model
channel

!(%|#)

%

decoder

observed     

arg max

e

p (e | i) = arg max

e

p (i | e)p (e)

learning language models
    goal: assign useful probabilities !(#) to sentences #
    input: many observations of training sentences #
    output: system capable of computing !(#)
    !(i saw a van)>>!(eyes awe of an)
    not only grammaticality: !(artichokes intimidate zippers)   0

    probabilities should broadly indicate plausibility of sentences

    in principle,    plausible    depends on the domain, context, speaker   

    one option: empirical distribution over training sentences   

p(x1 . . . xn) =

c(x1 . . . xn)

n

for sentence x = x1 . . . xn

    problem: does not generalize (at all)
    need to assign non-zero id203 to previously unseen sentences!

decompose id203
    assumption: word choice depends on 

previous words only

p(x) =

<latexit sha1_base64="scez3fmsalddfwljrqokazathry=">aaacuhicbvdlbhmxfl0t+mlki4ulg6trpvrqozegfravqrjh2uqersomi4/haa36jdsdjaz5el6gbvmz4lnyuscnuptyjmth59yhfsojupnz9jvppfpzxvvfejxupnn67hl368vnpxtl2zbqoe2oio4jrtjqcy/yyfhgzcxywxx5yeaffwxwca0++alhhstnik84jt5kzfen6y920shkjdv1gfghbr8ozppxjue55dw6kvfexmvv9iinfb+3u2w3lw2yodbdghekbwuclfvjwhxbg8mup4i4n8az8uug1nmqwjvmjwog0etyzsarkikzk8l8dy3aiuqnjtrgozyaq3c7aphotwuvkyxxf27zm4n/9wo3g7i03u/efyer03im6o3yssoq12gwh6q5zdslassewh7fj+gfsyt6ghka5pyp9o1qkymqq07bms5cyk1epdy2auwol+f0kaxfd94psto3vapjryqb8aq2oq8yduaipsijdihcd/gb1/az+zx8sf52ktvsfze8hhvopdfbj7e1</latexit>
<latexit sha1_base64="scez3fmsalddfwljrqokazathry=">aaacuhicbvdlbhmxfl0t+mlki4ulg6trpvrqozegfravqrjh2uqersomi4/haa36jdsdjaz5el6gbvmz4lnyuscnuptyjmth59yhfsojupnz9jvppfpzxvvfejxupnn67hl368vnpxtl2zbqoe2oio4jrtjqcy/yyfhgzcxywxx5yeaffwxwca0++alhhstnik84jt5kzfen6y920shkjdv1gfghbr8ozppxjue55dw6kvfexmvv9iinfb+3u2w3lw2yodbdghekbwuclfvjwhxbg8mup4i4n8az8uug1nmqwjvmjwog0etyzsarkikzk8l8dy3aiuqnjtrgozyaq3c7aphotwuvkyxxf27zm4n/9wo3g7i03u/efyer03im6o3yssoq12gwh6q5zdslassewh7fj+gfsyt6ghka5pyp9o1qkymqq07bms5cyk1epdy2auwol+f0kaxfd94psto3vapjryqb8aq2oq8yduaipsijdihcd/gb1/az+zx8sf52ktvsfze8hhvopdfbj7e1</latexit>
<latexit sha1_base64="scez3fmsalddfwljrqokazathry=">aaacuhicbvdlbhmxfl0t+mlki4ulg6trpvrqozegfravqrjh2uqersomi4/haa36jdsdjaz5el6gbvmz4lnyuscnuptyjmth59yhfsojupnz9jvppfpzxvvfejxupnn67hl368vnpxtl2zbqoe2oio4jrtjqcy/yyfhgzcxywxx5yeaffwxwca0++alhhstnik84jt5kzfen6y920shkjdv1gfghbr8ozppxjue55dw6kvfexmvv9iinfb+3u2w3lw2yodbdghekbwuclfvjwhxbg8mup4i4n8az8uug1nmqwjvmjwog0etyzsarkikzk8l8dy3aiuqnjtrgozyaq3c7aphotwuvkyxxf27zm4n/9wo3g7i03u/efyer03im6o3yssoq12gwh6q5zdslassewh7fj+gfsyt6ghka5pyp9o1qkymqq07bms5cyk1epdy2auwol+f0kaxfd94psto3vapjryqb8aq2oq8yduaipsijdihcd/gb1/az+zx8sf52ktvsfze8hhvopdfbj7e1</latexit>

nyi=1

p(xi | x1, . . . , xi 1)

    better?

event

    not really: last word still represents complete 

markov assumption

p(english | this is really written in)    
p(english | is really written in)    
p(english | really written in)    
p(english | written in)    
p(english | in)    
p(english)

<latexit sha1_base64="b396orl7mlvbzp3hq35wfdcbluq=">aaadzxicpvjnb9qwehu2ueqafgdiwoerk1b7wsuiqevwwqgorwjppxvuoc7srlxbiewj7srkx+lxcoeaz34g3o8dbbfcdgrlt29m3lhvxnraeurtr1evvnfz49bm7eto3xtb2zu79z/6qnesh7lsltstheetla5jkcbt2qewhcat4vznph/ycz1xlf1asxpziyzwjzuufkiz3ejtc6j3ooeltwgnyek0a25ucuuopspdoeft6xlcoewefptt9oglunbvjqbwnvxd5vok12z/74f/q/1+smdztj8dpiuaqybbgt5bxxgwfooxlwwmwpjaed/k0pryvjhsumox8mzjles5moaoqcsm+rxdrlydz4epyvy5cc3bgv21oxxg+5kpqqurnpxrutn5x1zp54jr02l8macr6izykjfdx40gqmd+p6budiwf7zrkygc0kicnwglj4eclcxdo8ujwxghbtlx2oyxvw+4m9lous4jz2bppv8hwxedvih3/sn/0emxhjnvmnri9lreddsteswm2zdl6hh2jvkxfez/irfhh/ghz2otwpq/ybxe/+qnr+bsp</latexit>
<latexit sha1_base64="b396orl7mlvbzp3hq35wfdcbluq=">aaadzxicpvjnb9qwehu2ueqafgdiwoerk1b7wsuiqevwwqgorwjppxvuoc7srlxbiewj7srkx+lxcoeaz34g3o8dbbfcdgrlt29m3lhvxnraeurtr1evvnfz49bm7eto3xtb2zu79z/6qnesh7lsltstheetla5jkcbt2qewhcat4vznph/ycz1xlf1asxpziyzwjzuufkiz3ejtc6j3ooeltwgnyek0a25ucuuopspdoeft6xlcoewefptt9oglunbvjqbwnvxd5vok12z/74f/q/1+smdztj8dpiuaqybbgt5bxxgwfooxlwwmwpjaed/k0pryvjhsumox8mzjles5moaoqcsm+rxdrlydz4epyvy5cc3bgv21oxxg+5kpqqurnpxrutn5x1zp54jr02l8macr6izykjfdx40gqmd+p6budiwf7zrkygc0kicnwglj4eclcxdo8ujwxghbtlx2oyxvw+4m9lous4jz2bppv8hwxedvih3/sn/0emxhjnvmnri9lreddsteswm2zdl6hh2jvkxfez/irfhh/ghz2otwpq/ybxe/+qnr+bsp</latexit>
<latexit sha1_base64="b396orl7mlvbzp3hq35wfdcbluq=">aaadzxicpvjnb9qwehu2ueqafgdiwoerk1b7wsuiqevwwqgorwjppxvuoc7srlxbiewj7srkx+lxcoeaz34g3o8dbbfcdgrlt29m3lhvxnraeurtr1evvnfz49bm7eto3xtb2zu79z/6qnesh7lsltstheetla5jkcbt2qewhcat4vznph/ycz1xlf1asxpziyzwjzuufkiz3ejtc6j3ooeltwgnyek0a25ucuuopspdoeft6xlcoewefptt9oglunbvjqbwnvxd5vok12z/74f/q/1+smdztj8dpiuaqybbgt5bxxgwfooxlwwmwpjaed/k0pryvjhsumox8mzjles5moaoqcsm+rxdrlydz4epyvy5cc3bgv21oxxg+5kpqqurnpxrutn5x1zp54jr02l8macr6izykjfdx40gqmd+p6budiwf7zrkygc0kicnwglj4eclcxdo8ujwxghbtlx2oyxvw+4m9lous4jz2bppv8hwxedvih3/sn/0emxhjnvmnri9lreddsteswm2zdl6hh2jvkxfez/irfhh/ghz2otwpq/ybxe/+qnr+bsp</latexit>

unigram models

    simplest solution: unigrams

p(x1 . . . xn) =

p(xi)

nyi=1

    generative process: pick a word, pick a word,     until you pick stop
    as a graphical model:

x1

x2

            .

xn-1

stop

    examples:

   
   
   
   
   

[fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass.]
[thrift, did, eighty, said, hard, 'm, july, bullish]
[that, or, limited, the]
[]
[after, any, on, consistently, hospital, lake, of, of, other, and, factors, raised, analyst, too, allowed, 
mexico, never, consider, fall, bungled, davison, that, obtain, price, lines, the, to, sass, the, the, 
further, board, a, details, machinists, the, companies, which, rivals, an, because, longer, oakes, 
percent, a, they, three, edward, it, currier, an, within, in, three, wrote, is, you, s., longer, institute, 
dentistry, pay, however, said, possible, to, rooms, hiding, eggs, approximate, financial, canada, 
the, so, workers, advancers, half, between, nasdaq]

    big problem with unigrams:!(the the the the)>>!(i like ice cream)!

bigram models

    condition on previous single word:

    pick start, pick a word conditioned on previous one, repeat until to pick stop

p(x1 . . . xn) =

p(xi|xi 1)

nyi=1

start

x1

x2

xn-1

stop

    generative process: 
    graphical model:

    any better?

    [texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., 
gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, 
fifty, five, yen]

    [outside, new, car, parking, lot, of, the, agreement, reached]
    [although, common, shares, rose, forty, six, point, four, hundred, dollars, from, thirty, 

seconds, at, the, greatest, play, disingenuous, to, be, reset, annually, the, buy, out, of, 
american, brands, vying, for, mr., womack, currently, sharedata, incorporated, 
believe, chemical, prices, undoubtedly, will, be, as, much, is, scheduled, to, 
conscientious, teaching]

    [this, would, be, a, record, november]

    but, what is the cost?

higher order id165s?

please close the door
please close the first window on the left

198015222 the first
194623024 the same
168504105 the following
158562063 the world
   
14112454 the door
-----------------
23135851162 the *

197302 close the window 
191125 close the door 
152500 close the gap 
116451 close the thread 
87298 close the deal
-----------------
3785230 close the *

3380 please close the door
1601 please close the window
1164 please close the new
1159 please close the gate
   
0 please close the first
-----------------
13951 please close the *

approximating ________

id165 model decomposition
    !-gram models (!>1): condition on !   1 previous words

1.3. trigram language models

p(x1 . . . xn) =

q(xi|xi (k 1) . . . xi 1)

nyi=1

1.3. trigram language models

where xi 2 v [ {st op} and x k+2 . . . x0 =    

7

we would have
    example: tri-gram (3-gram)
p(the dog barks stop) = q(the|*, *)   q(dog|*, the)   q(barks|the, dog)   q(stop|dog, barks)
note that in this expression we have one term for each word in the sentence (the,
dog, barks, and stop). each word depends only on the previous two words: this
    learning: estimate the distributions  
is the trigram assumption.

p(the dog barks stop) = q(the|*, *)   q(dog|*, the)   q(barks|the, dog)   q(stop|dog, barks)
note that in this expression we have one term for each word in the sentence (the,
dog, barks, and stop). each word depends only on the previous two words: this
is the trigram assumption.

the parameters satisfy the constraints that for any trigram u, v, w,

the parameters satisfy the constraints that for any trigram u, v, w,

q(w|u, v)   0

1 overall goal
1 overall goal

    due nov 30 11:59 pm, submission to blackboard    

(1)

p(xi)

p(x) =
p(x) =

p(x1 . . . xn) =

    simplest case: unigrams

proof for unigrams 

well defined distributions

    generative process: pick a word, pick a word,     until you pick stop
   

in this homework you will implement hidden markov model (id48) part-of-
speech tagger. brie   y, the input to the system will be a sentence (a sequence
of words and punctuation tokens), and the output will be the part-of-speech
tags of that sentence. you may work in groups of 2 students. students in
the same group get the same grade for this assignment.

in this homework you will implement hidden markov model (id48) part-of-
1 overall goal
speech tagger. brie   y, the input to the system will be a sentence (a sequence
in this homework you will implement hidden markov model (id48) part-of-
of words and punctuation tokens), and the output will be the part-of-speech
speech tagger. brie   y, the input to the system will be a sentence (a sequence
tags of that sentence. you may work in groups of 2 students. students in
of words and punctuation tokens), and the output will be the part-of-speech
the same group get the same grade for this assignment.
tags of that sentence. you may work in groups of 2 students. students in
the same group get the same grade for this assignment.
p(x1...xn)
p(x1...xn)

for all strings ! (of any length): "(!)   0
nyi=1
    claim:  the sum over string of all lengths is 1 :    ("(!)=1
1xn=1 xx1...xn
xx
1xn=1 xx1...xn
xx
1xn=1 xx1...xn
xx
1xn=1 xx1...xn
nyi=1
xx
p(x1...xn) = xx1...xn
xx1...xn
p(xi) =xx1
...xxn
nyi=1
p(xi) =xx1
xx1...xn
p(x1...xn) = xx1...xn
...xxn
nyi=1
xx1...xn
...xxn
p(xi) =xx1
p(x1...xn) = xx1...xn
p(x1)     ...     p(xn)
=xx1
p(x1)     ...    xxn
p(xn) = (1   ps)n 1ps where ps = p(stop)
=xx1
p(x1)     ...    xxn
p(xn) = (1   ps)n 1ps where ps = p(stop)
=xx1
p(x1)     ...    xxn
p(xn) = (1   ps)n 1ps where ps = p(stop)
1xn=1
1xn=1
xx
(1   ps)n 1ps = ps
1   (1   ps)
1xn=1
1xn=1
xx
(1   ps)n 1ps = ps
1xn=1
1xn=1
xx
1
1   (1   ps)
(1   ps)n 1ps = ps
(1   ps)n 1 = ps
= 1
2 programming portion

(1   ps)n 1 = ps
(1   ps)n 1 = ps

p(x1)     ...     p(xn)
p(x1)     ...     p(xn)

1   (1   ps)

p(x) =
p(x) =

p(x1...xn)

p(x1 . . . xn)

p(x) =

p(x) =

p(x) =

= 1

= 1

(2)

1

1

(1)+(2)

id165 model parameters

   

the parameters of an id165 model:
    maximum likelihood estimate: relative frequency

qm l(w) =

c(w)
c()

, qm l(w|v) =

c(v, w)

c(v)

, qm l(w|u, v) =

    general approach

where c is the empirical counts on a training set

    take a training set ! and a test set !   
    compute an estimate of the #   s from !
    use it to assign probabilities to other sentences, such as those in !   

c(u, v, w)

c(u, v)

,

. . .

 

s
t
n
u
o
c
g
n
n
a
r
t

i

i

198015222 the first
194623024 the same
168504105 the following
158562063 the world
   
14112454 the door
-----------------
23135851162 the *

q(door|the) =

14112454

2313581162

= 0.0006

regular languages?

    id165 models are (weighted) regular 

languages
    linguists argue that language isn   t regular.

    long-distance effects:    the computer which i had 
just put into the machine room on the fifth floor 
___.   
    recursive structure

    why can we often get away with id165 

models?

measuring model quality
    the goal isn   t to pound out fake sentences!

    obviously, generated sentences get    better    as we 
increase the model order
    more precisely: using ml estimators, higher order 
always gives better likelihood on train, but not test

    what we really want to know is:

    will our model prefer good sentences to bad ones?
    bad     ungrammatical!
    bad    unlikely
    bad = sentences that our acoustic model really likes 

but aren   t the correct answer

measuring model quality

    the shannon game:

    how well can we predict the next word?
when i eat pizza, i wipe off the ____
many children are allergic to ____
i saw a ____
    unigrams are terrible at this game.  (why?)

grease 0.5
sauce 0.4
dust 0.05
   .
mice 0.0001
   .
the     1e-100

claude shannon

    a better model of a text   

    is one which assigns a higher id203 to the word that 

actually occurs

measuring model quality

    for every sentences !(#)(%=1   )) we 
can estimate its id203 *(!(#))

    a natural measure of model quality:

p(x (i))

myi=1

    the higher this quantity is, the better we 

model unseen sentences

perplexity

    let m be the number of words in the corpus
    the average log id203 is:
mxi=1

myi=1
    the perplexity is:

log2 p(x (i))

p(x (i)) =

1
m

1
m

log2

    where:

p p = 2 l

l =

1
m

mxi=1

log2 p(x (i))

perplexity

1
m

mxi=1

l =

p p = 2 l
    lower is better!
    perplexity is the inverse id203 of the 
test set normalized by the number of words

log2 p(x (i))

    if we ever give a test id165 zero id203 

   perplexity will be infinity

    how can we avoid this?

perplexity

p p = 2 l

l =

1
m

mxi=1

log2 p(x (i))

    under a uniform distribution the perplexity will be the 

    let   s suppose ! sentences consisting of random digits

vocabulary size:
    what is the perplexity of this data according to a model 

that assign p=1/10 to each digit?

m pm
m pm

p p =2  1
=2  1
=2  log2

1

i=1 log2( 1

10 )|x(i)|

1
10

i=1 |x (i)| log2
10 = 2  log2 10 1

= 10

lower perplexity = better model
    training 38 million words, test 1.5 million words, 20k 

170

unigram bigram
962

word types wsj
id165 order
perplexity
"an estimate of an upper bound for the id178 of 
english". brown, peter f.; et al. (march 1992). 
computational linguistics18 (1)
important note:
    it   s easy to get bogus perplexities by having bogus probabilities 

trigram*
109

that sum to more than one over their event spaces.  

   

   

*with good-turing smoothing

shannon game

measuring model quality: speech
    word error rate (wer)

insertions + deletions + substitutions

true sentence size

correct answer:

andy saw a part of the movie

recognizer output: and he saw apart of the movie

    the    right    measure:

    task error driven
    for id103
    for a specific recognizer!

wer: 4/7 
= 57%

    common issue: intrinsic measures like perplexity are easier to use, 

but extrinsic ones are more credible

sparsity in language models

 

n
e
e
s
n
o
i
t
c
a
r
f

1

0.8

0.6

0.4

0.2

0

0

200000

    problems with id165 models:
    new words appear all the time:

    synaptitute
    132,701.03
    multidisciplinarization
    post-truth

    new id165s: even more often

    zipf   s law

    types (words) vs. tokens (word occurrences)
    broadly: most word types are rare ones
    specifically: 

    rank word types by token frequency
    frequency inversely proportional to rank
this property (try it!)

    this is particularly problematic when   

    not special to language: randomly generated character strings have 

    training set is small (does this happen for id38?)
    transferring domains: e.g., newswire, scientific literature, twitter

unigrams
bigrams
rules

600000
400000
number of words

800000

1000000

zeroes

    test set:

    denied the offer
    denied the loan

    training set:

    denied the allegations
    denied the reports
    denied the claims
    denied the request

p(   offer    | denied the) = 0

    bigrams with zero id203

    mean that we will assign 0 id203 to the test set!
    and hence we cannot compute perplexity (can   t 

divide by 0)!

parameter estimation

    the parameters of an id165 model:

    maximum likelihood estimate: relative frequency

qm l(w) =

c(w)
c()

, qm l(w|v) =

c(v, w)

c(v)

, qm l(w|u, v) =

c(u, v, w)

c(u, v)

,

. . .

where c is the empirical counts on a training set

    maximum likelihood estimates won   t get us very far
    need to smooth these estimates
    general method (procedurally)

    take your empirical counts
    modify them in various ways to improve estimates

    general method (mathematically)

    often can give estimators a formal statistical interpretation     but not 
    approaches that are mathematically obvious are not always what works

always

smoothing

    we often want to make estimates from sparse statistics:

p(w | denied the)
3 allegations
2 reports
1 claims
1 request
7 total

s
n
o
i
t
a
g
e

l
l

a

s
t
r
o
p
e
r

s
e
g
r
a
h
c

n
o

i
t

o
m

s
t
i
f

e
n
e
b

   

t
s
e
u
q
e
r

s
m
a
c

l

i

    smoothing flattens spiky distributions so they generalize better

p(w | denied the)
2.5 allegations
1.5 reports
0.5 claims
0.5 request
2 other
7 total

i
t

i
t

s
n
s
o
n
o
a
g
a
e
g
e
a
a

l
l

l
l

s
t
r
o
p
e
r

s
e
g
r
a
h
c

n
o

i
t

o
m

s
t
i
f

e
n
e
b

   

t
s
e
u
q
e
r

s
m
a
c

l

i

    very important all over nlp (and ml more generally), but easy to do badly!

add-one estimation

    also called laplace smoothing
    pretend we saw each word one more time than we 
    just add one to all the counts!

did

    id113 estimate:

    add-1 estimate:

pid113(xi | xi 1) =

c(xi 1, xi)
c(xi 1)

padd-1(xi | xi 1) =

c(xi 1, xi) + 1
c(xi 1) + v

more general formulation

    add-k: 

padd-k(xi | xi 1) =

c(xi 1, xi) + k
c(xi 1) + kv

padd-k(xi | xi 1) =

c(xi 1, xi) + m 1
c(xi 1) + m

v

    unigram prior smoothing:

padd-k(xi | xi 1) =

c(xi 1, xi) + mp (xi)

c(xi 1) + m

berkeley restaurant corpus
    can you tell me about any good cantonese

restaurants close by

    mid priced thai food is what i   m looking for
    tell me about chez panisse
    can you give me a listing of the kinds of food 

that are available

    i   m looking for a good place to eat breakfast
    when is caffe venezia open during the day

raw bigram counts

    from 9222 sentences

bigram probabilities

    normalize by unigrams:

    result:

add-1 on the berkeley restaurant 

corpus

add-1 smoothed bigrams

padd-1(xi | xi 1) =

c(xi 1, xi) + 1
c(xi 1) + v

reconstituted counts

padd-1(xi | xi 1) =

c(xi 1, xi) + 1
c(xi 1) + v

c   (xi 1, xi) =

(c(xi 1, xi) + 1)c(xi 1)

c(xi 1) + v

original vs. add-1 (reconstituted ) bigram counts

add-1 is a blunt instrument

    so add-1 isn   t used for id165s: 

    we   ll see better

    but add-1 is used to smooth other nlp 

models
    for text classification 
    in domains where the number of zeroes isn   t 

so big

add-1 smoothing

    classic solution: add counts (laplace smoothing)

c(w) +  
c() +  v

    add-one smoothing especially often talked about
for a bigram distribution, can add counts shaped like the unigram:

   

padd- (w) =

c(w) +  

=

pw0(c(w0) +  )
pw0 (c(v, w0) +  qm l(w0))

c(v, w) +  qm l(w)

puni- (w|v) =
    can consider hierarchical formulations: trigram is recursively centered on 

c(v) +  

smoothed bigram estimate, etc. [mackay and peto, 94]

=

c(v, w) +  pid113(w)

    bayesian: can be derived from dirichlet / multinomial conjugacy - prior 

shape shows up as pseudo-counts

    problem: works quite poorly!

linear interpolation

    problem: id113 is supported by few counts
    classic solution: mixtures of related, denser histories:
p (w|u, v) =  3pid113(w|u, v) +  2pid113(w|v) +  1pid113(w)
is this a well defined distribution?

    yes, if all !"   0 and they sum to 1

   

    the mixture approach tends to work better than add-  

approach
    can flexibly include multiple back-off contexts
    good ways of learning the mixture weights with em (later)
    but: not entirely clear why it works so much better

    all the details you could ever want: [chen and goodman, 98]

estimating lambdas

held-out test 

data

validation

data

development 

training data

    use a validation corpus

    choose !s to maximize the id203 of 
    then search for !s that give largest id203 to 
log p (x1, . . . , xn |  1, . . . ,  k) =xi

validation data:
    fix the id165 probabilities (on the training data)

validation set:

log p (xi | xi 1)

advanced smoothing algorithms
    intuition: use the count of things we   ve seen

once
    to help estimate the count of things we   ve never 

    used by many smoothing algorithms

seen

    good-turing
    kneser-ney
    also: witten-bell

invented during wwii by alan turing and 
later published by good. frequency 
estimates were needed for enigma code-
breaking effort

what actually works?

    trigrams and beyond:

    unigrams, bigrams generally useless
    trigrams much better (when there   s enough data)
    4-, 5-grams really useful in mt, but not so much for speech

    discounting
bell, etc   

    absolute discounting, good-turing, held-out estimation, witten-

    see [chen+goodman] reading for tons of graphs   

data vs. method?

    having more data is better   

y
p
o
r
t
n
e

10
9.5
9
8.5
8
7.5
7
6.5
6
5.5

100,000 katz
100,000 kn
1,000,000 katz
1,000,000 kn
10,000,000 katz
10,000,000 kn
all katz
all kn

    another issue: !>3 has huge costs in speech 

        but so is using a better estimator

id165 order

9 10 20

1

2

3

4

5

6

7

8

recognizers

practical issues

    we do everything in log space

    avoid underflow
    (also adding is faster than multiplying)
    (though log can be slower than multiplication)

log(p1    p2    p3    p4) = log p1 +log p2 +log p3 +log p4

web-scale id165s

   

google id165s

    serve as the incoming 92
    serve as the incubator 99
    serve as the independent 794
    serve as the index 223
    serve as the indication 72
    serve as the indicator 120
    serve as the indicators 45
    serve as the indispensable 111
    serve as the indispensible 40
    serve as the individual 234

http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-you.html

web-scale id165s

    how to deal with, e.g., google id165 corpus
    pruning

    only store id165s with count > threshold.

    remove singletons of higher-order id165s
    id178-based pruning (more advanced)

    efficiency

    efficient data structures like tries
    bloom filters: approximate language models
    store words as indexes, not strings

    use huffman coding to fit large numbers of words into two 
    quantize probabilities (4-8 bits instead of 8-byte float)

bytes

even more data!

tons of data closes gap, for extrinsic mt evaluation

http://www.aclweb.org/anthology/d07-1090.pdf

handling unknown words
if we know all the words in advanced
    vocabulary v is fixed
    closed vocabulary task

    often we don   t know this

    out of vocabulary = oov words
    open vocabulary task
instead: create an unknown word token <unk>
    training of <unk> probabilities

   

   

    create a fixed lexicon l of size v (e.g., rare words are not in l)
    at text id172 phase, any training word not in l changed to  
    now we train its probabilities like a normal word

<unk>

    at decoding time

    if text input: use unk probabilities for any word not in training

beyond id165 lms

    lots of ideas we won   t have time to discuss:

    caching models: recent words more likely to appear again
    trigger models: recent words trigger other words
    topic models

    more advanced ideas

    syntactic models: use tree models to capture long-distance 

syntactic effects [chelba and jelinek, 98]

    discriminative models: set id165 weights to improve final task 
accuracy rather than fit training set density [roark, 05, for asr;  
liang et. al., 06, for mt]

    structural zeroes: some id165s are syntactically forbidden, 

keep estimates at zero [mohri and roark, 06]

    bayesian document and ir models [daume 06]

case study: id46
    how can we tell what language a document is in?
the 38th parliament will meet on 
monday, october 4, 2004, at 11:00 a.m. 
the first item of business will be the 
election of the speaker of the house of 
commons. her excellency the governor 
general will open the first session of 
the 38th parliament on october 5, 2004, 
with a speech from the throne. 
    how to tell the french from the english?

la 38e l  gislature se r  unira    11 heures le 
lundi 4 octobre 2004, et la premi  re affaire 
   l'ordre du jour sera l(cid:1)  lection du 
pr  sident de la chambre des communes. 
son excellence la gouverneure g  n  rale 
ouvrira la premi  re session de la 38e 
l  gislature avec un discours du tr  ne le 
mardi 5 octobre 2004. 

    treat it as word-level text categorization?
    overkill, and requires a lot of training data

    you don   t actually need to know about words!

    option: build a character-level language model

                                                                 
patto di stabilit   e di crescita

na  ve-bayes models

    generative model: pick a topic, then generate a document 
    na  ve-bayes assumption: 

using a language model for that topic

    all words are independent given the topic.

|x|yi=1

q(xi | y)

p(y, x) = q(y)

y

x1

x2

. . .

xn

    compare to a unigram language model:

p(x1 . . . xn) =

p(xi)

nyi=1

one option: class-conditional lms
    can add a topic variable to richer language models

p(y, x) = q(y)

y

|x|yi=1

q(xi | y, xi 1)

start

x1

x2

. . .

xn

    could be characters instead of words, used for language id
    could sum out the topic variable and use as a language 
    how might a class-conditional id165 language model 

model
behave differently from a standard id165 model?

extras

notation: nc = frequency of 

frequency c

    nc = the count of things we   ve seen c times
    sam i am i am sam i do not eat
i   3
sam 2
am  2
do  1
not 1
eat 1

n1 = 3
n2 = 2
n3 = 1

good-turing smoothing intuition
    you are fishing (a scenario from josh goodman), and 
caught:
    10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish

    how likely is it that next species is trout?

    1/18

    how likely is it that next species is new (i.e. catfish or bass)
    let   s use our estimate of things-we-saw-once to estimate the 
    3/18 (because n1=3)

    assuming so, how likely is it that next species is trout?

new things.

    must be less than 1/18
    how to estimate? 

good-turing calculations
* (things with zero frequency) =
pgt

c* =

(c +1)nc+1

n1
n

nc

unseen (bass or catfish)

   c = 0:
   id113 p = 0/18 = 0
   p*

gt (unseen) = n1/n = 3/18

seen once (trout)

   c = 1
   id113 p = 1/18
   c*(trout) = 2 * n2/n1 = 2 * 1/3 = 2/3 
   p*

gt(trout) = 2/3 / 18 = 1/27

good-turing complications

    problem: what about 
   the   ?  (say c=4417)
    for small k, nk > nk+1
    for large k, too jumpy, 
zeroes wreck estimates

n1

n2 n3

    simple good-turing 
[gale and sampson]: 
replace empirical nk with 
a best-fit power law once 
counts get unreliable

n1

n2

good-turing numbers

    numbers from church 
    22 million words of ap 

and gale (1991)
newswire
c* =

(c +1)nc+1

nc

    it sure looks like 

c* = (c - .75)

count 
c
0
1
2
3
4
5
6
7
8
9

good turing c*

.0000270
0.446
1.26
2.24
3.24
4.22
5.19
6.21
7.24
8.25

absolute discounting

   

idea: observed id165s occur more in training than they will later:

future c* (next 22m)
0.448
1.25
2.24
3.23
    absolute discounting (bigram case)

count in 22m words
1
2
3
4

    no need to actually have held-out data; just subtract 0.75 (or some d)

c   (v, w) = c(v, w)   0.75 and q(w|v) =

    but, then we have    extra    id203 mass

c   (v, w)

c(v)

    question: how to distribute    between the unseen words?

   (v) = 1  xw

c   (v, w)

c(v)

katz backoff

c(v)

c   (v, w)

   (v) = 1  xw

   absolute discounting, with backoff to unigram estimates
c   (v, w) = c(v, w)    
   define seen and unseen bigrams:
a(v) = {w : c(v, w) > 0} b(v) = {w : c(v, w) = 0}
   now, backoff to maximum likelihood unigram estimates for unseen 
qbo(w|v) =( c   (v,w)
if w 2 a(v)
if w 2 b(v)

   (v)    

backed off to katz bigram estimate, etc

   can consider hierarchical formulations: trigram is recursively 
   can also have multiple count thresholds (instead of just 0 and >0)
   problem? 

pw02b(v) qm l(w0)

bigrams

qm l(w)

c(v)

   unigram estimates are bad predictors

kneser-ney smoothing
    better estimate for probabilities of lower-order 

francisco
glasses

reading___________?

unigrams!
    shannon game:  i can   t see without my 
       francisco    is more common than    glasses   
        but    francisco    always follows    san   
instead of  p(w):    how likely is w   
continuation?
    for each word, count the number of bigram types it 
completes
    every bigram type was a novel continuation the first time it 
was seen

   
    pcontinuation(w):     how likely is w to appear as a novel 

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

kneser-ney smoothing
    how many times does w appear as a 

novel continuation:

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

    normalized by the total number of word 

bigram types

{(wj   1,wj):c(wj   1,wj) > 0}

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}

{(wj   1,wj):c(wj   1,wj) > 0}

kneser-ney smoothing

context (san) will have a low continuation id203

    a frequent word (francisco) occurring in only one 
    replace unigram in discounting:
pkn(wi | wi   1) =

+  (wi   1)pcontinuation(wi)

max(c(wi   1,wi)    d,0)

c(wi   1)

   is a normalizing constant; the id203 mass we   ve discounted

  (wi   1) =

d
c(wi   1) {w :c(wi   1,w) > 0}

the normalized discount

71

the number of word types that can follow wi-1
= # of word types we discounted
= # of times we applied normalized discount

kneser-ney smoothing: recursive 

formulation

pkn(wi | wi   n+1
i   1

) =

i

)     d,0)
)

max(ckn(wi   n+1
ckn(wi   n+1
i   1
count(   )   for the highest order

+  (wi   n+1
i   1

continuationcount(   )    for lower order

)pkn(wi | wi   n+2
i   1

)

ckn(   ) =

!
#
"
$#

continuation count = number of unique single 
word contexts for   

smoothing at web-scale

       stupid backoff    (brants et al. 2007)
    no discounting, just use relative frequencies 

s(wi | wi   k+1

i   1 ) =

"
$$
#
$
$
%

i

)

count(wi   k+1
count(wi   k+1
0.4s(wi | wi   k+2
i   1

i   1 )   if  count(wi   k+1
)      otherwise

i

) > 0

s(wi) =

count(wi)

n

