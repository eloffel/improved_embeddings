cs5740: natural language processing

introduction

instructor: yoav artzi

ta: alane suhr

slides adapted from dan klein, luke zettlemoyer, and yejin choi

technicalities

    people:

    instructor: yoav artzi
    ta: alane suhr

    office hours: tbd

    office hours: tbd

    webpage (everything is there):

    http://www.cs.cornell.edu/courses/cs5740/2018s

p/

    discussion group on piazza
    assignments on cms

    repositories on github classroom

technicalities

    grading:

    40% assignments, 25% take-home exam, and 

30% class review quizzes, 5% participation

    participation = class + piazza 
    enrollment and prerequisites:

    at least b in cs 5785 (applied ml) or equivalent 
cornell course, and strong proven programming 
experience

    or: instructor permission
    audit? talk to me after class

technicalities

    collaboration:

    all assignments are in pairs (if you can   t find a 

partner, talk to me     don   t just go solo)

    use of external code/tools     specified in each 

assignment
    if have doubt     ask!

    late submissions:

    none
    only top-4 assignments count for the grade
    no late submission for final exam

    all assignments should be implemented in python

technicalities

    books (recommended, not required):

    d. jurafsky & james h. martin, speech and 

language processing

    y. goldberg, neural network methods in 

natural language processing (online within 
cornell)

    other material on the course website

technicalities

    come on time

    late? enter quietly and sit at the back
    quiz starts on time

    no laptops or phones in class

    except during the quiz

technicalities

    quizzes:

quizzes count

    first five minutes of every class, no extensions
    each quiz: 1.5% of the grade, up to 30%, only top 20 
    it is not possible to re-take a missed quiz
    a missed quiz gets zero
    just like an exam: no copying, chatting, and not taking the 

quiz remotely    all ai violations

    quiz practice

    phones and laptops
    http://socrative.com
    use netid to identify 
    today   s room: nlp18

tips

    work together with your partner, don   t 

simply divide the work
    discuss with each other

    beyond your group
    this is what piazza is for!

why are you here?

what is this class?
    depth-first technical nlp course
    learn the language of natural language 

processing

    what this class is not?

    it is not a tutorial to nltk, tensorflow, etc.
    stack overflow already does this well

class goals

    learn about the issues and techniques of 

modern nlp

    be able to read current research papers
    build realistic nlp tools
    understand the limitation of current 

techniques

main themes

    linguistic issues

    what are the range of language phenomena?
    what are the knowledge sources that let us make decisions?
    what representations are appropriate?

    statistical modeling methods

    increasingly complex model structures
    learning and parameter estimation
    efficient id136: id145, search, sampling

    engineering methods

    issues of scale
    where the theory breaks down (and what to do about it)
works in practice     

    we   ll focus on what makes the problems hard, and what 

main models

    generative models
    discriminative models

    neural networks
    id114

what is nlp?

    fundamental goal: deep understanding of broad language

    not just string processing or keyword matching!

    end systems that we want to build:

    simple:
    complex:

what is nlp?

    fundamental goal: deep understanding of broad language

    not just string processing or keyword matching!

    end systems that we want to build:

    simple: id147, text categorization   
    complex: id103, machine translation, information 

extraction, dialog interfaces, id53   

    unknown: human-level comprehension (is this just nlp?)

today
    prominent applications

    try to imagine approaches
    what   s behind current limitations?

    some history
    key problems
    if time allows: text classification

machine translation

    translate text from one language to another
    recombines fragments of example translations
    challenges:

    what fragments? how to combine?  [learning to translate]
    how to do it efficiently?  [fast translation search]

machine translation

machine translation

machine translation

machine translation

machine translation

machine translation

information extraction

    unstructured text to database entries
new york times co. named russell t. lewis, 45, president and general manager of its 
flagship new york times newspaper, responsible for all business-side activities. he was 
executive vice president and deputy general manager. he succeeds lance r. primis, 
who in september was named president and chief operating officer of the parent. 

person
russell t. lewis

russell t. lewis

lance r. primis

company
new york times 
newspaper
new york times 
newspaper
new york times co.

post
president and general 
manager
executive vice 
president
president and ceo

state
start

end

start

    sota: good performance on simple templates (e.g., person-
    harder without defining template

role)

tagging: back to text

http://nytlabs.com/projects/editor.html

natural language instruction

    what makes 
this possible?
    limitations?

language comprehension

bang, bang, his silver hammer came down upon her head

speech systems

    automatic id103 (asr)

    audio in, text out
    sota: 16% per, google claims 8% wer

   speech lab   

    text to speech (tts)

    text in, audio out
    sota: mechanical and monotone

nlp history: pre-statistics
(1) colorless green ideas sleep furiously.
(2) furiously sleep ideas green colorless

it is fair to assume that neither sentence (1) nor (2) (nor 
indeed any part of these sentences) had ever occurred in 
an english discourse. hence, in any statistical model for 
grammaticalness, these sentences will be ruled out on 
identical grounds as equally "remote" from english. yet (1), 
though nonsensical, is grammatical, while (2) is not.    
(chomsky 1957)

nlp history: pre-statistics
(1) colorless green ideas sleep furiously.
(2) furiously sleep ideas green colorless

it is fair to assume that neither sentence (1) nor (2) (nor 
indeed any part of these sentences) had ever occurred in 
an english discourse. hence, in any statistical model for 
grammaticalness, these sentences will be ruled out on 
identical grounds as equally "remote" from english. yet (1), 
though nonsensical, is grammatical, while (2) is not.    
(chomsky 1957)

nlp history: pre-statistics

    70s and 80s: more linguistic focus

    emphasis on deeper models, syntax and 

semantics

    toy domains / manually engineered systems
    weak empirical evaluation

nlp history: ml and empiricism

   whenever i fire a linguist our system 
performance improves.       jelinek, 1988

    1990s: empirical revolution

    corpus-based methods produce the first widely used tools
    deep linguistic analysis often traded for robust approximations
    empirical evaluationis essential

nlp history: ml and empiricism

   whenever i fire a linguist our system 
performance improves.       jelinek, 1988

    1990s: empirical revolution

    corpus-based methods produce the first widely used tools
    deep linguistic analysis often traded for robust approximations
    empirical evaluationis essential

   of course, we must not go overboard and mistakenly conclude that the 
successes of statistical nlp render linguistics irrelevant (rash statements to 
this effect have been made in the past, e.g., the notorious remark,    every time 
i fire a linguist, my performance goes up   ). the information and insight that 
linguists, psychologists, and others have gathered about language is 
invaluable in creating high-performance broad-domain language 
understanding systems; for instance, in the id103 setting 
described above, a better understanding of language structure can lead to 
better language models.   

- lillian lee (2001) http://www.cs.cornell.edu/home/llee/papers/cstb/index.html

nlp history: ml and empiricism

   whenever i fire a linguist our system 
performance improves.       jelinek, 1988

    1990s: empirical revolution

tools
approximations

    corpus-based methods produce the first widely used 
    deep linguistic analysis often traded for robust 
    empirical evaluationis essential
statistical approaches, scale to more data!
(again), and      

    2000s: richer linguistic representations used in 
    2010s: nlp+x, excitement about neural networks 

related fields

    computational linguistics

    using computational methods to learn more 
    we end up doing this and using it

about how language works

    cognitive science

    figuring out how the human brain works
    includes the bits that do language
    humans: the only working nlp prototype!

    speech

    mapping audio signals to text
    traditionally separate from nlp, converging?
    two components: acoustic models and 
    language models in the domain of stat nlp

language models

key problems

we can understand programming languages. 

why is nlp not solved? 

key problems

we can understand programming languages. 

why is nlp not solved? 

    ambiguity
    scale
    sparsity

key problem: ambiguity

key problem: ambiguity

    some headlines:

    enraged cow injures farmer with ax
    ban on nude dancing on governor   s desk
    teacher strikes idle kids
    hospitals are sued by 7 foot doctors
    iraqi head seeks arms
    stolen painting found by tree
    kids make nutritious snacks
    local hs dropouts cut in half

syntactic ambiguity

hurricane emily howled toward mexico 's caribbean coast on sunday packing 135 mph winds and 
torrential rain and causing panic in cancun , where frightened tourists squeezed into musty shelters .

    sota: ~90% accurate for many languages when 
given many training examples, some progress in 
analyzing languages given few or no examples

semantic ambiguity

at last, a computer that understands you like your mother.

[example from lillian lee]

semantic ambiguity

at last, a computer that understands you like your mother.
    direct meanings:

well]

    but there are other possibilities, e.g. mothercould 

    it understands you like your mother (does) [presumably 
    it understands (that) you like your mother
mean:
    a woman who has given birth to a child
    a stringy slimy substance consisting of yeast cells and 
bacteria; is added to cider or wine to produce vinegar
    context matters, e.g. what if previous sentence was:
    wow, amazon predicted that you would need to order a 

big batch of new vinegar brewing ingredients. (cid:0)

[example from lillian lee]

ambiguities in the wild

ambiguities in the wild

ambiguities in the wild: context 

matters

ambiguities in the wild: context 

matters

key problem: scale

    people didknow that language was ambiguous!

       but they hoped that all interpretations would be (cid:1)good(cid:2) ones 

(or ruled out pragmatically)

key problem: scale

    people didknow that language was ambiguous!

       but they hoped that all interpretations would be (cid:1)good(cid:2) ones 

(or ruled out pragmatically)

       they didn   t realize how bad it would be

key problem: sparsity

    a corpusis a collection of text

    often annotated in some way
    sometimes just lots of text
    balanced vs. uniform corpora

    examples

    newswire collections: 500m+ 
words
    brown corpus: 1m words of 
tagged (cid:1)balanced(cid:2) text
    id32: 1m words of 
parsed wsj
    canadian hansards: 10m+ 
words of aligned french / 
english sentences
who knows what

    the web: billions of words of 

key problem: sparsity
    however: sparsity is always a problem
    new unigram (word), bigram (word pair)

 

n
e
e
s
n
o
i
t
c
a
r
f

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

200000

600000
400000
number of words

800000

1000000

unigrams

bigrams

the nlp community

    conferences: acl, naacl, emnlp, 

eacl, connl, coling, *sem, lrec, 
cicling,    

    journals: cl, tacl,     
    also in ai and ml conferences: aaai, 

ijcai, icml, nips

