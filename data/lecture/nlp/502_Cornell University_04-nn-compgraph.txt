cs5740: natural language processing 

spring 2018

computation graphs

instructor: yoav artzi 

from practical neural networks for nlp / chris dyer, 

yoav goldberg, graham neubig / emnlp 2016

computation graphs

    the descriptive language of deep learning models 
    functional description of the required computation 

    can be instantiated to do two types of computation: 

    forward computation 

    backward computation

expression:
y = x>ax + b    x + c
graph:

a node is a {tensor, matrix, vector, scalar} value

x

an edge represents a function argument   
expression:
(and also data dependency). they are just   
y = x>ax + b    x + c
pointers to nodes.
a node with an incoming edge is a function of 
graph:
that edge   s tail node.
a node knows how to compute its value and the 
value of its derivative w.r.t each argument (edge) 
times a derivative of an arbitrary input       .
@f
@f (u)

f (u) = u>

@f (u)

@u

@f
@f (u)

=    @f

@f (u)   >

x

expression:
y = x>ax + b    x + c
graph:

functions can be nullary, unary,   
binary,     n-ary. often they are unary or binary.

f(u, v) = uv

f (u) = u>

a

x

expression:
y = x>ax + b    x + c
graph:

f (m, v) = mv

f(u, v) = uv

f (u) = u>

a

x

computation graphs are directed and acyclic (usually)

expression:
y = x>ax + b    x + c
graph:

f (m, v) = mv

f (x, a) = x>ax

f(u, v) = uv

f (u) = u>

a

x

x

a

@f (x, a)

@x

@f (x, a)

@a

= (a> + a)x

= xx>

expression:
y = x>ax + b    x + c
graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

expression:
y = x>ax + b    x + c
graph:

f (x1, x2, x3) =xi

y

xi

f (m, v) = mv

f(u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

variable names are just labelings of nodes.

algorithms

    graph construction
    forward propagation

    loop over nodes in topological order 

    compute the value of the node given its inputs 

    given my inputs, make a prediction (or compute an    error    with respect to a    target 

output   ) 

    backward propagation

    loop over the nodes in reverse topological order starting with a    nal goal node 
    compute derivatives of    nal goal node value with respect to each edge   s tail 

node 

    how does the output change if i make a small change to the inputs?

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv

f (u) = u>
x>

a

f (u, v) = u    v

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv
x>a

f (u) = u>
x>

a

f (u, v) = u    v

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f(u, v) = uv
x>a

f (u) = u>
x>

f (u, v) = u    v

b    x

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv
x>ax

f(u, v) = uv
x>a

f (u) = u>
x>

f (u, v) = u    v

b    x

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi
xi
x>ax + b    x + c

f (m, v) = mv
x>ax

f(u, v) = uv
x>a

f (u) = u>
x>

f (u, v) = u    v

b    x

a

x

b

c

the mlp

h = tanh(wx + b)
y = vh + a

the mlp

h = tanh(wx + b)
y = vh + a

f(u, v) = u + v

f (m, v) = mv

a

f (u) = tanh(u) v

h

b

f (u, v) = u + v

f(m, v) = mv

w

x

constructing graphs

two software models

    static declaration

    phase 1: de   ne an architecture   

(maybe with some primitive    ow control like loops and 
conditionals) 

    phase 2: run a bunch of data through it to train the 

model and/or make predictions 

    dynamic declaration

    graph is de   ned implicitly (e.g., using operator 

overloading) as the forward computation is executed 

hierarchical structure

words

sentences

s

vp

vp

np

pp

alice

gave a message

to bob

phrases

documents

this film was completely unbelievable.
the characters were wooden and the plot was absurd.
that being said, i liked it.

static declaration

    pros

    of   ine optimization/scheduling of graphs is powerful 

    limits on operations mean better hardware support 

    cons

    structured data (even simple stuff like sequences), even variable-

sized data, is complex  

    you effectively learn a new programming language (   the graph 

language   ) and you write programs in that language to process data. 

    examples: torch, theano, tensorflow

dynamic declaration

    pros

    library is less invasive 

    the forward computation is written in your favorite programming 

language with all its features, using your favorite algorithms 

    interleave construction and evaluation of the graph 

    cons

    little time for graph optimization 

    if the graph is static, effort can be wasted 

    examples: chainer, most automatic differentiation libraries, dynet

dynamic structure?

    hierarchical structures exist in language 

    we might want to let the network re   ect that hierarchy 

    hierarchical structure is easiest to process with 

traditional    ow-control mechanisms in your favorite 
languages 

    combinatorial algorithms (e.g., id145) 

    exploit independencies to compute over a large 

space of operations tractably

