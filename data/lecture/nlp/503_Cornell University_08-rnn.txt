cs5740: natural language processing

spring 2018

recurrent neural networks

instructor: yoav artzi

adapted from yoav goldberg   s book and slides by sasha rush

overview

    finite state models
    recurrent neural networks (id56s)
    training id56s
    id56 models
    long short-term memory (lstm)
    attention

text classification

    consider the example:
    goal: classify sentiment
how can you not see this movie?
you should not see this movie.

    model: bag of words
    how well will the classifier work?

    similar unigrams and bigrams

    generally: need to maintain a state to 

capture distant influences

finite state machines

    simple, classical way of representing 

    current state: saves necessary past 

state

information

    example: email address parsing

deterministic finite state machines

    !     states
           vocabulary 
    #$   !     start state 
    &:!       !     transition function
    maps input *+,   ,*. to states #+,   ,#.
    for all /    1,   ,1#2=&(#25+,*2)

    what does it do?

    can we use it for id52? language 

modeling? 

    acceptor

types of state machines

    compute final state !" and make a decision 
based on it: #=%(!")
    apply function #(=%(!() to produce output 
    compute final state !", and use it in another 

for each intermediate state

    transducers

    encoders
model 

recurrent neural networks

    motivation:

    neural network model, but with state
    how can we borrow ideas from id122s?

    id56s are id122s    

        with a twist
    no longer finite in the same sense

id56

   !=   $%&' - hidden state space
     =   $&) - input state space
   *+   ! - initial state vector
   -      $&)     $%&'      $%&' - transition 
    simple definition of -:
-12345*,7 =tanh(7,*=+?)

function

* notation: vectors and matrices are bold

elman (1990)

id56

representation

    map from dense sequence to dense 

   !",   ,!%   '",   ,'%
    for all (    1,   ,+',=.',/",!
   . is parameterized, and parameters are shared 
'0=.'1,!0 =   =.(...'4,!",!5,!1,!0)

between all steps

    example:

    hidden states !" can be used in different 

id56s

    similar to finite state machines

    output function maps vectors to symbols: 

#:   &'()      &+,-
#!" =softmax(!"7+9)

    for example: single layer + softmax

ways

    acceptor
    transducer
    encoder

graphical representation

recursive representation

unrolled representation

graphical representation

training

    id56s are trained with sgd and backprop
    define loss over outputs
    id26 through time (bptt)

    depends on supervision and task

    use unrolled representation
    run forward propagation
    run backward propagation
    update all weights

    weights are shared between time steps

   

    sum the contributions of each time step to the gradient
inefficient
    batch helps, common but tricky to implement with variable-size 
models

id56: acceptor architecture
    only care about the output from the last hidden 
    train: supervised, loss on prediction
    example:

state

    text classification

    bi-gram decomposition:

id38

    input: !=#$,   ,#'
    goal: compute ((!)
' ((#,   #,/$)
(! =+,-$
' ((#,   #$,   ,#,/$)
(! =+,-$

    with id56s, can do non-markovian models:

id56: transducer architecture
    predict output for every time step

id38

    input: !=#$,   ,#'
    goal: compute ((!)
' ((#,   #$,   ,#,/$)
    model:(! =+,-$
( #, #$,   ,#,/$ =01, =0(21,/$,3,)
01, =softmax(;,<+>)
    predict next token ?@, as we go:
?@,=argmax0(1,)

id56: transducer architecture
    predict output for every time step
    examples:

    id38
    id52
    ner

id56: transducer architecture

x =x1, . . . , xn
si =r(si 1, xi), i = 1, . . . , n

o(si) =softmax(siw + b)

  yi = arg max o(si)

<latexit sha1_base64="umxa3eseif2brpqsfxmvb9cybqq=">aaacyxicbvfdi9nafj3erzv+bfcffblyxlzysylc+rkwkijgg6vybaepytkdtmnojnhmzkknefif+uabp8vjtpvs1wubm+eee+zkbpxlydd3fzvurdt37t7bu+89epjo8x7v4mm5yqrn+jhlmtptmbouhejjfcj5nnecprhkk/jifdofxhjtrka+4trn85qulugeo2ipqpdncidwcgfkcrunvvlhwtbczgighupbghrbs6kjaseh8pwow1tivvapuzzimasrg61d6/g5oymgjq2eyeustjzgssv6mmcljzw8/ocd14mwbc9cuazwbzymp9vlkyhh9wyv6vx9kd8w3atbbvtjps6i3i8bmbupv8gknwyw+dnok6prmmlrlywmzym7oes+s1drljt51w6ihhewwucsafsphjbttlq0nwadxlbzxds7vyb8x29wypj2xgmvf8gvu7ooksrgbs1ayse0zyjxflcmhc0kbeu1zwix3zxcspvln8h561hgj4ivb/qn7zbpsueekefkiatkmjysj+smjalzpjjskzxl95p73s3dh1ds19nmpcxxyv35f3rw2uq=</latexit>
<latexit sha1_base64="umxa3eseif2brpqsfxmvb9cybqq=">aaacyxicbvfdi9nafj3erzv+bfcffblyxlzysylc+rkwkijgg6vybaepytkdtmnojnhmzkknefif+uabp8vjtpvs1wubm+eee+zkbpxlydd3fzvurdt37t7bu+89epjo8x7v4mm5yqrn+jhlmtptmbouhejjfcj5nnecprhkk/jifdofxhjtrka+4trn85qulugeo2ipqpdncidwcgfkcrunvvlhwtbczgighupbghrbs6kjaseh8pwow1tivvapuzzimasrg61d6/g5oymgjq2eyeustjzgssv6mmcljzw8/ocd14mwbc9cuazwbzymp9vlkyhh9wyv6vx9kd8w3atbbvtjps6i3i8bmbupv8gknwyw+dnok6prmmlrlywmzym7oes+s1drljt51w6ihhewwucsafsphjbttlq0nwadxlbzxds7vyb8x29wypj2xgmvf8gvu7ooksrgbs1ayse0zyjxflcmhc0kbeu1zwix3zxcspvln8h561hgj4ivb/qn7zbpsueekefkiatkmjysj+smjalzpjjskzxl95p73s3dh1ds19nmpcxxyv35f3rw2uq=</latexit>
<latexit sha1_base64="umxa3eseif2brpqsfxmvb9cybqq=">aaacyxicbvfdi9nafj3erzv+bfcffblyxlzysylc+rkwkijgg6vybaepytkdtmnojnhmzkknefif+uabp8vjtpvs1wubm+eee+zkbpxlydd3fzvurdt37t7bu+89epjo8x7v4mm5yqrn+jhlmtptmbouhejjfcj5nnecprhkk/jifdofxhjtrka+4trn85qulugeo2ipqpdncidwcgfkcrunvvlhwtbczgighupbghrbs6kjaseh8pwow1tivvapuzzimasrg61d6/g5oymgjq2eyeustjzgssv6mmcljzw8/ocd14mwbc9cuazwbzymp9vlkyhh9wyv6vx9kd8w3atbbvtjps6i3i8bmbupv8gknwyw+dnok6prmmlrlywmzym7oes+s1drljt51w6ihhewwucsafsphjbttlq0nwadxlbzxds7vyb8x29wypj2xgmvf8gvu7ooksrgbs1ayse0zyjxflcmhc0kbeu1zwix3zxcspvln8h561hgj4ivb/qn7zbpsueekefkiatkmjysj+smjalzpjjskzxl95p73s3dh1ds19nmpcxxyv35f3rw2uq=</latexit>
<latexit sha1_base64="umxa3eseif2brpqsfxmvb9cybqq=">aaacyxicbvfdi9nafj3erzv+bfcffblyxlzysylc+rkwkijgg6vybaepytkdtmnojnhmzkknefif+uabp8vjtpvs1wubm+eee+zkbpxlydd3fzvurdt37t7bu+89epjo8x7v4mm5yqrn+jhlmtptmbouhejjfcj5nnecprhkk/jifdofxhjtrka+4trn85qulugeo2ipqpdncidwcgfkcrunvvlhwtbczgighupbghrbs6kjaseh8pwow1tivvapuzzimasrg61d6/g5oymgjq2eyeustjzgssv6mmcljzw8/ocd14mwbc9cuazwbzymp9vlkyhh9wyv6vx9kd8w3atbbvtjps6i3i8bmbupv8gknwyw+dnok6prmmlrlywmzym7oes+s1drljt51w6ihhewwucsafsphjbttlq0nwadxlbzxds7vyb8x29wypj2xgmvf8gvu7ooksrgbs1ayse0zyjxflcmhc0kbeu1zwix3zxcspvln8h561hgj4ivb/qn7zbpsueekefkiatkmjysj+smjalzpjjskzxl95p73s3dh1ds19nmpcxxyv35f3rw2uq=</latexit>

id56: encoder architecture

    similar to acceptor
    difference: last state is used as input to 

another model and not for prediction

!"# ="#  %&="&

    example:

    sentence embedding

bidirectional id56s

    id56 decisions are based on historical data only
    when is it relevant? feasible?

    how can we account for future input?

bidirectional id56s

    how can we account for future input?

    id56 decisions are based on historical data only
    when is it relevant? feasible?
    probabilistic model, for example for id38:

    when all the input is available. not for real-time input.

( !(*%   *',   ,*%.',*%/',   ,*()

!" =$%&'

deep id56s

    can also make id56s deeper (vertically) to 

increase the model capacity

    special case of the transducer architecture

id56: generator
    generation conditioned on !"
+ #(-(   -*,   ,-(1*,%")
# $ %" ='()*

    probabilistic model:

id56: generator

    stop when generating the stop token
    during learning (usually): force predicting 

the annotated token and compute loss

sj =r(sj 1,  tj 1)

o(sj) =softmax(sjw + b)

  tj = arg max o(sj)

<latexit sha1_base64="0x57+9/hsx/9goozv/epedghbz8=">aaacp3icbvhbattaef2pt1s9ocljx5a6ltftjfqkyusgtc+ll06t+ajey0brlb326sluqngi/vo/om/9m65spth2bxyoz87mmz0jmyun+v4fx33w8nhjjwdpvwfpx7x81tg86pk011x0eapspqjbccut0uwjsgwylsaoleihi69vvv9tacpt5bzxmrjfme1kjdmgpcanxywgnivrycrxnf68p9cnw0wxpw3kj/soytpaasuab1hgvm62fn6qolcgyomfssomyvnee9zhfkk//gsbli3gqlfnsp6hgz7azjluolxu3rjr9nv+oug+cgrqjhvcjru/2stlesws5aqmgqz+hqmcnequromx3igm+akmymhharewo2k955k+s8yerqm2l0g6zrcrcoinwcwhvvajmt1crf4vn8wxoh8vmslyfanfgew5opjs6mh0irxgqfywanfszkr5ddrwtketlhdsfnkf9d61a78d/pjcvpxsr+oavczvyakjybm5jn/ifeks7rx1vjs3zq3bcjtuzx1spk5t1xyte+hcx31lz3o=</latexit>
<latexit sha1_base64="0x57+9/hsx/9goozv/epedghbz8=">aaacp3icbvhbattaef2pt1s9ocljx5a6ltftjfqkyusgtc+ll06t+ajey0brlb326sluqngi/vo/om/9m65spth2bxyoz87mmz0jmyun+v4fx33w8nhjjwdpvwfpx7x81tg86pk011x0eapspqjbccut0uwjsgwylsaoleihi69vvv9tacpt5bzxmrjfme1kjdmgpcanxywgnivrycrxnf68p9cnw0wxpw3kj/soytpaasuab1hgvm62fn6qolcgyomfssomyvnee9zhfkk//gsbli3gqlfnsp6hgz7azjluolxu3rjr9nv+oug+cgrqjhvcjru/2stlesws5aqmgqz+hqmcnequromx3igm+akmymhharewo2k955k+s8yerqm2l0g6zrcrcoinwcwhvvajmt1crf4vn8wxoh8vmslyfanfgew5opjs6mh0irxgqfywanfszkr5ddrwtketlhdsfnkf9d61a78d/pjcvpxsr+oavczvyakjybm5jn/ifeks7rx1vjs3zq3bcjtuzx1spk5t1xyte+hcx31lz3o=</latexit>
<latexit sha1_base64="0x57+9/hsx/9goozv/epedghbz8=">aaacp3icbvhbattaef2pt1s9ocljx5a6ltftjfqkyusgtc+ll06t+ajey0brlb326sluqngi/vo/om/9m65spth2bxyoz87mmz0jmyun+v4fx33w8nhjjwdpvwfpx7x81tg86pk011x0eapspqjbccut0uwjsgwylsaoleihi69vvv9tacpt5bzxmrjfme1kjdmgpcanxywgnivrycrxnf68p9cnw0wxpw3kj/soytpaasuab1hgvm62fn6qolcgyomfssomyvnee9zhfkk//gsbli3gqlfnsp6hgz7azjluolxu3rjr9nv+oug+cgrqjhvcjru/2stlesws5aqmgqz+hqmcnequromx3igm+akmymhharewo2k955k+s8yerqm2l0g6zrcrcoinwcwhvvajmt1crf4vn8wxoh8vmslyfanfgew5opjs6mh0irxgqfywanfszkr5ddrwtketlhdsfnkf9d61a78d/pjcvpxsr+oavczvyakjybm5jn/ifeks7rx1vjs3zq3bcjtuzx1spk5t1xyte+hcx31lz3o=</latexit>
<latexit sha1_base64="0x57+9/hsx/9goozv/epedghbz8=">aaacp3icbvhbattaef2pt1s9ocljx5a6ltftjfqkyusgtc+ll06t+ajey0brlb326sluqngi/vo/om/9m65spth2bxyoz87mmz0jmyun+v4fx33w8nhjjwdpvwfpx7x81tg86pk011x0eapspqjbccut0uwjsgwylsaoleihi69vvv9tacpt5bzxmrjfme1kjdmgpcanxywgnivrycrxnf68p9cnw0wxpw3kj/soytpaasuab1hgvm62fn6qolcgyomfssomyvnee9zhfkk//gsbli3gqlfnsp6hgz7azjluolxu3rjr9nv+oug+cgrqjhvcjru/2stlesws5aqmgqz+hqmcnequromx3igm+akmymhharewo2k955k+s8yerqm2l0g6zrcrcoinwcwhvvajmt1crf4vn8wxoh8vmslyfanfgew5opjs6mh0irxgqfywanfszkr5ddrwtketlhdsfnkf9d61a78d/pjcvpxsr+oavczvyakjybm5jn/ifeks7rx1vjs3zq3bcjtuzx1spk5t1xyte+hcx31lz3o=</latexit>

example: id134
    given: image !
    set "#=id98(!)
) * ! =+,-.

/ )(0,   0.,   ,0,4.,!)

    goal: generate caption

    model:

examples from karpathy
and fei-fei 2015

sequence-to-sequence

generator

    connect encoder and 
    many alternatives:

    set generator !"# to 
encoder output !$%
generator !"# with 

    concatenate 

each step input 
during generation 

    examples:
    machine translation
    chatbots
    id71
    can also generate 
other sequences     not 
only natural language!

sequence-to-sequence

x =x1, . . . , xn
i =re(se
se
i 1, xi), i = 1, . . . , n
c =oe(se
n )
j 1, [  tj 1; c])
sd
j =r(sd
j ) =softmax(sd
o(sd
  tj = arg max o(sd
j )

j w + b)

<latexit sha1_base64="ehk2mbe5mgd73rmjf/0txeimq/q=">aaadwhicbvjna9taef1zszo4h3hsyy9dtynnxsovqgilenoyekta6thg2wk1xtnrscuhhrubot9z6kh9k710pditpzqw8pbn7huzw3hxibra1k+jyu7tpzo4pko+fvl02xht5prorwncejdfqzt0pap4ictvosca9+oe09alem+7/1jke994okqkv+ii5soqtqtwbaookffekh24hdnwqoptz8/muwu3nhgeqrvgsxcc6uquclemonb5bl/ctmonzcqbox91wutiotkcor1wohtklc9ubjzv5kjt3lkbja5luw2zwwf23ylbinsmfdpml6n38m9lwarendylm4umomjnmknix5do862s1a2xw+u/el7edbyo7tjocj2hjhodmcouw9fd1a3vrbzvbuwcewnqzbm3bu27hhxlqy6rbvspgw3fomxogoifpk86qeixzfd0wgcashpynczkxcjhlwbg4eejphkhzndfzdruahf6urjovm3ncvj/uugk/suwezjokuv2yosnawaexzbbwcscybdqglje6f6btwlcgepdlizgb395f9y9bdtw2/78rn71ytmoq/kcvcqnypnzcku+kvvsjcz4yfyu7fx2k79myh6yrw+lfwp55jnzcpp0d+p+c7o=</latexit>
<latexit sha1_base64="ehk2mbe5mgd73rmjf/0txeimq/q=">aaadwhicbvjna9taef1zszo4h3hsyy9dtynnxsovqgilenoyekta6thg2wk1xtnrscuhhrubot9z6kh9k710pditpzqw8pbn7huzw3hxibra1k+jyu7tpzo4pko+fvl02xht5prorwncejdfqzt0pap4ictvosca9+oe09alem+7/1jke994okqkv+ii5soqtqtwbaookffekh24hdnwqoptz8/muwu3nhgeqrvgsxcc6uquclemonb5bl/ctmonzcqbox91wutiotkcor1wohtklc9ubjzv5kjt3lkbja5luw2zwwf23ylbinsmfdpml6n38m9lwarendylm4umomjnmknix5do862s1a2xw+u/el7edbyo7tjocj2hjhodmcouw9fd1a3vrbzvbuwcewnqzbm3bu27hhxlqy6rbvspgw3fomxogoifpk86qeixzfd0wgcashpynczkxcjhlwbg4eejphkhzndfzdruahf6urjovm3ncvj/uugk/suwezjokuv2yosnawaexzbbwcscybdqglje6f6btwlcgepdlizgb395f9y9bdtw2/78rn71ytmoq/kcvcqnypnzcku+kvvsjcz4yfyu7fx2k79myh6yrw+lfwp55jnzcpp0d+p+c7o=</latexit>
<latexit sha1_base64="ehk2mbe5mgd73rmjf/0txeimq/q=">aaadwhicbvjna9taef1zszo4h3hsyy9dtynnxsovqgilenoyekta6thg2wk1xtnrscuhhrubot9z6kh9k710pditpzqw8pbn7huzw3hxibra1k+jyu7tpzo4pko+fvl02xht5prorwncejdfqzt0pap4ictvosca9+oe09alem+7/1jke994okqkv+ii5soqtqtwbaookffekh24hdnwqoptz8/muwu3nhgeqrvgsxcc6uquclemonb5bl/ctmonzcqbox91wutiotkcor1wohtklc9ubjzv5kjt3lkbja5luw2zwwf23ylbinsmfdpml6n38m9lwarendylm4umomjnmknix5do862s1a2xw+u/el7edbyo7tjocj2hjhodmcouw9fd1a3vrbzvbuwcewnqzbm3bu27hhxlqy6rbvspgw3fomxogoifpk86qeixzfd0wgcashpynczkxcjhlwbg4eejphkhzndfzdruahf6urjovm3ncvj/uugk/suwezjokuv2yosnawaexzbbwcscybdqglje6f6btwlcgepdlizgb395f9y9bdtw2/78rn71ytmoq/kcvcqnypnzcku+kvvsjcz4yfyu7fx2k79myh6yrw+lfwp55jnzcpp0d+p+c7o=</latexit>
<latexit sha1_base64="ehk2mbe5mgd73rmjf/0txeimq/q=">aaadwhicbvjna9taef1zszo4h3hsyy9dtynnxsovqgilenoyekta6thg2wk1xtnrscuhhrubot9z6kh9k710pditpzqw8pbn7huzw3hxibra1k+jyu7tpzo4pko+fvl02xht5prorwncejdfqzt0pap4ictvosca9+oe09alem+7/1jke994okqkv+ii5soqtqtwbaookffekh24hdnwqoptz8/muwu3nhgeqrvgsxcc6uquclemonb5bl/ctmonzcqbox91wutiotkcor1wohtklc9ubjzv5kjt3lkbja5luw2zwwf23ylbinsmfdpml6n38m9lwarendylm4umomjnmknix5do862s1a2xw+u/el7edbyo7tjocj2hjhodmcouw9fd1a3vrbzvbuwcewnqzbm3bu27hhxlqy6rbvspgw3fomxogoifpk86qeixzfd0wgcashpynczkxcjhlwbg4eejphkhzndfzdruahf6urjovm3ncvj/uugk/suwezjokuv2yosnawaexzbbwcscybdqglje6f6btwlcgepdlizgb395f9y9bdtw2/78rn71ytmoq/kcvcqnypnzcku+kvvsjcz4yfyu7fx2k79myh6yrw+lfwp55jnzcpp0d+p+c7o=</latexit>

sequence-to-sequence training 

graph

long-range interactions

    promise: learn long-range interactions of 

language from data

    example:

how can you not see this movie?
you should not see this movie.

    sometimes: requires    remembering    early 

    key signal here is at !", but gradient is at !#

state

long-term gradients

    gradient go through (many) multiplications
    ok at end layers    close to the loss
    but: issue with early layers

    for example, derivative of tanh
%%&tanh&=1   tanh*&

    large activation    gradient disappears

    in other id180, values can 

become larger and larger

exploding gradients

    common when there is 
not saturation in activation 
(e.g., relu) and we get 
exponential blowup

    result: reasonable short-

term gradient, but bad 
long-term ones

    common heuristic:
    gradient clipping: 

bounding all gradients by 
maximum value

vanishing gradients

    for example: when tanh saturates

    occurs when multiplying small values

    mainly affects long-term gradients
    solving this is more complex

long short-term memory (lstm)

hochreiter and schmidhuber (1997)

lstm vs. elman id56

lstm

 (  )

<latexit sha1_base64="dnfa4qbot9ylps99to0e4emch+u=">aaab9hicbvbns8naej34wetx1aoxxslus0le0gpri8ck9goaudabtbt0dxn3n4us+ju8efdeqz/gm//gbzudtj4yelw3w8y8movmg9f9dtbwnza3tks75d29/ypdytfxwyezirrfep6obog15uzslmgg026qkbyhp51wddfzo2oqnevko5mknbb4ifnmcdzwcnznbglxfbil5qjfqbp1dw60srycvkfas1/58qoezijkqzjwuue5qqlyrawjne7lfqzpisk jpuokf1ue+p3qkzq0sothrtqrbc/x3ri6f1hmr2k6bzvavezpxp6+xmfgmyjlmm0mlwsykm45mgmyjoigpsgyfwikjyvzwrizyywjstmubgrf88ippx9y9t+49xfubt0ucjtifm6ibb9fqghtoqgsipmezvmkbm3zenhfny9g65hqzj/ahzucpqvorwa==</latexit>
<latexit sha1_base64="dnfa4qbot9ylps99to0e4emch+u=">aaab9hicbvbns8naej34wetx1aoxxslus0le0gpri8ck9goaudabtbt0dxn3n4us+ju8efdeqz/gm//gbzudtj4yelw3w8y8movmg9f9dtbwnza3tks75d29/ypdytfxwyezirrfep6obog15uzslmgg026qkbyhp51wddfzo2oqnevko5mknbb4ifnmcdzwcnznbglxfbil5qjfqbp1dw60srycvkfas1/58qoezijkqzjwuue5qqlyrawjne7lfqzpisk jpuokf1ue+p3qkzq0sothrtqrbc/x3ri6f1hmr2k6bzvavezpxp6+xmfgmyjlmm0mlwsykm45mgmyjoigpsgyfwikjyvzwrizyywjstmubgrf88ippx9y9t+49xfubt0ucjtifm6ibb9fqghtoqgsipmezvmkbm3zenhfny9g65hqzj/ahzucpqvorwa==</latexit>
<latexit sha1_base64="dnfa4qbot9ylps99to0e4emch+u=">aaab9hicbvbns8naej34wetx1aoxxslus0le0gpri8ck9goaudabtbt0dxn3n4us+ju8efdeqz/gm//gbzudtj4yelw3w8y8movmg9f9dtbwnza3tks75d29/ypdytfxwyezirrfep6obog15uzslmgg026qkbyhp51wddfzo2oqnevko5mknbb4ifnmcdzwcnznbglxfbil5qjfqbp1dw60srycvkfas1/58qoezijkqzjwuue5qqlyrawjne7lfqzpisk jpuokf1ue+p3qkzq0sothrtqrbc/x3ri6f1hmr2k6bzvavezpxp6+xmfgmyjlmm0mlwsykm45mgmyjoigpsgyfwikjyvzwrizyywjstmubgrf88ippx9y9t+49xfubt0ucjtifm6ibb9fqghtoqgsipmezvmkbm3zenhfny9g65hqzj/ahzucpqvorwa==</latexit>
<latexit sha1_base64="dnfa4qbot9ylps99to0e4emch+u=">aaab9hicbvbns8naej34wetx1aoxxslus0le0gpri8ck9goaudabtbt0dxn3n4us+ju8efdeqz/gm//gbzudtj4yelw3w8y8movmg9f9dtbwnza3tks75d29/ypdytfxwyezirrfep6obog15uzslmgg026qkbyhp51wddfzo2oqnevko5mknbb4ifnmcdzwcnznbglxfbil5qjfqbp1dw60srycvkfas1/58qoezijkqzjwuue5qqlyrawjne7lfqzpisk jpuokf1ue+p3qkzq0sothrtqrbc/x3ri6f1hmr2k6bzvavezpxp6+xmfgmyjlmm0mlwsykm45mgmyjoigpsgyfwikjyvzwrizyywjstmubgrf88ippx9y9t+49xfubt0ucjtifm6ibb9fqghtoqgsipmezvmkbm3zenhfny9g65hqzj/ahzucpqvorwa==</latexit>

output

cell state

ft =  (wf [ht 1, xt] + bf )
it =  (wi[ht 1, xt] + bf )
ct = ft   ct 1 + it   tanh(wc[ht 1, xt] + bc)
ot =  (wo[ht 1, xt] + bo)
ht = ot   tanh(ct)

<latexit sha1_base64="5rn75omq38gfp31kxtutibatfok=">aaadsnicpvjna9taef1l/ujdlyc99rlupcs0caqqsc6f0f56tkgoqy1frnyra8lqv+yoqozq/+u5t/6brhyryhzdcbkqzl558/rmmdgx3idn/e447qpht55upes+f/hy1eve9s6fuywmbeivupoyjoyjltkqoah2mwtgsliwuxz9pa6pbpg2xmnvmmtzmjgp5amnbcwubxd+bhmbne7kpioav/+ea8ongdlr4ff1lyybr1pfjrz41ufcile2k8qf/r5js9/hqdbtal5rlj9eljaybfebmijalcpctsxdwywgmm3boet2/uoglrlrg4du91jvs6rpyoxqk/t6gftrr+8nvhng9crfjh20ipoo9yuykfpktaivxjix7+uqlkqdp4jv3aawlcf0mkzz2kaszmye5fzkkrxrkqlollafbdxh2x0lyyyzzbfl1ibnaq0g/1ubf5cchiwxeqfm0rsfjyxaohb9v3jcnamgzjyhvhprfdouaelbxnnxlsffhxk9utga+n7a/3bcp/u8wmcweoveot3koxn0hr6iczre1dl0hs6ve7nh7g+xupso6nqwpw/qurj /sngu=</latexit>
<latexit sha1_base64="5rn75omq38gfp31kxtutibatfok=">aaadsnicpvjna9taef1l/ujdlyc99rlupcs0caqqsc6f0f56tkgoqy1frnyra8lqv+yoqozq/+u5t/6brhyryhzdcbkqzl558/rmmdgx3idn/e447qpht55upes+f/hy1eve9s6fuywmbeivupoyjoyjltkqoah2mwtgsliwuxz9pa6pbpg2xmnvmmtzmjgp5amnbcwubxd+bhmbne7kpioav/+ea8ongdlr4ff1lyybr1pfjrz41ufcile2k8qf/r5js9/hqdbtal5rlj9eljaybfebmijalcpctsxdwywgmm3boet2/uoglrlrg4du91jvs6rpyoxqk/t6gftrr+8nvhng9crfjh20ipoo9yuykfpktaivxjix7+uqlkqdp4jv3aawlcf0mkzz2kaszmye5fzkkrxrkqlollafbdxh2x0lyyyzzbfl1ibnaq0g/1ubf5cchiwxeqfm0rsfjyxaohb9v3jcnamgzjyhvhprfdouaelbxnnxlsffhxk9utga+n7a/3bcp/u8wmcweoveot3koxn0hr6iczre1dl0hs6ve7nh7g+xupso6nqwpw/qurj /sngu=</latexit>
<latexit sha1_base64="5rn75omq38gfp31kxtutibatfok=">aaadsnicpvjna9taef1l/ujdlyc99rlupcs0caqqsc6f0f56tkgoqy1frnyra8lqv+yoqozq/+u5t/6brhyryhzdcbkqzl558/rmmdgx3idn/e447qpht55upes+f/hy1eve9s6fuywmbeivupoyjoyjltkqoah2mwtgsliwuxz9pa6pbpg2xmnvmmtzmjgp5amnbcwubxd+bhmbne7kpioav/+ea8ongdlr4ff1lyybr1pfjrz41ufcile2k8qf/r5js9/hqdbtal5rlj9eljaybfebmijalcpctsxdwywgmm3boet2/uoglrlrg4du91jvs6rpyoxqk/t6gftrr+8nvhng9crfjh20ipoo9yuykfpktaivxjix7+uqlkqdp4jv3aawlcf0mkzz2kaszmye5fzkkrxrkqlollafbdxh2x0lyyyzzbfl1ibnaq0g/1ubf5cchiwxeqfm0rsfjyxaohb9v3jcnamgzjyhvhprfdouaelbxnnxlsffhxk9utga+n7a/3bcp/u8wmcweoveot3koxn0hr6iczre1dl0hs6ve7nh7g+xupso6nqwpw/qurj /sngu=</latexit>
<latexit sha1_base64="5rn75omq38gfp31kxtutibatfok=">aaadsnicpvjna9taef1l/ujdlyc99rlupcs0caqqsc6f0f56tkgoqy1frnyra8lqv+yoqozq/+u5t/6brhyryhzdcbkqzl558/rmmdgx3idn/e447qpht55upes+f/hy1eve9s6fuywmbeivupoyjoyjltkqoah2mwtgsliwuxz9pa6pbpg2xmnvmmtzmjgp5amnbcwubxd+bhmbne7kpioav/+ea8ongdlr4ff1lyybr1pfjrz41ufcile2k8qf/r5js9/hqdbtal5rlj9eljaybfebmijalcpctsxdwywgmm3boet2/uoglrlrg4du91jvs6rpyoxqk/t6gftrr+8nvhng9crfjh20ipoo9yuykfpktaivxjix7+uqlkqdp4jv3aawlcf0mkzz2kaszmye5fzkkrxrkqlollafbdxh2x0lyyyzzbfl1ibnaq0g/1ubf5cchiwxeqfm0rsfjyxaohb9v3jcnamgzjyhvhprfdouaelbxnnxlsffhxk9utga+n7a/3bcp/u8wmcweoveot3koxn0hr6iczre1dl0hs6ve7nh7g+xupso6nqwpw/qurj /sngu=</latexit>

image by tim rockt  schel

input

attention

    in seq-to-seq models, a single vector is 

connects encoding and decoding
    worrying?
    all the input string information must encoded into 

a fixed-length vector

    the decoder must recover all this information 

from a fixed-length vector 

    attention relaxes the assumption that single 

vector must be used to encode the input 
sentence regardless of length

attention

    encode input sentence as a sequence of 

vectors

    at each step: pick what vector to use
    but: discrete choice is not differentiable

    make the choice soft

attention

(cid:18)(cid:24)(cid:15)(cid:21)(cid:15) (cid:36)(cid:48)(cid:47)(cid:37)(cid:42)(cid:53)(cid:42)(cid:48)(cid:47)(cid:38)(cid:37) (cid:40)(cid:38)(cid:47)(cid:38)(cid:51)(cid:34)(cid:53)(cid:42)(cid:48)(cid:47) (cid:56)(cid:42)(cid:53)(cid:41) (cid:34)(cid:53)(cid:53)(cid:38)(cid:47)(cid:53)(cid:42)(cid:48)(cid:47) (cid:19)(cid:17)(cid:24)

the

black

fox

jumped

</s>

predict

predict

predict

predict

predict

y1

y2

y3

y4

y5

s0

rd, od

s1

rd, od

s2

rd, od

s3

rd, od

s4

rd, od

concat

concat

concat

concat

concat

c0

c1

c2

c3

e[<s>]

<s>

e[the]

the

e[black]

black

e[fox]

fox

c4

e[jumped]

jumped

attend

attend

attend

attend

attend

bie

bie

bie

bie

bie

e[<s>]

<s>

e[a]

a

e[conditioning]

e[sequence]

conditioning

sequence

e[</s>]

</s>

(cid:39)(cid:74)(cid:72)(cid:86)(cid:83)(cid:70) (cid:18)(cid:24)(cid:15)(cid:22)(cid:27) (cid:52)(cid:70)(cid:82)(cid:86)(cid:70)(cid:79)(cid:68)(cid:70)(cid:14)(cid:85)(cid:80)(cid:14)(cid:84)(cid:70)(cid:82)(cid:86)(cid:70)(cid:79)(cid:68)(cid:70) (cid:51)(cid:47)(cid:47) (cid:72)(cid:70)(cid:79)(cid:70)(cid:83)(cid:66)(cid:85)(cid:80)(cid:83) (cid:88)(cid:74)(cid:85)(cid:73) (cid:66)(cid:85)(cid:85)(cid:70)(cid:79)(cid:85)(cid:74)(cid:80)(cid:79)(cid:15)

(cid:85)(cid:73)(cid:66)(cid:85) (cid:74)(cid:84)(cid:13) (cid:85)(cid:73)(cid:70)(cid:90) (cid:83)(cid:70)(cid:81)(cid:83)(cid:70)(cid:84)(cid:70)(cid:79)(cid:85) (cid:66) (cid:88)(cid:74)(cid:79)(cid:69)(cid:80)(cid:88) (cid:71)(cid:80)(cid:68)(cid:86)(cid:84)(cid:70)(cid:69) (cid:66)(cid:83)(cid:80)(cid:86)(cid:79)(cid:69) (cid:85)(cid:73)(cid:70) (cid:74)(cid:79)(cid:81)(cid:86)(cid:85) (cid:74)(cid:85)(cid:70)(cid:78) xi (cid:66)(cid:79)(cid:69) (cid:79)(cid:80)(cid:85) (cid:85)(cid:73)(cid:70) (cid:74)(cid:85)(cid:70)(cid:78) (cid:74)(cid:85)(cid:84)(cid:70)(cid:77)(cid:71)(cid:15) (cid:52)(cid:70)(cid:68)(cid:80)(cid:79)(cid:69)(cid:13)
(cid:67)(cid:90) (cid:73)(cid:66)(cid:87)(cid:74)(cid:79)(cid:72) (cid:66) (cid:85)(cid:83)(cid:66)(cid:74)(cid:79)(cid:66)(cid:67)(cid:77)(cid:70) (cid:70)(cid:79)(cid:68)(cid:80)(cid:69)(cid:74)(cid:79)(cid:72) (cid:68)(cid:80)(cid:78)(cid:81)(cid:80)(cid:79)(cid:70)(cid:79)(cid:85) (cid:85)(cid:73)(cid:66)(cid:85) (cid:74)(cid:84) (cid:85)(cid:83)(cid:66)(cid:74)(cid:79)(cid:70)(cid:69) (cid:75)(cid:80)(cid:74)(cid:79)(cid:85)(cid:77)(cid:90) (cid:88)(cid:74)(cid:85)(cid:73) (cid:85)(cid:73)(cid:70) (cid:69)(cid:70)(cid:68)(cid:80)(cid:69)(cid:70)(cid:83)(cid:13) (cid:85)(cid:73)(cid:70) (cid:70)(cid:79)(cid:68)(cid:80)(cid:69)(cid:70)(cid:83)
(cid:66)(cid:79)(cid:69) (cid:69)(cid:70)(cid:68)(cid:80)(cid:69)(cid:70)(cid:83) (cid:70)(cid:87)(cid:80)(cid:77)(cid:87)(cid:70) (cid:85)(cid:80)(cid:72)(cid:70)(cid:85)(cid:73)(cid:70)(cid:83) (cid:66)(cid:79)(cid:69) (cid:85)(cid:73)(cid:70) (cid:79)(cid:70)(cid:85)(cid:88)(cid:80)(cid:83)(cid:76) (cid:68)(cid:66)(cid:79) (cid:77)(cid:70)(cid:66)(cid:83)(cid:79) (cid:85)(cid:80) (cid:70)(cid:79)(cid:68)(cid:80)(cid:69)(cid:70) (cid:83)(cid:70)(cid:77)(cid:70)(cid:87)(cid:66)(cid:79)(cid:85) (cid:81)(cid:83)(cid:80)(cid:81)(cid:70)(cid:83)(cid:85)(cid:74)(cid:70)(cid:84) (cid:80)(cid:71) (cid:85)(cid:73)(cid:70) (cid:74)(cid:79)(cid:81)(cid:86)(cid:85)

attention

attend

x =x1, . . . , xn
se
i =re(se
i 1, xi), i = 1, . . . , n
ci =oe(se
i )
     j
i =sd
j 1    ci
   j =softmax(     j

1, . . . ,      j
n)

s0

(cid:18)(cid:24)(cid:15)(cid:21)(cid:15) (cid:36)(cid:48)(cid:47)(cid:37)(cid:42)(cid:53)(cid:42)(cid:48)(cid:47)(cid:38)(cid:37) (cid:40)(cid:38)(cid:47)(cid:38)(cid:51)(cid:34)(cid:53)(cid:42)(cid:48)(cid:47) (cid:56)(cid:42)(cid:53)(cid:41) (cid:34)(cid:53)(cid:53)(cid:38)(cid:47)(cid:53)(cid:42)(cid:48)(cid:47) (cid:19)(cid:17)(cid:24)

the

black

fox

jumped

</s>

predict

predict

predict

predict

predict

y1

y2

y3

y4

y5

rd, od

s1

rd, od

s2

rd, od

s3

rd, od

s4

rd, od

cj =

   j
i ci

nxi=1
j 1, [  tj 1; cj])
j w + b)

sd
j =r(sd
j ) =softmax(sd
o(sd
  tj = arg max o(sd
j )

<latexit sha1_base64="f9zv6vf8f3p2j6etykmqmegptrc=">aaaej3icdvnbi9nafj5nvkz11tvhxw4wlxzrsurqkmkifnzbvexuowndzdppp5sbmxnpcfk3vvhxfbfurb/9j86kf5jwbwjnzvno933nkpgsgalpwb8pdppk1wvxd2/ubt66fedu/ejeuyhttmifxehmbx4wngar7usmazpiomwhf9al7/k1rl98pfywopoglwkdhxgamz8rlfxkptk6a+jcmtghljppzxa5a7edssxfu5skwhfqm7vixtbuqfcy3ru9zimbssd2pu61y2ss1qamfdakfsaie3lndfpluvwtbysa42geothizjgfz12mswxfuvz8aw5r5fcizwwzbf9qukkxmhoxl0o8yjs7crzyxxifnuluqjrfsik0ven0lq01wohwpv9nrxmek796d839odow3hqeyznjff16cwxhuwhltfklbwnwvekqkm3tiofhw0yvbxwb3hwef7vgfkoqc9hx0x5qbr1hdaziwh5gr4mgwp8zt/5nrzekiy0kcbaqq9tk5cjdxdis0lzmpiimmfzikr2qmmihfaos+m9zekqye/bjrr5iqpetd2q4fgizegqpzyrdmk7+qzzmpf9illeosswnyeritwoqmehhaxpgkzhbugwycka8aplhjolut0svwd4det84f9qxry797lnj5nv6hyfoaxqimshgz9ejeovoub8r45pxxfhu/da/m1/nn+avfdq4wpfcr5vj/vkl/01bxg==</latexit>
<latexit sha1_base64="f9zv6vf8f3p2j6etykmqmegptrc=">aaaej3icdvnbi9nafj5nvkz11tvhxw4wlxzrsurqkmkifnzbvexuowndzdppp5sbmxnpcfk3vvhxfbfurb/9j86kf5jwbwjnzvno933nkpgsgalpwb8pdppk1wvxd2/ubt66fedu/ejeuyhttmifxehmbx4wngar7usmazpiomwhf9al7/k1rl98pfywopoglwkdhxgamz8rlfxkptk6a+jcmtghljppzxa5a7edssxfu5skwhfqm7vixtbuqfcy3ru9zimbssd2pu61y2ss1qamfdakfsaie3lndfpluvwtbysa42geothizjgfz12mswxfuvz8aw5r5fcizwwzbf9qukkxmhoxl0o8yjs7crzyxxifnuluqjrfsik0ven0lq01wohwpv9nrxmek796d839odow3hqeyznjff16cwxhuwhltfklbwnwvekqkm3tiofhw0yvbxwb3hwef7vgfkoqc9hx0x5qbr1hdaziwh5gr4mgwp8zt/5nrzekiy0kcbaqq9tk5cjdxdis0lzmpiimmfzikr2qmmihfaos+m9zekqye/bjrr5iqpetd2q4fgizegqpzyrdmk7+qzzmpf9illeosswnyeritwoqmehhaxpgkzhbugwycka8aplhjolut0svwd4det84f9qxry797lnj5nv6hyfoaxqimshgz9ejeovoub8r45pxxfhu/da/m1/nn+avfdq4wpfcr5vj/vkl/01bxg==</latexit>
<latexit sha1_base64="f9zv6vf8f3p2j6etykmqmegptrc=">aaaej3icdvnbi9nafj5nvkz11tvhxw4wlxzrsurqkmkifnzbvexuowndzdppp5sbmxnpcfk3vvhxfbfurb/9j86kf5jwbwjnzvno933nkpgsgalpwb8pdppk1wvxd2/ubt66fedu/ejeuyhttmifxehmbx4wngar7usmazpiomwhf9al7/k1rl98pfywopoglwkdhxgamz8rlfxkptk6a+jcmtghljppzxa5a7edssxfu5skwhfqm7vixtbuqfcy3ru9zimbssd2pu61y2ss1qamfdakfsaie3lndfpluvwtbysa42geothizjgfz12mswxfuvz8aw5r5fcizwwzbf9qukkxmhoxl0o8yjs7crzyxxifnuluqjrfsik0ven0lq01wohwpv9nrxmek796d839odow3hqeyznjff16cwxhuwhltfklbwnwvekqkm3tiofhw0yvbxwb3hwef7vgfkoqc9hx0x5qbr1hdaziwh5gr4mgwp8zt/5nrzekiy0kcbaqq9tk5cjdxdis0lzmpiimmfzikr2qmmihfaos+m9zekqye/bjrr5iqpetd2q4fgizegqpzyrdmk7+qzzmpf9illeosswnyeritwoqmehhaxpgkzhbugwycka8aplhjolut0svwd4det84f9qxry797lnj5nv6hyfoaxqimshgz9ejeovoub8r45pxxfhu/da/m1/nn+avfdq4wpfcr5vj/vkl/01bxg==</latexit>
<latexit sha1_base64="f9zv6vf8f3p2j6etykmqmegptrc=">aaaej3icdvnbi9nafj5nvkz11tvhxw4wlxzrsurqkmkifnzbvexuowndzdppp5sbmxnpcfk3vvhxfbfurb/9j86kf5jwbwjnzvno933nkpgsgalpwb8pdppk1wvxd2/ubt66fedu/ejeuyhttmifxehmbx4wngar7usmazpiomwhf9al7/k1rl98pfywopoglwkdhxgamz8rlfxkptk6a+jcmtghljppzxa5a7edssxfu5skwhfqm7vixtbuqfcy3ru9zimbssd2pu61y2ss1qamfdakfsaie3lndfpluvwtbysa42geothizjgfz12mswxfuvz8aw5r5fcizwwzbf9qukkxmhoxl0o8yjs7crzyxxifnuluqjrfsik0ven0lq01wohwpv9nrxmek796d839odow3hqeyznjff16cwxhuwhltfklbwnwvekqkm3tiofhw0yvbxwb3hwef7vgfkoqc9hx0x5qbr1hdaziwh5gr4mgwp8zt/5nrzekiy0kcbaqq9tk5cjdxdis0lzmpiimmfzikr2qmmihfaos+m9zekqye/bjrr5iqpetd2q4fgizegqpzyrdmk7+qzzmpf9illeosswnyeritwoqmehhaxpgkzhbugwycka8aplhjolut0svwd4det84f9qxry797lnj5nv6hyfoaxqimshgz9ejeovoub8r45pxxfhu/da/m1/nn+avfdq4wpfcr5vj/vkl/01bxg==</latexit>

concat

concat

concat

concat

concat

c0

c1

c2

c3

e[<s>]

<s>

e[the]

the

e[black]

black

e[fox]

fox

c4

e[jumped]

jumped

attend

attend

attend

attend

attend

bie

bie

bie

bie

bie

e[<s>]

<s>

e[a]

a

e[conditioning]

e[sequence]

conditioning

sequence

e[</s>]

</s>

(cid:39)(cid:74)(cid:72)(cid:86)(cid:83)(cid:70) (cid:18)(cid:24)(cid:15)(cid:22)(cid:27) (cid:52)(cid:70)(cid:82)(cid:86)(cid:70)(cid:79)(cid:68)(cid:70)(cid:14)(cid:85)(cid:80)(cid:14)(cid:84)(cid:70)(cid:82)(cid:86)(cid:70)(cid:79)(cid:68)(cid:70) (cid:51)(cid:47)(cid:47) (cid:72)(cid:70)(cid:79)(cid:70)(cid:83)(cid:66)(cid:85)(cid:80)(cid:83) (cid:88)(cid:74)(cid:85)(cid:73) (cid:66)(cid:85)(cid:85)(cid:70)(cid:79)(cid:85)(cid:74)(cid:80)(cid:79)(cid:15)

(cid:85)(cid:73)(cid:66)(cid:85) (cid:74)(cid:84)(cid:13) (cid:85)(cid:73)(cid:70)(cid:90) (cid:83)(cid:70)(cid:81)(cid:83)(cid:70)(cid:84)(cid:70)(cid:79)(cid:85) (cid:66) (cid:88)(cid:74)(cid:79)(cid:69)(cid:80)(cid:88) (cid:71)(cid:80)(cid:68)(cid:86)(cid:84)(cid:70)(cid:69) (cid:66)(cid:83)(cid:80)(cid:86)(cid:79)(cid:69) (cid:85)(cid:73)(cid:70) (cid:74)(cid:79)(cid:81)(cid:86)(cid:85) (cid:74)(cid:85)(cid:70)(cid:78) xi (cid:66)(cid:79)(cid:69) (cid:79)(cid:80)(cid:85) (cid:85)(cid:73)(cid:70) (cid:74)(cid:85)(cid:70)(cid:78) (cid:74)(cid:85)(cid:84)(cid:70)(cid:77)(cid:71)(cid:15) (cid:52)(cid:70)(cid:68)(cid:80)(cid:79)(cid:69)(cid:13)

attention

    many variants of attention function

    dot product (previous slide)
    mlp
    bi-linear transformation

    various ways to combine context vector 

into decoder computation

    see luong et al. 2015

