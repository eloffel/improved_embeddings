cs5740: natural language processing

spring 2018

id6

instructor: yoav artzi

slides adapted from michael collins

the id87

    have a model !(#|%) to estimate the id203 of an 
english sentence # given a french sentence %

    goal: translate from french to english

    estimate the parameters from training corpus
    a id87 has two components:

the language model
the translation model

!(#)
!(%|#)

    giving:

and

p(e|f ) =

p(e, f )
p(f )

=

p(e)p(f|e)

pe p(e)p(f|e)

arg max

e

p(e|f ) = arg max

e

p(e)p(f|e)

overview

    ibm model 1
    ibm model 2
    em training of models 1 and 2

ibm model 1: alignments

    how do we model !(#|%)?
    english sentence % has ' words %1   %*
french sentence # has + words #1   #,
    there are ('+1)+ possible alignments

    an alignment a identifies which english 
word each french word originated from

    formally, an alignent a is:
where

{a1, . . . , am}

aj 2 0 . . . l

ibm model 1: alignments

!=6, $=7
&= and the program has been implemented
'= le programme a ete mis en application

ibm model 1: alignments

!=6, $=7
&= and the program has been implemented
'= le programme a ete mis en application

    one alignment is

{2, 3, 4, 5, 6, 6, 6}

ibm model 1: alignments

!=6, $=7
&= and the program has been implemented
'= le programme a ete mis en application

    another (bad!) alignment is

{1, 1, 1, 1, 1, 1, 1}

ibm model 1: alignments

!=6, $=7
&= and the program has been implemented
'= le programme a ete mis en application

    another (bad!) alignment is

{1, 1, 1, 1, 1, 1, 1}

alignments in the ibm models
    we define two models:

p(a|e, m)

p(f|a, e, m)

    giving:

    also:

p(f, a|e, m) = p(a|e, m)p(f|a, e, m)

where ! is a set of all possible alignments
p(f|e, m) =xa2a

p(a|e, m)p(f|a, e, m)

most likely alignments
p(f, a|e, m) = p(a|e, m)p(f|a, e, m)

    we can also calculate:

p(a|f, e, m) =

for any alignment a

p(f, a|e, m)

pa2a p(f, a|e, m)

    for a given f,e pair, can also compute the most likely 

alignment (details in notes)

    the original ibm models are rarely used for translation, 

but still key for recovering alignments 

example alignment

    french:

le conseil a rendu son avis , et nous devons    pr  sent adopter un 
nouvel avis sur la base de la premi  re position .

    english:

the council has stated its position , and now , on the basis of the 
first position , we again have to give our opinion .

    alignment:

the/le council/conseil has/   stated/rendu its/son position/avis ,/,
and/et now/pr  sent ,/null on/sur the/le basis/base of/de the/la
first/premi  re position/position ,/null we/nous again/null
have/devons to/a give/adopter our/nouvel opinion/avis ./.

ibm model 1: alignments
    in ibm model 1 all alignments a are 

equally likely:

p(a|e, m) =

1

(1 + l)m

    reasonable assumption?

    simplifying assumption, but it gets things 

started    

ibm model 1: translation 

probabilities

    next step: come up with an estimate for

p(f|a, e, m)

    in model 1, this is:

p(f|a, e, m) =

myj=1

t(fj|eaj )

ibm model 1: example

!=6, $=7
&= and the program has been implemented
'= le programme a ete mis en application

a = {2, 3, 4, 5, 6, 6, 6}

ibm model 1: example

p(f|e)
le
programme
a
ete
mis
en
application

and

the

0.2
0.05
0.1
0.05
0.2
0.25
0.01

0.6
0.2
0.1
0.05
0.05
0.1
0.03

program has

0.1
0.45
0.15
0.05
0.05
0.25
0.01

been

0.025
0.1
0.2
0.05
0.05
0.25
0.02

0.05
0.1
0.15
0.7
0.25
0.1
0.03

implemented

0.025
0.1
0.3
0.1
0.4
0.05
0.9

ibm model 1: example

!=6, $=7
&= and the program has been implemented
'= le programme a ete mis en application

a = {2, 3, 4, 5, 6, 6, 6}

p(f|a, e) =t(le|the)     t(programme|program)

    t(a|has)     t(ete|been)
    t(mis|implemented)     t(en|implemented)
    t(application|implemented) = 0.0006804

p(f, a | e, 7) = 8.26186e   10

ibm model 1: the generative 

process

to generate a french string ! from an english string ":
    step 1: pick an alignment # with id203

1

    step 2: pick the french words with id203

(l + 1)m

translating 
french  english

p(f|a, e, m) =

the final result:

myj=1

t(fj|eaj )

p(f, a|e, m) = p(a|e, m)     p(f|a, e, m) =

1

(1 + l)m

myj=1

t(fj|eaj )

example lexical entry

    de la situation au niveau des n  gociations de l   ompi    
... of the current position in the wipo negotiations ...

nous ne sommes pas en mesure de d  cider,    
we are not in position to decide    

... le point de vue de la commission face    ce probl  me complexe .
    the commission    s position on this complex problem .

overview

    ibm model 1
    ibm model 2
    em training of models 1 and 2

ibm model 2

    only difference: we now introduce alignment 

distortion parameters

q(i|j, l, m)

    id203 that j   th french word is connected to 
i   th english word, given sentence length of e and f
are l and m

    define

where
    gives

myj=1
p(a|e, m) =
a = {a1, . . . , am}
myj=1

p(f, a|e, m) =

q(aj|j, l, m)

q(aj|j, l, m)t(fj|eaj )

example

example

example

ibm model 2: the generative 

to generate a french string ! from an english string ":

process

translating 
french  english

    step 1: pick an alignment  

with id203

p(a|e, m) =

    step 2: pick the french words with id203

q(aj|j, l, m)

a = {a1, . . . , am}
myj=1
myj=1

t(fj|eaj )

p(f|a, e, m) =

the final result:

p(f, a|e, m) = p(a|e, m)     p(f|a, e, m) =

myj=1

q(aj|j, l, m)t(fj|eaj )

recovering alignments

   

if we have parameters q and t, we can easily recover the 
most likely alignment for any sentence pair

given a sentence pair 
define

e1, e2, . . . , el, f1, f2, . . . , fm

for 

aj = arg max

a2{0...l}

q(a|j, l, m)     t(fj, ea)

j = 1 . . . m
e = and the program has been implemented

f = le programme a ete mis en application

overview

    ibm model 1
    ibm model 2
    em training of models 1 and 2

the parameter estimation problem
input:
   

(e(k), f (k)), k = 1 . . . n

each e(k) is an english sentence, each f(k) is a french 
sentence

    output: parameters for
t(f|e)

q(i|j, l, m)

    a key challenge: we do not have alignments in our 

training examples
e(100) = and the program has been implemented

f(100) =  le programme a ete mis en application

parameter estimation if alignments 

are observed

    assume alignments are observed in training data
e(100) = and the program has been implemented
f(100) =  le programme a ete mis en application
a(100) = <2,3,4,5,6,6,6>

    training data is

(e(k), f (k), a(k)), k = 1 . . . n

each e(k) is an english sentence, each f(k) is a french 
sentence, each a(k) is an alignment

    maximum-likelihood parameter estimates are trivial:
tm l(f|e) =

qm l(j|i, l, m) =

count(e, f )
count(e)

count(j, i, l, m)
count(i, l, m)

p
s
e
u
d
o
c
o
d
e

 

parameter estimation with the em 

algorithm
(e(k), f (k)), k = 1 . . . n

input:
each e(k) is an english sentence, each f(k) is a french 
sentence

   

    the algorithm is related to algorithm with observed 

alignments, but with two key differences:
    iterative: start with initial (e.g., random) choice of q and t 

parameters, at each iteration: compute some    counts    base on 
data and parameters, and re-estimate parameters

    the definition of of the delta function is different:

p
s
e
u
d
o
c
o
d
e

 

p
s
e
u
d
o
c
o
d
e

 

justification for the algorithm
input:
each e(k) is an english sentence, each f(k) is a french 
sentence

(e(k), f (k)), k = 1 . . . n

   

    the log-likelihood function:

    the maximum-likelihood estimates are:

    the em algorithm will converge to a local maximum of 

the log-likelihood function

summary

    key ideas in the id6:
    alignment variables
    translation parameters, e.g., t(chien|dog)
    distortion parameters, e.g., q(2|1,6,7)
    the em algorithm: an iterative algorithm for 
training the q and t parameters
    once parameters are trained, can recover the 
most likely alignment on our training examples
e(100) = and the program has been implemented
f(100) =  le programme a ete mis en application

