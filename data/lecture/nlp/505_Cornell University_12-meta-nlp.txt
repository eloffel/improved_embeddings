cs5740: natural language processing

spring 2018

meta nlp

instructor: yoav artzi

slides inspired and adapted from sam bowman

overview
    starting your nlp project

    literature, data, evaluation, and 

hyperparameter tuning

    ethics in nlp

literature review

    do this early!
    why? 

    don   t re-invent the wheel!
    learn about common tricks, resources, and 

libraries that will make your life easier

literature review

    identifying papers:
acl anthology)

1. do a keyword search on google scholar (or the 

2. download the papers that seem most relevant
3. skim the abstracts, intros, & previous work 

sections
identify papers that look relevant, appear often, 
& have lots of citations on google scholar

4.

5. download those papers 
6. return to step 3

h/t bill maccartney

literature review
    where to find the most trustworthy papers:

    nlp: proceedings of acl conferences (acl, naacl, 

eacl, emnlp, conll, lrec), journal of computational 
linguistics, tacl, coling, arxiv*
    machine learning/ai: proceedings of nips, icml, iclr, 
aaai, ijcai, and arxiv*
    computational linguistics: journals like linguistic inquiry, 
nllt, semantics and pragmatics

    what to mention:

    general problem/task definition
    relevant methods and results
    comparisons with your work and other related work
    open issues

acquiring datasets

    find existing datasets

    acl anthology
    linguistic data consortium (ldc)

    find them in the wild

    example: ubuntu dialogue corpus, stackoverlow data 
    careful: easy to violate copyright and terms of service!

    build them

    write detailed guidelines, and work with experts
    write simple guidelines, and crowdsource

    generate them
    super easy
    controlled    help analysis
    careful: artificial data does not reflect the real world!

quantitative evaluation

   

follow prior work and use existing metrics
    new task? create a metric before you start testing. it must be independent of your 
    use ablations to study the effectiveness of your choices (and don   t adopt 

model!

fancy solutions that don   t really help)
    example:

    bilstm sentiment classifier with glove embeddings
    bilstm sentiment classifier with random embeddings
    lstm sentiment classifier with glove embeddings
    cbow classifier with glove embeddings
    cbow classifier with random embeddings

    consider controlled human evaluation when standard, and even when less 

standard
    example: summarization, machine translation
test for statistical significance when differences are small and models are 
complex

   
    consider extrinsic evaluation on downstream tasks
    negative results are informative too!

qualitative evaluation and error 

analysis

    your goal: convince that your hypothesis is correct
   

interesting hypotheses are often hard to evaluate with 
standard/intuitive quantitative metrics
    example: attention-based id4 models can learn the same 

kinds of alignments as phrase-based mt systems, but 
generalize better to unfamiliar words and phrases.

    qualitative evidence can help!
    how to start? 

    look to prior work!
    show examples of system output.
    identify qualitative categoriesof system error and count them.
    visualize your embedding spaces with tools like id167 or pca.
    visualize your hidden states with tools like lstmvis.
    plot how your model performance varies with amount of data.
    ambitious? build an online demo!

formative vs. summative 

evaluation

when the cook tastes the soup, that   s formative; when the 
customer tastes the soup, that   s summative. 
    formative evaluation: guiding further investigations

    typically: lightweight, automatic, intrinsic
    compare design option a to option b
    tune hyperparameters: smoothing, weighting, learning rate

    summative evaluation: reporting results

    compare your approach to previous approaches
    compare different majorvariants of your approach
    only use the test set here
    generally only bother with human or extrinsic evaluations here
    common mistake: don   t save all your qualitative evaluation 
for the summative evaluation!

hyperparameter tuning

    tune your baseline

    you must tune the hyperparameters of your baselines 

just as thoroughly as you tune them for any new 
model you propose!

    failure to do this invalidates your comparisons
    don   t tune on the test set!
    read the fine print while you   re doing your 
literature review to get a sense of what 
hyperparameters to worry about and what kinds 
of value to expect.
hyperparameter, you probably should.

    if you   re not sure whether to tune a 

hyperparameter tuning

    grid search: inefficient.
    bayesian optimization: optimal, but public 

packages aren   t great.

    good read: random search (bergstra and bengio

   12)    easy, and near-optimal
    define distributions over all your hyperparameters.
    sample n times for n experiments.
    look for patterns in your results.
    adjust the distributions and repeat until you run out of 

resources or performance stops improving.

ethics in data-driven models
   instead of relying on algorithms, which we can be 
accused of manipulating for our benefit, we have turned to 
machine learning, an ingenious way of disclaiming 
responsibility for anything. machine learning is like money 
laundering for bias. it's a clean, mathematical apparatus 
that gives the status quo the aura of logical inevitability. 
the numbers don't lie.    

- maciej ceg  owski

example: pitfalls in word 

embeddings
occupations most similar to she:

nurse, librarian, nanny, stylist, dancer

occupations most similar to he:

architect, captain, philosopher, legend, hero

impossible to avoid these issues altogether 
when learning from naturally occurring text. 

source: bolukbasi et al.    16, quantifyiang and reducing stereotypes in id27s

data and biases

    deploying biased models in the wrong places can 
lead to harms far worse than bad user experiences
    r  sum   screening, exam scoring, predictive policing...

    some ml techniques can amplify biases in data

    zhao et al. 2017 on multi-label image classifiers:

    in training data, women appear in cooking scenes 33% more 
    in model   s labeling of similar test data, women are detected in 

often than men
cooking scenes 68% more often than men

    model de-biasing can be complex, political, and 

maybe even impossible to do fully
        and it may harm performance on reasonable metrics.

data and exclusion
    colloquial african-american english isn   t well 

represented in training data for language 
identification, parsing, etc., 
    so technologies like translation and intelligent assistants 
    result: users are forced to choose between avoiding their 

aren   t as usable for its speakers
preferred dialect and missing out on the benefits of the 
technology
    similar situation for english varieties in singapore, 

india, caribbean, etc., and for regional/minority 
languages in general

see blodgett and o   connor    17

talk about the data

    when discussing your models, be as clear as you can 
about:
    what your data looks like, why it was collected, and what 
    who (country, region, gender, native language, etc.) 
    any known biases in your dataset (including the obvious 

kind of information your system learns from it
produced the text and labels in your dataset
ones)

    especially important when writing for nontechnical 
    hard: when possible, build useful confidence metrics

potential users/clients

    make it clear to the user when the system is out of its 

comfort zone

nlp and intentional harm
    nlp technologies can also be used to cause 

intentional harms
    surveillance by repressive states
    generation of effective email spam/fake reviews
    suppression of minority dialects/languages

    most nlp technologies have some potential to do 
harm    be aware of what they are, and do what 
you can to avoid advancing those use cases
    if you   re presented with a clearly unethical (or 

illegal) application: quit. there are plenty of other 
funders/employers out there

