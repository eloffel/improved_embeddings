cs5740: natural language processing

spring 2018

text classification

instructor: yoav artzi

slides adapted from dan klein, dan jurafsky, chris manning, 
michael collins, luke zettlemoyer, yejin choi, and slav petrov

overview

    classification problems

    supervised learning

    spam vs. non-spam, text genre, word 

sense, etc.
   na  ve bayes
   id148 (maximum id178 
models)
   weighted linear models and the 
id88
   neural networks

supervised learning: data

    learning from annotated data
    often the biggest problem
    why?

reality 
check

    annotation requires specific expertise
    annotation is expensive
    data is private and not accessible
    often difficult to define and be consistent

    before fancy models     always think about the 

data, and how much of it your model needs

held-out data
    important tool for estimating 

generalization:
    train on one set, and evaluate during 

development on another
    test data: only use once!

training data

development 

data

held-out test 

data

classification

    automatically make a decision about inputs

    example: document    category
    example: image of digit    digit
    example: image of object    object type
    example: query + webpage    best match
    example: symptoms    diagnosis
       

    three main ideas:

    represenation as feature vectors
    scoring by linear functions
    learning by optimization

example: spam filter

input: email

   
    output: spam/ham
    setup:

    get a large collection of example 
emails, each labeled    spam    or 
   ham   
    note: someone has to hand label 
all this data!
    goal: learn to predict labels of 
new, future emails
    features: the attributes used to 
make the ham / spam decision
    words: free!
    text patterns: $dd, caps
    non-text: senderincontacts
       

dear sir.

first, i must solicit your confidence in this 
transaction, this is by virture of its nature 
as being utterly confidencial and top 
secret.    

to be removed from future 
mailings, simply reply to this 
message and put "remove" in the 
subject.

99  million email addresses
for only $99

ok, iknow this is blatantly ot but i'm 
beginning to go insane. had an old dell 
dimension xps sitting in the corner and 
decided to put it to use, i know it was 
working pre being stuck in the corner, but 
when i plugged it in, hit the power nothing 
happened.

more examples

    variants of spam
    comments, email

    abuse and harassment
    fake reviews
    simple intent (   siri, what is the time?   )
    topic (for news) and domain (for papers)
    medical

    disease, surgical procedure, insurance code

   

   

probabilistic classifiers

two broad approaches to predicting a class y* 

joint / generative (e.g., na  ve bayes)
    work with a jointprobabilistic model of the data
    assume functional form for p(x|y), p(y)
    estimate probabilities from data
    use bayes    rule to calculate p(y|x)
    e.g., represent p(y,x) as na  ve bayes model, compute y*=argmaxy
    advantages: learning weights is easy and well understood

p(y,x)= argmaxy p(y)p(x|y)

    conditional / discriminative (e.g., id28)

    work with conditionalid203 p(y|x)
    we can then directly compute y* = argmaxy p(y|x)
    estimate parameters from data
    advantages: don   t have to model p(x)! can develop feature rich

models for  p(y|x)

text categorization

    goal: classify documents into broad semantic topics

obama is hoping to rally support 
for his $825 billion stimulus 
package on the eve of a crucial 
house vote. republicans have 
expressed reservations about 
the proposal, calling for more tax 
cuts and less spending. gop 
representatives seemed doubtful 
that any deals would be made.

california will open the 2009 
season at home against 
maryland sept. 5 and will play a 
total of six games in memorial 
stadium in the final football 
schedule announced by the 
pacific-10 conference friday. 
the original schedule called for 
12 games over 12 weekends. 

    which one is the politics document? (and how much deep processing did 

that decision take?)
first approach: bag-of-words and na  ve-bayes models

   
    more approaches later   
    usually begin with a labeled corpus containing examples of each class

    input:

general text classification

    document !	of length |!|	is a sequence of 
    one of $ labels %	

x = hx1, . . . , x|x|i

tokens:

    output:

na  ve-bayes models
    generative model: pick a topic, then 

generate a document

    na  ve-bayes assumption: 

    all words are independent given the topic.

p(y, x) = q(y)

y

|x|yi=1

q(xi | y)

x1

x2

. . .

xn

using nb for classification
    we have a joint model of topics and documents

|x|yi=1

q(xi | y)

p(y, x) = q(y)

    to assign a label !    to a new document 
   $%,$',   ,$)   :
|x|yi=1

p(y, x) = arg max

y    = arg max

q(y)

y

y

q(xi | y)

numerical/speed issues?

learning: maximum likelihood 

estimate (id113)

p(y, x) = q(y)
    parameters to estimate:

|x|yi=1
   !" =$% for each topic "	
   ! ' " =$(% for each topic "	and word '

q(xi | y)

    data:

    objective:

arg max

   

nyj=1

j=1

{(x (j), y(j))}n
nyj=1

   

p(y(j), x (j)) = arg max

q(y(j))

|x (j)|yi=1

q(xi | y(j))

p(y, x) = q(y)

id113
|x|yi=1
q(xi | y)
nyj=1

q(y(j))

   

arg max

   

nyj=1

p(y(j), x (j)) = arg max

    how do we do learning? we count!

|x (j)|yi=1

q(xi | y(j))

q(y) =    y =

c(y)

n

q(x | y) =    xy =

c(x, y)
c(y)

learning complexity?

sparsity issues?

word sparsity

q(y) =    y =

c(y)

n

q(x | y) =    xy =

c(x, y)
c(y)

    we have a joint model of topics and documents

using nb for classification
    to assign a label !    to a new document    $%,$',   ,$)   :
    we get + $, ! =0 when /$,,! =0
|x|yi=1

    solution: smoothing + accounting for unknowns

p(y, x) = arg max

y    = arg max

q(xi | y)

p(y, x) = q(y)

q(xi | y)

|x|yi=1

q(y)

y

y

    more when we discuss language models

example: word-sense 

disambiguation

    example: 

    living plant vs. manufacturing plant

    how do we tell these senses apart?
the plant which had previously sustained the town   s economy 

       context   

shut down after an extended labor strike. the plants at the 

entrance, dry and wilted, the first victims of    
    it   s just text categorization! (at the word level)
    each word sense represents a topic

case study: word senses

    words have multiple distinct meanings, or senses:

    plant: living plant, manufacturing plant,    
    title: name of a work, ownership document, form of address, material at the start of 

a film,    

    many levels of sense distinctions

   

river bank, money bank

    homonymy: totally unrelated meanings 
    polysemy: related meanings 
    systematic polysemy: productive meaning extensions or metaphor
    sense distinctions can be extremely subtle (or not)

    metonymy such as organizations to their buildings

star in sky, star on tv

   

    granularity of senses needed depends a lot on the task
    why is it important to model word senses?

    translation, parsing, information retrieval

android 
vs. apple

id51

    example: living plant vs. manufacturing plant
    how do we tell these senses apart?

       context   

the plant which had previously sustained the town   s economy 

shut down after an extended labor strike. the plants at the 

entrance, dry and wilted, the first victims of    

    maybe it   s just text categorization
    each word sense represents a topic
    run a na  ve-bayes classifier?

    bag-of-words classification works ok for noun senses

    90% on classic, shockingly easy examples (line, interest, star)
    80% on senseval-1 nouns
    70% on senseval-1 verbs

verb wsd

    why are verbs harder?

    verbal senses less topical
    more sensitive to structure, argument choice

    verb example:    serve   

    [function] the tree stump serves as a table
    [enable] the scandal served to increase his popularity
    [dish] we serve meals for the homeless
    [enlist] she served her country
    [jail] he served six years for embezzlement
    [tennis] it was agassi's turn to serve
    [legal] he was served by the sheriff

better features

    there are smarter features:

    argument selectional preference:

    serve np[meals] vs. serve np[papers] vs. serve np[country]

    sub-categorization:

   
   
   
   

[function] serve pp[as]
[enable] serve vp[to]
[tennis] serve <intransitive>
[food] serve np {pp[to]}

    can be captured poorly (but robustly) with modified na  ve bayes 

approach 

    other constraints (yarowsky 95)

    one-sense-per-discourse (only true for broad topical distinctions)
    one-sense-per-collocation (pretty reliable when it kicks in: 

manufacturing plant, flowering plant)

complex features with nb

    example:

washington county jail served 11,166 meals last 
month - a figure that translates to feeding some 
120 people three times daily for 31 days. 

    so we have a decision to make based on a set of cues:

    context:jail, context:county, context:feeding,    
    local-context:jail, local-context:meals
    subcat:np, direct-object-head:meals

    not clear how build a generative derivation for these:
    choose topic, then decide on having a transitive usage, then 

pick    meals    to be the object   s head, then generate other 
words?

    how about the words that appear in multiple features?
    hard to make this work (though maybe possible)
    no real reason to try

where we are?

    so far: na  ve bayes models for classification

    assumption: features are independent given the 

    generative models, estimating !(#   %) and !(%)
    estimating !(%   #) directly

    easy to estimate (just count!)
    next: discriminative models

label (often violated in practice)

    very flexible feature handling
    require numerical optimization methods

a discriminative approach
    view wsd as a discrimination task, directly estimate:

p(sense | context:jail, context:county, 

context:feeding,    
local-context:jail, local-context:meals
subcat:np, direct-object-head:meals,    .)

    have to estimate multinomial (over senses) where there 

are a huge number of things to condition on

    many feature-based classification techniques out there

    discriminative models extremely popular in the nlp 

community!

feature representations

washington county jail served
11,166 meals last month - a 
figure that translates to feeding 
some 120 people three times 
daily for 31 days. 

    features are indicator functions  
which count the occurrences of 
certain patterns in the input

input ! and class "

initially: we will have different 
feature values for every pair of 

   

context:jail = 1
context:county = 1 
context:feeding = 1
context:game = 0

   

local-context:jail = 1
local-context:meals = 1

   

object-head:meals = 1
object-head:ball = 0

example: text classification

    goal: classify document to categories

    win the election    
    win the game    
    see a movie    

politics
sports
other

    classically: based on words in the document
    but other information sources are potentially relevant:

    document length
    average word length
    document   s source
    document layout

some notation

input

output space

output

true output

x (j)
y
y

    win the election ...

sports, politics, other

sports

y(j)

politics

feature 
vector

 (x (j), y)

[1 0 1 0 0 0 0 0 0 0 0 0]

sports+   win   

politics+   win   

block feature vectors
    input has features, which are multiplied by 

outputs to form the candidates

    win the election    

(cid:1)win(cid:2)

(cid:1)election(cid:2)

 (x, sp ort s) = [1 0 1 0 0 0 0 0 0 0 0 0]
 (x, p olit ics) = [0 0 0 0 1 0 1 0 0 0 0 0]
 (x, ot her) = [0 0 0 0 0 0 0 0 1 0 1 0]

non-block feature vectors
sentence !

    sometimes the features of candidates cannot be 
    example: a parse tree   s features may be the rules used for 
s
np

decomposed in this regular way

vp

s

 (x,

np
n    n

) = [1 0 1 0 1]
vp
v

s

 (x,

np
vp
n v    n

) = [1 1 0 1 0]

    different candidates will often share features
    we   ll return to the non-block case later

np
n    n
vp
v

np
n
vp
v    n

linear models: scoring

    in a linear model, each feature gets a weight in w
 (x, sp ort s) = [1 0 1 0 0 0 0 0 0 0 0 0]
 (x, p olit ics) = [0 0 0 0 1 0 1 0 0 0 0 0]
w = [ 1 1  1  2 1  1 1  2  2  1  1 1]

    we compare !   s on the basis of their linear 

scores:

score(x, y; w) = w>     (x, y)

score(x, p olit ics; w) = 1     1 + 1     1 = 2

linear models: prediction rule
w = [ 1 1  1  2 1  1 1  2  2  1  1 1]
    the linear prediction rule:

prediction(x, w) = arg max
y2y

w> (x, y)

 (x, sports) = [1 0 1 0 . . . ]
 (x, politics) = [. . . 1 0 1 0 . . . ]
 (x, other) = [. . . 1 0 1 0]

score(x, sports, w) = 1     1 + ( 1)     1 = 0
score(x, politics, w) = 1     1 + 1     1 = 2

score(x, other, w) = ( 2)     1 + ( 1)     1 =  3

prediction(x, w) = politics

na  ve-bayes as a linear model
    (multinomial) na  ve-bayes is a linear model:

 (x, y) = [. . . 0 . . . ,1,

w =

[. . . . . . , log p (y),

#v1,
log p (v1|y),
score(x, y, w) = w> (x, y)

#v2,
log p (v2|y),

. . . ,#vn,
. . . , log p (vn|y),

. . . ]
. . . ]

#vk log p (vk|y)

y

x1

x2

. . .

xn

= log p (y) +xk
= log(p (y)yk
= log(p (y) yd2xi

= log p (x, y)

p (vk|y)#vk )
p (d|y))

    goal: choose    best    vector ! given training data
how to pick weights?

    best for classification

    the ideal: the weights which have greatest test set 

accuracy / f1 / whatever
    but, don   t have the test set
    must compute weights from training set

    maybe we want weights which give best training set 

accuracy?
    hard optimization problem
    may not (does not) generalize to test set

    easy to overfit

maximum id178 models 

    learning: maximize the (log) conditional likelihood of training 

(maxent)
    maximum id178 (id28)
    model: use the scores as probabilities:
exp (w     (x, y))

p(y|x; w) =

i=1

py0 exp (w     (x, y0))
nxi=1
nyi=1

p(y(i)|x (i); w) =
l(w)

data

{(x (i), y(i))}n

l(w) = log

w    = arg max

w

    prediction:

y    = arg max

p(y | x; w)

y

make positive
normalize

log p(y(i)|x (i); w)

unconstrained optimization
    unfortunately, !"#$!%&	((&) doesn   t have a close formed solution
nxi=1
w    = arg max

log p (y(i) | x (i); w)

    the maxent objective is an unconstrained optimization problem

l(w) =

w

l(w)

    basic idea: move uphill from current guess
    gradient ascent / descent follows the gradient incrementally
    at local optimum, derivative vector is zero
    will converge if step sizes are small enough, but not efficient
    all we need is to be able to evaluate the function and its derivative

unconstrained optimization
    once we have a function !, we can find a local optimum by 

iteratively following the gradient

    for convex functions:

    basic gradient ascent isn   t very efficient, but there are simple 

    a local optimum will be global
    does this mean that all is good?
enhancements which take into account previous gradients: 
conjugate gradient, l-bfgs
iterative scaling, but they aren   t better

    there are special-purpose optimization techniques for maxent, like 

derivative of the maxent objective
py0 ew   (x,y0)

log p(y(i) | x (i); w)

p(y | x; w) =

    some necessities:

nxi=1

ew   (x,y)

l(w) =

w     (x, y) = w1      1(x, y) + w2      2(x, y) +        + wn      n(x, y)

@
@x
@
@x

logau =

logeu =

1

u loge a

1

u loge e

@
@x
@
@x

u

u =

1
u

@
@x

u

@
@x

eu = eu @
@x

u

derivative of the maxent objective
py0 ew   (x,y0)

log p(y(i) | x (i); w)

p(y | x; w) =

nxi=1

ew   (x,y)

@
@wj

@
@wj

l(w) =

l(w) =

log

log p (y(i)|x (i); w)
ew   (x (i),y(i))
py0 ew   (x (i),y0)

nxi=1
nxi=1
nxi=1    log ew   (x (i),y(i))   logxy0
nxi=1   w     (x (i), y(i))   logxy0

@
@wj

@
@wj

@
@wj

ew   (x (i),y0)   
ew   (x (i),y0)   

1

nxi=1    j(x (i), y(i))  
nxi=1    j(x (i), y(i))  xy0
nxi=1    j(x (i), y(i))  xy0

py0 ew   (x (i),y0)xy0
py00 ew   (x (i),y00)
p (y0|x (i); w) j(x (i), y0)   

ew   (x (i),y0)

ew   (x (i),y0) j(x (i), y0)   
 j(x (i), y0)   

=

=

=

=

=

=

l(w) =

nxi=1

derivative of the maxent objective
py0 ew   (x,y0)
p (y0|x (i); w) j(x (i), y0)   

nxi=1    j(x (i), y(i))  xy0

log p(y(i) | x (i); w)

p(y | x; w) =

ew   (x,y)

l(w) =

@
@wj

total count of feature j 
in correct candidates

expected count of 
feature j in predicted 

candidates

expected counts

    the optimum parameters are the ones for 

which each feature   s predicted expectation 
equals its empirical expectation 

@
@wj

l(w) =

nxi=1    j(x (i), y(i))  xy0

p (y0|x (i); w) j(x (i), y0)   

context-word:jail+cat:prison

context-word:jail+cat:food

1.0

0.0

actual 
counts

+0.3

-0.3

0.7

0.3

empirical 
counts

a very nice objective
    the maxent objective behaves nicely:
    differentiable (so many ways to optimize)
    convex (so no local optima)

convexity guarantees a single, global maximum value because 

any higher points are greedily reachable

what about overfitting?
    for na  ve bayes, we were worried about zero 

counts in id113 estimates
    can that happen here?

    id173 (smoothing) for id148
    instead, we worry about large feature weights
    add a l2 id173 term to the likelihood to push 

weights towards zero

l(w) =

nxi=1

log p(y(i)|x (i); w)  

 
2||w||2

formed solution

derivative of the regularized 

    we will have to differentiate and use gradient ascent

maxent objective

    unfortunately, !"#$!%&	((&) still doesn   t have a close 
exp(w     (x (i), y))!  
p(y|x (i); w) j(x (i), y)!    wj

nxi=1 w     (x (i), y(i))   logxy
nxi=1  j(x (i), y(i))  xy

 
2||w||2

l(w) =

l(w) =

@
@wj

total count of feature j 
in correct candidates

expected count of 
feature j in predicted 

candidates

big weights 

are bad

example: ner id173

because of id173, 
the more common prefixes 
have larger weights even 
though entire-word features 
are more specific

local context

prev
at
in
x

cur
next
grace road
nnp
nnp
xx
xx

word
tag
sig

feature type
previous word
current word
beginning bigram
current pos tag
prev and cur tags
current signature
prev-cur-next sig
p. state - p-cur sig
   
total:

at

grace

feature

feature weights
pers
-0.73
0.03
0.45
0.47
-0.10
0.80
-0.69
-0.20

gr
nnp
in nnp

x-xx-xx
o-x-xx

xx

loc
0.94
0.00
-0.04
0.45
0.14
0.46
0.37
0.82

-0.58

2.68

(often) local conditional probabilities

    two probabilistic approaches to predicting classes y* 

    joint: work with a jointprobabilistic model of the data, weights are 

learning classifiers
    e.g., represent p(y,x) as na  ve bayes model, compute !   =$%&'$(!	*(!,-)
    conditional: work with id155 *(!   -)
    we can then direct compute !   	=	$%&'$(!	*(!   -) can develop feature 
richmodels for  *(!   -).
    linear predictor: !   	=	$%&'$(!	0   2(-,!)

    but, why estimate a distribution at all?

    id88 algorithm

    online (or batch)
    error driven
    simple, additive updates

    the id88 algorithm

    iteratively processes the training set, reacting to training errors
    can be thought of as trying to drive down training error

id88 learning
    the online (binary   !=  1) id88 algorithm:
    visit training instances (&',!(')) one by one, until all correct
    if correct (!   ==!(')): no change, goto next example!
y    = sign(w     (x (i)))

    start with zero weights

    make a prediction

    if wrong: adjust weights

w = w   y    (x (i))

published: july 8, 1958

copyright    the new york times

july 8, 1958

two simple examples

data set i:

x (1) = [1, 1], y(1) = 1
x (2) = [1, 1], y(2) = 1
x (3) = [ 1, 1], y(3) =  1

data set ii:

x (1) = [1, 1], y(1) = 1
x (2) = [1, 1], y(2) = 1
x (3) = [ 1, 1], y(3) =  1
x (4) = [0.25, 0.25], y(4) =  1

geometric interpretation

    the id88 finds a separating 

hyperplane

x (1) = [1, 1], y(1) = 1
x (2) = [1, 1], y(2) = 1
x (3) = [ 1, 1], y(3) =  1
w = [1, 1]
finding the hyperplane:

x (3)

w    [x, y] = 1     x + 1     y = 0

x (1)

x (2)

geometric interpretation ii

    start with zero weights
    visit training instances 

(x(i),y(i)) one by one, until 
all correct
    make a prediction
y    = sign(w     (x (i)))
    if correct (y*==y(i)): no 
change, goto next 
example!

    if wrong: adjust weights
w = w   y    (x (i))

w
 y        (x (i))

 (x (i))

w0

y    = 1, y(i) =  1

<latexit sha1_base64="hgpbwsbmfptksmrfjjd0wuz6kw8=">aaab/xicbvdlssnafl2pr1pfuxhlzraivbqkiqgloejgzqvjc21ajtnjo3tyygyihfdwv9y4uhhrf7jzb5y2waj1wiuz59zl3hu8mdoplovlkmznlywufzdlk6tr6xvm5ta9jbjbqemihommhyxllksoyortziwodjxog97weuw3hqiqlarvvbptn8d9kpmmykwlrrmtdg7rjbkpunrjkuxgpb/hdtcsw1vrajrl7jyuiue9a362exfjahoqwrgulduklzthorjhdfrqj5lgmaxxn7y0dxfapztn1h+hfa30kb8jxafce/xnriydkdpa050bvgp51xul/3mtrpnnbsbcofe0jnop/iqjfafxfqjhbcwkp5pgipjefzebfpgonvhjh2d/pxmwocfvi6p1e1quxevpfgex9qacnpxbdw6gdg4qyoajxudvedsejtfjfdpampkzbfgf4+mbuaesrw==</latexit>
<latexit sha1_base64="hgpbwsbmfptksmrfjjd0wuz6kw8=">aaab/xicbvdlssnafl2pr1pfuxhlzraivbqkiqgloejgzqvjc21ajtnjo3tyygyihfdwv9y4uhhrf7jzb5y2waj1wiuz59zl3hu8mdoplovlkmznlywufzdlk6tr6xvm5ta9jbjbqemihommhyxllksoyortziwodjxog97weuw3hqiqlarvvbptn8d9kpmmykwlrrmtdg7rjbkpunrjkuxgpb/hdtcsw1vrajrl7jyuiue9a362exfjahoqwrgulduklzthorjhdfrqj5lgmaxxn7y0dxfapztn1h+hfa30kb8jxafce/xnriydkdpa050bvgp51xul/3mtrpnnbsbcofe0jnop/iqjfafxfqjhbcwkp5pgipjefzebfpgonvhjh2d/pxmwocfvi6p1e1quxevpfgex9qacnpxbdw6gdg4qyoajxudvedsejtfjfdpampkzbfgf4+mbuaesrw==</latexit>
<latexit sha1_base64="hgpbwsbmfptksmrfjjd0wuz6kw8=">aaab/xicbvdlssnafl2pr1pfuxhlzraivbqkiqgloejgzqvjc21ajtnjo3tyygyihfdwv9y4uhhrf7jzb5y2waj1wiuz59zl3hu8mdoplovlkmznlywufzdlk6tr6xvm5ta9jbjbqemihommhyxllksoyortziwodjxog97weuw3hqiqlarvvbptn8d9kpmmykwlrrmtdg7rjbkpunrjkuxgpb/hdtcsw1vrajrl7jyuiue9a362exfjahoqwrgulduklzthorjhdfrqj5lgmaxxn7y0dxfapztn1h+hfa30kb8jxafce/xnriydkdpa050bvgp51xul/3mtrpnnbsbcofe0jnop/iqjfafxfqjhbcwkp5pgipjefzebfpgonvhjh2d/pxmwocfvi6p1e1quxevpfgex9qacnpxbdw6gdg4qyoajxudvedsejtfjfdpampkzbfgf4+mbuaesrw==</latexit>

geometric interpretation

    the id88 finds a separating 

hyperplane
x (1) = [1, 1], y(1) = 1
x (2) = [1, 1], y(2) = 1
x (3) = [ 1, 1], y(3) =  1
x (4) = [0.25, 0.25], y(4) =  1

w = [0, 0]
w = [1, 1]
w = [0.75, 0.75]
w = [0.5, 0.5]
w = [0.25, 0.25]

x (1)

x (4)

x (2)

x (3)

is there a separating hyperplane?

adding bias

    decision rule:

y    = sign(w     (x (i)) + b)

    algorithm stays the same!
    only difference: dummy always-on feature
x (1) = [1, 1], y(1) = 1
x (2) = [1, 1], y(2) = 1
x (3) = [ 1, 1], y(3) =  1
w = [0, 0] 2 r2

x (1) = [1, 1, 1], y(1) = 1
x (2) = [1, 1, 1], y(2) = 1
x (3) = [1, 1, 1], y(3) =  1
w = [0, 0, 0] 2 r3

simple example with bias

data set:

x (1) = [1, 1], y(1) = 1
x (2) = [1, 1], y(2) = 1
x (3) = [ 1, 1], y(3) =  1
x (4) = [0.25, 0.25], y(4) =  1

separable case

multiclass id88

    if we have multiple classes:
    a weight vector for each class:

wy

    score (activation) of a class y:

    prediction highest score wins

wy     (x)
y    = arg max

wy     (x)

y

w2     (x)
biggest
w2

w1

w1     (x)
biggest

w3
w3     (x)
biggest

    start with zero weights

multiclass id88
    visit training instances ("#,%(#)) one by one
wy   
    if correct (%   ==%(#)): no change, continue!
wy     (x (i))

y    = arg max

    make a prediction

y

    if wrong: adjust weights

 (x)

wyi

wy(i) = wy(i) +  (x (i))
wy    = wy       (x (i))

multiclass id88: rewrite
    compare all possible outputs

w     (x, y1)

biggest

    highest score wins
    approximate visualization 

(usually hard)

w     (x, y2)

biggest

w     (x, y3)

biggest

y    = arg max

y

w     (x, y)

id88 learning

    start with zero weights
    visit training instances (x(i),y(i)) one by one
w     (x (i), y)

y    = arg max

    make a prediction

y

    if correct (y*==y(i)): no change, goto next example!
    if wrong: adjust weights

w = w +  (x (i), y(i))    (x (i), y   )

from maxent to the id88
    prediction:
    update:

w = w +  (x (i), y(i))    (x (i), y   )

w     (x (i), y)

y    = arg max

y

    maxent gradient for xi:

@
@wj

l(w) =  j(x (i), y(i))  xy0
     j(x (i), y(i))    j(x (i), y   )
where y    = arg max

w     j(x (i), y)

y

approximate 
expectation 
with max!

expectation
p (y0|x(i); w) j(x(i), y0)

id88 learning

separable

    no counting or computing 
probabilities on training set
    separability: some parameters get 
the training set perfectly correct
    convergence: if the training is 
separable, id88 will 
eventually converge
number of mistakes (binary case) 
related to the margin or degree of 
separability

    mistake bound: the maximum 

non-separable

problems with the id88
    noise: if the data isn   t 

separable, weights might 
thrash
    averaging weight vectors 

over time can help 
(averaged id88)

    mediocre generalization: 

    overtraining: test / held-

finds a    barely    
separating solution
out accuracy usually 
rises, then falls
    overtraining is a kind of 

overfitting

three views of classification

    na  ve bayes:

    parameters from data statistics
    parameters: probabilistic interpretation
    training: one pass through the data

    id148:

    parameters from gradient ascent
    parameters: linear, probabilistic model, 
    training: gradient ascent (usually batch), 

and discriminative
regularize to stop overfitting

    the id88:

    parameters from reactions to mistakes
    parameters: discriminative interpretation
    training: go through the data until 

validation accuracy maxes out

training

data

development

data

held-out

data

a note on features: tf/idf

    more frequent terms in a document are more important:

fij = frequency of term i in document j

    may want to normalize term frequency (tf)by dividing by the 

frequency of the most common term in the document:

    terms that appear in many differentdocuments are lessindicative:

tfij = fij/maxi{fij}

dfi = document frequency of term i = number of documents containing term i

idfi = inverse document frequency of term i = log2(n/dfi)

n = total number of documents

    an indication of a term   s discrimination power
    log used to dampen the effect relative to tf
    a typical combined term important indicator is tf-idfweighting

wij = tfijidfi = tfij log2(n/dfi)

