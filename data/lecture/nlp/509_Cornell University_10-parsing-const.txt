cs5740: natural language processing

spring 2018

constituency parsing

instructor: yoav artzi

slides adapted from dan klein, dan jurafsky, chris manning, 
michael collins, luke zettlemoyer, yejin choi, and slav petrov

overview

    the constituency parsing problem
    cky parsing

    chomsky normal form

    the id32

constituency (phrase structure) 

trees

    phrase structure organizes words into 

nested constituents

constituency (phrase structure) 

trees

    phrase structure organizes words into 

nested constituents

    linguists can, and do, argue about details

constituency tests

    distribution: a constituent behaves as a 
unit that can appear in different places:
    john talked to the children about drugs.
    john talked [to the children] [about drugs].
    john talked [about drugs] [to the children].
    *john talked drugs to the children about

constituency tests

    distribution / movement / dislocation
    substitution by pro-form

    he, she, it, they, ...
    question / answer
    deletion
    conjunction / coordination

constituency (phrase structure) 

trees

    phrase structure organizes words into 

nested constituents

    linguists can, and do, argue about details
    lots of ambiguity

s

vp

np

np

n(cid:1)

pp

np

new art critics write reviews with computers

context-free grammars (id18)
    writing parsing rules:

    n    fed
    v    raises
    np    n
    s    np vp
    vp    v np
    np    n n
    np    np pp
    n    interest
    n    raises

context-free grammars

    a context-free grammar is a tuple <n,    , s, r>

    n : the set of non-terminals

    phrasal categories: s, np, vp, adjp, etc.
    parts-of-speech (pre-terminals): nn, jj, dt, vb

       : the set of terminals (the words)
    s : the start symbol

    often written as root or top
    not usually the sentence non-terminal s     why not?

    of the form x     y1 y2     yn, with x     n, n   0, yi    (n       )

    r : the set of rules

    examples: s     np vp,   vp     vp cc vp
    also called rewrites, productions, or local trees

example grammar
a context-free grammar for english

n = {s, np, vp, pp, dt, vi, vt, nn, in}
s = s
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
r = s     np vp
vt     saw
vp     vi
nn     man
vp     vt np
nn     woman
vp     vp pp
nn     telescope
np     dt nn
dt     the
np     np pp
in     with
pp     in np
in     in

s=sentence, vp-verb phrase, np=noun phrase, pp=prepositional phrase,
note:
s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun, in=preposition
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,
in=preposition

13

a context-free grammar for english

example parse

n = {s, np, vp, pp, dt, vi, vt, nn, in}
s = s
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
r = s     np vp
vt     saw
vp     vi
nn     man
vp     vt np
nn     woman
vp
vp     vp pp
nn     telescope
np     dt nn
dt
vi
dt     the
n = {s, np, vp, pp, dt, vi, vt, nn, in}
np     np pp
the man sleeps     
in     with
pp     in np
in     in
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
vt     saw
nn     man
s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
note:
nn     woman
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,
nn     telescope
in=preposition
dt     the
in     with
in     in

nn

np

13

s

s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,

s=sentence, vp-verb phrase, np=noun phrase, pp=prepositional phrase,
dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun, in=preposition

a context-free grammar for english

example parse

n = {s, np, vp, pp, dt, vi, vt, nn, in}
s = s
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
r = s     np vp
vt     saw
vp     vi
nn     man
vp     vt np
nn     woman
vp
vp     vp pp
nn     telescope
np     dt nn
dt
vi
dt     the
n = {s, np, vp, pp, dt, vi, vt, nn, in}
np     np pp
the man sleeps     
in     with
pp     in np
in     in
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
vt     saw
nn     man
s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
note:
nn     woman
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,
nn     telescope
in=preposition
dt     the
in     with
in     in

nn

np

s

13
the man saw the woman with the telescope 

s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,

s=sentence, vp-verb phrase, np=noun phrase, pp=prepositional phrase,
dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun, in=preposition

a context-free grammar for english

example parse

n = {s, np, vp, pp, dt, vi, vt, nn, in}
s = s
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
r = s     np vp
vt     saw
vp     vi
nn     man
vp     vt np
nn     woman
vp
vp     vp pp
nn     telescope
np     dt nn
dt
vi
dt     the
n = {s, np, vp, pp, dt, vi, vt, nn, in}
np     np pp
the man sleeps     
in     with
pp     in np
in     in
   = {sleeps, saw, man, woman, telescope, the, with, in}
vi     sleeps
vp
vt     saw
nn     man
s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
note:
nn     woman
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,
nn     telescope
in=preposition
np
dt     the
in     with
in     in

nn

np

np

np

dt

vp

pp

in

vt

s

s

nn

dt
13
the man saw the woman with the telescope 

nn

nn

dt

s=sentence, vp=verb phrase, np=noun phrase, pp=prepositional
phrase, dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun,

s=sentence, vp-verb phrase, np=noun phrase, pp=prepositional phrase,
dt=determiner, vi=intransitive verb, vt=transitive verb, nn=noun, in=preposition

headed phrase structure
    in nlp, id18 non-terminals often have 
    phrases are headed by particular word types 

internal structure
with some modifiers:
    vp        vb*    
    np        nn*    
    adjp        jj*    
    advp        rb*    

    this x-bar theory grammar (in a nutshell)
    this captures a dependency

pre 1990 (   classical   ) nlp parsing
    wrote symbolic grammar (id18 or often richer) and lexicon

s    np vp
np    (dt) nn
np    nn nns
np    nnp
vp    v np

nn    interest
nns    rates
nns    raises
vbp    interest

vbz    rates

    used grammar/proof systems to prove parses from words
    this scaled very badly and didn   t give coverage. for 

sentence:
fed raises interest rates 0.5% in effort to control inflation
    minimal grammar:
    simple 10 rule grammar:
    real-size broad-coverage grammar:  millions of parses

36 parses
592 parses

ambiguities: pp attachment
the children ate the cake with a spoon.

attachments

    i cleaned the dishes from dinner

    i cleaned the dishes with detergent

    i cleaned the dishes in my pajamas

    i cleaned the dishes in the sink

syntactic ambiguity i

    prepositional phrases:

    particle vs. preposition:
    complement structures

they cooked the beans in the pot on the stove with 
handles. 
the puppy tore up the staircase. 
the tourists objected to the guide that they couldn   t 
hear.
she knows you like the back of her hand. 
    gerund vs. participial adjective
visiting relatives can be boring.
changing schedules frequently confused 
passengers. 

syntactic ambiguity ii

    modifier scope within nps

impractical design requirements
plastic cup holder 

    multiple gap constructions
the chicken is ready to eat.
the contractors are rich enough to sue. 

    coordination scope:

small rats and mice can squeeze into 
holes or cracks in the wall. 

classical nlp parsing:

the problem and its solution

    categorical constraints can be added to grammars to 

limit unlikely/weird parses for sentences
    but the attempt makes the grammars not robust

    in traditional systems, commonly 30% of sentences in even an 

edited text would have noparse.

    a less constrained grammar can parse more 

no way to choose between them

sentences
    but simple sentences end up with ever more parses with 
    we need mechanisms that allow us to find the most 
likely parse(s) for a sentence
    statistical parsing lets us work with very loose grammars 
that admit millions of parses for sentences but still quickly 
find the best parse(s)

the rise of annotated data:
the id32 (ptb)

( (s

(np-sbj (dt the) (nn move))
(vp (vbd followed)
(np
(np (dt a) (nn round))
(pp (in of)
(np
(np (jj similar) (nns increases))
(pp (in by)
(np (jj other) (nns lenders)))
(pp (in against)
(np (nnp arizona) (jj real) (nn estate) (nns loans))))))

(, ,)
(s-adv
(np-sbj (-none- *))
(vp (vbg reflecting)
(np
(np (dt a) (vbg continuing) (nn decline))
(pp-loc (in in)
(np (dt that) (nn market)))))))

(. .)))

[marcus et al. 1993]

the rise of annotated data

    starting off, building a treebank seems a lot 

slower and less useful than building a 
grammar

    but a treebank gives us many things

    reusability of the labor

    many parsers, pos taggers, etc.
    valuable resource for linguistics

    broad coverage
    frequencies and distributional information
    a way to evaluate systems

ptb non-terminals

the id32: an overview

9

table 1.2. the id32 syntactic tagset
adjp
advp
np
pp
s
sbar
sbarq
sinv
sq
vp
whadvp
whnp
whpp
x
0
t

adjective phrase
adverb phrase
noun phrase
prepositional phrase
simple declarative clause
subordinate clause
direct question introduced by wh-element
declarative sentence with subject-aux inversion
yes/no questions and subconstituent of sbarq excluding wh-element
verb phrase
wh-adverb phrase
wh-noun phrase
wh-prepositional phrase
constituent of unknown or uncertain category
   understood    subject of in   nitive or imperative
zero variant of that in subordinate clauses
trace of wh-constituent

+ all pos tags

predicate-argument structure.

the new style of annotation provided

non local phenomena

    dislocation / gapping

    which book should peter buy?
    a debate arose which continued until the 

election.

    binding

    reference

    control

    the irs audits itself

    i want to go
    i want you to go

data for parsing experiments

ptb size

    50,000 annotated sentences

i penn wsj treebank = 50,000 sentences with associated trees
    penn wsj treebank:
i usual set-up: 40,000 training sentences, 2400 test sentences
    usual set-up:
    40,000 training
an example tree:
    2,400 test

top

s

np

vp

nnp

nnps

vbd

np

pp

np

pp

advp

in

np

cd

nn

in

np

qp

$

cd

cd

punc,

rb

np

prp$

jj

nn

cc

jj

nn

nns

in

pp

np

np

sbar

nnp

punc,

whadvp

s

wrb

np

vp

dt

nn

vbz

np

qp

nns

punc.

rb

cd

canadian

utilities

had

1988

revenue

of

c$

1.16

billion

,

mainly

from

its

natural

gas

and

electric

utility

businesses

in

alberta

,

where

the

company

serves

about

800,000

customers

.

canadian utilities had 1988 revenue of c$ 1.16 billion ,

probabilistic context-free 

in the following sections we answer these questions through de   ning proba-
bilistic context-free grammars (pid18s), a natural generalization of context-free
grammars.
grammars (pid18)
3.2 de   nition of pid18s
id140 (pid18s) are de   ned as follows:
    n : the set of non-terminals
de   nition 1 (pid18s) a pid18 consists of:

    a context-free grammar is a tuple <n,    , s, r>

    phrasal categories: s, np, vp, adjp, etc.
    parts-of-speech (pre-terminals): nn, jj, dt, vb

    r : the set of rules

       : the set of terminals (the words)
1. a context-free grammar g = (n,   , s, r).
    s : the start symbol
2. a parameter

    often written as root or top
    not usually the sentence non-terminal s

    of the form x     y1 y2     yn, with x     n, n   0, yi    (n       )
    id203 q(r) for each r     r, such that for all x     n:

for each rule               r. the parameter q(         ) can be interpreted as
the conditional probabilty of choosing rule           in a left-most derivation,
    a pid18 adds a distribution q:
given that the non-terminal being expanded is   . for any x     n, we have
the constraint

    examples: s     np vp,   vp     vp cc vp
    also called rewrites, productions, or local trees

q(         )

!          r:  =x

q(         ) = 1

in addition we have q(         )     0 for any               r.

7

a probabilistic context-free grammar (pid18)

pid18 example

s     np vp
vp     vi
vp     vt np
vp     vp pp
np     dt nn
np     np pp
pp     p
np
    id203 of a tree t with rules

1.0
0.4
0.4
0.2
0.3
0.7
1.0

1.0
vi     sleeps
1.0
vt     saw
0.7
nn     man
0.2
nn     woman
nn     telescope 0.1
1.0
dt     the
in     with
0.5
0.5
in     in

is

  1       1,   2       2, . . . ,   n       n

n

p(t) =

q(  i       i)

where q(         ) is the id203 for rule          .

!i=1

44

a probabilistic context-free grammar (pid18)

pid18 example
1.0
vi     sleeps
1.0
s     np vp
1.0
vt     saw
0.4
vp     vi
0.7
nn     man
0.4
vp     vt np
0.2
nn     woman
0.2
vp     vp pp
nn     telescope 0.1
0.3
np     dt nn
1.0
dt     the
a probabilistic context-free grammar (pid18)
0.7
np     np pp
in     with
0.5
1.0
pp     p
np
0.5
in     in
1.0
vi     sleeps
    id203 of a tree t with rules
1.0
vt     saw
0.7
nn     man
is
0.2
nn     woman
nn     telescope 0.1
p(t) =
dt     the
1.0
0.5
in     with
where q(         ) is the id203 for rule          .
0.5
in     in

  1       1,   2       2, . . . ,   n       n

the man sleeps     

!i=1

n

    id203 of a tree t with rules

44

q(  i       i)
the man saw the woman with the telescope 

s

a probabilistic context-free grammar (pid18)

pid18 example
1.0
vi     sleeps
1.0
s     np vp
1.0
vt     saw
1.0
0.4
vp     vi
nn     man
0.7
vp
t1=
0.4
vp     vt np
0.4
0.3
0.2
nn     woman
0.2
vp     vp pp
nn
vi
nn     telescope 0.1
1.0
0.7
0.3
np     dt nn
dt     the
1.0
a probabilistic context-free grammar (pid18)
0.7
np     np pp
p(t1)=1.0*0.3*1.0*0.7*0.4*1.0
in     with
0.5
1.0
pp     p
np
s
in     in
0.5
1.0
1.0
vi     sleeps
    id203 of a tree t with rules
1.0
vt     saw
t2=
0.7
nn     man
is
0.2
nn     woman
0.3
nn     telescope 0.1
p(t) =
nn
dt     the
1.0
0.2
0.5
in     with
where q(         ) is the id203 for rule          .
0.5
in     in

  1       1,   2       2, . . . ,   n       n
0.4
np

dt
1.0
the man sleeps     

in
0.5

dt
1.0

np

np

vp

pp

vp

0.2

0.4

n

np

!i=1

0.3
nn
0.7

vt
1.0
q(  i       i)
dt
1.0
the man saw the woman with the telescope 
p(ts)=1.0*0.3*1.0*0.7*0.2*0.4*1.0*0.3*1.0*0.2*0.4*0.5*0.3*1.0*0.1
44

0.3
nn
0.1

dt
1.0

    id203 of a tree t with rules

learning and id136

    model

    the id203 of a tree t with n rules   i      i, i = 1..n

p(t) =

q( i !    i)

nyi=1

    learning

probabilities

    read the rules off of labeled sentences, use ml estimates for 

qm l(  !    ) =

count(  !    )

count( )
    and use all of our standard smoothing tricks!
id136
    for input sentence s, define t(s) to be the set of trees whose yieldis s 

(whose leaves, read left to right, match the words in s)

   

t (s) = arg max
t   t (s)

p(t)

the constituency parsing problem

s

vp

np

np

n

n
fish     people     fish     tanks

n

v

pid18
rule prob   i
s    np vp   0
np    np np
   
n    fish
n    people  43
v    fish
   

  1

  42

  44

a recursive parser

bestscore(x,i,j,s)

if (j == i)
else

return q(x->s[i])
return max q(x->yz) *

bestscore(y,i,k,s) *
bestscore(z,k+1,j,s)

    will this parser work?
    why or why not?
    q: remind you of anything?  can we adapt 

this to other models / id136 tasks?

cocke-kasami-younger (cky) 

constituency parsing

fish   people  fish    tanks

cocke-kasami-younger (cky) 

constituency parsing
s    np vp
vp    v np
vp    v @vp_v
vp    v pp
@vp_v    np pp
np    np np
np    np pp
pp    p np

0.9
0.5
0.3
0.1
1.0
0.1
0.2
1.0

vp 0.06
np 0.14
v 0.6
n 0.2

np 0.35
v 0.1
n 0.5

people                           fish

cocke-kasami-younger (cky) 

constituency parsing

s    np vp
vp    v np
vp    v @vp_v
vp    v pp
@vp_v    np pp
np    np np
np    np pp
pp    p np

0.9
0.5
0.3
0.1
1.0
0.1
0.2
1.0

s 0.0189

np 0.35
v 0.1
n 0.5

vp 0.06
np 0.14
v 0.6
n 0.2

people                           fish

cocke-kasami-younger (cky) 

constituency parsing
s    np vp
vp    v np
vp    v @vp_v
vp    v pp
@vp_v    np pp
np    np np
np    np pp
pp    p np

s 0.0189
np 0.0098

0.9
0.5
0.3
0.1
1.0
0.1
0.2
1.0

vp 0.06
np 0.14
v 0.6
n 0.2

np 0.35
v 0.1
n 0.5

people                           fish

cocke-kasami-younger (cky) 

constituency parsing
s    np vp
vp    v np
vp    v @vp_v
vp    v pp
@vp_v    np pp
np    np np
np    np pp
pp    p np

s 0.0189
np 0.0098
vp 0.007

0.9
0.5
0.3
0.1
1.0
0.1
0.2
1.0

vp 0.06
np 0.14
v 0.6
n 0.2

np 0.35
v 0.1
n 0.5

people                           fish

t   t (i,j,x)

  (i, j, x) = max

  (1, n, s) = arg max
t   tg(s)

note in particular, that

cky parsing

the algorithm are as follows:
thus   (i, j, x) is the highest score for any parse tree that dominates words
xi . . . xj, and has non-terminal x as its root. the score for a tree t is again taken
(we de   ne   (i, j, x) = 0 if t (i, j, x) is the empty set).
to be the product of scores for the rules that it contains (i.e. if the tree t contains
    for a given sentence x1 . . . xn, de   ne t (i, j, x) for any x     n, for any
i=1 q(  i       i)).
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m
(i, j) such that 1     i     j     n, to be the set of all parse trees for words
thus   (i, j, x) is the highest score for any parse tree that dominates words
note in particular, that
xi . . . xj such that non-terminal x is at the root of the tree.
xi . . . xj, and has non-terminal x as its root. the score for a tree t is again taken
    we will store: score of the max parse of xi to 
to be the product of scores for the rules that it contains (i.e. if the tree t contains
xj with root non-terminal x
    de   ne
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m
t 2 tg(x)

because by de   nition   (1, n, s) is the score for the highest id203 parse tree
    so we can compute the most likely parse:
spanning words x1 . . . xn, with s as its root.
(we de   ne   (i, j, x) = 0 if t (i, j, x) is the empty set).
the key observation in the cky algorithm is that we can use a recursive de   ni-
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
tion of the    values, which allows a simple bottom-up id145 algo-
    via the recursion:
thus   (i, j, x) is the highest score for any parse tree that dominates words
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
xi . . . xj, and has non-terminal x as its root. the score for a tree t is again taken
because by de   nition   (1, n, s) is the score for the highest id203 parse tree
values for the cases where j = i, then the cases where j = i + 1, and so on.
(q(x     y z)      (i, s, y )      (s + 1, j, z))
  (i, j, x) = max
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
to be the product of scores for the rules that it contains (i.e. if the tree t contains
spanning words x1 . . . xn, with s as its root.
all x     n,
i=1 q(  i       i)).
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m
the key observation in the cky algorithm is that we can use a recursive de   ni-
    with base case:
the next section of this note gives justi   cation for this recursive de   nition.
note in particular, that
tion of the    values, which allows a simple bottom-up id145 algo-
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
values for the cases where j = i, then the cases where j = i + 1, and so on.
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
this is a natural de   nition: the only way that we can have a tree rooted in node
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
values for   (i, j, x) such that j = i + 2; and so on.

if x     xi     r
otherwise
  (1, n, s) = arg max
t   tg(s)

  (i, i, x) = " q(x     xi)

  (1, n, s) = arg max
t   tg(s)

i=1 q(  i       i)).
p(t)

for all x     n,

= arg max

s   {i...(j   1)}

t   t (i,j,x)

x   y z   r,

0

input: a sentence s = x1 . . . xn, a pid18 g = (n,   , s, r, q).
initialization:
for all i     {1 . . . n}, for all x     n,

because by de   nition   (1, n, s) is the score for the highest id203 parse tree
spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
the cky algorithm
tion of the    values, which allows a simple bottom-up id145 algo-
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
  (i, i, x) = ! q(x     xi)
values for the cases where j = i, then the cases where j = i + 1, and so on.
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
all x     n,
initialization: for i = 1     n and all x in n
  (i, i, x) = " q(x     xi)

if x     xi     r
otherwise

if x     xi     r
otherwise

    for l = 1 . . . (n     1)

algorithm:

   
   

0

0

    for l = 1     (n-1) 

    for i = 1 . . . (n     l)
    for i = 1     (n-l) and j = i+l
    set j = i + l
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
    for all x in n
    for all x     n, calculate
for all x     n,

this is a natural de   nition: the only way that we can have a tree rooted in node
x spanning word xi is if the rule x     xi is in the grammar, in which case the
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
there are no trees rooted in x spanning word xi.

[iterate all phrases of length l]

[iterate all phrase lengths]

[iterate all non-terminals]

(q(x     y z)      (i, s, y )      (s + 1, j, z))
(q(x     y z)      (i, s, y )      (s + 1, j, z))

  (i, j, x) = max
  (i, j, x) = max
x   y z   r,
x   y z   r,
s   {i...(j   1)}
s   {i...(j   1)}
the next section of this note gives justi   cation for this recursive de   nition.
    also, store back pointers
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
values for   (i, j, x) such that j = i + 2; and so on.
note that the algorithm also stores backpointer values bp(i, j, x) for all values
of (i, j, x). these values record the rule x     y z and the split-point s leading to
the highest scoring parse tree. the backpointer values allow recovery of the highest

output: return   (1, n, s) = maxt   t (s) p(t), and backpointers bp which allow recovery
of arg maxt   t (s) p(t).

(q(x     y z)      (i, s, y )      (s + 1, j, z))

bp(i, j, x) = arg max

s   {i...(j   1)}

and

(1)

x   y z   r,

12

probabilistic cky parser

0.8
0.1
1.0

0.05
0.03

s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
0.01     0.004    0.006

s     verb np
s     vp pp
np      i   |  he  |  she |  me
0.1   0.02  0.02    0.06

np     houston | nwa
0.16           .04

det    the |  a  |   an 

0.6    0.1   0.05    

np     det nominal
nominal     book | flight | meal | money

0.6

0.03    0.15   0.06     0.06
0.2
0.5

nominal     nominal nominal
nominal     nominal pp
verb    book | include | prefer

0.5      0.04        0.06

vp     verb np
vp     vp pp
prep     through | to | from
0.2          0.3   0.3

pp     prep np

0.5
0.3

1.0

b

oo

k            the              flig

ht       thro

u

g

h       h

o

usto

n

probabilistic cky parser

0.8
0.1
1.0

0.05
0.03

s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
0.01     0.004    0.006

s     verb np
s     vp pp
np      i   |  he  |  she |  me
0.1   0.02  0.02    0.06

np     houston | nwa
0.16           .04

det    the |  a  |   an 

0.6    0.1   0.05    

np     det nominal
nominal     book | flight | meal | money

0.6

0.03    0.15   0.06     0.06
0.2
0.5

nominal     nominal nominal
nominal     nominal pp
verb    book | include | prefer

0.5      0.04        0.06

vp     verb np
vp     vp pp
prep     through | to | from
0.2          0.3   0.3

pp     prep np

0.5
0.3

1.0

s :.01, 
verb:.5 
nominal:.03

none

b

oo

det:.6

k            the              flig

s:.05*.5*.054

=.00135

vp:.5*.5*.054

=.0135

none

np:.6*.6*.15

=.054

none

nominal:.15

none

s:.03*.0135*.032
=.00001296
s:.05*.5*
.000864
=.0000216

np:.6*.6*
.0024
=.000864

nominal:
.5*.15*.032
=.0024

prep:.2

pp:1.0*.2*.16

=.032

ht       thro

u

g

h       h

np:.16

o

usto

n

probabilistic cky parser

s :.01, 
verb:.5 
nominal:.03

none

b

oo

det:.6

k            the              flig

pick most 
probable
parse

s:.05*.5*.054

=.00135

vp:.5*.5*.054

=.0135

none

np:.6*.6*.15

=.054

none

nominal:.15

none

s:.03*.0135*.032
=.00001296
s:.0000216

np:.6*.6*
.0024
=.000864

nominal:
.5*.15*.032
=.0024

ht       thro

prep:.2

pp:1.0*.2*.16

=.032

u

g

h       h

np:.16

o

usto

n

input: a sentence s = x1 . . . xn, a pid18 g = (n,   , s, r, q).
initialization:
for all i     {1 . . . n}, for all x     n,

because by de   nition   (1, n, s) is the score for the highest id203 parse tree
spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
the cky algorithm
tion of the    values, which allows a simple bottom-up id145 algo-
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
  (i, i, x) = ! q(x     xi)
values for the cases where j = i, then the cases where j = i + 1, and so on.
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
all x     n,
initialization: for i = 1     n and all x in n
  (i, i, x) = " q(x     xi)

if x     xi     r
otherwise

if x     xi     r
otherwise

    for l = 1 . . . (n     1)

algorithm:

   
   

0

0

    for l = 1     (n-1) 

    for i = 1 . . . (n     l)
    for i = 1     (n-l) and j = i+l
    set j = i + l
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
    for all x in n
    for all x     n, calculate
for all x     n,

this is a natural de   nition: the only way that we can have a tree rooted in node
x spanning word xi is if the rule x     xi is in the grammar, in which case the
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
there are no trees rooted in x spanning word xi.

[iterate all phrases of length l]

[iterate all phrase lengths]

[iterate all non-terminals]

(q(x     y z)      (i, s, y )      (s + 1, j, z))
(q(x     y z)      (i, s, y )      (s + 1, j, z))

  (i, j, x) = max
  (i, j, x) = max
x   y z   r,
x   y z   r,
s   {i...(j   1)}
s   {i...(j   1)}
the next section of this note gives justi   cation for this recursive de   nition.
    also, store back pointers
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
values for   (i, j, x) such that j = i + 2; and so on.
note that the algorithm also stores backpointer values bp(i, j, x) for all values
of (i, j, x). these values record the rule x     y z and the split-point s leading to
the highest scoring parse tree. the backpointer values allow recovery of the highest

output: return   (1, n, s) = maxt   t (s) p(t), and backpointers bp which allow recovery
of arg maxt   t (s) p(t).

(q(x     y z)      (i, s, y )      (s + 1, j, z))

bp(i, j, x) = arg max

s   {i...(j   1)}

and

(1)

x   y z   r,

12

time: theory

    for each length (<= n)

    for each i (<= n)

    for each split point k
    for each rule x     y z 
   do constant work
    total time: |rules|*n3

x

y

z

i                       k                      j

time: practice

    parsing with the vanilla treebank grammar:
~ 20k rules

(not an 
optimized 
parser!)
observed 
exponent: 
3.6

    why   s it worse in practice?

    longer sentences    unlock    more of the grammar
    all kinds of systems issues don   t scale

input: a sentence s = x1 . . . xn, a pid18 g = (n,   , s, r, q).
initialization:
for all i     {1 . . . n}, for all x     n,

because by de   nition   (1, n, s) is the score for the highest id203 parse tree
spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
the cky algorithm
tion of the    values, which allows a simple bottom-up id145 algo-
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
  (i, i, x) = ! q(x     xi)
values for the cases where j = i, then the cases where j = i + 1, and so on.
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
all x     n,
initialization: for i = 1     n and all x in n
  (i, i, x) = " q(x     xi)

if x     xi     r
otherwise

if x     xi     r
otherwise

    for l = 1 . . . (n     1)

algorithm:

   
   

0

0

    for l = 1     (n-1) 

    for i = 1 . . . (n     l)
    for i = 1     (n-l) and j = i+l
    set j = i + l
the recursive de   nition is as follows: for all (i, j) such that 1     i < j     n,
    for all x in n
    for all x     n, calculate
for all x     n,

this is a natural de   nition: the only way that we can have a tree rooted in node
x spanning word xi is if the rule x     xi is in the grammar, in which case the
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
there are no trees rooted in x spanning word xi.

[iterate all phrases of length l]

[iterate all phrase lengths]

[iterate all non-terminals]

(q(x     y z)      (i, s, y )      (s + 1, j, z))
(q(x     y z)      (i, s, y )      (s + 1, j, z))

  (i, j, x) = max
  (i, j, x) = max
x   y z   r,
x   y z   r,
s   {i...(j   1)}
s   {i...(j   1)}
the next section of this note gives justi   cation for this recursive de   nition.
    also, store back pointers
figure 6 shows the    nal algorithm, based on these recursive de   nitions. the
algorithm    lls in the    values bottom-up:    rst the   (i, i, x) values, using the base
case in the recursion; then the values for   (i, j, x) such that j = i + 1; then the
values for   (i, j, x) such that j = i + 2; and so on.
note that the algorithm also stores backpointer values bp(i, j, x) for all values
of (i, j, x). these values record the rule x     y z and the split-point s leading to
the highest scoring parse tree. the backpointer values allow recovery of the highest

output: return   (1, n, s) = maxt   t (s) p(t), and backpointers bp which allow recovery
of arg maxt   t (s) p(t).

(q(x     y z)      (i, s, y )      (s + 1, j, z))

bp(i, j, x) = arg max

s   {i...(j   1)}

and

(1)

x   y z   r,

12

memory

    how much memory does this require?

    have to store the score cache
    cache size: 

    |symbols|*n2 doubles

    pruning: beams

    score[x][i][j] can get too large (when?)
    can keep beams (truncated maps score[i][j]) 

which only store the best few scores for the span 
[i,j]     exact?

    pruning: coarse-to-fine

    use a smaller grammar to rule out most x[i,j]

data for parsing experiments

let   s parse with cky!

i penn wsj treebank = 50,000 sentences with associated trees

    any problem?

i usual set-up: 40,000 training sentences, 2400 test sentences

an example tree:

top

s

np

vp

nnp

nnps

vbd

np

pp

np

pp

advp

in

np

cd

nn

in

np

qp

$

cd

cd

punc,

rb

np

prp$

jj

nn

cc

jj

nn

nns

in

pp

np

np

sbar

nnp

punc,

whadvp

s

wrb

np

vp

dt

nn

vbz

np

qp

nns

punc.

rb

cd

canadian

utilities

had

1988

revenue

of

c$

1.16

billion

,

mainly

from

its

natural

gas

and

electric

utility

businesses

in

alberta

,

where

the

company

serves

about

800,000

customers

.

canadian utilities had 1988 revenue of c$ 1.16 billion ,

chomsky normal form

    x, y, z     n and w     t 

    all rules are of the form x    y z or x    w
    a transformation to this form doesn   t change the weak generative 

capacity of a id18
    that is, it recognizes the same language

    but maybe with different trees

    empties and unaries are removed recursively
    n-ary rules are divided by introducing new nonterminals (n > 2)

vp

vbd   np   pp   pp

[vp     vbd np pp    ]

vp

[vp     vbd np    ]
vbd            np

pp

pp

special case: unary rules

    chomsky normal form (cnf):

    all rules of the form x     y z or x     w
    makes parsing easier!

    can also allow unary rules

    all rules of the form x     y z, x     y, or x     w
    conversion to/from the normal form is easier
    q: how does this change cky?
    warning: watch for unary cycles   

t   tg(s)

   
   

cky with unary rules

because by de   nition   (1, n, s) is the score for the highest id203 parse tree
spanning words x1 . . . xn, with s as its root.
the key observation in the cky algorithm is that we can use a recursive de   ni-
tion of the    values, which allows a simple bottom-up id145 algo-
rithm. the algorithm is    bottom-up   , in the sense that it will    rst    ll in   (i, j, x)
values for the cases where j = i, then the cases where j = i + 1, and so on.
input: a sentence s = x1 .. xn and a pid18 = <n,    ,s, r, q>
the base case in the recursive de   nition is as follows: for all i = 1 . . . n, for
initialization: for i = 1     n:
all x     n,
    step 1: for all x in n:

  (i, i, x) = " q(x     xi)

0

if x     xi     r
otherwise

    step 2: for all x in n:
this is a natural de   nition: the only way that we can have a tree rooted in node
x spanning word xi is if the rule x     xi is in the grammar, in which case the
tree has score q(x     xi); otherwise, we set   (i, i, x) = 0, re   ecting the fact that
    for i = 1     (n-l) and j = i+l
[iterate all phrases of length l]
there are no trees rooted in x spanning word xi.

[iterate all phrase lengths]

(q(x ! y )        (i, i, y ))

   u (i, i, x) = max

    for l = 1     (n-1) 

x!y 2r

    step 1: (binary) 
    for all x in n

[iterate all non-terminals]

   b(i, j, x) =

12
x!y z2r,s2{i...(j 1)}

max

    step 2: (unary)
    for all x in n

   u (i, j, x) = max

x!y 2r

[iterate all non-terminals]
(q(x ! y )        b(i, j, y ))

(q(x ! y z)        u (i, s, y )        u (s + 1, j, z)

must always have 
one and exactly 
one unary rule!

unary closure

    rather than zero or more unaries, always exactly one
    calculate closure close(r) for unary rules in r 
   y with q(x   y) = q(x   z1)*q(z1   z2)*   *q(zk    y)

    add x   y if there exists a rule chain x   z1, z1   z2,..., zk
    add x   x with q(x   x)=1 for all x in n
vp

sbar

vp

vbd

np

np

dt

nn

vbd

np

dt

nn

s

vp

sbar

vp

   
    reconstruct unary chains afterwards (with extra 

in cky and chart: alternate unary and binary layers
marking)

the cky algorithm is a dynamic-programming algorithm. key de   nitions in

the algorithm are as follows:

other chart computations

    for a given sentence x1 . . . xn, de   ne t (i, j, x) for any x     n, for any
(i, j) such that 1     i     j     n, to be the set of all parse trees for words
    max inside score
xi . . . xj such that non-terminal x is at the root of the tree.
    de   ne

    score of the max parse of xi to xj with root x

  (i, j, x) = max
    marginalize over internal structure

t   t (i,j,x)

p(t)

(we de   ne   (i, j, x) = 0 if t (i, j, x) is the empty set).
    max outside score
    sum inside/outside
thus   (i, j, x) is the highest score for any parse tree that dominates words
xi . . . xj, and has non-terminal x as its root. the score for a tree t is again taken
to be the product of scores for the rules that it contains (i.e. if the tree t contains
rules   1       1,   2       2, . . . ,   m       m, then p(t) =!m

note in particular, that

i=1 q(  i       i)).

k+1

k

i

j

  (1, n, s) = arg max

other chart computations

    max inside score
    max outside score

    score of max parse of the 
complete span with a gap 
between i and j
    details in notes

    sum inside/outside

k

i-1 i

j

other chart computations

i

k

k+1

j

    max inside score
    max outside score
    sum inside/outside
    do sums instead of 

maxes

i

k

k+1

j

k

i-1 i

j

just like sequences

    locally normalized:

    generative
    maxent

    globally normalized:

    crfs

    additive, un-normalized:

    id88

treebank parsing

( (s

(np-sbj (dt the) (nn move))
(vp (vbd followed)
(np
(np (dt a) (nn round))
(pp (in of)
(np
(np (jj similar) (nns increases))
(pp (in by)
(np (jj other) (nns lenders)))
(pp (in against)
(np (nnp arizona) (jj real) (nn estate) (nns loans))))))

(, ,)
(s-adv
(np-sbj (-none- *))
(vp (vbg reflecting)
(np
(np (dt a) (vbg continuing) (nn decline))
(pp-loc (in in)
(np (dt that) (nn market)))))))

(. .)))

[marcus et al. 1993]

treebank grammars
    need a pid18 for broad coverage parsing.
    can take a grammar right off the trees:

1

root     s
s     np vp .
np     prp
1
vp     vbd adjp

1

1

   ..

typical experimental setup

    the id32 is divided into 

sections:
    training: sections 2-18
    development: section 22 (also 0-1 and 24)
    testing: section 23

    evaluation?

evaluating constituency parsing

evaluating constituency parsing
    recall:

    recall = (# correct constituents in candidate) / (# constituents in 

gold)

    precision:

    precision = (# correct constituents in candidate) / (# 

constituents in candidate)

    labeled precision and labeled recall require getting the 

non-terminal label on the constituent node correct to 
count as correct.

    f1 is the harmonic mean of precision and recall.

    f1= (2 * precision * recall) / (precision + recall)

evaluating constituency parsing
gold standard brackets: 
s-(0:11), np-(0:2), vp-(2:9), vp-(3:9), np-(4:6), pp-
(6-9), np-(7,9), np-(9:10)
candidate brackets: 
s-(0:11), np-(0:2), vp-(2:10), vp-(3:10), np-(4:6), 
pp-(6-10), np-(7,10)

    precision:
    recall:
40%
    f1:
    also, tagging accuracy: 11/11 =  100%

3/7 =  42.9%
3/8  = 37.5% 

how good are pid18s?
penn wsj parsing performance:

~ 73% f1

    robust 
    partial solution for grammar ambiguity 

    usually admit everything, but with low id203

    a pid18 gives some idea of the plausibility of a parse
    but not so good because the independence assumptions are 

    give a probabilistic language model 
    but in the simple case it performs worse than a trigram model
    the problem seems to be that pid18s lack the lexicalization 
of a trigram model

too strong

the missing information?

s

vp

vp

pp

np

vt

np

in

np

nn

dt
nn
the man saw the woman with the    hat

nn

dt

dt

extra slides

chomsky normal form

    x, y, z     n and w     t 

    all rules are of the form x    y z or x    w

    a transformation to this form doesn   t change 

the weak generative capacity of a id18
    that is, it recognizes the same language

    but maybe with different trees

    empties and unaries are removed recursively
    n-ary rules are divided by introducing new 

nonterminals (n > 2)

example: before binarization

root

s

v

np

n

vp

np

n

pp

p

people

fish

tanks

with

np

n
rods

example: after binarization

root

s

np

vp

n

v

@vp_v

np

n

pp

p

np

n

people

fish

tanks

with

rods

a phrase structure grammar

s    np vp
vp    v np
vp    v np pp
np    np np
np    np pp
np    n
np    e
pp    p np

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 1: remove epsilon rules
s    np vp
vp    v np
vp    v np pp
np    np np
np    np pp
np    n
np    e
pp    p np

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 1: remove epsilon rules
s    np vp
vp    v np
vp    v np pp
np    np np
np    np pp
np    n
np    e
pp    p np

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 1: remove epsilon rules
s    np vp
vp    v np
vp    v np pp
np    np np
np    np pp
np    n
np    e
pp    p np

recognizing the 
same language?
for every rule 
with np, create a 
unary rule

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 1: remove epsilon rules
s    np vp
s    vp
vp    v np
vp    v
vp    v np pp
vp    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
s    vp
vp    v np
vp    v
vp    v np pp
vp    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
s    vp
vp    v np
vp    v
vp    v np pp
vp    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

recognizing the 
same language?
work your way 
down to 
propagate

step 2: remove unary rules
s    np vp
s    vp
vp    v np
vp    v
vp    v np pp
vp    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

recognizing the 
same language?
work your way 
down to 
propagate

step 2: remove unary rules
s    np vp
s    vp
vp    v np
vp    v
vp    v np pp
vp    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

just added a 
unary rule!
need to apply 
until they are all 
gone

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v
s    v
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

just added a 
unary rule!
need to apply 
until they are all 
gone

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v
s    v
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

recognizing the 
same language?
yes!

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np
np    np pp
np    pp
np    n
pp    p np
pp    p

only place n 
appears
so can get rid of 
it altogether

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np pp
np    pp
pp    p np
pp    p

np    people
np    fish 
np    tanks
np    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: remove unary rules
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np pp
np    pp
pp    p np
pp    p

np    people
np    fish 
np    tanks
np    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

step 2: binarize
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np pp
np    p np
pp    p np

np    people
np    fish 
np    tanks
np    rods
v    people
s    people
vp    people
v    fish
s    fish
vp    fish
v    tanks
s    tanks
vp    tanks
p    with
pp    with

chomsky normal form

step 2: binarize
s    np vp
vp    v np
s    v np
vp    v np pp
s    v np pp
vp    v pp
s    v pp
np    np np
np    np pp
np    p np
pp    p np

np    people
np    fish 
np    tanks
np    rods
v    people
s    people
vp    people
v    fish
s    fish
vp    fish
v    tanks
s    tanks
vp    tanks
p    with
pp    with

chomsky normal form

step 2: binarize
s    np vp
vp    v np
s    v np
vp    v @vp_v
@vp_v    np pp
s    v @s_v
@s_v    np pp
vp    v pp
s    v pp
np    np np
np    np pp
np    p np
pp    p np

np    people
np    fish 
np    tanks
np    rods
v    people
s    people
vp    people
v    fish
s    fish
vp    fish
v    tanks
s    tanks
vp    tanks
p    with
pp    with

chomsky normal form: source

s    np vp
vp    v np
vp    v np pp
np    np np
np    np pp
np    n
np    e
pp    p np

n    people
n    fish 
n    tanks
n    rods
v    people
v    fish
v    tanks
p    with

chomsky normal form

s    np vp
vp    v np
s    v np
vp    v @vp_v
@vp_v    np pp
s    v @s_v
@s_v    np pp
vp    v pp
s    v pp
np    np np
np    np pp
np    p np
pp    p np

np    people
np    fish 
np    tanks
np    rods
v    people
s    people
vp    people
v    fish
s    fish
vp    fish
v    tanks
s    tanks
vp    tanks
p    with
pp    with

chomsky normal form

    you should think of this as a transformation for efficient 
    with some extra book-keeping in symbol names, you can 
   

parsing
even reconstruct the same trees with a detransform
in practice full chomsky normal form is a pain
    reconstructing n-aries is easy
    reconstructing unaries/empties is trickier

    binarization is crucial for cubic time id18 parsing

    the rest isn   t necessary; it just makes the algorithms cleaner 

and a bit quicker

treebank: empties and unaries

root

s-hln

root

s

np-subj

-none-

vp

vb

np

-none-

vp

vb

root

root

root

s

vp

vb

s

vb

e

atone

e

atone

atone

atone

atone

ptb tree

nofunctags

noempties

low

high
nounaries

