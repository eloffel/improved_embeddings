cs5740: natural language processing

spring 2018

lexical semantics

instructor: yoav artzi

slides adapted from dan jurafsky, chris manning, slav petrov, dipanjan das, 

and david weiss

overview

    id51 (wsd)

    id138

    id14 (srl)
    continuous representations

lemma and wordform

    a lemma (or citation form)

    basic part of the word, same stem, rough 

semantics
    a wordform

    the    inflected    word as it appears in text

wordform lemma
banks
sung
duermes

bank
sing
dormir

word senses

    one lemma can have many meanings:
sense 1:

       a bank can hold the investments in a 
custodial account   

1

          as agriculture burgeons on the east 
bank the river will shrink even more   

sense 2:
    sense (or word sense)

2

    a discrete representation of an aspect of a word   s 

meaning.

    the lemma bank here has two senses

homonymy

homonyms: words that share a form but have 
unrelated, distinct meanings:

bank1: financial institution,  bank2:  sloping land
bat1: club for hitting a ball,  bat2:  nocturnal flying mammal

1. homographs (bank/bank, bat/bat)
2. homophones:
1. write and right
2. piece and peace

homonymy in nlp

       bat care   

    information retrieval
    machine translation
    text-to-speech

    bat:  murci  lago (animal) or  bate (for baseball)

    bass (stringed instrument) vs. bass (fish)

quick test for multi sense words
    zeugma

    when a word applies to two others in different 

senses

which flights serve breakfast?
does lufthansa serve philadelphia?
does lufthansa serve breakfast and san jose?

    the conjunction sounds    weird    
    so we have two senses for serve

synonyms

    word that have the same meaning in some or all 

contexts.
    filbert / hazelnut
    couch / sofa
    big / large
    automobile / car
    vomit / throw up
    water / h20
        they can be substituted for each other

    two words are synonyms if    
    very few (if any) examples of perfect synonymy

    often have different notions of politeness, slang, etc.

synonyms

    perfect synonymy is rare
    can we define it better in term of senses?
    consider the words big and large
    are they synonyms?

    how big is that plane?
    would i be flying on a large or small plane?

    how about here:

    miss nelson became a kind of big sister to benjamin.
    miss nelson became a kind of large sister to benjamin.

    why?

    big has a sense that means being older, or grown up
    large lacks this sense
    synonymous relations are defined between senses

antonyms

    senses that are opposites with respect to one feature of 

meaning

    otherwise, they are very similar!

dark   short fast  rise  hot   up    in 
light  long  slow  fall  cold  down  out

    antonyms can

    define a binary opposition: in/out
    be at the opposite ends of a scale: fast/slow
    be reversives: rise/fall

    very tricky to handle with some representations     remember for 

later!

hyponymy and hypernymy

    one sense is a hyponym of another if the first sense is more 

specific, denoting a subclass of the other
    car is a hyponym of vehicle
    mangois a hyponym of fruit

    conversely hypernym/superordinate (   hyper is super   )

    vehicleis a hypernym of car
    fruitis a hypernym of mango

    usually transitive 

    (a hypo b and b hypo c entails a hypo c)

superordinate/hyper
subordinate/hyponym car

vehicle fruit

mango

furniture
chair

id138

    a hierarchically organized lexical database
    on-line thesaurus + aspects of a dictionary

    word senses and sense relations
    some other languages available or under development 

(arabic, finnish, german, portuguese   )

category
noun
verb
adjective
adverb

unique strings
117,798
11,529
22,479
4,481

id138

http://id138web.princeton.edu/perl/webwn

id138

http://id138web.princeton.edu/perl/webwn

senses and synsets in id138
    each word in id138 has at least one sense
    the synset (synonym set), the set of near-synonyms, is 

a set of senses with a shared gloss

    example: chump as a noun with the gloss:

   a person who is gullible and easy to take advantage 
of   

    this sense of    chump    is shared with 9 words:

chump1, fool2, gull1, mark9, patsy1, fall 
guy1, sucker1, soft touch1, mug2

    all these senses have the same gloss    they form a 

synset

id138 noun relations

id138 3.0

    http://id138web.princeton.edu/perl/webwn

    where it is:

    libraries
    python:

    java:

    and more:
projects/

    nltk: http://www.nltk.org/home

    jwnl, extjwnl on sourceforge

    https://id138.princeton.edu/id138/related-

id51

i play bass in a jazz band

musical_instrument

she was grilling a bass on the stove top

freshwater_fish

supervised wsd

    given: a lexicon (e.g., id138) and a 

word in a sentence

    goal: classify the sense of the word
    linear model:

p(sense | word, context) / e      (sense,word,context)

feature function looking at the sense, word, and context

supervised wsd

    given: a lexicon (e.g., id138) and a 

word in a sentence

    goal: classify the sense of the word
    linear model:

p(sense | word, context) =

e      (sense,word,context)

ps0 e      (s0,word,context)

summing over all senses for the word (e.g., from id138)

unsupervised wsd

    goal: induce the senses of each word and 

classify in context

1. for each word in context, compute some 
2. cluster each instance using a id91 
3. cluster labels are word senses

features
algorithm

more reading: section 20.10 of j&m

semantic roles

    some word senses (a.k.a. predicates) 

represent events

    events have participants that have 

specific roles (as arguments)

    predicate-argument structure at the type 

level can be stored in a lexicon

sematic roles

    propbank: a semantic role lexicon

run.01 (operate)

arg0 (operator)
arg1 (machine/operation)
arg2 (employer)
arg3 (co-worker)
arg4 (instrument)

frame

semantic roles

sematic roles

    propbank: a semantic role lexicon

run.01 (operate)

arg0 (operator)
arg1 (machine/operation)
arg2 (employer)
arg3 (co-worker)
arg4 (instrument)

run.02 (walk quickly)

arg0 (runner)
arg1 (course/race)
arg2 (opponent)

also: framenet, an 
alternative role lexicon

https://verbs.colorado.edu/propbank/framesets-english/

id14

    task: given a sentence, disambiguate predicate 

frames and annotate semantic roles 

mr. stromach wants to resume a more influential role in running the company.
arg1

arg0

ii. role labeling

run.01

i. frame identification

role identification

    classification models similar to wsd

mr. stromach wants to resume a more influential role in running the company.

run.01

i. frame identification

role labeling

sentence spans:

potential roles:

mr. stromach

score(mr. stromach, arg0, context)

a more influential role

s

c

o

c o m p a n y , a r g 1 , c o n t e x t )

(

r

e

a

s c o r e ( t h e

the company

influential role

company

m

o

r

e

i
n

   

u

e

n

t
i
a
l

r

score(in   uentialrole,none,context)

score(company,none,context)

o
l
e
,

n

n

o

e

,

c

o

arg0

arg1

arg2

arg3

arg4

best matching 
between spans 
and roles

score can come 
from any classifier 
(linear, id166, nn)

n
t
e

x
t
)

none

srl

    http://cogcomp.cs.illinois.edu/page/demo

_view/srl

    example: the challenges facing iraqi 
forces in mosul include narrow streets, 
suicide bombers and small drones that 
the terror group has used to target 
soldiers.

http://cogcomp.cs.illinois.edu/page/demo_view/srl

word similarity

    task: given two words, predict how similar 

they are

the distributional hypothesis:

a word is characterized by the company it keeps

(john firth, 1957)

know them by the company they keep!

word similarity

a bottle of tesg  ino is on the table. 
everybody likes tesg  ino. 
tesg  ino makes you drunk. 
we make tesg  ino out of corn.

    occurs before drunk
    occurs after bottle
    is the direct object of likes
       

similar to 
bear, wine, 
whiskey,    

jurafsky and martin, 20.7

word similarity

    given a vocabulary of ! words
    represent a word " as:

~w = (f1, f2, f3, . . . , fn)

binary (or count) features indicating 
the presence of the ith word in the 
vocabulary in the word   s context

    for example:

~tseg  uino = (1, 1, 0, . . . )

corn

drunk

matrix

word similarity
~tseg  uino = (1, 1, 0, . . . )

~beer = (0, 1, 0, . . . )

    similarity can be measured using vector distance metrics
    for example, cosine similarity:

similarity(w, u) =

w    u
kwkkuk

=

npi=1
s npi=1

w2

wiui

is npi=1

u2
i

which gives values between -1 (completely different), 0 
(orthogonal), and 1 (the same)

vector-space models

    words represented by vectors
    in contrast to the discrete class 

representation of word senses

    common methods (and packages): 

id97, glove

id97

vectors from raw text

    open-source package for learning word 
    widely used across academia/industry
    another common package: glove
    goal: good id27s

    embedding are vectors in a low dimensional 
    similar words should be close to one another 

space

    two models:

    skip-gram (today)
    cbow (further reading: mikolov et al. 2013)

the skip-gram model

    given: corpus ! of pairs (#,%) where # is a word and 
% is context
of size ')

    context may be a single neighboring word (in window 

    later: different definition

    consider the parameterized id203

    goal: maximize the corpus id203
p(c|w;    )

arg max

p(c|w;    )
    y(w,c)2d

    wait! but we want good embeddings, so who cares 

about context pairs?

the skip-gram model

    goal: maximize the corpus id203

arg max

    y(w,c)2d

p(c|w;    )

where:

if ! is the dimensionality of the vectors, we 

pc02c evc0  vw

have                                 parameters 

p(c|w;    ) =

evc  vw

d     |v | + d     |c|

the skip-gram model

    goal: maximize the corpus id203

    the log of the objective is:

arg max

    y(w,c)2d
p(c|w;    )
(log evc  vw   logxc0

arg max

    x(w,c)2d

    not tractable in practice

    sum over all context     intractable
    approximated via negative sampling

evc0  vw )

negative sampling for skip-gram
    efficient way of deriving id27s

    consider a word-context pair (",$)

    let the id203 that this pair was 

observed:

    therefore, the id203 that it was not 

p(d = 1|w, c)

observed is:

1   p(d = 1|w, c)

negative sampling

    parameterization:

p(d = 1|w, c) =
    new learning objective:

1

1 + e vc  vw

arg max

    need to get !   
    y(w,c)2d

p(d = 1|w, c) y(w,c)2d0

p(d = 0|w, c)

negative sampling

    for a given k, the size of d    is k-times 

bigger than d

    each context c is a word
    for each observed word-context pair, k

samples are generated based on unigram 
distribution

negative sampling

    new learning objective:
arg max

    y(w,c)2d

    original learning objective:

p(d = 1|w, c) y(w,c)2d0
    y(w,c)2d

p(c|w;    )

arg max

    how does the new objective approximate 

the original one?

p(d = 0|w, c)

the skip-gram model

vectors of the words !"

    optimized for word-context pairs
    to get id27, take the 
    but why does it work?
intuitively: words that share many 
   
contexts will be similar
    formal: 

    neural id27 as implicit matrix 
factorization / levy and goldberg 2014
    a latent variable model approach to pmi-
based id27s / arora et al. 
2016

word galaxy

    word galaxy

    http://anthonygarvan.github.io/wordgalaxy/

    embeddings for word substitution 

    http://ghostweather.com/files/id97pride/

structured contexts

australian scientist discovers star with telescope

skip-gram context with n=2

    just looking at neighboring words, often 
doesn   t capture arguments and modifiers
    can we use anything except adjacency to 

get context?

levy and goldberg 2014

structured contexts

amod

nsubj

prep
dobj

pobj

australian scientist discovers star with telescope

amod

nsubj

collapsing    prep    links

prep_with

dobj

australian scientist discovers star with telescope

scientist/nsubj

star/dobj

telescope/prep_with

levy and goldberg 2014

structured 
context

levy and goldberg 2014

id27s vs. sparse 

vectors

    count vectors: sparse and large
    embedded vectors: small dense
    one advantage: dimensionality
    more contested advantage: better 

generalization
    see levy et al. 2015 (improving distributional 

similarity with lessons learned from word 
embeddings) for detailed analysis

applications

    word vectors are often input to various 

end applications
    parsing, co-reference resolution, named-

entity recognition, id14, etc.

    input to sentence models, including 
recurrent and recursive architectures

