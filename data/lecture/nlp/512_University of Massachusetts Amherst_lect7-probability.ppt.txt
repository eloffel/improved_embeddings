id203

lecture #7

introduction to natural language processing

cmpsci 585, fall 2007
university of massachusetts  amherst

andrew mccallum

andrew mccallum, umass amherst

today   s main points

    remember (or learn) about id203 theory

    samples, events, tables, counting
    bayes    rule, and its application
    a little calculus?
    random variables
    bernoulli and multinomial distributions: the work-

horses of computational linguistics.

    multinomial distributions from shakespeare.

andrew mccallum, umass amherst

id203 theory

    id203 theory deals with predicting how

likely it is that something will happen.
    toss 3 coins,

how likely is it that all come up heads?

    see phrase    more lies ahead   ,

how likely is it that    lies    is noun?

    see    nigerian minister of defense    in email,

how likely is it that the email is spam?

    see    le chien est noir   ,

how likely is it that the correct translation is
   the dog is black   ?

andrew mccallum, umass amherst

id203 and compling

    id203 is the backbone of modern

computational linguistics... because:
    language is ambiguous
    need to integrate evidence

    simple example (which we will revisit later)

    i see the first word of a news article:    glacier   
    what is the id203 the language is french?

english?

    now i see the second word:    melange   .
    now what are the probabilities?

andrew mccallum, umass amherst

experiments and sample spaces

    experiment (or trial)

    repeatable process by which observations are made
    e.g. tossing 3 coins

    observe basic outcome from

sample space,   , (set of all possible basic outcomes), e.g.
    one coin toss, sample space    = { h, t };

basic outcome = h or t

    three coin tosses,    = { hhh, hht, hth,   , ttt }
    part-of-speech of a word,    = { cc1, cd2, ct3,    ,  wrb36}
    lottery tickets, |  | = 107
    next word in shakespeare play, |  | = size of vocabulary
    number of words in your ph.d. thesis    = { 0, 1,         }
    length of time of    a    sounds when i said    sample   .

discrete,
countably in   nite

continuous,
uncountably in   nite

andrew mccallum, umass amherst

events and event spaces

    an event, a, is a set of basic outcomes,

i.e., a subset of  the sample space,   .
    intuitively, a question you could ask about an outcome.
       = {hhh, hht, hth, htt, thh, tht, tth, ttt}
    e.g. basic outcome = thh
    e.g. event =    has exactly 2 h   s   , a={thh, hht, hth}
    a=   is the certain event,  a=    is the impossible event.
    for    not a   , we write a

    a common event space, f, is the power set of the

sample space,   .  (power set is written 2  )
    intuitively: all possible questions you could ask about a

basic outcome.

andrew mccallum, umass amherst

id203

    a id203 is a number between 0 and 1.

    0 indicates impossibility
    1 indicates certainty

    a id203 function, p, (or id203 distribution)

assigns id203 mass to events in the event
space, f.
    p : f     [0,1]
    p(  ) = 1
    countable additivity:  for disjoint events aj in f

p(   j aj) =   j p(aj)

    we call p(a)    the id203 of event a   .
    well-defined id203 space consists of

    sample space   
    event space f
    id203 function p

andrew mccallum, umass amherst

id203 (more intuitively)

    repeat an experiment many, many times.

(let t = number of times.)

    count the number of basic outcomes that are

a member of event a.
(let c = this count.)

    the ratio c/t will approach (some unknown)

but constant value.

    call this constant    the id203 of event a   ;

write it p(a).

why is the id203 this ratio of counts?  

stay tuned! id113 at end.

andrew mccallum, umass amherst

example: counting

       a coin is tossed 3 times.

what is the likelihood of 2 heads?   
    experiment: toss a coin three times,

   = {hhh, hht, hth, htt, thh, tht, tth, ttt}

    event: basic outcome has exactly 2 h   s

a = {thh, hth, hht}

    run experiment 1000 times (3000 coin tosses)
    counted 373 outcomes with exactly 2 h   s
    estimated p(a) = 373/1000 = 0.373

andrew mccallum, umass amherst

example: uniform distribution

       a fair coin is tossed 3 times.

what is the likelihood of 2 heads?   
    experiment: toss a coin three times,

   = {hhh, hht, hth, htt, thh, tht, tth, ttt}

    event: basic outcome has exactly 2 h   s

a = {thh, hth, hht}

    assume a uniform distribution over outcomes

    each basic outcome is equally likely
    p({hhh}) = p({hht}) =     = p({ttt})

    p(a) = |a| / |  | = 3 / 8 = 0.375

andrew mccallum, umass amherst

id203 (again)

    a id203 is a number between 0 and 1.

    0 indicates impossibility
    1 indicates certainty

    a id203 function, p, (or id203 distribution)

distributes id203 mass of 1 throughout the event
space, f.
    p : f     [0,1]
    p(  ) = 1
    countable additivity:  for disjoint events aj in f

p(   j aj) =   j p(aj)

    the above are axioms of id203 theory
   

immediate consequences:
    p(   ) = 0,  p(a) = 1 - p(a),  a     b -> p(a)     p(b),

  a       p(a) = 1, for a = basic outcome.

andrew mccallum, umass amherst

vocabulary summary

    experiment 
    sample
    sample space
    event

= a repeatable process
= a possible outcome
= all samples for an experiment
= a set of samples

    id203
distribution

    uniform

distribution

= assigns a id203 to each sample

= all samples are equi-probable

andrew mccallum, umass amherst

collaborative exercise

       you roll a fair die, then roll it again.  what is
the id203 that you get the same number
from both rolls?

    explain in terms of event spaces and basic

outcomes.

andrew mccallum, umass amherst

joint and id155

    joint id203 of a and b:

p(a     b) is usually written p(a,b)

    id155 of a given b:

p(a|b) = p(a,b)
                p(b)

  

a

b

a     b

updated id203 of an
  event given some evidence
p(a) = prior id203 of a
p(a|b) = posterior id203
  of a given evidence b

andrew mccallum, umass amherst

joint id203 table

what does it look like    under the hood   ?
p(precipitation, temperature)
rain
0.00
0.00
0.01
0.03
0.04
0.04
0.03
0.03
0.02
0.02

sleet
0.00
0.00
0.01
0.01
0.00
0.00
0.00
0.00
0.00
0.00

sun
0.09
0.08
0.05
0.06
0.06
0.06
0.07
0.07
0.08
0.08

10s
20s
30s
40s
50s
60s
70s
80s
90s
100s

snow
0.01
0.02
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00

andrew mccallum, umass amherst

it takes 40 numbers

id155 table

what does it look like    under the hood   ?
p(precipitation | temperature)
rain
0.0
0.0
0.1
0.3
0.4
0.4
0.3
0.3
0.2
0.2

sleet
0.0
0.0
0.1
0.1
0.0
0.0
0.0
0.0
0.0
0.0

10s
20s
30s
40s
50s
60s
70s
80s
90s
100s

sun
0.9
0.8
0.5
0.6
0.6
0.6
0.7
0.7
0.8
0.8

snow
0.1
0.2
0.3
0.0
0.0
0.0
0.0
0.0
0.0
0.0

andrew mccallum, umass amherst

it takes 40 numbers

two useful rules

    multiplication rule

p(a,b) = p(a|b) p(b)

(equivalent to id155 definition from previous slide)

    total id203 rule (sum rule)

p(a) = p(a,b) + p(a,b)

or more generally, if b can take on n values

p(a) =   i=1..n p(a,bi)

(from additivity axiom)

andrew mccallum, umass amherst

bayes rule

    p(a,b) = p(b,a), since p(a     b) = p(b     a)
    therefore p(a|b) p(b) = p(b|a) p(a), and thus   
    p(a|b) = p(b|a)p(a)

                    p(b)

   normalizing constant   

  

a

b

a     b

andrew mccallum, umass amherst

bayes rule lets you swap the
  order of the dependence
  between events   
calculate p(a|b) in terms of p(b|a).

reverend thomas bayes

    rumored to have been tutored by

de moivre.

    was elected a fellow of the royal

society in 1742 despite the fact
that at that time he had no
published works on mathematics!

   

   essay towards solving a problem
in the doctrine of chances   
published in the philosophical
transactions of the royal society
of london in 1764.

same year mozart wrote his symphony #1 in e-   at.

1702 - 1761

andrew mccallum, umass amherst

independence

    can we compute p(a,b) from p(a) and p(b)?
    recall:

p(a,b) = p(b|a) p(a)   (multiplication rule)

    we are almost there: how does p(b|a) relate to p(b)?

p(b|a) = p(b) iff b and a are independent!

    examples:

    two coin tosses
    color shirt i   m wearing today, what bill clinton had for breakfast.

    two events a, b are independent from each other if

p(a,b) = p(a) p(b)     equivalent to p(b) = p(b|a) (if p(a)     0)

    otherwise they are dependent.

andrew mccallum, umass amherst

joint id203 with independence
independence means we need far fewer numbers!

p(precipitation, temperature)              p(precipitation) p(temperature)

sun
0.09
0.08
0.05
0.06
0.06
0.06
0.07
0.07
0.08
0.08

rain
0.00
0.00
0.01
0.03
0.04
0.04
0.03
0.03
0.02
0.02

sleet
0.00
0.00
0.01
0.01
0.00
0.00
0.00
0.00
0.00
0.00

snow
0.01
0.02
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00

10s
20s
30s
40s
50s
60s
70s
80s
90s
100s

it takes 40 numbers

andrew mccallum, umass amherst

sun
0.5

rain
0.3

sleet
0.05

snow
0.15

10s
20s
30s
40s
50s
60s
70s
80s
90s
100s

0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
it takes 14 numbers

chain rule

p(a1, a2, a3, a4,    an) =

 p(a1| a2, a3, a4,    an)
 p(a2, a3, a4,    an)

analogous to p(a,b) = p(a|b) p(b).

andrew mccallum, umass amherst

chain rule

p(a1, a2, a3, a4,    an) =

 p(a1| a2, a3, a4,    an)
 p(a2| a3, a4,    an)
 p(a3, a4,    an)

andrew mccallum, umass amherst

chain rule

p(a1, a2, a3, a4,    an) =

 p(a1| a2, a3, a4,    an)
 p(a2| a3, a4,    an)
 p(a3| a4,    an)
    
 p(an)

furthermore, if a1   an are all independent from each other   

andrew mccallum, umass amherst

chain rule

if a1   an are all independent from each other
p(a1, a2, a3, a4,    an) =

 p(a1)
 p(a2)
 p(a3)
    
 p(an)

andrew mccallum, umass amherst

example: two ways, same answer

       a fair coin is tossed 3 times.

what is the likelihood of 3 heads?   
    experiment: toss a coin three times,

   = {hhh, hht, hth, htt, thh, tht, tth,
ttt}

    event: basic outcome has exactly 3 h   s

a = {hhh}
    chain rule

p(hhh) = p(h) p(h|h) p(h|hh)

        = p(h) p(h) p(h) = (1/2)3 = 1/8

    size of event spaces

p(hhh) = |a| / |  | = 1/8

andrew mccallum, umass amherst

collaborative exercise

   

   suppose one is interested in a rare syntactic
construction, parasitic gaps, which occur on average
once in 100,000 sentences.  peggy linguist has
developed a complicated pattern matcher that
attempts to identify sentences with parasitic gaps.
it's pretty good,but its not perfect: if a sentence has a
parasitic gap, it will say so with id203 0.95, if it
doesn't it will wrongly say so with id203 0.005.

    suppose the test says that a sentence contains a

parasitic gap.  what is the id203 that this is true?

andrew mccallum, umass amherst

finding most likely posterior event

    p(a|b) = p(b|a)p(a)    (for example, p(   lies   =noun|   more lies ahead   )

                    p(b)

    want to find most likely a given b,

but p(b) is sometimes a pain to calculate   

    arg maxa p(b|a)p(a) = arg maxa p(b|a)p(a)

                      p(b)

because b is constant while changing a

andrew mccallum, umass amherst

random variables

    a random variable is a function x :        q

    in general q=   n, but more generally simply q=   
    makes it easier to talk about numerical values related to event

space

    random variable is discrete if q is countable.
    example: coin q={0,1}, die q=[1,6]
    called an indicator variable or bernoulli trial if q     {0,1}

    example:

    suppose event space comes from tossing two dice.
    we can define a random variable x that is the sum of their faces
    x :        {2,..12}

because a random variable has a numeric range, we can often do math more
easily by working with values of the random variable than directly with events.

andrew mccallum, umass amherst

id203 mass function
    p(x=x) = p(ax) where ax = {a        : x(a)=x}
    often written just p(x), when x is clear from context.
write x ~ p(x) for    x is distributed according to p(x)   .
in english:
    id203 mass function, p   
    maps some value x (of random variable x) to   
    the id203 random variable x taking value x
    equal to the id203 of the event ax
    this event is the set of all basic outcomes, a, for which the

   

random variable x(a) is equal to x.

    example, again:

    event space = roll of two dice; e.g. a=<2,5>, |  |=36
    random variable x is the sum of the two faces
    p(x=4) = p(a4), a4 = {<1,3>, <2,2>, <3,1>}, p(a4) = 3/36

random variables will be used throughout the
introduction to id205, coming next class.

andrew mccallum, umass amherst

expected value

        is a weighted average, or mean, of a random variable

e[x] =   x     x(  ) x    p(x)

    example:

    x = value of one roll of a fair six-sided die:

e[x] = (1+2+3+4+5+6)/6 = 3.5

    x = sum of two rolls   

e[x] = 7

   

if y ~ p(y=y) is a random variable, then any function g(y)
defines a new random variable, with expected value

e[g(y)] =   y     y(  ) g(y)    p(y)

    for example,

    let g(y) = ay+b, then e[g(y)] = a e[y] + b
    e[x+y] = e[x] + e[y]
    if x and y are independent, e[xy] = e[x] e[y]

andrew mccallum, umass amherst

variance

    variance, written   2
    measures how consistent the value is over

multiple trials
       how much on average the variable   s value differs from

the its mean.   

    var[x] = e[(x-e[x])2]

    standard deviation =     var[x] =   

andrew mccallum, umass amherst

joint and conditional probabilities

with random variables

    joint and id155 rules

    analogous to id203 of events!

    joint id203

p(x,y) = p(x=x, y=y)

    marginal distribution p(x) obtained from the joint p(x,y)

p(x) =   y p(x,y)       (by the total id203 rule)

    bayes rule

    chain rule

p(x|y) = p(y|x) p(x) / p(y)

p(w,x,y,z) = p(z) p(y|z) p(x|y,z) p(w|x,y,z)

andrew mccallum, umass amherst

parameterized distributions

    common id203 mass functions with

same mathematical form   

       just with different constants employed.
    a family of functions, called a distribution.
    different numbers that result in different

members of the distribution, called
parameters.

    p(a;b)

andrew mccallum, umass amherst

binomial distribution

    a discrete distribution with two outcomes

   = {0, 1}      

         (hence bi-nomial)

    make n experiments.
   toss a coin n times.   
   

   

interested in the id203 that r of the n
experiments yield 1.

    careful!  it   s not a uniform distribution. (q = prob of h)

   

where

andrew mccallum, umass amherst

! p(r=r|n,q)=nr" # $ % &     qr(1(q)n(r! nr" # $ % &     =n!(n(r)!r!pictures of binomial distribution

binomial (n,q):

andrew mccallum, umass amherst

multinomial distribution

    a discrete distribution with m outcomes

   = {0, 1, 2,    m}

    make n experiments.
    examples:    roll a m-sided die n times.   

   assuming each word is independent from the next,
generate an n-word sentence from a vocabulary of size m.   

   

interested in the id203 of obtaining counts
c = c1, c2,    cm from the n experiments.

andrew mccallum, umass amherst

unigram language model

! p(c|n,q)=n!c1!c2!...cm!" # $ % &     (qi)cii=1..m(parameter estimation

    we have been assuming that p is given, but

most of the time it is unknown.

    so we assume a parametric family of

distributions and estimate its parameters   

       by finding parameter values most likely to

have generated the observed data (evidence).

       treating the parameter value as a random

variable!
not the only way of doing parameter estimation.

this is maximum likelihood parameter estimation.

andrew mccallum, umass amherst

maximum likelihood parameter estimation

example: binomial

    toss a coin 100 times, observe r  heads
    assume a binomial distribution

    order doesn   t matter, successive flips are independent
    one parameter is q  (id203 of flipping a head)
    binomial gives p(r|n,q).  we know r and n.
    find arg maxq p(r|n, q)

andrew mccallum, umass amherst

maximum likelihood parameter estimation

example: binomial

    toss a coin 100 times, observe r  heads
    assume a binomial distribution

    order doesn   t matter, successive flips are independent
    one parameter is q  (id203 of flipping a head)
    binomial gives p(r|n,q).  we know r and n.
    find arg maxq p(r|n, q)

(notes for board)

andrew mccallum, umass amherst

our familiar ratio-of-counts
is the maximum likelihood estimate!

! likelihood=p(r=r|n,q)=nr" # $ % &     qr(1(q)n(rlog(likelihood=l=log(p(r|n,q)))log(qr(1(q)n(r)=rlog(q)+(n(r)log(1(q)*l*q=rq(n(r1(q+r(1(q)=(n(r)q+q=rnbinomial parameter estimation examples

    make 1000 coin flips, observe 300 heads

    p(heads) = 300/1000

    make 3 coin flips, observe 2 heads

    p(heads) = 2/3 ??

    make 1 coin flips, observe 1 tail

    p(heads) = 0 ???
    make 0 coin flips
    p(heads) = ???

    we have some    prior    belief about p(heads) before

we see any data.

    after seeing some data, we have a    posterior    belief.

andrew mccallum, umass amherst

maximum a posteriori
 parameter estimation

    we   ve been finding the parameters that maximize

    p(data|parameters),
not the parameters that maximize
    p(parameters|data)    (parameters are random variables!)

    p(q|n,r) = p(r|n,q) p(q|n) = p(r|n,q) p(q)
      p(r|n)               constant

    and let p(q) = 2 q(1-q)

andrew mccallum, umass amherst

maximum a posteriori parameter estimation

example: binomial

2

andrew mccallum, umass amherst

! posterior=p(r|n,q)p(q)=nr" # $ % &     qr(1(q)n(r(6q(1(q))log(posterior=l)log(qr+1(1(q)n(r+1)=(r+1)log(q)+(n(r+1)log(1(q)*l*q=(r+1)q((n(r+1)1(q+(r+1)(1(q)=(n(r+1)q+q=r+1n+2bayesian decision theory

    we can use such techniques for choosing

among models:
    which among several models best explains the data?

    likelihood ratio

p(model1 | data)  =   p(data|model1) p(model1)
p(model2 | data)       p(data|model2) p(model2)

andrew mccallum, umass amherst

...back to our example: french vs english

    p(french | glacier, melange) versus

p(english | glacier, melange) ?

    we have real data for
    shakespeare   s haid113t
    charles dickens    oliver twist

    p(haid113t |    hand   ,    death   )
p(oliver |    hand   ,    death   )

andrew mccallum, umass amherst

continuing homework assignment?

    ec for whatever you hand in by tonight.

    for next week:

    more time to create your own grammar
    modify parser to keep trace, and print parse trees
    try an additional grammar of your own creation,

and investigate ambiguities

    work in small teams!

andrew mccallum, umass amherst

document classification

with probabilistic id38

   temporal reasoning for
planning has long
been studied formally.
we discuss the semantics
of several planning...   
  

.

machine
learning

planning

prog. lang.
semantics

garbage
collection

multimedia gui

testing
document:

categories:

training
  data:

   neural networks
and other machine
learning methods 
of classi   cation      

andrew mccallum, umass amherst

   plannning 
with temporal
reasoning 
has been      

      based on
the semantics
of program
dependence   

   garbage
collection for
strongly-typed
languages       

   multimedia
streaming
video for      

   user
studies
of gui      

a probabilistic approach to classification:

   na  ve bayes   

pick the most probable class, given the evidence:

- a class (like    planning   )
- a document (like    language intelligence proof...   )

bayes rule:

   na  ve bayes   :

andrew mccallum, umass amherst

- the i th word in d (like    proof   )

