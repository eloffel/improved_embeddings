machine translation

lecture #17

introduction to natural language processing

cmpsci 585, fall 2007

andrew mccallum

(including slides from michael collins, and dan klein)

the challenges of machine translation

lexicalambiguityexample1:bookthe   ightreservarreadthebooklibroexample2:theboxwasinthepenthepenwasonthetableexample3:killamanmatarkillaprocessacabardifferingwordordersenglishwordorderissubject   verb   objectjapanesewordorderissubject   object   verbenglish:ibmboughtlotusjapanese:ibmlotusboughtenglish:sourcessaidthatibmboughtlotusyesterdayjapanese:sourcesyesterdayibmlotusboughtthatsaidsyntacticstructureisnotpreservedacrosstranslationsthebottle   oatedintothecavelabotellaentroalacuerva   otando(thebottleenteredthecave   oating)syntacticambiguitycausesproblemsjohnhitthedogwiththestickjohngolpeoelperroconelpalo/queteniaelpalopronounresolutionthecomputeroutputsthedata;itisfast.lacomputadoraimprimelosdatos;esrapidathecomputeroutputsthedata;itisstoredinascii.lacomputadoraimprimelosdatos;estanalmacendosenasciidifferingtreatmentsoftensefromdorret.al1998:marywenttomexico.duringherstayshelearnedspanish.wentiba(simplepast/preterit)marywenttomexico.whenshereturnedshestartedtospeakspanish.wentfue(ongoingpast/imperfect)thebesttranslationmaynotbe1-1(frommanningandschuetze):accordingtooursurvey,1988salesofmineralwaterandsoftdrinksweremuchhigherthanin1987,re   ectingthegrowingpopularityoftheseproducts.coladrinkmanufacturersinparticularachievedaboveaveragegrowthrates.quantauxeauxmineralesetauxlimonades,ellesrecontrenttoujoursplusd   adeptes.eneffetnotresondagefaitressortirdesventesnettementsuperieuresacellesde1987,pourlesboissonsabasedecolanotamment.withregardtothemineralwatersandthelemonades(softdrinks)theyencounterstillmoreusers.indeedoursurveymakesstandoutthesalesclearlysuperiortothosein1987forcola-baseddrinksespecially.machine translation: example

history

    1950   s: intensive research activity in mt
    1960   s: direct word-for-word replacement
    1966 (alpac): nrc report on mt

    conclusion: mt no longer worthy of serious 

scientific investigation.

    1966-1975: `recovery period   
    1975-1985: resurgence (europe, japan)
    1985-present: gradual resurgence (us)

http://ourworld.compuserve.com/homepages/wjhutchins/mts-93.htm

levels of transfer

interlingua

semantic
composition

semantic
decomposition

(vauquois 
triangle)

semantic
structure

semantic
analysis

syntactic
structure

syntactic
analysis

word

structure

morphological
analysis

source text

semantic
transfer

syntactic
transfer

direct

semantic
structure

semantic
generation

syntactic
structure

syntactic
generation

word

structure

morphological
generation

target text

general approaches

    rule-based approaches

    expert system-like rewrite systems
    interlingua methods (analyze and generate)
    lexicons come from humans
    can be very fast, and can accumulate a lot of knowledge over 

time (e.g. systran)

    statistical approaches
    word-to-word translation
    phrase-based translation
    syntax-based translation (tree-to-tree, tree-to-string)
    trained on parallel corpora
    usually noisy-channel (at least in spirit)

the coding view

       one naturally wonders if the problem of 

translation could conceivably be treated as a 
problem in cryptography.  when i look at an article 
in russian, i say:    this is really written in english, 
but it has been coded in some strange symbols. i 
will now proceed to decode.         

    warren weaver (1955:18, quoting a letter he wrote in 1947)

mt system components

language model

translation model

source
p(e)

best
e

e

channel
p(f|e)

f

decoder

observed     

f

why not simply p(e|f)?
more data for p(e).

argmax p(e|f) = argmax p(f|e)p(e)

e

e
finds an english translation which is both fluent 
and semantically faithful to the french source

abriefintroductiontostatisticalmtparallelcorporaareavailableinseverallanguagepairsbasicidea:useaparallelcorpusasatrainingsetoftranslationexamplesclassicexample:ibmworkonfrench-englishtranslation,usingthecanadianhansards.(1.7millionsentencesof30wordsorlessinlength).ideagoesbacktowarrenweaver(1949):suggestedapplyingstatisticalandcryptanalytictechniquestotranslation.examplefromkoehnandknighttutorialtranslationfromspanishtoenglish,candidatetranslationsbasedonalone:quehambretengoyowhathungerhave=0.000014hungryiamso=0.000001iamsohungry=0.0000015haveithathunger=0.000020with:quehambretengoyowhathungerhave=0.0000140.000001hungryiamso=0.0000010.0000014iamsohungry=0.00000150.0001haveithathunger=0.0000200.00000098thesentencealignmentproblemmighthave1003sentences(insequence)ofenglish,987sentences(insequence)offrench:butwhichenglishsentence(s)correspondstowhichfrenchsentence(s)?mighthave1-1alignments,1-2,2-1,2-2etc.thesentencealignmentproblemclearlyneededbeforewecantrainatranslationmodelalsousefulforothermulti-lingualproblemstwobroadclassesofmethodswe   llcover:   methodsbasedonsentencelengthsalone.   methodsbasedonlexicalmatches,or   cognates   .sentencelengthmethods(galeandchurch,1993):methodassumesparagraphalignmentisknown,sentencealignmentisnotknown.de   ne:   =lengthofenglishsentence,incharacters   =lengthoffrenchsentence,incharactersassumption:givenlength,lengthhasagaussian/normaldistributionwithmean,andvarianceforsomeconstantsand.result:wehaveacostforanypairsoflengthsand.eachpossiblealignmenthasacostinthiscase,iflengthofis,andlengthofis,totalcostiswheretermscorrespondtocostsfor1-1,1-2,2-1and2-2alignments.dynamicprogrammingcanbeusedtosearchforthelowestcostalignmentmethodsbasedoncognatesintuition:relatedwordsindifferentlanguagesoftenhavesimilarspellingse.g.,governmentandgouvernementcognatematchescan   anchor   sentence-sentencecorrespondencesamethodfrom(church1993):trackall4-gramsofcharacterswhichareidenticalinthetwotexts.amethodfrom(melamed1993),measuressimilarityofwordsand:whereisthelongestcommonsubsequence(notnecessarilycontiguous)inand.e.g.,government,gouvernementtoday

    the components of a simple mt system

    you already know about the lm
    word-alignment based tms

    ibm models 1 and 2, id48 model

    a simple decoder

    not today

    more complex word-level and phrase-level tms
    tree-to-tree and tree-to-string tms
    more sophisticated decoders

a word-level tm?

    what might a model of p(f|e) look like?

what can go 
wrong here?

how to estimate this?

ibm model 1 (brown 93)

    alignments: a hidden vector called an alignment specifies which 

english source is responsible for each french target word.

1-to-many alignments

many-to-1 alignments

many-to-many alignments

monotonic translation

japan shaken by two new quakes

le japon secou   par deux nouveaux s  ismes 

local order change

japan is at the junction of four tectonic plates

le japon est au confluent de quatre plaques tectoniques

ibm model 2

    alignments tend to the diagonal (broadly at least)

    other schemes for biasing alignments towards the diagonal:

    relative alignment
    asymmetric distances
    learning a multinomial over distances

ibm model 2 - alternative

    model                                 as a simple dense table.

    in other words, a simple multinomial over i for each j, i, j

    e.g. d(i=2 | j=1, i=6, j=7)

how to learn these parameters

from pairs of sentences?

em for models 1/2

    model 1 parameters:

translation probabilities (word pairs)
distortion parameters (1 only)

    start with  
    for each sentence:

            uniform, including

    for each french position j

    calculate posterior over english positions

    (or just use best single alignment)
    increment count of word fj with word ei by these amounts
    also re-estimate distortion probabilities for model 2

    iterate until convergence

notation switch:

l = i      length of english document
m = j    length of french document

ibmmodel2onlydifference:wenowintroducealignmentordistortionparametersid203that   thfrenchwordisconnectedto   thenglishword,givensentencelengthsofandareandrespectivelyde   negivesnote:model1isaspecialcaseofmodel2,whereforall.anexampleandtheprogramhasbeenimplementedleprogrammeaetemisenapplicationibmmodel2:thegenerativeprocesstogenerateafrenchstringfromanenglishstring:step1:pickthelengthof(alllengthsequallyprobable,id203)step2:pickanalignmentwithid203step3:pickthefrenchwordswithid203the   nalresult:em training of alignment and translation parameters

ahiddenvariableproblemwehave:and:whereisthesetofallpossiblealignments.ahiddenvariableproblemtrainingdataisasetofpairs,likelihoodiswhereisthesetofallpossiblealignments.weneedtomaximizethisfunctionw.r.t.thetranslationparameters,andthealignmentprobabilitiesemcanbeusedforthisproblem:initializeparametersrandomly,andateachiterationchoosewherearetheparametervaluesatthe   thiteration.models1and2haveasimplestructurewehave,,andwherewecanthinkofthepairsasbeinggeneratedindependentlyacrucialstepintheemalgorithmsaywehavethefollowingpair:andtheprogramhasbeenimplementedleprogrammeaetemisenapplicationgiventhatwasgeneratedaccordingtomodel2,whatistheid203that?formally:theanswerfollowsdirectlybecausethepairsareindependent:(1)(2)where(2)followsfrom(1)becauseageneralresultalignmentprobabilitieshaveasimplesolution!e.g.,saywehave,,andtheprogramhasbeenimplementedleprogrammeaetemisenapplicationid203of   mis   beingconnectedto   the   :wheretheemalgorithmformodel2de   neforisthe   thenglishsentenceforisthe   thfrenchsentenceisthelengthofisthelengthofisthe   thwordinisthe   thwordincurrentparametersareforallwe   llseehowtheemalgorithmre-estimatestheandparametersstep1:calculatethealignmentprobabilitiescalculateanarrayofalignmentprobabilities(for,,):where,,andi.e.,theid203ofbeingalignedto.step2:calculatingtheexpectedcountscalculatethetranslationcountsisexpectednumberoftimesthatisalignedwithinthecorpusstep2:calculatingtheexpectedcountscalculatethesourcecountsisexpectednumberoftimesthatisalignedwithanyfrenchwordinthecorpusstep2:calculatingtheexpectedcountscalculatethealignmentcountshere,isexpectednumberoftimesthatisalignedtoinenglish/frenchsentencesoflengthsandrespectivelyisnumberoftimesthatwehavesentencesandoflengthsandrespectivelystep3:re-estimatingtheparametersnewtranslationprobabilitiesarethende   nedasnewalignmentprobabilitiesarede   nedasthisde   nesthemappingfromtoasummaryoftheemprocedurestartwithparametersasforallcalculatealignmentprobabilitiesundercurrentparameterscalculateexpectedcounts,,,andfromthealignmentprobabilitiesre-estimateandfromtheexpectedcountssome examples of training

anexampleoftrainingmodels1and2examplewillusefollowingtranslations:e[1]=thedogf[1]=lechiene[2]=thecatf[2]=lechate[3]=thebusf[3]=l   autobusnb:iwon   tuseanullwordinitial(random)parameters:thele0.23thechien0.2thechat0.11thel   0.25theautobus0.21dogle0.2dogchien0.16dogchat0.33dogl   0.12dogautobus0.18catle0.26catchien0.28catchat0.19catl   0.24catautobus0.03busle0.22buschien0.05buschat0.26busl   0.19busautobus0.27alignmentprobabilities:ijka(i,j,k)1100.5264232379597262100.4735767620402741200.5525179956058172200.4474820043941831110.4665326020665332110.5334673979334671210.3563645444225072210.6436354555774931120.5719504383362472120.4280495616637531220.4390813117245082220.560918688275492expectedcounts:thele0.99295584002626thechien0.552517995605817thechat0.356364544422507thel   0.571950438336247theautobus0.439081311724508dogle0.473576762040274dogchien0.447482004394183dogchat0dogl   0dogautobus0catle0.533467397933467catchien0catchat0.643635455577493catl   0catautobus0busle0buschien0buschat0busl   0.428049561663753busautobus0.560918688275492oldandnewparameters:oldnewthele0.230.34thechien0.20.19thechat0.110.12thel   0.250.2theautobus0.210.15dogle0.20.51dogchien0.160.49dogchat0.330dogl   0.120dogautobus0.180catle0.260.45catchien0.280catchat0.190.55catl   0.240catautobus0.030busle0.220buschien0.050buschat0.260busl   0.190.43busautobus0.270.57thele0.230.340.460.560.640.71thechien0.20.190.150.120.090.06thechat0.110.120.10.080.060.04thel   0.250.20.170.150.130.11theautobus0.210.150.120.10.080.07dogle0.20.510.460.390.330.28dogchien0.160.490.540.610.670.72dogchat0.3300000dogl   0.1200000dogautobus0.1800000catle0.260.450.410.360.30.26catchien0.2800000catchat0.190.550.590.640.70.74catl   0.2400000catautobus0.0300000busle0.2200000buschien0.0500000buschat0.2600000busl   0.190.430.470.470.470.48busautobus0.270.570.530.530.530.52after20iterations:thele0.94thechien0thechat0thel   0.03theautobus0.02dogle0.06dogchien0.94dogchat0dogl   0dogautobus0catle0.06catchien0catchat0.94catl   0catautobus0busle0buschien0buschat0busl   0.49busautobus0.51model2hasseverallocalmaxima   goodone:thele0.67thechien0thechat0thel   0.33theautobus0dogle0dogchien1dogchat0dogl   0dogautobus0catle0catchien0catchat1catl   0catautobus0busle0buschien0buschat0busl   0busautobus1model2hasseverallocalmaxima   badone:thele0thechien0.4thechat0.3thel   0theautobus0.3dogle0.5dogchien0.5dogchat0dogl   0dogautobus0catle0.5catchien0catchat0.5catl   0catautobus0busle0buschien0buschat0busl   0.5busautobus0.5anotherbadone:thele0thechien0.33thechat0.33thel   0theautobus0.33dogle1dogchien0dogchat0dogl   0dogautobus0catle1catchien0catchat0catl   0catautobus0busle0buschien0buschat0busl   1busautobus0alignmentparametersforgoodsolution:logid203alignmentparametersfor   rstbadsolution:logid203alignmentparametersforsecondbadsolution:logid203improvingtheconvergencepropertiesofmodel2outof100randomstarts,only60convergedtothebestlocalmaximamodel1convergestothesame,globalmaximumeverytime(seethebrownet.alpaper)methodinibmpaper:runmodel1toestimateparameters,thenusetheseastheinitialparametersformodel2in100testsusingthismethod,model2convergedtothecorrectpointeverytime.evaluation of machine translation

evaluationofmachinetranslationsystemsmethod1:humanevaluationsaccurate,butexpensive,slow   cheap   andfastevaluationisessentialwe   lldiscussoneprominentmethod:id7(papineni,roukos,wardandzhu,2002)evaluationofmachinetranslationsystemsid7(papineni,roukos,wardandzhu,2002):candidate1:itisaguidetoactionwhichensuresthatthemilitaryalwaysobeysthecommandsoftheparty.candidate2:itistoinsurethetroopsforeverhearingtheactivityguidebookthatpartydirect.reference1:itisaguidetoactionthatensuresthatthemilitarywillforeverheedpartycommands.reference2:itistheguidingprinciplewhichguaranteesthemilitaryforcesalwaysbeingunderthecommandoftheparty.reference3:itisthepracticalguideforthearmyalwaystoheedthedirectionsoftheparty.unigramprecisionunigramprecisionofacandidatetranslation:whereisnumberofwordsinthecandidate,isthenumberofwordsinthecandidatewhichareinatleastonereferencetranslation.e.g.,candidate1:itisaguidetoactionwhichensuresthatthemilitaryalwaysobeysthecommandsoftheparty.(onlyobeysismissingfromallreferencetranslations)modi   edunigramprecisionproblemwithunigramprecision:candidate:thethethethethethethereference1:thecatsatonthematreference2:thereisacatonthematprecision=7/7=1???modi   edunigramprecision:   clipping      eachwordhasa   cap   .e.g.,cap(the)=2   acandidatewordcanonlybecorrectamaximumoftimes.e.g.,incandidateabove,,andtheiscorrecttwiceinthecandidatemodi   edid165precisioncangeneralizemodi   edunigramprecisiontootherid165s.forexample,forcandidates1and2above:precisionaloneisn   tenoughcandidate1:ofthereference1:itisaguidetoactionthatensuresthatthemilitarywillforeverheedpartycommands.reference2:itistheguidingprinciplewhichguaranteesthemilitaryforcesalwaysbeingunderthecommandoftheparty.reference3:itisthepracticalguideforthearmyalwaystoheedthedirectionsoftheparty.butrecallisn   tusefulinthiscasestandardmeasureusedinadditiontoprecisionisrecall:whereisnumberofid165sincandidatethatarecorrect,isnumberofwordsinthereferences.candidate1:ialwaysinvariablyperpetuallydo.candidate2:ialwaysdoreference1:ialwaysdoreference1:iinvariablydoreference1:iperpetuallydosentencebrevitypenaltystep1:foreachcandidate,computeclosestmatchingreference(intermsoflength)e.g.,ourcandidateislength,referencesarelength.bestmatchisoflength.step2:sayisthelengthofthe   thcandidate,islengthofbestmatchforthe   thcandidate,thencompute(ithink!fromthepapinenipaper,althoughmightmakemoresense?)step3:computebrevitypenaltyifife.g.,ifforall(candidatesarealways10%tooshort)thenthefinalscorecorpusprecisionforanyid165isi.e.numberofcorrectngramsinthecandidates(after   clipping   )dividedbytotalnumberofngramsinthecandidatesfinalscoreistheni.e.,multipliedbythegeometricmeanoftheunigram,bigram,trigram,andfour-gramprecisionsdecoding

    in these word-to-word models
    finding best alignments is easy
    finding translations is hard (why?)

bag    generation    (decoding)

bag generation is a tsp

    imagine bag generation 

with a bigram lm
    words are nodes
    edge weights are p(w|

w   )

    valid sentences are 

hamiltonian paths

    not the best news for 

word-based mt!

it

not

is

.

clear

decoding, anyway

    simplest possible decoder:

    enumerate sentences, score each with tm and lm

    greedy decoding:

    assign each french word it   s most likely english translation
    operators:

    change a translation
    insert a word into the english (zero-fertile french)
    remove a word from the english (null-generated french)
    swap two adjacent english words

    do hill-climbing (or annealing)

    you should be able to build a model 1/2 translator now
    more on word alignment, decoding next class

greedy decoding

