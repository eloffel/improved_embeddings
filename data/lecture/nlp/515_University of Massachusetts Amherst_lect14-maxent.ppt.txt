maximum id178

lecture #13

introduction to natural language processing

cmpsci 585, fall 2007

university of massachusetts  amherst

andrew mccallum

(slides from jason eisner and dan klein)

1

summary of half of the course (statistics)

id203 is useful
    we love id203 distributions!
    we   ve learned how to define & use p(   ) functions.
    pick best output text t from a set of candidates
    id103; machine translation; ocr; spell correction...
    maximize p1(t) for some appropriate distribution p1
   
    maximize p(t | i); equivalently maximize joint id203 p(i,t)
    id103 & other tasks above are cases of this too:

    pick best annotation t for a fixed input i

    often define p(i,t) by noisy channel: p(i,t) = p(t) * p(i | t)

text categorization; parsing; part-of-speech tagging    

    we   re maximizing an appropriate p1(t) defined by p(t | i)
    pick best id203 distribution (a meta-problem!)

really, pick best parameters   : train id48, pid18, id165s, clusters    

   
    maximum likelihood; smoothing;
    smoothing: max p(  |data) = max p(  , data) =p(  )p(data|  )

2

summary of other half of the course (linguistics)

id203 is flexible

    we love id203 distributions!
    we want p(   ) to define id203 of linguistic objects

    we   ve learned how to define & use p(   ) functions.

fsts; viterbi, collocations)

    sequences of words, tags, morphemes, phonemes (id165s, id122s,
    vectors (na  ve bayes; id91 word senses)
    trees of (non)terminals (pid18s; cky, earley)

    we   ve also seen some not-so-probabilistic stuff

    syntactic features, morphology.  could be stochasticized?
    methods can be quantitative & data-driven but not fully probabilistic:

id91, collocations,   

    but probabilities have wormed their way into most things
    p(   ) has to capture our intuitions about the ling. data

3

really so alternative?
an alternative tradition
    old ai hacking technique:

    possible parses (or whatever) have scores.
    pick the one with the best score.
    how do you define the score?

    completely ad hoc!
    throw anything you want into the stew
    add a bonus for this, a penalty for that, etc.

penalties by hand to improve performance.

       learns    over time     as you adjust bonuses and
    total kludge, but totally flexible too    
    can throw in any intuitions you might have

4

really so alternative?
an alternative tradition
    old ai hacking technique:

probabilistic revolution
not really a revolution,

    possible parses (or whatever) have scores.
    pick the one with the best score.
critics say
    how do you define the score?

    completely ad hoc!
log-probabilities no more
    throw anything you want into the stew
than scores in disguise
    add a bonus for this, a penalty for that, etc.
       learns    over time     as you adjust bonuses and
   we   re just adding stuff up
like the old corrupt regime
    total kludge, but totally flexible too    
did,    admits spokesperson
    can throw in any intuitions you might have

penalties by hand to improve performance.    

5

nuthin    but adding weights
    id165s:     + log p(w7 | w5,w6) + log(w8 | w6, w7) +    
    pid18: log p(np vp | s) + log p(papa | np) + log p(vp pp | vp)    
    id48 tagging:     + log p(t7 | t5, t6) + log p(w7 | t7) +    
    noisy channel: [log p(source)] + [log p(data | source)]
    na  ve bayes:
log p(class) + log p(feature1 | class) + log p(feature2 | class)    
    note: just as in id203, bigger weights are better.

6

nuthin    but adding weights
    id165s:     + log p(w7 | w5,w6) + log(w8 | w6, w7) +    
    pid18: log p(np vp | s) + log p(papa | np) + log p(vp pp | vp)    
    id48 tagging:     + log p(t7 | t5, t6) + log p(w7 | t7) +    
    noisy channel: [log p(source)] + [log p(data | source)]
    na  ve bayes:

log(class) + log(feature1 | class) + log(feature2 | class) +    
    can regard any linguistic object as a collection of features
(here, doc = a collection of words, but could have non-word features)
    weight of the object = total weight of features
    our weights have always been conditional log-probs (    0)

but that is going to change in a few minutes!

7

probabilists   rally   behind  their  paradigm

   .2, .4, .6, .8!  we   re not gonna take your bait!   
    can estimate our parameters automatically

    e.g., log p(t7 | t5, t6)               (trigram tag id203)
   
from supervised or unsupervised data (ratio of counts)

    our results are more meaningful

    can use probabilities to place bets, quantify risk
    e.g., how sure are we that this is the correct parse?

    our results can be meaningfully combined     modularity!

    multiply indep. conditional probs     normalized, unlike scores
    p(english text) * p(english phonemes | english text) * p(jap.

phonemes | english phonemes) * p(jap. text | jap. phonemes)

    p(semantics) * p(syntax | semantics) * p(morphology | syntax) *

p(phonology | morphology) * p(sounds | phonology)

8

probabilists regret being bound by principle

    ad-hoc approach does have one advantage
    consider e.g. na  ve bayes for text categorization:

    buy this supercalifragilistic ginsu knife set

for only $39 today    

    some useful features:

    contains buy
spa m
ha m
    contains supercalifragilistic
    contains a dollar amount under $100
.5  .02
    contains an imperative sentence
    reading level = 8th grade
    mentions money (use word classes and/or regexp to detect this)
    na  ve bayes: pick c maximizing p(c) * p(feat 1 | c) *    
    what assumption does na  ve bayes make?  true here?

.9  .1

9

probabilists regret being bound by principle

    ad-hoc approach does have one advantage
    consider e.g. na  ve bayes for text categorization:

    buy this supercalifragilistic ginsu knife set

for only $39 today    

    some useful features:
spa m
ha m
.5  .02

50% of spam has this     25x more likely than in ham

    contains a dollar amount under $100

but here are the emails with both features     only 25x!
90% of spam has this     9x more likely than in ham

.9  .1

    mentions money

na  ve bayes claims
.5*.9=45% of spam
has both features    
25*9=225x more
likely than in ham.

    na  ve bayes: pick c maximizing p(c) * p(feat 1 | c) *    
    what assumption does na  ve bayes make?  true here?

10

probabilists regret being bound by principle

    but ad-hoc approach does have one advantage

    can adjust scores to compensate for feature overlap    

    some useful features of this message:
spa m
ha m
    contains a dollar amount under $100
.5  .02

log prob
spa m
ha m
-1   -5.6

subtract    money    score

already included

adjusted
spa m
ha m
-.85  -2.3

.9  .1

    mentions money

-.15  -3.3
    na  ve bayes: pick c maximizing p(c) * p(feat 1 | c) *    
    what assumption does na  ve bayes make?  true here?

-.15  -3.3

11

revolution corrupted by bourgeois values
    na  ve bayes needs overlapping but independent features
    but not clear how to restructure these features like that:

    contains buy
    contains supercalifragilistic
    contains a dollar amount under $100
    contains an imperative sentence
    reading level = 7th grade
    mentions money (use word classes and/or regexp to detect this)
       
    boy, we   d like to be able to throw all that useful stuff in
without worrying about feature overlap/independence.
    well, maybe we can add up scores and pretend like we

got a log id203:

12

revolution corrupted by bourgeois values
    na  ve bayes needs overlapping but independent features
    but not clear how to restructure these features like that:
    contains buy
+4
    contains supercalifragilistic
+0.2
    contains a dollar amount under $100
+1
    contains an imperative sentence
+2
    reading level = 7th grade
 -3
    mentions money (use word classes and/or regexp to detect this)
+5
       
    
    boy, we   d like to be able to throw all that useful stuff in
without worrying about feature overlap/independence.
    well, maybe we can add up scores and pretend like we

a l :  

.

5

7

7

o

t

t

got a log id203: log p(feats | spam) = 5.77

    oops, then p(feats | spam) = exp 5.77 = 320.5

13

renormalize by 1/z to get a
o
s
o w n  
    log-linear model
t h i n g   <   1  
t o   1 !
s u m s
    p(feats | spam) = exp 5.77 = 320.5
    p(m | spam) = (1/z(  )) exp    i   i fi(m)  where

  d
a l e
y
r
e
v
a n d  

c
e

s

 

m is the email message
  i is weight of feature i
fi(m)   {0,1} according to whether m has feature i

more generally, allow fi(m) = count or strength of feature.

1/z(  ) is a normalizing factor making    m p(m | spam)=1

(summed over all possible messages m!  hard to find!)

    the weights we add up are basically arbitrary.
    they don   t have to mean anything, so long as they give us
    why is it called    log-linear   ?

a good id203.

14

why bother?
    gives us probs, not just scores.

    can use them to bet, or combine w/ other probs.

    we can now learn weights from data!

    choose weights   i that maximize logprob of labeled
training data = log    j p(cj) p(mj | cj)
    where cj   {ham,spam} is classification of message mj
    and p(mj | cj) is log-linear model from previous slide
    convex function     easy to maximize!  (why?)
    but: p(mj | cj) for a given    requires z(  ): hard!

15

attempt to cancel out z
    set weights to maximize     j p(cj) p(mj | cj)
    where p(m | spam) = (1/z(  )) exp    i   i fi(m)
    but normalizer z(  ) is awful sum over all possible emails

    so instead: maximize     j p(cj | mj)

    doesn   t model the emails mj, only their classifications cj
    makes more sense anyway given our feature set

    p(spam | m) = p(spam)p(m|spam) / (p(spam)p(m|spam)+p(ham)p(m|ham))
    z appears in both numerator and denominator
    alas, doesn   t cancel out because z differs for the spam and ham models
    but we can fix this    

16

so: modify setup a bit
    instead of having separate models
    have just one joint model p(m,c)
       gives us both p(m,spam) and p(m,ham)
    equivalent to changing feature set to:

       p(m|spam)*p(spam)     vs.     p(m|ham)*p(ham)

    weight of this feature is log p(spam) + a constant

   old spam model   s weight for    contains buy   

    spam
    spam and contains buy
    spam and contains supercalifragilistic
       
    ham
    ham and contains buy
    ham and contains supercalifragilistic

    weight of this feature is log p(ling) + a constant

   old ling model   s weight for    contains buy   
    no real change, but 2 categories now share single

feature set and single value of z(  )

17

now we can cancel out z
now p(m,c) = (1/z(  )) exp    i   i fi(m,c)  where c   {ham, spam}
    old: choose weights   i that maximize prob of labeled
training data =    j p(mj, cj)
    new: choose weights   i that maximize prob of labels
given messages =    j p(cj | mj)

    now z cancels out of id155!

    p(spam | m) = p(m,spam) / (p(m,spam) + p(m,ham))
   = exp    i   i fi(m,spam) / (exp    i   i fi(m,spam) + exp    i   i fi(m,ham))
    easy to compute now    
       j p(cj | mj) is still convex, so easy to maximize too

18

maximum id178

    suppose there are 10 classes, a through j.
   
    question: given message m: what is your guess for p(c | m)?

i don   t give you any other information.

    suppose i tell you that 55% of all messages are in class a.
    question: now what is your guess for p(c | m)?
    suppose i also tell you that 10% of all messages contain buy
    question: now what is your guess for p(c | m),
    ouch!

and 80% of these are in class a or c.

if m contains buy?

19

maximum id178

a
.051

b
.0025

c
.029

d
.0025

e
.0025

f
.0025

g
.0025

h
.0025

i
.0025

j
.0025

buy

.499

.0446

other
.0446
    column a sums to 0.55   (   55% of all messages are in class a   )

.0446

.0446

.0446

.0446

.0446

.0446

.0446

20

maximum id178

a
.051

b
.0025

c
.029

d
.0025

e
.0025

f
.0025

g
.0025

h
.0025

i
.0025

buy

.499

.0446

.0446

.0446

other
    column a sums to 0.55
    row buy sums to 0.1   (   10% of all messages contain buy   )

.0446

.0446

.0446

.0446

.0446

j
.0025

.0446

21

maximum id178

a
.051

b
.0025

c
.029

d
.0025

e
.0025

f
.0025

g
.0025

h
.0025

i
.0025

j
.0025

buy

.0446

.0446

.0446

.0446

.0446

.0446

.0446

.499

.0446

.0446

other
    column a sums to 0.55
    row buy sums to 0.1
   

(buy, a) and (buy, c) cells sum to 0.08  (   80% of the 10%   )

    given these constraints, fill in cells    as equally as possible   :

maximize the id178  (related to cross-id178, perplexity)

id178 = -.051 log .051 - .0025 log .0025 - .029 log .029 -    
largest if probabilities are evenly distributed

22

maximum id178

a
.051

b
.0025

c
.029

d
.0025

e
.0025

f
.0025

g
.0025

h
.0025

i
.0025

j
.0025

buy

.0446

.0446

.0446

.0446

.0446

.0446

.0446

.499

.0446

.0446

other
    column a sums to 0.55
    row buy sums to 0.1
   

(buy, a) and (buy, c) cells sum to 0.08  (   80% of the 10%   )

    given these constraints, fill in cells    as equally as possible   :

maximize the id178

    now p(buy, c) = .029  and  p(c | buy) = .29
    we got a compromise: p(c | buy) < p(a | buy) < .55

23

generalizing to more features

<$100

other
a
.051

buy

other

.499

b
.0025

c
.029

d
.0025

e
.0025

f
.0025

g
.0025

   h
.0025

.0446

.0446

.0446

.0446

.0446

.0446

.0446

24

what we just did
    for each feature (   contains buy   ), see what
    many distributions p(c,m) would predict these
fractions (including the unsmoothed one where all
mass goes to feature combos we   ve actually seen)

fraction of training data has it

    of these, pick distribution that has max id178
    amazing theorem: this distribution has the
form p(m,c) = (1/z(  )) exp    i   i fi(m,c)
    so it is log-linear.  in fact it is the same log-linear
distribution that maximizes    j p(mj, cj) as before!
    gives another motivation for our log-linear approach.

25

log-linear form derivation
    say we are given some constraints in the form of

feature expectations:

   

in general, there may be many distributions p(x) that
satisfy the constraints.  which one to pick?

    the one with maximum id178 (making fewest possible

additional assumptions---occum   s razor)

    this yields an optimization problem

26

log-linear form derivation

27

maxent = max likelihood

28

30

31

32

33

34

35

by gradient ascent or conjugate gradient.

36

37

38

overfitting

    if we have too many features, we can choose
weights to model the training data perfectly.

    if we have a feature that only appears in spam
training, not ling training, it will get weight     to
maximize p(spam | feature) at 1.

    these behaviors overfit the training data.
    will probably do poorly on test data.

39

solutions to overfitting

   

   

   

   
   

throw out rare features.

require every feature to occur > 4 times, and > 0
times with ling, and > 0 times with spam.

    only keep 1000 features.

add one at a time, always greedily picking the one
that most improves performance on held-out data.

smooth the observed feature counts.
smooth the weights by using a prior.
    max p(  |data) = max p(  , data) =p(  )p(data|  )
   

decree p(  ) to be high when most weights close to 0

40

41

42

recipe for a conditional
maxent classifier

1. gather constraints from training data:

initialize all parameters to zero.

2.
3. classify training data with current parameters.  calculate

expectations.

4. gradient is
5. take a step in the direction of the gradient
6. until convergence, return to step 3.

43

