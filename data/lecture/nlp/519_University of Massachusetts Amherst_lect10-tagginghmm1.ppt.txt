part-of-speech tagging &
hidden markov model intro

lecture #10

introduction to natural language processing

cmpsci 585, fall 2007
university of massachusetts  amherst

andrew mccallum

andrew mccallum, umass amherst

today   s main points

    tips for hw#4
    summary of course feedback

    part-of-speech tagging
    what is it?  why useful?

    return to recipe for nlp problems
    id48

    definition
    generative model
    next time: id145 with viterbi algorithm

andrew mccallum, umass amherst

class surveys very helpful

    learning something?

    yes!  very edifying!
    yes. lots.  statistical nlp is a lot of fun.
    yes!  both theory and practice.
    yes, i have been learning a lot.  particularly since the
id203 class pretty much everything is new to me.
    yes. i went to the google talk on machine translation

and mostly understood it, based entirely on
experience from this class.

    yes.  my understanding of id145 has

greatly increased.

andrew mccallum, umass amherst

class surveys

    pace and lectures

    i like that we cover a large breadth of material and don   t doddle.
    balance between theory and applications is great.
    the slides are really good.  i also like when math is demo   ed on

the whiteboard.

    everything working well.
    i like the quizzes.  helps me know what i should be learning.
    in-class exercises very helpful.  let   s have more!
    pace: 5 just right, 3 slightly too fast, 3 slightly too slow.

    love the in-class exercises and group discussions.
    enthusiasm is motivating and contagious.  available after class to

offer deeper insights, answer questions, etc.

    love hearing about nlp people history lessons

andrew mccallum, umass amherst

class surveys

    homeworks

aspect!

    homework assignments are fantastic, especially the open-ended

    the reinforce the learning.
    interesting, fun, promotes creativity, very much unlike other

homeworks that just    have to be done   .  i like particularly that we get a
choice... room for doing stuff one finds interesting.

    fun because we get to play around; lots of freedom!
    helpful that some of the less interesting infrastructure (file reading...)

is provided.

    initially confused about the report format.  an example would help.

(but comfortable with them now.)

    make grading rubric / expectations more clear.
    grading harsh--points off for not going above and beyond, even

though the specified requirements were met.  hard to tell how much
creativity is enough.

andrew mccallum, umass amherst

class surveys

    workload

    (no one complaining.)
       work is fun, so it feels like less.   

andrew mccallum, umass amherst

class surveys

    suggestions & concerns

    would like more exercises and take-home quizzes.
    post slides sooner.
    make hw grading policy more clear.

andrew mccallum, umass amherst

hw #4 tasks

    naive bayes

    document classification (spam dataset provided)
    part-of-speech tagger

    id165 language model

    train and generate language

    look for phase changes?
    experiment with different smoothing methods?

    foreign language classifier
    rank output of a machine translation system

andrew mccallum, umass amherst

hw#4 help
evaluation

result of running classifier on a test set:

filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
...

true spam

true ham

pred spam
pred ham

tp

fn

fp

tn

accuracy = (tp+tn) / (tp+tn+fp+fn)
precision = tp / (tp+fp)
recall = tp / (tp+fn)
f1 = harmonic mean of precision & recall

andrew mccallum, umass amherst

hw#4 help

precision-recall curve

typically if p(spam) > 0.5, then label as spam, but can change 0.5    threshold    
each threshold yields a new precision/recall pair.  plot them:

andrew mccallum, umass amherst

hw#4 help

accuracy-coverage curve

result of running classifier on a test set:

filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
...

true spam

true ham

pred spam
pred ham

tp

fn

fp

tn

accuracy = (tp+tn) / (tp+tn+fp+fn)
precision = tp / (tp+fp)
recall = tp / (tp+fn)
f1 = harmonic mean of precision & recall

andrew mccallum, umass amherst

hw#4 help

working with log-probabilities

    getting back to p(c|d)

    subtract a constant to make all non-positive
    exp()

andrew mccallum, umass amherst

hw#4 help

the importance of train / test splits

    when measuring accuracy, we want an

estimate on how well a classifier will do on
   future data   .

       testing    on the    training data    doesn   t do

this.

    split data.  train on one half.  test on the

other half.

andrew mccallum, umass amherst

id52 and

id48

andrew mccallum, umass amherst

grammatical categories: parts-of-speech

    nouns: people, animals, concepts, things
    verbs: expresses action in the sentence
    adjectives: describe properties of nouns

sad
intelligent
green
fat
   

    the                      one is in the corner.

   substitution test   

andrew mccallum, umass amherst

the part-of-speech tagging task

input:   the lead paint is unsafe
output: the/det lead/n paint/n is/v unsafe/adj

    uses:

    text-to-speech (how do we pronounce    lead   ?)
    can differentiate word senses that involve part of speech differences (what is

the meaning of    interest   )

    can write regexps like det adj* n* over the output (for filtering

collocations)

    can be used as simpler    backoff    context in various markov models when too

little is known about a particular history based on words instead.

    preprocessing to speed up parser (but a little dangerous)
    tagged text helps linguists find interesting syntactic constructions in texts

(   ssh    used as a verb)

andrew mccallum, umass amherst

tagged data sets

    designed to be a representative sample from 1961

    brown corpus

    news, poetry,    

    87 different tags

    claws5    c5   

    62 different tags

    id32
    45 different tags
    most widely used currently

andrew mccallum, umass amherst

part-of-speech tags, examples

    part-of-speech
tag examples
happy, bad
    adjective
jj
    adjective, comparative
jjr
happier, worse
3, fifteen
    adjective, cardinal number cd
    adverb
rb
often, particularly
and, or
cc
    conjunction, coordination
    conjunction, subordinating
in
although, when
this, each, other, the, a, some
    determiner
dt
    determiner, postdeterminer jj
many, same
aircraft, data
nn
    noun
women, books
nns
    noun, plural
    noun, proper, singular
nnp
london, michael
nnps australians, methodists
    noun, proper, plural
    pronoun, personal
prp
wp
    pronoun, question
    verb, base present form
vbp

you, we, she, it
who, whoever
take, live

andrew mccallum, umass amherst

closed, open

    closed set tags

    determiners
    prepositions
       

    open set tags

    noun
    verb

andrew mccallum, umass amherst

why is this such a big part of nlp?

input:   the lead paint is unsafe
output: the/det lead/n paint/n is/v unsafe/adj

    the first statistical nlp task
    been done to death by different methods
    easy to evaluate (how many tags are correct?)
    canonical finite-state task

    can be done well with methods that look at local context
    (though should    really    do it by parsing!)

andrew mccallum, umass amherst

np
nnp
fed

ambiguity in language

fed raises interest rates 0.5%
in effort to control inflation

s

vp

ny times headline 17 may 2000

v

np

np

pp

raises

nn

interest

nn
rates

cd nn
%
0.5

np

pp
in nn
effort

v
to

vp

v

control

vp

np
nn

in   ation

andrew mccallum, umass amherst

part of speech ambiguities

part-of-speech ambiguities

vbz
nns

vb
vbz
nns

vbz
nns

cd nn
nnp
fed  raises  interest rates  0.5  %

in effort to
control inflation

andrew mccallum, umass amherst

degree of supervision

    supervised: training corpus is tagged by humans
    unsupervised: training corpus isn   t tagged
    partly supervised: e.g. training corpus isn   t tagged, but
you have a dictionary giving possible tags for each word

    we   ll start with the supervised case (in later classes we

may move to lower levels of supervision).

andrew mccallum, umass amherst

current performance

input:   the lead paint is unsafe
output: the/det lead/n paint/n is/v unsafe/adj

    using state-of-the-art automated method,

how many tags are correct?
    about 97% currently
    but baseline is already 90%

    baseline is performance of simplest possible method:
    tag every word with its most frequent tag
    tag unknown words as nouns

andrew mccallum, umass amherst

recipe for solving an nlp task

input:   the lead paint is unsafe
output: the/det lead/n paint/n is/v unsafe/adj

observations
tags

1) data: notation, representation
2) problem: write down the problem in notation
3) model: make some assumptions, define a parametric

model (often generative model of the data)
id136: how to search through possible answers to
find the best one

4)

5) learning: how to estimate parameters
6)

implementation: engineering considerations for an
efficient implementation

andrew mccallum, umass amherst

work out several alternatives

on the board   

andrew mccallum, umass amherst

(hidden) markov model tagger

    view sequence of tags as a markov chain.

assumptions:
    limited horizon

    time invariant (stationary)

    we assume that a word   s tag only depends on the

previous tag (limited horizon) and that his
dependency does not change over time (time
invariance)

    a state (part of speech) generates a word.  we

assume it depends only on the state.

andrew mccallum, umass amherst

the markov property

    a stochastic process has the markov property if the
id155 distribution of future states of
the process, given the current state, depends only
upon the current state, and conditionally independent
of the past states (the path of the process) given the
current state.

    a process with the markov property is usually called

a markov process, and may be described as
markovian.

andrew mccallum, umass amherst

id48 as finite state machine

transitions

p(xt+1|xt)

nn

dt

vbp

emissions

jj

in

for
above
in
   
p(ot|xt)

andrew mccallum, umass amherst

id48 as id110

    top row is unobserved states, interpreted as pos tags
    bottom row is observed output observations (words)

andrew mccallum, umass amherst

applications of id48s

    nlp

    part-of-speech tagging
    id40
    information extraction
    id42 (ocr)

    id103
    modeling acoustics

    id161

    gesture recognition

    biology

    gene finding
    protein structure prediction

    economics, climatology, communications, robotics   

andrew mccallum, umass amherst

(one) standard id48 formalism
(x, o, xs, a, b) are all variables.  model    = (a, b)

   
    x is state sequence of length t; o is observation seq.
    xs is a designated start state (with no incoming

transitions).  (can also be separated into    as in book.)

    a is matrix of transition probabilities (each row is a

id155 table (cpt)

    b is matrix of output probabilities (vertical cpts)

    id48 is a probabilistic (nondeterministic) finite state

automaton, with probabilistic outputs (from vertices, not
arcs, in the simple case)

andrew mccallum, umass amherst

probabilistic id136 in an id48

three fundamental questions for an id48:

1) compute the id203 of a given observation

sequence, when tag sequence is hidden
(id38)

2) given an observation sequence, find the most likely

hidden state sequence (tagging)  do this next

3) given observation sequence(s) and a set of states,

find the parameters that would make the
observations most likely (parameter estimation)

andrew mccallum, umass amherst

most likely hidden state sequence

    given o = (o1,   ,ot) and model    = (a,b)
    we want to find

    p(o,x|   ) = p(o|x,   ) p(x|    )
    p(o|x,   ) = b[x1|o1] b[x2|o2]     b[xt|ot]
    p(x|   ) = a[x1|x2] a[x2|x3]     a[xt-1|xt]
    arg maxx p(o,x|   ) = arg max x1, x2,    xt
    problem: arg max is exponential in sequence length!

andrew mccallum, umass amherst

representation for paths: trellis

states
x1

x2

x3

x4

time

1

        2

    3

4

   

  t

andrew mccallum, umass amherst

representation for paths: trellis

states
x1

x2

x3

x4

time

1

        2

    3

4

   

  t

andrew mccallum, umass amherst

representation for paths: trellis

states
x1

x2

x3

x4

a[x4, x2] b[o4]

time

1

        2

  t
  i(t) = id203 of most likely path that ends at state i at time t.

    3

   

4

andrew mccallum, umass amherst

finding id203 of most likely path

using id145

    efficient computation of max over all states
    intuition: id203 of the first t observations is
the same for all possible t+1 length sequences.

    define forward score:

    compute it recursively from the beginning
    (then must remember best paths to get arg max.)

andrew mccallum, umass amherst

finding the most likely state path

with the viterbi algorithm

[viterbi 1967]

    used to efficiently find the state sequence that gives

the highest id203 to the observed outputs

    maintains two id145 tables:

    the id203 of the best path (max)

    the state transitions of the best path (arg)

    note that this is different from finding the most likely

tag for each time t!

andrew mccallum, umass amherst

viterbi recipe

    initialization

    induction

store backtrace

    termination and path readout

id203 of entire best seq.

andrew mccallum, umass amherst

