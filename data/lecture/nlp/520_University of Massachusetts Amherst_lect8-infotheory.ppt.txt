classification & id205

lecture #8

introduction to natural language processing

cmpsci 585, fall 2007
university of massachusetts  amherst

andrew mccallum

andrew mccallum, umass amherst

today   s main points

    automatically categorizing text

    parameter estimation and smoothing
    a general recipe for a statistical compling model
    building a spam filter

    id205

    what is information?  how can you measure it?
    id178, cross id178, information gain

andrew mccallum, umass amherst

maximum likelihood parameter estimation

example: binomial

    toss a coin 100 times, observe r  heads
    assume a binomial distribution

    order doesn   t matter, successive flips are independent
    one parameter is q  (id203 of flipping a head)
    binomial gives p(r|n,q).  we know r and n.
    find arg maxq p(r|n, q)

andrew mccallum, umass amherst

maximum likelihood parameter estimation

example: binomial

    toss a coin 100 times, observe r  heads
    assume a binomial distribution

    order doesn   t matter, successive flips are independent
    one parameter is q  (id203 of flipping a head)
    binomial gives p(r|n,q).  we know r and n.
    find arg maxq p(r|n, q)

(notes for board)

andrew mccallum, umass amherst

our familiar ratio-of-counts
is the maximum likelihood estimate!

! likelihood=p(r=r|n,q)=nr" # $ % &     qr(1(q)n(rlog(likelihood=l=log(p(r|n,q)))log(qr(1(q)n(r)=rlog(q)+(n(r)log(1(q)*l*q=rq(n(r1(q+r(1(q)=(n(r)q+q=rnbinomial parameter estimation examples

    make 1000 coin flips, observe 300 heads

    p(heads) = 300/1000

    make 3 coin flips, observe 2 heads

    p(heads) = 2/3 ??

    make 1 coin flips, observe 1 tail

    p(heads) = 0 ???

    make 0 coin flips
    p(heads) = ???

    we have some    prior    belief about p(heads) before we see any

data.

    after seeing some data, we have a    posterior    belief.

andrew mccallum, umass amherst

maximum a posteriori
 parameter estimation

    we   ve been finding the parameters that maximize

    p(data|parameters),
not the parameters that maximize
    p(parameters|data)    (parameters are random variables!)

    p(q|n,r) = p(r|n,q) p(q|n) = p(r|n,q) p(q)
      p(r|n)               constant

    and let p(q) = 2 q(1-q)

andrew mccallum, umass amherst

maximum a posteriori parameter estimation

example: binomial

2

andrew mccallum, umass amherst

! posterior=p(r|n,q)p(q)=nr" # $ % &     qr(1(q)n(r(6q(1(q))log(posterior=l)log(qr+1(1(q)n(r+1)=(r+1)log(q)+(n(r+1)log(1(q)*l*q=(r+1)q((n(r+1)1(q+(r+1)(1(q)=(n(r+1)q+q=r+1n+2bayesian decision theory

    we can use such techniques for choosing

among models:
    which among several models best explains the data?

    likelihood ratio

p(model1 | data)  =   p(data|model1) p(model1)
p(model2 | data)       p(data|model2) p(model2)

andrew mccallum, umass amherst

...back to our example: french vs english

    p(french | glacier, melange) versus

p(english | glacier, melange) ?

    we have real data for

    jane austin
    william shakespeare

    p(austin |    stars   ,    thou   )

p(shakespeare |    stars   ,    thou   )

andrew mccallum, umass amherst

statistical spam filtering

   are you free to meet
with dan jurafsky
today at 3pm?  he wants
to talk about
computational methods
for noun coreference.   

.

real
email

spam
email

   speaking at awards ceremony...   
   coming home for dinner...   
   free for a research meeting at 6pm...    
   computational linguistics of   ce hours...    

   nigerian minister awards      
   earn money at home today!...   
   free cash   
   just hours per day...    

testing
document:

categories:

training
  data:

andrew mccallum, umass amherst

document classification

by machine learning

   temporal reasoning for
planning has long
been studied formally.
we discuss the semantics
of several planning...   
  

.

machine
learning

planning

prog. lang.
semantics

garbage
collection

multimedia gui

testing
document:

categories:

training
  data:

   neural networks
and other machine
learning methods 
of classi   cation      

andrew mccallum, umass amherst

   plannning 
with temporal
reasoning 
has been      

      based on
the semantics
of program
dependence   

   garbage
collection for
strongly-typed
languages       

   multimedia
streaming
video for      

   user
studies
of gui      

work out na  ve bayes formulation

interactively on the board

andrew mccallum, umass amherst

recipe for solving a
nlp task statistically

1) data: notation, representation
2) problem: write down the problem in notation
3) model: make some assumptions, define a

parametric model

4) id136: how to search through possible

answers to find the best one

5) learning: how to estimate parameters
6) implementation: engineering considerations

for an efficient implementation

andrew mccallum, umass amherst

(engineering) components of a
na  ve bayes document classifier

    split documents into training and testing
    cycle through all documents in each class
    tokenize the character stream into words
    count occurrences of each word in each class
    estimate p(w|c) by a ratio of counts (+1 prior)
    for each test document, calculate p(c|d) for

each class

    record predicted (and true) class, and keep

accuracy statistics

andrew mccallum, umass amherst

a probabilistic approach to classification:

   na  ve bayes   

pick the most probable class, given the evidence:

- a class (like    planning   )
- a document (like    language intelligence proof...   )

bayes rule:

   na  ve bayes   :

andrew mccallum, umass amherst

- the i th word in d (like    proof   )

! c"=argmaxcjpr(cj|d)! cj! d! pr(cj|d)=pr(cj)pr(d|cj)pr(d)! "pr(cj)pr(wdi|cj)i=1|d|#pr(ck)pr(wdi|ck)i=1|d|#ck$! wdiparameter estimation in na  ve bayes

estimate of p(c)

estimate of p(w|c)

andrew mccallum, umass amherst

! p(cj)=1+count(d"cj)|c|+count(d"ck)k#! p(wi|cj)=1+count(wi,dk)dk"cj#|v|+count(wt,dk)dk"cj#t=1|v|#id205

andrew mccallum, umass amherst

what is information?

       the sun will come up tomorrow.   

       condi rice was shot and killed this morning.   

andrew mccallum, umass amherst

efficient encoding

    i have a 8-sided die.

how many bits do i need to tell you what face
i just rolled?

    my 8-sided die is unfair

    p(1)=1/2, p(2)=1/8, p(3)=   =p(8)=1/16

0

1

0
2

1

0

1

1

0

0
3

1
4

0

5

1
6

1

0
7

1
8

2.375

andrew mccallum, umass amherst

id178 (of a random variable)

    average length of message needed to

transmit the outcome of the random
variable.

    first used in:

    data compression
    transmission rates over noisy channel

andrew mccallum, umass amherst

   coding    interpretation of id178

    given some distribution over events p(x)   
    what is the average number of bits needed to
encode a message (a event, string, sequence)

    = id178 of p(x):

    notation: h(x) = hp(x)=h(p)=hx(p)=h(px)

what is the id178 of a fair coin?  a fair 32-sided die?
what is the id178 of an unfair coin that always comes up heads?
what is the id178 of an unfair 6-sided die that always {1,2}
upper and lower bound?  (prove lower bound?)

andrew mccallum, umass amherst

! h(p(x))="px#x$(x)log2(p(x))id178 and expectation

    recall

    then

e[x] =   x     x(  ) x    p(x)

e[-log2(p(x))] =   x     x(  ) -log2(p(x))    p(x)
= h(x)

andrew mccallum, umass amherst

id178 of a coin

andrew mccallum, umass amherst

id178, intuitively

    high id178 ~    chaos   , fuzziness,

opposite of order

    comes from physics:

    id178 does not go down unless energy is used

    measure of uncertainty

    high id178: a lot of uncertainty about the

outcome, uniform distribution over outcomes

    low id178: high certainty about the outcome

andrew mccallum, umass amherst

claude shannon

    claude shannon

1916 - 2001
creator of id205

    lays the foundation for

implementing logic in digital
circuits as part of his masters
thesis!  (1939)
   a mathematical theory of
communication    (1948)

   

1950

andrew mccallum, umass amherst

joint id178 and conditional id178

    two random variables: x (space   ), y (  )
    joint id178

    no big deal: (x,y) considered a single event:
h(x,y) = -   x        y      p(x,y) log2 p(x,y)

    conditional id178:

h(x|y) = -   x        y      p(x,y) log2 p(x|y)
(weighted average, and weights are not conditional)

    recall that h(x) = e[-log2(p(x))]

    how much extra information you need to supply to

transmit x given that the other person knows y.

andrew mccallum, umass amherst

conditional id178 (another way)

andrew mccallum, umass amherst

! h(y|x)=p(x)h(y|x=x)x"=p(x)(#p(y|x)log2(p(y|x))y"x"=#p(x)p(y|x)log2(p(y|x))y"x"=#p(x,y)log2(p(y|x))y"x"chain rule for id178

    since, like random variables, id178 is

based on an expectation..

h(x, y) = h(y|x) + h(x)

h(x, y) = h(x|y) + h(y)

andrew mccallum, umass amherst

cross id178

    what happens when you use a code that is

sub-optimal for your event distribution?
    i created my code to be efficient for a fair 8-sided die.
    but the coin is unfair and always gives 1 or 2 uniformly.
    how many bits on average for the optimal code?

how many bits on average for the sub-optimal code?

andrew mccallum, umass amherst

! h(p,q)="px#x$(x)log2(q(x))kl divergence

    what are the average number of bits that are
wasted by encoding events from distribution p
using distribution q?

a sort of    distance    between distributions p and q, but
it is not symmetric!
it does not satisfy the triangle inequality!

andrew mccallum, umass amherst

! d(p||q)=h(p,q)"h(p)="px#x$(x)log2(q(x))+px#x$(x)log2(p(x))=px#x$(x)log2(p(x)q(x))mutual information

    recall: h(x) = average #  bits for me to tell you which event

occurred from distribution p(x).

    now, first i tell you event y     y, h(x|y) = average # bits

necessary to tell you which event occurred from distribution
p(x)?

    by how many bits does knowledge of y lower the id178 of x?

andrew mccallum, umass amherst

! i(x;y)=h(x)"h(x|y)=h(x)+h(y)"h(x,y)="px#(x)log21p(x)"py#(y)log21p(y)+px,y#(x,y)log2p(x,y)=px,y#(x,y)log2p(x,y)p(x)p(y)mutual information

    symmetric, non-negative.
    measure of independence.

    i(x;y) = 0 when x and y are independent
    i(x;y) grows both with degree of dependence and id178 of

the variables.

    sometimes also called    information gain   

    used often in nlp

    id91 words
    id51
    feature selection   

andrew mccallum, umass amherst

pointwise mutual information

    previously measuring mutual information

between two random variables.

    could also measure mutual information

between two events

andrew mccallum, umass amherst

! i(x,y)=logp(x,y)p(x)p(y)