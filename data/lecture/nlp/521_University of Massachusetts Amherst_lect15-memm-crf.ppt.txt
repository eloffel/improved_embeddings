sequence labeling

    inputs: x = (x1,    , xn)
    labels: y = (y1,    , yn)
    typical goal: given x, predict y

    example sequence labeling tasks

    part-of-speech tagging
    named-entity-recognition (ner)
    label people, places, organizations

ner example:

first solution:

maximum id178 classifier
    conditional model p(y|x).

    do not waste effort modeling p(x), since x

is given at test time anyway.

    allows more complicated input features,

since we do not need to model
dependencies between them.

    feature functions f(x,y):

    f1(x,y) = { word is boston & y=location }
    f2(x,y) = { first letter capitalized & y=name }
    f3(x,y) = { x is an html link & y=location}

first solution: maxent classifier

    how should we choose a classifier?

    principle of maximum id178

    we want a classifier that:

    matches feature constraints from training data.
    predictions maximize id178.

    there is a unique, exponential family
distribution that meets these criteria.

first solution: maxent classifier

    p(y|x;  ), id136, learning, and

gradient.

    (on board)

first solution: maxent classifier

    problem with using a maximum id178

classifier for sequence labeling:

    it makes decisions at each position

independently!

second solution: id48

    defines a generative process.
    can be viewed as a weighted finite

state machine.

! p(y,x)=p(yt|yt"1)p(x|yt)t#second solution: id48

    id48 problems: (on board)
    id203 of an input sequence.
    most likely label sequence given an input

sequence.

    learning with known label sequences.
    learning with unknown label sequences?

second solution: id48

    how can represent we multiple features

in an id48?
    treat them as conditionally independent

given the class label?
    the example features we talked about are not

independent.

    try to model a more complex generative

process of the input features?
    we may lose tractability (i.e. lose a dynamic

programming for exact id136).

second solution: id48

    let   s use a conditional model instead.

third solution: memm

    use a series of maximum id178

classifiers that know the previous label.
    define a viterbi algorithm for id136.

! p(y|x)=pyt"1(yt|x)t#third solution: memm

    finding the most likely label sequence
given an input sequence and learning.

    (on board)

third solution: memm

    combines the advantages of maximum

id178 and id48!

    but there is a problem   

problem with memms: label bias

    in some state space configurations,

memms essentially completely ignore
the inputs.

    example (on board).

    this is not a problem for id48s,
because the input sequence is
generated by the model.

fourth solution:

conditional random field

    conditionally-trained, undirected

graphical model.

    for a standard linear-chain structure:

! p(y|x)="k(yt,yt#1,x)t$"k(yt,yt#1,x)=exp%kk&f(yt,yt#1,x)    ( ) * + , fourth solution: crf

    finding the most likely label sequence
given an input sequence and learning.
(on board)

fourth solution: crf

    have the advantages of memms, but

avoid the label bias problem.

    crfs are globally normalized, whereas

memms are locally normalized.

    widely used and applied.  crfs give
state-the-art results in many domains.

example applications

    crfs have been applied to:

    part-of-speech tagging
    named-entity-recognition
    table extraction
    gene prediction
    chinese id40
    extracting information from research

papers.

    many more   

