noisy channel, id165s & smoothing

lecture #9

introduction to natural language processing

cmpsci 585, fall 2007
university of massachusetts  amherst

andrew mccallum

andrew mccallum, umass amherst

today   s main points

    application of the id87
    markov models

    including markov property definition

    smoothing

    laplace, (lidstone   s, held-out, good-turing.)

andrew mccallum, umass amherst

id87

encoder

w

message
from a    nite
alphabet

y

input to
channel

channel
p(x|y)

x

output from

channel

w   

decoder

attempt to

reconstruct message

based on output

    optimize encoder for throughput and accuracy.

    compression: remove all redundancy
    accuracy: adding controlled redundancy

    capacity: rate at which can transmit information with

arbitrarily low id203 of error in w   

andrew mccallum, umass amherst

noisy channel in nlp
y   

x

y

input to
channel

channel
p(y|x)

decoder

output from

channel

attempt to

reconstruct message

based on output

language
model

channel
prob.

p(y)
p(l1) in a language
model

p(x|y)
translation model

prob of language
text

model of ocr
errors

prob of pos
sequences

prob of word
sequences

id203 of word
given tag

acoustic model

application
machine
translation

input
l1 word sequences

output
l2 word sequences

optical character
recognition (ocr)

actual text

part of speech
(pos) tagging

pos tag
sequences

speech
recognition

document
classification

word sequences

class label

andrew mccallum, umass amherst

text with ocr
errors

english word
sequence

acoustic speech
signal

word sequence in
document

class prior
id203

p(l1) from each
class

probabilistic id38

    assigns id203 p(t) to a word sequence

t = w1w2 w3w4 w5w6   

    chain rule and joint/conditional probabilities for text t:

the chain rule leads to a history-based model: 
we predict following things from past things.

andrew mccallum, umass amherst

id165 models

the classic example of a statistical model of language

    each word is predicted according to a conditional

distribution based on limited context

    id155 table (cpt): p(x|   both   )

    p(of|both) = 0.066
    p(to|both) = 0.041
    p(in|both) = 0.038

    a.k.a. markov (chain) models

    sequences of random variables in which the future variable is

determined by the present variable, but is independent of the way
in which the present state arose from its predecessors

andrew mccallum, umass amherst

1-gram model

w2
in

w3
both

w4
??

w1
<s>

first-order markov model, p(wt|wt-1)

    simplest linear graphical model
    words are random variables, arrows are

direct dependencies between them (cpts)

    these simple engineering models have been

amazingly successful.

andrew mccallum, umass amherst

n-th order markov models

    first order markov assumption = bigram

    similarly, n-th order markov assumption

    most commonly, trigram (2nd order)

andrew mccallum, umass amherst

andrei andreyevich markov

    graduate of saint petersburg

university (1878), where he began a
professor in 1886.

    mathematician, teacher political activist

    in 1913, when the government

celebrated the 300th anniversary of the
house of romanov family, markov
organized a counter-celebration of the
200th anniversary of bernoulli   s
discovery of the law of large
numbers.

    markov was also interested in poetry
and he made studies of poetic style.

1856 - 1922

andrew mccallum, umass amherst

markov   s model

    took 20,000 characters from pushkin   s eugene

onegin to see if it could be approximated by a
simple chain of characters.

vowel

consonant

vowel
0.128
0.663

consonant

0.872
0.337

andrew mccallum, umass amherst

markov approximations to english

    zero-order approximation, p(c)

    xfoml rxkxrjffuj zlpwcfwkcrj

ffjeyvkcqsghyd qpaamkbzaacibzlhjqd

    first-order approximation, p(c|c)

    ocro hli rgwr nwielwis eu ll

nbnesebya th eei alhenhttpa oobttva

    second-order approximation, p(c|c,c)

    on ie antsoutinys are t inctore st be s

deamy achin d ilonasive tucoowe at
teasonare fuso tizin andy tobe seace
ctisbe

andrew mccallum, umass amherst

[from shannon   s original paper]

markov approximations to english (cont.)

    third-order approximation, p(w|w,w,w)

    in no ist lat whey cratict froure birs
grocid pondenome of demonstures of
the reptabin is regoactiona of cre
    markov random field with 1000    features   
    was reaser in there to will was by

homes thing be reloverated ther
which consists at fores anditing with
proveral the chestraing for have to
intrally of qut diveral this offect
inatever thifer contranded stater

[della pietra, della pietra & lafferty, 1997]

andrew mccallum, umass amherst

word-based approximations

    first-order approximation

    representing and speedily is an good apt or come
can different natural here he the a in came the to
of to expert gray come to furnishes the line
message had be

    second-order approximation

    the head and in frontal attack on an english writer
that the character of this point is therefore another
method for the letters that the time of who ever
told the problem for an unexpected

shannon   s comment (1948):    it would be interesting if further approximations could
be constructed, but the labor involved becomes enormous at the next stage.   

andrew mccallum, umass amherst

id165 models

    core language model for the engineering task of

better predicting the next word:
    id103
    ocr
    context-sensitive id147

    it has only recently that improvements have

been made for these tasks [alshawi    96, wu    97]
    but linguistically, they are appallingly simple and

na  ve.

andrew mccallum, umass amherst

why might id165 models not work?

    relationships (say between subject and verb)

can be arbitrarily distant and convoluted, as
linguists love to point out:
    the man on the sidewalk, without pausing to look
at what was happening down the street, and quite
oblivious to the situation that was about to befall
him, confidently strode into the center of the road.

andrew mccallum, umass amherst

why do they work?

    that kind of thing doesn   t happen much
    collins (1997)

    74% of dependencies (in the id32,
wsj) are with an adjacent word (95% with one
less than 5 words away), once one treats simple
nps as units

andrew mccallum, umass amherst

evaluation of language models

    best evaluation of id203 model is task-based!
    as substitute for evaluating one component, standardly

use corpus per-word cross id178:

    or perplexity

    units = average number of choices, scaled for uniform distr.
    high = unpredictable

andrew mccallum, umass amherst

parameter estimation

andrew mccallum, umass amherst

maximum likelihood estimate

    relative frequency
    makes training data a probable as possible
    overfits

andrew mccallum, umass amherst

limitations of the

maximum likelihood estimator

    problem: often infinitely surprised when

unseen word appears, p(unseen) = 0
    problem: this happens commonly
    probabilities of zero-count words are too low
    probabilities of nonzero-count words are too high
    estimates for high count words are fairly accurate
    estimates for low count words are unstable
    we need    smoothing   

andrew mccallum, umass amherst

sparsity

    how often does an every day word like    kick    occur in a

million words of text?
       kick   : about 10 [depends vastly on genre, of course]
       wrist   : about 5

    normally we want to know about something bigger than a
single word, like how often you    kick a ball   , or how often
the dative alternation    he kicked the baby a toy    occurs.

    how often can we expect that to occur in 1 million words?
    almost never.
   

   there   s no data like more data   
    must be of the right domain

andrew mccallum, umass amherst

severity of the sparse data problem

count

1
2
3
>4
>0

possible

2-grams
8,045,024
2,065,469
970,434
3,413,290
14,494,217
6.8 x 1010

3-grams
53,737.350
9,229,958
3,654,791
8,728,789
75,349,888
1.7 x 1016

vocab size 260,741 words, 365m words training

andrew mccallum, umass amherst

the zero problem

    necessarily some zeros

    trigram model: 1.7 x 1016 parameters
    but only 2.6 x 106 words of training data

    how should we distribute some id203

mass over all possibilities in the model
    optimal situation: even the least frequent trigram
would occur several times, in order to distinguish
its id203 versus other trigrams

    optimal situation cannot happen, unfortunately

(how much data would we need?)

    two kinds of zeros: p(w|h)=0, or even p(h)=0

andrew mccallum, umass amherst

laplace smoothing

    v is the vocabulary size (assume fixed,

closed vocabulary)

    this is the bayesian maximum a posteriori

estimator you get by assuming a uniform prior
on multinomials (a dirichlet prior)

andrew mccallum, umass amherst

dirichlet distribution

       multinomial    is a die: a distribution over a

finite alphabet of outcomes

       dirichlet    is a dice generator: a distribution

over multinomials!

    it is a conjugate prior for multinomials

andrew mccallum, umass amherst

dirichlet examples

for   1 =   2 =   

andrew mccallum, umass amherst

laplace smoothing

    problem: gives too much id203 mass to unseens
    not good for large vocabulary, comparatively little data

(nlp!)

    e.g. 10,000 word vocab, 1,000,000 words of training data,

but    comes across    occurs 10 times.  of those, 8 times
next word is    as   
    pid113(as|comes across) = 0.8
    plaplace(as|comes across) = (8+1)/(10+10000)=0.0009

    quick fix: lidstone   s law (mitchell   s 1997    m-estimate   ):

andrew mccallum, umass amherst

how much mass to allocate to unseens?

    for laplace smoothing,

in p(.|comes across), 10,000/10,010 of the
prob mass is given to unseen events.

    how do we know that this is too much?

andrew mccallum, umass amherst

absolute discounting

   

   

idea is that we want to discount counts of seen things
a little, and reallocate this id203 mass to
unseens

    by subtracting a fixed count, id203 estimates for

commonly seen things are scarcely affected, while
probabilities of rare things are greatly affected
if the discount is around   =0.75, then seeing
something once is not so different than not having
seen it at all

andrew mccallum, umass amherst

held out estimator

    how do you know how likely you are to see a new

word type in the future (in a certain context)?
    examine some further text and find out

(empirical held-out estimators = validation)

    divide data into two pots: training data, validation data
    n = number of (non-unique) bigrams in training
    nr = number of unique bigrams with freq r in training
    tr = number of times that all bigrams appearing r times

in the training data appeared in the validation data.

andrew mccallum, umass amherst

tr/nr is an    improved estimate for r   .

pots of data for

estimating and testing models

    major error: testing on your training data
    overfitting: expect future events to be too much like

the events on which it was trained, rather than
allowing sufficiently for other possibilities.

    training data
    validation data
    testing data

    development test data
    final test data

    don   t report results on just one test set, but average

of many, and report variance    use a test of
statistical significance

andrew mccallum, umass amherst

held-out estimator with cross-validation

    reshuffle the training data several times into

pots of training and validation data

    calculate pho(w1w2) for each split, then

average them.

    extreme case of cross-validation:

leave-one-out cross validation
    train on n-1 of the words, validate on 1 word
    how much id203 mass would be reserved for

the unseen words in this case?

andrew mccallum, umass amherst

good-turing smoothing

    derivation reflects leave-one-out estimation
    for each word token in data, call it the validation

set; remaining data is training set.

    the validation-set word has count r in training set.
    see how often any word has r counts in training
set.  (how many different words have count r?)

    this will happen every time word left out has r+1

counts in original data

    so total count mass of r count words is assigned

from mass of r+1 count words =nr+1 x (r+1)

    apply to low counts; not needed (harmful!) for high

count words

andrew mccallum, umass amherst

good-turing smoothing

    all words with same count get same

id203 (as before)

    count mass of words with r+1 occurrences is

assigned to words with r occurrences.

    r* is corrected frequency estimates for a word

occurring r times

andrew mccallum, umass amherst

estimated frequencies in ap newswire

(church & gale 1991)

r=fid113
0
1
2
3
4
5
6
7
8

femprical
0.000027
0.4480
1.25
2.24
3.23
4.21
5.23
6.21
7.21

flap
0.000137
.000274
0.000411
0.000548
0.000685
0.000822
0.000959
0.00109
0.00123

fdel
0.000037
0.396
1.24
2.23
3.22
4.22
5.20
6.21
7.18

fgt
0.000027
0.446
1.26
2.24
3.24
4.22
5.19
6.21
7.24

andrew mccallum, umass amherst

differentiating based on history

    so far the methods considered have all used
nothing but the raw frequency of an id165.
       the large    p(large|the)
       the mauve    p(mauve|the)
    if c(   the large   ) = c(   the mauve   ) (e.g. = 0)

then p(large|the) = p(mauve|the)

    this doesn   t seem right

    also use frequency of its (n-1)-gram

    p(large)
    p(mauve)

andrew mccallum, umass amherst

linear interpolation

    estimate id203 of an id165 from a weighted

average of low-order   high order id165s.

    where      s sum to 1
    set      s by hand or from held-out data
   
    also known as    shrinkage    [stein 1957]

they can be functions of (equivalence-classed histories)

    works surprisingly well!

andrew mccallum, umass amherst

assigning id203 to

the    unseen    event

    at test time, see word, u, that wasn   t seen at

training time

    p(u) = ?

    replace all singleton word tokens in training

data with special token, <unk>

andrew mccallum, umass amherst

smoothing: rest of the story

    other methods:

    backoff (katz 1987): try four-gram, if zero-count,

try tri-gram, if zero-count, try bi-gram,   

    kneser and ney (1995): backoff id165 counts

not proportional to frequency of id165 in training
data but to expectation of how often it should
occur in novel trigram (since one only uses backoff
estimate when trigam not found)

    witten-bell discounting
    smoothed maximum id178 models
    see (chen and goodman 1998) for a survey.

andrew mccallum, umass amherst

    progress in the field is often dominated, not
by the need to create fancier more complex
models,

    but by the need to do a good job of estimating
parameters for the simpler models we already
have.

andrew mccallum, umass amherst

statistical id38

noam chomsky
but it must be recognized that the
notion of    id203 of a sentence   
is an entirely useless one, under any
known interpretation of the term.

(1969)

andrew mccallum, umass amherst

fred jelinek
anytime a linguist leaves the group,
the [speech] recognition rate goes up.
  (while at ibm speech group, 1988).

    progress in the field is often dominated, not
by the need to create fancier more complex
models,

    but by the need to do a good job of estimating
parameters for the simpler models we already
have.

    real benefit comes from targeted

enhancements, and sharp tool set of
excellent estimation techniques

andrew mccallum, umass amherst

distinctiveness of nlp as an ml problem

    most structure is hidden
    relational, id124 nature
    long pipelines, with cascading errors
    large and strange, sparse discrete distributions
    large scale
    feature-driven; performance driven

andrew mccallum, umass amherst

hw#4

as, usual, your choice:
    naive bayes classifier

    spam vs ham
    english vs french vs spanish vs klingon
       sliding window    part-of-speech    tagger

    id165 language model

    train and generate language
    use for id147 (there vs their)

andrew mccallum, umass amherst

hw#4 help

accuracy evaluation

result of running classifier on a test set:

filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
...

true spam

true ham

pred spam
pred ham

tp

fn

fp

tn

accuracy = (tp+tn) / (tp+tn+fp+fn)
precision = tp / (tp+fp)
recall = tp / (tp+fn)
f1 = harmonic mean of precision & recall

andrew mccallum, umass amherst

hw#4 help

precision-recall curve

result of running classifier on a test set:

filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
...

true spam

true ham

pred spam
pred ham

tp

fn

fp

tn

accuracy = (tp+tn) / (tp+tn+fp+fn)
precision = tp / (tp+fp)
recall = tp / (tp+fn)
f1 = harmonic mean of precision & recall

andrew mccallum, umass amherst

hw#4 help

accuracy-coverage curve

result of running classifier on a test set:

filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
filename trueclass predclass p(predclass|doc)
...

true spam

true ham

pred spam
pred ham

tp

fn

fp

tn

accuracy = (tp+tn) / (tp+tn+fp+fn)
precision = tp / (tp+fp)
recall = tp / (tp+fn)
f1 = harmonic mean of precision & recall

andrew mccallum, umass amherst

hw#4 help

working with log-probabilities

    getting back to p(c|d)

    subtract a constant to make all non-positive
    exp()

andrew mccallum, umass amherst

