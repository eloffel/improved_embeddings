lecture 5: id18s

introduction to natural language processing

cs 585
fall 2007

andrew mccallum

also includes material from chris manning.

today   s main points

    in-class hands-on exercise
    a brief introduction to a little syntax.
    de   ne id18s.

give some examples.

    chomsky normal form. converting to it.
    parsing as search

top-down, bottom up (shift-reduce), and the problems with each.

administration

    your e   orts in hw1 looks good! will get hw1 back to you on thursday.

might want to wait to hand in hw2 until after you get it back.

    will send ping email to cs585-class@cs.umass.edu.

language structure and meaning

we want to know how meaning is mapped onto what language structures.
commonly in english in ways like this:

[thing the dog] is [place in the garden]

[thing the dog] is [property    erce]

[action [thing the dog] is chasing [thing the cat]]

[state [thing the dog] was sitting [place in the garden]
yesterday]]

[time

[action [thing we] ran [path out into the water]]

[action [thing the dog] barked [property/manner loudly]]

[action [thing the dog] barked [property/amount nonstop for    ve
hours]]

word categories: traditional parts of speech

noun
verb
pronoun
adverb
adjective
conjunction
preposition
interjection

names of things
action or state
used for noun
modi   es v, adj, adv
modi   es noun
joins things
relation of n
an outcry

boy, cat, truth
become, hit
i, you, we
sadly, very
happy, clever
and, but, while
to, from, into
ouch, oh, alas, psst

part of speech    substitution test   

the {sad, intelligent, green, fat, ...} one is in the corner.

constituency

the idea: groups of words may behave as a single unit or phrase, called a
consituent.

e.g. noun phrase

kermit the frog
they
december twenty-sixth
the reason he is running for president

constituency

sentences have parts, some of which appear to have subparts. these
groupings of words that go together we will call constituents.

(how do we know they go together? coming in a few slides...)

i hit the man with a cleaver
i hit [the man with a cleaver]
i hit [the man] with a cleaver

you could not go to her party
you [could not] go to her party
you could [not go] to her party

constituent phrases

for constituents, we usually name them as phrases based on the word that
heads the constituent:

the man from amherst
extremely clever
down the river
killed the rabbit

is a noun phrase (np) because the head man is a noun
is an adjective phrase (ap) because the head clever is an adjective
is a prepositional phrase (pp) because the head down is a preposition
is a verb phrase (vp) because the head killed is a verb

note that a word is a constituent (a little one). sometimes words also act
as phrases. in:

joe grew potatoes.
joe and potatoes are both nouns and noun phrases.

compare with:

the man from amherst grew beautiful russet potatoes.

we say joe counts as a noun phrase because it appears in a place that a
larger noun phrase could have been.

evidence constituency exists

1. they appear in similar environments (before a verb)

kermit the frog comes on stage
they come to massachusetts every summer
december twenty-sixth comes after christmas
the reason he is running for president comes out only now.
but not each individual word in the consituent
*the comes our... *is comes out... *for comes out...

2. the constituent can be placed in a number of di   erent locations
consituent = prepositional phrase: on december twenty-sixth
on december twenty-sixth i   d like to    y to florida.
i   d like to    y on december twenty-sixth to florida.
i   d like to    y to florida on december twenty-sixth.
but not split apart
*on december i   d like to    y twenty-sixth to florida.
*on i   d like to    y december twenty-sixth to florida.

context-free grammar

the most common way of modeling constituency.

id18 = context-free grammar = phrase structure grammar
= bnf = backus-naur form

the idea of basing a grammar on constituent structure dates back to
wilhem wundt (1890), but not formalized until chomsky (1956), and,
independently, by backus (1959).

context-free grammar

g = ht, n, s, ri

    t is set of terminals (lexicon)
    n is set of non-terminals for nlp, we usually distinguish out a set

p     n of preterminals which always rewrite as terminals.

    s is start symbol (one of the nonterminals)
    r is rules/productions of the form x       , where x is a nonterminal

and    is a sequence of terminals and nonterminals (may be empty).

    a grammar g generates a language l.

an example context-free grammar

g = ht, n, s, ri
t = {that, this, a, the, man, book,    ight, meal, include, read, does}
n = {s, np, nom, vp, det, noun, verb, aux}
s = s
r = {
s     np vp
s     aux np vp
s     vp
np     det nom
nom     noun
nom     noun nom
vp     verb
vp     verb np

det     that | this | a | the
noun     book |    ight | meal | man
verb     book | include | read
aux     does

}

application of grammar rewrite rules

det     that | this | a | the
noun     book |    ight | meal | man
verb     book | include | read
aux     does

s     np vp
s     aux np vp
s     vp
np     det nom
nom     noun
nom     noun nom
vp     verb
vp     verb np
s     np vp
    det nom vp
    the nom vp
    the noun vp
    the man vp
    the man verb np
    the man read np
    the man read det nom
    the man read this nom
    the man read this noun
    the man read this book

parse tree

s

        
hhhh
nom

hhhhhhhh
vp
     
hhhhh
np

verb

np

    

det

the

noun

read

det

    

hhhh
nom

man

this

noun

book

id18s can capture recursion

example of seemingly endless recursion of embedded prepositional phrases:
pp     prep np
np     noun pp

[s the mailman ate his [n p lunch [p p with his friend [p p from the cleaning
sta    [p p of the building [p p at the intersection [p p on the north end [p p
of town]]]]]]].

(bracket notation)

grammaticality

a id18 de   nes a formal
words) that can be derived by the grammar.

language = the set of all sentences (strings of

sentences in this set said to be grammatical.

sentences outside this set said to be ungrammatical.

the id154

    type 0 languages / grammars

rewrite rules          
where    and    are any string of terminals and nonterminals

    context-sensitive languages / grammars

rewrite rules   x             
where x is a non-terminal, and   ,   ,    are any string of terminals and
nonterminals, (   must be non-empty).
    context-free languages / grammars

rewrite rules x       
where x is a nonterminal and    is any string of terminals and
nonterminals

    regular languages / grammars

rewrite rules x       y
where x, y are single nonterminals, and    is a string of terminals; y
might be missing.

parsing regular grammars

(languages that can be generated by    nite-state automata.)
finite state automaton     regular expression     regular grammar

space needed to parse: constant

time needed to parse: linear (in the length of the input string)

cannot do embedded recursion, e.g. anbn. (context-free grammars can.)
in the language: ab, aaabbb; not in the language: aabbb

the cat likes tuna    sh.
the cat the dog chased likes tuna    sh
the cat the dog the boy loves chased likes tuna    sh.

john, always early to rise, even after a sleepless night    lled with the cries
of the neighbor   s baby, goes running every morning.

john and mary, always early to rise, even after a sleepless night    lled with
the cries of the neighbor   s baby, go running every morning.

parsing context-free grammars

(languages that can be generated by pushdown automata.)

widely used for surface syntax description (correct word order speci   cation)
in natural languages.

space needed to parse: stack (sometimes a stack of stacks)
in general, proportional to the number of levels of recursion in the data.

time needed to parse: in general o(n3).

can to anbn, but cannot do anbncn.

chomsky normal form
all rules of the form x     y z or x     a or s      .
(s is the only non-terminal that can go to  .)
any id18 can be converted into this form.
how would you convert the rule w     xy az to chomsky normal form?

chomsky normal form conversion

these steps are used in the conversion:

1. make s non-recursive

2. eliminate all epsilon except the one in s (if there is one)

3. eliminate all chain rules

4. remove useless symbols (the ones not used in any production).

how would you convert the following grammar?
s     abs
s      
a      
a     xyz
b     wb
b     v

parsing context-sensitive grammars

(languages that can be recognized by a non-deterministic turing machine
whose tape is bounded by a constant times the length of the input.)

natural languages are really not context-free: e.g. pronouns more likely in
object rather than subject of a sentence.

but parsing is pspace-complete! (recognized by a turing machine using
a polynomial amount of memory, and unlimited time.)

often work with mildly context-sensitive grammars. more on this next
week. e.g. tree-adjoining grammars. time needed to parse, e.g. o(n6) or
o(n5)...

bottom-up versus top-down science

    empiricist

britain: francis bacon, john locke
knowledge is induced and reasoning proceeds based on data from the
real world.
    rationalist

continental europe: descartes
learning and reasoning is guided by prior knowledge and innate ideas.

what is parsing?

we want to run the grammar backwards to    nd the structure.

parsing can be viewed as a search problem.

we search through the legal rewritings of the grammar.
we want to    nd all structures matching an input string of words (for the
moment)

we can do this bottom-up or top-down
this distinction is independent of depth-   rst versus breadth-   rst; we can do
either both ways.
doing this we build a search tree which is di   erent from the parse tree.

recognizers and parsers

    a recognizer is a program for which a given grammar and a given
sentence returns yes if the sentence is accepted by the grammar (i.e.,
the sentence is in the language), and no otherwise.

    a parser in addition to doing the work of a recognizer also returns the

set of parse trees for the string.

soundness and completeness

    a parser is sound if every parse it returns is valid/correct.
    a parser terminates if it is guaranteed not to go o    into an in   nite loop.
    a parser is complete if for any given grammar and sentence it is sound,

produces every valid parse for that sentence, and terminates.

    (for many cases, we settle for sound but incomplete parsers:

e.g.

probabilistic parsers that return a k-best list.)

top-down parsing

top-down parsing is goal-directed.

    a top-down parser starts with a list of constituents to be built.
    it rewrites the goals in the goal list by matching one against the lhs of
    and expanding it with the rhs,
    ...attempting to match the sentence to be derived.

the grammar rules,

if a goal can be rewritten in several ways, then there is a choice of which
rule to apply (search problem)

can use depth-   rst or breadth-   rst search, and goal ordering.

top-down parsing example (breadth-   rst)

det     that | this | a | the
noun     book |    ight | meal | man
verb     book | include | read
aux     does

s     np vp
s     aux np vp
s     vp
np     det nom
nom     noun
nom     noun nom
vp     verb
vp     verb np

book that    ight.

(work out top-down, breadth-   rst search on the board...)

top-down parsing example (breadth-   rst)

s

    
np
   
hhh
det

nom

s

hhhh

vp

verb

s

   hh
vp
np

s

     

hhhhh

aux

np

vp

s

     
np
   
hhh
det

nom

hhhhh
vp
   
hhh
np

verb

s

vp

...

s

vp

verb

...

s

s

vp
   
hhh
vp

verb

vp

    

verb

book

hhhh
np
hhh
   

det

nom

that

noun

   ight

problems with top-down parsing

    left recursive rules... e.g. np     np pp... lead to in   nite recursion
    will do badly if there are many di   erent rules for the same lhs. consider
if there are 600 rules for s, 599 of which start with np, but one of which
starts with a v, and the sentence starts with a v.

    useless work: expands things that are possible top-down but not there

(no bottom-up evidence for them).

    top-down parsers do well if there is useful grammar-driven control: search

is directed by the grammar.

    top-down is hopeless for rewriting parts of speech (preterminals) with
in practice that is always done bottom-up as lexical

words (terminals).
lookup.

    repeated work: anywhere there is common substructure.

bottom-up parsing

top-down parsing is data-directed.

    the initial goal list of a bottom-up parser is the string to be parsed.
    if a sequence in the goal
    parsing is    nished when the goal list contains just the start symbol.

sequence may be replaced by the lhs of the rule.

list matches the rhs of a rule, then this

if the rhs of several rules match the goal list, then there is a choice of
which rule to apply (search problem)

can use depth-   rst or breadth-   rst search, and goal ordering.

the standard presentation is as id132.

bottom-up parsing example

det     that | this | a | the
noun     book |    ight | meal | man
verb     book | include | read
aux     does

s     np vp
s     aux np vp
s     vp
np     det nom
nom     noun
nom     noun nom
vp     verb
vp     verb np

book that    ight.

(work out bottom-up search on the board...)

id132

stack
()
(book)
(verb)
(verb that)
(verb det)
(verb det    ight)
(verb det noun)
(verb det nom)
(verb np)
(verb)
(s)

input remaining action
book that    ight
that    ight
that    ight
   ight
   ight

shift
reduce, verb     book, (choice #1 of 2)
shift
reduce, det     that
shift
reduce, noun        ight
reduce, nom     noun
reduce, np     det nom
reduce, vp     verb np
reduce, s     v
success!

ambiguity may lead to the need for backtracking.

shift reduce parser

start with the sentence to be parsed in an input bu   er.

    a    shift    action correponds to pushing the next input symbol from the

bu   er onto the stack

    a    reduce    action occurrs when we have a rule   s rhs on top of the
stack. to perform the reduction, we pop the rule   s rhs o    the stack
and replace it with the terminal on the lhs of the corresponding rule.

(when either    shift    or    reduce    is possible, choose one arbitrarily.)

if you end up with only the start symbol on the stack, then success!
if you don   t, and you cannot and no    shift    or    reduce    actions are possible,
backtrack.

shift reduce parser

in a top-down parser, the main decision was which production rule to pick.
in a bottom-up shift-reduce parser there are two decisions:

1. should we shift another symbol, or reduce by some rule?

2. if reduce, then reduce by which rule?

both of which can lead to the need to backtrack

problems with bottom-up parsing

    unable to deal with empty categories:

termination problem, unless
rewriting empties as constituents is somehow restricted (but then it   s
generally incomplete)

    useless work: locally possible, but globally impossible.
    ine   cient when there is great lexical ambiguity (grammar-driven control
it attempts to parse

might help here). conversely, it is data-directed:
the words that are there.

    repeated work: anywhere there is common substructure.

    both top-down (ll) and bottom-up (lr) parsers can (and frequently

do) do work exponential in the sentence length on nlp problems.

principles for success

    left recursive structures must be found, not predicted.
    empty categories must be predicted, not found.
    don   t waste e   ort

re-working what was previously parsed before

backtracking.

an alternative way to    x things:

    grammar

transformations can    x both left-recursion and epsilon

productions.

    then you parse the same language but with di   erent trees.
    but linguists tend to hate you, because the structure of the re-written

grammar isn   t what they wanted.

coming next...

a id145 solution for parsing: cyk (and maybe also earley   s
algorithm).

(then later in the semester.)
probabilistic version of these models. find the most likely parse when
several are possible.

