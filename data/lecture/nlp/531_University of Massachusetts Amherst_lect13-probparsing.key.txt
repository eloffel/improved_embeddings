probabilistic parsing

in practice

lecture #15

computational linguistics
cmpsci 591n, spring 2006

andrew mccallum

(including slides from michael collins, chris manning, jason eisner, mary harper)

today   s main points

    training data
    how to evaluate parsers
    limitations of pid18s, enhancements & 
alternatives
- lexicalized pid18s
- structure sensitivity
- left-corner parsing
- faster parsing with id125
- dependency parsers
    current state of the art

andrew mccallum, umass

treebanks

    pure grammar induction approaches tend not to 

produce the parse trees that people want

    solution

   give a some example of parse trees that we want
   make a learning tool learn a grammar

    treebank

   a collection of such example parses
   penntreebank is most widely used

treebanks

    id32

(np arizona real estate loans)

    trees are represented via bracketing
    fairly    at structures for noun phrases
    tagged with grammatical and semantic functions
    use empty nodes(*) to indicate understood subjects and 

(-sbj ,    loc,    )

extraction gaps

(  ( s ( np-sbj  the move)
         ( vp  followed
                  ( np  ( np a round )
                           ( pp  of
                                   (np  ( np similar increases )
                                           ( pp by
                                                  ( np other lenders ) )
                                           ( pp against
                                                  ( np arizona real estate loans )))))
                  ,
                   ( s-adv ( np-sbj * )
                                 ( vp  reflecting
                                           ( np a continuing decline )
                                           ( pp-loc  in
                                                             (np  that market ))))))
    . )

treebanks

    many people have argued that it is better to have 
linguists constructing treebanks than grammars

    because it is easier
    than

- to work out the correct parse of sentences
- to try to determine what all possible manifestations of a 

certain rule or grammatical construct are

treebanking issues

    type of data
- task dependent (newspaper, journals, 
novels, technical manuals, dialogs, email)
    size
- the more the better!  (resource-limited)
    parse representation
- dependency vs parse tree
- attributes.  what do encode? words, 
- reference & bookkeeping: date time, who 

morphology, syntax, semantics...

did what

andrew mccallum, umass

organizational issues

    team
- 1 team leader; bookkeeping/hiring
- 1 guideline person
- 1 linguistic issues person
- 3-5 annotators
- 1-2 technical staff/programming
- 2 checking persons
    double annotation if possible.

andrew mccallum, umass

treebanking plan

    the main points (after getting funding)
- planning
- basic guidelines development
- annotation & guidelines refinement
- consistency checking, guidelines finalization
- packaging and distribution
    time needed
- on the order of 2 years per 1 million words
- only about 1/3 of the total effort is annotation

andrew mccallum, umass

parser evaluation

andrew mccallum, umass

evaluation

ultimate goal is to build system for ie, qa, mt

people are rarely interested in syntactic analysis for its own 
sake
evaluate the system for evaluate the parser

for simplicity and modularization, and convenience

compare parses from a parser with the result of hand 
parsing of a sentence(gold standard)

what is objective criterion that we are trying to 
maximize?

evaluation

tree accuracy (exact match)

it is a very tough standard!!!
but in many ways it is a sensible one to use

parseval measures

for some purposes, partially correct parses can be useful
originally for non-statistical parsers
evaluate the component pieces of a parse
measures : precision, recall, crossing brackets

evaluation

(labeled) precision

how many brackets in the parse match those in the correct 
tree (gold standard)?

(labeled) recall

how many of the brackets in the correct tree are in the 
parse?

crossing brackets

average of how many constituents in one tree cross over 
constituent boundaries in the other tree
b1 
b2 
b3       ( 
b4 
 

         ( 
 
 
 
w2 

 
      ( 
 
 
w1 

 
       )
 
 
w4 

 
         ( 
w5 

 
 
 
 
w3 

        )
        )
w7 

 
 
w6 

        )

 

w8

problems with parseval

even vanilla pid18 performs quite well

it measures success at the level of individual decisions

you must make many consecutive decisions correctly to be 
correct on the entire tree. 

problems with parseval (2)

behind story

the structure of id32

flat     few brackets     low crossing brackets
troublesome brackets are avoided 
         high precision/recall

the errors in precision and recall are minimal

in some cases wrong pp attachment penalizes precision, 
recall and crossing bracket accuracy minimally.

on the other hand, attaching low instead of high, then every 
node in the right-branching tree will be wrong: serious harm 

3
62%

evaluation

do parseval measures succeed in real tasks?
many small parsing mistakes might not affect tasks of 
semantic interpretation
(bonnema 1996,1997) 

tree accuracy of the parser : 62%
correct semantic interpretations : 88%

(hermajakob and mooney 1997) 
english to german translation

at the moment, people feel parseval measures are 
adequate for the comparing parsers

lexicalized parsing

limitations of pid18s

    pid18s assume:
- place invariance
- context free: p(rule) independent of 
    words outside span
    also, words with overlapping derivation
- ancestor free: p(rule) independent of
    non-terminals above.

    lack of sensitivity to lexical information
    lack of sensitivity to structural frequencies

andrew mccallum, umass

lack of lexical dependency
means that
p(vp     v np np)
is independent of the particular verb 
involved!
... but much more likely with ditransitive 
verbs (like gave).
he gave the boy a ball.
he ran to the store.

andrew mccallum, umass

the need for lexical dependency
probabilities dependent on lexical words

problem 1 : verb subcategorization

vp expansion is independent of the choice of verb
however    

 
 
vp -> v
vp -> v np
vp -> v pp
vp -> v sbar
vp -> v s

verb

come

take

think

want

4.6%
5.7%
9.5%
2.6%
0.2% 13.9%
1.1% 32.1%
0.3%
3.1%
34.5%
7.1%
0.3% 73.0%
6.6%
0.2%
4.8% 70.8%
1.3%
2.2%

including actual words information when making decisions 
about tree structure is necessary

weakening the independence 

assumption of pid18
probabilities dependent on lexical words

problem 2 : phrasal attachment

lexical content of phrases provide information for decision
syntactic category of the phrases provide very little 
information
standard pid18 is worse than id165 models

another case of pp attachment 

ambiguity

anothercaseofppattachmentambiguity(a)snpnnsworkersvpvpvbddumpednpnnssacksppinintonpdtannbinanother case of pp attachment 

ambiguity

(b)snpnnsworkersvpvbddumpednpnpnnssacksppinintonpdtannbinanother case of pp attachment 

ambiguity

(a)rulessnpvpnpnnsvpvpppvpvbdnpnpnnsppinnpnpdtnnnnsworkersvbddumpednnssacksinintodtannbin(b)rulessnpvpnpnnsnpnpppvpvbdnpnpnnsppinnpnpdtnnnnsworkersvbddumpednnssacksinintodtannbinifnpnpppnpvpvpppvpthen(b)ismoreprobable,else(a)ismoreprobable.attachmentdecisioniscompletelyindependentofthewordsa case of coordination ambiguity

acaseofcoordinationambiguity(a)npnpnpnnsdogsppininnpnnshousesccandnpnnscats(b)npnpnnsdogsppininnpnpnnshousesccandnpnnscats(a)rulesnpnpccnpnpnpppnpnnsppinnpnpnnsnpnnsnnsdogsininnnshousesccandnnscats(b)rulesnpnpccnpnpnpppnpnnsppinnpnpnnsnpnnsnnsdogsininnnshousesccandnnscatsherethetwoparseshaveidenticalrules,andthereforehaveidenticalid203underanyassignmentofpid18ruleprobabilitiesweakening the independence 

assumption of pid18
probabilities dependent on lexical words

solution

lexicalize id18 : each phrasal node with its head word

background idea

strong lexical dependencies between heads and their 
dependents

heads in context-free rules

headsincontext-freerulesaddannotationsspecifyingthe   head   ofeachrule:snpvpvpvivpvtnpvpvpppnpdtnnnpnpppppinnpvisleepsvtsawnnmannnwomannntelescopedttheinwithininnote:s=sentence,vp=verbphrase,np=nounphrase,pp=prepositionalphrase,dt=determiner,vi=intransitiveverb,vt=transitiveverb,nn=noun,in=prepositionmore about heads

moreaboutheadseachcontext-freerulehasone   special   childthatistheheadoftherule.e.g.,snpvp(vpisthehead)vpvtnp(vtisthehead)npdtnnnn(nnisthehead)acoreideainlinguistics(x-bartheory,head-drivenphrasestructuregrammar)someintuitions:   thecentralsub-constituentofeachrule.   thesemanticpredicateineachrule.rules which recover heads:

example rules for nps

ruleswhichrecoverheads:anexampleofrulesfornpsiftherulecontainsnn,nns,oid56p:choosetherightmostnn,nns,oid56pelseiftherulecontainsannp:choosetheleftmostnpelseiftherulecontainsajj:choosetherightmostjjelseiftherulecontainsacd:choosetherightmostcdelsechoosetherightmostchilde.g.,npdtnnpnnnpdtnnnnpnpnpppnpdtjjnpdtadding headwords to trees

addingheadwordstotreess(questioned)np(lawyer)dtthennlawyervp(questioned)vtquestionednp(witness)dtthennwitnessaconstituentreceivesitsheadwordfromitsheadchild.snpvp(sreceivesheadwordfromvp)vpvtnp(vpreceivesheadwordfromvt)npdtnn(npreceivesheadwordfromnn)adding headtags to trees

addingheadtagstotreess(questioned,vt)np(lawyer,nn)dtthennlawyervp(questioned,vt)vtquestionednp(witness,nn)dtthennwitnessalsopropogatepart-of-speechtagsupthetrees(we   llseesoonwhythisisuseful!)explosion of number of rules

new rules might look like:
vp[gave]     v[gave] np[man] np[book]
but this would be a massive explosion in number of 
rules (and parameters)

lexicalized parsing, with smoothing

lexicalized parsing [charniak 1997]

enrichingapid18!anaivepid18withtraditionalnonterminals(np,pp,etc.)worksquitepoorlyduetotheindependenceassump-tionsitembodies(charniak1996)!fix:encodemoreinformationintothenonterminalspace"structuresensitivity(manningandcarpenter1997;johnson1998b)!expansionofnodesdependsalotontheirpositioninthetree(independentoflexicalcontent)!e.g.,enrichnodesbyalsorecordingtheirparents:snpisdi   erenttovpnp377enrichingapid18(2)"(head)lexicalization(collins1997;charniak1997)!theheadwordofaphrasegivesagoodrepresen-tationofthephrase   sstructureandmeaning!putsthepropertiesofwordsbackintoapid18swalkednpsuennpsuesuevpwalkedvbdwalkedwalkedppintopintointonpstoredtthethennstorestore378parsingviaclassi   cationdecisions:charniak(1997)!averysimple,conservativemodeloflexicalizedpid18!probabilisticconditioningis   top-down   (butactualcom-putationisbottom-up)srosenppro   tsjjcorporatecorporatennspro   tspro   tsvprosevroserose379charniak(1997)examplesrosenpvprosea.h=pro   ts;c=npb.ph=rose;pc=sc.p(h|ph,c,pc)d.p(r|h,c,pc)srosenppro   tsvprosesrosenppro   tsjjnnspro   tsvprose380charniak(1997)linearinterpolation/shrinkage  p(h|ph,c,pc)=  1(e)pid113(h|ph,c,pc)+  2(e)pid113(h|c(ph),c,pc)+  3(e)pid113(h|c,pc)+  4(e)pid113(h|c)!  i(e)ishereafunctionofhowmuchonewouldexpecttoseeacertainoccurrence,giventheamountoftrainingdata,wordcounts,etc.!c(ph)issemanticclassofparentheadword!techniqueslikethesefordealingwithdatasparsenessarevitaltosuccessfulmodelconstruction381charniak(1997)shrinkageexamplep(prft|rose,np,s)p(corp|prft,jj,np)p(h|ph,c,pc)00.245p(h|c(ph),c,pc)0.003520.0150p(h|c,pc)0.0006270.00533p(h|c)0.0005570.00418!allowsutilizationofrichhighlyconditionedestimates,butsmootheswhensu   cientdataisunavailable!onecan   tjustuseid113s:onecommonlyseespreviouslyunseenevents,whichwouldhaveid2030.382[charniak 1997]

generate head, then head constituent & rule

h=head word, c=head consituent 

ph=parent head word, parent head constituent

enrichingapid18!anaivepid18withtraditionalnonterminals(np,pp,etc.)worksquitepoorlyduetotheindependenceassump-tionsitembodies(charniak1996)!fix:encodemoreinformationintothenonterminalspace"structuresensitivity(manningandcarpenter1997;johnson1998b)!expansionofnodesdependsalotontheirpositioninthetree(independentoflexicalcontent)!e.g.,enrichnodesbyalsorecordingtheirparents:snpisdi   erenttovpnp377enrichingapid18(2)"(head)lexicalization(collins1997;charniak1997)!theheadwordofaphrasegivesagoodrepresen-tationofthephrase   sstructureandmeaning!putsthepropertiesofwordsbackintoapid18swalkednpsuennpsuesuevpwalkedvbdwalkedwalkedppintopintointonpstoredtthethennstorestore378parsingviaclassi   cationdecisions:charniak(1997)!averysimple,conservativemodeloflexicalizedpid18!probabilisticconditioningis   top-down   (butactualcom-putationisbottom-up)srosenppro   tsjjcorporatecorporatennspro   tspro   tsvprosevroserose379charniak(1997)examplesrosenpvprosea.h=pro   ts;c=npb.ph=rose;pc=sc.p(h|ph,c,pc)d.p(r|h,c,pc)srosenppro   tsvprosesrosenppro   tsjjnnspro   tsvprose380charniak(1997)linearinterpolation/shrinkage  p(h|ph,c,pc)=  1(e)pid113(h|ph,c,pc)+  2(e)pid113(h|c(ph),c,pc)+  3(e)pid113(h|c,pc)+  4(e)pid113(h|c)!  i(e)ishereafunctionofhowmuchonewouldexpecttoseeacertainoccurrence,giventheamountoftrainingdata,wordcounts,etc.!c(ph)issemanticclassofparentheadword!techniqueslikethesefordealingwithdatasparsenessarevitaltosuccessfulmodelconstruction381charniak(1997)shrinkageexamplep(prft|rose,np,s)p(corp|prft,jj,np)p(h|ph,c,pc)00.245p(h|c(ph),c,pc)0.003520.0150p(h|c,pc)0.0006270.00533p(h|c)0.0005570.00418!allowsutilizationofrichhighlyconditionedestimates,butsmootheswhensu   cientdataisunavailable!onecan   tjustuseid113s:onecommonlyseespreviouslyunseenevents,whichwouldhaveid2030.382smoothing in [charniak 1997]

enrichingapid18!anaivepid18withtraditionalnonterminals(np,pp,etc.)worksquitepoorlyduetotheindependenceassump-tionsitembodies(charniak1996)!fix:encodemoreinformationintothenonterminalspace"structuresensitivity(manningandcarpenter1997;johnson1998b)!expansionofnodesdependsalotontheirpositioninthetree(independentoflexicalcontent)!e.g.,enrichnodesbyalsorecordingtheirparents:snpisdi   erenttovpnp377enrichingapid18(2)"(head)lexicalization(collins1997;charniak1997)!theheadwordofaphrasegivesagoodrepresen-tationofthephrase   sstructureandmeaning!putsthepropertiesofwordsbackintoapid18swalkednpsuennpsuesuevpwalkedvbdwalkedwalkedppintopintointonpstoredtthethennstorestore378parsingviaclassi   cationdecisions:charniak(1997)!averysimple,conservativemodeloflexicalizedpid18!probabilisticconditioningis   top-down   (butactualcom-putationisbottom-up)srosenppro   tsjjcorporatecorporatennspro   tspro   tsvprosevroserose379charniak(1997)examplesrosenpvprosea.h=pro   ts;c=npb.ph=rose;pc=sc.p(h|ph,c,pc)d.p(r|h,c,pc)srosenppro   tsvprosesrosenppro   tsjjnnspro   tsvprose380charniak(1997)linearinterpolation/shrinkage  p(h|ph,c,pc)=  1(e)pid113(h|ph,c,pc)+  2(e)pid113(h|c(ph),c,pc)+  3(e)pid113(h|c,pc)+  4(e)pid113(h|c)!  i(e)ishereafunctionofhowmuchonewouldexpecttoseeacertainoccurrence,giventheamountoftrainingdata,wordcounts,etc.!c(ph)issemanticclassofparentheadword!techniqueslikethesefordealingwithdatasparsenessarevitaltosuccessfulmodelconstruction381charniak(1997)shrinkageexamplep(prft|rose,np,s)p(corp|prft,jj,np)p(h|ph,c,pc)00.245p(h|c(ph),c,pc)0.003520.0150p(h|c,pc)0.0006270.00533p(h|c)0.0005570.00418!allowsutilizationofrichhighlyconditionedestimates,butsmootheswhensu   cientdataisunavailable!onecan   tjustuseid113s:onecommonlyseespreviouslyunseenevents,whichwouldhaveid2030.382[charniak 1997] smoothing example

enrichingapid18!anaivepid18withtraditionalnonterminals(np,pp,etc.)worksquitepoorlyduetotheindependenceassump-tionsitembodies(charniak1996)!fix:encodemoreinformationintothenonterminalspace"structuresensitivity(manningandcarpenter1997;johnson1998b)!expansionofnodesdependsalotontheirpositioninthetree(independentoflexicalcontent)!e.g.,enrichnodesbyalsorecordingtheirparents:snpisdi   erenttovpnp377enrichingapid18(2)"(head)lexicalization(collins1997;charniak1997)!theheadwordofaphrasegivesagoodrepresen-tationofthephrase   sstructureandmeaning!putsthepropertiesofwordsbackintoapid18swalkednpsuennpsuesuevpwalkedvbdwalkedwalkedppintopintointonpstoredtthethennstorestore378parsingviaclassi   cationdecisions:charniak(1997)!averysimple,conservativemodeloflexicalizedpid18!probabilisticconditioningis   top-down   (butactualcom-putationisbottom-up)srosenppro   tsjjcorporatecorporatennspro   tspro   tsvprosevroserose379charniak(1997)examplesrosenpvprosea.h=pro   ts;c=npb.ph=rose;pc=sc.p(h|ph,c,pc)d.p(r|h,c,pc)srosenppro   tsvprosesrosenppro   tsjjnnspro   tsvprose380charniak(1997)linearinterpolation/shrinkage  p(h|ph,c,pc)=  1(e)pid113(h|ph,c,pc)+  2(e)pid113(h|c(ph),c,pc)+  3(e)pid113(h|c,pc)+  4(e)pid113(h|c)!  i(e)ishereafunctionofhowmuchonewouldexpecttoseeacertainoccurrence,giventheamountoftrainingdata,wordcounts,etc.!c(ph)issemanticclassofparentheadword!techniqueslikethesefordealingwithdatasparsenessarevitaltosuccessfulmodelconstruction381charniak(1997)shrinkageexamplep(prft|rose,np,s)p(corp|prft,jj,np)p(h|ph,c,pc)00.245p(h|c(ph),c,pc)0.003520.0150p(h|c,pc)0.0006270.00533p(h|c)0.0005570.00418!allowsutilizationofrichhighlyconditionedestimates,butsmootheswhensu   cientdataisunavailable!onecan   tjustuseid113s:onecommonlyseespreviouslyunseenevents,whichwouldhaveid2030.382[charniak 1997]

rule id203 with similar smoothing

p(r|h,hc,pc)=  1(e)p(r|h,hc,pc)  2(e)p(r|h,hc)  3(e)p(r|c(h),hc)  4(e)p(r|hc,pc)  5(e)p(r|hc)sparseness and the id32

sparseness&thepenntreebank!thepenntreebank   1millionwordsofparsedenglishwsj   hasbeenakeyresource(becauseofthewidespreadrelianceonsupervisedlearning)!but1millionwordsislikenothing:"965,000constituents,butonly66whadjp,ofwhichonly6aren   thowmuchorhowmany,butthereisanin   nitespaceofthese(howclever/original/incompetent(atriskassessmentandevaluation))!mostoftheprobabilitiesthatyouwouldliketocompute,youcan   tcompute383sparseness&thepenntreebank(2)!mostintelligentprocessingdependsonbilexicalstatis-tics:likelihoodsofrelationshipsbetweenpairsofwords.!extremelysparse,evenontopicscentraltothewsj:"stocksplummeted2occurrences"stocksstabilized1occurrence"stocksskyrocketed0occurrences"#stocksdiscussed0occurrences!sofartherehasbeenverymodestsuccessaugmentingthepenntreebankwithextraunannotatedmaterialsorusingsemanticclassesorclusters(cf.charniak1997,charniak2000)   assoonastherearemorethantinyamountsofannotatedtrainingdata.384probabilisticparsing!charniak(1997)expandseachphrasestructuretreeinasinglestep.!thisisgoodforcapturingdependenciesbetweenchildnodes!butitisbadbecauseofdatasparseness!apuredependency,onechildatatime,modelisworse!butonecandobetterbyinbetweenmodels,suchasgeneratingthechildrenasamarkovprocessonbothsidesofthehead(collins1997;charniak2000)385correctingwrongcontext-freedomassumptionshorizonalmarkovorderverticalorderh=0h=1h   2h=2h=   v=0noannotation71.2772.573.4672.9672.62(854)(3119)(3863)(6207)(9657)v   1sel.parents74.7577.4277.7777.5076.91(2285)(6564)(7619)(11398)(14247)v=1allparents74.6877.4277.8177.5076.81(2984)(7312)(8367)(12132)(14666)v   2sel.gparents76.5078.5979.0778.9778.54(4943)(12374)(13627)(19545)(20123)v=2allgparents76.7479.1879.7479.0778.72(7797)(15740)(16994)(22886)(22002)386correctingwrongcontext-freedomassumptionsvp  stotovp  vpvbseepp  vpinifnp  ppnnadvertisingnnsworksvp  sto  vptovp  vpvb  vpseesbar  vpin  sbarifs  sbarnp  snn  npadvertisingvp  svbz  vpworks(a)(b)387correctingwrongcontext-freedomassumptionscumulativeindiv.annotationsizef1   f1   f1baseline761977.720.000.00unary-internal806578.150.430.43unary-dt807880.092.370.22unary-rb808180.252.530.48tag-pa852080.622.902.57split-in854181.193.472.17split-aux903481.663.940.62split-cc919081.693.970.17split-%925581.814.090.20tmp-np959482.254.531.12gapped-s974182.284.560.22poss-np982083.065.340.33split-vp1049985.728.001.41base-np1166086.048.320.78dominates-v1409786.919.191.47right-rec-np1527687.049.321.99388sparseness and the id32

sparseness&thepenntreebank!thepenntreebank   1millionwordsofparsedenglishwsj   hasbeenakeyresource(becauseofthewidespreadrelianceonsupervisedlearning)!but1millionwordsislikenothing:"965,000constituents,butonly66whadjp,ofwhichonly6aren   thowmuchorhowmany,butthereisanin   nitespaceofthese(howclever/original/incompetent(atriskassessmentandevaluation))!mostoftheprobabilitiesthatyouwouldliketocompute,youcan   tcompute383sparseness&thepenntreebank(2)!mostintelligentprocessingdependsonbilexicalstatis-tics:likelihoodsofrelationshipsbetweenpairsofwords.!extremelysparse,evenontopicscentraltothewsj:"stocksplummeted2occurrences"stocksstabilized1occurrence"stocksskyrocketed0occurrences"#stocksdiscussed0occurrences!sofartherehasbeenverymodestsuccessaugmentingthepenntreebankwithextraunannotatedmaterialsorusingsemanticclassesorclusters(cf.charniak1997,charniak2000)   assoonastherearemorethantinyamountsofannotatedtrainingdata.384probabilisticparsing!charniak(1997)expandseachphrasestructuretreeinasinglestep.!thisisgoodforcapturingdependenciesbetweenchildnodes!butitisbadbecauseofdatasparseness!apuredependency,onechildatatime,modelisworse!butonecandobetterbyinbetweenmodels,suchasgeneratingthechildrenasamarkovprocessonbothsidesofthehead(collins1997;charniak2000)385correctingwrongcontext-freedomassumptionshorizonalmarkovorderverticalorderh=0h=1h   2h=2h=   v=0noannotation71.2772.573.4672.9672.62(854)(3119)(3863)(6207)(9657)v   1sel.parents74.7577.4277.7777.5076.91(2285)(6564)(7619)(11398)(14247)v=1allparents74.6877.4277.8177.5076.81(2984)(7312)(8367)(12132)(14666)v   2sel.gparents76.5078.5979.0778.9778.54(4943)(12374)(13627)(19545)(20123)v=2allgparents76.7479.1879.7479.0778.72(7797)(15740)(16994)(22886)(22002)386correctingwrongcontext-freedomassumptionsvp  stotovp  vpvbseepp  vpinifnp  ppnnadvertisingnnsworksvp  sto  vptovp  vpvb  vpseesbar  vpin  sbarifs  sbarnp  snn  npadvertisingvp  svbz  vpworks(a)(b)387correctingwrongcontext-freedomassumptionscumulativeindiv.annotationsizef1   f1   f1baseline761977.720.000.00unary-internal806578.150.430.43unary-dt807880.092.370.22unary-rb808180.252.530.48tag-pa852080.622.902.57split-in854181.193.472.17split-aux903481.663.940.62split-cc919081.693.970.17split-%925581.814.090.20tmp-np959482.254.531.12gapped-s974182.284.560.22poss-np982083.065.340.33split-vp1049985.728.001.41base-np1166086.048.320.78dominates-v1409786.919.191.47right-rec-np1527687.049.321.99388lexicalized, markov out from head

collins 1997: 

markov model out from head

sparseness&thepenntreebank!thepenntreebank   1millionwordsofparsedenglishwsj   hasbeenakeyresource(becauseofthewidespreadrelianceonsupervisedlearning)!but1millionwordsislikenothing:"965,000constituents,butonly66whadjp,ofwhichonly6aren   thowmuchorhowmany,butthereisanin   nitespaceofthese(howclever/original/incompetent(atriskassessmentandevaluation))!mostoftheprobabilitiesthatyouwouldliketocompute,youcan   tcompute383sparseness&thepenntreebank(2)!mostintelligentprocessingdependsonbilexicalstatis-tics:likelihoodsofrelationshipsbetweenpairsofwords.!extremelysparse,evenontopicscentraltothewsj:"stocksplummeted2occurrences"stocksstabilized1occurrence"stocksskyrocketed0occurrences"#stocksdiscussed0occurrences!sofartherehasbeenverymodestsuccessaugmentingthepenntreebankwithextraunannotatedmaterialsorusingsemanticclassesorclusters(cf.charniak1997,charniak2000)   assoonastherearemorethantinyamountsofannotatedtrainingdata.384probabilisticparsing!charniak(1997)expandseachphrasestructuretreeinasinglestep.!thisisgoodforcapturingdependenciesbetweenchildnodes!butitisbadbecauseofdatasparseness!apuredependency,onechildatatime,modelisworse!butonecandobetterbyinbetweenmodels,suchasgeneratingthechildrenasamarkovprocessonbothsidesofthehead(collins1997;charniak2000)385correctingwrongcontext-freedomassumptionshorizonalmarkovorderverticalorderh=0h=1h   2h=2h=   v=0noannotation71.2772.573.4672.9672.62(854)(3119)(3863)(6207)(9657)v   1sel.parents74.7577.4277.7777.5076.91(2285)(6564)(7619)(11398)(14247)v=1allparents74.6877.4277.8177.5076.81(2984)(7312)(8367)(12132)(14666)v   2sel.gparents76.5078.5979.0778.9778.54(4943)(12374)(13627)(19545)(20123)v=2allgparents76.7479.1879.7479.0778.72(7797)(15740)(16994)(22886)(22002)386correctingwrongcontext-freedomassumptionsvp  stotovp  vpvbseepp  vpinifnp  ppnnadvertisingnnsworksvp  sto  vptovp  vpvb  vpseesbar  vpin  sbarifs  sbarnp  snn  npadvertisingvp  svbz  vpworks(a)(b)387correctingwrongcontext-freedomassumptionscumulativeindiv.annotationsizef1   f1   f1baseline761977.720.000.00unary-internal806578.150.430.43unary-dt807880.092.370.22unary-rb808180.252.530.48tag-pa852080.622.902.57split-in854181.193.472.17split-aux903481.663.940.62split-cc919081.693.970.17split-%925581.814.090.20tmp-np959482.254.531.12gapped-s974182.284.560.22poss-np982083.065.340.33split-vp1049985.728.001.41base-np1166086.048.320.78dominates-v1409786.919.191.47right-rec-np1527687.049.321.99388modelingruleproductionsasmarkovprocessesstep1:generatecategoryofheadchilds(told,v[6])s(told,v[6])vp(told,v[6])vps,told,v[6]modelingruleproductionsasmarkovprocessesstep2:generateleftmodi   ersinamarkovchains(told,v[6])??vp(told,v[6])s(told,v[6])np(hillary,nnp)vp(told,v[6])vps,told,v[6]np(hillary,nnp)s,vp,told,v[6],leftmodelingruleproductionsasmarkovprocessesstep2:generateleftmodi   ersinamarkovchains(told,v[6])??np(hillary,nnp)vp(told,v[6])s(told,v[6])np(yesterday,nn)np(hillary,nnp)vp(told,v[6])vps,told,v[6]np(hillary,nnp)s,vp,told,v[6],leftnp(yesterday,nn)s,vp,told,v[6],leftmodelingruleproductionsasmarkovprocessesstep2:generateleftmodi   ersinamarkovchains(told,v[6])??np(yesterday,nn)np(hillary,nnp)vp(told,v[6])s(told,v[6])stopnp(yesterday,nn)np(hillary,nnp)vp(told,v[6])vps,told,v[6]np(hillary,nnp)s,vp,told,v[6],leftnp(yesterday,nn)s,vp,told,v[6],leftstops,vp,told,v[6],leftmodelingruleproductionsasmarkovprocessesstep3:generaterightmodi   ersinamarkovchains(told,v[6])stopnp(yesterday,nn)np(hillary,nnp)vp(told,v[6])??s(told,v[6])stopnp(yesterday,nn)np(hillary,nnp)vp(told,v[6])stopvps,told,v[6]np(hillary,nnp)s,vp,told,v[6],leftnp(yesterday,nn)s,vp,told,v[6],leftstops,vp,told,v[6],leftstops,vp,told,v[6],rightare   nement:addingadistancevariableifpositionisadjacenttothehead.s(told,v[6])??vp(told,v[6])s(told,v[6])np(hillary,nnp)vp(told,v[6])vps,told,v[6]np(hillary,nnp)s,vp,told,v[6],left,adding dependency on structure

weakening the independence 

assumption of pid18
probabilities dependent on structural context

pid18s are also de   cient on purely structural grounds too
really context independent?

expansion
np     prp
np     nnp
np     dt nn
np     nn
np     np sbar
np     np pp

% as subj % as obj
2.1%
0.9%
4.6%
2.8%
2.6%
14.1%

13.7%
3.5%
5.6%
1.4%
0.5%
5.6%

weakening the independence 

assumption of pid18

sparseness&thepenntreebank!thepenntreebank   1millionwordsofparsedenglishwsj   hasbeenakeyresource(becauseofthewidespreadrelianceonsupervisedlearning)!but1millionwordsislikenothing:"965,000constituents,butonly66whadjp,ofwhichonly6aren   thowmuchorhowmany,butthereisanin   nitespaceofthese(howclever/original/incompetent(atriskassessmentandevaluation))!mostoftheprobabilitiesthatyouwouldliketocompute,youcan   tcompute383sparseness&thepenntreebank(2)!mostintelligentprocessingdependsonbilexicalstatis-tics:likelihoodsofrelationshipsbetweenpairsofwords.!extremelysparse,evenontopicscentraltothewsj:"stocksplummeted2occurrences"stocksstabilized1occurrence"stocksskyrocketed0occurrences"#stocksdiscussed0occurrences!sofartherehasbeenverymodestsuccessaugmentingthepenntreebankwithextraunannotatedmaterialsorusingsemanticclassesorclusters(cf.charniak1997,charniak2000)   assoonastherearemorethantinyamountsofannotatedtrainingdata.384probabilisticparsing!charniak(1997)expandseachphrasestructuretreeinasinglestep.!thisisgoodforcapturingdependenciesbetweenchildnodes!butitisbadbecauseofdatasparseness!apuredependency,onechildatatime,modelisworse!butonecandobetterbyinbetweenmodels,suchasgeneratingthechildrenasamarkovprocessonbothsidesofthehead(collins1997;charniak2000)385correctingwrongcontext-freedomassumptionshorizonalmarkovorderverticalorderh=0h=1h   2h=2h=   v=0noannotation71.2772.573.4672.9672.62(854)(3119)(3863)(6207)(9657)v   1sel.parents74.7577.4277.7777.5076.91(2285)(6564)(7619)(11398)(14247)v=1allparents74.6877.4277.8177.5076.81(2984)(7312)(8367)(12132)(14666)v   2sel.gparents76.5078.5979.0778.9778.54(4943)(12374)(13627)(19545)(20123)v=2allgparents76.7479.1879.7479.0778.72(7797)(15740)(16994)(22886)(22002)386correctingwrongcontext-freedomassumptionsvp  stotovp  vpvbseepp  vpinifnp  ppnnadvertisingnnsworksvp  sto  vptovp  vpvb  vpseesbar  vpin  sbarifs  sbarnp  snn  npadvertisingvp  svbz  vpworks(a)(b)387correctingwrongcontext-freedomassumptionscumulativeindiv.annotationsizef1   f1   f1baseline761977.720.000.00unary-internal806578.150.430.43unary-dt807880.092.370.22unary-rb808180.252.530.48tag-pa852080.622.902.57split-in854181.193.472.17split-aux903481.663.940.62split-cc919081.693.970.17split-%925581.814.090.20tmp-np959482.254.531.12gapped-s974182.284.560.22poss-np982083.065.340.33split-vp1049985.728.001.41base-np1166086.048.320.78dominates-v1409786.919.191.47right-rec-np1527687.049.321.99388faster parsing 
with id125

pruning for speed

- heuristically throw away constituents that 
probably won   t make it into a complete parse.  
- use probabilities to decide which ones.  

- so probs are useful for speed as well as accuracy!

- both safe and unsafe methods exist

- throw x away if p(x) < 10-200 
(and lower this threshold if we don   t get a parse)
- throw x away if p(x) < 100 * p(y) 
for some y that spans the same set of words
- throw x away if p(x)*q(x) is small, where q(x) is an 
estimate of id203 of all rules needed to combine 
x with the other words in the sentence

 

54

id33

 

55

phrase structure grammars and 

dependency grammars

phrase structure grammar describes the structure of 
sentences with phrase structure tree
alternatively, a dependency grammar describes the 
structure with dependencies between words

one word is the head of a sentence and all other words are 
dependent on that word
dependent on some other word which connects to the 
headword through a sequence of dependencies

phrase structure grammars and 

dependency grammars
two key advantages of dependency grammar are

easy to use lexical information

disambiguation decisions are being made directly with words
no need to build a large superstructure
not necessary to worry about how to lexicalize a ps tree

dependencies are one way of decomposing ps rules

lots of rare    at trees in id32     sparse data
can get reasonable probabilistic estimate if we decompose it

evaluation

method

pid18s (charniak 97)

id90 (magerman 95)

recall

70.6%

84.0%

lexicalized with backoff (charniak 97)

86.7%

lexicalized with markov (collins 97 m1)

87.5%

    with subcategorization (collins 97 m2)

88.1%

maxent-inspired (charniak 2000)

90.1%

precision

74.8%

84.3%

86.6%

87.7%

88.3%

90.1%

