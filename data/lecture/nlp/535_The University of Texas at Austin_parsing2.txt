more pid18 parsing

yoav goldberg

bar ilan university

(with slides from michael collins, christopher manning and mark johnson)

1 / 1

reminder

constituency trees

s

vt

heard

np

prp

i

vp

sbar

in

that

s

np

dt

nn

the

lawyer

vp

vbd

np

pp

questioned

dt

nn

in

np

np

nn

the

witness

under

nn

yesterday

oath

2 / 1

reminder

pid18 parsing

(cid:73) assume trees are generated by a (p)id18.
(cid:73) extract grammar rules from treebank.
(cid:73) each rule in a derivation has a score.
(cid:73) parsing:    nd the tree with the overall best score.

(cid:73) using the cky algorithm

3 / 1

extracting id18 from trees

(cid:73) the leafs of the trees de   ne   
(cid:73) the internal nodes of the trees de   ne n
(cid:73) add a special s symbol on top of all trees
(cid:73) each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

4 / 1

extracting id18 from trees

(cid:73) the leafs of the trees de   ne   
(cid:73) the internal nodes of the trees de   ne n
(cid:73) add a special s symbol on top of all trees
(cid:73) each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

s     np vp

np

det

noun

a

banana

4 / 1

extracting id18 from trees

(cid:73) the leafs of the trees de   ne   
(cid:73) the internal nodes of the trees de   ne n
(cid:73) add a special s symbol on top of all trees
(cid:73) each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

s     np vp
np     adj noun

np

det

noun

a

banana

4 / 1

extracting id18 from trees

(cid:73) the leafs of the trees de   ne   
(cid:73) the internal nodes of the trees de   ne n
(cid:73) add a special s symbol on top of all trees
(cid:73) each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

s     np vp
np     adj noun
adj     fruit

np

det

noun

a

banana

4 / 1

from id18 to pid18

(cid:73) english is not generated from id18     it   s generated by a

pid18!

5 / 1

from id18 to pid18

(cid:73) english is not generated from id18     it   s generated by a
(cid:73) pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
(cid:73) all probabilities for the same lhs sum to 1.

5 / 1

from id18 to pid18

(cid:73) english is not generated from id18     it   s generated by a
(cid:73) pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
(cid:73) all probabilities for the same lhs sum to 1.
(cid:73) multiplying all the rule probs in a derivation gives the

id203 of the derivation.

(cid:73) we want the tree with maximum id203.

5 / 1

from id18 to pid18

(cid:73) english is not generated from id18     it   s generated by a
(cid:73) pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
(cid:73) all probabilities for the same lhs sum to 1.
(cid:73) multiplying all the rule probs in a derivation gives the

id203 of the derivation.

(cid:73) we want the tree with maximum id203.

more formally

p(tree, sent) =

(cid:89)

l   r   deriv(tree)

q(l     r)

5 / 1

from id18 to pid18

(cid:73) english is not generated from id18     it   s generated by a
(cid:73) pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
(cid:73) all probabilities for the same lhs sum to 1.
(cid:73) multiplying all the rule probs in a derivation gives the

id203 of the derivation.

(cid:73) we want the tree with maximum id203.

more formally

p(tree, sent) =

(cid:89)

l   r   deriv(tree)

q(l     r)

tree = arg max

tree   trees(sent)

p(tree|sent) = arg max
tree   trees(sent)

p(tree, sent)

5 / 1

pid18 example

a simple pid18
1.0 s     np vp
0.3 np     adj noun
0.7 np     det noun
1.0 vp     vb np
-
0.2 adj     fruit
0.2 noun        ies
1.0 vb     like
1.0 det     a
0.4 noun     banana
0.4 noun     tomato
0.8 adj     angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1    0.3    0.2    0.7    1.0    0.2    1    1    0.4 =
0.0033

6 / 1

pid18 example

a simple pid18
1.0 s     np vp
0.3 np     adj noun
0.7 np     det noun
1.0 vp     vb np
-
0.2 adj     fruit
0.2 noun        ies
1.0 vb     like
1.0 det     a
0.4 noun     banana
0.4 noun     tomato
0.8 adj     angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1    0.3    0.2    0.7    1.0    0.2    1    1    0.4 =
0.0033

6 / 1

pid18 example

a simple pid18
1.0 s     np vp
0.3 np     adj noun
0.7 np     det noun
1.0 vp     vb np
-
0.2 adj     fruit
0.2 noun        ies
1.0 vb     like
1.0 det     a
0.4 noun     banana
0.4 noun     tomato
0.8 adj     angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1    0.3    0.2    0.7    1.0    0.2    1    1    0.4 =
0.0033

6 / 1

pid18 example

a simple pid18
1.0 s     np vp
0.3 np     adj noun
0.7 np     det noun
1.0 vp     vb np
-
0.2 adj     fruit
0.2 noun        ies
1.0 vb     like
1.0 det     a
0.4 noun     banana
0.4 noun     tomato
0.8 adj     angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1    0.3    0.2    0.7    1.0    0.2    1    1    0.4 =
0.0033

6 / 1

parsing with pid18

(cid:73) parsing with a pid18 is    nding the most probable

derivation for a given sentence.

(cid:73) this can be done quite ef   ciently with dynamic

programming (the cky algorithm)

7 / 1

parsing with pid18

(cid:73) parsing with a pid18 is    nding the most probable

derivation for a given sentence.

(cid:73) this can be done quite ef   ciently with dynamic

programming (the cky algorithm)

obtaining the probabilities

(cid:73) we estimate them from the treebank.
(cid:73) q(lhs     rhs) =
(cid:73) we can also add id188, as before.
(cid:73) dealing with unknown words - like in the id48

count(lhs   rhs)
count(lhs      )

7 / 1

the big question

does this work?

8 / 1

evaluation

9 / 1

parsing evaluation

(cid:73) let   s assume we have a parser, how do we know how

good it is?

    compare output trees to gold trees.

10 / 1

parsing evaluation

(cid:73) let   s assume we have a parser, how do we know how

good it is?

    compare output trees to gold trees.

(cid:73) but how do we compare trees?
(cid:73) credit of 1 if tree is correct and 0 otherwise, is too harsh.

10 / 1

parsing evaluation

(cid:73) let   s assume we have a parser, how do we know how

good it is?

    compare output trees to gold trees.

(cid:73) but how do we compare trees?
(cid:73) credit of 1 if tree is correct and 0 otherwise, is too harsh.

(cid:73) represent each tree as a set of labeled spans.

(cid:73) np from word 1 to word 5.
(cid:73) vp from word 3 to word 4.
(cid:73) s from word 1 to word 23.
(cid:73) . . .

(cid:73) measure precision, recall and f1 over these spans, as in

the segmentation case.

10 / 1

evaluation:representingtreesasconstituentssnpdtthennlawyervpvtquestionednpdtthennwitnesslabelstartpointendpointnp12np45vp35s15precisionandrecalllabelstartpointendpointnp12np45np48pp68np78vp38s18labelstartpointendpointnp12np45pp68np78vp38s18ig=numberofconstituentsingoldstandard=7ip=numberinparseoutput=6ic=numbercorrect=6recall=100%  cg=100%  67precision=100%  cp=100%  66parsing evaluation

(cid:73) is this a good measure?

(cid:73) why? why not?

11 / 1

parsing evaluation

how well does the pid18 parser we learned do?

not very well: about 73% f1 score.

12 / 1

problems with pid18s

13 / 1

weaknessesofprobabilisticcontext-freegrammarsmichaelcollins,columbiauniversityweaknessesofpid18silackofsensitivitytolexicalinformationilackofsensitivitytostructuralfrequenciessnpnnpibmvpvtboughtnpnnplotusp(t)=q(s   npvp)  q(nnp   ibm)  q(vp   vnp)  q(vt   bought)  q(np   nnp)  q(nnp   lotus)  q(np   nnp)anothercaseofppattachmentambiguity(a)snpnnsworkersvpvpvbddumpednpnnssacksppinintonpdtannbin(b)snpnnsworkersvpvbddumpednpnpnnssacksppinintonpdtannbin(a)ruless   npvpnp   nnsvp   vpppvp   vbdnpnp   nnspp   innpnp   dtnnnns   workersvbd   dumpednns   sacksin   intodt   ann   bin(b)ruless   npvpnp   nnsnp   npppvp   vbdnpnp   nnspp   innpnp   dtnnnns   workersvbd   dumpednns   sacksin   intodt   ann   binifq(np   nppp)>q(vp   vppp)then(b)ismoreprobable,else(a)ismoreprobable.attachmentdecisioniscompletelyindependentofthewordsacaseofcoordinationambiguity(a)npnpnpnnsdogsppininnpnnshousesccandnpnnscats(b)npnpnnsdogsppininnpnpnnshousesccandnpnnscats(a)rulesnp   npccnpnp   npppnp   nnspp   innpnp   nnsnp   nnsnns   dogsin   innns   housescc   andnns   cats(b)rulesnp   npccnpnp   npppnp   nnspp   innpnp   nnsnp   nnsnns   dogsin   innns   housescc   andnns   catsherethetwoparseshaveidenticalrules,andthereforehaveidenticalid203underanyassignmentofpid18ruleprobabilitiesstructuralpreferences:closeattachment(a)npnpnnppinnpnpnnppinnpnn(b)npnpnpnnppinnpnnppinnpnniexample:presidentofacompanyinafricaibothparseshavethesamerules,thereforereceivesameid203underapid18i   closeattachment   (structure(a))istwiceaslikelyinwallstreetjournaltext.lexicalized pid18s

pid18 problem 1
lack of sensitivity to lexical information (words)

solution

(cid:73) make pid18 aware of words (lexicalized pid18)
(cid:73) main idea: head words

14 / 1

head words

each constituent has one words which captures its    essence   .

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)
(cid:73) (np the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)
(cid:73) (np the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)
(cid:73) (np the large hat)
(cid:73) (pp with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)
(cid:73) (np the large hat)
(cid:73) (pp with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)
(cid:73) (np the large hat)
(cid:73) (pp with the large hat)

15 / 1

head words

each constituent has one words which captures its    essence   .

(cid:73) (s john saw the young boy with the large hat)
(cid:73) (vp saw the young boy with the large hat)
(cid:73) (np the young boy with the large hat)
(cid:73) (np the large hat)
(cid:73) (pp with the large hat)

(cid:73) hat is the    semantic head   
(cid:73) with is the    functional head   
(cid:73) (it is common to choose the functional head)

15 / 1

headsincontext-freerulesaddannotationsspecifyingthe   head   ofeachrule:s   npvpvp   vivp   vtnpvp   vpppnp   dtnnnp   nppppp   innpvi   sleepsvt   sawnn   mannn   womannn   telescopedt   thein   within   inmoreaboutheadsieachcontext-freerulehasone   special   childthatistheheadoftherule.e.g.,s   npvp(vpisthehead)vp   vtnp(vtisthehead)np   dtnnnn(nnisthehead)iacoreideainsyntax(e.g.,seex-bartheory,head-drivenphrasestructuregrammar)isomeintuitions:ithecentralsub-constituentofeachrule.ithesemanticpredicateineachrule.ruleswhichrecoverheads:anexamplefornpsiftherulecontainsnn,nns,oid56p:choosetherightmostnn,nns,oid56pelseiftherulecontainsannp:choosetheleftmostnpelseiftherulecontainsajj:choosetherightmostjjelseiftherulecontainsacd:choosetherightmostcdelsechoosetherightmostchilde.g.,np   dtnnpnnnp   dtnnnnpnp   npppnp   dtjjnp   dtruleswhichrecoverheads:anexampleforvpsiftherulecontainsviorvt:choosetheleftmostviorvtelseiftherulecontainsanvp:choosetheleftmostvpelsechoosetheleftmostchilde.g.,vp   vtnpvp   vpppaddingheadwordstotreessnpdtthennlawyervpvtquestionednpdtthennwitness   s(questioned)np(lawyer)dt(the)thenn(lawyer)lawyervp(questioned)vt(questioned)questionednp(witness)dt(the)thenn(witness)witnessaddingheadwordstotrees(continued)s(questioned)np(lawyer)dt(the)thenn(lawyer)lawyervp(questioned)vt(questioned)questionednp(witness)dt(the)thenn(witness)witnessiaconstituentreceivesitsheadwordfromitsheadchild.s   npvp(sreceivesheadwordfromvp)vp   vtnp(vpreceivesheadwordfromvt)np   dtnn(npreceivesheadwordfromnn)dependency representation

16 / 1

dependency representation

if we take the head-annotated trees and    forget    about the
constituents, we get a representation called    dependency
structure   .

dependency structure capture the relation between words in a
sentence.

17 / 1

dependency representation

s(questioned)

np(lawyer)

vp(questioned)

dt(the)

nn(lawyer)

the

lawyer

vt(questioned)

np(witness)

questioned

dt(the)

nn(witness)

the

witness

18 / 1

dependency representation

questioned

lawyer

questioned

the

lawyer

the

lawyer

questioned

witness

questioned

the

witness

the

witness

18 / 1

dependency representation

questioned

lawyer

questioned

the

lawyer

the

lawyer

questioned

witness

questioned

the

witness

the

witness

18 / 1

dependency representation

questioned

lawyer

witness

the

the

18 / 1

dependency representation

questioned

lawyer

witness

the

the

18 / 1

dependency representation

18 / 1

dependency representation

18 / 1

dependency representation

dependency representation is very common.
we will return to it in the future.

18 / 1

back to lexicalized parsing

19 / 1

s(questioned)

np(lawyer)

vp(questioned)

dt(the)

nn(lawyer)

the

lawyer

vt(questioned)

np(witness)

questioned

dt(the)

nn(witness)

the

witness

20 / 1

chomskynormalformacontextfreegrammarg=(n,  ,r,s)inchomskynormalformisasfollowsinisasetofnon-terminalsymbolsi  isasetofterminalsymbolsirisasetofruleswhichtakeoneoftwoforms:ix   y1y2forx   n,andy1,y2   nix   yforx   n,andy     is   nisadistinguishedstartsymbolwecan   ndthehighestscoringparseunderapid18inthisform,ino(n3|n|3)timewherenisthelengthofthestringbeingparsed.lexicalizedcontext-freegrammarsinchomskynormalforminisasetofnon-terminalsymbolsi  isasetofterminalsymbolsirisasetofruleswhichtakeoneofthreeforms:ix(h)   1y1(h)y2(w)forx   n,andy1,y2   n,andh,w     ix(h)   2y1(w)y2(h)forx   n,andy1,y2   n,andh,w     ix(h)   hforx   n,andh     is   nisadistinguishedstartsymbolanexamples(saw)   2np(man)vp(saw)vp(saw)   1vt(saw)np(dog)np(man)   2dt(the)nn(man)np(dog)   2dt(the)nn(dog)vt(saw)   sawdt(the)   thenn(man)   mannn(dog)   dogparametersinalexicalizedpid18ianexampleparameterinapid18:q(s   npvp)ianexampleparameterinalexicalizedpid18:q(s(saw)   2np(man)vp(saw))parsingwithlexicalizedid18sithenewformofgrammarlooksjustlikeachomskynormalformid18,butwithpotentiallyo(|  |2  |n|3)possiblerules.inaively,parsingannwordsentenceusingthedynamicprogrammingalgorithmwilltakeo(n3|  |2|n|3)time.but|  |canbehuge!!icrucialobservation:atmosto(n2  |n|3)rulescanbeapplicabletoagivensentencew1,w2,...wnoflengthn.thisisbecauseanyruleswhichcontainalexicalitemthatisnotoneofw1...wn,canbesafelydiscarded.itheresult:wecanparseino(n5|n|3)time.s(saw)np(man)dt(the)thenn(man)manvp(saw)vp(saw)vt(saw)sawnp(dog)dt(the)thenn(dog)dogpp(with)in(with)withnp(telescope)dt(the)thenn(telescope)telescopep(t)=q(s(saw)   2np(man)vp(saw))  q(np(man)   2dt(the)nn(man))  q(vp(saw)   1vp(saw)pp(with))  q(vp(saw)   1vt(saw)np(dog))  q(pp(with)   1in(with)np(telescope))  ...amodelfromcharniak(1997)ianexampleparameterinalexicalizedpid18:q(s(saw)   2np(man)vp(saw))ifirststep:decomposethisparameterintoaproductoftwoparametersq(s(saw)   2np(man)vp(saw))=q(s   2npvp|s,saw)  q(man|s   2npvp,saw)amodelfromcharniak(1997)(continued)q(s(saw)   2np(man)vp(saw))=q(s   2npvp|s,saw)  q(man|s   2npvp,saw)isecondstep:usesmoothedestimationforthetwoparameterestimatesq(s   2npvp|s,saw)=  1  qml(s   2npvp|s,saw)+  2  qml(s   2npvp|s)q(man|s   2npvp,saw)=  3  qml(man|s   2npvp,saw)+  4  qml(man|s   2npvp)+  5  qml(man|np)amodelfromcharniak(1997)(continued)q(s(saw)   2np(man)vp(saw))=q(s   2npvp|s,saw)  q(man|s   2npvp,saw)isecondstep:usesmoothedestimationforthetwoparameterestimatesq(s   2npvp|s,saw)=  1  qml(s   2npvp|s,saw)+  2  qml(s   2npvp|s)q(man|s   2npvp,saw)=  3  qml(man|s   2npvp,saw)+  4  qml(man|s   2npvp)+  5  qml(man|np)accurate unlexicalized parsing

21 / 1

accurate unlexicalized parsing

pid18 problem 2
lack of sensitivity to structural information

22 / 1

accurate unlexicalized parsing

pid18 problem 2
lack of sensitivity to structural information

solution

(cid:73) this problem is also solved by lexicalization.

(cid:73) (maybe that   s the main problem that   s being solved by

lexicalization)

22 / 1

accurate unlexicalized parsing

pid18 problem 2
lack of sensitivity to structural information

solution

(cid:73) this problem is also solved by lexicalization.

(cid:73) (maybe that   s the main problem that   s being solved by

lexicalization)

(cid:73) but can we do without lexicalizing the grammar?

22 / 1

  5. accurate unlexicalized parsing: pid18s and independence   the symbols in a pid18 define independence assumptions:   at any node, the material inside that node is independent of the material outside that node, given the label of that node.   any information that statistically connects behavior inside and outside a node must flow through that node.npsvps     np vpnp     dt nnnp  michael collins (2003, colt)  non-independence i   independence assumptions are often too strong.   example: the expansion of an np is highly dependent on the parent of the np (i.e., subjects vs. objects).11%9%6%np ppdt nnprp9%9%21%np ppdt nnprp7%4%23%np ppdt nnprpall npsnps under snps under vp  non-independence ii   who cares?   nb, id48s, all make false assumptions!   for generation, consequences would be obvious.   for parsing, does it impact accuracy?   symptoms of overly strong assumptions:   rewrites get used where they don   t belong.   rewrites get used too often or too rarely.in the ptb, this construction is for possesives  breaking up the symbols   we can relax independence assumptions by encoding dependencies into the pid18 symbols:   what are the most useful features to encode?parent annotation[johnson 98]marking possesive nps  annotations   annotations split the grammar categories into sub-categories.   conditioning on history vs. annotating   p(np^s     prp) is a lot like p(np     prp | s)   p(np-pos     nnp pos) isn   t history conditioning.   feature grammars vs. annotation   can think of a symbol like np^np-pos as np [parent:np, +pos]   after parsing with an annotated grammar, the annotations are then stripped for evaluation.  experimental process   we   ll take a highly conservative approach:   annotate as sparingly as possible   highest accuracy with fewest symbols   error-driven, manual hill-climb, adding one annotation type at a time  unlexicalized pid18s   what do we mean by an    unlexicalized    pid18?   grammar rules are not systematically specified down to the level of lexical items   np-stocks is not allowed   np^s-cc is fine   closed vs. open class words (np^s-the)   long tradition in linguistics of using function words as features or markers for selection   contrary to the bilexical idea of semantic heads   open-class selection really a proxy for semantics   honesty checks:   number of symbols: keep the grammar very small   no smoothing: over-annotating is a real danger  tag splits   problem: treebank tags are too coarse.   example: sentential, pp, and other prepositions are all marked in.   partial solution:   subdivide the in tag.annotationf1sizeprevious78.38.0ksplit-in80.38.1k  other tag splits   unary-dt: mark demonstratives as dt^u (   the x    vs.    those   )   unary-rb: mark phrasal adverbs as rb^u (   quickly    vs.    very   )   tag-pa: mark tags with non-canonical parents (   not    is an rb^vp)   split-aux: mark auxiliary verbs with    aux [cf. charniak 97]   split-cc: separate    but    and    &    from other conjunctions   split-%:    %    gets its own tag.f1size80.48.1k80.58.1k81.28.5k81.69.0k81.79.1k81.89.3k  treebank splits   the treebank comes with annotations (e.g., -loc, -subj, etc).   whole set together hurt the baseline.   some (-subj) were less effective than our equivalents.   one in particular was very useful (np-tmp) when pushed down to the head tag.   we marked gapped s nodes as well.annotationf1sizeprevious81.89.3knp-tmp82.29.6kgapped-s82.39.7k  yield splits   problem: sometimes the behavior of a category depends on something inside its future yield.   examples:   possessive nps   finite vs. infinite vps   lexical heads!   solution: annotate future elements into nodes.annotationf1sizeprevious82.39.7kposs-np83.19.8ksplit-vp85.710.5k  distance / recursion splits   problem: vanilla pid18s cannot distinguish attachment heights.   solution: mark a property of higher or lower sites:   contains a verb.   is (non)-recursive.   base nps [cf. collins 99]   right-recursive npsannotationf1sizeprevious85.710.5kbase-np86.011.7kdominates-v86.914.1kright-rec-np87.015.2knpvpppnpv-v  a fully annotated tree  final test set results   beats    first generation    lexicalized parsers.parserlplrf1cb0 cbmagerman 9584.984.684.71.2656.6collins 9686.385.886.01.1459.9klein & m 0386.985.786.31.1060.3charniak 9787.487.587.41.0062.1collins 9988.788.688.60.9067.1automatic state splits

23 / 1

more tree re-structuring

(based on work by jessica ficler at biu)

24 / 1

argument cluster coordination

i bought john a mic on monday and richie a guitar on saturday

25 / 1

argument cluster coordination

26 / 1

abstractimproved parsing for argument-clusters coordinationjessica ficlerand yoavgoldbergi bought johna microphone on monday and richiea guitar on saturdaylong story short   argument clusters coordination (acc) are vp coordination with non-constituent conjuncts that are parallel in structure.   the id32annotation expresses the linguistic nuances that may appear in such coordinations.   but ptb representation are not easily learnable by current day parsers.   we suggest an alternative, simpler representation scheme which:(1) is better suited for training a parser.(2) is capable of representing most of the argument cluster coordination cases in the id32.   we obtain x2.7improvement in recovering  acc structures compared to a parser trained on the ptb trees on corpus of 4thgrade science  exams.argument clusters coordination in the ptb   the co-indexation breaks the context-free assumption of pid18 parsers   pid18 parsers ignore the indexes -all the information about the acc construction is lost   ignoring the indexes result in    weird    id18 rules such as vp -> np ppalternative representation for aid35oals:   respect the context-free nature of the parser   avoid incorrect syntactic derivations   express the symmetry that occur in many acc phraseswe change the ptb representation for acc as follows:   the verb and non-indexed elements are moved to under the main vp   new non terminal symbols for argument clusters   the conjunction of clusters receives a dedicated phrase level.results -ptbdedicated labels for argument clusters: reflects the syntactic labels of the arguments:                               41.6%                                          21.2%                                  5.3%coordinationphrasetypereflects the main element in clusters.                             55.2%                               28.8%the main verb is between the indexed argumentsthere is an indexed phrase that is not a direct child of the clusterwe converted 125 trees in the training sets of the ptb to the new representation and trained the berkeley parser.f1prdataset90.8891.0990.8991.2190.8890.97ptb  modifieddev90.5790.8490.7991.0690.3690.62ptb  modifiedtestgeneral parsing results (evalb on ptb)abstractresults    regents data-set regents data-set-includes 281 sentences with coordination 54 of them include accmary paid ([$11.08][for berries])and ([$9.31][for peaches])acc clustersand argumentsspans:wendy (ran 19 miles)and (walked 9 miles)constituent conjuncts spans:results on recovering conjuncts/clusters spans:rtask4754.3ptb  modifiedconjuncts/clusters24.164.8ptb  modifiedclustersresults on recovering clusters and arguments spansr61.580ptb  modifieddedicated labelslimitationsargument cluster coordination

i bought john a mic on monday and richie a guitar on saturday

27 / 1

argument cluster coordination

i bought john a mic on monday and

a guitar on saturday

28 / 1

argument cluster coordination

i

bought john

[ [a microphone] [on monday] ]

and

[ [a guitar] [on saturday] ]

29 / 1

argument cluster coordination

pay healthvest

[ [5 million usd] [right away] ]

and

[ [additional amounts] [in the future] ]

30 / 1

abstractimproved parsing for argument-clusters coordinationjessica ficlerand yoavgoldbergi bought johna microphone on monday and richiea guitar on saturdaylong story short   argument clusters coordination (acc) are vp coordination with non-constituent conjuncts that are parallel in structure.   the id32annotation expresses the linguistic nuances that may appear in such coordinations.   but ptb representation are not easily learnable by current day parsers.   we suggest an alternative, simpler representation scheme which:(1) is better suited for training a parser.(2) is capable of representing most of the argument cluster coordination cases in the id32.   we obtain x2.7improvement in recovering  acc structures compared to a parser trained on the ptb trees on corpus of 4thgrade science  exams.argument clusters coordination in the ptb   the co-indexation breaks the context-free assumption of pid18 parsers   pid18 parsers ignore the indexes -all the information about the acc construction is lost   ignoring the indexes result in    weird    id18 rules such as vp -> np ppalternative representation for aid35oals:   respect the context-free nature of the parser   avoid incorrect syntactic derivations   express the symmetry that occur in many acc phraseswe change the ptb representation for acc as follows:   the verb and non-indexed elements are moved to under the main vp   new non terminal symbols for argument clusters   the conjunction of clusters receives a dedicated phrase level.results -ptbdedicated labels for argument clusters: reflects the syntactic labels of the arguments:                               41.6%                                          21.2%                                  5.3%coordinationphrasetypereflects the main element in clusters.                             55.2%                               28.8%the main verb is between the indexed argumentsthere is an indexed phrase that is not a direct child of the clusterwe converted 125 trees in the training sets of the ptb to the new representation and trained the berkeley parser.f1prdataset90.8891.0990.8991.2190.8890.97ptb  modifieddev90.5790.8490.7991.0690.3690.62ptb  modifiedtestgeneral parsing results (evalb on ptb)abstractresults    regents data-set regents data-set-includes 281 sentences with coordination 54 of them include accmary paid ([$11.08][for berries])and ([$9.31][for peaches])acc clustersand argumentsspans:wendy (ran 19 miles)and (walked 9 miles)constituent conjuncts spans:results on recovering conjuncts/clusters spans:rtask4754.3ptb  modifiedconjuncts/clusters24.164.8ptb  modifiedclustersresults on recovering clusters and arguments spansr61.580ptb  modifieddedicated labelslimitationsabstractimproved parsing for argument-clusters coordinationjessica ficlerand yoavgoldbergi bought johna microphone on monday and richiea guitar on saturdaylong story short   argument clusters coordination (acc) are vp coordination with non-constituent conjuncts that are parallel in structure.   the id32annotation expresses the linguistic nuances that may appear in such coordinations.   but ptb representation are not easily learnable by current day parsers.   we suggest an alternative, simpler representation scheme which:(1) is better suited for training a parser.(2) is capable of representing most of the argument cluster coordination cases in the id32.   we obtain x2.7improvement in recovering  acc structures compared to a parser trained on the ptb trees on corpus of 4thgrade science  exams.argument clusters coordination in the ptb   the co-indexation breaks the context-free assumption of pid18 parsers   pid18 parsers ignore the indexes -all the information about the acc construction is lost   ignoring the indexes result in    weird    id18 rules such as vp -> np ppalternative representation for aid35oals:   respect the context-free nature of the parser   avoid incorrect syntactic derivations   express the symmetry that occur in many acc phraseswe change the ptb representation for acc as follows:   the verb and non-indexed elements are moved to under the main vp   new non terminal symbols for argument clusters   the conjunction of clusters receives a dedicated phrase level.results -ptbdedicated labels for argument clusters: reflects the syntactic labels of the arguments:                               41.6%                                          21.2%                                  5.3%coordinationphrasetypereflects the main element in clusters.                             55.2%                               28.8%the main verb is between the indexed argumentsthere is an indexed phrase that is not a direct child of the clusterwe converted 125 trees in the training sets of the ptb to the new representation and trained the berkeley parser.f1prdataset90.8891.0990.8991.2190.8890.97ptb  modifieddev90.5790.8490.7991.0690.3690.62ptb  modifiedtestgeneral parsing results (evalb on ptb)abstractresults    regents data-set regents data-set-includes 281 sentences with coordination 54 of them include accmary paid ([$11.08][for berries])and ([$9.31][for peaches])acc clustersand argumentsspans:wendy (ran 19 miles)and (walked 9 miles)constituent conjuncts spans:results on recovering conjuncts/clusters spans:rtask4754.3ptb  modifiedconjuncts/clusters24.164.8ptb  modifiedclustersresults on recovering clusters and arguments spansr61.580ptb  modifieddedicated labelslimitationsabstractimproved parsing for argument-clusters coordinationjessica ficlerand yoavgoldbergi bought johna microphone on monday and richiea guitar on saturdaylong story short   argument clusters coordination (acc) are vp coordination with non-constituent conjuncts that are parallel in structure.   the id32annotation expresses the linguistic nuances that may appear in such coordinations.   but ptb representation are not easily learnable by current day parsers.   we suggest an alternative, simpler representation scheme which:(1) is better suited for training a parser.(2) is capable of representing most of the argument cluster coordination cases in the id32.   we obtain x2.7improvement in recovering  acc structures compared to a parser trained on the ptb trees on corpus of 4thgrade science  exams.argument clusters coordination in the ptb   the co-indexation breaks the context-free assumption of pid18 parsers   pid18 parsers ignore the indexes -all the information about the acc construction is lost   ignoring the indexes result in    weird    id18 rules such as vp -> np ppalternative representation for aid35oals:   respect the context-free nature of the parser   avoid incorrect syntactic derivations   express the symmetry that occur in many acc phraseswe change the ptb representation for acc as follows:   the verb and non-indexed elements are moved to under the main vp   new non terminal symbols for argument clusters   the conjunction of clusters receives a dedicated phrase level.results -ptbdedicated labels for argument clusters: reflects the syntactic labels of the arguments:                               41.6%                                          21.2%                                  5.3%coordinationphrasetypereflects the main element in clusters.                             55.2%                               28.8%the main verb is between the indexed argumentsthere is an indexed phrase that is not a direct child of the clusterwe converted 125 trees in the training sets of the ptb to the new representation and trained the berkeley parser.f1prdataset90.8891.0990.8991.2190.8890.97ptb  modifieddev90.5790.8490.7991.0690.3690.62ptb  modifiedtestgeneral parsing results (evalb on ptb)abstractresults    regents data-set regents data-set-includes 281 sentences with coordination 54 of them include accmary paid ([$11.08][for berries])and ([$9.31][for peaches])acc clustersand argumentsspans:wendy (ran 19 miles)and (walked 9 miles)constituent conjuncts spans:results on recovering conjuncts/clusters spans:rtask4754.3ptb  modifiedconjuncts/clusters24.164.8ptb  modifiedclustersresults on recovering clusters and arguments spansr61.580ptb  modifieddedicated labelslimitationsabstractimproved parsing for argument-clusters coordinationjessica ficlerand yoavgoldbergi bought johna microphone on monday and richiea guitar on saturdaylong story short   argument clusters coordination (acc) are vp coordination with non-constituent conjuncts that are parallel in structure.   the id32annotation expresses the linguistic nuances that may appear in such coordinations.   but ptb representation are not easily learnable by current day parsers.   we suggest an alternative, simpler representation scheme which:(1) is better suited for training a parser.(2) is capable of representing most of the argument cluster coordination cases in the id32.   we obtain x2.7improvement in recovering  acc structures compared to a parser trained on the ptb trees on corpus of 4thgrade science  exams.argument clusters coordination in the ptb   the co-indexation breaks the context-free assumption of pid18 parsers   pid18 parsers ignore the indexes -all the information about the acc construction is lost   ignoring the indexes result in    weird    id18 rules such as vp -> np ppalternative representation for aid35oals:   respect the context-free nature of the parser   avoid incorrect syntactic derivations   express the symmetry that occur in many acc phraseswe change the ptb representation for acc as follows:   the verb and non-indexed elements are moved to under the main vp   new non terminal symbols for argument clusters   the conjunction of clusters receives a dedicated phrase level.results -ptbdedicated labels for argument clusters: reflects the syntactic labels of the arguments:                               41.6%                                          21.2%                                  5.3%coordinationphrasetypereflects the main element in clusters.                             55.2%                               28.8%the main verb is between the indexed argumentsthere is an indexed phrase that is not a direct child of the clusterwe converted 125 trees in the training sets of the ptb to the new representation and trained the berkeley parser.f1prdataset90.8891.0990.8991.2190.8890.97ptb  modifieddev90.5790.8490.7991.0690.3690.62ptb  modifiedtestgeneral parsing results (evalb on ptb)abstractresults    regents data-set regents data-set-includes 281 sentences with coordination 54 of them include accmary paid ([$11.08][for berries])and ([$9.31][for peaches])acc clustersand argumentsspans:wendy (ran 19 miles)and (walked 9 miles)constituent conjuncts spans:results on recovering conjuncts/clusters spans:rtask4754.3ptb  modifiedconjuncts/clusters24.164.8ptb  modifiedclustersresults on recovering clusters and arguments spansr61.580ptb  modifieddedicated labelslimitationsabstractimproved parsing for argument-clusters coordinationjessica ficlerand yoavgoldbergi bought johna microphone on monday and richiea guitar on saturdaylong story short   argument clusters coordination (acc) are vp coordination with non-constituent conjuncts that are parallel in structure.   the id32annotation expresses the linguistic nuances that may appear in such coordinations.   but ptb representation are not easily learnable by current day parsers.   we suggest an alternative, simpler representation scheme which:(1) is better suited for training a parser.(2) is capable of representing most of the argument cluster coordination cases in the id32.   we obtain x2.7improvement in recovering  acc structures compared to a parser trained on the ptb trees on corpus of 4thgrade science  exams.argument clusters coordination in the ptb   the co-indexation breaks the context-free assumption of pid18 parsers   pid18 parsers ignore the indexes -all the information about the acc construction is lost   ignoring the indexes result in    weird    id18 rules such as vp -> np ppalternative representation for aid35oals:   respect the context-free nature of the parser   avoid incorrect syntactic derivations   express the symmetry that occur in many acc phraseswe change the ptb representation for acc as follows:   the verb and non-indexed elements are moved to under the main vp   new non terminal symbols for argument clusters   the conjunction of clusters receives a dedicated phrase level.results -ptbdedicated labels for argument clusters: reflects the syntactic labels of the arguments:                               41.6%                                          21.2%                                  5.3%coordinationphrasetypereflects the main element in clusters.                             55.2%                               28.8%the main verb is between the indexed argumentsthere is an indexed phrase that is not a direct child of the clusterwe converted 125 trees in the training sets of the ptb to the new representation and trained the berkeley parser.f1prdataset90.8891.0990.8991.2190.8890.97ptb  modifieddev90.5790.8490.7991.0690.3690.62ptb  modifiedtestgeneral parsing results (evalb on ptb)abstractresults    regents data-set regents data-set-includes 281 sentences with coordination 54 of them include accmary paid ([$11.08][for berries])and ([$9.31][for peaches])acc clustersand argumentsspans:wendy (ran 19 miles)and (walked 9 miles)constituent conjuncts spans:results on recovering conjuncts/clusters spans:rtask4754.3ptb  modifiedconjuncts/clusters24.164.8ptb  modifiedclustersresults on recovering clusters and arguments spansr61.580ptb  modifieddedicated labelslimitationsreranking

36 / 1

discriminative reranking

so far

(cid:73) scoring of trees is decomposable to grammar rules.
(cid:73) this is needed for cky to work.
(cid:73) what are we missing?

37 / 1

discriminative reranking

so far

(cid:73) scoring of trees is decomposable to grammar rules.
(cid:73) this is needed for cky to work.
(cid:73) what are we missing?

moving forward
how can we incorporate arbitrary features?

37 / 1

discriminative reranking

(cid:73) if we could enumerate over all the possible trees for a

sentence, we could:

(cid:73) extract (arbitrary) features from each tree.
(cid:73) use features + weight vector to score the tree.
(cid:73) return the highest scoring tree.

38 / 1

discriminative reranking

(cid:73) if we could enumerate over all the possible trees for a

sentence, we could:

(cid:73) extract (arbitrary) features from each tree.
(cid:73) use features + weight vector to score the tree.
(cid:73) return the highest scoring tree.

(cid:73) problem: we can   t enumerate all trees. this is why we use

cky.

38 / 1

discriminative reranking

(cid:73) if we could enumerate over all the possible trees for a

sentence, we could:

(cid:73) extract (arbitrary) features from each tree.
(cid:73) use features + weight vector to score the tree.
(cid:73) return the highest scoring tree.

(cid:73) problem: we can   t enumerate all trees. this is why we use

cky.

(cid:73) solution

(cid:73) modify the cky algorithm to return the k-best trees.
(cid:73) then apply the algorithm above to the k-best list.

(cid:73) assumption: the    base    models is good enough such that

the trees in the k-best list are good.

38 / 1

discriminative reranking

(cid:73) the reranking approach allows us to incorporate arbitrary

features.

(cid:73) but we still need to de   ne good features.

39 / 1

discriminative reranking

feature examples (slides by mark johnson)

40 / 1

expt1:only\old"features   features:1logcharniakid203,10,124rulefeatures   charniak   sparseralreadyconditionsonlocaltrees!   featureselection:featuresmustvaryon5ormoresentences   results:f-score=0.894;baseline=0.890;   4%errorreduction)discriminativetrainingalonecanimproveaccuracyrootsnpwdtthatvpvbdwentppinovernpnpdtthejjpermissiblennlineppinfornpadjpjjwarmccandjjfuzzynnsfeelings..28lexicalizedandparent-annotatedrules   lexicalizationassociateseachconstituentwithitshead   ancestorannotationprovidesalittle   verticalcontext      contextannotationindicatesconstructionsthatonlyoccurinmainclause(c.f.,emonds)rootsnpwdtthatvpvbdwentppinovernpnpdtthejjpermissiblennlineppinfornpadjpjjwarmccandjjfuzzynnsfeelings..headsancestorcontextrule29functionalandlexicalheads   thereareatleasttwosensiblenotionsofhead(c.f.,grimshaw){functionalheads:determinersofnps,auxilaryverbsofvps,etc.{lexicalheads:rightmostnsofnps,mainverbsinvps,etc.   inamaxentmodel,itiseasytouseboth!sdtannrecordnndatevpvbzhasrbn   tvpvbnbeenvpvbnset..npfunctionalfunctionallexical30functional-lexicalheaddependencies   thesynsemheadsfeaturescollectpairsoffunctionalandlexicalheadsofphrases   thiscapturesnumberagreementinnpsandaspectsofotherhead-to-headdependenciesrootsnpdtthennsrulesvpvbpforcesnpnnsexecutivesvptotovpvbreportnpnnspurchases..31id165rulefeaturesgeneralizerules   collectsadjacentconstituentsinalocaltree   alsoincludesrelationshiptohead   constituentscanbeancestor-annotatedandlexicalizedrootsnpdtthennclashvpauxisnpnpdtannsignppinofnpnpdtajjnewnntoughnessccandnndivisivenessppininnpnpnnpjapanpos   sjjonce-cozyjj   nancialnnscircles..leftofhead,non-adjacenttohead32headtoheaddependencies   head-to-headdependenciestrackthefunction-argumentdependenciesinatree   co-ordinationleadstophraseswithmultipleheadsorfunctors   parameterizedbyheadtypeandnumberofgovernorstoincluderootsnpwdtthatvpvbdwentppinovernpnpdtthejjpermissiblennlineppinfornpadjpjjwarmccandjjfuzzynnsfeelings..33subject-verbagreement   thesubjverbagrfeaturesaretheposofthesubjectnp   slexicalheadandthevp   sfunctionalheadrootsnpdtthennsrulesvpvbpforcesnpnnsexecutivesvptotovpvbreportnpnnspurchases..34headtreesrecordalldependencies   headtreesconsistofa(lexical)head,allofitsprojectionsand(optionally)allofthesiblingsofthesenodes   thesecorrespondroughlytotagelementarytreesrootsnpprptheyvpvbdwerevpvbnconsultedppininnpnnadvance..35treeid165   atreeid165aretreefragmentsthatconnectsequencesofadjacentnwordsrootsnpwdtthatvpvbdwentppinovernpnpdtthejjpermissiblennlineppinfornpadjpjjwarmccandjjfuzzynnsfeelings..36rightmostbranchbias   therightbranchfeature   svalueisthenumberofnodesontheright-mostbranch(ignoringpunctuation)   re   ectsthetendancytowardrightbranchinginenglish   only2di   erentfeatures,butveryusefulin   nalmodel!rootwdtthatwentoverdtthejjpermissiblennlineinforjjwarmccandjjfuzzynnsfeelings..ppvpsnpppnpnpvbdinnpadjp37constituentheavynessandlocation   heavynessmeasurestheconstituent   scategory,its(binned)sizeand(binned)closenesstotheendofthesentencerootsnpwdtthatvpvbdwentppinovernpnpdtthejjpermissiblennlineppinfornpadjpjjwarmccandjjfuzzynnsfeelings..>5words=1punctuation38coordinationparallelism(1)   thecoparfeatureindicatesthedepthtowhichadjacentconjunctsareparallelrootsnpprptheyvpvpvbdwerevpvbnconsultedppininnpnnadvanceccandvpvdbwerevpvbnsurprisedppinatnpnpdtthennactionvpvbntaken..isomorphictreestodepth439coordinationparallelism(2)   thecolenparfeatureindicatesthedi   erenceinlengthinadjacentconjunctsandwhetherthispaircontainsthelastconjunct.rootsnpprptheyvpvpvbdwerevpvbnconsultedppininnpnnadvanceccandvpvdbwerevpvbnsurprisedppinatnpnpdtthennactionvpvbntaken..4words6wordscolenparfeature:(2,true)40experimentalresultswithallfeatures   featuresmustvaryonparsesofatleast5sentencesintrainingdata   inthisexperiment,724,550features   gaussianid173,adjustedviacross-validationonsection23   f-scoreonsection23=0.912(15%errorreductionovercharniakparser)41summary

(cid:73) weaknesses of pid18s
(cid:73) lexicalization
(cid:73) head rules

(cid:73) and the connection to dependency representation

(cid:73) parsing with lexicalized grammars

(cid:73) tree annotations
(cid:73) reranking

41 / 1

