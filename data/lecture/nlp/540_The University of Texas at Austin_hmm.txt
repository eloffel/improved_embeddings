id52 and id48s

yoav goldberg

bar ilan university

1 / 49

the tagging problem

input
holly came from miami , f.l.a ,
hitch-hiked her way across the usa

output
holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp ,/,
hitch-hiked/vbd her/prp way/nn across/in the/dt usa/nnp

assign a tag from a given tagset to each word in a sentence.

2 / 49

our goal

training set
1 pierre/nnp vinken/nnp ,/, 61/cd years/nns old/jj ,/, will/md join/vb
the/dt board/nn as/in a/dt nonexecutive/jj director/nn nov./nnp 29/cd ./.
2 mr./nnp vinken/nnp is/vbz chairman/nn of/in elsevier/nnp n.v./nnp ,/,
the/dt dutch/nnp publishing/vbg group/nn ./.
3 rudolph/nnp agnew/nnp ,/, 55/cd years/nns old/jj and/cc former/jj
chairman/nn of/in consolidated/nnp gold/nnp fields/nnp plc/nnp ,/,
was/vbd named/vbn a/dt nonexecutive/jj director/nn of/in this/dt
british/jj industrial/jj conglomerate/nn ./.
. . .
. . .
38,219 that/dt could/md cost/vb him/prp the/dt chance/nn to/to
in   uence/vb the/dt outcome/nn and/cc perhaps/rb join/vb the/dt
winning/vbg bidder/nn ./.

(cid:73) from the training set, learn a function/algorithm that maps

new sentences to their tag sequences.

3 / 49

information sources

with/in such/pdt a/dt lopsided/jj book/nn of/in options/nns
,/, traders/nns say/vbp ,/, chemical/nnp was/vbd more/rbr
vulnerable/jj to/in erroneous/jj valuation/nn
assumptions/nns ./.

4 / 49

information sources

with/in such/pdt a/dt lopsided/jj book/nn of/in options/nns
,/, traders/nns say/vbp ,/, chemical/nnp was/vbd more/rbr
vulnerable/jj to/in erroneous/jj valuation/nn
assumptions/nns ./.

(cid:73) local:

(cid:73) the word    book    is likely to be a noun.
(cid:73) the word    lopsided    is likely to be an adjective.

4 / 49

information sources

with/in such/pdt a/dt lopsided/jj book/nn of/in options/nns
,/, traders/nns say/vbp ,/, chemical/nnp was/vbd more/rbr
vulnerable/jj to/in erroneous/jj valuation/nn
assumptions/nns ./.

(cid:73) local:

(cid:73) the word    book    is likely to be a noun.
(cid:73) the word    lopsided    is likely to be an adjective.

(cid:73) contextual:

(cid:73) noun are likely to follow adjectives or determiners.
(cid:73) verbs are not likely to follow determiners.

4 / 49

information sources

(cid:73)    i asked him to book a    ight   
(cid:73)    the trash can take care of itself   
(cid:73)    the trash can is in the garage.   
(cid:73)    fruit    ies like a banana.   

5 / 49

the supervised tagging problem

formally

(cid:73) we have training examples x(i), y(i) for i = 1, . . . , m.

(cid:73) each x(i) is an input x1, . . . , xn
(cid:73) each y(i) is an output y1, . . . , yn

(a crazy dog barked)
(dt jj nn vbd)

6 / 49

the supervised tagging problem

formally

(cid:73) we have training examples x(i), y(i) for i = 1, . . . , m.

(cid:73) each x(i) is an input x1, . . . , xn
(cid:73) each y(i) is an output y1, . . . , yn

(a crazy dog barked)
(dt jj nn vbd)

(cid:73) task: learn a function f mapping inputs x to labels f (x) = y

6 / 49

the supervised tagging problem

conditional model

(cid:73) learn a distribution p(y|x) from training examples.
(cid:73) de   ne f (x) = argmaxyp(y|x)

7 / 49

the supervised tagging problem

conditional model

(cid:73) learn a distribution p(y|x) from training examples.
(cid:73) de   ne f (x) = argmaxyp(y|x)
(cid:73) how do we compute p(y|x)?

7 / 49

how do we de   ne p(y|x)?

(cid:73) if we could compute p(x, y), then p(y|x) = p(x,y)
(cid:73) . . . and p(x) is constant.
(cid:73) . . . so arg maxy p(y|x) = arg maxy p(x, y)
    lets try to learn p(x, y) instead.

p(x)

8 / 49

how do we de   ne p(y|x)?

(cid:73) if we could compute p(x, y), then p(y|x) = p(x,y)
(cid:73) . . . and p(x) is constant.
(cid:73) . . . so arg maxy p(y|x) = arg maxy p(x, y)
    lets try to learn p(x, y) instead.

p(x)

p(x,y)?

(cid:73) why not work with p(y|x) directly?

8 / 49

how do we de   ne p(y|x)?

(cid:73) if we could compute p(x, y), then p(y|x) = p(x,y)
(cid:73) . . . and p(x) is constant.
(cid:73) . . . so arg maxy p(y|x) = arg maxy p(x, y)
    lets try to learn p(x, y) instead.

p(x)

p(x,y)?

(cid:73) why not work with p(y|x) directly?
(cid:73) we are working with probabilities.
(cid:73) we   ll see shortly that we can compute p(x, y) using basic
(cid:73) it is not so easy for p(y|x).

id203 rules.

8 / 49

how do we de   ne p(y|x)?

(cid:73) if we could compute p(x, y), then p(y|x) = p(x,y)
(cid:73) . . . and p(x) is constant.
(cid:73) . . . so arg maxy p(y|x) = arg maxy p(x, y)
    lets try to learn p(x, y) instead.

p(x)

p(x,y)?

(cid:73) why not work with p(y|x) directly?
(cid:73) we are working with probabilities.
(cid:73) we   ll see shortly that we can compute p(x, y) using basic
(cid:73) it is not so easy for p(y|x).

id203 rules.

(cid:73) what do we gain/loose from working with p(x, y)?

8 / 49

question 1: score computation
assume someone gave us a x, y pair.
how do we compute p(x, y)?

9 / 49

p( holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp ,/,
hitch-hiked/vbd her/prp way/nn across/in the/dt usa/nnp )
?

p( holly/nnp came/vbz from/in miami/jj ,/, f.l.a/nnp ,/,
hitch-hiked/in her/prp way/vbz across/in the/cd usa/nnp )
?

p( holly/nn came/nn from/nn miami/nn ,/nn f.l.a/nn ,/nn
hitch-hiked/nn her/nn way/nn across/nn the/nn usa/nn )
?

p( holly/nnp came/vbz from/in miami/nnp ,/, f.l.a/nnp ,/,
hitch-hiked/vbd her/prp way/jj across/in the/dt usa/nnp )
?

10 / 49

p(x,y)

generative model

(cid:73) working with the joint id203 p(x, y) suggests the use of

a generative model.

(cid:73) de   ne a generative story of how the data was created.
(cid:73) the story doesn   t have to be true. it has to be reasonable.
(cid:73) reasonable?? in terms of the independence assumptions.

11 / 49

p(x,y)

our generative story
how does a sentence come to life?

(cid:73) first, a sequence of tags is created.

12 / 49

p(x,y)

our generative story
how does a sentence come to life?

(cid:73) first, a sequence of tags is created.
(cid:73) then, each tag is replaced with a word.

12 / 49

p(x,y)

our generative story
how does a sentence come to life?

(cid:73) first, a sequence of tags is created.
(cid:73) then, each tag is replaced with a word.

(cid:73) all we see are the words. we need to guess the tags.
(cid:73) noisy channel interpretation: our pure message was y.

but something changed our message to x instead.

12 / 49

p(x,y)

our generative story
how does a sentence come to life?

(cid:73) first, a sequence of tags is created.
(cid:73) then, each tag is replaced with a word.

(cid:73) all we see are the words. we need to guess the tags.
(cid:73) noisy channel interpretation: our pure message was y.

but something changed our message to x instead.

(cid:73) rewrite p(x, y) = p(y)p(x|y)

12 / 49

p(x, y) = p(y)p(x|y)

(cid:73) no assumptions so far.
(cid:73) but breaking into p(y) and p(x|y) makes our life easier.

(cid:73) why?
(cid:73) (and why not break things into p(x) and p(y|x)?)

13 / 49

p(x, y) = p(y)p(x|y)

p(y)

(cid:73) first attempt     id113 (id113)

p(y) = p(y1, y2, . . . , yn) =

count(y1, y2, . . . , yn)

num of training examples

14 / 49

p(x, y) = p(y)p(x|y)

p(y)

(cid:73) first attempt     id113 (id113)

p(y) = p(y1, y2, . . . , yn) =

count(y1, y2, . . . , yn)

num of training examples

problem?

14 / 49

p(x, y) = p(y)p(x|y)

p(y)

(cid:73) second attempt     use chain rule

p(y) = p(y1, y2, . . . , yn) =p(y1)

   p(y2|y1)
   p(y3|y1, y2)
   p(y4|y1, y2, y3)
. . .
   p(yn|y1, y2, y3, . . . , yn   1)

15 / 49

p(x, y) = p(y)p(x|y)

p(y)

(cid:73) second attempt     use chain rule

p(y) = p(y1, y2, . . . , yn) =p(y1)

   p(y2|y1)
   p(y3|y1, y2)
   p(y4|y1, y2, y3)
. . .
   p(yn|y1, y2, y3, . . . , yn   1)

(cid:73) is this any better?

15 / 49

p(x, y) = p(y)p(x|y)

p(y)     markov assumption

(cid:73) does the tag of the    rst word really in   uences the tag of

the seventh word?

16 / 49

p(x, y) = p(y)p(x|y)

p(y)     markov assumption

(cid:73) does the tag of the    rst word really in   uences the tag of

the seventh word?

(cid:73) and the does it in   uence the tag of the 4th word?

16 / 49

p(x, y) = p(y)p(x|y)

p(y)     markov assumption

(cid:73) does the tag of the    rst word really in   uences the tag of

the seventh word?

(cid:73) and the does it in   uence the tag of the 4th word?
(cid:73) let assume only the previous tag matters:

16 / 49

p(x, y) = p(y)p(x|y)

p(y)     markov assumption

(cid:73) does the tag of the    rst word really in   uences the tag of

the seventh word?

(cid:73) and the does it in   uence the tag of the 4th word?
(cid:73) let assume only the previous tag matters:

p(yi|y1, y2, . . . , yi   2, yi   1)     q(yi|yi   1)

16 / 49

p(x, y) = p(y)p(x|y)

p(y)

(cid:73) chain rule + markov assumption

p(yi|y1, y2, . . . , yi   2, yi   1)     q(yi|yi   1)

p(y) = p(y1, y2, . . . , yn) =q(y1|start)
   q(y2|y1)
   q(y3|y2)
   q(y4|y3)
. . .
   q(yn|yn   1)

17 / 49

p(x, y) = p(y)p(x|y)

p(y)     2nd-order markov assumption

(cid:73) let assume only the two previous tag matter:

p(yi|y1, y2, . . . , yi   2, yi   1)     q(yi|yi   2, yi   1)

18 / 49

p(x, y) = p(y)p(x|y)

p(y)

(cid:73) chain rule + 2nd-order markov assumption

p(yi|y1, y2, . . . , yi   2, yi   1)     q(yi|yi   1, yi   2)

p(y) = p(y1, y2, . . . , yn) =q(y1|start, start)
   q(y2|start, y1)
   q(y3|y1, y2)
   q(y4|y2, y3)
. . .
   q(yn|yn   2, yn   1)

19 / 49

estimating q(yi|yi   2, yi   1)

(cid:73) here it is quite safe to use id113 estimates (why?)

q(c|a, b) =

count(a, b, c)
count(a, b)

20 / 49

estimating q(yi|yi   2, yi   1)

(cid:73) here it is quite safe to use id113 estimates (why?)

q(c|a, b) =

count(a, b, c)
count(a, b)

(cid:73) we could still get zero probabilities.

(cid:73) is this a bad thing?

20 / 49

estimating q(yi|yi   2, yi   1)

(cid:73) here it is quite safe to use id113 estimates (why?)

q(c|a, b) =

count(a, b, c)
count(a, b)

(cid:73) we could still get zero probabilities.

(cid:73) is this a bad thing?

(cid:73) to be on the safe side, we could use interpolation:

q(c|a, b) =   1

count(a, b, c)
count(a, b)

+   2

count(b, c)
count(b)

+   3

count(c)

num words

20 / 49

estimating q(yi|yi   2, yi   1)

(cid:73) here it is quite safe to use id113 estimates (why?)

q(c|a, b) =

count(a, b, c)
count(a, b)

(cid:73) we could still get zero probabilities.

(cid:73) is this a bad thing?

(cid:73) to be on the safe side, we could use interpolation:

q(c|a, b) =   1

count(a, b, c)
count(a, b)

+   2

count(b, c)
count(b)

+   3

count(c)

num words

  1 +   2 +   3 = 1   i > 0

20 / 49

estimating q(yi|yi   2, yi   1)

(cid:73) here it is quite safe to use id113 estimates (why?)

q(c|a, b) =

count(a, b, c)
count(a, b)

(cid:73) we could still get zero probabilities.

(cid:73) is this a bad thing?

(cid:73) to be on the safe side, we could use interpolation:

q(c|a, b) =   1

count(a, b, c)
count(a, b)

+   2

count(b, c)
count(b)

+   3

count(c)

num words

  1 +   2 +   3 = 1   i > 0

(cid:73) how would you set the    values?

20 / 49

p(x, y) = p(y)p(x|y)

we can compute p(y)

p(y) = p(y1, y2, . . . , yn) =q(y1|start, start)
   q(y2|start, y1)
   q(y3|y1, y2)
   q(y4|y2, y3)
. . .
   q(yn|yn   2, yn   1)

21 / 49

p(x, y) = p(y)p(x|y)

we can compute p(y)

p(y) = p(y1, y2, . . . , yn) =q(y1|start, start)
   q(y2|start, y1)
   q(y3|y1, y2)
   q(y4|y2, y3)
. . .
   q(yn|yn   2, yn   1)
q(yi|yi   2, yi   i)

n(cid:89)

=

i=1

21 / 49

p(x, y) = p(y)p(x|y)

we can compute p(y)

p(y) = p(y1, y2, . . . , yn) =q(y1|start, start)
   q(y2|start, y1)
   q(y3|y1, y2)
   q(y4|y2, y3)
. . .
   q(yn|yn   2, yn   1)
q(yi|yi   2, yi   i)

n(cid:89)

=

i=1

moving on to p(x|y)

21 / 49

p(x|y)

p(x|y) =p(x1, x2, . . . , xn|y1, y2, . . . , yn) =

22 / 49

p(x|y)

p(x|y) =p(x1, x2, . . . , xn|y1, y2, . . . , yn) =

p(x1|y1, . . . , yn)

22 / 49

p(x|y)

p(x|y) =p(x1, x2, . . . , xn|y1, y2, . . . , yn) =

p(x1|y1, . . . , yn)
   p(x2|x1, y1, . . . , yn)

22 / 49

p(x|y)

p(x|y) =p(x1, x2, . . . , xn|y1, y2, . . . , yn) =

p(x1|y1, . . . , yn)
   p(x2|x1, y1, . . . , yn)
   p(x3|x1, x2, y1, . . . , yn)
   p(x4|x1, x2, x3, y1, . . . , yn)
. . .
   p(xn|x1, x2, . . . , xn, y1, . . . , yn)

22 / 49

p(x|y)

p(x|y) =p(x1, x2, . . . , xn|y1, y2, . . . , yn) =

p(x1|y1, . . . , yn)
   p(x2|x1, y1, . . . , yn)
   p(x3|x1, x2, y1, . . . , yn)
   p(x4|x1, x2, x3, y1, . . . , yn)
. . .
   p(xn|x1, x2, . . . , xn, y1, . . . , yn)

(cid:73) what   s a reasonable assumption to make here?

22 / 49

p(x|y)

p(x|y)     independence assumption

(cid:73) we   ll assume that a word depends only on its tag.

p(xi|x1, . . . , xi   1, y1, . . . , yn)     e(xi|yi)

23 / 49

p(x|y)

p(x|y)     independence assumption

(cid:73) we   ll assume that a word depends only on its tag.

p(xi|x1, . . . , xi   1, y1, . . . , yn)     e(xi|yi)

(cid:73) a terrible assumption if we were generating sentences!

23 / 49

p(x|y)

p(x|y)     independence assumption

(cid:73) we   ll assume that a word depends only on its tag.

p(xi|x1, . . . , xi   1, y1, . . . , yn)     e(xi|yi)

(cid:73) a terrible assumption if we were generating sentences!
(cid:73) . . . but we don   t use this model to generate sentences.
(cid:73) the sentence is given. we are looking for a tag sequence.

23 / 49

estimating e(xi|yi)
(cid:73) id113 again:

e(book|nn) =

count(book, nn)

count(nn)

24 / 49

estimating e(xi|yi)
(cid:73) id113 again:

e(book|nn) =

count(book, nn)

count(nn)

(cid:73) do you see any problem here?

24 / 49

estimating e(xi|yi)
(cid:73) id113 again:

e(book|nn) =

count(book, nn)

count(nn)

(cid:73) do you see any problem here?

(cid:73) (we   ll get to this later)

24 / 49

p(x|y)

p(x|y) =p(x1, x2, . . . , xn, y1, y2, . . . , yn) =

e(x1|y1)
   e(x2|y2)
   e(x3|y3)
   e(x4|y4)
. . .
   e(xn|yn)
e(xi|yi)

n(cid:89)

=

i=1

25 / 49

p(x, y) = p(y)p(x|y)

a bigram tagging model (   rst order id48)

n(cid:89)

n(cid:89)

p(x, y) = p(y)p(x|y) =

q(yi|yi   1)

e(xi|yi)

i=1

i=1

q(yi|yi   1) : transition probabilities
e(xi|yi) : emission probabilities

26 / 49

p(x, y) = p(y)p(x|y)

a bigram tagging model (   rst order id48)

n(cid:89)

n(cid:89)

p(x, y) = p(y)p(x|y) =

q(yi|yi   1)

e(xi|yi)

i=1

i=1

q(yi|yi   1) : transition probabilities
e(xi|yi) : emission probabilities

s

y1

x1

y2

x2

y3

x3

y4

x4

y5

x5

e

26 / 49

p(x, y) = p(y)p(x|y)

a trigram tagging model (second order id48)

n(cid:89)

n(cid:89)

p(x, y) = p(y)p(x|y) =

q(yi|yi   2, yi   1)

e(xi|yi)

i=1

i=1

q(yi|yi   2, yi   1) : transition probabilities
e(xi|yi) : emission probabilities

27 / 49

p(x, y) = p(y)p(x|y)

a trigram tagging model (second order id48)

n(cid:89)

n(cid:89)

p(x, y) = p(y)p(x|y) =

q(yi|yi   2, yi   1)

e(xi|yi)

i=1

i=1

q(yi|yi   2, yi   1) : transition probabilities
e(xi|yi) : emission probabilities

s

y1

x1

y2

x2

y3

x3

y4

x4

y5

x5

e

27 / 49

second-order id48 example

p( holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp )

n(cid:89)

n(cid:89)

q(yi|yi   2, yi   1)

e(xi|yi) =

i=1

=
i=1
q(nnp|start, start)    q(vbd|start, nnp)    q(in|nnp, vbd)
   q(nnp|vbd, in)    q(,|in, nnp)    q(nnp|nnp, )
   e(holly|nnp)    e(came|vbd)    e(from|in)
   e(miami|nnp)    e(,|, )    e(f.l.a|nnp)

28 / 49

second-order id48 example

p( holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp )

n(cid:89)

q(yi|yi   2, yi   1)
=
i=1
q(nnp|start, start)
   q(vbd|start, nnp)
   q(in|nnp, vbd)
   q(nnp|vbd, in)
   q(,|in, nnp)
   q(nnp|nnp, )

n(cid:89)

i=1

e(xi|yi) =
  e(holly|nnp)
  e(came|vbd)
  e(from|in)
  e(miam|nnp)
  e(,|, )
  e(f.l.a|nnp)

28 / 49

second-order id48 example

p( holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp )

n(cid:89)

q(yi|yi   2, yi   1)
=
i=1
q(nnp|start, start)
   q(vbd|start, nnp)
   q(in|nnp, vbd)
   q(nnp|vbd, in)
   q(,|in, nnp)
   q(nnp|nnp, )

n(cid:89)

i=1

e(xi|yi) =
  e(holly|nnp)
  e(came|vbd)
  e(from|in)
  e(miam|nnp)
  e(,|, )
  e(f.l.a|nnp)

problem

(cid:73) we are multiplying many small numbers
(cid:73) end-result will by tiny

29 / 49

solution:(cid:81)    (cid:80)

argmaxyp(x, y) = argmaxy log p(x, y)

30 / 49

solution:(cid:81)    (cid:80)

argmaxyp(x, y) = argmaxy log p(x, y)

n(cid:89)

i=1

argmaxy

n(cid:89)

i=1

e(xi|yi)

n(cid:89)

q(yi|yi   2, yi   1)   

n(cid:89)

= argmaxy log(

q(yi|yi   2, yi   1)   

i=1

i=1

e(xi|yi))

30 / 49

solution:(cid:81)    (cid:80)

argmaxyp(x, y) = argmaxy log p(x, y)

n(cid:89)

i=1

argmaxy

= argmaxy log(

n(cid:89)

= argmaxy

n(cid:89)

i=1

n(cid:89)
n(cid:88)

i=1

q(yi|yi   2, yi   1)   

e(xi|yi)

n(cid:88)

q(yi|yi   2, yi   1)   

i=1

log q(yi|yi   2, yi   1) +

i=1

i=1

e(xi|yi))

log e(xi|yi)

30 / 49

second order id48     log space

log p( holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp )

n(cid:88)

=

log q(yi|yi   2, yi   1)
i=1
log q(nnp|start, start)
+ log q(vbd|start, nnp)
+ log q(in|nnp, vbd)
+ log q(nnp|vbd, in)
+ log q(,|in, nnp)
+ log q(nnp|nnp, )

n(cid:88)

i=1

log e(xi|yi) =
+
+ log e(holly|nnp)
+ log e(came|vbd)
+ log e(from|in)
+ log e(miam|nnp)
+ log e(,|, )
+ log e(f.l.a|nnp)

31 / 49

decoding

32 / 49

decoding

argmaxy ?
remember, we want to tag sentences.

(cid:73) we can compute p(x, y)
(cid:73) we are given words x = x1, . . . , xn
(cid:73) we are looking for a sequence y = y1, . . . , yn

s.t. p(x, y) is maximized.

how do we search for y?

33 / 49

argmaxyp(x, y)

solution 1

(cid:73) go over all possible sequences y.

problem

(cid:73) there are very many such sequences. (how many?)

34 / 49

argmaxyp(x, y)

solution 2

(cid:73) choose the highest scoring tag t1 for e(x1|y1)q(y1|start)
(cid:73) choose the highest scoring tag t2 for e(x2|y2)q(y2|start, t1)
(cid:73) choose the highest scoring tag t3 for e(x3|y3)q(y3|t1, t2)
(cid:73) . . .

35 / 49

argmaxyp(x, y)

solution 2

(cid:73) choose the highest scoring tag t1 for e(x1|y1)q(y1|start)
(cid:73) choose the highest scoring tag t2 for e(x2|y2)q(y2|start, t1)
(cid:73) choose the highest scoring tag t3 for e(x3|y3)q(y3|t1, t2)
(cid:73) . . .

complexity: o(kn) where k is tagset size.

35 / 49

argmaxyp(x, y)

solution 2

(cid:73) choose the highest scoring tag t1 for e(x1|y1)q(y1|start)
(cid:73) choose the highest scoring tag t2 for e(x2|y2)q(y2|start, t1)
(cid:73) choose the highest scoring tag t3 for e(x3|y3)q(y3|t1, t2)
(cid:73) . . .

complexity: o(kn) where k is tagset size.

problem

(cid:73) will not produce optimal solution. (why?)

35 / 49

argmaxyp(x, y)

solution: id145

(cid:73) the viterbi algorithm.

36 / 49

bigram viterbi

v(i, t)
maximum id203 of a tag sequence ending
in tag t at time i.

37 / 49

bigram viterbi

v(i, t)
maximum id203 of a tag sequence ending
in tag t at time i.

recursive de   nition

v(0, start) = 1

37 / 49

bigram viterbi

v(i, t)
maximum id203 of a tag sequence ending
in tag t at time i.

recursive de   nition

v(0, start) = 1

v(i, t)

37 / 49

bigram viterbi

v(i, t)
maximum id203 of a tag sequence ending
in tag t at time i.

recursive de   nition

v(0, start) = 1

v(i, t) = max

t(cid:48) v(i     1, t(cid:48))q(t|t(cid:48))e(wi|t)

37 / 49

trigram viterbi

v(i, t, r)
maximum id203 of a tag sequence ending
in tags t,r at time i.

38 / 49

trigram viterbi

v(i, t, r)
maximum id203 of a tag sequence ending
in tags t,r at time i.

recursive de   nition

v(0, start, start) = 1

v(i, t, r) = max

t(cid:48) v(i     1, t(cid:48), t)q(r|t(cid:48), t)e(wi|r)

38 / 49

trigram viterbi     algorithm

input:
sentence: w1, . . . , wn
parameters: e(w|t), q(t|u, v)
tagset: t

output:
id203 of best tag sequence y1, . . . , yn
algorithm:

(cid:73) for i = 1, . . . , n

(cid:73) for t     t, r     t

v(i, t, r) = maxt(cid:48) v(i     1, t(cid:48), t)q(r|t(cid:48), t)e(wi|r)

return: maxt   t,r   t v(n, t, r)

39 / 49

trigram viterbi with back-pointers     algorithm

input:
sentence: w1, . . . , wn
parameters: e(w|t), q(t|u, v)
tagset: t

output:
id203 of best tag sequence y1, . . . , yn
algorithm:

(cid:73) for i = 1, . . . , n

(cid:73) for t     t, r     t

v(i, t, r) = maxt(cid:48) v(i     1, t(cid:48), t)q(r|t(cid:48), t)e(wi|r)
bp(i, t, r) = arg maxt(cid:48) v(i     1, t(cid:48), t)q(r|t(cid:48), t)e(wi|r)

(cid:73) set yn   1, yn = arg maxt,r v(n, t, r)
(cid:73) for i = n     2 . . . 1 set yi = bp(i + 2, yi+1, yi+2)

return: y1, . . . , yn

40 / 49

runtime

o(n     |t|3)

why?

41 / 49

supervised second-order id48 tagger
(trigram tagger)

training

(cid:73) using corpus of tagged sentences, compute:

(cid:73) count(tag1,tag2,tag3), count(tag1,tag2), count(tag),

count(tag,word)

(cid:73) calculate e, q based on counts

42 / 49

supervised second-order id48 tagger
(trigram tagger)

training

(cid:73) using corpus of tagged sentences, compute:

(cid:73) count(tag1,tag2,tag3), count(tag1,tag2), count(tag),

count(tag,word)

(cid:73) calculate e, q based on counts

tagging

(cid:73) when given a sentence x = x1, . . . , xn

(cid:73) use the viterbi algorithm to    nd
argmaxyp(y|x) = argmaxyp(x, y)

(cid:73) using the e and q quantities from training.

42 / 49

order considerations

(cid:73) first order markov: p(yi|y1, . . . , yi   1) = q(yi|yi   1)
(cid:73) second order markov: p(yi|y1, . . . , yi   1) = q(yi|yi   2, yi   1)

is there any reason to prefer the    rst- over the second-order?

why not do third-order?

43 / 49

id48s     dealing with rare or unseen words

our training set is of limited size

(cid:73) some words will not be seen in the corpus.

44 / 49

id48s     dealing with rare or unseen words

our training set is of limited size

(cid:73) some words will not be seen in the corpus.

(cid:73) so?

44 / 49

id48s     dealing with rare or unseen words

our training set is of limited size

(cid:73) some words will not be seen in the corpus.

(cid:73) so?

(cid:73) some words will only be seen once.

44 / 49

id48s     dealing with rare or unseen words

our training set is of limited size

(cid:73) some words will not be seen in the corpus.

(cid:73) so?

(cid:73) some words will only be seen once.

(cid:73) so?

44 / 49

id48s     dealing with rare or unseen words

how do we calculate

e(word|tag)

for unseen or infrequent words?

45 / 49

id48s     dealing with rare or unseen words

how do we calculate

e(word|tag)

for unseen or infrequent words?

46 / 49

id48     sumary

s

s

y1

x1

y1

x1

y2

x2

y2

x2

y3

x3

y3

x3

y4

x4

y4

x4

y5

x5

y5

x5

e

e

47 / 49

id48     summary

the id48 tagging algorithm

(cid:73) f (x) = argmaxyp(y|x) = argmaxyp(x, y)

(cid:73) model p(x, y) = p(y)p(x|y) =(cid:81) q(yi|yi   1)    e(xi|yi)

(cid:73) learn tables for transitions q and emissions e by counting.
(cid:73) find best y for a given x using viterbi.
(cid:73) hardest part: good e(word|tag) for rare/unseen words.

48 / 49

id48     summary

(cid:73) for a long time, the best tagging algorithm available.
(cid:73) nowadays, more accurate models exist (we   ll see some of

them).

(cid:73) id48 still useful for unsupervised learning.

(cid:73) you a lot of text (without labels)
(cid:73) and a dictionary mapping words to possible tags.
    can learn q and e using the em algorithm.

49 / 49

