unsupervised learning and id96

yoav goldberg

bar ilan university

(with slides from david blei, zornitsa kozerava)

1 / 1

previously

supervised learning

(cid:73) get labeled training data
(cid:73) represent data as (features, label) pairs
(cid:73) train a classi   er / model to predict labels based on features

today

(cid:73) what if we don   t have training data?
(cid:73) can we still do something useful?

unsupervised learning
things we can do without labeled data

2 / 1

unsupervised methods

option 1:    naturally occurring    labels / bootstrap

(cid:73) be creative and    nd data which can be used as labels.

(cid:73) e.g., we want to identify paragraphs. maybe some website

indicate this via their html tags?

(cid:73) automatically create your own training set

(cid:73) write simple rule-based system to collect easy examples

(cid:73) high precision, low recall

(cid:73) use the easy examples as training data

(cid:73) hope it will generalize well.
(cid:73) careful not to overlap your features with the rules too much!

3 / 1

unsupervised methods

option 2: write and algorithm and hope it works

(cid:73) example: assignment 2.

(cid:73) represent words by their contexts
(cid:73) de   ne the assoc function
(cid:73) de   ne the joint function
(cid:73) use this to get a useful result     lists of similar words

(cid:73) can be very effective
(cid:73) but no    learning    involved.
(cid:73) what to do when this doesn   t work?

4 / 1

unsupervised methods

option 3: latent-variable generative modeling

(cid:73) de   ne a    generative story    of how the data was generated

(cid:73) this story doesn   t have to be very convincing or realistic

(cid:73) the story can include    latent variables   , stuff that you

would like to see but you don   t

(cid:73) for example: id48 pos-tagging, where we treat the tags

as latent.

(cid:73) search for an assignment of latent variables such that the

data has high id203 under the model.

(cid:73) usually, this search is hard.
(cid:73) approximate!

(cid:73) em
(cid:73) mcmc (id150)

5 / 1

unsupervised learning
example: id48

example: id48

(cid:73) we want to train a pos-tagger, but don   t have labeled data.
(cid:73) we do have a dictionary, associating some words with their

possible pos tags, and also a lot of text.

(cid:73) we will use the dictionary and the text to train a bigram

id48 model.

6 / 1

unsupervised learning
example: id48

the bigram-id48 generative story:
to generate a tagged sentence (w, t) = (w1, . . . , wn, t1, . . . , tn):

(cid:73) start with tag t0 = start.
(cid:73) for i in 1, . . . , n:

(cid:73) draw a random tag ti from the transition distribution
(cid:73) draw a random word wi from the tag distribution p(wi|ti)

p(ti|ti   1)

recall the supervised case

(cid:73) we observe both the words and the tags.
(cid:73) we estimate q = p(ti|ti   1) and e = p(wi|ti) based on our
observations.
(cid:73) done

7 / 1

notation     discrete distributions

we say that x     discrete(  , k)
iff:
(cid:73) x can get one of k values
(cid:73)    is a vector with k entries
(cid:73)   i     0
(cid:73) (cid:80)i   i = 1
(cid:73) p(x = i) =   i

example
p(tj|tj   1) is a discrete
distribution.

tj     discrete(  ,|t|)

where:
(cid:73) |t| is the size of the tagset
(cid:73) we can get a uniform
distribution if we set:

(cid:73)   i = 1/|t|

(cid:73) we can also estimate   

from data using id113:

(cid:73)   tj = count(tj   1,tj)
count(tj   1)

8 / 1

example id48:

the unsupervised case

(cid:73) we don   t get to see the tags. they are latent.
(cid:73) but, for a given tag assignment, we can:

(cid:73) estimate parameters
(cid:73) calculate corpus id203

(cid:73) search for tag assignments such that if we estimate

parameters from them, and then use the parameters to
calculate the corpus id203, we will get high id203.

(cid:73) this search looks hard!
(cid:73) and it is.
(cid:73) two possible approximations:

(cid:73) em algorithm
(cid:73) id150

9 / 1

id150

w = w1, . . . , wn
t = t1, . . . , tn

(cid:73) we are interested in the tag assignment that will maximize

p(w, t)
(cid:73) for a    xed w, arg maxt p(w, t) = arg maxt p(t|w)

10 / 1

id150

w = w1, . . . , wn
t = t1, . . . , tn

(cid:73) we are interested in the tag assignment that will maximize

p(w, t)
(cid:73) for a    xed w, arg maxt p(w, t) = arg maxt p(t|w)
(cid:73) if we could sample from p(t|w), we will, with high
id203, get t such that p(t|w) is high.

10 / 1

id150

w = w1, . . . , wn
t = t1, . . . , tn

(cid:73) we are interested in the tag assignment that will maximize

p(w, t)
(cid:73) for a    xed w, arg maxt p(w, t) = arg maxt p(t|w)
(cid:73) if we could sample from p(t|w), we will, with high
id203, get t such that p(t|w) is high.
(cid:73) ok. . . but how do we sample from p(t|w)?

10 / 1

id150

w = w1, . . . , wn
t = t1, . . . , tn

(cid:73) we are interested in the tag assignment that will maximize

p(w, t)
(cid:73) for a    xed w, arg maxt p(w, t) = arg maxt p(t|w)
(cid:73) if we could sample from p(t|w), we will, with high
id203, get t such that p(t|w) is high.
(cid:73) ok. . . but how do we sample from p(t|w)?
(cid:73) id150 is a    magical    way of doing that

(cid:73) to uncover the magic, see id114 class

10 / 1

id150

main idea
(cid:73) in order to sample p(t|w) = p(t1, t2, . . . , tn|w):
(cid:73) start with a random assignment of t1, . . . , tn. then:

(cid:73) sample t1 based on t2, . . . , tn, w

(cid:73) p(t1|t2, t3, . . . , tn, w)

(cid:73) sample t2 based on t1, t3, . . . , tn, w
(cid:73) . . .
(cid:73) sample tk based on t1, . . . , tk   1, tk+1, . . . , tn, w
(cid:73) . . . and so on

(cid:73) after many iterations, we will get samples from p(t|w)

11 / 1

id150

calculating p(tk|t1, . . . , tk   1, tk+1, . . . , tn, w)
(cid:73) notation: t   k = t1, . . . , tk   1, tk+1, . . . , tn.
(cid:73) we can estimate q and e as previously, based on w and the

assignments to t   k.

(cid:73) now we get:

p(tk|t   k)     q(tk|tk   1)e(wk|tk)q(tk+1|tk)

(cid:73) (why? and what does     means?)

(cid:73) calculate this for every possible value of tk.
(cid:73) normalize

12 / 1

drawsfromdistributionsx   d	screte(  ,k)p=math.random()sum=0.0foriin0...k-1{sum+=theta[i];if(sum>=p)returni}multinomialisageneralizationofdiscrete32the id150 algorithm

sampling from p(t|w) for t = t1, . . . , tn
initialize t with random values
calculate parameters (collect counts) based on t,w.
for many iterations do
for i     1, . . . , n do

   forget    value of ti (decrease counts)
calculate p(ti|t   i) based on modi   ed counts
sample new value for ti from p(ti|t   i)

13 / 1

putting it all together

training id48 from text and dictionary using gibbs
sampling

for each word, assign a random tag from the set allowed by
the dictionary
calculate q, e based on this tag assignment
for many iterations do

for every sentence do

for i     1, . . . , length do

   forget    value of ti (decrease counts)
calculate p(ti|t   i) based on modi   ed counts
(set prob of tags not in dictionary to 0. normalize.)
sample new value for ti from p(ti|t   i)

14 / 1

putting it all together

training id48 from text and dictionary using gibbs
sampling

for each word, assign a random tag from the set allowed by
the dictionary
calculate q, e based on this tag assignment
for many iterations do

for every sentence do

for i     1, . . . , length do

   forget    value of ti (decrease counts)
calculate p(ti|t   i) based on modi   ed counts
(set prob of tags not in dictionary to 0. normalize.)
sample new value for ti from p(ti|t   i)
calculate    nal q and e based on the    nal state
(can also average several states)

14 / 1

id48 - discussion

why do you expect this to work?

why do we need the tag dictionary?

15 / 1

id96 / lda

theproblemwithinformationwww.betaversion.org/~stefano/linotype/news/26/asmoreinformationbecomesavailable,itbecomesmoredi   culttoaccesswhatwearelookingfor.weneednewtoolstohelpusorganize,search,andunderstandthesevastamountsofinformation.d.bleitopicmodelstopicmodelingcandida hofer topicmodelingprovidesmethodsforautomaticallyorganizing,understanding,searching,andsummarizinglargeelectronicarchives.1uncoverthehiddentopicalpatternsthatpervadethecollection.2annotatethedocumentsaccordingtothosetopics.3usetheannotationstoorganize,summarize,andsearchthetexts.d.bleitopicmodelsdiscovertopicsfromacorpus   genetics      evolution      disease      computers   humanevolutiondiseasecomputergenomeevolutionaryhostmodelsdnaspeciesbacteriainformationgeneticorganismsdiseasesdatagenesliferesistancecomputerssequenceoriginbacterialsystemgenebiologynewnetworkmoleculargroupsstrainssystemssequencingphylogeneticcontrolmodelmaplivinginfectiousparallelinformationdiversitymalariamethodsgenetiid19roupparasitenetworksmappingnewparasitessoftwareprojecttwounitednewsequencescommontuberculosissimulationsd.bleitopicmodelsmodeltheevolutionoftopicsovertime1880190019201940196019802000ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo1880190019201940196019802000ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooorelativitylaserforcenerveoxygenneuron"theoretical physics""neuroscience"d.bleitopicmodelsmodelconnectionsbetweentopicswild typemutantmutationsmutantsmutationplantsplantgenegenesarabidopsisp53cell cycleactivitycyclinregulationamino acidscdnasequenceisolatedproteingenediseasemutationsfamiliesmutationrnadnarna polymerasecleavagesitecellscellexpressioncell linesbone marrowunited stateswomenuniversitiesstudentseducationsciencescientistssaysresearchpeopleresearchfundingsupportnihprogramsurfacetipimagesampledevicelaseropticallightelectronsquantummaterialsorganicpolymerpolymersmoleculesvolcanicdepositsmagmaeruptionvolcanismmantlecrustupper mantlemeteoritesratiosearthquakeearthquakesfaultimagesdataancientfoundimpactmillion years agoafricaclimateoceanicechangesclimate changecellsproteinsresearchersproteinfoundpatientsdiseasetreatmentdrugsclinicalgeneticpopulationpopulationsdifferencesvariationfossil recordbirdsfossilsdinosaursfossilsequencesequencesgenomednasequencingbacteriabacterialhostresistanceparasitedevelopmentembryosdrosophilagenesexpressionspeciesforestforestspopulationsecosystemssynapsesltpglutamatesynapticneuronsneuronsstimulusmotorvisualcorticalozoneatmosphericmeasurementsstratosphereconcentrationssunsolar windearthplanetsplanetco2carboncarbon dioxidemethanewaterreceptorreceptorsligandligandsapoptosisproteinsproteinbindingdomaindomainsactivatedtyrosine phosphorylationactivationphosphorylationkinasemagneticmagnetic    eldspinsuperconductivitysuperconductingphysicistsparticlesphysicsparticleexperimentsurfaceliquidsurfaces   uidmodelreactionreactionsmoleculemoleculestransition stateenzymeenzymesironactive sitereductionpressurehigh pressurepressurescoreinner corebrainmemorysubjectslefttaskcomputerprobleminformationcomputersproblemsstarsastronomersuniversegalaxiesgalaxyvirushivaidsinfectionvirusesmiceantigent cellsantigensimmune responsed.bleitopicmodelsannotateimagesautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53skywatertreemountainpeopleautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53scotlandwaterflowerhillstreeautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53skywaterbuildingpeoplewaterautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53fishwateroceantreecoralautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53peoplemarketpatterntextiledisplayautomaticimageannotationbirds nest leaves branch treepredicted caption:predicted caption:people market pattern textile displaysky water tree mountain peoplepredicted caption:fish water ocean tree coralsky water buildings people mountainpredicted caption:predicted caption:predicted caption:scotland water flowers hills treeprobabilisticmodelsoftextandimages   p.5/53birdsnesttreebranchleavesd.bleitopicmodelsdiscoverin   uentialarticlesyearweightedinfluence0.0000.0050.0100.0150.0200.0250.0301880190019201940196019802000jared m. diamond, distributional ecology of new guinea birds. science (1973) [296 citations]w. b. scott, the isthmus of panama in its relation to the animal life of north and south america, science (1916)[3 citations]william k. gregory, the new anthropogeny: twenty-five stages of vertebrate evolution, from silurian chordate to man, science (1933)[3 citations]derek e. wildman et al., implications of natural selection in shaping 99.4% nonsynonymous dna identity between humans and chimpanzees: enlarging genus homo, pnas (2003)[178 citations]organizeandbrowselargecorporalatentdirichletallocationd.bleitopicmodelslatentdirichletallocation(lda)simpleintuition:documentsexhibitmultipletopics.generativemodelforldagene     0.04dna      0.02genetic  0.01.,,life     0.02evolve   0.01organism 0.01.,,brain    0.04neuron   0.02nerve    0.01...data     0.02number   0.02computer 0.01.,,topicsdocumentstopic proportions andassignments   eachtopicisadistributionoverwords   eachdocumentisamixtureofcorpus-widetopics   eachwordisdrawnfromoneofthosetopicstheposteriordistributiontopicsdocumentstopic proportions andassignments   inreality,weonlyobservethedocuments   theotherstructurearehiddenvariablestheposteriordistributiontopicsdocumentstopic proportions andassignments   ourgoalistoinferthehiddenvariables   i.e.,computetheirdistributionconditionedonthedocumentsp(topics,proportions,assignments|documents)graphicalmodels(aside)      yx1x2xnxnyn      nodesarerandomvariables   edgesdenotepossibledependence   observedvariablesareshaded   platesdenotereplicatedstructured.bleitopicmodelsgraphicalmodels(aside)      yx1x2xnxnyn      structureofthegraphde   nesthepatternofconditionaldependencebetweentheensembleofrandomvariables   e.g.,thisgraphcorrespondstop(y,x1,...,xn)=p(y)nyn=1p(xn|y)d.bleitopicmodelsldaasagraphicalmodel  dzd,nwd,nndk  k    proportionsparameterper-documenttopic proportionsper-wordtopic assignmentobservedwordtopicstopicparameter   nodesarerandomvariables;edgesindicatedependence.   shadednodesareobserved;unshadednodesarehidden.   platesindicatereplicatedvariables.lda generative story

we have k topics, and a vocabulary v of |v| words.
each topic   k is a distribution over words.
a document d is created by

(cid:73) sample length nd from a poisson distribution

(cid:73) (alternatively, assume nd is given)

(cid:73) sample topic proportions   d from a dirichlet distribution

with parameter   .
(cid:73) for each position i     1, . . . , n:

(cid:73) sample topic zi from   d
(cid:73) sample word wi from the distribution   zi

17 / 1

lda generative story

we have k topics, and a vocabulary v of |v| words.
each topic   k is a distribution over words.
a document d is created by

(cid:73) sample length nd from a poisson distribution

(cid:73) (alternatively, assume nd is given)

(cid:73) sample topic proportions   d from a dirichlet distribution

with parameter   .
(cid:73) for each position i     1, . . . , n:

(cid:73) sample topic zi from   d
(cid:73) sample word wi from the distribution   zi

assumptions

(cid:73) we do not care about the word-order (   bag of words   )
(cid:73) each word is independent of the other words given its topic

17 / 1

lda generative story

we have k topics, and a vocabulary v of |v| words.
each topic   k is a distribution over words.
a document d is created by

(cid:73) sample length nd from a poisson distribution

(cid:73) (alternatively, assume nd is given)

(cid:73) sample topic proportions   d from a dirichlet distribution

with parameter   .
(cid:73) for each position i     1, . . . , n:

(cid:73) sample topic zi from   d
(cid:73) sample word wi from the distribution   zi

assumptions

(cid:73) we do not care about the word-order (   bag of words   )
(cid:73) each word is independent of the other words given its topic

17 / 1

the dirichlet distribution

(cid:73) the dirichlet distribution is a    distribution over distributions   
(cid:73) when you sample        dirichlet(  , k):

(cid:73)    is a k-dim vector
(cid:73)   i     0
(cid:73) (cid:80)i   i = 1

18 / 1

the dirichlet distribution

(cid:73) the dirichlet distribution is a    distribution over distributions   
(cid:73) when you sample        dirichlet(  , k):

(cid:73)    is a k-dim vector
(cid:73)   i     0
(cid:73) (cid:80)i   i = 1

the id203 of seeing a particular vector    is:
i=1     i   1
i
b(  )

pdirichlet(  ,k)(  ) = (cid:81)k
b(  ) = (cid:81)k
  ((cid:80)k

i=1   (  i)
i=1   i)

-    is the gamma function, generalization of factorial.

- generally,    is a k-dim vector, but we will assume    symmetric    dirichlet, in
which    is a single scalar (and   i =    for all i     {1, . . . , k})

18 / 1

the dirichlet distribution

the dirichlet distribution is a    distribution over distributions   

pp(  |  ) = (cid:81)k

i=1     i   1
i
b(  )

(cid:73)    controls the shape, mean and sparsity of   

19 / 1

  =1itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910  =10itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910  =100itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910  =1itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910  =0.1itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910  =0.01itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910  =0.001itemvalue0.00.20.40.60.81.00.00.20.40.60.81.00.00.20.40.60.81.0 1llllllllll 6llllllllll11llllllllll12345678910 2llllllllll 7llllllllll12llllllllll12345678910 3llllllllll 8llllllllll13llllllllll12345678910 4llllllllll 9llllllllll14llllllllll12345678910 5llllllllll10llllllllll15llllllllll12345678910the dirichlet distribution

for draws    from a symmetric dirichlet distribution:

   = 1 all    are equally likely
   > 1 uniform    are more likely
   < 1 spikey    are more likely

20 / 1

lda generative story

we have k topics, and a vocabulary v of |v| words.
each topic   k is a distribution over words.   k     dirichlet(  ,|v|)
a document d is created by

(cid:73) sample length nd from a poisson distribution

(cid:73) (alternatively, assume nd is given)

(cid:73) sample topic proportions   d from a dirichlet distribution

with parameter   .
(cid:73) for each position i     1, . . . , n:

(cid:73) sample topic zi from   d
(cid:73) sample word wi from the distribution   zi

21 / 1

lda generative story

we have k topics, and a vocabulary v of |v| words.
each topic   k is a distribution over words.   k     dirichlet(  ,|v|)
a document d is created by

(cid:73) sample length nd from a poisson distribution

(cid:73) (alternatively, assume nd is given)

(cid:73) sample topic proportions   d from a dirichlet distribution

with parameter   .
(cid:73) for each position i     1, . . . , n:

(cid:73) sample topic zi from   d
(cid:73) sample word wi from the distribution   zi

   controls how many topics we expect to see in our
documents

21 / 1

latentdirichletallocation(lda)topicsdocumentstopic proportions andassignments   ourgoalistoinferthehiddenvariables   i.e.,computetheirdistributionconditionedonthedocumentsp(topics,proportions,assignments|documents)ldaasagraphicalmodel  dzd,nwd,nndk  k    proportionsparameterper-documenttopic proportionsper-wordtopic assignmentobservedwordtopicstopicparameter   nodesarerandomvariables;edgesindicatedependence.   shadednodesareobserved;unshadednodesarehidden.   platesindicatereplicatedvariables.ldaasagraphicalmodel  dzd,nwd,nndk  k    proportionsparameterper-documenttopic proportionsper-wordtopic assignmentobservedwordtopicstopicparameterp(  ,  ,z,w)= k(cid:89)i=1p(  i|  )  d(cid:89)d=1p(  d|  )n(cid:89)n=1p(zd,n|  d)p(wd,n|  1:k,zd,n) ldaasagraphicalmodel  dzd,nwd,nndk  k       thisjointde   nesaposterior,p(  ,z,  |w).   fromacollectionofdocuments,infer   per-wordtopicassignmentzd,n   per-documenttopicproportions  d   per-corpustopicdistributions  k   thenuseposteriorexpectationstoperformthetaskathand:informationretrieval,documentsimilarity,exploration,andothers.exampleid13618162636465666768696topicsid2030.00.10.20.30.4d.bleitopicmodelsexampleid136   genetics      evolution      disease      computers   humanevolutiondiseasecomputergenomeevolutionaryhostmodelsdnaspeciesbacteriainformationgeneticorganismsdiseasesdatagenesliferesistancecomputerssequenceoriginbacterialsystemgenebiologynewnetworkmoleculargroupsstrainssystemssequencingphylogeneticcontrolmodelmaplivinginfectiousparallelinformationdiversitymalariamethodsgenetiid19roupparasitenetworksmappingnewparasitessoftwareprojecttwounitednewsequencescommontuberculosissimulationsd.bleitopicmodelsexampleid136(ii)d.bleitopicmodelsexampleid136(ii)problemmodelselectionspeciesproblemsratemaleforestmathematicalconstantmalesecologynumberdistributionfemales   shnewtimesexecologicalmathematicsnumberspeciesconservationuniversitysizefemalediversitytwovaluesevolutionpopulation   rstvaluepopulationsnaturalnumbersaveragepopulationecosystemsworkratessexualpopulationstimedatabehaviorendangeredmathematiciansdensityevolutionarytropicalchaosmeasuredgeneticforestschaoticmodelsreproductiveecosystemd.bleitopicmodelswhy does lda work?

22 / 1

whydoeslda   work   ?   ldatradesofftwogoals.1foreachdocument,allocateitswordstoasfewtopicsaspossible.2foreachtopic,assignhighid203toasfewtermsaspossible.   thesegoalsareatodds.   puttingadocumentinasingletopicmakes#2hard:allofitswordsmusthaveid203underthattopic.   puttingveryfewwordsineachtopicmakes#1hard:tocoveradocument   swords,itmustassignmanytopicstoit.   tradingoffthesegoals   ndsgroupsoftightlyco-occurringwords.what do we get out of lda?

(cid:73) topic assignments z
(cid:73) topic proportions (how strong is topic k in document j?)
(cid:73) topics distributions (how strong is word i in topic k?)

(cid:73) also: which topics are related to word i?

23 / 1

what do we get out of lda?

(cid:73) topic assignments z
(cid:73) topic proportions (how strong is topic k in document j?)
(cid:73) topics distributions (how strong is word i in topic k?)

(cid:73) also: which topics are related to word i?

so?

(cid:73) which topics are in our corpus?
(cid:73) find similar docs (by comparing    topic vectors    of docs)
(cid:73) find related words (by comparing    topic vectors    of words)
(cid:73) id183:    nd documents related to words x,y,z,

even if all or some of these words did not appear in the
document

(cid:73) . . .

23 / 1

7'8-9$:'4*)$       p(t|k)       for all t and k, is a term by topic matrix""" (gives which terms make up a topic)"       p(k|doc)  for all k and doc, is a topic by document ""            matrix (gives which topics are in a document)">?8,<1>&#56)a1-1$'>$7#"#$f'&831$      b-0*5$6$.*=.$9'))*9;'5$,&-g*5$(a$h&1.$?&64*$.'$9'))*?*$1.34*5.1$      i6.6$/61$>'))',-5?$9/6&69.*&-1;91$      jk2lllm$,'&4$.a8*1$$n1.'8$,'&41$&*+'0*4o$      pq2lllm$4'93+*5.1$      k2lll2lllm$,'&4$.'@*51$      r-54$.'8-91$-5$./*$46.6$7'8-91$-5$./*$<4396;'56)$f'&831$n7#"#o$printing paper print printed type process ink press image printer prints printers copy copies form offset graphic surface produced characters play plays stage audience theater actors drama shakespeare actor theatre playwright performance dramatic costumes comedy tragedy characters scenes opera performed team game basketball players player play playing soccer played ball teams basket football score court games try coach gym shot judge trial court case jury accused guilty defendant justice evidence witnesses crime lawyer witness attorney hearing innocent defense charge criminal hypothesis experiment scientific observations scientists experiments scientist experimental test method hypotheses tested evidence based observation science facts data results explanation study test studying homework need class math try teacher write plan arithmetic assignment place studied carefully decide important notebook review s   pqd$4'912$jkd$,'&41$s   tqll$.'8-912$*u?ue$%')a1*+a$printing paper print printed type process ink press image printer prints printers copy copies form offset graphic surface produced characters play plays stage audience theater actors drama shakespeare actor theatre playwright performance dramatic costumes comedy tragedy characters scenes opera performed team game basketball players player play playing soccer played ball teams basket football score court games try coach gym shot judge trial court case jury accused guilty defendant justice evidence witnesses crime lawyer witness attorney hearing innocent defense charge criminal hypothesis experiment scientific observations scientists experiments scientist experimental test method hypotheses tested evidence based observation science facts data results explanation study test studying homework need class math try teacher write plan arithmetic assignment place studied carefully decide important notebook review 7/&**$i'93+*5.1$,-./$./*$,'&4$v8)6aw$n53+(*&1$x$9')'&1$!$.'8-9$611-?5+*5.1o$lda id136

how do we    t an lda model to the data?

24 / 1

fitting an lda model to our data

use an existing tool!

(cid:73) mallet (java)
(cid:73) gensym (python)
(cid:73) many other tools available

(cid:73) (see david blei   s website)

25 / 1

fitting an lda model to our data

but how are the tools implemented?
and what if we want a slightly different story?

26 / 1

fitting an lda model to our data

but how are the tools implemented?
and what if we want a slightly different story?

(cid:73) exact id136 is intractable.
(cid:73) use an approximate algorithm.

26 / 1

fitting an lda model to our data

but how are the tools implemented?
and what if we want a slightly different story?

(cid:73) exact id136 is intractable.
(cid:73) use an approximate algorithm.
(cid:73) current tools use modern complex algorithms:

(cid:73) fast
(cid:73) scale well to huge number of topics and documents
(cid:73) beyond the scope of this course

26 / 1

fitting an lda model to our data

but how are the tools implemented?
and what if we want a slightly different story?

(cid:73) exact id136 is intractable.
(cid:73) use an approximate algorithm.
(cid:73) current tools use modern complex algorithms:

(cid:73) fast
(cid:73) scale well to huge number of topics and documents
(cid:73) beyond the scope of this course

(cid:73) but for    tting a small to medium data, we can use gibbs

(cid:73) (gibbs is also our best bet for implementing modi   cations

sampling.

of lda)

26 / 1

lda gibbs sampler

recall:

(cid:73) inputs:   ,   , k
(cid:73) obeserved variables: words, w = wd,n
(cid:73) unobserved:    =   1, . . . ,   d,    =   1, . . . ,   k, z = zd,n

we need to sample from
p(z,   ,   |w,   ,   )
in gibbs:
initialize random z
then, repeatedly:
(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

27 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

28 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

these lines are easy:

p(zd,i = k | z   d,i,   d,   , w) =   d

k      k

wd,i

  d
k id203 of generating topic k in doc d
  k
wd,i id203 of generating word wd,i from topic k

28 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

what does this line mean?

29 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

what does this line mean?
we need to sample   d from p(  |z,   ).
(cid:73) given z, we can derive an id113 estimate of   d:

  d
k =

count(zd,i = k)

nd

29 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

what does this line mean?
we need to sample   d from p(  |z,   ).
(cid:73) given z, we can derive an id113 estimate of   d:

  d
k =

count(zd,i = k)

nd

(cid:73) but no. we need to sample. what does it mean to sample

  ?

29 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

what does this line mean?
we need to sample   d from p(  |z,   ).
(cid:73) given z, we can derive an id113 estimate of   d:

  d
k =

count(zd,i = k)

nd

(cid:73) but no. we need to sample. what does it mean to sample

  ?

(cid:73) under the bayesian philosophy, we do not commit to a

single estimate of   . instead, we have a distribution
p(  d|z,   ) of possible   d, based on our prior belief    and
the data we saw z.

29 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

what does this line mean?
we need to sample   d from p(  |z,   ).

30 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .

what does this line mean?
we need to sample   d from p(  |z,   ).
because   d     dirichlet(  , k), and because dirichlet is
conjugate to multinomial, we have:

  d|z,        dirichlet(   + cd)

where cd is s k-dim vector based on counts from z, with cd
the number of items in document d with topic k.

k is

30 / 1

lda gibbs sampler

(cid:73) for each k     {1,       , k}, sample   k based on z, w,   
(cid:73) sample   d based on z, w,   
(cid:73) sample zd,1 based on z   d,1,   d,   , w
(cid:73) sample zd,2 based on z   d,2,   d,   , w
(cid:73) . . .
we need to sample   d from p(  |z,   ).

  d|z,        dirichlet(   + cd)

(cid:73) there are algorithms for sampling from dirichlet, but we

don   t need to actualy used them.

(cid:73) instead, we will use the the collapsed gibbs sampler.

31 / 1

collapsed gibbs sampler

recall:

(cid:73) inputs:   ,   , k
(cid:73) obeserved variables: words, w = wd,n
(cid:73) unobserved:    =   1, . . . ,   d,    =   1, . . . ,   k, z = zd,n

we need to sample from
p(z,   ,   |w,   ,   )
but actually, we are ok with just z. can we get rid of   ,   ?
(cid:73) if   ,    were discrete, we could marginalize over them.

32 / 1

collapsed gibbs sampler

recall:

(cid:73) inputs:   ,   , k
(cid:73) obeserved variables: words, w = wd,n
(cid:73) unobserved:    =   1, . . . ,   d,    =   1, . . . ,   k, z = zd,n

we need to sample from
p(z,   ,   |w,   ,   )
but actually, we are ok with just z. can we get rid of   ,   ?
(cid:73) if   ,    were discrete, we could marginalize over them.
(cid:73) but they are continuous, so instead we need to integrate

p(z|w,   ,   ) =(cid:90) (cid:90) p(z,   ,   |w,   ,   )d  d  

32 / 1

collapsed gibbs sampler

recall:

(cid:73) inputs:   ,   , k
(cid:73) obeserved variables: words, w = wd,n
(cid:73) unobserved:    =   1, . . . ,   d,    =   1, . . . ,   k, z = zd,n

we need to sample from
p(z,   ,   |w,   ,   )
but actually, we are ok with just z. can we get rid of   ,   ?
(cid:73) if   ,    were discrete, we could marginalize over them.
(cid:73) but they are continuous, so instead we need to integrate

p(z|w,   ,   ) =(cid:90) (cid:90) p(z,   ,   |w,   ,   )d  d  

32 / 1

collapsed gibbs sampler

p(zd,i = k|w, z   d,i,   ,   ) =(cid:90) (cid:90) p(zd,i = k,   ,    | w, z   d,i,   ,   )d  d  
=(cid:90) p(zd,i = k|  )p(  |  )d  (cid:90) p(wd,i = v|w   d,i, zd,i = k, z   d,i,   )p(  |  )d  

you don   t really need to know how to integrate!
just remember that for dirichlet:

(cid:90) p(x|data,   )p(  |  )d   =

cx +   

|data| + k  

where cx is the count of event x in the data, and |data| =(cid:80)x(cid:48) c(cid:48)

is the number of samples in the data.

x

33 / 1

collapsed gibbs sampler

just remember that for dirichlet:

(cid:90) p(x|data,   )p(  |  )d   =

cx +   

|data| + k  

x

is the number of samples in the data.

where cx is the count of event x in the data, and |data| =(cid:80)x(cid:48) c(cid:48)
use this rule twice (once for each(cid:82) ), and get:

cd
k +   
k(cid:48) + k  

p(zd,i = k|z   d,i,   ,   , wi) =

vk
wi +   
wi(cid:48) + |v|  
k number of words in doc d with topic k in z   d,i
cd
vk
wi number of times word wi is assigned to topic k in
z   d,i

(cid:80)k(cid:48) cd

(cid:80)i(cid:48) vk

k number of topics
|v| vocabulary size

34 / 1

collapsed gibbs sampler

rule of thumb
in id113 land:

p(xn = k|x1, x2, . . . , xn   1) =

in dirichlet-prior    land:

count(k)
n     1

p(xn = k|x1, x2, . . . , xn   1,   ) =

count(k) +   
n     1 + k  

derivation in mackay and peto (1994)

35 / 1

collapsed gibbs sampler

(cid:73) initialize random topics z
(cid:73) for many iterations, for each document d, for each word i:

(cid:73) forget zd,i getting z   d,i
(cid:73) sample new assignment for zd,i based on equation below.

cd
k +   
k(cid:48) + k  

p(zd,i = k|z   d,i,   ,   , wi) =

vk
wi +   
wi(cid:48) + |v|  
k number of words in doc d with topic k in z   d,i
cd
vk
wi number of times word wi is assigned to topic k in
z   d,i

(cid:80)k(cid:48) cd

(cid:80)i(cid:48) vk

k number of topics
|v| vocabulary size

36 / 1

lda evaluation

we have topics, are they good?

37 / 1

lda evaluation

internal evaluation
if we want to compare two different lda models on the same
data:

(cid:73) compare the id203 that is assigned to the data by

each model.
(cid:73) higher id203     better model

38 / 1

lda evaluation

internal evaluation
if we want to compare two different lda models on the same
data:

(cid:73) compare the id203 that is assigned to the data by

each model.
(cid:73) higher id203     better model
(cid:73) but this does not tell us much about how useful the topics
are. . .

38 / 1

lda evaluation

internal evaluation
if we want to compare two different lda models on the same
data:

(cid:73) compare the id203 that is assigned to the data by

each model.
(cid:73) higher id203     better model
(cid:73) but this does not tell us much about how useful the topics
are. . .

external (task-based) evaluation

(cid:73) use the lda topics as features in another task
(cid:73) measure the accuracy of the other task

38 / 1

lda evaluation

internal evaluation
if we want to compare two different lda models on the same
data:

(cid:73) compare the id203 that is assigned to the data by

each model.
(cid:73) higher id203     better model
(cid:73) but this does not tell us much about how useful the topics
are. . .

external (task-based) evaluation

(cid:73) use the lda topics as features in another task
(cid:73) measure the accuracy of the other task
(cid:73) good! but we need to have a task that we can

automatically measure.

38 / 1

lda evaluation

human evaluation
if we just want to know if our topics are    good    we can ask
people.

(cid:73) but what is a good topic?

39 / 1

lda evaluation

human evaluation
if we just want to know if our topics are    good    we can ask
people.

(cid:73) but what is a good topic?
(cid:73)    intruder detection   

(cid:73) take top words from a topic.
(cid:73) insert a random word which is high in another topic.
(cid:73) can a human identify the random word?
(cid:73) yes     good topic

39 / 1

other applications of lda

40 / 1

change the de   nition of document

selectional preferences
take parsed corpus:

documents each verb is a document

words each subject of a verb is a    word    in the document
topics each topic is one    kind    of arguments

41 / 1

42 / 1

model is slightly different - topic generates two groups of things.

(how would you change the gibbs sampler?)

43 / 1

change the de   nition of document

beyond nlp
dataset of users who watched movies

documents each user is a document

words each movie is a word
topics each topic is a    taste    or    genre   

(cid:73) high topic-word prob: movie belong to genre
(cid:73) high topic-doc prob: user likes genre

can recommend new movies to users

44 / 1

extendingldabyemerginggroups.bothmodalitiesaredrivenbythecommongoalofincreasingdatalikelihood.considerthevotingexampleagain;resolutionsthatwouldhavebeenas-signedthesametopicinamodelusingwordsalonemaybeassignedtodi   erenttopicsiftheyexhibitdistinctvotingpatterns.distinctword-basedtopicsmaybemergediftheentitiesvoteverysimilarlyonthem.likewise,multipledif-ferentdivisionsofentitiesintogroupsaremadepossiblebyconditioningthemonthetopics.theimportanceofmodelingthelanguageassociatedwithinteractionsbetweenpeoplehasrecentlybeendemonstratedintheauthor-recipient-topic(art)model[16].inartthewordsinamessagebetweenpeopleinanetworkaregeneratedconditionedontheauthor,recipientandasetoftopicsthatdescribesthemessage.themodelthuscap-turesboththenetworkstructurewithinwhichthepeopleinteractaswellasthelanguageassociatedwiththeinter-actions.inexperimentswithenronandacademicemail,theartmodelisabletodiscoverrolesimilarityofpeoplebetterthansnamodelsthatconsidernetworkconnectivityalone.however,theartmodeldoesnotexplicitlycapturegroupsformedbyentitiesinthenetwork.thegtmodelsimultaneouslyclustersentitiestogroupsandclusterswordsintotopics,unlikemodelsthatgener-atetopicssolelybasedonworddistributionssuchaslatentdirichletallocation[4].inthiswaythegtmodeldiscov-erssalienttopicsrelevanttorelationshipsbetweenentitiesinthesocialnetwork   topicswhichthemodelsthatonlyexaminewordsareunabletodetect.wedemonstratethecapabilitiesofthegtmodelbyap-plyingittotwolargesetsofvotingdata:onefromussen-ateandtheotherfromthegeneralassemblyoftheun.themodelclustersvotingentitiesintocoalitionsandsi-multaneouslydiscoverstopicsforwordattributesdescribingtherelations(billsorresolutions)betweenentities.we   ndthatthegroupsobtainedfromthegtmodelaresigni   -cantlymorecohesive(p-value<.01)thanthoseobtainedfromtheblockstructuresmodel.thegtmodelalsodis-coversnewandmoresalienttopicsinboththeunandsen-atedatasets   incomparisonwithtopicsdiscoveredbyonlyexaminingthewordsoftheresolutions,thegttopicsareeithersplitorjoinedtogetherasin   uencedbythevoters   patternsofbehavior.2.group-topicmodelthegroup-topicmodelisadirectedgraphicalmodelthatclustersentitieswithrelationsbetweenthem,aswellasat-tributesofthoserelations.therelationsmaybeeitherdi-rectedorundirectedandhavemultipleattributes.inthispaper,wefocusonundirectedrelationsandhavewordsastheattributesonrelations.inthegenerativeprocessforeachevent(aninteractionbetweenentities),themodel   rstpicksthetopictoftheeventandthengeneratesallthewordsdescribingtheeventwhereeachwordisgeneratedindependentlyaccordingtoamultinomialdistribution  t,speci   ctothetopict.togeneratetherelationalstructureofthenetwork,   rstthegroupassignment,gstforeachentitysischosencondition-allyonthetopic,fromaparticularmultinomialdistribution  tovergroupsforeachtopict.giventhegroupassignmentsonaneventb,thematrixv(b)isgeneratedwhereeachcellv(b)gigjrepresentshowoftenthegroupsoftwosenatorsbe-havedthesameornotduringtheeventb,(e.g.,votedthesymboldescriptiongitentityi   sgroupassignmentintopicttbtopicofaneventbw(b)kthekthtokenintheeventbv(b)ijentityiandj   sgroupsbehavedsame(1)ordi   erently(2)ontheeventbsnumberofentitiestnumberoftopiid19numberofgroupsbnumberofeventsvnumberofuniquewordsnbnumberofwordtokensintheeventbsbnumberofentitieswhoparticipatedintheeventbtable1:notationusedinthispaper!"wv#$tg%&nbsb2tbg2stbfigure1:thegroup-topicmodelsameornotonabill).theelementsofvaresampledfromabinomialdistribution  (b)gigj.ournotationissummarizedintable1,andthegraphicalmodelrepresentationofthemodelisshowninfigure1.withoutconsideringthetopicofanevent,orbytreat-ingalleventsinacorpusasre   ectingasingletopic,thesimpli   edmodel(onlytherightpartoffigure1)becomesequivalenttothestochasticblockstructuresmodel[17].tomatchtheblockstructuresmodel,eacheventde   nesare-lationship,e.g.,whetherintheeventtwoentities   groupsbehavethesameornot.ontheotherhand,inourmodelarelationmayhavemultipleattributes(whichinourexper-imentsarethewordsdescribingtheevent,generatedbyaper-topicmultinomial).whenweconsiderthecompletemodel,thedatasetisdy-namicallydividedintotsub-blockseachofwhichcorre-spondstoatopic.thecompletegtmodelisasfollows,tb   uniform(1t)wit|  t   multinomial(  t)  t|     dirichlet(  )git|  t   multinomial(  t)  t|     dirichlet(  )v(b)ij|  (b)gigj   binomial(  (b)gigj)  (b)gh|     beta(  ).wewanttoperformjointid136on(text)attributesandrelationstoobtaintopic-wisegroupmemberships.sinceid136cannotbedoneexactlyonsuchcomplicatedprob-abilisticgraphicalmodels,weemploygibbssamplingtocon-ductid136.notethatweadoptconjugatepriorsinourindianbuffetprocesscompounddirichletprocessbselectsasubsetofatomsforeachdistribution,andthegammarandomvariables  determinetherelativemassesassociatedwiththeseatoms.2.4.focusedtopicmodelssupposehparametrizesdistributionsoverwords.then,theicdde   nesagenerativetopicmodel,whereitisusedtogenerateasetofsparsedistributionsoveranin   nitenum-berofcomponents,called   topics.   eachtopicisdrawnfromadirichletdistributionoverwords.inordertospecifyafullygenerativemodel,wesamplethenumberofwordsforeachdocumentfromanegativebinomialdistribution,n(m)     nb(   kbmk  k,1/2).2thegenerativemodelformdocumentsis1.fork=1,2,...,(a)samplethesticklength  kaccordingtoeq.1.(b)sampletherelativemass  k   gamma(  ,1).(c)drawthetopicdistributionoverwords,  k   dirichlet(  ).2.form=1,...,m,(a)sampleabinaryvectorbmaccordingtoeq.1.(b)drawthetotalnumberofwords,n(m)     nb(   kbmk  k,1/2).(c)samplethedistributionovertopics,  m   dirichlet(bm    ).(d)foreachwordwmi,i=1,...,n(m)  ,i.drawthetopicindexzmi   discrete(  m).ii.drawthewordwmi   discrete(  zmi).wecallthisthefocusedtopicmodel(ftm)becausethein   nitebinarymatrixbservestofocusthedistributionovertopicsontoa   nitesubset(seefigure1).thenumberoftopicswithinasingledocumentisalmostsurely   nite,thoughthetotalnumberoftopicsisunbounded.thetopicdistributionforthemthdocument,  m,isdrawnfromadirichletdistributionoverthetopicsselectedbybm.thedirichletdistributionmodelsuncertaintyabouttopicpro-portionswhilemaintainingtherestrictiontoasparsesetoftopics.theicdmodelsthedistributionovertheglobaltopicpro-portionparameters  separatelyfromthedistributionoverthebinarymatrixb.thiscapturestheideathatatopicmayappearinfrequentlyinacorpus,butmakeupahighpropor-tionofthosedocumentsinwhichitoccurs.conversely,atopicmayappearfrequentlyinacorpus,butonlywithlowproportion.2notationn(m)kisthenumberofwordsassignedtothekthtopicofthemthdocument,andweuseadotnotationtorepresentsummation-i.e.n(m)  =pkn(m)k.figure1.graphicalmodelforthefocusedtopicmodel3.relatedmodelstitsias(2007)introducedthein   nitegamma-poissonpro-cess,adistributionoverunboundedmatricesofnon-negativeintegers,anduseditasthebasisforatopicmodelofimages.inthismodel,thedistributionoverfeaturesforthemthimageisgivenbyadirichletdistributionoverthenon-negativeelementsofthemthrowofthein   nitegamma-poissonprocessmatrix,withparameterspropor-tionaltothevaluesattheseelements.whilethisresultsinasparsematrixofdistributions,thenumberofzeroentriesinanycolumnofthematrixiscorrelatedwiththevaluesofthenon-zeroentries.columnswhichhaveentrieswithlargevalueswillnottypicallybesparse.therefore,thismodelwillnotdecoupleacross-dataprevalenceandwithin-dataproportionsoftopics.intheicdthenumberofzeroentriesiscontrolledbyaseparateprocess,theibp,fromthevaluesofthenon-zeroentries,whicharecontrolledbythegammarandomvariables.thesparsetopicmodel(sparsetm,wang&blei,2009)usesa   nitespikeandslabmodeltoensurethateachtopicisrepresentedbyasparsedistributionoverwords.thespikesaregeneratedbybernoullidrawswithasingletopic-wideparameter.thetopicdistributionisthendrawnfromasymmetricdirichletdistributionde   nedoverthesespikes.theicdalsousesaspikeandslabapproach,butallowsanunboundednumberof   spikes   (duetotheibp)andamoregloballyinformative   slab   (duetothesharedgammarandomvariables).weextendthesparsetm   sapproxima-tionoftheexpectationofa   nitemixtureofdirichletdis-tributions,toapproximatethemorecomplicatedmixtureofdirichletdistributionsgivenineq.2.recentworkbyfoxetal.(2009)usesdrawsfromanibptoselectsubsetsofanin   nitesetofstates,tomodelmulti-pledynamicsystemswithsharedstates.(astateinthedy-namicsystemislikeacomponentinamixedmembershipmodel.)theid203oftransitioningfromtheithstatetothejthstateinthemthdynamicsystemisdrawnfromadirichletdistributionwithparametersbmj  +    i,j,wherechang,blei!nd"dwd,nzd,nk#kyd,d'$nd'"d'wd',nzd',nfigure2:atwo-documentsegmentofthertm.thevariableyindicateswhetherthetwodocumentsarelinked.thecompletemodelcontainsthisvariableforeachpairofdocuments.theplatesindicatereplication.thismodelcapturesboththewordsandthelinkstructureofthedatashowninfigure1.formulation,inspiredbythesupervisedldamodel(bleiandmcauliffe2007),ensuresthatthesamelatenttopicas-signmentsusedtogeneratethecontentofthedocumentsalsogeneratestheirlinkstructure.modelswhichdonotenforcethiscoupling,suchasnallapatietal.(2008),mightdividethetopicsintotwoindependentsubsets   oneforlinksandtheotherforwords.suchadecompositionpre-ventsthesemodelsfrommakingmeaningfulpredictionsaboutlinksgivenwordsandwordsgivenlinks.insec-tion4wedemonstrateempiricallythatthertmoutper-formssuchmodelsonthesetasks.3id136,estimation,andpredictionwiththemodelde   ned,weturntoapproximateposte-riorid136,parameterestimation,andprediction.wedevelopavariationalid136procedureforapproximat-ingtheposterior.weusethisprocedureinavariationalexpectation-maximization(em)algorithmforparameterestimation.finally,weshowhowamodelwhoseparame-tershavebeenestimatedcanbeusedasapredictivemodelofwordsandlinks.id136inposteriorid136,weseektocomputetheposteriordistributionofthelatentvariablescondi-tionedontheobservations.exactposteriorid136isin-tractable(bleietal.2003;bleiandmcauliffe2007).weappealtovariationalmethods.invariationalmethods,wepositafamilyofdistributionsoverthelatentvariablesindexedbyfreevariationalpa-rameters.thoseparametersare   ttobeclosetothetrueposterior,whereclosenessismeasuredbyrelativeid178.seejordanetal.(1999)forareview.weusethefully-factorizedfamily,q(  ,z|  ,  )=   d[q  (  d|  d)   nqz(zd,n|  d,n)],(3)where  isasetofdirichletparameters,oneforeachdoc-ument,and  isasetofmultinomialparameters,oneforeachwordineachdocument.notethateq[zd,n]=  d,n.minimizingtherelativeid178isequivalenttomaximiz-ingthejensen   slowerboundonthemarginalid203oftheobservations,i.e.,theevidencelowerbound(elbo),l=   (d1,d2)eq[logp(yd1,d2|zd1,zd2,  ,  )]+   d   neq[logp(wd,n|  1:k,zd,n)]+   d   neq[logp(zd,n|  d)]+   deq[logp(  d|  )]+h(q),(4)where(d1,d2)denotesalldocumentpairs.the   rsttermoftheelbodifferentiatesthertmfromlda(bleietal.2003).theconnectionsbetweendocumentsaffecttheob-jectiveinapproximateposteriorid136(and,below,inparameterestimation).wedeveloptheid136procedureundertheassumptionthatonlyobservedlinkswillbemodeled(i.e.,yd1,d2isei-ther1orunobserved).1wedothisfortworeasons.first,whileonecan   xyd1,d2=1wheneveralinkisob-servedbetweend1andd2andsetyd1,d2=0otherwise,thisapproachisinappropriateincorporawheretheabsenceofalinkcannotbeconstruedasevidenceforyd1,d2=0.inthesecases,treatingtheselinksasunobservedvariablesismorefaithfultotheunderlyingsemanticsofthedata.forexample,inlargesocialnetworkssuchasfacebooktheab-senceofalinkbetweentwopeopledoesnotnecessarilymeanthattheyarenotfriends;theymayberealfriendswhoareunawareofeachother   sexistenceinthenetwork.treatingthislinkasunobservedbetterrespectsourlackofknowledgeaboutthestatusoftheirrelationship.second,treatingnon-linkslinksashiddendecreasesthecomputationalcostofid136;sincethelinkvariablesareleavesinthegraphicalmodeltheycanberemovedwhen-1sumsoverdocumentpairs(d1,d2)areunderstoodtorangeoverpairsforwhichalinkhasbeenobserved.!!t"#k$k%m&d!d'parse trees grouped into m documents(a)overallgraphicalmodelw1:laidw2:phrasesw6:forw5:hisw4:somew5:mindw7:yearsw3:inz1z2z3z4z5z5z6z7(b)sentencegraphicalmodelfigure1:inthegraphicalmodelofthestm,adocumentismadeupofanumberofsentences,representedbyatreeoflatenttopicszwhichinturngeneratewordsw.thesewords   topicsarechosenbythetopicoftheirparent(asencodedbythetree),thetopicweightsforadocument  ,andthenode   sparent   ssuccessorweights  .(forclarity,notalldependenciesofsentencenodesareshown.)thestructureofvariablesforsentenceswithinthedocumentplateisontheright,asdemonstratedbyanautomaticparseofthesentence   somephraseslaidinhismindforyears.   thestmassumesthatthetreestructureandwordsaregiven,butthelatenttopicszarenot.isgoingtobeanounconsistentastheobjectofthepreposition   of.   thematically,becauseitisinatravelbrochure,wewouldexpecttoseewordssuchas   acapulco,      costarica,   or   australia   morethan   kitchen,      debt,   or   pocket.   ourmodelcancapturethesekindsofregularitiesandexploittheminpredictiveproblems.previouseffortstocapturelocalsyntacticcontextincludesemanticspacemodels[6]andsimilarityfunctionsderivedfromdependencyparses[7].thesemethodssuccessfullydeterminewordsthatsharesimilarcontexts,butdonotaccountforthematicconsistency.theyhavedif   cultywithpol-ysemouswordssuchas      y,   whichcanbeeitheraninsectoratermfrombaseball.withasenseofdocumentcontext,i.e.,arepresentationofwhetheradocumentisaboutsportsoranimals,themeaningofsuchtermscanbedistinguished.othertechniqueshaveattemptedtocombinelocalcontextwithdocumentcoherenceusinglinearsequencemodels[8,9].whilethesemodelsarepowerful,orderingwordssequentiallyremovestheimportantconnectionsthatarepreservedinasyntacticparse.moreover,thesemodelsgener-atewordseitherfromthesyntacticorthematiccontext.inthesyntactictopicmodel,wordsareconstrainedtobeconsistentwithboth.theremainderofthispaperisorganizedasfollows.wedescribethesyntactictopicmodel,anddevelopanapproximateposteriorid136techniquebasedonvariationalmethods.westudyitsperformancebothonsyntheticdataandhandparseddata[10].weshowthatthestmcapturesrelationshipsmissedbyothermodelsandachieveslowerheld-outperplexity.2thesyntactictopicmodelwedescribethesyntactictopicmodel(stm),adocumentmodelthatcombinesobservedsyntacticstructureandlatentthematicstructure.tomotivatethismodel,wereturntothetravelbrochuresentence   inthenearfuture,youcould   ndyourselfin.   .thewordthat   llsintheblankisconstrainedbyitssyntacticcontextanditsdocumentcontext.thesyntacticcontexttellsusthatitisanobjectofapreposition,andthedocumentcontexttellsusthatitisatravel-relatedword.thestmattemptstocapturethesejointin   uencesonwords.itmodelsadocumentcorpusasexchangeablecollectionsofsentences,eachofwhichisassociatedwithatreestructuresuchasa2thisprovidesaninferentialspeed-upthatmakesitpossibleto   tmodelsatvaryinggranularities.asex-amples,journalarticlesmightbeexchangeablewithinanissue,anassumptionwhichismorerealisticthanonewheretheyareexchangeablebyyear.otherdata,suchasnews,mightexperienceperiodsoftimewithoutanyobservation.whiletheddtmrequiresrepresent-ingalltopicsforthediscretetickswithintheseperiods,thecdtmcananalyzesuchdatawithoutasacri   ceofmemoryorspeed.withthecdtm,thegranularitycanbechosentomaximizemodel   tnessratherthantolimitcomputationalcomplexity.wenotethatthecdtmandddtmarenottheonlytopicmodelstotaketimeintoconsideration.topicsovertimemodels(tot)[23]anddynamicmixturemodels(dmm)[25]alsoincludetimestampsintheanalysisofdocuments.thetotmodeltreatsthetimestampsasobservationsofthelatenttopics,whiledmmassumesthatthetopicmixtureproportionsofeachdocumentisdependentonprevioustopicmix-tureproportions.inbothtotanddmm,thetopicsthemselvesareconstant,andthetimeinformationisusedtobetterdiscoverthem.inthesettinghere,weareinterestedininferringevolvingtopics.therestofthepaperisorganizedasfollows.insec-tion2wedescribetheddtmanddevelopthecdtmindetail.section3presentsane   cientposteriorin-ferencealgorithmforthecdtmbasedonsparsevaria-tionalmethods.insection4,wepresentexperimentalresultsontwonewscorpora.2continuoustimedynamictopicmodelsinatimestampeddocumentcollection,wewouldliketomodelitslatenttopicsaschangingthroughthecourseofthecollection.innewsdata,forexample,asingletopicwillchangeasthestoriesassociatedwithitdevelop.thediscrete-timedynamictopicmodel(ddtm)buildsontheexchangeabletopicmodeltoprovidesuchmachinery[2].intheddtm,documentsaredividedintosequentialgroups,andthetopicsofeachsliceevolvefromthetopicsofthepreviousslice.documentsinagroupareassumedexchangeable.morespeci   cally,atopicisrepresentedasadistribu-tionoverthe   xedvocabularyofthecollection.theddtmassumesthatadiscrete-timestatespacemodelgovernstheevolutionofthenaturalparametersofthemultinomialdistributionsthatrepresentthetopics.(recallthatthenaturalparametersofthemultino-mialarethelogsoftheprobabilitiesofeachitem.)thisisatime-seriesextensiontothelogisticnormaldistribution[26].figure1:graphicalmodelrepresentationofthecdtm.theevolutionofthetopicparameters  tisgovernedbybrownianmotion.thevariablestistheobservedtimestampofdocumentdt.adrawbackoftheddtmisthattimeisdiscretized.iftheresolutionischosentobetoocoarse,thentheassumptionthatdocumentswithinatimestepareex-changeablewillnotbetrue.iftheresolutionistoo   ne,thenthenumberofvariationalparameterswillex-plodeasmoretimepointsareadded.choosingthedis-cretizationshouldbeadecisionbasedonassumptionsaboutthedata.however,thecomputationalconcernsmightpreventanalysisattheappropriatetimescale.thus,wedevelopthecontinuoustimedynamictopicmodel(cdtm)formodelingsequentialtime-seriesdatawitharbitrarygranularity.thecdtmcanbeseenasanaturallimitoftheddtmatits   nestpos-sibleresolution,theresolutionatwhichthedocumenttimestampsaremeasured.inthecdtm,westillrepresenttopicsintheirnaturalparameterization,butweusebrownianmotion[14]tomodeltheirevolutionthroughtime.leti,j(j>i>0)betwoarbitrarytimeindexes,siandsjbethetimestamps,and   sj,sibetheelapsedtimebetweenthem.inak-topiccdtmmodel,thedistributionofthekth(1   k   k)topic   sparameterattermwis:  0,k,w   n(m,v0)  j,k,w|  i,k,w,s   n     i,k,w,v   sj,si   ,(1)wherethevarianceincreaseslinearlywiththelag.thisconstructionisusedasacomponentinthefullgenerativeprocess.(note:ifj=i+1,wewrite   sj,sias   sjforshort.)1.foreachtopick,1   k   k,(a)draw  0,k   n(m,v0i).(a)(b)figure1:(a)ldamodel.(b)mg-ldamodel.isstillnotdirectlydependentonthenumberofdocumentsand,therefore,themodelisnotexpectedtosu   erfromover-   tting.anotherapproachistouseamarkovchainmontecarloalgorithmforid136withlda,asproposedin[14].insection3wewilldescribeamodi   cationofthissamplingmethodfortheproposedmulti-grainldamodel.bothldaandplsamethodsusethebag-of-wordsrep-resentationofdocuments,thereforetheycanonlyexploreco-occurrencesatthedocumentlevel.thisis   ne,providedthegoalistorepresentanoveralltopicofthedocument,butourgoalisdi   erent:extractingratableaspects.themaintopicofallthereviewsforaparticularitemisvirtu-allythesame:areviewofthisitem.therefore,whensuchtopicmodelingmethodsareappliedtoacollectionofre-viewsfordi   erentitems,theyinfertopicscorrespondingtodistinguishingpropertiesoftheseitems.e.g.whenappliedtoacollectionofhotelreviews,thesemodelsarelikelytoin-fertopics:hotelsinfrance,newyorkhotels,youthhostels,or,similarly,whenappliedtoacollectionofmp3players   reviews,thesemodelswillinfertopicslikereviewsofipodorreviewsofcreativezenplayer.thoughtheseareallvalidtopics,theydonotrepresentratableaspects,butratherde-   neid91softherevieweditemsintospeci   ctypes.infurtherdiscussionwewillrefertosuchtopicsasglobaltopics,becausetheycorrespondtoaglobalpropertyoftheobjectinthereview,suchasitsbrandorbaseofoperation.dis-coveringtopicsthatcorrelatewithratableaspects,suchascleanlinessandlocationforhotels,ismuchmoreproblem-aticwithldaorplsamethods.mostofthesetopicsarepresentinsomewayineveryreview.therefore,itisdi   culttodiscoverthembyusingonlyco-occurrenceinformationatthedocumentlevel.inthiscaseexceedinglylargeamountsoftrainingdataisneededandaswellasaverylargenum-beroftopicsk.eveninthiscasethereisadangerthatthemodelwillbeover   ownbyvery   ne-grainglobaltopicsortheresultingtopicswillbeintersectionofglobaltopicsandratableaspects,likelocationforhotelsinnewyork.wewillshowinsection4thatthishypothesisiscon   rmedexperimentally.onewaytoaddressthisproblemwouldbetoconsiderco-occurrencesatthesentencelevel,i.e.,applyldaorplsatoindividualsentences.butinthiscasewewillnothaveasuf-   cientco-occurrencedomain,anditisknownthatldaandplsabehavebadlywhenappliedtoveryshortdocuments.thisproblemcanbeaddressedbyexplicitlymodelingtopictransitions[5,15,33,32,28,16],butthesetopicid165modelsareconsiderablymorecomputationallyexpensive.also,likeldaandplsa,theywillnotbeabletodistin-guishbetweentopicscorrespondingtoratableaspectsandglobaltopicsrepresentingpropertiesoftherevieweditem.inthefollowingsectionwewillintroduceamethodwhichexplicitlymodelsbothtypesoftopicsande   cientlyinfersratableaspectsfromlimitedamountoftrainingdata.2.2mg-ldaweproposeamodelcalledmulti-grainlda(mg-lda),whichmodelstwodistincttypesoftopics:globaltopicsandlocaltopics.asinplsaandlda,thedistributionofglobaltopicsis   xedforadocument.however,thedistributionoflocaltopicsisallowedtovaryacrossthedocument.awordinthedocumentissampledeitherfromthemixtureofglobaltopicsorfromthemixtureoflocaltopicsspeci   cforthelocalcontextoftheword.thehypothesisisthatratableaspectswillbecapturedbylocaltopicsandglobaltopicswillcapturepropertiesofrevieweditems.forexamplecon-sideranextractfromareviewofalondonhotel:   ...publictransportinlondonisstraightforward,thetubestationisaboutan8minutewalk...oryoucangetabusfor  1.50   .itcanbeviewedasamixtureoftopiclondonsharedbytheentirereview(words:   london   ,   tube   ,        ),andtheratableaspectlocation,speci   cforthelocalcontextofthesentence(words:   transport   ,   walk   ,   bus   ).localtopicsareexpectedtobereusedbetweenverydi   erenttypesofitems,whereasglobaltopicswillcorrespondonlytopartic-ulartypesofitems.inordertocaptureonlygenuinelocaltopics,weallowalargenumberofglobaltopics,e   ectively,creatingabottleneckattheleveloflocaltopics.ofcourse,thisbottleneckisspeci   ctoourpurposes.otherapplica-tionsofmulti-graintopicmodelsconceivablymightpreferthebottleneckreversed.finally,wenotethatourde   nitionofmulti-grainissimplyfortwo-levelsofgranularity,globalandlocal.inprinciplethough,thereisnothingpreventingthemodeldescribedinthissectionfromextendingbeyondtwolevels.onemightexpectthatforothertasksevenmorelevelsofgranularitycouldbebene   cial.werepresentadocumentasasetofslidingwindows,eachcoveringtadjacentsentenceswithinit.eachwindowvindocumentdhasanassociateddistributionoverlocaltopics  locd,vandadistributionde   ningpreferenceforlocaltopicsversusglobaltopics  d,v.awordcanbesampledusinganywindowcoveringitssentences,wherethewindowischosenaccordingtoacategoricaldistribution  s.importantly,thefactthatthewindowsoverlap,permitstoexploitalargerco-occurrencedomain.thesesimpletechniquesarecapableofmodelinglocaltopicswithoutmoreexpensivemodelingoftopicstransitionsusedin[5,15,33,32,28,16].introductionofasymmetricaldirichletpriordir(  )forthedistribution  spermitstocontrolsmoothnessoftopictransitionsinourmodel.theformalde   nitionofthemodelwithkglglobalandkloclocaltopicsisthefollowing.first,drawkglworddistributionsforglobaltopics  glzfromadirichletpriordir(  gl)andklocworddistributionsforlocaltopics  locz!fromdir(  loc).then,foreachdocumentd:   chooseadistributionofglobaltopics  gld   dir(  gl).   foreachsentenceschooseadistribution  d,s(v)   dir(  ).   foreachslidingwindowv113www 2008 / refereed track: data mining - modelingapril 21-25, 2008    beijing, chinamccallum,wang,&corrada-emmanuel!"zwid44(lda)[blei, ng, jordan, 2003]ndxzwauthor-topic model(at)[rosen-zvi, grif   ths, steyvers, smyth 2004]nd"#!$ta#$txzwauthor-recipient-topic model(art)[this paper]nd"#!$ta,azwauthor model(multi-label mixture model)[mccallum 1999]nd#$aadadrdadddddfigure1:threerelatedmodels,andtheartmodel.inallmodels,eachobservedword,w,isgeneratedfromamultinomialworddistribution,  z,speci   ctoaparticulartopic/author,z,howevertopicsareselecteddi   erentlyineachofthemodels.inlda,thetopicissampledfromaper-documenttopicdistribution,  ,whichinturnissampledfromadirichletovertopics.intheauthormodel,thereisonetopicassociatedwitheachauthor(orcategory),andauthorsaresampleduniformly.intheauthor-topicmodel,thetopicissampledfromaper-authormultinomialdistribution,  ,andauthorsaresampleduniformlyfromtheobservedlistofthedocument   sauthors.intheauthor-recipient-topicmodel,thereisaseparatetopic-distributionforeachauthor-recipientpair,andtheselectionoftopic-distributionisdeterminedfromtheobservedauthor,andbyuniformlysam-plingarecipientfromthesetofrecipientsforthedocument.itsgenerativeprocessforeachdocumentd,asetofauthors,ad,isobserved.togenerateeachword,anauthorxischosenuniformlyfromthisset,thenatopiczisselectedfromatopicdistribution  xthatisspeci   ctotheauthor,andthenawordwisgeneratedfromatopic-speci   cmultinomialdistribution  z.however,asdescribedpreviously,noneofthesemodelsissuitableformodelingmessagedata.anemailmessagehasonesenderandingeneralmorethanonerecipients.wecouldtreatboththesenderandtherecipientsas   authors   ofthemessage,andthenemploytheatmodel,butthisdoesnotdistinguishtheauthorandtherecipientsofthemessage,whichisundesirableinmanyreal-worldsituations.amanagermaysendemailtoasecretaryandviceversa,butthenatureoftherequestsandlanguageusedmaybequitedi   erent.evenmoredramatically,considerthelargequantityofjunkemailthatwereceive;modelingthetopicsofthesemessagesasundistinguishedfromthetopicswewriteaboutasauthorswouldbeextremelyconfoundingandundesirablesincetheydonotre   ectourexpertiseorroles.alternativelywecouldstillemploytheatmodelbyignoringtherecipientinformationofemailandtreatingeachemaildocumentasifitonlyhasoneauthor.however,inthiscase(whichissimilartotheldamodel)wearelosingallinformationabouttherecipients,andtheconnectionsbetweenpeopleimpliedbythesender-recipientrelationships.252   ldacanbeembeddedinmorecomplicatedmodels,embodyingfurtherintuitionsaboutthestructureofthetexts.   e.g.,itcanbeusedinmodelsthataccountforsyntax,authorship,wordsense,dynamics,correlation,hierarchies,andotherstructure.summary

45 / 1

unsupervised learning

(cid:73) de   ne generative story
(cid:73) include hidden (   latent   ) variables
(cid:73) find probable assignments to latent variables
(cid:73) can use id150

46 / 1

unsupervised learning

(cid:73) de   ne generative story
(cid:73) include hidden (   latent   ) variables
(cid:73) find probable assignments to latent variables
(cid:73) can use id150

id96 / lda

(cid:73) a very powerful and useful model. use it
(cid:73) generative story for lda
(cid:73) dirichlet distributions     can encourage sparsity
(cid:73) examples of lda usage
(cid:73) gibbs sampler for lda (brie   y)

(cid:73) relevant for every model with dirichlet

(cid:73) evaluation: quantify human judgement (   intruder

detection   )

(cid:73) creative de   nition of documents

46 / 1

