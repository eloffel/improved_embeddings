a note on latent semantic analysis

yoav goldberg

yoav.goldberg@gmail.com

january 18, 2014

the purpose of this document is to explain why lsa works     speci   cally,
why (and when) is it (mathematically) justi   ed to use the similarity between
term vectors or document vectors.

all of the material here appears in the highly cited paper    indexing by
latent semantic analysis    [1] (as well as other publications introducing the
lsa and lsi methods). however, it is unfortunately not discussed much in
nlp publications that use it.

1

intro to latent semantic analysis

1.1 the vector space model

the departure point of lsa (latent semantic analysis / indexing), is the
vector-space model.
we have a corpus of d documents over a vocabulary of v words. we arrange
the corpus in a matrix c of dimensions v    d, where cij is the amount of
association between word i and document j. the amount of association is
either the count, or a function based on the count such as pmi, tf-idf and
so on. while choosing the association measure is important for obtaining good
performance, it is not important to this explanation. similarly, the documents
can be generalized to any context a word appears in (e.g. same sentence, k
preceding and following words, syntactic relations, and so on).

each row in c is associated with a word and each column is associated with
a document. each word vector re   ects the contexts the word appears in, and
each document vector re   ects the words that appear in it.

based on the intuition that words appearing in similar documents (contexts)
are similar, we can measure the similarity between words by measuring the
similarity between their corresponding vectors (matrix rows). similarly, we
can measure the similarity between documents using the similarity between
their corresponding document vectors (matrix columns). one common way of
measuring similarity is the cosine similarity measure: simcos(x, y) =

(cid:104)x , y(cid:105)
(cid:107)x(cid:107)(cid:107)y(cid:107) .

1

1.2 lsa (id84)

one problem with the vector space model is that of data sparsity     some entries
in the matrix c may be incorrect because we did not observe enough data
points. lsa (latent semantic analysis / indexing) is a way of    smoothing    the
matrix: based on robust patterns in the data, some of the counts are       xed   .
this has the e   ect, for example, of adding words to contexts that they were not
seen with, if other words in this context seem to co-locate with each other.

another e   ect of lsa is representing each word (or document) as a dense
k-dimensional vector instead of a sparse d-dimensional (or v dimensional) one,
where k (cid:28) v and k (cid:28) d (typical choices are 50 < k < 300). one can then
compute similarities based on the dense k-dim vectors instead of the sparse
high-dimensional ones. the purpose of this document is to explain why the
similarities in the low-dimensional representation are equivalent to similarities
in the high-dimensional space.

2 the mathematics of lsa

2.1 the svd

lsa builds upon the mathematical technique of singular value decomposition
(svd). using svd, the matrix c is decomposed to a multiplication of three
matrices:

where u0 is v    v,   0 is diagonal v    d and v0 is d    d.

c = u0  0v0

matrices u0 and v0 are orthonormal (meaning their rows are both unit-
length and orthogonal to each other). the diagonal of   0 contain the singular
values of c, in decreasing order.

we then keep the k largest values of   0, zeroing the rest. this zeros the
corresponding rows and columns of u0 and v0 as they will contain only zeroes
after the multiplication with the modi   ed   0. after deleting the zero rows and
columns from all matrices, we are left with matrices u (v    k),    (k    k) and
v (k    d).

the product

c(cid:48) = u   v

is a (v    d) matrix of rank k. the eckart-young theorem states that c(cid:48) is the
best rank-k approximation of c, in the sense that:

c(cid:48) = arg min

m

(cid:107)m     c(cid:107)2 s.t. m is rank-k

c(cid:48) can be thought of as a smoothed version of c in the sense that it uses only
the k most in   uential directions in the data. empirically, the matrix c(cid:48) can
produce more robust similarities (when compared to human judgement on how
similar should the words / documents be) than the matrix c.

2

because the svd is unique, applying svd to c(cid:48) will reconstruct u   v (the
resulting matrices will be v   v, v   d, d   d but are equal to the matrices de   ned
above after removal of zero-valued rows and columns).
2.2 not constructing c(cid:48) explicitly
when using lsa in practice, the matrix c(cid:48) is never constructed explicitly.
instead, similarity is computed based on word- and document-vectors.

in order to compute similarities between words, we consider the word vectors:

w = u   

this is a v    k matrix, in which row wi correspond to word i in the vocabulary.
the similarity of words i and j can be computed as sim(wi, wj).

in order to compute similarities between documents, we consider the docu-

ment vectors:

d =   v

this is a k    d matrix, in which column d,i correspond to document i in the
corpus. the similarity of documents i and j can be computed as sim(d,i, d,j).
2.3 justi   cation for using w instead of c(cid:48)
why can we use w instead of c(cid:48) for computing word similarities? we assume
for now that similarities are equivalent to dot products sim(x, y) = (cid:104)x , y(cid:105). we
will show that (cid:104)c(cid:48)
j(cid:105).
i , c(cid:48)
ij = (cid:104)wi , wj(cid:105). remember that v v t = i

j(cid:105) = (cid:104)wi , wj(cid:105).
consider the similarity matrix sc(cid:48)
similarly for sw = w w t , we have sw
because v is orthonormal. now:

= c(cid:48)c(cid:48)t , in which sc(cid:48)

ij = (cid:104)c(cid:48)

i , c(cid:48)

sc(cid:48)

= c(cid:48)c(cid:48)t = (u   v )(u   v )t = (u   v )(v t   t u t )
= (u   )(v v t )(  t u t ) = (u   )(u   )t = w w t = sw

the argument for using d instead of c(cid:48) is parallel.

note that the multiplication w d does not produce c(cid:48). note also that many
works compute similarities based on either u or v directly, without multiplying
  v . under this de   nition
by   . similarly, some de   ne w = u
w d = c(cid:48) but the rows (columns) of w (d) again cannot be used instead of
   
the rows (columns) of c(cid:48). while using rows in u or u
   as the representation
of words is wrong in the mathematical sense (it is not equivalent to computing
similarities between rows in c(cid:48)), it does produce very good results empirically.

   and d =

   

   

3

references

[1] scott deerwester, susan dumais, georg furnas, thomas landauer, and
richard harshman. indexing by latent semantic analysis. journal of the
american society for information science, 41(6):391   407, 1990.

4

