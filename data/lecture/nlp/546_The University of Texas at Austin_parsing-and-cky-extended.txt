parsing, pid18s, and the cky algorithm

yoav goldberg

(some slides taken from michael collins)

november 18, 2013

1 / 48

what is grammar?

cs498jh: introduction to nlp 

6

natural language parsing

i sentences in natural language have structure.
i linguists create linguistic theories for de   ning this

structure.

i the parsing problem is recovering that structure.

2 / 48

what is grammar?
grammar formalisms
(= linguists    programming languages)
a precise way to de   ne and describe
the structure of sentences. 
(n.b.: there are many different formalisms out there, which each de   ne their 
own data structures and operations)

speci   c grammars
(= linguists    programs)
implementations (in a particular formalism) for a particular 
language (english, chinese,....)

cs498jh: introduction to nlp 

7

can we de   ne a program that 
generates all english sentences? 

the number of sentences is in   nite.
but we need our program to be    nite.

cs498jh: introduction to nlp 

8

overgeneration
english

john saw mary.

i ate sushi with tuna.

john mary saw.

 with tuna sushi ate i.

did you went there? 

....

i want you to go there.

did you go there? 

i ate the cake that john had 

made for me yesterday

john made some cake.

.....

undergeneration

cs498jh: introduction to nlp 

9

basic sentence structure

i   eat   sushi.

noun

(subject)

verb
(head)

noun
(object)

cs498jh: introduction to nlp 

10

a    nite-state-automaton (fsa)

noun 
(subject)

verb (head)

noun 
(object)

cs498jh: introduction to nlp 

12

a hidden markov model (id48)

noun 
(subject)

verb (head)

noun 
(object)

i, you, ....

eat, drink

sushi, ...

cs498jh: introduction to nlp 

13

words take arguments

i eat sushi.        
i eat sushi you. ???
i sleep sushi  ???
i give sushi   ???
i drink sushi   ? 

subcategorization: 
intransitive verbs (sleep)  take only a subject.
transitive verbs (eat) take also one (direct) object. 
ditransitive verbs (give) take also one (indirect) object.
selectional preferences: 
the object of eat should be edible.

cs498jh: introduction to nlp 

14

a better fsa

noun 
(subject)

transitive 
verb (head)

noun 
(object)

intransitive 
verb (head)

cs498jh: introduction to nlp 

15

natural language parsing

previously

i structure is a sequence.
i each item can be tagged.
i we can mark some spans.

3 / 48

natural language parsing

previously

i structure is a sequence.
i each item can be tagged.
i we can mark some spans.

today

i hierarchical structure.

3 / 48

hierarchical structure?

4 / 48

language is recursive

the ball

the big ball

the big, red ball

the big, red, heavy ball

....

adjectives can modify nouns.
the number of modi   ers/adjuncts a word can have is (in theory) 
unlimited.  

cs498jh: introduction to nlp 

16

another fsa

adjective

determiner

noun 

cs498jh: introduction to nlp 

17

recursion can be 
more complex

the ball

the ball in the garden

the ball in the garden behind the house

the ball in the garden behind the house next to the school

....

cs498jh: introduction to nlp 

18

what is the structure
 of a sentence?

sentence structure is hierarchical:
a sentence consists of words (i, eat, sushi, with, tuna)
..which form phrases or constituents:    sushi with tuna   
sentence structure de   nes dependencies
between words or phrases:

 [                                 ]  
 [                  ]   
 [                                           ]  
 [                                                   ]  
i   eat   sushi    with  tuna

cs498jh: introduction to nlp 

22

strong vs. weak 
generative capacity
formal language theory:
    de   nes language as string sets
    is only concerned with generating these strings

(weak generative capacity)

formal/theoretical syntax (in linguistics):
    de   nes language as sets of strings with (hidden) structure
    is also concerned with generating the right structures

(strong generative capacity)

cs498jh: introduction to nlp 

23

structure
example 1: math

3*2+5*3

5 / 48

structure
example 1: math

add

3*2+5*3

mul

+

mul

3 * 2

5 * 3

5 / 48

structure
example 1: math

add

3*2+5*3

mul

+

mul

3 * 2

5 * 3

+

*

*

3 2

5 3

5 / 48

constituency (phrase structure) 

trees

    phrase structure organizes words into 

nested constituents

constituency (phrase structure) 

trees

    phrase structure organizes words into 

nested constituents

    linguists can, and do, argue about details

programming languages?

6 / 48

structure
example 2: language data

fruit    ies like a banana

7 / 48

structure
example 2: language data

fruit    ies like a banana

constituency structure

dependency structure

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

like

   ies

banana

fruit

a

7 / 48

dependency representation

questioned

lawyer

witness

the

the

18 / 1

dependency representation

18 / 1

dependency representation

18 / 1

dependency representation

dependency representation is very common.
we will return to it in the future.

18 / 1

structure

constituency structure

i in this class we concentrate on constituency parsing:

mapping from sentences to trees with labeled nodes and
the sentence words at the leaves.

8 / 48

why is parsing interesting?

i it   s a    rst step towards understanding a text.
i many other language tasks use sentence structure as their

input.

9 / 48

some things that are done with parse trees

i grammar checking
i word id91
i information extraction
i id53
i sentence simpli   cation
i machine translation
i . . . and more

10 / 48

some things that are done with parse trees

words in similar grammatical
relations share meanings.

i grammar checking
i word id91
i information extraction
i id53
i sentence simpli   cation
i machine translation
i . . . and more

10 / 48

some things that are done with parse trees

extract factual relations from text

answer questions

i grammar checking
i word id91
i information extraction
i id53
i sentence simpli   cation
i machine translation
i . . . and more

10 / 48

some things that are done with parse trees

i grammar checking
i word id91
i information extraction
i id53
i sentence simpli   cation
i machine translation
i . . . and more

the    rst new product , atf
protype , is a line of digital
postscript typefaces that will be
sold in packages of up to six fonts .

atf protype is a line of digital
postscript typefaces will be sold in
packages of up to six fonts.

10 / 48

some things that are done with parse trees

reorder

insert

translate

i grammar checking
i word id91
i information extraction
i id53
i sentence simpli   cation
i machine translation
i . . . and more

10 / 48

some things that are done with parse trees

i grammar checking
i word id91
i information extraction
i id53
i sentence simpli   cation
i machine translation
i . . . and more

10 / 48

why is parsing hard?
ambiguity

fat people eat candy

11 / 48

why is parsing hard?
ambiguity

fat people eat candy

s

np

vp

adj

nn

vb

fat

people

eat

np

nn

candy

11 / 48

why is parsing hard?
ambiguity

fat people eat candy

s

np

vp

adj

nn

vb

fat

people

eat

np

nn

candy

fat people eat accumulates

11 / 48

why is parsing hard?
ambiguity

fat people eat candy

s

np

vp

adj

nn

vb

fat

people

eat

np

nn

candy

fat people eat accumulates

s

np

adjp

vp

vb

nn

vb

accumulates

nn

fat

people

eat

11 / 48

why is parsing hard?
real sentences are long. . .

   former beatle paul mccartney today was ordered to pay
nearly $50m to his estranged wife as their bitter divorce battle
came to an end .    

   welcome to our columbus hotels guide, where you   ll    nd
honest, concise hotel reviews, all discounts, a lowest rate
guarantee, and no booking fees.   

12 / 48

let   s learn how to parse

let   s learn how to parse . . . but    rst lets review some stuff we

learned at automata and formal languages.

context-free grammars (id18s) 
capture recursion

language has complex constituents
(   the garden behind the house   ) 
syntactically, these constituents behave 
just like simple ones.
(   behind the house    can always be omitted)
id18s de   ne nonterminal categories
to capture equivalent constituents. 

cs498jh: introduction to nlp 

24

id18s

a id18 g = (n,    , r, s) where:

i n is a set of non-terminal symbols
i     is a set of terminal symbols
i r is a set of rules of the form x ! y1y2        yn
for n   0, x 2 n, yi 2 (n [    )
i s 2 n is a special start symbol

14 / 48

id18s

a simple grammar
n = {s, np, vp, adj, det, vb, noun}
   = {fruit,    ies, like, a, banana, tomato, angry}
s =   s   
r =

s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry

15 / 48

left-most derivations

left-most derivation is a sequence of strings s1,       , sn where
i s1 = s the start symbol
i sn 2       , meaning sn is only terminal symbols
i each si for i = 2       n is derived from si 1 by picking the

left-most non-terminal x in si 1 and replacing it by some  
where x !   is a rule in r.
for example: [s],[np vp],[adj noun vp], [fruit noun vp], [fruit
   ies vp],[fruit    ies vb np],[fruit    ies like np], [fruit    ies like det
noun], [fruit    ies like a], [fruit    ies like a banana]

16 / 48

left-most derivation example

s

17 / 48

left-most derivation example

s
np vp

s ! np vp

17 / 48

left-most derivation example

s
np vp
adj noun vp

np ! adj noun

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp

adj ! fruit

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp

noun !    ies

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np

vp ! vb np

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np
fruit    ies like np

vb ! like

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np
fruit    ies like np
fruit    ies like det noun

np ! det noun

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np
fruit    ies like np
fruit    ies like det noun
fruit    ies like a noun

det ! a

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np
fruit    ies like np
fruit    ies like det noun
fruit    ies like a noun
fruit    ies like a banana

noun ! banana

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np
fruit    ies like np
fruit    ies like det noun
fruit    ies like a noun
fruit    ies like a banana

i the resulting derivation can be written as a tree.

17 / 48

left-most derivation example

s
np vp
adj noun vp
fruit noun vp
fruit    ies vp
fruit    ies vb np
fruit    ies like np
fruit    ies like det noun
fruit    ies like a noun
fruit    ies like a banana

i the resulting derivation can be written as a tree.
i many trees can be generated.

17 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

18 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

18 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

example

s

np

vp

adj

noun

angry

flies

vb

like

np

det

noun

a

banana

18 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

example

s

np

vp

adj

noun

angry

flies

vb

like

np

det

noun

a

tomato

18 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

example

s

np

vp

adj

noun

vb

np

angry

banana

like

det

noun

a

tomato

18 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

example

s

np

vp

det

noun

a

banana

vb

like

np

det

noun

a

tomato

18 / 48

id18s

a simple grammar
s ! np vp
np ! adj noun
np ! det noun
vp ! vb np
-
adj ! fruit
noun !    ies
vb ! like
det ! a
noun ! banana
noun ! tomato
adj ! angry
. . .

example

s

np

vp

det

noun

a

banana

vb

like

np

adj

noun

angry

banana

18 / 48

a brief introduction to english syntax

19 / 48

product details (from amazon)
hardcover: 1779 pages
publisher: longman; 2nd revised edition
language: english
isbn-10: 0582517346
isbn-13: 978-0582517349
product dimensions: 8.4 x 2.4 x 10 inches
shipping weight: 4.6 pounds

a brief overview of english syntax

parts of speech (tags from the brown corpus):

  nouns

nn = singular noun e.g., man, dog, park
nns = plural noun
nnp = proper noun e.g., smith, gates, ibm

e.g., telescopes, houses, buildings

  determiners

dt = determiner e.g., the, a, some, every

  adjectives

jj = adjective e.g., red, green, large, idealistic

a fragment of a noun phrase grammar

  n   nn
  n   nn   n
  n
  n   jj
  n     n
  n
np   dt   n

nn   box
nn   car
nn   mechanic
nn   pigeon
dt   the
dt   a

jj   fast
jj   metal
jj   idealistic
jj   clay

prepositions, and prepositional phrases

  prepositions

in = preposition e.g., of, in, out, beside, as

an extended grammar

  n   nn
  n   nn   n
  n   jj
  n
  n
  n     n
np   dt   n
pp   in
np
  n     n
pp

nn   box
nn   car
nn   mechanic
nn   pigeon
dt   the
dt   a

jj   fast
jj   metal
jj   idealistic
jj   clay
in   in
in   under
in   of
in   on
in   with
in   as

generates:
in a box, under the box, the fast car mechanic under the pigeon in
the box, . . .

an extended grammar

  n   nn
  n   nn   n
  n
  n   jj
  n     n
  n
np   dt   n
pp   in
np
  n     n
pp

verbs, verb phrases, and sentences

  basic verb types

vi = intransitive verb
vt = transitive verb
vd = ditransitive verb

e.g., sleeps, walks, laughs
e.g., sees, saw, likes
e.g., gave

  basic vp rules
vp     vi
vp     vt np
vp     vd np np

  basic s rule

s     np vp

examples of vp:
sleeps, walks, likes the mechanic, gave the mechanic the fast car
examples of s:
the man sleeps, the dog walks, the dog gave the mechanic the fast car

pps modifying verb phrases

a new rule: vp     vp pp
new examples of vp:
sleeps in the car, walks like the mechanic, gave the mechanic the
fast car on tuesday, . . .

complementizers, and sbars

  complementizers

comp = complementizer

e.g., that

  sbar

sbar     comp s

examples:
that the man sleeps, that the mechanic saw the dog . . .

more verbs

  new verb types

v[5]
v[6]
v[7]

e.g., said, reported
e.g., told, informed
e.g., bet

  new vp rules

sbar
np

vp     v[5] sbar
vp     v[6] np
vp     v[7] np
examples of new vps:
said that the man sleeps
told the dog that the mechanic likes the pigeon
bet the pigeon $50 that the mechanic owns a fast car

sbar

coordination

  a new part-of-speech:

cc = coordinator

e.g., and, or, but

  new rules

    np
      n
    vp
    s

np
  n
vp
s
sbar     sbar cc sbar

cc np
cc   n
cc vp
cc s

we   ve only scratched the surface...

  agreement

the dogs laugh vs. the dog laughs

  wh-movement

the dog that the cat liked

  active vs. passive

the dog saw the cat vs.
the cat was seen by the dog

  if you   re interested in reading more:

syntactic theory: a formal introduction, 2nd
edition. ivan a. sag, thomas wasow, and emily
m. bender.

id18s and center embedding

the mouse ate the corn.

the mouse that the snake ate ate the corn.

the mouse that the snake that the hawk ate ate ate the corn.

....

these sentences are all grammatical.
they can be generated by a id18:

s                    np    vp
np                  np   relclause
relclause      that  np ate

linguists distinguish between a speaker   s 
- competence (grammatical knowledge) and 
- performance (processing and memory limitations) 

cs498jh: introduction to nlp 

28

parsing with (p)id18s

20 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i natural language is not generated by a id18.

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i natural language is not generated by a id18.

solution

i we assume really hard that it is.

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i we don   t have the grammar.

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i we don   t have the grammar.

solution

i we   ll ask a genius linguist to write it!

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i how do we    nd the chain of derivations?

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i how do we    nd the chain of derivations?

solution

i with id145! (soon)

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i real grammar: hundreds of possible derivations per

sentence.

21 / 48

parsing with id18s

let   s assume. . .

i let   s assume natural language is generated by a id18.
i . . . and let   s assume we have the grammar.
i then parsing is easy: given a sentence,    nd the chain of

derivations starting from s that generates it.

problem

i real grammar: hundreds of possible derivations per

sentence.

solution

i no problem! we   ll choose the best one. (sooner)

21 / 48

obtaining a grammar

let a genius linguist write it

i hard. many rules, many complex interactions.
i genius linguists don   t grow on trees !

22 / 48

obtaining a grammar

let a genius linguist write it

i hard. many rules, many complex interactions.
i genius linguists don   t grow on trees !

an easier way - ask a linguist to grow trees

i ask a linguist to annotate sentences with tree structure.
i (this need not be a genius     smart is enough.)
i then extract the rules from the annotated trees.

22 / 48

obtaining a grammar

let a genius linguist write it

i hard. many rules, many complex interactions.
i genius linguists don   t grow on trees !

an easier way - ask a linguist to grow trees

i ask a linguist to annotate sentences with tree structure.
i (this need not be a genius     smart is enough.)
i then extract the rules from the annotated trees.

treebanks

i english treebank: 40k sentences, manually annotated

with tree structure.

i hebrew treebank: about 5k sentences

22 / 48

treebank sentence example

( (s

(np-sbj

(np (nnp pierre) (nnp vinken) )
(, ,)
(adjp

(np (cd 61) (nns years) )
(jj old) )

(, ,) )

(vp (md will)

(vp (vb join)

(np (dt the) (nn board) )
(pp-clr (in as)

(np (dt a) (jj nonexecutive) (nn director) ))

(np-tmp (nnp nov.) (cd 29) )))

(. .) ))

23 / 48

supervised learning from a treebank

((fruit/adj    ies/nn) (like/vb
(a/det banana/nn)))
(time/nn (   ies/vb (like/in
(an/det (arrow/nn)))))

. . . . . . . . .
. . . . . . . . .

24 / 48

extracting id18 from trees

i the leafs of the trees de   ne    
i the internal nodes of the trees de   ne n
i add a special s symbol on top of all trees
i each node an its children is a rule in r

25 / 48

extracting id18 from trees

i the leafs of the trees de   ne    
i the internal nodes of the trees de   ne n
i add a special s symbol on top of all trees
i each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

25 / 48

extracting id18 from trees

i the leafs of the trees de   ne    
i the internal nodes of the trees de   ne n
i add a special s symbol on top of all trees
i each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

s ! np vp

np

det

noun

a

banana

25 / 48

extracting id18 from trees

i the leafs of the trees de   ne    
i the internal nodes of the trees de   ne n
i add a special s symbol on top of all trees
i each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

s ! np vp
np ! adj noun

np

det

noun

a

banana

25 / 48

extracting id18 from trees

i the leafs of the trees de   ne    
i the internal nodes of the trees de   ne n
i add a special s symbol on top of all trees
i each node an its children is a rule in r

extracting rules

s

np

vp

adj

noun

fruit

flies

vb

like

s ! np vp
np ! adj noun
adj ! fruit

np

det

noun

a

banana

25 / 48

from id18 to pid18

i english is not generated from id18 ) it   s generated by a

pid18!

26 / 48

from id18 to pid18

i english is not generated from id18 ) it   s generated by a
i pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
i all probabilities for the same lhs sum to 1.

26 / 48

from id18 to pid18

i english is not generated from id18 ) it   s generated by a
i pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
i all probabilities for the same lhs sum to 1.
i multiplying all the rule probs in a derivation gives the

id203 of the derivation.

i we want the tree with maximum id203.

26 / 48

from id18 to pid18

i english is not generated from id18 ) it   s generated by a
i pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
i all probabilities for the same lhs sum to 1.
i multiplying all the rule probs in a derivation gives the

id203 of the derivation.

i we want the tree with maximum id203.

more formally

p(tree, sent) = yl!r2deriv(tree)

p(l ! r )

26 / 48

from id18 to pid18

i english is not generated from id18 ) it   s generated by a
i pid18: probabilistic id18. just like a id18,

pid18!

but each rule has an associated id203.
i all probabilities for the same lhs sum to 1.
i multiplying all the rule probs in a derivation gives the

id203 of the derivation.

i we want the tree with maximum id203.

more formally

p(tree, sent) = yl!r2deriv(tree)

p(l ! r )

tree = arg max

tree2trees(sent)

p(tree|sent) = arg max

tree2trees(sent)

p(tree, sent)

26 / 48

pid18 example

a simple pid18
1.0 s ! np vp
0.3 np ! adj noun
0.7 np ! det noun
1.0 vp ! vb np
-
0.2 adj ! fruit
0.2 noun !    ies
1.0 vb ! like
1.0 det ! a
0.4 noun ! banana
0.4 noun ! tomato
0.8 adj ! angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1   0.3   0.2   0.7   1.0   0.2   1   1   0.4 =
0.0033

27 / 48

pid18 example

a simple pid18
1.0 s ! np vp
0.3 np ! adj noun
0.7 np ! det noun
1.0 vp ! vb np
-
0.2 adj ! fruit
0.2 noun !    ies
1.0 vb ! like
1.0 det ! a
0.4 noun ! banana
0.4 noun ! tomato
0.8 adj ! angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1   0.3   0.2   0.7   1.0   0.2   1   1   0.4 =
0.0033

27 / 48

pid18 example

a simple pid18
1.0 s ! np vp
0.3 np ! adj noun
0.7 np ! det noun
1.0 vp ! vb np
-
0.2 adj ! fruit
0.2 noun !    ies
1.0 vb ! like
1.0 det ! a
0.4 noun ! banana
0.4 noun ! tomato
0.8 adj ! angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1   0.3   0.2   0.7   1.0   0.2   1   1   0.4 =
0.0033

27 / 48

pid18 example

a simple pid18
1.0 s ! np vp
0.3 np ! adj noun
0.7 np ! det noun
1.0 vp ! vb np
-
0.2 adj ! fruit
0.2 noun !    ies
1.0 vb ! like
1.0 det ! a
0.4 noun ! banana
0.4 noun ! tomato
0.8 adj ! angry

example

s

np

vp

adj

noun

fruit

flies

vb

like

np

det

noun

a

banana

1   0.3   0.2   0.7   1.0   0.2   1   1   0.4 =
0.0033

27 / 48

parsing with pid18

i parsing with a pid18 is    nding the most probable

derivation for a given sentence.

i this can be done quite ef   ciently with dynamic

programming (the cky algorithm)

28 / 48

parsing with pid18

i parsing with a pid18 is    nding the most probable

derivation for a given sentence.

i this can be done quite ef   ciently with dynamic

programming (the cky algorithm)

obtaining the probabilities

i we estimate them from the treebank.
i p(lhs ! rhs) = count(lhs!rhs)
count(lhs!   )
i we can also add id188, as before.
i dealing with unknown words - like in the id48

28 / 48

the cky algorithm

29 / 48

the problem

input

i sentence (a list of words)

i n     sentence length

i id18 grammar (with weights on rules)
i g     number of non-terminal symbols

output

i a parse tree / the best parse tree

but. . .

i exponentially many possible parse trees!

solution

i id145!

30 / 48

cky

cocke kasami younger

31 / 48

cky

cocke kasami younger
196?

31 / 48

cky

cocke kasami younger
196?

1965

31 / 48

cky

cocke kasami younger
196?

1967

1965

31 / 48

3 interesting problems

i recognition

i parsing

i disambiguation

32 / 48

3 interesting problems

i recognition

i can this string be generated by the grammar?

i parsing

i disambiguation

32 / 48

3 interesting problems

i recognition

i can this string be generated by the grammar?

i parsing

i show me a possible derivation. . .

i disambiguation

32 / 48

3 interesting problems

i recognition

i can this string be generated by the grammar?

i parsing

i show me a possible derivation. . .

i disambiguation

i show me the best derivation

32 / 48

3 interesting problems

i recognition

i can this string be generated by the grammar?

i parsing

i show me a possible derivation. . .

i disambiguation

i show me the best derivation

cky can do all of these in polynomial time

32 / 48

3 interesting problems

i recognition

i can this string be generated by the grammar?

i parsing

i show me a possible derivation. . .

i disambiguation

i show me the best derivation

cky can do all of these in polynomial time

i for any cnf grammar

32 / 48

cnf
chomsky normal form

de   nition
a id18 is in cnf form if it only has rules like:
i a ! b c
i a !    
a,b,c are non terminal symbols
    is a terminal symbol (a word. . . )

i all terminal symbols are rhs of unary rules
i all non terminal symbols are rhs of binary rules

cky can be easily extended to handle also unary rules: a ! b

33 / 48

binarization

fact

i any id18 grammar can be converted to cnf form

34 / 48

binarization

fact

i any id18 grammar can be converted to cnf form

spe   cifally for natural language grammars
i we already have a !    

34 / 48

binarization

fact

i any id18 grammar can be converted to cnf form

spe   cifally for natural language grammars
i we already have a !    

i (a !       is also easy to handle)

34 / 48

binarization

fact

i any id18 grammar can be converted to cnf form

spe   cifally for natural language grammars
i we already have a !    
i unary rules (a ! b) are ok

i (a !       is also easy to handle)

34 / 48

binarization

fact

i any id18 grammar can be converted to cnf form

spe   cifally for natural language grammars
i we already have a !    
i unary rules (a ! b) are ok
i only problem:s ! np pp vp pp

i (a !       is also easy to handle)

34 / 48

binarization

fact

i any id18 grammar can be converted to cnf form

i (a !       is also easy to handle)

spe   cifally for natural language grammars
i we already have a !    
i unary rules (a ! b) are ok
i only problem:s ! np pp vp pp
binarization
! np np|pp.vp.pp
s
np|pp.vp.pp ! pp np.pp|vp.pp
np.pp|vp.pp ! vp np.pp.vp|pp

34 / 48

finally, cky

recognition

i main idea:

i build parse tree from bottom up
i combine built trees to form bigger trees using grammar

rules

i when left with a single tree, verify root is s

i exponentially many possible trees. . .

i search over all of them in polynomial time using dp
i shared structure     smaller trees

35 / 48

main idea

if we know:

i wi . . . wj is an np
i wj+1 . . . wk is a vp
and grammar has rule:
i s ! np vp
then we know:

i s can derive wi . . . wk

36 / 48

data structure

(half a) two dimensional array (n x n)

37 / 48

data structure

on its side

38 / 48

data structure

each cell: all nonterminals than can derive word i to word j

sue

saw

her

girl

with

a

telescope

38 / 48

data structure

each cell: all nonterminals than can derive word i to word j
imagine each cell as a g dimensional array

sue

saw

her

girl

with

a

telescope

38 / 48

filling the table

sue

saw

her

girl

with

a

telescope

39 / 48

handling unary rules?

sue

saw

her

girl

with

a

telescope

40 / 48

which order?

sue

saw

her

boy

with

a

telescope

41 / 48

complexity?

42 / 48

complexity?

i n2g cells to    ll

42 / 48

complexity?

i n2g cells to    ll
i g2n ways to    ll each one

42 / 48

complexity?

i n2g cells to    ll
i g2n ways to    ll each one

o(g3n3)

42 / 48

a note on implementation

smart implementation can reduce the runtime:

i worst case is still o(g3n3), but it helps in practice
i no need to check all grammar rules a ! bc at each

location:

i only those compatible with b or c of current split
i prune binarized symbols which are too long for current

position

i once you found 1 way to derive a can break out of loop
i order grammar rules from frequent to infrequent

i need both ef   cient random access and iteration over

possible symbols

i keep both hash and list, implemented as arrays

43 / 48

finding a parse

parsing     we want to actually    nd a parse tree

easy: also keep a possible split point for each nt

44 / 48

pid18 parsing and disambiguation

disambiguation     we want the best parse tree

easy: for each nt, keep best split point, and score.

45 / 48

implementation tricks
#1: sum instead of product

as in the id48 - multiplying probabilities is evil

i keeping the product of many    oating point numbers is

dangerous, because product get really small

i either grow in runtime
i or loose precision (over   owing to 0)
i either way, multiplying    oats is expensive

solution: use sum of logs instead
i remember: log(p1     p2) = log(p1) + log(p2)
) use log probabilities instead of probabilities
) add instead of multiply

46 / 48

implementation tricks
#2: two passes

some recognition speedup tricks are no longer possible

i need the best way to derive a symbol, not just one way

) can   t abort of loops early. . .

solution: two passes

i pass 1: just recognition
i pass 2: disambiguation, with many pruned symbols at

each cell

47 / 48

summary

i hierarchical structure
i constituent / phrase-based parsing
i id18s, derivations
i (some) english syntax
i pid18
i cky

48 / 48

parsing with pid18

i parsing with a pid18 is    nding the most probable

derivation for a given sentence.

i this can be done quite ef   ciently with dynamic

programming (the cky algorithm)

obtaining the probabilities

i we estimate them from the treebank.
i q(lhs ! rhs) = count(lhs!rhs)
count(lhs!   )
i we can also add id188, as before.
i dealing with unknown words - like in the id48

7 / 1

the big question

does this work?

8 / 1

evaluation

9 / 1

parsing evaluation

i let   s assume we have a parser, how do we know how

good it is?

) compare output trees to gold trees.

10 / 1

parsing evaluation

i let   s assume we have a parser, how do we know how

good it is?

) compare output trees to gold trees.

i but how do we compare trees?
i credit of 1 if tree is correct and 0 otherwise, is too harsh.

10 / 1

parsing evaluation

i let   s assume we have a parser, how do we know how

good it is?

) compare output trees to gold trees.

i but how do we compare trees?
i credit of 1 if tree is correct and 0 otherwise, is too harsh.

i represent each tree as a set of labeled spans.

i np from word 1 to word 5.
i vp from word 3 to word 4.
i s from word 1 to word 23.
i . . .

i measure precision, recall and f1 over these spans, as in

the segmentation case.

10 / 1

evaluation: representing trees as constituents

s

np

vp

dt

nn

the

lawyer

vt

np

questioned

dt

nn

label

start point

end point

the

witness

np
np
vp
s

1
4
3
1

2
5
5
5

precision and recall

label

start point

end point

label

start point

end point

np
np
np
pp
np
vp
s

1
4
4
6
7
3
1

2
5
8
8
8
8
8

np
np
pp
np
vp
s

1
4
6
7
3
1

2
5
8
8
8
8

i g = number of constituents in gold standard = 7
i p = number in parse output = 6
i c = number correct = 6

recall = 100%    

c
g

= 100%    

6
7

precision = 100%    

c
p

= 100%    

6
6

parsing evaluation

i is this a good measure?

i why? why not?

11 / 1

parsing evaluation

how well does the pid18 parser we learned do?

not very well: about 73% f1 score.

12 / 1

problems with pid18s

13 / 1

weaknesses of id140

michael collins, columbia university

weaknesses of pid18s

i lack of sensitivity to lexical information

i lack of sensitivity to structural frequencies

s

np

vp

nnp

vt

np

ibm

bought

nnp

lotus

p(t) = q(s ! np vp)
   q(vp ! v np)
   q(np ! nnp)
   q(np ! nnp)

   q(nnp ! ibm)
   q(vt ! bought)
   q(nnp ! lotus)

another case of pp attachment ambiguity

(a)

s

np

nns

workers

vp

vp

pp

vbd

np

in

np

dumped

nns

into

dt

sacks

a

nn

bin

(b)

s

np

nns

workers

vbd

dumped

vp

np

nns

sacks

np

pp

in

np

into

dt

a

nn

bin

(a)

rules
s ! np vp
np ! nns
vp ! vp pp
vp ! vbd np
np ! nns
pp ! in np
np ! dt nn
nns ! workers
vbd ! dumped
nns ! sacks
in ! into
dt ! a
nn ! bin

(b)

rules
s ! np vp
np ! nns
np ! np pp
vp ! vbd np
np ! nns
pp ! in np
np ! dt nn
nns ! workers
vbd ! dumped
nns ! sacks
in ! into
dt ! a
nn ! bin

if q(np ! np pp) > q(vp ! vp pp) then (b) is more
probable, else (a) is more probable.
attachment decision is completely independent of the
words

a case of coordination ambiguity

np

cc

and

(a)

(b)

np

np

nns

dogs

pp

in

in

np

nns

houses

np

np

nns

cats

np

nns

dogs

in

in

pp

np

nns

houses

np

cc

and

np

nns

cats

(a)

rules
np ! np cc np
np ! np pp
np ! nns
pp ! in np
np ! nns
np ! nns
nns ! dogs
in ! in
nns ! houses
cc ! and
nns ! cats

rules
np ! np cc np
np ! np pp
np ! nns
pp ! in np
np ! nns
np ! nns
nns ! dogs
in ! in
nns ! houses
cc ! and
nns ! cats
here the two parses have identical rules, and
therefore have identical id203 under any
assignment of pid18 rule probabilities

(b)

