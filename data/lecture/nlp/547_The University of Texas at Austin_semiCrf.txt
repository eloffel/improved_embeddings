semi-markov id49 for

information extraction

sunita sarawagi

william w. cohen

indian institute of technology

center for automated learning & discovery

bombay, india

sunita@iitb.ac.in

carnegie mellon university
wcohen@cs.cmu.edu

abstract

we describe semi-markov conditional random    elds (semi-crfs), a con-
ditionally trained version of semi-markov chains.
intuitively, a semi-
crf on an input sequence x outputs a    segmentation    of x, in which
labels are assigned to segments (i.e., subsequences) of x rather than to
individual elements xi of x. importantly, features for semi-crfs can
measure properties of segments, and transitions within a segment can
be non-markovian. in spite of this additional power, exact learning and
id136 algorithms for semi-crfs are polynomial-time   often only a
small constant factor slower than conventional crfs.
in experiments
on    ve id39 problems, semi-crfs generally outper-
form conventional crfs.

1 introduction

conditional random    elds (crfs) are a recently-introduced formalism [12] for represent-
ing a conditional model pr(y|x), where both x and y have non-trivial structure (often
sequential). here we introduce a generalization of sequential crfs called semi-markov
conditional random    elds (or semi-crfs). recall that semi-markov chain models extend
id48 (id48s) by allowing each state si to persist for a non-unit length
of time di. after this time has elapsed, the system will transition to a new state s0, which
depends only on si; however, during the    segment    of time between i to i + di, the be-
havior of the system may be non-markovian. semi-markov models are fairly common in
certain applications of statistics [8, 9], and are also used in id23 to model
hierarchical id100 [19].
semi-crfs are a conditionally trained version of semi-markov chains. in this paper, we
present id136 and learning methods for semi-crfs. we also argue that segments often
have a clear intuitive meaning, and hence semi-crfs are more natural than conventional
crfs. we focus here on id39 (ner), in which a segment corresponds
to an extracted entity; however, similar arguments might be made for several other tasks,
such as gene-   nding [11] or np-chunking [16].
in ner, a semi-markov formulation allows one to easily construct entity-level features
(such as    entity length    and    similarity to other known entities   ) which cannot be easily
encoded in crfs. experiments on    ve different ner problems show that semi-crfs often
outperform conventional crfs.

2 crfs and semi-crfs

2.1 de   nitions

a crf models pr(y|x) using a markov random    eld, with nodes corresponding to ele-
ments of the structured object y, and potential functions that are conditional on (features
of) x. learning is performed by setting parameters to maximize the likelihood of a set of
(x, y) pairs given as training data. one common use of crfs is for sequential learning
problems like np chunking [16], id52 [12], and ner [15]. for these problems
the markov    eld is a chain, and y is a linear sequence of labels from a    xed set y. for in-
stance, in the ner application, x might be a sequence of words, and y might be a sequence
in {i, o}|x|, where yi = i indicates    word xi is inside a name    and yi = o indicates the
opposite.
assume a vector f of local feature functions f = hf 1, . . . , f ki, each of which maps a pair
(x, y) and an index i to a measurement f k(i, x, y)     r. let f (i, x, y) be the vector of these
f (i, x, y). for the case of ner, the components
of f might include the measurement f 13(i, x, y) = [[xi is capitalized]]    [[yi = i]], where
the indicator function [[c]] = 1 if c if true and zero otherwise; this implies that f 13(x, y)
would be the number of capitalized words xi paired with the label i. following previous
work [12, 16] we will de   ne a conditional random    eld (crf) to be an estimator of the
form

measurements, and let f(x, y) = p|x|

i

pr(y|x, w) =

1

z(x)

ew  f(x,y)

(1)

where w is a weight vector over the components of f, and z(x) =py0 ew  f(x,y0).

to extend this to the semi-markov case,
let s = hs1, . . . , spi denote a segmenta-
tion of x, where segment sj = htj, uj, yji consists of a start position tj, an end
position uj, and a label yj     y . conceptually, a segment means that the tag yj
is given to all xi   s between i = tj and i = uj, inclusive. we assume segments
have positive length, and adjacent segments touch, that is tj and uj always satisfy
1     tj     uj     |s| and tj+1 = uj + 1. for ner, a correct segmentation of
the sentence    i went skiing with fernando pereira in british columbia    might be s =
h(1, 1, o), (2, 2, o), (3, 3, o), (4, 4, o), (5, 6, i), (7, 7, o), (8, 9, i)i, corresponding to the
label sequence y = ho, o, o, o, i, i, o, i, ii.

a triple (j, x, s) to a measurement gk(j, x, s)     r, and de   ne g(x, s) = p|s|

we assume a vector g of segment feature functions g = hg1, . . . , gki, each of which maps
j g(j, x, s).
we also make a restriction on the features, analogous to the usual markovian assumption
made in crfs, and assume that every component gk of g is a function only of x, sj, and
the label yj   1 associated with the preceding segment sj   1. in other words, we assume that
every gk(j, x, s) can be rewritten as

(2)
for an appropriately de   ned g0k. in the rest of the paper, we will drop the g0 notation and
use g for both versions of the segment-level feature functions. a semi-crf is then an
estimator of the form

gk(j, x, s) = g0k(yj, yj   1, x, tj, uj)

pr(s|x, w) =

1

z(x)

ew  g(x,s)

(3)

where again w is a weight vector for g and z(x) =ps0 ew  g(x,s0).

2.2 an ef   cient id136 algorithm

the id136 problem for a semi-crf is de   ned as follows: given w and x,    nd the
best segmentation, argmax s pr(s|x, w), where pr(s|x, w) is de   ned by equation 3. an

ef   cient id136 algorithm is suggested equation 2, which implies that

argmax s pr(s|x, w) = argmax sw    g(x, s) = argmax sw   xj

g(yj, yj   1, x, tj, uj)

let l be an upper bound on segment length. let si:l denote set of all partial segmentation
starting from 1 (the    rst index of the sequence) to i, such that the last segment has the
label y and ending position i. let vx,g,w (i, y) denote the largest value of w    g(x, s0)
for any s0     si:l. omitting the subscripts, the following recursive calculation implements a
semi-markov analog of the usual viterbi algorithm:

v (i, y) =( maxy 0,d=1...l v (i     d, y0) + w    g(y, y0, x, i     d, i)

0
      

if i > 0
if i = 0
if i < 0

(4)

the best segmentation then corresponds to the path traced by maxy v (|x|, y).

2.3 semi-markov crfs vs order-l crfs

since conventional crfs need not maximize over possible segment lengths d, id136
for semi-crfs is more expensive. however, equation 4 shows that the additional cost is
only linear in l. for ner, a reasonable value of l might be four or    ve.1 since in the
worst case l     |x|, the algorithm is always polynomial, even when l is unbounded.
for    xed l, it can be shown that semi-crfs are no more expressive than order-l crfs.
for order-l crfs, however the additional computational cost is exponential in l. the
difference is that semi-crfs only consider sequences in which the same label is assigned
to all l positions, rather than all |y|bl length-l sequences. this is a useful restriction, as
it leads to faster id136.
semi-crfs are also a natural restriction, as it is often convenient to express features in
terms of segments. as an example, let dj denote the length of a segment, and let   
be the average length of all segments with label i. now consider the segment feature
gk1(j, x, s) = (dj       )2    [[yj = i]]. after training, the contribution of this feature toward
pr(s|x) associated with a length-d entity will be proportional to ewk  (d     )2   i.e., it allows
the learner to model a gaussian distribution of entity lengths.
in contrast, the feature gk2(j, x, y) = dj    [[yj = i]] would model an exponential dis-
it turns out that gk2 is equivalent to the local feature function
tribution of lengths.
f (i, x, y) = [[yi = i]], in the following sense: for every triple x, y, s, where y are the

tags for s,pj gk2(j, x, s) =pi f (i, s, y). thus any semi-crf model based on the single

feature gk2 could also be represented by a conventional crf.

in general, a semi-crf model can be factorized in terms of an equivalent order-1 crf
model iff the sum of the segment features can be rewritten as a sum of local features. thus
the degree to which semi-crfs are non-markovian depends on the feature set.

2.4 learning algorithm

during training the goal is to maximize log-likelihood over a given training set t =
`=1. following the notation of sha and pereira [16], we express the log-
{(x`, s`)}n
likelihood over the training sequences as

l(w) =x`

log pr(s`|x`, w) =x`

(w    g(x`, s`)     log zw(x`))

(5)

1assuming that non-entity words are placed in unit-length segments, as we do below.

we wish to    nd a w that maximizes l(w). equation 5 is convex, and can thus be maxi-
mized by gradient ascent, or one of many related methods. (in our implementation we use
a limited-memory quasi-id77 [13, 14].) the gradient of l(w) is the following:

   l(w) = x`
= x`

g(x`, s`)     ps0 g(s0, x`)ew  g(x`,s0)

zw(x`)

g(x`, s`)     epr(s0|w)g(x`, s0)

(6)

(7)

the    rst set of terms are easy to compute. however, we must use the markov property of
g and a id145 step to compute the normalizer, zw(x`), and the expected
value of the features under the current weight vector, epr(s0|w)g(x`, s0). we thus de   ne
ew  g(s0,x) where again si:y denotes all segmentations from

1 to i ending at i and labeled y. for i > 0, this can be expressed recursively as

  (i, y) as the value ofps0   si:y

  (i, y) =

l

xd=1 xy 0   y

  (i     d, y0)ew  g(y,y 0,x,i   d,i)

with the base cases de   ned as   (0, y) = 1 and   (i, y) = 0 for i < 0. the value of zw(x)

can then be written as zw(x) =py   (|x|, y).
a similar approach can be used to compute the expectation ps0 g(x`, s0)ew  g(x`,s0).
ps0   si:y

sum
gk(s0, x`)ew  g(x`,s0), restricted to the part of the segmentation ending at

position i. the following recursion2 can then be used to compute   k(i, y):

component of g,

the k-th

  k(i, y)

value

for

the

the

let

be

of

  k(i, y) =

l

xd=1 xy 0   y

(  k(i     d, y0) +   (i     d, y0)gk(y, y0, x, i     d, i))ew  g(y,y 0,x,i   d,i)

finally we let epr(s0|w)gk(s0, x) = 1

3 experiments with ner data

zw(x)py   k(|x|, y).

3.1 baseline algorithms and datasets

in our experiments, we trained semi-crfs to mark entity segments with the label i, and
put non-entity words into unit-length segments with label o. we compared this with two
versions of crfs. the    rst version, which we call crf/1, labels words inside and outside
entities with i and o, respectively. the second version, called crf/4, replaces the i tag
with four tags b, e, c, and u , which depend on where the word appears in an entity [2].
we compared the algorithms on    ve ner problems, associated with three different corpora.
the address corpus contains 4,226 words, and consists of 395 home addresses of students
in a major university in india [1]. we considered extraction of city names and state names
from this corpus. the jobs corpus contains 73,330 words, and consists of 300 computer-
related job postings [4]. we considered extraction of company names and job titles. the
18,121-word email corpus contains 216 email messages taken from the cspace email
corpus [10], which is mail associated with a 14-week, 277-person management game. here
we considered extraction of person names.

2as in the forward-backward algorithm for chain crfs [16], space requirements here can be
reduced from m l|y| to m |y|, where m is the length of the sequence, by pre-computing an appro-
priate set of    values.

3.2 features

as features for crf, we used indicators for speci   c words at location i, or locations within
three words of i. following previous ner work [7]), we also used indicators for capi-
talization/letter patterns (such as    aa+    for a capitalized word, or    d    for a single-digit
number).
as features for semi-crfs, we used the same set of word-level features, as well their
logical extensions to segments. speci   cally, we used indicators for the phrase inside a
segment and the capitalization pattern inside a segment, as well as indicators for words
and capitalization patterns in 3-word windows before and after the segment. we also used
indicators for each segment length (d = 1, . . . , l), and combined all word-level features
with indicators for the beginning and end of a segment.

to exploit more of the power of semi-crfs, we also implemented a number of dictionary-
derived features, each of which was based on different dictionary d and similarity function
sim. letting xsj denote the subsequence hxtj . . . xuj i, a dictionary feature is de   ned as
gd,sim (j, x, s) = argmax u   dsim(xsj , u)   i.e., the distance from the word sequence xsj
to the closest element in d.

for each of the extraction problems, we assembled one external dictionary of strings, which
were similar (but not identical) to the entity names in the documents. for instance, for
city names in the address data, we used a web page listing cities in india. due to vari-
ations in the way entity names are written, rote matching these dictionaries to the data
gives relatively low f1 values, ranging from 22% (for the job-title extraction task) to 57%
(for the person-name task). we used three different similarity metrics (jaccard, tfidf,
and jarowinkler) which are known to work well for name-matching in data integration
tasks [5]. all of the distance metrics are non-markovian   i.e., the distance-based segment
features cannot be decomposed into sums of local features. more detail on the distance
metrics, feature sets, and datasets above can be found elsewhere [6].
we also extended the semi-crf algorithm to construct, on the    y, an internal segment
dictionary of segments labeled as entities in the training data. to make measurements on
training data similar to those on test data, when    nding the closest neighbor of xsj in the
internal dictionary, we excluded all strings formed from x, thus excluding matches of xsj to
itself (or subsequences of itself). this feature could be viewed as a sort of nearest-neighbor
classi   er; in this interpretation the semi-crf is performing a sort of bi-level stacking [20].

for completeness in the experiments, we also evaluated local versions of the dictionary
features. speci   cally, we constructed dictionary features of the form f d,sim (i, x, y) =
argmax u   dsim(xi, u), where d is either the external dictionary used above, or an internal
word dictionary formed from all words contained in entities. as before, words in x were
excluded in    nding near neighbors to xi.

3.3 results and discussion

we evaluated f1-measure performance3 of crf/1, crf/4, and semi-crfs, with and with-
out internal and external dictionaries. a detailed tabulation of the results are shown in ta-
ble 1, and figure 1 shows f1 values plotted against training set size for a subset of three of
the tasks, and four of the learning methods. in each experiment performance was averaged
over seven runs, and evaluation was performed on a hold-out set of 30% of the documents.
in the table the learners are trained with 10% of the available data   as the curves show,
performance differences are often smaller with more training data. gaussian priors were
used for all algorithms, and for semi-crfs, a    xed value of l was chosen for each dataset
based on observed entity lengths. this ranged between 4 and 6 for the different datasets.
in the baseline con   guration in which no dictionary features are used, semi-crfs perform

3f1 is de   ned as 2*precision*recall/(precision+recall.)

address_state

address_city

email_person

100

80

60

40

20

y
c
a
r
u
c
c
a
n
a
p
s
 

 

1
f

0

0

100

80

60

40

20

y
c
a
r
u
c
c
a
n
a
p
s
 

 

1
f

0.7

0

0

crf/4
semicrf
crf/1

0.2

0.1
0.6
fraction of available training data

0.3

0.4

0.5

y
c
a
r
u
c
c
a
n
a
p
s
 

 

1
f

100

80

60

40

20

0

0.7

crf/4
semicrf
crf/1

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5

fraction of available training data

crf/4
semicrf
crf/1

0.2

0.1
0.6
fraction of available training data

0.3

0.4

0.5

figure 1: f1 as a function of training set size. algorithms marked with    +dict    include external
dictionary features, and algorithms marked with    +int    include internal dictionary features. we do
not use internal dictionary features for crf/4 since they lead to reduced accuracy.

baseline
f1

+internal dict
f1    base

+external dict
f1    base

+both dictionaries

f1    base    extern

20.8
28.5
67.6
70.3
51.4

15.0
23.7
70.9
73.2
54.8

25.6
33.8
72.2
75.9
60.2

44.5
3.8
48.0
60.0
16.5

25.4
7.9
64.5
70.6
20.6

35.5
37.5
74.8
75.3
59.7

113.9
-86.7
-29.0
-14.7
-67.9

69.3
-66.7
-9.0
-3.6
-62.4

38.7
10.9
3.6
-0.8
-0.8

69.2
38.6
81.4
80.4
55.3

46.8
36.4
82.5
80.8
61.2

62.7
41.1
82.8
84.0
60.9

232.7
35.4
20.4
14.4
7.6

212.0
53.6
16.4
10.4
11.7

144.9
21.5
14.7
10.7
1.2

55.2
19.9
64.7
69.8
15.6

43.1
14.6
74.8
76.3
25.1

65.2
40.2
83.7
83.6
60.9

165.4
-30.2
-4.3
-0.7
-69.6

187.3
-38.4
5.5
4.2
-54.2

154.7
18.9
15.9
10.1
1.2

-67.3
-65.6
-24.7
-15.1
-77.2

-24.7
-92.0
-10.9
-6.1
-65.9

9.8
-2.5
1.2
-0.5
0.0

crf/1
state
title
person
city
company
crf/4
state
title
person
city
company
semi-crf
state
title
person
city
company

table 1: comparing various methods on    ve ie tasks, with and without dictionary features. the
column    base is percentage change in f1 values relative to the baseline. the column    extern is is
change relative to using only external-dictionary features.

best on all    ve of the tasks. when internal dictionary features are used, the performance
of semi-crfs is often improved, and never degraded by more than 2.5%. however, the
less-natural local version of these features often leads to substantial performance losses for
crf/1 and crf/4. semi-crfs perform best on nine of the ten task variants for which
internal dictionaries were used. the external-dictionary features are helpful to all the algo-
rithms. semi-crfs performs best on three of    ve tasks in which only external dictionaries
were used.
overall, semi-crf performs quite well. if we consider the tasks with and without external
dictionary features as separate    conditions   , then semi-crfs using all available informa-
tion4 outperform both crf variants on eight of ten    conditions   .
we also compared semi-crf to order-l crfs, with various values of l.5 in table 2 we

4i.e., the both-dictionary version when external dictionaries are available, and the internal-

dictionary only version otherwise.

to l     3 for computational reasons.

5order-l crfs were implemented by replacing the label set y with y l. we limited experiments

crf/1

crf/4

semi-crf

address state
address city
email persons

l = 1 l = 2 l = 3
19.2
71.2
66.7

20.8
70.3
67.6

20.1
71.0
63.7

l = 1 l = 2 l = 3
16.4
73.7
70.4

15.0
73.2
70.9

16.4
73.9
70.7

25.6
75.9
72.2

table 2: f1 values for different order crfs

show the result for l = 1, l = 2, and l = 3, compared to semi-crf. for these tasks, the
performance of crf/4 and crf/1 does not seem to improve much by simply increasing
order.

4 related work

semi-crfs are similar to nested id48s [1], which can also be trained discrimini-
tively [17]. the primary difference is that the    inner model    for semi-crfs is of short,
uniformly-labeled segments with non-markovian properties, while nested id48s allow
longer, diversely-labeled, markovian    segments   .
dyanamic crfs [18] can, with an appropriate network architecture, be used to implement
semi-crfs. another non-markovian model recently used for ner is relational markov
networks (rmns) [3]. however, in both dynamic crfs and rmns, id136 is not
tractable, so a number of approximations must be made in training and classi   cation. an
interesting question for future research is whether the tractible extension to crf infer-
ence considered here can can be used to improve id136 methods for rmns or dynamic
crfs.
in recent prior work [6], we investigated semi-markov learning methods for ner based
on a voted id88 training algorithm [7]. the voted id88 has some advantages
in ease of implementation, and ef   ciency.6 however, semi-crfs perform somewhat bet-
ter, on average, than our id88-based learning algorithm. probabilistically-grounded
approaches like crfs also are preferable to margin-based approaches like the voted percep-
tron in certain settings, e.g., when it necessary to estimate con   dences in a classi   cation.

5 concluding remarks

semi-crfs are a tractible extension of crfs that offer much of the power of higher-order
models without the associated computational cost. a major advantage of semi-crfs is that
they allow features which measure properties of segments, rather than individual elements.
for applications like ner and gene-   nding [11], these features can be quite natural.

appendix

an implementation of semi-crfs is available at http://crf.sourceforge.net, and a ner
package that uses it is available on http://minorthird.sourceforge.net.

references

[1] v. r. borkar, k. deshmukh, and s. sarawagi. automatic text segmentation for extracting
structured records. in proc. acm sigmod international conf. on management of data, santa
barabara,usa, 2001.

6 in particular, the voted id88 algorithm is more stable numerically, as it does not need to
compute a partition function. our crf implementation does all computations in log space, which is
relatively expensive.

[2] a. borthwick, j. sterling, e. agichtein, and r. grishman. exploiting diverse knowledge sources
via maximum id178 in id39. in sixth workshop on very large corpora
new brunswick, new jersey. association for computational linguistics., 1998.

[3] r. bunescu and r. j. mooney. relational markov networks for collective information extrac-
in proceedings of the icml-2004 workshop on statistical relational learning (srl-

tion.
2004), banff, canada, july 2004.

[4] m. e. califf and r. j. mooney. bottom-up relational learning of pattern matching rules for

information extraction. journal of machine learning research, 4:177   210, 2003.

[5] w. w. cohen, p. ravikumar, and s. e. fienberg. a comparison of string distance metrics for
name-matching tasks. in proceedings of the ijcai-2003 workshop on information integration
on the web (iiweb-03), 2003. to appear.

[6] w. w. cohen and s. sarawagi. exploiting dictionaries in named entity extraction: combining
semi-markov extraction processes and data integration methods. in proceedings of the tenth
acm sigkdd international conference on knowledge discovery and data mining, 2004. to
appear.

[7] m. collins. discriminative training methods for id48: theory and exper-
in empirical methods in natural language processing

iments with id88 algorithms.
(emnlp), 2002.

[8] x. ge. segmental semi-markov models and applications to sequence analysis. phd thesis,

university of california, irvine, december 2002.

[9] j. janssen and n. limnios. semi-markov models and applications. kluwer academic, 1999.
[10] r. e. kraut, s. r. fussell, f. j. lerch, and j. a. espinosa. coordination in teams: evi-dence
from a simulated management game. to appear in the journal of organizational behavior,
2004.

[11] a. krogh. gene    nding: putting the parts together. in m. j. bishop, editor, guide to human

genome computing, pages 261   274. academic press, 2nd edition, 1998.

[12] j. lafferty, a. mccallum, and f. pereira. conditional random    elds: probabilistic models for
in proceedings of the international conference on

segmenting and labeling sequence data.
machine learning (icml-2001), williams, ma, 2001.

[13] d. c. liu and j. nocedal. on the limited memory bfgs method for large-scale optimization.

mathematic programming, 45:503   528, 1989.

[14] r. malouf. a comparison of algorithms for maximum id178 parameter estimation. in pro-
ceedings of the sixth conference on natural language learning (conll-2002), pages 49   55,
2002.

[15] a. mccallum and w. li. early results for id39 with conditional random
   elds, feature induction and web-enhanced lexicons. in proceedings of the seventh conference
on natural language learning (conll-2003), edmonton, canada, 2003.

[16] f. sha and f. pereira. id66 with conditional random    elds. in in proceedings of

hlt-naacl, 2003.

[17] m. skounakis, m. craven, and s. ray. hierarchical id48 for information
extraction. in proceedings of the 18th international joint conference on arti   cial intelligence,
acapulco, mexico. morgan kaufmann., 2003.

[18] c. sutton, k. rohanimanesh, and a. mccallum. dynamic conditional random    elds: factor-

ized probabilistic models for labeling and segmenting sequence data. in icml, 2004.

[19] r. sutton, d. precup, and s. singh. between mdps and semi-mdps: a framework for temporal

abstraction in id23. arti   cial intelligence, 112:181   211, 1999.

[20] d. h. wolpert. stacked generalization. neural networks, 5:241   259, 1992.

