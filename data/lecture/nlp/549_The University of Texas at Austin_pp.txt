simple probabilistic modeling

and pp attachment

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john gave mary a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    one morning, graucho shot an elephant in his 

pajamas

    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
these are all the same (how?)

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
these are all the same (how?)

 

 

ambiguity

??

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
these are all the same (how?)

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
verb  np(1)  preposition  np(2)

 

 

ambiguity

verb  np(1)  preposition  np(2)

verb  np(1)  preposition  np(2)

 

 

ambiguity

verb  np(1)  preposition  np(2)
ate     pizza          with             olives
ate     pizza          with             olives

verb  np(1)  preposition  np(2)
ate     pizza          with          my hands

 

 

the n-v pp attachment problem

    you get a 4-tuple: (verb, np1, prep, np2)

    talked the girl in a harsh voice
    shot an elephant in his pajamas
    found a sack of money
    filling the garden with flowers

    need to decide: v or n

    v means a v-prep relation   (ate with my hands)
    n means a n-prep relation  (pizza with olives)

    a binary classification task

 

 

the n-v pp attachment problem

    you get a 4-tuple: (verb, np1, prep, np2)

    talked the girl in a harsh voice
    shot an elephant in his pajamas
    found a sack of money
    filling the garden with flowers

    need to decide: v or n

where do 
the tuples
come from???

    v means a v-prep relation   (ate with my hands)
    n means a n-prep relation  (pizza with olives)

    a binary classification task

 

 

one morning i shot an elephant in my pajamas. 
how he got into my pajamas i'll never know.

- graucho marx

sometimes, must use discourse...

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
verb  np(1)  preposition  np(2)

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
verb  np(1)  preposition  np(2)

 

 

ambiguity

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
verb  noun(1)  preposition  noun(2)

 

 

consider only the head (   main   ) words

modeling choice: 
ambiguity

is this a reasonable thing to do?

why?

why not?

(what do we gain? what do we lose?)

    i saw the dog with the blue hat
    he talked to the girl in a harsh voice
    graucho shot an elephant in his pajamas
    john found a sack of money
    he thought about filling the garden with flowers
    collect the young children after school
    i saw a boy on the hill with a telescope
verb  noun(1)  preposition  noun(2)

 

 

the n-v pp attachment problem

    you get a 4-tuple: (verb, noun1, prep, noun2)

    talked girl in voice
    shot elephant in pajamas
    found sack of money
    filling garden with flowers

    need to decide: v or n

    v means a v-prep relation   (ate with hands)
    n means a n-prep relation   (pizza with olives)

    a binary classification task

 

 

how do we solve it?

    assume supervised classification:

    you get 4000 (or 40,000, or 400,000) tuples 

with their correct answer.
    talked girl in voice      v
    shot elephant in pajamas     v
    found sack of money     n
    filling garden with flowers     v
    ...

    someone hands new a new tuple. need to decide 

based on previous observation.

 

 

step 1 (always)     look at the data

 

 

step 1 (always)     look at the data

step 2 (always)     define accuracy measure

 

 

step 1 (always)     look at the data

step 2 (always)     define accuracy measure

acc = correct / (correct + incorrect)

 

 

how do we solve it?

    id155:

if p(v | verb, noun1, prep, noun2) > 0.5
         say v
else:
         say n

 

for example, p(v | saw, boy, with, hat)

 

id113

p( v | verb, noun1, prep, noun2) =

count(v,verb, noun1, prep, noun2)
count(*,verb, noun1, prep, noun2)

count(...)  is number of times we saw the event
in the training data

    this is called id113 estimation. (maximum likelihood)

 

 

id113

p( v | verb, noun1, prep, noun2) =

count(v,verb, noun1, prep, noun2)
count(*,verb, noun1, prep, noun2)

count(...)  is number of times we saw the event
in the training data

 

is this reasonable? why?

 

id113

p( v | verb, noun1, prep, noun2) =

count(v,verb, noun1, prep, noun2)
count(*,verb, noun1, prep, noun2)

count(...)  is number of times we saw the event
in the training data

problem: data sparsity and overfitting

is this reasonable? why?

 

 

another option (majority baseline)

p( v | verb, noun1, prep, noun2)     p(v)

is this reasonable?

what score would you expect?

 

 

another option

p( v | verb, noun1, prep, noun2)     p(v|noun1)

is this reasonable?

what score would you expect?

 

 

another option

p( v | verb, noun1, prep, noun2)     p(v|prep)

is this reasonable?

what score what score would you expect?

 

 

another option

p( v | verb, noun1, prep, noun2)     p(v|prep)

is this reasonable?

what score what score would you expect?

this one is actually pretty good! (why?)

 

 

another option

p( v | verb, noun1, prep, noun2)     p(v|prep)

is this reasonable?

what score what score would you expect?

this one is actually pretty good! (why?)

 

can we do better?

 

p(v| verb, prep) ?

p(v| noun1, prep) ?

p(v| noun1, noun2) ?

p(v| verb, noun1, noun2) ?

p(v| verb, noun1, prep) ?

 

 

how do we combine the different 

probabilities?

    remember, for a function to be a id203 

function, we must have:
    always positive
    sum to one

    (do we care if our scoring function is a 

id203 function? why?)

 

 

how do we combine the different 

probabilities?

    one way of combining probabilities to obtain a 

id203 is linear interpolation

pinterpolate=  1 p1+  2 p2+  3 p3...+  k p k
  1+   2+  3+...+  k=1

 

 

collins and brooks' estimation

    interpolate

p(v|v,n1,p), p(v|v,p,n2), p(v|n1,p,n2) into ptriplet

    interpolate

p(v|v,p), p(v|n1,p), p(v|p,n2) into ppair

 

 

collins and brooks' estimation

    interpolate

p(v|v,n1,p), p(v|v,p,n2), p(v|n1,p,n2) into ptriplet

    interpolate

p(v|v,p), p(v|n1,p), p(v|p,n2) into ppair

notice we always include p (the preposition). 

we do not have p(v|n1,n2) for example. 

 

why?

 

collins and brooks' estimation

    interpolate

p(v|v,n1,p), p(v|v,p,n2), p(v|n1,p,n2) into ptriplet

    interpolate

p(v|v,n1,p) = #(v, v, n1, p, *) / #(*, v, n1, p, *)

p(v|v,p), p(v|n1,p), p(v|p,n2) into ppair

 

 

collins and brooks' estimation

    interpolate

p(v|v,n1,p), p(v|v,p,n2), p(v|n1,p,n2) into ptriplet

    interpolate

p(v|v,n1,p) = #(v, v, n1, p, *) / #(*, v, n1, p, *)

p(v|v,p), p(v|n1,p), p(v|p,n2) into ppair

p(v|v, p) = #(v, v, *, p, *) / #(*, v, *, p, *)

 

 

how do we combine the different 

probabilities?

    one way of combining probabilities to obtain a 

id203 is linear interpolation

pinterpolate=  1 p1+  2 p 2+  3 p3...+  k p k
  1+  2+  3+...+  k=1

 

 

collins and brooks' interpolation

  v , n1, p=

  v , p ,n 2=

count (v , n1, p)

count (v ,n1, p)+count (v , p ,n 2)+count (n1, p ,n 2)

count (v , p ,n 2)

count (v , n1, p)+count (v , p ,n 2)+count (n1, p ,n 2)

  n1, p ,n 2=

count (n1, p , n2)

count (v , n1, p)+count (v , p ,n 2)+count (n1, p ,n 2)

 

 

collins and brooks' interpolation

  v , n1, p=

count (v , n1, p)

count (v ,n1, p)+count (v , p ,n 2)+count (n1, p ,n 2)

  v , p ,n2=

count (v , p ,n2)

count (v , n1 , p)+count (v , p ,n2)+count (n1 , p ,n2)

  n1, p , n2=

count (n1 , p ,n2)

count (v ,n1 , p)+count (v , p , n2)+count (n1 , p ,n2)

give more weight to events that occurred 
more times in the training data.

 

 

collins and brooks' estimation

p3(v|v,n1,p,n2) =

 count(v,v,n1,p) + count(v,v,p,n2) + count(v,n1,p,n2) 
 count(*,v,n1,p)  + count(*,v,p,n2 )+ count(*,n1,p,n2) 

 

 

collins and brooks' estimation

p3(v|v,n1,p,n2) =

 count(v,v,n1,p) + count(v,v,p,n2) + count(v,n1,p,n2) 
 count(*,v,n1,p)  + count(*,v,p,n2)+ count(*,n1,p,n2) 

this follows from

p3(v   v ,n1, p ,n 2)=  v ,n1, p p(v   v ,n1, p)
+  n1, p ,n 2 p(v   n1, p , n2)
+  v , p ,n 2 p(v   v , p ,n 2)

 

p id113(v   v , n1, p)= count (v ,v , n1, p)
count (   ,v ,n1, p)

 

collins and brooks' estimation

p3(v|v,n1,p,n2) =

 count(v,v,n1,p) + count(v,v,p,n2) + count(v,n1,p,n2) 
 count(*,v,n1,p)  + count(*,v,p,n2)+ count(*,n1,p,n2) 

p2(v|v,n1,p,n2) =

 count(v,v,p) + count(v,n1,p) + count(v,p,n2) 
 count(*,v,p) + count(*,n1,p)+  count(*,p,n2) 

 

 

collins and brooks' estimation

p3(v|v,n1,p,n2) =

 count(v,v,n1,p) + count(v,v,p,n2) + count(v,n1,p,n2) 
 count(*,v,n1,p)  + count(*,v,p,n2)+ count(*,n1,p,n2) 

p2(v|v,n1,p,n2) =

 count(v,v,p) + count(v,n1,p) + count(v,p,n2) 
 count(*,v,p) + count(*,n1,p)+  count(*,p,n2) 

p1(v|v,n1,p,n2) = count(v,p) / count(*,p)

 

 

collins and brooks' estimation

p3(v|v,n1,p,n2) =

 count(v,v,n1,p) + count(v,v,p,n2) + count(v,n1,p,n2) 
 count(*,v,n1,p)  + count(*,v,p,n2)+ count(*,n1,p,n2) 

p2(v|v,n1,p,n2) =

 count(v,v,p) + count(v,n1,p) + count(v,p,n2) 
 count(*,v,p) + count(*,n1,p)+  count(*,p,n2) 

p1(v|v,n1,p,n2) = count(v,p) / count(*,p)

 

 

combine using backoff

collins and brooks' estimation -

back-off

p(v|v,n1,p,n2) =

if count(v,n1,p,n2) > 0

use p4

else if   count(v,n1,p)  + count(v,p,n2)+ count(n1,p,n2) > 0

use p3 

else if   count(v,p)  + count(n1,p)+ count(p,n2, *) > 0

use p2

else if   count(p) > 0

use p1

else

use p0 = count(v) / count(v+n)

 

 

collins and brooks' estimation -

back-off

    combination of probabilistic model and a 

heuristic

    returns a well behaved id203 score

    but not really well motivated by id203 theory

    works well

        heuristics can be good, if designed well

 

 

collins and brooks' estimation -

back-off

    combination of probabilistic model and a heuristic
    returns a well behaved id203 score

    but not really well motivated by id203 theory

    works well

        heuristics can be good, if designed well

    will be nice to have a method that allows to easily 

integrate many clues without resorting to heuristics.

 

 

further improvements

    we've seen

    (saw,john,with,dog)

    but not

    (saw,jack,with,dog)

can we still say something about the second 
case?

 

 

 

 

