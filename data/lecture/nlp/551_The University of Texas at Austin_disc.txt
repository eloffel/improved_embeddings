feature-based discriminative models

more sequence models

yoav goldberg

bar ilan university

1 / 58

reminder

pp-attachment
he saw a mouse with a telescope.
saw, mouse, with, telescope     v

pos-tagging
holly/nnp came/vbd from/in miami/nnp ,/, f.l.a/nnp ,/,
hitch-hiked/vbd her/prp way/nn across/in the/dt usa/nnp

both were solved with a probabilistic model and id113
estimates, based on counting and dividing.

2 / 58

today

discriminative training

3 / 58

sentence-boundary-detection revisited

4 / 58

georg "mr. george" svendsen (19 march 1894     1966) was a norwegian journalist and crime novelist.he was born in eidanger, and started his journalistic career in bratsberg-demokraten before moving on to demokraten where he was a subeditor. in 1921 he was hired in fremtidenand replaced in demokraten by evald o. solbakken. in 1931 he was hired in arbeiderbladet. under the pen name "mr. george" he became known for his humorous articles in the newspaper. at his death he was also called "the last of the three great criminal and police reporters in oslo", together with fridtjof knutsen and axel kielland. he was also known for practising journalism as a trade in itself, and not as a part of a political career. he retired in 1964, and died in 1966.he released the criminal novels mannen med lj  en (1942), ridderne av   ksen (1945) and den hvite streken (1946), and translated the book s.s. murder by quentin patrick as mord ombord in 1945. he released several historical books: r  rleggernes fagforenings historie gjennem 50   r (1934), telefonmont  renes forening oslo, gjennem 50   r (1939), norsk n  rings- og nydelsesmiddelarbeiderforbund: 25-  rs beretning (1948), de tause vitner: av rettskjemiker ch. bruffs memoarer (1949, with fridtjof knudsen) and elektriske mont  rers fagforening gjennom 50   r (1949).sentence-boundary-detection revisited

(cid:73) p(boundary | subeditor . in)
(cid:73) p(boundary | o . solbakken)
(cid:73) p(boundary | solbakken . in)
(cid:73) p(boundary | arbeiderbladet . under)
(cid:73) p(boundary | mr . george)
(cid:73) p(boundary | 1945 . he)

5 / 58

sentence-boundary-detection revisited

(cid:73) p(boundary | l=subeditor . r=in)
(cid:73) p(boundary | l=o . r=solbakken)
(cid:73) p(boundary | l=solbakken . r=in)
(cid:73) p(boundary | l=arbeiderbladet . r=under)
(cid:73) p(boundary | l=mr . r=george)
(cid:73) p(boundary | l=1945 . r=he)

5 / 58

sentence-boundary-detection revisited

(cid:73) p(boundary | l=subeditor . r=in)
(cid:73) p(boundary | l=o . r=solbakken)
(cid:73) p(boundary | l=solbakken . r=in)
(cid:73) p(boundary | l=arbeiderbladet . r=under)
(cid:73) p(boundary | l=mr . r=george)
(cid:73) p(boundary | l=1945 . r=he)

useful indicators

(cid:73) identity of l
(cid:73) identity of r
(cid:73) length of l
(cid:73) is r capitalized
(cid:73) . . .

5 / 58

sentence-boundary-detection revisited

(cid:73) p(boundary | l=subeditor . r=in)
(cid:73) p(boundary | l=o . r=solbakken)
(cid:73) p(boundary | l=solbakken . r=in)
(cid:73) p(boundary | l=arbeiderbladet . r=under)
(cid:73) p(boundary | l=mr . r=george)
(cid:73) p(boundary | l=1945 . r=he)

useful indicators
(cid:73) identity of l
(cid:73) identity of r
(cid:73) length of l
(cid:73) is r capitalized
(cid:73) . . .

highly correlated.
do we need both?
how do we integrate information?

5 / 58

sentence-boundary-detection revisited

p(boundary|l=subeditor, r=in, len(l)=9, iscap(r)=true,

iscap(l)=false, . . . )

(cid:73) how do we compute the id203 model?
(cid:73) do we really need a id203 model here?
probabilities     scores
(cid:73) instead of p(y|x), compute score(x, y)
(cid:73) return arg maxy score(x, y)

(cid:73) or, if we just have two classes, return class 1 iff score(x) > 0

(cid:73) but how do we compute the score?

6 / 58

feature representation

each indicator will be a feature.
feature

(cid:73) a feature is a function   i(x) looking at a particular property

of x and returning a value.

(cid:73) generally, the value can be any number,   i(x)     r
(cid:73) often, the value is binary,   i(x)     {0, 1}.

feature extractor / feature vector

(cid:73) a feature extractor is a collection of features

   =   1, . . . ,   m

(cid:73) a feature vector is the result of    applied on a particular x

(cid:73)   (x) =   1(x), . . . ,   m(x)
(cid:73) for binary features:   (x)     {0, 1}m

7 / 58

feature representation

feature examples

1
0
1
0
1
0

(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)

  1(l.r) =

  2(l.r) =

  57(l.r) =

  1039(l.r) =

  1040(l.r) =

  1045(l.r) =

if l=   subeditor   
otherwise
if l=   o   
otherwise
if r=   george   
otherwise
if len(l)=1
otherwise
if len(l)=2
otherwise
if len(l)>4
otherwise

1
0
1
0
1
0

(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)

1 if iscap(r)
0 otherwise
1 if isnumber(l)
0 otherwise
1 if l=   us    and iscap(r)
0 otherwise
1 if len(l)=1 and iscap(r)
0 otherwise
1 if l=   ca    and r=   the   
0 otherwise
1 if l=   ca    and r=   snow   
0 otherwise

  1092(l.r) =

  1093(l.r) =

  2045(l.r) =

  3066(l.r) =

  5032(l.r) =

  5033(l.r) =

8 / 58

feature representation

feature vector
  (l=subeditor . r=in) = 1, 0, 0, 0, 0, . . . , 1, 0, 0, 0, 1 . . .
  (l=mr . r=george) = 0, 0, 0, 1, 0, . . . , 0, 0, 1, 0, 1 . . .

(cid:73) each example is mapped to a very long vector of 0 and 1.
(cid:73) most values are 0.

sparse representation
we don   t need to write the zeros:

  (l=subeditor . r=in) = 1:1 1045:1 1092:1 . . .

9 / 58

machine learning

we had a set of example:

(cid:73) subeditor . in     boundary
(cid:73) o . solbakken     no-boundary
(cid:73) mr . george     no-boundary
(cid:73) . . .

we can map them to a list of vectors

(cid:73)   (l=subeditor . r=in) = 1:1 1045:1 1092:1 . . .
(cid:73)   (l=o . r=solbakken) = 2:1 1039:1 1092:1 . . .
(cid:73)   (l=mr . r=george) = 57:1 1040:1 1092:1 . . .
(cid:73) . . .

and a list of answers:

(cid:73) +1,    1,    1, . . .

10 / 58

feature representation

why did we do this?

(cid:73) we mapped each example x to a vector of numbers   (x).
(cid:73) we mapped each answer y to a number.
(cid:73) instead of p(y|x) we now have p(y|  (x)) y     {   1, +1}.

(cid:73) x was a sequence of words.
(cid:73)   (x) is a vector of numbers.

(cid:73) the    eld of machine learning is very good at learning to

classify vectors.

11 / 58

machine learning

dataset
+1 1:1 1045:1 1092:1 . . .
   1 2:1 1039:1 1092:1 . . .
   1 57:1 1040:1 . . .
. . .

(cid:73) we can feed this dataset to a machine-learning algorithm.
(cid:73) many machine learning algorithms exist:

(cid:73) id166, boosting, id90, knn, id28,

id79s, id88, neural networks, . . .

(cid:73) the important distinctions:

(cid:73) output type:

(cid:73) scoring type:

(cid:73) linear vs. non linear

(cid:73) binary, multiclass, numeric, structured

12 / 58

https://www.kaggle.com/surveys/2017

13 / 58

types of learning problems

binary

(cid:73) answer is binary: y     {   1, +1}

(cid:73) boundary/no-boundary, verb-attach/noun-attach, yes/no

multiclass

(cid:73) answer is one of k options: y     {1, . . . , k}

(cid:73) choose the part-of-speech of a word.
(cid:73) identify the language of a document.

numeric / regression

(cid:73) answer is a number: y     r

(cid:73) predict the height of a person.

structured

(cid:73) answer is a complex object:

(cid:73) predict a sequence of tags for a sentence.

14 / 58

generic nlp solution

(cid:73) find an annotated corpus
(cid:73) split it into train and test parts
(cid:73) convert it to a vector representation

(cid:73) decide on output type
(cid:73) decide on features
(cid:73) convert each training example to feature vector
(cid:73) train a machine-learning model on training set
(cid:73) apply machine-learning model to test set
(cid:73) measure accuracy

15 / 58

linear models

16 / 58

machine learning

(cid:73) many learning algorithms exist.
(cid:73) some of them are based on a linear model:

(cid:73) id166, boosting, logistic-regression, id88

(cid:73) others are non-linear:

(cid:73) kernel-id166, decision-trees, knn, random-forests,

neural-networks.

(cid:73) we will work mostly with linear classi   ers in this course.
(cid:73) neural networks are useful and popular. but there   s a

separate course for them.

17 / 58

linear models

(cid:73) some features are indicators for    boundary   

and others are good indicators for    no boundary   .

(cid:73) we will assign a score (weight) to each feature.

(cid:73) features in favor of boundary will receive a positive score.
(cid:73) features in favor of no boundary will receive a negative

score.

(cid:73) use the sum of the scores to classify.

18 / 58

linear models

(cid:73) some features are indicators for    boundary   

and others are good indicators for    no boundary   .

(cid:73) we will assign a score (weight) to each feature.

(cid:73) features in favor of boundary will receive a positive score.
(cid:73) features in favor of no boundary will receive a negative

score.

(cid:73) use the sum of the scores to classify.

binary linear classi   er

(cid:88)

i   1,...,m

(cid:40)

score(x) =

wi      i(x) = w      (x)

  y = predict(x) =

score(x)     0
1
   1 score(x) < 0

18 / 58

setting the weights

feature examples

1
0
1
0
1
0

(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)

  1(l.r) =

  2(l.r) =

  57(l.r) =

  1039(l.r) =

  1040(l.r) =

  1045(l.r) =

if l=   subeditor   
otherwise
if l=   o   
otherwise
if r=   george   
otherwise
if len(l)=1
otherwise
if len(l)=2
otherwise
if len(l)>4
otherwise

1
0
1
0
1
0

(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)

1 if iscap(r)
0 otherwise
1 if isnumber(l)
0 otherwise
1 if l=   us    and iscap(r)
0 otherwise
1 if len(l)=1 and iscap(r)
0 otherwise
1 if l=   ca    and r=   the   
0 otherwise
1 if l=   ca    and r=   snow   
0 otherwise

  1092(l.r) =

  1093(l.r) =

  2045(l.r) =

  3066(l.r) =

  5032(l.r) =

  5033(l.r) =

19 / 58

setting the weights

binary linear classi   er

(cid:88)

i   1,...,m

(cid:40)

score(x) =

wi      i(x) = w      (x)

  y = predict(x) =

score(x)     0
1
   1 score(x) < 0

machine learning
provide algorithms for learning the values of w from examples.

(cid:73) a supervised binary learning problem:

(cid:73) input: a set of (x, y) examples, features extractor   .
(cid:73) output: weights w such that w      (x) classi   es many of the

y   s correctly.

20 / 58

setting weights: the id88 algorithm

  y     sign(w      (x))
if   y < 0 and y     0 then
w     w +   (x)

1: w     0
2: for x, y in training examples do
3:
4:
5:
6:
7:
8: return w

if   y     0 and y < 0 then

w     w       (x)

21 / 58

setting the weights

multiclass linear classi   er

(cid:88)

i   1,...,m

score(x, y) =

i      i(x, y) = w      (x, y)
w(y)

  y = predict(x) = arg max

y

score(x, y)

22 / 58

setting the weights

multiclass linear classi   er

(cid:88)

i   1,...,m

score(x, y) =

i      i(x, y) = w      (x, y)
w(y)

  y = predict(x) = arg max

y

score(x, y)

alternative view

score(x, y) =

(cid:88)

i   1,...,m

wi      i(x) = w(y)      (x)

  y = predict(x) = arg max

y

score(x, y)

here, each class receives its own weight vector.

22 / 58

setting weights: the multiclass id88 algorithm

  y     arg maxy(cid:48)(w      (x, y(cid:48)))
if   y (cid:54)= y then

1: w     0
2: for x, y in training examples do
3:
4:
5:
6:
7: return w

w     w +   (x, y)
w     w       (x,   y)

23 / 58

setting weights: the multiclass id88 algorithm

1: w     0
2: for x, y in training examples do
3:
4:
5:
6: return w

  y     arg maxy(cid:48)(w      (x, y(cid:48)))
if   y (cid:54)= y then

w     w +   (x, y)       (x,   y)

24 / 58

id88     averaged id88

  y     arg maxy(cid:48)(w      (x, y(cid:48)))
if   y (cid:54)= y then

1: w     0
2: for x, y in training examples do
3:
4:
5:
6: return w
averaged id88

w     w +   (x, y)       (x,   y)

(cid:73) the id88 algorithm is not a very good learner
(cid:73) but can be easily improved

25 / 58

id88     averaged id88

  y     arg maxy(cid:48)(w      (x, y(cid:48)))
if   y (cid:54)= y then

1: w     0
2: for x, y in training examples do
3:
4:
5:
6: return w
averaged id88

w     w +   (x, y)       (x,   y)

(cid:73) the id88 algorithm is not a very good learner
(cid:73) but can be easily improved
(cid:73) instead of returning w, return avg(w)

(cid:73) avg(w) is the average of all versions of w seen in training

25 / 58

id88     averaged id88

  y     arg maxy(cid:48)(w      (x, y(cid:48)))
if   y (cid:54)= y then

1: w     0
2: for x, y in training examples do
3:
4:
5:
6: return w
averaged id88

w     w +   (x, y)       (x,   y)

(cid:73) the id88 algorithm is not a very good learner
(cid:73) but can be easily improved
(cid:73) instead of returning w, return avg(w)

(cid:73) avg(w) is the average of all versions of w seen in training

(cid:73) require a bit more book-keeping for calculating the

average at the end

(cid:73) the result is a very competitive algorithm

25 / 58

pp-attachment revisited

we calculated
p(v|v = saw, n1 = mouse, p = with, n2 = telescope)
problems

(cid:73) was not trivial to come up with a formula.
(cid:73) hard to add more sources of information.

new solution

(cid:73) encode as a binary or multiclass classi   cation.
(cid:73) decide on features.
(cid:73) apply learning algorithm.

26 / 58

pp-attachment

multiclass classi   cation problem

(cid:73) previously, we had a binary classi   cation problem:

    x = (v,n1,p,n2)
    y    {v,n}

(cid:73) let   s do a multiclass problem:

    y    {v,n,other}

27 / 58

pp-attachment

types of features
(cid:73) single items

(cid:73) identity of v
(cid:73) identity of p
(cid:73) identity of n1
(cid:73) identity of n2

(cid:73) pairs

(cid:73) identity of (v,p)
(cid:73) identity of (n1,p)
(cid:73) identity of (p,n1)

(cid:73) triplets

(cid:73) identity of (v,n1,p)
(cid:73) identity of (v,p,n2)
(cid:73) identity of (n1,p,n2)

(cid:73) quadruple

(cid:73) identity of (v,n1,p,n2)

28 / 58

pp-attachment

types of features
(cid:73) single items

(cid:73) identity of v
(cid:73) identity of p
(cid:73) identity of n1
(cid:73) identity of n2

(cid:73) pairs

(cid:73) identity of (v,p)
(cid:73) identity of (n1,p)
(cid:73) identity of (p,n1)

(cid:73) triplets

(cid:73) identity of (v,n1,p)
(cid:73) identity of (v,p,n2)
(cid:73) identity of (n1,p,n2)

(cid:73) quadruple

(cid:73) identity of (v,n1,p,n2)

(cid:40)
(cid:40)
(cid:40)
(cid:40)

1
0
1
0
1
0
1
0

  1039 =

  1040 =

  1041 =

  1042 =
. . .

if v=   ate    and y=   n   
otherwise
if v=   ate    and y=   v   
otherwise
if v=   ate    and y=   o   
otherwise
if v=   saw    and y=   n   
otherwise

28 / 58

pp-attachment

types of features
(cid:73) single items

(cid:73) identity of v
(cid:73) identity of p
(cid:73) identity of n1
(cid:73) identity of n2

(cid:73) pairs

(cid:40)
(cid:40)
(cid:40)

1
0
1
0

  2349 =

1 if v=   ate    p=   with    and y=   n   
0 otherwise

(cid:73) identity of (v,p)
(cid:73) identity of (n1,p)
(cid:73) identity of (p,n1)

(cid:73) triplets

(cid:73) identity of (v,n1,p)
(cid:73) identity of (v,p,n2)
(cid:73) identity of (n1,p,n2)

(cid:73) quadruple

(cid:73) identity of (v,n1,p,n2)

  2350 =

  2351 =
. . .

if v=   ate    p=   with    and y=   v   
otherwise
if v=   ate    p=   with    and y=   o   
otherwise

28 / 58

pp-attachment

types of features
(cid:73) single items

(cid:73) identity of v
(cid:73) identity of p
(cid:73) identity of n1
(cid:73) identity of n2

(cid:73) pairs

(cid:40)
(cid:40)
(cid:40)

1
0
1
0

  2349 =

1 if v=   ate    p=   with    and y=   n   
0 otherwise

(cid:73) identity of (v,p)
(cid:73) identity of (n1,p)
(cid:73) identity of (p,n1)

(cid:73) triplets

(cid:73) identity of (v,n1,p)
(cid:73) identity of (v,p,n2)
(cid:73) identity of (n1,p,n2)

(cid:73) quadruple

(cid:73) identity of (v,n1,p,n2)

  2350 =

  2351 =
. . .

if v=   ate    p=   with    and y=   v   
otherwise
if v=   ate    p=   with    and y=   o   
otherwise

28 / 58

pp-attachment

more features?
(cid:73) corpus level

(cid:73) did we see the (v,p) pair in a 5-word window in a big

(cid:73) did we see the (n1,p) pair in a 5-word window in a big

(cid:73) did we see the (n1,p,n2) triplet in a 5-word window in a big

corpus?

corpus?

corpus?

(cid:73) we can also use counts, or binned counts.

(cid:73) distance

(cid:73) distance (in words) between v and p
(cid:73) distance (in words) between n1 and p
(cid:73) . . .

29 / 58

pp-attachment

(cid:73) compute features for each example.
(cid:73) feed into a machine learning algorithm (for example id166

or id88).

(cid:73) get a weight vector.
(cid:73) classify new examples.

30 / 58

pos-tagging revisited

31 / 58

pos-tagging

x = x1, . . . , xn = holly came from miami , fla

y = y1, . . . , yn = nnp vbd in nnp , nnp

32 / 58

pos-tagging

x = x1, . . . , xn = holly came from miami , fla

y = y1, . . . , yn = nnp vbd in nnp , nnp

(cid:73) our job is to predict y. we will score each yi in turn.

32 / 58

pos-tagging

x = x1, . . . , xn = holly came from miami , fla

y = y1, . . . , yn = nnp vbd in nnp , nnp

(cid:73) our job is to predict y. we will score each yi in turn.

(cid:73) reminder: in the id48 case we had:

score(yi|x, yi   1, yi   2) = q(yi|yi   1, yi   2)e(xi|yi)

where e, q are id113 estimates.

32 / 58

pos-tagging

x = x1, . . . , xn = holly came from miami , fla

y = y1, . . . , yn = nnp vbd in nnp , nnp

(cid:73) our job is to predict y. we will score each yi in turn.

(cid:73) reminder: in the id48 case we had:

score(yi|x, yi   1, yi   2) = q(yi|yi   1, yi   2)e(xi|yi)

where e, q are id113 estimates.
(cid:73) in the feature based approach:

predict(yi|x, y[0:i   1]) = arg max
yi   tagset

w      (x, y[0:i   1], i, yi)

(cid:73) we de   ne    to take position into account:   (x, y[0:i   1], i, yi)

32 / 58

pos-tagging

feature examples

sequence:

(cid:40)
(cid:40)
(cid:40)
(cid:40)

  349 =

  355 =

  392 =

local:

  231 =

1 if yi   1=   jj   ,     yi   2=   dt        yi=   nn   
0 otherwise
1 if     yi   1=   jj        yi=   nn   
0 otherwise
1 if yi=   nn   
0 otherwise

1 if xi=   saw        yi=   vbd   
0 otherwise

33 / 58

pos-tagging

feature examples

sequence:

(cid:40)
(cid:40)
(cid:40)
(cid:40)

  349 =

  355 =

  392 =

local:

  231 =

1 if yi   1=   jj   ,     yi   2=   dt        yi=   nn   
0 otherwise
1 if     yi   1=   jj        yi=   nn   
0 otherwise
1 if yi=   nn   
0 otherwise

1 if xi=   saw        yi=   vbd   
0 otherwise

(cid:73) these correspond to

q and e from the
id48.

(cid:73) did we gain anything

yet?

33 / 58

pos-tagging

more feature examples

(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)

1
0
1
0

1
0
1
0
1
0

3 suf   x:

  121 =

  122 =
. . .

2 suf   x:

  222 =

  225 =

  228 =
. . .

if xi endswith    ing        yi=   vbd   
otherwise
if xi endswith    int        yi=   vbd   
otherwise

if xi endswith    ed        yi=   vbd   
otherwise
if xi endswith    ng        yi=   vbd   
otherwise
if xi endswith    he        yi=   vbd   
otherwise

34 / 58

pos-tagging

more feature examples

word-features:

(cid:40)
(cid:40)
(cid:40)
(cid:40)
(cid:40)

1
0
1
0
1
0
1
0
1
0

  552 =

  553 =

  554 =

  555 =

  556 =
. . .

if hashyphen(xi)     yi=   jj   
otherwise
if hasdigit(xi)     yi=   jj   
otherwise
if alldigits(xi)     yi=   jj   
otherwise
if isupper(xi)     yi=   jj   
otherwise
if allupper(xi)     yi=   jj   
otherwise

35 / 58

pos-tagging

more feature examples

(cid:40)
(cid:40)
(cid:40)
(cid:40)

  621 =

next word:
1
0
1
0

  649 =

  721 =

prev word:
1
0
1
0

  749 =
. . .

if xi+1 endswith    ing        yi=   jj   
otherwise
if xi+1=   yesterday        yi=   jj   
otherwise

. . .

if xi   1 endswith    ing        yi=   jj   
otherwise
if xi   1=   yesterday        yi=   jj   
otherwise

36 / 58

pos-tagging

(cid:73) we gained much more    exibility in what information

sources we can use in the scoring.

(cid:73) but how do we tag?

(cid:73) can we use viterbi?

37 / 58

pos-tagging

(cid:73) we cannot use viterbi because the scores are

   meaningless   .

(cid:73) but greedy tagging works quite well!
1: input: sentence x, parameter-vector w, feature extractor   
2: y     none
3: for i in 1, . . . ,|x| do
4:
5: return y

yi     arg maxyi   tagset w      (x, y[0:i   1], i, yi)

38 / 58

pos-tagging

(cid:73) we cannot use viterbi because the scores are

   meaningless   .

(cid:73) but greedy tagging works quite well!
1: input: sentence x, parameter-vector w, feature extractor   
2: y     none
3: for i in 1, . . . ,|x| do
4:
5: return y
(cid:73) this was a bad idea before. what changed?

yi     arg maxyi   tagset w      (x, y[0:i   1], i, yi)

38 / 58

pos-tagging

improving greedy tagging

training algorithm
1: w     0
2: for x, y in examples do
for i in 1, . . . ,|x| do
3:
4:
5:
6:
7: return w

  yi     arg maxyi   tagset w      (x, y[0:i   1], i, yi)
if   yi (cid:54)= yi then

w     w +   (x, y[0:i   1], i, yi)       (x, y[0:i   1], i,   yi)

39 / 58

pos-tagging

improving greedy tagging
better training algorithm
1: w     0
2: for x, y in examples do
y(cid:48)     y
3:
for i in 1, . . . ,|x| do
4:
5:
6:
7:

w     w +   (x, y(cid:48)

i       yi
y(cid:48)
8:
9: return w

  yi     arg maxyi   tagset w      (x, y(cid:48)
if   yi (cid:54)= yi then

[0:i   1], i, yi)
[0:i   1], i, yi)       (x, y(cid:48)

[0:i   1], i,   yi)

40 / 58

pos-tagging

improving greedy tagging
better training algorithm
1: w     0
2: for x, y in examples do
y(cid:48)     y
3:
for i in 1, . . . ,|x| do
4:
5:
6:
7:

w     w +   (x, y(cid:48)

i       yi
y(cid:48)
8:
9: return w

  yi     arg maxyi   tagset w      (x, y(cid:48)
if   yi (cid:54)= yi then

[0:i   1], i, yi)
[0:i   1], i, yi)       (x, y(cid:48)

[0:i   1], i,   yi)

(cid:73) predict yi based on previous predictions,

can recover from errors.

40 / 58

pos-tagging

(cid:73) greedy tagging can be quite accurate (and very fast).

41 / 58

pos-tagging

(cid:73) greedy tagging can be quite accurate (and very fast).
(cid:73) but a global search is still preferable.
(cid:73) what if we do want to have global search?

41 / 58

pos-tagging

(cid:73) greedy tagging can be quite accurate (and very fast).
(cid:73) but a global search is still preferable.
(cid:73) what if we do want to have global search?
    we need meaningful scores.

41 / 58

log-linear modeling

42 / 58

log-linear modeling

(cid:73) our linear classi   ers give us a score:

score(x, y[0:i   1], i, yi) = w      (x, y[0:i   1], i, yi)

(cid:73) this is great, if we are looking for the best y.
(cid:73) but sometimes we do want a id203:

p(yi|x, y[0:i   1], i)

43 / 58

log-linear modeling

(cid:73) our linear classi   ers give us a score:

score(x, y[0:i   1], i, yi) = w      (x, y[0:i   1], i, yi)

(cid:73) this is great, if we are looking for the best y.
(cid:73) but sometimes we do want a id203:

p(yi|x, y[0:i   1], i)

(cid:73) in a log linear model, we de   ne:

p(yi|x, y[0:i   1], i) =

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)

i )

43 / 58

log-linear modeling
name

(cid:73) in a log linear model, we de   ne:

p(yi|x, y[0:i   1], i) =

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)

i )

(cid:73) why is it called log-linear?

log p(yi|x, y[0:i   1], i) = w    (x, y[0:i   1], i, yi)   log (cid:88)

i   tagset
y(cid:48)

ew    (x,y[0:i   1],i,y(cid:48)
i )

44 / 58

log-linear modeling
training objective

(cid:73) in a log linear model, we de   ne:

p(yi|x, y[0:i   1], i) =

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)

i )

(cid:73) the exponentiation makes everything positive.
(cid:73) the denominator is normalizing everything to sum to 1.

    the result is a formal id203.
    but is it the    real    id203?

45 / 58

log-linear modeling
training objective

(cid:73) in a log linear model, we de   ne:

p(yi|x, y[0:i   1], i) =

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)

i )

(cid:73) the exponentiation makes everything positive.
(cid:73) the denominator is normalizing everything to sum to 1.

    the result is a formal id203.
    but is it the    real    id203?

(cid:73) the job of learning is to    nd w such to maximize:

(cid:88)

(cid:88)

x,y   data

i   1,...,|x|

log p(yi|x, y[0:i   1], i)

(maximize the log likelihood of the data)

45 / 58

log-linear modeling
smoothing

(cid:73) in a log linear model, we de   ne:

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)
(cid:73) the job of learning is to    nd w such to maximize:

p(yi|x, y[0:i   1], i) =
(cid:88)

(cid:88)

i )

x,y   data

i   1,...,|x|

log p(yi|x, y[0:i   1], i)

46 / 58

log-linear modeling
smoothing

(cid:73) in a log linear model, we de   ne:

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)
(cid:73) the job of learning is to    nd w such to maximize:

p(yi|x, y[0:i   1], i) =
(cid:88)

(cid:88)

log p(yi|x, y[0:i   1], i)

i )

x,y   data

i   1,...,|x|

(cid:73) we can make it better by adding a id173 term:

(cid:88)

(cid:88)

x,y   data

i   1,...,|x|

log p(yi|x, y[0:i   1], i)       
2

(cid:88) w2

i

(cid:73) this prefers that most weights are small     avoid over   tting

46 / 58

log-linear modeling
note

(cid:73) this form of the model and objective is specialized to our

tagging setup:

p(yi|x, y, i) =

(cid:88)

(cid:88)

x,y   data

i   1,...,|x|

arg max
w   rm

(cid:80)

ew    (x,y[0:i   1],i,yi)
i   tagset ew    (x,y[0:i   1],i,y(cid:48)
y(cid:48)

i )

log p(yi|x, y[0:i   1], i)       
2

(cid:88) w2

i

(cid:73) the general form is:

p(y|x) =

(cid:88)

(cid:80)

ew    (x,y)
y(cid:48) ew    (x,y(cid:48))
log p(y|x)       
2

arg max
w   rm

x,y   data

(cid:88) w2

i

47 / 58

log-linear modeling
summary

(cid:73) we de   ned p(y|x) =
(cid:73) and train the model to optimize the data log-likelihood.

ew    (x,y)
i   tagset ew    (x,y)
y(cid:48)

(cid:80)

(cid:73) we also added a id173 term.

48 / 58

log-linear modeling
summary

(cid:80)

(cid:73) we de   ned p(y|x) =
(cid:73) and train the model to optimize the data log-likelihood.

ew    (x,y)
i   tagset ew    (x,y)
y(cid:48)

(cid:73) we also added a id173 term.

(cid:73) this is a very common approach, and goes by two names:

(cid:73) multinomail id28
(cid:73) maximum id178 model (maxent)

48 / 58

log-linear modeling
summary

(cid:80)

(cid:73) we de   ned p(y|x) =
(cid:73) and train the model to optimize the data log-likelihood.

ew    (x,y)
i   tagset ew    (x,y)
y(cid:48)

(cid:73) we also added a id173 term.

(cid:73) this is a very common approach, and goes by two names:

(cid:73) multinomail id28
(cid:73) maximum id178 model (maxent)

(cid:73) there are many tools for training such models:

(cid:73) megam
(cid:73) weka
(cid:73) liblinear (when you pass the correct options)
(cid:73) vw (when you pass in the correct options)
(cid:73) scikit-learn (when you pass in the correct options)
(cid:73) many others
(cid:73) (some of you implemented it in the deep-learning course...)

48 / 58

back to tagging

memm     maximum id178 markov model

49 / 58

memm     maxent markov models

(cid:73) in a trigram id48, we had:

(cid:89)

i   1,...,|x|

p(x, y) =

q(yi|yi   1, yi   2)e(xi|yi)

  y = arg max

t

p(x, y)

50 / 58

memm     maxent markov models

(cid:73) in a trigram id48, we had:

(cid:89)

i   1,...,|x|

p(x, y) =

q(yi|yi   1, yi   2)e(xi|yi)

  y = arg max

t

p(x, y)

(cid:73) in a trigram memm we have:

(cid:89)

i   1,...,|x|

p(y|x) =

p(yi|x, yi   1, yi   2)

  y = arg max

y

p(y|x)

50 / 58

memm     maxent markov models

(cid:73) in a trigram id48, we had:

(cid:89)

i   1,...,|x|

p(x, y) =

q(yi|yi   1, yi   2)e(xi|yi)

  y = arg max

t

p(x, y)

(cid:73) in a trigram memm we have:

p(y|x) =

p(yi|x, yi   1, yi   2) =

(cid:89)

i   1,...,|x|

(cid:89)

i   1,...,|x|

(cid:80)

ew    (x,y[0:i   1],i,yi)
ew    (x,y[0:i   1],i,y(cid:48)
i )
y(cid:48)
i

  y = arg max

y

p(y|x)

50 / 58

memm     maxent markov models

(cid:89)

(cid:80)

ew    (x,y[0:i   1],i,yi)
ew    (x,y[0:i   1],i,y(cid:48)
i )
y(cid:48)
i

(cid:73) in a trigram memm we have:

(cid:89)

p(y|x) =

p(yi|x, yi   1, yi   2) =

i   1,...,|x|

i   1,...,|x|
p(y|x)
    train w using a maxent learner (new)
    decode (solve the argmax) using viterbi (like before)

  y = arg max

y

51 / 58

pos-tagging

summary

(cid:73) memm combine the    exibility of the feature-based

approach with the global search and id203 score of
id48

(cid:73) but greedy decoding is very fast and very competitive.
(cid:73) memm still a    locally-trained    model.

52 / 58

structured id88

53 / 58

pos-tagging

(cid:73) memm combine the    exibility of the feature-based

approach with the global search and id203 score of
id48

(cid:73) but greedy decoding is very fast and very competitive.
(cid:73) memm still a    locally-trained    model.
    can we have a globally trained model?

54 / 58

pos-tagging

(cid:73) memm combine the    exibility of the feature-based

approach with the global search and id203 score of
id48

(cid:73) but greedy decoding is very fast and very competitive.
(cid:73) memm still a    locally-trained    model.
    can we have a globally trained model?
(cid:73) assume for now that we do not care about id203.

54 / 58

pos-tagging

    can we have a globally trained model?
(cid:73) assume for now that we do not care about id203.
(cid:73) de   ne score(x, y, i, yi) = w      (x, y, i, yi)

(cid:73) then the sequence score is score(x, y) =(cid:80)

(cid:73) we can    nd arg maxy score(x, y) using viterbi.

i w      (x, y, i, yi)

(cid:73) (assuming    is    well behaved        not looking at yi   3 or yi+1)

(cid:73) let   s write:

  (x, y) =

  (x, y, i, yi)

(cid:88)

(cid:73) and now: score(x, y) = w      (x, y)
(cid:73) we need w      (x, y) > w      (x, y(cid:48)) for all incorrect y(cid:48).

i

55 / 58

structured id88

  (x, y) =

(cid:88)

  (x, y, i, yi)

i

score(x, y) = w      (x, y)

we need w      (x, y) > w      (x, y(cid:48)) for all incorrect y(cid:48)

56 / 58

structured id88

  (x, y) =

(cid:88)

  (x, y, i, yi)

i

score(x, y) = w      (x, y)

we need w      (x, y) > w      (x, y(cid:48)) for all incorrect y(cid:48)

training     structured id88
1: w     0
2: for x, y in examples do
3:
4:
5:
6: return w

  y     arg maxy(cid:48) w      (x, y(cid:48))
if   y (cid:54)= y then

w     w +   (x, y)       (x,   y)

(cid:46) using viterbi

56 / 58

structured id88

  (x, y) =

(cid:88)

  (x, y, i, yi)

i

score(x, y) = w      (x, y)

we need w      (x, y) > w      (x, y(cid:48)) for all incorrect y(cid:48)

training     structured id88
1: w     0
2: for x, y in examples do
3:
4:
5:
6: return w

  y     arg maxy(cid:48) w      (x, y(cid:48))
if   y (cid:54)= y then

w     w +   (x, y)       (x,   y)

(cid:46) using viterbi

56 / 58

structured id88 tagger

(cid:73) globally optimized     optimizing directly what we care about
(cid:73) does not provide probabilities

57 / 58

structured id88 tagger

(cid:73) globally optimized     optimizing directly what we care about
(cid:73) does not provide probabilities

(cid:73) there are global model that do provide probabilities     crfs
(cid:73) crfs are the log-linear version of the structured-percepton.

57 / 58

summary

discriminative learning

(cid:73) feature representation, feature vectors
(cid:73) linear models (binary, multiclass)

(cid:73) the id88 algorithm

(cid:73) id148
(cid:73) id173

58 / 58

summary

discriminative learning

(cid:73) feature representation, feature vectors
(cid:73) linear models (binary, multiclass)

(cid:73) the id88 algorithm

(cid:73) id148
(cid:73) id173

sequence tagging
(cid:73) id48 (last time)
(cid:73) greedy, classi   er based
(cid:73) greedy, classi   er based     improved
(cid:73) memm
(cid:73) structured id88

58 / 58

