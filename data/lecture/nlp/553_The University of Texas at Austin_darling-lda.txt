a theoretical and practical implementation

tutorial on id96 and id150

william m. darling

school of computer science

university of guelph

december 1, 2011

abstract

this technical report provides a tutorial on the theoretical details
of probabilistic id96 and gives practical steps on implement-
ing topic models such as id44 (lda) through the
id115 approximate id136 algorithm gibbs sam-
pling.

1

introduction

following its publication in 2003, blei et al.   s id44 (lda)
[3] has made id96     a sub   eld of machine learning applied to ev-
erything from computational linguistics [4] to bioinformatics [8] and political
science [2]     one of the most popular and most successful paradigms for both
supervised and unsupervised learning. despite id96   s undisputed
popularity, however, it is for many     particularly newcomers     a di   cult area
to break into due to its relative complexity and the common practice of leav-
ing out implementation details in papers describing new models. while key
update equations and other details on id136 are often included, the inter-
mediate steps used to arrive at these conclusions are often left out due to space
constraints, and what details are given are rarely enough to enable most re-
searchers to test the given results for themselves by implementing their own
version of the described model. the purpose of this technical report is to help
bridge the gap between the model de   nitions provided in research publications
and the practical implementations that are required for performing learning in
this exciting area. ultimately, it is hoped that this tutorial will help enable the
reader to build his or her own novel topic models.

this technical report will describe what id96 is, how various mod-
els (lda in particular) work, and most importantly, how to implement a work-
ing system to perform learning with topic models. id96 as an area
will be introduced through the section on lda, as it is the    original    topic model

1

and its modularity allows the basics of the model to be used in more complicated
topic models.1 following the introduction to id96 through lda, the
problem of posterior id136 will be discussed. this section will concentrate
   rst on the theory of the stochastic approximate id136 technique gibbs sam-
pling and then it will discuss implementation details for building a topic model
gibbs sampler.

2 id44

lda is a generative probabilistic model for collections of grouped discrete data
[3]. each group is described as a random mixture over a set of latent topics where
each topic is a discrete distribution over the collection   s vocabulary. while lda
is applicable to any corpus of grouped discrete data, from now on i will refer
to the standard nlp use case where a corpus is a collection of documents, and
the data are words. the generative process for a document collection d under
the lda model is as follows:

1. for k = 1...k:

(a)   (k)     dirichlet(  )
2. for each document d     d:

(a)   d     dirichlet(  )
(b) for each word wi     d:
i. zi     discrete(  d)
ii. wi     disctete(  (zi))

where k is the number of latent topics in the collection,   (k) is a discrete
id203 distribution over a    xed vocabulary that represents the kth topic
distribution,   d is a document-speci   c distribution over the available topics, zi is
the topic index for word wi, and    and    are hyperparameters for the symmetric
dirichlet distributions that the discrete distributions are drawn from.

the generative process described above results in the following joint distri-

bution:

p(w, z,   ,   |  ,   ) = p(  |  )p(  |  )p(z|  )p(w|  z)

(1)

the unobserved (latent) variables z,   , and    are what is of interest to us. each
  d is a low-dimensional representation of a document in    topic   -space, each zi
represents which topic generated the word instance wi, and each   (k) represents
a k    v matrix where   i,j = p(wi|zj). therefore, one of the most interesting
aspects of lda is that it can learn, in an unsupervised manner, words that

1while lda is an extension to probabilistic latent semantic analysis [12] (which in turn has
ideological routes in the id105 technique lsi), the id96    revolution   
really took o    with the introduction of lda likely due to its fully probabilistic grounding.

2

   environment   

emission

environmental

air

permit
plant
facility

unit
epa
water
station

   travel   
travel
hotel

roundtrip

fares
special

o   er
city
visit
miles
deal

   fantasy football   

game
yard

defense
allowed
fantasy
point
passing

rank

against
team

table 1: three topics learned using lda on the enron email dataset.

we would associate with certain topics, and this is expressed through the topic
distributions   . an example of the top 10 words for 3 topics learned using lda
on the enron email dataset2 is shown in figure 1 (the topic labels are added
manually).

3

id136

the key problem in id96 is posterior id136. this refers to reversing
the de   ned generative process and learning the posterior distributions of the
latent variables in the model given the observed data. in lda, this amounts to
solving the following equation:

p(  ,   , z|w,   ,   ) =

p(  ,   , z, w|  ,   )

p(w|  ,   )

(2)

unfortunately, this distribution is intractable to compute. the id172
factor in particular, p(w|  ,   ), cannot be computed exactly. all is not lost,
however, as there are a number of approximate id136 techniques available
that we can apply to the problem including variational id136 (as used in the
original lda paper) and id150 (as we will use here).

3.1 id150

3.1.1 theory

id150 is one member of a family of algorithms from the markov chain
monte carlo (mcmc) framework [9]. the mcmc algorithms aim to construct
a markov chain that has the target posterior distribution as its stationary dis-
tribution. in other words, after a number of iterations of stepping through the
chain, sampling from the distribution should converge to be close to sampling

2http://www.cs.cmu.edu/~enron/.

3

from the desired posterior. id150 is based on sampling from condi-
tional distributions of the variables of the posterior.

for example, to sample x from the joint distribution p(x) = p(x1, ..., xm),
where there is no closed form solution for p(x), but a representation for the
conditional distributions is available, using id150 one would perform
the following (from [1]):

1. randomly initialize each xi

2. for t = 1, ..., t :

2.1 xt+1

2.2 xt+1

2 , x(t)

1     p(x1|x(t)
2     p(x2|x(t+1)
m     p(xm|x(t+1)

3 , ..., x(t)
m )
3 , ..., x(t)
, x(t)
m )
, x(t+1)

1

1

2

2.m xt+1

, ..., x(t+1)
m   1 )

this procedure is repeated a number of times until the samples begin to con-
verge to what would be sampled from the true distribution. while convergence
is theoretically guaranteed with id150, there is no way of knowing
how many iterations are required to reach the stationary distribution. there-
fore, diagnosing convergence is a real problem with the id150 ap-
proximate id136 method. however, in practice it is quite powerful and has
fairly good performance. typically, an acceptable estimation of convergence
can be obtained by calculating the log-likelihood or even, in some situations, by
inspection of the posteriors.

for lda, we are interested in the latent document-topic portions   d, the
topic-word distributions   (z), and the topic index assignments for each word
zi. while conditional distributions     and therefore an lda id150
algorithm     can be derived for each of these latent variables, we note that both
  d and   (z) can be calculated using just the topic index assignments zi (i.e. z is a
su   cient statistic for both these distributions).3 therefore, a simpler algorithm
can be used if we integrate out the multinomial parameters and simply sample
zi. this is called a collapsed gibbs sampler.

the collapsed gibbs sampler for lda needs to compute the id203 of a
topic z being assigned to a word wi, given all other topic assignments to all other
words. somewhat more formally, we are interested in computing the following
posterior up to a constant:

p(zi|z   i,   ,   , w)

(3)

where z   i means all topic allocations except for zi. to begin, the rules of
id155 tell us that:

p(zi|z   i,   ,   , w) =

p(zi, z   i, w|  ,   )
p(z   i, w|  ,   )

3  d,z = n(d,z)+  

(cid:80)|z| n(d,z)+   ,   z,w =

(cid:80)|w | n(z,w)+   .

n(z,w)+  

    p(zi, z   i, w|  ,   ) = p(z, w|  ,   )

(4)

4

we then have:

(cid:90) (cid:90)

p(w, z|  ,   ) =

p(z, w,   ,   |  ,   )d  d  

(5)

following the lda model de   ned in equation (1), we can expand the above
equation to get:

p(w, z|  ,   ) =

p(  |  )p(  |  )p(z|  )p(w|  z)d  d  

then, we group the terms that have dependent variables:

p(w, z|  ,   ) =

p(z|  )p(  |  )d  

p(w|  z)p(  |  )d  

(cid:90)

(6)

(7)

(cid:90) (cid:90)
(cid:90)

both terms are multinomials with dirichlet priors. because the dirichlet dis-
tribution is conjugate to the multinomial distribution, our work is vastly sim-
pli   ed; multiplying the two results in a dirichlet distribution with an adjusted
parameter. beginning with the    rst term, we have:

(cid:89)

k

    k
d,kd  d

(cid:90)

(cid:90) (cid:89)

p(z|  )p(  |  )d   =

=

=

1

b(  )

  d,zi

(cid:90) (cid:89)

i

1

b(  )

k
b(nd,   +   )

b(  )

  nd,k+  k
d,k

d  d

(8)

where nd,k is the number of times words in document d are assigned to topic k, a
   indicates summing over that index, and b(  ) is the multinomial beta function,
b(  ) =
k   k) . similarly, for the second term (calculating the likelihood of
words given certain topic assignments):

k   (  k)

(cid:81)
  ((cid:80)
(cid:90)

p(w|  z)p(  |  )d   =

    w
k,wd  k

combining equations (8) and (9), the expanded joint distribution is then:

p(w, z|  ,   ) =

(9)

(10)

(cid:89)

1

(cid:89)

b(  )

w

k

    w+nk,w
k,w

d  k

  zd,i,wd,i

(cid:90) (cid:89)

(cid:89)

i

1

(cid:90) (cid:89)
(cid:89)
(cid:89)

k

d

=

=

b(  )

w
b(nk,   +   )

b(  )

k

(cid:89)

k

b(nk,   +   )

b(  )

(cid:89)

d

b(nd,   +   )

b(  )

5

the id150 equation for lda can then be derived using the chain rule
(where we leave the hyperparameters    and    out for clarity).4 note that the
superscript (   i) signi   es leaving the ith token out of the calculation:

p(zi|z(   i), w) =

p(w|z)

p(w(   i)|z(   i))p(wi)

  

p(z)

p(w, z)

d

=

p(z(   i))

p(w, z(   i))

b(nd,   +   )
b(n(   i)
d,   +   )

   (cid:89)
(cid:89)
b(nk,   +   )
      (nd,k +   k)  ((cid:80)k
b(n(   i)
k,   +   )
d,k +   k)  ((cid:80)k
k=1 n(   i)
d,k +   k)
k=1 nd,k +   k)
(cid:80)
n(   i)
k,w +   w
w(cid:48) n(   i)

    (n(   i)

  (n(   i)

d,k +   k)

k,w(cid:48) +   w(cid:48)

k

     (nk,w +   w)  ((cid:80)w
k,w +   w)  ((cid:80)w

  (n(   i)

w=1 n(   i)
k,w +   w)
w=1 nk,w +   w)

(11)

3.1.2 implementation

implementing an lda collapsed gibbs sampler is surprisingly straightforward.
it involves setting up the requisite count variables, randomly initializing them,
and then running a loop over the desired number of iterations where on each
loop a topic is sampled for each word instance in the corpus. following the
gibbs iterations, the counts can be used to compute the latent distributions   d
and   k.

the only required count variables include nd,k, the number of words assigned
to topic k in document d; and nk,w, the number of times word w is assigned
to topic k. however, for simplicity and e   ciency, we also keep a running count
of nk, the total number of times any word is assigned to topic k. finally, in
addition to the obvious variables such as a representation of the corpus (w), we
need an array z which will contain the current topic assignment for each of the
n words in the corpus.

because the id150 procedure involves sampling from distributions
conditioned on all other variables (in lda this of course includes all other cur-
rent topic assignments, but not the current one), before building a distribution
from equation (11), we must remove the current assignment from the equation.
we can do this by decrementing the counts associated with the current assign-
ment because the topic assignments in lda are exchangeable (i.e.
the joint
id203 distribution is invariant to permutation). we then calculate the
(unnormalized) id203 of each topic assignment using equation (11). this
discrete distribution is then sampled from and the chosen topic is set in the z
array and the appropriate counts are then incremented. see algorithm 1 for
the full lda id150 procedure.

4for the full, nothing-left-out derivation, please see [5] and [11].

6

input: words w     documents d
output: topic assignments z and counts nd,k, nk,w, and nk
begin

for i = 0     n     1 do

randomly initialize z and increment counters
foreach iteration do
word     w[i]
topic     z[i]
nd,topic-=1; nword,topic-=1; ntopic-=1
for k = 0     k     1 do
p(z = k|  ) = (nd,k +   k) nk,w+  w
nk+    w

end
topic     sample from p(z|  )
z[i]     topic
nd,topic+=1; nword,topic+=1; ntopic+=1

end

end
return z, nd,k, nk,w, nk

end

algorithm 1: lda id150

4 extensions to lda

while lda     the    simplest    topic model     is useful in and of itself, a great
deal of novel research surrounds extending the basic lda model to    t a speci   c
task or to improve the model by describing a more complex generative process
that results in a better model of the real world. there are countless papers
delineating such extensions and it is not my intention to go through them all
here. instead, this section will outline some of the ways that lda can and has
been extended with the goal of explaining how id136 changes as a result of
additions to a model and how to implement those changes in a gibbs sampler.

4.1 lda with a background distribution

one of the principal problems with lda is that for useful results, stop-words
must be removed in a pre-processing step. without this    ltering, very common
words such as the, of, to, and, a, etc. will pervade the learned topics, hiding the
statistical semantic word patterns that are of interest. while stop-word removal
does a good job at solving this problem, it is an ad hoc measure that results in a
model resting on a non-coherent theoretical basis. further, stop-word removal
is not without problems. stop-word lists must often be domain-dependent,
and there are inevitably cases where    ltering results in under-coverage or over-
coverage, causing the model to continue being plagued by noise, or missing
patterns that may be of interest to us.

one approach to keep stop-words out of the topic distributions is to imag-

7

ine all stop-words being generated by a    background    distribution [6, 7, 10].
the background distribution is the same as a topic     it is a discrete id203
distribution over the corpus vocabulary     but every document draws from the
background as well as the topics speci   c to that document. [7] and [10] use this
approach to separate high-content words from less-important words to perform
id57.
[6] uses a similar model for information re-
trieval where a word can either be generated from a background distribution, a
document-speci   c distribution, or one of t topic distributions shared amongst
all the documents. the generative process is similar to that of lda, except
that there is a multinomial variable x associated with each word that is over the
three di   erent    sources    of words. when x = 0, the background distribution
generates the word, when x = 1, the document-speci   c distribution generates
the word, and when x = 2, one of the topic distributions generates the word.

here, we will describe a simpler model where only a background distribution
is added to lda. a binomial variable x is associated with each word that
decides whether the word will be generated by the topic distributions or by the
background. the generative process is then:

1.        dirichlet(  )

2. for k = 1...k:

(a)   (k)     dirichlet(  )
3. for each document d     d:

(a)   d     dirichlet(  )
(b)   d     dirichlet(  )
(c) for each word wi     d:
i. xi     discrete(  d)
ii. if x = 0:

a. wi     discrete(  )

iii. else:

a. zi     discrete(  d)
b. wi     disctete(  (zi))

where    is the background distribution, and   d is a document-speci   c binomial
sampled from a dirichlet prior   .

developing a gibbs sampler for this model is similar to the lda imple-
mentation, but we have to be careful about when counts are incremented and
decremented. we only adjust the background-based counts when the back-
ground was sampled as the word generator, and we only adjust the topic counts
when it is the converse. we must update the x-based counts each time, however,
because we sample the route that led to the word being generated each time.
the sampler must compute the id203 not only of a topic being chosen
for the given document and the id203 of that topic generating the given

8

word, it must also compute the id203 that the model is in the topic-model
state. this too, however, is straightforward to implement. a distribution of
t + 1 components can be created for each word (on each iteration) where the
   rst component corresponds to the background distribution generating the word
and the other t are the probabilities for each topic having generated the word.

5 conclusion

lda and other topic models are an exciting development in machine learn-
ing and the surface has only been scratched on their potential in a number of
diverse    elds. this report has sought to aid researchers new to the    eld in
both understanding the mathematical underpinnings of id96 and in
implementing algorithms to make use of this new pattern recognition paradigm.

references

[1] christopher m. bishop. pattern recognition and machine learning (infor-
mation science and statistics). springer-verlag new york, inc., secaucus,
nj, usa, 2006.

[2] david m. blei and sean gerrish. predicting legislative roll calls from text.

in international conference on machine learning, 2011.

[3] david m. blei, andrew y. ng, and michael i. jordan. latent dirichlet

allocation. j. mach. learn. res., 3:993   1022, 2003.

[4] jordan boyd-graber, david blei, and xiaojin zhu. a topic model for
in empirical methods in natural language

id51.
processing, 2007.

[5] bob carpenter. integrating out multinomial parameters in latent dirichlet
allocation and naive bayes for collapsed id150. technical report,
lingpipe, inc., 2010.

[6] chaitanya chemudugunta, padhraic smyth, and mark steyvers. modeling
general and speci   c aspects of documents with a probabilistic topic model.
in nips, pages 241   248, 2006.

[7] hal daum  e, iii and daniel marcu. bayesian query-focused summarization.
in acl-44: proceedings of the 21st international conference on compu-
tational linguistics and the 44th annual meeting of the association for
computational linguistics, pages 305   312, morristown, nj, usa, 2006.
association for computational linguistics.

[8] georg k gerber, robin d dowell, tommi s jaakkola, and david k gi   ord.
automated discovery of functional generality of human gene expression
programs. plos comput biol, 3(8):e148, 2007.

9

[9] w. r. gilks, s. richardson, and d. j. spiegelhalter. markov chain monte

carlo in practice. chapman and hall/crc, 1999.

[10] aria haghighi and lucy vanderwende. exploring content models for multi-
document summarization. in naacl    09: proceedings of human language
technologies: the 2009 annual conference of the north american chapter
of the association for computational linguistics, pages 362   370, morris-
town, nj, usa, 2009. association for computational linguistics.

[11] gregor heinrich. parameter estimation for text analysis. technical report,

university of leipzig, germany, 2008.

[12] thomas hofmann. probabilistic latent semantic analysis. in in proc. of

uncertainty in arti   cial intelligence, uai99, pages 289   296, 1999.

10

