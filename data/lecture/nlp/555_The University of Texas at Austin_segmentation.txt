sequence segmentation models

yoav goldberg

bar ilan university

1 / 52

part-of-speech tagging: discussion

i part-of-speech tagging is a    solved    problem.
i it is robust, well understood, and performs well.
i there are many off-the-shelf, downloadable tools:

i tnt
i stanford tagger
i id166tool
i clearnlp
i . . . many others

i not all is good, though.

i drop in accuracy when moving to a new domain.
i web, reviews, email,    ction, medical, old text, . . .

6 / 52

part-of-speech tagging: discussion

i part-of-speech tagging is a    solved    problem.
i it is robust, well understood, and performs well.
i there are many off-the-shelf, downloadable tools:

i tnt
i stanford tagger
i id166tool
i clearnlp
i . . . many others

i not all is good, though.

i drop in accuracy when moving to a new domain.
i web, reviews, email,    ction, medical, old text, . . .

i languages other than english.

i what if there is not much training data available?
i in   ections (many word forms ! oov, but also more hints)
i words may not be white-space delimited:  ;*"",  952/&.

6 / 52

bad language?

    wei xu     socialmedia-class.org 

lti

non-standard spellings

multi-word
abbreviations

hashtags

also: at-mentions, urls, emoticons, symbols, typos, etc.

lti

id32 id121 is unsuitable for twitter:

@user1 omg ur from pa ? i am too (: where abouts ?

you   re

i   m going to

@user2 ima get me a flip phone for real

lti

id32 id121 is unsuitable for twitter:

nominal+verbal

@user1 omg ur from pa ? i am too (: where abouts ?

you   re

i   m going to

@user2 ima get me a flip phone for real

nominal+verbal

solution:  

don   t try to tokenize these
instead, introduce compound tags

lti

hashtags

twitter hashtags are sometimes used as ordinary words 
(35% of the time) and other times as topic markers

proper noun

innovative , but traditional , too ! another 
fun one to watch on the #ipad ! 
http://bit.ly/ @user1 #utcd2 #utpol #tcot

we only use    hashtag    for topic markers

hashtag

lti

twitter discourse marker

retweet construction:

rt @user1 : i never bought candy bars from 
those kids on my doorstep so i guess they   re 
all in gangs now .

lti

twitter discourse marker

retweet construction:

rt @user1 : i never bought candy bars from 
those kids on my doorstep so i guess they   re 
all in gangs now .

twitter discourse marker

rt @user2 : lmbo ! this man filed an 
emergency motion for continuance on account 
of the rangers game tonight !        wow lmao

lti

! twitter-specific tags:

hashtag
at-mention
url / email address
emoticon
twitter discourse marker
other (multi-word abbreviations, symbols, garbage)

lti

today: sequence segmentation

pierre vinken , 61 years old , will join the board as a
nonexecutive director , nov. 21, 1987

+ chunking

7 / 52

today: sequence segmentation

pierre vinken , 61 years old , will join the board as a
nonexecutive director , nov. 21, 1987

pierre vinken ,

61 years old ,

the board

z
z

np

}|

z

np

{
}|

adjp

}|
{

a nonexecutive director ,

nov. 21, 1987.

vp

+ chunking
z }| {
{
}|

will join
np

z

z
{

np

}|

{

pp

z}|{as

7 / 52

today: sequence segmentation

pierre vinken , 61 years old , will join the board as a
nonexecutive director , nov. 21, 1987

pierre vinken ,

61 years old ,

the board

z
z

np

}|

z

np

{
}|

adjp

}|
{

a nonexecutive director ,

nov. 21, 1987.

vp

+ chunking
z }| {
{
}|

will join
np

z

z
{

alternative notation

np

}|

{

pp

z}|{as

[np pierre vinken ] , [adjp 61 years old ] , [vp will join ] [np
the board ] [pp as ] [np a nonexecutive director ] , [np nov.
21, 1987 ].

7 / 52

today: sequence segmentation

pierre vinken , 61 years old , will join the board as a
nonexecutive director , nov. 21, 1987

pierre vinken ,

61 years old ,

the board

z
z

np

}|

z

np

{
}|

adjp

}|
{

a nonexecutive director ,

nov. 21, 1987.

vp

+ chunking
z }| {
{
}|

will join
np

z

z
{

np

}|

{

pp

z}|{as

input: sequence. output: typed, non-overlapping spans

7 / 52

today: sequence segmentation

this is a very useful and general task.

8 / 52

sequence segmentation
chunking

chunking
pierre vinken , 61 years old , will join the board as a
nonexecutive director , nov. 21, 1987

pierre vinken ,

61 years old ,

the board

z
z

np

}|

z

np

{
}|

adjp

}|
{

{

z

a nonexecutive director ,

nov. 21, 1987.

+
vp

will join
np

z }| {
}|

z
{

np

}|

{

pp

z}|{as

9 / 52

sequence segmentation
chunking

np chunking
pierre vinken , 61 years old , will join the board as a
nonexecutive director , nov. 21, 1987

pierre vinken , 61 years old , will join

the board as

a nonexecutive director ,

nov. 21, 1987.

+

np

}|

np

}|

{

z
{

{

z

z
z

np

}|

np

{
}|

10 / 52

sequence segmentation
named entities

id39 (ner)
paris whitney hilton (born february 17, 1981) is an american
television personality and businesswoman. she is the
great-granddaughter of conrad hilton, the founder of hilton
hotels. born in new york city and raised in both california and
new york, hilton began a modeling career when she signed
with donald trump   s modeling agency .

identify things
person / location / organization / time / other.

11 / 52

sequence segmentation
named entities

id39 (ner)

{

{

per

}|

z

z
z

z

per

paris whitney hilton (born
television personality and businesswoman . she is the

february 17, 1981) is an american

great-granddaughter of

hilton hotels . born in
loc

loc

california and

new york,

{

org

}|
}|

{

loc

new york city and raised in both

conrad hilton , the founder of

}|
}|
z}|{hilton began a modeling career when

z
z
{

org

per

she signed with

donald trump   s modeling agency .

time

}|
{
{

}|

{

}|

z

z

11 / 52

sequence segmentation
named entities

id39 (ner)

{

{

per

}|

time

}|
{
{

z

z
z

z

per

paris whitney hilton (born
television personality and businesswoman . she is the

february 17, 1981) is an american

great-granddaughter of

hilton hotels . born in
loc

loc

california and

new york,

{

org

}|
}|

{

loc

new york city and raised in both

conrad hilton , the founder of

}|
}|
z}|{hilton began a modeling career when

z
z
{

org

per

donald trump   s modeling agency .

she signed with
q
what   s the difference between np-chunking and ner? when
would you use each?

}|

z

z

}|

{

11 / 52

sequence segmentation
information extraction

bibliographies

12 / 52

sequence segmentation
information extraction

aho, alfred v., and jeffrey d. ullman (1972) the theory of parsing,
translation and compiling, in two volumes, prentice-hall, inc.,
englewood cliffs, nj.

knuth, d.e. (1965)    on the translation of language from left to right,   
information and control 8.6, 607-609

13 / 52

sequence segmentation
information extraction

aho, alfred v., and

jeffrey d. ullman (

author

}|

{

publisher

author

}|

z

title

}|

year
1972)

z}|{

{

{

prentice-hall, inc., englewood cliffs, nj.

the theory of parsing, translation and compiling, in two volumes,

}|

{
{
z}|{
}|

year
1965)    

journal

z

author

}|

information and control

title

}|

volume

z}|{8.6 ,

{

z

607-609

pages

}|

{

knuth, d.e. (

on the translation of language from left to right,   

z
z
z
z
z

{

13 / 52

sequence segmentation
information extraction

seminar announcements

14 / 52

we will begin a new topic on tuesday. andzy zucker will speak
on interactions between logic and topological dynamics.

mathematical logic seminar     november 12 2013
time:

12:00     13:20

room: wean hall 7201

department of mathematical sciences
cmu

title:

interactions between logic and topological dynamics

15 / 52

the cmu mathematical logic seminar will meet tue 5 nov
12:00   1:30 in wean hall 7201.

alexei kolesnikov will conclude his series of talks, and speak
about generalized martin   s axiom and disjoint amalgamation.

note to newcomers: seminar follows a    brown bag lunch   
format and for
the    rst ten minutes or so we chat and have lunch. the actual
talk usually runs 12:10-1:20 or so.

16 / 52

z

z

we will begin a new topic on

tuesday.

andzy zucker will speak

on

interactions between logic and topological dynamics.

topic

z
}|
z

{

date

}|

{

z

speaker

}|

{

{

date

}|

{

mathematical logic seminar    

november 12 2013

time:

12:00     13:20

seminar

}|

time

}|

location

{

}|

{

room:

wean hall 7201

department of mathematical sciences
cmu

title:

interactions between logic and topological dynamics

topic

}|

{

17 / 52

z
z

z

z
z
z

z

date

}|

{

the cmu mathematical logic seminar will meet

tue 5 nov

12:00   1:30 in

wean hall 7201.

time

}|

{

z

speaker

seminar

}|

location

}|

{

{

{

topic

}|

}|

z

alexei kolesnikov will conclude his series of talks, and talk

about

generalized martin   s axiom and disjoint amalgamation.

note to newcomers: seminar follows a    brown bag lunch   
format and for
the    rst ten minutes or so we chat and have lunch. the actual
talk usually runs 12:10-1:20 or so.

{

18 / 52

sequence segmentation
information extraction

gene mention identi   cation
beta-endorphin , acth and cortisol secretion were measured
in twelve healthy adult males after nasal spray administration
200 iu salmon calcitonin .

19 / 52

sequence segmentation
information extraction

gene mention identi   cation
beta-endorphin , acth and cortisol secretion were measured
in twelve healthy adult males after nasal spray administration
200 iu salmon calcitonin .

acth and cortisol secretion were measured
in twelve healthy adult males after nasal spray administration

+

beta-endorphin ,z }| {
z
}|
200 iuz

{
}|

salmon calcitonin .

{

19 / 52

sequence segmentation
information extraction

bioinformatics

20 / 52

sequence segmentation
information extraction

medical     clinical patient notes

21 / 52

she had a liver function test and amylase and lipase
postoperatively and she had a digoxin level of 1.0 on 06/04/05 .
the patient had a cbc on admission of 14.1 with a hematocrit
of 33.8 .
her cbc remained stable on 06/05/05 .
she had a white blood cell of 7.7 , hematocrit of 30.6 .
the patient had a mrsa nasal culture obtained on 06/03/05 ,
which revealed rare staphylococcus aureus .
the patient had a chest x-ray on admission , which was clear .
no pleural effusion or pneumothorax .

source:
https://www.i2b2.org/nlp/relations/documentation.php

22 / 52

she had a liver function test and amylase and lipase
postoperatively and she had a digoxin level of 1.0 on 06/04/05 .
the patient had a cbc on admission of 14.1 with a hematocrit
of 33.8 .
her cbc remained stable on 06/05/05 .
she had a white blood cell of 7.7 , hematocrit of 30.6 .
the patient had a mrsa nasal culture obtained on 06/03/05 ,
which revealed rare staphylococcus aureus .
the patient had a chest x-ray on admission , which was clear .
no pleural effusion or pneumothorax .

what test were performed?
what problems were looked for?

source:
https://www.i2b2.org/nlp/relations/documentation.php

22 / 52

she had

a liver function test and

amylase and

postoperatively and she had

a digoxin level of 1.0 on 06/04/05 .

test

}|
z }| {

test

{

z

z

test

test

}|
}|

{

{

test

lipase

z }| {
z

test

}|

{

a cbc on admission of 14.1 with a

hematocrit

her cbc remained stable on 06/05/05 .

she had

a white blood cell of 7.7 ,

hematocrit of 30.6 .

the patient had
of 33.8 .

test

}|

z

the patient had a

mrsa nasal culture obtained on 06/03/05 ,

z

{

z

z

test

}|

{

{

{

test

}|
z

z
z

{

{

problem

test

}|
}|
{
}|

test

}|
z

problem

problem

no

pleural effusion or

pneumothorax .

z

}|

{

the patient had

a chest x-ray on admission , which was clear .

which revealed

rare staphylococcus aureus .

22 / 52

sequence segmentation
information extraction

many other domains

i housing adds
i restaurant reviews
i camera reviews
i police reports
i politics / news
i . . .

23 / 52

evaluation

24 / 52

sequence segmentation     evaluation

i assume we have a system. how do we know how well we

did?

i in tagging, calculate the percentage of correct words
i why is segmentation different?

25 / 52

sequence segmentation     evaluation

i assume we have a system. how do we know how well we

did?

i in tagging, calculate the percentage of correct words
i why is segmentation different?

metric 1

i percentage of correct sentences.

25 / 52

sequence segmentation     evaluation

i assume we have a system. how do we know how well we

did?

i in tagging, calculate the percentage of correct words
i why is segmentation different?

metric 1

i percentage of correct sentences.
i problem: too harsh. we need a segment-level metric.

25 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions

num correct predictions
num of gold segments

26 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions

num correct predictions
num of gold segments

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman .

per

}|

gold:
z
predicted:
}|
z}|{paris

per

loc

z

{

{

z

z

time

}|

}|

time

{

{

whitney hilton (born

february 17, 1981) is an

american television personality and businesswoman .

z

org

}|

{

26 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions =

1
4

num correct predictions
num of gold segments

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman .

per

}|

gold:
z
predicted:
}|
z}|{paris

per

loc

z

{

{

z

z

time

}|

}|

time

{

{

whitney hilton (born

february 17, 1981) is an

american television personality and businesswoman .

z

org

}|

{

26 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions =

num correct predictions
num of gold segments =

1
4

1
2

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman .

per

}|

gold:
z
predicted:
}|
z}|{paris

per

loc

z

{

{

z

z

time

}|

}|

time

{

{

whitney hilton (born

february 17, 1981) is an

american television personality and businesswoman .

z

org

}|

{

26 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions

num correct predictions
num of gold segments

further considerations

i what do we consider    correct   ?
? off-by-one errors. . .
? type-errors. . .

27 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions

num correct predictions
num of gold segments

we have two numbers. what do we optimize?

i in general,

i higher precision ! lower recall
i higher recall ! lower precision

27 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions

num correct predictions
num of gold segments

we have two numbers. what do we optimize?

i in general,

i higher precision ! lower recall
i higher recall ! lower precision

i need to balance them.

27 / 52

sequence segmentation     evaluation

solution
precision

recall

num correct predictions

num all predictions

num correct predictions
num of gold segments

we have two numbers. what do we optimize?

i in general,

i higher precision ! lower recall
i higher recall ! lower precision

i need to balance them.
i f1

f1 =

precision     recall
precision + recall

27 / 52

sequence segmentation     evaluation

f1 =

precision     recall
precision + recall

f1 gives equal weight to precision and recall.
we may prefer one over the other.

28 / 52

sequence segmentation     evaluation

f1 =

precision     recall
precision + recall

f1 gives equal weight to precision and recall.
we may prefer one over the other.

in general,

f  = (1 +  2)

precision     recall

( 2     precision) + recall

28 / 52

methods

29 / 52

sequence segmentation     methods

concrete example     np chunking
data visualization tools are often used to communicate
complex information in a more intuitive way . by representing
high dimensional data ( e.g. objects with 4 or more variables ) by
two-dimensional points , in such a way that similar objects are
represented by nearby points and dissimilar objects are
represented by distant points , scientists can gain intuition by
examining the presence of structure and id91 in plots of
the data .

30 / 52

sequence segmentation     methods

concrete example     np chunking
data visualization tools are often used to communicate
complex information in a more intuitive way . by representing
high dimensional data ( e.g. objects with 4 or more variables ) by
two-dimensional points , in such a way that similar objects are
represented by nearby points and dissimilar objects are
represented by distant points , scientists can gain intuition by
examining the presence of structure and id91 in plots of
the data .

30 / 52

sequence segmentation     methods

concrete example     np chunking
data visualization tools are often used to communicate
complex information in a more intuitive way . by representing
high dimensional data ( e.g. objects with 4 or more variables ) by
two-dimensional points , in such a way that similar objects are
represented by nearby points and dissimilar objects are
represented by distant points , scientists can gain intuition by
examining the presence of structure and id91 in plots of
the data .
look at the pos-tags
nn nn nns vbp rb vbn to vb jj nn in dt jjr jj nn . in
vbn jj jj nn ( fw ) nns in cd cc jjr nns ) in jj nns , in
pdt dt nn in jj nns vbp vbd in jj nns cc jj nns vbp
vbd in jj nns , nns md vb nn in vbg dt nn in
nn jj nn in nns in dt nn .

30 / 52

sequence segmentation     methods

concrete example     np chunking
data visualization tools are often used to communicate
complex information in a more intuitive way . by representing
high dimensional data ( e.g. objects with 4 or more variables ) by
two-dimensional points , in such a way that similar objects are
represented by nearby points and dissimilar objects are
represented by distant points , scientists can gain intuition by
examining the presence of structure and id91 in plots of
the data .
look at the pos-tags
nn nn nns vbp rb vbn to vb jj nn in dt jjr jj nn . in
vbn jj jj nn ( fw ) nns in cd cc jjr nns ) in jj nns , in
pdt dt nn in jj nns vbp vbd in jj nns cc jj nns vbp
vbd in jj nns , nns md vb nn in vbg dt nn in
nn jj nn in nns in dt nn .

30 / 52

np chunking

look at the pos-tags
nn nn nns vbp rb vbn to vb jj nn in dt jjr jj nn . in
vbn jj jj nn ( fw ) nns in cd cc jjr nns ) in jj nns , in
pdt dt nn in jj nns vbp vbd in jj nns cc jj nns vbp
vbd in jj nns , nns md vb nn in vbg dt nn in
nn jj nn in nns in dt nn .
approach 1

i write regular-expressions over pos-tags.

31 / 52

np chunking

look at the pos-tags
nn nn nns vbp rb vbn to vb jj nn in dt jjr jj nn . in
vbn jj jj nn ( fw ) nns in cd cc jjr nns ) in jj nns , in
pdt dt nn in jj nns vbp vbd in jj nns cc jj nns vbp
vbd in jj nns , nns md vb nn in vbg dt nn in
nn jj nn in nns in dt nn .
approach 1

i write regular-expressions over pos-tags.

approach 2

i memorize pos-tag patterns.
i match the longest pattern.

31 / 52

np chunking

approach 1

i write regular-expressions over pos-tags.

approach 2

i memorize pos-tag patterns.
i match the longest pattern.

downsides

i 1 is hard to maintain.
i 2 will cover the common cases, but may not generalize.

31 / 52

np chunking

approach 1

i write regular-expressions over pos-tags.

approach 2

i memorize pos-tag patterns.
i match the longest pattern.

downsides

i 1 is hard to maintain.
i 2 will cover the common cases, but may not generalize.
i what if pos-tagger is wrong?

31 / 52

np chunking

approach 1

i write regular-expressions over pos-tags.

approach 2

i memorize pos-tag patterns.
i match the longest pattern.

downsides

i 1 is hard to maintain.
i 2 will cover the common cases, but may not generalize.
i what if pos-tagger is wrong?
i what about other tasks?

31 / 52

sequence segmentation     methods

need a learning approach

32 / 52

sequence segmentation     learning

[ data visualization tools ] are often used to communicate
[ complex information ] in [ a more intuitive way ] . by representing
[ high dimensional data ] ( e.g. [ objects ] with [ 4 or more variables ]
) by [ two-dimensional points ] , in [ such a way ] that [ similar objects
] are represented by [ nearby points ] and [ dissimilar objects ] are
represented by [ distant points ] , [ scientists ] can gain [ intuition ] by
examining [ the presence ] of [ structure and id91 ] in [ plots ] of
[ the data ] .
suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none

33 / 52

sequence segmentation     learning

[ data visualization tools ] are often used to communicate
[ complex information ] in [ a more intuitive way ] . by representing
[ high dimensional data ] ( e.g. [ objects ] with [ 4 or more variables ]
) by [ two-dimensional points ] , in [ such a way ] that [ similar objects
] are represented by [ nearby points ] and [ dissimilar objects ] are
represented by [ distant points ] , [ scientists ] can gain [ intuition ] by
examining [ the presence ] of [ structure and id91 ] in [ plots ] of
[ the data ] .
suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

33 / 52

sequence segmentation     learning

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

example
? data visualization tools are often used to communicate
complex information in a more intuitive way . by representing

arg max

y2{],[,][,none}

w     (-start-, -start-, ? data/nn, visualization/jj, y)

33 / 52

sequence segmentation     learning

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

example
data ? visualization tools are often used to communicate
complex information in a more intuitive way . by representing

arg max

y2{],[,][,none}

w   (-start, data/nn, ?, visualization/jj, tools/nns, y)

33 / 52

sequence segmentation     learning

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

example
data visualization ? tools are often used to communicate
complex information in a more intuitive way . by representing

arg max

y2{],[,][,none}

w   (data/nn, visualization/jj, ? tools/nns, are/vbp, y)

33 / 52

sequence segmentation     learning

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

example
data visualization tools ? are often used to communicate
complex information in a more intuitive way . by representing

arg max

y2{],[,][,none}

w   (visualization/jj, tools/nns, ?, are/vbp, often/rb, y)

33 / 52

sequence segmentation     learning

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

example
data visualization tools are ? often used to communicate
complex information in a more intuitive way . by representing

arg max

y2{],[,][,none}

w     (tools/nns, are/vbp, ?, often/rb, used/vbd, y)

33 / 52

sequence segmentation     learning

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

example
data visualization tools are often used to communicate
complex information in a more intuitive way . by representing

arg max

y2{],[,][,none}

w     ( tools/nns, are/vbp, ?, often/rb, used/vbd
}

sliding window

{z

|

, y)

33 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4

or, with span-types: n     (2 + 2     num_types)

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4
problems?

or, with span-types: n     (2 + 2     num_types)

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4
problems?

or, with span-types: n     (2 + 2     num_types)

i capturing the structures we need

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4
problems?

or, with span-types: n     (2 + 2     num_types)

i capturing the structures we need

i [ a [ b ][ c ] d e ][ f ] g ][

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4
problems?

or, with span-types: n     (2 + 2     num_types)

i capturing the structures we need

i [ a [ b ][ c ] d e ][ f ] g ][

i modeling / learning

34 / 52

suggestion     open / close

i classify each space between words in sequence as:

i [-type
i ]
i none
i ][-type

complexity?
n     4
problems?

or, with span-types: n     (2 + 2     num_types)

i capturing the structures we need

i [ a [ b ][ c ] d e ][ f ] g ][

i modeling / learning

i inside and outside words have the same class

34 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

data visualization tools are often used to communicate complex
information in a more intuitive way . by representing

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.
score(1, 3, np)

}|

{

z

data visualization tools are often used to communicate complex
information in a more intuitive way . by representing

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

z

score(1, 3, vp)

}|

{

data visualization tools are often used to communicate complex
information in a more intuitive way . by representing

35 / 52

the elegant/correct solution

word i to word j.

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from
z

data
information in a more intuitive way . by representing

visualization tools are often used to communicate complex

score(2, 3, np)

}|

{

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

data visualization
information in a more intuitive way . by representing

are often used to communicate complex

tools

score(3, 3, np)

z}|{

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

data visualization
information in a more intuitive way . by representing

tools are often used to communicate complex

z

score(3, 7, np)

}|

{

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

data visualization
information in a more intuitive way . by representing

tools are often used to communicate complex

z

score(3, 7, np)

}|

{

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i s1, . . . , sn : the input sequence.
i t1, . . . , tn : the pos-tags of the input sequence.

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

data visualization
information in a more intuitive way . by representing

tools are often used to communicate complex

z

score(3, 7, np)

}|

{

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i learn w such that:

i good spans have positive scores
i bad spans have negative scores

35 / 52

the elegant/correct solution

i assign a score to each possible span:
) score(i, j, type)     score of having span of type type from

word i to word j.

data visualization
information in a more intuitive way . by representing

tools are often used to communicate complex

z

score(3, 7, np)

}|

{

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i learn w such that:

i good spans have positive scores
i bad spans have negative scores

i possible features?

i discuss

35 / 52

the elegant/correct solution

) score(i, j, type)     score of having span of type type from

word i to word j.

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i a segmentation y is a set of consistent triplets (i, j, type)

36 / 52

the elegant/correct solution

) score(i, j, type)     score of having span of type type from

word i to word j.

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i a segmentation y is a set of consistent triplets (i, j, type)

) 1     i     j     n
) no overlaps

36 / 52

the elegant/correct solution

) score(i, j, type)     score of having span of type type from

word i to word j.

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i a segmentation y is a set of consistent triplets (i, j, type)

) 1     i     j     n
) no overlaps

i the set of all segmentations: y

36 / 52

the elegant/correct solution

) score(i, j, type)     score of having span of type type from

word i to word j.

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i a segmentation y is a set of consistent triplets (i, j, type)

) 1     i     j     n
) no overlaps

i the set of all segmentations: y
i we want:
y2y x(i,j,type)2y

arg max

score(i, j, type)

36 / 52

the elegant/correct solution

) score(i, j, type)     score of having span of type type from

word i to word j.

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i a segmentation y is a set of consistent triplets (i, j, type)

) 1     i     j     n
) no overlaps

i the set of all segmentations: y
i we want:
y2y x(i,j,type)2y

arg max

score(i, j, type)

i can solve with a dynamic program

36 / 52

the elegant/correct solution

) score(i, j, type)     score of having span of type type from

word i to word j.

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

i a segmentation y is a set of consistent triplets (i, j, type)

) 1     i     j     n
) no overlaps

i the set of all segmentations: y
i we want:
y2y x(i,j,type)2y

arg max

score(i, j, type)

i can solve with a dynamic program
i this approach is called    semi-markov   

36 / 52

the elegant/correct solution

i a segmentation y is a set of consistent triplets (i, j, type)
i the set of all segmentations: y
i we search for:
y2y x(i,j,type)2y

score(i, j, type)

arg max

score(i, j, type) = w     (i, j, type, s1, . . . , sn, t1, . . . , tn)

training
find w such that, for a correct segmentation y,
8y0 2 y, y0 6= y:
x(i,j,type)2y

score(i, j, type) > x(i0,j0,type0)2y0

score(i0, j0, type0)

37 / 52

the elegant/correct solution

i assume we can train w.
i when we receive a new sequence s1, . . . , sn, we need to

solve:

complexity?

arg max

y2y x(i,j,type)2y

score(i, j, type)

38 / 52

the elegant/correct solution

i assume we can train w.
i when we receive a new sequence s1, . . . , sn, we need to

solve:

arg max

y2y x(i,j,type)2y

score(i, j, type)

complexity?

i we need to compute score(i, j, type)

for all 1     i     j     n
for all type.

) complexity is at least o(kn2), where k is the number of

types.

38 / 52

the elegant/correct solution

i assume we can train w.
i when we receive a new sequence s1, . . . , sn, we need to

solve:

arg max

y2y x(i,j,type)2y

score(i, j, type)

complexity?

i we need to compute score(i, j, type)

for all 1     i     j     n
for all type.

) complexity is at least o(kn2), where k is the number of
i not cheap.

types.

38 / 52

a different approach

39 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n

40 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n cheap!

40 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n cheap!
problems?

40 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n cheap!
problems?

i capturing the structures we need

40 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n cheap!
problems?

i capturing the structures we need

i [ a [ b ][ c ] d e ][ f ] g ][

40 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n cheap!
problems?

i capturing the structures we need

i [ a [ b ][ c ] d e ][ f ] g ][

i modeling / learning

40 / 52

reminder

open / close

i classify each space between words in sequence as:

i [
i ]
i none
i ][

complexity?
(2k + 2)n cheap!
problems?

i capturing the structures we need

i [ a [ b ][ c ] d e ][ f ] g ][

i modeling / learning

i inside and outside words have the same class

40 / 52

sequence segmentation     io encoding

suggestion     inside / out

i classify each word in sequence as:

i inside a segment
i out of segment

data/i-np visualization/i-np tools/i-np are/o often/o used/o to/o
communicate/o complex/i-np information/i-np in/o a/i-np
more/i-np intuitive/i-np way/i-np ./o by/o representing/o

41 / 52

sequence segmentation     io encoding

suggestion     inside / out

i classify each word in sequence as:

i inside a segment
i out of segment

data/i-np visualization/i-np tools/i-np are/o often/o used/o to/o
communicate/o complex/i-np information/i-np in/o a/i-np
more/i-np intuitive/i-np way/i-np ./o by/o representing/o
what did we gain?

41 / 52

sequence segmentation     io encoding

suggestion     inside / out

i classify each word in sequence as:

i inside a segment
i out of segment

data/i-np visualization/i-np tools/i-np are/o often/o used/o to/o
communicate/o complex/i-np information/i-np in/o a/i-np
more/i-np intuitive/i-np way/i-np ./o by/o representing/o
what did we gain? is this suf   cient?

41 / 52

sequence segmentation     io encoding

suggestion     inside / out

i classify each word in sequence as:

i inside a segment
i out of segment

data/i-np visualization/i-np tools/i-np are/o often/o used/o to/o
communicate/o complex/i-np information/i-np in/o a/i-np
more/i-np intuitive/i-np way/i-np ./o by/o representing/o
what did we gain? is this suf   cient?
np

np

np

z}|{he saw

z

a blue tiger

yesterday

{

z

}|

{

}|

41 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment

does this solve the previous problems?

42 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment

does this solve the previous problems?
b-np
he

b-np i-np i-np
tiger

o
saw

blue

a

b-np

yesterday

42 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment

does this solve the previous problems?
b-np
he

b-np i-np i-np
tiger

o
saw

blue

a

b-np

yesterday

further elaboration

i add single class, for a single-word segment
i add end class, for last-word of segment

i why?

42 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment

does this solve the previous problems?

b-np i-np e-np
tiger

blue

s-np

yesterday

s-np
he

o
saw

a
further elaboration

i add single class, for a single-word segment
i add end class, for last-word of segment

i why?

42 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out (/ single / end)

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)

43 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out (/ single / end)

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)

43 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out (/ single / end)

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)

we reduced sequence segmentation

to sequence tagging

43 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out (/ single / end)

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)

we reduced sequence segmentation

to sequence tagging

we know how to train a tagger

43 / 52

sequence segmentation     bio encoding

suggestion     begin / inside / out (/ single / end)

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)

44 / 52

sequence segmentation     bio encoding

we reduced sequence segmentation

to sequence tagging

i what did we gain over the    semi-markov    approach?

45 / 52

sequence segmentation     bio encoding

we reduced sequence segmentation

to sequence tagging

i what did we gain over the    semi-markov    approach?
i what did we lose?

45 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo
i boooioooiibbo

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo
i boooioooiibbo
i soooioooiebso

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo
i boooioooiibbo
i soooioooiebso
i soooeoooiebso

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo
i boooioooiibbo
i soooioooiebso
i soooeoooiebso

i no good solution, just be aware of this

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo
i boooioooiibbo
i soooioooiebso
i soooeoooiebso

i no good solution, just be aware of this
i when training, choose one.

46 / 52

sequence segmentation     bio encoding

i classify each word in sequence as:

i begin of segment
i inside a segment
i out of segment
i (end of segment)
i (single-word segment)
notice: spurious ambiguity

i several sequences encode the same structure:

i booobooobibbo
i boooboooiibbo
i boooioooiibbo
i soooioooiebso
i soooeoooiebso

i no good solution, just be aware of this
i when training, choose one.
i when predicting, know how to handle all.

46 / 52

a closer look at id39

47 / 52

ner     reminder

id39 (ner)
paris whitney hilton (born february 17, 1981) is an american
television personality and businesswoman. she is the
great-granddaughter of conrad hilton, the founder of hilton
hotels. born in new york city and raised in both california and
new york, hilton began a modeling career when she signed
with donald trump   s modeling agency .

identify things
person / location / organization / time / other.

48 / 52

ner     reminder

id39 (ner)

{

{

per

}|

z

z
z

z

per

paris whitney hilton (born
television personality and businesswoman . she is the

february 17, 1981) is an american

great-granddaughter of

hilton hotels . born in
loc

loc

california and

new york,

{

org

}|
}|

{

loc

new york city and raised in both

conrad hilton , the founder of

}|
}|
z}|{hilton began a modeling career when

z
z
{

org

per

she signed with

donald trump   s modeling agency .

time

}|
{
{

}|

{

}|

z

z

48 / 52

id39

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman . she is

z

per

}|

{

the great-granddaughter of

loc

loc

time

{

per

}|
}|

z
z
z}|{hilton began a modeling career when she signed with

conrad hilton , the founder of
per

hilton hotels . born in

}|

org

{

z

{

z
z

{

z

{

}|

new york,

z
}|
california and
design questions
assume we are using bio tagging.

new york city and raised in both

donald trump   s modeling agency .

loc

}|

{

org

}|

{

49 / 52

id39

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman . she is

z

per

}|

{

the great-granddaughter of

loc

loc

time

{

per

}|
}|

z
z
z}|{hilton began a modeling career when she signed with

conrad hilton , the founder of
per

hilton hotels . born in

}|

org

{

z

{

z
z

{

z

{

}|

new york,

z
}|
california and
design questions
assume we are using bio tagging.

new york city and raised in both

donald trump   s modeling agency .

loc

}|

{

org

}|

{

i bio of biose?

49 / 52

id39

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman . she is

z

per

}|

{

the great-granddaughter of

loc

loc

time

{

per

}|
}|

z
z
z}|{hilton began a modeling career when she signed with

conrad hilton , the founder of
per

hilton hotels . born in

}|

org

{

z

{

z
z

{

z

{

}|

new york,

z
}|
california and
design questions
assume we are using bio tagging.

new york city and raised in both

donald trump   s modeling agency .

loc

}|

{

org

}|

{

i bio of biose?
i greedy or non-greedy tagger?

49 / 52

id39

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman . she is

z

per

}|

{

the great-granddaughter of

loc

loc

time

{

per

}|

z
z
z}|{hilton began a modeling career when she signed with

conrad hilton , the founder of
per

hilton hotels . born in

}|

}|

org

{

z

{

z
z

{

z

{

}|

new york,

z
}|
california and
design questions
assume we are using bio tagging.

new york city and raised in both

donald trump   s modeling agency .

loc

}|

{

org

}|

{

i bio of biose?
i greedy or non-greedy tagger?

49 / 52

id39

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman . she is

z

per

}|

{

the great-granddaughter of

loc

loc

time

{

per

}|
}|

z
z
z}|{hilton began a modeling career when she signed with

conrad hilton , the founder of
per

hilton hotels . born in

}|

org

{

z

{

z
z

{

z

{

}|

new york,

z
}|
california and
design questions
assume we are using bio tagging.

new york city and raised in both

donald trump   s modeling agency .

loc

}|

{

org

}|

{

i bio of biose?
i greedy or non-greedy tagger?

49 / 52

id39

paris whitney hilton (born

february 17, 1981) is an american television personality and businesswoman . she is

z

per

}|

{

the great-granddaughter of

loc

loc

time

{

per

}|
}|

z
z
z}|{hilton began a modeling career when she signed with

conrad hilton , the founder of
per

hilton hotels . born in

}|

org

{

z

{

z
z

{

z

{

}|

new york,

z
}|
california and
design questions
assume we are using bio tagging.

new york city and raised in both

donald trump   s modeling agency .

loc

}|

{

org

}|

{

i bio of biose?
i greedy or non-greedy tagger?
i features!!!

49 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

i should we rely on capitalization?

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

i should we rely on capitalization?
i maybe: is the word usually capitalized, in a large corpus?

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

i should we rely on capitalization?
i maybe: is the word usually capitalized, in a large corpus?

i gazetteers (lists): is word in list of persons / locations /

organizations?

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

i should we rely on capitalization?
i maybe: is the word usually capitalized, in a large corpus?

i gazetteers (lists): is word in list of persons / locations /

organizations?

i where do we    nd gazetteers?
i how do we match against them?

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

i should we rely on capitalization?
i maybe: is the word usually capitalized, in a large corpus?

i gazetteers (lists): is word in list of persons / locations /

organizations?

i where do we    nd gazetteers?
i how do we match against them?

i context-aggregation (prev/next words of other occurrences

of current word)

50 / 52

id39

paris whitney hilton/?? (born february 17, 1981) is an american television personality and businesswoman. she is

the great-granddaughter of conrad hilton, the founder of hilton hotels. born in new york city and raised in both

california and new york, hilton began a modeling career when she signed with donald trump   s modeling agency .
features

i current word, previous word, next word.
i is (current/prev/next) word capitalized?

i should we rely on capitalization?
i maybe: is the word usually capitalized, in a large corpus?

i gazetteers (lists): is word in list of persons / locations /

organizations?

i where do we    nd gazetteers?
i how do we match against them?

i context-aggregation (prev/next words of other occurrences

of current word)

i maybe better: prev/next non-function-word?

50 / 52

id39

design questions

i features!!!
i separate identi   cation from classi   cation?

51 / 52

id39

design questions

i features!!!
i separate identi   cation from classi   cation?
}|
1) z
paris whitney hilton (bornz
is the great-granddaughter ofz
hilton hotels . born inz
{
z
}|
}|
new york,z}|{hilton began a modeling career when she signed withz
california andz

conrad hilton , the founder ofz

{
{

}|

}|

}|

{

{

{

new york city and raised in both

donald trump   s modeling agency .

}|

{
}|

{

february 17, 1981) is an american television personality and businesswoman . she

51 / 52

id39

design questions

february 17, 1981) is an american television personality and businesswoman . she

new york city and raised in both

donald trump   s modeling agency .

}|

{
}|

{

{

{

{

}|

}|

}|

{
{

conrad hilton , the founder ofz

i features!!!
i separate identi   cation from classi   cation?
}|
1) z
paris whitney hilton (bornz
is the great-granddaughter ofz
hilton hotels . born inz
z
{
}|
}|
new york,z}|{hilton began a modeling career when she signed withz
california andz
2)
paris whitney hilton ! per / loc / org / time ?
february 17, 1981
conrad hilton
hilton hotels
new york city
california
new york
hilton
donald trump   s modeling agency

51 / 52

id39

design questions

february 17, 1981) is an american television personality and businesswoman . she

new york city and raised in both

donald trump   s modeling agency .

}|

{
}|

{

{

{

{

}|

}|

}|

{
{

conrad hilton , the founder ofz

i features!!!
i separate identi   cation from classi   cation?
}|
1) z
paris whitney hilton (bornz
is the great-granddaughter ofz
hilton hotels . born inz
z
{
}|
}|
new york,z}|{hilton began a modeling career when she signed withz
california andz
2)
paris whitney hilton ! per / loc / org / time ?
february 17, 1981
conrad hilton
hilton hotels
new york city
california
new york
hilton
donald trump   s modeling agency
why?

51 / 52

summary

sequence segmentation

i many tasks can be cast as sequence segmentation
i evaluation: precision / recall / f 
i solutions

i elegant: semi-markov model
i ef   cient: bio(se) tagging

i spurious ambiguity

i features!!

i gazetteers
i context aggregation

i separate identi   cation from classi   cation

. . . when it makes sense

52 / 52

