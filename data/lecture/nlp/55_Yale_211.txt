nlp
introduction to nlp
language models
probabilistic language models
assign a id203 to a sentence
p(s) = p(w1,w2,w3,...,wn)
different from deterministic methods using id18
the sum of the probabilities of all possible sentences must be 1.
predicting the next word
example
let   s meet in times    
general electric has lost some market    
formula
p(wn|w1,w2,...,wn-1)

predicting the next word
what word follows    your   ? 
http://norvig.com/ngrams/count_2w.txt
your abilities 160848 
your ability 1116122 
your ablum 112926 
your academic 274761 
your acceptance 783544 
your access 492555 
your accommodation 320408 
your account 8149940 
your accounting 128409 
your accounts 257118 
your action 121057 
your actions 492448
your activation 459379 

your active 140797 
your activities 226183 
your activity 156213 
your actual 302488 
your ad 1450485 
your address 1611337 
your admin 117943 
your ads 264771 
your advantage 242238 
your adventure 109658 
your advert 101178 
your advertisement 172783
uses of language models
id103
p(   recognize speech   ) > p(   wreck a nice beach   )
text generation
p(   three houses   ) > p(   three house   )
id147
p(   my cat eats fish   ) > p(   my xat eats fish   )
machine translation
p(   the blue house   ) > p(   the house blue   )
other uses
ocr
summarization
document classification
usually coupled with a translation model (later)
id203 of a sentence
how to compute the id203 of a sentence?
what if the sentence is novel?
what we need to estimate:
p(s)=p(w1,w2,w3   wn)
using the chain rule:
p(s)= p(w1) p(w2|w1) p(w3|w1,w2)    p(wn|w1,w2   wn-1)
example:
p(   i would like the pepperoni and spinach pizza   )=?
id165 models
predict the id203 of a word based on the words before:
p(square|let   s meet in times)
markov assumption
only look at limited history
id165 models
unigram     no context: p(square)
bigram: p(square|times)
trigram: p(square|in times)

random text (brown corpus)
2-grams:
	the 53-year-old shea was no acceptable formula to help the abuse of events were a wall in 1908 , called upon his hand in southern new orleans , miss garson was named maurice couve de havilland signed a privilege resolution had had happened on a tax applied to the chisholm , the thriving systems of the `` pride and musician , and moscow made good team spirit of the culmination of the metal tube through the amateur , but rather than a special prosecutor . this knowledge of each member of these savings of golf course can see the 13 straight 69 . since 1927 by harry truman cleveland of railroad retirement age groups . no vacancy '' . `` i have to congressmen . the remainder of the rear bumper and on a benefit in u.s. amateur , as far as a thrill a $100 u.s. if not indicted . the state's occupation tax dollars over the newest product of the address he attended arlington state university will pay half years . 
random text (brown corpus)
3-grams: 
	the fulton county jail and `` a very strong central government of laos that the presence of picket lines and featuring a flared skirt and lace jacket with bateau neckline and princesse skirt accented by lace appliques . her acting began with the members of the government -- such control is necessary to build in a final exchange between moscow and washington last week . of course , since the views of another one . it urged that the games are not essential to provide federal contributions to the 85-student north carolina group to play , was addressing a meeting in the manufacture of a tax bill since most of his uncle and aunt , also was particularly struck by the reams came in from shareholders of these co-operative systems , the 9th precinct of the guiding spirits of the armed services committee . davis received 1,119 votes in saturday's election , the executive organs of participation can hardly escape the impression that he made no attempt to get agreement among the conference's top four in rushing   
random text (brown corpus)
4-grams:
	the broadcast said anderson , a seattle ex-marine and havana businessman , and mcnair , of miami , were condemned on charges of smuggling arms to cuban rebels . anderson operated three havana automobile service stations and was commander of the havana american legion post before it disbanded since the start of august have shown gains averaging nearly 10% above last year . that , too , in improving motorists' access to many turnpikes . the kansas turnpike offers an illustration . net earnings of that road rose from 62 per cent of the prices that the avid buyers bid it up to . dallas and north texas is known world-wide as the manufacturing and distribution center of cotton gin machinery and equipment . the firm is design-conscious , sales-conscious , advertising-conscious . `` hodges predicted : ' i think we should certainly follow through on it '' . rep. henry c. grover , who teaches history in the houston public schools , would reduce from 24 to 12 semester hours the so-called `` blue law ''    
higher order id165s
it is possible to go to 3,4,5-grams
longer id165s suffer from sparseness

id165s
shakespeare unigrams
29,524 types, approx. 900k tokens
bigrams
346,097 types, approx. 900k tokens
how many bigrams are never seen in the data?
notice!
very sparse data!

google 1-t corpus
1 trillion word tokens
number of tokens
1,024,908,267,229 
number of sentences
95,119,665,584 
number of unigrams
13,588,391 
number of bigrams
314,843,401 
number of trigrams
977,069,902 
number of fourgrams
1,313,818,354 
number of fivegrams
1,176,470,663
https://catalog.ldc.upenn.edu/ldc2006t13
estimation
can we compute the conditional probabilities directly?
no, because the data is sparse
markov assumption
p(   musical    |    i would like two tickets for the   ) = p(   musical | the   )
or
p(   musical    |    i would like two tickets for the   ) = p(   musical | for the   )
maximum likelihood estimates
use training data
count how many times a given context appears in it.
unigram example:
the word    pizza    appears 700 times in a corpus of 10,000,000 words.
therefore the id113 for its id203 is p   (   pizza   ) = 700/10,000,000 = 0.00007
bigram example:
the word    with    appears 1,000 times in the corpus.
the phrase    with spinach    appears 6 times
therefor the id113 for p   (spinach|with) = 6/1,000 = 0.006
these estimates may not be good for corpora from other genres
example
p(   <s> i will see you on monday</s>   ) =
p(i|<s>) 
x p(will|i) 
x p(see|will) 
x p(you|see) 
x p(on|you) 
x p(monday|on) 
x p(</s>|monday)
example from jane austen
p(   elizabeth looked at darcy   )
use maximum likelihood estimates for the id165 probabilities
unigram: p(wi)=c(wi)/v
bigram: p(wi|wi-1) = c(wi-1,wi)/c(wi-1)
values
p(   elizabeth   ) = 474/617091 = .000768120
p(   looked|elizabeth   ) = 5/474 = .010548523
p(   at|looked   ) = 74/337 = .219584569
p(   darcy|at   ) = 3/4055 = .000739827
bigram id203
p(   elizabeth looked at darcy   ) = .000000001316 = 1.3 x 10-9
unigram id203
p(   elizabeth looked at darcy   ) = 474/617091 * 337/617091 * 4055/617091 * 304/617091 = .000000000001357 = 1.3 x 10-12
p(   looked darcy elizabeth at   ) = ?

generative models
unigram: 
generate a word, then generate the next one, until you generate </s>.


bigram: 
generate <s>, generate a word, then generate the next one based on the previous one, etc., until you generate </s>.
engineering trick
the id113 values are often on the order of 10-6 or less
multiplying 20 such values gives a number on the order of 10-120
this leads to underflow
use logarithms instead 
10-6 (in base 10) becomes -6
use sums instead of products
tools
http://www.speech.cs.cmu.edu/slm_info.html 
http://www.speech.sri.com/projects/srilm/
https://kheafield.com/code/kenlm/ 
http://htk.eng.cam.ac.uk/ 

nlp
