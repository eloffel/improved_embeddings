parsing,	contd

prof.	sameer	singh
cs	295:	statistical	nlp
winter	2017

february	9,	2017

based	on	slides	from	dan	jurafsky,	noah	smith,	slav	petrov,	and	everyone	else	they	copied	from.

outline

syntactic	parsing:	cky	algorithm

probabilistic	id18s

dependency	parsing

cs	295:	statistical	nlp	(winter	2017)

2

outline

syntactic	parsing:	cky	algorithm

probabilistic	id18s

dependency	parsing

cs	295:	statistical	nlp	(winter	2017)

3

review	of	syntactic	parsing

context-free	
grammar

parse	tree

cs	295:	statistical	nlp	(winter	2017)

4

dynamic	programming

t[i,j]	=	set	of	all	valid	non-terminals	for	the	constituent	span	(i,j)

base	case

rule:	a	  word[j]
a	should	be	in	t[j-1,j]

recursion

rule:	a	  b	c

if	you	find	a	k	such	that

b	is	in	t[i,k],	and
c	is	in	t[k,j],	then	a	should	be	in	t[i,j]

a (j-1,j)

word[j]

(i,j)
a

b
(i,k)

c
(k,j)

cs	295:	statistical	nlp	(winter	2017)

5

cky	algorithm

book

the

flight through twa

cs	295:	statistical	nlp	(winter	2017)

6

cky	algorithm

cs	295:	statistical	nlp	(winter	2017)

7

cky	algorithm:	complexity

|n|:	number	of	non-terminals
|r|:	number	of	rules

n:	number	of	tokens	in	the	sentence

memory

time

cs	295:	statistical	nlp	(winter	2017)

8

outline

syntactic	parsing:	cky	algorithm

probabilistic	id18s

dependency	parsing

cs	295:	statistical	nlp	(winter	2017)

9

ambiguity:	which	parse?

i	shot	an	elephant	in	my	pajamas.

cs	295:	statistical	nlp	(winter	2017)

10

finding	the	best parse	tree

cats	scratch	people	with	cats	with	claws.

cs	295:	statistical	nlp	(winter	2017)

11

probabilistic	id18s

same	as	a	regular	context-free	grammar:
    terminal,	non-terminals,	and	rules
    additionally,	attach	a	id203	to	each	rule!

rule:	a	  b	c

id203:	p(a	  b	c | a)

compute	the	id203	of	a	parse	tree:

cs	295:	statistical	nlp	(winter	2017)

12

example	of	a	pid18

cs	295:	statistical	nlp	(winter	2017)

13

estimating	the	probabilities

cs	295:	statistical	nlp	(winter	2017)

14

the	parsing	problem

given	sentence	x and		grammar	g,

recognition

is	sentence	x in	the	grammar?	if	so,	prove	it.
   proof   	is	a	deduction,	valid	parse	tree.

parsing

show	one	or	more	derivations	for	x in	g.

even	with	small	grammars,	grows	exponentially!

cs	295:	statistical	nlp	(winter	2017)

15

probabilistic	cky	algorithm

t[i,j,a]	=	id203	of	the	best	parse	with	root	a	for	the	span	(i,j)

base	case

rule:	p(	a	  word[j]	)
t[j-1,j,a]	=	p(word[j]	|	a)

recursion

rule:	p(	a	  b	c )

try	every	position	k,	and	every	non-terminal	pair:

t[i,j,a]	=	 max			p(b	c|	a)		t[i,k,b] t[k,j,c]

a (j-1,j)

word[j]

(i,j)
a

b
(i,k)

c
(k,j)

cs	295:	statistical	nlp	(winter	2017)

16

lexicalized	pid18s

cs	295:	statistical	nlp	(winter	2017)

17

lexicalizing	a	id18

cs	295:	statistical	nlp	(winter	2017)

18

lexicalizing	a	id18

cs	295:	statistical	nlp	(winter	2017)

19

lexicalizing	a	id18

cs	295:	statistical	nlp	(winter	2017)

20

outline

syntactic	parsing:	cky	algorithm

probabilistic	id18s

dependency	parsing

cs	295:	statistical	nlp	(winter	2017)

21

dependencies

represent	only	the	syntactic	dependencies   

cs	295:	statistical	nlp	(winter	2017)

22

nested	structure	=	subtrees

cs	295:	statistical	nlp	(winter	2017)

23

dependency	labels

cs	295:	statistical	nlp	(winter	2017)

24

dependency	labels

cs	295:	statistical	nlp	(winter	2017)

25

dependency	trees

cs	295:	statistical	nlp	(winter	2017)

26

projective	vs	non-projective

cs	295:	statistical	nlp	(winter	2017)

27

evaluating	dependency	parses

root   she  saw   the   video   lecture 

root   she  saw   the   video   lecture 

0       1      2      3       4           5

0       1      2      3       4           5

gold
she
2
1
saw
0
2
the
3
5
5
4
video
2				lecture
5

nsubj
root	
det
nn
dobj

parsed
she
2
1
saw
0
2
the
3
4
5
4
video
2				lecture
5

cs	295:	statistical	nlp	(winter	2017)

nsubj
root	
det
nsubj
ccomp

28

parsing	algorithms

transition-based
    fast,	greedy,	linear-time
    trained	for	greedy	search
    features	decide	what	to	do	next
    beam	search,	i.e.	k-best

graph-based
    slower,	exhaustive	algorithms
    dynamic	programming,	id136
    features	used	to	score	whole	trees

y
c
a
r
u
c
c
a

running	time

cs	295:	statistical	nlp	(winter	2017)

29

transition-based	parsing

cs	295:	statistical	nlp	(winter	2017)

30

transition-based	parsing

cs	295:	statistical	nlp	(winter	2017)

31

transition-based	parsing

cs	295:	statistical	nlp	(winter	2017)

32

transition-based	parsing

cs	295:	statistical	nlp	(winter	2017)

33

transition-based	parsing

cs	295:	statistical	nlp	(winter	2017)

34

transition-based	parsing

cs	295:	statistical	nlp	(winter	2017)

35

graph-based	parsing

cs	295:	statistical	nlp	(winter	2017)

36

eisner	algorithm

cs	295:	statistical	nlp	(winter	2017)

37

upcoming   

homework

    homework	2	is	due	on	monday
    grades	for	homework	1	will	be	released	today.

project

    status	report	due	in	~2	weeks:	february	21,	2017
   
    only	5	pages

instructions	coming	soon

summaries

    paper	summaries:	february	17,	february	28,	march	14
    only	1 page	each

cs	295:	statistical	nlp	(winter	2017)

38

