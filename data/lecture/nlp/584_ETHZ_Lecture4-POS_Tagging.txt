id52

intro to nlp - ethz - 11/03/2013

summary

    parts of speech 
    tagsets 
    id52
    id48 tagging:

    most likely tag sequence
    id203 of an observation
    parameter estimation
    evaluation

pos ambiguity

"squad helps dog bite victim"

    bite -> verb?
    bite -> noun?

parts of speech (pos)

traditional parts of speech:

    noun, verb, adjective, preposition, adverb, 
article, interjection, pronoun, conjunction, 
etc.

    called: parts-of-speech, lexical 

categories, word classes, morphological 
classes, lexical tags...

examples

n (noun): car, squad, dog, bite, victim
v (verb): help, bite
adj (adjective): purple, tall
adv (adverb): unfortunately, slowly
p (preposition): of, by, to
pro (pronoun): i, me, mine
det (determiner): the, a, that, those

open and closed classes

1. closed class: small stable set
a. auxiliaries: may, can, will, been, ...
b. prepositions: of, in, by, ...
c. pronouns: i, you, she, mine, his, them, ...
d. usually function words, short, grammar role

2. open class:

a. new ones are created all the time ("to google/tweet", 

"e-quaintance", "captcha", "cloud computing", 
"netbook", "webinar", "widget")

b. english has 4: nouns, verbs, adjectives, adverbs
c. many languages have these 4, not all!

open class words

1. nouns:

a. proper nouns: zurich, ibm, albert einstein, the 

godfather, ... capitalized in many languages.
b. common nouns: the rest, also capitalized in 

german, mass/count nouns (goat/goats, 
snow/*snows)

2. verbs:

a. morphological affixes in english: eat/eats/eaten

3. adverbs: tend to modify things:

a. john walked home extremely slowly yesterday
b. directional/locative adverbs (here, home, downhill) 
c. degree adverbs (extremely, very, somewhat)
d. manner adverbs (slowly, slinkily, delicately)

4. adjectives: qualify nouns and noun phrases

closed class words

1. prepositions: on, under, over, ... 
2. particles: up, down, on, off, ... 
3. determiners: a, an, the, ... 
4. pronouns: she, who, i, ..
5. conjunctions: and, but, or, ... 
6. auxiliary verbs: can, may should, ...
7. numerals: one, two, three, third, .

prepositions with corpus 

frequencies

conjunctions

pronouns

auxiliaries

applications

    a useful pre-processing step in many tasks
    syntactic parsing: important source of 

information for syntactic analysis

    machine translation
    information retrieval: id30, filtering
    id39
    summarization?

applications

id133, for correct pronunciation of 
"ambiguous" words:

    lead /lid/ (guide) vs. /led/ (chemical)
    insult vs. insult
    object vs. object
    overflow vs. overflow
    content vs. content

summarization

    idea: filter out sentences starting with 

certain pos tags

    use pos statistics from gold standard titles 

(might need cross-validation)

summarization

    idea: filter out sentences starting with 

certain pos tags:

    title1: "apple introduced siri, an intelligent 

personal assistant to which you can ask 
questions"

    title2: "especially now that a popular new 
feature from apple is bound to make other 
phones users envious: voice control with 
siri"

    title3: "but siri, apple's personal assistant 

application on the iphone 4s, doesn't 
disappoint"

summarization

    idea: filter out sentences starting with 

certain pos tags:

    title1: "apple introduced siri, an intelligent 

personal assistant to which you can ask 
questions"

    title2: "especially now that a popular new 
feature from apple is bound to make other 
phones users envious: voice control with 
siri"

    title3: "but siri, apple's personal assistant 

application on the iphone 4s, doesn't 
disappoint"

id52

    the process of assigning a part-of-speech 

tag (label) to each word in a text.

    pre-processing: id121

word
squad
helps
dog
bite
victim

tag
n
v
n
n
n

choosing a tagset

1. there are many parts of speech, potential 

distinctions we can draw

2. for id52, we need to choose a 

standard set of tags to work with
3. coarse tagsets n, v, adj, adv. 

a. a universal pos tagset? http://en.wikipedia.

org/wiki/part-of-speech_tagging

4. more commonly used set is finer grained, 

the    id32 tagset   , 45 tags

ptb tagset

examples*

1. i/prp need/vbp a/dt flight/nn from/in 
atlanta/nn
2. does/vbz this/dt flight/nn serve/vb 
dinner/nns
3. i/prp have/vb a/dt friend/nn living/vbg 
in/in denver/nnp
4. can/vbp you/prp list/vb the/dt nonstop/jj 
afternoon/nn flights/nns

examples*

1. i/prp need/vbp a/dt flight/nn from/in 
atlanta/nnp
2. does/vbz this/dt flight/nn serve/vb 
dinner/nns
3. i/prp have/vb a/dt friend/nn living/vbg 
in/in denver/nnp
4. can/vbp you/prp list/vb the/dt nonstop/jj 
afternoon/nn flights/nns

examples*

1. i/prp need/vbp a/dt flight/nn from/in 
atlanta/nnp
2. does/vbz this/dt flight/nn serve/vb 
dinner/nn
3. i/prp have/vb a/dt friend/nn living/vbg 
in/in denver/nnp
4. can/vbp you/prp list/vb the/dt nonstop/jj 
afternoon/nn flights/nns

examples*

1. i/prp need/vbp a/dt flight/nn from/in 
atlanta/nnp
2. does/vbz this/dt flight/nn serve/vb 
dinner/nn
3. i/prp have/vbp a/dt friend/nn living/vbg 
in/in denver/nnp
4. can/vbp you/prp list/vb the/dt nonstop/jj 
afternoon/nn flights/nns

examples*

1. i/prp need/vbp a/dt flight/nn from/in 
atlanta/nnp
2. does/vbz this/dt flight/nn serve/vb 
dinner/nn
3. i/prp have/vbp a/dt friend/nn living/vbg 
in/in denver/nnp
4. can/md you/prp list/vb the/dt nonstop/jj 
afternoon/nn flights/nns

complexities

    book/vb that/dt flight/nn ./.
    there/ex are/vbp 70/cd children/nns 

there/rb ./.

    mrs./nnp shaefer/nnp never/rb got/vbd 

around/rp to/to joining/vbg ./.

    all/dt we/prp gotta/vbn do/vb is/vbz 

go/vb around/in the/dt corner/nn ./.

    unresolvable ambiguity:

    the duchess was entertaining last night .

words

pos wsj

pos universal

the

oboist

heinz

holliger

has

taken

a

hard

line

about

the

problems

.

dt

nn

nnp

nnp

vbz

vbn

dt

jj

nn

in

dt

nns

.

det

noun

noun

noun

verb

verb

det

adj

noun

adp

det

noun

.

id52

    words often have more than one pos: back

    the back door = jj
    on my back = nn
    win the voters back = rb
    promised to back the bill = vb

the id52 problem is to determine the 
pos tag for a particular instance of a word.

word type tag ambiguity

methods

1. rule-based

a. start with a dictionary
b. assign all possible tags
c. write rules by hand to remove tags in context

2. stochastic

a. supervised/unsupervised
b. generative/discriminative
c.
d. id48s

independent/structured output

rule-based tagging

1. start with a dictionary:

she
promised
to
back
the
bill

prp
vbn, vbd
to
vb, jj, rb, nn
dt
nn, vb

rule-based tagging

2. assign all possible tags:

vbn
to
vbd
promised to

prp
she

nn
rb
jj
vb
back

nn
vb
bill

dt
the

rule-based tagging

3. introduce rules to reduce ambiguity:

nn
rb
jj
vb
back

vbn
to
vbd
promised to

prp
she
rule: "<start> prp {vbn, vbd}" -> "<start> prp vbd"

dt
the

nn
vb
bill

statistical models for id52

1. a classic: one of the first successful 

applications of statistical methods in nlp

2. extensively studied with all possible 

approaches (sequence models benchmark)
3. simple to get started on: data, eval, literature
4. an introduction to more complex 

segmentation and labelling tasks: ner, 
id66, global optimization

5. an introduction to id48s, used in many 

variants in id52 and related tasks.

supervision and resources

1. supervised case: data with words manually 

annotated with pos tags

2. partially supervised: annotated data + un-

annotated data

3. unsupervised: only raw text available
4. resources: dictionaries with words possible 

tags

5. start with the supervised task

id48s

id48 = (q,o,a,b)
1. states: q=q1..qn [the part of speech tags]
2. observation symbols: o = o1..ov [words]
3. transitions:

a. a = {aij}; aij = p(ts=qj|ts-1=qi)
b.
c. special vector of initial/final probabilities

ts | ts-1 = qi ~ multi(ai)

4. emissions:

a. b = {bik}; bik = p(ws = ok|ts=qi) 
b. ws| ts = qi ~ multi(bi)

markov chain interpretation

tagging process as a (hidden) markov process
    independence assumptions

1. limited horizon

2. time-invariant

3. observation depends only on the state

complete data likelihood

the joint id203 of a sequence of words 
and tags, given a model:

generative process:
1. generate a tag sequence
2. emit the words for each tag

id136 in id48s

three fundamental problems:
1. given an observation (e.g., a sentence) find 
the most likely sequence of states (e.g., pos 
tags)

2. given an observation, compute its 

id203

3. given a dataset of observation (sequences) 

estimate the model's parameters:

theta = (a,b)

id48s and fsa

id48s and fsa

also bayes nets, directed id114, 
etc.

other applications of id48s

    nlp
 

- id39, id66
- id40
- id42

    id103
    id161     image segmentation
    biology - protein structure prediction
economics, climatology, robotics...

pos as sequence classification
    observation: a sequence of n words w1:n
    response: a sequence of n tags t1:n 
    task: find the predicted t'1:n such that:

the best possible tagging for the sequence.

bayes rule reformulation

id48 id52

    how can we find t'1:n?
    enumeration of all possible sequences?

id48 id52

    how can we find t'1:n?
    enumeration of all possible sequences? 

    o(|tagset|n) !

    id145: viterbi algorithm

viterbi algorithm

example: model

a =

n
0.8
0.3
0.6

v
n
start

v
0.2
0.7
0.4

end
0.3
0.7

b =  

v

n

board

backs

plan

0.3

0.4

0.3

0.2

0.4

0.4

example: observation

    sentence: "board backs plan"
    find the most likely tag sequence
    ds(t) = id203 of most likely path ending 

at state s at time t

viterbi algorithm: example

end

v

n

start

board

backs

time

1

2

plan

3

viterbi: forward pass

end

v

n

start

board

backs

time

1

2

plan

3

viterbi: forward pass

end

v

n

start

d=.12

d=.24

board

backs

time

1

2

plan

3

viterbi: forward pass

end

v

n

start

d=.12

d=.24

board

backs

time

1

2

plan

3

viterbi: forward pass

end

v

n

start

d=.12

d=.050

d=.24

d=.019

board

backs

time

1

2

plan

3

viterbi: forward pass

end

v

n

start

d=.12

d=.050

d=.24

d=.019

board

backs

time

1

2

plan

3

viterbi: forward pass

d=.12

d=.050

d=.005

d=.24

d=.019

d=.016

end

v

n

start

board

backs

time

1

2

plan

3

viterbi: forward pass

d=.12

d=.050

d=.005

d=.24

d=.019

d=.016

end

v

n

start

board

backs

time

1

2

plan

3

end

v

n

start

viterbi: forward pass

d=.12

d=.050

d=.005

d=.011

d=.24

d=.019

d=.016

board

backs

time

1

2

plan

3

end

v

n

start

viterbi: backtrack

d=.12

d=.050

d=.005

d=.011

d=.24

d=.019

d=.016

board

backs

time

1

2

plan

3

end

v

n

start

viterbi: backtrack

d=.12

d=.050

d=.005

d=.011

d=.24

d=.019

d=.016

board

backs

time

1

2

plan

3

end

v

n

start

viterbi: backtrack

d=.12

d=.050

d=.005

d=.011

d=.24

d=.019

d=.016

board

backs

time

1

2

plan

3

viterbi: output

d=.011

end

v

n

start

board/n backs/v plan/n

time

1

2

3

observation id203

    given id48 theta = (a,b) and observation 

sequence w1:n compute p(w1:n|theta)

    applications: id38
    complete data likelihood:

    sum over all possible tag sequences:

forward algorithm

    id145: each state of the 

trellis stores a value alphai(s) = id203 of 
being in state s having observed w1:i

    sum over all paths up to i-1 leading to s

    init: 

forward algorithm

forward computation

end

v

n

start

a=.12

a=.24

board

backs

time

1

2

plan

3

forward computation

end

v

n

start

a=.12

a=.058

a=.24

a=.034

board

backs

time

1

2

plan

3

end

v

n

start

forward computation

a=.12

a=.058

a=.014

a=.24

a=.034

a=.022

board

backs

time

1

2

plan

3

end

v

n

start

forward computation

a=.12

a=.058

a=.014

a=0.2

a=.24

a=.034

a=.022

board

backs

time

1

2

plan

3

parameter estimation

maximum likelihood estimates (id113) on data
1. transition probabilities:

2. emission probabilities:

implementation details

1. start/end states
2. log space/rescaling
3. vocabularies: model pruning
4. higher order models: 

a. states representation
b. estimation and sparsity: deleted interpolation

evaluation

    so once you have you pos tagger running 

how do you evaluate it?

1. overall error rate with respect to a gold- 

standard test set.
a. er = # words incorrectly tagged/# words tagged
2. error rates on particular tags (and pairs) 
3. error rates on particular words (especially 

unknown words)

evaluation

     the result is compared with a manually 

coded    gold standard   

    typically accuracy > 97% on wsj ptb
    this may be compared with result for a 

baseline tagger (one that uses no context).
    baselines (most frequent tag) can achieve up to 

90% accuracy.

    important: 100% is impossible even for 

human annotators.

summary

    parts of speech 
    tagsets 
    id52
    id48 tagging:

    most likely tag sequence (decoding)
    id203 of an observation (word sequence)
    parameter estimation (supervised)
    evaluation

next class

    unsupervised id52 models (id48s)
    parameter estimation: forward-backward 

algorithm

    discriminative sequence models: maxent, 

crf, id88, id166, etc. 

    read j&m 5-6
    pre-process and pos tag the data: report 

problems & baselines

