ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   12:	
   

bayesian	
   id136,	
   
unsupervised	
   nlp	
   

1	
   

generagve	
   story	
   template	
   

2	
   

key	
   quangges	
   

our	
   data	
   is	
   a	
   set	
   of	
   samples:	
   	
   

3	
   

gibbs	
   sampling	
   template	
   

4	
   

topic	
   modeling	
   

blei	
   et	
   al.	
   (2003)	
   

5	
   

lda	
   generagve	
   story	
   

6	
   

gibbs	
   sampling	
   for	
   lda	
   

7	
   

gibbs	
   sampling	
   for	
   lda	
   

8	
   

       we	
   now	
   have	
   a	
   way	
   to	
   generate	
   samples	
   from	
   

the	
   posterior	
   for	
   the	
   lda	
   model	
   
       how	
   should	
   we	
   do	
   the	
   following?	
   

      get	
   topic	
   assignments	
   for	
   each	
   word	
   in	
   the	
   
document	
   collecgon?	
   
      get	
   topic	
   distribugon	
   for	
   a	
   document?	
   
      get	
   esgmates	
   of	
   topic-     word	
   distribugons	
   for	
   each	
   
topic?	
   

9	
   

lda	
   

10	
   

conjugate	
   priors	
   

       dirichlet	
   is	
   (simplest)	
   conjugate	
   prior	
   to	
   mulgnomial	
   

       dirichlet	
   hyperparameters	
   are	
   like	
      pseudo-     observagons   	
   

       de   nigon:	
      posterior	
   obtained	
   from	
   a	
   given	
   prior	
   in	
   

the	
   prior	
   family	
   and	
   a	
   given	
   likelihood	
   funcgon	
   belongs	
   
to	
   the	
   same	
   prior	
   family   	
   

       direct	
   result	
   of	
      algebraic	
   similarity   	
   between	
   prior	
   

family	
   and	
   likelihood	
   

       o^en	
   leads	
   to	
   tractability	
   &	
   closed-     form	
   analygc	
   

solugons	
   for	
   posterior	
   

11	
   

key	
   quangges	
   

our	
   data	
   is	
   a	
   set	
   of	
   samples:	
   	
   

12	
   

collapsed	
   gibbs	
   sampling	
   for	
   lda	
   

       the	
   collapsed	
   posterior	
   is	
   tricky	
   to	
   work	
   with	
   
because	
   all	
   latent	
   variables	
   become	
   coupled	
   

       i.e.,	
   we	
   now	
   have	
   fewer	
   independence	
   
assumpgons	
   to	
   help	
   us	
   simplify	
   things	
   

       [on	
   board]	
   

13	
   

collapsed	
   gibbs	
   sampling	
   for	
   lda	
   

14	
   

expectagon	
   maximizagon	
   (em)	
   

       em	
   is	
   an	
   algorithmic	
   template	
   that	
      nds	
   a	
   

local	
   maximum	
   of	
   the	
   marginal	
   likelihood	
   of	
   
the	
   observed	
   data	
   

15	
   

expectagon	
   maximizagon	
   (em)	
   

       working	
   instead	
   with	
   the	
   log-     likelihood:	
   

       where	
   qi	
   is	
   some	
   distribugon	
   over	
   values	
   for	
   z	
   

16	
   

expectagon	
   maximizagon	
   (em)	
   

       working	
   instead	
   with	
   the	
   log-     likelihood:	
   

via	
   jensen   s	
   
inequality	
   

17	
   

expectagon	
   maximizagon	
   (em)	
   

       maximize	
   lower	
   bound	
   of	
   the	
   log-     likelihood:	
   

       alternate	
   between	
   opgmizing	
   wrt	
   q	
   and	
   theta	
   

18	
   

em	
   

          e   	
   step:	
   	
   

      compute	
   posteriors	
   over	
   latent	
   variables:	
   

19	
   

em	
   

          e   	
   step:	
   	
   

      compute	
   posteriors	
   over	
   latent	
   variables:	
   

          m   	
   step:	
   

      update	
   parameters	
   given	
   posteriors:	
   

20	
   

em	
   for	
   lda	
   

          e   	
   step:	
   	
   

      compute	
   posteriors	
   over	
   latent	
   variables:	
   

21	
   

em	
   for	
   lda	
   

          e   	
   step:	
   	
   

      compute	
   posteriors	
   over	
   latent	
   variables:	
   

          m   	
   step:	
   

      update	
   parameters	
   given	
   posteriors:	
   

22	
   

