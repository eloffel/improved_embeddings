ttic	31210:

advanced	natural	language	processing

kevin	gimpel
spring	2017

lecture	14:

finish	up	bayesian/unsupervised	nlp,	

start	structured	prediction

1

    today	and	wednesday:	structured	prediction
    no	class	monday	may	29	(memorial	day)
    final	class	is	wednesday	may	31

2

    assignment	3	has	been	posted,	due	thursday	june	1
    final	project	report	due	friday,	june	9

3

key	quantities

our	data	is	a	set	of	samples:	

4

gibbs	sampling	template

5

lda

6

expectation	maximization	(em)

    em	is	an	algorithmic	template	that	finds	a	

local	maximum	of	the	marginal	likelihood	of	
the	observed	data

7

em

       e   	step:	

    compute	posteriors	over	latent	variables:

       m   	step:

    update	parameters	given	posteriors:

8

different	views	of	the	dirichlet process	(dp)
    last	time	we	discussed	the	   stick-breaking   	

view	of	the	dp

    today	we   ll	briefly	discuss	the	   chinese	

restaurant	process   	view

    with	both	views,	we	still	have	the	same	dp	

hyperparameters
(base	distribution	&	concentration	parameter)

9

base	distribution							for	dp

    our	unbounded	distribution	over	items	will	

choose	them	from	the	base	distribution

    base	distribution	usually	has	infinite	support
    simple	example	base	distribution	for	our	

morph	lexicon:

10

concentration	parameter
    in	stick-breaking	process,	concentration	

parameter	determines	how	much	of	the	stick	
we	break	off	each	time

    high	concentration	==	small	parts	of	stick

full	stick

11

    the	stick-breaking	construction	of	the	dp	is	

useful	for	specifying	models	and	defining	
id136	algorithms

    another	useful	way	of	representing	a	draw	
from	a	dp	is	with	the	chinese	restaurant	
process	(crp)
    crp	provides	a	distribution	over	partitions	with	an	

unbounded	number	of	parts

12

    imagine	a	chinese	restaurant	with	an	infinite	

number	of	tables   

1

2

3

4

   

13

    first	customer	sits	at	first	table:

1

2

3

4

   

14

    second	customer							enters,	chooses	a	table:

1

2

3

4

   

15

    second	customer							enters,	
chooses	table	1:

1

2

3

4

   

16

    second	customer							enters,	
chooses	table	1:
chooses	new	table:	

1

2

3

4

   

17

    second	customer							enters,	
chooses	table	1

1

2

3

4

   

18

    third	customer							enters,	

1

2

3

4

   

19

    third	customer							enters,	
chooses	table	1:
chooses	new	table:	

1

2

3

4

   

20

    third	customer							enters,	
chooses	new	table

1

2

3

4

   

21

    fourth	customer							enters,	
p(choose	table	1):																p(choose	table	2):	
p(choose	new	table):	

1

2

3

4

   

22

1

2

3

4

   

23

    large	value	of	concentration	parameter:

1

full	stick

2

3

4

   

24

    small	value	of	concentration	parameter:

1

2

3

4

   

25

a	draw	g	from	a	dp

(stick-breaking	representation)

draw	infinite	probabilities	from	
stick-breaking	process	with	parameter	s

draw	atoms	from	base	distribution

atoms	can	be	repeated!

26

a	representation	of	g	drawn	from	a	dp

(chinese	restaurant	process	representation)

draw	table	assignments	for	n customers	
with	parameter	s

for	each	occupied	table,	draw	atom	
from	base	distribution

each	draw	from	g	is	an	atom,	where	its	
id203	comes	from	the	number	of	
customers	at	its	table

number	of

tables	occupied

27

when	to	be	bayesian?

    if	you   re	doing	unsupervised	learning	or	

learning	with	latent	variables

    if	you	want	to	marginalize	out	some	model	

parameters

    if	you	want	to	learn	the	structure/architecture	

of	your	model

    if	you	want	to	learn	a	potentially-unbounded	

lexicon	(bayesian	nonparametrics)

28

what	is	structured	prediction?

29

modeling,	id136,	learning

30

modeling,	id136,	learning

modeling:	define		score	function

    modeling:	how	do	we	assign	a	score	to	an	

(x,y)	pair	using	parameters				?

31

modeling,	id136,	learning

id136:	solve														_ modeling:	define		score	function

    id136:	how	do	we	efficiently	search	over	

the	space	of	all	labels?

32

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

    learning:	how	do	we	choose				?

learning:	choose	_

33

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

structured	prediction:	
size	of	output	space	is	exponential	in	size	of	input
or	is	unbounded	(e.g.,	machine	translation)
(we	can   t	just	enumerate	all	possible	outputs)

34

simplest	kind	of	structured	prediction:	sequence	labeling

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

35

formulating	segmentation	tasks	as	sequence	labeling	

via	b-i-o	labeling:

named	entity	recognition

o																				o														o					b-person			i-person						o										o																	o

some			questioned			if									tim										cook							   s						first						product	

o														o									o																	o																	o								o					b-organization						o
would						be						a						breakaway				hit				for												apple														.

b	=	   begin   
i	=	   inside   
o	=	   outside   

36

constituent	parsing

(s	(np	the	man)	(vp	walked	(pp	to	(np	the	park))))

s

np

vp

pp

np

nn

dt
vbd						in				dt				nn
the	man	walked	to	the	park

key:
s	=	sentence
np	=	noun	phrase
vp	=	verb	phrase
pp	=	prepositional	phrase
dt	=	determiner
nn	=	noun
vbd	=	verb	(past	tense)
in	=	preposition

37

dependency	parsing

source:          $  konnten  sie  es    bersetzen  ?

   wall   	symbol

reference:     $  could  you  translate  it  ?

38

coreference resolution

as	we	head	towards	training	camp,	the	philadelphia	
eagles	have	finally	filled	most	of	their	needs	on	offense.	
one	of	the	main	goals	for	this	off-season	was	to	find	
weapons	for	the	team   s	franchise	quarterback,	carson	
wentz.	the	eagles needed	a	wide	receiver	who	could	
stretch	the	field	and	give	wentz the	opportunity	to	throw	
the	long	ball.	

they signed	receiver	torrey	smith	to	a	3-year	deal.	
while	the	signing	of	smith was	huge	for	the	team,	the	
biggest	signing	the	eagles	made	was	former	chicago	
bears	receiver	alshon jeffery.	he had	a	solid	5-year	stint	
in	chicago,	but	as	the	team	started	to	fall	apart,	jeffery
was	forced	to	explore	other	options.

39

coreference resolution

input:	a	document
output:	a	set	of	   mentions   	(textual	spans	in	document),	
and	memberships	of	those	mentions	in	clusters

40

(cid:96) question & answer systems 

semantic	role	labeling

   who      did what to whom      at where? 

 

the police officer detained the suspect at the scene of the crime 

arg0 
agent

v 

predicate

arg2 
theme

am-loc 
location

input:	a	sentence
output:	one	span	in	the	sentence	identified	as	a	
predicate,	and	a	set	of	other	spans	identified	as	particular	
roles	for	that	predicate

j&m/slp3

30 

supervised	word	alignment

given	parallel	sentences,	predict	word	alignments:

brown	et	al.	(1990)

42

machine	translation

    phrase-based	model	(koehn	et	al.,	2003):

sie es   bersetzen : you translate it

konnten : could

sie : you

  bersetzen :

translate

es : it

konnten sie : could you

es   bersetzen : translate it

sie : you

konnten : could

es : it

es : it

  bersetzen :

translate

? : ?

? : ?

input:	a	sentence	in	the	source	language
output:	a	segmentation	of	the	source	sentence	into	
segments,	a	translation	of	each	segment,	and	an	ordering	
of	the	translations

key	categories	of	structured	prediction
    i	think	of	structured	prediction	methods	in	

two	primary	categories:
score-based	and	search-based

44

score-based	structured	prediction

    focus	on	defining	the	score	function	of	the	

structured	input/output	pair:

    in	dependency	parsing,	this	is	called	   graph-based	

parsing   	because	minimum	spanning	tree	algorithms	
can	be	used	to	find	the	globally-optimal	max-scoring	
tree

45

search-based	structured	prediction
    focus	on	the	procedure	for	searching	through	the	
structured	output	space	(usually	involves	simple	
greedy	or	beam	search)

    design	a	classifier	to	score	a	small	number	of	

decisions	at	each	position	in	the	search
   

this	classifier	can	use	information	about	the	current	state	
as	well	as	the	entire	history	of	the	search

    in	dependency	parsing,	this	is	called	   transition-
based	parsing   	because	it	consists	of	greedily,	
sequentially	deciding	what	parsing	decision	to	make

46

structured	prediction

    to	make	sp	practical,	we	need	to	decompose	

the	sp	problem	into	parts

    this	is	true	whether	we	are	going	to	use	

search-based	or	score-based	sp
    score-based:	score	function	decomposes	

additively	into	scores	of	parts

    search-based:	search	factors	into	a	sequence	of	

decisions,	each	one	adding	a	part	to	the	final	
output	structure

47

