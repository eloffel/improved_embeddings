ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   13:	
   

introducbon	
   to	
   bayesian	
   
nonparametrics	
   for	
   nlp	
   

1	
   

   nonparametric   ?	
   

       nonparametric	
   does	
   not	
   mean	
      no	
   parameters   	
   

      

it	
   means	
   that	
      the	
   number	
   of	
   parameters	
   grows	
   as	
   
the	
   data	
   grows   	
   

       for	
   our	
   purposes,	
   think	
   of	
   it	
   as	
      some	
   component	
   of	
   
the	
   model	
   permits	
   an	
   unbounded	
   set	
   of	
   something   	
   

2	
   

parametric	
   or	
   nonparametric?	
   

model	
   

gaussian	
   mixture	
   model	
   (gmm)	
   

hidden	
   markov	
   model	
   (with	
   gmm	
   emissions)	
   

hidden	
   markov	
   model	
   (for	
   part-     of-     speech	
   tagging,	
   
with	
   mulbnomial	
   emissions)	
   

n-     gram	
   language	
   models	
   

lda	
   

lstm	
   language	
   model	
   

character-     level	
   lstm	
   language	
   model	
   

parametric	
   or	
   
nonparametric?	
   

parametric	
   

parametric	
   

nonparametric*	
   

nonparametric	
   

nonparametric*	
   

nonparametric*	
   

parametric	
   (assuming	
   
   xed	
   set	
   of	
   characters)	
   

*parametric	
   if	
   vocab	
      xed	
   

3	
   

parametric	
   or	
   nonparametric?	
   

model	
   

gaussian	
   mixture	
   model	
   (gmm)	
   

hidden	
   markov	
   model	
   (with	
   gmm	
   emissions)	
   

hidden	
   markov	
   model	
   (for	
   part-     of-     speech	
   tagging,	
   
with	
   mulbnomial	
   emissions)	
   

n-     gram	
   language	
   models	
   

lda	
   

lstm	
   language	
   model	
   

character-     level	
   lstm	
   language	
   model	
   

parametric	
   or	
   
nonparametric?	
   

parametric	
   

parametric	
   

nonparametric*	
   

nonparametric	
   

nonparametric*	
   

nonparametric*	
   

parametric	
   (assuming	
   
   xed	
   set	
   of	
   characters)	
   

*parametric	
   if	
   vocab	
      xed	
   

4	
   

          nonparametric	
   modeling   	
   in	
   terms	
   of	
   vocab	
   has	
   
a	
   lot	
   of	
   engineering	
   solubons:	
   
       use	
   unk	
   for	
   unknown	
   words,	
   do	
   smoothing	
   of	
   high-     

order	
   n-     grams,	
   etc.	
   

       in	
   this	
   case,	
   unbounded	
   part	
   of	
   model	
   is	
   mostly	
   

determined	
   by	
   observed	
   data,	
   heurisbcs	
   are	
   
useful	
   

       modeling	
   gets	
   more	
   interesbng	
   when	
   unbounded	
   

part	
   of	
   model	
   relates	
   to	
   latent	
   variables	
   

5	
   

       when	
   might	
   you	
   want	
   to	
   permit	
   an	
   

unbounded	
   set	
   of	
   latent	
   items	
   in	
   a	
   model?	
   

6	
   

in   nite	
   mixture	
   model	
   

       number	
   of	
   mixture	
   components	
   is	
   unbounded	
   

(grows	
   depending	
   on	
   the	
   data)	
   

       e.g.,	
   lda	
   with	
   an	
   unbounded	
   set	
   of	
   topics	
   

7	
   

   in   nite   	
   id48	
   

       id48s	
   permit	
   in   nite	
   sequences	
   already	
   
       what   s	
   new	
   here?	
   
       in   nite	
   number	
   of	
   hidden	
   states:	
   

beal	
   et	
   al.	
   (2002),	
   teh	
   et	
   al.	
   (2005)	
   

8	
   

   in   nite   	
   pid18	
   

       pid18s	
   can	
   already	
   handle	
   in   nite-     length	
   derivabons	
   
          in   nite   	
   here	
   means	
   an	
   in   nite	
   number	
   of	
   nonterminals:	
   

liang	
   et	
   al.	
   (2007),	
   finkel	
   et	
   al.	
   (2007)	
   

9	
   

       when	
   might	
   you	
   want	
   to	
   permit	
   an	
   

unbounded	
   set	
   of	
   latent	
   items	
   in	
   a	
   model?	
   
      number	
   of	
   topics	
   in	
   lda	
   
      number	
   of	
   gaussians	
   in	
   a	
   gaussian	
   mixture	
   model	
   
      number	
   of	
   hidden	
   states	
   in	
   an	
   id48	
   
      number	
   of	
   nonterminals	
   in	
   a	
   pid18	
   
      morph	
   lexicon	
   for	
   morphological	
   segmentabon	
   
      lexicon	
   for	
   chinese	
   word	
   segmentabon	
   
      number	
   of	
   coreference	
   chains	
   in	
   coreference	
   
resolubon	
   
      number	
   of	
   dimensions	
   in	
   an	
   embedding	
   (?!)	
   

10	
   

       we	
   need	
   priors	
   over	
   distribubons	
   that	
   permit	
   

an	
   unbounded	
   set	
   of	
   items	
   

11	
   

lda	
   generabve	
   story	
   

dimensionality	
   of	
   alpha	
   	
   

must	
   be	
   k	
   (the	
   number	
   of	
   topics)	
   

12	
   

dirichlet	
   process	
   (dp)	
   

          distribubon	
   over	
   distribubons   	
   

       unlike	
   dirichlet	
   distribubon,	
   dp	
   does	
   not	
   

require	
   pre-     specifying	
   number	
   of	
   components	
   

       we   ll	
   now	
   describe	
   how	
   a	
   dp	
   generates	
   a	
   

distribubon	
   over	
   an	
   unbounded	
   set	
   of	
   items	
   

13	
   

running	
   example	
   

       let   s	
   say	
   we   re	
   trying	
   to	
   segment	
   words	
   into	
   
morphological	
   units	
   without	
   any	
   supervision:	
   
      walking	
        	
   walk	
   +	
   ing	
   
      restarted	
        	
   re	
   +	
   start	
   +	
   ed	
   

       what	
   is	
   the	
   unbounded	
   set	
   of	
   latent	
   items	
   

here?	
   
      lexicon	
   of	
   possible	
   morphological	
   units	
   

14	
   

dirichlet	
   process	
   (dp)	
   

       contains	
   a	
      base	
   distribubon   	
   	
   
       simple	
   example	
   base	
   distribubon	
   for	
   our	
   

morph	
   lexicon:	
   

       e.g.,	
   id203	
   of	
      ing   :	
   

15	
   

dirichlet	
   processes	
   

       our	
   unbounded	
   distribubon	
   over	
   items	
   will	
   
choose	
   its	
   items	
   by	
   sampling	
   from	
   the	
   base	
   
distribubon	
   

       base	
   distribubon	
   typically	
   has	
   an	
   in   nite	
   set	
   of	
   

items	
   with	
   nonzero	
   id203,	
   as	
   in	
   our	
   
example:	
   

16	
   

items	
   and	
   probabilibes	
   
       base	
   distribubon	
   provides	
   the	
   items	
   

(   atoms   ),	
   as	
   many	
   as	
   we	
   want	
   

       where	
   do	
   their	
   probabilibes	
   come	
   from?	
   
       we	
   need	
   an	
   in   nite	
   set	
   of	
   probabilibes	
   that	
   

sum	
   to	
   1	
   

       dps	
   have	
   another	
   parameter:	
   concentrabon	
   

(strength)	
   parameter	
   s	
   

17	
   

sbck-     breaking	
   process	
   

full	
   sbck	
   

   	
   

       the	
   betas	
   form	
   an	
   in   nite	
   sequence	
   that	
   sums	
   to	
   1	
   
       they	
   provide	
   probabilibes	
   for	
   an	
   in   nite	
   set	
   of	
   items!	
   

18	
   

sbck-     breaking	
   process	
   

full	
   sbck	
   

       the	
   betas	
   form	
   an	
   in   nite	
   sequence	
   that	
   sums	
   to	
   1	
   
       they	
   provide	
   probabilibes	
   for	
   an	
   in   nite	
   set	
   of	
   items!	
   

19	
   

beta	
   distribubon	
   

20	
   

sbck-     breaking	
   with	
   high	
   concentrabon	
   (s	
   =	
   10)	
   

full	
   sbck	
   

21	
   

sbck-     breaking	
   with	
   high	
   concentrabon	
   (s	
   =	
   10)	
   

full	
   sbck	
   

       high	
   concentrabon	
   =	
   more	
   of	
   id203	
   mass	
   

preserved	
   for	
   other	
   pieces	
   in	
   the	
   sbck	
   

22	
   

sbck-     breaking	
   with	
   low	
   concentrabon	
   (s	
   =	
   0.1)	
   

full	
   sbck	
   

23	
   

sbck-     breaking	
   with	
   low	
   concentrabon	
   (s	
   =	
   0.1)	
   

full	
   sbck	
   

       low	
   concentrabon	
   =	
   stronger	
   power	
   law	
   

e   ects	
   in	
   resulbng	
   probabilibes	
   

24	
   

a	
   draw	
   g	
   from	
   a	
   dp	
   

draw	
   in   nite	
   probabilibes	
   from	
   	
   
sbck-     breaking	
   process	
   with	
   parameter	
   s	
   

draw	
   atoms	
   from	
   base	
   distribubon	
   

atoms	
   can	
   be	
   repeated!	
   

25	
   

a	
   draw	
   g	
   from	
   a	
   dp	
   

draw	
   in   nite	
   probabilibes	
   from	
   	
   
sbck-     breaking	
   process	
   with	
   parameter	
   s	
   

draw	
   atoms	
   from	
   base	
   distribubon	
   

atoms	
   can	
   be	
   repeated!	
   

26	
   

       the	
   sbck-     breaking	
   construcbon	
   of	
   the	
   dp	
   is	
   
useful	
   for	
   specifying	
   models	
   and	
   de   ning	
   
id136	
   algorithms	
   

27	
   

dirichlet	
   process	
   mixture	
   model	
   
       generabve	
   story	
   for	
   dataset	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   :	
   

what	
   should	
   the	
   base	
   distribubon	
   be?	
   

       each	
   x	
   is	
   generated	
   from	
   a	
   single	
   mixture	
   component	
   
       the	
   number	
   of	
   mixture	
   components	
   is	
   unbounded	
   

28	
   

