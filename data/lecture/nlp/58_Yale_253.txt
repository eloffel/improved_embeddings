nlp
introduction to nlp
lexicalized parsing
limitations of pid18s
the probabilities don   t depend on the specific words
e.g., give someone something (2 arguments) vs. see something (1 argument)
it is not possible to disambiguate sentences based on semantic information
e.g., eat pizza with pepperoni vs. eat pizza with fork
lexicalized grammars - idea
use the head of a phrase as an additional source of information
vp[ate] -> v[ate]
fundamental idea in syntax, cf. x-bar theory, hpsg
constituents receive their heads from their head child
parent annotation
[johnson 1998]
lexicalization

cake
with
fork
ate
child
ate
ate
head extraction example (collins)
np -> dt nnp nn        (rightmost)
np -> dt nn nnp        (rightmost)
np -> np pp                 (leftmost)
np -> dt jj                  (rightmost)
np -> dt                       (rightmost leftover child)
collins parser (1997) 1/2
generative, lexicalized model
horizontal markovization
only condition on the head (also on the distance d from the head)
types of rules
lhs     lnln   1   l1h r1   rm   1rm 
h gets generated first
l gets generated next
r gets generated last

collins parser (1997) 2/2
maximum likelihood estimates


smoothing (lexicalized, unlexicalized,    unheaded   )

smoothedp (ppof-in | vpthink-vb) =    1 p (ppof-in | vpthink-vb)  +
                           +    2 p (ppof-in | vp-vb) + (1      1      2) p (ppof-in | vp)) 
pml (ppof-in | vpthink-vb) = 
       count (ppof-in right of the head vpthink-vb) /
           count (symbols right of the head vpthink-vb)
issues with lexicalized grammars
sparseness of training data
many probabilities are difficult to estimate from the id32
e.g., whadjp (when not    how much    or    how many    only appears 6 times out of 1m constituents)
smoothing is essential
combinatorial explosion
parameterization is essential

discriminative reranking
issues with statistical parsers
a parser may return many parses of a sentence, with small differences in probabilities
the top returned parse may not necessarily be the best because the pid18 may be deficient
other considerations may need to be taken into account
parse tree depth
left attachment vs. right attachment
discourse structure
can you think of others features that may affect the reranking?

answer
considerations that may affect the reranking
parse tree depth
left attachment vs. right attachment
discourse structure
can you think of others?
consistency across sentences
or other stages of the nlu pipeline

discriminative reranking
n-best list
get the parser to produce a list of n-best parses (where n can be in the thousands)
reranking
train a discriminative classifier to rerank these parses based on external information such as a bigram id203 score or the amount of right branching in the tree


statistical parser performance
f1 (sentences <= 40 words)
charniak (2000)     90.1%
charniak and johnson (2005)     92% (discriminative reranking) 
all words
charniak and johnson (2005)     91.4%
fossum and knight (2009)     92.4% (combining constituent parsers)

notes
complexity of lexicalized parsing 
o(n5g3v3), instead of o(n3) because of the lexicalization
n = sentence length
g = number of non-terminals
v = vocabulary size
use id125 (charniak; collins)
sparse data
40,000 sentences; 12,409 rules (collins)
15% of all test sentences contain a rule not seen in training (collins)

notes
complements (arguments) vs. adjuncts (additional information)
np-c (collins)
subcategorization
e.g., transitive vs. intransitive verbs
parent annotation
np^s (johnson 1998)

example from michael collins
notes
learning pid18 without an annotated corpus
use em (inside-outside) (baker 1979), limited success
summary
lexicalization takes f1 from 75% to 90+%
most errors come from attachment ambiguities: pp and cc
markovization
horizontal (forgetful binarization)
vertical (generalized parent annotation)
note: infinite vertical markovization is inefficient (klein and manning 2003)
collins and charniak are generative models
reranking is a discriminative model
nlp
