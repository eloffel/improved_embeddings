ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   10:	
   

introducbon	
   to	
   bayesian	
   nlp	
   

1	
   

       assignment	
   2	
   has	
   been	
   posted,	
   due	
   may	
   17	
   
       grades	
   for	
   assignment	
   1	
   will	
   be	
   emailed	
   to	
   

you	
   soon	
   

       project	
   proposal	
   details	
   posted,	
   due	
   may	
   10	
   

2	
   

addibonal	
   reading	
   

       for	
   this	
   segment	
   of	
   the	
   

course,	
   the	
   opbonal	
   
text	
   is	
   cohen	
   (2016)	
   
       there	
   is	
   a	
   copy	
   in	
   the	
   

ttic	
   library	
   

       readings	
   will	
   be	
   drawn	
   
from	
   this	
   book	
   for	
   the	
   
next	
   few	
   lectures	
   

3	
   

mobvabon	
   

       in	
   most	
   neural	
   nlp,	
   we	
   assume	
   parameters	
   

and	
   architectures	
   are	
      xed	
   

       consider	
   a	
   one-     hidden-     layer	
   mlp:	
   

	
   

4	
   

mobvabon	
   

       in	
   most	
   neural	
   nlp,	
   we	
   assume	
   parameters	
   

and	
   architectures	
   are	
      xed	
   

       consider	
   a	
   one-     hidden-     layer	
   mlp:	
   

	
   
       now	
   let   s	
   be	
   more	
   explicit	
   about	
   what	
   we   re	
   

condiboning	
   on:	
   

5	
   

mobvabon	
   

       how	
   do	
   we	
   get	
   back	
   to	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ?	
   
	
   

6	
   

mobvabon	
   

       how	
   do	
   we	
   get	
   back	
   to	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ?	
   
       marginalize	
   over	
   new	
   random	
   variables:	
   
	
   

7	
   

mobvabon	
   

       how	
   do	
   we	
   get	
   back	
   to	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ?	
   
       marginalize	
   over	
   new	
   random	
   variables:	
   
	
   

8	
   

going	
   further   	
   

       marginalize	
   over	
   architectures	
   &	
   parameters:	
   

9	
   

going	
   further   	
   

       marginalize	
   over	
   architectures	
   &	
   parameters:	
   

       the	
   bayesian	
   framework	
   gives	
   us	
   a	
   vocabulary	
   
to	
   discuss	
   this	
   kind	
   of	
   thing	
   and	
   methods	
   for	
   
approximabng	
   these	
   computabons	
   

10	
   

why	
      bayesian   ?	
   

11	
   

bayesian	
   nlp	
   

       typically	
   used	
   with	
   unsupervised	
   learning:	
   

      we	
   have	
   data	
   
      we	
   hypothesize	
   some	
   latent	
   variables	
   through	
   
which	
   the	
   data	
   are	
   generated	
   
      we	
   de   ne	
   the	
      generabve	
   story   	
   that	
   describes	
   
how	
   latent	
   variables	
   are	
   generated,	
   then	
   how	
   
data	
   is	
   generated	
   using	
   latent	
   variables	
   
      we	
   parameterize	
   the	
   distribubons	
   &	
   add	
   the	
   
parameters	
   themselves	
   to	
   the	
   generabve	
   story	
   

12	
   

generabve	
   story	
   template	
   

13	
   

mulbnomial	
   distribubon	
   

       parameterized	
   by	
   a	
   vector	
   of	
   probabilibes,	
   

one	
   for	
   drawing	
   each	
   outcome	
   

       i.e.,	
   prob.	
   of	
   drawing	
   outcome	
   i	
   for	
   variable	
   a:	
   

14	
   

mulbnomial	
   distribubon	
   

       parameterized	
   by	
   a	
   vector	
   of	
   probabilibes,	
   

one	
   for	
   drawing	
   each	
   outcome	
   

       i.e.,	
   prob.	
   of	
   drawing	
   outcome	
   i	
   for	
   variable	
   a:	
   

       when	
   we	
   want	
   to	
   draw	
   from	
   this	
   distribubon,	
   

we	
   will	
   write:	
   

15	
   

       we	
   should	
   more	
   accurately	
   call	
   this	
   a	
   

   categorical	
   distribubon   	
   

       a	
   mulbnomial	
   is	
   more	
   general	
   (permits	
   more	
   

than	
   1	
   instance	
   of	
   an	
   event)	
   

       but	
   mulbnomial	
   is	
   used	
   frequently	
   to	
   mean	
   	
   

categorical	
   in	
   this	
   literature,	
   so	
   we   ll	
   oden	
   use	
   
mulbnomial	
   

16	
   

id44

david m. blei
computer science division
university of california
berkeley, ca 94720, usa
andrew y. ng
computer science department
stanford university
stanford, ca 94305, usa
michael i. jordan
computer science division and department of statistics
university of california
berkeley, ca 94720, usa

blei@cs.berkeley.edu

ang@cs.stanford.edu

jordan@cs.berkeley.edu

       generabve	
   model	
   for	
   document	
   collecbons	
   

editor: john lafferty
using	
   latent	
   variables	
   that	
   can	
   be	
   interpreted	
   
as	
      topics   	
   
       learns	
   a	
   mulbnomial	
   distribubon	
   over	
   words	
   

we describe id44 (lda), a generative probabilistic model for collections of
discrete data such as text corpora. lda is a three-level hierarchical bayesian model, in which each
item of a collection is modeled as a    nite mixture over an underlying set of topics. each topic is, in
turn, modeled as an in   nite mixture over an underlying set of topic probabilities. in the context of
text modeling, the topic probabilities provide an explicit representation of a document. we present
ef   cient approximate id136 techniques based on variational methods and an em algorithm for
empirical bayes parameter estimation. we report results in document modeling, text classi   cation,
and collaborative    ltering, comparing to a mixture of unigrams model and the probabilistic lsi
model.

for	
   each	
   topic	
   

abstract

17	
   

1. introduction

latent	
   dirichlet	
   allocabon	
   

(blei	
   et	
   al.,	
   2003)	
   

mulbnomial	
   distribubons	
   over	
   words	
   for	
   four	
   topics:	
   

the william randolph hearst foundation will give $1.25 million to lincoln center, metropoli-
tan opera co., new york philharmonic and juilliard school.    our board felt
that we had a
real opportunity to make a mark on the future of the performing arts with these grants an act
every bit as important as our traditional areas of support in health, medical research, education

18	
   

19	
   

generabve	
   story	
   for	
   simple	
   lda	
   

       simpli   ed	
   lda	
   model,	
   and	
   only	
   showing	
   generabve	
   story	
   for	
   a	
   

single	
   document:	
   

20	
   

generabve	
   story	
   for	
   simple	
   lda	
   

       simpli   ed	
   lda	
   model,	
   and	
   only	
   showing	
   generabve	
   story	
   for	
   a	
   

single	
   document:	
   

mulbnomial	
   distribubon	
   over	
   words	
   for	
   topic	
   	
   

21	
   

generabve	
   story	
   for	
   simple	
   lda	
   

       simpli   ed	
   lda	
   model,	
   and	
   only	
   showing	
   generabve	
   story	
   for	
   a	
   

single	
   document:	
   

what	
   should	
   we	
   keep	
   in	
   mind	
   	
   
when	
   choosing	
   this	
   distribubon?	
   

22	
   

dirichlet	
   distribubon	
   

       distribubon	
   over	
   vectors	
   with	
   entries	
   that	
   are	
   

all	
   posibve	
   and	
   sum	
   to	
   1	
   

       so	
   it   s	
   kind	
   of	
   like	
   a	
      distribubon	
   over	
   

(mulbnomial)	
   distribubons   	
   

normalizabon	
   term	
   that	
   depends	
   on	
   

23	
   

dirichlet	
   distribubon	
   
       parameterized	
   by	
   a	
   posibve	
   vector	
   

24	
   

[see	
   jupyter	
   notebook]	
   

25	
   

generabve	
   story	
   for	
   simple	
   lda	
   

       simpli   ed	
   lda	
   model,	
   and	
   only	
   showing	
   generabve	
   story	
   for	
   a	
   

single	
   document:	
   

26	
   

generabve	
   story	
   for	
   lda	
   

       now	
   we	
   show	
   explicitly	
   the	
   generabon	
   of	
   the	
   word	
   

mulbnomials	
   (once	
   for	
   the	
   document	
   collecbon)	
   

       where	
   should	
   the	
   hyperparameters	
   (alpha	
   and	
   psi)	
   

come	
   from?	
   

27	
   

graphical	
   model	
   for	
   lda	
   

28	
   

