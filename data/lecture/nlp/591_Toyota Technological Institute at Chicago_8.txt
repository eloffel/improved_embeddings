ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   8:	
   

neural	
   machine	
   translaeon	
   

1	
   

       neural	
   methods	
   have	
   transformed	
   machine	
   

translaeon	
   

       neural	
   machine	
   translaeon	
   (id4)	
   systems	
   
are	
   typically	
   based	
   on	
   sequence-     to-     sequence	
   
models	
   with	
   aneneon	
   

       today	
   we   ll	
   describe	
   a	
   number	
   of	
   

enhancements/modi   caeons	
   that	
   improve	
   
translaeon	
   quality	
   

2	
   

input	
   id56	
   (   encoder   )	
   

3	
   

output	
   id56	
   (   decoder   )	
   

4	
   

output	
   id56	
   (   decoder   )	
   

5	
   

output	
   id56	
   (   decoder   )	
   

6	
   

aneneon	
   for	
   id4	
   

       luong	
   et	
   al.	
   (2015)	
   simplify	
   &	
   generalize	
   the	
   
model	
   of	
   bahdanau,	
   and	
   compare	
   di   erent	
   
ways	
   of	
   de   ning	
   aneneon	
   

   e   ec&ve	
   approaches	
   to	
   a1en&on-     based	
   neural	
   machine	
   transla&on   	
   
luong	
   et	
   al.	
   (2015)	
   

7	
   

simplifying	
   aneneon	
   for	
   id4	
   

same	
   in	
   both:	
   

bahdanau	
   et	
   al.	
   (simpli   ed	
   a	
   bit	
   for	
   clarity):	
   

luong	
   et	
   al.:	
   

just	
   comes	
   from	
   decoder	
   id56	
   

how	
   is	
   this	
   simpler?	
   

8	
   

aneneon	
   funceons	
   

bahdanau	
   et	
   al.:	
   

luong	
   et	
   al.:	
   

   	
   

9	
   

global	
   aneneon	
   

global	
   =	
   computed	
   over	
   all	
   hidden	
   vectors	
   of	
   input	
   

10	
   

luong	
   et	
   al.	
   (2015)	
   

global	
   content-     based	
   aneneon	
   funceons	
   

global	
   =	
   computed	
   over	
   all	
   hidden	
   vectors	
   of	
   input	
   
content-     based	
   =	
   aneneon	
   funceon	
   looks	
   at	
   source	
   
hidden	
   vectors	
   

dot	
   product	
   (   dot   ):	
   
bilinear	
   (   general   ):	
   
feed-     forward	
   (   concat   ):	
   

parameter	
   vector	
   

11	
   

luong	
   et	
   al.	
   (2015)	
   

global	
   loca&on-     based	
   aneneon	
   funceon	
   

global	
   =	
   computed	
   over	
   all	
   hidden	
   vectors	
   of	
   input	
   
loca&on-     based	
   =	
   aneneon	
   funceon	
   does	
   not	
   look	
   
at	
   source	
   hidden	
   vectors	
   themselves,	
   just	
   posieons:	
   

parameter	
   vector	
   
for	
   posi&on	
   u	
   in	
   
source	
   sentence	
   

12	
   

luong	
   et	
   al.	
   (2015)	
   

results	
   

id7 

feed-     forward	
   (   concat   )	
   did	
   not	
   work	
   well!	
   

luong	
   et	
   al.	
   (2015)	
   

13	
   

local	
   aneneon	
   

local	
   =	
   computed	
   over	
   a	
   subset	
   of	
   input	
   hidden	
   vectors	
   

at	
   decoder	
   step	
   	
   	
   ,	
   
   nd	
   posieon	
   	
   	
   	
   	
   	
   in	
   source	
   
sentence,	
   
	
   
compute	
   aneneon	
   over	
   a	
   
window	
   centered	
   at	
   that	
   
posieon	
   in	
   the	
   source	
   
sentence	
   

14	
   

luong	
   et	
   al.	
   (2015)	
   

local	
   aneneon	
   

local-     m:	
   	
   set	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   assumes	
   roughly	
   monotonic	
   
alignment	
   between	
   decoder	
   posieons	
   and	
   source	
   
sentence	
   posieons	
   
local-     p:	
   	
   predict	
   	
   	
   	
   	
   	
   	
   based	
   on	
   decoder	
   hidden	
   state	
   
and	
   some	
   addieonal	
   parameters	
   

luong	
   et	
   al.	
   (2015)	
   

15	
   

results	
   

id7 

   e   ec&ve	
   approaches	
   to	
   a1en&on-     based	
   neural	
   machine	
   transla&on   	
   
luong	
   et	
   al.	
   (2015)	
   
16	
   

results	
   

id7 

   e   ec&ve	
   approaches	
   to	
   a1en&on-     based	
   neural	
   machine	
   transla&on   	
   
luong	
   et	
   al.	
   (2015)	
   
17	
   

   input	
   feeding   	
   of	
   decoder	
   hidden	
   states	
   

18	
   

luong	
   et	
   al.	
   (2015)	
   

modeling	
   coverage	
   

       id4	
   someemes	
   doesn   t	
   translate	
   all	
   source	
   words,	
   

or	
   translates	
   them	
   muleple	
   emes	
   

   modeling	
   coverage	
   for	
   neural	
   machine	
   transla&on   	
   
tu	
   et	
   al.	
   (2016)	
   

19	
   

       id4	
   someemes	
   doesn   t	
   translate	
   all	
   source	
   words,	
   

or	
   translates	
   them	
   muleple	
   emes:	
   

   modeling	
   coverage	
   for	
   neural	
   machine	
   transla&on   	
   tu	
   et	
   al.	
   (2016)	
   

20	
   

results:	
   modeling	
   coverage	
   

   modeling	
   coverage	
   for	
   neural	
   machine	
   transla&on   	
   
tu	
   et	
   al.	
   (2016)	
   

21	
   

id136	
   

22	
   

beam	
   search	
   

       to	
      nd	
   a	
   translaeon,	
   greedy	
   search	
   just	
   picks	
   

most-     probable	
   word	
   at	
   each	
   posieon	
   

       but	
   does	
   this	
   give	
   us	
   any	
   guarantees	
   about	
   the	
   

enere	
   translaeon?	
   

       beam	
   search	
   can	
   be	
   used	
   to	
   approximately	
   
   nd	
   the	
   most-     probable	
   complete	
   translaeon	
   

23	
   

beam	
   search	
   

let	
   beam	
   size	
   =	
   2:	
   

x	
   

   chat	
   smarter	
   with	
   allo   	
   
pranav	
   khaitan,	
   google	
   research	
   blog	
   

24	
   

learning	
   

25	
   

concern	
   

       there   s	
   a	
   mismatch	
   between	
   training	
   and	
   test!	
   
       (what	
   is	
   it?)	
   

26	
   

scheduled	
   sampling	
   

   scheduled	
   sampling	
   for	
   sequence	
   predic&on	
   with	
   recurrent	
   neural	
   networks   	
   
bengio	
   et	
   al.	
   (2015)	
   

27	
   

scheduled	
   sampling	
   results	
   

   always	
   sampling   	
   did	
   not	
   work	
   well!	
   

   scheduled	
   sampling	
   for	
   sequence	
   predic&on	
   with	
   recurrent	
   neural	
   networks   	
   
bengio	
   et	
   al.	
   (2015)	
   

28	
   

