ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   11:	
   

id136	
   in	
   bayesian	
   nlp	
   

1	
   

       project	
   proposals	
   due	
   wednesday,	
   may	
   10	
   

2	
   

bayesian	
   modeling	
   

       this	
   typically	
   means:	
   

      de   ning	
   parameters	
   as	
   random	
   variables	
   and	
   
giving	
   them	
   prior	
   distribulons	
   
      marginalizing	
   out	
   random	
   variables	
   whenever	
   
possible	
   (like	
   those	
   parameter	
   random	
   variables)	
   
      aiming	
   to	
   compute	
   posterior	
   over	
   all	
   latent	
   
variables	
   condiloned	
   on	
   observalons	
   (and	
   maybe	
   
some	
   hyperparameters)	
   

3	
   

molvalon	
   

       a	
   bayesian	
   approach	
   is	
   more	
   impacrul	
   when	
   

there	
   are	
   latent	
   variables	
   

       nlp	
   has	
   a	
   lot	
   of	
   models	
   with	
   latent	
   variables	
   
       unsupervised	
   learning	
   in	
   nlp:	
      consider	
   the	
   

unseen	
   output	
   as	
   a	
   latent	
   variable   	
   

4	
   

topic	
   modeling	
   

blei	
   et	
   al.	
   (2003)	
   

5	
   

word	
   alignment	
   

parallel	
   sentences	
   are	
   observed,	
   word	
   
alignments	
   are	
   latent	
   variables:	
   

brown	
   et	
   al.	
   (1990)	
   

6	
   

unsupervised	
   part-     of-     speech	
   tagging	
   

sentences	
   are	
   observed,	
   part-     of-     speech	
   tags	
   
are	
   treated	
   as	
   latent	
   variables:	
   

y1 

y2 

y3 

y4 

jusln	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   bieber	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   president	
   

7	
   

unsupervised	
   dependency	
   parsing	
   

sentences	
   are	
   observed,	
   dependency	
   parse	
   
trees	
   are	
   treated	
   as	
   latent	
   variables:	
   
source:          $  konnten  sie  es    bersetzen  ?

reference:     $  could  you  translate  it  ?

klein	
   &	
   manning	
   
smith	
   &	
   eisner	
   

8	
   

latent	
   syntaclc	
   categories	
   for	
   parsing	
   

       split	
   penn	
   treebank	
   syntaclc	
   categories	
   into	
   

   ner	
   subcategories	
   

petrov	
   (2009)	
   

9	
   

morphological	
   segmentalon	
   

sirts	
   &	
   goldwater	
   (2013)	
   

10	
   

morphological	
   segmentalons,	
   pos,	
   and	
   syntaclc	
   trees	
   

snyder	
   &	
   barzilay	
   

11	
   

generalve	
   stories	
   

       we	
   hypothesize	
   latent	
   variables	
   through	
   which	
   

data	
   are	
   generated	
   

       de   ne	
      generalve	
   story   	
   that	
   describes	
   how	
   
latent	
   variables	
   are	
   generated,	
   then	
   how	
   data	
   
is	
   generated	
   using	
   latent	
   variables	
   

       we	
   parameterize	
   the	
   distribulons	
   &	
   add	
   
parameter	
   generalon	
   to	
   generalve	
   story	
   

12	
   

generalve	
   story	
   template	
   

13	
   

id136	
   

       in	
   general,	
   id136	
   roughly	
   means	
      calculate	
   
       examples:	
   

stalslcal	
   quanlles	
   of	
   interest   	
   

      compute	
   the	
   mode	
   of	
   some	
   random	
   variables	
   
when	
   condiloning	
   on	
   some	
   and	
   marginalizing	
   out	
   
others	
   
      compute	
   marginals	
   of	
   some	
   random	
   variables	
   
(variable	
   posteriors	
   when	
   marginalizing	
   out	
   
everything	
   else)	
   
      compute	
   posterior	
   distribulon	
   over	
   some	
   subset	
   
of	
   random	
   variables	
   

14	
   

learning?	
   

       in	
   bayesian	
   nlp,	
   there   s	
   oeen	
   no	
      learning   	
   
       there	
   is	
   only	
      id136   	
   
       just	
   de   ne	
   model	
   and	
   do	
   id136	
   to	
   
calculate	
   what	
   we	
   want	
   to	
   calculate	
   
      no	
   parameters	
   are	
   being	
   eslmated	
   from	
   data*	
   
      we	
   are	
   not	
   oplmizing	
   any	
   loss	
   funclon*	
   
      there	
   is	
   no	
   gradient	
   descent*	
   

*	
   typically	
   

15	
   

generalve	
   story	
   template	
   

our	
   data	
   is	
   a	
   set	
   of	
   samples:	
   	
   

16	
   

key	
   quanlles	
   

our	
   data	
   is	
   a	
   set	
   of	
   samples:	
   	
   

17	
   

markov	
   chain	
   monte	
   carlo	
   (mcmc)	
   
       mcmc	
   algorithms	
   are	
   widely	
   used	
   in	
   bayesian	
   

modeling	
   but	
   also	
   useful	
   more	
   generally	
   
       can	
   be	
   used	
   to	
   generate	
   samples	
   from	
   

distribulons	
   that	
   are	
   hard	
   to	
   sample	
   from	
   

       samples	
   can	
   be	
   used	
   to	
   eslmate	
   quanlles	
   of	
   

interest	
   

       these	
   eslmates	
   are	
   unbiased	
   

18	
   

gibbs	
   sampling	
   

       gibbs	
   sampling	
   is	
   the	
   simplest	
   and	
   most	
   

widely-     used	
   mcmc	
   algorithm	
   (at	
   least	
   in	
   nlp)	
   

19	
   

gibbs	
   sampling	
   template	
   

20	
   

gibbs	
   sampling	
   template	
   

at	
   convergence,	
   each	
   lme	
   we	
   update	
   any	
   value	
   of	
   any	
   random	
   	
   
variable	
   in	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   we	
   have	
   another	
   sample	
   from	
   the	
   posterior	
   
	
   
these	
   samples	
   can	
   be	
   used	
   to	
   eslmate	
   any	
   quanlty	
   of	
   interest	
   	
   
while	
   o   ering	
   some	
   nice	
   theorelcal	
   properles	
   

21	
   

disadvantages	
   of	
   gibbs	
   sampling?	
   

nearby	
   samples	
   are	
   not	
   necessarily	
   uncorrelated,	
   so	
   it	
   	
   
	
   	
   	
   	
   can	
   take	
   many	
   samples	
   for	
   good	
   eslmates,	
   especially	
   	
   
	
   	
   	
   	
   of	
   rare	
   events	
   
	
   
guarantees	
   are	
   at	
   convergence,	
      burn-     in   	
   lme	
   	
   
	
   	
   	
   	
   can	
   be	
   hard	
   to	
   eslmate	
   &	
   depends	
   on	
   inilalizalon	
   
	
   
	
   

22	
   

lda	
   id136	
   cheat	
   sheet	
   

23	
   

