ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   5:	
   

sequence-     to-     sequence	
   modeling	
   

and	
   sentence	
   embeddings	
   

1	
   

recurrent	
   neural	
   networks	
   

   hidden	
   vector   	
   

2	
   

   output   	
   recurrent	
   neural	
   networks	
   

   hidden	
   vector   	
   

   output	
   symbol   	
   

3	
   

   output   	
   recurrent	
   neural	
   networks	
   

       y	
   is	
   a	
   symbol,	
   not	
   a	
   vector	
   
       o	
   is	
   the	
      output   	
   vocabulary	
   
       we	
   have	
   a	
   new	
   parameter	
   vector	
   emb(y)	
   for	
   
       emb(y)	
   	
   =	
   x?	
   
       id203	
   distribuuon	
   over	
   output	
   symbols?	
   

each	
   output	
   symbol	
   in	
   o	
   

4	
   

5	
   

example:	
   language	
   modeling	
   

   	
   	
   if	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   car	
   	
      	
   

6	
   

language	
   modeling:	
   training	
   

   	
   	
   if	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   car	
   	
      	
   

7	
   

language	
   modeling:	
   training	
   

   	
   	
   if	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   car	
   	
      	
   

   	
   

8	
   

language	
   modeling:	
   decoding	
   

       we   ll	
   use	
   the	
   term	
      decoding   	
   to	
   mean	
   

roughly	
      test-     ume	
   id136   	
   

       for	
   language	
   modeling,	
   decoding	
   means	
   
   output	
   the	
   highest-     id203	
   sentence   	
   

9	
   

language	
   modeling:	
   decoding	
   

<s>	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

   the   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

10	
   

language	
   modeling:	
   decoding	
   

<s>	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

   one   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

11	
   

language	
   modeling:	
   decoding	
   

<s>	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   the	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   one	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

12	
   

concern	
   

       there   s	
   a	
   mismatch	
   between	
   training	
   and	
   test!	
   
       (what	
   is	
   it?)	
   

13	
   

sequence-     to-     sequence	
   modeling	
   

       data:	
   <input	
   sequence,	
   output	
   sequence>	
   pairs	
   
       use	
   one	
   neural	
   network	
   to	
   encode	
   input	
   

sequence	
   as	
   a	
   vector	
   

       use	
   an	
   output	
   id56	
   to	
   generate	
   the	
   output	
   

sequence	
   (condiuoned	
   on	
   the	
   input	
   sequence	
   
vector)	
   

       more	
   generally	
   called	
      encoder-     decoder   	
   

architectures	
   

14	
   

emnlp	
   2013	
   

emnlp	
   2013	
   

emnlp	
   2014	
   

nips	
   2014	
   

nips	
   2014	
   

unsupervised	
   sentence	
   models	
   
       how	
   should	
   we	
   evaluate	
   sentence	
   models?	
   
       we	
   consider	
   two	
   ways	
   here:	
   

      sentence	
   similarity:	
   

       two	
   sentences	
   with	
   similar	
   meanings	
   should	
   have	
   high	
   
cosine	
   similariues	
   
       metric:	
   corr.	
   between	
   similariues	
   &	
   human	
   judgments	
   

      sentence	
   classi   cauon:	
   

       train	
   a	
   linear	
   classi   er	
   using	
   the	
      xed	
   sentence	
   
representauon	
   as	
   the	
   input	
   features	
   
       metric:	
   average	
   accuracy	
   over	
   6	
   tasks	
   

20	
   

evaluauon	
   1:	
   semanuc	
   textual	
   similarity	
   (sts)	
   
other	
   ways	
   are	
   needed.	
   

	
   	
   

we	
   must	
      nd	
   other	
   ways.	
   	
   

4.4	
   

i	
   absolutely	
   do	
   believe	
   there	
   was	
   an	
   iceberg	
   in	
   
those	
   waters.	
   
	
   
i	
   don't	
   believe	
   there	
   was	
   any	
   iceberg	
   at	
   all	
   
anywhere	
   near	
   the	
   titanic.	
   

	
   	
   

1.2	
   

results	
   reported	
   on	
   datasets	
   from	
   6	
   domains	
   

21	
   

paragraph	
   vectors	
   

       represent	
   sentence	
   (or	
   paragraph)	
   by	
   

predicung	
   its	
   own	
   words	
   or	
   context	
   words	
   

le	
   &	
   mikolov	
   (2014)	
   

22	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   

evaluauon	
   
sts	
   
44	
   

classi   ca?on	
   

70.4	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

23	
   

sentence	
   autoencoders	
   

       encode	
   sentence	
   as	
   vector,	
   then	
   decode	
   it	
   
       minimize	
   reconstrucuon	
   error	
   (using	
   squared	
   

error	
   or	
   cross	
   id178)	
   of	
   original	
   words	
   in	
   
sentence	
   

24	
   

recursive	
   neural	
   net	
   autoencoders	
   

       composiuon	
   based	
   on	
   syntacuc	
   parse	
   

socher,	
   huang,	
   pennington,	
   ng,	
   manning	
   (2011)	
   

25	
   

lstm	
   autoencoders	
   

       encode	
   sentence,	
   decode	
   sentence	
   

li,	
   luong,	
   jurafsky	
   (2015)	
   

26	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   

evaluauon	
   
sts	
   
44	
   
43	
   

classi   ca?on	
   

70.4	
   
75.9	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

27	
   

lstm	
   denoising	
   autoencoders	
   
       encode	
      corrupted   	
   sentence,	
   decode	
   sentence	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

28	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   

evaluauon	
   
sts	
   
44	
   
43	
   
38	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

29	
   

other	
   training	
   criteria	
   for	
   sentence	
   embeddings	
   
       encode	
   sentence,	
   decode	
   other	
   things	
   from	
   it:	
   

      decode	
   its	
   translauon	
   
      decode	
   neighboring	
   sentences	
   
      predict	
   words	
   in	
   the	
   sentence	
   and	
   predict	
   words	
   
in	
   neighboring	
   sentences	
   

30	
   

neural	
   machine	
   translauon	
   
       encode	
   source	
   sentence,	
   decode	
   translauon	
   

sutskever,	
   vinyals,	
   le	
   (2014)	
   
cho,	
   van	
   merrienboer,	
   gulcehre,	
   bahdanau,	
   bougares,	
   schwenk,	
   bengio	
   (2014)	
   

31	
   

encoder	
   as	
   a	
   sentence	
   embedding	
   model?	
   

sutskever,	
   vinyals,	
   le	
   (2014)	
   

32	
   

encoder	
   as	
   a	
   sentence	
   embedding	
   model?	
   

33	
   

sutskever,	
   vinyals,	
   le	
   (2014)	
   

cho,	
   van	
   merrienboer,	
   gulcehre,	
   bahdanau,	
   bougares,	
   schwenk,	
   bengio	
   (2014)	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
neural	
   mt	
   encoder	
   

evaluauon	
   
sts	
   
44	
   
43	
   
38	
   
42	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   
76.9	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

35	
   

skip-     thoughts	
   

       encode	
   sentence	
   using	
   an	
   id56	
   
       decode	
   two	
   neighboring	
   sentences	
   
       use	
   di   erent	
   id56s	
   for	
   previous	
   and	
   next	
   sentences	
   
       also	
   pass	
   center	
   sentence	
   vector	
   on	
   each	
   decoding	
   step	
   
   i	
   got	
   back	
   home	
   	
   	
   i	
   could	
   see	
   the	
   cat	
   on	
   the	
   steps	
   	
   	
   this	
   was	
   strange	
      	
   

kiros,	
   zhu,	
   salakhutdinov,	
   zemel,	
   torralba,	
   urtasun,	
   fidler	
   (2015)	
   

36	
   

skip-     thoughts	
   

query	
   sentence:	
   	
   
im	
   sure	
   youll	
   have	
   a	
   glamorous	
   evening	
   ,	
   she	
   said	
   ,	
   
giving	
   an	
   exaggerated	
   wink	
   .	
   
	
   
nearest	
   neighbor:	
   
im	
   really	
   glad	
   you	
   came	
   to	
   the	
   party	
   tonight	
   ,	
   he	
   said	
   ,	
   
turning	
   to	
   her	
   .	
   

kiros,	
   zhu,	
   salakhutdinov,	
   zemel,	
   torralba,	
   urtasun,	
   fidler	
   (2015)	
   

37	
   

skip-     thoughts	
   

query	
   sentence:	
   	
   
if	
   he	
   had	
   a	
   weapon	
   ,	
   he	
   could	
   maybe	
   take	
   out	
   their	
   last	
   
imp	
   ,	
   and	
   then	
   beat	
   up	
   errol	
   and	
   vanessa	
   .	
   
	
   
nearest	
   neighbor:	
   
if	
   he	
   could	
   ram	
   them	
   from	
   behind	
   ,	
   send	
   them	
   sailing	
   
over	
   the	
   far	
   side	
   of	
   the	
   levee	
   ,	
   he	
   had	
   a	
   chance	
   of	
   
stopping	
   them	
   .	
   

kiros,	
   zhu,	
   salakhutdinov,	
   zemel,	
   torralba,	
   urtasun,	
   fidler	
   (2015)	
   

38	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   

evaluauon	
   
sts	
   
44	
   
43	
   
38	
   
42	
   
31	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   
76.9	
   
85.3	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wieung,	
   bansal,	
   gimpel,	
   livescu	
   (2016)	
   

39	
   

fastsent	
   

       encode	
   center	
   sentence	
   using	
   sum	
   
       decode	
   to	
   predict	
   words	
   in	
   neighboring	
   sentences	
   
       di   erent	
   embedding	
   spaces	
   for	
      input   	
   and	
      output   	
   words	
   

   i	
   got	
   back	
   home	
   	
   	
   i	
   could	
   see	
   the	
   cat	
   on	
   the	
   steps	
   	
   	
   this	
   was	
   strange	
      	
   

home	
   

was	
   

strange	
   

   	
   

i	
   could	
   see	
   the	
   cat	
   on	
   the	
   steps	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

40	
   

fastsent	
   +	
   autoencoder	
   

       encode	
   center	
   sentence	
   using	
   sum	
   
       decode	
   to	
   predict	
   words	
   in	
   current+neighboring	
   sentences	
   

   i	
   got	
   back	
   home	
   	
   	
   i	
   could	
   see	
   the	
   cat	
   on	
   the	
   steps	
   	
   	
   this	
   was	
   strange	
      	
   

home	
   

cat	
   

   	
   

was	
   

strange	
   

i	
   could	
   see	
   the	
   cat	
   on	
   the	
   steps	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   

41	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
fastsent	
   
fastsent	
   +	
   autoencoder	
   

evaluauon	
   
sts	
   
44	
   
43	
   
38	
   
42	
   
31	
   
64	
   
62	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   
76.9	
   
85.3	
   
79.3	
   
79.7	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wieung,	
   bansal,	
   gimpel,	
   livescu	
   (2016)	
   

42	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
fastsent	
   
fastsent	
   +	
   autoencoder	
   
c-     phrase	
   

sts	
   
44	
   
43	
   
38	
   
42	
   
31	
   
64	
   
62	
   
67	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   
76.9	
   
85.3	
   
79.3	
   
79.7	
   
81.7	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wieung,	
   bansal,	
   gimpel,	
   livescu	
   (2016)	
   

43	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
fastsent	
   
fastsent	
   +	
   autoencoder	
   
c-     phrase	
   
avg.	
   pretrained	
   word	
   embeddings	
   

sts	
   
44	
   
43	
   
38	
   
42	
   
31	
   
64	
   
62	
   
67	
   
65	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   
76.9	
   
85.3	
   
79.3	
   
79.7	
   
81.7	
   
80.6	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wieung,	
   bansal,	
   gimpel,	
   livescu	
   (2016)	
   

44	
   

paraphrase	
   database	
   (ppdb)	
   
(ganitkevitch,	
   van	
   durme,	
   and	
   callison-     burch,	
   2013)	
   

credit:	
   chris	
   callison-     burch	
   

45	
   

good	
   

be	
   given	
   the	
   opportunity	
   to	
   

training	
   data:	
   phrase	
   pairs	
   from	
   ppdb	
   
	
   
	
   
	
   
	
   
	
   
	
   

i	
   can	
   hardly	
   hear	
   you	
   .	
   
and	
   the	
   establishment	
   
laying	
   the	
   foundauons	
   
making	
   every	
   e   ort	
   

great	
   

   	
   

   	
   

pave	
   the	
   way	
   
to	
   do	
   its	
   utmost	
   

have	
   the	
   possibility	
   of	
   
you	
   're	
   breaking	
   up	
   .	
   

as	
   well	
   as	
   the	
   development	
   

	
   

	
   tens	
   of	
   millions	
   more!	
   

	
   
	
   
	
   
	
   
	
   
	
   

sentence	
   embedding	
   model	
   
paragraph	
   vector	
   
lstm	
   autoencoder	
   
lstm	
   denoising	
   autoencoder	
   
neural	
   mt	
   encoder	
   
skip	
   thought	
   
fastsent	
   
fastsent	
   +	
   autoencoder	
   
c-     phrase	
   
avg.	
   pretrained	
   word	
   embeddings	
   
ours	
   (avg.	
   trained	
   on	
   ppdb)	
   	
   

sts	
   
44	
   
43	
   
38	
   
42	
   
31	
   
64	
   
62	
   
67	
   
65	
   
71	
   

classi   ca?on	
   

70.4	
   
75.9	
   
78.9	
   
76.9	
   
85.3	
   
79.3	
   
79.7	
   
81.7	
   
80.6	
   
n/a	
   

hill,	
   cho,	
   korhonen	
   (2016)	
   
wieung,	
   bansal,	
   gimpel,	
   livescu	
   (2016)	
   

47	
   

