ttic	31210:

advanced	natural	language	processing

kevin	gimpel
spring	2017

lecture	2:

elements	of	neural	nlp

1

    please	email	me	with	the	following:

    name
    email	address
    whether	you	are	taking	the	course	for	credit

    i	will	use	the	email	addresses	for	the	course	

mailing	list

2

assignment	1

    assignment	1	has	been	posted;	due	in	one	

week

3

roadmap

review	of	ttic	31190	(week	1)

   
    deep	learning	for	nlp	(weeks	2-4)
    generative	models	&	bayesian	id136	(week	5)
    bayesian	nonparametrics in	nlp	(week	6)
    em	for	unsupervised	nlp	(week	7)
    syntax/semantics	and	structure	prediction	(weeks	8-9)
    applications	(week	10)

4

what	is	a	classifier?

    a	function	from	inputs	x to	classification	labels	y
    one	simple	type	of	classifier:

    for	any	input	x,	assign	a	score	to	each	label	y,	

parameterized	by	vector	

:

    classify	by	choosing	highest-scoring	label:

5

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

6

notation

    we   ll	use	boldface	for	vectors:

   

individual	entries	will	use	subscripts	and	no	boldface,	e.g.,	for	
entry	i:

7

what	is	a	neural	network?

    just	think	of	a	neural	network	as	a	function
    it	has	inputs	and	outputs
    the	term	   neural   	typically	means	a	particular	

type	of	functional	building	block	(   neural	
layers   ),	but	the	term	has	expanded	to	mean	
many	things

    neural	modeling	is	now	better	thought	of	as	a	

modeling	strategy	(leveraging	   distributed	
representations   	or	   representation	
learning   ),	or	a	family	of	related	methods	

8

classifier	framework

    linear	model	score	function:

    we	can	also	use	a	neural	network	for	the	score	

function!

9

neural	layer	=	affine	transform	+	nonlinearity

nonlinearity

affine	transform

    this	is	a	single	   layer   	of	a	neural	network
    input	vector	is	
    vector	of	   hidden	units   	is	

10

nonlinearities

    most	common:	elementwise application	of	g

function	to	each	entry	in	vector

    examples   

11

tanh:

12

(logistic)	sigmoid:

13

rectified	linear	unit	(relu):

14

2-layer	network

    this	is	a	2-layer	neural	network
    input	vector	is	
    output	vector	is

15

2-layer	neural	network	for	sentiment	classification

16

use	softmax function	to	convert	scores	into	probabilities

17

why	nonlinearities?

2-layer	network:

written	in	a	single	equation:

    if	g	is	linear,	then	we	can	rewrite	the	above	as	

a	single	affine	transform

    can	you	prove	this?	(use	distributivity of	

matrix	multiplication)

18

understanding	the	score	function

entry	2	of	bias	vector

row	vector	corresponding	to	row	2	of	

19

parameter	sharing

parameters	not	

shared	between	labels

parameters	

shared	between	

labels

20

word	embeddings

       basic	unit   	of	neural	nlp

    we   ll	talk	about	ways	to	learn	word	

embeddings next	week

    today:	we   ll	assume	we	have	word	

embeddings as	a	black	box

21

word	embeddings

turian et	al. (2010)

22

two	ways	to	represent	word	embeddings
   
    1:	create	

-dimensional	   one-hot   	vector	for	
each	word,	multiply	by	word	embedding	matrix:

=	size	of	vocab

=	vocabulary	,	

    2:	store	embeddings in	a	hash/dictionary	data	

structure,	do	lookup	to	find	embedding	for	word:

    these	are	equivalent,	second	can	be	much	faster	

(though	first	can	be	fast	if	using	sparse	
operations)

23

two	ways	to	represent	word	embeddings
   
    1:	create	

-dimensional	   one-hot   	vector	for	
each	word,	multiply	by	word	embedding	matrix:

=	size	of	vocab

=	vocabulary	,	

    2:	store	embeddings in	a	hash/dictionary	data	

structure,	do	lookup	to	find	embedding	for	word:

    these	are	equivalent,	second	can	be	much	faster	

(though	first	can	be	fast	if	using	sparse	
operations)

24

two	ways	to	represent	word	embeddings
   
    1:	create	

-dimensional	   one-hot   	vector	for	
each	word,	multiply	by	word	embedding	matrix:

=	size	of	vocab

=	vocabulary	,	

    2:	store	embeddings in	a	hash/dictionary	data	

structure,	do	lookup	to	find	embedding	for	word:

    these	are	equivalent,	second	can	be	much	faster	

(though	first	can	be	fast	if	using	sparse	
operations)

25

encoders

    encoder:	a	function	to	represent	a	word	

sequence	as	a	vector

    simplest:	average	word	embeddings:

    many	other	functions	possible!

26

attention

    attention	is	a	useful	generic	tool
    often	used	to	replace	a	sum	with	an	attention-

weighted	sum

    e.g.,	for	a	word	summing	encoder:

    many	other	functions	possible!

27

attention

    attention	is	a	useful	generic	tool
    often	used	to	replace	a	sum	with	an	attention-

weighted	sum

    e.g.,	for	a	word	summing	encoder:

    many	other	functions	possible!

28

ling	et	al.	(emnlp	2015)

    not	all	contexts	are	created	equal:	better	

word	representations	with	variable	attention

where all parameters k and s are    xed at zero.
computing the attention of all words in the input
requires 2b operations, as it simply requires re-
trieving one value from the lookup matrix k for
each word and one value from the bias s for each
word in the window. considering that these mod-
els must be trainable on billions of tokens, ef   -
ciency is paramount. although more sophisticated
attentional models are certainly imaginable (bah-
danau et al., 2014), ours is a good balance of com-
putational ef   ciency and modeling expressivity.

2.3 parameter learning
gradients of the id168 with respect to
the parameters (w, o, k, s) are computed with
id26, and parameters are updated after
each training instance using a    xed learning rate.

antartica

has

little

rainfall

with

the

south

pole making

it

a

continental

desert

figure 1: illustration of the inferred attention pa-
rameters for a sentence from our training data
when predicting the word south; darker cells in-
dicate higher weights.

content words, such as antartica, rainfall, conti-
nental and desert are attributed higher weights as
these words provide hints that the predicted word

29

recurrent	neural	networks

   hidden	vector   

30

neural	similarity	learning

    a	common	need:	
compute	similarity/affinity	of	two	things
    maybe	two	things	of	the	same	type,
    two	things	with	different	types	being	mapped	to	

same	space,	or

    two	things	with	different	types	being	mapped	to	

different	spaces,	but	being	compared	with	a	
learned	similarity	function

    examples?

31

synonym	pairs

   

faruqui et	al.	(naacl	2014),	wieting et	al.	(tacl	2014),	inter	alia

contamination

pollution

converged
captioned

outwit
bad	
broad

convergence

subtitled
thwart
villain
general

permanent

permanently

bed

carefree
absolutely

   

sack

reckless
urgently

   

32

translation	pairs

    haghighi et	al.	(acl	2008),	mikolov et	al.	(2013),	faruqui and	dyer	(eacl	

2014)

dog
man
woman

city

person

the
the
the
   

hund
mann
frau
stadt
man
der
die
das
   

33

sentence	pairs

this	was	also	true	for	pompeii ,	where	the	temple	of	jupiter that	
was	already	there	was	enlarged	and	made	more	roman	when	
the	romans	took	over	.

this	held	true	for	pompeii ,	where	the	previously	existing	temple	
of	jupiter was	enlarged	and	romanized upon	conquest	.

34

captions	and	images

    richard	socher,	andrej	karpathy,	quoc v.	le,	christopher	d.	manning,	
andrew	y.	ng.	   grounded	compositional	semantics	for	finding	and	
describing	images	with	sentences,   	tacl	2014.

35

questions	and	answers

    mohit iyyer,	jordan	boyd-graber,	leonardo	claudino,	richard	socher,	and	

hal	daum   iii.	   a	neural	network	for	factoid	question	answering	over	
paragraphs,   	emnlp	2014

    antoine	bordes,	sumit chopra,	and	jason	weston.	   question	answering	

with	subgraph embeddings,   	emnlp 2014.

36

commonsense	knowledge	tuples

    xiang	li,	aynaz taheri,	lifu tu,	kevin	gimpel.	   commonsense	knowledge	

base	completion,   	acl 2016.

<   cake   ,	usedfor,	   satisfy	hunger   >

37

stories	and	endings

    story:

    jennifer	has	a	big	exam	tomorrow.	she	got	so	

stressed,	she	pulled	an	all-nighter.	she	went	into	
class	the	next	day,	weary	as	can	be.	her	teacher	
stated	that	the	test	is	postponed	for	next	week.

    ending:

    jennifer	felt	bittersweet	about	it.

    from	roc	story	corpus	(mostafazadeh et	al.,	

2016)

38

    other	examples/applications	you	can	think	of?

    sometimes	direction	matters,	sometimes	not
    sometimes	there	is	a	particular	kind	of	

relation	being	named	for	each	pair,	sometimes	
not	(i.e.,	just	one	kind)

39

neural	similarity	modeling

       siamese	networks   	(broid113y	et	al.,	1993)

    these	typically	share	parameters,	hence	the	name

40

similarity	modeling

    siamese	networks	typically	share	parameters	

across	the	two	networks

    but	it   s	also	common	to	not	share	parameters	
when	the	items	have	different	types,	but	we	
still	want	to	relate	them	in	some	way	
    whether	map	to	same	space	+	compute	sim	or	
map	each	to	some	other	space	+	compute	sim

41

similarity	functions
    many	choices	for	similarity	functions
    we   ll	go	over	some	in	the	next	few	slides
    throughout,	keep	in	mind:

    output	range
    symmetric?
    introduces	new	parameters?
    connections	among	similarity	functions?
    notes/tips	on	using	these

42

dot	product

range?
symmetric	or	asymmetric?
introduces	parameters?	 no

symmetric

43

cosine	similarity

range?
symmetric	or	asymmetric?
no
introduces	parameters?	
generalizes	anything?

symmetric

dot	product	when	vectors	
have	norm	1

44

bilinear	function

range?
symmetric	or	asymmetric?
introduces	parameters?
generalizes	anything?	

yes

asymmetric	in	general

dot	product	if							is	identity

45

notes	on	using	bilinear	functions

similarity	can	depend	on	relation	by	using	different	
bilinear	weight	matrices	for	different	relations:

often							is	initialized	to	the	identity	matrix	(and	
regularized	back	to	it)	
potential	issue:							might	be	very	huge

46

l1/l2	distances

range?
symmetric	or	asymmetric?
introduce	parameters? no

symmetric

47

deep	neural	network

concatenate	vectors,	pass	to	dnn,	use	scalar	for	final	output:

   

   

48

deep	neural	network

   

   

depends	on	nonlinearity

range?
symmetric?
asymmetric
introduces	parameters?
generalizes	anything?	

yes

yes,	can	represent	any	function!

49

notes	on	dnn	similarity	functions

since	dnns	are	so	powerful,	things	could	go	horribly	wrong.
in	practice,	often	pass	additional	quantities:

   

   

50

deep	neural	network

similarity	can	depend	on	relation:

   

   

(   cake   )

(   usedfor   )

(   satisfy	hunger   )

51

learning	for	similarity

    we	want	to	learn	input	representations	as	

well	as	all	parameters	of	

    we   ll	just	write	all	these	parameters	as	
    how	about	this?

    any	potential	problems	with	this?

52

(better)	learning	for	similarity

    contrastive	hinge	loss:

is	a	   negative   	example

   
    any	potential	problems	with	this?	

53

learning	with	neural	networks

    we	can	use	any	of	our	loss	functions	from	before,	

as	long	as	we	can	compute	(sub)gradients

    algorithm	for	doing	this	efficiently:	

id26

    it   s	basically	just	the	chain	rule	of	derivatives

54

computation	graphs

    a	useful	way	to	represent	the	computations	
performed	by	a	neural	model	(or	any	model!)

    why	useful?	makes	it	easy	to	implement	

automatic	differentiation	(id26)
    many	neural	net	toolkits	let	you	define	your	

model	in	terms	of	computation	graphs	
(theano,	(py)torch,	tensorflow,	cntk,	dynet,	
penne,	etc.)

55

id26

    id26 has	become	associated	with	
neural	networks,	but	it   s	much	more	general

    i	also	use	id26 to	compute	
gradients	in	linear models	for	structured	
prediction

56

a	simple	computation	graph:

    represents	expression	   a	+	3   

57

a	slightly	bigger	computation	graph:

    represents	expression	   (a	+	3)2 +	4a2   

58

operators	can	have	more	than	2	operands:

    still	represents	expression	   (a	+	3)2 +	4a2   

59

    more	concise:

60

overfitting &	id173
    when	we	can	fit	any	function,	overfitting

becomes	a	big	concern

    overfitting:	learning	a	model	that	does	well	on	
the	training	set	but	doesn   t	generalize	to	new	
data

    there	are	many	strategies	to	reduce	overfitting
(we   ll	use	the	general	term	id173 for	
any	such	strategy)	

    you	used	early	stopping	in	assignment	1,	

which	is	one	kind	of	id173

61

id173	terms

    most	common:	penalize	large	parameter	values
    intuition:	large	parameters	might	be	instances	of	

overfitting
    examples:

l2 id173:
(also	called	tikhonov id173	
or	ridge	regression)
l1 id173:
(also	called	basis	pursuit	or	lasso)

62

id173	terms

l2 id173:

differentiable,	widely-used

l1 id173:

not	differentiable	(but	is	subdifferentiable)
leads	to	sparse	solutions	(many	parameters	become	zero!)

63

dropout

    popular	id173	method	for	neural	

networks

    randomly	   drop	out   	(set	to	zero)	some	of	the	

vector	entries	in	the	layers

64

optimization	algorithms

    many	choices:

    sgd
    adagrad
    adadelta
    rmsprop
    adam
    sgd	with	momentum

    we	don   t	have	time	to	go	through	these	in	class,	

but	you	should	try	using	them!	(most	toolkits	
have	implementations	of	these	and	others)

65

2-transformation	(1-layer)	network

vector	of	label	scores

    we   ll	call	this	a	   2-transformation   	neural	

network,	or	a	   1-layer   	neural	network

    input	vector	is	
    score	vector	is
    one	hidden	vector											(   hidden	layer   )

66

1-layer	neural	network	for	sentiment	classification

67

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun																						prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    let   s	use	the	center	word	+	two	words	to	the	right:

vector	for	yo

vector	for	last

vector	for	name

if	name is	to	the	right	of	yo,	then	yo is	probably	a	form	of	your

   
    but	our	x above	uses	separate	dimensions	for	each	position!

    i.e.,	name	is	two	words	to	the	right
    what	if	name	is	one	word	to	the	right?		

68

convolution

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

vector	for	yo

vector	for	last

vector	for	name

69

pooling

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

how	do	we	convert	this	into	a	fixed-length	vector?
use	pooling:

max-pooling:	returns	maximum	value	in	
average pooling:	returns	average	of	values	in	

vector	for	yo

vector	for	last

vector	for	name

70

pooling

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

how	do	we	convert	this	into	a	fixed-length	vector?
use	pooling:

max-pooling:	returns	maximum	value	in	
average pooling:	returns	average	of	values	in	

vector	for	yo

vector	for	last

vector	for	name

then,	this	single	filter							produces	a	single	feature	
value	(the	output	of	some	kind	of	pooling).
in	practice,	we	use	many	filters	of	many	different	
lengths	(e.g.,	id165s	rather	than	words).	

71

convolutional	neural	networks

    convolutional	neural	networks	(convnets or	id98s)	use	

filters	that	are	   convolved	with   	(matched	against	all	
positions	of)	the	input

    think	of	convolution	as	   perform	the	same	operation	

everywhere	on	the	input	in	some	systematic	order   

       convolutional	layer   	=	set	of	filters	that	are	convolved	

with	the	input	vector	(whether	x or	hidden	vector)

    could	be	followed	by	more	convolutional	layers,	or	by	a	

    often	used	in	nlp	to	convert	a	sentence	into	a	feature	

type	of	pooling

vector

72

recurrent	neural	networks

   hidden	vector   

73

long	short-term	memory	(lstm)	recurrent	neural	networks

74

backward	&	bidirectional	lstms

bidirectional:	

if	shallow,	just	use	forward	and	backward	lstms	in	parallel,	concatenate	
final	two	hidden	vectors,	feed	to	softmax

75

deep	lstm
(2-layer)

layer	1

layer	2

76

recursive	neural	networks	for	nlp
    first,	run	a	constituent	parser	on	the	sentence
    convert	the	constituent	tree	to	a	binary	tree	

(each	rewrite	has	exactly	two	children)

    construct	vector	for	sentence	recursively	at	each	

rewrite	(   split	point   ):	

77

cost	functions

    cost	function:	scores	output	against	a	gold	standard

    should	reflect	the	evaluation	metric	for	your	task

    usual	conventions:
   
   

for	classification,	what	cost	should	we	use?
for	classification,	what	cost	should	we	use?

78

