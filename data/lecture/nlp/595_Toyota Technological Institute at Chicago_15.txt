ttic	31210:

advanced	natural	language	processing

kevin	gimpel
spring	2017

lecture	15:

structured	prediction

1

    no	class	monday	may	29	(memorial	day)
    final	class	is	wednesday	may	31

2

    assignment	3	has	been	posted,	due	thursday	june	1
    final	project	report	due	friday,	june	9

3

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

structured	prediction:	
size	of	output	space	is	exponential	in	size	of	input
or	is	unbounded	(e.g.,	machine	translation)
(we	can   t	just	enumerate	all	possible	outputs)

4

    2	categories	of	structured	prediction:

score-based	and	search-based

5

score-based	structured	prediction

    focus	on	defining	the	score	function	of	the	

structured	input/output	pair:

    cleanly	separates	score	function,	id136	

algorithm,	and	training	loss

6

modeling	in	score-based	sp

    define	score	as	a	sum	or	product	over	   parts   	

of	the	structured	input/output	pair:

7

parts	functions	in	score-based	sp

    for	an	id48:

    each	word-label	pair	forms	a	part,	and	each	

label	bigram	forms	a	part

8

parts	functions	in	score-based	sp

    for	a	linear	chain	crf:

    each	label	bigram	forms	a	part	(each	of	which	

includes	entire	input!)

9

parts	functions	in	score-based	sp

    for	a	pid18:

    each	context-free	grammar	rule	forms	a	part

10

parts	functions	in	score-based	sp

    for	an	arc-factored	dependency	parser:

    each	dependency	arc	forms	a	part

11

id136	in	score-based	sp

    id136	algorithms	are	defined	based	on	

decomposition	of	score	into	parts

    smaller	parts	=	easier	to	define	efficient	exact	

id136	algorithms

12

id136	algorithms	for	score-based	sp

    exact	id136	algorithms	are	often	based	on	

dynamic	programming

13

dynamic	programming	(dp)

    a	class	of	algorithms	that	break	problems	into	
smaller	pieces	and	reuse	solutions	for	pieces
    applicable	if	problem	has	certain	properties	(optimal	

substructure	and	overlapping	sub-problems)

    in	nlp,	we	use	dp	to	iterate	over	exponentially-large	

output	spaces	in	polynomial	time
    viterbi	and	forward/backward	for	id48s
    cky	for	pid18s
    eisner	algorithm	for	(arc-factored)	dependency	parsing

14

viterbi	algorithm

    recursive	equations	+	memoization:
base	case:	
returns	score	of	sequence	starting	with	label	y for	first	word

recursive	case:
computes	score	of	max-scoring	label	sequence	
that	ends	with	label	y at	position	t

final	value	is	in:

15

viterbi	algorithm

    space	and	time	complexity?
    can	be	read	off	from	the	recursive	equations:
space	complexity:
size	of	memoization table,	which	is	#	of	unique	indices	of	recursive	equations

length	of	
sentence

*

number	
of	labels

so,	space	complexity	is	o(|x|	|l|)

16

viterbi	algorithm

    space	and	time	complexity?
    can	be	read	off	from	the	recursive	equations:
time	complexity:
size	of	memoization table	*	complexity	of	computing	each	entry

length	of	
sentence

*

number	
of	labels

*

each	entry	requires	

iterating	through	the	labels

so,	time	complexity	is	o(|x|	|l|	|l|)	=	o(|x|	|l|2)	

17

feature	locality

    feature	locality:	how	big	are	the	parts?
    for	efficient	id136	(w/	dp	or	other	

methods),	we	need	to	be	mindful	of	this

    parts	can	be	arbitrarily	big	in	terms	of	input,	

but	not	in	terms	of	output!

    id48	parts	are	small	in	both	the	input	and	

output	(only	two	pieces	at	a	time)

18

learning	with	score-based	sp:
empirical	risk	minimization

19

cost	functions

    cost	function:	how	different	are	these	two	structures?

typically	used	to	compare	predicted	structure	to	gold	standard

   
    should	reflect	evaluation	metric	for	task

    usual	conventions:
   

for	classification,	what	cost	should	we	use?

20

cost	functions

   

for	classification,	we	used:

    how	about	for	sequences?

       hamming	cost   :	

       0-1	cost   :

21

empirical	risk	minimization

    this	is	intractable	so	we	typically	minimize	a	surrogate	

loss	function instead

22

loss	functions	for	score-based	sp

loss

name

cost	
(   0-1   )

percep-

tron

hinge

where	used

mert	(och,	2003)

structured	
id88	
(collins,	2002)
structured	id166s
(taskar et	al.,	

inter	alia)

23

loss	functions	for	score-based	sp

loss

name

cost	
(   0-1   )

percep-

tron

hinge

where	used

mert	(och,	2003)

structured	
id88	
(collins,	2002)
structured	id166s
(taskar et	al.,	

inter	alia)

24

loss	functions	for	score-based	sp

loss

name

cost	
(   0-1   )

percep-

tron

hinge

log

where	used

mert	(och,	2003)

structured	
id88	
(collins,	2002)
structured	id166s
(taskar et	al.,	

inter	alia)

crfs	(lafferty	et	

al.,	2001)

25

loss	functions	for	score-based	sp

loss

name

cost	
(   0-1   )

percep-

tron

hinge

log

where	used

mert	(och,	2003)

structured	
id88	
(collins,	2002)
structured	id166s
(taskar et	al.,	

inter	alia)

crfs	(lafferty	et	

al.,	2001)

softma

x-

margin

povey et	al.	

(2008),	gimpel	&	

smith	(2010)

26

id88

max	to	softmax

add	cost	
function

conditional
likelihood

add	cost	
function

max-margin

max	to	softmax

softmax-margin

27

results:	named	entity	recognition

(gimpel	&	smith,	2010)

id88
f1:	85.27

add	cost	
function

max-margin
f1:	85.55

max	to	softmax

conditional
likelihood
f1:	85.54

add	cost	
function

max	to	softmax

softmax-margin

f1:	86.03

28

id136	algorithms	for	score-based	sp

    dynamic	programming

    exact,	but	parts	must	be	small	for	efficiency
    dynamic	programming	+	   cube	pruning   

    permits	approximate	incorporation	of	large	parts	

(   non-local	features   )	while	still	using	dynamic	
program	backbone

    integer	linear	programming

29

cube	pruning

(chiang,	2007;	huang	&	chiang,	2007)

    modification	to	dynamic	programming	algorithms	for	

decoding	to	use	non-local	features	approximately

    keeps	a	k-best	list	of	derivations	for	each	item

    applies	non-local	feature	functions	on	these	derivations	when	

defining	new	items

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

cky	algorithm

s

np

pp

np

vp

np

np

vbz

nn

nnp nnp

there  near  the  top  of  the  list  is  quarterback troy aikman
0

7

1

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

   ngramtree    feature
(charniak	&	johnson,	2005)

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

   ngramtree    feature
(charniak	&	johnson,	2005)

s

np

pp

np

vp

np

np

pp

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

   ngramtree    feature
(charniak	&	johnson,	2005)

s

np

pp

np

vp

non-local	features	break	dynamic	programming!

np

pp

np

np

np

np

rb in

dt nn

in

dt nn

vbz

nn

nnp nnp

there near the   top    of   the    list    is  quarterback troy aikman

cnp,0,7 = cnp,0,1    cpp,1,7      np   np pp

s

np

pp

np

vp

np

np

vbz

nn

nnp nnp

there  near  the  top  of  the  list  is  quarterback troy aikman
0

7

1

cnp,0,7 = cnp,0,1    cpp,1,7      np   np pp

cnp,0,1 =

np
ex
there
0.4

pp

np

np
rb
there
0.3

pp

np

np
nnp
there
0.02

pp

np

pp

np

np

pp

np

np

pp

np

np

in

dt nn

in

dt nn

in

dt jj

in

dt nn

rb

dt nn

in

dt nn

near the   top    of   the    list

near the   top    of   the    list

near the   top    of   the    list

cpp,1,7 =

0.2

0.1

0.05

cnp,0,7 = cnp,0,1    cpp,1,7      np   np pp

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.08
0.06
0.004

0.1

0.04
0.03

0.002

0.05

0.02
0.015

0.001

np
ex
there

np
rb
there

cpp,1,7

cnp,0,1

0.4
0.3
0.02

np
nnp
there

cnp,0,7 = cnp,0,1    cpp,1,7      np   np pp

  np   np pp= 0.5

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.1

0.05

cpp,1,7

cnp,0,1

0.4
0.3
0.02

0.08     0.5
0.06     0.5
0.004     0.5

0.04     0.5
0.03     0.5

0.02     0.5
0.015     0.5

0.002     0.5

0.001     0.5

np
nnp
there

np
ex
there

np
rb
there

cnp,0,7 = cnp,0,1    cpp,1,7      np   np pp

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.04
0.03
0.002

0.1

0.02
0.015

0.001

0.05

0.01
0.0075

0.0005

np
ex
there

np
rb
there

cpp,1,7

cnp,0,1

0.4
0.3
0.02

np
nnp
there

np

pp

np

  there ex np np pp in near = 0.2

np

np

pp

np

ex in

dt nn

in

dt nn

there near the   top    of   the    list

np
ex
there

np
rb
there

cpp,1,7

cnp,0,1

0.4
0.3
0.02

np
nnp
there

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.1

0.04     0.2

0.02     0.2

0.03
0.002

0.015

0.001

0.05

0.01
0.0075

0.0005

  there ex np np pp in near = 0.2
  there rb np np pp in near = 0.6
  there nnp np np pp in near = 0.1
  there ex np np pp rb near = 0.1
  there rb np np pp rb near = 0.4
  there nnp np np pp rb near = 0.2
cpp,1,7

cnp,0,1

np
ex
there

np
rb
there

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.1

0.05

0.4
0.3
0.02

0.04     0.2
0.03     0.6
0.002     0.1

0.02     0.2
0.015     0.6

0.01     0.1
0.0075     0.4

0.001     0.1

0.0005     0.2

np
nnp
there

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.008
0.018
0.0002

0.1

0.004
0.009

0.0001

0.05

0.001
0.003

0.0001

np
ex
there

np
rb
there

cpp,1,7

cnp,0,1

0.4
0.3
0.02

np
nnp
there

pp

np

np

...

pp

np

np

...

pp

np

np

...

in

dt nn

in

dt jj

rb

dt

nn

near the   top    ...

near the   top    ...

near the   top    ...

0.2

0.008
0.018
0.0002

0.1

0.004
0.009

0.0001

0.05

0.001
0.003

0.0001

np
ex
there

np
rb
there

cpp,1,7

cnp,0,1

0.4
0.3
0.02

np
nnp
there

np
ex
there

np
rb
there

cpp,1,7

cnp,0,1

0.4
0.3
0.02

np
nnp
there

0.2

0.008
0.018
0.0002

0.1

0.004
0.009

0.0001

0.05

0.001
0.003

0.0001

np

pp

np
...

np

pp

np
...

np

pp

np
...

np

np

np

np

np

np

rb in

dt nn

rb in

dt jj

ex in

dt nn

there near the   top ...

there near the   top ...

there near the   top ...

cnp,0,7

0.018

0.009

0.008

clarification

    cube	pruning	does	not	actually	expand	all	k2 proofs	as	this	

example	showed

   

it	uses	a	fast	approximation	that	only	looks	at	o(k) proofs

integer	linear	programming

    (on	board)

51

