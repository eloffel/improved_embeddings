ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   9:	
   

id4,	
      nishing	
   up	
   neural	
   nlp	
   

1	
   

       assignment	
   2	
   has	
   been	
   posted,	
   due	
   may	
   17	
   
       grades	
   for	
   assignment	
   1	
   will	
   be	
   emailed	
   to	
   

you	
   soon	
   

       project	
   proposal	
   details	
   posted,	
   due	
   may	
   10	
   

2	
   

input	
   id56	
   (   encoder   )	
   

3	
   

output	
   id56	
   (   decoder   )	
   

4	
   

learning	
   

5	
   

minimum	
   risk	
   training	
   

          bayes	
   risk   	
   is	
   an	
   alternaxve	
   training	
   objecxve	
   

that	
   has	
   been	
   classically	
   used	
   in	
   speech	
   
recognixon	
   and	
   machine	
   translaxon	
   
       it	
   permits	
   the	
   use	
   of	
   nearly-     arbitrary	
   

evaluaxon	
   metrics	
   in	
   training	
   (through	
   the	
   
speci   caxon	
   of	
   a	
      cost	
   funcxon   )	
   

       [on	
   board]	
   

   minimum	
   risk	
   training	
   for	
   neural	
   machine	
   transla6on   	
   
shen	
   et	
   al.	
   (2016)	
   

6	
   

minimum	
   risk	
   training	
   

   minimum	
   risk	
   training	
   for	
   neural	
   machine	
   transla6on   	
   
shen	
   et	
   al.	
   (2016)	
   

7	
   

8	
   

google   s	
   id4	
   system	
   

9	
   

google   s	
   id4	
   system	
   

10	
   

google   s	
   id4	
   system	
   

they	
   use	
   a	
   procedure	
   that	
   determinis6cally	
   segments	
   any	
   
character	
   sequence	
   into	
   wordpieces	
   
	
   
vocab:	
   8k-     32k	
   wordpieces	
   
	
   
they	
      rst	
   learn	
   a	
      wordpiece	
   model   	
   
we   ll	
   talk	
   more	
   about	
   this	
   sort	
   of	
   thing	
   in	
   the	
   coming	
   weeks	
   

11	
   

google   s	
   id4	
   system	
   

12	
   

13	
   

14	
   

understanding	
   neural	
   nlp	
   

15	
   

how	
   do	
   we	
   understand	
   what	
   informaxon	
   

is	
   captured	
   in	
   an	
   embedding?	
   
       one	
   approach	
   is	
   to	
   abempt	
   to	
   predict	
   

di   erent	
   things	
   and	
   see	
   what	
   the	
   accuracy	
   is	
   
       e.g.,	
   from	
   a	
   sentence	
   embedding,	
   abempt	
   to	
   
predict	
   sentence	
   length,	
   words	
   in	
   sentence,	
   
and	
   word	
   order	
   in	
   sentence	
   (adi	
   et	
   al.,	
   2017)	
   
       they	
   compare	
   word	
   averaging	
   (   cbow   )	
   and	
   

an	
   lstm	
   (   ed   )	
   

16	
   

predicxng	
   sentence	
   length	
   from	
   sentence	
   embeddings	
   

length	
   predic6on	
   =	
   predict	
   the	
   length	
   bin	
   of	
   the	
   sentence,	
   using	
   8	
   
length	
   bins	
   (majority	
   baseline	
   gives	
   20%	
   accuracy)	
   

adi	
   et	
   al.	
   (iclr	
   2017)	
   

17	
   

predicxng	
   words	
   in	
   sentence	
   (   content   )	
   from	
   sentence	
   embeddings	
   

content	
   predic6on	
   =	
   predict	
   whether	
   a	
   word	
   is	
   contained	
   in	
   the	
   
sentence	
   (baseline	
   is	
   50%)	
   

adi	
   et	
   al.	
   (iclr	
   2017)	
   

18	
   

predicxng	
   word	
   order	
   from	
   sentence	
   embeddings	
   

order	
   predic6on	
   =	
   given	
   two	
   random	
   words	
   in	
   sentence,	
   predict	
   
which	
   comes	
      rst	
   (baseline	
   is	
   50%)	
   

adi	
   et	
   al.	
   (iclr	
   2017)	
   

19	
   

do	
   lstms	
   capture	
   syntacxc	
   agreement?	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

20	
   

do	
   lstms	
   capture	
   syntacxc	
   agreement?	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

21	
   

yes,	
   if	
   you	
   directly	
   train	
   to	
   predict	
   the	
   verb	
   form:	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

22	
   

lstm	
   unit	
   that	
   remembers	
   number	
   (singular	
   or	
   plural)	
   of	
   

earlier	
   noun	
   (x):	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

23	
   

in	
   a	
   relaxve	
   clause	
   (rc),	
   same	
   lstm	
   unit	
   recognizes	
   that	
   verb	
   

number	
   agrees	
   with	
   y:	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

24	
   

but	
   how	
   about	
   if	
   you	
   train	
   the	
   lstm	
   as	
   a	
   

language	
   model?	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

25	
   

then	
   it	
   doesn   t	
   do	
   well	
   at	
   predicxng	
   agreement:	
   

linzen	
   et	
   al.	
   (tacl	
   2016)	
   

26	
   

