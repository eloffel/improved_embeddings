ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   4:	
   

word	
   embeddings	
   (2)	
   

1	
   

collobert	
   et	
   al.	
   (2011)	
   

journal of machine learning research 12 (2011) 2493-2537

submitted 1/10; revised 11/10; published 8/11

natural language processing (almost) from scratch

ronan collobert   
jason weston   
l  eon bottou   
michael karlen
koray kavukcuoglu  
pavel kuksa  
nec laboratories america
4 independence way
princeton, nj 08540

editor: michael collins

ronan@collobert.com
jweston@google.com
leon@bottou.org
michael.karlen@gmail.com
koray@cs.nyu.edu
pkuksa@cs.rutgers.edu

we propose a uni   ed neural network architecture and learning algorithm that can be applied to var-
ious natural language processing tasks including part-of-speech tagging, chunking, named entity
recognition, and id14. this versatility is achieved by trying to avoid task-speci   c

abstract

2	
   

input window

word of interest

text

cat

sat on the mat

w1

1 w1

2

wk

1 wk

2

. . .

. . .

w1
n

wk
n

d

concat

n1

hu

feature 1

.
.
.

feature k

lookup table

ltw 1

.
.
.

ltw k

linear

m 1

     

hardtanh

linear

m 2

     

n2

hu = #tags

3	
   

figure 1: window approach network.

collobert	
   et	
   al.	
   pairwise	
   ranking	
   loss	
   

       	
   	
   	
   	
   	
   	
   is	
   training	
   set	
   of	
   11-     word	
   windows	
   
       	
   	
   	
   	
   	
   	
   is	
   vocabulary	
   
       what	
   is	
   going	
   on	
   here?	
   

      make	
   actual	
   text	
   window	
   have	
   higher	
   score	
   than	
   
all	
   windows	
   with	
   center	
   word	
   replaced	
   by	
   w	
   

4	
   

collobert	
   et	
   al.	
   pairwise	
   ranking	
   loss	
   

       	
   	
   	
   	
   	
   	
   is	
   training	
   set	
   of	
   11-     word	
   windows	
   
       	
   	
   	
   	
   	
   	
   is	
   vocabulary	
   
       this	
   stll	
   sums	
   over	
   entre	
   vocabulary,	
   so	
   it	
   

should	
   be	
   as	
   slow	
   as	
   log	
   loss   	
   

       why	
   can	
   it	
   be	
   faster?	
   

      when	
   using	
   sgd,	
   summaton	
        	
   sample	
   

5	
   

certainly require high capacity models, obtaining suf   ciently small con   dence intervals on the test
the 0.2 bits per character that separate human subjects from simple id165 models? since such tasks
set id178 may require prohibitively large training sets.16 the id178 criterion lacks dynamical
certainly require high capacity models, obtaining suf   ciently small con   dence intervals on the test
collobert	
   et	
   al.	
   (2011)	
   
range because its numerical value is largely determined by the most frequent phrases. in order to
set id178 may require prohibitively large training sets.16 the id178 criterion lacks dynamical
learn syntax, rare but legal phrases are no less signi   cant than common phrases.
range because its numerical value is largely determined by the most frequent phrases. in order to
it is therefore desirable to de   ne alternative training criteria. we propose here to use a pairwise
learn syntax, rare but legal phrases are no less signi   cant than common phrases.
ranking approach (cohen et al., 1998). we seek a network that computes a higher score when
it is therefore desirable to de   ne alternative training criteria. we propose here to use a pairwise
given a legal phrase than when given an incorrect phrase. because the ranking literature often deals
ranking approach (cohen et al., 1998). we seek a network that computes a higher score when
with information retrieval applications, many authors de   ne complex ranking criteria that give more
given a legal phrase than when given an incorrect phrase. because the ranking literature often deals
weight to the ordering of the best ranking instances (see burges et al., 2007; cl  emenc  on and vayatis,
with information retrieval applications, many authors de   ne complex ranking criteria that give more
2007). however, in our case, we do not want to emphasize the most common phrase over the rare
weight to the ordering of the best ranking instances (see burges et al., 2007; cl  emenc  on and vayatis,
but legal phrases. therefore we use a simple pairwise criterion.
2007). however, in our case, we do not want to emphasize the most common phrase over the rare
we consider a window approach network, as described in section 3.3.1 and figure 1, with
but legal phrases. therefore we use a simple pairwise criterion.
parameters    which outputs a score f  (x) given a window of text x = [w]
. we minimize the
we consider a window approach network, as described in section 3.3.1 and figure 1, with
ranking criterion with respect to   :
parameters    which outputs a score f  (x) given a window of text x = [w]
. we minimize the
ranking criterion with respect to   :
(17)
   !       x   x
(17)
   !       x   x
where x is the set of all possible text windows with dwin words coming from our training corpus, d
is the dictionary of words, and x(w) denotes the text window obtained by replacing the central word
where x is the set of all possible text windows with dwin words coming from our training corpus, d
of text window [w]
is the dictionary of words, and x(w) denotes the text window obtained by replacing the central word
okanohara and tsujii (2007) use a related approach to avoiding the id178 criteria using a
of text window [w]
binary classi   cation approach (correct/incorrect phrase). their work focuses on using a kernel
okanohara and tsujii (2007) use a related approach to avoiding the id178 criteria using a
classi   er, and not on learning id27s as we do here. smith and eisner (2005) also
binary classi   cation approach (correct/incorrect phrase). their work focuses on using a kernel
propose a contrastive criterion which estimates the likelihood of the data conditioned to a    negative   
classi   er, and not on learning id27s as we do here. smith and eisner (2005) also
neighborhood. they consider various data neighborhoods, including sentences of length dwin drawn
propose a contrastive criterion which estimates the likelihood of the data conditioned to a    negative   
from ddwin. their goal was however to perform well on some tagging task on fully unsupervised
neighborhood. they consider various data neighborhoods, including sentences of length dwin drawn
data, rather than obtaining generic id27s useful for other tasks.

max!0 , 1     f  (x) + f  (x(w))" ,
max!0 , 1     f  (x) + f  (x(w))" ,

by the word w.
by the word w.

   
w   d
   
w   d

dwin1
dwin1

dwin1
dwin1

6	
   

collobert	
   et	
   al.	
   (2011)	
   

       631m	
   word	
   tokens,	
   100k	
   vocab	
   size,	
   11-     word	
   

input	
   window,	
   4	
   weeks	
   of	
   training	
   
       they	
   didn   t	
   care	
   about	
   ge[ng	
   good	
   

perplexites,	
   just	
   good	
   word	
   embeddings	
   for	
   
their	
   downstream	
   nlp	
   tasks	
   

       so	
   a	
   pairwise	
   ranking	
   loss	
   makes	
   sense	
   in	
   this	
   

context	
   

7	
   

collobert	
   et	
   al.	
   (2011)	
   
       word	
   embedding	
   nearest	
   neighbors:	
   

collobert, weston, bottou, karlen, kavukcuoglu and kuksa

france

454

austria
belgium
germany

italy
greece
sweden
norway
europe
hungary

switzerland

jesus
1973
god
sati
christ
satan
kali
indra
vishnu
ananda
parvati
grace

xbox
6909
amiga

playstation

msx
ipod
sega

hd

psnumber

dreamcast

geforce
capcom

reddish
11724
greenish
bluish
pinkish
purplish
brownish
greyish
grayish
whitish
silvery

yellowish

scratched

megabits

87025
octets
mb/s
bit/s
baud
carats
kbit/s

29869
nailed
smashed
punched
popped
crimped
scraped
megahertz
screwed
sectioned megapixels
slashed
ripped

gbit/s
amperes

table 7: id27s in the word lookup table of the language model neural network lm1
trained with a dictionary of size 100,000. for each column the queried word is followed
by its index in the dictionary (higher means more rare) and its 10 nearest neighbors (using
the euclidean metric, which was chosen arbitrarily).

8	
   
and semantic properties of the neighbors are clearly related to those of the query word. these
results are far more satisfactory than those reported in table 7 for embeddings obtained using purely

id97	
   (mikolov	
   et	
   al.,	
   2013a)	
   

9	
   

id97	
   (mikolov	
   et	
   al.,	
   2013b)	
   

distributed representations of words and phrases

and their compositionality

tomas mikolov
google inc.
mountain view

mikolov@google.com

ilya sutskever
google inc.
mountain view

ilyasu@google.com

kai chen
google inc.
mountain view

kai@google.com

greg corrado
google inc.
mountain view

gcorrado@google.com

jeffrey dean
google inc.
mountain view

jeff@google.com

abstract

the recently introduced continuous skip-gram model is an ef   cient method for
learning high-quality distributed vector representations that capture a large num-
ber of precise syntactic and semantic word relationships. in this paper we present
several extensions that improve both the quality of the vectors and the training
speed. by subsampling of the frequent words we obtain signi   cant speedup and
also learn more regular word representations. we also describe a simple alterna-
tive to the hierarchical softmax called negative sampling.

10	
   

mikolov	
   et	
   al.	
   (2013a)	
   

11	
   

      

cbow	
   
like	
   collobert	
   et	
   al.	
   (2011),	
   except:	
   
       no	
   hidden	
   layers	
   
       averages	
   vectors	
   of	
   context	
   words	
   

rather	
   than	
   concatenatng	
   

       objectve	
   is	
   to	
   classify	
   center	
   word,	
   
so	
   it   s	
   a	
   multclass	
   classi   er	
   over	
   all	
   
possible	
   words	
   (could	
   be	
   very	
   slow!)	
   
       collobert	
   et	
   al	
   were	
   essentally	
   just	
   
training	
   a	
   binary	
   classi   er,	
   where	
   
negatve	
   examples	
   are	
   drawn	
   
randomly	
   from	
   vocab	
   

12	
   

mikolov	
   et	
   al.	
   (2013a)	
   

13	
   

skip-     gram	
   

       skip-     gram	
   objectve:	
   

sum	
   over	
   
positons	
   in	
   

corpus	
   

sum	
   over	
   context	
   
words	
   in	
   window	
   

(size	
   =	
   2c	
   +	
   1)	
   

14	
   

skip-     gram	
   model	
   uses	
   two	
   di   erent	
   vector	
   spaces:	
   

   output   	
   or	
   

   outside   	
   vector	
   

   input   	
   or	
   

   inside   	
   vector	
   

15	
   

skip-     gram	
   model	
   uses	
   two	
   di   erent	
   vector	
   spaces:	
   
	
   
	
   
why?	
   
which	
   should	
   we	
   use	
   as	
   our	
   word	
   embeddings?	
   

16	
   

normalizaton	
   requires	
   sum	
   over	
   what?	
   

17	
   

normalizaton	
   requires	
   sum	
   over	
   entre	
   vocabulary:	
   

18	
   

hierarchical	
   sojmax	
   
(morin	
   and	
   bengio,	
   2005)	
   

       based	
   on	
   a	
   new	
   generatve	
   story	
   for	
   	
   
       but	
   the	
   generatve	
   story	
   is	
   so	
   simple!	
   

      just	
   draw	
   from	
   the	
   conditonal	
   distributon	
   

       how	
   can	
   we	
   make	
   it	
   more	
   e   cient?	
   
       we	
   stll	
   need	
   it	
   to	
   be	
   true	
   that:	
   

19	
   

hierarchical	
   sojmax	
   
(morin	
   and	
   bengio,	
   2005)	
   

       based	
   on	
   a	
   new	
   generatve	
   story	
   for	
   	
   
       but	
   the	
   generatve	
   story	
   is	
   so	
   simple!	
   

      just	
   draw	
   from	
   the	
   conditonal	
   distributon	
   

       how	
   can	
   we	
   make	
   it	
   more	
   e   cient?	
   
       we	
   stll	
   need	
   it	
   to	
   be	
   true	
   that:	
   

20	
   

hierarchical	
   sojmax	
   in	
   id97	
   

       new	
   generatve	
   story	
   for	
   	
   
       idea:	
   	
   

       a	
   random	
   walk	
   through	
   the	
   vocabulary	
   biased	
   by	
   v	
   

       build	
   binary	
   tree	
   to	
   represent	
   vocabulary	
   
       to	
   generate	
   a	
   context	
   word	
   of	
   center	
   word	
   v,	
   start	
   at	
   

the	
   root	
   and	
   keep	
      ipping	
   biased	
   coins	
   (biased	
   
according	
   to	
   v)	
   to	
   choose	
   lej	
   or	
   right	
   

       stop	
   on	
   reaching	
   a	
   leaf	
   

       parameters	
   of	
   this	
   generatve	
   model	
   are	
   vectors	
   
for	
   individual	
   split	
   points	
   in	
   the	
   tree,	
   and	
   input	
   
vector	
   of	
   v	
   

21	
   

generatve	
   story	
   for	
   

root	
   

   ip	
   a	
   coin	
   with	
   pr(heads)	
   =	
   	
   
if	
   tails,	
   

the	
   

1	
   

	
   output	
      the   	
   as	
   u	
   

2	
   

red	
   

if	
   heads,	
   

	
      ip	
   coin	
   w/	
   pr(heads)	
   =	
   

   	
   

dog	
   

if	
   heads,	
   

	
   output	
      red   	
   as	
   u	
   

if	
   tails,	
   

	
      ip	
   coin	
   w/	
   pr(heads)	
   =	
   	
   

   	
   

22	
   

generatve	
   story	
   for	
   

root	
   

   ip	
   a	
   coin	
   with	
   pr(heads)	
   =	
   	
   
if	
   tails,	
   

1	
   

	
   output	
      the   	
   as	
   u	
   

the	
   

2	
   

red	
   

   	
   

dog	
   

if	
   heads,	
   

	
      ip	
   coin	
   w/	
   pr(heads)	
   =	
   

if	
   heads,	
   

	
   output	
      red   	
   as	
   u	
   

   	
   

what	
   is	
   the	
   normalizaton	
   constant?	
   

23	
   

generatve	
   story	
   for	
   

root	
   

   ip	
   a	
   coin	
   with	
   pr(heads)	
   =	
   	
   
if	
   tails,	
   

the	
   

1	
   

	
   output	
      the   	
   as	
   u	
   

2	
   

red	
   

if	
   heads,	
   

	
      ip	
   coin	
   w/	
   pr(heads)	
   =	
   

   	
   

dog	
   

if	
   heads,	
   

	
   output	
      red   	
   as	
   u	
   

   	
   
what	
   is	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ?	
   

24	
   

generatve	
   story	
   for	
   

root	
   

   ip	
   a	
   coin	
   with	
   pr(heads)	
   =	
   	
   
if	
   tails,	
   

1	
   

	
   output	
      the   	
   as	
   u	
   

the	
   

2	
   

red	
   

   	
   

dog	
   

if	
   heads,	
   

	
      ip	
   coin	
   w/	
   pr(heads)	
   =	
   

if	
   heads,	
   

	
   output	
      red   	
   as	
   u	
   

   	
   

can	
   you	
   prove	
   that	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ?	
   

25	
   

hierarchical	
   sojmax	
   for	
   skip-     gram	
   

(mikolov	
   et	
   al.,	
   2013)	
   

       each	
   word	
   has	
   a	
   unique	
   path	
   from	
   root	
   
       rather	
   than	
   learn	
   output	
   vectors	
   for	
   all	
   words,	
   
learn	
   output	
   vectors	
   only	
   for	
   internal	
   nodes	
   of	
   
the	
   binary	
   tree	
   

       how	
   should	
   we	
   arrange	
   the	
   words	
   into	
   a	
   

binary	
   tree?	
   

26	
   

       how	
   about	
   this	
   binary	
   tree?	
   

root	
   

obtusely	
   

gomez-     like	
   

albertopolis	
   

   	
   

a	
   

the	
   

27	
   

       how	
   about	
   this	
   binary	
   tree?	
   

root	
   

obtusely	
   

gomez-     like	
   

albertopolis	
   

   	
   

a	
   

the	
   

why	
   is	
   this	
   tree	
   bad?	
   

give	
   two	
   reasons.	
   

28	
   

       how	
   about	
   this	
   binary	
   tree?	
   

root	
   

the	
   

a	
   

that	
   

   	
   

obtusely	
   
why	
   is	
   this	
   tree	
   bad?	
   

gomez-     like	
   

29	
   

       id97	
   uses	
   hu   man	
   coding	
   (common	
   words	
   

have	
   short	
   codes,	
   i.e.,	
   are	
   near	
   top	
   of	
   tree)	
   

30	
   

negatve	
   sampling	
   
(mikolov	
   et	
   al.,	
   2013)	
   

       rather	
   than	
   sum	
   over	
   entre	
   vocabulary,	
   

generate	
   samples	
   and	
   sum	
   over	
   them	
   

       instead	
   of	
   a	
   multclass	
   classi   er,	
   use	
   a	
   binary	
   

classi   er:	
   

       similar	
   to	
   idea	
   of	
   collobert	
   et	
   al	
   but	
   uses	
   log	
   

loss	
   instead	
   of	
   hinge	
   loss	
   

31	
   

negatve	
   sampling	
   
(mikolov	
   et	
   al.,	
   2013)	
   

       neg	
   contains	
   2-     20	
   words	
   sampled	
   from	
   some	
   

distributon	
   
      e.g.,	
   uniform,	
   unigram,	
   or	
   smoothed	
   unigram	
   
      smoothed:	
   raise	
   probabilites	
   to	
   power	
     ,	
   
renormalize	
   to	
   get	
   a	
   distributon	
   

32	
   

alternatves	
   

       noise-     contrastve	
   estmaton	
   (gutmann	
   &	
   hyvarinen,	
   

2010;	
   2012)	
   

       applied	
   to	
   language	
   modeling	
   by	
   mnih	
   &	
   teh	
   (2012)	
   

33	
   

glove	
   

(pennington	
   et	
   al.,	
   2014)	
   

glove: global vectors for word representation

jeffrey pennington, richard socher, christopher d. manning

computer science department, stanford university, stanford, ca 94305

jpennin@stanford.edu, richard@socher.org, manning@stanford.edu

abstract

recent methods for learning vector space
representations of words have succeeded
in capturing    ne-grained semantic and
syntactic regularities using vector arith-
metic, but the origin of these regularities
has remained opaque. we analyze and
make explicit the model properties needed
for such regularities to emerge in word
vectors. the result is a new global log-

the    ner structure of the word vector space by ex-
amining not the scalar distance between word vec-
tors, but rather their various dimensions of dif-
ference. for example, the analogy    king is to
queen as man is to woman    should be encoded
in the vector space by the vector equation king  
queen = man   woman. this evaluation scheme
favors models that produce dimensions of mean-
34	
   
ing, thereby capturing the multi-id91 idea of
distributed representations (bengio, 2009).

other	
   work	
   on	
   word	
   embeddings	
   

       actve	
   research	
   area	
   (probably	
   too	
   actve)	
   
       other	
   directons:	
   

      multple	
   embeddings	
   for	
   a	
   single	
   word	
   
corresponding	
   to	
   di   erent	
   word	
   senses	
   
      using	
   subword	
   informaton	
   (e.g.,	
   characters)	
   in	
   
word	
   embeddings	
   
      tailoring	
   embeddings	
   for	
   di   erent	
   nlp	
   tasks	
   

35	
   

       pretrained	
   word	
   embeddings	
   are	
   really	
   useful!	
   

       what	
   about	
   embeddings	
   for	
   phrases	
   and	
   

sentences?	
   

	
   

36	
   

unsupervised	
   sentence	
   models	
   

37	
   

       how	
   are	
   sentence	
   embeddings	
   useful?	
   

      mult-     document	
   summarizaton	
   
      automatc	
   essay	
   grading	
   
      evaluaton	
   of	
   text	
   generaton	
   systems	
   
      machine	
   translaton	
   
      entailment/id136	
   

38	
   

unsupervised	
   sentence	
   models	
   
       how	
   should	
   we	
   evaluate	
   sentence	
   models?	
   
       we	
   consider	
   two	
   kinds	
   of	
   evaluatons	
   here:	
   

      sentence	
   similarity:	
   intrinsic	
   evaluaton	
   of	
   
sentence	
   embedding	
   space,	
   no	
   additonal	
   learned	
   
parameters	
   
      sentence	
   classi   caton:	
   train	
   a	
   linear	
   classi   er	
   
(logistc	
   regression)	
   using	
   the	
      xed	
   sentence	
   
representaton	
   as	
   the	
   input	
   features	
   
       reportng	
   average	
   accuracies	
   over	
   6	
   tasks	
   

39	
   

