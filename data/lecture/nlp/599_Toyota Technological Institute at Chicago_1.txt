ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   
	
   

lecture	
   1:	
   introducbon	
   

1	
   

       please	
   email	
   me	
   with	
   the	
   following:	
   

      name	
   
      email	
   address	
   
      whether	
   you	
   are	
   taking	
   the	
   course	
   for	
   credit	
   

       i	
   will	
   use	
   the	
   email	
   addresses	
   for	
   the	
   course	
   

mailing	
   list	
   

2	
   

what	
   is	
   natural	
   language	
   processing?	
   

3	
   

what	
   is	
   natural	
   language	
   processing?	
   
an	
   experimental	
   computer	
   science	
   research	
   area	
   
that	
   includes	
   problems	
   and	
   solubons	
   pertaining	
   to	
   

the	
   understanding	
   of	
   human	
   language	
   

4	
   

text	
   classi   cabon	
   

5	
   

senbment	
   analysis	
   

6	
   

machine	
   translabon	
   

new	
   poll:	
   will	
   you	
   buy	
   an	
   apple	
   watch?	
   

7	
   

quesbon	
   answering	
   

8	
   

summarizabon	
   

the	
   apple	
   watch	
   has	
   drawbacks.	
   there	
   are	
   other	
   

smartwatches	
   that	
   o   er	
   more	
   capabilibes.	
   	
   

9	
   

dialog	
   systems	
   

user:	
   schedule	
   a	
   meebng	
   with	
   max	
   and	
   david	
   on	
   thursday.	
   
computer:	
   thursday	
   won   t	
   work	
   for	
   david.	
   how	
   about	
   friday?	
   
user:	
   i   d	
   prefer	
   monday	
   then,	
   but	
   friday	
   would	
   be	
   ok	
   if	
   necessary.	
   
	
   

10	
   

part-     of-     speech	
   tagging	
   

determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   proper	
   	
   	
   	
   	
   proper	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
	
   	
   	
   some	
   	
   	
   	
   	
   	
   quesboned	
   	
   	
   	
   	
   	
   if	
   	
   	
   	
   	
   	
   	
   tim	
   	
   	
   	
   	
   	
   cook	
   	
   	
   	
   	
   	
      s	
   	
   	
   	
   	
   	
      rst	
   	
   	
   	
   	
   	
   product	
   	
   
	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   proper	
   	
   	
   	
   	
   punc.	
   
	
   	
   	
   would	
   	
   	
   	
   	
   	
   be	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   breakaway	
   	
   	
   	
   	
   	
   hit	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   apple	
   	
   	
   	
   	
   	
   	
   	
   .	
   

11	
   

part-     of-     speech	
   tagging	
   

determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   proper	
   	
   	
   	
   	
   proper	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
	
   	
   	
   
	
   	
   	
   some	
   	
   	
   	
   	
   	
   quesboned	
   	
   	
   	
   	
   	
   if	
   	
   	
   	
   	
   	
   	
   tim	
   	
   	
   	
   	
   	
   cook	
   	
   	
   	
   	
   	
      s	
   	
   	
   	
   	
   	
      rst	
   	
   	
   	
   	
   	
   product	
   	
   
	
   
	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   proper	
   	
   	
   	
   	
   punc.	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   punc.	
   
	
   	
   
	
   	
   	
   would	
   	
   	
   	
   	
   	
   be	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   breakaway	
   	
   	
   	
   	
   	
   hit	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   apple	
   	
   	
   	
   	
   	
   	
   	
   .	
   

12	
   

part-     of-     speech	
   tagging	
   

determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   proper	
   	
   	
   	
   	
   proper	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
	
   	
   	
   
	
   	
   	
   some	
   	
   	
   	
   	
   	
   quesboned	
   	
   	
   	
   	
   	
   if	
   	
   	
   	
   	
   	
   	
   tim	
   	
   	
   	
   	
   	
   cook	
   	
   	
   	
   	
   	
      s	
   	
   	
   	
   	
   	
      rst	
   	
   	
   	
   	
   	
   product	
   	
   
	
   
	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   proper	
   	
   	
   	
   	
   punc.	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   punc.	
   
	
   	
   
	
   	
   	
   would	
   	
   	
   	
   	
   	
   be	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   breakaway	
   	
   	
   	
   	
   	
   hit	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   apple	
   	
   	
   	
   	
   	
   	
   	
   .	
   

named	
   enbty	
   recognibon	
   

some	
   quesboned	
   if	
   tim	
   cook   s	
      rst	
   product	
   would	
   be	
   a	
   breakaway	
   hit	
   for	
   apple.	
   

person	
   

organization	
   

13	
   

syntacbc	
   parsing	
   

14	
   

enbty	
   linking	
   

en.wikipedia.org/wiki/dell
infobox type: company

en.wikipedia.org/wiki/michael_dell
infobox type: person

models. finally, as a structured crf, it is concep-
tually no more complex than its component models
and its behavior can be understood using the same
intuition.

organization

person

revenues of $14.5 billion were posted by dell1. the company1 ...

figure 1: coreference can help resolve ambiguous cases
coreference	
   resolubon	
   
of semantic types or entity links: propagating information
across coreference arcs can inform us that, in this context,
dell is an organization and should therefore link to the
article on dell in wikipedia.

we apply our model to two datasets, ace 2005
and ontonotes, with different mention standards
and layers of annotation. in both settings, our joint
model outperforms our independent baseline mod-
els. on ace, we achieve state-of-the-art entity link-
ing results, matching the performance of the system
of fahrni and strube (2014). on ontonotes, we
match the performance of the best published coref-
erence system (bj  orkelund and kuhn, 2014) and
outperform two strong ner systems (ratinov and
roth, 2009; passos et al., 2014).

shown that tighter integration of coreference and
entity linking is promising (hajishirzi et al., 2013;

15	
   

   gure	
   credit:	
   durrex	
   &	
   klein	
   (2014)	
   

2 motivating examples

   winograd	
   schema   	
   

coreference	
   resolubon	
   

the	
   man	
   couldn't	
   lih	
   his	
   son	
   because	
   he	
   was	
   so	
   weak.	
   
	
   
	
   
	
   
	
   
the	
   man	
   couldn't	
   lih	
   his	
   son	
   because	
   he	
   was	
   so	
   heavy.	
   
	
   
	
   

16	
   

   winograd	
   schema   	
   

coreference	
   resolubon	
   

the	
   man	
   couldn't	
   lih	
   his	
   son	
   because	
   he	
   was	
   so	
   weak.	
   
	
   
	
   
	
   
	
   
the	
   man	
   couldn't	
   lih	
   his	
   son	
   because	
   he	
   was	
   so	
   heavy.	
   
	
   
	
   

man	
   

son	
   

17	
   

reading	
   comprehension	
   

once	
   there	
   was	
   a	
   boy	
   named	
   fritz	
   who	
   loved	
   to	
   draw.	
   he	
   drew	
   
everything.	
   in	
   the	
   morning,	
   he	
   drew	
   a	
   picture	
   of	
   his	
   cereal	
   with	
   
milk.	
   his	
   papa	
   said,	
      don   t	
   draw	
   your	
   cereal.	
   eat	
   it!   	
   	
   
aher	
   school,	
   fritz	
   drew	
   a	
   picture	
   of	
   his	
   bicycle.	
   his	
   uncle	
   said,	
   
   don't	
   draw	
   your	
   bicycle.	
   ride	
   it!   	
   
   	
   

	
   

what	
   did	
   fritz	
   draw	
      rst?	
   
	
   	
   	
   	
   a)	
   the	
   toothpaste	
   
	
   	
   	
   	
   b)	
   his	
   mama	
   
	
   	
   	
   	
   c)	
   cereal	
   and	
   milk	
   
	
   	
   	
   	
   d)	
   his	
   bicycle	
   

18	
   

course	
   overview	
   
       new	
   course,	
      rst	
   bme	
   being	
   o   ered	
   

       prerequisite:	
   ttic	
   31190	
   (nlp)	
   

       aimed	
   at	
   senior	
   graduate	
   students	
   

       my	
   o   ce	
   hours:	
   by	
   appointment,	
   ttic	
   531	
   
       teaching	
   assistant:	
   john	
   wiebng,	
   ttic	
   phd	
   student	
   

19	
   

grading	
   

       3	
   assignments	
   (10%,	
   15%,	
   15%)	
   
       course	
   project	
   (30%)	
   
       class	
   parbcipabon	
   (30%)	
   
       no	
      nal	
   

20	
   

course	
   philosophy	
   

       goal:	
   use	
   our	
   bme	
   well	
   

      maximum	
   amount	
   learned	
   for	
   minimum	
   bme	
   
investment	
   

       our	
   in-     class	
   bme	
   is	
   very	
   important	
   
       your	
   parbcipabon	
   grade	
   	
   	
   	
   	
   	
   number	
   of	
   wrong	
   

answers	
   you	
   give	
   

21	
   

course	
   philosophy	
   

       goal:	
   use	
   our	
   bme	
   well	
   

      maximum	
   amount	
   learned	
   for	
   minimum	
   bme	
   
investment	
   

       our	
   in-     class	
   bme	
   is	
   very	
   important	
   
       some	
   class	
   meebngs	
   will	
   be	
   more	
   interacbve,	
   

involving	
   programming	
   exercises,	
   pen-     and-     
paper	
   exercises,	
   data	
   analysis	
   in	
   small	
   
groupsyour	
   parbcipabon	
   grade	
   	
   	
   	
   	
   	
   number	
   of	
   
wrong	
   answers	
   you	
   give	
   

22	
   

class	
   parbcipabon	
   

       class	
   parbcipabon	
   is	
   worth	
   30%	
   
       your	
   parbcipabon	
   grade	
   	
   	
   	
   	
   	
   number	
   of	
   wrong	
   

answers	
   you	
   give	
   

       if	
   you	
   have	
   good	
   reason	
   to	
   miss	
   class,	
   let	
   me	
   

know!	
   

23	
   

assignments	
   

       mini-     research	
   projects:	
   formal	
   exposibon,	
   
implementabon,	
   experimentabon,	
   analysis,	
   
developing	
   new	
   methods	
   

       assignment	
   1	
   has	
   been	
   posted;	
   due	
   april	
   10	
   
       it   s	
   a	
   (relabvely)	
   short	
   warm-     up	
   assignment	
   
that	
   will	
   help	
   you	
   catch	
   up	
   if	
   you	
   didn   t	
   take	
   
the	
   prerequisite	
   

24	
   

project	
   

       replicate	
   [part	
   of]	
   a	
   published	
   nlp	
   paper,	
   or	
   

de   ne	
   your	
   own	
   project.	
   

       the	
   project	
   may	
   be	
   done	
   individually	
   or	
   in	
   a	
   

group	
   of	
   two.	
   each	
   group	
   member	
   will	
   receive	
   
the	
   same	
   grade.	
   

       more	
   details	
   to	
   come.	
   

25	
   

collaborabon	
   policy	
   

       you	
   are	
   welcome	
   to	
   discuss	
   assignments	
   with	
   
others	
   in	
   the	
   course,	
   but	
   solubons	
   and	
   code	
   
must	
   be	
   wrixen	
   individually	
   

26	
   

textbooks	
   

       all	
   are	
   opbonal	
   
       speech	
   and	
   language	
   processing,	
   2nd	
   ed.	
   	
   

       some	
   chapters	
   of	
   3rd	
   edibon	
   are	
   online	
   

       bayesian	
   analysis	
   in	
   nlp	
   by	
   shay	
   cohen	
   

       will	
   be	
   available	
   in	
   the	
   ttic	
   library	
   

27	
   

roadmap	
   

       review	
   of	
   ttic	
   31190	
   (week	
   1)	
   
       deep	
   learning	
   for	
   nlp	
   (weeks	
   2-     4)	
   
       generabve	
   models	
   &	
   bayesian	
   id136	
   (week	
   5)	
   
       bayesian	
   nonparametrics	
   in	
   nlp	
   (week	
   6)	
   
       em	
   for	
   unsupervised	
   nlp	
   (week	
   7)	
   
       syntax/semanbcs	
   and	
   structure	
   predicbon	
   (weeks	
   8-     9)	
   
       applicabons	
   (week	
   10)	
   

28	
   

the	
   first	
   couple	
   weeks	
   

5	
   

       i	
   will	
   be	
   away	
   wed.	
   march	
   29	
   and	
   wed.	
   april	
   
       sorry	
   about	
   this	
   l   	
   
       wed.	
   march	
   29:	
   

      class	
   will	
   be	
   opbonal	
   
      ta	
   will	
   hold	
   an	
   o   ce	
   hour	
   during	
   class	
   for	
   anyone	
   
who	
   has	
   quesbons	
   about	
   assignment	
   1,	
   deep	
   
learning	
   toolkits,	
   python,	
   ttic	
   31190,	
   etc.	
   

       wed.	
   april	
   5:	
   

      class	
   will	
   be	
   canceled	
   

29	
   

why	
   is	
   nlp	
   hard?	
   

       ambiguity	
   and	
   variability	
   of	
   linguisbc	
   expression:	
   

       variability:	
   many	
   forms	
   can	
   mean	
   the	
   same	
   thing	
   
       ambiguity:	
   one	
   form	
   can	
   mean	
   many	
   things	
   

       there	
   are	
   many	
   di   erent	
   kinds	
   of	
   ambiguity	
   
       each	
   nlp	
   task	
   has	
   to	
   address	
   a	
   disbnct	
   set	
   of	
   kinds	
   

30	
   

what	
   is	
   a	
   classi   er?	
   

       a	
   funcbon	
   from	
   inputs	
   x	
   to	
   classi   cabon	
   labels	
   y	
   

	
   

31	
   

what	
   is	
   a	
   classi   er?	
   

       a	
   funcbon	
   from	
   inputs	
   x	
   to	
   classi   cabon	
   labels	
   y	
   
       one	
   simple	
   type	
   of	
   classi   er:	
   

      for	
   any	
   input	
   x,	
   assign	
   a	
   score	
   to	
   each	
   label	
   y,	
   
parameterized	
   by	
   vector	
   	
   	
   	
   :	
   

	
   

32	
   

what	
   is	
   a	
   classi   er?	
   

       a	
   funcbon	
   from	
   inputs	
   x	
   to	
   classi   cabon	
   labels	
   y	
   
       one	
   simple	
   type	
   of	
   classi   er:	
   

      for	
   any	
   input	
   x,	
   assign	
   a	
   score	
   to	
   each	
   label	
   y,	
   
parameterized	
   by	
   vector	
   	
   	
   	
   :	
   

	
   
      classify	
   by	
   choosing	
   highest-     scoring	
   label:	
   

33	
   

modeling,	
   id136,	
   learning	
   

34	
   

modeling,	
   id136,	
   learning	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

modeling:	
   de   ne	
   	
   score	
   funcbon	
   

learning:	
   choose	
   _	
   	
   	
   

       we	
   will	
   use	
   this	
   same	
   paradigm	
   throughout	
   
the	
   course,	
   even	
   when	
   the	
   output	
   space	
   size	
   
is	
   exponenhal	
   in	
   the	
   size	
   of	
   the	
   input	
   or	
   is	
   
unbounded	
   (e.g.,	
   machine	
   translahon)	
   

35	
   

notabon	
   

       we   ll	
   use	
   boldface	
   for	
   vectors:	
   

      

individual	
   entries	
   will	
   use	
   subscripts	
   and	
   no	
   boldface,	
   e.g.,	
   for	
   
entry	
   i:	
   

36	
   

modeling,	
   id136,	
   learning	
   

modeling:	
   de   ne	
   	
   score	
   funcbon	
   

       modeling:	
   how	
   do	
   we	
   assign	
   a	
   score	
   to	
   an	
   

(x,y)	
   pair	
   using	
   parameters	
   	
   	
   	
   ?	
   

37	
   

modeling,	
   id136,	
   learning	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
    modeling:	
   de   ne	
   	
   score	
   funcbon	
   

       id136:	
   how	
   do	
   we	
   e   ciently	
   search	
   over	
   

the	
   space	
   of	
   all	
   labels?	
   

38	
   

modeling,	
   id136,	
   learning	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

modeling:	
   de   ne	
   	
   score	
   funcbon	
   

       learning:	
   how	
   do	
   we	
   choose	
   	
   	
   	
   ?	
   

learning:	
   choose	
   _	
   	
   	
   

39	
   

applicabons	
   of	
   our	
   classi   cabon	
   framework	
   

text	
   classi   cabon:	
   
	
   
	
   
	
   
	
   	
   

=	
   {objecbve,	
   subjecbve}	
   

x	
   

the	
   hulk	
   is	
   an	
   anger	
   fueled	
   monster	
   with	
   
incredible	
   strength	
   and	
   resistance	
   to	
   damage	
   .	
   
in	
   trying	
   to	
   be	
   daring	
   and	
   original	
   ,	
   it	
   comes	
   o   	
   
as	
   only	
   occasionally	
   sabrical	
   and	
   never	
   fresh	
   .	
   

y	
   

objecbve	
   

subjecbve	
   

40	
   

applicabons	
   of	
   our	
   classi   cabon	
   framework	
   

word	
   sense	
   classi   er	
   for	
   bass:	
   
	
   
	
   
	
   
	
   	
   

=	
   {bass1,	
   bass2,	
      ,	
   bass8}	
   

x	
   

he   s	
   a	
   bass	
   in	
   the	
   choir	
   .	
   

our	
   bass	
   is	
   line-     caught	
   from	
   the	
   
atlanbc	
   .	
   

y	
   

bass3	
   

bass4	
   

41	
   

applicabons	
   of	
   our	
   classi   cabon	
   framework	
   

skip-     gram	
   model	
   as	
   a	
   classi   er:	
   
	
   
	
   
	
   
	
   	
   

x	
   

agriculture	
   

agriculture	
   

agriculture	
   

y	
   

<s>	
   

is	
   

the	
   

=	
   v	
   	
   	
   	
   	
   (the	
   enbre	
   vocabulary)	
   

corpus	
   (english	
   wikipedia):	
   
agriculture	
   is	
   the	
   tradi3onal	
   mainstay	
   of	
   the	
   
cambodian	
   economy	
   .	
   
but	
   benares	
   has	
   been	
   destroyed	
   by	
   an	
   earthquake	
   .	
   
   	
   

42	
   

simplest	
   kind	
   of	
   structured	
   predicbon:	
   sequence	
   labeling	
   

part-     of-     speech	
   tagging	
   

determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   proper	
   	
   	
   	
   	
   proper	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
	
   	
   	
   
	
   	
   	
   some	
   	
   	
   	
   	
   	
   quesboned	
   	
   	
   	
   	
   	
   if	
   	
   	
   	
   	
   	
   	
   tim	
   	
   	
   	
   	
   	
   cook	
   	
   	
   	
   	
   	
      s	
   	
   	
   	
   	
   	
      rst	
   	
   	
   	
   	
   	
   product	
   	
   
	
   
	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   proper	
   	
   	
   	
   	
   punc.	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjecbve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   punc.	
   
	
   	
   
	
   	
   	
   would	
   	
   	
   	
   	
   	
   be	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   breakaway	
   	
   	
   	
   	
   	
   hit	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   apple	
   	
   	
   	
   	
   	
   	
   	
   .	
   

43	
   

formulabng	
   segmentabon	
   tasks	
   as	
   sequence	
   labeling	
   

via	
   b-     i-     o	
   labeling:	
   

named	
   enbty	
   recognibon	
   

	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   b-     person	
   	
   	
   i-     person	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   
	
   	
   	
   some	
   	
   	
   quesboned	
   	
   	
   if	
   	
   	
   	
   	
   	
   	
   	
   	
   tim	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   cook	
   	
   	
   	
   	
   	
   	
      s	
   	
   	
   	
   	
   	
      rst	
   	
   	
   	
   	
   	
   product	
   	
   
	
   
	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   	
   	
   	
   o	
   	
   	
   	
   	
   b-     organization	
   	
   	
   	
   	
   	
   o	
   
	
   	
   	
   would	
   	
   	
   	
   	
   	
   be	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   breakaway	
   	
   	
   	
   hit	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   apple	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   

b	
   =	
      begin   	
   
	
   i	
   =	
      inside   	
   
o	
   =	
      outside   	
   
	
   

44	
   

consbtuent	
   parsing	
   

(s	
   (np	
   the	
   man)	
   (vp	
   walked	
   (pp	
   to	
   (np	
   the	
   park))))	
   

s	
   

np	
   

vp	
   

pp	
   

np	
   

dt	
    nn	
   
vbd	
   	
   	
   	
   	
   	
   in	
   	
   	
   	
   dt	
   	
   	
   	
   nn	
   
the	
   man	
   walked	
   to	
   the	
   park	
   

key:	
   
s	
   =	
   sentence	
   
np	
   =	
   noun	
   phrase	
   
vp	
   =	
   verb	
   phrase	
   
pp	
   =	
   preposibonal	
   phrase	
   
dt	
   =	
   determiner	
   
nn	
   =	
   noun	
   
vbd	
   =	
   verb	
   (past	
   tense)	
   
in	
   =	
   preposibon	
   
	
   

45	
   

dependency	
   parsing	
   

source:          $  konnten  sie  es    bersetzen  ?

   wall   	
   symbol	
   

reference:     $  could  you  translate  it  ?

46	
   

several	
   kinds	
   of	
   semanbc	
   parsing	
   

       semanbc	
   role	
   labeling	
   (srl)	
   
       frame-     semanbc	
   parsing	
   
          semanbc	
   parsing   	
   (   rst-     order	
   logic)	
   
       abstract	
   meaning	
   representabon	
   (amr)	
   
       dependency-     based	
   composibonal	
   semanbcs	
   

47	
   

applications 

semanbc	
   role	
   labeling	
   

(cid:96) question & answer systems 

   who      did what to whom      at where? 

 

the police officer detained the suspect at the scene of the crime 

agent	
   
arg0 

predicate	
   

v 

theme	
   
arg2 

locabon	
   
am-loc 

j&m/slp3	
   

semanbc	
   role	
   labeling	
   (srl)	
   	
   

    id14

22.6

       the	
   task	
   of	
      nding	
   the	
   semanbc	
   roles	
   of	
   each	
   

recall that the difference between these two models of semantic roles is that
framenet (22.27) employs many frame-speci   c frame elements as roles, while prop-
bank (22.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speci   c labels, along with the more general argm labels. some
examples:

argument	
   of	
   each	
   predicate	
   in	
   a	
   sentence.	
   

       framenet	
   versus	
   propbank:	
   

(22.27)

(22.28)

[the program]

can   t [blame]

[you]
cognizer
[the san francisco examiner]
arg0

target evaluee
issued
target arg1

[for being unable to identify it]
reason

[a special edition]

[yesterday]
argm-tmp

a simpli   ed id14 algorithm is sketched in fig. 22.4. while
there are a large number of algorithms, many of them use some version of the steps
in this algorithm.

most algorithms, beginning with the very earliest semantic role analyzers (sim-
mons, 1973), begin by parsing, using broad-coverage parsers to assign a parse to the
input string. figure 22.5 shows a parse of (22.28) above. the parse is then traversed

j&m/slp3	
   

49	
   

machine	
   translabon	
   

applicabons	
   of	
   our	
   classi   er	
   framework	
   

task	
   

input	
   (x)	
   

output	
   (y)	
   

text	
   

classi   cabon	
   

a	
   sentence	
   

gold	
   standard	
   

label	
   for	
   x	
   

word	
   sense	
   

disambiguabon	
   

learning	
   skip-     
gram	
   word	
   
embeddings	
   

part-     of-     speech	
   

tagging	
   

instance	
   of	
   a	
   
parbcular	
   word	
   
(e.g.,	
   bass)	
   with	
   

its	
   context	
   	
   

instance	
   of	
   a	
   

word	
   in	
   a	
   corpus	
   

a	
   sentence	
   

gold	
   standard	
   
word	
   sense	
   of	
   x	
   

a	
   word	
   in	
   the	
   
context	
   of	
   x	
   in	
   

a	
   corpus	
   

gold	
   standard	
   
part-     of-     speech	
   

tags	
   for	
   x	
   

output	
   space	
   (	
   	
   	
   	
   	
   )	
   
pre-     de   ned,	
   small	
   

label	
   set	
   (e.g.,	
   

{posibve,	
   negabve})	
   

pre-     de   ned	
   sense	
   
inventory	
   from	
   
id138	
   for	
   bass	
   

size	
   of	
   

2-     10	
   

2-     30	
   

vocabulary	
   

|v|	
   

all	
   possible	
   part-     of-     
speech	
   tag	
   sequences	
   
with	
   same	
   length	
   as	
   x	
   

|p||x|	
   

51	
   

applicabons	
   of	
   classi   er	
   framework	
   (conbnued)	
   

task	
   

input	
   (x)	
   

output	
   (y)	
   

output	
   space	
   (	
   	
   	
   	
   	
   )	
   

size	
   of	
   

named	
   
enbty	
   

recognibon	
   

a	
   sentence	
   

consbtuent	
   

parsing	
   

a	
   sentence	
   

gold	
   standard	
   named	
   

enbty	
   labels	
   for	
   x	
   

(bio	
   tags)	
   

all	
   possible	
   bio	
   label	
   
sequences	
   with	
   same	
   

length	
   as	
   x	
   

|p||x|	
   

gold	
   standard	
   

consbtuent	
   parse	
   
(labeled	
   brackebng)	
   

of	
   x	
   

all	
   possible	
   labeled	
   

brackebngs	
   of	
   x	
   

dependency	
   

parsing	
   

a	
   sentence	
   

gold	
   standard	
   

dependency	
   parse	
   
(labeled	
   directed	
   
spanning	
   tree)	
   of	
   x	
   

all	
   possible	
   labeled	
   

directed	
   spanning	
   trees	
   

of	
   x	
   

exponenbal	
   
in	
   length	
   of	
   x	
   

(catalan	
   
number)	
   

exponenbal	
   
in	
   length	
   of	
   x	
   

machine	
   
translabon	
   

a	
   sentence	
   

a	
   translabon	
   of	
   x	
   

all	
   possible	
   translabons	
   

of	
   x	
   

potenbally	
   

in   nite	
   

52	
   

modeling	
   

53	
   

model	
   families	
   

       linear	
   models	
   

       lots	
   of	
   freedom	
   in	
   de   ning	
   features,	
   though	
   feature	
   
engineering	
   required	
   for	
   best	
   performance	
   
       learning	
   uses	
   opbmizabon	
   of	
   a	
   loss	
   funcbon	
   
       one	
   can	
   (try	
   to)	
   interpret	
   learned	
   feature	
   weights	
   

       stochasbc/generabve	
   models	
   

       linear	
   models	
   with	
   simple	
      features   	
   (counts	
   of	
   events)	
   
       learning	
   is	
   easy:	
   count	
   &	
   normalize	
   (but	
   smoothing	
   needed)	
   
       easy	
   to	
   generate	
   samples	
   

       neural	
   networks	
   

       can	
   usually	
   get	
   away	
   with	
   less	
   feature	
   engineering	
   
       learning	
   uses	
   opbmizabon	
   of	
   a	
   loss	
   funcbon	
   
       hard	
   to	
   interpret	
   (though	
   we	
   try!),	
   but	
   ohen	
   works	
   best	
   

54	
   

special	
   case	
   of	
   linear	
   models:	
   	
   
stochasbc/generabve	
   models	
   

model	
   

tasks	
   

context	
   expansion	
   

n-     gram	
   language	
   models	
   

hidden	
   markov	
   models	
   

language	
   modeling	
   (for	
   

mt,	
   asr,	
   etc.)	
   

part-     of-     speech	
   tagging,	
   
named	
   enbty	
   recognibon,	
   

word	
   id91	
   

increase	
   n	
   

increase	
   order	
   of	
   id48	
   (e.g.,	
   bigram	
   

id48	
        	
   trigram	
   id48)	
   

probabilisbc	
   context-     free	
   

grammars	
   

consbtuent	
   parsing	
   

increase	
   size	
   of	
   rules,	
   e.g.,	
      axening,	
   

parent	
   annotabon,	
   etc.	
   

       all	
   use	
   id113	
   +	
   smoothing	
   (though	
   probably	
   di   erent	
   kinds	
   of	
   smoothing)	
   
       all	
   assign	
   id203	
   to	
   sentences	
   (some	
   assign	
   id203	
   jointly	
   to	
   pairs	
   
       all	
   have	
   the	
   same	
   trade-     o   	
   of	
   increasing	
      context   	
   (feature	
   size)	
   and	
   

of	
   <sentence,	
   something	
   else>)	
   

needing	
   more	
   data	
   /	
   bexer	
   smoothing	
   

55	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

       two	
   features:	
   

	
   	
   	
   	
   	
   where	
   
	
   
       what	
   should	
   the	
   weights	
   be?	
   

56	
   

higher-     order	
   binary	
   feature	
   templates	
   

unigram	
   binary	
   template:	
   
	
   
bigram	
   binary	
   template:	
   
	
   
trigram	
   binary	
   features	
   
	
   	
   	
   	
   	
   	
      	
   
	
   

57	
   

2-     transformabon	
   (1-     layer)	
   network	
   

vector	
   of	
   label	
   scores	
   

       we   ll	
   call	
   this	
   a	
      2-     transformabon   	
   neural	
   

network,	
   or	
   a	
      1-     layer   	
   neural	
   network	
   

       input	
   vector	
   is	
   	
   
       score	
   vector	
   is	
   
       one	
   hidden	
   vector	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   (   hidden	
   layer   )	
   

58	
   

1-     layer	
   neural	
   network	
   for	
   senbment	
   classi   cabon	
   

59	
   

neural	
   networks	
   for	
   twixer	
   part-     of-     speech	
   tagging	
   

other	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   det	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   pronoun	
   	
   

intj	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   pronoun	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   prep	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   adj	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   prep	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   verb	
   	
   
ikr	
   	
   smh	
   	
   he	
   	
   asked	
   	
      r	
   	
   yo	
   	
   last	
   	
   name	
   	
   so	
   	
   he	
   	
   can	
   
	
   
      
	
   

let   s	
   use	
   the	
   center	
   word	
   +	
   two	
   words	
   to	
   the	
   right:	
   

vector	
   for	
   yo	
   

vector	
   for	
   last	
   

vector	
   for	
   name	
   

      
if	
   name	
   is	
   to	
   the	
   right	
   of	
   yo,	
   then	
   yo	
   is	
   probably	
   a	
   form	
   of	
   your	
   
       but	
   our	
   x	
   above	
   uses	
   separate	
   dimensions	
   for	
   each	
   posibon!	
   

       i.e.,	
   name	
   is	
   two	
   words	
   to	
   the	
   right	
   
       what	
   if	
   name	
   is	
   one	
   word	
   to	
   the	
   right?	
   	
   	
   

60	
   

convolubon	
   

=	
      feature	
   map   ,	
   has	
   an	
   entry	
   for	
   each	
   word	
   posibon	
   in	
   context	
   window	
   /	
   sentence	
   

vector	
   for	
   yo	
   

vector	
   for	
   last	
   

vector	
   for	
   name	
   

61	
   

pooling	
   

=	
      feature	
   map   ,	
   has	
   an	
   entry	
   for	
   each	
   word	
   posibon	
   in	
   context	
   window	
   /	
   sentence	
   

how	
   do	
   we	
   convert	
   this	
   into	
   a	
      xed-     length	
   vector?	
   
use	
   pooling:	
   

	
   max-     pooling:	
   returns	
   maximum	
   value	
   in	
   	
   
	
   average	
   pooling:	
   returns	
   average	
   of	
   values	
   in	
   	
   

vector	
   for	
   yo	
   

vector	
   for	
   last	
   

vector	
   for	
   name	
   

62	
   

pooling	
   

=	
      feature	
   map   ,	
   has	
   an	
   entry	
   for	
   each	
   word	
   posibon	
   in	
   context	
   window	
   /	
   sentence	
   

how	
   do	
   we	
   convert	
   this	
   into	
   a	
      xed-     length	
   vector?	
   
use	
   pooling:	
   

	
   max-     pooling:	
   returns	
   maximum	
   value	
   in	
   	
   
	
   average	
   pooling:	
   returns	
   average	
   of	
   values	
   in	
   	
   

vector	
   for	
   yo	
   

vector	
   for	
   last	
   

vector	
   for	
   name	
   

then,	
   this	
   single	
      lter	
   	
   	
   	
   	
   	
   	
   produces	
   a	
   single	
   feature	
   	
   
value	
   (the	
   output	
   of	
   some	
   kind	
   of	
   pooling).	
   
in	
   pracbce,	
   we	
   use	
   many	
      lters	
   of	
   many	
   di   erent	
   	
   
lengths	
   (e.g.,	
   n-     grams	
   rather	
   than	
   words).	
   	
   

63	
   

convolubonal	
   neural	
   networks	
   

       convolubonal	
   neural	
   networks	
   (convnets	
   or	
   id98s)	
   use	
   
   lters	
   that	
   are	
      convolved	
   with   	
   (matched	
   against	
   all	
   
posibons	
   of)	
   the	
   input	
   
       think	
   of	
   convolubon	
   as	
      perform	
   the	
   same	
   operabon	
   
everywhere	
   on	
   the	
   input	
   in	
   some	
   systemabc	
   order   	
   
          convolubonal	
   layer   	
   =	
   set	
   of	
      lters	
   that	
   are	
   convolved	
   
       could	
   be	
   followed	
   by	
   more	
   convolubonal	
   layers,	
   or	
   by	
   
       ohen	
   used	
   in	
   nlp	
   to	
   convert	
   a	
   sentence	
   into	
   a	
   feature	
   

with	
   the	
   input	
   vector	
   (whether	
   x	
   or	
   hidden	
   vector)	
   

a	
   type	
   of	
   pooling	
   

vector	
   

64	
   

recurrent	
   neural	
   networks	
   

   hidden	
   vector   	
   

65	
   

long	
   short-     term	
   memory	
   (lstm)	
   recurrent	
   neural	
   networks	
   

66	
   

backward	
   &	
   bidirecbonal	
   lstms	
   

bidirecbonal:	
   	
   

if	
   shallow,	
   just	
   use	
   forward	
   and	
   backward	
   lstms	
   in	
   parallel,	
   concatenate	
   	
   
   nal	
   two	
   hidden	
   vectors,	
   feed	
   to	
   sohmax	
   

67	
   

deep	
   lstm	
   
(2-     layer)	
   

layer	
   1	
   

layer	
   2	
   

68	
   

recursive	
   neural	
   networks	
   for	
   nlp	
   
          rst,	
   run	
   a	
   consbtuent	
   parser	
   on	
   the	
   sentence	
   
       convert	
   the	
   consbtuent	
   tree	
   to	
   a	
   binary	
   tree	
   
       construct	
   vector	
   for	
   sentence	
   recursively	
   at	
   each	
   

(each	
   rewrite	
   has	
   exactly	
   two	
   children)	
   

rewrite	
   (   split	
   point   ):	
   	
   

69	
   

learning	
   

70	
   

cost	
   funcbons	
   

       cost	
   funcbon:	
   scores	
   output	
   against	
   a	
   gold	
   standard	
   

       should	
   re   ect	
   the	
   evaluabon	
   metric	
   for	
   your	
   task	
   

       usual	
   convenbons:	
   
      
      

for	
   classi   cabon,	
   what	
   cost	
   should	
   we	
   use?	
   
for	
   classi   cabon,	
   what	
   cost	
   should	
   we	
   use?	
   

71	
   

empirical	
   risk	
   minimizabon	
   

(vapnik	
   et	
   al.)	
   

       replace	
   expectabon	
   with	
   sum	
   over	
   examples:	
   

72	
   

empirical	
   risk	
   minimizabon	
   

(vapnik	
   et	
   al.)	
   

       replace	
   expectabon	
   with	
   sum	
   over	
   examples:	
   

problem:	
   np-     hard	
   even	
   for	
   binary	
   
classi   cabon	
   with	
   linear	
   models	
   

73	
   

empirical	
   risk	
   minimizabon	
   with	
   surrogate	
   loss	
   funcbons	
   

       given	
   training	
   data:	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
	
   	
   	
   	
   where	
   each	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   a	
   label	
   
       we	
   want	
   to	
   solve	
   the	
   following:	
   

many	
   possible	
   loss	
   
funcbons	
   to	
   consider	
   

opbmizing	
   

74	
   

name	
   

cost	
   (   0-     1   )	
   

id88	
   

hinge	
   

log	
   

loss	
   funcbons	
   

loss	
   

where	
   used	
   

	
   intractable,	
   but	
   

underlies	
      direct	
   error	
   

minimizabon   	
   

id88	
   algorithm	
   
(rosenblax,	
   1958)	
   

support	
   vector	
   

machines,	
   other	
   large-     

margin	
   algorithms	
   
logisbc	
   regression,	
   
condibonal	
   random	
   
   elds,	
   maximum	
   
id178	
   models	
   

75	
   

(sub)gradients	
   of	
   losses	
   for	
   linear	
   models	
   

entry	
   j	
   of	
   (sub)gradient	
   of	
   loss	
   for	
   linear	
   model	
   

not	
   subdi   erenbable	
   in	
   general	
   

name	
   

cost	
   (   0-     1   )	
   

id88	
   

hinge	
   

log	
   

76	
   

(sub)gradients	
   of	
   losses	
   for	
   linear	
   models	
   

entry	
   j	
   of	
   (sub)gradient	
   of	
   loss	
   for	
   linear	
   model	
   

not	
   subdi   erenbable	
   in	
   general	
   

name	
   

cost	
   (   0-     1   )	
   

id88	
   

hinge	
   

log	
   

expectabon	
   of	
   feature	
   value	
   with	
   respect	
   to	
   distribubon	
   
over	
   y	
   (where	
   distribubon	
   is	
   de   ned	
   by	
   theta)	
   
	
   
alternabve	
   notabon:	
   

77	
   

visualizabon	
   

	
   

e
r
o
c
s

   ve	
   possible	
   outputs	
   

78	
   

visualizabon	
   

	
   
t
s
o
c

   ve	
   possible	
   outputs	
   

79	
   

visualizabon	
   

	
   
t
s
o
c

gold	
   standard	
   

80	
   

visualizabon	
   

	
   
t
s
o
c

gold	
   standard	
   

81	
   

visualizabon	
   

	
   
t
s
o
c
	
   
+
e
r
o
c
s

	
   

gold	
   standard	
   

82	
   

id88	
   loss:	
   

83	
   

id88	
   loss:	
   

	
   

e
r
o
c
s

gold	
   standard	
   

84	
   

id88	
   loss:	
   

	
   

e
r
o
c
s

gold	
   standard	
   

85	
   

id88	
   loss:	
   

	
   

e
r
o
c
s

e   ect	
   of	
   learning?	
   

gold	
   standard	
   

86	
   

id88	
   loss:	
   

	
   

e
r
o
c
s

e   ect	
   of	
   learning:	
   
gold	
   standard	
   will	
   
have	
   highest	
   score	
   

gold	
   standard	
   

87	
   

hinge	
   loss:	
   

88	
   

hinge	
   loss:	
   

	
   
t
s
o
c
	
   
+
e
r
o
c
s

	
   

gold	
   standard	
   

89	
   

hinge	
   loss:	
   

	
   
t
s
o
c
	
   
+
e
r
o
c
s

	
   

gold	
   standard	
   

90	
   

hinge	
   loss:	
   

	
   
t
s
o
c
	
   
+
e
r
o
c
s

	
   

e   ect	
   of	
   learning?	
   

gold	
   standard	
   

91	
   

hinge	
   loss:	
   

	
   
t
s
o
c
	
   
+
e
r
o
c
s

	
   

e   ect	
   of	
   learning:	
   

score	
   of	
   gold	
   standard	
   

will	
   be	
   higher	
   than	
   
score+cost	
   of	
   all	
   

others	
   

gold	
   standard	
   

92	
   

regularized	
   empirical	
   risk	
   minimizabon	
   

       given	
   training	
   data:	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
	
   	
   	
   	
   where	
   each	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   a	
   label	
   
       we	
   want	
   to	
   solve	
   the	
   following:	
   

regularizahon	
   

strength	
   

regularizahon	
   

term	
   

93	
   

regularizabon	
   terms	
   

       most	
   common:	
   penalize	
   large	
   parameter	
   values	
   
       intuibon:	
   large	
   parameters	
   might	
   be	
   instances	
   of	
   
over   (cid:128)ng	
   
       examples:	
   

l2	
   regularizahon:	
   
(also	
   called	
   tikhonov	
   regularizabon	
   	
   
or	
   ridge	
   regression)	
   
l1	
   regularizahon:	
   
(also	
   called	
   basis	
   pursuit	
   or	
   lasso)	
   

94	
   

dropout	
   

       popular	
   regularizabon	
   method	
   for	
   neural	
   

networks	
   

       randomly	
      drop	
   out   	
   (set	
   to	
   zero)	
   some	
   of	
   the	
   

vector	
   entries	
   in	
   the	
   layers	
   

95	
   

id136	
   

96	
   

exponenbally-     large	
   search	
   problems	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

       when	
   output	
   is	
   a	
   sequence	
   or	
   tree,	
   this	
   

argmax	
   requires	
   iterabng	
   over	
   an	
   
exponenbally-     large	
   set	
   

97	
   

learning	
   requires	
   solving	
   exponenbally-     hard	
   

problems	
   too!	
   

entry	
   j	
   of	
   (sub)gradient	
   of	
   loss	
   for	
   linear	
   model	
   

loss	
   

id88	
   

hinge	
   

log	
   

compubng	
   each	
   of	
   these	
   terms	
   
requires	
   iterabng	
   through	
   every	
   

possible	
   output	
   

98	
   

dynamic	
   programming	
   (dp)	
   

       what	
   is	
   dynamic	
   programming?	
   

       a	
   family	
   of	
   algorithms	
   that	
   break	
   problems	
   into	
   smaller	
   

pieces	
   and	
   reuse	
   solubons	
   for	
   those	
   pieces	
   

       only	
   applicable	
   when	
   the	
   problem	
   has	
   certain	
   properbes	
   

(ophmal	
   substructure	
   and	
   overlapping	
   sub-     problems)	
   

      

in	
   this	
   class,	
   we	
   use	
   dp	
   to	
   iterate	
   over	
   exponenbally-     
large	
   output	
   spaces	
   in	
   polynomial	
   bme	
   

       we	
   focus	
   on	
   a	
   parbcular	
   type	
   of	
   dp	
   algorithm:	
   

memoizahon	
   

99	
   

implemenbng	
   dp	
   algorithms	
   
       even	
   if	
   your	
   goal	
   is	
   to	
   compute	
   a	
   sum	
   or	
   a	
   

max,	
   focus	
      rst	
   on	
   counhng	
   mode	
   (count	
   the	
   
number	
   of	
   unique	
   outputs	
   for	
   an	
   input)	
   

       memoizabon	
   =	
   recursion	
   +	
   saving/reusing	
   

solubons	
   
      start	
   by	
   de   ning	
   recursive	
   equabons	
   
         memoize   	
   by	
   creabng	
   a	
   table	
   to	
   store	
   all	
   
intermediate	
   results	
   from	
   recursive	
   equabons,	
   use	
   
them	
   when	
   requested	
   

100	
   

id136	
   in	
   id48s	
   

       since	
   the	
   output	
   is	
   a	
   sequence,	
   this	
   argmax	
   

requires	
   iterabng	
   over	
   an	
   exponenbally-     large	
   set	
   

       last	
   week	
   we	
   talked	
   about	
   using	
   dynamic	
   
programming	
   (dp)	
   to	
   solve	
   these	
   problems	
   

       for	
   id48s	
   (and	
   other	
   sequence	
   models),	
   the	
   for	
   

solving	
   this	
   is	
   called	
   the	
   viterbi	
   algorithm	
   

101	
   

viterbi	
   algorithm	
   

       recursive	
   equabons	
   +	
   memoizabon:	
   

base	
   case:	
   	
   
returns	
   id203	
   of	
   sequence	
   starbng	
   with	
   label	
   y	
   for	
      rst	
   word	
   

recursive	
   case:	
   
computes	
   id203	
   of	
   max-     id203	
   label	
   
sequence	
   that	
   ends	
   with	
   label	
   y	
   at	
   posibon	
   m	
   

   nal	
   value	
   is	
   in:	
   

102	
   

viterbi	
   algorithm	
   

       space	
   and	
   bme	
   complexity?	
   
       can	
   be	
   read	
   o   	
   from	
   the	
   recursive	
   equabons:	
   

space	
   complexity:	
   
size	
   of	
   memoizabon	
   table,	
   which	
   is	
   #	
   of	
   unique	
   indices	
   of	
   recursive	
   equabons	
   

length	
   of	
   
sentence	
   

*

number	
   
of	
   labels	
   

so,	
   space	
   complexity	
   is	
   o(|x|	
   |l|)	
   

103	
   

viterbi	
   algorithm	
   

       space	
   and	
   hme	
   complexity?	
   
       can	
   be	
   read	
   o   	
   from	
   the	
   recursive	
   equabons:	
   

hme	
   complexity:	
   
size	
   of	
   memoizabon	
   table	
   *	
   complexity	
   of	
   compubng	
   each	
   entry	
   

length	
   of	
   
sentence	
   

*

number	
   
of	
   labels	
   

each	
   entry	
   requires	
   

iterahng	
   through	
   the	
   labels	
   

*

so,	
   hme	
   complexity	
   is	
   o(|x|	
   |l|	
   |l|)	
   =	
   o(|x|	
   |l|2)	
   	
   

104	
   

feature	
   locality	
   

       feature	
   locality:	
   how	
      big   	
   are	
   your	
   features?	
   
       when	
   designing	
   e   cient	
   id136	
   algorithms	
   
(whether	
   w/	
   dp	
   or	
   other	
   methods),	
   we	
   need	
   
to	
   be	
   mindful	
   of	
   this	
   

       features	
   can	
   be	
   arbitrarily	
   big	
   in	
   terms	
   of	
   the	
   

input,	
   but	
   not	
   in	
   terms	
   of	
   the	
   output!	
   

       the	
   features	
   in	
   id48s	
   are	
   small	
   in	
   both	
   the	
   
input	
   and	
   output	
   sequences	
   (only	
   two	
   pieces	
   
at	
   a	
   bme)	
   

105	
   

de   ning	
   features	
   

       this	
   is	
   a	
   large	
   part	
   of	
   nlp	
   
       last	
   20	
   years:	
   feature	
   engineering	
   
       last	
   2	
   years:	
   representahon	
   learning	
   	
   

       in	
   this	
   course,	
   we	
   will	
   do	
   both	
   
       learning	
   representabons	
   doesn   t	
   mean	
   that	
   we	
   

don   t	
   have	
   to	
   look	
   at	
   the	
   data	
   or	
   the	
   output!	
   
       there   s	
   sbll	
   plenty	
   of	
   engineering	
   required	
   in	
   

representabon	
   learning	
   

106	
   

de   ning	
   features	
   

       this	
   is	
   a	
   large	
   part	
   of	
   nlp	
   
       last	
   20	
   years:	
   feature	
   engineering	
   
       last	
   2	
   years:	
   representahon	
   learning	
   	
   

       in	
   this	
   course,	
   we   ll	
   do	
   both	
   
       learning	
   representabons	
   doesn   t	
   mean	
   that	
   we	
   

don   t	
   have	
   to	
   look	
   at	
   the	
   data	
   or	
   the	
   output!	
   
       there   s	
   sbll	
   plenty	
   of	
   engineering	
   required	
   in	
   

representabon	
   learning	
   

107	
   

feature	
   engineering	
   

       ohen	
   decried	
   as	
      costly,	
   hand-     crahed,	
   

expensive,	
   domain-     speci   c   ,	
   etc.	
   

       but	
   in	
   pracbce,	
   simple	
   features	
   typically	
   give	
   

the	
   bulk	
   of	
   the	
   performance	
   

       let   s	
   get	
   concrete:	
   how	
   should	
   we	
   de   ne	
   

features	
   for	
   text	
   classi   cabon?	
   

108	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

109	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

	
   	
   	
   	
   	
   is	
   now	
   a	
   vector	
   because	
   
it	
   is	
   a	
   sequence	
   of	
   words	
   

110	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

	
   	
   	
   	
   	
   is	
   now	
   a	
   vector	
   because	
   
it	
   is	
   a	
   sequence	
   of	
   words	
   

let   s	
   consider	
   senbment	
   analysis:	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
	
   	
   

111	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

	
   	
   	
   	
   	
   is	
   now	
   a	
   vector	
   because	
   
it	
   is	
   a	
   sequence	
   of	
   words	
   

let   s	
   consider	
   senbment	
   analysis:	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
	
   	
   

so,	
   here	
   is	
   our	
   senbment	
   classi   er	
   that	
   uses	
   a	
   linear	
   model:	
   
	
   
	
   	
   

112	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

       two	
   features:	
   

	
   	
   	
   	
   	
   where	
   
	
   
       what	
   should	
   the	
   weights	
   be?	
   

113	
   

feature	
   engineering	
   for	
   text	
   classi   cabon	
   

       two	
   features:	
   

	
   	
   	
   	
   	
   where	
   
	
   
       what	
   should	
   the	
   weights	
   be?	
   

114	
   

id136	
   for	
   text	
   classi   cabon	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

       trivial	
   (loop	
   over	
   labels)	
   

115	
   

text	
   classi   cabon	
   

116	
   

learning	
   for	
   text	
   classi   cabon	
   

learning:	
   choose	
   _	
   	
   	
   

       there	
   are	
   many	
   ways	
   to	
   choose	
   

117	
   

experimental	
   pracbce	
   

       in	
   the	
   beginning,	
   we	
   just	
   had	
   data	
   
          rst	
   innovabon:	
   split	
   into	
   train	
   and	
   test	
   
      mobvabon:	
   simulate	
   condibons	
   of	
   applying	
   
system	
   in	
   pracbce	
   

       but,	
   there   s	
   a	
   problem	
   with	
   this   	
   

      we	
   need	
   to	
   explore	
   and	
   evaluate	
   methodological	
   
choices	
   
      aher	
   mulbple	
   evaluabons	
   on	
   test,	
   it	
   is	
   no	
   longer	
   a	
   
simulabon	
   of	
   real-     world	
   condibons	
   

118	
   

experimental	
   pracbce	
   

       in	
   the	
   beginning,	
   we	
   just	
   had	
   data	
   
          rst	
   innovabon:	
   split	
   into	
   train	
   and	
   test	
   
      mobvabon:	
   simulate	
   condibons	
   of	
   applying	
   
system	
   in	
   pracbce	
   

       but,	
   there   s	
   a	
   problem	
   with	
   this   	
   

      we	
   need	
   to	
   explore	
   and	
   evaluate	
   methodological	
   
choices	
   
      aher	
   mulbple	
   evaluabons	
   on	
   test,	
   it	
   is	
   no	
   longer	
   a	
   
simulabon	
   of	
   real-     world	
   condibons	
   

119	
   

experimental	
   pracbce	
   

       in	
   the	
   beginning,	
   we	
   just	
   had	
   data	
   
          rst	
   innovabon:	
   split	
   into	
   train	
   and	
   test	
   
      mobvabon:	
   simulate	
   condibons	
   of	
   applying	
   
system	
   in	
   pracbce	
   

       but,	
   there   s	
   a	
   problem	
   with	
   this   	
   

      we	
   need	
   to	
   explore	
   and	
   evaluate	
   methodological	
   
choices	
   
      aher	
   mulbple	
   evaluabons	
   on	
   test,	
   it	
   is	
   no	
   longer	
   a	
   
simulabon	
   of	
   real-     world	
   condibons	
   

120	
   

experimental	
   pracbce	
   

       in	
   the	
   beginning,	
   we	
   just	
   had	
   data	
   
          rst	
   innovabon:	
   split	
   into	
   train	
   and	
   test	
   
      mobvabon:	
   simulate	
   condibons	
   of	
   applying	
   
system	
   in	
   pracbce	
   

       but,	
   there   s	
   a	
   problem	
   with	
   this   	
   

      we	
   need	
   to	
   explore	
   and	
   evaluate	
   methodological	
   
choices	
   
      aher	
   mulbple	
   evaluabons	
   on	
   test,	
   it	
   is	
   no	
   longer	
   a	
   
simulabon	
   of	
   real-     world	
   condibons	
   

121	
   

experimental	
   pracbce	
   

       we	
   need	
   to	
   explore/evaluate	
   methodological	
   choices	
   
       what	
   should	
   we	
   do?	
   

       some	
   use	
   cross	
   validabon	
   on	
   train,	
   but	
   this	
   is	
   slow	
   and	
   

doesn   t	
   quite	
   simulate	
   real-     world	
   se(cid:128)ngs	
   (why?)	
   

       second	
   innovabon:	
   divide	
   data	
   into	
   train,	
   test,	
   and	
   a	
   

third	
   set	
   called	
   development	
   or	
   validabon	
   
       use	
   development/validabon	
   to	
   evaluate	
   choices	
   
       then,	
   when	
   ready	
   to	
   write	
   the	
   paper,	
   evaluate	
   the	
   best	
   

model	
   on	
   test	
   

       are	
   we	
   done	
   yet?	
   	
   no!	
   	
   there   s	
   sbll	
   a	
   problem	
   

122	
   

experimental	
   pracbce	
   

       we	
   need	
   to	
   explore/evaluate	
   methodological	
   choices	
   
       what	
   should	
   we	
   do?	
   

       some	
   use	
   cross	
   validabon	
   on	
   train,	
   but	
   this	
   is	
   slow	
   and	
   

doesn   t	
   quite	
   simulate	
   real-     world	
   se(cid:128)ngs	
   (why?)	
   

       second	
   innovabon:	
   divide	
   data	
   into	
   train,	
   test,	
   and	
   a	
   
third	
   set	
   called	
   development	
   (dev)	
   or	
   validabon	
   (val)	
   
       use	
   dev/val	
   to	
   evaluate	
   choices	
   
       then,	
   when	
   ready	
   to	
   write	
   the	
   paper,	
   evaluate	
   the	
   best	
   

model	
   on	
   test	
   

       are	
   we	
   done	
   yet?	
   	
   no!	
   	
   there   s	
   sbll	
   a	
   problem:	
   

       over   (cid:128)ng	
   to	
   dev/val	
   

123	
   

experimental	
   pracbce	
   

       we	
   need	
   to	
   explore/evaluate	
   methodological	
   choices	
   
       what	
   should	
   we	
   do?	
   

       some	
   use	
   cross	
   validabon	
   on	
   train,	
   but	
   this	
   is	
   slow	
   and	
   

doesn   t	
   quite	
   simulate	
   real-     world	
   se(cid:128)ngs	
   (why?)	
   

       second	
   innovabon:	
   divide	
   data	
   into	
   train,	
   test,	
   and	
   a	
   
third	
   set	
   called	
   development	
   (dev)	
   or	
   validabon	
   (val)	
   
       use	
   dev/val	
   to	
   evaluate	
   choices	
   
       then,	
   when	
   ready	
   to	
   write	
   the	
   paper,	
   evaluate	
   the	
   best	
   

model	
   on	
   test	
   

       are	
   we	
   done	
   yet?	
   	
   no!	
   	
   there   s	
   sbll	
   a	
   problem:	
   

       over   (cid:128)ng	
   to	
   dev/val	
   

124	
   

experimental	
   pracbce	
   

       we	
   need	
   to	
   explore/evaluate	
   methodological	
   choices	
   
       what	
   should	
   we	
   do?	
   

       some	
   use	
   cross	
   validabon	
   on	
   train,	
   but	
   this	
   is	
   slow	
   and	
   

doesn   t	
   quite	
   simulate	
   real-     world	
   se(cid:128)ngs	
   (why?)	
   

       second	
   innovabon:	
   divide	
   data	
   into	
   train,	
   test,	
   and	
   a	
   
third	
   set	
   called	
   development	
   (dev)	
   or	
   validabon	
   (val)	
   
       use	
   dev/val	
   to	
   evaluate	
   choices	
   
       then,	
   when	
   ready	
   to	
   write	
   the	
   paper,	
   evaluate	
   the	
   best	
   

model	
   on	
   test	
   

       are	
   we	
   done	
   yet?	
   	
   no!	
   	
   there   s	
   sbll	
   a	
   problem:	
   

       over   (cid:128)ng	
   to	
   dev/val	
   

125	
   

experimental	
   pracbce	
   

       best	
   pracbce:	
   split	
   data	
   into	
   train,	
   development	
   (dev),	
   
development	
   test	
   (devtest),	
   and	
   test	
   
       train	
   model	
   on	
   train,	
   tune	
   hyperparameter	
   values	
   on	
   dev,	
   
do	
   preliminary	
   tesbng	
   on	
   devtest,	
   do	
      nal	
   tesbng	
   on	
   test	
   a	
   
single	
   bme	
   when	
   wribng	
   the	
   paper	
   
       even	
   bexer	
   to	
   have	
   even	
   more	
   test	
   sets!	
   test1,	
   test2,	
   etc.	
   

       experimental	
   credibility	
   is	
   a	
   huge	
   component	
   of	
   doing	
   
       when	
   you	
   publish	
   a	
   result,	
   it	
   had	
   bexer	
   be	
   replicable	
   

useful	
   research	
   

without	
   tuning	
   anything	
   on	
   test	
   

126	
   

don   t	
   cheat!	
   

127	
   

