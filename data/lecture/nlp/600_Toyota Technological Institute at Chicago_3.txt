ttic	
   31210:	
   

advanced	
   natural	
   language	
   processing	
   

kevin	
   gimpel	
   
spring	
   2017	
   

	
   

lecture	
   3:	
   

word	
   embeddings	
   

1	
   

assignment	
   1	
   

       assignment	
   1	
   due	
   tonight	
   

2	
   

roadmap	
   

       review	
   of	
   ttic	
   31190	
   (week	
   1)	
   
       deep	
   learning	
   for	
   nlp	
   (weeks	
   2-     4)	
   
       generamve	
   models	
   &	
   bayesian	
   id136	
   (week	
   5)	
   
       bayesian	
   nonparametrics	
   in	
   nlp	
   (week	
   6)	
   
       em	
   for	
   unsupervised	
   nlp	
   (week	
   7)	
   
       syntax/semanmcs	
   and	
   structure	
   predicmon	
   (weeks	
   8-     9)	
   
       applicamons	
   (week	
   10)	
   

3	
   

neural	
   similarity	
   modeling	
   
          siamese	
   networks   	
   (broid113y	
   et	
   al.,	
   1993)	
   
       two	
   idenmcal	
   networks	
   with	
   shared	
   parameters	
   
       at	
   end,	
   similarity	
   computed	
   between	
   two	
   representamons	
   

4	
   

similarity	
   funcmons	
   
       many	
   choices	
   for	
   similarity	
   funcmons	
   
       we	
   talked	
   about	
   some	
   during	
   lecture	
   2	
   

5	
   

learning	
   for	
   similarity	
   

       we	
   want	
   to	
   learn	
   input	
   representamon	
   
funcmon	
   	
   	
   	
   	
   	
   	
   as	
   well	
   as	
   any	
   parameters	
   of	
   
similarity	
   funcmon	
   
       we   ll	
   just	
   write	
   all	
   these	
   parameters	
   as	
   	
   
       how	
   about	
   this	
   loss?	
   (loss	
   a	
   on	
   your	
   handout)	
   

       any	
   potenmal	
   problems	
   with	
   this?	
   

6	
   

(beber)	
   learning	
   for	
   similarity	
   
       contrasmve	
   hinge	
   loss	
   (loss	
   b	
   on	
   handout):	
   

       	
   	
   	
   	
   is	
   a	
      negamve   	
   example	
   
       any	
   potenmal	
   problems	
   with	
   this?	
   	
   

7	
   

(beber)	
   learning	
   for	
   similarity	
   

       large-     margin	
   contrasmve	
   hinge	
   loss:	
   

       	
   	
   	
   	
   	
   is	
   the	
      margin   	
   

8	
   

(beber)	
   learning	
   for	
   similarity	
   

       large-     margin	
   contrasmve	
   hinge	
   loss:	
   

	
   
       how	
   should	
   we	
   choose	
   negamve	
   examples?	
   

9	
   

(beber)	
   learning	
   for	
   similarity	
   

       large-     margin	
   contrasmve	
   hinge	
   loss:	
   

	
   
       how	
   should	
   we	
   choose	
   negamve	
   examples?	
   

      random:	
   just	
   pick	
   v	
   randomly	
   from	
   the	
   data	
   
      max:	
   

      many	
   other	
   ways	
   depending	
   on	
   problem	
   

10	
   

aside:	
   

11	
   

recurrent	
   neural	
   networks	
   

   hidden	
   vector   	
   

12	
   

recurrent	
   neural	
   networks	
   

mulmplicamve	
   integramon	
   	
   
recurrent	
   neural	
   networks	
   

13	
   

14	
   

15	
   

id56	
   

mi-     id56	
   

16	
   

word	
   embeddings	
   

turian	
   et	
   al.	
   (2010)	
   

17	
   

journal of machine learning research 3 (2003) 1137   1155

submitted 4/02; published 2/03

a neural probabilistic language model

yoshua bengio
r  jean ducharme
pascal vincent
christian jauvin
d  partement d   informatique et recherche op  rationnelle
centre de recherche math  matiques
universit   de montr  al, montr  al, qu  bec, canada

bengioy@iro.umontreal.ca
ducharme@iro.umontreal.ca
vincentp@iro.umontreal.ca
jauvinc@iro.umontreal.ca

editors: jaz kandola, thomas hofmann, tomaso poggio and john shawe-taylor

       idea:	
   use	
   a	
   neural	
   network	
   for	
   n-     gram	
   

abstract

language	
   modeling:	
   

a goal of statistical id38 is to learn the joint id203 function of sequences of
words in a language. this is intrinsically dif   cult because of the curse of dimensionality: a word
sequence on which the model will be tested is likely to be different from all the word sequences seen
during training. traditional but very successful approaches based on id165s obtain generalization
by concatenating very short overlapping sequences seen in the training set. we propose to    ght the
curse of dimensionality by learning a distributed representation for words which allows each
training sentence to inform the model about an exponential number of semantically neighboring
sentences. the model learns simultaneously (1) a distributed representation for each word along
with (2) the id203 function for word sequences, expressed in terms of these representations.
generalization is obtained because a sequence of words that has never been seen before gets high
id203 if it is made of words that are similar (in the sense of having a nearby representation) to
words forming an already seen sentence. training such large models (with millions of parameters)
within a reasonable time is itself a signi   cant challenge. we report on experiments using neural
networks for the id203 function, showing on two text corpora that the proposed approach

18	
   

journal of machine learning research 3 (2003) 1137   1155

submitted 4/02; published 2/03

a neural probabilistic language model

yoshua bengio
r  jean ducharme
pascal vincent
christian jauvin
d  partement d   informatique et recherche op  rationnelle
centre de recherche math  matiques
universit   de montr  al, montr  al, qu  bec, canada

bengioy@iro.umontreal.ca
ducharme@iro.umontreal.ca
vincentp@iro.umontreal.ca
jauvinc@iro.umontreal.ca

editors: jaz kandola, thomas hofmann, tomaso poggio and john shawe-taylor

       this	
   is	
   not	
   the	
   earliest	
   paper	
   on	
   using	
   neural	
   

abstract

       see	
   paper	
   for	
   citamons	
   of	
   earlier	
   work	
   

networks	
   for	
   n-     gram	
   language	
   models,	
   but	
   it   s	
   
the	
   most	
   well-     known	
   and	
      rst	
   to	
   scale	
   up	
   

a goal of statistical id38 is to learn the joint id203 function of sequences of
words in a language. this is intrinsically dif   cult because of the curse of dimensionality: a word
sequence on which the model will be tested is likely to be different from all the word sequences seen
during training. traditional but very successful approaches based on id165s obtain generalization
by concatenating very short overlapping sequences seen in the training set. we propose to    ght the
curse of dimensionality by learning a distributed representation for words which allows each
training sentence to inform the model about an exponential number of semantically neighboring
sentences. the model learns simultaneously (1) a distributed representation for each word along
with (2) the id203 function for word sequences, expressed in terms of these representations.
generalization is obtained because a sequence of words that has never been seen before gets high
id203 if it is made of words that are similar (in the sense of having a nearby representation) to
words forming an already seen sentence. training such large models (with millions of parameters)
within a reasonable time is itself a signi   cant challenge. we report on experiments using neural
networks for the id203 function, showing on two text corpora that the proposed approach

19	
   

data sets (with millions or tens of millions of examples). finally, an important contribution of
neural	
   probabilismc	
   language	
   models	
   
this paper is to show that training such large-scale model is expensive but feasible, scales to large
contexts, and yields good comparative results (section 4).
many operations in this paper are in matrix notation, with lower case v denoting a column vector
and v0 its transpose, aj the j-th row of a matrix a, and x.y = x0y.
1.1 fighting the curse of dimensionality with distributed representations
in a nutshell, the idea of the proposed approach can be summarized as follows:

(bengio	
   et	
   al.,	
   2003)	
   

1. associate with each word in the vocabulary a distributed word feature vector (a real-

valued vector in rm),

2. express the joint id203 function of word sequences in terms of the feature vectors

of these words in the sequence, and

3. learn simultaneously the word feature vectors and the parameters of that id203

function.

the feature vector represents different aspects of the word: each word is associated with a point
in a vector space. the number of features (e.g. m =30, 60 or 100 in the experiments) is much
smaller than the size of the vocabulary (e.g. 17,000). the id203 function is expressed as a
product of conditional probabilities of the next word given the previous ones, (e.g. using a multi-
layer neural network to predict the next word given the previous ones, in the experiments). this
function has parameters that can be iteratively tuned in order to maximize the log-likelihood of
the training data or a regularized criterion, e.g. by adding a weight decay penalty.2 the feature

20	
   

model	
   (bengio	
   et	
   al.,	
   2003)	
   
i-th output = p(wt = i | context)

. . .

softmax

. . .

most  computation here

. . .

tanh

. . .

. . .

c(wt n+1)
. . .
table
look   up
c
in

index for

wt n+1

. . .

c(wt 1)

c

c(wt 2)
. . .
matrix
shared parameters
across words
wt 2

index for

wt 1

index for

figure 1: neural architecture: f (i,wt 1,       ,wt n+1) = g(i,c(wt 1),       ,c(wt n+1)) where g is the

neural network and c(i) is the i-th word feature vector.

21	
   

bengio	
   et	
   al.	
   (2003)	
   

       experiments:	
   

      they	
   minimized	
   log	
   loss	
   of	
   next	
   word	
   condimoned	
   
on	
   a	
      xed	
   number	
   of	
   previous	
   words	
   
      no	
   id56s	
   here.	
   just	
   a	
   feed-     forward	
   network.	
   
      ~800k	
   training	
   tokens,	
   vocab	
   size	
   of	
   17k	
   
      they	
   trained	
   for	
   5	
   epochs,	
   which	
   took	
   3	
   weeks	
   on	
   
40	
   cpus!	
   

22	
   

c

no

30

yes

100

60
60
60
60
30
30
30
30
30
30

yes
yes
yes
yes
yes
yes
yes
yes
no
no

265
352
334
332
332
348
354
326
335
343
327
327

mlp10
del. int.
kneser-ney back-off
kneser-ney back-off
kneser-ney back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off

h m direct mix
50
no
50
yes
no
0
0
yes
no
50
50
yes
no
50
50
yes
100
no
yes
100

5
a neural probabilistic language model
3
31
3
4
5
3
3
3
3
3
4
5

252
336
experiments	
   (bengio	
   et	
   al.,	
   2003)	
   
323
321
321
valid.
334
284
340
275
312
327
319
286
326
296
312
273
312
309
284
280
265
352
334
332
332
348
354
326
335
343
327
327

mlp1
mlp2
mlp3
mlp4
mlp5
mlp6
mlp7
mlp8
mlp9
table 1: comparative results on the brown corpus. the deleted interpolation trigram has a test per-
mlp10
plexity that is 33% above that of the neural network with the lowest validation perplexity.
del. int.
the difference is 24% in the case of the best id165 (a class-based model with 500 word
kneser-ney back-off
classes). n : order of the model. c : number of word classes in class-based id165s. h :
kneser-ney back-off
number of hidden units. m : number of word features for mlps, number of classes for
kneser-ney back-off
class-based id165s. direct: whether there are direct connections from word features to
class-based back-off
outputs. mix: whether the output probabilities of the neural network are mixed with the
class-based back-off
output of the trigram (with a weight of 0.5 on each). the last three columns give perplexity
class-based back-off
on the training, validation and test sets.
class-based back-off
class-based back-off
class-based back-off
class-based back-off

probabilities. on the other hand, without those connections the hidden units form a tight bottleneck
which might force better generalization.

n
150
5
200
5
500
5
1000
5
2000
5
500
5
500
3
3
5
5
3
3
4
5
3
3
3
3
3
4
5

test.
268
257
310
272
279
259
293
270
276
252
336
323
321
321
334
340
312
319
326
312
312

train.
182
201
209
210
175
31

150
200
500
1000
2000
500
500

23	
   

a neural probabilistic language model

c

60
60
60
60
30
30
30
30
30
30

yes
yes
yes
yes
yes
yes
yes
yes
no
no

train.
182
201
209
210
175
31

h m direct mix
no
50
50
yes
no
0
0
yes
no
50
50
yes
50
no
yes
50
100
no
yes
100

experiments	
   (bengio	
   et	
   al.,	
   2003)	
   
test.
valid.
268
284
mlp1
mlp2
257
275
mlp3
310
327
mlp4
272
286
mlp5
279
296
mlp6
259
273
mlp7
293
309
mlp8
270
284
mlp9
276
280
252
mlp10
265
del. int.
336
352
kneser-ney back-off
323
334
kneser-ney back-off
321
332
       hidden	
   layer	
   (h	
   >	
   0)	
   helps	
   
kneser-ney back-off
321
332
class-based back-off
334
348
       interpolamng	
   with	
   n-     gram	
   model	
   (   mix   )	
   helps	
   
class-based back-off
340
354
       using	
   higher	
   word	
   embedding	
   dimensionality	
   helps	
   
312
class-based back-off
326
class-based back-off
335
319
       5-     gram	
   model	
   beber	
   than	
   trigram	
   
class-based back-off
343
326
class-based back-off
312
327
class-based back-off
327
312

n
5
5
5
5
5
5
3
3
5
5
3
       observamons:	
   
3
4
5
3
3
3
3
3
4
5

150
200
500
1000
2000
500
500

24	
   

a neural probabilistic language model

experiments	
   
c
c

h m direct mix
h m direct mix
no
50
50
no
50
yes
50
yes
0
no
0
no
yes
0
0
yes
50
no
50
no
yes
50
50
yes
50
no
50
no
50
yes
50
yes
no
100
100
no
100
yes
100
yes

yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
no
no
no
no

60
60
60
60
60
60
60
60
30
30
30
30
30
30
30
30
30
30
30
30

train.
train.
182
182
201
201
209
209
210
210
175
175
31
31

mlp1
mlp1
mlp2
mlp2
mlp3
mlp3
mlp4
mlp4
mlp5
mlp5
mlp6
mlp6
mlp7
mlp7
mlp8
mlp8
mlp9
mlp9
mlp10
mlp10
del. int.
del. int.
kneser-ney back-off
kneser-ney back-off
kneser-ney back-off
kneser-ney back-off
kneser-ney back-off
kneser-ney back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off
class-based back-off

n
n
5
5
5
5
5
5
5
5
5
5
5
5
3
3
3
3
5
5
5
5
3
3
3
3
4
4
5
5
3
3
3
3
3
3
3
3
3
3
4
4
5
5

150
150
200
200
500
500
1000
1000
2000
2000
500
500
500
500

valid.
valid.
284
284
275
275
327
327
286
286
296
296
273
273
309
309
284
284
280
280
265
265
352
352
334
334
332
332
332
332
348
348
354
354
326
326
335
335
343
343
327
327
327
327

test.
test.
268
268
257
257
310
310
272
272
279
279
259
259
293
293
270
270
276
276
252
252
336
336
323
323
321
321
321
321
334
334
340
340
312
312
319
319
326
326
312
312
312
312

25	
   
table 1: comparative results on the brown corpus. the deleted interpolation trigram has a test per-
table 1: comparative results on the brown corpus. the deleted interpolation trigram has a test per-

bengio	
   et	
   al.	
   (2003)	
   

       they	
   discuss	
   how	
   the	
   word	
   embedding	
   space	
   

might	
   be	
   interesmng	
   to	
   examine	
   but	
   they	
   
don   t	
   do	
   this	
   

       they	
   suggest	
   that	
   a	
   good	
   way	
   to	
   visualize/

interpret	
   word	
   embeddings	
   would	
   be	
   to	
   use	
   2	
   
dimensions	
   j   	
   

       they	
   discussed	
   handling	
   polysemous	
   words,	
   
unknown	
   words,	
   id136	
   speed-     ups,	
   etc.	
   

26	
   

collobert	
   et	
   al.	
   (2011)	
   

journal of machine learning research 12 (2011) 2493-2537

submitted 1/10; revised 11/10; published 8/11

natural language processing (almost) from scratch

ronan collobert   
jason weston   
l  eon bottou   
michael karlen
koray kavukcuoglu  
pavel kuksa  
nec laboratories america
4 independence way
princeton, nj 08540

editor: michael collins

ronan@collobert.com
jweston@google.com
leon@bottou.org
michael.karlen@gmail.com
koray@cs.nyu.edu
pkuksa@cs.rutgers.edu

we propose a uni   ed neural network architecture and learning algorithm that can be applied to var-
ious natural language processing tasks including part-of-speech tagging, chunking, named entity
recognition, and id14. this versatility is achieved by trying to avoid task-speci   c

abstract

27	
   

input window

word of interest

text

cat

sat on the mat

w1

1 w1

2

wk

1 wk

2

. . .

. . .

w1
n

wk
n

d

concat

n1

hu

feature 1

.
.
.

feature k

lookup table

ltw 1

.
.
.

ltw k

linear

m 1

     

hardtanh

linear

m 2

     

n2

hu = #tags

28	
   

figure 1: window approach network.

collobert	
   et	
   al.	
   pairwise	
   ranking	
   loss	
   

       	
   	
   	
   	
   	
   	
   is	
   training	
   set	
   of	
   11-     word	
   windows	
   
       	
   	
   	
   	
   	
   	
   is	
   vocabulary	
   
       what	
   is	
   going	
   on	
   here?	
   (loss	
   c	
   on	
   handout)	
   

29	
   

collobert	
   et	
   al.	
   pairwise	
   ranking	
   loss	
   

       	
   	
   	
   	
   	
   	
   is	
   training	
   set	
   of	
   11-     word	
   windows	
   
       	
   	
   	
   	
   	
   	
   is	
   vocabulary	
   
       what	
   is	
   going	
   on	
   here?	
   

      make	
   actual	
   text	
   window	
   have	
   higher	
   score	
   than	
   
all	
   windows	
   with	
   center	
   word	
   replaced	
   by	
   w	
   

30	
   

collobert	
   et	
   al.	
   pairwise	
   ranking	
   loss	
   

       	
   	
   	
   	
   	
   	
   is	
   training	
   set	
   of	
   11-     word	
   windows	
   
       	
   	
   	
   	
   	
   	
   is	
   vocabulary	
   
       this	
   smll	
   sums	
   over	
   enmre	
   vocabulary,	
   so	
   it	
   

should	
   be	
   as	
   slow	
   as	
   log	
   loss   	
   

       why	
   can	
   it	
   be	
   faster?	
   

      when	
   using	
   sgd,	
   summamon	
        	
   sample	
   

31	
   

