id170 with neural networks

in id103

liang lu

ttic

19 april 2016

outline

    id103 as a id170 problem
    id48
    connectionist temporal classi   cation
    neural segmental conditional random field
    encoder-decoder with attention

2 of 48

id170

general supervised training:

input: x        output: y

(1)

    classi   cation

    input (x): scalar or vector,
    output(y ): discrete class label
    loss: (usually) 0-1 loss

    regression

    input (x): scalar or vector
    output (y ): real number
    loss: (usually) mean square error

3 of 48

id170

general supervised training:

input: x        output: y

(2)

    id170

    input (x): set or sequence,
    output (y ): sequence, tree, or graph
    loss: ?

4 of 48

id170

general sequence transduction:

input: x1 :t        output: y1:l

(3)

    id103

    input (x): a sequence of vectors (length = t)
    output (y ): a sequence of class labels (length = l)
    loss: id153 (optimal, but not di   erentiable)

    challenges

    t > l: segmentation problem
    xt    ?: alignment problem

5 of 48

hidden markov model

    general sequence transduction:

input: x1 :t        output: y1:l

(4)

    frame-level classi   cation problem:

input: x1 :t        hidden:q1:t        output: y1:l

(5)

6 of 48

qt   1qtqt+1xt+1xtxt   1hidden markov model

    given (x, q)1:t , mini-batch training of nn is straightforward
    problem: how to get the hidden labels q1:t ?
    expectation-maximization algorithm

    e: given x1:t , y1:l,   old , compute p(q1:t|x1:t , y1:l;   old )

(cid:123)(cid:122)

(cid:124)

(cid:125)

constrained decoding

    m: given x1:t , q1:t , update model   new       old +     

    usually do many iterations

7 of 48

hidden markov model

    decoding and constrained decoding

    t is the number of time steps
    n is the number of id48 states

8 of 48

............tnhidden markov model

    decoding graph: h     c     l     g

    h: id48 transition ids to context dependent phones
    c : context dependent phones to context independent phones
    l: context independent phones to words
    g : words to sequences of words

    example: http://vpanayotov.blogspot.com/2012/06/kaldi-

decoding-graph-construction.html

9 of 48

hidden markov model

    limitations:

    conditional independence: given q1:t , every pair of x are independent
    local (frame-level) id172: p(qt|xt)
    not end-to-end, many iterations to update q1:t

10 of 48

connectionist temporal classi   cation

    enumerate all the hidden labels (paths)

input: x1 :t        hidden:

                       output: y1:l

    marginalize out the hidden variables

p(y1:l|x1:t ) =

    again, local id172

p(q1:t|x1:t ) =

t

11 of 48

p(q1:t|x1:t )

p(qt|xt)

q1:t
...
q1:t

                q1:t
(cid:88)
(cid:89)

q1:t     (y )

(6)

(7)

(8)

connectionist temporal classi   cation

    how to enumerate all the paths?

    can you enumerate all the paths for mt?

[1] r. collobert, et al,    wav2letter: an end-to-end convnet-based id103
system   , arxiv 2016

12 of 48

connectionist temporal classi   cation

    role of the blank state (   ), separating duplicated labels

y: abbc        q: {a, b}     {b, c}
q:    aaa   bb   bbb-cc           y: abbc

    conditional maximum likelihood training

(cid:88)

p(y1:l|x1:t ) =

p(q1:t|x1:t )

(9)

q1:t     (y )

    forward-backward algorithm to compute the summed id203

13 of 48

connectionist temporal classi   cation

[1] a. graves, et al,    connectionist temporal classi   cation: labelling unsegmented
sequence data with recurrent neural networks   , icml 2006

14 of 48

connectionist temporal classi   cation

    gram-ctc: ctc with character id165s

[1] h. liu, et al,    gram-ctc: automatic unit selection and target decomposition for
sequence labelling   , arxiv 2017
15 of 48

connectionist temporal classi   cation

q: why most of the frames are labelled as blank?

[1] a. senior, et al,    acoustic modelling with cd-ctc-smbr lstm id56s   , asru 2015.

16 of 48

connectionist temporal classi   cation

remarks:
    suitable for end-to-end training

    independence assumption: p(q1:t|x1:t ) =(cid:81)

t p(qt|xt)

    scalable to large dataset
    works with lstm, id98, but not dnn

17 of 48

(segmental) conditional random field

sequence transduction for speech:

input: x1 :t        output: y1:l

    crf still require an alignment model for id103
    segmental crf is equipped with implicit alignment

18 of 48

crfsegmental crf(segmental) conditional random field

    crf [la   erty et al. 2001]

p(y1:l | x1:t ) =

1

z (x1:t )

(cid:16)

exp

(cid:89)

j

(cid:17)

w(cid:62)  (yj , x1:t )

where l = t .

    segmental (semi-markov) crf [sarawagi and cohen 2004]

p(y1:l, e ,| x1:t ) =

1

z (x1:t )

j

exp

w(cid:62)  (yj , ej , x1:t )

(cid:89)

(cid:16)

(10)

(11)

(cid:17)

where ej = (cid:104)sj , nj(cid:105) denotes the beginning (sj ) and end (nj ) time
tag of yj ; e = {e1:l} is the latent segment label.

19 of 48

(segmental) conditional random field

j exp(cid:0)w(cid:62)  (yj , x1:t )(cid:1)
(cid:81)

1

z (x1:t )

    learnable parameter w
    engineering the feature function   (  )
    designing   (  ) is much harder for speech than nlp

20 of 48

segmental recurrent neural network

    using (recurrent) neural networks to learn the feature function

  (  ).

21 of 48

x1x2x3x4y2y1x5x6y3segmental recurrent neural network

    more memory e   cient

22 of 48

x1x2x3x4x5x6y1y2y3copy actioncopied hidden statesegmental recurrent neural network

    comparing to previous segmental models

    m. ostendorf et al.,    from id48   s to segment models: a uni   ed view

of stochastic modeling for id103   , ieee trans. speech
and audio proc. 1996

    j. glass,    a probabilistic framework for segment-based speech

recognition   , computer speech & language, 2002

    markovian framework vs. crf framework (local vs. global

id172)

    neural network feature (and end-to-end training)

23 of 48

related works

    (segmental) crfs for speech
    neural crfs
    structured id166s
    two good review papers

    m. gales, s. watanabe and e. fosler-lussier,    structured discriminative
models for id103   , ieee signal processing magazine, 2012

    e. fosler-lussier et al.    conditional random    elds in speech, audio, and

language processing, proceedings of the ieee, 2013

24 of 48

segmental recurrent neural network

    training criteria

    conditional maximum likelihood

l(  ) = log p(y1:l | x1:t )

= log

p(y1:l, e | x1:t )

(12)

(cid:88)

e

    hinge loss     similar to structured id166
    marginalized hinge loss

[1] h. tang, et al,    end-to-end training approaches for discriminative segmental models   ,
slt, 2016

25 of 48

segmental recurrent neural network

    viterbi decoding

    partially viterbi decoding

y   1:l = arg max
y1:l

log

(cid:88)

e

p(y1:l, e | x1:t )

    full viterbi decoding

y   1:l, e    = arg max
y1:l,e

log p(y1:l, e | x1:t )

(13)

(14)

26 of 48

segmental recurrent neural network

remarks:
    no independence assumption
    globally (sequence-level) normalized model
    computationally expensive, not very scalable

27 of 48

scale to large vocabulary asr

    why segmental crf expensive?

p(y1:l, e ,| x1:t ) =

1

z (x1:t )

j

(cid:89)

(cid:16)

exp

w(cid:62)  (yj , ej , x1:t )

(cid:17)

(15)

where ej = (cid:104)sj , nj(cid:105) denotes the beginning (sj ) and end (nj ) time
tag.

(cid:88)

j(cid:89)

z (x1:t ) =

exp f (yj , ej , x1:t ) .

(16)

y ,e

j=1

    computation complexity is o(t 2|v|)

28 of 48

scale to large vocabulary asr

    analogous to large softmax for id38

(cid:80)

p(w ) =

exp(zw )
w(cid:48)   v exp(zw(cid:48))

(17)

    noise contrastive estimation
    importance sampling
    can we try similar ideas for scrf?

29 of 48

attention model

sequence transduction for speech:

input: x1 :t        output: y1:l

compute the id155

l(cid:89)
l(cid:89)

l=1

p(y1:l|x1:t ) =

p(yl|y<1, x1:t )

   

p(yl|y<1, cl )
cl = attenc(y<1, x1:t )

l=1

30 of 48

(18)

(19)

(20)

attention model

31 of 48

x1x2x3x4x5   y1attention model

32 of 48

x1x2x3x4x5   y1y2attention model

33 of 48

x1x2x3x4x5   y1y2y3attention model

34 of 48

x1x2x3x4x5   y1y2y3y4attention model

35 of 48

x1x2x3x4x5   y1y2y3y4y5attention model

36 of 48

x1x2x3x4x5   y1y2y3y4y5encoderattentiondecoderh1:t=id56(x1:t)cj=attend(h1:t)p(yj|y1,      ,yj   1,cj)attention model

encoder with pyramid id56

37 of 48

x1x2x3x4      x1x2x3x4      a) concatenate / addb) skipattention model

    remarks

    monotonic alignment   
    independence assumption for inputs   
    long input sequence    
    length mismatch    
    locally normalized for each output token

p(y1:l|x1:t )    

38 of 48

(cid:89)

l

p(yl|y<l , cl )

(21)

attention model

    locally normalized models:

    conditional independence assumption
    label bias problem
    we care more about the sequence level loss in id103
          

[1] d. andor, et al,    globally normalized transition-based neural networks   , acl, 2016

39 of 48

id103

    locally to globally normalized models:

    id48s: ce     sequence training
    ctc: ce     sequence training
    attention model: minimum bayes risk training
p(y|x)a(y ,   y )

(cid:88)

l =

y      

(22)

    would be interesting to look at this for speech

[1] s. shen, et al,    minimum risk training for id4   , acl, 2016

[2]s. wiseman, a. rush,    sequence-to-sequence learning as beam-search optimization   ,

emnlp, 2016

40 of 48

experimental results

    timit dataset (    1 million frames)
    wsj (    30 million frames)
    swbd (    100 millon frames)

41 of 48

experiments on timit

table: results on timit. lm = language model, sd = speaker dependent
feature

system
id48-dnn
ctc [graves 2013]
id56 transducer [graves 2013]
attention model [chorowski 2015]
segmental id56
segmental id56

lm sd per
    18.5
   
   18.4
  
   17.7
   
   17.6
   
   18.9
  
    17.3
  

42 of 48

experiments on wsj

table: results on wsj. lm = language model

system
lm wer(%)
   
id48-dnn (phone)
ctc [graves & jaitly 2014]
  
   
ctc [graves & jaitly 2014]
   
ctc [miao 2015]
   
gram-ctc [liu 2017]
attention model [chan 2016]
   
attention model [chorowski 2016]    

3 - 4
30.1
8.7
7.3
6.8
9.6
6.7

43 of 48

experiments on swbd

table: results on swbd. lm = language model

system
lm wer(%)
   
id48-dnn (phone)
   
id48-dnn (phone) (2000h)
ctc [zweig 2016]
  
   
ctc [zweig 2016]
   
gram-ctc [liu 2017] (2000h)
attention model [lu 2016]
  
attention model [toshniwal 2017]   

9.6
5.5
24.7
14.0
7.3
26.8
23.1

44 of 48

multitask learning

    weaknesses of end-to-end models

    attention model     alignment problem in the early stage of training
    ctc model     conditional independence assumption
    sid56 model     large computational cost

    multitask learning to mitigate the weaknesses

[1] s. kim, t. hori, s. watanabe,    joint ctc-attention based end-to-end speech

recognition using id72    , icassp 2017.
45 of 48

multitask learning

[1] s. kim, t. hori, s. watanabe,    joint ctc-attention based end-to-end speech

recognition using id72    , icassp 2017.

46 of 48

multitask learning

[1] l. lu et al.,    id72 with ctc and segmental crf for speech

recognition   , arxiv 2017.

47 of 48

05101520number of epochs0.350.40.450.50.550.60.650.70.750.8error (%)mtl with ctc pretrainingsid56 with random initializatioid4l with random initializationconclusion

    id170 for id103
    end-to-end training models
    flexibility vs. scalability
    other deep learning architectures

48 of 48

