1
1
1
cs 388: 
natural language processing:
id29
raymond j. mooney
university of texas at austin
representing meaning
representing the meaning of natural language is ultimately a difficult philosophical question, i.e. the    meaning of meaning   .
traditional approach is to map ambiguous nl to unambiguous logic in first-order predicate calculus (fopc).
standard id136 (theorem proving) methods exist for fopc that can determine when one statement entails (implies) another. questions can be answered by determining what potential responses are entailed by given nl statements and background knowledge all encoded in fopc.

2
model theoretic semantics
meaning of traditional logic is based on model theoretic semantics which defines meaning in terms of a model (a.k.a. possible world),  a set-theoretic structure that defines a (potentially infinite) set of objects with properties and relations between them.
a model is a connecting bridge between language and the world by representing the abstract objects and relations that exist in a possible world. 
an interpretation is a mapping from logic to the model that defines predicates extensionally, in terms of the set of tuples of objects that make them true (their denotation or extension).
the extension of  red(x) is the set of all red things in the world.
the extension of father(x,y) is the set of all pairs of objects <a,b> such that  a is b   s father.

3
truth-conditional semantics
model theoretic semantics gives the truth conditions for a sentence, i.e. a model satisfies a logical sentence iff the sentence evaluates to true in the given model. 
the meaning of a sentence is therefore defined as the set of all  possible worlds in which it is true.
4
what is id29?
mapping a natural-language sentence to a detailed representation of its complete meaning in a fully formal language that:
has a rich ontology of types, properties, and relations.
supports automated reasoning or execution.
5
6
geoquery:
 a database query application
query application for a u.s. geography database containing about 800 facts [zelle & mooney, 1996] 


what is the smallest state by area?
query
answer(x1,smallest(x2,(state(x1),area(x1,x2))))

id29

rhode island
answer

prehistory 1600   s
gottfried leibniz (1685) developed a formal conceptual language, the characteristica universalis, for use by an automated reasoner, the calculus ratiocinator.
7
   the only way to rectify our reasonings is to make them as tangible as those of the mathematicians, so that we can find our error at a glance, and when there are disputes among persons, we can simply say: let us calculate, without further ado, to see who is right.   

interesting book on leibniz
8
prehistory 1850   s 
george boole (laws of thought, 1854) reduced id118 to an algebra over binary-valued variables.
9
his book is subtitled    on which are founded the mathematical theories of logic and probabilities    and tries to formalize both forms of human reasoning.
prehistory 1870   s 
gottlob frege (1879) developed begriffsschrift (concept writing), the first formalized quantified predicate logic.
10
prehistory 1910   s 
bertrand russell and alfred north whitehead (principia mathematica, 1913) finalized the development of modern first-order predicate logic (fopc).
11



interesting book on russell
12
history from philosophy and linguistics
richard montague (1970) developed a formal method for mapping natural-language to fopc using church   s id198 of functions and the fundamental principle of semantic compositionality for
13
recursively computing the meaning of each syntactic constituent from the meanings of its sub-constituents.

later called    montague grammar    or    montague semantics   
interesting book on montague
14
see aifric campbell   s (2009) novel the semantics of murder for a fictionalized account of his mysterious death in 1971 (homicide or homoerotic asphyxiation??).
early history in ai
bill woods (1973) developed the first nl database interface (lunar) to answer scientists    questions about moon rooks
15
using a manually developed augmented transition network (atn) grammar.
early history in ai
dave waltz (1975) developed the next nl database interface (planes) to query a database of aircraft maintenance for the us air force.
i learned about this early work as a student of dave   s at uiuc in the early 1980   s.
16






(1943-2012)
early commercial history
gary hendrix founded symantec (   semantic technologies   ) in 1982 to commercialize nl database 
17
interfaces based on manually developed semantic grammars, but they switched to other markets when this was not profitable. 
hendrix got his bs and ms at ut austin working with my former ut nlp colleague, bob simmons (1925-1994). 
1980   s: the    fall    of id29
manual development of a new semantic grammar for each new database did not    scale well    and was not commercially viable.
the failure to commercialize nl database interfaces led to decreased research interest in the problem.
18
19
id29
id29: transforming natural language (nl) sentences into completely formal logical forms or meaning representations (mrs).
sample application domains where mrs are directly executable by another computer system to perform some task.
clang: robocup coach language 
geoquery: a database query application 


20
clang: robocup coach language
in robocup coach competition teams compete to coach simulated players [http://www.robocup.org]
the coaching instructions are given in a formal language called clang [chen et al. 2003] 

simulated soccer field
clang
if the ball is in our goal area then player 1 should intercept it.

(bpos (goal-area our) (do our {1} intercept))

id29

procedural semantics
the meaning of a sentence is a formal representation of a procedure that performs some action that is an appropriate response.
answering questions
following commands
in philosophy, the    late    wittgenstein was known for the    meaning as use    view of semantics compared to the model theoretic view of the    early    wittgenstein and other logicians.
21
22
22

most existing work on computational semantics is based on predicate logic

	what is the smallest state by area?
	answer(x1,smallest(x2,(state(x1),area(x1,x2))))
	
	x1 is a logical variable that denotes    the smallest state by area   

predicate logic query language
23
23

functional query language (funql)
transform a logical language into a functional, variable-free language (kate et al., 2005)




what is the smallest state by area?
answer(x1,smallest(x2,(state(x1),area(x1,x2))))

answer(smallest_one(area_1(state(all))))

24
learning semantic parsers
manually programming robust semantic parsers is difficult due to the complexity of the task.
semantic parsers can be learned automatically from sentences paired with their logical form.




nl   mr 
training exs
25
engineering motivation
most computational language-learning research strives for broad coverage while sacrificing depth.
   scaling up by dumbing down   
realistic id29 currently entails domain dependence.
domain-dependent natural-language interfaces have a large potential market.
learning makes developing specific applications more tractable.
training corpora can be easily developed by tagging  existing corpora of formal statements with natural-language glosses.
26
cognitive science motivation
most natural-language learning methods require supervised training data that is not available to a child.
general lack of negative feedback on grammar.
no pos-tagged or treebank data.
assuming a child can infer the likely meaning of an utterance from context, nl   mr pairs are more cognitively plausible training data.

27
our semantic-parser learners 
chill+wolfie (zelle & mooney, 1996; thompson & mooney, 1999, 2003) 
separates parser-learning and semantic-lexicon learning.
learns a deterministic parser using ilp techniques.
cocktail (tang & mooney, 2001)
improved ilp algorithm for chill.
silt (kate, wong & mooney, 2005) 
learns symbolic transformation rules for mapping directly from nl to lf.
scissor (ge & mooney, 2005) 
integrates semantic interpretation into collins    statistical syntactic parser.
wasp (wong & mooney, 2006)
uses syntax-based id151 methods.
krisp (kate & mooney, 2006)
uses a series of id166 classifiers employing a string-kernel to iteratively build semantic representations.

28
chill
(zelle & mooney, 1992-96)
semantic parser acquisition system using inductive logic programming (ilp) to induce a parser written in prolog.
starts with a deterministic parsing    shell    written in prolog and learns to control the operators of this parser to produce the given i/o pairs.
requires a semantic lexicon, which for each word gives one or more possible meaning representations.
parser must disambiguate words, introduce proper semantic representations for each, and then put them together in the right way to produce a proper representation of the sentence.
29
chill example
u.s. geographical database 
sample training pair
cu  l es el capital del estado con la poblaci  n m  s grande? 
answer(c, (capital(s,c), largest(p, (state(s), population(s,p)))))
sample semantic lexicon
cu  l :             answer(_,_)
capital:          capital(_,_)
estado:           state(_)
m  s grande:  largest(_,_)
poblaci  n:     population(_,_)
30
wolfie
(thompson & mooney, 1995-1999)
learns a semantic lexicon for chill from the same corpus of semantically annotated sentences.
determines hypotheses for word meanings by finding largest isomorphic common subgraphs shared by meanings of sentences in which the word appears.
uses a greedy-covering style algorithm to learn a small lexicon sufficient to allow compositional construction of the correct representation from the words in a sentence.
31
wolfie + chill
semantic parser acquisition
nl   mr 
training exs
id152
approach to semantic analysis based on building up an mr compositionally based on the syntactic structure of a sentence.
build mr recursively bottom-up from the parse tree.
buildmr(parse-tree)
       if parse-tree is a terminal node (word) then
               return an atomic lexical meaning for the word.
       else 
            for each child, subtreei, of parse-tree
                   create its mr by calling buildmr(subtreei)
       return an mr by properly combining the resulting mrs
             for its children into an mr for the overall parse-tree. 
composing mrs from parse trees
33
what is the capital of ohio?
s
np
vp
wp
what
answer(capital(loc_2(stateid('ohio'))))
capital(loc_2(stateid('ohio')))
answer()
answer()
answer()
np
capital(loc_2(stateid('ohio')))
vbz
v
is
dt
n
pp
loc_2(stateid('ohio'))
capital()
in
np
nnp
ohio
stateid('ohio')
the
capital
of
loc_2()
capital()
stateid('ohio')
stateid('ohio')
loc_2()
   
   
   
   
   
disambiguation with 
id152 
the composition function that combines the mrs of the children of a node, can return     if there is no sensible way to compose the children   s meanings.
could compute all parse trees up-front and then compute semantics for each, eliminating any that ever generate a     semantics for any constituent.
more efficient method: 
when filling (cky) chart of syntactic phrases, also compute all possible id152 of each phrase as it is constructed and make an entry for each.
if a given phrase only gives     semantics, then remove this phrase from the table, thereby eliminating any parse that includes this meaningless phrase.
composing mrs from parse trees

35
what is the capital of ohio?
s
np
vp
wp
what
np
vbz
v
is
dt
n
pp
in
np
nnp
ohio
riverid('ohio')
the
capital
of
loc_2()
riverid('ohio')
riverid('ohio')
loc_2()
   
composing mrs from parse trees

what is the capital of ohio?
s
np
vp
wp
what
np
vbz
v
is
dt
n
capital()
the
capital
capital()
   
   
   
   
   
pp
loc_2(stateid('ohio'))
in
np
nnp
ohio
stateid('ohio')
of
loc_2()
stateid('ohio')
stateid('ohio')
loc_2()
capital()
   
37
37
wasp
a machine translation approach to id29
uses id151 techniques
synchronous context-free grammars (sid18) (wu, 1997; melamed, 2004; chiang, 2005)
word alignments (brown et al., 1993; och & ney, 2003)
hence the name: word alignment-based id29
38
38
a unifying framework for 
parsing and generation
natural languages
machine
translation


39
39
a unifying framework for 
parsing and generation
natural languages
formal languages

id29
machine
translation


40
40
a unifying framework for 
parsing and generation
natural languages
formal languages

id29

tactical generation
machine
translation


41
41
a unifying framework for 
parsing and generation
natural languages
formal languages

id29

tactical generation
machine
translation



synchronous parsing
42
42
a unifying framework for 
parsing and generation
natural languages
formal languages

id29

tactical generation
machine
translation


compiling:
aho & ullman
(1972)



synchronous parsing
43
43
synchronous context-free grammars (sid18)
developed by aho & ullman (1972) as a theory of compilers that combines syntax analysis and code generation in a single phase
generates a pair of strings in a single derivation
44
44
query     what is city
city     the capital city
city     of state
state     ohio
context-free semantic grammar
query
city
what
is



city
the
capital



45
45


query     what is city / answer(city)
productions of 
synchronous context-free grammars
natural language
formal language

46
46
state     ohio / stateid('ohio')
what is the capital of ohio
synchronous context-free 
grammar derivation
query
query




answer(capital(loc_2(stateid('ohio'))))
47
47
probabilistic parsing model
ohio

of
state


city
city
capital    (    city    )






loc_2    (    state    )


stateid    (    'ohio'    )




capital
city


state     ohio / stateid('ohio')
d1
48
48
probabilistic parsing model
ohio

of
river


city
city
capital    (    city    )






loc_2    (    river    )


riverid    (    'ohio'    )




capital
city


river     ohio / riverid('ohio')
d2
49
49

city
capital    (    city    )






loc_2    (    state    )


stateid    (    'ohio'    )




probabilistic parsing model
city
capital    (    city    )






loc_2    (    river    )


riverid    (    'ohio'    )




state     ohio / stateid('ohio')
river     ohio / riverid('ohio')
0.5
0.3
0.5
0.5
0.05
0.5

  

  
1.3
1.05
pr(d1|capital of ohio) = exp(      ) / z
pr(d2|capital of ohio) = exp(        ) / z
d1
d2
id172 constant


50
50

overview of wasp
lexical acquisition
parameter estimation
id29
unambiguous id18 of mrl
training set, {(e,f)}
lexicon, l
parsing model parameterized by   
input sentence, e'
output mr, f'
training
testing
51
51
lexical acquisition
transformation rules are extracted from word alignments between an nl sentence, e, and its correct mr, f, for each training example, (e, f)
52
52
word alignments
a mapping from french words to their meanings expressed in english
and    the    program    has    been    implemented
le    programme    a      t      mis    en    application







53
53
lexical acquisition
train a statistical word alignment model (ibm model 5) on training set
obtain most probable n-to-1 word alignments for each training example
extract transformation rules from these word alignments
lexicon l consists of all extracted transformation rules
54
54
word alignment for id29
how to introduce syntactic tokens such as parens?
(  (  true  )  (  do  our  {  1  }  (  pos  (  half  our  )  )  )  )
the    goalie    should    always    stay    in    our    half







55
55








use of mrl grammar
the
goalie
should
always
stay
in
our
half
rule     (condition directive)
condition     (true)
directive     (do team {unum} action)
team     our
unum     1
action     (pos region)
region     (half team)
team     our









top-down, left-most derivation of an un-ambiguous id18
n-to-1
56
56
team


extracting transformation rules
the
goalie
should
always
stay
in
our
half
rule     (condition directive)
condition     (true)
directive     (do team {unum} action)
team     our
unum     1
action     (pos region)
region     (half team)
team     our







team     our / our
57
57
region


team


extracting transformation rules
the
goalie
should
always
stay
in
half
rule     (condition directive)
condition     (true)
directive     (do team {unum} action)
team     our
unum     1
action     (pos region)
region     (half team)
team     our








region     (half our)

58
58
action
action     (pos (half our))




region
extracting transformation rules
the
goalie
should
always
stay
in
rule     (condition directive)
condition     (true)
directive     (do team {unum} action)
team     our
unum     1
action     (pos region)






region     (half our)

59
59
based on maximum-id178 model:



features fi (d) are number of times each transformation rule is used in a derivation d
output translation is the yield of most probable derivation
probabilistic parsing model
60
60
parameter estimation
maximum conditional log-likelihood criterion


since correct derivations are not included in training data, parameters   * are learned in an unsupervised manner
em algorithm combined with improved iterative scaling, where hidden variables are correct derivations (riezler et al., 2000)
61
61
experimental corpora
clang 
300 randomly selected pieces of coaching advice from the log files of the 2003 robocup coach competition
22.52 words on average in nl sentences
14.24 tokens on average in formal expressions

geoquery [zelle & mooney, 1996] 
250 queries for the given u.s. geography database
6.87 words on average in nl sentences
5.32 tokens on average in formal expressions
also translated into spanish, turkish, & japanese.

62
62
experimental methodology
evaluated using standard 10-fold cross validation
correctness
clang: output exactly matches the correct representation
geoquery: the resulting query retrieves the same answer as the correct representation 
metrics

63
63
precision learning curve for clang
64
64
recall learning curve for clang
65
65
precision learning curve for geoquery
66
66
recall learning curve for geoquery
67
67
precision learning curve for geoquery (wasp)
68
68
recall learning curve for geoquery (wasp)
69
69
tactical id86
mapping a formal mr into nl
can be done using id151
previous work focuses on using generation in interlingual mt (haji   et al., 2004)
there has been little, if any, research on exploiting statistical mt methods for generation
70
70
tactical generation
can be seen as inverse of id29

((true) (do our {1} (pos (half our))))
the goalie should always stay in our half

id29
71
71
tactical generation:
generation by inverting wasp
same synchronous grammar is used for both generation and id29


query     what is city / answer(city)
nl:
mrl:

id29:
72
72
generation by inverting wasp
same procedure for lexical acquisition
chart generator very similar to chart parser, but treats mrl as input
log-linear probabilistic model inspired by pharaoh (koehn et al., 2003), a phrase-based mt system
uses a bigram language model for target nl
resulting system is called wasp-1
73
73

geoquery (nist score; english)
74
74

robocup (nist score; english)

contiguous phrases only
similar human evaluation results in terms of fluency and adequacy
lstms for id29
lstm encoder/decoder model has effectively been used to map natural language sentences into formal meaning representations (dong & lapata, 2016; kocisky, et al., 2016).
exploits neural attention and methods for decoding into semantic trees rather than sequences.
75
conclusions
id29 maps nl sentences to completely formal mrs.
semantic parsers can be effectively learned from supervised corpora consisting of only sentences paired with their formal mrs (and possibly also sapts).
learning methods can be based on:
adding semantics to an existing statistical syntactic parser and then using id152.
using id166 with string kernels to recognize concepts in the nl and then composing them into a complete mr using the mrl grammar.
using probabilistic synchronous context-free grammars to learn an nl/mr grammar that supports both id29 and generation.
