1
cs 388: 
natural language processing
introduction
raymond j. mooney
university of texas at austin
natural language processing
nlp is the branch of computer science focused on developing systems that allow computers to communicate with people using everyday language.
also called computational linguistics
also concerns how computational methods can aid the understanding of human language

2
related areas
artificial intelligence
formal language (automata) theory
machine learning
linguistics
psycholinguistics
cognitive science
philosophy of language
3
4
communication
the goal in the production and comprehension of natural language is communication.
communication for the speaker:
intention: decide when and what information should be transmitted (a.k.a. content selection, strategic generation).  may require planning and reasoning about agents    goals and beliefs.
generation: translate the information to be communicated (in internal logical representation or    language of thought   ) into string of words in desired natural language (a.k.a. surface realization, tactical generation).
synthesis: output the string in desired modality, text or speech.
5
communication (cont)
communication for the hearer:
perception: map input modality to a string of words, e.g. id42 (ocr) or id103.
analysis: determine the information content of the string.
syntactic interpretation (parsing): find the correct parse tree showing the phrase structure of the string.
semantic interpretation: extract the (literal) meaning of the string (logical form).
pragmatic interpretation: consider effect of the overall context on altering the literal meaning of a sentence.
incorporation: decide whether or not to believe the content of the string and add it to the kb.
6
communication (cont)
7
syntax, semantic, pragmatics
syntax concerns the proper ordering of words and its affect on meaning.
the dog bit the boy.
the boy bit the dog.
* bit boy dog the the.
colorless green ideas sleep furiously.
semantics concerns the (literal) meaning of words, phrases, and sentences.
   plant    as a photosynthetic organism
   plant    as a manufacturing facility
   plant    as the act of sowing
pragmatics concerns the overall communicative and social context and its effect on interpretation.
the ham sandwich wants another beer. (co-reference, anaphora)
john thinks vanilla.  (ellipsis) 
8
modular comprehension

 semantics

9
ambiguity
natural language is highly ambiguous and must be disambiguated.
i saw the man on the hill with a telescope.
i saw the grand canyon flying to la.
time flies like an arrow.
horse flies like a sugar cube.
time runners like a coach.
time cars like a porsche.
10
ambiguity is ubiquitous
id103
   recognize speech    vs.    wreck a nice beach   
   youth in asia    vs.    euthanasia   
syntactic analysis
   i ate spaghetti with chopsticks    vs.    i ate spaghetti with meatballs.   
semantic analysis
   the dog is in the pen.    vs.    the ink is in the pen.   
   i put the plant in the window    vs.    ford put the plant in mexico   
pragmatic analysis
from    the pink panther strikes again   :
clouseau: does your dog bite? 
hotel clerk: no. 
clouseau: [bowing down to pet the dog] nice doggie. 
[dog barks and bites clouseau in the hand] 
clouseau: i thought you said your dog did not bite! 
hotel clerk: that is not my dog. 
11
ambiguity is explosive
ambiguities compound to generate enormous numbers of possible interpretations.
in english, a sentence ending in n prepositional phrases has over 2n syntactic interpretations (cf. catalan numbers).
   i saw the man with the telescope   : 2 parses
   i saw the man on the hill with the telescope.   : 5 parses
   i saw the man on the hill in texas with the telescope   :     14 parses
   i saw the man on the hill in texas with the telescope at noon.   : 42 parses
   i saw the man on the hill in texas with the telescope at noon on monday     132 parses

12
humor and ambiguity
many jokes rely on the ambiguity of language:
groucho marx: one morning i shot an elephant in my pajamas.  how he got into my pajamas, i   ll never know.
she criticized my apartment, so i knocked her flat.
noah took all of the animals on the ark in pairs. except the worms, they came in apples.
policeman to little boy:    we are looking for a thief with a bicycle.    little boy:    wouldn   t you be better using your eyes.   
why is the teacher wearing sun-glasses. because the class is so bright.
why is language ambiguous?
having a unique linguistic expression for every possible conceptualization that could be conveyed would make language overly complex and linguistic expressions unnecessarily long.
allowing resolvable ambiguity permits shorter linguistic expressions, i.e. data compression.
language relies on people   s ability to use their knowledge and id136 abilities to properly resolve ambiguities.
infrequently, disambiguation fails, i.e. the compression is lossy.
13
natural languages vs. computer languages
ambiguity is the primary difference between natural and computer languages.
formal programming languages are designed to be unambiguous, i.e. they can be defined by a grammar that produces a unique parse for each sentence in the language.
programming languages are also designed for efficient (deterministic) parsing, i.e. they are deterministic context-free languages (dcfls).
a sentence in a dcfl can be parsed in o(n) time where n is the length of the string.


14
natural language tasks
processing natural language text involves many various syntactic, semantic and pragmatic tasks in addition to other problems.
15
syntactic tasks

id40
breaking a string of characters (graphemes) into a sequence of words.
in some written languages (e.g. chinese) words are not separated by spaces.
even in english, characters other than white-space can be used to separate words [e.g. , ; . - : ( ) ]
examples from english urls:
jumptheshark.com     jump the shark .com
myspace.com/pluckerswingbar  
        myspace .com pluckers wing bar
        myspace .com plucker swing bar
   
morphological analysis
morphology is the field of linguistics that studies the internal structure of words. (wikipedia)
a morpheme is the smallest linguistic unit that has semantic meaning (wikipedia)
 e.g.    carry   ,    pre   ,    ed   ,    ly   ,    s   
morphological analysis is the task of segmenting a word into its morphemes:
carried      carry + ed (past tense)
independently      in + (depend + ent) + ly 
googlers      (google + er) + s (plural)
unlockable      un + (lock + able)  ?
                            (un + lock) + able  ?
part of speech (pos) tagging
annotate each word in a sentence with a part-of-speech.



useful for subsequent syntactic parsing and id51.
i     ate   the  spaghetti  with   meatballs.  
pro  v   det        n       prep        n
john  saw  the  saw  and  decided  to  take  it     to   the   table.
pn      v   det    n   con      v     part  v   pro prep det    n
phrase chunking
find all non-recursive noun phrases (nps) and verb phrases (vps) in a sentence.
[np i]  [vp ate]  [np the  spaghetti]  [pp with]   [np meatballs].
[np he ] [vp reckons ] [np the current account deficit ] [vp will narrow ] [pp to ] [np only # 1.8 billion ] [pp in ] [np september ] 
syntactic parsing
produce the correct syntactic parse tree for a sentence.
semantic tasks

23
id51 (wsd)
words in natural language usually have a fair number of different possible meanings.
ellen has a strong interest in computational linguistics.
ellen pays a large amount of interest on her credit card.
for many tasks (id53, translation), the proper sense of each ambiguous word in a sentence must be determined.

24
id14 (srl)
for each clause, determine the semantic role played by each noun phrase that is an argument to the verb.
agent   patient   source   destination   instrument
john drove mary from austin to dallas in his toyota prius.
the hammer broke the window.
also referred to a    case role analysis,       thematic analysis,    and    shallow id29   
25
id29
a semantic parser maps a natural-language sentence to a complete, detailed semantic representation (logical form).
for many applications, the desired output is immediately executable by another program.
example: mapping an english database query to prolog:
   how many cities are there in the us?
   answer(a, count(b, (city(b), loc(b, c), 
                                      const(c, countryid(usa))),
                                a))


id123
determine whether one natural language sentence entails (implies) another under an ordinary interpretation.
id123 problems 
from pascal challenge
pragmatics/discourse tasks

id2/
co-reference
determine which phrases in a document refer to the same underlying entity.
john put the carrot on the plate and ate it. 

bush started the war in iraq.  but the president needed the consent of congress.
some cases require difficult reasoning.
today was jack's birthday. penny and janet went to the store. they were going to get presents. janet decided to get a kite. "don't do that," said penny. "jack has a kite. he will make you take it back." 





ellipsis resolution
frequently words and phrases are omitted from sentences when they can be inferred from context.
"wise men talk because they have something to say; 
fools, because they have to say something.    (plato)
"wise men talk because they have something to say; 
fools talk because they have to say something.    (plato)
other tasks

32
information extraction (ie)
identify phrases in language that refer to specific types of entities and relations in text.
id39 is task of identifying names of people, places, organizations, etc. in text.
      people    organizations    places
michael dell is the ceo of  dell computer corporation and lives in austin texas. 
id36 identifies specific relations between entities.
michael dell is the ceo of  dell computer corporation and lives in austin texas.




id53
directly answer natural language questions based on information presented in a corpora of textual documents (e.g. the web).
when was barack obama born?   (factoid)
august 4, 1961
who was president when barack obama was born?
john f. kennedy
how many presidents have there been since barack obama was born?
9

reading comprehension
read a passage of text and answer questions about it.
example from stanford squad dataset.
34
text summarization
produce a short summary of a longer document or article.
article: with a split decision in the final two primaries and a flurry of superdelegate endorsements, sen. barack obama sealed the democratic presidential nomination last night after a grueling and history-making campaign against sen. hillary rodham clinton that will make him the first african american to head a major-party ticket. before a chanting and cheering audience in st. paul, minn., the first-term senator from illinois savored what once seemed an unlikely outcome to the democratic race with a nod to the marathon that was ending and to what will be another hard-fought battle, against sen. john mccain, the presumptive republican nominee   .
summary:  senator barack obama was declared the presumptive democratic presidential nominee.
machine translation (mt)
translate a sentence from one natural language to another.
hasta la vista, beb         
   until we see each other again, baby.
37
ambiguity resolution 
is required for translation
syntactic and semantic ambiguities must be properly resolved for correct translation:
   john plays the guitar.           john toca la guitarra.   
   john plays soccer.           john juega el f  tbol.   
an apocryphal story is that an early mt system gave the following results when translating from english to russian and then back to english:
   the spirit is willing but the flesh is weak.                         the liquor is good but the meat is spoiled.   
   out of sight, out of mind.           invisible idiot.    
38
resolving ambiguity
choosing the correct interpretation of linguistic utterances  requires knowledge of:
syntax
an agent is typically the subject of the verb
semantics
michael and ellen are names of people
austin is the name of a city (and of a person)
toyota is a car company and prius is a brand of car
pragmatics
world knowledge
credit cards require users to pay financial interest
agents must be animate and a hammer is not animate

39
manual knowledge acquisition
traditional,    rationalist,    approaches to language processing require human specialists to specify and formalize the required knowledge.
manual knowledge engineering, is difficult, time-consuming, and error prone.
   rules    in language have numerous exceptions and irregularities.
   all grammars leak.   : edward sapir (1921)
manually developed systems were expensive to develop and their abilities were limited and    brittle    (not robust).
40
automatic learning approach
use machine learning methods to automatically acquire the required knowledge from appropriately annotated text corpora.
variously referred to as the    corpus based,       statistical,    or    empirical    approach.
statistical learning methods were first applied to id103 in the late 1970   s and became the dominant approach in the 1980   s.
during the 1990   s, the statistical training approach expanded and came to dominate almost all areas of nlp.
41
learning approach
42
advantages of the learning approach
large amounts of electronic text are now available.
annotating corpora is easier and requires less expertise than manual knowledge engineering.
learning algorithms have progressed to be able to handle large amounts of data and produce accurate probabilistic knowledge.
the probabilistic knowledge acquired allows robust processing that handles linguistic regularities as well as exceptions.
43
the importance of id203
unlikely interpretations of words can combine to generate spurious ambiguity:
   the a are of i    is a valid english noun phrase (abney, 1996)
   a    is an adjective for the letter a
   are    is a noun for an area of land (as in hectare)
   i    is a noun for the letter i
   time flies like an arrow    has 4 parses, including those meaning:
insects of a variety called    time flies    are fond of a particular arrow.
a command to record insects    speed in the manner that an arrow would.
some combinations of words are more likely than others:
   vice president gore    vs.    dice precedent core   
statistical methods allow computing the most likely interpretation by combining probabilistic evidence from a variety of uncertain knowledge sources.
44
human id146
human children obviously learn languages from experience.
however, it is controversial to what extent prior knowledge of    universal grammar    (chomsky, 1957) facilitates this acquisition process.
computational studies of language learning may help us to understand human language learning, and to elucidate to what extent language learning must rely on prior grammatical knowledge due to the    poverty of the stimulus.   
existing empirical results indicate that a great deal of linguistic knowledge can be effectively acquired from reasonable amounts of real linguistic data without specific knowledge of a    universal grammar.    

pipelining problem
assuming separate independent components for id103, syntax, semantics, pragmatics, etc. allows for more convenient modular software development.
however, frequently constraints from    higher level    processes are needed to disambiguate    lower level    processes.
example of syntactic disambiguation relying on semantic disambiguation:
at the zoo, several men were showing a group of students various types of flying animals. suddenly, one of the students hit the man with a bat.

45
pipelining problem (cont.)
if a hard decision is made at each stage, cannot backtrack when a later stage indicates it is incorrect.
if attach    with a bat    to the verb    hit    during syntactic analysis, then cannot reattach it to     man    after    bat    is disambiguated during later semantic or pragmatic processing.
46
increasing module bandwidth
if each component produces multiple scored interpretations, then later components can rerank these interpretations.
47
words
parse
trees
literal
meanings
meaning
(contextualized)

sound 
waves

problem: number of interpretations grows combinatorially.
solution: efficiently encode combinations of interpretations.
word lattices
compact parse forests
global integration/
joint id136
integrated interpretation that combines phonetic/syntactic/semantic/pragmatic constraints.
48
difficult to design and implement.
potentially computationally complex.

sound 
waves
meaning
(contextualized)

early history: 1950   s
shannon (the father of id205)  explored probabilistic models of natural language (1951).
chomsky (the extremely influential linguist) developed formal models of syntax, i.e. finite state and context-free grammars (1956).
first computational parser developed at u penn as a cascade of finite-state transducers (joshi, 1961; harris, 1962).
bayesian methods developed for id42 (ocr) (bledsoe & browning, 1959).
history: 1960   s
work at mit ai lab on id53 (baseball) and dialog (eliza).
semantic network models of language for id53 (simmons, 1965).
first electronic corpus collected, brown corpus, 1 million words (kucera and francis, 1967).
bayesian methods used to identify document authorship (the federalist papers) (mosteller & wallace, 1964).


history: 1970   s
   natural language understanding    systems developed that tried to support deeper semantic interpretation.
shrdlu (winograd, 1972) performs tasks in the    blocks world    based on nl instruction.
schank et al. (1972, 1977) developed systems for conceptual representation of language and for understanding short stories using hand-coded knowledge of scripts, plans, and goals. 
prolog programming language developed to support logic-based parsing (colmeraurer, 1975).
initial development of id48 (id48s) for statistical id103 (baker, 1975; jelinek, 1976).
history: 1980   s
development of more complex (mildly context sensitive) grammatical formalisms, e.g. unification grammar, hpsg, tree-adjoning grammar.
symbolic work on discourse processing and nl generation.
initial use of statistical (id48) methods for syntactic analysis (id52) (church, 1988).


history: 1990   s
rise of statistical methods and empirical evaluation causes a    scientific revolution    in the field.
initial annotated corpora developed for training and testing systems for id52, parsing, wsd, information extraction, mt, etc.
first id151 systems developed at ibm for canadian hansards corpus (brown et al., 1990).
first robust statistical parsers developed (magerman, 1995; collins, 1996; charniak, 1997).
first systems for robust information extraction developed (e.g. muc competitions).

history: 2000   s
increased use of a variety of ml methods, id166s, id28 (i.e. max-ent), crf   s, etc.
continued developed of corpora and competitions on shared data.
trec q/a
senseval/semeval
conll shared tasks (ner, srl   ) 
increased emphasis on unsupervised, semi-supervised, and active learning as alternatives to purely supervised learning.
shifting focus to semantic tasks such as wsd, srl, and id29.
history: 2010   s
grounded language: connecting language to perception and action.
image and video description
visual id53 (vqa)
id176 (hri) in nl
deep learning: neural network learning with many layers or recurrence.
long short term memory (lstm) recurrent neural networks using encoder/decoder sequence-to-sequence mapping.
id4 (id4)
spreading to syntactic/id29 and most other nlp tasks.

55
relevant scientific conferences
association for computational linguistics (acl)
north american association for computational linguistics (naacl)
international conference on computational linguistics (coling)
empirical methods in natural language processing (emnlp)
conference on computational natural language learning (conll)
 international association for machine translation (imta)
56
