1
1
1
cs 388: 
natural language processing
machine translation
raymond j. mooney
university of texas at austin
machine translation
automatically translate one natural language into another.
2
     mary didn   t slap the green witch.

maria no di   una bofetada a la bruja verde.

3
ambiguity resolution 
is required for translation
syntactic and semantic ambiguities must be properly resolved for correct translation:
   john plays the guitar.           john toca la guitarra.   
   john plays soccer.           john juega el f  tbol.   
an apocryphal story is that an early mt system gave the following results when translating from english to russian and then back to english:
   the spirit is willing but the flesh is weak.                         the liquor is good but the meat is spoiled.   
   out of sight, out of mind.           invisible idiot.    
word alignment
shows mapping between words in one language and the other.
4
     mary didn   t slap the green witch.

maria no di   una bofetada a la bruja verde.
translation quality
achieving literary quality translation is very difficult.
existing mt systems can generate rough translations that frequently at least convey the gist of a document.
high quality translations possible when specialized to narrow domains, e.g. weather forcasts.
some mt systems used in computer-aided translation in which a bilingual human post-edits the output to produce more readable accurate translations.
frequently used to aid localization of software interfaces and documentation to adapt them to other languages.

5
linguistic issues making mt difficult
morphological issues with agglutinative, fusion and polysynthetic languages with complex word structure.
syntactic variation between svo (e.g. english), sov (e.g. hindi), and vso (e.g. arabic)  languages.
svo languages use prepositions
sov languages use postpositions
pro-drop languages regularly omit subjects that must be inferred.
6
lexical gaps
some words in one language do not have a corresponding term in the other.
rivi  re (river that flows into ocean) and fleuve (river that does not flow into ocean) in french
schedenfraude (feeling good about another   s pain) in german.
oyakoko (filial piety) in japanese

7
vauquois triangle
8
interlingua
semantic
structure
semantic
structure
syntactic
structure
syntactic
structure
words
words
source language
target language
srl &
wsd
parsing
semantic
parsing
tactical
generation


direct translation
syntactic transfer
semantic transfer

direct transfer
morphological analysis
mary didn   t slap the green witch.     
    mary do:past not slap the green witch.
lexical transfer
mary do:past not slap the green witch.

maria no dar:past una bofetada a la verde bruja.
lexical reordering
maria no dar:past una bofetada a la bruja verde.
morphological generation
maria no di   una bofetada a la bruja verde.

9

syntactic transfer
simple lexical reordering does not adequately handle more dramatic reordering such as that required to translate from an svo to an sov language.
need syntactic transfer rules that map parse tree for one language into one for another.
english to spanish:   
np     adj nom       np     nom adj
 english to japanese:
vp     v np       vp     np v
pp     p np       pp     np p
10
semantic transfer
some transfer requires semantic information.
semantic roles can determine how to properly express information in another language.
in chinese, pps that express a goal, destination, or benefactor occur before the verb but those expressing  a recipient occur after the verb.
transfer rule
english to chinese
vp     v pp[+benefactor]       vp     pp[+benefactor] v
11
statistical mt
manually encoding comprehensive bilingual lexicons and transfer rules is difficult.
smt acquires knowledge needed for translation from a parallel corpus or bitext that contains the same set of documents in two languages.
the canadian hansards (parliamentary proceedings in french and english) is a well-known parallel corpus.  
first align the sentences in the corpus based on simple methods that use coarse cues like sentence length to give bilingual sentence pairs.
12
picking a good translation
a good translation should be faithful and correctly convey the information and tone of the original source sentence.
a good translation should also be fluent, grammatically well structured and readable in the target language.
final objective:
13
id87
based on analogy to information-theoretic model used to decode messages transmitted via a communication channel that adds errors.
assume that source sentence was generated by a    noisy    transformation of some target language sentence and then use bayesian analysis to recover  the most likely target sentence that generated it.
14
translate foreign language sentence f=f1, f2,    fm  to an
english sentence    = e1, e2,    ei that maximizes p(e | f)
bayesian analysis of noisy channel
15
translation model    language model


a decoder determines the most probable
  translation    given f
language model
use a standard id165 language model for p(e).
can be trained on a large, unsupervised mono-lingual corpus for the target language e.
could use a more sophisticated pid18 language model to capture long-distance dependencies.
terabytes of web data have been used to build a large 5-gram model of english.

16
word alignment
directly constructing phrase alignments is difficult, so rely on first constructing word alignments.
can learn to align from supervised word alignments, but human-aligned bitexts are rare and expensive to construct.
typically use an unsupervised em-based approach to compute a word alignment from unannotated parallel corpus. 
17
one to many alignment
to simplify the problem, typically assume each word in f aligns to 1 word in e (but assume each word in e may generate more than one word in f).
some words in f may be generated by the null element of e.
therefore, alignment can be specified by a vector a giving, for each word in f, the index of the word in e which generated it. 
18
null  mary didn   t slap the green witch.

       maria no di   una bofetada a la bruja verde.
0           1            2             3        4         5            6
1           2         3      3               3           0    4       6            5
ibm model 1
first model proposed in seminal paper by brown et al. in 1993 as part of candide, the first complete smt system.
assumes following simple generative model of producing f from e=e1, e2,    ei 
choose length, j, of f sentence: f=f1, f2,    fj
choose a 1 to many alignment a=a1, a2,    aj 
for each position in f, generate a word fj from the aligned word in e: eaj
19
verde.
1           2        3      3                3         0   4       6            5
sample ibm model 1 generation
20
null  mary didn   t slap the green witch.
0           1            2             3        4         5            6
maria
no
di  
una
bofetada
a
la
bruja
computing p(f | e) in ibm model 1
assume some length distribution p(j | e) 
assume all alignments are equally likely. since there are (i + 1)j  possible alignments:
21
assume t(fx,ey) is the id203 of translating ey as fx, therefore:
determine p(f | e) by summing over all alignments:  
decoding for ibm model 1
goal is to find the most probable alignment given a parameterized model.
22
since translation choice for each position j is independent, 
the product is maximized by maximizing each term:
id48-based word alignment
ibm model 1 assumes all alignments are equally likely and does not take into account locality:
if two words appear together in one language, then their translations are likely to appear together in the result in the other language.
an alternative model of word alignment based on an id48 model does account for locality by making longer jumps in switching from translating one word to another less likely.
id48 model
assumes the hidden state is the specific word occurrence ei in e currently being translated (i.e. there are i states, one for each word in e).
assumes the observations from these hidden states are the possible translations fj of ei. 
generation of f from e then consists of moving to the initial e word to be translated, generating a translation, moving to the next word to be translated, and so on.
sample id48 generation
  mary didn   t slap the green witch.
maria
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
maria
no
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
maria
no
di  
      1            2             3        4         5            6



sample id48 generation
  mary didn   t slap the green witch.
maria
no
di  
una
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
maria
no
di  
una
bofetada
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
maria
no
di  
una
bofetada
a
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
maria
no
di  
una
bofetada
a
la
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
maria
no
di  
una
bofetada
a
la
bruja
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
verde.
maria
no
di  
una
bofetada
a
la
bruja
      1            2             3        4         5            6


sample id48 generation
  mary didn   t slap the green witch.
verde.
maria
no
di  
una
bofetada
a
la
bruja
      1            2             3        4         5            6
id48 parameters
transition and observation parameters of states for id48s for all possible source sentences are    tied    to reduce the number of free parameters that have to be estimated. 
observation probabilities: bj(fi)=p(fi | ej) the same for all states representing an occurrence of the same english word.
state transition probabilities: aij = s(j   i) the same for all transitions that involve the same jump width (and direction).
computing p(f | e) in the id48 model 
given the observation and state-transition probabilities, p(f | e) (observation likelihood) can be computed using the standard forward algorithm for id48s.
36
decoding for the id48 model 
use the standard viterbi algorithm to efficiently compute the most likely alignment (i.e. most likely state sequence).
37
training word alignment models
both the ibm model 1 and id48 model can be trained on a parallel corpus to set the required parameters.
for supervised (hand-aligned) training data, parameters can be estimated directly using frequency counts.
for unsupervised training data, em can be used to estimate parameters, e.g. baum-welch for the id48 model.
39
sketch of em algorithm for
word alignment 
randomly set model parameters. 
   (making sure they represent legal distributions)
until converge (i.e. parameters no longer change) do:
      e step: compute the id203 of all possible        
                   alignments of the training data using the current 
                   model. 
      m step: use these alignment id203 estimates to 
                    re-estimate values for all of the parameters.
note: use id145 (as in baum-welch)
to avoid explicitly enumerating all possible alignments
sample em trace for alignment
(ibm model 1 with no null generation)
green house
casa verde
the house
la casa
training
corpus
translation
probabilities
assume uniform
initial probabilities
compute
alignment
probabilities
p(a, f | e)
1/3 x 1/3 = 1/9
1/3 x 1/3 = 1/9
1/3 x 1/3 = 1/9
1/3 x 1/3 = 1/9
normalize 
to get
p(a | f, e)
example cont.
1/2
1/2
1/2
1/2
compute 
weighted 
translation 
counts
normalize
rows to sum 
to one to 
estimate p(f | e)
example cont.
1/2 x 1/4=1/8
recompute
alignment
probabilities
p(a, f | e)
1/2 x 1/2=1/4
1/2 x 1/2=1/4
1/2 x 1/4=1/8
normalize 
to get
p(a | f, e)


continue em iterations until translation
             parameters converge
translation
probabilities
decoding
goal is to find a translation that maximizes the product of the translation and language models.
43
cannot explicitly enumerate and test the combinatorial space of all possible translations.
the optimal decoding problem for all reasonable model   s (e.g. ibm model 1) is np-complete.
heuristically search the space of translations using a*, beam-search, etc. to approximate the solution to this difficult optimization problem.

evaluating mt
human subjective evaluation is the best but is time-consuming and expensive.
automated evaluation comparing the output to multiple human reference translations is cheaper and correlates with human judgements.
44
human evaluation of mt
ask humans to estimate mt output on several dimensions.
fluency: is the result grammatical, understandable, and readable in the target language. 
fidelity: does the result correctly convey  the information in the original source language.
adequacy:  human judgment on a fixed scale. 
bilingual judges given source and target language.
monolingual judges given reference translation and mt result.
informativeness: monolingual judges must answer questions about the source sentence given only the mt translation (task-based evaluation).
45
computer-aided translation evaluation
edit cost: measure the number of changes that a human translator must make to correct the mt output.
number of words changed
amount of time taken to edit
number of keystrokes needed to edit
46
automatic evaluation of mt
collect one or more human reference translations of the source.
compare mt output to these reference translations.
score result based on similarity to the reference translations.
id7
nist
ter
meteor
47
id7
determine number of id165s of various sizes that the mt output shares with the reference translations.
compute a modified precision measure of the id165s in mt result.
48
id7 example
49

















cand 1: mary no slap the witch green
cand 2: mary did not give a smack to a green witch.
ref 1: mary did not slap the green witch.
ref 2: mary did not smack the green witch.
ref 3: mary did not hit a green sorceress. 
cand 1 unigram precision:  5/6



id7 example
50


cand 1 bigram precision:  1/5

cand 1: mary no slap the witch green.
cand 2: mary did not give a smack to a green witch.
ref 1: mary did not slap the green witch.
ref 2: mary did not smack the green witch.
ref 3: mary did not hit a green sorceress. 
id7 example
51



















clip match count of  each id165 to maximum
count of the id165 in any single reference
translation







ref 1: mary did not slap the green witch.
ref 2: mary did not smack the green witch.
ref 3: mary did not hit a green sorceress. 
cand 1: mary no slap the witch green.
cand 2: mary did not give a smack to a green witch.
cand 2 unigram precision:  7/10


id7 example
52














ref 1: mary did not slap the green witch.
ref 2: mary did not smack the green witch.
ref 3: mary did not hit a green sorceress. 
cand 2 bigram precision:  4/9


cand 1: mary no slap the witch green.
cand 2: mary did not give a smack to a green witch.
modified id165 precision
average id165 precision over all id165s up to size n (typically 4) using geometric mean.
53
cand 1:
cand 2:
brevity penalty
not easy to compute recall to complement precision since there are multiple alternative gold-standard references and don   t need to match all of them.
instead, use a penalty for translations that are shorter than the reference translations.
define effective reference length, r, for each sentence as the length of the reference sentence with the largest number of id165 matches.  let  c be the candidate sentence length.


54
id7 score 
final id7 score:  id7 = bp     p
     cand 1: mary no slap the witch green.
best ref: mary did not slap the green witch.


cand 2: mary did not give a smack to a green witch. 
best ref: mary did not smack the green witch.
55
id7 score issues
id7 has been shown to correlate with human evaluation when comparing outputs from different smt systems.
however, it is does not correlate with human judgments when comparing smt systems with manually developed mt (systran) or mt with human translations.
other mt id74 have been proposed that claim to overcome some of the limitations of id7.
56
syntax-based 
id151
recent smt methods have adopted a syntactic transfer approach. 
improved results demonstrated for translating between more distant language pairs, e.g. chinese/english.
57
58
synchronous grammar
multiple parse trees in a single derivation.
used by (chiang, 2005; galley et al., 2006).
describes the hierarchical structures of a sentence and its translation, and also the correspondence between their sub-parts.
59


x     x            /  what is x
chinese:
english:

synchronous productions
has two rhss, one for each language.
60
syntax-based mt example
input:                                   
61
syntax-based mt example
x
x

input:                                  
62
syntax-based mt example
what is        x
x


x
x                 


input:                                   

x     x           / what is x

63
syntax-based mt example
x              
what is        x
the capital        x
x




x
x                 




input:                                  

x     x        / the capital x

64
syntax-based mt example
x              
what is        x
the capital        x
of       x
x






x
x                 




x           


input:                                  

x     x     / of x

65
syntax-based mt example
x              
what is        x
the capital        x
of       x
ohio
x







            

x
x                 




x           


input:                                   
x                  / ohio
66
syntax-based mt example
x              
what is        x
the capital        x
of       x
ohio
x







            

x
x                 




x           


input:                                   
output: what is the capital of ohio?
synchronous derivations
and translation model
need to make a probabilistic version of synchronous grammars to create a translation model for p(f | e).
each synchronous production rule is given a weight   i that is used in a maximum-id178 (log linear) model.
parameters are learned to maximize the conditional log-likelihood of the training data.
67
id4 (id4)
encoder/decoder framework maps sentence in source language to a "deep vector" then another lstm maps this vector to a sentence in the target language
68
f1, f2,   ,fn
encoder
lstm
e1, e2,   ,em
hn
decoder
lstm




train model "end to end" on sentence-aligned parallel corpus.
id4 with language model
vanilla lstm approach does not use a language model so does not exploit monolingual data for the target language.
can integrate an lstm language model using    deep fusion.   

69
tm
tm
decoder predicts the next word from a concatenation of the hidden states of both the translation and language lstm models.
softmax
concatenate
conclusions
mt methods can usefully exploit various amounts of syntactic and semantic processing along the vauquois triangle.
statistical mt methods can automatically learn a translation system from a parallel corpus.
typically use a noisy-channel model to exploit both a bilingual translation model and a monolingual language model.
neural lstm methods are currently the state-of-the-art.  
70
