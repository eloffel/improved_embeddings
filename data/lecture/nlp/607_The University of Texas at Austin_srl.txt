1
1
cs 388: 
natural language processing:
id14
raymond j. mooney
university of texas at austin
2
id14
(srl)
for each clause, determine the semantic role played by each noun phrase that is an argument to the verb.
agent   patient   source   destination   instrument
john drove mary from austin to dallas in his toyota prius.
the hammer broke the window.
also referred to a    case role analysis,       thematic analysis,    and    shallow id29   
3
semantic roles
origins in the linguistic notion of case (fillmore, 1968)
a variety of semantic role labels have been proposed, common ones are:
agent: actor of an action
patient: entity affected by the action
instrument: tool used in performing action.
beneficiary: entity for whom action is performed
source: origin of the affected entity
destination: destination of the affected entity


4
use of semantic roles
semantic roles are useful for various tasks.
id53
   who    questions usually use agents
   what    question usually  use patients
   how    and    with what    questions usually use instruments
   where    questions frequently use sources and destinations.
   for whom    questions usually use beneficiaries
   to whom    questions usually use destinations
machine translation generation
semantic roles are usually expressed using particular, distinct syntactic constructions in different languages.
5
srl and syntactic cues 
frequently semantic role is indicated by a particular syntactic position (e.g. object of a particular preposition).
agent:  subject
patient: direct object
instrument: object of    with    pp
beneficiary: object of    for    pp
source: object of    from    pp
destination: object of    to    pp
however, these are preferences at best:
the hammer hit the window.
the book was given to mary by john.
john went to the movie with mary.
john bought the car for $21k.
john went to work by bus.


6
selectional restrictions
selectional restrictions are constraints that certain verbs place on the filler of certain semantic roles.
agents should be animate
beneficiaries should be animate
instruments should be tools
patients of    eat    should be edible
sources and destinations of    go    should be places.
sources and destinations of    give    should be animate.
taxanomic abstraction hierarchies or ontologies (e.g. hypernym links in id138) can be used to determine if such constraints are met.
   john    is a    human    which is a    mammal    which is a    vertebrate    which is an    animate   

7
use of sectional restrictions
selectional restrictions can help rule in or out certain semantic role assignments.
   john bought the car for $21k   
beneficiaries should be animate
instrument of a    buy    should be money
   john went to the movie with mary   
instrument should be inanimate
   john drove mary to school in the van    
       john drove the van to work with mary.   
instrument of a    drive    should be a vehicle 
8
selectional restrictions and
syntactic ambiguity
many syntactic ambiguities like pp attachment can be resolved using selectional restrictions.
   john ate the spaghetti with meatballs.   
      john ate the spaghetti with chopsticks.   
instruments should be tools
patients of    eat    must be edible
   john hit the man with a dog.   
      john hit the man with a hammer.    
instruments should be tools 
9
selectional restrictions and
id51
many lexical ambiguities can be resolved using selectional restrictions.
ambiguous nouns
   john wrote it with a pen.   
instruments of    write    should be writingimplements
   the bat ate the bug.   
agents (particularly of    eat   ) should be animate
patients of    eat    should be edible
ambiguous verbs
   john fired the secretary.   
       john fired the rifle.   
patients of dischargeweapon should be weapons
patients of ceaseemploment should be human

10
empirical methods for srl
difficult to acquire all of the selectional restrictions and taxonomic knowledge needed for srl.
difficult to efficiently and effectively apply knowledge in an integrated fashion to simultaneously determine correct parse trees, word senses, and semantic roles.
statistical/empirical methods can be used to automatically acquire and apply the knowledge needed for effective and efficient srl.
11
srl as sequence labeling
srl can be treated as an sequence labeling  problem.
for each verb, try to extract a value for each of the possible semantic roles for that verb.
employ any of the standard sequence labeling methods
token classification
id48s
crfs
12
srl with parse trees
parse trees help identify semantic roles through exploiting syntactic clues like    the agent is usually the subject of the verb   .
parse tree is needed to identify the true subject.
s
npsg                                   vpsg
det     n        pp
prep         nppl
the     man
by     the store near the dog
ate the apple.
















   the man by the store near the dog ate an apple.   
   the man    is the agent of    ate    not    the dog   .
13
srl with parse trees 
assume that a syntactic parse is available.
for each predicate (verb), label each node in the parse tree as either not-a-role or one of the possible semantic roles.
s
color code:
not-a-role
agent  
patient   
source   
destination   
instrument
beneficiary

14
srl as parse node classification
treat problem as classifying parse-tree nodes.
can use any machine-learning classification method.
critical issue is engineering the right set of features for the classifier to use.
15
features for srl
phrase type: the syntactic label of the candidate role filler (e.g. np).
parse tree path: the path in the parse tree between the predicate and the candidate role filler.
16
parse tree path feature: example 1
s
np                           vp


np            pp


the

prep   np


with

the

v        np


bit

a


big

dog

girl

boy
det  a  n



det  a  n



  

adj a


  

det  a  n



  



path feature value:

   v     vp     s     np
17
parse tree path feature: example 2
s
np                           vp


np            pp


the

prep   np


with

the

v        np


bit

a


big

dog

girl

boy
det  a  n



det  a  n



  

adj a


  

det  a  n



  



path feature value:

v     vp     s     np     pp     np
18
features for srl
phrase type: the syntactic label of the candidate role filler (e.g. np).
parse tree path: the path in the parse tree between the predicate and the candidate role filler.
position: does candidate role filler precede or follow the predicate in the sentence?
voice: is the predicate an active or passive verb?
head word: what is the head word of the candidate role filler?

19
head word feature example
there are standard syntactic rules for determining which word in a phrase is the head.
s
np                            vp


np            pp


the

prep   np


with

the

v        np


bit

a


big

dog

girl

boy
det  a  n



det  a  n



  

adj a


  

det  a  n



  


head word:
      dog

20
complete srl example
s


21
issues in parse node classification
many other useful features have been proposed.
if the parse-tree path goes through a pp, what is the preposition?
results may violate constraints like    an action has at most one agent   ?
use some method to enforce constraints when making final decisions. i.e. determine the most likely assignment of roles that also satisfies a set of known constraints.
due to errors in syntactic parsing, the parse tree is likely to be incorrect.
try multiple top-ranked parse trees and somehow combine results.
integrate syntactic parsing and srl.


22
more issues in parse node classification
break labeling into two steps:
first decide if node is an argument or not.
if it is an argument, determine the type.
23
srl datasets
framenet: 
developed at univ. of california at berkeley
based on notion of frames
propbank:
developed at univ. of pennsylvania
based on elaborating their treebank
salsa:
developed at universit  t des saarlandes
german version of framenet
24
framenet
project at uc berkeley led by chuck fillmore for developing a database of frames, general semantic concepts with an associated set of roles.
roles are specific to frames, which are    invoked    by multiple words, both verbs and nouns.
judgement frame
invoked by: v: blame, praise, admire; n: fault, admiration
roles: judge, evaluee, and reason
specific frames chosen, and then sentences that employed these frames selected from the british national corpus and annotated by linguists for semantic roles.
initial version: 67 frames, 1,462 target words,                             _                     49,013 sentences, 99,232 role fillers


25
framenet results
gildea and jurafsky (2002) performed srl experiments with initial framenet data.
assumed correct frames were identified and the task was to fill their roles.
automatically produced syntactic analyses using collins (1997) statistical parser.
used simple bayesian method with smoothing to classify parse nodes.
achieved 80.4% correct role assignment. increased to 82.1% when frame-specific roles were collapsed to 16 general thematic categories.
26
propbank
project at u penn lead by martha palmer to add semantic roles to the id32.
roles (arg0 to argn) specific to each individual verb to avoid having to agree on a universal set.
arg0 basically    agent   
arg1 basically    patient   
annotated over 1m words of wall street journal text with existing gold-standard parse trees.
statistics:
43,594 sentences       99,265 propositions (verbs + roles)
3,324 unique verbs    262,281 role assignments

27
connl srl shared task
conll (conference on computational natural language learning) is the annual meeting for the signll (special interest group on natural language learning) of acl.
each year, conll has a    shared task    competition.
propbank id14 was used as the shared task for conll-04 and conll-05. 
in conll-05, 19 teams participated.
28
conll-05 learning approaches 
maximum id178 (8 teams)
id166 (7 teams)
snow (1 team)  (ensemble of enhanced id88s)
id90 (1 team)
adaboost (2 teams) (ensemble of id90)
nearest neighbor (2 teams)
tree crf (1 team)
combination of approaches (2 teams)
29
conll experimental method
trained on 39,832 wsj sentences
tested on 2,416 wsj sentences
also tested on 426 brown corpus sentences to test generalizing beyond financial news.
metrics:
precision: (# roles correctly assigned) / (# roles assigned)
recall: (# roles correctly assigned) / (total # of roles)
f-measure: harmonic mean of precision and recall

30
best result from conll-05
univ. of illinois system based on snow with global constraints enforced using integer id135.
31
issues in srl
how to properly integrate syntactic parsing, wsd, and role assignment so they all aid each other.
how can srl be used to aid end-use applications:
id53
machine translation
id111
