vector-space (distributional)
lexical semantics
2
information retrieval system
ir
system
the vector-space model
graphic representation
term weights: term frequency
term weights: inverse document frequency
tf-idf weighting
similarity measure
cosine similarity measure
vector-space (distributional)
lexical semantics
represent word meanings as points (vectors) in a (high-dimensional) euclidian space.
dimensions encode aspects of the context in which the word appears (e.g. how often it co-occurs with another specific word).
   you will know a word by the company that it keeps.     (j.r. firth, 1957)
semantic similarity defined as distance between points in this semantic space.

10
sample lexical vector space
11
dog
cat
man
woman
bottle
cup
water
rock
computer
robot
simple word vectors
for a given target word, w, create a bag-of-words    document    of all of the words that co-occur with the target word in a large corpus.
window of k words on either side.
all words in the sentence, paragraph, or document.
for each word, create a (tf-idf weighted) vector from the    document    for that word.
compute semantic relatedness of words as the cosine similarity of their vectors.
12
other contextual features
use syntax to move beyond simple bag-of-words features.
produced typed (edge-labeled) dependency parses for each sentence in a large corpus.
for each target word, produce features for it having specific dependency links to specific other words (e.g. subj=dog, obj=food, mod=red)
13
other feature weights
14
id84
word-based features result in extremely high-dimensional spaces that can easily result in over-fitting.
reduce the dimensionality of the space by using various mathematical techniques to create a smaller set of k new dimensions that most account for the variance in the data.
singular value decomposition (svd) used in latent semantic analysis (lsa)
principle component analysis (pca)
15
sample id84
16





















sample id84
17





















neural id97
(mikolov et al., 2013)
learn an    embedding    of words that supports effective prediction of surrounding    skip gram    of words.
18
skip-gram id97
network architecture
19
softmax classifier
id97 math
softmax classifier predicts surrounding words from a id27.


train to maximize the id203 of skip-gram predictions.
20
evaluation of vector-space 
lexical semantics
have humans rate the semantic similarity of a large set of word pairs.
(dog, canine): 10; (dog, cat): 7; (dog, carrot): 3; (dog, knife): 1
compute vector-space similarity of each pair.
compute correlation coefficient (pearson or spearman) between human and machine ratings.
21
toefl synonymy test
lsa shown to be able to pass toefl synonymy test.
22
vector-space 
word sense induction (wsi)
create a context-vector for each individual occurrence of the target word, w.
cluster these vectors into k groups.
assume each group represents a    sense    of the word and compute a vector for this sense by taking the mean of each cluster.
23
sample word sense induction
24








occurrence vectors for    bat   
hit

flew
wooden
player
cave
vampire
ate
baseball
sample word sense induction
25








occurrence vectors for    bat   
hit

flew
wooden
player
cave
vampire
ate
baseball
+
+
word sense and vector semantics
having one vector per word ignores the impact of homonymous senses.
similarity of ambiguous words violates the triangle inequality.
26

club


bat
association
b
a
c
c      a + b
multi-prototype vector space models
(reisinger & mooney, 2010)
do wsi and create a multiple sense-specific vectors for ambiguous words.
similarity of two words is the maximum similarity of sense vectors of each.
27

club-1


bat-1
association
club-2


bat-2

mouse-1
vector-space word meaning in context
compute a semantic vector for an individual occurrence of a word based on its context.
combine a standard vector for a word with vectors representing the immediate context.
28
example using dependency context
29
fired
hunter
gun
nsubj
dobj
fired
boss
secretary
nsubj
dobj
compute vector for nsubj-boss by summing contextual vectors for all word occurrences that have    boss    as a subject.
compute vector for dobj-secretary by summing contextual vectors for all word occurrences that have    secretary    as a direct object. 
compute    in context    vector for    fire    in    boss fired secretary    by  adding nsubj-boss and dobj-secretary vectors to the general vector for    fire   
compositional vector semantics
compute vector meanings of phrases and sentences by combining (composing) the vector meanings of its words.
simplest approach is to use vector addition or component-wise multiplication to combine word vectors.
evaluate on human judgements of sentence-level semantic similarity (semantic textual similarity, sts, semeval competition).
30
other vector semantics computations
31
   skip-thought vectors    (kiros et al., nips 2015)
use lstms to encode whole sentences into lower-dimensional vectors.
vectors trained to predict previous and next sentences.

32
sentence-level 
neural language models
   jim jumped from the plane and 
opened his parachute.   
encoder
lstm
decoder
lstm
s
e
n
t.

v
e
c
t
o
r
   jim landed on the ground.   
conclusions
a word   s meaning can be represented as a vector that encodes distributional information about the contexts in which the word tends to occur.
lexical semantic similarity can be judged by comparing vectors (e.g. cosine similarity).
vector-based word senses can be automatically induced by id91 contexts.
contextualized vectors for word meaning can be constructed by combining lexical and contextual vectors.
33
