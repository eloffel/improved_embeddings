1
1
cs 388: 
natural language processing:
part-of-speech tagging,
sequence labeling, and
id48 (id48s)
raymond j. mooney
university of texas at austin
2
id52
annotate each word in a sentence with a part-of-speech marker.
lowest level of syntactic analysis.


useful for subsequent syntactic parsing and id51.
john  saw  the  saw  and  decided  to  take  it     to   the   table.
nnp vbd dt  nn  cc  vbd     to vb  prp in dt    nn
3
english pos tagsets
original brown corpus used a large set of 87 pos tags.
most common in nlp today is the id32 set of 45 tags.
tagset used in these slides.
reduced from the brown set for use in the context of a parsed corpus (i.e. treebank).
the c5  tagset used for the british national corpus (bnc) has 61 tags.


4
english parts of speech
noun (person, place or thing)
singular (nn):  dog, fork
plural (nns):  dogs, forks
proper (nnp, nnps): john, springfields
personal pronoun (prp): i, you, he, she, it
wh-pronoun  (wp): who, what
verb (actions and processes)
base, infinitive (vb):  eat
past tense (vbd):  ate
gerund (vbg):  eating
past participle (vbn):  eaten
non 3rd person singular present tense (vbp): eat
3rd person singular present tense: (vbz): eats
modal (md): should, can
to (to): to (to eat)
5
english parts of speech (cont.)
adjective (modify nouns)
basic (jj): red, tall
comparative (jjr): redder, taller
superlative (jjs): reddest, tallest
adverb (modify verbs)
basic (rb): quickly
comparative (rbr): quicker
superlative (rbs): quickest
preposition (in): on, in, by, to, with
determiner:
basic (dt) a, an, the
wh-determiner (wdt): which, that
coordinating conjunction (cc): and, but, or,
particle (rp): off (took off), up (put up)



closed vs. open class 
closed class categories are composed of a small, fixed set of grammatical function words for a given language.
pronouns, prepositions, modals, determiners, particles, conjunctions
open class categories have large number of words and new ones are easily invented.
nouns (googler, textlish), verbs (google), adjectives (geeky), abverb (automagically) 
6
7
ambiguity in id52
   like    can be a verb or a preposition
i like/vbp candy.
time flies like/in an arrow.
   around    can be a preposition, particle, or adverb
i bought it at the shop around/in the corner.
i never got around/rp to getting a car.
a new prius costs around/rb $25k.
8
id52 process
usually assume a separate initial id121 process that separates and/or disambiguates punctuation, including detecting sentence boundaries.
degree of ambiguity in english (based on brown corpus)
11.5% of word types are ambiguous.
40% of word tokens are ambiguous.
average id52 disagreement amongst expert human judges for the id32 was 3.5%
based on correcting the output of an initial automated tagger, which was deemed to be more accurate than tagging from scratch.
baseline: picking the most frequent tag for each specific word type gives about 90% accuracy
93.7% if use model for unknown words for id32 tagset.

9
id52 approaches
rule-based: human crafted rules based on lexical and other linguistic knowledge.
learning-based: trained on human annotated corpora like the id32.
statistical models:  hidden markov model (id48), maximum id178 markov model (memm), conditional random field (crf)
rule learning: transformation based learning (tbl)
neural networks: recurrent networks like long short term memory (lstms)
generally, learning-based approaches have been found to be more effective overall, taking into account the total amount of human expertise and effort involved.
10
classification learning
typical machine learning addresses the problem of classifying a feature-vector description into a fixed number of classes.
there are many standard learning methods for this task:
id90 and rule learning
na  ve bayes and id110s
id28 / maximum id178 (maxent)
id88 and neural networks
support vector machines (id166s)
nearest-neighbor / instance-based
11
beyond classification learning
standard classification problem assumes individual cases are disconnected and independent (i.i.d.: independently and identically distributed).
many nlp problems do not satisfy this assumption and involve making many connected decisions, each resolving a different ambiguity, but which are mutually dependent.
more sophisticated learning and id136 techniques are needed to handle such situations in general.
12
sequence labeling problem
many nlp problems can viewed as sequence labeling.
each token in a sequence is assigned a label.
labels of tokens are dependent on the labels of other tokens in the sequence, particularly their neighbors (not i.i.d).
13
information extraction
identify phrases in language that refer to specific types of entities and relations in text.
id39 is task of identifying names of people, places, organizations, etc. in text.
      people    organizations    places
michael dell is the ceo of  dell computer corporation and lives in austin texas. 
extract pieces of information relevant to a specific  application, e.g. used car ads:
       make    model   year    mileage   price
for sale, 2002 toyota prius,  20,000 mi, $15k or best offer. available starting july 30, 2006.
	
14
id14
for each clause, determine the semantic role played by each noun phrase that is an argument to the verb.
agent   patient   source   destination   instrument
john drove mary from austin to dallas in his toyota prius.
the hammer broke the window.
also referred to a    case role analysis,       thematic analysis,    and    shallow id29   
15
bioinformatics
sequence labeling also valuable in labeling genetic sequences in genome analysis.
extron   intron
agctaacgttcgatacggattacagcct
16
problems with sequence labeling as classification
not easy to integrate information from category of tokens on both sides.
difficult to propagate uncertainty between decisions and    collectively    determine the most likely joint assignment of categories to all of the tokens in a sequence. 
17
probabilistic sequence models
probabilistic sequence models allow integrating uncertainty over multiple, interdependent classifications and collectively determine the most likely global assignment.
two standard models
hidden markov model  (id48)
conditional random field (crf)
18
markov model / markov chain
a finite state machine with probabilistic state transitions.
makes markov assumption that next state only depends on the current state and independent of previous history.
19
sample markov model for pos

0.95

0.9

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25

start

0.1
0.5
0.4





det
noun
propnoun
verb
20
sample markov model for pos

0.95

0.9

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25

start

0.1
0.5
0.4





det
noun
propnoun
verb
p(propnoun verb det noun) = 0.4*0.8*0.25*0.95*0.1=0.0076
21
hidden markov model
probabilistic generative model for sequences.
assume an underlying set of hidden (unobserved, latent) states in which the model can be (e.g. parts of speech).
assume probabilistic transitions between states over time (e.g. transition from pos to another pos as sequence is generated).
assume a probabilistic generation of tokens from states (e.g. words generated for each pos).
22
sample id48 for pos

start

0.1
0.5
0.4

23
sample id48 generation

start

0.1
0.5
0.4

24
sample id48 generation
propnoun
john
mary
alice
jerry
tom
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1



0.25


start

0.1
0.5
0.4

25
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a



verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john 

start

0.1
0.5
0.4

26
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john 

start

0.1
0.5
0.4

27
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john bit 

start

0.1
0.5
0.4

28
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john bit 

start

0.1
0.5
0.4

29
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john bit the

start

0.1
0.5
0.4

30
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john bit the

start

0.1
0.5
0.4

31
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john bit the apple

start

0.1
0.5
0.4

32
sample id48 generation
propnoun
john
mary
alice
jerry
tom
noun
cat
dog
car
pen
bed
apple
a
the
the
the
that
a
the
a
verb
bit
ate
saw
played
hit

0.95

0.9
gave

0.05

stop

0.5

0.1

0.8

0.1
0.1


0.25

0.25
john bit the apple

start

0.1
0.5
0.4

33
formal definition of an id48
a set of n +2 states s={s0,s1,s2,     sn, sf}
distinguished start state:  s0
distinguished final state: sf
a set of m possible observations v={v1,v2   vm}
a state transition id203 distribution a={aij}



observation id203 distribution for each state j b={bj(k)}

total parameter set   ={a,b}
34
id48 generation procedure
to generate a sequence of t observations:  o = o1 o2     ot
set initial state q1=s0
for t = 1 to t
      transit to another state qt+1=sj based on transition 
          distribution aij for state qt
      pick an observation ot=vk based on being in state qt using 
          distribution bqt(k)

35
three useful id48 tasks
observation likelihood: to classify and order sequences.
most likely state sequence (decoding): to tag each token in a sequence with a label.
maximum likelihood training (learning): to train models to fit empirical training data.
36
id48: observation likelihood
given a sequence of observations, o, and a model with a set of parameters,   , what is the id203 that this observation was generated by this model: p(o|   ) ?
allows id48 to be used as a language model: a formal probabilistic model of a language that assigns a id203 to each string saying how likely that string was to have been generated by the language.
useful for two tasks:
sequence classification
most likely sequence

37
sequence classification
assume an id48 is available for each category (i.e. language).
what is the most likely category for a given observation sequence, i.e. which category   s id48 is most likely to have generated it?
used in id103 to find most likely word model to have generate a given  sound or phoneme sequence.
austin
boston


38
most likely sequence
of two or more possible sequences, which one was most likely generated by a given model?
used to score alternative word sequence interpretations in id103.
ordinary english
39
id48: observation likelihood
na  ve solution
consider all possible state sequences, q, of length t that the model could have traversed in generating the given observation sequence.
compute the id203 of a given state sequence from a, and multiply it by the probabilities of generating each of given observations in each of the corresponding states in this sequence to get p(o,q|   ) = p(o| q,   ) p(q|   ) .
sum this over all possible state sequences to get p(o|   ).
computationally complex: o(tnt).






40
id48: observation likelihood
efficient solution
due to the markov assumption, the id203 of being in any state at any given time t only relies on the id203 of being in each of the possible states at time t   1.
forward algorithm: uses id145 to exploit this fact to efficiently compute observation likelihood in o(tn2) time.
compute a forward trellis that compactly and implicitly encodes information about all possible state paths.
forward trellis 
41

s1

s2

sn
   
   
   



   
   
   

s0

sf



   
   
   
                
                
                
                
t1
t2
t3
tt-1
tt
continue forward in time until reaching final time point and sum id203 of ending in final state.
forward probabilities
let    t(j) be the id203 of being in state j after seeing the first t observations (by summing over all initial paths leading to j).
42
forward step
43
consider all possible ways of getting to sj at time t by coming from all possible states si and determine id203 of each.
sum these to get the total id203 of being in state sj  at time t  while accounting for the first t    1 observations.
then multiply by the id203 of actually observing ot in sj.
computing the forward probabilities
initialization

recursion


termination
44
forward computational complexity
requires only o(tn2) time to compute the id203 of an observed sequence given a model.
exploits the fact that all state sequences must merge into one of the n possible states at any point in time and the markov assumption that only the last state effects the next one.

45
46
most likely state sequence (decoding)
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

47
most likely state sequence
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

det   noun  propnoun verb 

48
most likely state sequence
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

det   noun  propnoun verb 


49
most likely state sequence
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

det   noun  propnoun verb 



50
most likely state sequence
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

det   noun  propnoun verb 




51
most likely state sequence
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

det   noun  propnoun verb 





52
most likely state sequence
given an observation sequence, o, and a model,   ,  what is the most likely state sequence,q=q1,q2,   qt, that generated this sequence from this model?
used for sequence labeling, assuming each state corresponds to a tag, it determines the globally best assignment of tags to all tokens in a sequence using a principled approach grounded in id203 theory.

det   noun  propnoun verb 






53
id48: most likely state sequence
efficient solution
obviously, could use na  ve algorithm based on examining every possible state sequence of length t.
id145 can also be used to exploit the markov assumption and efficiently determine the most likely state sequence for a given observation and model.
standard procedure is called the viterbi algorithm (viterbi, 1967) and also has o(n2t) time complexity.
viterbi scores
recursively compute the id203 of the most likely subsequence of states that accounts for the first t observations and ends in state sj. 
54
also record    backpointers    that subsequently allow backtracing the most probable state sequence.
btt(j) stores the state at time t-1 that maximizes the id203 that system was in state sj at time t (given the observed sequence).
computing the viterbi scores
initialization

recursion


termination
55
analogous to forward algorithm except take max instead of sum
computing the viterbi backpointers
initialization

recursion


termination
56
final state in the most probable state sequence. follow 
backpointers to initial state to construct full sequence.
viterbi backpointers 
57

s1

s2

sn
   
   
   



   
   
   

s0

sf



   
   
   
                
                
                
                
t1
t2
t3
tt-1
tt













viterbi backtrace 
58

s1

s2

sn
   
   
   



   
   
   

s0

sf



   
   
   
                
                
                
                
t1
t2
t3
tt-1
tt














most likely sequence: s0 sn s1 s2    s2 sf     
id48 learning
supervised learning:  all training sequences are completely labeled (tagged).
unsupervised learning: all training sequences are unlabelled (but generally know the number of tags, i.e. states).
semisupervised learning: some training sequences are labeled, most are unlabeled.
59
60
supervised id48 training
if training sequences are labeled (tagged) with the underlying state sequences that generated them, then the parameters,   ={a,b} can all be estimated directly.
supervised parameter estimation
estimate state transition probabilities based on tag bigram and unigram statistics in the labeled data.


estimate the observation probabilities based on tag/word co-occurrence statistics in the labeled data.


use appropriate smoothing if training data is sparse.
61
learning and using id48 taggers
use a corpus of labeled sequence data to easily construct an id48 using supervised training.
given a novel unlabeled test sequence to tag, use the viterbi algorithm to predict the most likely (globally optimal) tag sequence.


62
evaluating taggers
train on training set of labeled sequences.
possibly tune parameters based on performance on a development set.
measure accuracy on a disjoint test set.
generally measure tagging accuracy, i.e. the percentage of tokens tagged correctly.
accuracy of most modern pos taggers, including id48s is 96   97% (for penn tagset trained on about 800k words) .
generally matching human agreement level.

63
64
unsupervised 
maximum likelihood training
ah  s  t  e  n
a  s  t  i  n
oh  s  t  u  n
eh  z  t  en 


.
.
.

training sequences
65
maximum likelihood training
given an observation sequence, o, what set of parameters,   , for a given model maximizes the id203 that this data was generated from this model (p(o|   ))?
used to train an id48 model and properly induce its parameters from a set of training data.
only need to have an unannotated observation sequence (or set of sequences) generated from the model. does not need to know the correct state sequence(s) for the observation sequence(s). in this sense, it is unsupervised.

id47


simple proof from definition of id155:
qed:
(def. cond. prob.)
(def. cond. prob.)
maximum likelihood vs. 
maximum a posteriori (map)
the map parameter estimate is the most likely given the observed data, o.
if all parameterizations are assumed to be equally likely a priori, then id113 and map are the same.
if parameters are given priors (e.g. gaussian or lapacian with zero mean), then map is a principled way to perform smoothing or id173.
68
id48: maximum likelihood training
efficient solution
there is no known efficient algorithm for finding the parameters,   , that truly maximizes p(o|   ).
however, using iterative re-estimation, the baum-welch algorithm (a.k.a. forward-backward) , a version of a standard statistical procedure called expectation maximization (em), is able to locally maximize p(o|   ).
in practice, em is able to find a good set of parameters that provide a good fit to the training data in many cases.
69
em algorithm
iterative method for learning probabilistic categorization model from unsupervised data.
initially assume random assignment of examples to categories.
learn an initial probabilistic model by estimating model parameters     from this randomly labeled data.
iterate following two steps until convergence:
expectation (e-step): compute p(ci | e) for each example given the current model, and probabilistically re-label the examples based on these posterior id203 estimates.
maximization (m-step): re-estimate the model parameters,    , from the probabilistically re-labeled data.

70
em
unlabeled examples
assign random probabilistic labels to unlabeled data
initialize:
71
em

give soft-labeled training data to a probabilistic learner
initialize:
72
em


 produce a probabilistic classifier
initialize:
73
em

relabel unlabled data using the trained classifier





e step:
74
em


continue em iterations until probabilistic labels on unlabeled data converge.
retrain classifier on relabeled data
m step:
75
sketch of baum-welch  (em) algorithm 
for training id48s
assume an id48 with n states.
randomly set its parameters   =(a,b) 
   (making sure they represent legal distributions)
until converge (i.e.    no longer changes) do:
      e step:  use the forward/backward procedure to  
                    determine the id203 of various possible 
                    state sequences for generating the training data
      m step: use these id203 estimates to 
                    re-estimate values for all of the parameters   
see textbook for detailed equations for e and m steps
em properties
each iteration changes the parameters in a way that is guaranteed to increase the likelihood of the data: p(o|   ).
anytime algorithm: can stop at any time prior to convergence to get approximate solution.
converges to a local maximum.

semi-supervised learning
em algorithms can be trained with a mix of labeled and unlabeled data.
em basically predicts a probabilistic (soft) labeling of the instances and then iteratively retrains using supervised learning on these predicted labels (   self training   ).
em can also exploit supervised data: 
1) use supervised learning on labeled data to initialize the parameters (instead of initializing them randomly).
2) use known labels for supervised data instead of predicting soft labels for these examples during retraining iterations.
78
semi-supervised em
unlabeled examples







79
semi-supervised em


80
semi-supervised em


81
semi-supervised em
unlabeled examples







82
semi-supervised em


continue retraining iterations until probabilistic labels on unlabeled data converge.
semi-supervised results
use of additional unlabeled data improves on supervised learning when amount of labeled data is very small and amount of unlabeled data is large.
can degrade performance when there is sufficient labeled data to learn a decent model and when unsupervised learning tends to create labels that are incompatible with the desired ones.
there are negative results for semi-supervised id52 since unsupervised learning tends to learn semantic labels (e.g. eating verbs, animate nouns) that are better at predicting the data than purely syntactic labels (e.g. verb, noun).
conclusions
id52 is the lowest level of syntactic analysis.
it is an instance of sequence labeling, a collective classification task that also has applications in information extraction, phrase chunking, id14, and bioinformatics.
id48s are a standard generative probabilistic model for sequence labeling that allows for efficiently computing the globally most probable sequence of labels and supports supervised, unsupervised and semi-supervised learning.
