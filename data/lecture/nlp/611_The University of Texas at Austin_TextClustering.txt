1
text id91

2
id91
partition unlabeled examples into disjoint subsets of clusters, such that:
examples within a cluster are very similar
examples in different clusters are very different
discover new categories in an unsupervised manner (no sample category labels provided).
3
.
id91 example
4
hierarchical id91
build a tree-based hierarchical taxonomy (dendrogram) from a set of unlabeled examples.




recursive application of a standard id91 algorithm can produce a hierarchical id91.


5
aglommerative vs. divisive id91
aglommerative (bottom-up) methods start with each example in its own cluster and iteratively combine them to form larger and larger clusters.
divisive (partitional, top-down) separate all examples immediately into clusters.
6
direct id91 method
direct id91 methods require a specification of the number of clusters, k, desired.
a id91 evaluation function assigns a real-value quality measure to a id91.
the number of clusters can be determined automatically by explicitly generating id91s for multiple values of k and choosing the best result according to a id91 evaluation function.
7
hierarchical agglomerative id91 (hac)
assumes a similarity function for determining the similarity of two instances.
starts with all instances in a separate cluster and then repeatedly joins the two clusters that are most similar until there is only one cluster.
the history of merging forms a binary tree or hierarchy.
8
hac algorithm
start with all instances in their own cluster.
until there is only one cluster:
      among the current clusters, determine the two 
           clusters, ci and cj, that are most similar.
      replace ci and cj with a single cluster ci     cj 
9
cluster similarity
assume a similarity function that determines the similarity of two instances: sim(x,y).
cosine similarity of document vectors.
how to compute similarity of two clusters each possibly containing multiple instances?
single link: similarity of two most similar members.
complete link: similarity of two least similar members.
group average: average similarity between members.
10
single link agglomerative id91
use maximum similarity of pairs:


can result in    straggly    (long and thin) clusters due to chaining effect.
appropriate in some domains, such as id91 islands. 	
11
single link example















12
complete link agglomerative id91
use minimum similarity of pairs:


makes more    tight,    spherical clusters that are typically preferable.
13
complete link example















14
computational complexity
in the first iteration, all hac methods need to compute similarity of all pairs of n individual instances which is o(n2).
in each of the subsequent n   2 merging iterations, it must compute the distance between the most recently created cluster and all other existing clusters.
in order to maintain an overall o(n2) performance, computing similarity to each other cluster must be done in constant time.

15
computing cluster similarity
after merging ci and cj, the similarity of the resulting cluster to any other cluster, ck, can be computed by:
single link:

complete link:
16
group average agglomerative id91
use average similarity across all pairs within the merged cluster to measure the similarity of two clusters.


compromise between single and complete link.
averaged across all ordered pairs in the merged cluster instead of unordered pairs between the two clusters (to encourage tighter final clusters).
17
computing group average similarity
assume cosine similarity and normalized vectors with unit length.
always maintain sum of vectors in each cluster.

compute similarity of clusters in constant time:

18
non-hierarchical id91
typically must provide the number of desired clusters, k.
randomly choose k instances as seeds, one per cluster.  
form initial clusters based on these seeds.
iterate, repeatedly reallocating instances to different clusters to improve the overall id91.
stop when id91 converges or after a fixed number of iterations. 
19
id116
assumes instances are real-valued vectors.
clusters based on centroids, center of gravity, or mean of points in a cluster, c:


reassignment of instances to clusters is based on distance to the current cluster centroids.
20
distance metrics
euclidian distance (l2 norm):

l1 norm:

cosine similarity (transform to a distance by subtracting from 1):
21
id116 algorithm
let d be the distance measure between instances.
select k random instances {s1, s2,    sk} as seeds.
until id91 converges or other stopping criterion:
      for each instance xi:
          assign xi to the cluster cj such that d(xi, sj) is minimal.
      (update the seeds to the centroid of each cluster)
      for each cluster cj
             sj =    (cj) 
22
id116 example
(k=2)












reassign clusters
converged!
23
time complexity
assume computing distance between two instances is o(m) where m is the dimensionality of the vectors.
reassigning clusters: o(kn) distance computations, or o(knm).
computing centroids: each instance vector gets added once to some centroid: o(nm).
assume these two steps are each done once for i iterations:  o(iknm).
linear in all relevant factors, assuming a fixed number of iterations, more efficient than o(n2) hac.
24
id116 objective
the objective of id116 is to minimize the total sum of the squared distance of every point to its corresponding cluster centroid.
finding the global optimum is np-hard.
the id116 algorithm is guaranteed to converge a local optimum.
25
seed choice
results can vary based on random seed selection.
some seeds can result in poor convergence rate, or convergence to sub-optimal id91s.
select good seeds using a heuristic or the results of another method.
26
buckshot algorithm
combines hac and id116 id91.
first randomly take a sample of instances of size    n 
run group-average hac on this sample, which takes only o(n) time.
use the results of hac as initial seeds for id116.
overall algorithm is o(n) and avoids problems of bad seed selection.

27
text id91
hac and id116 have been applied to text in a straightforward way.
typically use normalized, tf/idf-weighted vectors and cosine similarity.
optimize computations for sparse vectors.
applications:
during retrieval, add other documents in the same cluster as the initial retrieved documents to improve recall.
id91 of results of retrieval to present more organized results to the user (   la northernlight folders).
automated production of hierarchical taxonomies of documents for browsing purposes (   la yahoo & dmoz).
28
soft id91
id91 typically assumes that each instance is given a    hard    assignment to exactly one cluster.
does not allow uncertainty in class membership or for an instance to belong to more than one cluster.
soft id91 gives probabilities that an instance belongs to each of a set of clusters.
each instance is assigned a id203 distribution across a set of discovered categories (probabilities of all categories must sum to 1).
29
expectation maximumization (em)
probabilistic method for soft id91.
direct method that assumes k clusters:{c1, c2,    ck} 
soft version of id116.
assumes a probabilistic model of categories that allows computing p(ci | e) for each category, ci, for a given example, e.
for text, typically assume a na  ve-bayes category model.
parameters     = {p(ci), p(wj | ci): i   {1,   k}, j    {1,   ,|v|}}
30
em algorithm
iterative method for learning probabilistic categorization model from unsupervised data.
initially assume random assignment of examples to categories.
learn an initial probabilistic model by estimating model parameters     from this randomly labeled data.
iterate following two steps until convergence:
expectation (e-step): compute p(ci | e) for each example given the current model, and probabilistically re-label the examples based on these posterior id203 estimates.
maximization (m-step): re-estimate the model parameters,    , from the probabilistically re-labeled data.

31
em
unlabeled examples
assign random probabilistic labels to unlabeled data
initialize:
32
em

give soft-labeled training data to a probabilistic learner
initialize:
33
em


 produce a probabilistic classifier
initialize:
34
em

relabel unlabled data using the trained classifier





e step:
35
em


continue em iterations until probabilistic labels on unlabeled data converge.
retrain classifier on relabeled data
m step:
36
learning from probabilistically labeled data 
instead of training data labeled with    hard    category labels, training data is labeled with    soft    probabilistic category labels.
when estimating model parameters     from training data, weight counts by the corresponding id203 of the given category label.
for example, if p(c1 | e) = 0.8 and p(c2 | e) = 0.2,        each word wj in e contributes only 0.8 towards the counts n1 and n1j, and 0.2 towards the counts n2 and n2j .

37
na  ve bayes em
randomly assign examples probabilistic category labels.
use standard na  ve-bayes training to learn a probabilistic model 
      with parameters     from the labeled data.
until convergence or until maximum number of iterations reached:
          e-step: use the na  ve bayes model     to compute p(ci | e) for
                each category and example, and re-label each example 
                using these id203 values as soft category labels.
          m-step: use standard na  ve-bayes training to re-estimate the 
                parameters     using these new probabilistic category labels.
38
semi-supervised learning
for supervised categorization, generating labeled training data is expensive.
idea: use unlabeled data to aid supervised categorization.
use em in a semi-supervised mode by training em on both labeled and unlabeled data.
train initial probabilistic model on user-labeled subset of data instead of randomly labeled unsupervised data. 
labels of user-labeled examples are    frozen    and never relabeled during em iterations.
labels of unsupervised data are constantly probabilistically relabeled by em.
39
semi-supervised em
unlabeled examples







40
semi-supervised em


41
semi-supervised em


42
semi-supervised em
unlabeled examples







43
semi-supervised em


continue retraining iterations until probabilistic labels on unlabeled data converge.
44
semi-supervised em results
experiments on assigning messages from 20 usenet newsgroups their proper newsgroup label.
with very few labeled examples (2 examples per class), semi-supervised em significantly improved predictive accuracy:
27%  with 40 labeled messages only.
43%  with 40 labeled  + 10,000 unlabeled messages.
with more labeled examples, semi-supervision can actually decrease accuracy, but refinements to standard em can help prevent this.
must weight labeled data appropriately more than unlabeled data.
for semi-supervised em to work, the    natural id91 of data    must be consistent with the desired categories
failed when applied to english id52 (merialdo, 1994)
45
semi-supervised em example
assume    catholic    is present in both of the labeled documents for soc.religion.christian, but    baptist    occurs in none of the labeled data for this class.
from labeled data, we learn that    catholic    is highly indicative of the    christian    category.
when labeling unsupervised data, we label several documents with    catholic    and    baptist    correctly with the    christian    category.
when retraining, we learn that    baptist    is also indicative of a    christian    document.
final learned model is able to correctly assign documents containing only    baptist    to    christian   .
46
issues in id91
how to evaluate id91?
internal: 
tightness and separation of clusters (e.g. id116 objective)
fit of probabilistic model to data
external
compare to known class labels on benchmark data
improving search to converge faster and avoid local minima.
overlapping id91.
47
conclusions
unsupervised learning induces categories from unlabeled data.
there are a variety of approaches, including:
hac
id116
em
semi-supervised learning uses both labeled and unlabeled data to improve results.
