1
1
cs 388: 
natural language processing:
statistical parsing
raymond j. mooney
university of texas at austin
statistical parsing
statistical parsing uses a probabilistic model of syntax in order to assign probabilities to each parse tree.
provides principled approach to resolving syntactic ambiguity.
allows supervised learning of parsers from tree-banks of parse trees provided by human linguists.
also allows unsupervised learning of parsers from unannotated text, but the accuracy of such parsers has been limited.
2
3
probabilistic id18
(pid18)
a pid18 is a probabilistic version of a id18 where each production has a id203.
probabilities of all productions rewriting a given non-terminal must add to 1, defining a distribution for each non-terminal.
string generation is now probabilistic where production probabilities are used to non-deterministically select a production for rewriting a given non-terminal.
simple pid18 for atis english
s     np vp                     
s     aux np vp               
s     vp                           
np     pronoun
np     proper-noun
np     det nominal
nominal     noun
nominal     nominal noun
nominal     nominal pp
vp     verb
vp     verb np
vp     vp pp
pp     prep np
grammar

0.8
0.1
0.1
0.2
0.2
0.6
0.3
0.2
0.5
0.2
0.5
0.3
1.0
prob
+



+
+
+
1.0
1.0
1.0
1.0
det     the | a   | that | this
            0.6  0.2  0.1    0.1
noun     book | flight | meal | money
                0.1     0.5      0.2     0.2
verb     book | include | prefer
               0.5      0.2        0.3
pronoun     i    | he | she | me
                   0.5  0.1  0.1    0.3
proper-noun     houston | nwa
                              0.8         0.2
aux     does
             1.0
prep     from | to   | on | near | through
             0.25  0.25  0.1    0.2     0.2
lexicon
5
sentence id203
assume productions for each node are chosen independently.
id203 of derivation is the product of the probabilities of its productions.





p(d1) = 0.1 x 0.5 x 0.5 x 0.6 x 0.6 x 
              0.5 x 0.3 x 1.0 x 0.2 x 0.2 x 
              0.5 x 0.8
               =  0.0000216

d1
s
vp
verb          np
     det    nominal
nominal     pp
book
prep        np
through
houston
proper-noun
the
flight









noun







0.5
0.5
0.6
0.6
0.5
1.0
0.2
0.3
0.5
0.2
0.8
0.1
syntactic disambiguation
resolve ambiguity by picking most probable parse tree.
6
6

d2
vp
verb          np
     det    nominal
book
prep        np
through
houston
proper-noun
the
flight







noun







0.5
0.5
0.6
0.6
1.0
0.2
0.3
0.5
0.2
0.8
s
vp
0.1
pp
0.3
p(d2) = 0.1 x 0.3 x 0.5 x 0.6 x 0.5 x
              0.6 x 0.3 x 1.0 x 0.5 x 0.2 x
              0.2 x 0.8
               =  0.00001296
sentence id203
id203 of a sentence is the sum of the probabilities of all of its derivations.

7
p(   book the flight through houston   ) = 
p(d1) + p(d2) = 0.0000216 + 0.00001296
                       = 0.00003456
                                   

8
three useful pid18 tasks
observation likelihood: to classify and order sentences.
most likely derivation: to determine the most likely parse tree for a sentence.
maximum likelihood training: to train a pid18 to fit empirical training data.
pid18: most likely derivation
there is an analog to the viterbi algorithm to efficiently determine the most probable derivation (parse tree) for a sentence.
10
pid18: most likely derivation
there is an analog to the viterbi algorithm to efficiently determine the most probable derivation (parse tree) for a sentence.
probabilistic cky
cky can be modified for pid18 parsing by including in each cell a id203 for each non-terminal.
cell[i,j] must retain the most probable derivation of each constituent (non-terminal) covering words i +1 through j together with its associated id203.
when transforming the grammar to cnf, must set production probabilities to preserve the id203 of derivations.

 probabilistic grammar conversion
s     np vp
s     aux np vp

s     vp



np     pronoun

np     proper-noun

np     det nominal
nominal     noun 

nominal     nominal noun
nominal     nominal pp
vp     verb

vp     verb np
vp     vp pp
pp     prep np
original grammar
chomsky normal form
s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
          0.01     0.004    0.006
s     verb np
s     vp pp
np      i   |  he  |  she |  me
          0.1   0.02  0.02    0.06
np     houston | nwa
             0.16           .04
np     det nominal
nominal     book | flight | meal | money
                      0.03    0.15   0.06     0.06
nominal     nominal noun
nominal     nominal pp
vp     book | include | prefer
             0.1      0.04        0.06
vp     verb np
vp     vp pp
pp     prep np
0.8
0.1

0.1



0.2
 0.2
 0.6
0.3

0.2
0.5
0.2

0.5
0.3
1.0
0.8
0.1
1.0


0.05
0.03




0.6


0.2
0.5


0.5
0.3
1.0
probabilistic cky parser
13
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
probabilistic cky parser
14
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054

vp:.5*.5*.054
     =.0135
probabilistic cky parser
15
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
probabilistic cky parser
16
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
probabilistic cky parser
17
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
probabilistic cky parser
18
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024
probabilistic cky parser
19
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024

np:.6*.6*
       .0024
     =.000864
probabilistic cky parser
20
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024

np:.6*.6*
       .0024
     =.000864
s:.05*.5*
     .000864
   =.0000216
probabilistic cky parser
21
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024

np:.6*.6*
       .0024
     =.000864
s:.0000216
s:.03*.0135*
    .032
  =.00001296
probabilistic cky parser
22
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024

np:.6*.6*
       .0024
     =.000864
s:.0000216
pick most probable
parse, i.e. take max to
combine probabilities
of multiple derivations
of each constituent in
each cell.
23
pid18: observation likelihood
there is an analog to forward algorithm for id48s called the inside algorithm for efficiently determining how likely a string is to be produced by a pid18.
can use a pid18 as a language model to choose between alternative sentences for id103 or machine translation. 
inside algorithm
use cky probabilistic parsing algorithm but combine probabilities of multiple derivations of any constituent using addition instead of max.
24
25
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024

np:.6*.6*
       .0024
     =.000864
s:.0000216
s:..00001296
probabilistic cky parser 
for inside computation
26
  book       the        flight    through  houston














s :.01, vp:.1, 
verb:.5 
nominal:.03
noun:.1
det:.6

nominal:.15
noun:.5
none

np:.6*.6*.15
     =.054
vp:.5*.5*.054
     =.0135
s:.05*.5*.054
     =.00135
none
none
none
prep:.2
np:.16
propnoun:.8
pp:1.0*.2*.16
       =.032
nominal:
.5*.15*.032
=.0024

np:.6*.6*
       .0024
     =.000864
   +.0000216
   =.00003456
s: .00001296
sum probabilities
of multiple derivations
of each constituent in
each cell.
probabilistic cky parser 
for inside computation
27
pid18: supervised training
if parse trees are provided for training sentences, a grammar and its parameters can be can all be estimated directly from counts accumulated from the tree-bank (with appropriate smoothing).

.
.
.

tree bank
estimating production probabilities
set of production rules can be taken directly from the set of rewrites in the treebank.
parameters can be directly estimated from frequency counts in the treebank.
28
29
pid18: maximum likelihood training
given a set of sentences, induce a grammar that maximizes the id203 that this data was generated from this grammar.
assume the number of non-terminals in the grammar is specified.
only need to have an unannotated set of sequences generated from the model. does not need correct parse trees for these sentences. in this sense, it is unsupervised.

30
pid18: maximum likelihood training
john ate the apple
a dog bit mary
mary hit the dog
john gave mary the cat.


.
.
.

training sentences
inside-outside
the inside-outside algorithm is a version of em for unsupervised learning of a pid18.
analogous to baum-welch (forward-backward) for id48s
given the number of non-terminals, construct all possible cnf productions with these non-terminals and observed terminal symbols.
use em to iteratively train the probabilities of these productions to locally maximize the likelihood of the data.
see manning and sch  tze text for details
experimental results are not impressive, but recent work imposes additional constraints to improve unsupervised grammar learning.

32
vanilla pid18 limitations
since probabilities of productions do not rely on specific words or concepts, only general structural disambiguation is possible (e.g. prefer to attach pps to nominals).
consequently, vanilla pid18s cannot resolve syntactic ambiguities that require semantics to resolve, e.g. ate with fork vs. meatballs.
in order to work well, pid18s must be lexicalized, i.e. productions must be specialized to specific words by including their head-word in their lhs non-terminals (e.g. vp-ate).
example of importance of lexicalization
a general preference for attaching pps to nps rather than vps can be learned by a vanilla pid18.
but the desired preference can depend on specific words.
33
34
example of importance of lexicalization
a general preference for attaching pps to nps rather than vps can be learned by a vanilla pid18.
but the desired preference can depend on specific words.
s
np           vp
john       v     np 
put    the dog  in the pen










x
head words
syntactic phrases usually have a word in them that is most    central    to the phrase.
linguists have defined the concept of a lexical head of a phrase.
simple rules can identify the head of any phrase by percolating head words up the parse tree.
head of a vp is the main verb
head of an np is the main noun
head of a pp is the preposition
head of a sentence is the head of its vp

lexicalized productions
specialized productions can be generated by including the head word and its pos of each non-terminal as part of that non-terminal   s symbol.
s
vp
vbd          np
     dt    nominal
nominal   pp
liked
in            np
in
the
dog








nn





     dt    nominal


nn
the
pen



nnp
np
john




pen-nn
pen-nn
in-in
dog-nn
dog-nn
dog-nn
liked-vbd
liked-vbd
john-nnp
nominaldog-nn     nominaldog-nn ppin-in 
lexicalized productions
s
vp
vp                             pp
     dt    nominal
put
in            np
in
the
dog





nn




     dt    nominal


nn
the
pen



nnp
np
john




pen-nn
pen-nn
in-in
dog-nn
dog-nn
put-vbd
put-vbd
john-nnp

np
vbd



put-vbd
vpput-vbd     vpput-vbd ppin-in 
parameterizing lexicalized productions
accurately estimating parameters on such a large number of very specialized productions could require enormous amounts of treebank data.
need some way of estimating parameters for lexicalized productions that makes reasonable independence assumptions so that accurate probabilities for very specific rules can be learned.
collins (1999) introduced one approach to learning effective parameters for a lexicalized grammar.

39
treebanks
english id32: standard corpus for testing syntactic parsing consists of 1.2 m words of text from the wall street journal (wsj).
typical to train on about 40,000 parsed sentences and test on an additional standard disjoint test set of 2,416 sentences.
chinese id32: 100k words from the xinhua news service.
other corpora existing in many languages, see the wikipedia article    treebank   
first wsj sentence
40
( (s 
    (np-sbj 
      (np (nnp pierre) (nnp vinken) )
      (, ,) 
      (adjp 
        (np (cd 61) (nns years) )
        (jj old) )
      (, ,) )
    (vp (md will) 
      (vp (vb join) 
        (np (dt the) (nn board) )
        (pp-clr (in as) 
          (np (dt a) (jj nonexecutive) (nn director) ))
        (np-tmp (nnp nov.) (cd 29) )))
    (. .) ))

41
parsing id74
parseval metrics measure the fraction of the constituents that match between the computed and human parse trees.  if p is the system   s parse tree and t is the human parse tree (the    gold standard   ):
recall = (# correct constituents in p) / (# constituents in t)
precision = (# correct constituents in p) / (# constituents in p)
labeled precision and labeled recall require getting the non-terminal label on the constituent node correct to count as correct.
 f1 is the harmonic mean of precision and recall.
computing id74
correct tree t
s
vp
verb          np
     det    nominal
nominal     pp
book
prep        np
through
houston
proper-noun
the
flight









noun







computed tree p
vp
verb          np
     det    nominal
book
prep        np
through
houston
proper-noun
the
flight







noun







s
vp
pp
# constituents: 12
# constituents: 12
# correct constituents: 10

recall = 10/12= 83.3%
precision = 10/12=83.3%
f1 = 83.3%
43
treebank results
results of current state-of-the-art systems on the english penn wsj treebank are 91-92% labeled f1.
human parsing
computational parsers can be used to predict human reading time as measured by tracking the time taken to read each word in a sentence.
psycholinguistic studies show that words that are more probable given the preceding lexical and syntactic context are read faster.
john put the dog in the pen with a lock.
john put the dog in the pen with a bone in the car.
john liked the dog in the pen with a bone.
modeling these effects requires an incremental statistical parser that incorporates one word at a time into a continuously growing parse tree.
44
garden path sentences
people are confused by sentences that seem to have a particular syntactic structure but then suddenly violate this structure, so the  listener is    lead down the garden path   .
the horse raced past the barn fell.
vs. the horse raced past the barn broke his leg.
the complex houses married students.
the old man the sea.
while anna dressed the baby spit up on the bed.
incremental computational parsers can try to predict and explain the problems encountered parsing such sentences.
45
center embedding
nested expressions are hard for humans to process beyond 1 or 2 levels of nesting.
the rat the cat chased died.
the rat the cat the dog bit chased died.
the rat the cat the dog the boy owned bit chased died.
requires remembering and popping incomplete constituents from a stack and strains human short-term memory.
equivalent    tail embedded    (tail recursive) versions are easier to understand since no stack is required.
the boy owned a dog that bit a cat that chased a rat that died.
46
dependency grammars
an alternative to phrase-structure grammar is to define a parse as a directed graph between the words of a sentence representing dependencies between the words.
47
liked
john
dog
pen
in
the
the
liked
john
dog
pen
in
the
the
nsubj
dobj
det
det
typed 
dependency
parse
dependency graph from parse tree
can convert a phrase structure parse to a dependency tree by making the head of each non-head child of a node depend on the head of the head child.
48
s
vp
vbd          np
     dt    nominal
nominal   pp
liked
in            np
in
the
dog








nn





     dt    nominal


nn
the
pen



nnp
np
john




pen-nn
pen-nn
in-in
dog-nn
dog-nn
dog-nn
liked-vbd
liked-vbd
john-nnp
liked
john
dog
pen
in
the
the
unification grammars
in order to handle agreement issues more effectively, each constituent has a list of features such as number, person, gender, etc. which may or not be specified for a given constituent.
in order for two constituents to combine to form a larger constituent, their features must unify, i.e. consistently combine into a merged set of features.
expressive grammars and parsers (e.g. hpsg) have been developed using this approach and have been partially integrated with modern statistical models of disambiguation.
49
mildly context-sensitive grammars
some grammatical formalisms provide a degree of context-sensitivity that helps capture aspects of nl syntax that are not easily handled by id18s.
id34 (tag) is based on combining tree fragments rather than individual phrases.
id35 (id35) consists of: 
categorial lexicon that associates a syntactic and semantic category with each word.
combinatory rules that define how categories combine to form other categories.
50
statistical parsing conclusions
statistical models such as pid18s allow for probabilistic resolution of ambiguities.
pid18s can be easily learned from treebanks.
lexicalization and non-terminal splitting are required to effectively resolve many ambiguities.
current statistical parsers are quite accurate but not yet at the level of human-expert agreement.
51
