1
1
cs 388: 
natural language processing:
discriminative training and
id49 (crfs)
for sequence labeling
raymond j. mooney
university of texas at austin
2
joint distribution
the joint id203 distribution for a set of random variables, x1,   ,xn gives the id203 of every combination of values (an n-dimensional array with vn values if all variables are discrete with v values, all vn values must sum to 1): p(x1,   ,xn)




the marginal id203 of all possible conjunctions (assignments of values to some subset of variables) can be calculated by summing the appropriate subset of values from the joint distribution.


therefore, all conditional probabilities can also be calculated.
positive
negative
3
probabilistic classification
let y be the random variable for the class which takes values {y1,y2,   ym}.
let x be the random variable describing an instance consisting of a vector of values for n features <x1,x2   xn>, let xk be a possible vector value for x and xij a possible value for xi.
for classification, we need to compute p(y=yi | x=xk) for i = 1   m
could be done using joint distribution but this requires estimating an exponential number of parameters.

4
bayesian categorization
determine category of xk by determining for each yi


p(x=xk) can be determined since categories are complete and disjoint.

5
bayesian categorization (cont.)
need to know:
priors: p(y=yi) 
conditionals: p(x=xk | y=yi)
p(y=yi) are easily estimated from data. 
if ni of the examples in d are in yi then p(y=yi) =  ni / |d|
too many possible instances (e.g. 2n for binary features) to estimate all p(x=xk | y=yi).
still need to make some sort of independence assumptions about the features to make learning tractable.

6
na  ve bayes generative model
size          color        shape 
positive
negative
pos
neg
pos
pos
pos
neg
neg
sm
med
lg
lg
med
sm
sm
med
lg
red
red
red
red
red
blue
blue
grn
circ
circ
circ
circ
sqr
tri
tri
circ
sqr
tri
sm
lg
med
sm
lg
med
lg
sm
blue
red
grn
blue
grn
red
grn
blue
circ
sqr
tri
circ
sqr
circ
tri
category
7
na  ve bayes id136 problem
size          color        shape 
positive
negative
pos
neg
pos
pos
pos
neg
neg
sm
med
lg
lg
med
sm
sm
med
lg
red
red
red
red
red
blue
blue
grn
circ
circ
circ
circ
sqr
tri
tri
circ
sqr
tri
sm
lg
med
sm
lg
med
lg
sm
blue
red
grn
blue
grn
red
grn
blue
circ
sqr
tri
circ
sqr
circ
tri
category
lg  red circ 
8
na  ve bayesian categorization
if we assume features of an instance are independent given the category (conditionally independent).


therefore, we then only need to know p(xi | y) for each possible pair of a feature-value and a category.
if y and all xi and binary, this requires specifying only 2n parameters:
p(xi=true | y=true) and p(xi=true | y=false) for each xi
p(xi=false | y) = 1     p(xi=true | y)

compared to specifying 2n parameters without any independence assumptions.
9
generative vs. discriminative models
 generative models and are not directly designed  to maximize the performance of classification. they model the complete joint distribution p(x,y).
classification is then done using bayesian id136 given the generative model of the joint distribution.
but a generative model can also be used to perform any other id136 task, e.g. p(x1 |  x2,    xn, y)
   jack of all trades, master of none.   
discriminative models are specifically designed and trained to maximize performance of classification. they only model the conditional distribution p(y | x).
by focusing on modeling the conditional distribution, they generally perform better on classification than generative models when given a reasonable amount of training data.
id28
assumes a parametric form for directly estimating p(y | x). for binary concepts, this is:




equivalent to a one-layer id26 neural net.
id28 is the source of the sigmoid function used in id26.
objective function for training is somewhat different.

id28 as a log-linear model
id28 is basically a linear model, which is demonstrated by taking logs.
also called a maximum id178 model (maxent) because it can be shown that standard training for id28 gives the distribution with maximum id178 that is consistent with the training data.
id28 training

weights are set during training to maximize the conditional data likelihood :


    where d is the set of training examples and yd and xd denote, respectively, the values of y and x for example d.

equivalently viewed as maximizing the conditional log likelihood (cll)
id28 training
like neural-nets, can use standard id119 to find the parameters (weights) that optimize the cll objective function.
many other more advanced training methods are possible to speed convergence.
conjugate gradient
generalized iterative scaling (gis)
improved iterative scaling (iis)
limited-memory quasi-newton (l-bfgs)
preventing overfitting in id28
to prevent overfitting, one can use id173 (a.k.a. smoothing) by penalizing large weights by changing the training objective:
this can be shown to be equivalent to map parameter estimation assuming a guassian prior for w with zero mean and a variance related to 1/  .
where    is a constant that determines the amount of smoothing
multinomial id28
(maxent)
id28 can be generalized to multi-class problems (where y has a multinomial distribution).
create feature functions for each combination of a class value y   and each feature xj and another for the    bias weight    of each class.
f y  , j (y, x) = xj  if  y= y   and 0 otherwise
f y   (y, x) = 1  if  y= y   and 0 otherwise
the final conditional distribution is:
(normalizing constant)
(  k are weights)
id114
if no assumption of independence is made, then an exponential number of parameters must be estimated for sound probabilistic id136.
no realistic amount of training data is sufficient to estimate so many parameters.
if a blanket assumption of conditional independence is made, efficient training and id136 is possible, but such a strong assumption is rarely warranted.
id114 use directed or undirected graphs over a set of random variables to explicitly specify variable dependencies and allow for less restrictive independence assumptions while limiting the number of parameters that must be estimated.
id110s: directed acyclic graphs that indicate causal structure.
markov networks: undirected graphs that capture general dependencies.
id110s
directed acyclic graph (dag)
nodes are random variables
edges indicate causal influences
id155 tables
each node has a id155 table (cpt) that gives the id203 of each of its values given every possible combination of values for its parents (conditioning case).
roots (sources) of the dag that have no parents are given prior probabilities.
burglary
earthquake
alarm
johncalls
marycalls




joint distributions for bayes nets
a id110 implicitly defines a joint distribution.
example
na  ve bayes as a bayes net
na  ve bayes is a simple bayes net
y
x1
x2
   
xn



priors p(y) and conditionals p(xi|y) for na  ve bayes provide cpts for the network.
markov networks
undirected graph over a set of random variables, where an edge represents a dependency.
the markov blanket of a node, x,  in a markov net is the set of its neighbors in the graph (nodes that have an edge connecting to x).
every node in a markov net is conditionally independent of every other node given its markov blanket.
distribution for a markov network
the distribution of a markov net is most compactly described in terms of a set of potential functions (a.k.a. factors, compatibility functions),   k, for each clique, k, in the graph.
for each joint assignment of values to the variables in clique k,   k assigns a non-negative real value that represents the compatibility of these values.
the joint distribution of a markov network is then defined by:
    where x{k} represents the joint assignment of the variables in clique k, and z is a normalizing constant that makes a joint distribution that sums to 1.
sample markov network
burglary
earthquake
alarm
johncalls
marycalls








id28 as a markov net
id28 is a simple markov net
y
x1
x2
   
xn



but only models the conditional distribution, p(y | x) and not the full joint p(x,y)
same as a discriminatively trained na  ve bayes.
generative vs. discriminative 
sequence labeling models
id48s are generative models and are not directly designed  to maximize the performance of sequence labeling. they model the joint distribution p(o,q).
id48s are trained to have an accurate probabilistic model of the underlying language, and not all aspects of this model benefit the sequence labeling task.
id49 (crfs) are specifically designed and trained to maximize performance of sequence labeling. they model the conditional distribution p(q | o)
classification
y
x1
x2
   
xn



y
x1
x2
   
xn



na  ve 
bayes
logistic
regression

conditional
      generative



discriminative
sequence labeling
y2
x1
x2
   
xt



id48
linear-chain crf

conditional
      generative



discriminative
y1
yt


..
y2
x1
x2
   
xt



y1
yt


..
simple linear chain crf features
modeling the conditional distribution is similar to that used in multinomial id28.
create feature functions fk(yt, yt   1, xt)
feature for each state transition pair i, j
fi,j(yt, yt   1, xt) = 1 if yt = i and  yt   1 = j and 0 otherwise
feature for each state observation pair i, o
fi,o(yt, yt   1, xt) = 1 if yt = i and  xt = o and 0 otherwise
note: number of features grows quadratically in the number of states (i.e. tags).

28
conditional distribution for
linear chain crf 
using these feature functions for a simple linear chain crf, we can define:
29
adding token features to a crf
can add token features xi,j
30
   


x1,1
x1,m


   

x2,1
x2,m


   

xt,1
xt,m


   
   
can add additional feature functions for each token feature to model conditional distribution.
y1
y2
yt
features in id52
for id52, use lexicographic features of tokens.
capitalized?
start with numeral?
ends in given suffix (e.g.    s   ,    ed   ,    ly   )?

31
enhanced linear chain crf
(standard approach)
can also condition transition on the current token features.
32
   

x1,1


x2,1


   
   




   

   
xt,1




x1,m
x2,m
xt,m
add feature functions:
fi,j,k(yt, yt   1, x) 1 if yt = i and  yt   1 = j and xt    1,k = 1                        and 0 otherwise

y2
yt
y1
supervised learning 
(parameter estimation)
as in id28, use l-bfgs optimization procedure, to set    weights to maximize cll of the supervised training data. 
see paper for details.
33
sequence tagging  
(id136)
variant of viterbi algorithm can be used to efficiently, o(tn2), determine the globally most probable label sequence for a given token sequence using a given log-linear model of the id155 p(y | x).
see paper for details.
34
skip-chain crfs
can model some long-distance dependencies (i.e. the same word appearing in different parts of the text) by including long-distance edges in the markov model.
35
y2
x1
x2
   
x3



y1
y3


michael     dell     said


   
   
   



    dell   bought

   

y100
x100
y101
x101
additional links make exact id136 intractable, so must resort to approximate id136 to try to find the most probable labeling.
36
crf results
experimental results verify that they have superior accuracy on various sequence labeling tasks.
id52
noun phrase chunking
id39
id14
however, crfs are much slower to train and do not scale as well to large amounts of training data.
training for pos on full id32 (~1m words) currently takes    over a week.   
skip-chain crfs improve results on ie.


crf summary
crfs are a discriminative approach to sequence labeling whereas id48s are generative.
discriminative methods are usually more accurate since they are trained for a specific performance task.
crfs also easily allow adding additional token features without making additional independence assumptions.
training time is increased since a complex optimization procedure is needed to fit supervised training data.
crfs are a state-of-the-art method for sequence labeling.
37
