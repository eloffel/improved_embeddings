1
cs 388: 
natural language processing:
id165 language models
raymond j. mooney
university of texas at austin
language models
formal grammars (e.g. regular, context free) give a hard    binary    model of the legal  sentences in a language.
for nlp, a probabilistic model of a language that gives a id203 that a string is a member of a language is more useful.
to specify a correct id203 distribution, the id203 of all sentences in a language must sum to 1.
uses of language models
id103
   i ate a cherry    is a more likely sentence than    eye eight uh jerry    
ocr & handwriting recognition
more probable sentences are more likely correct readings.
machine translation
more likely sentences are probably better translations.
generation
more likely sentences are probably better nl generations.  
context sensitive id147
   their are problems wit this sentence.   
completion prediction
a language model also supports predicting the completion of a sentence.
please turn off your cell _____
your program does not ______
predictive text input systems can guess what you are typing and give choices on how to complete it.

id165 models
estimate id203 of each word given prior context.
p(phone | please turn off your cell)
number of parameters required grows exponentially with the number of words of prior context.
an id165 model uses only n   1 words of prior context.
unigram:  p(phone)
bigram:  p(phone | cell)
trigram:  p(phone | your cell)
the markov assumption is the presumption that the future behavior of a dynamical system only depends on its recent history.  in particular, in a kth-order markov model, the next state only depends on the k most recent states, therefore an id165 model is a (n   1)-order markov model.
id165 model formulas
word sequences

chain rule of id203

bigram approximation

id165 approximation

estimating probabilities
id165 conditional probabilities can be estimated from raw text based on the relative frequency of word sequences.




to have a consistent probabilistic model, append a unique start (<s>) and end (</s>) symbol to every sentence and treat these as additional words.


bigram:
id165:
generative model & id113
an id165 model can be seen as a probabilistic automata for generating sentences.



relative frequency estimates can be proven to be maximum likelihood estimates (id113) since they maximize the id203 that the model m will generate the training corpus t.

initialize sentence with n   1 <s> symbols
until </s> is generated do:
      stochastically pick the next word based on the conditional  
      id203 of each word given the previous n    1 words.
example from textbook
p(<s> i want english food </s>) 
    = p(i | <s>) p(want | i) p(english | want)       
        p(food | english) p(</s> | food)
    = .25 x .33 x .0011 x .5 x .68 = .000031

p(<s> i want chinese food </s>) 
    = p(i | <s>) p(want | i) p(chinese | want)       
        p(food | chinese) p(</s> | food)
    = .25 x .33 x .0065 x .52 x .68 = .00019

train and test corpora
a language model must be trained on a large corpus of text to estimate good parameter values.
model can be evaluated based on its ability to predict a high id203 for a disjoint (held-out) test corpus (testing on the training corpus would give an optimistically biased estimate).
ideally, the training (and test) corpus should be representative of the actual application data.
may need to adapt a general model to a small amount of new (in-domain) data by adding highly weighted small corpus to original training data.

unknown words
how to handle words in the test corpus that did not occur in the training data, i.e. out of vocabulary (oov) words?
train a model that includes an explicit symbol for an unknown word (<unk>).
choose a vocabulary in advance and replace other words in the training corpus with <unk>.
replace the first occurrence of each word in the training data with <unk>.

evaluation of language models
ideally, evaluate use of model in end application (extrinsic, in vivo)
realistic
expensive
evaluate on ability to model test corpus (intrinsic).
less realistic
cheaper
verify at least once that intrinsic evaluation correlates with an extrinsic one.
perplexity
measure of how well a model    fits    the test data.
uses the id203 that the model assigns to the test corpus.
normalizes for the number of words in the test corpus and takes the inverse.
measures the weighted average branching factor in predicting the next word (lower is better).
sample perplexity evaluation
models trained on 38 million words from the wall street journal (wsj) using a 19,979 word vocabulary.
evaluate on a disjoint set of 1.5 million wsj words.
smoothing
since there are a combinatorial number of possible word sequences, many rare (but not impossible) combinations never occur in training, so id113 incorrectly assigns zero to many parameters (a.k.a. sparse data).
if a new combination occurs during testing, it is given a id203 of zero and the entire sequence gets a id203 of zero (i.e. infinite perplexity).
in practice, parameters are smoothed (a.k.a. regularized) to reassign some id203 mass to unseen events.
adding id203 mass to unseen events requires removing it from seen ones (discounting) in order to maintain a joint distribution that sums to 1.
laplace (add-one) smoothing
   hallucinate    additional training data in which each possible id165 occurs exactly once and adjust estimates accordingly.



   where v is the total number of possible (n   1)-grams (i.e. the vocabulary size for a bigram model).
bigram:
id165:
tends to reassign too much mass to unseen events, so can be adjusted to add 0<   <1 (normalized by    v instead of v).
advanced smoothing
many advanced techniques have been developed to improve smoothing for language models.
good-turing
interpolation
backoff
kneser-ney
class-based (cluster) id165s
model combination
as n increases, the power (expressiveness) of an id165 model increases, but the ability to estimate accurate parameters from sparse data decreases (i.e. the smoothing problem gets worse).
a general approach is to combine the results of multiple id165 models of increasing complexity (i.e. increasing n).
interpolation
linearly combine estimates of id165 models of increasing order.
learn proper values for    i by training to (approximately) maximize the likelihood of an independent development (a.k.a. tuning) corpus.
interpolated trigram model:
where:
backoff
only use lower-order model when data for higher-order model is unavailable (i.e. count is zero).
recursively back-off to weaker models until data is available.
where p* is a discounted id203 estimate to reserve mass for unseen events and       s are back-off weights (see text for details).
a problem for id165s:
long distance dependencies
many times local context does not provide the most useful predictive clues, which instead are provided by long-distance dependencies.
syntactic dependencies
   the man next to the large oak tree near the grocery store on the corner is tall.   
   the men next to the large oak tree near the grocery store on the corner are tall.   
semantic dependencies
   the bird next to the large oak tree near the grocery store on the corner flies rapidly.   
   the man next to the large oak tree near the grocery store on the corner talks rapidly.   
more complex models of language are needed to handle such dependencies.
summary
language models assign a id203 that a sentence is a legal string in a language.
they are useful as a component of many nlp systems, such as asr, ocr, and mt.
simple id165 models are easy to train on unsupervised corpora and can provide useful estimates of sentence likelihood.
id113 gives inaccurate parameters for models trained on sparse data.
smoothing techniques adjust parameter estimates to account for unseen (but not impossible) events.

