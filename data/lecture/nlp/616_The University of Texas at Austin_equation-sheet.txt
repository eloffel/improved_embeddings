id165 model formulas
word sequences

chain rule of id203

bigram approximation

id165 approximation

estimating probabilities
id165 conditional probabilities can be estimated from raw text based on the relative frequency of word sequences.




to have a consistent probabilistic model, append a unique start (<s>) and end (</s>) symbol to every sentence and treat these as additional words.


bigram:
id165:
perplexity
measure of how well a model    fits    the test data.
uses the id203 that the model assigns to the test corpus.
normalizes for the number of words in the test corpus and takes the inverse.
measures the weighted average branching factor in predicting the next word (lower is better).
laplace (add-one) smoothing
   hallucinate    additional training data in which each possible id165 occurs exactly once and adjust estimates accordingly.



   where v is the total number of possible (n   1)-grams (i.e. the vocabulary size for a bigram model).
bigram:
id165:
tends to reassign too much mass to unseen events, so can be adjusted to add 0<   <1 (normalized by    v instead of v).
interpolation
linearly combine estimates of id165 models of increasing order.
learn proper values for    i by training to (approximately) maximize the likelihood of an independent development (a.k.a. tuning) corpus.
interpolated trigram model:
where:
6
formal definition of an id48
a set of n +2 states s={s0,s1,s2,     sn, sf}
distinguished start state:  s0
distinguished final state: sf
a set of m possible observations v={v1,v2   vm}
a state transition id203 distribution a={aij}



observation id203 distribution for each state j b={bj(k)}

total parameter set   ={a,b}
forward probabilities
let    t(j) be the id203 of being in state j after seeing the first t observations (by summing over all initial paths leading to j).
7
computing the forward probabilities
initialization

recursion


termination
8
viterbi scores
recursively compute the id203 of the most likely subsequence of states that accounts for the first t observations and ends in state sj. 
9
also record    backpointers    that subsequently allow backtracing the most probable state sequence.
btt(j) stores the state at time t-1 that maximizes the id203 that system was in state sj at time t (given the observed sequence).
computing the viterbi scores
initialization

recursion


termination
10
analogous to forward algorithm except take max instead of sum
computing the viterbi backpointers
initialization

recursion


termination
11
final state in the most probable state sequence. follow 
backpointers to initial state to construct full sequence.
supervised parameter estimation
estimate state transition probabilities based on tag bigram and unigram statistics in the labeled data.


estimate the observation probabilities based on tag/word co-occurrence statistics in the labeled data.


use appropriate smoothing if training data is sparse.
12
13
simple artificial neuron model
(linear threshold unit)
model network as a graph with cells as nodes and synaptic connections as weighted edges from node i to node j, wji

model net input to cell as


cell output is: 



(tj is threshold for unit j)


netj
oj
tj

0
1




14
id88 learning rule
update weights by:

     where    is the    learning rate   
     tj is the teacher specified output for unit j.
equivalent to rules:
if output is correct do nothing.
if output is high, lower weights on active inputs
if output is low, increase weights on active inputs
also adjust threshold to compensate:
15
id88 learning algorithm
iteratively update weights until convergence.




each execution of the outer loop is typically called an epoch.
initialize weights to random values
until outputs of all training examples are correct
      for each training pair, e, do: 
             compute current output oj for e given its inputs
             compare current output to target value, tj , for e
             update synaptic weights and threshold using learning rule
id18s (id18)
n a set of non-terminal symbols (or variables)
    a set of terminal symbols (disjoint from n)
r a set of productions or rules of the form a      , where a is a non-terminal and     is a string of symbols from (       n)*
s, a designated non-terminal called the start symbol

estimating production probabilities
set of production rules can be taken directly from the set of rewrites in the treebank.
parameters can be directly estimated from frequency counts in the treebank.
17
