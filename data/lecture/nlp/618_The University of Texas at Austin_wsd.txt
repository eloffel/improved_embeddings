1
1
cs 388: 
natural language processing:
id51
raymond j. mooney
university of texas at austin
2
lexical ambiguity
most words in natural languages have multiple possible meanings.
   pen    (noun)
the dog is in the pen.
the ink is in the pen.
   take    (verb)
take one pill every morning.
take the first right past the stoplight.
syntax helps distinguish meanings for different parts of speech of an ambiguous word.
   conduct    (noun or verb)
john   s conduct in class is unacceptable.
john will conduct the orchestra on thursday.



3
motivation for
id51 (wsd)
many tasks in natural language processing require disambiguation of ambiguous words.
id53
information retrieval
machine translation
id111
phone help systems
understanding how people disambiguate words is an interesting problem that can provide insight in psycholinguistics.
4
sense inventory
what is a    sense    of a word?
homonyms (disconnected meanings) 
bank: financial institution
bank: sloping land next to a river
polysemes (related meanings with joint etymology) 
bank: financial institution as corporation
bank: a building housing such an institution
sources of sense inventories
dictionaries
lexical databases

5
id138
a detailed database of semantic relationships between english words.
developed by famous cognitive psychologist george miller and a team at princeton university.
about 144,000 english words.
nouns, adjectives, verbs, and adverbs grouped into about 109,000 synonym sets called synsets.
6
id138 synset relationships
antonym: front     back
attribute: benevolence     good (noun to adjective)
pertainym: alphabetical     alphabet (adjective to noun)
similar: unquestioning     absolute
cause: kill     die
entailment: breathe     inhale
holonym: chapter     text (part to whole)
meronym: computer     cpu (whole to part)
hyponym: plant     tree (specialization)
hypernym: apple     fruit (generalization)
7
euroid138
id138s for
dutch
italian
spanish
german
french
czech
estonian
8
id138 senses
id138s senses (like many dictionary senses) tend to be very fine-grained.
   play    as a verb has 35 senses, including
play a role or part:    gielgud played haid113t   
pretend to have certain qualities or state of mind:    john played dead.   
difficult to disambiguate to this level for people and computers. only expert lexicographers are perhaps able to reliably differentiate senses.
not clear such fine-grained senses are useful for nlp.
several proposals for grouping senses into coarser, easier to identify senses (e.g. homonyms only).

9
senses based on needs of translation
only distinguish senses that are translate to different words in some other language.
play: tocar vs. jugar
know: conocer vs. saber
be: ser vs. estar
leave: salir vs dejar
take: llevar vs. tomar vs. sacar
may still require overly fine-grained senses
river in french is either:
fleuve: flows into the ocean
rivi  re: does not flow into the ocean
10
learning for wsd
assume part-of-speech (pos), e.g. noun, verb, adjective, for the target word is determined.
treat as a classification problem with the appropriate potential senses for the target word given its pos as the categories.
encode context using a set of features to be used for disambiguation.
train a classifier on labeled data encoded using these features.
use the trained classifier to disambiguate future instances of the target word given their contextual features.
11
feature engineering
the success of machine learning requires instances to be represented using an effective set of features that are correlated with the categories of interest.
feature engineering can be a laborious process that requires substantial human expertise and knowledge of the domain.
in nlp it is common to extract many (even thousands of) potentially features and use a learning algorithm that works well with many relevant and irrelevant features.
12
contextual features
surrounding bag of words.
pos of neighboring words
local collocations
syntactic relations
experimental  evaluations indicate that all of these features are useful; and the best results comes from integrating all of these cues in the disambiguation process.
13
surrounding bag of words
unordered individual words near the ambiguous word.
words in the same sentence.
may include words in the previous sentence or surrounding paragraph.
gives general topical cues of the context.
may use feature selection to determine a smaller set of words that help discriminate possible senses.
may just remove common    stop words    such as articles, prepositions, etc.
14
pos of neighboring words
use part-of-speech of immediately neighboring words.
provides evidence of local syntactic context.
p-i is the pos of the word i positions to the left of the target word.
pi is the pos of the word i positions to the right of the target word.
typical to include features for: 
     p-3, p-2, p-1, p1, p2, p3                                                   
15
local collocations
specific lexical context immediately adjacent to the word.
for example, to determine if    interest    as a noun refers to    readiness to give attention    or    money paid for the use of money   , the following collocations are useful:
   in the interest of   
   an interest in   
   interest rate   
   accrued interest   
ci,j is a feature of the sequence of words from local position i to j relative to the target word.
c-2,1 for    in the interest of    is    in the of   
typical to include:  
single word context: c-1,-1 , c1,1, c-2,-2,  c2,2 
two word context: c-2,-1, c-1,1 ,c1,2
three word context: c-3,-1, c-2,1, c-1,2, c1,3
16
syntactic relations
(ambiguous verbs)
for an ambiguous verb, it is very useful to know its direct object.
   played the game   
   played the guitar   
   played the risky and long-lasting card game   
   played the beautiful and expensive guitar   
   played the big brass tuba at the football game   
   played the game listening to the drums and the tubas   
may also be useful to know its subject:
   the game was played while the band played.   
   the game that included a drum and a tuba was played on friday.   


17
syntactic relations
(ambiguous nouns)
for an ambiguous noun, it is useful to know what verb it is an object of:
   played the piano and the horn   
   wounded by the rhinoceros    horn   
may also be useful to know what verb it is the subject of:
   the bank near the river loaned him $100   
   the bank is eroding and the bank has given the city the money to repair it   

18
syntactic relations
(ambiguous adjectives)
for an ambiguous adjective, it useful to know the noun it is modifying.
   a brilliant young man   
   a brilliant yellow light   
   a wooden writing desk   
   a wooden acting performance   

19
using syntax in wsd
produce a parse tree for a sentence using a syntactic parser.




for ambiguous verbs, use the head word of its direct object and of its subject as features.
for ambiguous nouns, use verbs for which it is the object and the subject as features.
for ambiguous adjectives, use the head word (noun) of its np as a feature.

20
evaluation of wsd
   in vitro   :  
corpus developed in which one or more ambiguous words are labeled with explicit sense tags according to some sense inventory.
corpus used for training and testing wsd and evaluated using accuracy (percentage of labeled words correctly disambiguated).
use most common sense selection as a baseline.
   in vivo   :
incorporate wsd system into some larger application system, such as machine translation, information retrieval, or id53.
evaluate relative contribution of different wsd methods by measuring performance impact on the overall system on final task (accuracy of mt, ir, or qa results).

21
lexical sample vs. all word tagging
lexical sample:  
choose one or more ambiguous words each with a sense inventory.
from a larger corpus, assemble sample occurrences of these words.
have humans mark each occurrence with a sense tag.
all words:
select a corpus of sentences.
for each ambiguous word in the corpus, have humans mark it with a sense tag from an broad-coverage lexical database (e.g. id138).
22
senseeval
standardized international    competition    on wsd.
organized by the association for computational linguistics (acl) special interest group on the lexicon (siglex).
after 2007, evolved in broader    semeval    competition.
23
senseval 1: 1998
datasets for
english
french 
italian
lexical sample in english
noun: accident, behavior, bet, disability, excess, float, giant, knee, onion, promise, rabbit, sack, scrap, shirt, steering
verb: amaze, bet, bother, bury, calculate, consumer, derive, float, invade, promise, sack, scrap, sieze
adjective: brilliant, deaf, floating, generous, giant, modest, slight, wooden
indeterminate: band, bitter, hurdle, sanction, shake
total number of ambiguous english words tagged: 8,448
24
senseval 1 english sense inventory
senses from the hector id69 project.
multiple levels of granularity
coarse grained (avg. 7.2 senses per word)
fine grained (avg. 10.4 senses per word)
25
senseval metrics
fixed training and test sets, same for each system.
system can decline to provide a sense tag for a word if it is sufficiently uncertain.
measured quantities:
a: number of words assigned senses
c: number of words assigned correct senses
t: total number of test words
metrics:
precision = c/a
recall = c/t
26
senseval 1 overall english results
27
senseval 2: 2001
more languages: chinese, danish, dutch, czech, basque, estonian, italian, korean, spanish, swedish, japanese, english
includes an    all-words    task as well as lexical sample.
includes a    translation    task for japanese, where senses correspond to distinct translations of a word into another language.
35 teams competed with over 90 systems entered.
28
senseval 2 results
29
senseval 2 results
30
senseval 2 results
31
issues in wsd
what is the right granularity of a sense inventory?
integrating wsd with other nlp tasks
syntactic parsing
id14
id29
does wsd actually improve performance on some real end-user task?
information retrieval
information extraction
machine translation
id53
