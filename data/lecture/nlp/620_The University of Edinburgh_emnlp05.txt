empirical method in natural language processing

tagging (i): part-of-speech tagging with id48

lecture 5

philipp koehn

21 january 2008

pk

emnlp

21 january 2008

parts of speech

1

    open class words (or content words)

    nouns, verbs, adjectives, adverbs
    mostly content-bearing: they refer to objects, actions, and features in the

world

    open class, since there is no limit to what these words are, new ones are

added all the time (email, website).

    close class words

    pronouns, determiners, prepositions, connectives, ...
    there is a limited number of these
    mostly functional: to tie the concepts of a sentence together

pk

emnlp

21 january 2008

parts of speech (2)

2

    there are about 30-100 parts of speech

    distinguish between names and abstract nouns?
    distinguish between plural noun and singular noun?
    distinguish between past tense verb and present tense word?

    identifying the parts of speech is a    rst step towards syntactic analysis

pk

emnlp

21 january 2008

ambiguous words

3

    for instance: like

    verb: i like the class.
    preposition: he is like me.

    another famous example: time    ies like an arrow
    most of the time, the local context disambiguated the part of speech

pk

emnlp

21 january 2008

part-of-speech tagging

4

    task: given a text of english, identify the parts of speech of each word
    example

    input: word sequence

time    ies like an arrow

    output: tag sequence

time/nn    ies/vb like/p an/det arrow/nn

    what will help us to tag words with their parts-of-speech?

pk

emnlp

21 january 2008

relevant knowledge for id52

5

    the word itself

    some words may only be nouns, e.g. arrow
    some words are ambiguous, e.g. like,    ies
    probabilities may help, if one tag is more likely than another

    local context

    two determiners rarely follow each other
    two base form verbs rarely follow each other
    determiner is almost always followed by adjective or noun

pk

emnlp

21 january 2008

bayes rule

6

    we want to    nd the best part-of-speech tag sequence t for a sentence s:

    bayes rule gives us:

argmaxt p(t|s)

p(t|s) = p(s|t ) p(t )

p(s)

    we can drop p(s) if we are only interested in argmaxt :

argmaxt p(t|s) = argmaxt p(s|t ) p(t )

pk

emnlp

21 january 2008

decomposing the model

7

    the mapping p(s|t ) can be decomposed into

p(s|t ) =y

p(wi|ti)

i

    p(t ) could be called a part-of-speech language model, for which we can use

an id165 model:

p(t ) = p(t1) p(t2|t1) p(t3|t1, t2)...p(tn|tn   2, tn   1)

    we can estimate p(s|t ) and p(t ) with id113 (and

maybe some smoothing)

pk

emnlp

21 january 2008

hidden markov model (id48)

8

    the model we just developed is a hidden markov model
    elements of an id48 model:

    a set of states (here: the tags)
    an output alphabet (here: words)
    intitial state (here: beginning of sentence)
    state transition probabilities (here: p(tn|tn   2, tn   1))
    symbol emission probabilities (here: p(wi|ti))

pk

emnlp

21 january 2008

graphical representation

9

    when tagging a sentence, we are walking through the state graph:

    state transition probabilities: p(tn|tn   1)

pk

emnlp

21 january 2008

vbnnindetstartendgraphical representation (2)

10

    at each state we emit a word:

    symbol emission probabilities: p(wi|ti)

pk

emnlp

21 january 2008

vblikefliessearch for the best tag sequence

11

    we have de   ned a model, but how do we use it?

    given: word sequence
    wanted: tag sequence

    if we consider a speci   c tag sequence, it is straight-forward to compute its

p(s|t ) p(t ) =y

p(wi|ti) p(ti|ti   2, ti   1)

id203

    problem: if we have on average c choices for each of the n words, there are

cn possible tag sequences, maybe too many to e   ciently evaluate

i

pk

emnlp

21 january 2008

walking through the states

12

    first, we go to state nn to emit time:

pk

emnlp

21 january 2008

vbnndetinstarttimewalking through the states (2)

13

    then, we go to state vb to emit    ies:

pk

emnlp

21 january 2008

vbnndetinstarttimevbnndetinflieswalking through the states (3)

14

    of course, there are many possible paths:

pk

emnlp

21 january 2008

vbnndetinstarttimevbnndetinfliesvbnndetinlikevbnndetinanviterbi algorithm

15

    intuition: since state transition out of a state only depend on the current state

(and not previous states), we can record for each state the optimal path

    we record:

    cheapest cost to state j at step s in   j(s)
    backtrace from that state to best predecessor   j(s)

    stepping through all states at each time steps allows us to compute

      j(s + 1) = max1   i   n   i(s) p(ti|tj) p(ws|tj)
      j(s + 1) = argmax1   i   n   i(s) p(ti|tj) p(ws|tj)

    best    nal state is argmax1   i   n   i(s + 1), we can backtrack from there

pk

emnlp

21 january 2008

other tagging tasks

16

    a number of problems can be framed as tagging problems:
    basenp chunking:

for text processing purposes it is useful to detect base

noun phrases that correspond to concepts, e.g. department of defense

    id39: it may also be useful to    nd names of persons,

organizations, etc. in the text, e.g. tony blair

    accent restoration: when keyboards lack the proper keys, it is common to

not write the accents in spanish or french. we may want to restore them.

    case restoration:

if we just get lowercased text, we may want to restore

proper casing, e.g. the river thames

pk

emnlp

21 january 2008

basenp chunking

17

    task:    nd basic noun phrases (facilitates parsing, information extraction)
    example: [ the student ] said [ the exam question ] is hard
    three tags

    b = beginning of basenp
    i = continuing basenp (internal)
    o = other word

    example: the/b student/i said/o the/b exam/i question/i is/o hard/o
    tagging task: assign tags (b, i, o) to each word

pk

emnlp

21 january 2008

