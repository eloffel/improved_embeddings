data intensive linguistics

lecture 16

machine translation (iii): decoding

philipp koehn

29 february 2008

philipp koehn

emnlp lecture 16

29 february 2008

id151

    components: translation model, language model, decoder

1

philipp koehn

emnlp lecture 16

29 february 2008

statistical analysisstatistical analysisforeign/englishparallel textenglishtexttranslationmodellanguagemodeldecoding algorithmphrase-based translation

2

    foreign input is segmented in phrases

    any sequence of words, not necessarily linguistically motivated

    each phrase is translated into english
    phrases are reordered

philipp koehn

emnlp lecture 16

29 february 2008

morgen  fliege  ich   nach kanada  zur konferenztomorrow  i  will fly   to the conference  in canadaphrase translation table

3

    phrase translations for    den vorschlag   :

english
the proposal
   s proposal
a proposal
the idea
this proposal
proposal
of the proposal
the proposals

  (e|f)
0.6227
0.1068
0.0341
0.0250
0.0227
0.0205
0.0159
0.0159

english
the suggestions
the proposed
the motion
the idea of
the proposal ,
its proposal
it
...

  (e|f)
0.0114
0.0114
0.0091
0.0091
0.0068
0.0068
0.0068
...

philipp koehn

emnlp lecture 16

29 february 2008

decoding process

4

    build translation left to right

    select foreign words to be translated

philipp koehn

emnlp lecture 16

29 february 2008

brujamarianoverdelaadiounabofetadadecoding process

5

    build translation left to right

    select foreign words to be translated
       nd english phrase translation
    add english phrase to end of partial translation

philipp koehn

emnlp lecture 16

29 february 2008

brujamarianomaryverdelaadiounabofetadadecoding process

6

    build translation left to right

    select foreign words to be translated
       nd english phrase translation
    add english phrase to end of partial translation
    mark foreign words as translated

philipp koehn

emnlp lecture 16

29 february 2008

brujanoverdelaadiounabofetadamarymariadecoding process

7

    one to many translation

philipp koehn

emnlp lecture 16

29 february 2008

brujamarianomarydid notverdelaadiounabofetadadecoding process

8

    many to one translation

philipp koehn

emnlp lecture 16

29 february 2008

brujamarianodio una bofetadamarydid notslapverdelaadecoding process

9

    many to one translation

philipp koehn

emnlp lecture 16

29 february 2008

brujamarianodio una bofetadamarydid notslaptheverdea ladecoding process

10

    reordering

philipp koehn

emnlp lecture 16

29 february 2008

brujamarianodio una bofetadaa lamarydid notslapthegreenverdedecoding process

11

    translation    nished

philipp koehn

emnlp lecture 16

29 february 2008

brujamariawitchnoverdemarydid notslapthegreendio una bofetadaa latranslation options

12

    look up possible phrase translations

    many di   erent ways to segment words into phrases
    many di   erent ways to translate each phrase

philipp koehn

emnlp lecture 16

29 february 2008

bofetadaunadioalaverdebrujanomariamarynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslaphypothesis expansion

13

    start with empty hypothesis

    e: no english words
    f: no foreign words covered
    p: id203 1

philipp koehn

emnlp lecture 16

29 february 2008

dioalaverdebrujanomariamarynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslape: f: ---------p: 1unabofetadahypothesis expansion

14

    pick translation option
    create hypothesis

    e: add english phrase mary
    f:    rst foreign word covered
    p: id203 0.534

philipp koehn

emnlp lecture 16

29 february 2008

dioalaverdebrujanomariamarynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslape: maryf: *--------p: .534e: f: ---------p: 1unabofetadaa quick word on probabilities

15

    not going into detail here, but...
    translation model

    phrase translation id203 p(mary|maria)
    reordering costs
    phrase/word count costs
    ...

    language model
    uses trigrams:
    p(mary did not) =

p(mary|start)   p(did|mary,start)    p(not|mary did)

philipp koehn

emnlp lecture 16

29 february 2008

hypothesis expansion

16

    add another hypothesis

philipp koehn

emnlp lecture 16

29 february 2008

dioalaverdebrujanomariamarynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslape: maryf: *--------p: .534e: witchf: -------*-p: .182e: f: ---------p: 1unabofetadahypothesis expansion

17

    further hypothesis expansion

philipp koehn

emnlp lecture 16

29 february 2008

dio una bofetadaalaverdebrujanomariamarynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslape: maryf: *--------p: .534e: witchf: -------*-p: .182e: f: ---------p: 1e: ... slapf: *-***----p: .043hypothesis expansion

18

    ... until all foreign words covered

       nd best hypothesis that covers all foreign words
    backtrack to read o    translation

philipp koehn

emnlp lecture 16

29 february 2008

dio una bofetadabruja verdemariamarynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslape: maryf: *--------p: .534e: witchf: -------*-p: .182e: f: ---------p: 1e: slapf: *-***----p: .043e: did notf: **-------p: .154e: slapf: *****----p: .015e: thef: *******--p: .004283e:green witchf: *********p: .000271a lanohypothesis expansion

19

    adding more hypothesis
    explosion of search space

philipp koehn

emnlp lecture 16

29 february 2008

marynotdid notgiveaslaptothewitchgreenbyto thetogreen witchthe witchdid not givenoa slapslaptheslape: maryf: *--------p: .534e: witchf: -------*-p: .182e: f: ---------p: 1e: slapf: *-***----p: .043e: did notf: **-------p: .154e: slapf: *****----p: .015e: thef: *******--p: .004283e:green witchf: *********p: .000271nodioalaverdebrujanomariaunabofetadaexplosion of search space

20

    number of hypotheses is exponential with respect to sentence length
    decoding is np-complete [knight, 1999]
    need to reduce search space

    risk free: hypothesis recombination
    risky: histogram/threshold pruning

philipp koehn

emnlp lecture 16

29 february 2008

hypothesis recombination

21

    di   erent paths to the same partial translation

philipp koehn

emnlp lecture 16

29 february 2008

p=1marydid not givegivedid notp=0.534p=0.164p=0.092p=0.044p=0.092hypothesis recombination

22

    di   erent paths to the same partial translation
    combine paths

    drop weaker path
    keep pointer from weaker path (for lattice generation)

philipp koehn

emnlp lecture 16

29 february 2008

p=1marydid not givegivedid notp=0.534p=0.164p=0.092p=0.092hypothesis recombination

23

    recombined hypotheses do not have to match completely
    no matter what is added, weaker path can be dropped, if:

    last two english words match (matters for language model)
    foreign word coverage vectors match (e   ects future path)

philipp koehn

emnlp lecture 16

29 february 2008

p=1marydid not givegivedid notp=0.534p=0.164p=0.092joedid not givep=0.092p=0.017hypothesis recombination

24

    recombined hypotheses do not have to match completely
    no matter what is added, weaker path can be dropped, if:

    last two english words match (matters for language model)
    foreign word coverage vectors match (e   ects future path)

    combine paths

philipp koehn

emnlp lecture 16

29 february 2008

p=1marydid not givegivedid notp=0.534p=0.164p=0.092joedid not givep=0.09225

pruning
    hypothesis recombination is not su   cient
    heuristically discard weak hypotheses early
    organize hypothesis in stacks, e.g. by

    same foreign words covered
    same number of foreign words covered
    same number of english words produced

    compare hypotheses in stacks, discard bad ones

    histogram pruning: keep top n hypotheses in each stack (e.g., n=100)
    threshold pruning: keep hypotheses that are at most    times the cost of

best hypothesis in stack (e.g.,    = 0.001)

philipp koehn

emnlp lecture 16

29 february 2008

hypothesis stacks

26

    organization of hypothesis into stacks

    here: based on number of foreign words translated
    during translation all hypotheses from one stack are expanded
    expanded hypotheses are placed into stacks

philipp koehn

emnlp lecture 16

29 february 2008

123456comparing hypotheses

27

    comparing hypotheses with same number of foreign words covered

    hypothesis that covers easy part of sentence is preferred
    need to consider future cost of uncovered parts

philipp koehn

emnlp lecture 16

29 february 2008

maria noe: mary did notf: **-------p: 0.154a lae: thef: -----**--p: 0.354dio una bofetadabruja verdebetterpartialtranslationcoverseasier part--> lower costfuture cost estimation

28

    estimate cost to translate remaining part of input
    step 1: estimate future cost for each translation option

    look up translation model cost
    estimate language model cost (no prior context)
    ignore reordering model cost
    lm * tm = p(to) * p(the|to) * p(to the|a la)

philipp koehn

emnlp lecture 16

29 february 2008

a lato thefuture cost estimation: step 2

29

    step 2:    nd cheapest cost among translation options

philipp koehn

emnlp lecture 16

29 february 2008

a lato thetothecost = 0.0372cost = 0.0299cost = 0.0354future cost estimation: step 3

30

    step 3:    nd cheapest future cost path for each span
    can be done e   ciently by id145
    future cost for every span can be pre-computed

philipp koehn

emnlp lecture 16

29 february 2008

bofetadaunadioalaverdebrujanomariabofetadaunadioalaverdebrujanomariafuture cost estimation: application

31

    use future cost estimates when pruning hypotheses
    for each uncovered contiguous span:

    look up future costs for each maximal contiguous uncovered span
    add to actually accumulated cost for translation option for pruning

philipp koehn

emnlp lecture 16

29 february 2008

dio una bofetadaalaverdebrujanomariamaryslape: maryf: *--------p: .534e: f: ---------p: 1e: ... slapf: *-***----p: .043futurecostfuturecostcoveredcoveredfc: .0006672 p*fc:.000029 0.10.006672*id67

32

    pruning might drop hypothesis that lead to the best path (search error)
    id67: safe pruning

    future cost estimates have to be accurate or underestimates
    lower bound for id203 is established early by

depth    rst search: compute cost for one complete translation

    if cost-so-far and future cost are worse than lower bound, hypothesis can be

safely discarded

    not commonly done, since not aggressive enough

philipp koehn

emnlp lecture 16

29 february 2008

limits on reordering

33

    reordering may be limited

    monotone translation: no reordering at all
    only phrase movements of at most n words

    reordering limits speed up search (polynomial instead of exponential)
    current reordering models are weak, so limits improve translation quality

philipp koehn

emnlp lecture 16

29 february 2008

word lattice generation

34

    search graph can be easily converted into a word lattice

    can be further mined for n-best lists
    enables reranking approaches
    enables discriminative training

philipp koehn

emnlp lecture 16

29 february 2008

p=1marydid not givegivedid notp=0.534p=0.164p=0.092joedid not givep=0.092marydid not givegivedid notjoedid not givesample n-best list

35

    simple n-best list:

translation ||| reordering lm tm wordpenalty ||| score
this is a small house ||| 0 -27.0908 -1.83258 -5 ||| -28.9234
this is a little house ||| 0 -28.1791 -1.83258 -5 ||| -30.0117
it is a small house ||| 0 -27.108 -3.21888 -5 ||| -30.3268
it is a little house ||| 0 -28.1963 -3.21888 -5 ||| -31.4152
this is an small house ||| 0 -31.7294 -1.83258 -5 ||| -33.562
it is an small house ||| 0 -32.3094 -3.21888 -5 ||| -35.5283
this is an little house ||| 0 -33.7639 -1.83258 -5 ||| -35.5965
this is a house small ||| -3 -31.4851 -1.83258 -5 ||| -36.3176
this is a house little ||| -3 -31.5689 -1.83258 -5 ||| -36.4015
it is an little house ||| 0 -34.3439 -3.21888 -5 ||| -37.5628
it is a house small ||| -3 -31.5022 -3.21888 -5 ||| -37.7211
this is an house small ||| -3 -32.8999 -1.83258 -5 ||| -37.7325
it is a house little ||| -3 -31.586 -3.21888 -5 ||| -37.8049
this is an house little ||| -3 -32.9837 -1.83258 -5 ||| -37.8163
the house is a little ||| -7 -28.5107 -2.52573 -5 ||| -38.0364
the is a small house ||| 0 -35.6899 -2.52573 -5 ||| -38.2156
is it a little house ||| -4 -30.3603 -3.91202 -5 ||| -38.2723
the house is a small ||| -7 -28.7683 -2.52573 -5 ||| -38.294
it    s a small house ||| 0 -34.8557 -3.91202 -5 ||| -38.7677
this house is a little ||| -7 -28.0443 -3.91202 -5 ||| -38.9563
it    s a little house ||| 0 -35.1446 -3.91202 -5 ||| -39.0566
this house is a small ||| -7 -28.3018 -3.91202 -5 ||| -39.2139

philipp koehn

emnlp lecture 16

29 february 2008

xml markup

36

er erzielte <number english=   17.55   >17,55</number> punkte .
    add additional translation options

    number translation
    name translation
    additional options

    provide multiple translations
    provide id203 distribution along with translations
    allow bypassing of provided translations

philipp koehn

emnlp lecture 16

29 february 2008

