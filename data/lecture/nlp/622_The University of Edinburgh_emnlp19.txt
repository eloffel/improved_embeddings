empirical methods in natural language processing

lecture 19

machine translation (vi):

factored translation models

philipp koehn

10 march 2008

philipp koehn

emnlp lecture 19

10 march 2008

id151 today

1

    best performing methods based on phrases

    short sequences of words
    no use of explicit syntactic information
    no use of morphological information
    currently best performing method
    progress in syntax-based translation

    tree transfer models using syntactic annotation
    still no use of morphological information
    slower, more complex, and lower translation quality
    active research, closing the performance gap?

philipp koehn

emnlp lecture 19

10 march 2008

morphology for machine translation

    models treat car and cars as completely di   erent words

2

    training occurrences of car have no e   ect on learning translation of cars
    if we only see car, we do not know how to translate cars
    rich morphology (german, arabic, finnish, czech, ...)     many word forms

    better approach

    analyze surface word forms into lemma and morphology, e.g.: car +plural
    translate lemma and morphology separately
    generate target surface form

philipp koehn

emnlp lecture 19

10 march 2008

factored translation models

3

    factored represention of words

surface
lemma

part of speech     part of speech

surface
lemma

morphology
word class

morphology
word class

...

...

    goals

    generalization, e.g. by translating lemmas, not surface forms
    richer model, e.g. using syntax for reordering, id38)

philipp koehn

emnlp lecture 19

10 march 2008

decomposing translation: example

4

    translate lemma and syntactic information separately

lemma

   

lemma

part-of-speech

morphology     morphology

part-of-speech

philipp koehn

emnlp lecture 19

10 march 2008

decomposing translation: example

5

    generate surface form on target side

surface   

lemma

part-of-speech

morphology

philipp koehn

emnlp lecture 19

10 march 2008

translation process

6

    extension of phrase model

    translation step is one-to-one mapping of word sequences

    mapping of foreign words into english words broken up into steps

    translation step: maps foreign factors into english factors
    generation step: maps english factors into english factors

    order of mapping steps is chosen to optimize search

philipp koehn

emnlp lecture 19

10 march 2008

translation process: example

7

input: (autos, auto, nns)
1. translation step: lemma     lemma

(?, car, ?), (?, auto, ?)

2. generation step: lemma     part-of-speech

(?, car, nn), (?, car, nns), (?, auto, nn), (?, auto, nns)

3. translation step: part-of-speech     part-of-speech

(?, car, nn), (?, car, nns), (?, auto, nnp), (?, auto, nns)

4. generation step: lemma,part-of-speech     surface

(car, car, nn), (cars, car, nns), (auto, auto, nn), (autos, auto, nns)

philipp koehn

emnlp lecture 19

10 march 2008

integration with factored language models
    factored language models: back-o    to factors with richer statistics

8

    if preceding word is rare, current word hard to predict
    back-o    to part-of-speech tags
    example

    count(scotland is) = count(scotland    sh) = count(scotland yellow) = 0
    count(nnp is) > count(nnp    sh) > count(nnp yellow)

    gains shown for id103 and translation

philipp koehn

emnlp lecture 19

10 march 2008

richer models for machine translation

9

    reordering is often due to syntactic reasons

    french-english: nn adj     adj nn
    chinese-english: nn1 f nn2     nn1 nn2
    arabic-english: vb nn     nn vb

    syntactic coherence may be modeled using syntactic tags

    id165 models of part-of-speech tags may aid grammaticality of output
    sequence models over morphological tags may aid agreement (e.g., case,

number, and gender agreement in noun phrases)

philipp koehn

emnlp lecture 19

10 march 2008

adding linguistic markup to output

10

    high order language models over pos
    motivation: syntactic tags should enforce syntactic sentence structure
    results: no major impact with 7-gram pos model
    analysis: local grammatical coherence already fairly good, pos sequence lm

model not strong enough to support major restructuring

philipp koehn

emnlp lecture 19

10 march 2008

wordwordpart-of-speechoutputinputlocal agreement (esp. within noun phrases)

11

    high order language models over pos and morphology
    motivation

    det-sgl noun-sgl good sequence
    det-sgl noun-plural bad sequence

philipp koehn

emnlp lecture 19

10 march 2008

wordwordpart-of-speechoutputinputmorphology12

agreement within noun phrases

    experiment: 7-gram pos, morph lm in addition to 3-gram word lm
    results

method
baseline

factored model

agreement errors in np

15% in np     3 words
4% in np     3 words

devtest

test

18.22 id7 18.04 id7
18.25 id7 18.22 id7

    example

... zur zwischenstaatlichen methoden ...

... zu zwischenstaatlichen methoden ...

    baseline:
    factored model:

    example

    baseline:
    factored model:

... das zweite wichtige   anderung ...

... die zweite wichtige   anderung ...

philipp koehn

emnlp lecture 19

10 march 2008

morphological generation model

13

    our motivating example
    translating lemma and morphological information more robust

philipp koehn

emnlp lecture 19

10 march 2008

lemmalemmapart-of-speechoutputinputmorphologypart-of-speechwordwordinitial results

14

    results on 1 million word news commentary corpus (german   english)

system
baseline

with pos lm
morphgen model

in-doman out-of-domain

18.19
19.05
14.38

15.01
15.03
11.65

    what went wrong?

    why back-o    to lemma, when we know how to translate surface forms?
    loss of information

philipp koehn

emnlp lecture 19

10 march 2008

solution: alternative decoding paths

15

    allow both surface form translation and morphgen model

    prefer surface model for known words
    morphgen model acts as back-o   

philipp koehn

emnlp lecture 19

10 march 2008

lemmalemmapart-of-speechoutputinputmorphologypart-of-speechwordwordorresults

16

    model now beats the baseline:

system
baseline

with pos lm
morphgen model
both model paths

in-doman out-of-domain

18.19
19.05
14.38
19.47

15.01
15.03
11.65
15.23

philipp koehn

emnlp lecture 19

10 march 2008

adding annotation to the source

    source words may contain insu   cient information to map phrases

17

    english-german: what case for noun phrases?
    chinese-english: plural or singular
    pronoun translation: what do they refer to?

    idea:

add additional

information to the source that makes the required

information available locally (where it is needed)

philipp koehn

emnlp lecture 19

10 march 2008

case information for english   german

18

    detect in english, if noun phrase is subject/object (using parse tree)
    map information into case morphology of german
    use case morphology to generate correct word form

philipp koehn

emnlp lecture 19

10 march 2008

outputinputcasewordwordsubject/object19

factored models: open questions

    what is the best decomposition into translation and generation steps?
    same segmentation for all translation steps?
    what information is useful?

    translation: mostly lexical, or lemmas for richer statistics
    reordering: syntactic information useful
    language model: syntactic information for overall grammatical coherence

    use of annotation tools vs. automatically discovered word classes
    other decoding steps besides phrase translation and word generation?

philipp koehn

emnlp lecture 19

10 march 2008

