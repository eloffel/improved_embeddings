empirical methods in natural language processing

lecture 12

text classi   cation and id91

philipp koehn

14 february 2008

philipp koehn

emnlp lecture 12

14 february 2008

type of learning problems

1

    supervised learning

    labeled training data
    methods: id48, naive bayes, maximum id178, transformation-based

learning, decision lists, ...

    example: id38, id52 with labeled corpus

    unsupervised learning

    labels have to be automatically discovered
    method: id91 (this lecture)

philipp koehn

emnlp lecture 12

14 february 2008

2

semi-supervised learning
    some of the training data is labeled, vast majority is not
    boostrapping

    train initial classi   er on labeled data
    label additional data with initial classi   er
    iterate

    active learning

    train initial classi   er with con   dence measure
    request from human annotator to label most informative examples

philipp koehn

emnlp lecture 12

14 february 2008

goals of learning

3

    density estimation: p(x)

    learn the distribution of a random variable
    example: id38

    classi   cation: p(c|x)

    predict correct class (from a    nite set)
    example: part-of-speech tagging, id51

    regression: p(x, y)

    predicting a function f(x) = y with real-numbered input and output
    rare in natural languages (words are discrete, not continuous)

philipp koehn

emnlp lecture 12

14 february 2008

text classi   cation

4

    classi   cation problem
    first, supervised methods

    the usual suspects
    classi   cation by id38

    then, unsupervised methods

    id91

philipp koehn

emnlp lecture 12

14 february 2008

the task

5

    the task

    given a set of documents
    sort them into categories

    example

    sorting news stories into: politics, sports, arts, etc.
    classifying job adverts into job types: clerical, teaching, ...
       ltering email into spam and no-spam

philipp koehn

emnlp lecture 12

14 february 2008

the usual approach

6

    represent document by features

    words
    bigrams, etc.
    word senses
    syntactic relations

    learn a model that predicts a category using the features

    naive bayes argmaxcp(c) q

    maximum id178 argmaxc
    decision/transformation rules {f0     cj, ..., fn     ck}

q
i p(c|fi)
i   fi

i

1
z

    set-up very similar to id51

philipp koehn

emnlp lecture 12

14 february 2008

id38 approach

7

    collect documents for each class
    train a language model pc
    classify a new document d by

lm for each class c separately

argmaxcpc

lm(d)

    intuition: which language model most likely produces the document?
    e   ectively uses words and id165 features

philipp koehn

emnlp lecture 12

14 february 2008

id91

8

    unsupervised learning

    given: a set of documents
    wanted: grouping into appropriate classes

    agglomerative id91

    group the two most similar documents together
    repeat

philipp koehn

emnlp lecture 12

14 february 2008

agglomerative id91

9

    start: 9 documents, 9 classes

   

   

   

   

   

   

   

   

   

d2

d1

d5
    documents d3 and d7 are most similar
   

d4

d3

   

   

d2

d1

d4
    documents d1 and d5 are most similar
   
,,,
lll
d3
d7

   
,,,
lll
d1
d5

d2

   

   
,,,
lll
d3
d7

d6

d7

d8

d9

   

   

   

   

d5

d6

d8

d9

   

   

   

   

d4

d6

d8

d9

philipp koehn

emnlp lecture 12

14 february 2008

agglomerative id91 (2)

10

    documents d6 and d8 are most similar

   

   
,,,
lll
d1
d5

   
,,,
lll
d6
d8
    document d4 and class {d8, d6} are most similar

   
,,,
lll
d3
d7

d2

d4

   

   
,,,
lll
d1
d5

   

d2

   
aaaaaa
,,,
   
d4
,,,
lll
d3
d7

   
,,,
lll
d6
d8

   

d9

   

d9

philipp koehn

emnlp lecture 12

14 february 2008

agglomerative id91 (3)

11

   
,,,
lll
d1
d5

    document d2 and class {d6, d8} are most similar
   
aaaaaa
,,,
   
d2
lll
,,,
d8
d6
    document d9 and class {d3, d4, d7} are most similar

   
aaaaaa
,,,
   
d4
lll
,,,
d3
d7

   

d9

   
,,,
lll
d1
d5

   
xxxxxxxx
,,,
   
aaaaaa
d9
,,,
   
d4
,,,
lll
d3
d7

   
aaaaaa
,,,
   
d2
,,,
lll
d6
d8

philipp koehn

emnlp lecture 12

14 february 2008

agglomerative id91 (4)

12

    class {d1, d5} and class {d2, d6, d8} are most similar

   
        
ppppppp
   
   
aaaaaa
,,,
lll
,,,
   
d2
d1
d5
lll
,,,
d8
d6
    if we stop now, we have two classes

   
xxxxxxxx
,,,
   
aaaaaa
d9
,,,
   
d4
lll
,,,
d3
d7

philipp koehn

emnlp lecture 12

14 february 2008

similarity

13

    we loosely used the concept similarity
    how do we know how similar two documents are?
    how do we represent documents in the    rst place?

philipp koehn

emnlp lecture 12

14 february 2008

vector representation of documents

documents are represented by a vector of
word counts.

example document
manchester united won 2     1 against
chelsea , barcelona tied madrid 1     1 ,
and bayern m  unchen won 4     2 against
n  urnberg

the word counts may be normalized, so
all the vector components add up to one.

manchester

united

won

2
   
1

against
chelsea

,

barcelona

tied

madrid

and

bayern
m  unchen

4

n  urnberg

14

0bbbbbbbbbbbbbbbbbbbbbbbbbbbbb@

1ccccccccccccccccccccccccccccca

1
1
2
2
3
3
2
1
2
1
1
1
1
1
1
1
1

1ccccccccccccccccccccccccccccca

0bbbbbbbbbbbbbbbbbbbbbbbbbbbbb@

0.04
0.04
0.08
0.08
0.12
0.12
0.08
0.04
0.08
0.04
0.04
0.04
0.04
0.04
0.04
0.04
0.04

philipp koehn

emnlp lecture 12

14 february 2008

    a popular similarity metric for vectors is the cosine

similarity

pm
ppm
i=1 xi   pm
i=1 xi    yi

i=1 yi

15

=       x          y

sim(      x ,      y ) =

    we also need to de   ne the similarity between

    a document and a class
    two classes

philipp koehn

emnlp lecture 12

14 february 2008

similarity with classes

16

    single link

    merge two classes based on similarity of their most similar members

    compete link

    merge two classes based on similarity of their least similar members

    group average

    de   ne class vector, or center of class, as

x

      x    c

      x

      c =

1
m

    compare with other vectors using similarity metric

philipp koehn

emnlp lecture 12

14 february 2008

additional considerations

17

    stop words

    words such as and and the are very frequent and not very informative
    we may want to ignore them

    complexity

with every other document

    at any point in the id91 algorithm, we have to compare every document
    complexity quadratic with the number of documents o(n2)
    when do we stop?

    when we have a pre-de   ned number of classes
    when the lowest similarity is higher than a pre-de   ned threshold

philipp koehn

emnlp lecture 12

14 february 2008

18

other id91 methods
    top-down hierarchical id91, or divisive id91

    start with one class
    divide up classes that are least coherent

    id116 id91

    create initial clusters with arbitrary center of cluster
    assign documents to the cluster with the closests center
    compute center of cluster
    iterate until convergence

philipp koehn

emnlp lecture 12

14 february 2008

