empirical methods in natural language processing

id38 (ii): smoothing and back-o   

lecture 4

philipp koehn

17 january 2008

pk

emnlp

17 january 2008

id38 example

1

    training set

there is a big house

i buy a house

they buy the new house

    model

p(big|a) = 0.5
p(house|a) = 0.5
p(new|the) = 1

p(a|is) = 1

p(is|there) = 1
p(buy|i) = 1
p(house|big) = 1
p(house|new) = 1

p(buy|they) = 1
p(a|buy) = 0.5
p(the|buy) = 0.5

p(they| < s >) = .333

    test sentence s: they buy a big house
    p(s) = 0.333

   0.5|{z}big

   1|{z}

house

   1|{z}buy

   0.5|{z}a

|{z}

they

= 0.0833

pk

emnlp

17 january 2008

2

evaluation of language models

    we want to evaluate the quality of language models
    a good language model gives a high id203 to real english
    we measure this with cross id178 and perplexity

pk

emnlp

17 january 2008

3

= 0.0833

cross-id178

    average id178 of each word prediction
    example:

p(s) = 0.333
they

|{z}
h(p, m) =    1
5
=    1
5
=    1
5

   1|{z}

   1|{z}buy
   0.5|{z}a
   0.5|{z}big
}
|
{z
|{z}buy
| {z }a
+ 0|{z}buy
+    1|{z}big
+    1|{z}a
| {z }

+ log 0.5

(   1.586

(log 0.333

log p(s)

+ log 1

they

they

house

+ log 0.5

| {z }
+ 0|{z}

big

house

|{z}

+ log 1
house

)

) = 0.7173

pk

emnlp

17 january 2008

perplexity

4

    perplexity is de   ned as

p p = 2h(p,m)

pn
i=1 log m(wn|w1,...,wn   1)

= 2    1

n

    in out example h(m, p) = 0.7173     p p = 1.6441
    intuitively, perplexity is the average number of choices at each point (weighted

by the model)

    perplexity is the most common measure to evaluate language models

pk

emnlp

17 january 2008

perplexity example

5

prediction

plm(i|</s><s>)
plm(would|<s>i)
plm(like|i would)
plm(to|would like)
plm(commend|like to)
plm(the|to commend)

plm(on|the rapporteur)
plm(his|rapporteur on)

plm(work|on his)
plm(.|his work)
plm(</s>|work .)

plm(rapporteur|commend the)

plm

-log2 plm

0.109043
0.144482
0.489247
0.904727
0.002253
0.471831
0.147923
0.056315
0.193806
0.088528
0.290257
0.999990
average

3.197
2.791
1.031
0.144
8.794
1.084
2.763
4.150
2.367
3.498
1.785
0.000

2.633671

pk

emnlp

17 january 2008

perplexity for lm of di   erent order

6

word

i

would

like
to

commend

the

rapporteur

on
his
work

.

</s>
average
perplexity

unigram bigram trigram 4-gram
3.197
2.791
1.290
0.113
8.633
0.880
2.350
1.862
1.978
2.394
1.510
0.000
2.251
4.758

6.684
8.342
9.129
5.081
15.487
3.885
10.840
6.765
10.678
9.993
4.896
4.828
8.051
265.136

3.197
2.884
2.026
0.402
12.335
1.402
7.319
4.140
7.316
4.816
3.020
0.005
4.072
16.817

3.197
2.791
1.031
0.144
8.794
1.084
2.763
4.150
2.367
3.498
1.785
0.000
2.634
6.206

pk

emnlp

17 january 2008

recap from last lecture

7

    if we estimate probabilities solely from counts, we give id203 0 to unseen

events (bigrams, trigrams, etc.)

    one attempt to address this was with add-one smoothing.

pk

emnlp

17 january 2008

add-one smoothing: results

8

church and gale (1991a) experiment: 22 million words training, 22 million words
testing, from same domain (ap news wire), counts of bigrams:

frequency r actual frequency expected frequency
in training

in test (add one)

in test

0
1
2
3
4
5

0.000027

0.448
1.25
2.24
3.23
4.21

0.000132
0.000274
0.000411
0.000548
0.000685
0.000822

we overestimate 0-count bigrams (0.000132 > 0.000027), but since there are so
many, they use up so much id203 mass that hardly any is left.

pk

emnlp

17 january 2008

deleted estimation: results

9

    much better:

frequency r actual frequency
in training

in test

expected frequency
in test (good turing)

0
1
2
3
4
5

0.000027

0.000037

0.448
1.25
2.24
3.23
4.21

0.396
1.24
2.23
3.22
4.22

    still overestimates unseen bigrams (why?)

pk

emnlp

17 january 2008

good-turing discounting

10

    method based on the assumption of binomial distribution of frequencies.
    translate real counts r for words with adjusted counts r   :

r    = (r + 1)nr+1
nr

nr is the count of counts: number of words with frequency r.

    the id203 mass reserved for unseen events is n1/n .
    for large r (where nr   1 is often 0), so various other methods can be applied
(don   t adjust counts, curve    tting to id75). see manning+sch  utze
for details.

pk

emnlp

17 january 2008

good-turing discounting: results

11

    almost perfect:

frequency r actual frequency
in training

in test

expected frequency
in test (good turing)

0
1
2
3
4
5

0.000027

0.000027

0.448
1.25
2.24
3.23
4.21

0.446
1.26
2.24
3.24
4.22

pk

emnlp

17 january 2008

is smoothing enough?

12

    if two events (bigrams, trigrams) are both seen with the same frequency, they

are given the same id203.

id165

count

scottish beer is

scottish beer green

beer is

beer green

0
0
45
0

    if there is not su   cient evidence, we may want to back o    to lower-order

id165s

pk

emnlp

17 january 2008

combining estimators

13

    we would like to use high-order id165 language models
    ... but there are many ngrams with count 0.
    linear interpolation pli of estimators pn of di   erent order n:

pli(wn|wn   2, wn   1) =   1 p1(wn)

+   2 p2(wn|wn   1)
+   3 p1(wn|wn   2, wn   1)

      1 +   2 +   3 = 1

pk

emnlp

17 january 2008

recursive interpolation

14

    interpolation can also be de   ned recursively

pi(wn|wn   2, wn   1) =

  (wn   2, wn   1)

p(wn|wn   2, wn   1)

+ (1       (wn   2, wn   1)) pi(wn|wn   1)

    how do we set the   (wn   2, wn   1) parameters?

    consider count(wn   2, wn   1)
    for higher counts of history:
    higher values of   (wn   2, wn   1)
    less id203 mass reserved for unseen events

pk

emnlp

17 january 2008

witten-bell smoothing

15

    count of history may not be fully adequate

    constant occurs 993 in europarl corpus, 415 di   erent words follow
    spite occurs 993 in europarl corpus, 9 di   erent words follow

    witten-bell smoothing uses diversity of history
    reserved id203 for unseen events:
415+993 = 0.295

    1       (constant) = 415
    1       (spite) = 9

9+993 = 0.009

pk

emnlp

17 january 2008

    another approach is to back-o    to lower order id165 language models

back-o   

16

                                       

  (wn|wn   2, wn   1)

if count(wn   2, wn   1, wn) > 0

  (wn   2, wn   1) pbo(wn|wn   1)

otherwise

pbo(wn|wn   2, wn   1) =

    each trigram id203 distribution is changed to a function    that reserves

some id203 mass for unseen events: p

w   (wn|wn   2, wn   1) < 1

    the remaining id203 mass is used in the weight   (wn   2, wn   1), which is

given to the back-o    path.

pk

emnlp

17 january 2008

back-o    with good turing discounting

    good turing discounting is used for all positive counts

17

p(big|a)
p(house|a)
p(new|a)

count

p

gt count

  

3
3
1

3

3

7 = 0.43
7 = 0.43
7 = 0.14

1

2.24
2.24
0.446

2.24

2.24

7 = 0.32
7 = 0.32
7 = 0.06

0.446

    1     (0.32 + 0.32 + 0.06) = 0.30 is left for back-o      (a)
    note: actual value for    is slightly higher, since the predictions of the lower-

order model to seen events at this level are not used.

pk

emnlp

17 january 2008

absolute discounting

18

    subtract a    xed number d from each count

  (wn|w1, ..., wn   1) = c(w1, ..., wn)     d
w c(w1, ..., wn   1, w)

p

    typical counts 1 and 2 are treated di   erently

pk

emnlp

17 january 2008

19

consider diversity of histories

    words di   er in the number of di   erent history they follow

    foods, indicates, providers occur 447 times each in europarl
    york also occurs 447 times in europarl
    but: york almost always follows new

    when building a unigram model for back-o   

    what is a good value for p(foods) ?
    what is a good value for p(york) ?

pk

emnlp

17 january 2008

kneser-ney smoothing

20

    currently most popular smoothing method
    combines

    absolute discounting
    considers diversity of predicted words for back-o   
    considers diversity of histories for lower order id165 models
    interpolated version: always add in back-o    probabilities

pk

emnlp

17 january 2008

perplexity for di   erent language models

    trained on english europarl corpus, ignoring trigram and 4-gram singletons

21

smoothing method
good-turing
witten-bell
modi   ed kneser-ney
interpolated modi   ed kneser-ney

bigram trigram 4-gram

96.2
97.1
95.4
94.5

62.9
63.8
61.6
59.3

59.9
60.4
58.6
54.0

pk

emnlp

17 january 2008

22

other methods in id38

    id38 is still an active    eld of research
    there are many back-o    and interpolation methods
    skip id165 models: back-o    to p(wn|wn   2)
    factored language models: back-o    to word stems, part-of-speech tags
    syntactic language models: using parse trees
    language models trained on billions and trillions of words

pk

emnlp

17 january 2008

