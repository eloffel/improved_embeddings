empirical methods in natural language processing

lecture 1

introduction (i): words and id203

philipp koehn

lecture given by tommy herbert

7 january 2008

pk

emnlp

7 january 2008

welcome to emnlp

1

    lecturer: philipp koehn
    ta: tommy herbert
    lectures: mondays and thursdays, 17:10, dht 4.18
    practical sessions: 4 extra sessions
    project (worth 30%) will be given out next week
    exam counts for 70% of the grade

pk

emnlp

7 january 2008

outline

2

    introduction: words, id203, id205, id165s and language

modeling
    methods:
id91

tagging,    nite state machines,

statistical modeling, parsing,

    applications: id51,

information retrieval,

text

categorisation, summarisation, information extraction, id53

    id151

pk

emnlp

7 january 2008

references

3

    manning and sch  utze:    foundations of statistical language processing   ,

1999, mit press, available online

    jurafsky and martin:    speech and language processing   , 2000, prentice hall.
    koehn:    id151   , 2007, cambridge university press,

not yet published.

    also: research papers, other handouts

pk

emnlp

7 january 2008

4

what are empirical methods in natural language

processing?

    empirical methods: work on corpora using statistical models or other machine

learning methods

    natural language processing: computational linguistics vs. natural language

processing

pk

emnlp

7 january 2008

quotes

5

it must be recognized that the notion    id203 of a sentence    is an

entirely useless one, under any known interpretation of this term.
noam chomsky, 1969

whenever i    re a linguist our system performance improves.

frederick jelinek, 1988

pk

emnlp

7 january 2008

con   icts?

6

    scientist vs. engineer
    explaining language vs. building applications
    rationalist vs. empiricist
    insight vs. data analysis

pk

emnlp

7 january 2008

why is language hard?

7

    ambiguities on many levels
    rules, but many exceptions
    no clear understand how humans process language
    ignore humans, learn from data?

pk

emnlp

7 january 2008

language as data

8

a lot of text is now available in digital form
    billions of words of news text distributed by the ldc
    billions of documents on the web (trillion of words?)
    ten thousands of sentences annotated with syntactic trees for a number of

languages (around one million words for english)

    10s   100s of million words translated between english and other languages

pk

emnlp

7 january 2008

word counts

9

one simple statistic: counting words in mark twain   s tom sawyer:

word count
the
3332
2973
and
1775
1725
1440
1161
1027
906
877

a
to
of
was
it
in
that

from manning+sch  utze, page 21

pk

emnlp

7 january 2008

counts of counts

count

count of count

10

1
2
3
4
5
6
7
...
10

11-50
51-100
> 100

3993
1292
664
410
243
199
172
...
91
540
99
102

    3993 singletons (words that
occur only once in the text)
    most words occur only a very

few times.

    most of the text consists of
a few hundred high-frequency
words.

pk

emnlp

7 january 2008

zipf   s law

11

zipf   s law: f    r = k
count f

rank r

1
2
3
10
20
30
100
1000
8000

word
the
and

a
he
but
be
two

family

applausive

3332
2973
1775
877
410
294
104
8
1

f    r
3332
5944
5235
8770
8400
8820
10400
8000
8000

pk

emnlp

7 january 2008

probabilities

12

    given word counts we can estimate a id203 distribution:

p

p (w) = count(w)

w0 count(w0)

    this type of estimation is called id113. why? we

will get to that later.

    estimating probabilities based on frequencies is called the frequentist approach

to id203.

    this id203 distribution answers the question: if we randomly pick a word

out of a text, how likely will it be word w?

pk

emnlp

7 january 2008

a bit more formal

13

    we introduced a random variable w .
    we de   ned a id203 distribution p, that tells us how likely the variable

w is the word w:

prob(w = w) = p(w)

pk

emnlp

7 january 2008

joint probabilities

14

    sometimes, we want to deal with two random variables at the same time.
    example: words w1 and w2 that occur in sequence (a bigram)

we model this with the distribution: p(w1, w2)

    if the occurrence of words in bigrams is independent, we can reduce this to

p(w1, w2) = p(w1)p(w2). intuitively, this not the case for word bigrams.

    we can estimate joint probabilities over two variables the same way we

estimated the id203 distribution over a single variable:
p(w1, w2) =

count(w1,w2)

p

w10,w20 count(w10,w20)

pk

emnlp

7 january 2008

conditional probabilities

15

    another useful concept is id155

p(w2|w1)
it answers the question:
value for the second random variable w2?

if the random variable w1 = w1, how what is the

    mathematically, we can de   ne id155 as

p(w2|w1) = p(w1,w2)

p(w1)

    if w1 and w2 are independent: p(w2|w1) = p(w2)

pk

emnlp

7 january 2008

chain rule

16

    a bit of math gives us the chain rule:

p(w2|w1) = p(w1,w2)
p(w1) p(w2|w1) = p(w1, w2)

p(w1)

    what if we want to break down large joint probabilities like p(w1, w2, w3)?

we can repeatedly apply the chain rule:
p(w1, w2, w3) = p(w1) p(w2|w1) p(w3|w1, w2)

pk

emnlp

7 january 2008

bayes rule

17

    finally, another important rule: bayes rule

p(x|y) = p(y|x) p(x)

p(y)

    it can easily derived from the chain rule:

p(x, y) = p(x, y)
p(x|y) p(y) = p(y|x) p(x)
p(x|y) = p(y|x) p(x)

p(y)

pk

emnlp

7 january 2008

