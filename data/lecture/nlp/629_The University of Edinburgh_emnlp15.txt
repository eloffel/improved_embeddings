empirical methods in natural language processing

lecture 15

machine translation (ii):

word-based models and the em algorithm

philipp koehn

25 february 2008

philipp koehn

emnlp lecture 15

25 february 2008

lexical translation

1

    how to translate a word     look up in dictionary

haus     house, building, home, household, shell.

    multiple translations

    some more frequent than others
    for instance: house, and building most common
    special cases: haus of a snail is its shell

    note: during all the lectures, we will translate from a foreign language into

english

philipp koehn

emnlp lecture 15

25 february 2008

collect statistics

2

    look at a parallel corpus (german text along with english translation)

translation of haus count
8,000
house
building
1,600
200
home
household
150
50
shell

philipp koehn

emnlp lecture 15

25 february 2008

estimate translation probabilities

3

    id113

                                             

pf(e) =

if e = house,
if e = building,
if e = home,

0.8
0.16
0.02
0.015 if e = household,
0.005 if e = shell.

philipp koehn

emnlp lecture 15

25 february 2008

alignment

4

    in a parallel text (or when we translate), we align words in one language with

the words in the other

    word positions are numbered 1   4

philipp koehn

emnlp lecture 15

25 february 2008

dashausistkleinthehouseissmall123412345

alignment function
    formalizing alignment with an alignment function
    mapping an english target word at position i to a german source word at

position j with a function a : i     j

    example

a : {1     1, 2     2, 3     3, 4     4}

philipp koehn

emnlp lecture 15

25 february 2008

reordering

6

    words may be reordered during translation

a : {1     3, 2     4, 3     2, 4     1}

philipp koehn

emnlp lecture 15

25 february 2008

dashausistkleinthehouseissmall12341234one-to-many translation
    a source word may translate into multiple target words

7

a : {1     1, 2     2, 3     3, 4     4, 5     4}

philipp koehn

emnlp lecture 15

25 february 2008

dashausistklitzekleinthehouseisverysmall123412345dropping words

8

    words may be dropped when translated

    the german article das is dropped

a : {1     2, 2     3, 3     4}

philipp koehn

emnlp lecture 15

25 february 2008

dashausistkleinhouseissmall1231234inserting words

9

    words may be added during translation

    the english just does not have an equivalent in german
    we still need to map it to something: special null token

a : {1     1, 2     2, 3     3, 4     0, 5     4}

philipp koehn

emnlp lecture 15

25 february 2008

dashausistkleinthehouseisjustsmallnull1234123450ibm model 1

10

    generative model: break up translation process into smaller steps

    ibm model 1 only uses lexical translation

    translation id203

    for a foreign sentence f = (f1, ..., flf) of length lf
    to an english sentence e = (e1, ..., ele) of length le
    with an alignment of each english word ej to a foreign word fi according to

the alignment function a : j     i

p(e, a|f) =

 

(lf + 1)le
    parameter   is a id172 constant

ley

j=1

t(ej|fa(j))

philipp koehn

emnlp lecture 15

25 february 2008

11

example

haus

e
house
building
home
household
shell

t(e|f)
0.8
0.16
0.02
0.015
0.005

ist

t(e|f)
0.8
0.16
0.02
0.015
0.005

e
is
   s
exists
has
are

klein

e
small
little
short
minor
petty

t(e|f)
0.4
0.4
0.1
0.06
0.04

das

e
the
that
which
who
this

t(e|f)
0.7
0.15
0.075
0.05
0.025

43    t(the|das)    t(house|haus)    t(is|ist)    t(small|klein)
p(e, a|f) =  
43    0.7    0.8    0.8    0.4
=  
= 0.0028 

philipp koehn

emnlp lecture 15

25 february 2008

learning lexical translation models

12

    we would like to estimate the lexical translation probabilities t(e|f) from a

parallel corpus

    ... but we do not have the alignments
    chicken and egg problem
    if we had the alignments,

    we could estimate the parameters of our generative model
    we could estimate the alignments

    if we had the parameters,

philipp koehn

emnlp lecture 15

25 february 2008

em algorithm

13

    incomplete data

    if we had complete data, would could estimate model
    if we had model, we could    ll in the gaps in the data

    expectation maximization (em) in a nutshell

    initialize model parameters (e.g. uniform)
    assign probabilities to the missing data
    estimate model parameters from completed data
    iterate

philipp koehn

emnlp lecture 15

25 february 2008

em algorithm

14

    initial step: all alignments equally likely
    model learns that, e.g., la is often aligned with the

philipp koehn

emnlp lecture 15

25 february 2008

... la maison ... la maison blue ... la fleur ...... the house ... the blue house ... the flower ...em algorithm

15

    after one iteration
    alignments, e.g., between la and the are more likely

philipp koehn

emnlp lecture 15

25 february 2008

... la maison ... la maison blue ... la fleur ...... the house ... the blue house ... the flower ...em algorithm

16

    after another iteration
    it becomes apparent that alignments, e.g., between    eur and    ower are more

likely (pigeon hole principle)

philipp koehn

emnlp lecture 15

25 february 2008

... la maison ... la maison id7 ... la fleur ...... the house ... the blue house ... the flower ...em algorithm

17

    convergence
    inherent hidden structure revealed by em

philipp koehn

emnlp lecture 15

25 february 2008

... la maison ... la maison id7 ... la fleur ...... the house ... the blue house ... the flower ...em algorithm

18

    parameter estimation from the aligned corpus

philipp koehn

emnlp lecture 15

25 february 2008

... la maison ... la maison id7 ... la fleur ...... the house ... the blue house ... the flower ...p(la|the) = 0.453p(le|the) = 0.334p(maison|house) = 0.876p(id7|blue) = 0.563...ibm model 1 and em

19

    em algorithm consists of two steps
    expectation-step: apply model to the data

    parts of the model are hidden (here: alignments)
    using the model, assign probabilities to possible values

    maximization-step: estimate model from data

    take assign values as fact
    collect counts (weighted by probabilities)
    estimate model from counts

    iterate these steps until convergence

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em

20

    we need to be able to compute:

    expectation-step: id203 of alignments
    maximization-step: count collection

philipp koehn

emnlp lecture 15

25 february 2008

21

ibm model 1 and em

p(the|la) = 0.7

p(the|maison) = 0.1

p(house|la) = 0.05
p(house|maison) = 0.8

    probabilities

    alignments
   
   

@

@

   
   @

la   
the
maison   
house
p(e, a|f) = 0.56
p(a|e, f) = 0.824

la   
the
maison   
house
p(e, a|f) = 0.035
p(a|e, f) = 0.052
c(the|la) = 0.824 + 0.052

c(the|maison) = 0.118 + 0.007

,

,

   
   ,

la   
the
maison   
house
p(e, a|f) = 0.08
p(a|e, f) = 0.118

,
@

,
@,

   
   @

la   
the
maison   
house
p(e, a|f) = 0.005
p(a|e, f) = 0.007

c(house|la) = 0.052 + 0.007

c(house|maison) = 0.824 + 0.118

    counts

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em: expectation step

22

    we need to compute p(a|e, f)
    applying the chain rule:

p(a|e, f) = p(e, a|f)
p(e|f)

    we already have the formula for p(e, a|f) (de   nition of model 1)

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em: expectation step

23

    we need to compute p(e|f)

a

p(e|f) =x
lfx
lfx

=

=

p(e, a|f)

lfx
lfx

...

...

a(1)=0

a(le)=0

a(1)=0

a(le)=0

p(e, a|f)

 

(lf + 1)le

ley

j=1

t(ej|fa(j))

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em: expectation step

24

t(ej|fa(j))

t(ej|fa(j))

a(1)=0

a(le)=0

lfx

...

 

(lf + 1)le

 

lfx
lfx
lfx
ley

a(1)=0

...

ley
ley

j=1

(lf + 1)le

 

lfx

a(le)=0

j=1

t(ej|fi)

(lf + 1)le

j=1

i=0

p(e|f) =

=

=

    note the trick in the last line
    removes the need for an exponential number of products
    this makes ibm model 1 estimation tractable

philipp koehn

emnlp lecture 15

25 february 2008

2x

2x

a(1)=0

a(2)=0

2y

j=1

=  
32

the trick

t(ej|fa(j)) =

25

(case le = lf = 2)

= t(e1|f0) t(e2|f0) + t(e1|f0) t(e2|f1) + t(e1|f0) t(e2|f2)+

+ t(e1|f1) t(e2|f0) + t(e1|f1) t(e2|f1) + t(e1|f1) t(e2|f2)+
+ t(e1|f2) t(e2|f0) + t(e1|f2) t(e2|f1) + t(e1|f2) t(e2|f2) =

= t(e1|f0) (t(e2|f0) + t(e2|f1) + t(e2|f2)) +

+ t(e1|f1) (t(e2|f1) + t(e2|f1) + t(e2|f2)) +
+ t(e1|f2) (t(e2|f2) + t(e2|f1) + t(e2|f2)) =

= (t(e1|f0) + t(e1|f1) + t(e1|f2)) (t(e2|f2) + t(e2|f1) + t(e2|f2))

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em: expectation step

26

    combine what we have:

p(a|e, f) = p(e, a|f)/p(e|f)

(lf +1)le

 

qle
plf
qle
j=1 t(ej|fa(j))
i=0 t(ej|fi)
plf
t(ej|fa(j))
i=0 t(ej|fi)

j=1

(lf +1)le

 

ley

j=1

=

=

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em: maximization step

27

    now we have to collect counts
    evidence from a sentence pair e,f that word e is a translation of word f :

c(e|f; e, f) =x

lex

p(a|e, f)

  (e, ej)  (f, fa(j))

a

j=1

    with the same simplication as before:

c(e|f; e, f) =

plf
t(e|f)
i=0 t(e|fi)

lex

j=1

lfx

i=0

  (e, ej)

  (f, fi)

philipp koehn

emnlp lecture 15

25 february 2008

28

ibm model 1 and em: maximization step
    after collecting these counts over a corpus, we can estimate the model:

p
p
p
(e,f) c(e|f; e, f))
(e,f) c(e|f; e, f))

f

t(e|f; e, f) =

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 1 and em: pseudocode

29

initialize t(e|f) uniformly
do until convergence

set count(e|f) to 0 for all e,f
set total(f) to 0 for all f
for all sentence pairs (e_s,f_s)

for all words e in e_s

total_s(e) = 0
for all words f in f_s
total_s(e) += t(e|f)

for all words e in e_s

for all words f in f_s

count(e|f) += t(e|f) / total_s(e)
total(f)
+= t(e|f) / total_s(e)

for all f

for all e

t(e|f) = count(e|f) / total(f)

philipp koehn

emnlp lecture 15

25 february 2008

30

higher ibm models

ibm model 1
ibm model 2
ibm model 3
ibm model 4
ibm model 5

lexical translation
adds absolute reordering model
adds fertility model
relative reordering model
   xes de   ciency

    only ibm model 1 has global maximum

    training of a higher ibm model builds on previous model

    compuationally biggest change in model 3
    trick to simplify estimation does not work anymore
    exhaustive count collection becomes computationally too expensive
    sampling over high id203 alignments is used instead

philipp koehn

emnlp lecture 15

25 february 2008

ibm model 4

31

philipp koehn

emnlp lecture 15

25 february 2008

mary did not slap the green witchmary not slap slap slap the green witchmary not slap slap slap null the green witchmaria no daba una botefada a la verde brujamaria no daba una bofetada a la bruja verden(3|slap)p-nullt(la|the)d(4|4)