empirical methods in natural language processing

lecture 10

parsing (ii): probabilistic parsing models

philipp koehn

7 february 2008

philipp koehn

emnlp lecture 10

7 february 2008

1

parsing
    task: build the syntactic tree for a sentence
    grammar formalism

    phrase structure grammar
    context-free grammar

    parsing algorithm: cyk (chart) parsing
    open problems

    where do we get the grammar from?
    how do we resolve ambiguities

philipp koehn

emnlp lecture 10

7 february 2008

id32

2

    id32: english sentences annotated with syntax trees

    built at the university of pennsylvania
    40,000 sentences, about a million words
    real text from the wall street journal
    similar treebanks exist for other languages

    german
    french
    spanish
    arabic
    chinese

philipp koehn

emnlp lecture 10

7 february 2008

3

.

sample syntax tree

np-sbj
!!! cc

mr vinken

is

np

chairman

of

s
pppp

vp
ee

np-prd
pppp

pp
@@

((((((((((((
      
,

np

``````

np

"""

aaaa

((((((((((
     

np
hhhhhhhhh
qq

elsevier n.v.

the dutch publishing group

philipp koehn

emnlp lecture 10

7 february 2008

sample tree with part-of-speech

4

np-sbj
!!! cc

nnp

nnp

vbz

mr

vinken

is

np

nn

in

chairman

of

s
pppp

vp
ee

np-prd
pppp

pp
@@

.

.

((((((((((((
      

``````

np

np

"""

aaaa

nnp

nnp

,

,

((((((((((
     

np
hhhhhhhhh
qq

dt

nnp

vbg

nn

elsevier

n.v.

the

dutch

publishing

group

philipp koehn

emnlp lecture 10

7 february 2008

learning a grammar from the treebank

5

    context-free grammar: we have rules in the form
s     np-sbj vp

    we can collect these rules from the treebank
    we can even estimate probabilities for rules

p(s     np-sbj vp|s) = count(s     np-sbj vp)

count(s)

    probabilistic context-free grammar (pid18)

philipp koehn

emnlp lecture 10

7 february 2008

rules applications to build tree

s
(((((((((((((
ppppppp
vp
((((((((((((
e
e

np-sbj
    

lll
nnp

nnp

vbz

mr

vinken

is

       
np

np-prd
aaaaa
pp
     
@
@
np
in

nn

chairman

of

nnp

elsevier

6

s     np-sbj vp
np-sbj     nnp nnp
nnp     mr
nnp     vinken
vp     vbz np-prd
vbz     is
np-prd     np pp
np     nn
nn     chairman
pp     in np
in     of
np     nnp
nnp     elsevier

philipp koehn

emnlp lecture 10

7 february 2008

compute id203 of tree

7

    id203 of a tree is the product of the probabilities of the rule applications:

p(tree) =y

p(rulei)

    we assume that all rule applications are independent of each other

i

p(tree) = p(s     np-sbj vp|s)  

p(np-sbj     nnp nnp|np-sbj)  
...  
p(nnp     elsevier|nnp)

philipp koehn

emnlp lecture 10

7 february 2008

prepositional phrase attachment ambiguity

8

np-sbj
""""

@

s
           
aaaaaa
vp
          
s
s
!!!!!!
np

@
nnp

vbz

is

vinken

nnp

mr

np-prd
hhhhh
pp
""""
\
\
np
in

nn

chairman

of

nnp

elsevier

np-sbj
""""

@

s
           
aaaaaa
vp
          
pppppp
    
np-prd
pp
""""
\
\
np
in

@
nnp

vbz

np

is

vinken

nnp

mr

nn

of

nnp

chairman

elsevier

pp attached to np-prd

pp attached to vp

philipp koehn

emnlp lecture 10

7 february 2008

pp attachment ambiguity: rule applications

9

s     np-sbj vp
np-sbj     nnp nnp
nnp     mr
nnp     vinken
vp     vbz np-prd
vbz     is
np-prd     np pp
np     nn
nn     chairman
pp     in np
in     of
np     nnp
nnp     elsevier

s     np-sbj vp
np-sbj     nnp nnp
nnp     mr
nnp     vinken
vp     vbz np-prd pp
vbz     is
np-prd     np
np     nn
nn     chairman
pp     in np
in     of
np     nnp
nnp     elsevier

pp attached to np-prd

pp attached to vp

philipp koehn

emnlp lecture 10

7 february 2008

10

pp attachment ambiguity: di   erence in id203
    pp attachment to np-prd is preferred if

p(vp     vbz np-prd|vp)    p(np-prd     np pp|np-prd)

is larger than

p(vp     vbz np-prd pp|vp)    p(np-prd     np|np-prd)

    is this too general?

philipp koehn

emnlp lecture 10

7 february 2008

scope ambiguity

11

np
hhhhhhhhhhhh
xxxxxxxx
     
np
np
cc
       
c
c
np

and

nnp

c
pp
     
c
c
in

c
np

nnp

np
           
c
c
np

c
pp
         
c
c
in

nnp

c
np
ppppppp
     
@
@
cc
np
np

jim

john

from

john

from

nn

hoboken

correct:

nn

and

hoboken

false:

nnp

jim

and connects john and jim

and connects hoboken and jim

however: the same rules are applied

philipp koehn

emnlp lecture 10

7 february 2008

weakness of pid18

12

    independence assumption too strong
    non-terminal rule applications do not use lexical information
    not su   ciently sensitive to structural di   erences beyond parent/child node

relationships

philipp koehn

emnlp lecture 10

7 february 2008

head words

13

    recall dependency structure:

is
pppppp
!!!!!!

vinken

chairman

mr

elsevier

of

    direct relationships between words, some are the head of others

(see also head-driven phrase structure grammar)

philipp koehn

emnlp lecture 10

7 february 2008

adding head words to trees

14

s(is)

hhhhhhhhhhhh

np-sbj(vinken)
(((((((

xxxxx

vp(is)

xxxxx

nnp(mr)

nnp(vinken)

vbz(is)

mr

vinken

is

np-prd(chairman)

((((((((((((

hhhhhhhh

np(chairman)

pp(elsevier)
xxxxx

(((((((

nn(chairman)

in(of)

np(elsevier)

chairman

of

nnp(elsevier)

elsevier

philipp koehn

emnlp lecture 10

7 february 2008

head words in rules

15

    each context-free rule has one head child that is the head of the rule

    s     np vp
    vp     vbz np
    np     dt nn nn

    parent receives head word from head child
    head childs are not marked in the id32, but they are easy to recover

using simple rules

philipp koehn

emnlp lecture 10

7 february 2008

recovering heads

16

    rule for recovering heads for nps

    if rule contains nn, nns or nnp, choose rightmost nn, nns or nnp
    else if rule contains a np, choose leftmost np
    else if rule contains a jj, choose rightmost jj
    else if rule contains a cd, choose rightmost cd
    else choose rightmost child

    examples

    np     dt nnp nn
    np     np cc np
    np     np pp
    np     dt jj
    np     dt

philipp koehn

emnlp lecture 10

7 february 2008

using head nodes

17

    pp attachment to np-prd is preferred if

p(vp(is)     vbz(is) np-prd(chairman)|vp(is))
   p(np-prd(chairman)     np(chairman) pp(elsevier)|np-prd(chairman))

is larger than

p(vp(is)     vbz(is) np-prd(chairman) pp(elsevier)|vp(is))
   p(np-prd(chairman)     np(chairman)|np-prd(chairman))

    scope ambiguity: combining hoboken and jim should have low id203
p(np(hoboken)     np(hoboken) cc(and) np(john)|vp(hoboken))

philipp koehn

emnlp lecture 10

7 february 2008

sparse data concerns

18

    how often will we encounter

np(hoboken)     np(hoboken) cc(and) np(john)

    ... or even

np(jim)     np(jim) cc(and) np(john)

    if not seen in training, id203 will be zero

philipp koehn

emnlp lecture 10

7 february 2008

sparse data: dependency relations

19

    instead of using a complex rule

np(jim)     np(jim) cc(and) np(john)

    ... we collect statistics over dependency relations

head word

head tag

child node

child tag

direction

np
np

jim
jim

cc
np
       rst generate child tag: p(cc|np,jim,left)
    then generate child word: p(and|np,jim,left,cc)

and
john

left
left

philipp koehn

emnlp lecture 10

7 february 2008

sparse data: interpolation

20

    use of interpolation with back-o    statistics (recall: id38)
    generate child tag

p(cc|np, jim, left) =   1

count(cc, np, jim, left)

count(np, jim, left)

+   2

count(cc, np, left)

count(np, left)

    with 0       1     1,

0       2     1,

  1 +   2 = 1

philipp koehn

emnlp lecture 10

7 february 2008

sparse data: interpolation (2)

21

    generate child word

p(and|cc, np, jim, left) =   1

count(and, cc, np, jim, left)

count(cc, np, jim, left)

+   2

+   3

count(and, cc, np, left)

count(cc, np, left)

count(and, cc, left)

count(cc, left)

    with 0       1     1,

0       2     1,

0       3     1,

  1 +   2 +   3 = 1

philipp koehn

emnlp lecture 10

7 february 2008

what also helps

22

    adding a count for distance from head word
    part-of-speech of the head word and the child word also useful
    improving tags

    instead of general vb, distinguish between intransitive verb phrases vi, and

transitive verb phrases vt

    distinguish between complements (required attachments, e.g. object of a

transitive verb) and adjuncts (optional attachments, e.g. yesterday)

    not only use parent tag, but also grand-parent tag
    create n-best list of best parse trees, re-score

philipp koehn

emnlp lecture 10

7 february 2008

parsing algorithm

23

    e   cient parsing algorithm is tricky
    algorithm is similar to chart parsing, as presented
    impossible to search entire space of possible parse trees
    rest cost estimation, pruning

philipp koehn

emnlp lecture 10

7 february 2008

performance

24

    performance typically measured in recall/precision of dependency relations

    pid18: 74.8%/70.6%
    using lexical dependencies: 85.7%/85.3%
    latest models (collins): 89.0%/88.7%

    core sentence structure (complements, np chunks) recovered with over 90%

accuracy

    attachment ambiguities involving adjuncts are resolved with much lower

accuracy (   80% for pp attachment,    50-60% for coordination)

note: numbers quoted from lecture 4 parsing and syntax ii of mit class 6.891 natural language processing by

michael collins (2005)

philipp koehn

emnlp lecture 10

7 february 2008

