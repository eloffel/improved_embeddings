empirical methods in natural language processing

lecture 6

tagging (ii): transformation-based learning

and maximum id178 models

philipp koehn

24 january 2008

pk

emnlp

24 january 2008

tagging as supervised learning

1

    tagging is a supervised learning problem

    given: some annotated data (words annotated with pos tags)
    build model (based on features, i.e. representation of example)
    predict unseen data (pos tags for words)

    issues in supervised learning

    there is no data like more data
    feature engineering: how best represent the data
    over   tting to the training data?

    there are many algorithms for supervised learning (naive bayes, id90,

maximum id178, neural networks, support vector machines, ...)

pk

emnlp

24 january 2008

one tagging method: id48

2

    id48s make use of two id155 distributions

    tag sequence model p(tn|tn   2, tn   1)
    tag-word predicition model p(wn|tn)

    given these models, we can    nd the best sequence of tags for a sentence using

the viterbi algorithm

pk

emnlp

24 january 2008

how good is id48 tagging?

3

    labeling a sequence is very fast
    viterbi algorithm outputs best label sequence (previous tags a   ect labeling of

next tag), not just best tag for each word in isolation

    it is easy to get 2nd best sequence, 3rd best sequence, etc.
    but: uses only a very small window around word (n previous tags)

pk

emnlp

24 january 2008

more features

4

    consider a larger window

wn   4 wn   3 wn   2 wn   1 wn wn+1 wn+2 wn+3 wn+4
tn   4
tn+4

tn   1

tn   2

tn   3

tn+1

tn+2

tn+3

tn

    examples for useful features

    if one of the previous tags is md, then vb is likelier than vbp (basic verb

form instead of verb in singular present)

    if next tag is jj, then rbr is likelier than jjr (adverb instead of adjective)

pk

emnlp

24 january 2008

more features (2)

5

    lexical features

    if one of the previous tags is not, then vb is likelier than vbp

    morphological features

    if word ends in -tion it is most likely an nn
    if word ends in -ly it is most likely an adverb

pk

emnlp

24 january 2008

using additional features

    using more features in a id155 distribution?

6

p(ti|wi, f0, ..., fn)

    sparse data problems

(insu   cient statistics for reliable estimation of the distribution)
    idea: first apply id48, then    x errors with additional features

pk

emnlp

24 january 2008

applying the model to training data

7

    we can use the id48 tagger to tag the training data
    then, we can compare predicted tags to true tags

words:

the
boat
predicted: det
nn det nn
true tag: det nn vb det nn

old man
jj

the

    how can we    x these errors? possible transformation rules:

    change nn to vb if no verb in sentence

predicted: det jj vb det nn

    change jj to nn if followed by vb

predicted: det nn vb det nn

pk

emnlp

24 january 2008

transformation based learning

8

    first, baseline tagger

    most frequent tag for word: argmaxt p(t|w)
    hidden markov model tagger

    then apply transformations that    x the errors

    go through the sequence word by word
    if a feature is present in a current example,
    apply rule (change tag)

pk

emnlp

24 january 2008

learning transformations

9

    given: words with their true tags
    tag sentence with baseline tagger
    repeat

       nd transformation that minimizes error
    apply transformation to sentence
    add transformation to list

    output: ordered list of transformations

pk

emnlp

24 january 2008

10

applying the learned transformations

    given: a new sentence that we want to tag
    tag words with baseline tagger
    for each transformation rule (in the sequence they were learned):

    for each word (in sentence order):

   apply transformation, if it matches

    output: tags

pk

emnlp

24 january 2008

goal: minimizing error

    we need some metric to measure the error
    here: number of wrongly assigned tags

pn

error(d, m) = 1    

i=1   (tpredicted

i
n

11

, ti)

    general considerations for error functions:
    some errors are more costly than others
    detecting cancer, if healthy vs. detecting healthy when cancer
    sometimes error is di   cult to assess (machine translation output di   erent

from human translation may be still correct)

pk

emnlp

24 january 2008

12

over   tting
    it may be possible to    x all errors in training
    the last transformations learned may    x only one error each
    transformations that work in training may not work elsewhere, or may even

be generally harmful

    to avoid over   tting: stop early

pk

emnlp

24 january 2008

generative modeling vs. discriminative training

13

    id48s are an example for generative modeling

    a model m is created that predicts the training data d
    the model is broken up into smaller steps
    for each step, a id203 distribution is learned
    model is optimized on p(d|m), how well it predicts the data

    transformation-based learning is an example for discriminative training

    a method m is created to predict the training data d
    it is improved by reducing prediction error
    look for features that discriminate between faulty predictions and truth
    model is optimized on error(m, d), also called the id168

pk

emnlp

24 january 2008

probabilities vs. rules

14

    id48s: probabilities allow for graded decisions, instead of just yes/no
    transformation based learning: more features can be considered
    we would like to combine both
    maximum id178 models

pk

emnlp

24 january 2008

maximum id178

15

    each example (here: word w) is represented by a set of features {fi}, here:

    the word itself
    morphological properties of the word
    other words and tags surrounding the word

    the task is the classify the word into a class cj (here: the pos tag)
    how well a feature fi predicts a class cj is de   ned by a parameter   (fi, cj)
    maximum id178 model:

p(cj|w) = y

fi   w

  (fi, cj)

pk

emnlp

24 january 2008

maximum id178 training

16

    feature selection

    given the large number of possible features, which ones will be part of the

model?

    we do not want unreliable and rarely occurring features (avoid over   tting)
    good features help us to reduce the number of classi   cation errors

    setting the parameter values   (fi, cj)

      (fi, cj) are real numbered values, similar to probabilities
    we want to ensure that the expected co-occurrence of features and classes

matches between the training data and the model

    otherwise we want to have no bias in the model (maintain maximum id178)
    training algorithm: generalized iterative scaling

pk

emnlp

24 january 2008

id52 tools

17

    three commonly used, freely available tools for tagging:

    tnt by thorsten brants (2000): hidden markov model

http://www.coli.uni-saarland.de/ thorsten/tnt/

    brill tagger by eric brill (1995): transformation based learning

http://www.cs.jhu.edu/   brill/

    mxpost by adwait ratnaparkhi (1996): maximum id178 model

ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
    all have similar performance (   96% on id32 english)

pk

emnlp

24 january 2008

