empirical methods in natural language processing

id38 (i): from counts to smoothing

lecture 3

philipp koehn

14 january 2008

pk

emnlp

14 january 2008

language models

1

    language models answer the question: how likely is a string of english words

good english?
    the house is big     good
    the house is xxl     worse
    house big is the     bad
    uses of language models

    id103
    machine translation
    id42
    handwriting recognition
    language detection (english or finnish?)

pk

emnlp

14 january 2008

2

applying the chain rule
    given: a string of english words w = w1, w2, w3, ..., wn
    question: what is p(w )?
    sparse data: many good english sentences will not have been seen before.
    decomposing p(w ) using the chain rule:

p(w1, w2, w3, ..., wn) = p(w1) p(w2|w1) p(w3|w1, w2)...p(wn|w1, w2, ...wn   1)

pk

emnlp

14 january 2008

markov chain

3

    markov assumption:

    only previous history matters
    limited memory: only last k words are included in history
(older words less relevant)
    kth order markov model
    for instance 2-gram language model:

p(w1, w2, w3, ..., wn) = p(w1) p(w2|w1) p(w3|w2)...p(wn|wn   1)

    what is conditioned on, here wn   1 is called the history

pk

emnlp

14 january 2008

estimating id165 probabilities

4

    we are back in comfortable territory: id113

p(w2|w1) = count(w1, w2)
count(w1)

    collect counts over a large text corpus
    millions to billions of words are easy to get

pk

emnlp

14 january 2008

size of the model

5

    for each id165 (e.g. the big house), we need to store a id203
    assuming 20,000 distinct words

model

max. number of parameters

0th order (unigram)
1st order (bigram)
2nd order (trigram)
3rd order (4-gram)

20,000

20, 0002 = 400 million
20, 0003 = 8 trillion

20, 0004 = 160 quadrillion

    in practice, 3-gram lms are typically used

pk

emnlp

14 january 2008

size of model: practical example

6

    trained on 10 million sentences from the gigaword corpus (text collection
from new york times, wall street journal, and news wire sources), about 275
million words.

1-gram
716,706
2-gram 12,537,755
3-gram 22,174,483

    worst case for number of distinct id165s is linear with the corpus size.

pk

emnlp

14 january 2008

how good is the lm?

7

    a good model assigns a text of real english a high id203
    this can be also measured with cross id178:

    or, perplexity

h(w ) =

1
n

log p(w n
1 )

perplexity(w ) = 2h(w )

pk

emnlp

14 january 2008

training set and test set

8

    we learn the language model from a training set, i.e. we collect statistics for
id165s over that sample and estimate the conditional id165 probabilities.

    we evaluate the language model on a hold-out test set
    much smaller than training set (thousands of words)
    not part of the training set!

    we measure perplexity on the test set to gauge the quality of our language

model.

pk

emnlp

14 january 2008

example: unigram

9

    training set

there is a big house

i buy a house

they buy the new house

    model

p(there) = 0.0714
p(big) = 0.0714
p(buy) = 0.1429
p(new) = 0.0714

p(is) = 0.0714

p(house) = 0.2143
p(they) = 0.0714

p(a) = 0.1429
p(i) = 0.0714
p(the) = 0.0714

    test sentence s: they buy a big house
    p(s) = 0.0714

   0.1429

   0.0714

| {z }

buy

| {z }a

| {z }

   0.1429

big

| {z }

   0.2143

house

= 0.0000231

| {z }

they

pk

emnlp

14 january 2008

example: bigram

10

    training set

there is a big house

i buy a house

they buy the new house

    model

p(big|a) = 0.5
p(house|a) = 0.5
p(new|the) = 1

p(a|is) = 1

p(is|there) = 1
p(buy|i) = 1
p(house|big) = 1
p(house|new) = 1

p(buy|they) = 1
p(a|buy) = 0.5
p(the|buy) = 0.5

p(they| < s >) = .333

    test sentence s: they buy a big house
    p(s) = 0.333

   0.5|{z}big

   1|{z}

house

   1|{z}buy

   0.5|{z}a

|{z}

they

= 0.0833

pk

emnlp

14 january 2008

unseen events

11

    another example sentence s2: they buy a new house.
    bigram a new has never been seen before
    p(new|a) = 0     p(s2) = 0
    ... but it is a good sentence!

pk

emnlp

14 january 2008

two types of zeros

12

    unknown words

    handled by an unknown word token

    unknown id165s

    smoothing by giving them some low id203
    back-o    to lower order id165 model

    giving id203 mass to unseen events reduces available id203 mass for

seen events     not maximum likelihood estimates anymore

pk

emnlp

14 january 2008

add-one smoothing

13

for all possible id165s, add the count of one.
example:

bigram
a big
a house
a new
a the
a is
a there
a buy
a a
a i

count     p(w2|w1)

count+1     p(w2|w1)

1
1
0
0
0
0
0
0
0

0.5
0.5
0
0
0
0
0
0
0

2
2
1
1
1
1
1
1
1

0.18
0.18
0.09
0.09
0.09
0.09
0.09
0.09
0.09

pk

emnlp

14 january 2008

add-one smoothing

14

    this is bayesian estimation with a uniform prior.

recall: argmaxmp (m|d) = argmaxmp (d|m)    p (m)

    is too much id203 mass wasted on unseen events?
    are impossible/unlikely events estimated too high?

    how can we measure this?

pk

emnlp

14 january 2008

expected counts and test set counts

church and gale (1991a) experiment: 22 million words training, 22 million words
testing, from same domain (ap news wire), counts of bigrams:

15

frequency r actual frequency expected frequency
in training

in test (add one)

in test

0
1
2
3
4
5

0.000027

0.448
1.25
2.24
3.23
4.21

0.000132
0.000274
0.000411
0.000548
0.000685
0.000822

we overestimate 0-count bigrams (0.000132 > 0.000027), but since there are so
many, they use up so much id203 mass that hardly any is left.

pk

emnlp

14 january 2008

using held-out data

16

    we know from the test data, how much id203 mass should be assigned

to certain counts.

    we can not use the test data for estimation, because that would be cheating.
    divide up the training data: one half for count collection, one have for

collecting frequencies in unseen text.

    both halves can be switched and results combined to not lose out on training

data.

pk

emnlp

14 january 2008

deleted estimation

17

    counts in training ct(w1, ..., wn)
    counts how often an ngram seen in training is seen in held-out training

ch(w1, ..., wn)

    number of ngrams with training count r: nr
    total times ngrams of training count r seen in held-out data: tr
    held-out estimator:

ph(w1, ..., wn) = tr
nrn

where count(w1, ..., wn) = r

pk

emnlp

14 january 2008

using both halves

18

    both halves can be switched and results combined to not lose out on training

data

ph(w1, ..., wn) = t 01
n(n 01

r + t 10
r + n 10

r

r ) where count(w1, ..., wn) = r

pk

emnlp

14 january 2008

