empirical methods in natural language processing

lecture 8

tagging (iii): maximum id178 models

philipp koehn

31 january 2008

pk

emnlp

31 january 2008

id52 tools

1

    three commonly used, freely available tools for tagging:

    tnt by thorsten brants (2000): hidden markov model

http://www.coli.uni-saarland.de/ thorsten/tnt/

    brill tagger by eric brill (1995): transformation based learning

http://www.cs.jhu.edu/   brill/

    mxpost by adwait ratnaparkhi (1996): maximum id178 model

ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
    all have similar performance (   96% on id32 english)

pk

emnlp

31 january 2008

probabilities vs. rules

2

    we examined two supervised learning methods for the tagging task
    id48s: probabilities allow for graded decisions, instead of just yes/no
    transformation based learning: more features can be considered
    we would like to combine both     maximum id178 models

    a large number of features can be de   ned
    features are weighted by their importance

pk

emnlp

31 january 2008

features

3

    each tagging decision for a word occurs in a speci   c context
    for tagging, we consider as context the history hi

    the word itself
    morphological properties of the word
    other words surrounding the word
    previous tags

    we can de   ne a feature fj that allows us to learn how well a speci   c aspect

of histories hi is associated with a tag ti

pk

emnlp

31 january 2008

features (2)

4

    we observe in the data patterns such as:

the word like has in 50% of the cases the tag vb

    previously, in id48 models, this led us to introduce probabilities (as part of

the tag sequence model) such as

p(v b|like) = 0.5

pk

emnlp

31 january 2008

features (3)

5

    in a maximum id178 model, this information is captured by a feature

(

fj(hi, ti) =

1 if wi = like and ti = v b
0 otherwise

    the importance of a feature fj is de   ned by a parameter   j

pk

emnlp

31 january 2008

features (4)

6

    features may consider morphology

(

fj(hi, ti) =

1 if su   x(wi) =    ing    and ti = v b
0 otherwise

    features may consider tag sequences

(

fj(hi, ti) =

1 if ti   2 = det and ti   1 = n n and ti = v b
0 otherwise

pk

emnlp

31 january 2008

7

features in ratnaparkhi [1996]
frequent wi wi = x

rare wi x is pre   x of wi, |x|     4
x is su   x of wi, |x|     4
wi contains a number
wi contains uppercase character
wi contains hyphen
ti   1 = x
ti   2ti   1 = xy
wi   1 = x
wi   2 = x
wi+1 = x
wi+2 = x

all wi

pk

emnlp

31 january 2008

    features fj and parameters   j are used to compute the id203 p(hi, ti):

log-linear model

8

p(hi, ti) =y

  

fj(hi,ti)
j

j

    these types of models are called id148, since they can be

reformulated into

log p(hi, ti) =x

fj(hi, ti) log   j

    there are many learning methods for these models, maximum id178 is just

j

one of them

pk

emnlp

31 january 2008

conditional probabilities

9

    we de   ned a model p(hi, ti) for the joint id203 distribution for a history

hi and a tag ti

    conditional probabilities can be computed straight-forward by

p

p(ti|hi) = p(hi, ti)
i0 p(hi, ti0)

pk

emnlp

31 january 2008

tagging a sequence

10

    we want to tag a sequence w1, ..., wn
    this can be decomposed into:

p(t1, ..., tn|w1, ..., wn) =

ny

i=1

p(ti|hi)

    the history hi consist of all words w1, ..., wn and previous tags t1, ..., ti   1
    we cannot use viterbi search     heuristic id125 is used (more on

id125 in a future lecture on machine translation)

pk

emnlp

31 january 2008

questions for training

11

    feature selection

    given the large number of possible features, which ones will be part of the

model?

    we do not want redundant features
    we do not want unreliable and rarely occurring features (avoid over   tting)

    parameter values   j

      j are positive real numbered values
    how do we set them?

pk

emnlp

31 january 2008

feature selection

12

    feature selection in ratnaparkhi [1996]

    feature has to occur 10 times in the training data

    other feature selection methods

    use features with high mutual information
    add feature that reduces training error most, retrain

pk

emnlp

31 january 2008

setting the parameter values   j: goals

13

    the empirical expectation of a feature fj occurring in the training data is

de   ned by

  e(fj) =

1
n

fj(hi, ti)

nx

i=1

    the model expectation of that feature occurring is

e(fj) =x

p(h, t)fj(h, t)

h,t

    we require that   e(fj) = e(fj)

pk

emnlp

31 january 2008

empirical expectation

14

    consider the feature

(

1 if wi = like and ti = v b
0 otherwise
    computing the empirical expectation   e(fj):

fj(hi, ti) =

    if there are 10,000 words (and tags) in the training data
    ... and the word like occurs with the tag vb 20 times
    ... then

nx

i=1

10000x

i=1

  e(fj) =

1
n

fj(hi, ti) =

1

10000

fj(hi, ti) =

20

10000

= 0.002

pk

emnlp

31 january 2008

model expectation

15

    we de   ned the model expectation of a feature occurring as

e(fj) =x

p(h, t)fj(h, t)

    practically, we cannot sum over all possible histories h and tags t
    instead, we compute the model expectation of the feature on the training data:

h,t

nx

i=1

e(fj)     1
n

p(t|hi) fj(hi, t)

note: theoretically we have to sum over all t, but fj(hi, t) = 0 for all but one t

pk

emnlp

31 january 2008

goals of maximum id178 training

16

    recap: we require that   e(fj) = e(fj), or

nx

i=1

1
n

fj(hi, ti) =

1
n

p(t|hi) fj(hi, t)

nx

i=1

    otherwise we want maximum id178, i.e. we do not want to introduce any

additional order into the model (occam   s razor: simplest model is best)

    id178:

h(p) =x

p(h, t) log p(h, t)

h,t

pk

emnlp

31 january 2008

improved iterative scaling [berger, 1993]

17

input: feature functions f1, ..., fm, empirical distribution   p(x, y)
output: optimal parameter values   1, ...,   m
1. start with   i = 0 for all i     {1, 2, ..., n}
2. do for each i     {1, 2, ..., n}:

a.      i = 1
b. update   i       i +      i

c log   e(fi)

e(fi)

3. go to step 2 if not all the   i have converged

note: this algorithm requires that    t, h :p

with an additional    ller feature

i fi(t, h) = c, which can be ensured

pk

emnlp

31 january 2008

