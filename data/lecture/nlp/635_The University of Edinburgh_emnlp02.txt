empirical methods in natural language processing

lecture 2

introduction (ii)

id203 and id205

philipp koehn

lecture given by tommy herbert

10 january 2008

pk

emnlp

10 january 2008

recap

1

    given word counts we can estimate a id203 distribution:

p

p (w) = count(w)

w0 count(w0)

    another useful concept is id155

p(w2|w1)
    chain rule:

p(w1, w2) = p(w1) p(w2|w1)

    bayes rule:

p(x|y) = p(y|x) p(x)

p(y)

pk

emnlp

10 january 2008

expectation

2

    we introduced the concept of a random variable x

prob(x = x) = p(x)

    example: roll of a dice. there is a 1
    we de   ne the expectation e(x) of a random variable as:

6 chance that it will be 1, 2, 3, 4, 5, or 6.

e(x) =p

x p(x) x

    roll of a dice:

e(x) = 1

6    1 + 1

6    2 + 1

6    3 + 1

6    4 + 1

6    5 + 1

6    6 = 3.5

pk

emnlp

10 january 2008

variance

3

    variance is de   ned as

v ar(x) =p

x p(x) (x     e(x))2

v ar(x) = e((x     e(x))2) = e(x 2)     e2(x)

    intuitively, this is a measure how far events diverge from the mean (expectation)
    related to this is standard deviation, denoted as   .

v ar(x) =   2
e(x) =   

pk

emnlp

10 january 2008

variance (2)

4

    roll of a dice:

v ar(x) =

1
1
1
(1     3.5)2 +
(2     3.5)2 +
(3     3.5)2
6
6
6
1
1
1
(6     3.5)2
(5     3.5)2 +
(4     3.5)2 +
6
6
6
1
((   2.5)2 + (   1.5)2 + (   0.5)2 + 0.52 + 1.52 + 2.52)
6
1
6

(6.25 + 2.25 + 0.25 + 0.25 + 2.25 + 6.25)

+

=

=

= 2.917

pk

emnlp

10 january 2008

standard distributions

5

    uniform: all events equally likely

       x, y : p(x) = p(y)
    example: roll of one dice

    binomial: a serious of trials with only only two outcomes
    id203 p for each trial, occurrence r out of n times:

b(r; n, p) =(cid:0)n

(cid:1)pr(1     p)n   r

    a number of coin tosses

r

pk

emnlp

10 january 2008

standard distributions (2)

6

    normal: common distribution for continuous values

    value in the range [    inf, x], given expectation    and standard deviation   :

n(x;   ,   ) = 1   

e   (x     )2/(2  2)

2    

    also called bell curve, or gaussian
    examples: heights of people, iq of people, tree heights, ...

pk

emnlp

10 january 2008

estimation revisited

7

    we introduced last lecture an estimation of probabilities based on frequencies:

p

p (w) = count(w)

w0 count(w0)

    alternative view: bayesian: what is the most likely model given the data

p(m|d)

    model and data are viewed as random variables

    model m as random variable
    data d as random variable

pk

emnlp

10 january 2008

bayesian estimation

8

    reformulation of p(m|d) using bayes rule:

p(m|d) = p(d|m ) p(m )
argmaxm p(m|d) = argmaxm p(d|m) p(m)

p(d)

    p(m|d) answers the question: what is the most likely model given the data
    p(m) is a prior that prefers certain models (e.g. simple models)
    the frequentist estimation of word probabilities p(w) is the same as bayesian
estimation with a uniform prior (no bias towards a speci   c model), hence it is
also called the id113

pk

emnlp

10 january 2008

id178

9

    an important concept is id178:

h(x) =p

x    p(x) log2 p(x)

    a measure for the degree of disorder

pk

emnlp

10 january 2008

10

id178 example

one event

p(a) = 1

h(x) =     1 log2 1

= 0

pk

emnlp

10 january 2008

11

id178 example

2 equally likely events:

p(a) = 0.5
p(b) = 0.5

h(x) =     0.5 log2 0.5     0.5 log2 0.5

=     log2 0.5
= 1

pk

emnlp

10 january 2008

12

id178 example

4 equally likely events:

p(a) = 0.25
p(b) = 0.25
p(c) = 0.25
p(d) = 0.25

h(x) =     0.25 log2 0.25     0.25 log2 0.25
    0.25 log2 0.25     0.25 log2 0.25

=     log2 0.25
= 2

pk

emnlp

10 january 2008

13

id178 example

4 equally likely events, one more likely
than the others:

p(a) = 0.7
p(b) = 0.1
p(c) = 0.1
p(d) = 0.1

h(x) =     0.7 log2 0.7     0.1 log2 0.1
    0.1 log2 0.1     0.1 log2 0.1
=     0.7 log2 0.7     0.3 log2 0.1
=     0.7       0.5146     0.3       3.3219
= 0.36020 + 0.99658
= 1.35678

pk

emnlp

10 january 2008

14

id178 example

4 equally likely events, one much more
likely than the others:

(x)

p(a) = 0.97
p(b) = 0.01
p(c) = 0.01
p(d) = 0.01

h(x) =     0.97 log2 0.97     0.01 log2 0.01
    0.01 log2 0.01     0.01 log2 0.01
=     0.97 log2 0.97     0.03 log2 0.01
=     0.97       0.04394     0.03       6.6439
= 0.04262 + 0.19932
= 0.24194

pk

emnlp

10 january 2008

intuition behind id178

15

    a good model has low id178
    it is more certain about outcomes
    for instance a translation table

e
the
that

f
der
der

p(e|f)
0.8
0.2

is better than

e
the
that
...

f
der
der
...

p(e|f)
0.02
0.01
...

    a lot of statistical estimation is about reducing id178

pk

emnlp

10 january 2008

id205 and id178

16

    assume that we want to encode a sequence of events x
    each event is encoded by a sequence of bits
    for example

    coin    ip: heads = 0, tails = 1
    4 equally likely events: a = 00, b = 01, c = 10, d = 11
    3 events, one more likely than others: a = 0, b = 10, c = 11
    morse code: e has shorter code than q

    average number of bits needed to encode x     id178 of x

pk

emnlp

10 january 2008

17

the id178 of english
    we already talked about the id203 of a word p(w)
    but words come in sequence. given a number of words in a text, can we guess

the next word p(wn|w1, ..., wn   1)?

    example: newspaper article

pk

emnlp

10 january 2008

id178 for letter sequences

assuming a model with a limited window size

18

model

0th order
1st order
2nd order

human, unlimited

id178

4.76
4.03
2.8
1.3

pk

emnlp

10 january 2008

