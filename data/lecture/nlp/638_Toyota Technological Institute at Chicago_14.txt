ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	14:	

introduction	to	

computational	semantics

1

announcements

    if	you	haven   t	emailed	me	to	set	up	a	15-
minute	meeting	to	discuss	your	project	
proposal,	please	do	so
    times	posted	on	course	webpage
    let	me	know	if	none	of	those	work	for	you

    assignment	3	due	feb	29
    email	me	to	sign	up	for	your	(10-minute)	class	

presentation	on	3/3	or	3/8

2

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

3

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    computational	semantics	(today)

    compositionality
    semantic	parsing

    machine	translation	(thursday)
    other	nlp	applications	(next	tuesday)

4

compositional	semantics

       how	should	the	meanings	of	words	combine	
to	create	the	meaning	of	something	larger?   
    there   s	currently	a	lot	of	work	in	producing	

vector	representations	of	sentences	and	
documents

    simplest	case:	how	should	two	word	vectors	
be	combined	to	create	a	vector	for	a	bigram?

    explosion	of	work	in	this	area	in	the	neural	
network	era,	but	earlier	work	began	~2007

5

evaluating	compositional	semantics
    compute	similarity	of	two	bigrams	under	your	
model,	then	compute	correlation	with	human	
judgments:

bigramsim bigrampara

television	programme

tv set

training	programme

education	course

bedroom	window

education	officer

5.8

5.7

1.3

1.0

5.0

1.0

(mitchell	and	lapata,	2010)

6

7

bigram	composition	functions

8

bigram	similarity	results

9

why	does	multiplication	work?
    these	vectors	are	built	from	co-occurrence	
counts	(like	in	the	first	part	of	assignment	2)

    so	element-wise	multiplication	is	like	

performing	an	and	operation	on	context	
counts

    when	using	skip-gram	word	vectors	(or	other	

neural	network-derived	vectors),	addition	
often	works	better

10

11

results

sds	=	simple	
distributional	
semantic

nlm	=	neural	
language	model

12

topical

paraphrastic

bigrams

bigramsim

(mitchell	and	lapata,	2010)

13

topical

paraphrastic

bigrams

bigramsim

(mitchell	and	lapata,	2010)

bigramsim bigrampara

television	programme

tv set

training	programme

education	course

bedroom	window

education	officer

5.8

5.7

1.3

1.0

5.0

1.0

14

topical

paraphrastic

bigrams

bigramsim

(mitchell	and	lapata,	2010)

bigrampara

(wietinget	al.,	2015)

bigramsim bigrampara

television	programme

tv set

training	programme

education	course

bedroom	window

education	officer

5.8

5.7

1.3

1.0

5.0

1.0

15

topical

paraphrastic
phrasepara

words
can	not	be	separated	from
hoped	to	be	able	to
come	on	,	think	about	it
bigrams
how	do	you	mean	that

wordsim353

is	inseparable	from

looked	forward	to

bigramsim

people	,	please

(mitchell	and	lapata,	2010)

what	worst	feelings

5.0
siid113x-999
3.4
bigrampara
2.2
(this	talk)
1.6

phrases

phrasepara

(wietinget	al.,	2015)

16

training	data:	paraphrase	database

(ganitkevitch,	van	durme,	and	callison-burch,	2013)

from	ganitkevitch and	callison-burch	(2014)

17

    currently	there	is	a	lot	of	work	on	designing	
functional	architectures	for	bigram,	phrase,	
and	sentence	similarity
    e.g.,	word	averaging,	recurrent	neural	networks,	

lstms,	recursive	neural	networks,	etc.

    our	recent	results	find	that,	for	sentence	
similarity,	word	averaging	is	a	surprisingly	
strong	baseline

18

on	similar	data	to	training	data,	lstm	does	best

19

word

averaging

adding	layers	

to	word
averaging

but	when	evaluating	on	other	datasets,	

word	averaging	models	do	best!

20

--ray	mooney

   you can   t map all sentences into a cold, sterile space of 

meaningless, uninterpretable dimensions.   

symbolic representations can encode meaning much more 

efficiently.   

--my	interpretation

21

--ray	mooney

why	must	we	choose?

neural	architectures	for	text	understanding	

can	combine	discrete	(symbolic)
and	continuous	representations

22

syntax and	semantics

    syntax:	rules,	principles,	processes	that	govern	

sentence	structure	of	a	language

    semantics:	what	the	sentence	means

23

    we	saw	syntactic	parsing,	which	produces	a	

syntactic	structure	of	a	sentence
    helps	to	disambiguate	attachments,	
coordinations,	sometimes	word	sense

    now	we   ll	look	at	semantic	parsing,	which	

roughly	means	   produce	a	semantic	structure	
of	a	sentence   

24

several	kinds	of	semantic	parsing

    semantic	role	labeling	(srl)
    frame-semantic	parsing
       semantic	parsing   	(first-order	logic)
    abstract	meaning	representation	(amr)
    dependency-based	compositional	semantics

25

applications 

semantic	role	labeling

(cid:96) question & answer systems 

   who      did what to whom      at where? 

 

the police officer detained the suspect at the scene of the crime 

agent
arg0 

predicate

v 

theme
arg2 

location
am-loc 

j&m/slp3

can	we	figure	out	that	these	have	the	

same	meaning?

xyz	corporation	bought the	stock.
they	sold the	stock	to	xyz	corporation.
the	stock	was	bought by	xyz	corporation.
the	purchase of	the	stock	by	xyz	corporation...	
the	stock	purchase by	xyz	corporation...	

j&m/slp3

27

a	shallow	semantic	representation:	

semantic	roles

predicates	(bought,	sold,	purchase)	represent	an	event
semantic	roles	express	the	abstract	role	that	
arguments	of	a	predicate	can	take	in	the	event

more	specific

more	general

buyer

agent

proto-agent

j&m/slp3

28

getting	to	semantic	roles
(22.1) sasha broke the window.
(22.2) pat opened the door.

neo-davidsonian event	representation:

a neo-davidsonian event representation of these two sentences would be

sasha	broke	the	window
pat	opened	the	door

9e,x,y breaking(e)^ breaker(e,sasha)
9e,x,y opening(e)^ opener(e,pat)

^brokenthing(e,y)^window(y)
^openedthing(e,y)^ door(y)
subjects	of	break	and	open:	breaker and	opener
in this representation, the roles of the subjects of the verbs break and open are
deep	roles	specific	to	each	event	(breaking,	
breaker and opener respectively. these deep roles are speci   c to each event; break-
deep roles
opening)
ing events have breakers, opening events have openers, and so on.
hard	to	reason	about	them	for	applications	like	qa

if we are going to be able to answer questions, perform id136s, or do any
further kinds of natural language understanding of these events, we   ll need to know
a little more about the semantics of these arguments. breakers and openers have
something in common. they are both volitional actors, often animate, and they have
direct causal responsibility for their events.

j&m/slp3

thematic roles are a way to capture this semantic commonality between break-

thematic	roles

    breaker and	opener have	something	in	common!

    volitional	actors
    often	animate
    direct	causal	responsibility	for	their	events

    thematic	roles	are	a	way	to	capture	this	semantic	

commonality	between	breakers	and	eaters
    they	are	both	agents

    the	brokenthing and	openedthing are	themes.

    prototypically	inanimate	objects	affected	in	some	way	

by	the	action

j&m/slp3

30

a	typical	set	of	thematic	roles

2 chapter 22

    id14

22.2

    diathesis alternations

thematic role
de   nition
thematic role
the volitional causer of an event
agent
agent
the experiencer of an event
experiencer
experiencer
the non-volitional causer of the event
force
force
the participant most directly affected by an event
theme
theme
the end product of an event
result
result
the proposition or content of a propositional event
content
content
an instrument used in an event
instrument
instrument
the bene   ciary of an event
beneficiary
beneficiary
the origin of the object of a transfer event
source
source
the destination of an object of a transfer event
goal
goal
figure 22.1 some commonly used thematic roles with their de   nitions.
figure 22.2 some prototypical examples of various thematic roles.

example
the waiter spilled the soup.
john has a headache.
the wind blows debris from the mall into our yards.
only after benjamin franklin broke the ice...
the city built a regulation-size baseball diamond...
mona asked    you met mary ann at a supermarket?   
he poached cat   sh, stunning them with a shocking device...
whenever ann callahan makes hotel reservations for her boss...
i    ew in from boston.
i drove to portland.

(22.1) sasha broke the window.
(22.2) pat opened the door.

22.2 diathesis alternations

a neo-davidsonian event representation of these two sentences would be

9e,x,y breaking(e)^ breaker(e,sasha)
9e,x,y opening(e)^ opener(e,pat)

^brokenthing(e,y)^window(y)
^openedthing(e,y)^ door(y)

the main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple id136s that aren   t possible
from the pure surface string of words, or even from the parse tree. to extend the
earlier examples, if a document says that company a acquired company b, we   d
like to know that this answers the query was company b acquired? despite the fact
that the two sentences have very different surface syntax. similarly, this shallow
semantics might act as a useful intermediate language in machine translation.
j&m/slp3

semantic roles thus help generalize over different surface realizations of pred-
icate arguments. for example, while the agent is often realized as the subject of

31

problems	with	thematic	roles

hard	to	create	standard	set	of	roles	or	formally	
define	them
often	roles	need	to	be	fragmented	to	be	defined.
levin	and	rappaport	hovav (2015):	two	kinds	of	instruments
intermediary instruments	that	can	appear	as	subjects	

the	cook	opened	the	jar	with	the	new	gadget.	
the	new	gadget	opened	the	jar.	

enabling	instruments	that	cannot

shelly	ate	the	sliced	banana	with	a	fork.	
*the	fork	ate	the	sliced	banana.	

j&m/slp3

32

alternatives	to	thematic	roles

1. fewer	roles:	generalized	semantic	roles,	defined	

as	prototypes	(dowty 1991)
proto-agent	
proto-patient	

propbank

2. more	roles:	define	roles	specific	to	a	group	of	

predicates
framenet

j&m/slp3

semantic	role	labeling	(srl)	

    id14

22.6

    the	task	of	finding	the	semantic	roles	of	each	

recall that the difference between these two models of semantic roles is that
framenet (22.27) employs many frame-speci   c frame elements as roles, while prop-
bank (22.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speci   c labels, along with the more general argm labels. some
examples:

argument	of	each	predicate	in	a	sentence.

    framenet versus	propbank:

(22.27)

(22.28)

[the program]

can   t [blame]

[you]
cognizer
[the san francisco examiner]
arg0

target evaluee
issued
target arg1

[for being unable to identify it]
reason

[a special edition]

[yesterday]
argm-tmp

a simpli   ed id14 algorithm is sketched in fig. 22.4. while
there are a large number of algorithms, many of them use some version of the steps
in this algorithm.

most algorithms, beginning with the very earliest semantic role analyzers (sim-
mons, 1973), begin by parsing, using broad-coverage parsers to assign a parse to the
input string. figure 22.5 shows a parse of (22.28) above. the parse is then traversed

j&m/slp3

34

history

    semantic	roles	as	a	intermediate	semantics,	used	

early	in
    machine	translation	(wilks,	1973)
    question-answering	(hendrix	et	al.,	1973)
    spoken-language	understanding	(nash-webber,	1975)
    dialogue	systems	(bobrow et	al.,	1977)

    early	srl	systems

simmons	1973,	marcus	1980:	

    parser	followed	by	hand-written	rules	for	each	verb
    dictionaries	with	verb-specific	case	frames	(levin	1977)	

j&m/slp3

35

why	semantic	role	labeling?
    a	useful	shallow	semantic	representation
    improves	nlp	tasks	like:

    question	answering	

shen	and	lapata 2007,	surdeanu et	al.	2011

    machine	translation	

liu	and	gildea 2010,	lo	et	al.	2013

j&m/slp3

36

propbank

    palmer,	martha,	daniel	gildea,	and	paul	

kingsbury.	2005.	the	proposition	bank:	an	
annotated	corpus	of	semantic	roles.	
computational	linguistics,	 31(1):71   106	

j&m/slp3

37

propbank roles

following	dowty 1991

proto-agent

proto-patient

    volitional	involvement	in	event	or	state
    sentience	(and/or	perception)
    causes	an	event	or	change	of	state	in	another	participant	
    movement	(relative	to	position	of	another	participant)

    undergoes	change	of	state
    causally	affected	by	another	participant
    stationary	relative	to	movement	of	another	participant

j&m/slp3

38

propbank roles

    following	dowty 1991

roles	

    role	definitions	determined	verb	by	verb,	with	respect	to	the	other	

    semantic	roles	in	propbank are	thus	verb-sense	specific.

    each	verb	sense	has	numbered	argument:	arg0,	arg1,	arg2,   

arg0:	proto-agent
arg1: proto-patient
arg2:	usually:	benefactive,	instrument,	attribute,	or	end	state
arg3:	usually:	start	point,	benefactive,	instrument,	or	attribute
arg4	the	end	point
(arg2-arg5	are	not	really	that	consistent,	causes	a	problem	for	
labeling)

39

j&m/slp3

propbank frame	files

here are some slightly simpli   ed propbank entries for one sense each of the
verbs agree and fall. such propbank entries are called frame    les; note that the
de   nitions in the frame    le for each role (   other entity agreeing   ,    extent, amount
fallen   ) are informal glosses intended to be read by humans, rather than being formal
de   nitions.
(22.11) agree.01

arg0: agreer
arg1: proposition
arg2: other entity agreeing

ex1:
ex2:

[arg0 the group] agreed [arg1 it wouldn   t make an offer].
[argm-tmp usually] [arg0 john] agrees [arg2 with mary]
[arg1 on everything].

(22.12) fall.01

arg1: logical subject, patient, thing falling
arg2: extent, amount fallen
arg3: start point
arg4: end point, end state of arg1

j&m/slp3

40

the propbank semantic roles can be useful in recovering shallow semantic in-

the propbank semantic roles can be useful in recovering shallow semantic in-

advantage	of	a	probbank labeling

formation about verbal arguments. consider the verb increase:
(22.13) increase.01    go up incrementally   

formation about verbal arguments. consider the verb increase:
(22.13) increase.01    go up incrementally   
arg0: causer of increase
arg1:
arg2: amount increased by, ext, or mnr
arg3: start point
arg4: end point

arg0: causer of increase
thing increasing
arg1:
arg2: amount increased by, ext, or mnr
arg3: start point
arg4: end point

thing increasing

a propbank id14 would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case big
fruit co. is the agent and the price of bananas is the theme, despite the differing
surface forms.

a propbank id14 would allow us to infer the commonality in
this	would	allow	us	to	see	the	commonalities	in	these	3	sentences:
the event structures of the following three examples, that is, that in each case big
fruit co. is the agent and the price of bananas is the theme, despite the differing
surface forms.
(22.14)
(22.15)
(22.16)

[arg0 big fruit co. ] increased [arg1 the price of bananas].
propbank also has a number of non-numbered arguments called argms, (argm-
[arg1 the price of bananas] was increased again [arg0 by big fruit co. ]
tmp, argm-loc, etc) which represent modi   cation or adjunct meanings. these are
[arg1 the price of bananas] increased [arg2 5%].
j&m/slp3
relatively stable across predicates, so aren   t listed with each frame    le. data labeled

[arg0 big fruit co. ] increased [arg1 the price of bananas].
[arg1 the price of bananas] was increased again [arg0 by big fruit co. ]
[arg1 the price of bananas] increased [arg2 5%].

41

propbank also has a number of non-numbered arguments called argms, (argm-
tmp, argm-loc, etc) which represent modi   cation or adjunct meanings. these are
modifiers	or	adjuncts	of	the	predicate:	arg-m
relatively stable across predicates, so aren   t listed with each frame    le. data labeled
with these modi   ers can be helpful in training systems to detect temporal, location,
or directional modi   cation across predicates. some of the argm   s include:

argm-

yesterday evening, now
at the museum, in san francisco
down, to bangkok
clearly, with much enthusiasm
because ... , in response to the ruling
themselves, each other

tmp
when?
loc
where?
dir
where to/from?
mnr
how?
prp/cau why?
rec
adv
prd
while propbank focuses on verbs, a related project, nombank (meyers et al.,
2004) adds annotations to noun predicates. for example the noun agreement in
apple   s agreement with ibm would be labeled with apple as the arg0 and ibm as
the arg2. this allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.

miscellaneous
secondary predication

...ate the meat raw

j&m/slp3

42

event	by	different	nouns/verbs

capturing	descriptions	of	the	same	

while making id136s about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
id136s in many more situations, across different verbs, and also between verbs
and nouns. for example, we   d like to extract the similarity among these three sen-
tences:
(22.17)
(22.18)
(22.19) there has been a [arg2 5%] rise [arg1 in the price of bananas].

[arg1 the price of bananas] increased [arg2 5%].
[arg1 the price of bananas] rose [arg2 5%].

note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. we   d like a system to recognize that the

j&m/slp3

43

framenet

    baker	et	al.	1998,	fillmore	et	al.	2003,	fillmore	

and	baker	2009,	ruppenhofer et	al.	2006	

    roles	in	propbank are	specific	to	a	verb
    role	in	framenet are	specific	to	a	frame:	a	

background	knowledge	structure	that	defines	a	
set	of	frame-specific	semantic	roles,	called
frame	elements,	
    includes	a	set	of	predicates	that	use	these	roles
    each	word	evokes	a	frame	and	profiles	some	aspect	

of	the	frame

j&m/slp3

44

sentences.

for example, the change position on a scale frame is de   ned as follows:
   change	position	on	a	scale   	frame
this frame consists of words that indicate the change of an item   s posi-
tion on a scale (the attribute) from a starting point (initial value) to an
frame	consists	of	words	that	indicate	change	of	
end point (final value).
item   s	position	on	a	scale	(the	attribute)	from	
some of the semantic roles (frame elements) in the frame are de   ned as in
fig. 22.3. note that these are separated into core roles, which are frame speci   c, and
starting	point	(initial value)	to	end	point	(final
non-core roles, which are more like the arg-m arguments in propbank, expressed
value)
more general properties of time, location, and so on.

here are some example sentences:

core roles
non-core roles

(22.20)
(22.21)
(22.22)
(22.23)

[item oil] rose [attribute in price] [difference by 2%].
[item it] has increased [final state to having them 1 day a month].
[item microsoft shares] fell [final value to 7 5/8].
[item colon cancer incidence] fell [difference by 50%] [group among

men].

(22.24) a steady increase [initial value from 9.5] [final value to 14.3] [item

in dividends]

(22.25) a [difference 5%] [item dividend] increase...

45
note from these example sentences that the frame includes target words like rise,

j&m/slp3

fall, and increase. in fact, the complete frame consists of the following words:

attribute in a speci   ed way.

figure 22.3 the frame elements in the change position on a scale frame from the framenet labelers
guide (ruppenhofer et al., 2006).

   change	position	on	a	scale   	frame

soar
mushroom swell
swing
triple
tumble

edge
explode plummet
fall

verbs: dwindle move
advance
climb
decline
decrease    uctuate rise
diminish gain
grow
dip
double
increase skyrocket
jump
drop

rocket
shift

reach

slide

nouns: hike
decline
decrease

increase
rise

tumble

escalation shift
explosion
fall
   uctuation adverbs:
gain
increasingly
growth

framenet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be representing by inher-

j&m/slp3

46

   change	position	on	a	scale   	frame

8 chapter 22

    id14
core roles

attribute
difference
final state

the attribute is a scalar property that the item possesses.
the distance by which an item changes its position on the scale.
a description that presents the item   s state after the change in the attribute   s
value as an independent predication.
the position on the scale where the item ends up.

final value
initial state a description that presents the item   s state before the change in the at-

tribute   s value as an independent predication.

initial value the initial position on the scale from which the item moves away.
item
value range a portion of the scale, typically identi   ed by its end points, along which the

the entity that has a position on the scale.

duration
speed
group

values of the attribute    uctuate.

some non-core roles

the length of time over which the change takes place.
the rate of change of the value.
the group in which an item changes the value of an
attribute in a speci   ed way.

figure 22.3 the frame elements in the change position on a scale frame from the framenet labelers
guide (ruppenhofer et al., 2006).

j&m/slp3

47

