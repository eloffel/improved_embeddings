ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	13:	

dependency	syntax/parsing	

&	review	for	midterm

1

announcement

    project	proposal	due	today
    email	me	to	set	up	a	15-minute	meeting	next	

week	to	discuss	your	project	proposal

    times	posted	on	course	webpage
    let	me	know	if	none	of	those	work	for	you

2

announcement
    midterm	is	thursday,	room	#530
    closed-book,	but	you	can	bring	an	8.5x11	
sheet	(though	i	don   t	think	you   ll	need	to)

    we	will	start	at	10:35	am,	finish	at	11:50	am

3

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

4

what	is	syntax?

    rules,	principles,	processes	that	govern	

sentence	structure	of	a	language

    can	differ	widely	among	languages
    but	every	language	has	systematic	structural	

principles

5

constituent	parse	(bracketing/tree)
(s	(np	the	man)	(vp	walked	(pp	to	(np	the	park))))

s

np

vp

pp

np

nn

dt
vbd						in				dt				nn
the	man	walked	to	the	park

key:
s	=	sentence
np	=	noun	phrase
vp	=	verb	phrase
pp	=	prepositional	phrase
dt	=	determiner
nn	=	noun
vbd	=	verb	(past	tense)
in	=	preposition

6

constituent	parse	(bracketing/tree)
(s	(np	the	man)	(vp	walked	(pp	to	(np	the	park))))

s

np

vp

pp

np

nonterminals

nn

dt
vbd						in				dt				nn
the	man	walked	to	the	park

preterminals
terminals

7

penn	treebank	nonterminals

8

probabilistic	context-free	grammar	(pid18)
    assign	probabilities	to	rewrite	rules:

np	   dt		nn 0.5
0.3
np	   nns
0.2
np	   np		pp

nn	   man
nn	   park
nn	   walk
nn	      .

0.01
0.0004
0.002

given	a	treebank,	estimate	
these	probabilities	using	id113	
(   count	and	normalize   )

9

how	well	does	a	pid18	work?

    pid18	learned	from	the	penn	treebank	with	

id113	gets	about	73%	f1	score

    state-of-the-art	parsers	are	around	92%
    simple	modifications	can	improve	pid18s:

    smoothing
    tree	transformations	(selective	flattening)
    parent	annotation

10

parent	annotation

vp	   v		np		pp

vps    v		npvp ppvp

adds	more	information,	but	also	fragments	
counts,	making	parameter	estimates	noisier	
(since	we   re	just	using	id113)

11

how	well	does	a	pid18	work?

    pid18	learned	from	the	penn	treebank	with	

id113	gets	about	73%	f1	score

    state-of-the-art	parsers	are	around	92%
    simple	modifications	can	improve	pid18s:

    smoothing
    tree	transformations	(selective	flattening)
    parent	annotation
    lexicalization

12

collins	(1997)

13

lexicalized	pid18s

nonterminals are	decorated	with	

the	head word	of	the	subtree

14

lexicalization

    this	adds	a	lot	more	rules!
    many	more	parameters	to	estimate	  

smoothing	becomes	much	more	important
    e.g.,	right-hand	side	of	rule	might	be	factored	into	

several	steps

    but	it   s	worth	it	because	head	words	are	really	

useful	for	constituent	parsing

15

results	(collins,	1997)

16

head	rules

    how	are	heads	decided?
    most	researchers	use	deterministic	head	rules	

(magerman/collins)

    for	a	pid18	rule	a	   b1    	bn,	these	head	rules	

say	which	of	b1    	bn	is	the	head	of	the	rule

    examples:
s	   np		vp
vp	   vbd np		pp
np	   dt		jj		nn

17

head	annotation

from	noah	smith

18

lexical	head	annotation

from	noah	smith

19

lexical	head	annotation	   dependencies

remove	
nonlexical
parts:

from	noah	smith

20

dependencies

merge	
redundant	
nodes:

from	noah	smith

21

constituent	parse:

dependency	parse:

22

constituent	parse:

labeled dependency	parse:

nsubj

prep

dobj

det

pobj

det

nsubj =	   nominal	subject   
dobj =	   direct	object   
prep	=	   preposition	 modifier   
pobj =	   object	of	preposition   
det =	   determiner   

23

constituent	parse:

captures	some	semantic	
labeled dependency	parse:

relationships

nsubj

prep

dobj

det

pobj

det

nsubj =	   nominal	subject   
dobj =	   direct	object   
prep	=	   preposition	 modifier   
pobj =	   object	of	preposition   
det =	   determiner   

24

    how	(unlabeled)	dependency	trees	are	

typically	drawn:
    root	of	tree	is	represented	by	$	(   wall	symbol   )
    arrows	drawn	entirely	above	(or	below)	sentence
    arrows	are	directed	from	child	to	parent	(or	from	

parent	to	child);	you	will	see	both	in	practice   
don   t	get	confused!

source:          $  konnten  sie  es    bersetzen  ?

   wall   	symbol

reference:     $  could  you  translate  it  ?

25

crossing	dependencies

if	dependencies	cross	
(   nonprojective   ),	no	
longer	corresponds	to	

a	pid18

from	noah	smith

26

projective	vs.	nonprojective dependency	parsing
    english	dependency	treebanks are	mostly	

projective
    but	when	focusing	more	on	semantic	

relationships,	often	becomes	more	nonprojective
    some	(relatively)	free	word	order	languages,	

like	czech,	are	fairly	nonprojective

    nonprojective parsing	can	be	formulated	as	a	

minimum	spanning	tree	problem

    projective	parsing	cannot

27

dependency	parsing

    several	widely-used	algorithms
    different	guarantees	but	similar	performance	

in	practice

    graph-based:

    dynamic	programming	(eisner,	1997)
    minimum	spanning	tree	(mcdonald	et	al.,	2005)

    transition-based:

    shift-reduce	(nivre,	inter	alia)

28

dependency	parsers

    stanford	parser
    turboparser
    joakim nivre   s malt	parser
    ryan	mcdonald   s	mst	parser
    and	many	others	for	many	non-english	

languages

29

complexity	comparison

    constituent	parsing:	o(gn3)

    parsing	complexity	depends	on	grammar	structure	

(   grammar	constant   	g)

    since	it	has	lots	of	nonterminal-only	rules	at	the	top	of	
the	tree,	there	are	many	rule	probabilities	to	estimate

    dependency	parsing:	o(n3)

    operates	directly	on	words,	so	parsing	complexity	has	

no	grammar	constant

    features	designed	on	possible	dependencies	(pairs	of	

words)	and	larger	structures

    transition-based	parsing	algorithms	are	o(n),	though	

not	optimal;	also,	non-projective	parsing	is	faster

30

applications	of	dependency	parsing

    widely	used	for	nlp	tasks	because:

    faster	than	constituent	parsing
    captures	more	semantic	information

    text	classification	(features	on	dependencies)
    syntax-based	machine	translation
    relation	extraction

    e.g.,	extract	relation	between	sam	smith	and	aitech:
sam	smith	was	named	new	ceo	of	aitech.
    use	dependency	path	between	sam	smith	and	aitech:

    smith	   named,	named	   ceo,	ceo	   of,	of	   aitech

31

summary:	two	types	of	grammars

    phrase	structure	/	constituent	grammars

    inspired	mostly	by	chomsky	and	others
    only	appropriate	for	certain	languages	(e.g.,	english)

    dependency	grammars	

    closer	to	a	semantic	representation;	some	have	made	

this	more	explicit

    problematic	for	certain	syntactic	structures	(e.g.,	

conjunctions,	nesting	of	noun	phrases,	etc.)

    both	are	widely	used	in	nlp
    you	can	find	constituent	parsers	and	dependency	

parsers	for	several	languages	online

32

review

33

modeling,	id136,	learning

modeling:	define		score	function

    modeling:	how	do	we	assign	a	score	to	an	

(x,y)	pair	using	parameters				?

34

modeling,	id136,	learning

id136:	solve														_ modeling:	define		score	function

    id136:	how	do	we	efficiently	search	over	

the	space	of	all	labels?

35

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

    learning:	how	do	we	choose				?

learning:	choose	_

36

applications

37

applications	of	our	classification	framework

text	classification:

=	{objective,	subjective}

x

the	hulk	is	an	anger	fueled	monster	with	
incredible	strength	and	resistance	to	damage	.
in	trying	to	be	daring	and	original	,	it	comes	off	
as	only	occasionally	satirical	and	never	fresh	.

y

objective

subjective

38

applications	of	our	classification	framework

word	sense	classifier	for	bass:

=	{bass1,	bass2,	   ,	bass8}

x

he   s	a	bass	in	the	choir	.

our bass	is	line-caught	from	the	
atlantic	.

y

bass3

bass4

39

applications	of	our	classification	framework

skip-gram	model	as	a	classifier:

x

agriculture

agriculture

agriculture

y

<s>

is

the

=	v (the	entire	vocabulary)

corpus	(english	wikipedia):
agriculture	is	the	traditional	mainstay	of	the	
cambodian economy	.
but	benares has	been	destroyed	by	an	earthquake	.
   

40

simplest	kind	of	structured	prediction:	sequence	labeling

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

41

formulating	segmentation	tasks	as	sequence	labeling	

via	b-i-o	labeling:

named	entity	recognition

o																				o														o					b-person			i-person						o										o																	o

some			questioned			if									tim										cook							   s						first						product	

o														o									o																	o																	o								o					b-organization						o
would						be						a						breakaway				hit				for												apple														.

b	=	   begin   
i	=	   inside   
o	=	   outside   

42

applications	of	our	classifier	framework	so	far

task

input	(x)

output	(y)

text	

classification

a	sentence

gold	standard	

label for	x

output	space	(					)
pre-defined,	 small	

label	set (e.g.,	

{positive,	negative})

word	sense	
disambiguation

learning skip-
gram	word	
embeddings

part-of-speech	

tagging

instance	of	a	
particular	word	
(e.g.,	bass)	with

its	context

instance	of	a	
word	in	a	corpus

a	sentence

gold	standard	
word	sense	of	x

a	word	in	the	
context	of	x in	

a	corpus

gold	standard	
part-of-speech	

tags	for	x

pre-defined	sense	
inventory	from	
id138 for	bass

vocabulary

all	possible	part-of-
speech tag	sequences	
with	same	length	as	x

size	of

2-10

2-30

|v|

|p||x|

43

applications	of	classifier	framework	(continued)

task

input	(x)

output	(y)

output	space	(					)

size	of

named	
entity	

recognition

a	sentence

constituent	

parsing

a	sentence

gold	standard	named	

entity	labels for	x	

(bio	tags)

gold	standard	

constituent	parse	
(labeled	bracketing)	

of	x

all	possible	bio	label	
sequences	with	same	

length	as	x

all possible	labeled	

bracketings of	x

dependency	

parsing

a	sentence

gold	standard	

dependency	parse	
(labeled	directed	
spanning	tree)	of	x

all	possible	labeled	

directed	spanning	trees	

of	x

|p||x|

exponential
in	length	of	x

(catalan	
number)

exponential
in	length	of	x

44

    each	application	draws	from	particular	

linguistic	concepts	and	must	address	different	
kinds	of	linguistic	ambiguity/variability:
    word	sense:	sense	granularity,	relationships	

among	senses,	word	sense	ambiguity

    word	vectors:	distributional	properties,	sense	

ambiguity,	different	kinds	of	similarity

    part-of-speech:	tag	granularity,	tag	ambiguity
    parsing:	constituent/dependency	relationships,	

attachment	&	coordination	ambiguities

45

modeling

46

model	families

    linear	models

    lots	of	freedom	in	defining	features,	though	feature	

engineering	required	for	best	performance
    learning	uses	optimization	of	a	loss	function
    one	can	(try	to)	interpret	learned	feature	weights

    stochastic/generative	models

    linear	models	with	simple	   features   	(counts	of	events)
    learning	is	easy:	count	&	normalize	(but	smoothing	needed)
    easy	to	generate	samples

    neural	networks

    can	usually	get	away	with	less	feature	engineering
    learning	uses	optimization	of	a	loss	function
    hard	to	interpret	(though	we	try!),	but	often	works	best

47

special	case	of	linear	models:	
stochastic/generative	models

model

tasks

context	expansion

id165	language models

hidden	markov	models

language	modeling	 (for	

mt,	asr,	etc.)

part-of-speech	tagging,	
named	entity	recognition,

word	id91

increase	n

increase	order	of	id48	(e.g.,	bigram	

id48	   trigram id48)

probabilistic	context-free	

grammars

constituent	parsing

increase	size	of	rules,	e.g.,	flattening,	

parent	annotation,	etc.

   
   

   

all	use	id113	+	smoothing	(though	probably	different	kinds	of	smoothing)
all	assign	id203	to	sentences	(some	assign	id203	jointly	to	pairs	
of	<sentence,	something	else>)
all	have	the	same	trade-off	of	increasing	   context   	(feature	size)	and	
needing	more	data	/	better	smoothing

48

feature	engineering	for	text	classification

    two	features:

where

    what	should	the	weights	be?

49

higher-order	binary	feature	templates

unigram	binary	template:

bigram	binary	template:

trigram	binary	features

   

50

unigram	count	features

    a	``count      	feature	returns	the	count	of	a	

particular	word	in	the	text

    unigram	count	feature	template:

51

feature	count	cutoffs

    problem:	some	features	are	extremely	rare
    solution:	only	keep	features	that	appear	at	

least	k times	in	the	training	data

52

2-transformation	(1-layer)	network

vector	of	label	scores

    we   ll	call	this	a	   2-transformation   	neural	

network,	or	a	   1-layer   	neural	network

    input	vector	is	
    score	vector	is
    one	hidden	vector											(   hidden	layer   )

53

1-layer	neural	network	for	sentiment	classification

54

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    let   s	use	the	center	word	+	two	words	to	the	right:

vector	for	yo

vector	for	last

vector	for	name

   
if	name is	to	the	right	of	yo,	then	yo is	probably	a	form	of	your
    but	our	x above	uses	separate	dimensions	for	each	position!

    i.e.,	name	is	two	words	to	the	right
    what	if	name	is	one	word	to	the	right?		

55

convolution

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

vector	for	yo

vector	for	last

vector	for	name

56

pooling

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

how	do	we	convert	this	into	a	fixed-length	vector?
use	pooling:

max-pooling:	returns	maximum	value	in	
average pooling:	returns	average	of	values	in	

vector	for	yo

vector	for	last

vector	for	name

57

pooling

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

how	do	we	convert	this	into	a	fixed-length	vector?
use	pooling:

max-pooling:	returns	maximum	value	in	
average pooling:	returns	average	of	values	in	

vector	for	yo

vector	for	last

vector	for	name

then,	this	single	filter							produces	a	single	feature	
value	(the	output	of	some	kind	of	pooling).
in	practice,	we	use	many	filters	of	many	different	
lengths	(e.g.,	id165s	rather	than	words).	

58

convolutional	neural	networks

    convolutional	neural	networks	(convnets or	id98s)	use	

filters	that	are	   convolved	with   	(matched	against	all	
positions	of)	the	input

    think	of	convolution	as	   perform	the	same	operation	

everywhere	on	the	input	in	some	systematic	order   

       convolutional	layer   	=	set	of	filters	that	are	convolved	

with	the	input	vector	(whether	x or	hidden	vector)

    could	be	followed	by	more	convolutional	layers,	or	by	a	

    often	used	in	nlp	to	convert	a	sentence	into	a	feature	

type	of	pooling

vector

59

recurrent	neural	networks

   hidden	vector   

60

long	short-term	memory	(lstm)	recurrent	neural	networks

61

backward	&	bidirectional	lstms

bidirectional:	

if	shallow,	just	use	forward	and	backward	lstms	in	parallel,	concatenate	
final	two	hidden	vectors,	feed	to	softmax

62

deep	lstm
(2-layer)

layer	1

layer	2

63

recursive	neural	networks	for	nlp
    first,	run	a	constituent	parser	on	the	sentence
    convert	the	constituent	tree	to	a	binary	tree	

(each	rewrite	has	exactly	two	children)

    construct	vector	for	sentence	recursively	at	each	

rewrite	(   split	point   ):	

64

learning

65

cost	functions

    cost	function:	scores	output	against	a	gold	standard

    should	reflect	the	evaluation	metric	for	your	task

    usual	conventions:
   
   

for	classification,	what	cost	should	we	use?
for	classification,	what	cost	should	we	use?

66

empirical risk	minimization

(vapnik et	al.)

    replace	expectation	with	sum	over	examples:

67

empirical risk	minimization

(vapnik et	al.)

    replace	expectation	with	sum	over	examples:

problem:	np-hard	even	for	binary	
classification	with	linear	models

68

empirical	risk	minimization	with	surrogate	loss	functions

    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

many	possible	loss	
functions	to	consider	

optimizing

69

loss	functions

loss

name

cost	(   0-1   )

id88

hinge

log

where	used
intractable,	but	

underlies	   direct	error	

minimization   

id88	algorithm
(rosenblatt,	1958)

support	vector	

machines,	other	large-

margin	algorithms
logistic	regression,	
conditional	random	
fields,	maximum
id178	models

70

(sub)gradients	of	losses	for	linear	models

entry	j of	(sub)gradient	of	loss for	linear	model

not	subdifferentiable in	general

name

cost	(   0-1   )

id88

hinge

log

71

(sub)gradients	of	losses	for	linear	models

entry	j of	(sub)gradient	of	loss for	linear	model

not	subdifferentiable in	general

name

cost	(   0-1   )

id88

hinge

log

expectation	of	feature	value	with	respect	to	distribution	
over	y (where	distribution	 is	defined	by	theta)
alternative	notation:

72

visualization

e
r
o
c
s

five	possible	outputs

73

visualization

t
s
o
c

five	possible	outputs

74

visualization

t
s
o
c

gold	standard

75

visualization

t
s
o
c

gold	standard

76

visualization

t
s
o
c
	
+
e
r
o
c
s

	

gold	standard

77

id88	loss:

78

id88	loss:

e
r
o
c
s

gold	standard

79

id88	loss:

e
r
o
c
s

gold	standard

80

id88	loss:

e
r
o
c
s

effect	of	learning?

gold	standard

81

id88	loss:

e
r
o
c
s

effect	of	learning:
gold	standard	will	
have	highest	score

gold	standard

82

hinge	loss:

83

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

gold	standard

84

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

gold	standard

85

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

effect	of	learning?

gold	standard

86

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

effect	of	learning:

score	of	gold	standard	

will	be	higher	than	
score+cost of	all	

others

gold	standard

87

regularized empirical	risk	minimization

    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

id173	

strength

id173	

term

88

id173	terms

    most	common:	penalize	large	parameter	values
    intuition:	large	parameters	might	be	instances	of	

overfitting
    examples:

l2 id173:
(also	called	tikhonov id173	
or	ridge	regression)
l1 id173:
(also	called	basis	pursuit	or	lasso)

89

dropout

    popular	id173	method	for	neural	

networks

    randomly	   drop	out   	(set	to	zero)	some	of	the	

vector	entries	in	the	layers

90

id136

91

exponentially-large	search	problems

id136:	solve														_

    when	output	is	a	sequence	or	tree,	this	

argmax requires	iterating	over	an	
exponentially-large	set

92

learning	requires	solving	exponentially-hard	

problems	too!

entry	j of	(sub)gradient	of	loss for	linear	model

loss

id88

hinge

log

computing	 each	of	these	terms	
requires	iterating	through	every	

possible	output

93

dynamic	programming	(dp)

    what	is	dynamic	programming?

    a	family	of	algorithms	that	break	problems	into	smaller	

pieces	and	reuse	solutions	for	those	pieces

    only	applicable	when	the	problem	has	certain	properties	

(optimal	substructure	and	overlapping	sub-problems)

    in	this	class,	we	use	dp	to	iterate	over	exponentially-

large	output	spaces	in	polynomial	time

    we	focus	on	a	particular	type	of	dp	algorithm:	

memoization

94

implementing	dp	algorithms

    even	if	your	goal	is	to	compute	a	sum	or	a	

max,	focus	first	on	counting	mode (count	the	
number	of	unique	outputs	for	an	input)

    memoization =	recursion	+	saving/reusing	

solutions
    start	by	defining	recursive	equations
       memoize   	by	creating	a	table	to	store	all	

intermediate	results	from	recursive	equations,	use	
them	when	requested

95

id136	in	id48s

    since	the	output	is	a	sequence,	this	argmax

requires	iterating	over	an	exponentially-large	set

    last	week	we	talked	about	using	dynamic	
programming	(dp)	to	solve	these	problems

    for	id48s	(and	other	sequence	models),	the	for	

solving	this	is	called	the	viterbi	algorithm

96

viterbi	algorithm

    recursive	equations	+	memoization:
base	case:	
returns	id203	of	sequence	starting	with	label	y for	first	word

recursive	case:
computes	id203	of	max-id203	label	
sequence	that	ends	with	label	y at	position	m

final	value	is	in:

97

viterbi	algorithm

    space	and	time	complexity?
    can	be	read	off	from	the	recursive	equations:
space	complexity:
size	of	memoizationtable,	which	is	#	of	unique	indices	of	recursive	equations

length	of	
sentence

*

number	
of	labels

so,	space	complexity	is	o(|x|	|l|)

98

viterbi	algorithm

    space	and	time	complexity?
    can	be	read	off	from	the	recursive	equations:
time	complexity:
size	of	memoizationtable	*	complexity	of	computing	each	entry

length	of	
sentence

*

number	
of	labels

*

each	entry	requires	

iterating	through	the	labels

so,	time	complexity	is	o(|x|	|l|	|l|)	=	o(|x|	|l|2)	

99

feature	locality

    feature	locality:	how	   big   	are	your	features?
    when	designing	efficient	id136	algorithms	
(whether	w/	dp	or	other	methods),	we	need	
to	be	mindful	of	this

    features	can	be	arbitrarily	big	in	terms	of	the	

input,	but	not	in	terms	of	the	output!

    the	features	in	id48s	are	small	in	both	the	

input	and	output	sequences	(only	two	pieces	
at	a	time)

100

