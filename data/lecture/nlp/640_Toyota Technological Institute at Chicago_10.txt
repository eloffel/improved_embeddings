ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	10:	

neural	networks	for	nlp

1

announcements

    assignment	2	due	friday

    project	proposal	due	tuesday,	feb.	16

    midterm	on	thursday,	feb.	18

2

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

3

what	is	a	neural	network?

    just	think	of	a	neural	network	as	a	function
    it	has	inputs	and	outputs
    the	term	   neural   	typically	means	a	particular	

type	of	functional	building	block	(   neural	
layers   ),	but	the	term	has	expanded	to	mean	
many	things

4

classifier	framework

    linear	model	score	function:

    we	can	also	use	a	neural	network	for	the	score	

function!

5

neural	layer	=	affine	transform	+	nonlinearity

nonlinearity

affine	transform

    this	is	a	single	   layer   	of	a	neural	network
    input	vector	is	
    vector	of	   hidden	units   	is	

6

nonlinearities

    most	common:	elementwise application	of	g

function	to	each	entry	in	vector

    examples   

7

tanh:

8

(logistic)	sigmoid:

9

rectified	linear	unit	(relu):

10

2-layer	network

vector	of	label	scores

    this	is	a	2-layer	neural	network
    input	vector	is	
    output	vector	is

11

2-layer	neural	network	for	sentiment	classification

12

use	softmax function	to	convert	scores	into	probabilities

13

why	nonlinearities?

2-layer	network:

written	in	a	single	equation:

    if	g	is	linear,	then	we	can	rewrite	the	above	as	

a	single	affine	transform

    can	you	prove	this?	(use	distributivity of	

matrix	multiplication)

14

15

understanding	the	score	function

entry	2	of	bias	vector

row	vector	corresponding	to	row	2	of	

16

parameter	sharing

parameters	not	

shared	between	labels

parameters	

shared	between	

labels

17

observation

    with	linear	models:

    when	using	linear	models	for,	say,	sentiment	
classification,	every	feature	included	a	label
    no	parameters	were	shared	between	labels

    with	neural	networks

    we	now	have	parameters	shared	across	labels!
    we	still	have	some	parameters	that	are	devoted	to	

particular	labels

    to	define	x,	we	design	features	that	only	look	at	

the	input	(not	at	the	labels)

18

defining	input	features

    say	we   re	doing	sentiment	classification	and	

we	want	to	use	a	neural	network

    what	should	x be?

    it	has	to	be	independent	of	the	label
    it	has	to	be	a	fixed-length	vector

19

empirical	risk	minimization	with	surrogate	loss	functions
    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

many	possible	loss	
functions	to	consider	

optimizing

20

loss	functions

loss

name

cost	(   0-1   )

id88

hinge

log

where	used
intractable,	but	

underlies	   direct	error	

minimization   

id88	algorithm
(rosenblatt,	1958)

support	vector	

machines,	other	large-

margin	algorithms
logistic	regression,	
conditional	random	
fields,	maximum
id178	models

21

(sub)gradients	of	losses	for	linear	models

entry	j of	(sub)gradient	of	loss for	linear	model

not	subdifferentiable in	general

name

cost	(   0-1   )

id88

hinge

log

22

learning	with	neural	networks

    we	can	use	any	of	our	loss	functions	from	before,	

as	long	as	we	can	compute	(sub)gradients

    algorithm	for	doing	this	efficiently:	

id26

    it   s	basically	just	the	chain	rule	of	derivatives

23

computation	graphs

    a	useful	way	to	represent	the	computations	
performed	by	a	neural	model	(or	any	model!)

    why	useful?	makes	it	easy	to	implement	

automatic	differentiation	(id26)
    many	neural	net	toolkits	let	you	define	your	

model	in	terms	of	computation	graphs	
(theano,	torch,	tensorflow,	cntk,	id98,	
penne,	etc.)

24

id26

    id26 has	become	associated	with	

neural	networks,	but	it   s	much	more	general

    i	also	use	id26 to	compute	

gradients	in	linear models	for	structured	
prediction

25

a	simple	computation	graph:

    represents	expression	   a	+	3   

26

a	slightly	bigger	computation	graph:

    represents	expression	   (a	+	3)2 +	4a2   

27

operators	can	have	more	than	2	operands:

    still	represents	expression	   (a	+	3)2 +	4a2   

28

    more	concise:

29

overfitting &	id173
    when	we	can	fit	any	function,	overfitting

becomes	a	big	concern

    overfitting:	learning	a	model	that	does	well	on	
the	training	set	but	doesn   t	generalize	to	new	
data

    there	are	many	strategies	to	reduce	overfitting
(we   ll	use	the	general	term	id173 for	
any	such	strategy)	

    you	used	early	stopping	in	assignment	1,	

which	is	one	kind	of	id173

30

empirical	risk	minimization

    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

31

regularized empirical	risk	minimization

    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

id173	

strength

id173	

term

32

id173	terms

    most	common:	penalize	large	parameter	values
    intuition:	large	parameters	might	be	instances	of	

overfitting
    examples:

l2 id173:
(also	called	tikhonov id173	
or	ridge	regression)
l1 id173:
(also	called	basis	pursuit	or	lasso)

33

id173	terms

l2 id173:

differentiable,	widely-used

l1 id173:

not	differentiable	(but	is	subdifferentiable)
leads	to	sparse	solutions	(many	parameters	become	zero!)

34

dropout

    popular	id173	method	for	neural	

networks

    randomly	   drop	out   	(set	to	zero)	some	of	the	

vector	entries	in	the	layers

35

optimization	algorithms

    you	used	stochastic	gradient	descent	(sgd)	in	

assignment	1

    but	there	are	many	other	choices:

    adagrad
    adadelta
    adam
    sgd	with	momentum

    we	don   t	have	time	to	go	through	these	in	class,	

but	you	should	try	using	them!	(most		toolkits	
have	implementations	of	these	and	others)

36

