ttic	
   31190:	
   

natural	
   language	
   processing	
   

kevin	
   gimpel	
   
winter	
   2016	
   

	
   
	
   

lecture	
   8:	
   id136	
   in	
   
structured	
   prediceon	
   

1	
   

roadmap	
   

       classi   caeon	
   
       words	
   
      
lexical	
   semanecs	
   
      
language	
   modeling	
   
       sequence	
   labeling	
   
       syntax	
   and	
   syntacec	
   parsing	
   
       neural	
   network	
   methods	
   in	
   nlp	
   
       semanec	
   composieonality	
   
       semanec	
   parsing	
   
       unsupervised	
   learning	
   
       machine	
   translaeon	
   and	
   other	
   applicaeons	
   

2	
   

roadmap	
   

       classi   caeon	
   
       words	
   
      
lexical	
   semanecs	
   
      
language	
   modeling	
   
       sequence	
   labeling	
   
       neural	
   network	
   methods	
   in	
   nlp	
   
       syntax	
   and	
   syntacec	
   parsing	
   
       semanec	
   composieonality	
   
       semanec	
   parsing	
   
       unsupervised	
   learning	
   
       machine	
   translaeon	
   and	
   other	
   applicaeons	
   

3	
   

applicaeons	
   of	
   our	
   classi   er	
   framework	
   so	
   far	
   

task	
   

input	
   (x)	
   

output	
   (y)	
   

text	
   

classi   caeon	
   

a	
   sentence	
   

gold	
   standard	
   

label	
   for	
   x	
   

output	
   space	
   (	
   	
   	
   	
   	
   )	
   
pre-     de   ned,	
   small	
   

label	
   set	
   (e.g.,	
   

{posieve,	
   negaeve})	
   

word	
   sense	
   

disambiguaeon	
   

learning	
   skip-     
gram	
   word	
   
embeddings	
   

part-     of-     speech	
   

tagging	
   

instance	
   of	
   a	
   
parecular	
   word	
   
(e.g.,	
   bass)	
   with	
   

its	
   context	
   	
   

instance	
   of	
   a	
   

word	
   in	
   a	
   corpus	
   

a	
   sentence	
   

gold	
   standard	
   
word	
   sense	
   of	
   x	
   

pre-     de   ned	
   sense	
   
inventory	
   from	
   
id138	
   for	
   bass	
   

exponeneal	
   in	
   size	
   of	
   input!	
   
|v|	
   

a	
   word	
   in	
   the	
   
   structured	
   prediceon   	
   
context	
   of	
   x	
   in	
   
gold	
   standard	
   
part-     of-     speech	
   

vocabulary	
   

all	
   possible	
   part-     of-     
speech	
   tag	
   sequences	
   
with	
   same	
   length	
   as	
   x	
   

tags	
   for	
   x	
   

a	
   corpus	
   

|p||x|	
   

size	
   of	
   

2-     10	
   

2-     30	
   

4	
   

simplest	
   kind	
   of	
   structured	
   prediceon:	
   sequence	
   labeling	
   

part-     of-     speech	
   tagging	
   

determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   proper	
   	
   	
   	
   	
   proper	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
determiner	
   	
   	
   	
   	
   verb	
   (past)	
   	
   	
   	
   	
   	
   prep.	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   poss.	
   	
   	
   	
   	
   adj.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   
	
   	
   	
   
	
   	
   	
   some	
   	
   	
   	
   	
   	
   queseoned	
   	
   	
   	
   	
   	
   if	
   	
   	
   	
   	
   	
   	
   tim	
   	
   	
   	
   	
   	
   cook	
   	
   	
   	
   	
   	
      s	
   	
   	
   	
   	
   	
      rst	
   	
   	
   	
   	
   	
   product	
   	
   
	
   
	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjeceve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   proper	
   	
   	
   	
   	
   punc.	
   
	
   	
   	
   	
   modal	
   	
   	
   	
   	
   	
   	
   verb	
   	
   	
   	
   det.	
   	
   	
   	
   	
   	
   	
   	
   	
   adjeceve	
   	
   	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   prep.	
   	
   	
   	
   	
   	
   	
   noun	
   	
   	
   	
   	
   	
   punc.	
   
	
   	
   
	
   	
   	
   would	
   	
   	
   	
   	
   	
   be	
   	
   	
   	
   	
   	
   a	
   	
   	
   	
   	
   	
   breakaway	
   	
   	
   	
   	
   	
   hit	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   apple	
   	
   	
   	
   	
   	
   	
   	
   .	
   

named	
   enety	
   recognieon	
   

some	
   queseoned	
   if	
   tim	
   cook   s	
      rst	
   product	
   would	
   be	
   a	
   breakaway	
   hit	
   for	
   apple.	
   

person	
   

organization	
   

5	
   

hidden	
   markov	
   models	
   

y1 

x1 

y2 

x2 

y3 

x3 

y4 

x4 

	
   
transi=on	
   parameters:	
   
	
   	
   
emission	
   parameters:	
   	
   

6	
   

id48s	
   for	
   word	
   id91	
   

(brown	
   et	
   al.,	
   1992)	
   

y1 

y2 

y3 

y4 

jusen	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   bieber	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   for	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   president	
   

each	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   a	
   cluster	
   id	
   
so,	
   label	
   space	
   is	
   
simplifying	
   assumpeon:	
   

	
   each	
   word	
   is	
   in	
   exactly	
   one	
   cluster	
   

7	
   

id48s	
   for	
   word	
   id91	
   

(brown	
   et	
   al.,	
   1992)	
   

       given	
   a	
   set	
   of	
   sentences,	
   how	
   should	
   we	
   learn	
   the	
   
parameters	
   of	
   our	
   model?	
   
       how	
   about	
   we	
   use	
   maximum	
   likelihood	
   esemaeon,	
   e.g.:	
   

       problem:	
   we	
   don   t	
   have	
   any	
   	
   	
   	
   	
   	
   	
   	
   	
   	
      s!	
   	
   
       we	
   only	
   have	
   a	
   set	
   of	
   unlabeled	
   sentences:	
   

8	
   

       we	
   want	
   to	
   maximize	
   likelihood,	
   but:	
   

      our	
   id48	
   de   nes	
   
      our	
   data	
   only	
   contains	
   

       solueon:	
   marginalize	
   out	
   	
   
       this	
   idea	
   underlies	
   most	
   unsupervised	
   learning	
   

9	
   

       we	
   want	
   to	
   maximize	
   likelihood,	
   but:	
   

      our	
   id48	
   de   nes	
   
      our	
   data	
   only	
   contains	
   

       solueon:	
   marginalize	
   out	
   	
   
       this	
   idea	
   underlies	
   most	
   unsupervised	
   learning	
   

a	
   sum	
   over	
   an	
   

exponeneally-     large	
   set	
   

10	
   

       learning	
   requires	
   a	
   sum	
   over	
   an	
   exponeneally-     

large	
   set	
   (of	
   all	
   possible	
   id91s	
   of	
   the	
   
words)	
   

	
   
       it   s	
   actually	
   trivial	
   for	
   brown	
   id91	
   (why?)	
   
       for	
   any	
   id91,	
   we	
   can	
   easily	
   compute	
   the	
   
       problem:	
   there	
   are	
   too	
   many	
   possible	
   

log-     likelihood	
   of	
   the	
   data	
   

id91s	
   to	
   consider	
   them	
   all!	
   

11	
   

algorithm	
   for	
   brown	
   id91	
   

greedy	
   algorithm:	
   
	
   	
        	
   	
   iniealize	
   each	
   word	
   as	
   its	
   own	
   cluster	
   
	
   	
        	
   	
   greedily	
   merge	
   clusters	
   to	
   improve	
   data	
   likelihood	
   
	
   

outputs	
   hierarchical	
   id91	
   

12	
   

brown	
   clusters	
   as	
   vectors	
   

       by	
   tracing	
   order	
   in	
   which	
   clusters	
   are	
   merged,	
   we	
   

can	
   build	
   a	
   binary	
   tree	
   from	
   bohom	
   to	
   top	
   

       each	
   word	
   is	
   represented	
   by	
   its	
   binary	
   string	
   =	
   path	
   

from	
   root	
   to	
   leaf	
   

       each	
   intermediate	
   node	
   is	
   a	
   cluster	
   	
   
brown algorithm
       chairman	
   is	
   0010,	
      months   	
   =	
   01,	
   verbs	
   =	
   1:	
   

000
ceo

0

00

01

001

010

011

0011
president

november october

0010
chairman
    words merged according to contextual 

similarity

1

10

100
101
run sprint

11
walk

13	
   

saw	
   

peeped	
   
saww	
   

bought	
   
watched	
   

ate	
   

found	
   
checked	
   
   gured	
   

missed	
   
loved	
   
hated	
   

14	
   

   	
   

watching	
   
watchin	
   
watchn	
   

seeing	
   
   nding	
   
hearing	
   

loving	
   
enjoying	
   

liking	
   

saw	
   

peeped	
   
saww	
   

bought	
   
watched	
   

ate	
   

found	
   
checked	
   
   gured	
   

missed	
   
loved	
   
hated	
   

15	
   

   	
   

watching	
   
watchin	
   
watchn	
   

seeing	
   
   nding	
   
hearing	
   

loving	
   
enjoying	
   

liking	
   

saw	
   

peeped	
   
saww	
   

bought	
   
watched	
   

ate	
   

found	
   
checked	
   
   gured	
   

missed	
   
loved	
   
hated	
   

16	
   

random	
   

dirty	
   

common	
   

short	
   
tough	
   
rough	
   

quick	
   
simple	
   
unique	
   

verbs?	
   

   	
   

adjeceves?	
   

random	
   

dirty	
   

common	
   

short	
   
tough	
   
rough	
   

quick	
   
simple	
   
unique	
   

watching	
   
watchin	
   
watchn	
   

seeing	
   
   nding	
   
hearing	
   

loving	
   
enjoying	
   

liking	
   

saw	
   

peeped	
   
saww	
   

bought	
   
watched	
   

ate	
   

found	
   
checked	
   
   gured	
   

missed	
   
loved	
   
hated	
   

17	
   

   	
   

watching	
   
watchin	
   
watchn	
   

seeing	
   
   nding	
   
hearing	
   

loving	
   
enjoying	
   

liking	
   

random	
   

dirty	
   

common	
   

short	
   
tough	
   
rough	
   

quick	
   
simple	
   
unique	
   

saw	
   

peeped	
   
saww	
   

bought	
   
watched	
   

ate	
   

missed	
   
loved	
   
hated	
   

found	
   
checked	
   
   gured	
   
could	
   be	
   verbs	
   or	
   nouns,	
   but	
   

brown	
   id91	
   uses	
   one-     cluster-     per-     word	
   constraint	
   

18	
   

random	
   

dirty	
   

common	
   

:d	
   
^^	
   
=d	
   
*-     *	
   

;)	
   
:p	
   
:-     )	
   
xd	
   

:)	
   
(:	
   
=)	
   
:))	
   

:o	
   
o_o	
   
<<	
   
o.o	
   
	
   

:(	
   
:/	
   
-     _-     	
   
-     .-     	
   
	
   

short	
   
tough	
   
rough	
   

quick	
   
simple	
   
unique	
   

19	
   

       though	
   the	
   summaeon	
   is	
   trivial	
   for	
   brown	
   
id91,	
   in	
   general	
   we	
   need	
   to	
   be	
   able	
   to	
   
compute	
   summaeons	
   over	
   exponeneally-     large	
   
sets	
   for	
   sequence	
   models	
   and	
   other	
   
structured	
   prediceon	
   serngs	
   

20	
   

other	
   exponeneally-     large	
   problems	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

       when	
   output	
   is	
   a	
   sequence	
   (or	
   other	
   

structure),	
   this	
   argmax	
   requires	
   iteraeng	
   over	
   
an	
   exponeneally-     large	
   set	
   

21	
   

learning	
   requires	
   solving	
   exponeneally-     hard	
   

problems	
   too!	
   

entry	
   j	
   of	
   (sub)gradient	
   of	
   loss	
   for	
   linear	
   model	
   

loss	
   

id88	
   

hinge	
   

log	
   

compueng	
   each	
   of	
   these	
   terms	
   
requires	
   iteraeng	
   through	
   every	
   

possible	
   output	
   

22	
   

id136	
   in	
   structured	
   prediceon	
   

       think	
   of	
   id136	
   as	
      iteraeng	
   over	
   the	
   output	
   space   	
   
       speci   c	
   id136	
   problems:	
   

       compueng	
   argmax	
   in	
   classify()	
   for	
   classi   caeon	
   of	
   test	
   data	
   
       compueng	
   argmax	
   in	
   classify()	
   or	
   costclassify()	
   for	
   minimizing	
   

       compueng	
   feature	
   expectaeons	
   when	
   minimizing	
   log	
   loss	
   (requires	
   

id88/hinge	
   losses	
   

summing	
   over	
   outputs)	
   

       when	
   output	
   space	
   is	
   exponeneally-     large	
   (e.g.,	
   in	
   structured	
   

prediceon),	
   we	
   need	
   to	
   be	
   clever	
   about	
   how	
   we	
   do	
   this	
   

       today,	
   we   ll	
   discuss	
   dynamic	
   programming	
   for	
   id136	
   

23	
   

dynamic	
   programming	
   (dp)	
   

       what	
   is	
   dynamic	
   programming?	
   

       a	
   family	
   of	
   algorithms	
   that	
   break	
   problems	
   into	
   smaller	
   

pieces	
   and	
   reuse	
   solueons	
   for	
   those	
   pieces	
   

       only	
   applicable	
   when	
   the	
   problem	
   has	
   certain	
   properees	
   

(op=mal	
   substructure	
   and	
   overlapping	
   sub-     problems)	
   

      

in	
   this	
   class,	
   we	
   use	
   dp	
   to	
   iterate	
   over	
   exponeneally-     
large	
   output	
   spaces	
   in	
   polynomial	
   eme	
   

       we	
   focus	
   on	
   a	
   parecular	
   type	
   of	
   dp	
   algorithm:	
   

memoiza=on	
   

24	
   

implemeneng	
   dp	
   algorithms	
   
       even	
   if	
   your	
   goal	
   is	
   to	
   compute	
   a	
   sum	
   or	
   a	
   

max,	
   focus	
      rst	
   on	
   coun=ng	
   mode	
   (count	
   the	
   
number	
   of	
   unique	
   outputs	
   for	
   an	
   input)	
   

       memoizaeon	
   =	
   recursion	
   +	
   saving/reusing	
   

solueons	
   
      start	
   by	
   de   ning	
   recursive	
   equaeons	
   
         memoize   	
   by	
   creaeng	
   a	
   table	
   to	
   store	
   all	
   
intermediate	
   results	
   from	
   recursive	
   equaeons,	
   use	
   
them	
   when	
   requested	
   

25	
   

implemeneng	
   dp	
   algorithms	
   

       even	
   though	
   we	
   start	
   with	
   couneng	
   mode,	
   we	
   
need	
   to	
   keep	
   in	
   mind	
   how	
   the	
   model   s	
   score	
   
funceon	
   decomposes	
   across	
   parts	
   of	
   the	
   
outputs	
   
      i.e.,	
   how	
      large   	
   are	
   the	
   features?	
   	
   how	
   many	
   
items	
   in	
   the	
   output	
   sequence	
   are	
   needed	
   to	
   
compute	
   each	
   feature?	
   
      de   ne	
   a	
   funceon	
   called	
   partscore	
   that	
   
computes	
   all	
   the	
   features	
   (for	
   couneng	
   mode,	
   this	
   
funceon	
   will	
   return	
   1)	
   

26	
   

lab	
   

       we	
   will	
   now	
   talk	
   about	
   dynamic	
   programming	
   

on	
   the	
   whiteboard	
   and	
   implement	
   some	
   
algorithms	
   

27	
   

guide	
   to	
   designing/implemeneng	
   dp	
   algorithms	
   
1.    write	
   down	
   (or	
   draw)	
   all	
   possible	
   outputs	
   for	
   some	
   small	
   input	
   sizes	
   
2.   
iden=fy	
   subproblems	
   that	
   can	
   be	
   solved	
   independently	
   of	
   the	
   overall	
   
problem	
   (and	
   con   rm	
   that	
   solueons	
   can	
   be	
   reused)	
   

3.    write	
   down	
   recursive	
   formulas	
   on	
   paper	
   for	
   couneng	
   the	
   number	
   of	
   outputs	
   

given	
   an	
   input	
   size	
   

4.    work	
   out	
   (by	
   hand)	
   solueons	
   to	
   your	
   formulas	
   for	
   small	
   inputs,	
   con   rm	
   that	
   

counts	
   match	
   your	
   drawings	
   from	
   step	
   1	
   
implement	
   recursive	
   formulas,	
   con   rm	
   results	
   match	
   drawings,	
   compute	
   
counts	
   for	
   larger	
   input	
   sizes	
   
implement	
   memoizaeon	
   in	
   your	
   program:	
   create	
   a	
   table	
   t	
   (e.g.,	
   a	
   mule-     
dimensional	
   array)	
   indexed	
   by	
   signatures	
   of	
   subproblems,	
   save	
   subproblem	
   
solueons	
   auer	
   compueng	
   them,	
   use	
   them	
   when	
   possible	
   
con   rm	
   that	
   solueons	
   computed	
   by	
   memoizaeon	
   match	
   those	
   computed	
   by	
   
step	
   5	
   for	
   larger	
   input	
   sizes	
   (should	
   be	
   much	
   faster	
   to	
   compute!)	
   
   nally,	
   change	
   the	
   algorithm	
   from	
   couneng	
   to	
   compueng	
   sum/max	
   

5.   

6.   

7.   

8.   

28	
   

couneng	
   sequences	
   

       #	
   of	
   binary	
   sequences	
   of	
   length	
   n	
   
       #	
   of	
   binary	
   sequences	
   of	
   length	
   n	
   with	
   no	
      00   	
   
       #	
   of	
   binary	
   sequences	
   of	
   length	
   n	
   with	
   no	
      010   	
   
       #	
   of	
   binary	
   sequences	
   of	
   length	
   n	
   that	
   contain	
   at	
   least	
   

3	
   ones	
   (not	
   necessarily	
   consecueve)	
   

      

implement	
   max	
   over	
   binary	
   sequences	
   using	
   
backpointers	
   (use	
   a	
   feature	
   that	
   counts	
   instances	
   of	
   
   01   	
   and	
   give	
   it	
   a	
   weight	
   of	
   1)	
   

       extend	
   any	
   of	
   the	
   above	
   to	
   sequences	
   of	
   any	
   alphabet	
   

(not	
   just	
   binary)	
   

29	
   

