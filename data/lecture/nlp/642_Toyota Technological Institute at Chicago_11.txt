ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	11:	

recurrent	and	convolutional	

neural	networks	in	nlp

1

announcements

    assignment	3	assigned	yesterday,	due	feb.	29

    project	proposal	due	tuesday,	feb.	16

    midterm	on	thursday,	feb.	18

2

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

3

2-transformation	(1-layer)	network

vector	of	label	scores

    we   ll	call	this	a	   2-transformation   	neural	

network,	or	a	   1-layer   	neural	network

    input	vector	is	
    score	vector	is
    one	hidden	vector											(   hidden	layer   )

4

1-layer	neural	network	for	sentiment	classification

5

use	softmax function	to	convert	scores	into	probabilities

6

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																			 det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can	
add		u		on		fb lololol
verb														prep																			intj

pronoun	 					proper
noun

adj =	adjective
prep	=	preposition
intj =	interjection

    in	assignment	3,	you   ll	build	a	neural	network	classifier	

to	predict	a	word   s	pos	tag	based	on	its	context

7

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    e.g.,	predict	tag	of	yo given	context
    what	should	the	input	x	be?

    it	has	to	be	independent	of	the	label
    it	has	to	be	a	fixed-length	vector

8

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    e.g.,	predict	tag	of	yo given	context
    what	should	the	input	x	be?

word	vector	for	yo

9

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    e.g.,	predict	tag	of	yo given	context
    what	should	the	input	x	be?

word	vector	for	fir

word	vector	for	yo

10

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    when	using	word	vectors	as	part	of	input,	we	can	also	

treat	them	as	more	parameters	to	be	learned!

    this	is	called	   updating   	or	   fine-tuning   	the	vectors	

(since	they	are	initialized	using	something	like	id97)

word	vector	for	fir

word	vector	for	yo

11

neural	networks	for	twitter	part-of-speech	tagging

other																					verb																					det

noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can

    let   s	use	the	center	word	+	two	words	to	the	right:

vector	for	yo

vector	for	last

vector	for	name

   
if	name is	to	the	right	of	yo,	then	yo is	probably	a	form	of	your
    but	our	x above	uses	separate	dimensions	for	each	position!

    i.e.,	name	is	two	words	to	the	right
    what	if	name	is	one	word	to	the	right?		

12

features	and	filters

    we	could	use	a	feature	that	returns	1	if	name

is	to	the	right	of	the	center	word,	but	that	
does	not	use	the	word   s	embedding

    how	do	we	include	a	feature	like	   a	word	

similar	to	name appears	somewhere	to	the	
right	of	the	center	word   ?

    rather	than	always	specify	relative	position	
and	embedding,	we	want	to	add	filters that	
look	for	words	like	name	anywhere	in	the	
window	(or	sentence!)

13

filters

    for	now,	think	of	a	filter	as	a	vector	in	the	word	

vector	space

    the	filter	matches	a	particular	region	of	the	space
       match   	=	   has	high	dot	product	with   

14

convolution

    convolutional	neural	networks	use	a	bunch	of	

such	filters

    each	filter	is	matched	against	(dot	product	

computed	with)	each	word	in	the	entire	context	
window	or	sentence

    e.g.,	a	single	filter							is	a	vector	of	same	length	as	

word	vectors

15

convolution

vector	for	yo

vector	for	last

vector	for	name

16

convolution

vector	for	yo

vector	for	last

vector	for	name

17

convolution

vector	for	yo

vector	for	last

vector	for	name

18

convolution

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

vector	for	yo

vector	for	last

vector	for	name

19

pooling

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

how	do	we	convert	this	into	a	fixed-length	vector?
use	pooling:

max-pooling:	returns	maximum	value	in	
average pooling:	returns	average	of	values	in	

vector	for	yo

vector	for	last

vector	for	name

20

pooling

=	   feature	map   ,	has	an	entry	for	each	word	position	in	context	window	/	sentence

how	do	we	convert	this	into	a	fixed-length	vector?
use	pooling:

max-pooling:	returns	maximum	value	in	
average pooling:	returns	average	of	values	in	

vector	for	yo

vector	for	last

vector	for	name

then,	this	single	filter							produces	a	single	feature	
value	(the	output	of	some	kind	of	pooling).
in	practice,	we	use	many	filters	of	many	different	
lengths	(e.g.,	id165s	rather	than	words).	

21

convolutional	neural	networks

    convolutional	neural	networks	(convnets or	id98s)	use	

filters	that	are	   convolved	with   	(matched	against	all	
positions	of)	the	input

    informally,	think	of	convolution	as	   perform	the	same	
operation	everywhere	on	the	input	in	some	systematic	
order   

       convolutional	layer   	=	set	of	filters	that	are	convolved	

with	the	input	vector	(whether	x or	hidden	vector)

    could	be	followed	by	more	convolutional	layers,	or	by	a	

    often	used	in	nlp	to	convert	a	sentence	into	a	feature	

type	of	pooling

vector

22

recurrent	neural	networks

input	is	a	sequence:

not																									too																									bad

23

recurrent	neural	networks

input	is	a	sequence:

   hidden	vector   

24

recurrent	neural	networks

   hidden	vector   

25

disclaimer

    these	diagrams	are	often	useful	for	helping	us	
understand	and	communicate	neural	network	
architectures

    but	they	rarely	have	any	sort	of	formal	

semantics	(unlike	graphical	models)

    they	are	more	like	cartoons

26

long	short-term	memory	id56s	

(gateless)

   memory	cell   

27

long	short-term	memory	id56s	(gateless)

28

long	short-term	memory	id56s	(gateless)

29

long	short-term	memory	id56s	(gateless)

experiment:	text	classification
    stanford	sentiment	treebank

    binary	classification	(positive/negative)

    25-dim	word	vectors
    50-dim	cell/hidden	vectors
    classification	layer	on	final hidden	vector
    adagrad,	10	epochs,	mini-batch	size	10
    early	stopping	on	dev set

accuracy

80.6

30

output	gates

31

output	gates

32

output	gates

33

output	gates

this	is	pointwise
multiplication!				

is	a	vector

34

output	gates

this	is	pointwise
multiplication!				

is	a	vector

35

output	gates

logistic	sigmoid,	so	
output	ranges	from	

0	to	1

diagonal	
matrix

36

output	gates

gateless

output	gates

acc.
80.6
81.9

37

output	gates

gateless

output	gates

acc.
80.6
81.9

what   s	being	learned?

(demo)

38

input	gates

39

input	gates

again,	this	is	
pointwise

multiplication

40

input	gates

41

input	gates

diagonal	
matrix

42

input	gates

output	gates

difference

43

input	gates

gateless

output	gates
input	gates

acc.
80.6
81.9
84.4

44

input	and	output	gates

gateless

output	gates
input	gates

input	&	output	gates

acc.
80.6
81.9
84.4
84.6

45

forget	gates

46

forget	gates

47

forget	gates

48

forget	gates

gateless

output	gates
input	gates
forget	gates

acc.
80.6
81.9
84.4
82.1

49

all	gates

50

all	gates

acc.
80.6
81.9
84.4
84.6
82.1
84.1
82.6
85.3

gateless

output	gates
input	gates

input	&	output	gates

forget	gates

input	&	forget gates
forget	& output	gates

input,	forget,	output	gates

51

backward	&	bidirectional	lstms

bidirectional:	

if	shallow,	just	use	forward	and	backward	lstms	in	parallel,	concatenate	
final	two	hidden	vectors,	feed	to	softmax

52

backward	&	bidirectional	lstms

bidirectional:	

if	shallow,	just	use	forward	and	backward	lstms	in	parallel,	concatenate	
final	two	hidden	vectors,	feed	to	softmax
forward

backward

gateless

output	gates
input	gates
forget	gates

input,	forget,	output	gates

80.3
83.7
82.9
83.4
85.9

80.6
81.9
84.4
82.1
85.3

53

backward	&	bidirectional	lstms

bidirectional:	

if	shallow,	just	use	forward	and	backward	lstms	in	parallel,	concatenate	
final	two	hidden	vectors,	feed	to	softmax
forward

backward bidirectional

gateless

output	gates
input	gates
forget	gates

input,	forget,	output	gates

80.3
83.7
82.9
83.4
85.9

81.5
82.6
83.9
83.1
85.1

80.6
81.9
84.4
82.1
85.3

54

lstm

55

deep	lstm
(2-layer)

layer	1

layer	2

56

deep	lstm
(2-layer)

gateless

layer	1

input,	forget,	output

shallow	(50)
deep	(30,	30)
shallow	(50)
deep	(30,	30)

acc.
80.6
80.8
85.3
~85

layer	2

57

deep	bidirectional	lstms

concatenate	hidden	vectors	of	forward	&	backward	lstms,	connect	each	entry	
to	forward	and	backward	hidden	vectors	in	next	layer

58

(logistic)	sigmoid:

59

