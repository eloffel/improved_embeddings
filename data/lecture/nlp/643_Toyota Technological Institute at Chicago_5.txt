ttic	
   31190:	
   

natural	
   language	
   processing	
   

kevin	
   gimpel	
   
winter	
   2016	
   

	
   
	
   

lecture	
   5:	
   word	
   vectors	
   

1	
   

       assignment	
   1	
   now	
   due	
   thursday	
   11:59pm	
   

       assignment	
   2	
   will	
   be	
   assigned	
   on	
   thursday,	
   

due	
   tuesday,	
   feb.	
   2nd	
   

2	
   

homonymy	
   or	
   polysemy?	
   

axes	
   

an	
   edge	
   tool	
   with	
   a	
   heavy	
   

bladed	
   head	
   mounted	
   

across	
   a	
   handle	
   

a	
      xed	
   reference	
   line	
   for	
   

the	
   measurement	
   of	
   

coordinates	
   

3	
   

homonymy	
   or	
   polysemy?	
   

axes	
   

an	
   edge	
   tool	
   with	
   a	
   heavy	
   

bladed	
   head	
   mounted	
   

across	
   a	
   handle	
   

a	
      xed	
   reference	
   line	
   for	
   

the	
   measurement	
   of	
   

coordinates	
   

4	
   

homonymy	
   or	
   polysemy?	
   

axes	
   

an	
   imaginary	
   line	
   about	
   
which	
   a	
   body	
   rotates	
   

a	
      xed	
   reference	
   line	
   for	
   the	
   
measurement	
   of	
   coordinates	
   

5	
   

homonymy	
   or	
   polysemy?	
   

axes	
   

an	
   imaginary	
   line	
   about	
   
which	
   a	
   body	
   rotates	
   

a	
      xed	
   reference	
   line	
   for	
   the	
   
measurement	
   of	
   coordinates	
   

6	
   

homonymy	
   or	
   polysemy?	
   

down	
   

in	
   an	
   inacrve	
   or	
   inoperarve	
   

state	
   

being	
   or	
   moving	
   lower	
   in	
   

posiron	
   or	
   less	
   in	
   some	
   value	
   

7	
   

homonymy	
   or	
   polysemy?	
   

down	
   

in	
   an	
   inacrve	
   or	
   inoperarve	
   

state	
   

being	
   or	
   moving	
   lower	
   in	
   

posiron	
   or	
   less	
   in	
   some	
   value	
   

8	
   

homonymy	
   or	
   polysemy?	
   

down	
   

sou	
      ne	
   feathers	
   

	
   

being	
   or	
   moving	
   lower	
   in	
   

posiron	
   or	
   less	
   in	
   some	
   value	
   

9	
   

homonymy	
   or	
   polysemy?	
   

down	
   

sou	
      ne	
   feathers	
   

	
   

being	
   or	
   moving	
   lower	
   in	
   

posiron	
   or	
   less	
   in	
   some	
   value	
   

10	
   

homonymy	
   or	
   polysemy?	
   

down	
   

in	
   an	
   inacrve	
   or	
   
inoperarve	
   state	
   

	
   

being	
   or	
   moving	
   

lower	
   in	
   posiron	
   or	
   
less	
   in	
   some	
   value	
   

	
   

unhappy	
   

11	
   

is	
   a	
   (hyponym/hypernym/meronym/holonym)	
   of	
   

12	
   

is	
   a	
   (hyponym/hypernym/meronym/holonym)	
   of	
   

13	
   

piano1	
   

	
   
is	
   a	
   

(hyponym/hypernym)	
   

of	
   	
   
	
   

instrument1	
   

14	
   

piano1	
   

	
   
is	
   a	
   

(hyponym/hypernym)	
   

of	
   	
   
	
   

instrument1	
   

15	
   

       why	
   am	
   i	
   showing	
   you	
   pictures	
   instead	
   of	
   

words?	
   

       hypernymy,	
   meronymy,	
   etc.	
   are	
   relaronships	
   

between	
   synsets,	
   not	
   words	
   

16	
   

roadmap	
   

       classi   caron	
   
       words	
   
      

lexical	
   semanrcs	
   
       word	
   sense	
   
       word	
   vectors	
   
      
language	
   modeling	
   
       sequence	
   labeling	
   
       syntax	
   and	
   syntacrc	
   parsing	
   
       neural	
   network	
   methods	
   in	
   nlp	
   
       semanrc	
   composironality	
   
       semanrc	
   parsing	
   
       unsupervised	
   learning	
   
       machine	
   translaron	
   and	
   other	
   applicarons	
   

17	
   

ambiguity	
   
       one	
   form,	
   mulrple	
   meanings	
        	
   split	
   form	
   

       the	
   three	
   senses	
   of	
   fool	
   belong	
   to	
   di   erent	
   synsets	
   	
   

variability	
   
       mulrple	
   forms,	
   one	
   meaning	
        	
   merge	
   forms	
   

       each	
   synset	
   contains	
   senses	
   of	
   several	
   di   erent	
   words	
   	
   

18	
   

       are	
   we	
      nished?	
   	
   have	
   we	
   solved	
   the	
   problem	
   of	
   

      

represenrng	
   word	
   meaning?	
   
issues:	
   
       id138	
   has	
   limited	
   coverage	
   and	
   only	
   exists	
   for	
   a	
   small	
   set	
   

       wsd	
   requires	
   training	
   data,	
   whether	
   supervised	
   or	
   seeds	
   for	
   

of	
   languages	
   

semi-     supervised	
   

       id138	
   only	
   tells	
   us	
   whether	
   two	
   forms	
   are	
   similar	
   or	
   

di   erent,	
   not	
   the	
   amount	
   of	
   similarity/dissimilarity	
   

       be   er	
   approach:	
   jointly	
   learn	
   representarons	
   for	
   all	
   

words	
   from	
   raw,	
   unlabeled	
   text	
   

vector	
   representarons	
   of	
   words	
   

t-     sne	
   visualizaron	
   from	
   turian	
   et	
   al.	
   (2010)	
   

20	
   

why	
   vector	
   models	
   of	
   word	
   meaning?	
   
compurng	
   the	
   similarity	
   between	
   words	
   

tall	
   is	
   similar	
   to	
   height	
   
	
   
quesron	
   answering:	
   
q:	
   how	
   tall	
   is	
   mt.	
   everest?	
   
a:	
      the	
   o   cial	
   height	
   of	
   mount	
   everest	
   is	
   29029	
   feet   	
   

j&m/slp3	
   

21	
   

distriburonal	
   models	
   of	
   meaning	
   
=	
   vector	
   space	
   models	
   of	
   meaning	
   	
   

=	
   vector	
   semanrcs	
   

zellig	
   harris	
   (1954):	
   

          oculist	
   and	
   eye-     doctor	
      	
   occur	
   in	
   almost	
   the	
   same	
   

          if	
   a	
   and	
   b	
   have	
   almost	
   idenrcal	
   environments	
   we	
   say	
   that	
   

environments   	
   

they	
   are	
   synonyms.   	
   

	
   

j.r.	
   firth	
   (1957):	
   	
   

          you	
   shall	
   know	
   a	
   word	
   by	
   the	
   company	
   it	
   keeps!   	
   

j&m/slp3	
   

22	
   

warren	
   weaver	
   (1955):	
   
   but	
   if	
   one	
   lengthens	
   the	
   slit	
   in	
   the	
   opaque	
   
mask,	
   unrl	
   one	
   can	
   see	
   not	
   only	
   the	
   central	
   
word	
   in	
   quesron	
   but	
   also	
   say	
   n	
   words	
   on	
   either	
   
side,	
   then	
   if	
   n	
   is	
   large	
   enough	
   one	
   can	
   
unambiguously	
   decide	
   the	
   meaning	
   of	
   the	
   
central	
   word      	
   	
   

intuirons	
   of	
   distriburonal	
   models	
   

       suppose	
   i	
   gave	
   you	
   the	
   following	
   corpus:	
   

a	
   bo   le	
   of	
   tesg  ino	
   is	
   on	
   the	
   table	
   
everybody	
   likes	
   tesg  ino	
   
tesg  ino	
   makes	
   you	
   drunk	
   
we	
   make	
   tesg  ino	
   out	
   of	
   corn.	
   

       what	
   is	
   tesg  ino?	
   
       from	
   context,	
   we	
   can	
   guess	
   tesg  ino	
   is	
   an	
   alcoholic	
   

beverage	
   like	
   beer	
   
intuiron:	
   two	
   words	
   are	
   similar	
   if	
   they	
   have	
   similar	
   
word	
   contexts	
   

      

j&m/slp3	
   

many	
   ways	
   to	
   get	
   word	
   vectors	
   
some	
   based	
   on	
   counrng,	
   some	
   based	
   on	
   predicron/learning	
   
some	
   sparse,	
   some	
   dense	
   
some	
   have	
   interpretable	
   dimensions,	
   some	
   don   t	
   
	
   
shared	
   ideas:	
   

	
   model	
   meaning	
   of	
   a	
   word	
   by	
      embedding   	
   it	
   in	
   a	
   vector	
   space	
   
these	
   word	
   vectors	
   are	
   also	
   called	
      embeddings   	
   

	
   
contrast:	
   in	
   tradironal	
   nlp,	
   word	
   meaning	
   is	
   represented	
   by	
   a	
   
vocabulary	
   index	
   (   word	
   #545   ),	
   including	
   in	
   assignment	
   1!	
   

j&m/slp3	
   

25	
   

roadmap	
   

       count-     based	
   word	
   vectors	
   
       dimensionality	
   reducron	
   
       predicron-     based	
   word	
   vectors	
   

26	
   

distriburonal	
   word	
   vectors	
   

       we   ll	
   start	
   with	
   the	
   simplest	
   way	
   to	
   create	
   

word	
   vectors:	
   

       count	
   occurrences	
   of	
   context	
   words	
   

      so,	
   vector	
   for	
   pineapple	
   has	
   counts	
   of	
   words	
   in	
   the	
   
context	
   of	
   pineapple	
   in	
   a	
   dataset	
   
      one	
   entry	
   in	
   vector	
   for	
   each	
   unique	
   context	
   word	
   
      stack	
   these	
   vectors	
   for	
   all	
   words	
   in	
   a	
   vocabulary	
   v	
   
to	
   produce	
   a	
   count	
   matrix	
   c	
   
      c	
   is	
   called	
   the	
   word-     context	
   matrix	
   (or	
   word-     
word	
   co-     occurrence	
   matrix)	
   

27	
   

to the right, in which case the cell represents the number of times (in some training
corpus) the column word occurs in such a   4 word window around the row word.
for example here are 7-word windows surrounding four sample words from the
brown corpus (just one example of each word):

counrng	
   context	
   words	
   

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

preserve or jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

sugar

aardvark

pinch result

aardvark computer

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 17.2 shows a selection from the word-word
   
co-occurrence matrix computed from the brown corpus for these four words.
   
   
sugar
   
   

apricot
pineapple
digital
apricot
pineapple
information
   
information
figure 19.2 co-occurrence vectors for four words, computed from the brown corpus,
showing only six of the dimensions (hand-picked for pedagogical purposes). note that a
real vector would be vastly more sparse.

data
0
0
computer
1
6

data
0
0
1
6

...
...
...
...
...

0
0
2
1

1
1
0
0

0
0
0
0

0
0
1
4

1
1
0
0

digital

result

pinch

0
0
2
1

1
1
0
0

1
1
0
0

0
0
1
4

0
0
0
0

...

the shading in fig. 17.2 makes clear the intuition that the two words apricot

j&m/slp3	
   

word-     context	
   matrix	
   

       we	
   showed	
   4x6,	
   but	
   actual	
   matrix	
   is	
   |v|x|v|	
   

      very	
   large,	
   but	
   very	
   sparse	
   (mostly	
   zeroes)	
   
      lots	
   of	
   e   cient	
   algorithms	
   for	
   sparse	
   matrices	
   
      in	
   your	
   next	
   homework	
   assignment,	
   you	
   will	
   use	
   a	
   
smaller	
   vocabulary	
   vc	
   	
   for	
   the	
   context,	
   so	
   your	
   
word-     context	
   matrix	
   will	
   be	
   |v|x|vc|	
   

29	
   

context	
   window	
   size	
   
       size	
   of	
   context	
   window	
   a   ects	
   vectors	
   
       one	
   table	
   below	
   uses	
   window	
   size	
   1	
   and	
   the	
   
other	
   uses	
   window	
   size	
   10.	
   which	
   is	
   which?	
   
       (think	
   of	
   each	
   row	
   as	
   a	
   cluster):	
   

window	
   size	
   a	
   

mr.	
   mrs.	
   dr.	
   ms.	
   prof.	
   

truly	
   wildly	
   polircally	
      nancially	
   

his	
   your	
   her	
   its	
   

window	
   size	
   b	
   

takeo   	
   alrtude	
   airport	
   carry-     on	
   
	
   clinic	
   physician	
   doctor	
   medical	
   
   nancing	
   equity	
   investors	
      rms	
   

30	
   

context	
   window	
   size	
   
       size	
   of	
   context	
   window	
   a   ects	
   vectors	
   
       one	
   table	
   below	
   uses	
   window	
   size	
   1	
   and	
   the	
   
other	
   uses	
   window	
   size	
   10.	
   which	
   is	
   which?	
   
more	
   syntacrc/funcronal,	
   	
   
       (think	
   of	
   each	
   row	
   as	
   a	
   cluster):	
   
same	
   part-     of-     speech	
   tag	
   

more	
   semanrc/topical,	
   	
   
mix	
   of	
   part-     of-     speech	
   tags	
   

window	
   size	
   1	
   

mr.	
   mrs.	
   dr.	
   ms.	
   prof.	
   

truly	
   wildly	
   polircally	
      nancially	
   

his	
   your	
   her	
   its	
   

window	
   size	
   10	
   

takeo   	
   alrtude	
   airport	
   carry-     on	
   
	
   clinic	
   physician	
   doctor	
   medical	
   
   nancing	
   equity	
   investors	
      rms	
   

31	
   

measuring	
   similarity	
   

       given	
   2	
   word	
   vectors,	
   how	
   should	
   we	
   measure	
   their	
   
       most	
   measure	
   of	
   vectors	
   similarity	
   are	
   based	
   on	
   dot	
   

similarity?	
   

product	
   (or	
   inner	
   product):	
   

       high	
   when	
   vectors	
   have	
   large	
   values	
   in	
   same	
   dimensions	
   

j&m/slp3	
   

32	
   

problem	
   with	
   dot	
   product	
   

       dot	
   product	
   is	
   larger	
   if	
   vector	
   is	
   longer	
   
       vector	
   length:	
   

	
   
       frequent	
   words	
        	
   larger	
   counts	
        	
   larger	
   dot	
   products	
   
       this	
   is	
   bad:	
   we	
   don   t	
   want	
   a	
   similarity	
   metric	
   to	
   be	
   

sensirve	
   to	
   word	
   frequency	
   

j&m/slp3	
   

33	
   

soluron:	
   cosine	
   similarity	
   

       divide	
   dot	
   product	
   by	
   lengths	
   of	
   the	
   vectors	
   

       turns	
   out	
   to	
   be	
   the	
   cosine	
   of	
   the	
   angle	
   

between	
   them!	
   

j&m/slp3	
   

34	
   

cosine	
   as	
   a	
   similarity	
   metric	
   

direcrons	
   	
   

       -     1:	
   vectors	
   point	
   in	
   opposite	
   
       +1:	
   	
   vectors	
   point	
   in	
   same	
   
       0:	
   vectors	
   are	
   orthogonal	
   

direcrons	
   

       word	
   counts	
   are	
   non-     negarve,	
   

so	
   cosine	
   ranges	
   from	
   0	
   to	
   1	
   

j&m/slp3	
   

35	
   

problems	
   with	
   raw	
   counts	
   
       raw	
   word	
   counts	
   are	
   not	
   a	
   great	
   measure	
   of	
   

associaron	
   between	
   words	
   
      why	
   not?	
   
      very	
   skewed:	
   the	
   and	
   of	
   are	
   frequent,	
   but	
   not	
   the	
   
most	
   discriminarve	
   

       rather	
   have	
   a	
   measure	
   that	
   asks	
   whether	
   a	
   context	
   

word	
   is	
   informa@ve	
   about	
   the	
   center	
   word	
   
       posi@ve	
   pointwise	
   mutual	
   informa@on	
   (ppmi)	
   

j&m/slp3	
   

36	
   

pointwise	
   mutual	
   informa@on	
   (pmi)	
   
       do	
   two	
   events	
   x	
   and	
   y	
   co-     occur	
   more	
   ouen	
   than	
   

if	
   they	
   were	
   independent?	
   

       here,	
   x	
   is	
   the	
   center	
   word	
   and	
   y	
   is	
   the	
   word	
   in	
   the	
   

context	
   window	
   

       each	
   of	
   these	
   probabilires	
   can	
   be	
   esrmated	
   from	
   

the	
   counts	
   collected	
   from	
   the	
   corpus	
   
       replace	
   raw	
   counts	
   with	
   pmi	
   scores	
   

37	
   

posi@ve	
   pointwise	
   mutual	
   informa@on	
   (ppmi)	
   

       pmi	
   ranges	
   from	
      in   nity	
   to	
   +in   nity	
   
       but	
   negarve	
   values	
   are	
   problemarc:	
   

       things	
   are	
   co-     occurring	
   less	
   than	
   we	
   expect	
   by	
   

chance	
   

       unreliable	
   without	
   enormous	
   corpora	
   

       so	
   we	
   just	
   replace	
   negarve	
   pmi	
   values	
   by	
   0,	
   calling	
   it	
   

posirve	
   pmi	
   (ppmi)	
   

	
   
	
   

j&m/slp3	
   

alternarve	
   to	
   ppmi	
   

       e-     idf:	
   (that   s	
   a	
   hyphen	
   not	
   a	
   minus	
   sign)	
   
       product	
   of	
   two	
   factors:	
   

      term	
   frequency	
   (tf;	
   luhn,	
   1957):	
   count	
   of	
   word	
   (or	
   
possibly	
   log	
   of	
   count)	
   
      inverse	
   document	
   frequency	
   (idf;	
   sparck	
   jones,	
   1972)	
   

       n:	
   total	
   number	
   of	
   documents	
   
       dfi:	
   #	
   of	
   documents	
   with	
   word	
   i	
   

idfi = log n
dfi

!
#
#
"

$
&
&
%

j&m/slp3	
   

roadmap	
   

       count-     based	
   word	
   vectors	
   
       dimensionality	
   reducron	
   
       predicron-     based	
   word	
   vectors	
   

40	
   

sparse	
   versus	
   dense	
   vectors	
   

       so	
   far,	
   our	
   vectors	
   are	
   

       long	
   (length	
   |v|=	
   20,000	
   to	
   50,000)	
   
       sparse	
   (mostly	
   zero)	
   

       why	
   might	
   we	
   want	
   to	
   reduce	
   vector	
   dimensionality?	
   

41	
   

why	
   reduce	
   dimensionality?	
   

       short	
   vectors	
   may	
   be	
   easier	
   to	
   use	
   as	
   features	
   
       reducing	
   dimensionality	
   may	
   be   er	
   handle	
   

(fewer	
   weights	
   to	
   tune)	
   

variability	
   in	
   natural	
   language	
   due	
   to	
   synonymy:	
   
       car	
   and	
   automobile	
   are	
   synonyms,	
   but	
   represented	
   as	
   
       this	
   fails	
   to	
   capture	
   similarity	
   between	
   a	
   word	
   with	
   car	
   
as	
   a	
   neighbor	
   and	
   one	
   with	
   automobile	
   as	
   a	
   neighbor	
   

disrnct	
   dimensions	
   

j&m/slp3	
   

42	
   

dimensionality	
   reducron:	
   intuiron	
   
       approximate	
   an	
   n-     dimensional	
   dataset	
   using	
   

fewer	
   dimensions:	
   
      rotate	
   axes	
   into	
   a	
   new	
   space	
   
      in	
   which	
      rst	
   dimension	
   captures	
   most	
   variance	
   in	
   
original	
   dataset	
   
      next	
   dimension	
   captures	
   next	
   most	
   variance,	
   etc.	
   

       many	
   such	
   (related)	
   methods:	
   

      principal	
   component	
   analysis	
   (pca)	
   
      factor	
   analysis	
   
      singular	
   value	
   decomposiron	
   (svd)	
   

j&m/slp3	
   

43	
   

dimensionality	
   reducron	
   

pca dimension 1

pca dimension 2

6
6

5
5

4
4

3
3

2
2

1
1

1
1

2
2

3
3

4
4

5
5

6
6

j&m/slp3	
   

44	
   

singular	
   value	
   decomposiron	
   

svd	
   is	
   a	
   way	
   to	
   factorize	
   a	
   matrix	
   
any	
   rectangular	
   w	
   x	
   c	
   matrix	
   x	
   equals	
   the	
   product	
   of	
   3	
   
matrices:	
   
w:	
   rows	
   match	
   original	
   but	
   each	
   of	
   m	
   columns	
   represents	
   a	
   
dimension	
   in	
   a	
   new	
   latent	
   space,	
   such	
   that	
   	
   

       m	
   column	
   vectors	
   are	
   orthogonal	
   to	
   each	
   other	
   
       columns	
   are	
   ordered	
   by	
   the	
   amount	
   of	
   variance	
   in	
   the	
   

dataset	
   each	
   new	
   dimension	
   accounts	
   for	
   

s:	
   	
   diagonal	
   m	
   x	
   m	
   matrix	
   of	
   singular	
   values	
   expressing	
   the	
   
importance	
   of	
   each	
   dimension.	
   
c:	
   columns	
   corresponding	
   to	
   original	
   but	
   m	
   rows	
   
corresponding	
   to	
   singular	
   values	
   

j&m/slp3	
   

45	
   

original m. since the    rst dimensions encode the most variance, one way to view
the reconstruction is thus as modeling the most important information in the original
dataset.

svd	
   applied	
   to	
   word-     context	
   matrix	
   

svd applied to co-occurrence matrix x:

2666664

x

=

w

3777775

2666664

2666664

s1 0
0 ... 0
0 s2 0 ... 0
0 s3 ... 0
0
...
...
...
...
0 ... sv
0
0
|v|   |v|

...

3777775

2666664

c

3777775

|v|   |v|

|v|   |v|
taking only the top k dimensions after svd applied to co-occurrence matrix x:

|v|   |v|

s1 0
0 ... 0
0 s2 0 ... 0
0 s3 ... 0
0
...
...
...

...

...

i

c
k   |v|

j&m/slp3	
   

46	
   

(simplifying	
   here	
   by	
   assuming	
   the	
   matrix	
   has	
   rank	
   |v|)	
   

=

2666664

x

3777775

w

2666664

h

3777775

3777775
2666664

3777775

3777775

2666664

3777775

2666664

2666664

3777775
2666664

3777775

|v|   |v|

|v|   |v|
truncated	
   svd	
   on	
   word-     context	
   matrix	
   
taking only the top k dimensions after svd applied to co-occurrence matrix x:

|v|   |v|

0

0 . . . sv

0
|v|   |v|

2666664

x

|v|   |v|

3777775

=

2666664

w

|v|    k

s1 0
0 . . . 0
0 s2 0 . . . 0
0 s3 . . . 0
0
...
...
...
...
0 . . . sk
0
0
k    k

...

i

c

k   |v|

h

3777775

figure 19.11 svd factors a matrix x into a product of three matrices, w, s, and c. taking
the    rst k dimensions gives a |v|    k matrix wk that has one k-dimensioned row per word that
can be used as an embedding.

matrix	
   containing	
   	
   
new	
   k-     dimensional	
   

word	
   vectors;	
   

k	
   might	
   be	
   50	
   to	
   1000	
   

using only the top k dimensions (corresponding to the k most important singular
values), leads to a reduced |v|   k matrix wk, with one k-dimensioned row per word.

j&m/slp3	
   

47	
   

svd	
   embeddings	
   versus	
   sparse	
   vectors	
   

       dense	
   svd	
   embeddings	
   somermes	
   work	
   be   er	
   than	
   

sparse	
   ppmi	
   matrices	
   at	
   tasks	
   like	
   word	
   similarity	
   
       denoising:	
   low-     order	
   dimensions	
   may	
   represent	
   unimportant	
   

informaron	
   

       truncaron	
   may	
   help	
   the	
   models	
   generalize	
   be   er	
   to	
   unseen	
   data	
   
       having	
   a	
   smaller	
   number	
   of	
   dimensions	
   may	
   make	
   it	
   easier	
   for	
   

classi   ers	
   to	
   properly	
   weight	
   the	
   dimensions	
   for	
   the	
   task	
   
       dense	
   models	
   may	
   do	
   be   er	
   at	
   capturing	
   higher	
   order	
   co-     

occurrence	
   

j&m/slp3	
   

48	
   

roadmap	
   

       count-     based	
   word	
   vectors	
   
       dimensionality	
   reducron	
   
       predicron-     based	
   word	
   vectors	
   

49	
   

other	
   ways	
   to	
   learn	
   word	
   vectors	
   

       let   s	
   use	
   our	
   classi   caron	
   framework	
   
       we	
   want	
   to	
   use	
   unlabeled	
   text	
   to	
   train	
   the	
   

vectors	
   

       we	
   can	
   convert	
   our	
   unlabeled	
   text	
   into	
   a	
   

classi   caron	
   problem!	
   

       how?	
   	
   (there	
   are	
   many	
   possibilires)	
   

50	
   

other	
   ways	
   to	
   learn	
   word	
   vectors	
   

       aside:	
   any	
   labeled	
   dataset	
   can	
   be	
   used	
   to	
   learn	
   word	
   vectors	
   

(depending	
   on	
   model/features)	
   

       how	
   could	
   you	
   use	
   your	
   assignment	
   1	
   classi   ers	
   to	
   produce	
   

      

word	
   vectors?	
   
learned	
   feature	
   weights	
   for	
   my	
   5-     way	
   senrment	
   classi   er	
   
(binary	
   unigram	
   features),	
   for	
   two	
   words:	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   feel-     good	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   dull	
   

label	
   

strongly	
   posirve	
   

posirve	
   
neutral	
   
negarve	
   

weight	
   
0.025	
   
0.035	
   
-     0.045	
   

0	
   

label	
   

weight	
   

strongly	
   posirve	
   

posirve	
   
neutral	
   
negarve	
   

0	
   
0	
   

-     0.04	
   
0.015	
   
0.025	
   

51	
   

strongly	
   negarve	
   

-     0.015	
   

strongly	
   negarve	
   

modeling,	
   id136,	
   learning	
   for	
   word	
   vectors	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

modeling:	
   de   ne	
   	
   score	
   funcron	
   

learning:	
   choose	
   _	
   	
   	
   

(x,y)	
   pairs!	
   

       before	
   modeling/id136/learning,	
   we	
   must	
   de   ne	
   
       this	
   isn   t	
   text	
   classi   caron,	
   where	
   we	
   had	
   gold	
   
       we	
   have	
   to	
   think	
   of	
   ways	
   to	
   create	
   (x,y)	
   pairs	
   and	
   

standard	
   labels	
   for	
   y	
   

de   ne	
   the	
   spaces	
   of	
   inputs	
   and	
   outputs	
   

52	
   

modeling,	
   id136,	
   learning	
   for	
   word	
   vectors	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

modeling:	
   de   ne	
   	
   score	
   funcron	
   

learning:	
   choose	
   _	
   	
   	
   

       skip-     gram	
   (mikolov	
   et	
   al.,	
   2013):	
   	
   

       x	
   =	
   a	
   word	
   
       y	
   =	
   a	
   word	
   in	
   an	
   n-     word	
   window	
   of	
   x	
   in	
   a	
   corpus	
   

       con@nuous	
   bag-     of-     words	
   (cbow;	
   mikolov	
   et	
   al.,	
   2013):	
   

       x	
   =	
   a	
   sequence	
   of	
   n	
   words	
   with	
   the	
   center	
   word	
   omi   ed	
   
       y	
   =	
   the	
   center	
   word	
   

53	
   

modeling,	
   id136,	
   learning	
   for	
   word	
   vectors	
   

id136:	
   solve	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   _	
   	
   

modeling:	
   de   ne	
   	
   score	
   funcron	
   

this	
   becomes	
   much	
   more	
   expensive!	
   

(loops	
   over	
   all	
   word	
   types)	
   

learning:	
   choose	
   _	
   	
   	
   

       skip-     gram	
   (mikolov	
   et	
   al.,	
   2013):	
   	
   

       x	
   =	
   a	
   word	
   
       y	
   =	
   a	
   word	
   in	
   an	
   n-     word	
   window	
   of	
   x	
   in	
   a	
   corpus	
   

       con@nuous	
   bag-     of-     words	
   (cbow;	
   mikolov	
   et	
   al.,	
   2013):	
   

       x	
   =	
   a	
   sequence	
   of	
   n	
   words	
   with	
   the	
   center	
   word	
   omi   ed	
   
       y	
   =	
   the	
   center	
   word	
   

54	
   

skip-     gram	
   training	
   data	
   (window	
   size	
   =	
   5)	
   

corpus	
   (english	
   wikipedia):	
   
agriculture	
   is	
   the	
   tradioonal	
   mainstay	
   of	
   the	
   cambodian	
   economy	
   .	
   
but	
   benares	
   has	
   been	
   destroyed	
   by	
   an	
   earthquake	
   .	
   
   	
   

inputs	
   (x)	
   
agriculture	
   
agriculture	
   
agriculture	
   

is	
   
is	
   
is	
   
is	
   
the	
   
   	
   

outputs	
   (y)	
   

<s>	
   
is	
   
the	
   
<s>	
   

agriculture	
   

the	
   

tradironal	
   

is	
   
   	
   

55	
   

cbow	
   training	
   data	
   (window	
   size	
   =	
   5)	
   

corpus	
   (english	
   wikipedia):	
   
agriculture	
   is	
   the	
   tradioonal	
   mainstay	
   of	
   the	
   cambodian	
   economy	
   .	
   
but	
   benares	
   has	
   been	
   destroyed	
   by	
   an	
   earthquake	
   .	
   
   	
   

inputs	
   (x)	
   

{<s>,	
   is,	
   the,	
   tradironal}	
   

{<s>,	
   agriculture,	
   the,	
   tradironal}	
   

{agriculture,	
   is,	
   tradironal,	
   mainstay}	
   

{is,	
   the,	
   mainstay,	
   of}	
   

{the,	
   tradironal,	
   of,	
   the}	
   

{tradironal,	
   mainstay,	
   the,	
   cambodian}	
   
{mainstay,	
   of,	
   cambodian,	
   economy}	
   

   	
   

outputs	
   (y)	
   
agriculture	
   

is	
   
the	
   

tradironal	
   
mainstay	
   

of	
   
the	
   
   	
   

56	
   

skip-     gram	
   model	
   

       here   s	
   our	
   data:	
   

outputs	
   (y)	
   

inputs	
   (x)	
   
agriculture	
   
agriculture	
   
agriculture	
   

is	
   
   	
   

<s>	
   
is	
   
the	
   
<s>	
   
   	
   

       how	
   should	
   we	
   de   ne	
   the	
   score	
   funcron?	
   

57	
   

skip-     gram	
   score	
   funcron	
   

subvector	
   of	
   	
   	
   	
   	
   	
   
corresponding	
   to	
   	
   
word	
   x	
   as	
   an	
   input	
   

subvector	
   of	
   	
   	
   	
   	
   	
   
corresponding	
   to	
   	
   
word	
   y	
   as	
   an	
   output	
   

       dot	
   product	
   of	
   two	
   vectors,	
   one	
   for	
   each	
   word	
   
       subtlety:	
   di   erent	
   vector	
   spaces	
   for	
   input	
   and	
   output	
   
       no	
   interpretaron	
   to	
   vector	
   dimensions	
   (a	
   priori)	
   

58	
   

skip-     gram	
   parameterizaron	
   

0.4	
   
-     0.1	
   
0.2	
   
0.3	
   
-     0.6	
   
-     1.3	
   
0.9	
   
   	
   

input	
   vector	
   for	
   dog	
   
	
   
input	
   vector	
   for	
   cat	
   
   	
   
input	
   vectors	
   
	
   
	
   
output	
   vector	
   for	
   dog	
   
   	
   
output	
   vectors	
   

59	
   

what	
   will	
   the	
   skip-     gram	
   model	
   learn?	
   

       corpus:	
   

an	
   earthquake	
   destroyed	
   the	
   city	
   
the	
   town	
   was	
   destroyed	
   by	
   a	
   tornado	
   

       sample	
   of	
   training	
   pairs:	
   
inputs	
   (x)	
   
destroyed	
   
earthquake	
   
destroyed	
   
tornado	
   

   	
   

outputs	
   (y)	
   
earthquake	
   
destroyed	
   
tornado	
   
destroyed	
   

   	
   

       output	
   vector	
   for	
   destroyed	
   encouraged	
   to	
   be	
   similar	
   

to	
   input	
   vectors	
   of	
   earthquake	
   and	
   tornado	
   

60	
   

learning	
   

learning:	
   choose	
   _	
   	
   	
   

       you	
   could	
   use	
   any	
   loss	
   funcron	
   we	
   have	
   talked	
   about	
   
       mikolov	
   et	
   al.	
   (2013)	
   use	
   log	
   loss,	
   which	
   is	
   a	
   new	
   loss	
   

funcron	
   for	
   us	
   

61	
   

empirical	
   risk	
   minimizaron	
   with	
   surrogate	
   loss	
   funcrons	
   
       given	
   training	
   data:	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
	
   	
   	
   	
   where	
   each	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   a	
   label	
   
       we	
   want	
   to	
   solve	
   the	
   following:	
   

many	
   possible	
   loss	
   
funcrons	
   to	
   consider	
   

oprmizing	
   

62	
   

log	
   loss	
   

       minimize	
   negarve	
   log	
   of	
   condironal	
   

id203	
   of	
   output	
   given	
   input	
   
      somermes	
   called	
      maximizing	
   condironal	
   
likelihood   	
   

       but	
   wait,	
   we	
   don   t	
   have	
   a	
   probabilisrc	
   model,	
   

we	
   just	
   have	
   a	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   funcron	
   

63	
   

score	
        	
   id203	
   

       we	
   can	
   turn	
   our	
   score	
   into	
   a	
   id203	
   by	
   

exponenrarng	
   (to	
   make	
   it	
   posirve)	
   and	
   
normalizing:	
   

       this	
   is	
   ouen	
   called	
   a	
      soumax   	
   funcron	
   

64	
   

log	
   loss	
   

       similar	
   to	
   id88	
   loss!	
   
log	
   loss	
   is	
   used	
   in:	
   
       just	
   replace	
   max	
   with	
   soumax	
   

logisrc	
   regression	
   classi   ers,	
   
condironal	
   random	
      elds,	
   

maximum	
   id178	
   (   maxent   )	
   models	
   

65	
   

log	
   loss	
   

       problem:	
   very	
   expensive	
   due	
   to	
   iterarng	
   over	
   

approximarons	
   are	
   commonly	
   

used	
   in	
   pracrce:	
   
all	
   possible	
   outputs	
   (all	
   words!)	
   
hierarchical	
   soumax,	
   
negarve	
   sampling	
   

66	
   

id97	
   

gram	
   and	
   cbow	
   models	
   

       id97	
   toolkit	
   implements	
   training	
   for	
   skip-     
       very	
   fast	
   to	
   train,	
   even	
   on	
   large	
   corpora	
   
       pretrained	
   embeddings	
   available	
   

67	
   

embeddings	
   capture	
   relaronal	
   meaning!	
   

vector(king)	
      	
   vector(man)	
   +	
   vector(woman)	
      	
   vector(queen)	
   
vector(paris)	
      	
   vector(france)	
   +	
   vector(italy)	
      	
   vector(rome)	
   

j&m/slp3	
   

68	
   

