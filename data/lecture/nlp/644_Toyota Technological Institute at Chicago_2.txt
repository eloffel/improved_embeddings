ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	2:	text	classification

1

    please	email	me	(kgimpel@ttic.edu)	with	the	

following:
    your	name
    your	email	address
    whether	you	taking	the	class	for	credit

    i	will	use	your	address	to	create	a	mailing	list	

for	course	announcements

2

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

3

text	classification

spam	/	not	spam

   
    priority	level
   

category	(primary	/	social	/	promotions	/	updates)

4

sentiment	analysis

5

classification

    datasets
    features
    learning

6

nlp	datasets

    nlp	datasets	include	inputs	(usually	text)	and	

outputs	(usually	some	sort	of	annotation)

7

annotation

    supervised	machine	learning	needs	labeled	

datasets,	where	labels	are	called	ground	truth

    in	nlp,	labels	are	annotations	provided	by	

humans

    there	is	always	some	disagreement	among	

annotators,	even	for	simple	tasks

    these	annotations	are	called	a	gold	standard,	

not	ground	truth

8

how	are	nlp	datasets	developed?

1. paid,	trained	human	annotation

    this	is	the	traditional	approach
    researchers	write	annotation	guidelines,	recruit	&	

pay	annotators	(often	linguists)

    more	consistent	annotations,	but	costly	to	scale
    e.g.,	penn	treebank	(1993)

    1	million	words,	mostly	wall	street	journal,	annotated	

with	part-of-speech	tags	and	syntactic	parse	trees

9

example:	twitter	part-of-speech	annotation
17	cmu	researchers	annotated	~2000	tweets

gimpel,	schneider,	o'connor,	das,	mills,	eisenstein,	heilman,	
yogatama,	flanigan,	smith.	   part-of-speech	tagging	for	twitter:	
annotation,	features,	and	experiments,   	acl	2011.

1. paid,	trained	human	annotation
2. id104

    more	recent	trend
    amazon	mechanical	turk
    can   t	really	train	annotators,	but	easier	to	get	multiple	
annotations	for	each	input	(which	can	then	be	averaged)

    e.g.,	stanford	sentiment	treebank:

11

2. id104
3. naturally-occurring	annotation

    long	history:	used	by	ibm	for	speech	recognition	

and	statistical	machine	translation

    how	might	you	find	naturally-occurring	data	for:

credit:	brown	&	mercer,	20	years	 of	
bitext workshop,	2013

    conversational	agents
    summarization
    coreference resolution

12

2. id104
3. naturally-occurring	annotation

    long	history:	used	by	ibm	for	speech	recognition	

and	statistical	machine	translation

    how	might	you	find	naturally-occurring	data	for:

credit:	brown	&	mercer,	20	years	 of	
bitext workshop,	2013

    conversational	agents
    summarization
    coreference resolution

13

annotator	agreement

    given	annotations	from	two	annotators,	how	

should	we	measure	inter-annotator	
agreement?
    percent	agreement
    cohen   s	kappa	(cohen,	1960)	accounts	for	

agreement	by	chance

    generalizations	exist	for	more	than	two	annotators	

(fleiss,	1971)

14

annotator	agreement

    given	annotations	from	two	annotators,	how	

should	we	measure	inter-annotator	
agreement?
    percent	agreement?
    cohen   s	kappa	(cohen,	1960)	accounts	for	

agreement	by	chance

    generalizations	exist	for	more	than	two	annotators	

(fleiss,	1971)

15

annotator	agreement

    given	annotations	from	two	annotators,	how	

should	we	measure	inter-annotator	
agreement?
    percent	agreement?
    cohen   s	kappa	(cohen,	1960)	accounts	for	

agreement	by	chance

    generalizations	exist	for	more	than	two	annotators	

(fleiss,	1971)

16

text	classification	data
    there	are	many	annotated	datasets

    stanford	sentiment	treebank:	fine-grained	

sentiment	analysis	of	movie	reviews

    subjectivity/objectivity	sentence	classification
    binary	sentiment	analysis	of	customer	reviews
    trec	question	classification

17

    subjectivity/objectivity	classification:

the	hulk	is	an	anger	fueled	monster	with	incredible	strength	and	resistance	to	
damage	.

in	trying	to	be	daring	and	original	,	it	comes	off	as	only	occasionally	satirical	
and	never	fresh	.

solondz may	well	be	the	only	one	laughing	 at	his	own	joke

obstacles	pop	up	left	and	right	,	as	the	adventure	gets	wilder	and	wilder	.

18

    subjectivity/objectivity	classification:

the	hulk	is	an	anger	fueled	monster	with	incredible	strength	and	resistance	to	
damage	.

objective

in	trying	to	be	daring	and	original	,	it	comes	off	as	only	occasionally	satirical	
and	never	fresh	.

subjective

solondz may	well	be	the	only	one	laughing	 at	his	own	joke

obstacles	pop	up	left	and	right	,	as	the	adventure	gets	wilder	and	wilder	.

19

    subjectivity/objectivity	classification:

the	hulk	is	an	anger	fueled	monster	with	incredible	strength	and	resistance	to	
damage	.

objective

in	trying	to	be	daring	and	original	,	it	comes	off	as	only	occasionally	satirical	
and	never	fresh	.

subjective

solondz may	well	be	the	only	one	laughing	 at	his	own	joke

subjective

obstacles	pop	up	left	and	right	,	as	the	adventure	gets	wilder	and	wilder	.

objective

20

    subjectivity/objectivity	classification:

the	hulk	is	an	anger	fueled	monster	with	incredible	strength	and	resistance	to	
damage	.

objective

in	trying	to	be	daring	and	original	,	it	comes	off	as	only	occasionally	satirical	
and	never	fresh	.

subjective

solondz may	well	be	the	only	one	laughing	 at	his	own	joke

obstacles	pop	up	left	and	right	,	as	the	adventure	gets	wilder	and	wilder	.
    how	was	this	dataset	generated?

    imdb	plot	summaries:	objective
    rotten	tomatoes	snippets:	subjective

subjective

objective

21

    subjectivity/objectivity	classification:

the	hulk	is	an	anger	fueled	monster	with	incredible	strength	and	resistance	to	
damage	.

objective

in	trying	to	be	daring	and	original	,	it	comes	off	as	only	occasionally	satirical	
and	never	fresh	.

subjective

solondz may	well	be	the	only	one	laughing	 at	his	own	joke

obstacles	pop	up	left	and	right	,	as	the	adventure	gets	wilder	and	wilder	.
    how	was	this	dataset	generated?

    imdb	plot	summaries:	objective
    rotten	tomatoes	snippets:	subjective

subjective

objective

22

    subjectivity/objectivity	classification:

the	hulk	is	an	anger	fueled	monster	with	incredible	strength	and	resistance	to	
damage	.

objective

in	trying	to	be	daring	and	original	,	it	comes	off	as	only	occasionally	satirical	
and	never	fresh	.

subjective

solondz may	well	be	the	only	one	laughing	 at	his	own	joke

subjective

obstacles	pop	up	left	and	right	,	as	the	adventure	gets	wilder	and	wilder	.

objective

    how	might	you	generate	a	dataset	like	this?

23

    customer	review	sentiment	classification:

it	works	with	a	minimum	 of	fuss	.

size	- bigger	than	the	ipod

i 've	had	this	thing	just	over	a	month	and	the	headphone	 jack	has	already	
come	loose	.

you	can	manage	your	profile	,	change	the	contrast	of	backlight	,	make	
different	type	of	display	,	either	list	or	tabbed	.

i replaced	it	with	a	router	raizer and	it	works	much	better	.

24

    customer	review	sentiment	classification:

it	works	with	a	minimum	 of	fuss	.

size	- bigger	than	the	ipod

i 've	had	this	thing	just	over	a	month	and	the	headphone	 jack	has	already	
come	loose	.

you	can	manage	your	profile	,	change	the	contrast	of	backlight	,	make	
different	type	of	display	,	either	list	or	tabbed	.

i replaced	it	with	a	router	raizer and	it	works	much	better	.

positive

negative

negative

positive

negative

25

    question	classification:

who	invented	baseball	?

id98	is	an	acronym	for	what	?

which	latin	american	country	is	the	largest	?

how	many	small	businesses	are	there	in	the	u.s	.

human

abbreviation

location

number

what	would	you	add	to	the	clay	mixture	to	produce	bone	china	?

entity

what	is	the	root	of	all	evil	?

description

26

classification

    datasets
    features
    learning

27

classification	framework

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

28

classification	framework

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

our	linear	model	text	classifier:

29

features	for	nlp

    nlp	datasets	include	inputs	and	outputs
    features	are	usually	not	included
    you	have	to	define	your	own	features
    contrast	this	with	uci	datasets,	which	include	a	fixed-

length	   dense   	feature	vector	for	every	instance

30

features	for	nlp

    nlp	datasets	include	inputs	and	outputs
    features	are	usually	not	included
    you	have	to	define	your	own	features
    contrast	this	with	uci	datasets,	which	include	a	fixed-

length	dense feature	vector	for	every	instance

31

features	for	nlp

    nlp	datasets	include	inputs	and	outputs
    features	are	usually	not	included
    you	have	to	define	your	own	features
    contrast	this	with	uci	datasets,	which	include	a	fixed-

length	dense feature	vector	for	every	instance

    in	nlp,	features	are	usually	sparse

32

unigram	binary	features

    two	example	features:

where

    we	usually	think	in	terms	of	feature	templates
    unigram	binary	feature	template:

    to	create	features,	this	feature	template	is	
instantiated	for	particular	labels	and	words

33

unigram	binary	features

    two	example	features:

where

    we	usually	think	in	terms	of	feature	templates
    unigram	binary	feature	template:

    to	create	features,	this	feature	template	is	
instantiated	for	particular	labels	and	words

34

unigram	binary	features

    two	example	features:

where

    we	usually	think	in	terms	of	feature	templates
    unigram	binary	feature	template:

    to	create	features,	this	feature	template	is	
instantiated	for	particular	labels	and	words

35

higher-order	binary	feature	templates

unigram	binary	template:

bigram	binary	template:

trigram	binary	features

   

36

unigram	count	features

    a	``count      	feature	returns	the	count	of	a	

particular	word	in	the	text

    unigram	count	feature	template:

37

feature	count	cutoffs

    problem:	some	features	are	extremely	rare
    solution:	only	keep	features	that	appear	at	

least	k times	in	the	training	data

38

feature	count	cutoffs	(example)

    consider	the	following	training	dataset:

a	great	movie	!
not	such	a	great	movie

positive
negative

    with	the	following	single	feature	template:

    which	features	would	remain	in	the	model	

with	a	feature	count	cutoff	of	2?

39

feature	count	cutoffs	(example)

    consider	the	following	training	dataset:

a	great	movie	!
not	such	a	great	movie

positive
negative

    with	the	following	single	feature	template:

    which	features	would	remain	in	the	model	

with	a	feature	count	cutoff	of	2?
    none

40

feature	count	cutoffs	(example)

    consider	the	following	training	dataset:

a	great	movie	!
not	such	a	great	movie

positive
negative

    with	the	following	single	feature	template:

    which	features	would	remain	in	the	model	

with	a	feature	count	cutoff	of	1?

41

feature	count	cutoffs	(example)

    consider	the	following	training	dataset:

a	great	movie	!
not	such	a	great	movie

positive
negative

    with	the	following	single	feature	template:

    which	features	would	remain	in	the	model	

with	a	feature	count	cutoff	of	0?

42

classification

    datasets
    features
    learning

    empirical	risk	minimization
    surrogate	loss	functions
    gradient-based	optimization

43

learning:	empirical	risk	minimization
    in	a	machine	learning	course,	you	learn	about	

many	different	learning	frameworks

44

learning:	empirical	risk	minimization
    in	a	machine	learning	course,	you	learn	about	

many	different	learning	frameworks

    since	we	have	limited	time,	we	will	be	greedy	

and	focus	on	a	single	framework	that	
maximizes

(for	some	positive	constants															)
we	will	start	it	today	but	continue	to	add	to	it	later

45

cost	functions

    cost	function:	scores	outputs	against	a	gold	standard

    should	be	as	close	as	possible	to	the	actual	

evaluation	metric	for	your	task

    usual	conventions:
    for	classification,	what	cost	should	we	use?

    how	about	for	other	nlp	tasks?

46

cost	functions

    cost	function:	scores	outputs	against	a	gold	standard

    should	be	as	close	as	possible	to	the	actual	

evaluation	metric	for	your	task

    for	classification,	what	cost	should	we	use?

    how	about	for	other	nlp	tasks?

47

cost	functions

    cost	function:	scores	outputs	against	a	gold	standard

    should	be	as	close	as	possible	to	the	actual	

evaluation	metric	for	your	task

    for	classification,	what	cost	should	we	use?

    how	about	for	other	nlp	tasks?

48

risk	minimization

    given	training	data:																																

where	each

is	a	label

    assume	data	is	drawn	iid (independently	and	identically	

distributed)	from	(unknown)	joint	distribution	

    we	want	to	solve	the	following:

49

risk	minimization

    given	training	data:																																

where	each

is	a	label

    assume	data	is	drawn	iid (independently	and	identically	

distributed)	from	(unknown)	joint	distribution	

    we	want	to	solve	the	following:

problem:	p is	unknown

50

empirical risk	minimization

(vapnik et	al.)

    replace	expectation	with	sum	over	examples:

51

empirical risk	minimization

(vapnik et	al.)

    replace	expectation	with	sum	over	examples:

problem:	np-hard	even	for	binary	
classification	with	linear	models

52

solution:	replace	   cost	loss   	(also	
called	   0-1   	loss)	with	a	surrogate
function	that	is	easier	to	optimize

generalize	to	permit	any	loss	function

53

solution:	replace	   cost	loss   	(also	
called	   0-1   	loss)	with	a	surrogate
function	that	is	easier	to	optimize

generalize	to	permit	any	loss	function

cost	loss	/	0-1	loss:

54

classification

    datasets
    features
    learning

    empirical	risk	minimization
    surrogate	loss	functions
    gradient-based	optimization

55

surrogate	loss	functions

cost	loss	/	0-1	loss:

why	is	this	so	difficult	to	optimize?

56

surrogate	loss	functions

cost	loss	/	0-1	loss:

why	is	this	so	difficult	to	optimize?
not	necessarily	continuous,	can   t	use	

gradient-based	optimization

57

surrogate	loss	functions

cost	loss	/	0-1	loss:

max-score	loss:

58

surrogate	loss	functions

cost	loss	/	0-1	loss:

max-score	loss:

this	is	continuous,	but	what	are	its	drawbacks?

59

surrogate	loss	functions

cost	loss	/	0-1	loss:

max-score	loss:

id88	loss:

60

surrogate	loss	functions

cost	loss	/	0-1	loss:

max-score	loss:

id88	loss:

loss	function	underlying	id88	algorithm	

(rosenblatt,	1957-58)

61

surrogate	loss	functions

cost	loss	/	0-1	loss:

id88	loss:

hinge	loss:

62

surrogate	loss	functions

cost	loss	/	0-1	loss:

id88	loss:

hinge	loss:

loss	function	underlying	support	vector	machines

63

surrogate	loss	functions

cost	loss	/	0-1	loss:

id88	loss:

hinge	loss:

hinge	loss for	our	classification	setting:

tunable	hyperparameter

64

classification

    datasets
    features
    learning

    empirical	risk	minimization
    surrogate	loss	functions
    gradient-based	optimization

65

gradient	descent

    minimizes	a	function	f by	taking	steps	in	proportion	

to	the	negative	of	the	gradient:

66

gradient	descent

    minimizes	a	function	f	by	taking	steps	in	proportion	

to	the	negative	of	the	gradient:

:	stepsize at	iteration	t

:	gradient	of	

objective	function

    with	conditions	on	stepsize and	objective	function,	

will	converge	to	local	minimum

67

gradient	descent

    minimizes	a	function	f	by	taking	steps	in	proportion	

to	the	negative	of	the	gradient:

:	stepsize at	iteration	t

:	gradient	of	

objective	function

to	speed	convergence,	
can	use	line	search	to	
choose	better	stepsizes;	
also	see	l-bfgs

    with	conditions	on	stepsize and	objective	function,	

will	converge	to	local	minimum

68

gradient	descent

    minimizes	a	function	f	by	taking	steps	in	proportion	

to	the	negative	of	the	gradient:

:	stepsize at	iteration	t

efficiency	concern:	f is	a	

:	gradient	of	

sum	over	all	training	

examples!	

to	speed	convergence,	
can	use	line	search	to	
choose	better	stepsizes;	
also	see	l-bfgs

objective	function

    with	conditions	on	stepsize and	objective	function,	

will	converge	to	local	minimum

69

gradient	descent

    minimizes	a	function	f	by	taking	steps	in	proportion	

to	the	negative	of	the	gradient:

:	stepsize at	iteration	t

efficiency	concern:	f is	a	

:	gradient	of	

objective	function

examples!	

sum	over	all	training	

to	speed	convergence,	
can	use	line	search	to	
choose	better	stepsizes;	
also	see	l-bfgs
every	parameter	update	
requires	iterating	through	

    with	conditions	on	stepsize and	objective	function,	

will	converge	to	local	minimum

entire	training	set

70

gradient	descent

    minimizes	a	function	f by	taking	steps	in	proportion	

to	the	negative	of	the	gradient:

:	stepsize at	iteration	t

efficiency	concern:	f is	a	

:	gradient	of	

objective	function

examples!	

sum	over	all	training	

to	speed	convergence,	
can	use	line	search	to	
choose	better	stepsizes;	
also	see	l-bfgs
every	parameter	update	
requires	iterating	through	

    with	conditions	on	stepsize and	objective	function,	

will	converge	to	local	minimum

entire	training	set
   batch   	algorithm

71

stochastic gradient	descent

    applicable	when	objective	function	is	a	sum
    like	gradient	descent,	except	calculates	gradient	on	a	
single	example	at	a	time	(   online   )	or	on	a	small	set	
of	examples	(   mini-batch   )

72

stochastic gradient	descent

    applicable	when	objective	function	is	a	sum
    like	gradient	descent,	except	calculates	gradient	on	a	
single	example	at	a	time	(   online   )	or	on	a	small	set	
of	examples	(   mini-batch   )

    converges	much	faster	than	(batch)	gradient	descent
    with	conditions	on	stepsize and	objective	function,	

will	converge	to	local	minimum

    there	are	many	popular	variants:	

sgd+momentum, adagrad,	adadelta,	adam,	rmsprop,	etc.

73

what	if	f is	not	differentiable?

    some	loss	functions	are	not	differentiable:

    but	they	are subdifferentiable,	so	we	can	compute	

subgradients and	use	(stochastic)	subgradient
descent

74

subderivatives

    subderivative: generalization	of	derivative	for	

nondifferentiable,	convex	functions

    there	may	be	multiple	

subderivatives at	a	point	
(red	lines)

    this	set	is	called	the	subdifferential
    a	convex	function	g is	differentiable	at	point	x0 if	and	
only	if	the	subdifferential of	g at	x0	contains	only	the	
derivative	of	g	at	x0

75

stochastic	subid119
    just	like	stochastic	gradient	descent,	except	

replace	gradients	with	subgradients

    similarly	strong	theoretical	guarantees

76

calculating	subgradients

    at	points	of	differentiability,	just	use	your	rules	

for	calculating	gradients

    at	points	of	nondifferentiability,	just	find	a	
single	subgradient;	any	subgradient will	do

    e.g.,	max	of	convex	functions	(on	board)

77

    please	email	me	(kgimpel@ttic.edu)	with	the	

following:
    your	name
    your	email	address
    whether	you	taking	the	class	for	credit

    i	will	use	your	address	to	create	a	mailing	list	

for	course	announcements

78

