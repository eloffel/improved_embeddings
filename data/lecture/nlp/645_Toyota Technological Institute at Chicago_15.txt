ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	15:	
introduction

to	machine	translation

announcements

    assignment	3	due	monday
    email	me	to	sign	up	for	your	(10-minute)	class	

presentation	on	3/3	or	3/8

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    computational	semantics
    machine	translation
    other	nlp	applications

people	rely	on	machine	translation!

people	rely	on	machine	translation!

approaches	to	machine	translation:

the	vauquois triangle

interlingua	example

classification	framework	for	machine	translation

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

    modern	systems	are	data-driven
    first	we	need	data!

data?

data?

data?

data?

also:	
    news	articles
    company	websites
    laws	&	patents
    subtitles

parallel	data

    parallel	data:	bilingual	data	that	is	naturally	

aligned	at	some	level

    usually	aligned	at	the	document	level
    sentence-level	alignments	are	generated	

automatically
    how	might	you	design	an	algorithm	for	this?
    it	can	be	done	well	without	dictionaries!
    can	throw	out	sentences	that	don   t	align	with	

anything

learning	from	parallel	sentences

chickasaw

english

1. ofi	'at	kowi	'  	lhiyohli	
2. kowi	'at	ofi	'  	lhiyohli	
3. ofi	'at	shoha

1. the	dog	chases	the	cat
2. the	cat	chases	the	dog
3. the	dog	stinks

learning	from	parallel	sentences

chickasaw

english

1. ofi 'at	kowi	'  	lhiyohli
2. kowi	'at	ofi '  	lhiyohli
3. ofi 'at	shoha

1. the	dog chases the	cat
2. the	cat chases the	dog
3. the	dog stinks

machine	translation	evaluation
    human	judgments	are	ideal,	but	expensive
    what	other	problems	are	there	with	human	judgments?

    we	need	automatic	evaluation	metrics

    id7	(bilingual evaluation	understudy),	papineni et	al.	(2002)
    compare	id165	overlap	between	system	output	and	human-produced	

translation

    correlates	with	human	judgments	surprisingly	well,	but	only	at	the	

document	level	(not	sentence	level!)

    other	metrics	do	soft	matching	based	on	id30	and	

synonyms	from	id138

    this	is	not	a	solved	problem!

statistical machine	translation

one	naturally	wonders	if	the	problem	of	
translation	could	conceivably	be	treated	as	
a	problem	in	cryptography.	

when	i	look	at	an	article	in	arabic,	i	say:	
   this	is	really	written	in	english,	but	it	has	
been	coded	in	some	strange	symbols.	i	will	
now	proceed	to	decode.   

warren	weaver,	1947

noisy	channel	model

noisy	channel	model	for	translating	

french	(	f	)	to	english	(e)

e

efp
(
)|

f

)(ep

  
e

=

=

=

arg

max
e
arg
max
e
max
e

arg

fep
)
|(
epefp
)()|
(
fp
(
)|

epefp
(
)(

)

modeling	for	the	noisy	channel

    we	need	to	model	two	id203	distributions:	p(e)	

and	p(f	| e)
    p(e)	should	favor	fluent	translations
    p(f	| e)	should	favor	accurate/faithful	translations

modeling	for	the	noisy	channel

    we	need	to	model	two	id203	distributions:	p(e)	

and	p(f	| e)
    p(e)	should	favor	fluent	translations
    p(f	| e)	should	favor	accurate/faithful	translations

    let   s	start	with	p(e)

    how	do	we	compute	the	id203	of	an	english	

sentence?

    this	is	an	important	part	of	mt	(e.g.,	google)

word	alignments

word	alignments

is	a	   hidden   	variable	(not	part	of	training	data)

   
    for	each	french	word,	it	holds	the	index	of	the	aligned	

english	word	(or	null)

    remember:	our	goal	was	to	model	
    why	would	we	introduce	a	hidden	variable?

    to	make	it	   easier   	to	define	the	model
    we	often	want	to	share	certain	types	of	

information	across	multiple	instances	in	our	data
    latent	variables	are	a	natural	way	to	capture	this
    think	of	id91	(some	of	the	points	come	from	the	

same	cluster)

alignments	as	hidden	variables

    for	simplicity,	assume	that	each	french	word	aligns	

to	1	english	word	(or	to	null)

    analogy	to	id91:

    each	data	point	has	1	vote	which	it	can	distribute	among	

all	the	clusters

    here,	each	french	word	has	1	vote	which	it	can	distribute	

among	all	the	english	words	or	null

modeling	alignments:	ibm	model	1

modeling	alignments:	ibm	model	1

    how	do	we	obtain	

?

modeling	alignments:	ibm	model	1

    how	do	we	obtain	
    sum	over	all	alignments:

?

modeling	alignments:	ibm	model	1

parameters in the model,
learned using expectation maximization

aside:	are	alignments	always	hidden?
    certain	small	parallel	corpora	have	been	hand-aligned
   

issues	with	this?
    annotators	don   t	agree
    we	have	lots	of	parallel	text,	very	little	is	hand-aligned
    for	some	language	pairs,	we	will	never	have	manual	

alignments

    word	alignment	has	become	a	fundamental	part	of	mt,	and	

we	need	unsupervised	learning	to	solve	it!

ibm	model	1	example

    consider	a	training	set	of	two	sentence	pairs:

green

house

the

house

casa

verde

la

casa

initial parameter estimates:

after 1 iteration of em:

=	id203	of	translating	e into	f

ibm	model	1

ibm	model	2

ibm	model	3

moving	to	phrases

null

auf

diese

frage

habe

ich

leider

keine

antwort

bekommen

i

did

not

unfortunately

receive

an

answer

to

this

question

moving	to	phrases

not	necessarily	syntactic	phrases

auf

diese

frage

habe

ich

leider

keine

antwort

bekommen

i

did

not

unfortunately

receive

an

answer

to

this

question

   phrase-based   	translation

    relies	on	a	phrase	table

    massive	bilingual	phrase	dictionary,	with	probabilities

    to	build:

    find	the	best	word	alignment	for	each	sentence	pair
    extract	all	phrase	pairs	consistent with	the	word	alignment
    compute	probabilities	using	relative	frequency	estimation

phrase-based	translation

    relies	on	a	phrase	table

    massive	bilingual	phrase	dictionary,	with	probabilities

    to	build:

    find	the	best	word	alignment	for	each	sentence	pair
    extract	all	phrase	pairs	consistent with	the	word	alignment
    compute	probabilities	using	relative	frequency	estimation

auf

diese

frage

habe

ich

leider

keine

antwort

bekommen

i

did

not

unfortunately

receive

an

answer

to

this

question

phrase-based	translation

    relies	on	a	phrase	table

    massive	bilingual	phrase	dictionary,	with	probabilities

    to	build:

    find	the	best	word	alignment	for	each	sentence	pair
    extract	all	phrase	pairs	consistent with	the	word	alignment
    compute	probabilities	using	relative	frequency	estimation
to this question

auf diese frage

1.0

auf

diese

frage

habe

ich

leider

keine

antwort

bekommen

i

did

not

unfortunately

receive

an

answer

to

this

question

phrase-based	translation

    relies	on	a	phrase	table

    massive	bilingual	phrase	dictionary,	with	probabilities

    to	build:

    find	the	best	word	alignment	for	each	sentence	pair
    extract	all	phrase	pairs	consistent with	the	word	alignment
    compute	probabilities	using	relative	frequency	estimation
to this question
an answer
answer
   

auf diese frage
antwort
antwort

1.0
1.0
1.0

auf

diese

frage

habe

ich

leider

keine

antwort

bekommen

i

did

not

unfortunately

receive

an

answer

to

this

question

phrase-based	translation

    relies	on	a	phrase	table

    massive	bilingual	phrase	dictionary,	with	probabilities

    to	build:

    find	the	best	word	alignment	for	each	sentence	pair
    extract	all	phrase	pairs	consistent with	the	word	alignment
    compute	probabilities	using	relative	frequency	estimation:

german
auf diese frage
antwort
antwort
   

english
to this question
an answer
answer

count
1.0
1.0
1.0

german
auf diese frage
antwort
antwort
   

english
to this question
an answer
answer

p(e | f )

1.0
0.5
0.5

adding	syntax:	synchronous	context-free	grammars

id18

sid18

nn

sid18

id18

nn

noisy	channel

noisy	channel

predicted
translation

source
sentence

noisy	channel

assumes	we	have	the	right	model,	and	that	we	estimate	it	perfectly

noisy	channel

assumes	we	have	the	right	model,	and	that	we	estimate	it	perfectly

noisy	channel

assumes	we	have	the	right	model,	and	that	we	estimate	it	perfectly

extra	parameters	to	tune,	can	tune	to	optimize	id7

noisy	channel

assumes	we	have	the	right	model,	and	that	we	estimate	it	perfectly

extra	parameters	to	tune,	can	tune	to	optimize	id7

   tuning   

noisy	channel	   linear	model?

since	we   re	not	using	idealized	decoding	rule	anymore,

why	not	add	more	feature	functions?

   word	count	feature   :

noisy	channel	   linear	model?

since	we   re	not	using	idealized	decoding	rule	anymore,

why	not	add	more	feature	functions?

   word	count	feature   :

noisy	channel	   linear	model?

since	we   re	not	using	idealized	decoding	rule	anymore,

why	not	add	more	feature	functions?

   word	count	feature   :

   reverse	translation	model	feature   :

african
national
congress
                                    

opposition sanction

zimbabwe

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

african
national
congress
                                    

opposition sanction

zimbabwe

id7
score

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

predicted	translation

opposition	 to	sanctions	against	

zimbabwe	african	national	congress

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

african	national	congress	opposition	

sanctions	against	zimbabwe

predicted	translation

opposition	 to	sanctions	against	

zimbabwe	african	national	congress

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

african	national	congress	opposition	

sanctions	against	zimbabwe

african	sanctioning	to
zimbabwe   s	opposing

predicted	translation

opposition	 to	sanctions	against	

zimbabwe	african	national	congress

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

learning	moves	
translations	in	

this	plot

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

learning	moves	
translations	left	or	
right	in	this	plot

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

   ideal   	model

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

where   s	the	gold	

standard	translation?

model score

african
national
congress
                                    

opposition sanction

zimbabwe

gold	standard:

african	national	congress	opposes	

sanctions	against	zimbabwe

id7
score

issue:

gold	standard	translation	is	often

unreachable by	the	model

why?

limited	translation	rules,

free	translations,	

noisy	data
model score

free	translations

machine	translation:
sharon's	office	said,	leader	of	the	main	opposition	labor	
party	has	admitted	defeat	and	congratulatory	telephone	calls	
to	sharon.

human-generated	translation:
according	to	a	representative	of	sharon's	office,	the	leader	of	
the	main	opposition	labor	party	has	admitted	defeat	and	
made	the	obligatorycongratulating	telephone	call	to	sharon.

free	translations

even	if	gold	standard	translation	was	
reachable	by	model,	we	might	not	
machine	translation:
sharon's	office	said,	leader	of	the	main	opposition	labor	
party	has	admitted	defeat	and	congratulatory	telephone	calls	
to	sharon.

want	to	learn	from	it	directly

applicable	to	other	tasks:

human-generated	translation:
according	to	a	representative	of	sharon's	office,	the	leader	of	
the	main	opposition	labor	party	has	admitted	defeat	and	
made	the	obligatorycongratulating	telephone	call	to	sharon.

summarization

image	caption	generation

loss	functions

loss

name

cost	(   0-1   )

id88

hinge

log

where	used
intractable,	but	

underlies	   direct	error	

minimization   

id88	algorithm
(rosenblatt,	1958)

support	vector	

machines,	other	large-

margin	algorithms
logistic	regression,	
conditional	random	
fields,	maximum
id178	models

65

name

cost	(   0-1   )

id88

hinge

log

loss	functions

issue:	gold	standard	translation	is	
often	unreachable	by	the	model

loss

where	used
intractable,	but	

underlies	   direct	error	

minimization   

id88	algorithm
(rosenblatt,	1958)

support	vector	

machines,	other	large-

margin	algorithms
logistic	regression,	
conditional	random	
fields,	maximum
id178	models

66

name

cost	(   0-1   )

id88

hinge

log

intractable,	but	it	doesn   t	need	to	

loss	functions

compute	model	score	of	gold	standard!

loss

where	used
intractable,	but	

underlies	   direct	error	

minimization   

id88	algorithm
(rosenblatt,	1958)

support	vector	

machines,	other	large-

margin	algorithms
logistic	regression,	
conditional	random	
fields,	maximum
id178	models

67

mert,	och (2003)

notation

feature
weights

feature
vector

source
sentence

translation

latent

derivation

minimum	error	rate	training	(mert)

minimum	error	rate	training	(mert)

set	of	source sentences

references

decoder
outputs

minimum	error	rate	training	(mert)

   how	bad	are	these	translations?   

e.g.,	negative	id7

set	of	source sentences

references

decoder
outputs

minimum	error	rate	training	(mert)

   how	bad	are	these	translations?   

minimize	the	cost	of	the	decoder	output
e.g.,	negative	id7

intractable	in	general	    how	can	we	solve	it?

set	of	source sentences

references

decoder
outputs

minimum	error	rate	training	(mert)

   how	bad	are	these	translations?   

minimize	the	cost	of	the	decoder	output
e.g.,	negative	id7

intractable	in	general	    how	can	we	solve	it?

set	of	source sentences

generate	k-best	lists	of	translations,	

approximately	minimize	cost	on	k-best	lists,		

references

repeat	with	new	parameters

(pool	k-best	lists	across	iterates)

decoder
outputs

id7

model	score

id7

each	point	is	a	translation	

for	the	same	sentence

arabic-english,
phrase-based

model	score

id7

10,000-best	list,	

default	moses	weights

1-best:
28	id7

model	score

id7

same	sentence,
10,000-best	list

after	mert

1-best:
34	id7

model	score

id7

another	sentence,

default	moses	weights

1-best:
46	id7

model	score

id7

same	sentence,

after	mert

1-best:
62	id7

model	score

why	are	there	horizontal	   bands   ?

id7

model	score

id7

why	are	there	horizontal	   bands   ?

latent	derivations,

different	translations	
with	same	id7

model	score

references

decoder
outputs

what	are	some	issues	with	this	loss	function?

discontinuous	&	non-convex	    optimization	relies	on	
randomized	search
no	id173	    leads	to	overfitting

as	a	result,	mert	is	only	effective	for	very	small	
models	(<40	parameters)

many	researchers	tried	to	improve	mert:

id173	and	search	for	mert (cer et	al.,	2008)
random	restarts	in	mert	for	mt	(moore	&	quirk,	2008)
stabilizing	mert (foster	&	kuhn,	2009)

issues	remain:

better	hypothesis	testing	for	statistical	mt:	controlling	for	
optimizer	instability	(clark	et	al.,	2011)
they	suggest	running	mert	3-5	times	due	to	its	
instability

id88	loss

reference

id7
score

model score

id88	loss

reference

id7
score

model	prediction

model score

id88	loss	for	mt?

(collins,	2002)

reference

id7
score

model	prediction

model score

k-best	id88	for	mt

(liang	et	al.,	2006)

id7
score

model	prediction

model score

k-best	id88	for	mt

(liang	et	al.,	2006)

id7
score

model	prediction

model score

k-best	id88	for	mt

(liang	et	al.,	2006)

id7
score

id7	oracle	on	k-best	list

model	prediction

model score

ramp	loss	minimization

id7
score

model score

ramp	loss	minimization

id7
score

model	prediction

model score

ramp	loss	minimization

id7
score

model	prediction

   fear   		translation

model score

   fear   	ramp	loss

(do	et	al.,	2008)

id7
score

model	prediction

   fear   		translation

model score

   fear   	ramp	loss

(do	et	al.,	2008)

id7
score

model	prediction

gold	standard

   fear   		translation

model score

   hope   	ramp	loss

(mcallester &	keshet,	2011; liang	et	al.,	2006)

id7
score

model	prediction

model score

   hope   	ramp	loss

(mcallester &	keshet,	2011; liang	et	al.,	2006)

id7
score

   hope   		translation

model	prediction

model score

   hope-fear   	ramp	loss

(chiang	et	al.,	2008;	2009;	cherry	&	foster,	2012;	chiang,	2012)

id7
score

   hope   		translation

   fear   		translation

model score

   hope-fear   	ramp	loss

(chiang	et	al.,	2008;	2009;	cherry	&	foster,	2012;	chiang,	2012)

id7
score

   hope   		translation

argmax

hy,hi2tx(i)      >f (x(i), y, h)   cost(y(i), y)   

argmax

hy,hi2tx(i)      >f (x(i), y, h) + cost(y(i), y)   

   fear   		translation

model score

experiments
(gimpel,	2012)

averages	over	8 test	sets	across	3	language	pairs

mert
fear	ramp	(away	from	bad)
hope	ramp	(toward	good)
hope-fear	ramp	(toward	good	+	away	from	bad)

moses
%id7
35.9
34.9
35.2
35.7

hiero
%id7
37.0
34.2
36.0
37.0

pairwise	ranking	optimization

(hopkins	&	may,	2011)

id7
score

model score

