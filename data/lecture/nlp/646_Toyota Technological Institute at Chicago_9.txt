ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	9:	sequence	models

1

announcements

    on	thursday,	class	will	be	in	room	530	(the	

room	directly	behind	you)

2

announcements

    we	will	go	over	part	of	assignment	1	today	

(grades	coming	soon)

    assignment	2	was	due	wed.	feb.	3,	now	due	

fri.,	feb.	5

    project	proposal	due	tuesday,	feb.	16
    midterm	on	thursday,	feb.	18

3

other	naturally-occurring	data

    quality	of	scientific	journalism:

4

other	naturally-occurring	data

    memorability	of	quotations:

5

other	naturally-occurring	data

    sarcasm	(remove	#sarcasm	hash	tag	from	tweets):

6

other	naturally-occurring	data

    opening	weekend	movie	revenue	prediction	from	critic	reviews:

7

other	naturally-occurring	data

    predicting	novel	success	from	text	of	novels:

8

project	proposal

    due	feb.	16	(in	two	weeks)
    1-2	pages
    one	per	group
    include	the	following:
    members	of	your	group
    describe	the	task	you	are	going	to	work	on	(could	be	a	

new	task	you	create	or	an	existing	task)

    describe	the	methods	you	will	use/develop	for	the	task
    give	a	brief	review	of	related	work;	i.e.,	situate	your	

project	with	respect	to	the	literature	(www.aclweb.org
and	google	scholar	are	useful	for	this!)

    a	proposed	timeline

9

project	proposal	(cont   d)

    your	results	do	not	have	to	beat	the	state-of-

the-art!

    but	your	project	does	have	to	be	carefully	done,	

so	that	you	can	draw	conclusions

    you	are	welcome	to	start	by	replicating	an	nlp	
paper	(i	can	give	suggestions	if	you	need	some)

    during	the	week	of	feb.	22,	please	schedule	a	

meeting	with	me	to	discuss	your	project	
    details	to	follow

10

class	presentations

    final	two	class	meetings	(march	3rd and	march	

8th)	will	be	mostly	used	for	in-class	
presentations

    one	presentation	per	group
    10-15	minutes	per	presentation	(will	be	

determined	once	i	know	how	many	groups	
there	are)

    you	will	each	take	notes	and	email	me	

questions/feedback	for	the	presenter,	which	i	
will	anonymize and	send

11

project

    final	report	due	thursday,	march	17	(original	

date	of	the	final	exam)

    so	the	presentation	will	be	more	like	an	

   interim	progress	report   

12

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    neural	network	methods	in	nlp
    syntax	and	syntactic	parsing
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

13

simplest	kind	of	structured	prediction:	sequence	labeling

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

14

formulating	segmentation	tasks	as	sequence	labeling	

via	b-i-o	labeling:

named	entity	recognition

o																				o														o					b-person			i-person						o										o																	o

some			questioned			if									tim										cook							   s						first						product	

o														o									o																	o																	o								o					b-organization						o
would						be						a						breakaway				hit				for												apple														.

b	=	   begin   
i	=	   inside   
o	=	   outside   

15

    there	are	many	downloadable	part-of-speech	

taggers	and	named	entity	recognizers:
    stanford	pos	tagger,	ner	labeler
    turbotagger,	turboentityrecognizer
    illinois	entity	tagger
    cmu	twitter	pos	tagger
    alan	ritter   s	twitter	pos/ner	labeler

16

17

hidden	markov	models

y1

x1

y2

x2

y3

x3

y4

x4

transition	parameters:

emission	parameters:	

18

id48s	for	word	id91

(brown	et	al.,	1992)

y1

y2

y3

y4

justin

bieber

for												president

each																		is	a	cluster	id
so,	label	space	is

19

id48s	for	part-of-speech	tagging

proper	
noun

proper	
noun

prepo-
sition

noun

justin

bieber

for												president

each																		is	a	part-of-speech	tag
so,	label	space	is

what	parameters	need	to	be	learned?

transition	parameters:

emission	parameters:	

20

how	should	we	learn	the	id48	parameters?

transition	parameters:

emission	parameters:	

21

supervised	id48s

    given	a	dataset	of	input	sequences	and	annotated	

outputs:

proper	
noun

proper	
noun

prepo-
sition

noun

justin

bieber

for												president

    to	estimate	transition/emission	distributions,	use	

maximum	likelihood	estimation	(count	and	normalize):

22

estimates	of	tag	transition	probabilities

proper									modal							infinitive			adjective				noun							adverb			determiner
noun												verb													verb							

23

estimates	of	emission	probabilities

24

id136	in	id48s

    since	the	output	is	a	sequence,	this	argmax

requires	iterating	over	an	exponentially-large	set

    last	week	we	talked	about	using	dynamic	
programming	(dp)	to	solve	these	problems

    for	id48s	(and	other	sequence	models),	the	for	

solving	this	is	called	the	viterbi	algorithm

25

viterbi	algorithm

    recursive	equations	+	memoization:
base	case:	
returns	id203	of	sequence	starting	with	label	y for	first	word

recursive	case:
computes	id203	of	max-id203	label	
sequence	that	ends	with	label	y at	position	m

final	value	is	in:

26

example:
janet			will			back				the				bill	
proper									modal							infinitive					determiner				noun
noun												verb													verb							

27

janet					will						back								the							bill	
proper									modal							infinitive					determiner				noun
noun												verb													verb							

28

viterbi	algorithm	(on	board)

29

viterbi	algorithm

    space	and	time	complexity?
    can	be	read	off	from	the	recursive	equations:
space	complexity:
size	of	memoizationtable,	which	is	#	of	unique	indices	of	recursive	equations

length	of	
sentence

*

number	
of	labels

so,	space	complexity	is	o(|x|	|l|)

30

viterbi	algorithm

    space	and	time	complexity?
    can	be	read	off	from	the	recursive	equations:
time	complexity:
size	of	memoizationtable	*	complexity	of	computing	each	entry

length	of	
sentence

*

number	
of	labels

*

each	entry	requires	

iterating	through	the	labels

so,	time	complexity	is	o(|x|	|l|	|l|)	=	o(|x|	|l|2)	

31

linear	sequence	models

    let   s	generalize	id48s	and	talk	about	linear	

models	for	scoring	label	sequences	in	our	
classifier	framework:

    but	first,	how	do	we	know	that	this	scoring	

function	generalizes	id48s?

32

id48	as	a	linear	model

id48:

linear	model:

    what	are	the	feature	templates	and	weights?

33

id48	as	a	linear	model

id48:

linear	model:

feature	templates	and	weights:

34

linear	sequence	models

    so,	an	id48	is:

    a	linear	sequence	model
    with	particular	features	on	label	transitions	and	label-

observation	emissions

    and	uses	maximum	likelihood	estimation	(count	&	

normalize)	for	learning

    but	we	could	use	any	feature	functions	we	like,	and	

use	any	of	our	loss	functions	for	learning!

35

(chain)	conditional	random	fields

36

(chain)	conditional	random	fields

    linear	sequence	model	
    arbitrary	features	of	input	are	permitted
    test-time	id136	uses	viterbi	algorithm
    learning	done	by	minimizing	log	loss	(dp	

algorithms	used	to	compute	gradients)

37

max-margin	markov	networks

38

maximum-margin	markov	networks

    linear	sequence	model	
    arbitrary	features	of	input	are	permitted
    test-time	id136	uses	viterbi	algorithm
    learning	done	by	minimizing	hinge	loss	(dp	

algorithm	used	to	compute	subgradients)

39

feature	locality

    feature	locality:	roughly,	how	   big   	are	your	

features?

    when	designing	efficient	id136	algorithms	
(whether	w/	dp	or	other	methods),	we	need	to	
be	mindful	of	this

    features	can	be	arbitrarily	big	in	terms	of	the	

input	sequence

    but	features	cannot be	arbitrarily	big	in	terms	of	

the	output sequence!

    the	features	in	id48s	are	small	in	both	the	input	
and	output	sequences	(only	two	pieces	at	a	time)

40

are	these	features	big	or	small?

feature
feature	that	counts	instances	of	   the   	in	the	
input	sentence
feature	that	returns square	root	of	sum	of	
counts	of	am/is/was/were
feature	that	counts	   verb verb   	sequences
feature	that	counts	   determiner noun 
verb verb   	sequences
feature	that	counts	the	number	of	nouns	in	a	
sentence

feature that	returns	the	ratio	of	nouns	to	verbs

big	or	small?

small

small

small

pretty	big!

big,	but	we	can	
design	specialized	
algorithms	to	handle	
them	if	they   re	the
only	big	features
41

learning	with	linear	sequence	models
    given	a	linear	sequence	model	with	   small   	

features,	how	should	we	do	learning?

42

loss	functions	for	learning	linear	sequence	models

entry	j of	(sub)gradient	of	loss for	linear	model

loss

id88

hinge

log

same	gradients/subgradients as	
before,	though	computing	these	
terms	(id136)	requires	dp	

algorithms

43

implementing	dp	algorithms

    start	with	counting	mode,	but	keep	in	mind	
how	the	model   s	score	function	decomposes	
across	parts	of	the	outputs
    i.e.,	how	   large   	are	the	features?		how	many	
items	in	the	output	sequence	are	needed	to	
compute	each	feature?
    define	a	function	called	partscore that	
computes	all	the	features	(for	counting	mode,	this	
function	will	return	1)

44

neural	networks	in	nlp

    neural	networks
    deep	neural	networks
    neural	language	models
    recurrent	neural	networks	and	lstms
    convolutional	neural	networks

45

what	is	a	neural	network?

    just	think	of	a	neural	network	as	a	function
    it	has	inputs	and	outputs
    the	term	   neural   	typically	means	a	particular	

type	of	functional	building	block	(   neural	
layers   ),	but	the	term	has	expanded	to	mean	
many	things

46

