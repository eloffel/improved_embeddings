ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	4:	lexical	semantics

1

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

2

why	is	nlp	hard?

    ambiguity	and	variability	of	linguistic	expression:

    ambiguity:	one	form	can	mean	many	things
    variability:	many	forms	can	mean	the	same	thing

3

feature	engineering	for	text	classification
    two	features:

ambiguity:	   great   	can	mean	different	things	in	different	contexts
    let   s	say	we	set
    on	sentences	containing	   great   	in	the	

stanford	sentiment	treebank	training	data,	
this	would	get	us	an	accuracy	of	69%

    but	   great      	only	appears	in	83/6911	examples
variability:	many	other	words	can	indicate	positive	sentiment

4

    most	of	what	we	talked	about	on	tuesday	and	

what	we	will	talk	about	today:

    one	form,	multiple	meanings	   split	form

ambiguity

    multiple	forms,	one	meaning	   merge	forms

variability

5

ambiguity

    one	form,	multiple	meanings	   split	form

    id121	(adding	spaces):	

    didn   t    did	n   t
       yes?          	yes	?	   

    today:	word	sense	disambiguation:

    power	plant    power	plant1
    flowering	plant	   flowering	plant2

6

variability

    multiple	forms,	one	meaning	   merge	forms

    id121	(removing	spaces):	

    new	york	   newyork

    lemmatization:	
    walked    walk	
    walking    walk

    id30:

    automation    automat
    automates    automat

are	there	any	nlp	tasks	

where	doing	this	might	lose	

valuable	information?

7

variability

    multiple	forms,	one	meaning	   merge	forms

    id121	(removing	spaces):	

    new	york	   newyork

    lemmatization:	
    walked    walk	
    walking    walk

    id30:

    automation    automat
    automates    automat

    today/next	week:	word	representations

8

vector	representations	of	words

id167	visualization	from	turian et	al. (2010)

9

word	clusters

class-based  id165 models  of natural 
language 

peter  f.  brown" 
peter  v.  desouza* 
peter f. brown and vincent j. della pietra  class-based id165 models of natural language 
robert  l.  mercer* 
ibm t. j. watson research center 

vincent j.  della  pietra* 
jenifer  c.  lai* 

friday monday thursday wednesday tuesday saturday sunday weekends sundays saturdays 
we address the problem of predicting a word from previous words in a sample of text. in particular, 
june march july april january december october november september august 
we discuss id165 models based on classes of words. we also discuss several statistical algorithms 
people guys folks fellows ceos chaps doubters commies unfortunates blokes 
for assigning words to classes based on the frequency of their co-occurrence with other words. we 
find that we are able to extract classes that have the flavor of either syntactically  based groupings 
down backwards ashore sideways southward northward overboard aloft downwards adrift 
or semantically based groupings, depending on the nature of the underlying  statistics. 
water gas coal liquid acid sand carbon steam shale iron 
great big vast sudden mere sheer gigantic lifelong scant colossal 
1.  introduction 
man woman boy girl lawyer doctor guy farmer teacher citizen 
in a number of natural language processing tasks, we face the problem of recovering a 
american indian european japanese german african catholic israeli italian arab 
computational	 linguistics,	1992
string of english words after it has been garbled by passage through a noisy channel. 
pressure temperature  permeability density porosity stress velocity viscosity gravity tension 
to tackle this problem successfully, we must be able to estimate the id203 with 
which any particular string of english words will be presented as input to the noisy 
mother wife father son husband brother daughter sister boss uncle 
channel. in this paper, we discuss a method for making such estimates. we also discuss 
machine device controller processor cpu printer spindle subsystem compiler plotter 
the  related  topic  of assigning words  to  classes according to  statistical behavior in  a 
large body of text. 
john george james bob robert paul william jim david mike 
in the next section, we review the concept of a language model and give a defini- 
tion of id165 models. in section 3, we look at the subset of id165 models in which 
anyone someone anybody somebody 
the words are divided into classes. we show that for n  =  2 the maximum likelihood 
feet miles pounds degrees inches barrels tons acres meters bytes 
assignment of words to classes is equivalent to the assignment for which the average 

10

roadmap

    lexical	semantics

    word	sense
    word	sense	disambiguation
    word	representations

11

word	sense	ambiguity
    many	words	have	multiple	meanings

12

word	sense	ambiguity

credit:	a.	zwicky

13

terminology:	lemma and	wordform

    lemma

    wordform

    words	with	same	lemma	have	same	stem,	part	of	

speech,	rough	semantics

    inflected	word	as	it	appears	in	text

wordform

banks
sung

duermes

lemma
bank
sing
dormir

j&m/slp3

lemmas	have	senses

    one	lemma	bank can	have	many	meanings:
sense	1:
sense	2:

   a	bank1 can	hold	the	investments	in	a	custodial	account
   as	agriculture	burgeons	on	the	east	bank2 the	river	will	
shrink	even	more

    sense	(or	word	sense)

    a	discrete	representation	of	an	aspect	of	a	word   s	meaning

    the	lemma	bank here	has	two	senses

j&m/slp3

    two	ways	to	categorize	the	patterns	of	

multiple	meanings	of	words:
    homonymy:	the	multiple	meanings	are	unrelated	

(coincidental?)

    polysemy:	the	multiple	meanings	are	related

16

homonymy

homonyms:	words	that	share	a	form	but	have	unrelated,	
distinct	meanings:

    bank1:	financial	institution,				bank2:		sloping	land
    bat1:	club	for	hitting	a	ball,				bat2: nocturnal	flying	mammal

homographs:	same	spelling,	different	meanings

homophones:	same	pronunciation,	different	meanings

bank/bank,	bat/bat

write/right,	piece/peace

j&m/slp3

homonymy	causes	problems	for	nlp

    information	retrieval

    query: bat	care

    machine	translation

    bat:		murci  lago (animal)	or	bate (for	baseball)

    text-to-speech

    bass (stringed	instrument)	vs.	bass (fish)

j&m/slp3

polysemy

1:	the	bank was	constructed	in	1875	out	of	local	red	brick.
2:	i	withdrew	the	money	from	the	bank.
    are	these	the	same	sense?

    sense	2:	   a financial	institution   
    sense	1:	   the	building	belonging	to	a	financial	institution   

    a	polysemous word	has	related meanings

    most	non-rare	words	have	multiple	meanings

j&m/slp3

metonymy	or	systematic	polysemy:	

a	systematic	relationship	between	senses
    lots	of	types	of	polysemy	are	systematic

    school,	university,	hospital
    all	can	mean	the	institution	or	the	building

    a systematic	relationship:
    building												organization

    other	such	kinds	of	systematic	polysemy:	
author (jane	austen	wrote	emma)
tree (plums	have	beautiful	blossoms)	 													fruit (i	ate	a	preserved	plum)

works	of	author	(i	love	jane	austen)

j&m/slp3

how	do	we	know	when	a	word	has	more	than	one	sense?

       zeugma   	test:	two	senses	of	serve?

    which	flights	serve breakfast?
    does	lufthansa	serve philadelphia?
    ?does	lufthansa	serve	breakfast	and	

philadelphia?

    since	this	conjunction	sounds	weird,	we	say	
that	these	are	two	different	senses	of	serve

j&m/slp3

synonyms

    words	with	same	meaning	in	some	or	all	contexts:

    filbert	/	hazelnut
    couch	/	sofa
    big	/	large
    automobile	/	car
    vomit	/	throw	up
    water	/	h20

    two	lexemes	are	synonyms	if	they	can	be	
substituted	for	each	other	in	all	situations

j&m/slp3

synonyms

    few	(or	no)	examples	of	perfect	synonymy
    even	if	many	aspects	of	meaning	are	identical
    still	may	not	preserve	the	acceptability	based	on	
notions	of	politeness,	slang,	register,	genre,	etc.

    example:

    water	/	h20
    big	/	large
    brave	/	courageous

j&m/slp3

synonymy	is	a	relation	

between	senses rather	than	words

    consider	the	words	big and	large
    are	they	synonyms?

    how	big is	that	plane?
    would	i	be	flying	on	a	large or	small	plane?

    how	about	here:

    miss	nelson became	a	kind	of	big sister	to	benjamin.
    ?miss	nelson became	a	kind	of	large sister	to	benjamin.

    why?

    big has	a	sense	that	means	being	older	or	grown	up
    large lacks	this	sense

j&m/slp3

antonyms

    senses	that	are	opposites	with	respect	to	one	feature	of	

meaning

    otherwise,	they	are	very	similar!

dark/light short/long					fast/slow				rise/fall
hot/cold

up/down				in/out

    more	formally,	antonyms	can

    define	a	binary	opposition	or	be	at	opposite	ends	of	a	scale

    long/short,	fast/slow

    be	reversives:

    rise/fall,	up/down

j&m/slp3

hyponymy	and	hypernymy

    one	sense	is	a	hyponym of	another	if	the	first	
sense	is	more	specific,	denoting	a	subclass	of	
the	other
    car is	a	hyponym	of	vehicle
    mango is	a	hyponym	of	fruit

    conversely:	hypernym (   hyper	is	super   )

    vehicle is	a	hypernym of	car
    fruit is	a	hypernym of	mango

j&m/slp3

id138 3.0

    hierarchically	organized	lexical	database
    on-line	thesaurus	+	aspects	of	a	dictionary

    some	languages	available	or	under	development:	

arabic,	finnish,	german,	portuguese   
unique	strings

category

noun
verb

adjective
adverb

117,798
11,529
22,479
4,481

j&m/slp3

senses	of	bass in	id138

j&m/slp3

how	is	   sense   	defined	in	id138?
    synset (synonym	set):	set	of	near-synonyms;	

instantiates	a	sense	or	concept,	with	a	gloss

    example:	chump as	a	noun	with	gloss:

   a	person	who	is	gullible	and	easy	to	take	advantage	of   
    this	sense	of	chump is	shared	by	9	words:

chump1,	fool2,	gull1,	mark9,	patsy1,	fall	guy1,	sucker1,	soft	touch1,	mug2

    each	of	these senses	have	this	same	gloss

    (not	every sense;	sense	2	of	gull is	the	aquatic	bird)

j&m/slp3

ambiguity
    one	form,	multiple	meanings	   split	form

    the	three	senses	of	fool belong	to	different	synsets

variability
    multiple	forms,	one	meaning	   merge	forms

    each	synset contains	senses	of	several	different	words	

30

id138 hypernym hierarchy	for	bass

j&m/slp3

emotion

time
event

group
person
artifact
cognition
food
act
location
time
event

communic.    417 review

communic.    974 recommend

ical entries, the supersense categories are unlexical-
ized. the n:person category, for instance, contains
synsets for both principal and student. a different
sense of principal falls under n:possession.

249 love
530 day
431 experience perception 143 see
supersenses:	top	level	hypernyms in	hierarchy
consumption 93 have
82 get. . . done
body
(counts	from	schneider	&	smith   s	streusel	corpus)
64 cook
creation
46 put
contact
verb
competition 11 win
2922 is
stative
0    
weather
cognition 1093 know
all 15 vssts 7806

possession 339 price
205 quality
attribute
102 amount
quantity
noun
verb
noun
the 26 noun and 15 verb supersense categories are
88 dog
animal
listed with examples in table 1. some of the names
1469 place
2922 is
1469 place
group
87 hair
stative
body
overlap between the noun and verb inventories, but
1202 people
cognition 1093 know
1202 people
person
56 pain
state
971 car
971 car
they are to be considered separate categories; here-
artifact
natural obj. 54    ower
944 use
771 way
944 use
771 way
n/a (see   3.2)
social
cognition
35 portion
social
after, we will distinguish the noun and verb categories
relation
602 go
766 food
766 food
602 go
motion
food
1191 have
34 oil
motion
`a
substance
with pre   xes, e.g. n:cognition vs. v:cognition.
possession 309 pay
700 service
possession 309 pay
700 service
act
821 anyone
34 discomfort `
feeling
though id138 synsets are associated with lex-
638 area
274    x
274    x
638 area
location
change
28 process
54 fried
change
`j
process
ical entries, the supersense categories are unlexical-
530 day
249 love
530 day
249 love
time
emotion
25 reason
emotion
motive
ized. the n:person category, for instance, contains
431 experience perception 143 see
431 experience perception 143 see
event
phenomenon 23 result
synsets for both principal and student. a different
consumption 93 have
consumption 93 have
6 square
shape
82 get. . . done
possession 339 price
sense of principal falls under n:possession.
82 get. . . done
body
5 tree
body
communication
plant
64 cook
205 quality
64 cook
attribute
creation
as far as we are aware, the supersenses were
2 stuff
creation
other
46 put
102 amount
46 put
contact
quantity
all 26 nssts 9018
contact
originally intended only as a method of organizing
88 dog
competition 11 win
competition 11 win
animal
the id138 structure. but ciaramita and johnson
table 1: summary of noun and verb supersense cate-
0    
87 hair
0    
weather
body
weather
(2003) pioneered the coarse word sense disambigua-
gories. each entry shows the label along with the count
all 15 vssts 7806
56 pain
all 15 vssts 7806
j&m/slp3
32
state
and most frequent lexical item in the streusle corpus.
tion task of supersense tagging, noting that the su-
natural obj. 54    ower

originally intended only as a method of organizing
the id138 structure. but ciaramita and johnson
(2003) pioneered the coarse word sense disambigua-
tion task of supersense tagging, noting that the su-
persense categories provided a natural broadening
of the traditional named entity categories to encom-
with pre   xes, e.g. n:cognition vs. v:cognition.
pass all nouns. ciaramita and altun (2006) later
expanded the task to include all verbs, and applied
a supervised sequence modeling framework adapted
from ner. evaluation was against manually sense-
tagged data that had been automatically converted to
the coarser supersenses. similar taggers have since
been built for italian (picca et al., 2008) and chi-
nese (qiu et al., 2011), both of which have their own
id138s mapped to english id138.
(2003) pioneered the coarse word sense disambigua-

communic.    974 recommend

205 quality
102 amount
88 dog
87 hair
56 pain

communic.    417 review

   communic.

communic.    417 review

possession 339 price
attribute
quantity
animal
body
state
natural obj. 54    ower

is short for

meronymy

    part-whole	relation

    wheel	is	a	meronym of	car
    car	is	a	holonym of	wheel

j&m/slp3

33

id138 noun	relations

16.4

    id51: overview 7

from concepts to subtypes
from instances to their concepts

also called de   nition
superordinate from concepts to superordinates
subordinate

relation
hypernym
hyponym
instance hypernym instance
instance hyponym has-instance from concepts to concept instances
member meronym has-member from groups to their members
member holonym member-of
from members to their groups
from wholes to parts
part meronym
part holonym
from parts to wholes
from substances to their subparts
substance meronym
from parts of substances to wholes
substance holonym
semantic opposition between lemmas leader1 () follower1
antonym
lemmas w/same morphological root
derivationally

example
breakfast1 ! meal1
meal1 ! lunch1
austen1 ! author1
composer1 ! bach1
faculty2 ! professor1
copilot1 ! crew1
table2 ! leg3
course7 ! meal1
water1 ! oxygen1
gin1 ! martini1
destruction1 () destroy1

has-part
part-of

related form

figure 16.2 noun relations in id138.

relation
hypernym
troponym

de   nition
from events to superordinate events
from events to subordinate event

example
   y9 ! travel5
walk1 ! stroll1

j&m/slp3

id51: a survey

id138:	viewed	as	a	graph

10:9

fig. 3. an excerpt of the id138 semantic network.

j&m/slp3

35

roadmap

    lexical	semantics

    word	sense
    word	sense	disambiguation
    word	representations

36

word	sense	disambiguation

credit:	a.	zwicky

37

word	sense	disambiguation	(wsd)

    given:	

    a word	in	context	
    a fixed	inventory	of	potential	word	senses
    decide	which	sense	of	the	word	this	is
    why?	machine	translation,	question	answering,	

sentiment	analysis,	text-to-speech

    what	set	of	senses?

    english-to-spanish	machine	translation:	set	of	

spanish	translations

    text-to-speech:	homographs	like	bass and	bow
    in	general:	the	senses	in	a	thesaurus	like	id138

j&m/slp3

two	variants	of	wsd	task

    lexical	sample	task

    small	pre-selected	set	of	target	words	(line,	plant,	bass)
    inventory	of	senses	for	each	word
    supervised	learning:	train	a	classifier	for	each word

    all-words	task

    every	word	in	an	entire	text
    a lexicon	with	senses	for	each	word
    data	sparseness:	can   t	train	word-specific	classifiers

j&m/slp3

classification	framework

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

40

classification	for	word	sense	disambiguation	of	bass

    data:

    what	is	the	space	of	possible	inputs	and	outputs?
    what	do	(x,y)	pairs	look	like?
    x =	the	word	bass along	with	its	context
    y =	word	sense	of	bass (from	a	list	of	possible	senses)

41

8	senses	of	bass in	id138

j&m/slp3

inventory	of	sense	tags	for	bass

16.5

    supervised id51
roget

target word in context

id138 spanish
sense
bass4
bass4
bass7
bass7
figure 16.5 possible de   nitions for the inventory of sense tags for bass.

fish/insect . . .    sh as paci   c salmon and striped bass and. . .
fish/insect . . . produce    lets of smoked bass or sturgeon. . .
. . . exciting jazz bass player since ray brown. . .
music
. . . play bass because he doesn   t have to solo. . .
music

translation category
lubina
lubina
bajo
bajo

9

the set of senses are small, supervised machine learning approaches are often used
to handle lexical sample tasks. for each word, a number of corpus instances (con-
text sentences) can be selected and hand-labeled with the correct sense of the target
word in each. classi   er systems can then be trained with these labeled examples.
unlabeled target words in context can then be labeled using such a trained classi   er.
j&m/slp3
early work in id51 focused solely on lexical sample tasks

wsd	evaluation	and	baselines

    best	evaluation:	extrinsic	(   task-based   )

    embed	wsd	in	a	task	and	see	if	it	helps!

    intrinsic	evaluation often	done	for	convenience

    strong	baseline:	most	frequent	sense

j&m/slp3

most	frequent	sense

    id138	senses	are	ordered	by	frequency
    most	frequent	is	first
    sense	frequencies	come	from	semcor corpus

j&m/slp3

performance	ceiling

    human	inter-annotator	agreement

    compare	annotations	of	two	humans	on	same	data,	given	

same	tagging	guidelines

    human	agreements	on	all-words	corpora	with	

id138	style	senses:	75%-80%	

j&m/slp3

training	data	for	wsd

    semantic	concordance:	corpus	in	which	each	
open-class	word	is	labeled	with	a	sense	from	a	
specific	dictionary/thesaurus
    semcor:	234,000	words	from	brown	corpus,	

manually	tagged	with	id138 senses

    senseval-3	competition	corpora:	2081	tagged	

word	tokens

j&m/slp3

features	for	wsd?

intuition	from	warren	weaver	(1955):

features	for	wsd?

intuition	from	warren	weaver	(1955):

   if	one	examines	the	words	in	a	book,	one	at	a	
time	as	through	an	opaque	mask	with	a	hole	in	it	
one	word	wide,	then	it	is	obviously	impossible	to	
determine,	one	at	a	time,	the	meaning	of	the	
words   	
but	if	one	lengthens	the	slit	in	the	opaque	mask,	
until	one	can	see	not	only	the	central	word	in	
question	but	also	say	n	words	on	either	side,	then	if	n	is	large	
enough	one	can	unambiguously	decide	the	meaning	of	the	
central	word   	
the	practical	question	is	:	   what	minimum	value	of	n	will,	at	
least	in	a	tolerable	fraction	of	cases,	lead	to	the	correct	choice	
of	meaning	for	the	central	word?      

j&m/slp3

features	for	wsd

    collocational

    features	for	words	at	specific positions	near	target	

word

    bag-of-words

    features	for	words	that	occur	anywhere	in	a	

window	of	the	target	word	(regardless	of	position)

j&m/slp3

example

    using	a	window	of	+/- 3	from	the	target:

an	electric guitar	and	bass player	stand off to	
one	side	not	really	part	of	the	scene

j&m/slp3

semi-supervised	learning

problem:	supervised	learning	requires	large	
hand-built	resources

what	if	you	don   t	have	much	training	data?

solution:	id64

generalize	from	a	very	small	hand-labeled	seed	set

j&m/slp3

id64

       one	sense	per	collocation   	heuristic:

    a	word	reoccurring	in	collocation	with	the	same	

word	will	almost	surely	have	the	same	sense

    for	bass:

    play occurs	with	the	music	sense	of	bass	
    fish	occurs	with	the	fish	sense	of	bass

j&m/slp3

servation, a and b are observations labeled as sense-a or sense-b. the initial stage (a) shows only seed
sentences l0 labeled by collocates (   life    and    manufacturing   ). an intermediate stage is shown in (b) where
more collocates have been discovered (   equipment   ,    microscopic   , etc.) and more instances in v0 have been
moved into l1, leaving a smaller unlabeled set v1. figure adapted from yarowsky (1995).

sentences	extracted	using	fish and	play

we need more good teachers     right now, there are only a half a dozen who can play
the free bass with ease.
an electric guitar and bass player stand off to one side, not really part of the scene, just
as a sort of nod to gringo expectations perhaps.
the researchers said the worms spend part of their life cycle in such    sh as paci   c
salmon and striped bass and paci   c rock   sh or snapper.
and it all started when    shermen decided the striped bass in lake mead were too
skinny.

figure 16.10 samples of bass sentences extracted from the wsj by using the simple cor-
relates play and    sh.

strongly associated with the target senses tend not to occur with the other sense.
yarowsky de   nes his seedset by choosing a single collocation for each sense.

j&m/slp3

for example, to generate seed sentences for the    sh and musical musical senses

id64

       one	sense	per	collocation    heuristic:

    a	word	reoccurring	in	collocation	with	the	same	

word	will	almost	surely	have	the	same	sense

       one	sense	per	discourse   	heuristic:

    sense	of	a	word	is	highly	consistent	within	a	

document	(yarowsky,	1995)

    especially	topic-specific	words

j&m/slp3

stages	in	yarowsky id64	

algorithm	for	plant

?
?
?

?

life
?
?
?
a
?
a
a
a
a
a
aa
a
a
a
a
a
a
a
a
a

?
a
a

?

?

?

?
?
?
?

?
??
?
?

?
?

a
a
?
?

?
?

?
?

?

?

?

?

?

?

?

?

?

?

?

?
?

?

?

?

?
?
?

?

?

?

?

?
?

?

?

  0

?
?

?

?

?

?

?

??

?

?

?

?
?
?

?

?

?

?

?
?

?

?

?

?

?
?

?

?

v0

?
??

?

?

?
?

?
?
?
?

?

?

?
?

?

?

?

?
?
?

?
??
?
?

?

?
?
?

?
?

?

?

?

?
?

?

?
?

?

?

?

?

b

b
b
b

b
b

?
b
?
?
manufacturing

b
b
b
b
?

b

?

?

?
?

?
?
?
??
?
?
?
?
??
?
?

?

?

?

?
?
?
?
?

?

?

?

?
?

?
??

?

?
?

?
??

?
?

?

?

?
?
?
(a)

?

?
a
a
?
aa
a
?
?
?
?
?
??
?
?

?

?

?

?

?
?

?
?

?

?
?
a

?

?

?

?

?
?
?
?

?
?a
a
a

life
?
?
?
a
?
a
a
a
a
a
aa
a
a
a
a
a
a
a
a
a

?
a
a

a
a
?
?

a
?

?
?

?
a
animal
?
?

?

?

?

a

?
?

?

?

?

?

?

?

?

?

?
?

?

?

?

?
?
?

?

?

?

?

??

?
?

?

  1

?

?

?

b

b

b
??

?

?

b
?
?
?
equipment
(b)

?
?
?
?

?
?
?

?

?

?
?
?
?
?
?
?
microscopic
?

?
?
?

?

?

?
?

?

?

?
?

v1

?

?

?

?

?

?

?

?
?

?
employee
??

?
??
b
b

?

?
?

?

?

?

?

b

?
??

?

?

?
?

?

?

?
?

?
?
?

?

?
?

?

b

b
b
b

b
b

?
b
b
b
?
manufacturing

b
b
b
b
?

?

j&m/slp3

summary

    word	sense	disambiguation:	choosing	correct	

sense	in	context

    applications:	mt,	qa,	etc.
    main	intuition:	

    lots	of	information	in	a	word   s	context
    simple	algorithms	based	on	word	counts	can	be	

surprisingly	good

57

roadmap

    lexical	semantics

    word	sense
    word	sense	disambiguation
    word	representations

58

ambiguity
    one	form,	multiple	meanings	   split	form

    the	three	senses	of	fool belong	to	different	synsets

variability
    multiple	forms,	one	meaning	   merge	forms

    each	synset contains	senses	of	several	different	words	

59

    id138 splits	words	into	synsets; each	contains	

senses	of	several	words:

    are	we	finished?		have	we	solved	the	problem	of	

representing	word	meaning?

    are	we	finished?		have	we	solved	the	problem	of	

representing	word	meaning?

    issues:

    id138 has	limited	coverage and	only	exists	for	a	small	set	

of	languages

category

unique	strings

noun
verb

adjective
adverb

117,798
11,529
22,479
4,481

    wsd	requires	training	data,	whether	supervised	or	seeds	for	

semi-supervised

    better	approach:	jointly	learn	representations	for	all	

words	in	an	unsupervised	way

word	embeddings

(bengio et	al.,	2003)

id167	visualization	from	turian et	al. (2010)

62

