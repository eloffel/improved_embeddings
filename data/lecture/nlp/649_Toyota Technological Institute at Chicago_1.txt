ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	1:	introduction

1

what	is	natural	language	processing?

2

what	is	natural	language	processing?
an	experimental	computer	science	research	area
that	includes	problems	and	solutions	pertaining	to

the	understanding	of	human	language

3

text	classification

4

text	classification

spam	/	not	spam

   
    priority	level
   

category	(primary	/	social	/	promotions	/	updates)

5

sentiment	analysis

6

machine	translation

7

machine	translation

new	poll:	will	you	buy	an	apple	watch?

8

question	answering

9

summarization

10

summarization

the	apple	watch	has	drawbacks.	there	are	other	

smartwatches that	offer	more	capabilities.	

11

dialog	systems

user:	schedule	a	meeting	with	matt	and	david	on	thursday.
computer:	thursday	won   t	work	for	david.	how	about	friday?
user:	i   d	prefer	monday	then,	but	friday	would	be	ok	if	necessary.

12

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun

some						questioned						if							tim						cook						   s						first						product	

modal							verb				det.									adjective									noun				prep.						proper					punc.
would						be						a						breakaway						hit						for						apple								.

13

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

14

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

named	entity	recognition

some	questioned	if	tim	cook   s	first	product	would	be	a	breakaway	hit	for	apple.

person

organization

15

syntactic	parsing

16

entity	linking

en.wikipedia.org/wiki/dell
infobox type: company

en.wikipedia.org/wiki/michael_dell
infobox type: person

models. finally, as a structured crf, it is concep-
tually no more complex than its component models
and its behavior can be understood using the same
intuition.

organization

person

revenues of $14.5 billion were posted by dell1. the company1 ...

coreference resolution

figure 1: coreference can help resolve ambiguous cases
of semantic types or entity links: propagating information
across coreference arcs can inform us that, in this context,
dell is an organization and should therefore link to the
article on dell in wikipedia.

we apply our model to two datasets, ace 2005
and ontonotes, with different mention standards
and layers of annotation. in both settings, our joint
model outperforms our independent baseline mod-
els. on ace, we achieve state-of-the-art entity link-
ing results, matching the performance of the system
of fahrni and strube (2014). on ontonotes, we
match the performance of the best published coref-
erence system (bj  orkelund and kuhn, 2014) and
outperform two strong ner systems (ratinov and
roth, 2009; passos et al., 2014).

shown that tighter integration of coreference and
entity linking is promising (hajishirzi et al., 2013;

17

figure	credit:	durrett &	klein	(2014)

2 motivating examples

   winograd schema   	
coreference resolution

the	man	couldn't	lift	his	son	because	he was	so	weak.

the	man	couldn't	lift	his	son	because	he was	so	heavy.

18

   winograd schema   	
coreference resolution

the	man	couldn't	lift	his	son	because	he was	so	weak.

man

the	man	couldn't	lift	his	son	because	he was	so	heavy.

son

19

reading	comprehension

once	there	was	a	boy	named	fritz	who	loved	to	draw.	he	drew	
everything.	in	the	morning,	he	drew	a	picture	of	his	cereal	with	
milk.	his	papa	said,	   don   t	draw	your	cereal.	eat	it!   	
after	school,	fritz	drew	a	picture	of	his	bicycle.	his	uncle	said,	
   don't	draw	your	bicycle.	ride	it!   
   

what	did	fritz	draw	first?

a)	the	toothpaste
b)	his	mama
c)	cereal	and	milk
d)	his	bicycle

20

conspicuous	by	their	absence   
    speech	recognition	(see	ttic	31110)
    information	retrieval	and	web	search
    knowledge	representation
    recommender	systems

21

computational	linguistics	vs.	natural	language	processing
    how	do	they	differ?

22

computational	biology	vs.	bioinformatics

   computational	biology	=	the	study	of	biology	using	
computational	techniques.		the	goal	is	to	learn	new	biology,	
knowledge	about	living	systems.		it	is	about	science.

bioinformatics	=	the	creation	of	tools	(algorithms,	databases)	
that	solve	problems.		the	goal	is	to	build	useful	tools	that	work	
on	biological	data.		it	is	about	engineering.   

--russ	altman

23

computational	linguistics	vs.	natural	language	processing

    many	people	think	of	the	two	terms	as	synonyms

    computational	linguistics	is	more	inclusive;	more	likely	

to	include	sociolinguistics,	cognitive	linguistics,	and	
computational	social	science

    nlp	is	more	likely	to	use	machine	learning	and	involve	

engineering	/	system-building

24

is	nlp	science	or	engineering?

    goal	of	nlp	is	to	develop	technology,	which	takes	the	

form	of	engineering

    though	we	try	to	solve	today   s	problems,	we	seek	

principles	that	will	be	useful	for	the	future

    if	science,	it   s	not	linguistics	or	cognitive	science;	it   s	
the	science	of	computational	processing	of	language

    so	i	like	to	think	that	we   re	doing	the	science	of	

engineering

25

course	overview
    new	course,	first	time	being	offered

    aimed	at	first-year	phd	students

    instructor	office	hours:	mondays	3-4	pm,	ttic	531

    teaching	assistant:	lifu tu,	ttic	phd	student

26

prerequisites

    no	course	prerequisites,	but	i	will	assume:

    some	programming	experience	(no	specific	

language	required)

    familiarity	with	basics	of	id203,	calculus,	and	

linear	algebra

    undergraduates	with	relevant	background	are	

welcome	to	take	the	course.	please	bring	an	
enrollment	approval	form	to	me	if	you	can   t	
enroll	online.

27

grading

    3	assignments	(15%	each)
    midterm	exam	(15%)
    course	project	(35%):

    preliminary	report	and	meeting	with	instructor	(10%)
    class	presentation	(5%)
    final	report	(20%)

    class	participation	(5%)
    no	final

28

assignments

    mixture	of	formal	exercises,	implementation,	

experimentation,	analysis

       choose	your	own	adventure   	component	

based	on	your	interests,	e.g.:
    exploratory	data	analysis
    machine	learning
    implementation/scalability
    model	and	error	analysis
    visualization

29

project

    replicate	[part	of]	a	published	nlp	paper,	or	

define	your	own	project.

    the	project	may	be	done	individually	or	in	a	

group	of	two.	each	group	member	will	receive	
the	same	grade.

    more	details	to	come.

30

collaboration	policy

    you	are	welcome	to	discuss	assignments	with	
others	in	the	course,	but	solutions	and	code	
must	be	written	individually

31

textbooks

    all	are	optional
    speech	and	language	processing,	2nd ed.	

    some	chapters	of	3rd edition	are	online

    the	analysis	of	data,	volume	1:	id203

   

    freely	available	online
introduction	to	information	retrieval
    freely	available	online

32

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

33

why	is	nlp	hard?

    ambiguity	and	variability	of	linguistic	expression:

    variability:	many	forms	can	mean	the	same	thing
    ambiguity:	one	form	can	mean	many	things

    there	are	many	different	kinds	of	ambiguity
    each	nlp	task	has	to	address	a	distinct	set	of	kinds

34

word	sense	ambiguity
    many	words	have	multiple	meanings

35

word	sense	ambiguity

credit:	a.	zwicky

36

word	sense	ambiguity

credit:	a.	zwicky

37

attachment	ambiguity

38

meaning	ambiguity

39

text	classification

    simplest	user-facing	nlp	application
    email	(spam,	priority,	categories):

    sentiment:

    topic	classification
    others?

40

what	is	a	classifier?

41

what	is	a	classifier?

    a	function	from	inputs	x to	classification	labels	y

42

what	is	a	classifier?

    a	function	from	inputs	x to	classification	labels	y
    one	simple	type	of	classifier:

    for	any	input	x,	assign	a	score	to	each	label	y,	

parameterized	by	vector	

:

43

what	is	a	classifier?

    a	function	from	inputs	x to	classification	labels	y
    one	simple	type	of	classifier:

    for	any	input	x,	assign	a	score	to	each	label	y,	

parameterized	by	vector	

:

    classify	by	choosing	highest-scoring	label:

44

course	philosophy

    from	reading	papers,	one	gets	the	idea	that	machine	

learning	concepts	are	monolithic,	opaque	objects
    e.g.,	na  ve	bayes,	logistic	regression,	id166s,	crfs,	neural	

networks,	lstms,	etc.	

    nothing	is	opaque
    everything	can	be	dissected,	which	reveals	connections
    the	names	above	are	useful	shorthand,	but	not	useful	

for	gaining	understanding

45

course	philosophy

    we	will	draw	from	machine	learning,	linguistics,	and	
algorithms,	but	technical	material	will	be	(mostly)	self-
contained;	we	won   t	use	many	black	boxes

    we	will	focus	on	declarative	(rather	than	procedural)	
specifications,	because	they	highlight	connections	and	
differences

46

modeling,	id136,	learning

47

modeling,	id136,	learning

modeling:	define		score	function

    modeling:	how	do	we	assign	a	score	to	an	

(x,y)	pair	using	parameters				?

48

modeling,	id136,	learning

id136:	solve														_ modeling:	define		score	function

    id136:	how	do	we	efficiently	search	over	

the	space	of	all	labels?

49

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

    learning:	how	do	we	choose				?

learning:	choose	_

50

modeling,	id136,	learning

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

    we	will	use	this	same	paradigm	throughout	
the	course,	even	when	the	output	space	size	
is	exponential	in	the	size	of	the	input	or	is	
unbounded	(e.g.,	machine	translation)

51

notation

    we   ll	use	boldface	for	vectors:

   

individual	entries	will	use	subscripts	and	no	boldface,	e.g.,	for	
entry	i:

52

modeling:	linear	models

    score	function	is	linear	in				:

    f	:	feature	function	vector
   

:	weight	vector

53

modeling:	linear	models

    score	function	is	linear	in				:

    f	:	feature	function	vector
   
    how	do	we	define		f	?

:	weight	vector

54

defining	features

    this	is	a	large	part	of	nlp
    last	20	years:	feature	engineering
    last	2	years:	representation	learning

    in	this	course,	we	will	do	both
    learning	representations	doesn   t	mean	that	we	

don   t	have	to	look	at	the	data	or	the	output!
    there   s	still	plenty	of	engineering	required	in	

representation	learning

55

defining	features

    this	is	a	large	part	of	nlp
    last	20	years:	feature	engineering
    last	2	years:	representation	learning

    in	this	course,	we   ll	do	both
    learning	representations	doesn   t	mean	that	we	

don   t	have	to	look	at	the	data	or	the	output!
    there   s	still	plenty	of	engineering	required	in	

representation	learning

56

feature	engineering

    often	decried	as	   costly,	hand-crafted,	

expensive,	domain-specific   ,	etc.

    but	in	practice,	simple	features	typically	give	

the	bulk	of	the	performance

    let   s	get	concrete:	how	should	we	define	

features	for	text	classification?

57

feature	engineering	for	text	classification

58

feature	engineering	for	text	classification

is	now	a	vector	because	
it	is	a	sequence	of	words

59

feature	engineering	for	text	classification

is	now	a	vector	because	
it	is	a	sequence	of	words

let   s	consider	sentiment	analysis:																		

60

feature	engineering	for	text	classification

is	now	a	vector	because	
it	is	a	sequence	of	words

let   s	consider	sentiment	analysis:																		

so,	here	is	our	sentiment	classifier	that	uses	a	linear	model:

61

feature	engineering	for	text	classification

    two	features:

where

    what	should	the	weights	be?

62

feature	engineering	for	text	classification

    two	features:

where

    what	should	the	weights	be?

63

feature	engineering	for	text	classification
    two	features:

    let   s	say	we	set
    on	sentences	containing	   great   	in	the	

stanford	sentiment	treebank	training	data,	
this	would	get	us	an	accuracy	of	69%

    but	   great      	only	appears	in	83/6911	examples

64

feature	engineering	for	text	classification
    two	features:

ambiguity:	   great   	can	mean	different	things	in	different	contexts
    let   s	say	we	set
    on	sentences	containing	   great   	in	the	

stanford	sentiment	treebank	training	data,	
this	would	get	us	an	accuracy	of	69%

    but	   great      	only	appears	in	83/6911	examples
variability:	many	other	words	can	indicate	positive	sentiment

65

    usually,	great	indicates	positive	sentiment:

the	most	wondrous	love	story	in	years,	it	is	a	great film.
a	great companion	piece	to	other	napoleon	films	.

    sometimes	not.	why?

negation:	it's	not	a	great monster	movie	.
different	sense:	there's	a	great	deal	of	corny	dialogue	and	
preposterous	 moments	.
multiple	sentiments: a	great ensemble	cast	can't	lift	this	
heartfelt	enterprise	out	of	the	familiar.

    and	there	are	many	other	words	that	indicate	

positive	sentiment

66

    usually,	great	indicates	positive	sentiment:

the	most	wondrous	love	story	in	years,	it	is	a	great film.
a	great companion	piece	to	other	napoleon	films	.

    sometimes	not.	why?

negation:	it's	not	a	great monster	movie	.
different	sense:	there's	a	great	deal	of	corny	dialogue	and	
preposterous	 moments	.
multiple	sentiments: a	great ensemble	cast	can't	lift	this	
heartfelt	enterprise	out	of	the	familiar.

67

feature	engineering	for	text	classification

    what	about	a	feature	like	the	following?

    what	should	its	weight	be?

68

feature	engineering	for	text	classification

    what	about	a	feature	like	the	following?

    what	should	its	weight	be?
    doesn   t	matter.
    why?

69

text	classification

our	linear	sentiment	classifier:

70

id136 for	text	classification

id136:	solve														_

71

id136 for	text	classification

id136:	solve														_

    trivial	(loop	over	labels)

72

text	classification

73

learning	for	text	classification

learning:	choose	_

74

learning	for	text	classification

learning:	choose	_

    there	are	many	ways	to	choose

75

experimental	practice
    in	the	beginning,	we	just	had	data
    first	innovation:	split	into	train	and	test
    motivation:	simulate	conditions	of	applying	

system	in	practice

    but,	there   s	a	problem	with	this   

    we	need	to	explore	and	evaluate	methodological	

choices

    after	multiple	evaluations	on	test,	it	is	no	longer	a	

simulation	of	real-world	conditions

76

experimental	practice
    in	the	beginning,	we	just	had	data
    first	innovation:	split	into	train and	test
    motivation:	simulate	conditions	of	applying	

system	in	practice

    but,	there   s	a	problem	with	this   

    we	need	to	explore	and	evaluate	methodological	

choices

    after	multiple	evaluations	on	test,	it	is	no	longer	a	

simulation	of	real-world	conditions

77

experimental	practice
    in	the	beginning,	we	just	had	data
    first	innovation:	split	into	train and	test
    motivation:	simulate	conditions	of	applying	

system	in	practice

    but,	there   s	a	problem	with	this   

    we	need	to	explore	and	evaluate	methodological	

choices

    after	multiple	evaluations	on	test,	it	is	no	longer	a	

simulation	of	real-world	conditions

78

experimental	practice
    in	the	beginning,	we	just	had	data
    first	innovation:	split	into	train and	test
    motivation:	simulate	conditions	of	applying	

system	in	practice

    but,	there   s	a	problem	with	this   

    we	need	to	explore	and	evaluate	methodological	

choices

    after	multiple	evaluations	on	test,	it	is	no	longer	a	

simulation	of	real-world	conditions

79

experimental	practice

    we	need	to	explore/evaluate	methodological	choices
    what	should	we	do?

    some	use	cross	validation	on	train,	but	this	is	slow	and	

doesn   t	quite	simulate	real-world	settings	(why?)

    second	innovation:	divide	data	into	train,	test,	and	a	

third	set	called	development	or	validation
    use	development/validation	to	evaluate	choices
    then,	when	ready	to	write	the	paper,	evaluate	the	best	

model	on	test

    are	we	done	yet?		no!		there   s	still	a	problem

80

experimental	practice

    we	need	to	explore/evaluate	methodological	choices
    what	should	we	do?

    some	use	cross	validation	on	train,	but	this	is	slow	and	

doesn   t	quite	simulate	real-world	settings	(why?)

    second	innovation:	divide	data	into	train,	test,	and	a	
third	set	called	development (dev)	or	validation	(val)
    use	dev/val to	evaluate	choices
    then,	when	ready	to	write	the	paper,	evaluate	the	best	

model	on	test

    are	we	done	yet?		no!		there   s	still	a	problem:

    overfitting to	dev/val

81

experimental	practice

    we	need	to	explore/evaluate	methodological	choices
    what	should	we	do?

    some	use	cross	validation	on	train,	but	this	is	slow	and	

doesn   t	quite	simulate	real-world	settings	(why?)

    second	innovation:	divide	data	into	train,	test,	and	a	
third	set	called	development (dev)	or	validation	(val)
    use	dev/val to	evaluate	choices
    then,	when	ready	to	write	the	paper,	evaluate	the	best	

model	on	test

    are	we	done	yet?		no!		there   s	still	a	problem:

    overfitting to	dev/val

82

experimental	practice

    we	need	to	explore/evaluate	methodological	choices
    what	should	we	do?

    some	use	cross	validation	on	train,	but	this	is	slow	and	

doesn   t	quite	simulate	real-world	settings	(why?)

    second	innovation:	divide	data	into	train,	test,	and	a	
third	set	called	development (dev)	or	validation	(val)
    use	dev/val to	evaluate	choices
    then,	when	ready	to	write	the	paper,	evaluate	the	best	

model	on	test

    are	we	done	yet?		no!		there   s	still	a	problem:

    overfitting to	dev/val

83

experimental	practice

    best	practice:	split	data	into	train,	development	(dev),	

development	test	(devtest),	and	test
    train	model	on	train,	tune	hyperparameter values	on	dev,	
do	preliminary	testing	on	devtest,	do	final	testing	on	test	a	
single	time	when	writing	the	paper

    even	better	to	have	even	more	test	sets!	test1,	test2,	etc.

    experimental	credibility	is	a	huge	component	of	doing	

useful	research

    when	you	publish	a	result,	it	had	better	be	replicable	

without	tuning	anything	on	test

84

don   t	cheat!

85

