nlp
text similarity
id84
issues with vector similarity
polysemy (sim < cos)
bar, bank, jaguar, hot
synonymy (sim > cos)
building/edifice, large/big, spicy/hot
relatedness (people are really good at figuring this)
doctor/patient/nurse/treatment
semantic matching
which one should we rank higher?
query vocabulary & doc vocabulary mismatch!
if only we can represent documents/queries as concepts!
that   s where id84 helps
semantic concepts
semantic concepts
concept space = dimension reduction
number of concepts (k) is smaller than the number of words (n) or number of documents (m).
if we represent a document as a n-dimensional vector; and the corpus as an m*n matrix    
the goal is to reduce the dimensionality from n to k.
but how can we do that?  

toefl synonyms and sat analogies 
word similarity vs. analogies
example from peter turney
















29 degrees








29 degrees
vectors and matrices
a matrix is an m x n table of objects (in our case, numbers)
each row (or column) is a vector.
matrices of compatible dimensions can be multiplied together. 
what is the result of the multiplication below?
answer to the quiz
eigenvectors and eigenvalues
an eigenvector is an implicit    direction    for a matrix a


v (the eigenvector) is non-zero
   (the eigenvalue) can be any complex number, in principle.
computing eigenvalues:

eigenvectors and eigenvalues
example:


det (a-li) = (-1-l)*(-l)-3*2=0

then: l+l2-6=0;   l1=2;   l2=-3

for l1=2:


solutions: v1=v2
matrix decomposition
if s is a square matrix, it can be decomposed into ulu-1, where
     u = matrix of eigenvectors
	l = diagonal matrix of eigenvalues

su = ul
u-1su = l
s = ulu-1

example
svd: singular value decomposition
a=usvt
u is the matrix of orthogonal eigenvectors of aat
v is the matrix of orthogonal eigenvectors of ata (co-variance matrix)
the components of s are the eigenvalues of ata
properties
this decomposition exists for all matrices and is unique
u, v are column orthonormal
ut u = i; vt v = i
s is diagonal and sorted by absolute value of the singular values (large to small)
each column (row) of s corresponds to a principal component 
if a has 5 columns and 3 rows,  then u will be 5x5 and v will be 3x3
example (berry and browne)
t1: baby
t2: child
t3: guide
t4: health 
t5: home
t6: infant
t7: proofing
t8: safety
t9: toddler
d1: infant & toddler first aid
d2: babies & children   s room (for your home)
d3: child safety at home
d4: your baby   s health and safety: from infant to toddler
d5: baby proofing basics
d6: your guide to easy rust proofing
d7: beanie babies collector   s guide
example
d1: t6, t9
d2: t1, t2
d3: t2, t5, t8
d4: t1, t4, t6, t8, t9
d5: t1, t7
d6: t3, t7
d7: t1, t3
example
d1: t6, t9
d2: t1, t2
d3: t2, t5, t8
d4: t1, t4, t6, t8, t9
d5: t1, t7
d6: t3, t7
d7: t1, t3
d1
d2
d3
d4
d5
d6
d7
t2
t3
t4
t5
t6
t7
t8
t9
t1
document-term matrix
raw
normalized
svd decomposition
u =

[[-0.70 -0.09  0.02 -0.70  0.00  0.02  0.14 -0.00  0.00]
 [-0.26  0.30  0.47  0.20  0.00 -0.25 -0.16 -0.64  0.31]
 [-0.35 -0.45 -0.10  0.40  0.71 -0.01 -0.05  0.00  0.00]
 [-0.11  0.14 -0.15 -0.07 -0.00  0.48 -0.84  0.00 -0.00]
 [-0.26  0.30  0.47  0.20  0.00 -0.25 -0.16  0.64 -0.31]
 [-0.19  0.37 -0.50  0.13  0.00 -0.23  0.03 -0.31 -0.64]
 [-0.35 -0.45 -0.10  0.40 -0.71 -0.01 -0.05 -0.00  0.00]
 [-0.21  0.33  0.10  0.28  0.00  0.73  0.47 -0.00  0.00]
 [-0.19  0.37 -0.50  0.13  0.00 -0.23  0.03  0.31  0.64]]

v    =

[[-0.17 -0.45 -0.27 -0.40 -0.47 -0.32 -0.47]
 [ 0.42  0.23  0.42  0.40 -0.30 -0.50 -0.30]
 [-0.60  0.46  0.50 -0.39 -0.05 -0.12 -0.05]
 [ 0.23 -0.22  0.49 -0.13 -0.26  0.71 -0.26]
 [ 0.00  0.00  0.00  0.00 -0.71 -0.00  0.71]
 [-0.57 -0.49  0.25  0.61  0.01 -0.02  0.01]
 [ 0.24 -0.50  0.45 -0.37  0.34 -0.35  0.34]]


svd decomposition
s =

[[ 1.58  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  1.27  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  1.19  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.80  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.71  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.57  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.20]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]
 

svd decomposition
u*s*v    = 

[[ 0.00  0.58  0.00  0.45  0.71  0.00  0.71]
 [ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.71  0.71]
 [ 0.00  0.00  0.00  0.45  0.00  0.00  0.00]
 [ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
 [ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.71  0.71  0.00]
 [ 0.00  0.00  0.58  0.45  0.00  0.00  0.00]]



id84
low rank matrix approximation
a[m*n] = u[m*m]s[m*n]vt[n*n]
s is a diagonal matrix of eigenvalues
if we only keep the largest r eigenvalues 
a     u[m*r]s[r*r]vt[n*r]
rank-4 approximation of s
s =

[[ 1.58  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  1.27  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  1.19  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.80  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]
 

rank-4 approximation of a
u*s*v    = 

[[ 0.00  0.58  0.00  0.45  0.71  0.00  0.71]
 [ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.71  0.71]
 [ 0.00  0.00  0.00  0.45  0.00  0.00  0.00]
 [ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
 [ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.71  0.71  0.00]
 [ 0.00  0.00  0.58  0.45  0.00  0.00  0.00]

u*s4*v    = 

[[-0.00  0.60 -0.01  0.45  0.70  0.01  0.70]
 [-0.07  0.49  0.63  0.07  0.01 -0.01  0.01]
 [ 0.00 -0.01  0.01 -0.00  0.36  0.70  0.36]
 [ 0.20  0.05  0.01  0.22  0.05 -0.05  0.05]
 [-0.07  0.49  0.63  0.07  0.01 -0.01  0.01]
 [ 0.63 -0.06  0.03  0.53 -0.00  0.00 -0.00]
 [ 0.00 -0.01  0.01 -0.00  0.36  0.70  0.36]
 [ 0.22  0.25  0.43  0.23 -0.04  0.04 -0.04]
 [ 0.63 -0.06  0.03  0.53 -0.00  0.00 -0.00]]


rank-2 approximation of s
s =

[[ 1.58  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  1.27  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]
 

rank-2 approximation of a
u*s*v    = 

[[ 0.00  0.58  0.00  0.45  0.71  0.00  0.71]
 [ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.71  0.71]
 [ 0.00  0.00  0.00  0.45  0.00  0.00  0.00]
 [ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
 [ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.71  0.71  0.00]
 [ 0.00  0.00  0.58  0.45  0.00  0.00  0.00]

u*s2*v    = 

[[ 0.14  0.47  0.25  0.39  0.55  0.41  0.55]
 [ 0.23  0.27  0.27  0.31  0.08 -0.06  0.08]
 [-0.14  0.12 -0.09 -0.01  0.43  0.46  0.43]
 [ 0.10  0.12  0.12  0.14  0.03 -0.03  0.03]
 [ 0.23  0.27  0.27  0.31  0.08 -0.06  0.08]
 [ 0.25  0.24  0.28  0.31 -0.00 -0.14 -0.00]
 [-0.14  0.12 -0.09 -0.01  0.43  0.46  0.43]
 [ 0.23  0.24  0.27  0.30  0.03 -0.11  0.03]
 [ 0.25  0.24  0.28  0.31 -0.00 -0.14 -0.00]]

rank-2 representation
u*s2 = 

[[-1.10 -0.12  0.00  0.00  0.00  0.00  0.00]
 [-0.41  0.38  0.00  0.00  0.00  0.00  0.00]
 [-0.56 -0.57  0.00  0.00  0.00  0.00  0.00]
 [-0.18  0.18  0.00  0.00  0.00  0.00  0.00]
 [-0.41  0.38  0.00  0.00  0.00  0.00  0.00]
 [-0.30  0.47  0.00  0.00  0.00  0.00  0.00]
 [-0.56 -0.57  0.00  0.00  0.00  0.00  0.00]
 [-0.33  0.42  0.00  0.00  0.00  0.00  0.00]
 [-0.30  0.47  0.00  0.00  0.00  0.00  0.00]]

s2*v    = 

[[-0.26 -0.71 -0.42 -0.62 -0.74 -0.50 -0.74]
 [ 0.53  0.29  0.54  0.51 -0.38 -0.64 -0.38]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
 [ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]
t3,t7
d6
d7
t3,t7
d6
d7

t1

example
d1: t6, t9
d2: t1, t2
d3: t2, t5, t8
d4: t1, t4, t6, t8, t9
d5: t1, t7
d6: t3, t7
d7: t1, t3
d1
d2
d3
d4
d5
d6
d7
t2
t3
t4
t5
t6
t7
t8
t9
t1
semantic concepts
semantic concepts
quiz
can you explain why this graphic looks this way?
quiz
compare with this:
adding noise






quiz
let a be a document x term matrix.
what is a*a   ?
what about a   *a?
interpretation of svd
best direction to project on
the principal eigenvector is the dimension that explains most of the variance
finding hidden concepts
mapping documents, terms to a lower-dimensional space
turning the matrix into block-diagonal form
(same as finding bi-partite cores)
in the nlp/ir literature, svd is called lsa (lsi)
latent semantic analysis (indexing)
keep as many dimensions as necessary to explain 80-90% of the data (energy)
in practice, use 300 dimensions or so

fmri example
fmri
functional mri (magnetic resonance imaging)
used to measure activity in different parts of the brain when exposed to various stimuli
factor analysis 
paper
just, m. a., cherkassky, v. l., aryal, s., & mitchell, t. m. (2010). a neurosemantic theory of concrete noun representation based on the underlying brain codes. plos one, 5, e8622

[just et al. 2010]
[just et al. 2010]
[just et al. 2010]
external pointers
http://lsa.colorado.edu 
http://www.cs.utk.edu/~lsi 

example of lsi


data
inf
retrieval
brain
lung



=

cs
md





x
x


cs-concept
md-concept



term rep of concept

strength of cs-concept

dim. reduction

        a            =      u              l             vt
[example modified from christos faloutsos]
mapping queries and docs to the same space
qtconcept = qt v                
dtconcept = dt v


=


similarity with 
cs-concept

cs-concept




dt=
0  1   1  0   0



1.16   0
[example modified from christos faloutsos]
nlp
