ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	3:	words

1

    assignment	1	has	been	posted
    due	11:59	pm	on	wednesday,	january	20th

    we	will	start	class	5	minutes	late	from	now	on,	due	to	several	

students	taking	algorithms	across	campus

    my	office	hours	are	mondays	3-4pm,	#531	(or	by	appointment)
    ta	office	hours	are	thursdays	4-5pm,	#501

   

   

if	you   re	auditing,	you	may	still	turn	in	the	homework	and	we	will	
give	you	feedback	(though	we	may	not	give	your	homework	as	
much	attention	as	others)

if	you	didn   t	receive	an	email	from	me	this	details,	then	please	
email	me	with	your	name/email	and	let	me	know	whether	you	are	
taking	course	for	credit

2

today

    review	of	loss	functions	and	subgradients

from	last	week	(useful	for	homework)

    start	words	(more	about	words	and	lexical	

semantics	on	thursday)

3

empirical	risk	minimization	with	surrogate	loss	functions

    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

4

empirical	risk	minimization	with	surrogate	loss	functions

    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

many	possible	loss	
functions	to	consider	

optimizing

5

cost	functions

    cost	function:	scores	output	against	a	gold	standard

    should	reflect	the	evaluation	metric	for	your	task

    usual	convention:
    for	classification,	what	cost	should	we	use?

    how	about	for	other	nlp	tasks?

6

surrogate	loss	functions

cost	loss	/	0-1	loss:

max-score	loss:

7

visualization

e
r
o
c
s

five	possible	outputs

8

visualization

t
s
o
c

five	possible	outputs

9

visualization

t
s
o
c

gold	standard

10

visualization

t
s
o
c

gold	standard

11

visualization

t
s
o
c
	
+
e
r
o
c
s

	

gold	standard

12

13

e
r
o
c
s

gold	standard

14

e
r
o
c
s

effect	of	learning?

gold	standard

15

e
r
o
c
s

effect	of	learning:

score	of	gold	standard	

will	go	to	infinity

gold	standard

16

id88	loss:

17

id88	loss:

e
r
o
c
s

gold	standard

18

id88	loss:

e
r
o
c
s

gold	standard

19

id88	loss:

e
r
o
c
s

effect	of	learning?

gold	standard

20

id88	loss:

e
r
o
c
s

effect	of	learning:
gold	standard	will	
have	highest	score

gold	standard

21

hinge	loss:

22

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

gold	standard

23

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

gold	standard

24

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

effect	of	learning?

gold	standard

25

hinge	loss:

t
s
o
c
	
+
e
r
o
c
s

	

effect	of	learning:

score	of	gold	standard	

will	be	higher	than	
score+cost of	all	

others

gold	standard

26

subgradients of	loss	functions
    some	of	our	loss	functions	are	not	differentiable:

    but	they	are	subdifferentiable:

27

subgradient examples

28

subgradient examples

29

subgradient examples

    to	find	a	subgradient of	max	

of	convex	functions	at	a	point,	
choose	one	function	that	
achieves	the	max	at	that	point	
and	choose	any	of	its	
subgradients at	the	point

30

subgradients of	loss	functions
    some	of	our	loss	functions	are	not	differentiable:

    but	they	are	subdifferentiable:

31

subgradients of	loss	functions
    some	of	our	loss	functions	are	not	differentiable:

    but	they	are	subdifferentiable:

32

subgradients of	loss	functions

find	subgradient of	the	

function	that	achieves	the	max

33

subgradients of	loss	functions

find	subgradient of	the	

function	that	achieves	the	max

34

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

35

words

    what	is	a	word?
    id121
    morphology
    word	sense

36

what	is	a	word?

37

id121

    id121:	convert	a	character	stream	into	

words	by	adding	spaces

    for	certain	languages,	highly	nontrivial
    e.g.,	chinese	word	segmentation	is	a	widely-

studied	nlp	task

38

id121

    for	other	languages	(english),	id121	is	

easier	but	is	still	not	always	obvious

    the	data	for	your	homework	has	been	

tokenized:
    punctuation	has	been	split	off	from	words
    contractions	have	been	split

39

intricacies	of	id121

    separating	punctuation	characters	from	words?

    ,	   	?	!		   always	separate
    .    when	shouldn   t	we	separate	it?

    dr.,	mr.,	prof.,	u.s.,	etc.
    english	contractions:

    isn   t,	aren   t,	wasn   t,   	   is	n   t,	are	n   t,	was	n   t,   
    but	how	about	these:	can   t,	won   t	   ca n   t,	wo n   t
    ca and	wo are	then	different	forms	from	can and	will

40

intricacies	of	id121

    separating	punctuation	characters	from	words?

    ,	   	?	!		   always	separate
    .    when	shouldn   t	we	separate	it?

    dr.,	mr.,	prof.,	u.s.,	etc.
    english	contractions:

    isn   t,	aren   t,	wasn   t,   	   is	n   t,	are	n   t,	was	n   t,   
    but	how	about	these:	can   t,	won   t	   ca n   t,	wo n   t
    ca and	wo are	then	different	forms	from	can and	will

41

intricacies	of	id121

    separating	punctuation	characters	from	words?

    ,	   	?	!		   always	separate
    .    when	shouldn   t	we	separate	it?

    dr.,	mr.,	prof.,	u.s.,	etc.
    english	contractions:

    isn   t,	aren   t,	wasn   t,   	   is	n   t,	are	n   t,	was	n   t,   
    but	how	about	these:	can   t,	won   t	   ca n   t,	wo n   t
    ca and	wo are	then	different	forms	from	can and	will

42

    chinese	and	japanese:	no	spaces	between	words:

                                                                
                                                                    
    sharapova now	

lives	in				 us							southeastern					florida

    further	complicated	in	japanese,	with	multiple	alphabets	

intermingled
    dates/amounts	in	multiple	formats

                  500                                       $500k(   6,000      )

katakana

hiragana

kanji

romaji

end-user	can	express	query	entirely	in	hiragana!

j&m/slp3

word	segmentation	in	chinese
    chinese	words	are	composed	of	characters

    characters	are	generally	1	syllable	and	1	morpheme
    average	word	is	2.4	characters	long

    standard	baseline	segmentation	algorithm:	

    maximum	matching	 (also	called	greedy)

j&m/slp3

maximum	matching

word	segmentation	algorithm

given	a	chinese	word	list	and	a	string:
1) start	a	pointer	at	the	beginning	of	the	string
2) find	longest	word	in	dictionary	that	matches	

the	string	starting	at	pointer

3) move	the	pointer	over	the	word	in	string
4) go	to	2

j&m/slp3

maximum	matching	examples

    thecatinthehat
    thetabledownthere

the	cat	in	the	hat
the	table	down	there
theta	bled	own	there

    doesn   t	generally	work	in	english!

    but	works	astonishingly	well	in	chinese
                                                                
                                                                    
    modern	probabilistic	segmentation	algorithms	

even	better

j&m/slp3

effect	on	machine	translation

chang,	galley,	&	manning	(2008)

47

removing	spaces?

    id121	is	usually	about	adding	spaces
    but	might	we	also	want	to	remove	spaces?
    what	are	some	english	examples?

    names?

    new	york	   newyork

    non-compositional	compounds?

    hot	dog	   hotdog

    other	artifacts	of	our	spacing	conventions?

    new	york-long	island	railway

48

removing	spaces?

    id121	is	usually	about	adding	spaces
    but	might	we	also	want	to	remove	spaces?
    what	are	some	english	examples?

    names?

    new	york	   newyork

    non-compositional	compounds?

    hot	dog	   hotdog

    other	artifacts	of	our	spacing	conventions?

    new	york-long	island	railway

49

types	and	tokens

    once	text	has	been	tokenized,	let   s	count	the	words
    types:	entries	in	the	vocabulary
    tokens:	instances	of	types	in	a	corpus
    example	sentence:	if	they	want	to	go	,	they	should	go	.

    how	many	types?		
    how	many	tokens?		

    type/token	ratio:	useful	statistic	of	a	corpus	(here,	0.8)
    as	we	add	data,	what	happens	to	the	type-token	ratio?		
    indicates	what?

    high	type/token	ratio	   rich	morphology
    low	type/token	ratio	   poor	morphology

50

types	and	tokens

    once	text	has	been	tokenized,	let   s	count	the	words
    types:	entries	in	the	vocabulary
    tokens:	instances	of	types	in	a	corpus
    example	sentence:	if	they	want	to	go	,	they	should	go	.

    how	many	types?		8
    how	many	tokens?		10

    type/token	ratio:	useful	statistic	of	a	corpus	(here,	0.8)
    as	we	add	data,	what	happens	to	the	type-token	ratio?		
    indicates	what?

    high	type/token	ratio	   rich	morphology
    low	type/token	ratio	   poor	morphology

51

types	and	tokens

    once	text	has	been	tokenized,	let   s	count	the	words
    types:	entries	in	the	vocabulary
    tokens:	instances	of	types	in	a	corpus
    example	sentence:	if	they	want	to	go	,	they	should	go	.

    how	many	types?		8
    how	many	tokens?		10

    type/token	ratio:	useful	statistic	of	a	corpus	(here,	0.8)
    as	we	add	data,	what	happens	to	the	type-token	ratio?		
    indicates	what?

    high	type/token	ratio	  
    low	type/token	ratio	  

52

types	and	tokens

    once	text	has	been	tokenized,	let   s	count	the	words
    types:	entries	in	the	vocabulary
    tokens:	instances	of	types	in	a	corpus
    example	sentence:	if	they	want	to	go	,	they	should	go	.

    how	many	types?		8
    how	many	tokens?		10

    type/token	ratio:	useful	statistic	of	a	corpus	(here,	0.8)
    as	we	add	data,	what	happens	to	the	type-token	ratio?		
    indicates	what?

    high	type/token	ratio	   rich	morphology
    low	type/token	ratio	   poor	morphology

53

how	many	words	are	there?

    how	many	english	words	exist?
    when	we	increase	the	size	of	our	corpus,	what	

happens	to	the	number	of	types?
    a	bit	surprising:	vocabulary	continues	to	grow	in	

any	actual	dataset

    you   ll	just	never	see	all	the	words

    zipf   s law:	frequency	of	a	word	is	inversely	

proportional	to	its	rank

54

how	many	words	are	there?

    how	many	english	words	exist?
    when	we	increase	the	size	of	our	corpus,	what	

happens	to	the	number	of	types?
    a	bit	surprising:	vocabulary	continues	to	grow	in	

any	actual	dataset

    you   ll	just	never	see	all	the	words
    in	1	million	tweets,	15m	tokens,	600k	types
    in	56	million	tweets,	847m	tokens,	?	types

55

how	many	words	are	there?

    how	many	english	words	exist?
    when	we	increase	the	size	of	our	corpus,	what	

happens	to	the	number	of	types?
    a	bit	surprising:	vocabulary	continues	to	grow	in	

any	actual	dataset

    you   ll	just	never	see	all	the	words
    in	1	million	tweets,	15m	tokens,	600k	types
    in	56	million	tweets,	847m	tokens,	11m	types

56

how	are	words	distributed?

    zipf   s law:	frequency	of	a	word	is	inversely	

proportional	to	its	rank

blog	emis

57

how	are	words	distributed?

    zipf   s law:	frequency	of	a	word	is	inversely	

proportional	to	its	rank

   long	tail   

blog	emis

58

zipf   s law

    also	predicts	other	kinds	of	data:	population	of	cities	in	a	

country,	revenue	of	different	companies,	etc.

the	laurentian	
university	sports	
analytics	group

59

the	long	tail
    there	are	so	many	word	types!
    but	words	have	internal	structure

60

morphology

    morphemes:

    the	small	meaningful	units	that	make	up	words
    stems:	core	meaning-bearing	units
    affixes:	bits	and	pieces	that	adhere	to	stems

    often	with	grammatical	functions

j&m/slp3

kinds	of	word	formation

    inflection:	modifying	a	word	with	an	affix	to	

change	its	grammatical	function	(tense,	
number,	etc.)
    result	is	a	   different	form	of	the	same	word   
    examples:	book    books,	walk	   walked

    derivation:	adding	an	affix	to	a	stem	to	create	

a	new	word
    examples:	great    greatly,	great    greatness

    compounding:	combining	two	stems
    examples:	lawsuit,	keyboard,	bookcase

62

kinds	of	word	formation

    inflection:	modifying	a	word	with	an	affix	to	

change	its	grammatical	function	(tense,	
number,	etc.)
    result	is	a	   different	form	of	the	same	word   
    examples:	book    books,	walk	   walked

    derivation:	adding	an	affix	to	a	stem	to	create	

a	new	word
    examples:	great    greatly,	great    greatness

    compounding:	combining	two	stems
    examples:	lawsuit,	keyboard,	bookcase

63

kinds	of	word	formation

    inflection:	modifying	a	word	with	an	affix	to	

change	its	grammatical	function	(tense,	
number,	etc.)
    result	is	a	   different	form	of	the	same	word   
    examples:	book    books,	walk	   walked

    derivation:	adding	an	affix	to	a	stem	to	create	

a	new	word
    examples:	great    greatly,	great    greatness

    compounding:	combining	two	stems
    examples:	lawsuit,	keyboard,	bookcase

64

morphology

    usually,	morphological	derivation	is	simply	

splitting	a	word	into	its	morphemes:
    walked	=	walk	+	ed
    greatness	=	great	+	ness

    but	it	can	actually	be	a	hierarchical	structure

65

morphology

    ambiguity	in	hierarchical	morphological	

decomposition?
    rare,	but	it	does	happen
    unlockable =	un	+	lock	+	able

    what	does	this	word	mean?
    (un+lock)+able:	   able	to	be	unlocked   
    un+(lock+able):	   unable	to	be	locked   

66

morphology

    ambiguity	in	hierarchical	morphological	

decomposition?
    rare,	but	it	does	happen
    unlockable =	un	+	lock	+	able

    what	does	this	word	mean?
    (un+lock)+able:	   able	to	be	unlocked   
    un+(lock+able):	   unable	to	be	locked   

67

morphology

    ambiguity	in	hierarchical	morphological	

decomposition?
    rare,	but	it	does	happen
    unlockable =	un	+	lock	+	able

    what	does	this	word	mean?
    (un+lock)+able:	   able	to	be	unlocked   
    un+(lock+able):	   unable	to	be	locked   

68

morphology	in	nlp

    two	common	tasks:

    lemmatization
    id30

69

lemmatization

    lemmatization:	reduce	inflections	or	variant	

forms	to	base	form
    am,	are, is	    be
    car,	cars,	car's,	cars'     car

    the	boy's	cars	are	different	colors     the	boy	car	

be	different	color

    have	to	find	correct	dictionary	headword	form
    e.g.,	for	machine	translation:

    spanish	quiero (   i	want   ),	quieres (   you	want   )	same	

lemma	as	querer    want   

j&m/slp3

id30

    id30:	reduces	words	to	their	stems	via	

crude	chopping	of	affixes
    e.g.,	automate(s),	automatic,	automation all	

reduced	to	automat
    language	dependent
    key	step	in	information	retrieval

for	example	compressed	
and	compression	are	both	
accepted	as	equivalent	to	
compress.

for	exampl compress	and
compress	ar both	accept
as	equival to	compress

j&m/slp3

porter   s	algorithm

the	most	common	english	stemmer

step	1a

sses     ss
ies     i
ss     ss
s          

caresses     caress
ponies       poni
caress       caress
cats          cat

step	1b

step	2	(for	long	stems)

ational    ate relational    relate
izer    ize
ator    ate
   

digitizer     digitize
operator      operate

step	3	(for	longer	stems)

(*v*)ing        walking       walk
sing          sing

(*v*)ed        plastered     plaster
   

al          
able        
ate         
   

revival        reviv
adjustable     adjust
activate       activ

j&m/slp3

viewing	morphology	in	a	corpus

why	only	strip	   ing if	there	is	a	vowel?

(*v*)ing        walking       walk
sing          sing

j&m/slp3

73

viewing	morphology	in	a	corpus

why	only	strip	   ing if	there	is	a	vowel?

(*v*)ing        walking       walk
sing          sing

tr -sc 'a-za-z'  '\n' < shakes.txt | grep    ing$'  | sort | uniq -c | sort    nr 

1312 king
548 being
541 nothing
388 king
375 bring
358 thing
307 ring
152 something
145 coming
130 morning 

548 being
541 nothing
152 something
145 coming
130 morning
122 having
120 living
117 loving
116 being
102 going

tr -sc 'a-za-z' '\n' < shakes.txt | grep '[aeiou].*ing$' | sort | uniq -c | sort    nr

j&m/slp3

74

dealing	with	complex	morphology	is	

sometimes	necessary

    some	languages	requires	complex	morpheme	

segmentation
    turkish
    uygarlastiramadiklarimizdanmissinizcasina:    (behaving)	as	if	

you	are	among	those	whom	we	could	not	civilize   

    uygar `civilized   	+	las `become   	

+	tir `cause   	+	ama `not	able   	
+	dik `past   	+	lar    plural   
+	imiz    p1pl   	+	dan    abl   	
+	mis    past   	+	siniz    2pl   	+	casina    as	if   	

j&m/slp3

terminology:	lemma and	wordform

    lemma	or	citation	form

    same	stem,	part	of	speech,	rough	semantics

    wordform

    inflected	word	as	it	appears	in	text

wordform

banks
sung

duermes

lemma
bank
sing
dormir

j&m/slp3

