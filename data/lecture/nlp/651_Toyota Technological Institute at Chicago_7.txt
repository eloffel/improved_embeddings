ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	7:	sequence	models

1

announcements

    assignment	2	has	been	posted,	due	feb.	3
    midterm	scheduled	for	thursday,	feb.	18
    project	proposal	due	tuesday,	feb.	23
    thursday   s	class	will	be	more	like	a	lab	/	

flipped	class
    we	will	use	the	whiteboard	and	implement	things	

in	class,	so	bring	paper,	laptop,	etc.

2

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

3

language	modeling

    goal:	compute	the	id203	of	a	sequence	of	words:

markov	assumption	for	

language	modeling

andrei	markov

j&m/slp3

intuition	of	smoothing	(from	dan	klein)

    when	we	have	sparse	statistics:

p(w	|	denied	the)
3	allegations
2	reports
1	claims
1	request
7	total

   

steal	id203	mass	to	generalize	better:

p(w	|	denied	the)
2.5	allegations
1.5	reports
0.5	claims
0.5	request
2	other
7	total

s
n
o
i
t
a
g
e

l
l

a

s
t
r
o
p
e
r

s
n
s
o
n
i
o
t
a
i
t
g
a
e
g
e
a
l
l

l
l

a

s
t
r
o
p
e
r

s

m
a

i

l
c

t
s
e
u
q
e
r

k
c
a

t
t

a

n
a
m

e    

m
o
c
t
u
o

k
c
a

t
t

a

n
a
m

t
s
e
u
q
e
r

e    

m
o
c
t
u
o

s

i

m
a
l
c

   add-1   	estimation

    also	called	laplace	smoothing
    just	add	1	to	all	counts!

j&m/slp3

backoff and	interpolation
    sometimes	it	helps	to	use	less context

    condition	on	less	context	for	contexts	you	haven   t	

learned	much	about	

    backoff:	

    use	trigram	if	you	have	good	evidence,	otherwise	

bigram,	otherwise	unigram

    interpolation:	

    mixture	of	unigram,	bigram,	trigram	(etc.)	models

    interpolation	works	better

j&m/slp3

in simple linear interpolation, we combine different order id165s by linearly

interpolating all the models. thus, we estimate the trigram id203 p(wn|wn 2wn 1)
+l2p(wn|wn 1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
+l3p(wn)

linear	interpolation

  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)

    simple	interpolation:

such that the ls sum to 1:
  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)

li = 1

xi

such that the ls sum to 1:

+l2p(wn|wn 1)
+l3p(wn)

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in

li = 1

(4.25)

(4.24)

xi

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in
j&m/slp3

kneser-ney	smoothing

    better	estimate	for	probabilities	of	lower-order	unigrams!
    shannon	game:		i	can   t	see	without	my	reading___________?
       francisco   	is	more	common	than	   glasses   
       	but	   francisco   	always	follows	   san   

    unigram	is	most	useful	when	we	haven   t	seen	bigram!
    so	instead	of	unigram	p(w)	(   how	likely	is	w?   )
    use	pcontinuation(w) (   how	likely	is	w to	appear	as	a	novel	

continuation?   )
    for	each	word,	count	#	of	bigram	types	it	completes:
pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

j&m/slp3

kneser-ney	smoothing

    how	many	times	does	w appear	as	a	novel	continuation?
pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

    normalize	by	total	number	of	

word	bigram	types:

{(wj   1,wj):c(wj   1,wj) > 0}

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}

{(wj   1,wj):c(wj   1,wj) > 0}

j&m/slp3

id165	smoothing	summary

    add-1	estimation:

    ok	for	text	categorization,	not	for	language	modeling

    for	very	large	id165	collections	like	the	web:

    stupid	backoff

    most	commonly	used	method:
    modified	interpolated	kneser-ney

j&m/slp3

12

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

13

linguistic	phenomena:	summary	so	far   
    words	have	structure	(stems and	affixes)	
    words	have	multiple	meanings	(senses)	   word	

sense	ambiguity
    senses	of	a	word	can	be	homonymous	or	polysemous
    senses	have	relationships:

    hyponymy (   is	a   )
    meronymy (   part	of   ,	   member	of   )

    variability/flexibility	of	linguistic	expression

    many	ways	to	express	the	same	meaning	(as	you	saw	

in	assignment	1)

    word	vectors	tell	us	when	two	words	are	similar

    today:	part-of-speech

14

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun

some						questioned						if							tim						cook						   s						first						product	

modal							verb				det.									adjective									noun				prep.						proper					punc.
would						be						a						breakaway						hit						for						apple								.

15

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

16

part-of-speech	(pos)

    functional	category	of	a	word:

    noun,	verb,	adjective,	etc.
    how	is	the	word	functioning	in	its	context?

    dependent	on	context	like	word	sense,	but	

different	from	sense:
    sense	represents	word	meaning,	pos	represents	

word	function

    sense	uses	a	distinct	category	of	senses	per	word,	

pos	uses	same	set	of	categories	for	all	words

17

penn	

treebank	
tag	set

18

universal	tag	set

    many	use	smaller	sets	of	coarser	tags
    e.g.,	   universal	tag	set   	containing	12	tags:

    noun,	verb,	adjective,	adverb,	pronoun,	

determiner/article,	adposition (preposition	or	
postposition),	numeral,	conjunction,	particle,	
punctuation,	other

petrov,	das,	mcdonald	 (2011)

19

twitter	part-of-speech	tagging

other																					verb																			article																					noun															pronoun	

pronoun	 																					prep																	adj

prep																verb	
intj
ikr smh he		asked		fir		yo last		name		so		he		can	
add		u		on		fb lololol =d					#lolz
emoticon				hashtag
verb														prep																			intj

pronoun	 					proper
noun

adj =	adjective
prep	=	preposition
intj =	interjection

    we	removed	some	fine-grained	pos	tags,	then	added	

twitter-specific	tags:
hashtag
@-mention
url	/	email	address
emoticon
twitter	discourse	marker
other	(multi-word	abbreviations,	symbols,	garbage)

20

word	sense	vs.	part-of-speech

semantic or	syntactic?

indicates	meaning	of	word	in	its	

indicates	function	 of	word	in	its	

word	sense
semantic:

context

part-of-speech

syntactic:	

context

number	of	categories

|v|	words,	~5	senses	each	  

5|v|	categories!

typical	pos tag	sets	have	12	to	

45	tags

inter-annotator

agreement

low; some	sense	distinctions	

are	highly	subjective

high;	relatively	few	pos tags	

and	function	is	relatively	
shallow	/	surface-level

independent	or	joint	
classification	of	nearby	

words?

independent:	

can	classify	a	single	word	based	
on	context	words;	structured	

prediction	is	rarely	used

strong	relationship	between	

tags	of	nearby	words;	

structured	prediction	often	

joint:

used

21

how	might	pos	tags	be	useful?

    text	classification
    machine	translation
    question	answering

22

classification	framework

id136:	solve														_

modeling:	define		score	function

learning:	choose	_

23

applications	of	our	classification	framework

text	classification:

=	{objective,	subjective}

x

the	hulk	is	an	anger	fueled	monster	with	
incredible	strength	and	resistance	to	damage	.
in	trying	to	be	daring	and	original	,	it	comes	off	
as	only	occasionally	satirical	and	never	fresh	.

y

objective

subjective

24

applications	of	our	classification	framework

word	sense	classifier	for	bass:

=	{bass1,	bass2,	   ,	bass8}

x

he   s	a	bass	in	the	choir	.

our bass	is	line-caught	from	the	
atlantic	.

y

bass3

bass4

25

applications	of	our	classification	framework

skip-gram	model	as	a	classifier:

x

agriculture

agriculture

agriculture

y

<s>

is

the

=	v (the	entire	vocabulary)

corpus	(english	wikipedia):
agriculture	is	the	traditional	mainstay	of	the	
cambodian economy	.
but	benares has	been	destroyed	by	an	earthquake	.
   

26

applications	of	our	classifier	framework	so	far

task

input	(x)

output	(y)

text	

classification

a	sentence

gold	standard	

label for	x

output	space	(					)
pre-defined,	 small	

label	set (e.g.,	

{positive,	negative})

word	sense	
disambiguation

learning skip-
gram	word	
embeddings

part-of-speech	

tagging

instance	of	a	
particular	word	
(e.g.,	bass)	with

its	context

instance	of	a	
word	in	a	corpus

a	sentence

gold	standard	
word	sense	of	x

a	word	in	the	
context	of	x in	

a	corpus

gold	standard	
part-of-speech	

tags	for	x

pre-defined	sense	
inventory	from	
id138 for	bass

vocabulary

all	possible	part-of-
speech tag	sequences	
with	same	length	as	x

size	of

2-10

2-30

|v|

|p||x|

27

applications	of	our	classifier	framework	so	far

task

input	(x)

output	(y)

text	

classification

a	sentence

gold	standard	

label for	x

output	space	(					)
pre-defined,	 small
label	set (e.g.,	

{positive,	negative})

size	of

2-10

word	sense	
disambiguation

learning skip-
gram	word	
embeddings

part-of-speech	

tagging

instance	of	a	
particular	word	
(e.g.,	bass)	with

its	context

instance	of	a	
word	in	a	corpus

gold	standard	
word	sense	of	x

pre-defined	sense	
inventory	from	
id138 for	bass

exponential	in	size	of	input!
|v|

   structured	prediction   

a	word	in	the	
context	of	x in	

vocabulary

2-30

a	corpus

a	sentence

gold	standard	
part-of-speech	

tags	for	x

all	possible	part-of-
speech tag	sequences	
with	same	length	as	x

|p||x|

28

simplest	kind	of	structured	prediction:	sequence	labeling

part-of-speech	tagging

determiner					verb	(past)						prep.			proper					proper			poss.					adj.													noun
determiner					verb	(past)						prep.				noun								noun					poss.					adj.												noun

some						questioned						if							tim						cook						   s						first						product	
modal							verb				det.									adjective									noun				prep.						proper					punc.
modal							verb				det.									adjective									noun				prep.							noun						punc.
would						be						a						breakaway						hit						for						apple								.

named	entity	recognition

some	questioned	if	tim	cook   s	first	product	would	be	a	breakaway	hit	for	apple.

person

organization

29

learning

learning:	choose	_

30

empirical	risk	minimization	with	surrogate	loss	functions
    given	training	data:																																

where	each

is	a	label
    we	want	to	solve	the	following:

many	possible	loss	
functions	to	consider	

optimizing

31

loss	functions

loss

name

cost	(   0-1   )

id88

hinge

log

where	used
intractable,	but	

underlies	   direct	error	

minimization   

id88	algorithm
(rosenblatt,	1958)

support	vector	

machines,	other	large-

margin	algorithms
logistic	regression,	
conditional	random	
fields,	maximum
id178	models

32

(sub)gradients	of	losses	for	linear	models

entry	j of	(sub)gradient	of	loss for	linear	model

not	subdifferentiable in	general

name

cost	(   0-1   )

id88

hinge

log

whatever	loss	is	used	during	training,	
classify (not costclassify)	is	used	to	

predict	labels	for	dev/test	data!

33

(sub)gradients	of	losses	for	linear	models

entry	j of	(sub)gradient	of	loss for	linear	model

not	subdifferentiable in	general

name

cost	(   0-1   )

id88

hinge

log

expectation	of	feature	value	with	respect	to	distribution	
over	y (where	distribution	 is	defined	by	theta)
alternative	notation:

34

sequence	models

    models	that	assign	scores	(could	be	

probabilities)	to	sequences

    general	category	that	includes	many	models	

used	widely	in	practice:
    id165	language	models
    hidden	markov	models
       chain   	conditional	random	fields
    maximum	id178	markov	models

35

hidden	markov	models	(id48s)

    id48s	define	a	joint	id203	distribution	over	

input	sequences	x and	output	sequences	y:

    conditional	independence	assumptions	(   markov	

assumption   )	are	used	to	factorize	this	joint	
distribution	into	small	terms

    widely	used	in	nlp,	speech	recognition,	

bioinformatics,	many	other	areas

36

hidden	markov	models	(id48s)
    id48s	define	a	joint	id203	distribution	over	

input	sequences	x and	output	sequences	y:

    assumption:	output	sequence	y    generates   	input	

sequence	x:

    these	are	too	difficult	to	estimate,	let   s	use	markov	

assumptions

37

markov	assumption	for	

language	modeling

andrei	markov

trigram	model:

independence	and	conditional	independence
    independence:	two	random	variables	x and	y are	

independent	if:

(or																																		)

for	all	values	x and	y

    conditional	independence:	two	random	variables	x

and	y are	conditionally	independent	given	a	third	
variable	z if	

for	all	values	of	x,	y,	and	z

(or

)

39

markov	assumption	for	

language	modeling

andrei	markov

trigram	model:

conditional	independence	assumptions	of	id48s
    two	y   s	are	conditionally	independent	given	

the	y   s	between	them:

    an	x at	position	i is	conditionally	independent	

of	other	y   s	given	the	y at	position	i:	

41

graphical	model	for	an	id48

(for	a	sequence	of	length	4)

y1

x1

y2

x2

y3

x3

y4

x4

a	graphical	model	is	a	graph	in	which:

each	node	corresponds	to	a	random	variable

each	directed	edge	corresponds	to	a	conditional	id203	distribution	
of	the	target	node	given	the	source	node

conditional	independence	statements	among	random	variables	are	
encoded	by	the	edge	structure

42

graphical	model	for	an	id48

(for	a	sequence	of	length	4)

y1

x1

y2

x2

y3

x3

y4

x4

conditional	independence	statements	among	random	variables	are	
encoded	by	the	edge	structure	   we	only	have	to	worry	about	local	
distributions:

transition	parameters:

emission	parameters:	

43

graphical	model	for	an	id48

(for	a	sequence	of	length	4)

y1

x1

y2

x2

y3

x3

y4

x4

transition	parameters:

emission	parameters:	

44

   brown	id91   

class-based  id165 models  of natural 
language 

peter  f.  brown" 
peter  v.  desouza* 
peter f. brown and vincent j. della pietra  class-based id165 models of natural language 
robert  l.  mercer* 
ibm t. j. watson research center 

vincent j.  della  pietra* 
jenifer  c.  lai* 

friday monday thursday wednesday tuesday saturday sunday weekends sundays saturdays 
we address the problem of predicting a word from previous words in a sample of text. in particular, 
june march july april january december october november september august 
we discuss id165 models based on classes of words. we also discuss several statistical algorithms 
people guys folks fellows ceos chaps doubters commies unfortunates blokes 
for assigning words to classes based on the frequency of their co-occurrence with other words. we 
find that we are able to extract classes that have the flavor of either syntactically  based groupings 
down backwards ashore sideways southward northward overboard aloft downwards adrift 
or semantically based groupings, depending on the nature of the underlying  statistics. 
water gas coal liquid acid sand carbon steam shale iron 
great big vast sudden mere sheer gigantic lifelong scant colossal 
1.  introduction 
man woman boy girl lawyer doctor guy farmer teacher citizen 
in a number of natural language processing tasks, we face the problem of recovering a 
american indian european japanese german african catholic israeli italian arab 
computational	 linguistics,	1992
string of english words after it has been garbled by passage through a noisy channel. 
pressure temperature  permeability density porosity stress velocity viscosity gravity tension 
to tackle this problem successfully, we must be able to estimate the id203 with 
which any particular string of english words will be presented as input to the noisy 
mother wife father son husband brother daughter sister boss uncle 
channel. in this paper, we discuss a method for making such estimates. we also discuss 
machine device controller processor cpu printer spindle subsystem compiler plotter 
the  related  topic  of assigning words  to  classes according to  statistical behavior in  a 
large body of text. 
john george james bob robert paul william jim david mike 
in the next section, we review the concept of a language model and give a defini- 
tion of id165 models. in section 3, we look at the subset of id165 models in which 
anyone someone anybody somebody 
the words are divided into classes. we show that for n  =  2 the maximum likelihood 
feet miles pounds degrees inches barrels tons acres meters bytes 
assignment of words to classes is equivalent to the assignment for which the average 

45

brown	id91		(brown	et	al.,	1992)

hidden	markov	model	with	one-cluster-per-word	constraint

y1

y2

y3

y4

justin

bieber

for												president

46

brown	id91		(brown	et	al.,	1992)

hidden	markov	model	with	one-cluster-per-word	constraint

y1

y2

y3

y4

justin

bieber

for												president

algorithm:
   initialize	each	word	as	its	own	cluster
   greedily	merge	clusters	to	improve	data	likelihood

47

brown	id91		(brown	et	al.,	1992)

hidden	markov	model	with	one-cluster-per-word	constraint

y1

y2

y3

y4

justin

bieber

for												president

algorithm:
   initialize	each	word	as	its	own	cluster
   greedily	merge	clusters	to	improve	data	likelihood

outputs	hierarchical	id91

48

we	induced	1000	brown	clusters	from	56	million	
english	tweets	(1	billion	words)

only	words	that	appeared	at	least	40	times

(owoputi,	o   connor,	dyer,	gimpel,	schneider,	and	smith,	2013)

49

example	cluster

missed	loved	hated	misread	admired	
underestimated	resisted	adored	disliked	
regretted	missd fancied	luved prefered luvd
overdid	mistyped	misd misssed looooved
misjudged	lovedd loooved loathed	lurves lovd

50

example	cluster

missed	loved	hated	misread	admired	
underestimated	resisted	adored	disliked	
regretted	missd fancied	luved prefered luvd
overdid	mistyped	misd misssed looooved
misjudged	lovedd loooved loathed	lurves lovd

spelling
variation

51

   really   

really	rly	realy genuinely	rlly reallly realllly
reallyy rele realli relly reallllly reli reali sholl rily
reallyyy reeeeally realllllly reaally reeeally rili
reaaally reaaaally reallyyyy rilly reallllllly
reeeeeally reeally shol realllyyy reely relle
reaaaaally shole really2 reallyyyyy _really_
realllllllly reaaly realllyy reallii reallt genuinly relli
realllyyyy reeeeeeally weally reaaallly reallllyyy
reallllllllly reaallly realyy /really/	reaaaaaally reallu
reaaaallly reeaally rreally reallyreally eally reeeaaally reeeaaally
reaallyy reallyyyyyy    really- reallyreallyreallyrilli reallllyyyy relaly
reallllyy really-really	r3ally reeli reallie realllllyyyrli realllllllllly
reaaaly reeeeeeeally

52

   really   

really	rly	realy genuinely	rlly reallly realllly
reallyy rele realli relly reallllly reli reali sholl rily
reallyyy reeeeally realllllly reaally reeeally rili
reaaally reaaaally reallyyyy rilly reallllllly
reeeeeally reeally shol realllyyy reely relle
reaaaaally shole really2 reallyyyyy _really_
realllllllly reaaly realllyy reallii reallt genuinly relli
realllyyyy reeeeeeally weally reaaallly reallllyyy
reallllllllly reaallly realyy /really/	reaaaaaally reallu
reaaaallly reeaally rreally reallyreally eally reeeaaally reeeaaally
reaallyy reallyyyyyy    really- reallyreallyreallyrilli reallllyyyy relaly
reallllyy really-really	r3ally reeli reallie realllllyyyrli realllllllllly
reaaaly reeeeeeeally

53

   really   

really	rly	realy genuinely	rlly reallly realllly
reallyy rele realli relly reallllly reli reali sholl rily
reallyyy reeeeally realllllly reaally reeeally rili
reaaally reaaaally reallyyyy rilly reallllllly
reeeeeally reeally shol realllyyy reely relle
reaaaaally shole really2 reallyyyyy _really_
realllllllly reaaly realllyy reallii reallt genuinly relli
realllyyyy reeeeeeally weally reaaallly reallllyyy
reallllllllly reaallly realyy /really/	reaaaaaally reallu
reaaaallly reeaally rreally reallyreally eally reeeaaally reeeaaally
reaallyy reallyyyyyy    really- reallyreallyreallyrilli reallllyyyy relaly
reallllyy really-really	r3ally reeli reallie realllllyyyrli realllllllllly
reaaaly reeeeeeeally

54

   going	to   

gonna gunna gona gna guna gnna ganna qonna
gonnna gana qunna gonne goona gonnaa g0nna	
goina gonnah goingto gunnah gonaa gonan
gunnna going2	gonnnnagunnaa gonny gunaa
quna goonna qona gonns goinna gonnae qnna
gonnaaa gnaa

55

   so   

soo sooo soooo sooooo soooooo sooooooo
soooooooo sooooooooo soooooooooo
sooooooooooo soooooooooooo
sooooooooooooo soso soooooooooooooo
sooooooooooooooo soooooooooooooooo
sososo superrr sooooooooooooooooo ssooo
so0o	superrrr so0	soooooooooooooooooo
sosososo sooooooooooooooooooossoo sssooo
soooooooooooooooooooo#too	s0o	ssoooo s00

56

food-related	adjectives

hot	fried	peanut	homemade	grilled	spicy	soy	cheesy	coconut	
veggie	roasted	leftover	blueberry	icy	dunkin mashed	rotten	
mellow	boiling	crispy	peppermint	fruity	toasted	crunchy	
scrambled	creamy	boiled	chunky	funnel	soggy	clam	steamed	
cajun steaming	chewy	steamy	nacho	mince	reese's shredded	
salted	glazed	spiced	venti pickled	powdered	butternut	miso	beet	
sizzling

57

adjective	intensifiers/qualifiers

kinda hella sorta hecka kindof kindaa kinna hellla propa
helluh kindda justa #slick	helllla hela jii sortof hellaa
kida wiggity hellllla hekka hellah kindaaa hellaaa kindah
knda kind-of	slicc wiggidy helllllla jih jye kinnda odhee
kiinda heka sorda ohde kind've kidna baree rle hellaaaa
jussa

58

