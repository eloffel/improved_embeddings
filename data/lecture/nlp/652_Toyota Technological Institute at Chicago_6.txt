ttic	31190:

natural	language	processing

kevin	gimpel
winter	2016

lecture	6:	language	modeling

1

announcements

    assignment	1	due	tonight
    assignment	2	will	be	posted	today,	due	feb.	2
    midterm	scheduled	for	thursday,	feb.	18
    project	proposal	due	tuesday,	feb.	23

    short	(<1	page)
    briefly	describe	project	idea	and	plan	(with	

timeline)

    one	proposal	per	group	(groups	can	be	size	1	or	2)

2

distributional	word	vectors

    simplest	way	to	create	word	vectors:	
count	occurrences	of	context	words

3

to the right, in which case the cell represents the number of times (in some training
corpus) the column word occurs in such a   4 word window around the row word.
for example here are 7-word windows surrounding four sample words from the
brown corpus (just one example of each word):

counting	context	words

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

preserve or jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

sugar

aardvark

pinch result

aardvark computer

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 17.2 shows a selection from the word-word
   
co-occurrence matrix computed from the brown corpus for these four words.
   
   
sugar
   
   

apricot
pineapple
digital
apricot
pineapple
information
   
information
figure 19.2 co-occurrence vectors for four words, computed from the brown corpus,
showing only six of the dimensions (hand-picked for pedagogical purposes). note that a
real vector would be vastly more sparse.

data
0
0
computer
1
6

1
1
result
0
0

data
0
0
1
6

...
...
...
...
...

0
0
0
0

0
0
2
1

1
1
0
0

0
0
1
4

digital

pinch

0
0
2
1

1
1
0
0

1
1
0
0

0
0
1
4

0
0
0
0

...

the shading in fig. 17.2 makes clear the intuition that the two words apricot

j&m/slp3

word-context	matrix

    assume	a	vocabulary v and	a	context	

vocabulary vc (vc is	a	subset	of	v)

    build	the	word-context	matrix	c

    c is	a	|v|-by-|vc|	matrix	of	nonnegative	counts
    entry	(i,	j)	contains	the	number	of	times	context	

word	j appeared	within	w words	of	word	i in	a	
corpus

    then	build	the	pmi	matrix	p

5

pointwise mutual	information	(pmi)
    do	two	events	x and	y co-occur	more	often	than	if	

they	were	independent?

    here,	x is	the	center	word	and	y is	the	word	in	the	

context	window

    each	id203	can	be	estimated	from	counts	

collected	from	a	corpus

6

computing	pmi

center	word:	index
into	vocabulary	v

context	word:	index

into	context	vocabulary	vc

we	start	with	the	word-context	count	matrix	c:
=	number	of	times	context	word	j appears	in	window	of	word	i

7

computing	pmi

=	number	of	times	context	word	j appears	in	window	of	word	i

estimate	of	joint	id203:

estimates	of	center	
word	and	context	word	
marginal	probabilities:

same	

denominator	
for	all	terms

8

pmi(hong,	kong)		____		pmi(hong,	then)

<			>			=		?

9

pmi(hong,	kong)					>

pmi(hong,	then)

7.9

0.1

10

pmis	(1%	of	english	wikipedia,	window	size	=	3)

word
hong
neither
footballer

1980s
musician
benefit
gain
five
miles
prior

position

local
fire

context	word

kong
nor
plays
1970s
session
doubt
failed
stars

distance
unlike
affairs

processes

less

pmi
7.9
6.9
6.0
5.3
5.0
4.5
4.0
3.5
3.0
2.0
1.0
0.5
0.01

11

pmis	(1%	of	english	wikipedia,	window	size	=	10)

word
san
san
san
san
san
san

word
down
down
down
down
down
down

context	word

francisco

diego
juan

california

san
santa

context	word

laid
shot
turned
broken
step

shooting

pmi
5.7
5.7
4.7
3.7
3.6
3.3

pmi
3.8
3.0
2.9
2.6
2.6
2.5

12

evaluating	word	vectors

    extrinsic:

    question	answering,	spell	checking,	essay	grading

    intrinsic:

    correlation	between	vector	similarity	and	human	

word	similarity	judgments
    wordsim353:	353	noun	pairs	rated	0-10
sim(plane,car)=5.77

    toefl	multiple-choice	vocabulary	tests

j&m/slp3

roadmap

    classification
    words
   
lexical	semantics
   
language	modeling
    sequence	labeling
    syntax	and	syntactic	parsing
    neural	network	methods	in	nlp
    semantic	compositionality
    semantic	parsing
    unsupervised	learning
    machine	translation	and	other	applications

14

probabilistic	language	models
    today   s	goal:	assign	a	id203	to	a	sentence
    why?

    machine	translation:

    p(high	winds	tonite)	>	p(large winds	tonite)

    spelling	correction:

    the	office	is	about	fifteen	minuets from	my	house
    p(about	fifteen	minutes from)	>	p(about	fifteen	minuets from)

    speech	recognition:

    p(i	saw	a	van)	>>	p(eyes	awe	of	an)

    summarization,	question	answering,	etc.!

j&m/slp3

automatic	completion

16

automatic	completion

17

18

probabilistic	language	modeling

    goal:	compute	the	id203	of	a	sequence	of	words:

p(w)	=	p(w1,w2,w3,w4,w5   wn)

    related	task:	id203	of	next	word:

p(w5|w1,w2,w3,w4)

    a model	that	computes	either	of	these:

p(w)					or					p(wn|w1,w2   wn-1)
is	called	a	language	model	(lm)

j&m/slp3

how	to	compute	p(w)
    how	to	compute	this	joint	id203:

    p(its,	water,	is,	so,	transparent,	that)

    intuition:	let   s	rely	on	the	chain	rule	of	

id203

j&m/slp3

reminder:	chain	rule
    recall	definition	of	conditional	id203:

p(b|a)	=	p(a,b)/p(a)

rewriting:			p(a,b)	=	p(a)p(b|a)

    more	variables:

p(a,b,c,d)	=	p(a)p(b|a)p(c|a,b)p(d|a,b,c)

    in	general:
p(x1,x2,x3,   ,xn)	=	p(x1)p(x2|x1)p(x3|x1,x2)   p(xn|x1,   ,xn-1)

j&m/slp3

chain	rule	applied	to	computing	joint	

id203	of	words	in	sentence

p(w1w2   wn) =
  

   
i

p(wi | w1w2   wi   1)

p(   its	water	is	so	transparent   )	=

p(its)	   p(water	|	its)	   p(is	|	its	water)	
   p(so	|	its	water	is)	   p(transparent	|	its	water	is	so)

j&m/slp3

how	to	estimate	these	probabilities

    could	we	just	count	and	divide?
p(the |its water is so transparent that) =
count(its water is so transparent that the)
count(its water is so transparent that)

    no!	 too	many	possible	sentences!
    we   ll	never	see	enough	data	for	estimating	these

j&m/slp3

markov	assumption

    simplifying	assumption:
p(the |its water is so transparent that)     p(the |that)

andrei	markov

    or	maybe:

p(the |its water is so transparent that)     p(the |transparent that)

j&m/slp3

markov	assumption

p(w1w2   wn)    
  

p(wi | wi   k   wi   1)
    i.e.,	we	approximate	each	component	in	the	

   
i

product:
p(wi | w1w2   wi   1)     p(wi | wi   k   wi   1)
  

j&m/slp3

simplest	case:	unigram	model
p(w1w2   wn)    
  

p(wi)

   
i

automatically	generated	sentences	from	a	unigram	model:

fifth an of futures the an incorporated a a the 
inflation most dollars quarter in is mass

thrift did eighty said hard    m july bullish

that or limited the

    

j&m/slp3

bigram	model

condition	on	the	previous	word:

p(wi | w1w2   wi   1)     p(wi | wi   1)
  

automatically	generated	sentences	from	a	bigram	model:

texaco rose one in this issue is pursuing growth in a boiler 
house said mr. gurria mexico    s motion control proposal 
without permission from five hundred fifty five yen

outside new car parking lot of the agreement reached

this would be a record november

j&m/slp3

id165	models

    we	can	extend	to	trigrams,	4-grams,	5-grams
    in	general	this	is	an	insufficient	model	of	language

    because	language	has	long-distance	dependencies:
   the	computer	which	i	had	just	put	into	the	machine	room	on	
the	fifth	floor	crashed.   

    but	we	can	often	get	away	with	id165	models

j&m/slp3

estimating	bigram	probabilities

    the	maximum	likelihood	estimate

p(wi |wi   1) =

p(wi |wi   1) =

count(wi   1,wi)
count(wi   1)
c(wi   1,wi)
c(wi   1)

j&m/slp3

an	example

p(wi |wi   1) =

c(wi   1,wi)
c(wi   1)

<s>	i	am	sam	</s>
<s>	sam	i	am	</s>
<s>	i	do	not	like	green	eggs	and	ham	</s>

j&m/slp3

more	examples:	

berkeley	restaurant	project	sentences

tell	me	about	chez	panisse

    can	you	tell	me	about	any	good	cantonese restaurants	close	by
    mid	priced	thai food	is	what	i   m looking	for
   
    can	you	give	me	a	listing	of	the	kinds	of	food	that	are	available
   
    when	is	caffe venezia open	during	the	day

i   m looking	for	a	good	place	to	eat	breakfast

j&m/slp3

raw	bigram	counts

    counts	from	9,222	sentences
    e.g.,	   i want   	occurs	827	times

j&m/slp3

raw	bigram	probabilities

    normalize	by	unigram	counts:

    bigram	probabilities:

j&m/slp3

bigram	estimates	of	sentence	probabilities

p(<s>	i	want	english food	</s>)	=

p(i	|	<s>)			
   p(want	|	i)		
   p(english |	want)			
   p(food |	english)			
   p(</s>	|	food)
=		.000031

j&m/slp3

practical	issues
    we	do	everything	in	log	space

    avoid	underflow
    (also	adding	is	faster	than	multiplying)

log(p1    p2    p3    p4) = log p1 +log p2 +log p3 +log p4

j&m/slp3

language	modeling	toolkits

    srilm

    http://www.speech.sri.com/projects/srilm/

    kenlm

    https://kheafield.com/code/kenlm/

j&m/slp3

google	id165	release,	august	2006

   

j&m/slp3

google	id165	release

    serve as the incoming 92
    serve as the incubator 99
    serve as the independent 794
    serve as the index 223
    serve as the indication 72
    serve as the indicator 120
    serve as the indicators 45
    serve as the indispensable 111
    serve as the indispensible 40
    serve as the individual 234

http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-you.html

j&m/slp3

39

40

evaluation:	how	good	is	our	model?
    does	our	language	model	prefer	good	

sentences	to	bad	ones?
    assign	higher	id203	to	   real   	or	   frequently	

observed   	sentences	
    than	   ungrammatical   	or	   rarely	observed   	sentences?

j&m/slp3

extrinsic	evaluation	of	id165	models
    best	evaluation	for	comparing	models	a	and	b

    put	each	model	in	a	task

    spelling	corrector,	speech	recognizer,	mt	system

    run	the	task,	get	an	accuracy	for	a	and	for	b

    how	many	misspelled	words	corrected	properly
    how	many	words	translated	correctly

    compare	accuracy	for	a	and	b

j&m/slp3

difficulty	of	extrinsic	evaluation	of		id165	models

    extrinsic	evaluation	is	time-consuming

    days	or	weeks	depending	on	system

    so,	sometimes	use	intrinsic	evaluation:	perplexity

    bad	approximation	

    unless	the	test	data	looks	just like	the	training	data
    so	generally	only	useful	in	pilot	experiments

    but	is	helpful	to	think	about

j&m/slp3

intuition	of	perplexity

   

the	shannon	game:
    how	well	can	we	predict	the	next	word?

i	always	order	pizza	with	cheese	and	____
the	33rd president	of	the	us	was	____
i	saw	a	____

mushrooms 0.1
pepperoni 0.1
anchovies 0.01
   
fried	rice	0.0001
   
and 1e-100

    unigrams	are	terrible	at	this	game		(why?)

    a	better	model	of	a	text	is	one	which	assigns	a	higher	

id203	to	the	word	that	actually	occurs

j&m/slp3

perplexity	(pp)

best	language	model	is	one	that	best	predicts	unseen	test	set

    gives	the	highest	p(sentence)

perplexity	=	inverse	id203	of	test	
set,	normalized	by	number	of	words:

pp(w) = p(w1w2...wn)   

1
n

            =

n

1

p(w1w2...wn)

chain	rule:

for	bigrams:

minimizing	perplexity	is	the	same	as	maximizing	id203

j&m/slp3

perplexity	as	branching	factor

    given	a	sentence	consisting	of	random	digits
    what	is	the	perplexity	of	this	sentence	

according	to	a	model	that	assigns	id203	
1/10	to	each	digit?

j&m/slp3

lower	perplexity	=	better	model

    train:	38	million	words
    test:	1.5	million	words

id165	order:

unigram

bigram

trigram

perplexity:

962

170

109

j&m/slp3

its bigram id203), and so on.

to give an intuition for the increasing power of higher-order id165s, fig. 4.3
shows random sentences generated from unigram, bigram, trigram, and 4-gram
models trained on shakespeare   s works.

approximating	shakespeare

gram

gram

rote life have
   hill he late speaks; or! a more to leg less    rst you enter

king. follow.
   what means, sir. i confess she? then all sorts, he is trim, captain.

1    to him swallowed confess hear both. which. of save on trail for are ay device and
2    why dost stand forth thy canopy, forsooth; he is this palpable hit the king henry. live
3    fly, and will rid me these news of price. therefore the sadness of parting, as they say,
4    king henry. what! i will go seek the traitor gloucester. exeunt some of the watch. a

   tis done.
   this shall forbid it should be branded, if renown made it empty.

great banquet serv   d in;
   it cannot be but so.

gram

gram

figure 4.3 eight sentences randomly generated from four id165s computed from shakespeare   s works. all
characters were mapped to lower-case and punctuation marks were treated as words. output is hand-corrected
for capitalization to improve readability.

the longer the context on which we train the model, the more coherent the sen-
tences. in the unigram sentences, there is no coherent relation between words or any
sentence-   nal punctuation. the bigram sentences have some local word-to-word
j&m/slp3
coherence (especially if we consider that punctuation counts as a word). the tri-

shakespeare	as	corpus

    884,647	tokens,	29,066	types
    shakespeare	produced	300,000	bigram	types	

out	of	844	million	possible	bigrams
    99.96%	of	possible	bigrams	were	never	seen	(have	

zero	entries	in	the	table)

    4-grams	worse:	what's	coming	out	looks	like	

shakespeare	because	it	is shakespeare

j&m/slp3

wall	street	journal

4.3

    generalization and zeros

11

gram

were recession exchange new endorsed a acquire to six executives

1 months the my and issue of year foreign new exchange   s september
2 last december through the way to preserve the hudson corporation n.

b. e. c. taylor would seem to complete the major central planners one
point    ve percent of u. s. e. has already old m. x. corporation of living
on information such as more frequently    shing to keep her

gram

3 they also point to ninety nine point six billion dollars from two hundred

four oh six three percent of the rates of interest stores as mexico and
brazil on market conditions

gram

figure 4.4 three sentences randomly generated from three id165 models computed from
40 million words of the wall street journal, lower-casing all characters and treating punctua-
tion as words. output was then hand-corrected for capitalization to improve readability.

lap whatsoever in possible sentences, and little if any overlap even in small phrases.
this stark difference tells us that statistical models are likely to be pretty useless as

j&m/slp3

the	perils	of	overfitting

    id165s	only	work	well	for	word	prediction	if	the	

test	corpus	looks	like	the	training	corpus
    in	real	life,	it	often	doesn   t
    we	need	to	train	robust	models that	generalize!
    one	kind	of	generalization:	zeros!

    things	that	don   t	ever	occur	in	the	training	set

   but	occur	in	the	test	set

j&m/slp3

zeros

training	set:
   	denied	the	allegations
   	denied	the	reports
   	denied	the	claims
   	denied	the	request

p(offer |	denied	the)	=	0

test	set:
   	denied	the	offer
   	denied	the	loan

j&m/slp3

zero	id203	bigrams

    test	set	bigrams	with	zero	id203	   assign	

0	id203	to	entire	test	set!

    cannot	compute	perplexity	(can   t	divide	by	0)!

j&m/slp3

intuition	of	smoothing	(from	dan	klein)

    when	we	have	sparse	statistics:

p(w	|	denied	the)
3	allegations
2	reports
1	claims
1	request
7	total

   

steal	id203	mass	to	generalize	better:

p(w	|	denied	the)
2.5	allegations
1.5	reports
0.5	claims
0.5	request
2	other
7	total

s
n
o
i
t
a
g
e

l
l

a

s
t
r
o
p
e
r

s
n
s
o
n
i
o
t
a
i
t
g
a
e
g
e
a
l
l

l
l

a

s
t
r
o
p
e
r

s

m
a

i

l
c

t
s
e
u
q
e
r

k
c
a

t
t

a

n
a
m

e    

m
o
c
t
u
o

k
c
a

t
t

a

n
a
m

t
s
e
u
q
e
r

e    

m
o
c
t
u
o

s

i

m
a
l
c

   add-1   	estimation

    also	called	laplace	smoothing
    pretend	we	saw	each	word	one	more	time	than	we	

did

    just	add	1	to	all	counts!

    id113	estimate:

    add-1	estimate:

pid113(wi |wi   1) =

padd   1(wi |wi   1) =

c(wi   1,wi)
c(wi   1)
c(wi   1,wi)+1
c(wi   1)+v

j&m/slp3

maximum	likelihood	estimates

   

the	maximum	likelihood	estimate
    of	some	parameter	of	a	model	m	from	a	training	set	t
    maximizes	the	likelihood	 of	the	training	set	t	given	the	model	m
   
suppose	the	word	   bagel   	occurs	400	times	in	a	corpus	of	a	million	words
    what	is	the	id203	that	a	random	word	from	some	other	text	will	be	

   bagel   ?

    id113	estimate	is	400/1,000,000	=	.0004
    this	may	be	a	bad	estimate	for	some	other	corpus

    but	it	is	the	estimate that	makes	it	most	likely that	   bagel   	will	occur	400	times	in	

a	million	word	corpus.

j&m/slp3

berkeley	restaurant	corpus:

laplace	smoothed	bigram	counts

j&m/slp3

laplace-smoothed	bigrams

j&m/slp3

reconstituted	counts

j&m/slp3

compare	with	raw	bigram	counts

add-1	estimation	is	a	blunt	instrument
    so	add-1	isn   t	used	for	id165s:	

    we   ll	see	better	methods

    but	add-1	is	used	to	smooth	other	nlp	models

    text	classification	
    domains	where	the	number	of	zeros	isn   t	so	huge

j&m/slp3

backoff and	interpolation
    sometimes	it	helps	to	use	less context

    condition	on	less	context	for	contexts	you	haven   t	

learned	much	about	

    backoff:	

    use	trigram	if	you	have	good	evidence,	otherwise	

bigram,	otherwise	unigram

    interpolation:	

    mixture	of	unigram,	bigram,	trigram	(etc.)	models

    interpolation	works	better

j&m/slp3

in simple linear interpolation, we combine different order id165s by linearly

interpolating all the models. thus, we estimate the trigram id203 p(wn|wn 2wn 1)
+l2p(wn|wn 1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
+l3p(wn)

linear	interpolation

  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)

    simple	interpolation:

such that the ls sum to 1:
  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)

li = 1

xi

such that the ls sum to 1:

    lambdas	are	functions	of	context:

+l2p(wn|wn 1)
+l3p(wn)

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in

li = 1

(4.25)

(4.24)

xi

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in
j&m/slp3

how	to	set	the	lambdas?

    use	a	held-out corpus:

training	data

held-out	

data

test	
data

    choose	lambdas	to	maximize	id203	of	held-out	data:

    fix	id165	probabilities	(on	the	training	data)
    then	search	for	  s that	give	largest	id203	to	held-out	set:
logpm (  1...  k )(wi | wi   1)

logp(w1...wn | m(  1...  k)) =

   
i

    subtlety:	what	happens	if	we	use	training	data	to	learn	  s?

j&m/slp3

unknown	words:	open	vs.	closed	vocabulary	tasks

   

if	we	know	all	the	words	in	advance:	
    vocabulary	v is	fixed
       closed	vocabulary   	task
    often	we	don   t	know	this

    out-of-vocabulary	(oov) words
       open	vocabulary   	task

    so,	create	an	unknown	word	token	<unk>

    at	training	time:

    randomly	change	some	instances	of	rare	words	to	<unk>
    then	estimate	its	probabilities	like	a	normal	word

    at	test	time:

    replace	oov	words	with	<unk>

j&m/slp3

huge	web-scale	id165s

    how	to	deal	with,	e.g.,	google	id165	corpus?
    pruning:

    only	store	id165s	with	count	>	threshold.

    remove	singletons	of	higher-order	id165s

    id178-based	pruning

    efficiency

    efficient	data	structures	like	tries
    bloom	filters:	approximate	language	models
    store	words	as	indexes,	not	strings

    use	huffman	coding	to	fit	large	numbers	of	words	into	2	bytes
    quantize	probabilities	(4-8	bits	instead	of	8-byte	float)

j&m/slp3

67

smoothing	for	web-scale	id165s

       stupid	backoff   	(brants et	al.,	2007)
    no	discounting,	just	use	relative	frequencies	

s(wi | wi   k+1

i   1 ) =

"
$$
#
$
$
%

i

)

count(wi   k+1
count(wi   k+1
0.4s(wi | wi   k+2
i   1

) > 0
i   1 )   if  count(wi   k+1
)      otherwise

i

s(wi) =

count(wi)

n

j&m/slp3

68

id165	smoothing	summary

    add-1	estimation:

    ok	for	text	categorization,	not	for	language	modeling

    most	commonly	used	method:
    modified	interpolated	kneser-ney

    for	very	large	id165	collections	like	the	web:

    stupid	backoff

j&m/slp3

69

advanced	language	modeling

    discriminative	models:

    choose	id165	weights	to	improve	a	task,	not	

to	fit	the		training	set

    syntactic	language	models
    caching	models

    recently	used	words	are	more	likely	to	appear

pcache(w | history) =   p(wi | wi   2wi   1) +(1     )c(w     history)
| history |

    these	perform	very	poorly	for	speech	

recognition	(why?)

j&m/slp3

