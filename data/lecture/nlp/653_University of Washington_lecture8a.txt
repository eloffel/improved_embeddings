natural language processing (csep 517):

id65

roy schwartz

c(cid:13) 2017

university of washington

roysch@cs.washington.edu

may 15, 2017

1 / 59

to-do list

(cid:73) read: (jurafsky and martin, 2016a,b)

2 / 59

id65 models
aka, vector space models, id27s

                                 

-0.23
-0.21
-0.15
-0.61

...

-0.02
-0.12

                                 , vlion =

                                 

-0.72
-00.2
-0.71
-0.13

...

0-0.1
-0.11

                                 

vmountain =

3 / 59

id65 models
aka, vector space models, id27s

                                 

-0.23
-0.21
-0.15
-0.61

...

-0.02
-0.12

                                 , vlion =

                                 

-0.72
-00.2
-0.71
-0.13

...

0-0.1
-0.11

                                 

vmountain =

mountain

lion

4 / 59

id65 models
aka, vector space models, id27s

                                 

-0.23
-0.21
-0.15
-0.61

...

-0.02
-0.12

                                 , vlion =

                                 

-0.72
-00.2
-0.71
-0.13

...

0-0.1
-0.11

                                 

vmountain =

mountain

lion

  

5 / 59

id65 models
aka, vector space models, id27s

                                 

-0.23
-0.21
-0.15
-0.61

...

-0.02
-0.12

                                 , vlion =

                                 

-0.72
-00.2
-0.71
-0.13

...

0-0.1
-0.11

                                 

vmountain =

mountain lion

mountain

lion

6 / 59

id65 models
aka, vector space models, id27s

applications

deep learning models:
machine translation
id53

syntactic parsing

. . .

linguistic study

lexical semantics
multilingual studies

evolution of language

. . .

7 / 59

outline

vector space models

lexical semantic applications

id27s

compositionality

current research problems

8 / 59

outline

vector space models

lexical semantic applications

id27s

compositionality

current research problems

9 / 59

id65 hypothesis
harris (1954)

words that have similar contexts are likely to have similar meaning

10 / 59

id65 hypothesis
harris (1954)

words that have similar contexts are likely to have similar meaning

11 / 59

vector space models

(cid:73) representation of words by vectors of real numbers
(cid:73)    w     v, vw is function of the contexts in which w occurs
(cid:73) vectors are computed using a large text corpus

(cid:73) no requirement for any sort of annotation in the general case

12 / 59

v1.0: count models
salton (1971)

(cid:73) each element vwi     vw represents the co-occurrence of w with another word i

(cid:73) vdog = (cat: 10, leash: 15, loyal: 27, bone: 8, piano: 0, cloud: 0, . . . )

(cid:73) vector dimension is typically very large (vocabulary size)
(cid:73) main motivation: lexical semantics

13 / 59

count models
example

                                 

0
0
15
17
...
0
102

                                 , vcat =

                                 

                                 

0
2
11
13
...
20
11

vdog =

14 / 59

count models
example

                                 

0
0
15
17
...
0
102

                                 , vcat =

                                 

                                 

0
2
11
13
...
20
11

vdog =

dog

cat

15 / 59

count models
example

                                 

0
0
15
17
...
0
102

                                 , vcat =

                                 

                                 

0
2
11
13
...
20
11

vdog =

dog

cat

  

16 / 59

variants of count models

(cid:73) reduce the e   ect of high frequency words by applying a weighting scheme

(cid:73) pointwise mutual information (pmi), tf-idf

17 / 59

variants of count models

(cid:73) reduce the e   ect of high frequency words by applying a weighting scheme

(cid:73) pointwise mutual information (pmi), tf-idf

(cid:73) smoothing by id84

(cid:73) singular value decomposition (svd), principal component analysis (pca), matrix

factorization methods

18 / 59

variants of count models

(cid:73) reduce the e   ect of high frequency words by applying a weighting scheme

(cid:73) pointwise mutual information (pmi), tf-idf

(cid:73) smoothing by id84

(cid:73) singular value decomposition (svd), principal component analysis (pca), matrix

factorization methods

(cid:73) what is a context?

(cid:73) bag-of-words context, document context (latent semantic analysis (lsa)),

dependency contexts, pattern contexts

19 / 59

outline

vector space models

lexical semantic applications

id27s

compositionality

current research problems

20 / 59

vector space models
evaluation

(cid:73) vector space models as features

(cid:73) synonym detection

(cid:73) toefl (landauer and dumais, 1997)

(cid:73) word id91

(cid:73) cluto (karypis, 2002)

21 / 59

vector space models
evaluation

(cid:73) vector space models as features

(cid:73) synonym detection

(cid:73) toefl (landauer and dumais, 1997)

(cid:73) word id91

(cid:73) cluto (karypis, 2002)

(cid:73) vector operations

(cid:73) semantic similarity

(cid:73) rg-65 (rubenstein and goodenough, 1965), wordsim353 (finkelstein et al., 2001),

men (bruni et al., 2014), siid113x999 (hill et al., 2015)

(cid:73) word analogies

(cid:73) mikolov et al. (2013)

22 / 59

semantic similarity

w1
tiger

w2
cat

computer

keyboard

. . .

architecture

book
king

. . .

century
paper

cabbage

human score model score

7.35
7.62
. . .
3.78
7.46
0.23

0.8
0.54
. . .
0.03
0.66
-0.42

table: human scores taken from wordsim353 (finkelstein et al., 2001)

(cid:73) model scores are cosine similarity scores between vectors
(cid:73) model   s performance is the spearman/pearson correlation between human

ranking and model ranking

23 / 59

word analogy
mikolov et al. (2013)

france

woman

paris

man

italy

queen

rome

king

24 / 59

outline

vector space models

lexical semantic applications

id27s

compositionality

current research problems

25 / 59

v2.0: predict models
(aka id27s)

(cid:73) a new generation of vector space models
(cid:73) instead of representing vectors as cooccurrence counts, train a supervised machine

learning algorithm to predict p(word|context)

(cid:73) models learn a latent vector representation of each word

(cid:73) these representations turn out to be quite e   ective vector space representations
(cid:73) id27s

26 / 59

id27s

(cid:73) vector size is typically a few dozens to a few hundreds
(cid:73) vector elements are generally uninterpretable
(cid:73) developed to initialize feature vectors in deep learning models

(cid:73) initially language models, nowadays virtually every sequence level nlp task
(cid:73) bengio et al. (2003); collobert and weston (2008); collobert et al. (2011);

id97 (mikolov et al., 2013); glove (pennington et al., 2014)

27 / 59

id27s

(cid:73) vector size is typically a few dozens to a few hundreds
(cid:73) vector elements are generally uninterpretable
(cid:73) developed to initialize feature vectors in deep learning models

(cid:73) initially language models, nowadays virtually every sequence level nlp task
(cid:73) bengio et al. (2003); collobert and weston (2008); collobert et al. (2011);

id97 (mikolov et al., 2013); glove (pennington et al., 2014)

28 / 59

id97
mikolov et al. (2013)

(cid:73) a software toolkit for running various id27 algorithms

based on (goldberg and levy, 2014)

29 / 59

id97
mikolov et al. (2013)

(cid:73) a software toolkit for running various id27 algorithms
(cid:73) continuous bag-of-words: argmax

p(w|c(w);   )

(cid:89)

  

w   corpus

based on (goldberg and levy, 2014)

30 / 59

id97
mikolov et al. (2013)

(cid:73) a software toolkit for running various id27 algorithms
(cid:73) continuous bag-of-words: argmax

p(w|c(w);   )

(cid:89)

(cid:89)

  

w   corpus

p(c|w;   )

(cid:73) skip-gram: argmax

  

(w,c)   corpus

based on (goldberg and levy, 2014)

31 / 59

id97
mikolov et al. (2013)

(cid:89)

(cid:89)

  

w   corpus

p(c|w;   )

(cid:73) a software toolkit for running various id27 algorithms
(cid:73) continuous bag-of-words: argmax

p(w|c(w);   )

(cid:73) skip-gram: argmax

  

(w,c)   corpus

(cid:73) negative sampling: randomly sample negative (word,context) pairs, then:

(cid:89)

p(c|w;   )    (cid:89)

(w,c(cid:48))

argmax

  

(w,c)   corpus

(1     p(c(cid:48)|w;   ))

based on (goldberg and levy, 2014)

32 / 59

skip-gram with negative sampling (sgns)

(cid:73) obtained signi   cant improvements on a range of lexical semantic tasks
(cid:73) is very fast to train, even on large corpora
(cid:73) nowadays, by far the most popular id27 approach1

1along with glove (pennington et al., 2014)

33 / 59

embeddings in acl
number of papers in acl containing the word    embedding   

s
r
e
p
a
p

f
o

r
e
b
m
u
n

30

20

10

0

2011

2012

2013

2014

2015

2016

2017

34 / 59

count vs. predict

(cid:73) don   t count, predict! (baroni et al., 2014)

35 / 59

count vs. predict

(cid:73) don   t count, predict! (baroni et al., 2014)
(cid:73) but...

neural embeddings are implicitly id105 tools (levy and goldberg,
2014)

36 / 59

count vs. predict

(cid:73) don   t count, predict! (baroni et al., 2014)
(cid:73) but...

neural embeddings are implicitly id105 tools (levy and goldberg,
2014)
(cid:73) so?...

it   s all about hyper-parameter (levy et al., 2015)

37 / 59

count vs. predict

(cid:73) don   t count, predict! (baroni et al., 2014)
(cid:73) but...

neural embeddings are implicitly id105 tools (levy and goldberg,
2014)
(cid:73) so?...

it   s all about hyper-parameter (levy et al., 2015)

(cid:73) the bottom line:

id97 and glove are very good implementations

38 / 59

outline

vector space models

lexical semantic applications

id27s

compositionality

current research problems

39 / 59

compositionality

(cid:73) basic approach: average / weighted average

(cid:73) vgood + vday = vgood day

40 / 59

compositionality
id56s (goller and kuchler, 1996)

picture taken from socher et al. (2013)

41 / 59

compositionality
recurrent neural networks (elman, 1990)

h1

h2

h3

vor

vmaybe

vyes

or

maybe

yes

h4

v?

?

42 / 59

recurrent neural networks

(cid:73) in recent years, the most common method to represent sequence of texts is using

id56s

(cid:73) in particular, long short-term memory (lstm, hochreiter and schmidhuber (1997))

and gated recurrent unit (gru, cho et al. (2014))

43 / 59

recurrent neural networks

(cid:73) in recent years, the most common method to represent sequence of texts is using

id56s

(cid:73) in particular, long short-term memory (lstm, hochreiter and schmidhuber (1997))

and gated recurrent unit (gru, cho et al. (2014))

(cid:73) very recently, state-of-the-art models on tasks such as id14 and

coreference resolution stated to rely solely on deep networks with word
embeddings and lstm layers (he et al., 2017)

(cid:73) these tasks traditionally relied on syntactic information
(cid:73) many of these results come from the uw nlp group

44 / 59

id27s in id56s

(cid:73) pre-trained embeddings (   xed or tuned)
(cid:73) random initialization
(cid:73) a concatenation of both types

45 / 59

alternatives to id27s

(cid:73) character embeddings

(cid:73) machine translation (ling et al., 2015)
(cid:73) syntactic parsing (ballesteros et al., 2015)

(cid:73) character id165s (neubig et al., 2013; sch  utze, 2017)
(cid:73) pos tag embeddings (dyer et al., 2015)

46 / 59

outline

vector space models

lexical semantic applications

id27s

compositionality

current research problems

47 / 59

50 shades of similarity

(cid:73) what is similarity?

(cid:73) synonymy: high     tall
(cid:73) co-hyponymy: dog     cat
(cid:73) association: co   ee     cup
(cid:73) dissimilarity: good     bad
(cid:73) attributional similarity: banana     the sun (both are yellow)
(cid:73) morphological similarity: going     crying (same verb tense)
(cid:73) schwartz et al. (2015); rubinstein et al. (2015); cotterell et al. (2016)

(cid:73) de   nition is application dependent

48 / 59

what is a context?

(cid:73) most id27s rely on bag-of-word contexts

(cid:73) which capture general word association

(cid:73) other options exists

(cid:73) dependency links (pad  o and lapata, 2007)
(cid:73) symmetric patterns (e.g.,    x and y   , schwartz et al. (2015, 2016))
(cid:73) substitute vectors (yatbaz et al., 2012)
(cid:73) morphemes (cotterell et al., 2016)

(cid:73) di   erent context types translate to di   erent relations between similar vectors

49 / 59

external resources

(cid:73) guide vectors towards desired    avor of similarity
(cid:73) use dictionaries and/or thesauri

(cid:73) part of the model (yu and dredze, 2014; kiela et al., 2015)
(cid:73) post-processing (faruqui et al., 2015; mrk  si  c et al., 2016)

(cid:73) multimodal embeddings

50 / 59

multimodal embeddings

(cid:73) combination of textual representation and perceptual representation

(cid:73) most prominently visual

(cid:73) most approaches combine both types of vectors using methods such as canonical

correlation analysis (cca, e.g., gella et al. (2016))

(cid:73) the resulting embeddings often improve performance compared to text-only

embeddings

(cid:73) they are also able to capture visual attributes such as size and color, which are often

not captured by text only methods (rubinstein et al., 2015)

51 / 59

multilingual embeddings

(cid:73) mapping embeddings in di   erent languages into the same space

(cid:73) vdog     vperro

(cid:73) useful for multi-lingual tasks, as well as low-resource scenarios
(cid:73) most approaches use bilingual dictionaries or parallel corpora
(cid:73) recent approaches use more creative knowledge sources such as geospatial

contexts (cocos and callison-burch, 2017) and sentences ids in a parallel corpus
(levy et al., 2017)

52 / 59

summary

(cid:73) distributional semantic models (aka vector space models, id27s)

represent words using vectors of real numbers

(cid:73) these methods are able to capture lexical semantics such as similarity and

association

(cid:73) they also serve as a fundamental building block in virtually all deep learning

models in nlp

(cid:73) despite decades of research, many questions remain open

53 / 59

summary

(cid:73) distributional semantic models (aka vector space models, id27s)

represent words using vectors of real numbers

(cid:73) these methods are able to capture lexical semantics such as similarity and

association

(cid:73) they also serve as a fundamental building block in virtually all deep learning

models in nlp

(cid:73) despite decades of research, many questions remain open

roy schwartz homes.cs.washington.edu/~roysch/ roysch@cs.washington.edu

thank you!

54 / 59

references i

miguel ballesteros, chris dyer, and noah a. smith. improved transition-based parsing by modeling characters

instead of words with lstms. in proc. of emnlp, 2015.

marco baroni, georgiana dinu, and germ  an kruszewski. don   t count, predict! a systematic comparison of

context-counting vs. context-predicting semantic vectors. in proc. of acl, 2014.

yoshua bengio, r  ejean ducharme, pascal vincent, and christian jauvin. a neural probabilistic language model.

jmlr, 3:1137   1155, 2003.

elia bruni, nam-khanh tran, and marco baroni. multimodal id65. jair, 49(2014):1   47,

2014.

kyunghyun cho, bart van merrienboer, dzmitry bahdanau, and yoshua bengio. on the properties of neural

machine translation: encoder-decoder approaches. in proc. of ssst, 2014.

anne cocos and chris callison-burch. the language of place: semantic value from geospatial context. in proc.

of eacl, 2017.

ronan collobert and jason weston. a uni   ed architecture for natural language processing: deep neural

networks with multitask learning. in proc. of icml, 2008.

ronan collobert, jason weston, l  eon bottou, michael karlen, koray kavukcuoglu, and pavel kuksa. natural

language processing (almost) from scratch. jmlr, 12:2493   2537, 2011.

ryan cotterell, hinrich sch  utze, and jason eisner. morphological smoothing and extrapolation of word

embeddings. in proc. of acl, 2016.

55 / 59

references ii

chris dyer, miguel ballesteros, wang ling, austin matthews, and noah a. smith. transition-based dependency

parsing with stack long short-term memory. in proc. of acl, 2015.

je   rey l elman. finding structure in time. cognitive science, 14(2):179   211, 1990.

manaal faruqui, jesse dodge, sujay kumar jauhar, chris dyer, eduard hovy, and noah a. smith. retro   tting

word vectors to semantic lexicons. in proc. of naacl, 2015.

lev finkelstein, evgeniy gabrilovich, yossi matias, ehud rivlin, zach solan, gadi wolfman, and eytan ruppin.

placing search in context: the concept revisited. in proc. of www, 2001.

spandana gella, mirella lapata, and frank keller. unsupervised visual sense disambiguation for verbs using

multimodal embeddings. in proc. of naacl, 2016.

yoav goldberg and omer levy. id97 explained: deriving mikolov et al.?s negative-sampling

word-embedding method, 2014. arxiv:1402.3722.

christoph goller and andreas kuchler. learning task-dependent distributed representations by id26

through structure. in proc. of iid98, 1996.

zelig harris. distributional structure. word, 10(23):146   162, 1954.

luheng he, kenton lee, mike lewis, and luke zettlemoyer. deep id14: what works and

what   s next. in proc. of acl, 2017.

felix hill, roi reichart, and anna korhonen. siid113x-999: evaluating semantic models with (genuine) similarity

estimation. computational linguistics, 2015.

56 / 59

references iii

sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9(8):1735   1780,

1997.

dan jurafsky and james h. martin. vector semantics (draft chapter). chapter 15. 2016a. url

https://web.stanford.edu/~jurafsky/slp3/15.pdf.

dan jurafsky and james h. martin. semantics with dense vectors (draft chapter). chapter 16. 2016b. url

https://web.stanford.edu/~jurafsky/slp3/16.pdf.

george karypis. cluto-a id91 toolkit. technical report, dtic document, 2002.

douwe kiela, felix hill, and stephen clark. specializing id27s for similarity or relatedness. in proc.

of emnlp, 2015.

thomas k. landauer and susan t. dumais. a solution to plato   s problem: the latent semantic analysis theory

of acquisition, induction, and representation of knowledge. psychological review, 104(2):211, 1997.

omer levy and yoav goldberg. neural id27s as implicit id105. in proc. of nips,

2014.

omer levy, yoav goldberg, and ido dagan. improving distributional similarity with lessons learned from word

embeddings. tacl, 3:211   225, 2015.

omer levy, anders s  gaard, and yoav goldberg. a strong baseline for learning cross-lingual id27s

from sentence alignments. in proc. of eacl, 2017.

wang ling, isabel trancoso, chris dyer, and alan w black. character-based id4, 2015.

arxiv:1511.04586.

57 / 59

references iv

tomas mikolov, kai chen, greg corrado, and je   rey dean. e   cient estimation of word representations in

vector space, 2013. arxiv:1301.3781.

nikola mrk  si  c, diarmuid   o s  eaghdha, blaise thomson, milica ga  si  c, lina m. rojas-barahona, pei-hao su,

david vandyke, tsung-hsien wen, and steve young. counter-   tting word vectors to linguistic constraints.
in proc. of naacl, 2016.

graham neubig, taro watanabe, shinsuke mori, and tatsuya kawahara. substring-based machine translation.

machine translation, 27(2):139   166, 2013.

sebastian pad  o and mirella lapata. dependency-based construction of semantic space models. computational

linguistics, 33(2):161   199, 2007.

je   rey pennington, richard socher, and christopher d. manning. glove: global vectors for word

representation. in proc. of emnlp, 2014.

herbert rubenstein and john b goodenough. contextual correlates of synonymy. communications of the acm,

8(10):627   633, 1965.

dana rubinstein, e    levi, roy schwartz, and ari rappoport. how well do distributional models capture

di   erent types of semantic knowledge? in proc. of acl, 2015.

gerard salton. the smart retrieval system: experiments in automatic document processing. prentice-hall,

inc., upper saddle river, nj, usa, 1971.

hinrich sch  utze. nonsymbolic text representation. in proc. of eacl, 2017.

58 / 59

references v

roy schwartz, roi reichart, and ari rappoport. symmetric pattern based id27s for improved word

similarity prediction. in proc. of conll, 2015.

roy schwartz, roi reichart, and ari rappoport. symmetric patterns and coordinations: fast and enhanced

representations of verbs and adjectives. in proc. of naacl, 2016.

richard socher, alex perelygin, jean wu, jason chuang, christopher d. manning, andrew ng, and christopher

potts. recursive deep models for semantic compositionality over a sentiment treebank. 2013.

mehmet ali yatbaz, enis sert, and deniz yuret. learning syntactic categories using paradigmatic

representations of word context. in proc. of emnlp, 2012.

mo yu and mark dredze. improving lexical embeddings with semantic knowledge. in proc. of acl, 2014.

59 / 59

