natural language processing (csep 517):

language models, continued

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

april 3, 2017

1 / 102

to-do list

(cid:73) online quiz: due sunday
(cid:73) print, sign, and return the academic integrity statement (if you haven   t already)
(cid:73) read: smith (2017);

optionally, jurafsky and martin (2016), collins (2011)   2, and goldberg (2015)
  0   4, 10   13 if you want to know more about neural networks

(cid:73) a1 now due april 9 (sunday)
(cid:73) late policy: four late days

2 / 102

language models: de   nitions

(cid:73) v is a    nite set of (discrete) symbols ((cid:44)    words    or possibly characters); v = |v|
(cid:73) v    is the (in   nite) set of sequences of symbols from v whose    nal symbol is (cid:56)
(cid:73) p : v        r, such that:

(cid:73) for any x     v   , p(x)     0

p(x = x) = 1

(cid:73) (cid:88)

x   v   

(i.e., p is a proper id203 distribution.)

id38: estimate p from examples, x1:n = (cid:104)x1, x2, . . . , xn(cid:105).
evaluation on test data   x1:m: perplexity, 2    1

i=1 log2 p(  xi)

m

(cid:80)m

3 / 102

id148: de   nitions

we de   ne a conditional log-linear model p(y | x) as:
(cid:73) y is the set of events/outputs ((cid:44) for id38, v)
(cid:73) x is the set of contexts/inputs ((cid:44) for id165 id38, v n   1)
(cid:73)    : x    y     rd is a feature vector function
(cid:73) w     rd are the model parameters

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)
exp w      (x, y(cid:48))

4 / 102

breaking it down

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)

exp w      (x, y)

5 / 102

breaking it down

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)

exp w      (x, y)

linear score w      (x, y)

6 / 102

breaking it down

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)

exp w      (x, y)

linear score w      (x, y)
nonnegative

exp w      (x, y)

7 / 102

breaking it down

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)

exp w      (x, y)

linear score w      (x, y)
nonnegative

exp w      (x, y)

normalizer

(cid:88)

y(cid:48)   y

exp w      (x, y(cid:48)) = zw(x)

8 / 102

breaking it down

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)

exp w      (x, y)

linear score w      (x, y)
nonnegative

exp w      (x, y)

normalizer

exp w      (x, y(cid:48)) = zw(x)

(cid:88)

y(cid:48)   y

   log-linear    comes from the fact that:

log pw(y = y | x = x) = w      (x, y)     log zw(x)
constant in y

(cid:124)

(cid:123)(cid:122)

(cid:125)

this is an instance of the family of generalized linear models.

9 / 102

the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

as a simple example, let the two features be binary functions.

10 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

w       = w1  1 + w2  2 = 0

11 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

distance(w       = 0,   0) =

|w      0|
(cid:107)w(cid:107)2

    |w      0|

12 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

w      (x, y1) > w      (x, y3) > w      (x, y4) > 0     w      (x, y2)

13 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

pw(y1 | x) > pw(y3 | x) > pw(y4 | x) > pw(y2 | x)

14 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

15 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

pw(y3 | x) > pw(y1 | x) > pw(y2 | x) > pw(y4 | x)

16 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view

suppose we have instance x, y = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

log-linear parameter estimation tries to choose w so that pw(y | x) matches the
empirical distribution, c(x,y )
c(x) .

17 / 102

(x, y3)  1  2(x, y1)(x, y4)(x, y2)why build language models this way?

(cid:73) exploit features of histories for sharing of statistical strength and better

smoothing (lau et al., 1993)

(cid:73) condition the whole text on more interesting variables like the gender, age, or

political a   liation of the author (eisenstein et al., 2011)

(cid:73) interpretability!

(cid:73) each feature   k controls a factor to the id203 (ewk ).
(cid:73) if wk < 0 then   k makes the event less likely by a factor of
1
ewk .
(cid:73) if wk > 0 then   k makes the event more likely by a factor of ewk .
(cid:73) if wk = 0 then   k has no e   ect.

18 / 102

log-linear id165 models

(cid:96)(cid:89)
(cid:96)(cid:89)

j=1

j=1

pw(x = x) =

=

pw(xj = xj | x0:j   1 = x0:j   1)

exp w      (x0:j   1, xj)

(cid:96)(cid:89)

zw(x0:j   1)
exp w      (xj   n+1:j   1, xj)

zw(xj   n+1:j   1)

j   1
exp w      (hj, xj)

assumption=

(cid:96)(cid:89)

=

j=1

zw(hj)

19 / 102

example

the man who knew too

much
many
little
few
...

hippopotamus

20 / 102

what features in   (xj   n+1:j   1, xj)?

21 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   

22 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   
(cid:73)    gappy    id165s:    xj   2 = the     xj = man   

23 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   
(cid:73)    gappy    id165s:    xj   2 = the     xj = man   
(cid:73) spelling features:    xj   s    rst character is capitalized   

24 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   
(cid:73)    gappy    id165s:    xj   2 = the     xj = man   
(cid:73) spelling features:    xj   s    rst character is capitalized   
(cid:73) class features:    xj is a member of class 132   

25 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   
(cid:73)    gappy    id165s:    xj   2 = the     xj = man   
(cid:73) spelling features:    xj   s    rst character is capitalized   
(cid:73) class features:    xj is a member of class 132   
(cid:73) gazetteer features:    xj is listed as a geographic place name   

26 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   
(cid:73)    gappy    id165s:    xj   2 = the     xj = man   
(cid:73) spelling features:    xj   s    rst character is capitalized   
(cid:73) class features:    xj is a member of class 132   
(cid:73) gazetteer features:    xj is listed as a geographic place name   

you can de   ne any features you want!

(cid:73) too many features, and your model will over   t (cid:47)
(cid:73) too few (good) features, and your model will not learn (cid:47)

27 / 102

what features in   (xj   n+1:j   1, xj)?

(cid:73) traditional id165 features:    xj   1 = the     xj = man   
(cid:73)    gappy    id165s:    xj   2 = the     xj = man   
(cid:73) spelling features:    xj   s    rst character is capitalized   
(cid:73) class features:    xj is a member of class 132   
(cid:73) gazetteer features:    xj is listed as a geographic place name   

you can de   ne any features you want!

(cid:73)    feature selection    methods, e.g., ignoring features with very low counts, can help.

(cid:73) too many features, and your model will over   t (cid:47)
(cid:73) too few (good) features, and your model will not learn (cid:47)

28 / 102

   feature engineering   

(cid:73) many advances in nlp (not just id38) have come from careful

design of features.

29 / 102

   feature engineering   

(cid:73) many advances in nlp (not just id38) have come from careful

design of features.

(cid:73) sometimes    feature engineering    is used pejoratively.

30 / 102

   feature engineering   

(cid:73) many advances in nlp (not just id38) have come from careful

design of features.

(cid:73) sometimes    feature engineering    is used pejoratively.
(cid:73) some people would rather not spend their time on it!

31 / 102

   feature engineering   

(cid:73) many advances in nlp (not just id38) have come from careful

design of features.

(cid:73) sometimes    feature engineering    is used pejoratively.
(cid:73) some people would rather not spend their time on it!

(cid:73) there is some work on automatically inducing features (della pietra et al., 1997).

32 / 102

   feature engineering   

(cid:73) many advances in nlp (not just id38) have come from careful

design of features.

(cid:73) sometimes    feature engineering    is used pejoratively.
(cid:73) some people would rather not spend their time on it!

(cid:73) there is some work on automatically inducing features (della pietra et al., 1997).
(cid:73) more recent work in neural networks can be seen as discovering features (instead

of engineering them).

33 / 102

   feature engineering   

(cid:73) many advances in nlp (not just id38) have come from careful

design of features.

(cid:73) sometimes    feature engineering    is used pejoratively.
(cid:73) some people would rather not spend their time on it!

(cid:73) there is some work on automatically inducing features (della pietra et al., 1997).
(cid:73) more recent work in neural networks can be seen as discovering features (instead

of engineering them).

(cid:73) but in much of nlp, there   s a strong preference for interpretable features.

34 / 102

how to estimate w?

p  (x) =

id165

(cid:96)(cid:89)

j=1

  xj|hj

(cid:96)(cid:89)

log-linear id165

exp w      (hj, xj)

j   1

zw(hj)

parameters:

  v|h
   v     v, h     (v     {(cid:13)})n   1

wk
   k     {1, . . . , d}

id113:

    v|h =

c(hv)
c(h)

no closed form

35 / 102

id113 for w

(cid:73) let training data consist of {(hi, xi)}n

i=1.

36 / 102

id113 for w

(cid:73) let training data consist of {(hi, xi)}n
(cid:73) id113 is:

i=1.

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

log pw(xi | hi)

max
w   rd

= max
w   rd

log

exp w      (hi, xi)

zw(hi)

= max
w   rd

w      (hi, xi)     log

exp w      (hi, v)

(cid:123)(cid:122)

zw(hi)

(cid:125)

(cid:88)
(cid:124)

v   v

37 / 102

id113 for w

(cid:73) let training data consist of {(hi, xi)}n
(cid:73) id113 is:

i=1.

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

log pw(xi | hi)

max
w   rd

= max
w   rd

log

exp w      (hi, xi)

zw(hi)

= max
w   rd

w      (hi, xi)     log

exp w      (hi, v)

(cid:123)(cid:122)

zw(hi)

(cid:125)

(cid:88)
(cid:124)

v   v

(cid:73) this is concave in w.

38 / 102

id113 for w

(cid:73) let training data consist of {(hi, xi)}n
(cid:73) id113 is:

i=1.

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

log pw(xi | hi)

max
w   rd

= max
w   rd

log

exp w      (hi, xi)

zw(hi)

= max
w   rd

w      (hi, xi)     log

exp w      (hi, v)

(cid:123)(cid:122)

zw(hi)

(cid:125)

(cid:88)
(cid:124)

v   v

(cid:73) this is concave in w.
(cid:73) zw(hi) involves a sum over v terms.

39 / 102

id113 for w

n(cid:88)

i=1

max
w   rd

(cid:124)

w      (hi, xi)     log zw(hi)

(cid:123)(cid:122)

fi(w)

(cid:125)

40 / 102

id113 for w

n(cid:88)

w      (hi, xi)     log zw(hi)

(cid:124)

max
w   rd

(cid:123)(cid:122)
(cid:73) decrease the    softened max    score overall, log(cid:80)

hope/fear view: for each instance i,

fi(w)

i=1

(cid:73) increase the score of the correct output xi, score(xi) = w      (hi, xi)

(cid:125)

v   v exp score(v)

41 / 102

id113 for w

n(cid:88)

i=1

max
w   rd

(cid:124)

w      (hi, xi)     log zw(hi)

(cid:125)

fi(w)

(cid:123)(cid:122)
   (cid:88)
(cid:124)

v   v

gradient view:

   wfi =

(cid:123)(cid:122)

(cid:125)

  (hi, xi)

(cid:124)

observed features

pw(v | hi)      (hi, v)

(cid:123)(cid:122)

(cid:125)

expected features

setting this to zero means getting model   s expectations to match empirical
observations.

42 / 102

id113 for w: algorithms

(cid:73) batch methods (l-bfgs is popular)
(cid:73) stochastic gradient ascent/descent more common today, especially with special

tricks for adapting the step size over time

(cid:73) many specialized methods (e.g.,    iterative scaling   )

43 / 102

stochastic id119

goal: minimize(cid:80)n

i=1 fi(w) with respect to w.

input: initial value w, number of epochs t , learning rate   

for t     {1, . . . , t}:

(cid:73) choose a random permutation    of {1, . . . , n}.
(cid:73) for i     {1, . . . , n}:

w     w              wf  (i)

output: w

44 / 102

avoiding over   tting

id113:

n(cid:88)

i=1

max
w   rd

w      (hi, xi)     log zw(hi)

(cid:73) if   j(h, x) is (almost) always positive, we can always increase the objective (a

little bit) by increasing wj toward +   .

45 / 102

avoiding over   tting

id113:

n(cid:88)

i=1

max
w   rd

w      (hi, xi)     log zw(hi)

(cid:73) if   j(h, x) is (almost) always positive, we can always increase the objective (a

little bit) by increasing wj toward +   .

standard solution is to add a id173 term:

n(cid:88)

i=1

max
w   rd

(cid:88)

v   v

w      (hi, xi)     log

exp w      (hi, v)       (cid:107)w(cid:107)p

p

where    > 0 is a hyperparameter and p = 2 or 1.

46 / 102

id113 for w

if we had more time, we   d study this problem more carefully!

here   s what you must remember:

(cid:73) there is no closed form; you must use a numerical optimization algorithm like

stochastic id119.

(cid:73) id148 are powerful but expensive (zw(hi)).
(cid:73) id173 is very important; we don   t actually do id113.

(cid:73) just like for id165 models! only even more so, since id148 are even

more expressive.

47 / 102

quick recap

two kinds of language models so far:

representation?
hi is (n     1) previous symbols
featurized representation of (cid:104)hi, xi(cid:105)

id165

log-linear

estimation?

think about?

count and normalize

smoothing

iterative id119

features

48 / 102

neural network: de   nitions
warning: there is no widely accepted standard notation!

a feedforward neural network n   is de   ned by:

(cid:73) a function family that maps parameter values to functions of the form

n : rdin     rdout ; typically:

(cid:73) non-linear
(cid:73) di   erentiable with respect to its inputs
(cid:73)    assembled    through a series of a   ne transformations and non-linearities, composed

together

(cid:73) symbolic/discrete inputs handled through lookups.

(cid:73) parameter values   

(cid:73) typically a collection of scalars, vectors, and matrices
(cid:73) we often assume they are linearized into rd

49 / 102

a couple of useful functions

(cid:73) softmax : rk     rk

(cid:104)x1, x2, . . . , xk(cid:105) (cid:55)   

(cid:42)

(cid:73) tanh : r     [   1, 1]

ex1(cid:80)k

j=1 exj

,

ex2(cid:80)k

j=1 exj

, . . . ,

exk(cid:80)k

j=1 exj

(cid:43)

x (cid:55)    ex     e   x
ex + e   x

generalized to be elementwise, so that it maps rk     [   1, 1]k.

(cid:73) others include: relus, logistic sigmoids, prelus, . . .

50 / 102

   one hot    vectors

arbitrarily order the words in v, giving each an index in {1, . . . , v }.

let ei     rv contain all zeros, with the exception of a 1 in position i.

this is the    one hot    vector for the ith word in v.

51 / 102

feedforward neural network language model
(bengio et al., 2003)

de   ne the id165 id203 as follows:

p(   | (cid:104)h1, . . . , hn   1(cid:105)) = n  

      b

v

n   1(cid:88)

ehj

(cid:62)m

v    d

j=1

v

softmax

+

(cid:0)(cid:104)eh1, . . . , ehn   1(cid:105)(cid:1) =
      u

tanh

+ w
v    h

h

aj
d    v

+

            

n   1(cid:88)

j=1

e(cid:62)

hj

m tj
d    h

where each ehj     rv is a one-hot vector and h is the number of    hidden units    in the
neural network (a    hyperparameter   ).

parameters    include:

(cid:73) m     rv   d, which are called    embeddings    (row vectors), one for every word in v
(cid:73) feedforward nn parameters b     rv , a     r(n   1)  d  v , w     rv   h , u     rh ,

t     r(n   1)  d  h

52 / 102

breaking it down

look up each of the history words hj,   j     {1, . . . , n     1} in m; keep two copies.

v    d

(cid:62)m
(cid:62)m

v    d

ehj

v

ehj

v

53 / 102

breaking it down

look up each of the history words hj,   j     {1, . . . , n     1} in m; keep two copies.
rename the embedding for hj as mhj .

(cid:62)m = mhj
(cid:62)m = mhj

ehj

ehj

54 / 102

breaking it down

apply an a   ne transformation to the second copy of the history-id27s (u,
t)

mhj

+

u

h

n   1(cid:88)

j=1

mhj tj
d    h

55 / 102

breaking it down

apply an a   ne transformation to the second copy of the history-id27s (u,
t) and a tanh nonlinearity.

       u +

mhj

tanh

n   1(cid:88)

j=1

      

mhj tj

56 / 102

breaking it down

apply an a   ne transformation to everything (b, a, w).

n   1(cid:88)

j=1

+

b

v

mhj aj
d    v

       u +

+ w
v    h

tanh

      

n   1(cid:88)

j=1

mhj tj

57 / 102

breaking it down

apply a softmax transformation to make the vector sum to one.

softmax

       b +

n   1(cid:88)

j=1

+ w tanh

mhj aj

       u +

            

n   1(cid:88)

j=1

mhj tj

58 / 102

breaking it down

       b +

n   1(cid:88)

j=1

softmax

mhj aj

       u +

            

n   1(cid:88)

j=1

mhj tj

+ w tanh

like a log-linear language model with two kinds of features:
(cid:73) concatenation of context-id27s vectors mhj
(cid:73) tanh-a   ne transformation of the above

new parameters arise from (i) embeddings and (ii) a   ne transformation    inside    the
nonlinearity.

59 / 102

visualization

60 / 102

mu,tb, atanhsoftmaxwnumber of parameters

d = v d(cid:124)(cid:123)(cid:122)(cid:125)

m

+ v(cid:124)(cid:123)(cid:122)(cid:125)

b

+ (n     1)dv

(cid:124)

(cid:123)(cid:122)

a

(cid:125)

+ v h(cid:124)(cid:123)(cid:122)(cid:125)

w

+ h(cid:124)(cid:123)(cid:122)(cid:125)

u

(cid:124)

+ (n     1)dh

(cid:123)(cid:122)

t

(cid:125)

for bengio et al. (2003):

(cid:73) v     18000 (after oov processing)
(cid:73) d     {30, 60}
(cid:73) h     {50, 100}
(cid:73) n     1 = 5

so d = 461v + 30100 parameters, compared to o(v n) for classical id165 models.
(cid:73) forcing a = 0 eliminated 300v parameters and performed a bit better, but was

slower to converge.

(cid:73) if we averaged mhj instead of concatenating, we   d get to 221v + 6100 (this is a

variant of    continuous bag of words,    mikolov et al., 2013).

61 / 102

why does it work?

62 / 102

why does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

63 / 102

why does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

(cid:73) suppose we want y = xor(x1, x2); this can   t be expressed as a linear function of x1

and x2.

64 / 102

xor example

tuples where y = xor(x1, x2) are red; tuples where y (cid:54)= xor(x1, x2) are blue.

65 / 102

x1x2ywhy does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

(cid:73) suppose we want y = xor(x1, x2); this can   t be expressed as a linear function of x1

and x2. but:

z = x1    x2
y = x1 + x2     2z

66 / 102

xor example (d = 13)
credit: chris dyer (https://github.com/clab/id98/blob/master/examples/xor.cc)

(cid:88)
(cid:88)

x1   {0,1}

(cid:88)
(cid:88)

x2   {0,1}

x1   {0,1}

x2   {0,1}

min

v,a,w,b

min

v,a,w,b

(cid:18)
(cid:18)

(cid:62)(cid:18)

w
3    2

3

3

+ b

3

x

2

(cid:18)

w
3    2

x

2

(cid:19)

(cid:19)2
(cid:19)

+ a

(cid:19)2

xor(x1, x2)    v

xor(x1, x2)    v

(cid:62) tanh

+ b

3

+ a

67 / 102

llllllllllllllllllllllllllllll051015202530012345iterationsmean squared errorwhy does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

(cid:73) suppose we want y = xor(x1, x2); this can   t be expressed as a linear function of x1

and x2. but:

z = x1    x2
y = x1 + x2     2z

(cid:73) with high-dimensional inputs, there are a lot of conjunctive features to search

through. for id148, della pietra et al. (1997) did this, greedily.

68 / 102

why does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

(cid:73) suppose we want y = xor(x1, x2); this can   t be expressed as a linear function of x1

and x2. but:

z = x1    x2
y = x1 + x2     2z

(cid:73) with high-dimensional inputs, there are a lot of conjunctive features to search

through. for id148, della pietra et al. (1997) did this, greedily.

(cid:73) neural models seem to smoothly explore lots of approximately-conjunctive features.

69 / 102

why does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

(cid:73) suppose we want y = xor(x1, x2); this can   t be expressed as a linear function of x1

and x2. but:

z = x1    x2
y = x1 + x2     2z

(cid:73) with high-dimensional inputs, there are a lot of conjunctive features to search

through. for id148, della pietra et al. (1997) did this, greedily.

(cid:73) neural models seem to smoothly explore lots of approximately-conjunctive features.
(cid:73) modern answer: representations of words and histories are tuned to the prediction

problem.

70 / 102

why does it work?

(cid:73) historical answer: multiple layers and nonlinearities allow feature combinations a

linear model can   t get.

(cid:73) suppose we want y = xor(x1, x2); this can   t be expressed as a linear function of x1

and x2. but:

z = x1    x2
y = x1 + x2     2z

(cid:73) with high-dimensional inputs, there are a lot of conjunctive features to search

through. for id148, della pietra et al. (1997) did this, greedily.

(cid:73) neural models seem to smoothly explore lots of approximately-conjunctive features.
(cid:73) modern answer: representations of words and histories are tuned to the prediction

problem.

(cid:73) id27s: a powerful idea . . .

71 / 102

important idea: words as vectors

the idea of    embedding    words in rd is much older than neural language models.

72 / 102

important idea: words as vectors

the idea of    embedding    words in rd is much older than neural language models.
you should think of this as a generalization of the discrete view of v.

73 / 102

important idea: words as vectors

the idea of    embedding    words in rd is much older than neural language models.
you should think of this as a generalization of the discrete view of v.

(cid:73) considerable ongoing research on learning word representations to capture

linguistic similarity (turney and pantel, 2010); this is known as vector space
semantics.

74 / 102

words as vectors: example

75 / 102

babycatwords as vectors: example

76 / 102

babycatpigmouseparameter estimation

bad news for neural language models:

(cid:73) log-likelihood function is not concave.

(cid:73) so any perplexity experiment is evaluating the model and an algorithm for

estimating it.

(cid:73) calculating log-likelihood and its gradient is very expensive (5 epochs took 3

weeks on 40 cpus).

77 / 102

parameter estimation

bad news for neural language models:

(cid:73) log-likelihood function is not concave.

(cid:73) so any perplexity experiment is evaluating the model and an algorithm for

estimating it.

(cid:73) calculating log-likelihood and its gradient is very expensive (5 epochs took 3

weeks on 40 cpus).

good news:

(cid:73) n   is di   erentiable with respect to m (from which its inputs come) and    (its

parameters), so gradient-based methods are available.

(cid:73) essential: the chain rule from calculus (sometimes called    id26   )
lots more details in bengio et al. (2003) and (for nns more generally) in goldberg
(2015).

78 / 102

next up

more examples of neural language models (in brief):

(cid:73) the log-bilinear language model
(cid:73) recurrent neural network language models

79 / 102

log-bilinear language model
(mnih and hinton, 2007)

de   ne the id165 id203 as follows, for each v     v:

p(v | (cid:104)h1, . . . , hn   1(cid:105)) =

      n   1(cid:88)
(cid:18)
      n   1(cid:88)

j=1

j=1

exp

(cid:88)

v(cid:48)   v

exp

(cid:62)(cid:19)
(cid:62)(cid:19)

mv

      

+cv

d

mv(cid:48)

+cv

d

(cid:62)aj

d    d

+ b

d

mhj

d

(cid:18)

(cid:62)aj

d    d

mhj

d

+ b

d

      

80 / 102

log-bilinear language model
(mnih and hinton, 2007)

de   ne the id165 id203 as follows, for each v     v:

      

+cv

d

mv(cid:48)

+cv

d

exp

+ b

mv

j=1

      n   1(cid:88)
(cid:18)
      n   1(cid:88)
(cid:124)
(cid:123)(cid:122)

j=1

a

d

(cid:62)(cid:19)
(cid:62)(cid:19)
+ v(cid:124)(cid:123)(cid:122)(cid:125)

+ b

d

c

(cid:62)aj

d    d

mhj

d

(cid:18)

mhj

(cid:62)aj

d    d

(cid:125)

d

+ d(cid:124)(cid:123)(cid:122)(cid:125)

b

+ (n     1)d2

p(v | (cid:104)h1, . . . , hn   1(cid:105)) =

(cid:88)
(cid:73) number of parameters: d = v d(cid:124)(cid:123)(cid:122)(cid:125)

v(cid:48)   v

m

exp

      

81 / 102

log-bilinear language model
(mnih and hinton, 2007)

de   ne the id165 id203 as follows, for each v     v:

      

+cv

d

mv(cid:48)

+cv

d

      

exp

+ b

mv

j=1

      n   1(cid:88)
(cid:18)
      n   1(cid:88)
(cid:124)
(cid:123)(cid:122)

j=1

a

d

(cid:62)(cid:19)
(cid:62)(cid:19)
+ v(cid:124)(cid:123)(cid:122)(cid:125)

+ b

d

c

(cid:62)aj

d    d

mhj

d

(cid:18)

mhj

(cid:62)aj

d    d

(cid:125)

d

+ d(cid:124)(cid:123)(cid:122)(cid:125)

b

+ (n     1)d2

p(v | (cid:104)h1, . . . , hn   1(cid:105)) =

(cid:88)
(cid:73) number of parameters: d = v d(cid:124)(cid:123)(cid:122)(cid:125)

v(cid:48)   v

m

exp

(cid:73) the predicted word   s id203 depends on its vector mv, not just on the vectors

of the history words.

82 / 102

log-bilinear language model
(mnih and hinton, 2007)

de   ne the id165 id203 as follows, for each v     v:

      

+cv

d

mv(cid:48)

+cv

d

      

exp

+ b

mv

j=1

      n   1(cid:88)
(cid:18)
      n   1(cid:88)
(cid:124)
(cid:123)(cid:122)

j=1

a

d

(cid:62)(cid:19)
(cid:62)(cid:19)
+ v(cid:124)(cid:123)(cid:122)(cid:125)

+ b

d

c

(cid:62)aj

d    d

mhj

d

(cid:18)

mhj

(cid:62)aj

d    d

(cid:125)

d

+ d(cid:124)(cid:123)(cid:122)(cid:125)

b

+ (n     1)d2

p(v | (cid:104)h1, . . . , hn   1(cid:105)) =

(cid:88)
(cid:73) number of parameters: d = v d(cid:124)(cid:123)(cid:122)(cid:125)

v(cid:48)   v

m

exp

(cid:73) the predicted word   s id203 depends on its vector mv, not just on the vectors

of the history words.

(cid:73) training this model involves a sum over the vocabulary (like id148 we

saw earlier).

83 / 102

log-bilinear language model
(mnih and hinton, 2007)

de   ne the id165 id203 as follows, for each v     v:

      

+cv

d

mv(cid:48)

+cv

d

      

exp

+ b

mv

j=1

      n   1(cid:88)
(cid:18)
      n   1(cid:88)
(cid:124)
(cid:123)(cid:122)

j=1

a

d

(cid:62)(cid:19)
(cid:62)(cid:19)
+ v(cid:124)(cid:123)(cid:122)(cid:125)

+ b

d

c

(cid:62)aj

d    d

mhj

d

(cid:18)

mhj

(cid:62)aj

d    d

(cid:125)

d

+ d(cid:124)(cid:123)(cid:122)(cid:125)

b

+ (n     1)d2

p(v | (cid:104)h1, . . . , hn   1(cid:105)) =

(cid:88)
(cid:73) number of parameters: d = v d(cid:124)(cid:123)(cid:122)(cid:125)

v(cid:48)   v

m

exp

(cid:73) the predicted word   s id203 depends on its vector mv, not just on the vectors

of the history words.

(cid:73) training this model involves a sum over the vocabulary (like id148 we

saw earlier).

(cid:73) later work explored variations to make learning faster.

84 / 102

observations about neural language models (so far)

(cid:73) there   s no knowledge built in that the most recent word hn   1 should generally be

more informative than earlier ones.

(cid:73) this has to be learned.

(cid:73) in addition to choosing n, also have to choose dimensionalities like d and h.
(cid:73) parameters of these models are hard to interpret.

85 / 102

observations about neural language models (so far)

(cid:73) there   s no knowledge built in that the most recent word hn   1 should generally be

more informative than earlier ones.

(cid:73) this has to be learned.

(cid:73) in addition to choosing n, also have to choose dimensionalities like d and h.
(cid:73) parameters of these models are hard to interpret.

(cid:73) example: (cid:96)2-norm of aj and tj in the feedforward model correspond to the

importance of history position j.

(cid:73) individual id27s can be clustered and dimensions can be analyzed (e.g.,

tsvetkov et al., 2015).

86 / 102

observations about neural language models (so far)

(cid:73) there   s no knowledge built in that the most recent word hn   1 should generally be

more informative than earlier ones.

(cid:73) this has to be learned.

(cid:73) in addition to choosing n, also have to choose dimensionalities like d and h.
(cid:73) parameters of these models are hard to interpret.
(cid:73) architectures are not intuitive.

87 / 102

observations about neural language models (so far)

(cid:73) there   s no knowledge built in that the most recent word hn   1 should generally be

more informative than earlier ones.

(cid:73) this has to be learned.

(cid:73) in addition to choosing n, also have to choose dimensionalities like d and h.
(cid:73) parameters of these models are hard to interpret.
(cid:73) architectures are not intuitive.
(cid:73) still, impressive perplexity gains got people   s interest.

88 / 102

recurrent neural network

(cid:73) each input element is understood to be an element of a sequence: (cid:104)x1, x2, . . . , x(cid:96)(cid:105)
(cid:73) at each timestep t:

(cid:73) the tth input element xt is processed alongside the previous state st   1 to calculate

the new state (st).

(cid:73) the tth output is a function of the state st.
(cid:73) the same functions are applied at each iteration:

st = frecurrent(xt, st   1)
yt = foutput(st)

in id56 language models, words and histories are represented as vectors (respectively,
xt = ext and st).

89 / 102

id56 language model

the original version, by mikolov et al. (2010) used a    simple    id56 architecture along
these lines:

(cid:19)

a + s(cid:62)

t   1b + c

(cid:17)(cid:62)

(cid:18)(cid:16)
(cid:16)

(cid:17)

e(cid:62)
xtm
s(cid:62)
t u

st = frecurrent(ext, st   1) = sigmoid

yt = foutput(st) = softmax

p(v | x1, . . . , xt   1) = [yt]v

note: this is not an id165 (markov) model!

90 / 102

visualization

91 / 102

ma,b,csigmoidsoftmaxust - 1stytxtvisualization

92 / 102

ma,b,csigmoidsoftmaxuma,b,csigmoidsoftmaxuma,b,csigmoidsoftmaxuma,b,csigmoidsoftmaxuimprovements to id56 language models

the simple id56 is known to su   er from two related problems:

(cid:73)    vanishing gradients    during learning make it hard to propagate error into the

distant past.

(cid:73) state tends to change a lot on each iteration; the model    forgets    too much.

some variants:

(cid:73)    stacking    these functions to make deeper networks.
(cid:73) sundermeyer et al. (2012) use    long short-term memories    (lstms) and cho

et al. (2014) use    id149    (grus) to de   ne frecurrent.

(cid:73) mikolov et al. (2014) engineer the linear transformation in the simple id56 for

better preservation.

(cid:73) jozefowicz et al. (2015) used randomized search to    nd even better architectures.

93 / 102

comparison: probabilistic vs. connectionist modeling

what do we engineer?

probabilistic
features, assumptions

connectionist
architectures

theory?

as n gets large

not really

interpretation of parame-
ters?

often easy

usually hard

94 / 102

parting shots

95 / 102

parting shots

(cid:73) i said very little about estimating the parameters.

96 / 102

parting shots

(cid:73) i said very little about estimating the parameters.

(cid:73) at present, this requires a lot of engineering.

97 / 102

parting shots

(cid:73) i said very little about estimating the parameters.

(cid:73) at present, this requires a lot of engineering.
(cid:73) new libraries to help you are coming out all the time.

98 / 102

parting shots

(cid:73) i said very little about estimating the parameters.

(cid:73) at present, this requires a lot of engineering.
(cid:73) new libraries to help you are coming out all the time.
(cid:73) many of them use gpus to speed things up.

99 / 102

parting shots

(cid:73) i said very little about estimating the parameters.

(cid:73) at present, this requires a lot of engineering.
(cid:73) new libraries to help you are coming out all the time.
(cid:73) many of them use gpus to speed things up.

(cid:73) this progression is worth re   ecting on:
history:
(n     1)-gram discrete

represented as:

before 1996
1996   2003
2003   2010
since 2010

feature vector
embedded vector
embedded

unrestricted

100 / 102

references i

yoshua bengio, r  ejean ducharme, pascal vincent, and christian jauvin. a neural probabilistic language model.

journal of machine learning research, 3(feb):1137   1155, 2003. url
http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf.

kyunghyun cho, bart van merrienboer, caglar gulcehre, dzmitry bahdanau, fethi bougares, holger schwenk,

and yoshua bengio. learning phrase representations using id56 encoder   decoder for statistical machine
translation. in proc. of emnlp, 2014.

michael collins. id148, memms, and crfs, 2011. url

http://www.cs.columbia.edu/~mcollins/crf.pdf.

stephen della pietra, vincent della pietra, and john la   erty. inducing features of random    elds. ieee

transactions on pattern analysis and machine intelligence, 19(4):380   393, 1997.

jacob eisenstein, amr ahmed, and eric p xing. sparse additive generative models of text. in proc. of icml,

2011.

yoav goldberg. a primer on neural network models for natural language processing, 2015. url

http://u.cs.biu.ac.il/~yogo/nnlp.pdf.

rafal jozefowicz, wojciech zaremba, and ilya sutskever. an empirical exploration of recurrent network

architectures. in proc. of icml, 2015. url
http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf.

daniel jurafsky and james h. martin. id165s (draft chapter), 2016. url

https://web.stanford.edu/~jurafsky/slp3/4.pdf.

101 / 102

references ii

raymond lau, ronald rosenfeld, and salim roukos. trigger-based language models: a maximum id178

approach. in proc. of icassp, 1993.

tomas mikolov, martin kara     at, lukas burget, jan cernock`y, and sanjeev khudanpur. recurrent neural

network based language model. in proc. of interspeech, 2010. url http:
//www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_is100722.pdf.

tomas mikolov, kai chen, greg corrado, and je   rey dean. e   cient estimation of word representations in

vector space. in proceedings of iclr, 2013. url http://arxiv.org/pdf/1301.3781.pdf.

tomas mikolov, armand joulin, sumit chopra, michael mathieu, and marc   aurelio ranzato. learning longer

memory in recurrent neural networks, 2014. arxiv:1412.7753.

andriy mnih and geo   rey hinton. three new id114 for statistical language modelling. in proc. of

icml, 2007.

noah a. smith. probabilistic language models 1.0, 2017. url

http://homes.cs.washington.edu/~nasmith/papers/plm.17.pdf.

martin sundermeyer, ralf schl  uter, and hermann ney. lstm neural networks for id38. in proc.

of interspeech, 2012.

yulia tsvetkov, manaal faruqui, wang ling, guillaume lample, and chris dyer. evaluation of word vector

representations by subspace alignment. in proc. of emnlp, 2015.

peter d. turney and patrick pantel. from frequency to meaning: vector space models of semantics. journal of

arti   cial intelligence research, 37(1):141   188, 2010. url
https://www.jair.org/media/2934/live-2934-4846-jair.pdf.

102 / 102

