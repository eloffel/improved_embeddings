natural language processing (csep 517):

text classi   cation

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

april 10, 2017

1 / 71

to-do list

(cid:73) online quiz: due sunday
(cid:73) read: jurafsky and martin (2016b); collins (2011); jurafsky and martin (2016a)
(cid:73) a2 due april 23 (sunday)
(cid:73) final exam will be on canvas (posted    5/29, due    6/4)

2 / 71

text classi   cation

input: a piece of text x     v   , usually a document (r.v. x)
output: a label from a    nite set l (r.v. l)

standard line of attack:

1. human experts label some data.

2. feed the data to a supervised machine learning algorithm that constructs an

automatic classi   er classify : v        l

3. apply classify to as much data as you want!

note: we assume the texts are segmented already, even the new ones.

3 / 71

text classi   cation: examples

(cid:73) library-like subjects (e.g., the dewey decimal system)
(cid:73) news stories: politics vs. sports vs. business vs. technology ...
(cid:73) reviews of    lms, restaurants, products: postive vs. negative
(cid:73) author attributes: identity, political stance, gender, age, ...
(cid:73) email, arxiv submissions, etc.: spam vs. not
(cid:73) what is the reading level of a piece of text?
(cid:73) how in   uential will a scienti   c paper be?
(cid:73) will a piece of proposed legislation pass?

closely related: relevance to a query.

4 / 71

evaluation

accuracy:

a(classify) = p(classify(x) = l)

p(x = x, l = (cid:96))   

(cid:26) 1 if classify(x) = (cid:96)

0 otherwise

p(x = x, l = (cid:96))    1{classify(x) = (cid:96)}

(cid:88)
(cid:88)

x   v   ,(cid:96)   l

x   v   ,(cid:96)   l

=

=

where p is the true distribution over data. error is 1     a.
this is estimated using a test dataset (cid:104)  x1,   (cid:96)1(cid:105), . . .(cid:104)  xm,   (cid:96)m(cid:105):

m(cid:88)

1(cid:8)classify(  xi) =   (cid:96)i

(cid:9)

  a(classify) =

1
m

i=1

5 / 71

issues with test-set accuracy

6 / 71

issues with test-set accuracy

(cid:73) class imbalance: if p(l = not spam) = 0.99, then you can get   a     0.99 by

always guessing    not spam.   

7 / 71

issues with test-set accuracy

(cid:73) class imbalance: if p(l = not spam) = 0.99, then you can get   a     0.99 by

always guessing    not spam.   

(cid:73) relative importance of classes or cost of error types.

8 / 71

issues with test-set accuracy

(cid:73) class imbalance: if p(l = not spam) = 0.99, then you can get   a     0.99 by

always guessing    not spam.   

(cid:73) relative importance of classes or cost of error types.
(cid:73) variance due to the test data.

9 / 71

evaluation in the two-class case

suppose we have two classes, and one of them, t     l is a    target.   

(cid:73) e.g., given a query,    nd relevant documents.

precision and recall encode the goals of returning a    pure    set of targeted instances
and capturing all of them.

  p(classify) =

|c|
|b| =
|c|
|a| =
  f1(classify) = 2      p      r

  r(classify) =

|a     b|

|a     b|

|b|

|a|

  p +   r

10 / 71

actually in the target class;l = tbelieved to be in the target class;classify(x) = tcorrectly labeled as tabcanother view: contingency table

l = t

l (cid:54)= t

classify(x) = t
classify(x) (cid:54)= t a \ c (false negatives)

c (true positives)

b \ c (false positives) b

(true negatives)

a

11 / 71

evaluation with > 2 classes

macroaveraged precision and recall: let each class be the target and report the average
  p and   r across all classes.

microaveraged precision and recall: pool all one-vs.-rest decisions into a single
contingency table, calculate   p and   r from that.

12 / 71

cross-validation

remember that   a,   p,   r, and   f1 are all estimates of the classi   er   s quality under the
true data distribution.

(cid:73) estimates are noisy!

k-fold cross-validation:

(cid:73) partition the training set into k non-overlapping    folds    x1, . . . , xk.
(cid:73) for i     {1, . . . , k}:

(cid:73) train on x1:n \ xi, using xi as development data.
(cid:73) estimate quality on the ith development set:   ai

(cid:73) report the average:

k(cid:88)

i=1

  ai

  a =

1
k

and perhaps also the standard error.

13 / 71

statistical signi   cance

suppose we have two classi   ers, classify1 and classify2.

14 / 71

statistical signi   cance

suppose we have two classi   ers, classify1 and classify2.

is classify1 better? the    null hypothesis,    denoted h0, is that it isn   t. but if
  a1 (cid:29)   a2, we are tempted to believe otherwise.

15 / 71

statistical signi   cance

suppose we have two classi   ers, classify1 and classify2.

is classify1 better? the    null hypothesis,    denoted h0, is that it isn   t. but if
  a1 (cid:29)   a2, we are tempted to believe otherwise.

how much larger must   a1 be than   a2 to reject h0?

16 / 71

statistical signi   cance

suppose we have two classi   ers, classify1 and classify2.

is classify1 better? the    null hypothesis,    denoted h0, is that it isn   t. but if
  a1 (cid:29)   a2, we are tempted to believe otherwise.

how much larger must   a1 be than   a2 to reject h0?

frequentist view: how (im)probable is the observed di   erence, given h0 = true?

17 / 71

statistical signi   cance

suppose we have two classi   ers, classify1 and classify2.

is classify1 better? the    null hypothesis,    denoted h0, is that it isn   t. but if
  a1 (cid:29)   a2, we are tempted to believe otherwise.

how much larger must   a1 be than   a2 to reject h0?

frequentist view: how (im)probable is the observed di   erence, given h0 = true?

caution: statistical signi   cance is neither necessary nor su   cient for research
signi   cance or practical usefulness!

18 / 71

a hypothesis test for text classi   ers
mcnemar (1947)

1. the null hypothesis: a1 = a2
2. pick signi   cance level   , an    acceptably    high id203 of incorrectly rejecting

h0.

3. calculate the test statistic, k (explained in the next slide).
4. calculate the id203 of a more extreme value of k, assuming h0 is true; this

is the p-value.

5. reject the null hypothesis if the p-value is less than   .

the p-value is p(this observation | h0 is true), not the other way around!

19 / 71

mcnemar   s test: details

assumptions: independent (test) samples and binary measurements. count test set
error patterns:

classify1 is incorrect

classify1 is correct

classify2 is incorrect

classify2 is correct

c00

c01

c10

c11
m      a1

m      a2

if a1 = a2, then c01 and c10 are each distributed according to binomial(c01 + c10, 1

2 ).

test statistic k = min{c01, c10}

p-value =

1

2c01+c10   1

k(cid:88)

(cid:18)c01 + c10

(cid:19)

j=0

j

20 / 71

other tests

di   erent tests make di   erent assumptions.

sometimes we calculate an interval that would be    unsurprising    under h0 and test
whether a test statistic falls in that interval (e.g., t-test and wald test).

in many cases, there is no closed form for estimating p-values, so we use random
approximations (e.g., permutation test and paired bootstrap test).

if you do lots of tests, you need to correct for that!

read lots more in smith (2011), appendix b.

21 / 71

features in text classi   cation

running example: x =   the vodka was great, but don   t touch the hamburgers.   

a di   erent representation of the text sequence r.v. x: feature r.v.s.
for j     {1, . . . , d}, let fj be a discrete random variable taking a value in fj.

22 / 71

features in text classi   cation

running example: x =   the vodka was great, but don   t touch the hamburgers.   

a di   erent representation of the text sequence r.v. x: feature r.v.s.
for j     {1, . . . , d}, let fj be a discrete random variable taking a value in fj.

(cid:73) often, these are term (word and perhaps id165) frequencies.

e.g., fhamburgers(x) = 1, fthe(x) = 2, fdelicious(x) = 0, fdon   t touch(x) = 1.

23 / 71

features in text classi   cation

running example: x =   the vodka was great, but don   t touch the hamburgers.   

a di   erent representation of the text sequence r.v. x: feature r.v.s.
for j     {1, . . . , d}, let fj be a discrete random variable taking a value in fj.

(cid:73) often, these are term (word and perhaps id165) frequencies.

e.g., fhamburgers(x) = 1, fthe(x) = 2, fdelicious(x) = 0, fdon   t touch(x) = 1.

(cid:73) can also be word    presence    features.

e.g., fhamburgers(x) = 1, fthe(x) = 1, fdelicious(x) = 0, fdon   t touch(x) = 1.

24 / 71

features in text classi   cation

running example: x =   the vodka was great, but don   t touch the hamburgers.   

a di   erent representation of the text sequence r.v. x: feature r.v.s.
for j     {1, . . . , d}, let fj be a discrete random variable taking a value in fj.

(cid:73) often, these are term (word and perhaps id165) frequencies.

e.g., fhamburgers(x) = 1, fthe(x) = 2, fdelicious(x) = 0, fdon   t touch(x) = 1.

(cid:73) can also be word    presence    features.

e.g., fhamburgers(x) = 1, fthe(x) = 1, fdelicious(x) = 0, fdon   t touch(x) = 1.

(cid:73) transformations on word frequencies: logarithm, idf weighting

   v     v, idf(v) = log

n

|i : cxi(v) > 0|

25 / 71

features in text classi   cation

running example: x =   the vodka was great, but don   t touch the hamburgers.   

a di   erent representation of the text sequence r.v. x: feature r.v.s.
for j     {1, . . . , d}, let fj be a discrete random variable taking a value in fj.

(cid:73) often, these are term (word and perhaps id165) frequencies.

e.g., fhamburgers(x) = 1, fthe(x) = 2, fdelicious(x) = 0, fdon   t touch(x) = 1.

(cid:73) can also be word    presence    features.

e.g., fhamburgers(x) = 1, fthe(x) = 1, fdelicious(x) = 0, fdon   t touch(x) = 1.

(cid:73) transformations on word frequencies: logarithm, idf weighting

   v     v, idf(v) = log

n

|i : cxi(v) > 0|

(cid:73) disjunctions of terms

(cid:73) clusters
(cid:73) task-speci   c lexicons

26 / 71

probabilistic classi   cation

classi   cation rule:

classify(f ) = argmax

(cid:96)   l

= argmax

(cid:96)   l

= argmax

(cid:96)   l

p((cid:96) | f )

p((cid:96), f )
p(f )

p((cid:96), f )

27 / 71

na    ve bayes classi   er

p(l = (cid:96), f1 = f1, . . . , fd = fd) = p((cid:96))

p(fj = fj | (cid:96))

d(cid:89)
d(cid:89)

j=1

j=1

=   (cid:96)

  fj|j,(cid:96)

parameters:

(cid:73)    is the    class prior    (it sums to one)
(cid:73) for each feature function j and label (cid:96), a distribution over values      |j,(cid:96) (sums to

one for every (cid:104)j, (cid:96)(cid:105) pair)

the    bag of words    version of na    ve bayes:

fj = xj

|x|(cid:89)

p((cid:96), x) =   (cid:96)

  xj|(cid:96)

j=1

28 / 71

na    ve bayes: remarks

(cid:73) estimation by (smoothed) relative frequency estimation: easy!

29 / 71

na    ve bayes: remarks

(cid:73) estimation by (smoothed) relative frequency estimation: easy!
(cid:73) for continuous or integer-valued features, use di   erent distributions.

30 / 71

na    ve bayes: remarks

(cid:73) estimation by (smoothed) relative frequency estimation: easy!
(cid:73) for continuous or integer-valued features, use di   erent distributions.
(cid:73) the bag of words version equates to building a conditional language model for

each label.

31 / 71

na    ve bayes: remarks

(cid:73) estimation by (smoothed) relative frequency estimation: easy!
(cid:73) for continuous or integer-valued features, use di   erent distributions.
(cid:73) the bag of words version equates to building a conditional language model for

each label.

(cid:73) the collins reading assumes a binary version, with fv indicating whether v     v

occurs in x.

32 / 71

generative vs. discriminative classi   cation

na    ve bayes is the prototypical generative classi   er.

(cid:73) it describes a probabilistic process      generative story      for x and l.
(cid:73) but why model x? it   s always observed?

discriminative models instead:

(cid:73) seek to optimize a performance measure, like accuracy, or a computationally

convenient surrogate;

(cid:73) do not worry about p(x);
(cid:73) tend to perform better when you have reasonable amounts of data.

33 / 71

discriminative text classi   ers

(cid:73) multinomial id28 (also known as    max ent    and    log-linear   )
(cid:73) support vector machines
(cid:73) neural networks
(cid:73) id90

i   ll brie   y touch on three ways to train a classi   er with a linear decision rule.

34 / 71

linear models for classi   cation

  (cid:96) = argmax

(cid:96)   l

w      (x, (cid:96))

   linear    decision rule:

where    : v       l     rd.

parameters: w     rd

what does this remind you of?

35 / 71

linear models for classi   cation

   linear    decision rule:

  (cid:96) = argmax

(cid:96)   l

w      (x, (cid:96))

where    : v       l     rd.
parameters: w     rd

what does this remind you of?

some notational variants de   ne:

(cid:73) w(cid:96) for each (cid:96)     l
(cid:73)    : v        rd (similar to what we had for na    ve bayes)

36 / 71

the geometric view of linear classi   ers

suppose we have instance x, l = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

as a simple example, let the two features be binary functions.

37 / 71

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view of linear classi   ers

suppose we have instance x, l = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

w       = w1  1 + w2  2 = 0

38 / 71

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view of linear classi   ers

suppose we have instance x, l = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

distance(w       = 0,   0) =

|w      0|
(cid:107)w(cid:107)2

    |w      0|

39 / 71

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view of linear classi   ers

suppose we have instance x, l = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

w      (x, y1) > w      (x, y3) > w      (x, y4) > 0     w      (x, y2)

40 / 71

(x, y3)  1  2(x, y1)(x, y4)(x, y2)the geometric view of linear classi   ers

suppose we have instance x, l = {y1, y2, y3, y4}, and there are only two features,   1
and   2.

41 / 71

(x, y3)  1  2(x, y1)(x, y4)(x, y2)id113 for multinomial id28

when we discussed log-linear language models, we transformed the score into a
id203 distribution. here, that would be:

p(l = (cid:96) | x) =

(cid:80)

exp w      (x, (cid:96))
(cid:96)(cid:48)   l exp w      (x, (cid:96)(cid:48))

42 / 71

id113 for multinomial id28

when we discussed log-linear language models, we transformed the score into a
id203 distribution. here, that would be:

p(l = (cid:96) | x) =

(cid:80)

exp w      (x, (cid:96))
(cid:96)(cid:48)   l exp w      (x, (cid:96)(cid:48))

id113 can be rewritten as a maximization problem:

n(cid:88)

(cid:124)

w      (xi, (cid:96)i)

(cid:123)(cid:122)

hope

(cid:125)

    log

(cid:124)

(cid:88)

(cid:96)(cid:48)   l

  w = argmax

w

i=1

exp w      (xi, (cid:96)(cid:48))

(cid:123)(cid:122)

fear

(cid:125)

43 / 71

id113 for multinomial id28

when we discussed log-linear language models, we transformed the score into a
id203 distribution. here, that would be:

p(l = (cid:96) | x) =

(cid:80)

exp w      (x, (cid:96))
(cid:96)(cid:48)   l exp w      (x, (cid:96)(cid:48))

id113 can be rewritten as a maximization problem:

n(cid:88)

(cid:124)

w      (xi, (cid:96)i)

(cid:123)(cid:122)

hope

(cid:125)

    log

(cid:124)

(cid:88)

(cid:96)(cid:48)   l

  w = argmax

w

i=1

exp w      (xi, (cid:96)(cid:48))

(cid:123)(cid:122)

fear

(cid:125)

recall from language models:
(cid:73) be wise and regularize!
(cid:73) solve with batch or stochastic gradient methods.
(cid:73) wj has an interpretation.

44 / 71

log loss for (x, (cid:96))

another view is to minimize the negated log-likelihood, which is known as    log loss   :

(cid:122)
(cid:0)log(cid:80)

(cid:125)(cid:124)

(cid:123)
(cid:96)(cid:48)   l exp w      (x, (cid:96)(cid:48))(cid:1)   

fear

(cid:122)

(cid:125)(cid:124)

hope

(cid:123)

w      (x, (cid:96))

in the binary case, where the x-axis is the di   erence in scores between correct and
incorrect labels:

in blue is the log loss; in red is the    zero-one    loss (error).

45 / 71

   4   2024012345score differenceloss   log sum exp   

below, y-axis plots the    log(cid:80) exp    part of the objective function (with two labels),

against x, assuming the other score is one of {8, 0,   8}.

log(ex + e8), log(ex + e0), log(ex + e   8)

46 / 71

   10   50510   10   5051015hard maximum

why not use a hard max instead?

max(x, 8), max(x, 0), max(x,   8)

47 / 71

   10   50510   10   5051015hinge loss for (x, (cid:96))

(cid:18)

(cid:19)
(cid:96)(cid:48)   l w      (x, (cid:96)(cid:48))

max

    w      (x, (cid:96))

in the binary case:

in purple is the hinge loss, in blue is the log loss; in red is the    zero-one    loss (error).

48 / 71

   4   2024012345score differencelossminimizing hinge loss: id88

(cid:122)
(cid:18)

fear

(cid:125)(cid:124)

(cid:123)
(cid:19)
(cid:96)(cid:48)   l w      (x, (cid:96)(cid:48))

max

   

(cid:122)

(cid:125)(cid:124)

hope

(cid:123)

w      (x, (cid:96))

49 / 71

minimizing hinge loss: id88

(cid:122)
(cid:18)

fear

(cid:125)(cid:124)

(cid:123)
(cid:19)
(cid:96)(cid:48)   l w      (x, (cid:96)(cid:48))

max

   

(cid:122)

(cid:125)(cid:124)

hope

(cid:123)

w      (x, (cid:96))

when two labels are tied, the function is not di   erentiable.

50 / 71

minimizing hinge loss: id88

(cid:122)
(cid:18)

fear

(cid:125)(cid:124)

(cid:123)
(cid:19)
(cid:96)(cid:48)   l w      (x, (cid:96)(cid:48))

max

   

(cid:122)

(cid:125)(cid:124)

hope

(cid:123)

w      (x, (cid:96))

when two labels are tied, the function is not di   erentiable.

but it   s still sub-di   erentiable. solution: (stochastic) subid119!

51 / 71

log loss and hinge loss for (x, (cid:96))

log loss: (cid:0)log(cid:80)

(cid:18)

(cid:96)(cid:48)   l exp w      (x, (cid:96)(cid:48))(cid:1)     w      (x, (cid:96))
(cid:96)(cid:48)   l w      (x, (cid:96)(cid:48))

    w      (x, (cid:96))

(cid:19)

max

hinge loss:

in the binary case:

in purple is the hinge loss, in blue is the log loss; in red is the    zero-one    loss (error).

52 / 71

   4   2024012345score differencelossminimizing hinge loss: id88

n(cid:88)

i=1

min

w

(cid:18)

(cid:19)
(cid:96)(cid:48)   l w      (xi, (cid:96)(cid:48))

max

    w      (xi, (cid:96)i)

stochastic subid119 on the above is called the id88 algorithm.

(cid:73) for t     {1, . . . , t}:
(cid:16)

(cid:73) pick it uniformly at random from {1, . . . , n}.
(cid:73)   (cid:96)it     argmax(cid:96)   l w      (xit, (cid:96))
(cid:73) w     w       

(cid:17)
  (xit,   (cid:96)it)       (xit, (cid:96)it)

53 / 71

error costs

suppose that not all mistakes are equally bad.

e.g., false positives vs. false negatives in spam detection.

54 / 71

error costs

suppose that not all mistakes are equally bad.

e.g., false positives vs. false negatives in spam detection.

let cost((cid:96), (cid:96)(cid:48)) quantify the    badness    of substituting (cid:96)(cid:48) for correct label (cid:96).

55 / 71

error costs

suppose that not all mistakes are equally bad.

e.g., false positives vs. false negatives in spam detection.

let cost((cid:96), (cid:96)(cid:48)) quantify the    badness    of substituting (cid:96)(cid:48) for correct label (cid:96).

intuition: estimate the scoring function so that

score((cid:96)i)     score(  (cid:96))     cost((cid:96)i,   (cid:96))

56 / 71

general hinge loss for (x, (cid:96))

(cid:18)

(cid:96)(cid:48)   l w      (x, (cid:96)(cid:48)) + cost((cid:96), (cid:96)(cid:48))

max

(cid:19)

    w      (x, (cid:96))

in the binary case, with cost(   1, 1) = 1:

in blue is the general hinge loss; in red is the    zero-one    loss (error).

57 / 71

   4   20240123456general remarks

(cid:73) text classi   cation: many problems, all solved with supervised learners.

(cid:73) lexicon features can provide problem-speci   c guidance.

58 / 71

general remarks

(cid:73) text classi   cation: many problems, all solved with supervised learners.

(cid:73) lexicon features can provide problem-speci   c guidance.

(cid:73) na    ve bayes, log-linear, and linear id166 are all linear methods that tend to work

reasonably well, with good features and smoothing/id173.

(cid:73) you should have a basic understanding of the tradeo   s in choosing among them.

59 / 71

general remarks

(cid:73) text classi   cation: many problems, all solved with supervised learners.

(cid:73) lexicon features can provide problem-speci   c guidance.

(cid:73) na    ve bayes, log-linear, and linear id166 are all linear methods that tend to work

reasonably well, with good features and smoothing/id173.

(cid:73) you should have a basic understanding of the tradeo   s in choosing among them.
(cid:73) rumor: id79s are widely used in industry when performance matters

more than interpretability.

60 / 71

general remarks

(cid:73) text classi   cation: many problems, all solved with supervised learners.

(cid:73) lexicon features can provide problem-speci   c guidance.

(cid:73) na    ve bayes, log-linear, and linear id166 are all linear methods that tend to work

reasonably well, with good features and smoothing/id173.

(cid:73) you should have a basic understanding of the tradeo   s in choosing among them.
(cid:73) rumor: id79s are widely used in industry when performance matters

more than interpretability.

(cid:73) lots of papers about neural networks, though with hyperparameter tuning applied

fairly to linear models, the advantage is not clear (yogatama et al., 2015).

61 / 71

general remarks

(cid:73) text classi   cation: many problems, all solved with supervised learners.

(cid:73) lexicon features can provide problem-speci   c guidance.

(cid:73) na    ve bayes, log-linear, and linear id166 are all linear methods that tend to work

reasonably well, with good features and smoothing/id173.

(cid:73) you should have a basic understanding of the tradeo   s in choosing among them.
(cid:73) rumor: id79s are widely used in industry when performance matters

more than interpretability.

(cid:73) lots of papers about neural networks, though with hyperparameter tuning applied

fairly to linear models, the advantage is not clear (yogatama et al., 2015).

(cid:73) lots of work on feature design; most of the more sophisticated analysis methods

we cover starting next week can provide features for text categorization!

62 / 71

references i

michael collins. the naive bayes model, maximum-likelihood estimation, and the em algorithm, 2011. url

http://www.cs.columbia.edu/~mcollins/em.pdf.

koby crammer and yoram singer. on the algorithmic implementation of multiclass kernel-based vector

machines. journal of machine learning research, 2(5):265   292, 2001.

daniel jurafsky and james h. martin. id28 (draft chapter), 2016a. url

https://web.stanford.edu/~jurafsky/slp3/7.pdf.

daniel jurafsky and james h. martin. naive bayes and sentiment classi   cation (draft chapter), 2016b. url

https://web.stanford.edu/~jurafsky/slp3/6.pdf.

quinn mcnemar. note on the sampling error of the di   erence between correlated proportions or percentages.

psychometrika, 12(2):153   157, 1947.

noah a. smith. linguistic structure prediction. synthesis lectures on human language technologies. morgan

and claypool, 2011. url
http://www.morganclaypool.com/doi/pdf/10.2200/s00361ed1v01y201105hlt013.pdf.

dani yogatama, lingpeng kong, and noah a. smith. bayesian optimization of text representations. in proc. of

emnlp, 2015. url http://www.aclweb.org/anthology/d/d15/d15-1251.pdf.

63 / 71

extras

64 / 71

(linear) support vector machines

a di   erent motivation for the generalized hinge:

n(cid:88)

(cid:88)

i=1

(cid:96)   l

  w =

  i,(cid:96)      (xi, (cid:96))

where most only a small number of   i,(cid:96) are nonzero.

65 / 71

(linear) support vector machines

a di   erent motivation for the generalized hinge:

  w =

  i,(cid:96)      (xi, (cid:96))

where most only a small number of   i,(cid:96) are nonzero.

those   (xi, (cid:96)) are called    support vectors    because they    support    the decision
boundary.

  w      (x, (cid:96)(cid:48)) =

  i,(cid:96)      (xi, (cid:96))      (x, (cid:96)(cid:48))

n(cid:88)

(cid:88)

i=1

(cid:96)   l

(cid:88)

(i,(cid:96))   s

see crammer and singer (2001) for the multiclass version.

66 / 71

(linear) support vector machines

a di   erent motivation for the generalized hinge:

  w =

  i,(cid:96)      (xi, (cid:96))

where most only a small number of   i,(cid:96) are nonzero.

those   (xi, (cid:96)) are called    support vectors    because they    support    the decision
boundary.

  w      (x, (cid:96)(cid:48)) =

  i,(cid:96)      (xi, (cid:96))      (x, (cid:96)(cid:48))

see crammer and singer (2001) for the multiclass version.

really good tool: id166light, http://id166light.joachims.org

67 / 71

n(cid:88)

(cid:88)

i=1

(cid:96)   l

(cid:88)

(i,(cid:96))   s

support vector machines: remarks

(cid:73) id173 is critical; squared (cid:96)2 is most common, and often used in (yet

another) motivation around the idea of    maximizing margin    around the
hyperplane separator.

68 / 71

support vector machines: remarks

(cid:73) id173 is critical; squared (cid:96)2 is most common, and often used in (yet

another) motivation around the idea of    maximizing margin    around the
hyperplane separator.

(cid:73) often, instead of linear models that explicitly calculate w      , these methods are
   kernelized    and rearrange all calculations to involve inner-products between   
vectors.

(cid:73) example:

klinear(v, w) = v    w
kpolynomial(v, w) = (v    w + 1)p
kgaussian(v, w) = exp   (cid:107)v     w(cid:107)2

2

2  2

(cid:73) linear kernels are most common in nlp.

69 / 71

