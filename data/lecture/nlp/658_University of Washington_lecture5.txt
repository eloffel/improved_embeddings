natural language processing (csep 517):

phrase structure syntax and parsing

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

april 24, 2017

1 / 87

to-do list

(cid:73) online quiz: due sunday
(cid:73) ungraded mid-quarter survey: due sunday
(cid:73) read: jurafsky and martin (2008, ch. 12   14), collins (2011)
(cid:73) a3 due may 7 (sunday)

2 / 87

finite-state automata

a    nite-state automaton (plural    automata   ) consists of:

(cid:73) a    nite set of states s
(cid:73) initial state s0     s
(cid:73) final states f     s

(cid:73) a    nite alphabet   
(cid:73) transitions    : s           2s

(cid:73) special case: deterministic fsa de   nes    : s           s

a string x       n is recognizable by the fsa i    there is a sequence (cid:104)s0, . . . , sn(cid:105) such
that sn     f and

n(cid:94)

[[si       (si   1, xi)]]

this is sometimes called a path.

i=1

3 / 87

terminology from theory of computation

(cid:73) a regular expression can be:

(cid:73) an empty string (usually denoted  ) or a symbol from   
(cid:73) a concatentation of id157 (e.g., abc)
(cid:73) an alternation of id157 (e.g., ab|cd)
(cid:73) a kleene star of a regular expression (e.g., (abc)   )

(cid:73) a language is a set of strings.
(cid:73) a regular language is a language expressible by a regular expression.
(cid:73) important theorem: every regular language can be recognized by a fsa, and every

fsa   s language is regular.

4 / 87

proving a language isn   t regular

pumping lemma (for regular languages): if l is an in   nite regular language, then there
exist strings x, y, and z, with y (cid:54)=  , such that xynz     l, for all n     0.

if l is in   nite and x, y, z do not exist, then l is not regular.

5 / 87

s0ssfxyzproving a language isn   t regular

pumping lemma (for regular languages): if l is an in   nite regular language, then there
exist strings x, y, and z, with y (cid:54)=  , such that xynz     l, for all n     0.

if l is in   nite and x, y, z do not exist, then l is not regular.
if l1 and l2 are regular, then l1     l2 is regular.

6 / 87

s0ssfxyzproving a language isn   t regular

pumping lemma (for regular languages): if l is an in   nite regular language, then there
exist strings x, y, and z, with y (cid:54)=  , such that xynz     l, for all n     0.

if l is in   nite and x, y, z do not exist, then l is not regular.
if l1 and l2 are regular, then l1     l2 is regular.
if l1     l2 is not regular, and l1 is regular, then l2 is not regular.

7 / 87

s0ssfxyzclaim: english is not regular.

l1 = (the cat|mouse|dog)   (ate|bit|chased)    likes tuna    sh
l2 = english

l1     l2 = (the cat|mouse|dog)n(ate|bit|chased)n   1 likes tuna    sh

l1     l2 is not regular, but l1 is     l2 is not regular.

8 / 87

the cat likes tuna    sh

the cat the dog chased likes tuna    sh

the cat the dog the mouse scared chased likes tuna    sh

the cat the dog the mouse the elephant squashed scared chased likes tuna    sh

the cat the dog the mouse the elephant the    ea bit squashed scared chased likes tuna
   sh

the cat the dog the mouse the elephant the    ea the virus infected bit squashed scared
chased likes tuna    sh

9 / 87

linguistic debate

10 / 87

linguistic debate

chomsky put forward an argument like the one we just saw.

11 / 87

linguistic debate

chomsky put forward an argument like the one we just saw.

(chomsky gets credit for formalizing a hierarchy of types of languages: regular,
context-free, context-sensitive, recursively enumerable. this was an important
contribution to cs!)

12 / 87

linguistic debate

chomsky put forward an argument like the one we just saw.

(chomsky gets credit for formalizing a hierarchy of types of languages: regular,
context-free, context-sensitive, recursively enumerable. this was an important
contribution to cs!)

some are unconvinced, because after a few center embeddings, the examples become
unintelligible.

13 / 87

linguistic debate

chomsky put forward an argument like the one we just saw.

(chomsky gets credit for formalizing a hierarchy of types of languages: regular,
context-free, context-sensitive, recursively enumerable. this was an important
contribution to cs!)

some are unconvinced, because after a few center embeddings, the examples become
unintelligible.

nonetheless, most agree that natural language syntax isn   t well captured by fsas.

14 / 87

noun phrases

what, exactly makes a noun phrase? examples (jurafsky and martin, 2008):

(cid:73) harry the horse
(cid:73) the broadway coppers
(cid:73) they
(cid:73) a high-class spot such as mindy   s
(cid:73) the reason he comes into the hot box
(cid:73) three parties from brooklyn

15 / 87

constituents

more general than noun phrases: constituents are groups of words.

linguists characterize constituents in a number of ways, including:

16 / 87

constituents

more general than noun phrases: constituents are groups of words.

linguists characterize constituents in a number of ways, including:

(cid:73) where they occur (e.g.,    nps can occur before verbs   )
(cid:73) where they can move in variations of a sentence

(cid:73) on september 17th, i   d like to    y from atlanta to denver
(cid:73) i   d like to    y on september 17th from atlanta to denver
(cid:73) i   d like to    y from atlanta to denver on september 17th

17 / 87

constituents

more general than noun phrases: constituents are groups of words.

linguists characterize constituents in a number of ways, including:

(cid:73) where they occur (e.g.,    nps can occur before verbs   )
(cid:73) where they can move in variations of a sentence

(cid:73) on september 17th, i   d like to    y from atlanta to denver
(cid:73) i   d like to    y on september 17th from atlanta to denver
(cid:73) i   d like to    y from atlanta to denver on september 17th

(cid:73) what parts can move and what parts can   t

(cid:73) *on september i   d like to    y 17th from atlanta to denver

18 / 87

constituents

more general than noun phrases: constituents are groups of words.

linguists characterize constituents in a number of ways, including:

(cid:73) where they occur (e.g.,    nps can occur before verbs   )
(cid:73) where they can move in variations of a sentence

(cid:73) on september 17th, i   d like to    y from atlanta to denver
(cid:73) i   d like to    y on september 17th from atlanta to denver
(cid:73) i   d like to    y from atlanta to denver on september 17th

(cid:73) what parts can move and what parts can   t

(cid:73) *on september i   d like to    y 17th from atlanta to denver

(cid:73) what they can be conjoined with

(cid:73) i   d like to    y from atlanta to denver on september 17th and in the morning

19 / 87

recursion and constituents

this is the house

this is the house that jack built

this is the cat that lives in the house that jack built

this is the dog that chased the cat that lives in the house that jack built

this is the    ea that bit the dog that chased the cat that lives in the house the jack built

this is the virus that infected the    ea that bit the dog that chased the cat that lives in
the house that jack built

20 / 87

not constituents
(pullum, 1991)

(cid:73) if on a winter   s night a traveler (by italo calvino)
(cid:73) nuclear and radiochemistry (by gerhart friedlander et al.)
(cid:73) the fire next time (by james baldwin)
(cid:73) a tad overweight, but violet eyes to die for (by g.b. trudeau)
(cid:73) sometimes a great notion (by ken kesey)
(cid:73) [how can we know the] dancer from the dance (by andrew holleran)

21 / 87

context-free grammar

a context-free grammar consists of:

(cid:73) a    nite set of nonterminal symbols n

(cid:73) a start symbol s     n

(cid:73) a    nite alphabet   , called    terminal    symbols, distinct from n
(cid:73) production rule set r, each of the form    n           where

(cid:73) the lefthand side n is a nonterminal from n
(cid:73) the righthand side    is a sequence of zero or more terminals and/or nonterminals:

       (n       )   

(cid:73) special case: chomsky normal form constrains    to be either a single terminal

symbol or two nonterminals

22 / 87

an example id18 for a tiny bit of english
from jurafsky and martin (2008)
det     that | this | a
noun     book |    ight | meal | money
verb     book | include | prefer
pronoun     i | she | me
proper-noun     houston | nwa
aux     does
preposition     from | to | on | near
| through

s     np vp
s     aux np vp
s     vp
np     pronoun
np     proper-noun
np     det nominal
nominal     noun
nominal     nominal noun
nominal     nominal pp
vp     verb
vp     verb np
vp     verb np pp
vp     verb pp
vp     vp pp
pp     preposition np

23 / 87

example phrase structure tree

s

aux

does

np

vp

det

noun

this

   ight

verb

np

include

det

noun

a

meal

the phrase-structure tree represents both the syntactic structure of the sentence and
the derivation of the sentence under the grammar. e.g.,

corresponds to the

vp

rule vp     verb np.

verb np

24 / 87

the first phrase-structure tree
(chomsky, 1956)

sentence

np

vp

the man

v

np

took

the

book

25 / 87

where do natural language id18s come from?

as evidenced by the discussion in jurafsky and martin (2008), building a id18 for a
natural language by hand is really hard.

26 / 87

where do natural language id18s come from?

as evidenced by the discussion in jurafsky and martin (2008), building a id18 for a
natural language by hand is really hard.

(cid:73) need lots of categories to make sure all and only grammatical sentences are

included.

27 / 87

where do natural language id18s come from?

as evidenced by the discussion in jurafsky and martin (2008), building a id18 for a
natural language by hand is really hard.

(cid:73) need lots of categories to make sure all and only grammatical sentences are

included.

(cid:73) categories tend to start exploding combinatorially.

28 / 87

where do natural language id18s come from?

as evidenced by the discussion in jurafsky and martin (2008), building a id18 for a
natural language by hand is really hard.

(cid:73) need lots of categories to make sure all and only grammatical sentences are

included.

(cid:73) categories tend to start exploding combinatorially.
(cid:73) alternative grammar formalisms are typically used for manual grammar

construction; these are often based on constraints and a powerful algorithmic tool
called uni   cation.

29 / 87

where do natural language id18s come from?

as evidenced by the discussion in jurafsky and martin (2008), building a id18 for a
natural language by hand is really hard.

(cid:73) need lots of categories to make sure all and only grammatical sentences are

included.

(cid:73) categories tend to start exploding combinatorially.
(cid:73) alternative grammar formalisms are typically used for manual grammar

construction; these are often based on constraints and a powerful algorithmic tool
called uni   cation.

standard approach today:

1. build a corpus of annotated sentences, called a treebank. (memorable example:

the id32, marcus et al., 1993.)

2. extract rules from the treebank.

3. optionally, use statistical models to generalize the rules.

30 / 87

example from the id32

s

np-sbj

vp

np

nnp

nnp

,

,

adjp

np

pierre

vinken

cd

nns

jj

old

61

years

,

,

md

will

vp

vb

join

np

dt

nn

the

board

pp-clr

np

in

as

np-tmp

nnp

cd

nov.

29

dt

jj

nn

a

nonexecutive

director

31 / 87

lisp encoding in the id32

( (s

(np-sbj-1

(np (nnp rudolph) (nnp agnew) )
(, ,)
(ucp

(adjp

(np (cd 55) (nns years) )
(jj old) )

(cc and)
(np

(np (jj former) (nn chairman) )
(pp (in of)

(np (nnp consolidated) (nnp gold) (nnp fields) (nnp plc) ))))

(, ,) )

(vp (vbd was)

(vp (vbn named)

(s

(np-sbj (-none- *-1) )
(np-prd

(np (dt a) (jj nonexecutive) (nn director) )
(pp (in of)

(np (dt this) (jj british) (jj industrial) (nn conglomerate) ))))))

(. .) ))

32 / 87

some id32 rules with counts

40717 pp     in np
33803 s     np-sbj vp
22513 np-sbj     -none-
21877 np     np pp
20740 np     dt nn
14153 s     np-sbj vp .
12922 vp     to vp
11881 pp-loc     in np
11467 np-sbj     prp
11378 np     -none-
11291 np     nn
. . .
989 vp     vbg s
985 np-sbj     nn
983 pp-mnr     in np
983 np-sbj     dt
969 vp     vbn vp

100 vp     vbd pp-prd
100 prn     : np :
100 np     dt jjs
100 np-clr     nn
99 np-sbj-1     dt nnp
98 vp     vbn np pp-dir
98 vp     vbd pp-tmp
98 pp-tmp     vbg np
97 vp     vbd advp-tmp vp
. . .
10 whnp-1     wrb jj
10 vp     vp cc vp pp-tmp
10 vp     vp cc vp advp-mnr
10 vp     vbz s , sbar-adv
10 vp     vbz s advp-tmp

33 / 87

id32 rules: statistics

32,728 rules in the training section (not including 52,257 lexicon rules)
4,021 rules in the development section
overlap: 3,128

34 / 87

(phrase-structure) recognition and parsing

given a id18 (n , s,   ,r) and a sentence x, the recognition problem is:

is x in the language of the id18?

related problem: parsing:

show one or more derivations for x, using r.

35 / 87

(phrase-structure) recognition and parsing

given a id18 (n , s,   ,r) and a sentence x, the recognition problem is:

is x in the language of the id18?

the proof is a derivation.

related problem: parsing:

show one or more derivations for x, using r.

36 / 87

(phrase-structure) recognition and parsing

given a id18 (n , s,   ,r) and a sentence x, the recognition problem is:

is x in the language of the id18?

the proof is a derivation.

related problem: parsing:

show one or more derivations for x, using r.

with reasonable grammars, the number of parses is exponential in |x|.

37 / 87

ambiguity

s

vp

shot

np

np

i

s

vp

np

i

an

nominal

vp

pp

shot

np

in my pajamas

nominal

pp

elephant

in my pajamas

an nominal

elephant

38 / 87

parser evaluation

represent a parse tree as a collection of tuples (cid:104)(cid:104)(cid:96)1, i1, j1(cid:105),(cid:104)(cid:96)2, i2, j2(cid:105), . . . ,(cid:104)(cid:96)n, in, jn(cid:105),
where

(cid:73) (cid:96)k is the nonterminal labeling the kth phrase
(cid:73) ik is the index of the    rst word in the kth phrase
(cid:73) jk is the index of the last word in the kth phrase

(cid:28) (cid:104)s, 1, 6(cid:105),

(cid:104)vp, 4, 6(cid:105),

(cid:29)

(cid:104)np, 2, 3(cid:105),
(cid:104)np, 5, 6(cid:105)

example:

s

      

aux

does

np

vp

det

noun

this

   ight

verb

np

include

det

noun

a

meal

convert gold-standard tree and system hypothesized tree into this representation, then
estimate precision, recall, and f1.

39 / 87

tree comparison example

s

vp

shot

np

np

i

s

vp

np

i

an

nominal

nominal

pp

elephant

in

np

my

(cid:28) (cid:104)np, 3, 7(cid:105),
(cid:123)(cid:122)
(cid:124)

(cid:104)nominal, 4, 7(cid:105)

only in left tree

pajamas

(cid:29)
(cid:125)

(cid:42)
(cid:124)

vp

pp

shot

np

in

np

an nominal

my

pajamas

elephant

(cid:104)np, 1, 1(cid:105)

(cid:104)s, 1, 7(cid:105),(cid:104)vp, 2, 7(cid:105),
(cid:104)pp, 5, 7(cid:105),(cid:104)np, 6, 7(cid:105)

(cid:104)nominal, 4, 4(cid:105)

(cid:123)(cid:122)

in both trees

(cid:43)
(cid:125)

(cid:28) (cid:104)vp, 2, 4(cid:105),
(cid:123)(cid:122)
(cid:124)

(cid:104)np, 3, 4(cid:105)

(cid:29)
(cid:125)

only in right tree

40 / 87

two views of parsing

41 / 87

two views of parsing

1. incremental search: the state of the search is the partial structure built so far;

each action incrementally extends the tree.

42 / 87

two views of parsing

1. incremental search: the state of the search is the partial structure built so far;

each action incrementally extends the tree.

(cid:73) often greedy, with a statistical classi   er deciding what action to take in every state.

43 / 87

two views of parsing

1. incremental search: the state of the search is the partial structure built so far;

each action incrementally extends the tree.

(cid:73) often greedy, with a statistical classi   er deciding what action to take in every state.
2. discrete optimization: de   ne a scoring function and seek the tree with the highest

score.

44 / 87

two views of parsing

1. incremental search: the state of the search is the partial structure built so far;

each action incrementally extends the tree.

(cid:73) often greedy, with a statistical classi   er deciding what action to take in every state.
2. discrete optimization: de   ne a scoring function and seek the tree with the highest

score.

(cid:73) today: scores are de   ned using the rules.

predict(x) = argmax

t

s(r)ct(r) = argmax

t

(cid:89)

r   r

(cid:88)

r   r

ct(r) log s(r)

where t is constrained to include grammatical trees with x as their yield. denote
this set tx.

45 / 87

probabilistic context-free grammar

a probabilistic context-free grammar consists of:

(cid:73) a    nite set of nonterminal symbols n

(cid:73) a start symbol s     n

(cid:73) a    nite alphabet   , called    terminal    symbols, distinct from n
(cid:73) production rule set r, each of the form    n           where

(cid:73) the lefthand side n is a nonterminal from n
(cid:73) the righthand side    is a sequence of zero or more terminals and/or nonterminals:

       (n       )   

(cid:73) special case: chomsky normal form constrains    to be either a single terminal

symbol or two nonterminals

(cid:73) for each n     n , a id203 distribution over the rules where n is the lefthand

side, p(    | n ).

46 / 87

pid18 example

write down the start symbol. here: s

score:

s

1

47 / 87

pid18 example

s

aux np vp

choose a rule from the    s    distribution. here: s     aux np vp

score:

p(aux np vp | s)

48 / 87

pid18 example

s

aux

np

vp

does

choose a rule from the    aux    distribution. here: aux     does

score:

p(aux np vp | s)    p(does | aux)

49 / 87

pid18 example

s

aux

np

vp

does

det noun

choose a rule from the    np    distribution. here: np     det noun
score:

p(aux np vp | s)    p(does | aux)    p(det noun | np)

50 / 87

pid18 example

s

aux

np

vp

does

det

noun

this

choose a rule from the    det    distribution. here: det     this
score:

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)

51 / 87

pid18 example

s

aux

np

vp

does

det

noun

this

   ight

choose a rule from the    noun    distribution. here: noun        ight
score:

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)
   p(   ight | noun)

52 / 87

pid18 example

s

aux

does

np

vp

det

noun

verb np

this

   ight

choose a rule from the    vp    distribution. here: vp     verb np
score:

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)
   p(   ight | noun)    p(verb np | vp)

53 / 87

pid18 example

s

aux

does

np

vp

det

noun

verb

np

this

   ight

include

choose a rule from the    verb    distribution. here: verb     include
score:

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)
   p(   ight | noun)    p(verb np | vp)    p(include | verb)

54 / 87

pid18 example

s

aux

does

np

vp

det

noun

verb

np

det noun
choose a rule from the    np    distribution. here: np     det noun
score:

include

this

   ight

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)
   p(   ight | noun)    p(verb np | vp)    p(include | verb)
   p(det noun | np)

55 / 87

pid18 example

s

aux

does

np

vp

det

noun

this

   ight

verb

np

include

det

noun

choose a rule from the    det    distribution. here: det     a
score:

a

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)
   p(   ight | noun)    p(verb np | vp)    p(include | verb)
   p(det noun | np)    p(a | det)

56 / 87

pid18 example

s

aux

does

np

vp

det

noun

this

   ight

verb

np

include

det

noun

choose a rule from the    noun    distribution. here: noun     meal
score:

a

meal

p(aux np vp | s)    p(does | aux)    p(det noun | np)    p(this | det)
   p(   ight | noun)    p(verb np | vp)    p(include | verb)
   p(det noun | np)    p(a | det)    p(meal | noun)

57 / 87

pid18 as a noisy channel

source        t        channel        x

the pid18 de   nes the source model.

the channel is deterministic: it erases everything except the tree   s leaves (the yield).

decoding:

(cid:26) 1 if t     tx

0 otherwise

p(t)   

argmax

t

= argmax

t   tx

p(t)

58 / 87

probabilistic parsing with id18s

(cid:73) how to set the probabilities p(righthand side | lefthand side)?
(cid:73) how to decode/parse?

59 / 87

probabilistic cky
(cocke and schwartz, 1970; kasami, 1965; younger, 1967)

input:

(cid:73) a pid18 (n , s,   ,r, p(    |    )), in chomsky normal form
(cid:73) a sentence x (let n be its length)

output: argmax

t   tx

p(t | x) (if x is in the language of the grammar)

60 / 87

probabilistic cky

base case: for i     {1, . . . , n} and for each n     n :

for each i, k such that 1     i < k     n and each n     n :

si:i(n ) = p(xi | n )

si:k(n ) =

l,r   n ,j   {i,...,k   1} p(l r | n )    si:j(l)    s(j+1):k(r)

max

solution:

n

l

r

xi . . . xj

xj+1 . . . xk

s1:n(s) = max
t   tx

p(t)

61 / 87

parse chart

x1

x2

x3

x4

x5

62 / 87

parse chart

s1:1(   )

x1

s2:2(   )

x2

s3:3(   )

x3

s4:4(   )

x4

s5:5(   )

x5

63 / 87

parse chart

s1:1(   )

s1:2(   )

x1

s2:2(   )

s2:3(   )

x2

s3:3(   )

s3:4(   )

x3

s4:4(   )

s4:5(   )

x4

s5:5(   )

x5

64 / 87

parse chart

s1:1(   )

s1:2(   )

s1:3(   )

x1

s2:2(   )

s2:3(   )

s2:4(   )

x2

s3:3(   )

s3:4(   )

s3:5(   )

x3

s4:4(   )

s4:5(   )

x4

s5:5(   )

x5

65 / 87

parse chart

s1:1(   )

s1:2(   )

s1:3(   )

s1:4(   )

x1

s2:2(   )

s2:3(   )

s2:4(   )

s2:5(   )

x2

s3:3(   )

s3:4(   )

s3:5(   )

x3

s4:4(   )

s4:5(   )

x4

s5:5(   )

x5

66 / 87

parse chart

s1:1(   )

s1:2(   )

s1:3(   )

s1:4(   )

s1:5(   )

x1

s2:2(   )

s2:3(   )

s2:4(   )

s2:5(   )

x2

s3:3(   )

s3:4(   )

s3:5(   )

x3

s4:4(   )

s4:5(   )

x4

s5:5(   )

x5

67 / 87

remarks

(cid:73) space and runtime requirements?

68 / 87

remarks

(cid:73) space and runtime requirements? o(|n|n2) space, o(|r|n3) runtime.

69 / 87

remarks

(cid:73) space and runtime requirements? o(|n|n2) space, o(|r|n3) runtime.
(cid:73) recovering the best tree?

70 / 87

remarks

(cid:73) space and runtime requirements? o(|n|n2) space, o(|r|n3) runtime.
(cid:73) recovering the best tree? backpointers.

71 / 87

remarks

(cid:73) space and runtime requirements? o(|n|n2) space, o(|r|n3) runtime.
(cid:73) recovering the best tree? backpointers.
(cid:73) probabilistic earley   s algorithm does not require the grammar to be in chomsky

normal form.

72 / 87

the declarative view of cky

73 / 87

iknj + 1krijlp(l r | n)iinp(xi | n)1nsgoal:probabilistic cky with an agenda

1. initialize every item   s value in the chart to the    default    (zero).
2. place all initializing updates onto the agenda.
3. while the agenda is not empty or the goal is not reached:

(cid:73) pop the highest-priority update from the agenda (item i with value v)
(cid:73) if i = goal, then return v.
(cid:73) if v > chart(i):
(cid:73) chart(i)     v
(cid:73) find all combinations of i with other items in the chart, generating new possible

updates; place these on the agenda.

any priority function will work! but smart ordering will save time.

this idea can also be applied to other algorithms (e.g., viterbi).

74 / 87

starting point: phrase structure

s

np

dt

nn

nn

nn

the

luxury

auto

maker

np

jj

nn

last

year

vbd

vp

np

sold

cd

nn

1,214

cars

in

in

pp

np

dt

nnp

the

u.s.

75 / 87

parent annotation
(johnson, 1998)

sroot

nps

dtnp

nnnp

nnnp

nnnp

the

luxury

auto

maker

nps

jjnp

nnnp

last

year

vps

vbdvp

npvp

ppvp

sold

cdnp

nnnp

inpp

nppp

1,214

cars

in

dtnp

nnpnp

the

u.s.

increases the    vertical    markov order:

p(children | parent, grandparent)

76 / 87

headedness

s

np

jj

nn

last

year

np

dt

nn

nn

nn

the

luxury

auto

maker

vp

np

cd

1,214

nn

cars

in

in

vbd

sold

suggests    horizontal    markovization:

p(children | parent) = p(head | parent)   (cid:89)

i

pp

np

dt

the

nnp

u.s.

p(ith sibling | head, parent)

77 / 87

lexicalization

ssold

npyear

jjlast

nnyear

last

year

npmaker

dtthe

nnluxury

nnauto

nnmaker

the

luxury

auto

maker

vpsold

vbdsold

sold

npcars

ppin

cd1,214

nncars

1,214

cars

inin

in

npu.s.

dtthe

nnpu.s.

the

u.s.

each node shares a lexical head with its head child.

78 / 87

transformations on trees

starting around 1998, many di   erent ideas   both linguistic and statistical   about how
to transform treebank trees.

all of these make the grammar larger   and therefore all frequencies became
sparser   so a lot of research on smoothing the id203 rules.

parent annotation, headedness, markovization, and lexicalization; also category
re   nement by linguistic rules (klein and manning, 2003).

(cid:73) these are re   ected in some versions of the popular stanford and berkeley parsers.

79 / 87

tree decorations
(klein and manning, 2003)

(cid:73) mark nodes with only 1 child as unary
(cid:73) mark dts (determiners), rbs (adverbs) when they are only children
(cid:73) annotate pos tags with their parents
(cid:73) split in (prepositions; 6 ways), aux, cc, %
(cid:73) nps: temporal, possessive, base
(cid:73) vps annotated with head tag (   nite vs. others)
(cid:73) dominates-v
(cid:73) right-recursive np

80 / 87

machine learning and parsing

81 / 87

machine learning and parsing

(cid:73) de   ne arbitrary features on trees, based on linguistic knowledge; to parse, use a
pid18 to generate a k-best list of parses, then train a log-linear model to rerank
(charniak and johnson, 2005).

(cid:73) k-best parsing: huang and chiang (2005)

82 / 87

machine learning and parsing

(cid:73) de   ne arbitrary features on trees, based on linguistic knowledge; to parse, use a
pid18 to generate a k-best list of parses, then train a log-linear model to rerank
(charniak and johnson, 2005).

(cid:73) k-best parsing: huang and chiang (2005)

(cid:73) de   ne rule-local features on trees (and any part of the input sentence); minimize

hinge or log loss.

(cid:73) these exploit id145 algorithms for training (cky for arbitrary

scores, and the sum-product version).

83 / 87

machine learning and parsing

(cid:73) de   ne arbitrary features on trees, based on linguistic knowledge; to parse, use a
pid18 to generate a k-best list of parses, then train a log-linear model to rerank
(charniak and johnson, 2005).

(cid:73) k-best parsing: huang and chiang (2005)

(cid:73) de   ne rule-local features on trees (and any part of the input sentence); minimize

hinge or log loss.

(cid:73) these exploit id145 algorithms for training (cky for arbitrary

scores, and the sum-product version).

(cid:73) learn re   nements on the constituents, as latent variables (petrov et al., 2006).

84 / 87

machine learning and parsing

(cid:73) de   ne arbitrary features on trees, based on linguistic knowledge; to parse, use a
pid18 to generate a k-best list of parses, then train a log-linear model to rerank
(charniak and johnson, 2005).

(cid:73) k-best parsing: huang and chiang (2005)

(cid:73) de   ne rule-local features on trees (and any part of the input sentence); minimize

hinge or log loss.

(cid:73) these exploit id145 algorithms for training (cky for arbitrary

scores, and the sum-product version).

(cid:73) learn re   nements on the constituents, as latent variables (petrov et al., 2006).
(cid:73) neural, too:

(cid:73) socher et al. (2013) de   ne compositional vector grammars that associate each

phrase with a vector, calculated as a function of its subphrases    vectors. used
essentially to rerank.

(cid:73) dyer et al. (2016): recurrent neural network grammars, generative models like
pid18s that encode arbitrary previous derivation steps in a vector. parsing requires
some tricks.

85 / 87

references i

eugene charniak and mark johnson. coarse-to-   ne n-best parsing and maxent discriminative reranking. in

proc. of acl, 2005.

noam chomsky. three models for the description of language. id205, ieee transactions on, 2(3):

113   124, 1956.

john cocke and jacob t. schwartz. programming languages and their compilers: preliminary notes. technical

report, courant institute of mathematical sciences, new york university, 1970.

michael collins. id140, 2011. url

http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pid18s.pdf.

chris dyer, adhiguna kuncoro, miguel ballesteros, and noah a. smith. recurrent neural network grammars,

2016. to appear.

liang huang and david chiang. better k-best parsing. in proc. of iwpt, 2005.

mark johnson. pid18 models of linguistic tree representations. computational linguistics, 24(4):613   32, 1998.

daniel jurafsky and james h. martin. speech and language processing: an introduction to natural language

processing, computational linguistics, and id103. prentice hall, second edition, 2008.

tadao kasami. an e   cient recognition and syntax-analysis algorithm for context-free languages. technical

report afcrl-65-758, air force cambridge research lab, 1965.

dan klein and christopher d. manning. accurate unlexicalized parsing. in proc. of acl, 2003.

86 / 87

references ii

mitchell p. marcus, beatrice santorini, and mary ann marcinkiewicz. building a large annotated corpus of

english: the id32. computational linguistics, 19(2):313   330, 1993.

slav petrov, leon barrett, romain thibaux, and dan klein. learning accurate, compact, and interpretable tree

annotation. in proc. of coling-acl, 2006.

geo   rey k. pullum. the great eskimo vocabulary hoax and other irreverent essays on the study of language.

university of chicago press, 1991.

richard socher, john bauer, christopher d. manning, and andrew y. ng. parsing with compositional vector

grammars. in proc. of acl, 2013.

daniel h. younger. recognition and parsing of context-free languages in time n3. information and control, 10

(2), 1967.

87 / 87

