natural language processing (csep 517):

machine translation

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

may 15, 2017

1 / 59

to-do list

(cid:73) online quiz: due sunday
(cid:73) (jurafsky and martin, 2008, ch. 25), collins (2011, 2013)
(cid:73) a5 due may 28 (sunday)

2 / 59

evaluation

intuition: good translations are    uent in the target language and faithful to the
original meaning.

id7 score (papineni et al., 2002):

(cid:73) compare to a human-generated reference translation
(cid:73) or, better: multiple references
(cid:73) weighted average of id165 precision (across di   erent n)

there are some alternatives; most papers that use them report id7, too.

3 / 59

warren weaver to norbert wiener, 1947

one naturally wonders if the problem of translation could be conceivably treated as a
problem in cryptography. when i look at an article in russian, i say:    this is really
written in english, but it has been coded in some strange symbols. i will now proceed
to decode.   

4 / 59

id87s
review

a pattern for modeling a pair of random variables, x and y :
source        y        channel        x

5 / 59

id87s
review

a pattern for modeling a pair of random variables, x and y :
source        y        channel        x

(cid:73) y is the plaintext, the true message, the missing information, the output

6 / 59

id87s
review

a pattern for modeling a pair of random variables, x and y :
source        y        channel        x

(cid:73) y is the plaintext, the true message, the missing information, the output
(cid:73) x is the ciphertext, the garbled message, the observable evidence, the input

7 / 59

id87s
review

a pattern for modeling a pair of random variables, x and y :
source        y        channel        x

(cid:73) y is the plaintext, the true message, the missing information, the output
(cid:73) x is the ciphertext, the garbled message, the observable evidence, the input
(cid:73) decoding: select y given x = x.
y    = argmax

p(y | x)
p(x | y)    p(y)

y

= argmax

y

= argmax

y

p(x)
p(x | y)

(cid:124) (cid:123)(cid:122) (cid:125)

channel model

  

(cid:124)(cid:123)(cid:122)(cid:125)

p(y)

source model

8 / 59

bitext/parallel text

let f and e be two sequences in v    (french) and   v    (english), respectively.

we   re going to de   ne p(f | e), the id203 over french translations of english
sentence e.

in a noisy channel machine translation system, we could use this together with
source/language model p(e) to    decode    f into an english translation.

where does the data to estimate this come from?

9 / 59

ibm model 1
(brown et al., 1993)

let (cid:96) and m be the (known) lengths of e and f .
latent variable a = (cid:104)a1, . . . , am(cid:105), each ai ranging over {0, . . . , (cid:96)} (positions in e).

(cid:73) a4 = 3 means that f4 is    aligned    to e3.
(cid:73) a6 = 0 means that f6 is    aligned    to a special null symbol, e0.

p(f | e, m) =

=

p(f , a | e, m) =

=

m(cid:89)
m(cid:89)

i=1

(cid:96)(cid:88)

(cid:96)(cid:88)
(cid:96)(cid:88)
(cid:88)

a1=0

a2=0

a   {0,...,(cid:96)}m

      

p(f , a | e, m)

am=0

p(f , a | e, m)

p(ai | i, (cid:96), m)    p(fi | eai)

(cid:18) 1

(cid:19)m m(cid:89)

(cid:96) + 1

i=1

1

     fi|eai

=

(cid:96) + 1

i=1

  fi|eai

10 / 59

example: f is german

a = (cid:104)4, . . .(cid:105)

p(f , a | e, m) =

1

17 + 1

     noahs|noah   s

11 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is german

a = (cid:104)4, 5, . . .(cid:105)

p(f , a | e, m) =

1

17 + 1

     noahs|noah   s   

1

17 + 1

     arche|ark

12 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is german

a = (cid:104)4, 5, 6, . . .(cid:105)

p(f , a | e, m) =
  

1

17 + 1
1

17 + 1

     noahs|noah   s   
     war|was

1

17 + 1

     arche|ark

13 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is german

a = (cid:104)4, 5, 6, 8, . . .(cid:105)

p(f , a | e, m) =
  

1

     noahs|noah   s   
     war|was   
1

17 + 1

17 + 1
1

17 + 1

1

     arche|ark

17 + 1
     nicht|not

14 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is german

a = (cid:104)4, 5, 6, 8, 7, . . .(cid:105)

p(f , a | e, m) =
  
  

1

17 + 1
1

17 + 1

1

17 + 1

     noahs|noah   s   
     war|was   
1
     voller|   lled

17 + 1

1

     arche|ark

17 + 1
     nicht|not

15 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is german

a = (cid:104)4, 5, 6, 8, 7, ?, . . .(cid:105)

p(f , a | e, m) =
  
  

1

17 + 1

1

1

17 + 1

17 + 1

     noahs|noah   s   
     war|was   
1
     voller|   lled   

17 + 1
1

17 + 1

1

     arche|ark

17 + 1
     nicht|not

     productionsfactoren|?

16 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is german

a = (cid:104)4, 5, 6, 8, 7, ?, . . .(cid:105)

p(f , a | e, m) =
  
  

1

17 + 1

1

1

17 + 1

17 + 1

     noahs|noah   s   
     war|was   
1
     voller|   lled   

17 + 1
1

17 + 1

1

     arche|ark

17 + 1
     nicht|not

     productionsfactoren|?

problem: this alignment isn   t possible with ibm model 1! each fi is aligned to at
most one eai!

17 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

a = (cid:104)0, . . .(cid:105)

p(f , a | e, m) =

1

10 + 1

     mr|null

18 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

a = (cid:104)0, 0, 0, . . .(cid:105)

p(f , a | e, m) =
  

1

10 + 1
1

10 + 1

     mr|null   
     ,|null

1

10 + 1

     president|null

19 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

a = (cid:104)0, 0, 0, 1, . . .(cid:105)

p(f , a | e, m) =
  

1

10 + 1
1

10 + 1

     mr|null   
     ,|null   

1

     president|null

10 + 1
1

     noah   s|noahs

10 + 1

20 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

1

a = (cid:104)0, 0, 0, 1, 2, . . .(cid:105)
     mr|null   
     ,|null   
     ark|arche

10 + 1
1

10 + 1

1

p(f , a | e, m) =
  
  

10 + 1

1

     president|null

10 + 1
1

     noah   s|noahs

10 + 1

21 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

1

a = (cid:104)0, 0, 0, 1, 2, 3, . . .(cid:105)
     mr|null   
     ,|null   
     ark|arche   

10 + 1
1

10 + 1

1

p(f , a | e, m) =
  
  

10 + 1

1

     president|null

10 + 1
1

     noah   s|noahs
     was|war

10 + 1
1

10 + 1

22 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

a = (cid:104)0, 0, 0, 1, 2, 3, 5, . . .(cid:105)

p(f , a | e, m) =
  
  
  

1

10 + 1
1

10 + 1

1

1

10 + 1

10 + 1

     mr|null   
     ,|null   
     ark|arche   
        lled|voller

1

     president|null

10 + 1
1

     noah   s|noahs
     was|war

10 + 1
1

10 + 1

23 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .example: f is english

a = (cid:104)0, 0, 0, 1, 2, 3, 5, 4, . . .(cid:105)
1

1

p(f , a | e, m) =
  
  
  

     mr|null   
     ,|null   
     ark|arche   
        lled|voller   

10 + 1
1

10 + 1

1

1

10 + 1

10 + 1

10 + 1
1

10 + 1
1

10 + 1

1

10 + 1

     noah   s|noahs
     was|war
     not|nicht

     president|null

24 / 59

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .how to estimate translation distributions?

this is a problem of incomplete data: at training time, we see e and f , but not a.

25 / 59

how to estimate translation distributions?

this is a problem of incomplete data: at training time, we see e and f , but not a.

classical solution is to alternate:

(cid:73) given a parameter estimate for   , align the words.
(cid:73) given aligned words, re-estimate   .

traditional approach uses    soft    alignment.

26 / 59

   complete data    ibm model 1

let the training data consist of n word-aligned sentence pairs:
1 , f (1), a(1)(cid:105), . . . ,(cid:104)e(n ), f (n ), a(n )(cid:105).
(cid:104)e(1)
de   ne:

(cid:40)

  (k, i, j) =

1 if a(k)
i = j
0 otherwise

maximum likelihood estimate for   f|e:

n(cid:88)
n(cid:88)

k=1

(cid:88)
m(k)(cid:88)

i:f (k)

i =f

(cid:88)
(cid:88)

j:e(k)

j =e

  (k, i, j)

  (k, i, j)

c(e, f )

c(e)

=

k=1

i=1

j:e(k)

j =e

27 / 59

id113 with    soft    counts for ibm model 1

let the training data consist of n    softly    aligned sentence pairs,
(cid:104)e(1)
1 , f (1),(cid:105), . . . ,(cid:104)e(n ), f (n )(cid:105).
now, let   (k, i, j) be    soft,    interpreted as:

  (k, i, j) = p(a(k)

i = j)

maximum likelihood estimate for   f|e:

n(cid:88)
n(cid:88)

k=1

(cid:88)
m(k)(cid:88)

(cid:88)
(cid:88)

j:e(k)

i:f (k)

i =f

j =e

  (k, i, j)

  (k, i, j)

k=1

i=1

j:e(k)

j =e

28 / 59

expectation maximization algorithm for ibm model 1

1. initialize    to some arbitrary values.
2. e step: use current    to estimate expected (   soft   ) counts.

(cid:96)(k)(cid:88)
  (k, i, j)       

f (k)
i

|e(k)

j

  

f (k)
i

|e(k)
j(cid:48)

j(cid:48)=0

3. m step: carry out    soft    id113.

n(cid:88)
n(cid:88)

k=1

(cid:88)
m(k)(cid:88)

i:f (k)

i =f

(cid:88)
(cid:88)

j:e(k)

j =e

  (k, i, j)

  (k, i, j)

  f|e    

k=1

i=1

j:e(k)

j =e

29 / 59

expectation maximization

(cid:73) originally introduced in the 1960s for estimating id48s when the states really are

   hidden.   

(cid:73) can be applied to any generative model with hidden variables.
(cid:73) greedily attempts to maximize id203 of the observable data, marginalizing

over latent variables. for ibm model 1, that means:

n(cid:89)

k=1

n(cid:89)

(cid:88)

k=1

a

max

  

p  (f (k) | e(k)) = max

  

p  (f (k), a | e(k))

(cid:73) usually converges only to a local optimum of the above, which is in general not

convex.

(cid:73) strangely, for ibm model 1 (and very few other models), it is convex!

30 / 59

ibm model 2
(brown et al., 1993)

let (cid:96) and m be the (known) lengths of e and f .
latent variable a = (cid:104)a1, . . . , am(cid:105), each ai ranging over {0, . . . , (cid:96)} (positions in e).

(cid:73) e.g., a4 = 3 means that f4 is    aligned    to e3.

p(f | e, m) =

p(f , a | e, m)

p(f , a | e, m) =

p(ai | i, (cid:96), m)    p(fi | eai)

(cid:88)

a   {0,...,n}m

m(cid:89)
=   ai|i,(cid:96),m      fi|eai

i=1

31 / 59

ibm models 1 and 2, depicted

32 / 59

x1x2x3x4hidden markov modely1y2y3y4f1f2f3f4ibm 1 and 2a1a2a3a4eeeevariations

(cid:73) dyer et al. (2013) introduced a new parameterization:

  j|i,(cid:96),m     exp     

(cid:12)(cid:12)(cid:12)(cid:12) i

m

(cid:12)(cid:12)(cid:12)(cid:12)

    j
(cid:96)

(this is called fast align.)

(cid:73) ibm models 3   5 (brown et al., 1993) introduced increasingly more powerful

ideas, such as    fertility    and    distortion.   

33 / 59

from alignment to (phrase-based) translation

obtaining word alignments in a parallel corpus is a common    rst step in building a
machine translation system.

1. align the words.
2. extract and score phrase pairs.
3. estimate a global scoring function to optimize (a proxy for) translation quality.

4. decode french sentences into english ones.

(we   ll discuss 2   4.)

the noisy channel pattern isn   t taken quite so seriously when we build real systems,
but language models are really, really important nonetheless.

34 / 59

phrases?

phrase-based translation uses automatically-induced phrases . . . not the ones given by
a phrase-structure parser.

35 / 59

examples of phrases
courtesy of chris dyer.

german

das thema

es gibt

morgen

   iege ich

english
the issue
the point
the subject
the thema
there is
there are
tomorrow
will i    y
will    y
i will    y

p(   f |   e)
0.41
0.72
0.47
0.99
0.96
0.72
0.90
0.63
0.17
0.13

36 / 59

phrase-based translation model
originated by koehn et al. (2003).

r.v. a captures segmentation of sentences into phrases, alignment between them, and
reordering.

p(f , a | e) = p(a | e)   

|a|(cid:89)

i=1

p(   f i |   ei)

37 / 59

to the conferencemorgen     iegeichnach pittsburghzur konferenztomorrowiwill    yin pittsburghefaextracting phrases

after inferring word alignments, apply heuristics.

38 / 59

extracting phrases

after inferring word alignments, apply heuristics.

39 / 59

extracting phrases

after inferring word alignments, apply heuristics.

40 / 59

extracting phrases

after inferring word alignments, apply heuristics.

41 / 59

extracting phrases

after inferring word alignments, apply heuristics.

42 / 59

extracting phrases

after inferring word alignments, apply heuristics.

43 / 59

extracting phrases

after inferring word alignments, apply heuristics.

44 / 59

extracting phrases

after inferring word alignments, apply heuristics.

45 / 59

scoring whole translations

s(e, a; f ) =

(cid:124) (cid:123)(cid:122) (cid:125)

log p(e)

language model

(cid:124)

(cid:123)(cid:122)

+ log p(f , a | e)
translation model

(cid:125)

remarks:

(cid:73) segmentation, alignment, reordering are all predicted as well (not marginalized).
(cid:73) this does not factor nicely.

46 / 59

scoring whole translations

s(e, a; f ) =

log p(e)

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:123)(cid:122)

(cid:124)

language model

+ log p(e, a | f )
reverse t.m.

(cid:125)

(cid:124)

(cid:123)(cid:122)

+ log p(f , a | e)
translation model

(cid:125)

remarks:

(cid:73) segmentation, alignment, reordering are all predicted as well (not marginalized).
(cid:73) this does not factor nicely.
(cid:73) i am simplifying!

(cid:73) reverse translation model typically included.

47 / 59

scoring whole translations

s(e, a; f ) =   l.m.

log p(e)

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:123)(cid:122)

language model
+   r.t.m.log p(e, a | f )
reverse t.m.

(cid:124)

(cid:125)

+  t.m.

(cid:124)

(cid:123)(cid:122)

log p(f , a | e)
translation model

(cid:125)

remarks:

(cid:73) segmentation, alignment, reordering are all predicted as well (not marginalized).
(cid:73) this does not factor nicely.
(cid:73) i am simplifying!

(cid:73) reverse translation model typically included.
(cid:73) each log-id203 is treated as a    feature    and weights are optimized for id7

performance.

48 / 59

decoding: example

49 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarynotgiveaslaptothewitchgreennodid notdid not giveslapslapbyto thethethe witchhagbawdygreen witchdecoding: example

50 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarynotgiveaslaptothewitchgreennodid notdid not giveslapslapbyto thethethe witchhagbawdygreen witchdecoding: example

51 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarynotgiveaslaptothewitchgreennodid notdid not giveslapslapbyto thethethe witchhagbawdygreen witchdecoding
adapted from koehn et al. (2006).

typically accomplished with id125.
initial state: (cid:104)        . . .   

,       (cid:105) with score 0

|f|
goal state: (cid:104)        . . .   

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)

|f|

, e   (cid:105) with (approximately) the highest score

reaching a new state:

(cid:73) find an uncovered span of f for which a phrasal translation exists in the input

(   f ,   e)

(cid:73) new state appends   e to the output and    covers      f .
(cid:73) score of new state includes additional language model, translation model

components for the global score.

52 / 59

decoding example

(cid:104)                                   ,       (cid:105), 0

53 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarynotgiveaslaptothewitchgreennodid notdid not giveslapslapbyto thethethe witchhagbawdygreen witchdecoding example

(cid:104)                                   ,    mary   (cid:105), log pl.m.(mary) + log pt.m.(maria | mary)

54 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarynotgiveaslaptothewitchgreennodid notdid not giveslapslapbyto thethethe witchhagbawdygreen witchdecoding example

(cid:104)                                   ,    mary did not   (cid:105),

log pl.m.(mary did not) + log pt.m.(maria | mary)

+ log pt.m.(no | did not)

55 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarygiveaslaptothewitchgreendid notslapslapbyto thethethe witchhagbawdygreen witchdecoding example

(cid:104)                                   ,    mary did not slap   (cid:105),

log pl.m.(mary did not slap) + log pt.m.(maria | mary)

+ log pt.m.(no | did not) + log pt.m.(dio una bofetada | slap)

56 / 59

maria   no         dio  una   bofetada   a   la    bruja   verdamarytothewitchgreendid notslapbyto thethethe witchhagbawdygreen witchmachine translation: remarks

sometimes phrases are organized hierarchically (chiang, 2007).

extensive research on syntax-based machine translation (galley et al., 2004), but
requires considerable engineering to match phrase-based systems.

recent work on semantics-based machine translation (jones et al., 2012); remains to
be seen!

some good pre-neural overviews: lopez (2008); koehn (2009)

57 / 59

references i

peter f. brown, vincent j. della pietra, stephen a. della pietra, and robert l. mercer. the mathematics of

id151: parameter estimation. computational linguistics, 19(2):263   311, 1993.

david chiang. hierarchical phrase-based translation. computational linguistics, 33(2):201   228, 2007.

michael collins. id151: ibm models 1 and 2, 2011. url

http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf.

michael collins. phrase-based translation models, 2013. url

http://www.cs.columbia.edu/~mcollins/pb.pdf.

chris dyer, victor chahuneau, and noah a smith. a simple, fast, and e   ective reparameterization of ibm

model 2. in proc. of naacl, 2013.

michel galley, mark hopkins, kevin knight, and daniel marcu. what   s in a translation rule? in proc. of

naacl, 2004.

bevan jones, jacob andreas, daniel bauer, karl moritz hermann, and kevin knight. semantics-based machine

translation with hyperedge replacement grammars. in proc. of coling, 2012.

daniel jurafsky and james h. martin. speech and language processing: an introduction to natural language

processing, computational linguistics, and id103. prentice hall, second edition, 2008.

philipp koehn. id151. cambridge university press, 2009.

philipp koehn, franz josef och, and daniel marcu. statistical phrase-based translation. in proc. of naacl,

2003.

58 / 59

references ii

philipp koehn, marcello federico, wade shen, nicola bertoldi, ondrej bojar, chris callison-burch, brooke

cowan, chris dyer, hieu hoang, and richard zens. open source toolkit for id151:
factored translation models and confusion network decoding, 2006. final report of the 2006 jhu summer
workshop.

adam lopez. id151. acm computing surveys, 40(3):8, 2008.

kishore papineni, salim roukos, todd ward, and wei-jing zhu. id7: a method for automatic evaluation of

machine translation. in proc. of acl, 2002.

59 / 59

