natural language processing (csep 517):

dependency syntax and parsing

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

may 1, 2017

1 / 96

to-do list

(cid:73) online quiz: due sunday
(cid:73) read: k  ubler et al. (2009, ch. 1, 2, 6)
(cid:73) a3 due may 7 (sunday)
(cid:73) a4 due may 14 (sunday)

2 / 96

dependencies

informally, you can think of dependency structures as a transformation of
phrase-structures that

(cid:73) maintains the word-to-word relationships induced by lexicalization,
(cid:73) adds labels to them, and
(cid:73) eliminates the phrase categories.

there are also linguistic theories built on dependencies (tesni`ere, 1959; mel     cuk,
1987), as well as treebanks corresponding to those.

(cid:73) free(r)-word order languages (e.g., czech)

3 / 96

dependency tree: de   nition

let x = (cid:104)x1, . . . , xn(cid:105) be a sentence. add a special root symbol as    x0.   
a dependency tree consists of a set of tuples (cid:104)p, c, (cid:96)(cid:105), where

(cid:73) p     {0, . . . , n} is the index of a parent
(cid:73) c     {1, . . . , n} is the index of a child
(cid:73) (cid:96)     l is a label

di   erent annotation schemes de   ne di   erent label sets l, and di   erent constraints on
the set of tuples. most commonly:

(cid:73) the tuple is represented as a directed edge from xp to xc with label (cid:96).
(cid:73) the directed edges form an arborescence (directed tree) with x0 as the root

(sometimes denoted root).

4 / 96

example

s

np

vp

pronoun

we

verb

wash

np

determiner

noun

our

cats

phrase-structure tree.

5 / 96

example

s

verb

wash

np

pronoun

we

vp

np

determiner

noun

our

cats

phrase-structure tree with heads.

6 / 96

example

swash

verbwash

wash

npwe

pronounwe

we

vpwash

npcats

determinerour

nouncats

our

cats

phrase-structure tree with heads, lexicalized.

7 / 96

example

we

wash

our

cats

   bare bones    dependency tree.

8 / 96

example

we

wash

our

cats

who

stink

9 / 96

example

we

vigorously

wash

our

cats

who

stink

10 / 96

content heads vs. function heads
credit: nathan schneider

little

kids

were

always

watching

birds

with

   sh

little

kids

were

always

watching

birds

with

   sh

11 / 96

labels

root

sbj

dobj

pobj

prep

kids

saw

birds

with

   sh

key dependency relations captured in the labels include: subject, direct object,
preposition object, adjectival modi   er, adverbial modi   er.

in this lecture, i will mostly not discuss labels, to keep the algorithms simpler.

12 / 96

coordination structures

we

vigorously

wash

our

cats and dogs

who

stink

the bugbear of dependency syntax.

13 / 96

example

we

vigorously

wash

our

cats

and

dogs

who

stink

make the    rst conjunct the head?

14 / 96

example

we

vigorously

wash

our

cats

and

dogs

who

stink

make the coordinating conjunction the head?

15 / 96

example

we

vigorously

wash

our

cats

and

dogs

who

stink

make the second conjunct the head?

16 / 96

dependency schemes

(cid:73) transform the treebank: de   ne    head rules    that can select the head child of any

node in a phrase-structure tree and label the dependencies.

(cid:73) more powerful, less local rule sets, possibly collapsing some words into arc labels.

(cid:73) stanford dependencies are a popular example (de marne   e et al., 2006).

(cid:73) direct annotation.

17 / 96

three approaches to id33

1. id145 with the eisner algorithm.

2. transition-based parsing with a stack.

3. chu-liu-edmonds algorithm for arborescences.

18 / 96

dependencies and grammar

context-free grammars can be used to encode dependency structures.

for every head word and constellation of dependent children:

nhead     nleftmost-sibling . . . nhead . . . nrightmost-sibling

and for every v     v: nv     v and s     nv.

19 / 96

dependencies and grammar

context-free grammars can be used to encode dependency structures.

for every head word and constellation of dependent children:

nhead     nleftmost-sibling . . . nhead . . . nrightmost-sibling

and for every v     v: nv     v and s     nv.

a bilexical dependency grammar binarizes the dependents, generating only one per
rule.

20 / 96

dependencies and grammar

context-free grammars can be used to encode dependency structures.

for every head word and constellation of dependent children:

nhead     nleftmost-sibling . . . nhead . . . nrightmost-sibling

and for every v     v: nv     v and s     nv.

a bilexical dependency grammar binarizes the dependents, generating only one per
rule.

such a grammar can produce only projective trees, which are (informally) trees in
which the arcs don   t cross.

21 / 96

bilexical dependency grammar: derivation

s

nwash

nwe

we

nwash

nwash

ncats

wash

nour

ncats

our

cats

na    vely, the cky algorithm will require o(n5) runtime. why?

22 / 96

cky for bilexical context-free grammars

23 / 96

iknxhj + 1knxcijnxhp(nxh nxc | nxh)iknxhj + 1knxhijnxcp(nxc nxh  | nxh)cky example

24 / 96

we          wash           our           catsgoalnwenwashnour.ncatsncatsnwashnwashsid33 with the eisner algorithm
(eisner, 1996)

items:

(cid:73) both triangles indicate that xd is a descendant of xh.
(cid:73) both trapezoids indicate that xc can be attached as the child of xh.
(cid:73) in all cases, the words    in between    are descendants of xh.

25 / 96

hddhchhcid33 with the eisner algorithm
(eisner, 1996)

initialization:

goal:

26 / 96

iiii1p(xi | nxi)1iinp(nxi | s)goalid33 with the eisner algorithm
(eisner, 1996)

attaching a left dependent:

complete a left child:

27 / 96

ijj + 1kikp(nxi nxk | nxk)jkijikid33 with the eisner algorithm
(eisner, 1996)

attaching a right dependent:

complete a right child:

28 / 96

ijj + 1kikp(nxi nxk | nxi)ijjkikeisner algorithm example

29 / 96

we          wash           our           catsgoalthree approaches to id33

1. id145 with the eisner algorithm.

2. transition-based parsing with a stack.

3. chu-liu-edmonds algorithm for arborescences.

30 / 96

transition-based parsing

(cid:73) process x once, from left to right, making a sequence of greedy parsing decisions.

31 / 96

transition-based parsing

(cid:73) process x once, from left to right, making a sequence of greedy parsing decisions.
(cid:73) formally, the parser is a state machine (not a    nite-state machine) whose state

is represented by a stack s and a bu   er b.

32 / 96

transition-based parsing

(cid:73) process x once, from left to right, making a sequence of greedy parsing decisions.
(cid:73) formally, the parser is a state machine (not a    nite-state machine) whose state

is represented by a stack s and a bu   er b.

(cid:73) initialize the bu   er to contain x and the stack to contain the root symbol.

33 / 96

transition-based parsing

(cid:73) process x once, from left to right, making a sequence of greedy parsing decisions.
(cid:73) formally, the parser is a state machine (not a    nite-state machine) whose state

is represented by a stack s and a bu   er b.

(cid:73) initialize the bu   er to contain x and the stack to contain the root symbol.
(cid:73) the    arc standard    transition set (nivre, 2004):

(cid:73) shift the word at the front of the bu   er b onto the stack s.
(cid:73) right-arc: u = pop(s); v = pop(s); push(s, v     u).
(cid:73) left-arc: u = pop(s); v = pop(s); push(s, v     u).

(for labeled parsing, add labels to the right-arc and left-arc transitions.)

34 / 96

transition-based parsing

(cid:73) process x once, from left to right, making a sequence of greedy parsing decisions.
(cid:73) formally, the parser is a state machine (not a    nite-state machine) whose state

is represented by a stack s and a bu   er b.

(cid:73) initialize the bu   er to contain x and the stack to contain the root symbol.
(cid:73) the    arc standard    transition set (nivre, 2004):

(cid:73) shift the word at the front of the bu   er b onto the stack s.
(cid:73) right-arc: u = pop(s); v = pop(s); push(s, v     u).
(cid:73) left-arc: u = pop(s); v = pop(s); push(s, v     u).

(for labeled parsing, add labels to the right-arc and left-arc transitions.)

(cid:73) during parsing, apply a classi   er to decide which transition to take next, greedily.

no backtracking.

35 / 96

transition-based parsing: example

stack s:

root

actions:

bu   er b:

we
vigorously
wash
our
cats
who
stink

36 / 96

transition-based parsing: example

stack s:

we
root

actions: shift

bu   er b:

vigorously
wash
our
cats
who
stink

37 / 96

transition-based parsing: example

stack s:

vigorously
we
root

actions: shift shift

bu   er b:

wash
our
cats
who
stink

38 / 96

transition-based parsing: example

stack s:

wash
vigorously
we
root

actions: shift shift shift

bu   er b:

our
cats
who
stink

39 / 96

transition-based parsing: example

stack s:

vigorously

wash

we
root

actions: shift shift shift left-arc

bu   er b:

our
cats
who
stink

40 / 96

transition-based parsing: example

stack s:

we

vigorously

wash

root

actions: shift shift shift left-arc left-arc

bu   er b:

our
cats
who
stink

41 / 96

transition-based parsing: example

stack s:

our

we

vigorously

wash

root

actions: shift shift shift left-arc left-arc shift

bu   er b:

cats
who
stink

42 / 96

transition-based parsing: example

stack s:

cats
our

we

vigorously

wash

root

actions: shift shift shift left-arc left-arc shift shift

bu   er b:

who
stink

43 / 96

transition-based parsing: example

stack s:

our

cats

we

vigorously

wash

root

bu   er b:

who
stink

actions: shift shift shift left-arc left-arc shift shift left-arc

44 / 96

transition-based parsing: example

stack s:

who

our

cats

we

vigorously

wash

root

bu   er b:

stink

actions: shift shift shift left-arc left-arc shift shift left-arc shift

45 / 96

transition-based parsing: example

stack s:

stink
who

our

cats

we

vigorously

wash

root

bu   er b:

actions: shift shift shift left-arc left-arc shift shift left-arc shift
shift

46 / 96

transition-based parsing: example

stack s:

who

stink

our

cats

we

vigorously

wash

root

bu   er b:

actions: shift shift shift left-arc left-arc shift shift left-arc shift
shift right-arc

47 / 96

transition-based parsing: example

stack s:

our

cats

who

stink

bu   er b:

we

vigorously

wash

root

actions: shift shift shift left-arc left-arc shift shift left-arc shift
shift right-arc right-arc

48 / 96

transition-based parsing: example

stack s:

we

vigorously

wash

our

cats

who

stink

root

bu   er b:

actions: shift shift shift left-arc left-arc shift shift left-arc shift
shift right-arc right-arc right-arc

49 / 96

transition-based parsing: example

stack s:

root

we

vigorously

wash

our

cats

who

stink

bu   er b:

actions: shift shift shift left-arc left-arc shift shift left-arc shift
shift right-arc right-arc right-arc right-arc

50 / 96

the core of transition-based parsing: classi   cation

(cid:73) at each iteration, choose among {shift, right-arc, left-arc}.
(actually, among all l-labeled variants of right- and left-arc.)

51 / 96

the core of transition-based parsing: classi   cation

(cid:73) at each iteration, choose among {shift, right-arc, left-arc}.
(actually, among all l-labeled variants of right- and left-arc.)

(cid:73) features can look s, b, and the history of past actions   usually there is no

decomposition into local structures.

52 / 96

the core of transition-based parsing: classi   cation

(cid:73) at each iteration, choose among {shift, right-arc, left-arc}.
(actually, among all l-labeled variants of right- and left-arc.)

(cid:73) features can look s, b, and the history of past actions   usually there is no

decomposition into local structures.

(cid:73) training data:    oracle    transition sequence that gives the right tree converts into

2    n pairs: (cid:104)state, correct transition(cid:105). each word gets shifted once and
participates as a child in one arc.

53 / 96

transition-based parsing: remarks

(cid:73) can also be applied to phrase-structure parsing (e.g., sagae and lavie, 2006).

keyword:    shift-reduce    parsing.

(cid:73) the algorithm for making decisions doesn   t need to be greedy; can maintain

multiple hypotheses.

(cid:73) e.g., id125, which we   ll discuss in the context of machine translation later.

(cid:73) potential    aw: the classi   er is typically trained under the assumption that

previous classi   cation decisions were all correct.

(cid:73) as yet, no principled solution to this problem, but see    dynamic oracles    (goldberg

and nivre, 2012).

54 / 96

three approaches to id33

1. id145 with the eisner algorithm.

2. transition-based parsing with a stack.

3. chu-liu-edmonds algorithm for arborescences.

55 / 96

acknowledgment

slides are mostly adapted from those by swabha swayamdipta and sam thomson.

56 / 96

features in id33

for the eisner algorithm, the score of an unlabeled parse y was

sglobal(y) =

log p(xc | nxc) + log

n(cid:88)

c=1

          p(nxc nxp | nxp)

p(nxp nxc | nxp)
p(nxc | s)

if (cid:104)p, c(cid:105)     y     c < p     p > 0
if (cid:104)p, c(cid:105)     y     c > p     p > 0
if (cid:104)0, c(cid:105)     y

for transition-based parsing, we could use any past decisions to score the current
decision:

|a|(cid:88)

sglobal(y) = s(a) =

s(ai | a0:i   1)

i=1

we gave up on any guarantee of    nding the best possible y in favor of arbitrary
features.

(cid:73) for a neural network-based model that fully exploits this, see dyer et al. (2015).

57 / 96

graph-based id33
(mcdonald et al., 2005)

every possible directed edge e between a parent p and a child c gets a local score, s(e).

this set, e, contains o(n2) edges.
no incoming edges to x0, ensuring that it will be the root.

58 / 96

first-order graph-based (fog) id33
(mcdonald et al., 2005)

y    = argmax
y   e

sglobal(y) = argmax

y   e

(cid:88)

e   y

s(e)

subject to the constraint that y is an arborescence

classical algorithm to e   ciently solve this problem: chu and liu (1965), edmonds
(1967)

59 / 96

chu-liu-edmonds intuitions

(cid:73) every non-root node needs exactly one incoming edge.

60 / 96

chu-liu-edmonds intuitions

(cid:73) every non-root node needs exactly one incoming edge.
(cid:73) in fact, every connected component that doesn   t contain x0 needs exactly one

incoming edge.

61 / 96

chu-liu-edmonds intuitions

(cid:73) every non-root node needs exactly one incoming edge.
(cid:73) in fact, every connected component that doesn   t contain x0 needs exactly one

incoming edge.

high-level view of the algorithm:

1. for every c, pick an incoming edge (i.e., pick a parent)   greedily.

2. if this forms an arborescence, you are done!
3. otherwise, it   s because there   s a cycle, c.

(cid:73) arborescences can   t have cycles, so some edge in c needs to be kicked out.
(cid:73) we also need to    nd an incoming edge for c.
(cid:73) choosing the incoming edge for c determines which edge to kick out.

62 / 96

chu-liu-edmonds: recursive (ine   cient) de   nition

def maxarborescence(v , e, root):

# returns best arborescence as a map from each node to its parent
for c in v \ root:

bestinedge[c]     argmaxe   e:e=(cid:104)p,c(cid:105) e.s # i.e., s(e)

if bestinedge contains a cycle c:

# build a new graph where c is contracted into a single node
vc     new node()
v (cid:48)     v     {vc} \ c
e(cid:48)     {adjust(e, vc) for e     e \ c}
a     maxarborescence(v (cid:48), e(cid:48), root)
return {e.original for e     a}     c \ {a[vc].kicksout}

# each node got a parent without creating any cycles
return bestinedge

63 / 96

understanding chu-liu-edmonds

there are two stages:

(cid:73) contraction (the stu    before the recursive call)
(cid:73) expansion (the stu    after the recursive call)

64 / 96

chu-liu-edmonds: contraction

(cid:73) for each non-root node v, set bestinedge[v] to be its highest scoring incoming

edge.

(cid:73) if a cycle c is formed:

(cid:73) contract the nodes in c into a new node vc

adjust subroutine on next slide performs the following:
(cid:73) edges incoming to any node in c now get destination vc
(cid:73) for each node v in c, and for each edge e incoming to v from outside of c:

(cid:73) set e.kicksout to bestinedge[v], and
(cid:73) set e.s to be e.s     e.kicksout.s

(cid:73) edges outgoing from any node in c now get source vc

(cid:73) repeat until every non-root node has an incoming edge and no cycles are formed

65 / 96

chu-liu-edmonds: edge adjustment subroutine

def adjust(e, vc ):
e(cid:48)     copy(e)
e(cid:48).original     e
if e.dest     c:
e(cid:48).dest     vc
e(cid:48).kicksout     bestinedge[e.dest]
e(cid:48).s     e.s     e(cid:48).kicksout.s
elif e.src     c:
e(cid:48).src     vc
return e(cid:48)

66 / 96

contraction example

root

a : 5

b : 1

c : 1

f : 5

i : 8

v3

v1

d : 11

g : 10

v2

e : 4

h : 9

bestinedge

kicksout

v1
v2
v3

a
b
c
d
e
f
g
h
i

67 / 96

contraction example

root

a : 5

b : 1

c : 1

f : 5

i : 8

v3

v1

d : 11

g : 10

v2

e : 4

h : 9

bestinedge
g

kicksout

v1
v2
v3

a
b
c
d
e
f
g
h
i

68 / 96

contraction example

root

a : 5

b : 1

c : 1

f : 5

i : 8

v3

v1

d : 11

g : 10

v2

e : 4

h : 9

bestinedge
g
d

kicksout

v1
v2
v3

a
b
c
d
e
f
g
h
i

69 / 96

contraction example

root

a : 5     10 b : 1     11

c : 1

v1

v4
d : 11

g : 10

f : 5

i : 8     11

v3

v2

e : 4

h : 9     10

bestinedge
g
d

kicksout
g
d

g
d

v1
v2
v3

a
b
c
d
e
f
g
h
i

70 / 96

contraction example

root

a :    5

b :    10

c : 1

v4

v3

f : 5

i :    3

e : 4
h :    1

bestinedge
g
d

kicksout
g
d

g
d

v1
v2
v3
v4

a
b
c
d
e
f
g
h
i

71 / 96

contraction example

root

a :    5

b :    10

c : 1

v4

v3

f : 5

i :    3

e : 4
h :    1

bestinedge
g
d
f

kicksout
g
d

g
d

v1
v2
v3
v4

a
b
c
d
e
f
g
h
i

72 / 96

contraction example

root

a :    5

b :    10

c : 1

v4

v3

f : 5

i :    3

e : 4
h :    1

bestinedge
g
d
f
h

v1
v2
v3
v4

kicksout
g
d

g
d

a
b
c
d
e
f
g
h
i

73 / 96

contraction example

root

a :    5        1

b :    10        1

c : 1     5

v4

v3

v5
f : 5

i :    3

e : 4
h :    1

bestinedge
g
d
f
h

kicksout
g, h
d, h
f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

74 / 96

contraction example

root

b :    9

a :    4

c :    4

v5

bestinedge
g
d
f
h

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

75 / 96

contraction example

root

b :    9

a :    4

c :    4

v5

bestinedge
g
d
f
h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

76 / 96

chu-liu-edmonds: expansion

after the contraction stage, every contracted node will have exactly one bestinedge.
this edge will kick out one edge inside the contracted node, breaking the cycle.

(cid:73) go through each bestinedge e in the reverse order that we added them
(cid:73) lock down e, and remove every edge in kicksout(e) from bestinedge.

77 / 96

expansion example

root

b :    9

a :    4

c :    4

v5

bestinedge
g
d
f
h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

78 / 96

expansion example

root

b :    9

a :    4

c :    4

v5

bestinedge
a  g
d
f
a  h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

79 / 96

expansion example

root

a :    5

b :    10

c : 1

v4

v3

f : 5

i :    3

e : 4
h :    1

bestinedge
a  g
d
f
a  h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

80 / 96

expansion example

root

a :    5

b :    10

c : 1

v4

v3

f : 5

i :    3

e : 4
h :    1

bestinedge
a  g
d
f
a  h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

81 / 96

expansion example

root

a : 5

b : 1

c : 1

f : 5

i : 8

v3

v1

d : 11

g : 10

v2

e : 4

h : 9

bestinedge
a  g
d
f
a  h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

82 / 96

expansion example

root

a : 5

b : 1

c : 1

f : 5

i : 8

v3

v1

d : 11

g : 10

v2

e : 4

h : 9

bestinedge
a  g
d
f
a  h
a

kicksout
g, h
d, h
f

f

g
d

v1
v2
v3
v4
v5

a
b
c
d
e
f
g
h
i

83 / 96

observation

the set of arborescences strictly includes the set of projective dependency trees.

is this a good thing or a bad thing?

84 / 96

nonprojective example

root

att

vc

att

sbj

pu

tmp

pc

att

a

hearing

is

scheduled

on

the

issue

today

.

85 / 96

chu-liu-edmonds: notes

(cid:73) this is a greedy algorithm with a clever form of delayed backtracking to recover

from inconsistent decisions (cycles).

86 / 96

chu-liu-edmonds: notes

(cid:73) this is a greedy algorithm with a clever form of delayed backtracking to recover

from inconsistent decisions (cycles).

(cid:73) cle is exact: it always recovers an optimal arborescence.

87 / 96

chu-liu-edmonds: notes

(cid:73) this is a greedy algorithm with a clever form of delayed backtracking to recover

from inconsistent decisions (cycles).

(cid:73) cle is exact: it always recovers an optimal arborescence.
(cid:73) what about labeled dependencies?

(cid:73) as a matter of preprocessing, for each (cid:104)p, c(cid:105), keep only the top-scoring labeled edge.

88 / 96

chu-liu-edmonds: notes

(cid:73) this is a greedy algorithm with a clever form of delayed backtracking to recover

from inconsistent decisions (cycles).

(cid:73) cle is exact: it always recovers an optimal arborescence.
(cid:73) what about labeled dependencies?

(cid:73) as a matter of preprocessing, for each (cid:104)p, c(cid:105), keep only the top-scoring labeled edge.

(cid:73) tarjan (1977) o   ered a more e   cient, but unfortunately incorrect,

implementation.
camerini et al. (1979) corrected it.
the approach is not recursive; instead using a disjoint set data structure to keep
track of collapsed nodes.
even better: gabow et al. (1986) used a fibonacci heap to keep incoming edges
sorted, and    nds cycles in a more sensible way. also constrains root to have only
one outgoing edge.
with these tricks, o(n2) runtime.

89 / 96

more details on statistical id33

(cid:73) what about the scores? mcdonald et al. (2005) used carefully-designed features

and (something close to) the structured id88; kiperwasser and goldberg
(2016) used id182.

90 / 96

more details on statistical id33

(cid:73) what about the scores? mcdonald et al. (2005) used carefully-designed features

and (something close to) the structured id88; kiperwasser and goldberg
(2016) used id182.

(cid:73) what about higher-order parsing? requires approximate id136, e.g., dual

decomposition (martins et al., 2013).

91 / 96

important tradeo   s (and not just in nlp)

1. two extremes:

(cid:73) specialized algorithm that e   ciently solves your problem, under your assumptions.

e.g., chu-liu-edmonds for fog id33.

(cid:73) general-purpose method that solves many problems, allowing you to test the e   ect

of di   erent assumptions. e.g., id145, transition-based methods,
some forms of approximate id136.

92 / 96

important tradeo   s (and not just in nlp)

1. two extremes:

(cid:73) specialized algorithm that e   ciently solves your problem, under your assumptions.

e.g., chu-liu-edmonds for fog id33.

(cid:73) general-purpose method that solves many problems, allowing you to test the e   ect

of di   erent assumptions. e.g., id145, transition-based methods,
some forms of approximate id136.

2. two extremes:

(cid:73) fast (linear-time) but greedy
(cid:73) model-optimal but slow

93 / 96

important tradeo   s (and not just in nlp)

1. two extremes:

(cid:73) specialized algorithm that e   ciently solves your problem, under your assumptions.

e.g., chu-liu-edmonds for fog id33.

(cid:73) general-purpose method that solves many problems, allowing you to test the e   ect

of di   erent assumptions. e.g., id145, transition-based methods,
some forms of approximate id136.

2. two extremes:

(cid:73) fast (linear-time) but greedy
(cid:73) model-optimal but slow

(cid:73) dirty secret: the best way to get (english) dependency trees is to run

phrase-structure parsing, then convert.

94 / 96

references i

paolo m. camerini, luigi fratta, and francesco ma   oli. a note on    nding optimum branchings. networks, 9

(4):309   312, 1979.

y. j. chu and t. h. liu. on the shortest arborescence of a directed graph. science sinica, 14:1396   1400, 1965.

marie-catherine de marne   e, bill maccartney, and christopher d. manning. generating typed dependency

parses from phrase structure parses. in proc. of lrec, 2006.

chris dyer, miguel ballesteros, wang ling, austin matthews, and noah a. smith. transition-based dependency

parsing with stack long short-term memory. in proc. of acl, 2015.

jack edmonds. optimum branchings. journal of research of the national bureau of standards, 71b:233   240,

1967.

jason m. eisner. three new probabilistic models for id33: an exploration. in proc. of coling,

1996.

harold n. gabow, zvi galil, thomas spencer, and robert e. tarjan. e   cient algorithms for    nding minimum

spanning trees in undirected and directed graphs. combinatorica, 6(2):109   122, 1986.

yoav goldberg and joakim nivre. a dynamic oracle for arc-eager id33. in proc. of coling,

2012.

eliyahu kiperwasser and yoav goldberg. simple and accurate id33 using bidirectional lstm
feature representations. transactions of the association for computational linguistics, 4:313   327, 2016.

95 / 96

references ii

sandra k  ubler, ryan mcdonald, and joakim nivre. id33. synthesis lectures on human

language technologies. morgan and claypool, 2009. url
http://www.morganclaypool.com/doi/pdf/10.2200/s00169ed1v01y200901hlt002.

andr  e f. t. martins, miguel almeida, and noah a. smith. turning on the turbo: fast third-order

non-projective turbo parsers. in proc. of acl, 2013.

ryan mcdonald, fernando pereira, kiril ribarov, and jan hajic. non-projective id33 using

spanning tree algorithms. in proceedings of hlt-emnlp, 2005. url
http://www.aclweb.org/anthology/h/h05/h05-1066.pdf.

igor a. mel     cuk. dependency syntax: theory and practice. state university press of new york, 1987.

joakim nivre. incrementality in deterministic id33. in proc. of acl, 2004.

kenji sagae and alon lavie. a best-   rst probabilistic shift-reduce parser. in proc. of coling-acl, 2006.

robert e. tarjan. finding optimum branchings. networks, 7:25   35, 1977.
l. tesni`ere.   el  ements de syntaxe structurale. klincksieck, 1959.

96 / 96

