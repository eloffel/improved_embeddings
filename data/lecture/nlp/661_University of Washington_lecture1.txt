natural language processing (csep 517):

introduction & language models

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

march 27, 2017

1 / 87

what is nlp?

nl     {mandarin chinese, english, spanish, hindi, . . . , lushootseed}

automation of:

(cid:73) analysis (nl     r)
(cid:73) generation (r     nl)
(cid:73) acquisition of r from knowledge and data

what is r?

2 / 87

3 / 87

analysisgenerationrnl4 / 87

what does it mean to    know    a language?

5 / 87

levels of linguistic knowledge

6 / 87

phonologyorthographymorphologysyntaxsemanticspragmaticsdiscourseid102"shallower""deeper"speechtextlexemesorthography

7 / 87

                                                                                                                                                                                                                                                                                                                                                                                            .                                                                                                                                                                                                                                                         15          ....morphology

uygarla  st  ramad  klar  m  zdanm    ss  n  zcas  na

   (behaving) as if you are among those whom we could not civilize   

tifgosh et ha-leled ba-gan
   you will meet the boy in the park   

unfriend, obamacare, manfuckinghattan

8 / 87

the challenges of    words   

(cid:73) segmenting text into words (e.g., thai example)
(cid:73) morphological variation (e.g., turkish and hebrew examples)
(cid:73) words with multiple meanings: bank, mean
(cid:73) domain-speci   c meanings: latex
(cid:73) multiword expressions: make a decision, take out, make up, bad hombres

9 / 87

example: part-of-speech tagging

ikr

smh

he

asked

   r

yo

last

name

so

he

can

add

u

on

fb

lololol

10 / 87

example: part-of-speech tagging

i know, right

shake my head

ikr

smh

he

asked

for
   r

your
yo

last

name

so

he

can

add

you
u

facebook

laugh out loud

on

fb

lololol

11 / 87

example: part-of-speech tagging

i know, right

shake my head

ikr
!

smh

g

he
o

asked

v

for
   r
p

your
yo
d

last
a

name

n

interjection

acronym

pronoun

verb

prep.

det.

adj.

noun

so
p

he
o

can
v

add
v

you
u
on
o p

facebook

laugh out loud

fb
   

lololol

!

preposition

proper noun

12 / 87

syntax

np

vs.

np

np

noun

adj.

np

adj.

noun

processing

natural

noun

noun

natural

language

language

processing

13 / 87

morphology + syntax

a ship-shipping ship, shipping shipping-ships.

14 / 87

syntax + semantics

we saw the woman with the telescope wrapped in paper.

15 / 87

syntax + semantics

we saw the woman with the telescope wrapped in paper.

(cid:73) who has the telescope?

16 / 87

syntax + semantics

we saw the woman with the telescope wrapped in paper.

(cid:73) who has the telescope?
(cid:73) who or what is wrapped in paper?

17 / 87

syntax + semantics

we saw the woman with the telescope wrapped in paper.

(cid:73) who has the telescope?
(cid:73) who or what is wrapped in paper?
(cid:73) an event of perception, or an assault?

18 / 87

semantics

every    fteen minutes a woman in this country gives birth.

    groucho marx

19 / 87

semantics

every    fteen minutes a woman in this country gives birth. our job is to    nd
this woman, and stop her!

    groucho marx

20 / 87

can r be    meaning   ?

depends on the application!

(cid:73) giving commands to a robot
(cid:73) querying a database
(cid:73) reasoning about relatively closed, grounded worlds

harder to formalize:

(cid:73) analyzing opinions
(cid:73) talking about politics or policy
(cid:73) ideas in science

21 / 87

why nlp is hard

1. mappings across levels are complex.

(cid:73) a string may have many possible interpretations in di   erent contexts, and resolving

ambiguity correctly may rely on knowing a lot about the world.

(cid:73) richness: any meaning may be expressed many ways, and there are immeasurably

many meanings.

(cid:73) linguistic diversity across languages, dialects, genres, styles, . . .
2. appropriateness of a representation depends on the application.
3. any r is a theorized construct, not directly observable.
4. there are many sources of variation and noise in linguistic input.

22 / 87

desiderata for nlp methods
(ordered arbitrarily)

1. sensitivity to a wide range of the phenomena and constraints in human language

2. generality across di   erent languages, genres, styles, and modalities

3. computational e   ciency at construction time and runtime

4. strong formal guarantees (e.g., convergence, statistical e   ciency, consistency,

etc.)

5. high accuracy when judged against expert annotations and/or task-speci   c

performance

23 / 87

nlp ?= machine learning

(cid:73) to be successful, a machine learner needs bias/assumptions; for nlp, that might

be linguistic theory/representations.

(cid:73) r is not directly observable.
(cid:73) early connections to id205 (1940s)
(cid:73) symbolic, probabilistic, and connectionist ml have all seen nlp as a source of

inspiring applications.

24 / 87

nlp ?= linguistics

(cid:73) nlp must contend with nl data as found in the world
(cid:73) nlp     computational linguistics
(cid:73) linguistics has begun to use tools originating in nlp!

25 / 87

fields with connections to nlp

(cid:73) machine learning
(cid:73) linguistics (including psycho-, socio-, descriptive, and theoretical)
(cid:73) cognitive science
(cid:73) id205
(cid:73) logic
(cid:73) theory of computation
(cid:73) data science
(cid:73) political science
(cid:73) psychology
(cid:73) economics
(cid:73) education

26 / 87

the engineering side

(cid:73) application tasks are di   cult to de   ne formally; they are always evolving.
(cid:73) objective evaluations of performance are always up for debate.
(cid:73) di   erent applications require di   erent r.
(cid:73) people who succeed in nlp for long periods of time are foxes, not hedgehogs.

27 / 87

today   s applications

(cid:73) conversational agents
(cid:73) information extraction and id53
(cid:73) machine translation
(cid:73) opinion and id31
(cid:73) social media analysis
(cid:73) rich visual understanding
(cid:73) essay evaluation
(cid:73) mining legal, medical, or scholarly literature

28 / 87

factors changing the nlp landscape
(hirschberg and manning, 2015)

(cid:73) increases in computing power
(cid:73) the rise of the web, then the social web
(cid:73) advances in machine learning
(cid:73) advances in understanding of language in social context

29 / 87

administrivia

30 / 87

course website

http://courses.cs.washington.edu/courses/csep517/17sp/

31 / 87

your instructors

noah (instructor):

(cid:73) uw cse professor since 2015, teaching nlp since 2006, studying nlp since

1998,    rst nlp program in 1991

(cid:73) research interests: machine learning for structured problems in nlp, nlp for

social science

george (ta):

(cid:73) computer science ph.d. student
(cid:73) research interests: machine learning for multilingual nlp

32 / 87

outline of cse 517

1. probabilistic language models, which de   ne id203 distributions over text

passages. (about 2 weeks)

2. text classi   ers, which infer attributes of a piece of text by    reading    it. (about 1

week)

3. sequence models (about 1 week)
4. parsers (about 2 weeks)
5. semantics (about 2 weeks)
6. machine translation (about 1 week)

33 / 87

readings

(cid:73) main reference text: jurafsky and martin, 2008, some chapters from new edition

(jurafsky and martin, forthcoming) when available

(cid:73) course notes from the instructor and others
(cid:73) research articles

lecture slides will include references for deeper reading on some topics.

34 / 87

evaluation

(cid:73) approximately    ve assignments (a1   5), completed individually (50%).
(cid:73) quizzes (20%), given roughly weekly, online
(cid:73) an exam (30%), to take place at the end of the quarter

35 / 87

evaluation

(cid:73) approximately    ve assignments (a1   5), completed individually (50%).

(cid:73) some pencil and paper, mostly programming
(cid:73) graded mostly on your writeup (so please take written communication seriously!)

(cid:73) quizzes (20%), given roughly weekly, online
(cid:73) an exam (30%), to take place at the end of the quarter

36 / 87

to-do list

(cid:73) entrance survey: due wednesday
(cid:73) online quiz: due friday
(cid:73) print, sign, and return the academic integrity statement
(cid:73) read: jurafsky and martin (2008, ch. 1), hirschberg and manning (2015), and

smith (2017);
optionally, jurafsky and martin (2016) and collins (2011)   2

(cid:73) a1, out today, due april 7

37 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete

38 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )

39 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   

40 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   
(cid:73) joint id203: p(x = x, y = y)

41 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   
(cid:73) joint id203: p(x = x, y = y)
(cid:73) id155: p(x = x | y = y)

42 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   
(cid:73) joint id203: p(x = x, y = y)
(cid:73) id155: p(x = x | y = y) =

p(x = x, y = y)

p(y = y)

43 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   
(cid:73) joint id203: p(x = x, y = y)
(cid:73) id155: p(x = x | y = y) =
(cid:73) always true:

p(x = x, y = y)

p(y = y)

p(x = x, y = y) = p(x = x | y = y)    p(y = y) = p(y = y | x = x)    p(x = x)

44 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   
(cid:73) joint id203: p(x = x, y = y)
(cid:73) id155: p(x = x | y = y) =
(cid:73) always true:

p(x = x, y = y)

p(y = y)

p(x = x, y = y) = p(x = x | y = y)    p(y = y) = p(y = y | x = x)    p(x = x)

(cid:73) sometimes true: p(x = x, y = y) = p(x = x)    p(y = y)

45 / 87

very quick review of id203

(cid:73) event space (e.g., x , y)   in this class, usually discrete
(cid:73) random variables (e.g., x, y )
(cid:73) typical statement:    random variable x takes value x     x with id203

p(x = x), or, in shorthand, p(x)   
(cid:73) joint id203: p(x = x, y = y)
(cid:73) id155: p(x = x | y = y) =
(cid:73) always true:

p(x = x, y = y)

p(y = y)

p(x = x, y = y) = p(x = x | y = y)    p(y = y) = p(y = y | x = x)    p(x = x)

(cid:73) sometimes true: p(x = x, y = y) = p(x = x)    p(y = y)
(cid:73) the di   erence between true and estimated id203 distributions

46 / 87

language models: de   nitions

(cid:73) v is a    nite set of (discrete) symbols ((cid:44)    words    or possibly characters); v = |v|
(cid:73) v    is the (in   nite) set of sequences of symbols from v whose    nal symbol is (cid:56)
(cid:73) p : v        r, such that:

(cid:73) for any x     v   , p(x)     0

p(x = x) = 1

(cid:73) (cid:88)

x   v   

(i.e., p is a proper id203 distribution.)

id38: estimate p from examples, x1:n = (cid:104)x1, x2, . . . , xn(cid:105).

47 / 87

immediate objections

1. why would we want to do this?

2. are the nonnegativity and sum-to-one constraints really necessary?
3. is       nite v    realistic?

48 / 87

motivation: id87s

a pattern for modeling a pair of random variables, d and o:
source        d        channel        o

49 / 87

motivation: id87s

a pattern for modeling a pair of random variables, d and o:
source        d        channel        o

(cid:73) d is the plaintext, the true message, the missing information, the output

50 / 87

motivation: id87s

a pattern for modeling a pair of random variables, d and o:
source        d        channel        o

(cid:73) d is the plaintext, the true message, the missing information, the output
(cid:73) o is the ciphertext, the garbled message, the observable evidence, the input

51 / 87

motivation: id87s

a pattern for modeling a pair of random variables, d and o:
source        d        channel        o

(cid:73) d is the plaintext, the true message, the missing information, the output
(cid:73) o is the ciphertext, the garbled message, the observable evidence, the input
(cid:73) decoding: select d given o = o.
d    = argmax

p(d | o)
p(o | d)    p(d)

d

= argmax

d

= argmax

d

p(o)
p(o | d)

(cid:124) (cid:123)(cid:122) (cid:125)

channel model

  

(cid:124)(cid:123)(cid:122)(cid:125)

p(d)

source model

52 / 87

noisy channel example: id103

source        sequence in v           channel        acoustics

(cid:73) acoustic model de   nes p(sounds | d) (channel)
(cid:73) language model de   nes p(d) (source)

53 / 87

noisy channel example: id103
credit: luke zettlemoyer

log p(acoustics | word sequence)

word sequence
the station signs are in deep in english
the stations signs are in deep in english
the station signs are in deep into english
the station    s signs are in deep in english
the station signs are in deep in the english
the station signs are indeed in english
the station    s signs are indeed in english
the station signs are indians in english
the station signs are indian in english
the stations signs are indians in english
the stations signs are indians and english

-14732
-14735
-14739
-14740
-14741
-14757
-14760
-14790
-14799
-14807
-14815

54 / 87

noisy channel example: machine translation

also knowing nothing o   cial about, but having guessed and inferred
considerable about, the powerful new mechanized methods in
cryptography   methods which i believe succeed even when one does not
know what language has been coded   one naturally wonders if the problem of
translation could conceivably be treated as a problem in cryptography. when
i look at an article in russian, i say:    this is really written in english, but it
has been coded in some strange symbols. i will now proceed to decode.   

warren weaver, 1955

55 / 87

noisy channel examples

(cid:73) id103
(cid:73) machine translation
(cid:73) id42
(cid:73) spelling and grammar correction

56 / 87

immediate objections

1. why would we want to do this?

2. are the nonnegativity and sum-to-one constraints really necessary?
3. is       nite v    realistic?

57 / 87

evaluation: perplexity

intuitively, language models should assign high id203 to real language they have
not seen before.
for out-of-sample (   held-out    or    test   ) data   x1:m:

m(cid:89)

(cid:73) id203 of   x1:m is

p(  xi)

i=1

58 / 87

intuitively, language models should assign high id203 to real language they have
not seen before.
for out-of-sample (   held-out    or    test   ) data   x1:m:

evaluation: perplexity

m(cid:89)

(cid:73) id203 of   x1:m is

i=1

(cid:73) log-id203 of   x1:m is

p(  xi)

m(cid:88)

log2 p(  xi)

i=1

59 / 87

evaluation: perplexity

intuitively, language models should assign high id203 to real language they have
not seen before.
for out-of-sample (   held-out    or    test   ) data   x1:m:

m(cid:89)

(cid:73) id203 of   x1:m is

i=1

(cid:73) log-id203 of   x1:m is

p(  xi)

m(cid:88)

log2 p(  xi)

i=1

(cid:73) average log-id203 per word of   x1:m is

m(cid:88)

i=1

l =

1
m

log2 p(  xi)

if m =(cid:80)m

i=1 |  xi| (total number of words in the corpus)

60 / 87

evaluation: perplexity

intuitively, language models should assign high id203 to real language they have
not seen before.
for out-of-sample (   held-out    or    test   ) data   x1:m:

m(cid:89)

(cid:73) id203 of   x1:m is

i=1

(cid:73) log-id203 of   x1:m is

p(  xi)

m(cid:88)

log2 p(  xi)

i=1

(cid:73) average log-id203 per word of   x1:m is

m(cid:88)

i=1

l =

1
m

log2 p(  xi)

if m =(cid:80)m

i=1 |  xi| (total number of words in the corpus)

(cid:73) perplexity (relative to   x1:m) is 2   l

61 / 87

evaluation: perplexity

intuitively, language models should assign high id203 to real language they have
not seen before.
for out-of-sample (   held-out    or    test   ) data   x1:m:

m(cid:89)

(cid:73) id203 of   x1:m is

i=1

(cid:73) log-id203 of   x1:m is

p(  xi)

m(cid:88)

log2 p(  xi)

i=1

(cid:73) average log-id203 per word of   x1:m is

m(cid:88)

i=1

l =

1
m

log2 p(  xi)

if m =(cid:80)m

i=1 |  xi| (total number of words in the corpus)

(cid:73) perplexity (relative to   x1:m) is 2   l

lower is better.

62 / 87

understanding perplexity

m(cid:88)

i=1

    1
m

2

log2 p(  xi)

it   s a branching factor!

(cid:73) assign id203 of 1 to the test data     perplexity = 1
(cid:73) assign id203 of 1|v| to every word     perplexity = |v|
(cid:73) assign id203 of 0 to anything     perplexity =    
(cid:73) this motivates a stricter constraint than we had before:

(cid:73) for any x     v   , p(x) > 0

63 / 87

perplexity

(cid:73) perplexity on conventionally accepted test sets is often reported in papers.
(cid:73) generally, i won   t discuss perplexity numbers much, because:

(cid:73) perplexity is only an intermediate measure of performance.
(cid:73) understanding the models is more important than remembering how well they

perform on particular train/test sets.

(cid:73) if you   re curious, look up numbers in the literature; always take them with a grain

of salt!

64 / 87

immediate objections

1. why would we want to do this?

2. are the nonnegativity and sum-to-one constraints really necessary?
3. is       nite v    realistic?

65 / 87

is       nite v    realistic?

no

66 / 87

is       nite v    realistic?

no
no
n0
-no
notta
no
/no
//no
(no
|no

67 / 87

the id38 problem

input: x1:n (   training data   )
output: p : v        r+
(cid:44) p should be a    useful    measure of plausibility (not grammaticality).

68 / 87

a trivial language model

p(x) =

|{i | xi = x}|

n

=

cx1:n(x)

n

69 / 87

a trivial language model

p(x) =

|{i | xi = x}|

n

=

cx1:n(x)

n

what if x is not in the training data?

70 / 87

using the chain rule

                     

(cid:96)(cid:89)

p(x1 = x1 | x0 = x0)
   p(x2 = x2 | x0:1 = x0:1)
   p(x3 = x3 | x0:2 = x0:2)
...
   p(x(cid:96) = (cid:56) | x0:(cid:96)   1 = x0:(cid:96)   1)
p(xj = xj | x0:j   1 = x0:j   1)

                     

j=1

p(x = x) =

=

71 / 87

unigram model

(cid:96)(cid:89)

j=1

p(x = x) =

assumption=

maximum likelihood estimate:

p(xj = xj | x0:j   1 = x0:j   1)

(cid:96)(cid:89)

(cid:96)(cid:89)

  xj     (cid:96)(cid:89)

p  (xj = xj) =

j=1

j=1

j=1

   v     v,     v =

|{i, j | [xi]j = v}|

n

=

cx1:n(v)

n

where n =(cid:80)n

i=1 |xi|.

also known as    relative frequency estimation.   

    xj

72 / 87

73 / 87

unigram model

(cid:96)(cid:89)

j=1

p(x = x) =

assumption=

maximum likelihood estimate:

p(xj = xj | x0:j   1 = x0:j   1)

(cid:96)(cid:89)

(cid:96)(cid:89)

  xj     (cid:96)(cid:89)

p  (xj = xj) =

j=1

j=1

j=1

   v     v,     v =

|{i, j | [xi]j = v}|

n

=

cx1:n(v)

n

where n =(cid:80)n

i=1 |xi|.

also known as    relative frequency estimation.   

    xj

74 / 87

unigram models: assessment

pros:

(cid:73) easy to understand

(cid:73) cheap

(cid:73) good enough for information retrieval

(maybe)

cons:

(cid:73)    bag of words    assumption is

linguistically inaccurate
(cid:73) p(the the the the) (cid:29)
p(i want ice cream)

(cid:73) data sparseness; high variance in the

estimator

(cid:73)    out of vocabulary    problem

75 / 87

markov models     id165 models

(cid:96)(cid:89)

j=1

p(x = x) =

(cid:96)(cid:89)

p(xj = xj | x0:j   1 = x0:j   1)

assumption=

p  (xj = xj | xj   n+1:j   1 = xj   n+1:j   1)

j=1

(n     1)th-order markov assumption     id165 model

(cid:73) unigram model is the n = 1 case
(cid:73) for a long time, trigram models (n = 3) were widely used
(cid:73) 5-gram models (n = 5) are not uncommon now in mt

76 / 87

estimating id165 models

(cid:96)(cid:89)

  v
   v     v

c(v)
n

p  (x) =

parameters:

id113:

unigram bigram

(cid:96)(cid:89)

  xj

  xj|xj   1

j=1

j=1

j=1

trigram

(cid:96)(cid:89)

  xj|xj   2xj   1

  v|v(cid:48)
   v     v, v(cid:48)     v     {(cid:13)}

  v|v(cid:48)(cid:48)v(cid:48)
   v     v, v(cid:48), v(cid:48)(cid:48)     v     {(cid:13)}

(cid:80)

c(v(cid:48)v)
u   v c(v(cid:48)u)

(cid:80)

c(v(cid:48)(cid:48)v(cid:48)v)
u   v c(v(cid:48)(cid:48)v(cid:48)u)

general case:

(cid:96)(cid:89)

j=1

  xj|xj   n+1:j   1

  v|h,    v     v, h     (v     {(cid:13)})n   1

(cid:80)

c(hv)
u   v c(hu)

77 / 87

the problem with id113

(cid:73) the curse of dimensionality: the number of parameters grows exponentially in n
(cid:73) data sparseness: most id165s will never be observed, even if they are

linguistically plausible

(cid:73) no one actually uses the id113!

78 / 87

smoothing

a few years ago, i   d have spent a whole lecture on this! (cid:47)
(cid:73) simple method: add    > 0 to every count (including zero-counts) before

normalizing

(cid:73) what makes it hard: ensuring that the probabilities over all sequences sum to one

(cid:73) otherwise, perplexity calculations break

(cid:73) longstanding champion: modi   ed kneser-ney smoothing (chen and goodman,

1998)

(cid:73) stupid backo   : reasonable, easy solution when you don   t care about perplexity

(brants et al., 2007)

79 / 87

interpolation

if p and q are both language models, then so is

  p + (1       )q

for any        [0, 1].

(cid:73) this idea underlies many smoothing methods
(cid:73) often a new model q only beats a reigning champion p when interpolated with it
(cid:73) how to pick the    hyperparameter      ?

80 / 87

algorithms to know

(cid:73) score a sentence x
(cid:73) train from a corpus x1:n
(cid:73) sample a sentence given   

81 / 87

id165 models: assessment

pros:

(cid:73) easy to understand
(cid:73) cheap (with modern hardware; lin

and dyer, 2010)

(cid:73) good enough for machine

translation, id103, . . .

cons:

(cid:73) markov assumption is linguistically

inaccurate

(cid:73) (but not as bad as unigram

models!)

(cid:73) data sparseness; high variance in the

estimator

(cid:73)    out of vocabulary    problem

82 / 87

dealing with out-of-vocabulary terms

(cid:73) de   ne a special oov or    unknown    symbol unk. transform some (or all) rare

words in the training data to unk.

(cid:73) (cid:47) you cannot fairly compare two language models that apply di   erent unk

treatments!

(cid:73) build a language model at the character level.

83 / 87

what   s wrong with id165s?

data sparseness: most histories and most words will be seen only rarely (if at all).

84 / 87

what   s wrong with id165s?

data sparseness: most histories and most words will be seen only rarely (if at all).

next central idea: teach histories and words how to share.

85 / 87

id148: de   nitions

we de   ne a conditional log-linear model p(y | x) as:
(cid:73) y is the set of events/outputs ((cid:44) for id38, v)
(cid:73) x is the set of contexts/inputs ((cid:44) for id165 id38, v n   1)
(cid:73)    : x    y     rd is a feature vector function
(cid:73) w     rd are the model parameters

pw(y = y | x = x) =

(cid:88)

y(cid:48)   y

exp w      (x, y)
exp w      (x, y(cid:48))

86 / 87

references i

thorsten brants, ashok c. popat, peng xu, franz j. och, and je   rey dean. large language models in machine

translation. in proc. of emnlp-conll, 2007.

stanley f. chen and joshua goodman. an empirical study of smoothing techniques for id38.

technical report tr-10-98, center for research in computing technology, harvard university, 1998.

michael collins. id148, memms, and crfs, 2011. url

http://www.cs.columbia.edu/~mcollins/crf.pdf.

julia hirschberg and christopher d. manning. advances in natural language processing. science, 349(6245):

261   266, 2015. url https://www.sciencemag.org/content/349/6245/261.full.

daniel jurafsky and james h. martin. speech and language processing: an introduction to natural language

processing, computational linguistics, and id103. prentice hall, second edition, 2008.

daniel jurafsky and james h. martin. id165s (draft chapter), 2016. url

https://web.stanford.edu/~jurafsky/slp3/4.pdf.

daniel jurafsky and james h. martin. speech and language processing: an introduction to natural language

processing, computational linguistics, and id103. prentice hall, third edition, forthcoming.
url https://web.stanford.edu/~jurafsky/slp3/.

jimmy lin and chris dyer. data-intensive text processing with mapreduce. morgan and claypool, 2010.

noah a. smith. probabilistic language models 1.0, 2017. url

http://homes.cs.washington.edu/~nasmith/papers/plm.17.pdf.

87 / 87

