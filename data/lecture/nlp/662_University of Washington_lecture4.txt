natural language processing (csep 517):

sequence models

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

april 17, 2017

1 / 98

to-do list

(cid:73) online quiz: due sunday
(cid:73) read: collins (2011), which has somewhat di   erent notation; jurafsky and martin

(2016a,b,c)

(cid:73) a2 due april 23 (sunday)

2 / 98

linguistic analysis: overview

every linguistic analyzer is comprised of:

1. theoretical motivation from linguistics and/or the text domain
2. an algorithm that maps v    to some output space y.
3. an implementation of the algorithm

(cid:73) once upon a time: rule systems and crafted rules
(cid:73) most common now: supervised learning from annotated data
(cid:73) frontier: less supervision (semi-, un-, reinforcement, distant, . . . )

3 / 98

sequence labeling

after text classi   cation (v        l), the next simplest type of output is a sequence
labeling.

(cid:104)x1, x2, . . . , x(cid:96)(cid:105) (cid:55)    (cid:104)y1, y2, . . . , y(cid:96)(cid:105)

x (cid:55)    y

every word gets a label in l.
example problems:

(cid:73) part-of-speech tagging (church, 1988)
(cid:73) id147 (kernighan et al., 1990)
(cid:73) word alignment (vogel et al., 1996)
(cid:73) named-entity recognition (bikel et al., 1999)
(cid:73) compression (conroy and o   leary, 2001)

4 / 98

the simplest sequence labeler:    local    classi   er

de   ne features of a labeled word in context:   (x, i, y).

train a classi   er, e.g.,

  yi = argmax

y   l

s(x, i, y)

linear= argmax

y   l

w      (x, i, y)

decide the label for each word independently.

5 / 98

the simplest sequence labeler:    local    classi   er

de   ne features of a labeled word in context:   (x, i, y).

train a classi   er, e.g.,

  yi = argmax

y   l

s(x, i, y)

linear= argmax

y   l

w      (x, i, y)

decide the label for each word independently.

sometimes this works!

6 / 98

the simplest sequence labeler:    local    classi   er

de   ne features of a labeled word in context:   (x, i, y).

train a classi   er, e.g.,

  yi = argmax

y   l

s(x, i, y)

linear= argmax

y   l

w      (x, i, y)

decide the label for each word independently.

sometimes this works!

we can do better when there are predictable relationships between yi and yi+1.

7 / 98

generative sequence labeling: id48

(cid:96)+1(cid:89)

i=1

p(x, y) =

p(xi | yi)    p(yi | yi   1)

for each state/label y     l:

(cid:73) p(xi | yi = y) is the    emission    distribution for y
(cid:73) p(yi | yi   1 = y) is called the    transition    distribution for y

assume y0 is always a start state and y(cid:96)+1 is always a stop state; x(cid:96)+1 is always the
stop symbol.

8 / 98

graphical representation of id48

note: handling of beginning and end of sequence is a bit di   erent than before. last x

is known since p((cid:56) | (cid:56)) = 1.

9 / 98

x1x2x3x4y1y2y3y4y0y5x5structured vs. not

each of these has an advantage over the other:

(cid:73) the id48 lets the di   erent labels    interact.   
(cid:73) the local classi   er makes all of x available for every decision.

10 / 98

prediction with id48s

the classical id48 tells us to choose:

(cid:96)+1(cid:89)

i=1

argmax
y   l(cid:96)+1

p(xi,| yi)    p(yi | yi   1)

how to optimize over |l|(cid:96) choices without explicit enumeration?

11 / 98

prediction with id48s

the classical id48 tells us to choose:

(cid:96)+1(cid:89)

i=1

argmax
y   l(cid:96)+1

p(xi,| yi)    p(yi | yi   1)

how to optimize over |l|(cid:96) choices without explicit enumeration?

key: exploit the conditional independence assumptions:

yi   y 1:i   2 | yi   1
yi   y i+2:(cid:96) | yi+1

12 / 98

part-of-speech tagging example

suspect

   
   
   

i
   

   

the
   

   

noun
adj.
adv.
verb
num.
det.
punc.

present

   
   
   
   

forecast

   
   
   

is
   

   

pessimistic

.

   

with this very simple tag set, 78 = 5.7 million labelings.
(even restricting to the possibilities above, 288 labelings.)

   

13 / 98

two obvious solutions

brute force: enumerate all solutions, score them, pick the best.

greedy: pick each   yi according to:

  yi = argmax

y   l

p(y |   yi   1)    p(xi | y)

what   s wrong with these?

14 / 98

two obvious solutions

brute force: enumerate all solutions, score them, pick the best.

greedy: pick each   yi according to:

  yi = argmax

y   l

p(y |   yi   1)    p(xi | y)

what   s wrong with these?

consider:
   the old dog the footsteps of the young    (credit: julia hirschberg)
   the horse raced past the barn fell   

15 / 98

conditional independence

we can get an exact solution in polynomial time!

yi   y 1:i   2 | yi   1
yi   y i+2:(cid:96) | yi+1

given the adjacent labels to yi, others do not matter.

let   s start at the last position, (cid:96) . . .

16 / 98

high-level view of viterbi

(cid:73) the decision about y(cid:96) is a function of y(cid:96)   1, x(cid:96), and nothing else!

p(y(cid:96) = y | x, y1:((cid:96)   1)) = p

      y(cid:96) = y

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x(cid:96) = x(cid:96),

y(cid:96)+1 = (cid:56)

y(cid:96)   1 = y(cid:96)   1,

=

p(y(cid:96) = y, x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))
    p((cid:56) | y)    p(x(cid:96) | y)    p(y | y(cid:96)   1)

p(x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))

17 / 98

high-level view of viterbi

(cid:73) the decision about y(cid:96) is a function of y(cid:96)   1, x(cid:96), and nothing else!

p(y(cid:96) = y | x, y1:((cid:96)   1)) = p

      y(cid:96) = y

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x(cid:96) = x(cid:96),

y(cid:96)+1 = (cid:56)

y(cid:96)   1 = y(cid:96)   1,

=

p(y(cid:96) = y, x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))
    p((cid:56) | y)    p(x(cid:96) | y)    p(y | y(cid:96)   1)

p(x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))

(cid:73) if, for each value of y(cid:96)   1, we knew the best y1:((cid:96)   1), then picking y(cid:96) would be

easy.

18 / 98

high-level view of viterbi

(cid:73) the decision about y(cid:96) is a function of y(cid:96)   1, x(cid:96), and nothing else!

p(y(cid:96) = y | x, y1:((cid:96)   1)) = p

      y(cid:96) = y

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x(cid:96) = x(cid:96),

y(cid:96)+1 = (cid:56)

y(cid:96)   1 = y(cid:96)   1,

=

p(y(cid:96) = y, x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))
    p((cid:56) | y)    p(x(cid:96) | y)    p(y | y(cid:96)   1)

p(x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))

(cid:73) if, for each value of y(cid:96)   1, we knew the best y1:((cid:96)   1), then picking y(cid:96) would be

easy.

(cid:73) idea: for each position i, calculate the score of the best label pre   x y1:i ending in

each possible value for yi.

19 / 98

high-level view of viterbi

(cid:73) the decision about y(cid:96) is a function of y(cid:96)   1, x(cid:96), and nothing else!

p(y(cid:96) = y | x, y1:((cid:96)   1)) = p

      y(cid:96) = y

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x(cid:96) = x(cid:96),

y(cid:96)+1 = (cid:56)

y(cid:96)   1 = y(cid:96)   1,

=

p(y(cid:96) = y, x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))
    p((cid:56) | y)    p(x(cid:96) | y)    p(y | y(cid:96)   1)

p(x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))

(cid:73) if, for each value of y(cid:96)   1, we knew the best y1:((cid:96)   1), then picking y(cid:96) would be

easy.

(cid:73) idea: for each position i, calculate the score of the best label pre   x y1:i ending in

each possible value for yi.

(cid:73) with a little bookkeeping, we can then trace backwards and recover the best label

sequence.

20 / 98

chart data structure

x1

x2

. . . x(cid:96)

y
y(cid:48)
...
ylast

21 / 98

recurrence

first, think about the score of the best sequence.

let si(y) be the score of the best label sequence for x1:i that ends in y. it is de   ned
recursively:

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

22 / 98

recurrence

first, think about the score of the best sequence.

let si(y) be the score of the best label sequence for x1:i that ends in y. it is de   ned
recursively:

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max
s(cid:96)   1(y) = p(x(cid:96)   1 | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   2(y(cid:48))

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

23 / 98

recurrence

first, think about the score of the best sequence.

let si(y) be the score of the best label sequence for x1:i that ends in y. it is de   ned
recursively:

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max
s(cid:96)   1(y) = p(x(cid:96)   1 | y)    max
s(cid:96)   2(y) = p(x(cid:96)   2 | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   2(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    s(cid:96)   3(y(cid:48))

24 / 98

recurrence

first, think about the score of the best sequence.

let si(y) be the score of the best label sequence for x1:i that ends in y. it is de   ned
recursively:

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max
s(cid:96)   1(y) = p(x(cid:96)   1 | y)    max
s(cid:96)   2(y) = p(x(cid:96)   2 | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   2(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    s(cid:96)   3(y(cid:48))

...

si(y) = p(xi | y)    max

y(cid:48)   l p(y | y(cid:48))    si   1(y(cid:48))

25 / 98

recurrence

first, think about the score of the best sequence.
let si(y) be the score of the best label sequence for x1:i that ends in y. it is de   ned
recursively:

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max
s(cid:96)   1(y) = p(x(cid:96)   1 | y)    max
s(cid:96)   2(y) = p(x(cid:96)   2 | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   2(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    s(cid:96)   3(y(cid:48))

...

si(y) = p(xi | y)    max

y(cid:48)   l p(y | y(cid:48))    si   1(y(cid:48))

...

s1(y) = p(x1 | y)    p(y | y0)

26 / 98

viterbi procedure (part i: pre   x scores)

x1

x2

. . . x(cid:96)

y
y(cid:48)
...
ylast

27 / 98

viterbi procedure (part i: pre   x scores)

x2

. . . x(cid:96)

x1
s1(y)
s1(y(cid:48))

s1(ylast )

y
y(cid:48)
...
ylast

s1(y) = p(x1 | y)    p(y | y0)

28 / 98

viterbi procedure (part i: pre   x scores)

x1
s1(y)
s1(y(cid:48))

x2
s2(y)
s2(y(cid:48))

. . . x(cid:96)

s1(ylast )

s2(ylast )

y
y(cid:48)
...
ylast

si(y) = p(xi | y)    max

y(cid:48)   l p(y | y(cid:48))    si   1(y(cid:48))

29 / 98

viterbi procedure (part i: pre   x scores)

x1
s1(y)
s1(y(cid:48))

x2
s2(y)
s2(y(cid:48))

. . .

x(cid:96)
s(cid:96)(y)
s(cid:96)(y(cid:48))

s1(ylast )

s2(ylast )

s(cid:96)(ylast )

y
y(cid:48)
...
ylast

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

30 / 98

claim: max

y   l s(cid:96)(y) = max
y   l(cid:96)+1

p(x, y)

31 / 98

claim: max

y   l s(cid:96)(y) = max
y   l(cid:96)+1

p(x, y)

max
y   l s(cid:96)(y) = max

y   l p((cid:56) | y)    p(x(cid:96) | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

32 / 98

claim: max

y   l s(cid:96)(y) = max
y   l(cid:96)+1

p(x, y)

max
y   l s(cid:96)(y) = max

y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    p(x(cid:96)   1 | y(cid:48))    max

= max

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    s(cid:96)   2(y(cid:48)(cid:48))

33 / 98

claim: max

y   l s(cid:96)(y) = max
y   l(cid:96)+1

p(x, y)

max
y   l s(cid:96)(y) = max

= max

y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max
p(x(cid:96)   1 | y(cid:48))    max

= max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    p(x(cid:96)   1 | y(cid:48))    max
y(cid:48)   l p(y | y(cid:48))  

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    p(x(cid:96)   2 | y(cid:48)(cid:48))    max

y(cid:48)(cid:48)(cid:48)   l p(y(cid:48)(cid:48) | y(cid:48)(cid:48)(cid:48))    s(cid:96)   3(y(cid:48)(cid:48)(cid:48))

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    s(cid:96)   2(y(cid:48)(cid:48))

34 / 98

claim: max

y   l s(cid:96)(y) = max
y   l(cid:96)+1

p(x, y)

max
y   l s(cid:96)(y) = max

= max

y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max
p(x(cid:96)   1 | y(cid:48))    max

= max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    p(x(cid:96)   1 | y(cid:48))    max
y(cid:48)   l p(y | y(cid:48))  

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    p(x(cid:96)   2 | y(cid:48)(cid:48))    max

y(cid:48)(cid:48)(cid:48)   l p(y(cid:48)(cid:48) | y(cid:48)(cid:48)(cid:48))    s(cid:96)   3(y(cid:48)(cid:48)(cid:48))
p((cid:56) | y(cid:96))    p(x(cid:96) | y(cid:96))    p(y(cid:96) | y(cid:96)   1)    p(x(cid:96)   1 | y(cid:96)   1)    p(y(cid:96)   1 | y(cid:96)   2)  

= max
y   l(cid:96)+1
p(x(cid:96)   2 | y(cid:96)   2)       p(x1 | y1)    p(y1 | y0)

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    s(cid:96)   2(y(cid:48)(cid:48))

35 / 98

claim: max

y   l s(cid:96)(y) = max
y   l(cid:96)+1

p(x, y)

max
y   l s(cid:96)(y) = max

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    s(cid:96)   2(y(cid:48)(cid:48))

= max

y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max
y   l p((cid:56) | y)    p(x(cid:96) | y)    max
p(x(cid:96)   1 | y(cid:48))    max

= max

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))
y(cid:48)   l p(y | y(cid:48))    p(x(cid:96)   1 | y(cid:48))    max
y(cid:48)   l p(y | y(cid:48))  

y(cid:48)(cid:48)   l p(y(cid:48) | y(cid:48)(cid:48))    p(x(cid:96)   2 | y(cid:48)(cid:48))    max

y(cid:48)(cid:48)(cid:48)   l p(y(cid:48)(cid:48) | y(cid:48)(cid:48)(cid:48))    s(cid:96)   3(y(cid:48)(cid:48)(cid:48))
p((cid:56) | y(cid:96))    p(x(cid:96) | y(cid:96))    p(y(cid:96) | y(cid:96)   1)    p(x(cid:96)   1 | y(cid:96)   1)    p(y(cid:96)   1 | y(cid:96)   2)  
(cid:96)+1(cid:89)

= max
y   l(cid:96)+1
p(x(cid:96)   2 | y(cid:96)   2)       p(x1 | y1)    p(y1 | y0)

p(xi | yi)    p(yi | yi   1)

= max
y   l(cid:96)+1

i=1

36 / 98

high-level view of viterbi

(cid:73) the decision about y(cid:96) is a function of y(cid:96)   1, x(cid:96), and nothing else!

p(y(cid:96) = y | x, y1:((cid:96)   1)) = p

      y(cid:96) = y

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x(cid:96) = x(cid:96),

y(cid:96)+1 = (cid:56)

y(cid:96)   1 = y(cid:96)   1,

=

p(y(cid:96) = y, x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))
    p((cid:56) | y)    p(x(cid:96) | y)    p(y | y(cid:96)   1)

p(x(cid:96) = x(cid:96), y(cid:96)   1 = y(cid:96)   1, y(cid:96)+1 = (cid:56))

(cid:73) if, for each value of y(cid:96)   1, we knew the best y1:((cid:96)   1), then picking y(cid:96) would be

easy.

(cid:73) idea: for each position i, calculate the score of the best label pre   x y1:i ending in

each possible value for yi.

(cid:73) with a little bookkeeping, we can then trace backwards and recover the best label

sequence.

37 / 98

viterbi procedure (part i: pre   x scores and backpointers)

x1

x2

. . . x(cid:96)

y
y(cid:48)

...
ylast

38 / 98

viterbi procedure (part i: pre   x scores and backpointers)

x2

. . . x(cid:96)

y
y(cid:48)

...
ylast

x1

s1(y)
b1(y)
s1(y(cid:48))
b1(y(cid:48))

s1(ylast )
b1(ylast )

s1(y) = p(x1 | y)    p(y | y0)
b1(y) = y0

39 / 98

viterbi procedure (part i: pre   x scores and backpointers)

. . . x(cid:96)

y
y(cid:48)

...
ylast

x1

s1(y)
b1(y)
s1(y(cid:48))
b1(y(cid:48))

x2

s2(y)
b2(y)
s2(y(cid:48))
b2(y(cid:48))

s1(ylast )
b1(ylast )

s2(ylast )
b2(ylast )

si(y) = p(xi | y)    max

y(cid:48)   l p(y | y(cid:48))    si   1(y(cid:48))
p(y | y(cid:48))    si   1(y(cid:48))

bi(y) = argmax

y(cid:48)   l

40 / 98

viterbi procedure (part i: pre   x scores and backpointers)

y
y(cid:48)

...
ylast

. . .

x1

s1(y)
b1(y)
s1(y(cid:48))
b1(y(cid:48))

x2

s2(y)
b2(y)
s2(y(cid:48))
b2(y(cid:48))

x(cid:96)

s(cid:96)(y)
b(cid:96)(y)
s(cid:96)(y(cid:48))
b(cid:96)(y(cid:48))

s1(ylast )
b1(ylast )

s2(ylast )
b2(ylast )

s(cid:96)(ylast )
b(cid:96)(ylast )

s(cid:96)(y) = p((cid:56) | y)    p(x(cid:96) | y)    max
p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

b(cid:96)(y) = argmax

y(cid:48)   l p(y | y(cid:48))    s(cid:96)   1(y(cid:48))

y(cid:48)   l

41 / 98

full viterbi procedure

input: x, p(xi | yi), p(yi+1 | yi)

output:   y

1. for i     (cid:104)1, . . . , (cid:96)(cid:105):

(cid:73) solve for si(   ) and bi(   ).

2.   y(cid:96)     argmax
s(cid:96)(y)
y   l
3. for i     (cid:104)(cid:96), . . . , 1(cid:105):
(cid:73)   yi   1     b(yi)

(cid:73) special base case for i = 1 to handle start state y0 (no max)
(cid:73) general recurrence for i     (cid:104)2, . . . , (cid:96)     1(cid:105)
(cid:73) special case for i = (cid:96) to handle stopping id203

42 / 98

viterbi asymptotics

space: o(|l|(cid:96))

runtime: o(|l|2(cid:96))

x1

x2

. . . x(cid:96)

y
y(cid:48)
...
ylast

43 / 98

generalizing viterbi

(cid:73) instead of id48 parameters, we can    featurize    or    neuralize.   

44 / 98

generalizing viterbi

(cid:73) instead of id48 parameters, we can    featurize    or    neuralize.   

de   ne features of adjacent labeled words in context:   (x, i, y, y(cid:48))
   structured    classifer/predictor:

(cid:96)+1(cid:88)
(cid:96)+1(cid:88)

i=1

  y = argmax
y   l(cid:96)+1

id48= argmax
y   l(cid:96)+1

w      (x, i, yi, yi   1)

log p(xi | yi) + log p(yi | yi   1)

i=1

45 / 98

generalizing viterbi

(cid:73) instead of id48 parameters, we can    featurize    or    neuralize.   
(cid:73) viterbi instantiates an general algorithm called max-product variable

elimination, for id136 along a chain of variables with pairwise    links.   
id48s are the simplest example of a structured predictor: a collection of
classi   ers whose decisions depend on each other.

46 / 98

generalizing viterbi

(cid:73) instead of id48 parameters, we can    featurize    or    neuralize.   
(cid:73) viterbi instantiates an general algorithm called max-product variable

elimination, for id136 along a chain of variables with pairwise    links.   
id48s are the simplest example of a structured predictor: a collection of
classi   ers whose decisions depend on each other.

(cid:73) viterbi solves a special case of the    best path    problem.

47 / 98

y1 = ny1 = vy2 = ny2 = vy2 = ay3 = ny3 = vy3 = ay4 = ny4 = vy4 = ainitialy5 =    y1 = ay0 = ny0 = vy0 = ageneralizing viterbi

(cid:73) instead of id48 parameters, we can    featurize    or    neuralize.   
(cid:73) viterbi instantiates an general algorithm called max-product variable

elimination, for id136 along a chain of variables with pairwise    links.   
id48s are the simplest example of a structured predictor: a collection of
classi   ers whose decisions depend on each other.

(cid:73) viterbi solves a special case of the    best path    problem.
(cid:73) higher-order dependencies among y are also possible.

si(y, y(cid:48)) = max

y(cid:48)(cid:48)   l p(xi | y)    p(y | y(cid:48), y(cid:48)(cid:48))    si   1(y(cid:48), y(cid:48)(cid:48))

48 / 98

applications of sequence models

(cid:73) part-of-speech tagging (church, 1988)
(cid:73) supersense tagging (ciaramita and altun, 2006)
(cid:73) named-entity recognition (bikel et al., 1999)
(cid:73) multiword expressions (schneider and smith, 2015)
(cid:73) base noun phrase chunking (sha and pereira, 2003)

49 / 98

parts of speech
http://mentalfloss.com/article/65608/master-particulars-grammar-pop-culture-primer

50 / 98

parts of speech

(cid:73)    open classes   : nouns, verbs, adjectives, adverbs, numbers
(cid:73)    closed classes   :
(cid:73) modal verbs
(cid:73) prepositions (on, to)
(cid:73) particles (o   , up)
(cid:73) determiners (the, some)
(cid:73) pronouns (she, they)
(cid:73) conjunctions (and, or)

51 / 98

parts of speech in english: decisions

granularity decisions regarding:

(cid:73) verb tenses, participles
(cid:73) plural/singular for verbs, nouns
(cid:73) proper nouns
(cid:73) comparative, superlative adjectives and adverbs

some linguistic reasoning required:

(cid:73) existential there
(cid:73) in   nitive marker to
(cid:73) wh words (pronouns, adverbs, determiners, possessive whose)

interactions with id121:

(cid:73) punctuation
(cid:73) compounds (mark   ll, someone   s, gonna)

id32: 45 tags,    40 pages of guidelines (marcus et al., 1993)

52 / 98

parts of speech in english: decisions

granularity decisions regarding:

(cid:73) verb tenses, participles
(cid:73) plural/singular for verbs, nouns
(cid:73) proper nouns
(cid:73) comparative, superlative adjectives and adverbs

some linguistic reasoning required:

(cid:73) existential there
(cid:73) in   nitive marker to
(cid:73) wh words (pronouns, adverbs, determiners, possessive whose)

interactions with id121:

(cid:73) punctuation
(cid:73) compounds (mark   ll, someone   s, gonna)
(cid:73) social media: hashtag, at-mention, discourse marker (rt), url, emoticon,

abbreviations, interjections, acronyms

id32: 45 tags,    40 pages of guidelines (marcus et al., 1993)
tweetnlp: 20 tags, 7 pages of guidelines (gimpel et al., 2011)

53 / 98

example: part-of-speech tagging

ikr

smh

he

asked

   r

yo

last

name

so

he

can

add

u

on

fb

lololol

54 / 98

example: part-of-speech tagging

i know, right

shake my head

ikr

smh

he

asked

for
   r

your
yo

last

name

so

he

can

add

you
u

facebook

laugh out loud

on

fb

lololol

55 / 98

example: part-of-speech tagging

i know, right

shake my head

ikr
!

smh

g

he
o

asked

v

for
   r
p

your
yo
d

last
a

name

n

interjection

acronym

pronoun

verb

prep.

det.

adj.

noun

so
p

he
o

can
v

add
v

you
u
on
o p

facebook

laugh out loud

fb
   

lololol

!

preposition

proper noun

56 / 98

why pos?

(cid:73) text-to-speech: record, lead, protest
(cid:73) lemmatization: saw/v     see; saw/n     saw
(cid:73) quick-and-dirty multiword expressions: (adjective | noun)    noun (justeson and

katz, 1995)

(cid:73) preprocessing for harder disambiguation problems:

(cid:73) the georgia branch had taken on loan commitments . . .
(cid:73) the average of interbank o   ered rates plummeted . . .

57 / 98

a simple pos tagger

de   ne a map v     l.

58 / 98

a simple pos tagger

de   ne a map v     l.

how to pick the single pos for each word? e.g., raises, fed, . . .

59 / 98

a simple pos tagger

de   ne a map v     l.

how to pick the single pos for each word? e.g., raises, fed, . . .

id32: most frequent tag rule gives 90.3%, 93.7% if you   re clever about
handling unknown words.

60 / 98

a simple pos tagger

de   ne a map v     l.

how to pick the single pos for each word? e.g., raises, fed, . . .

id32: most frequent tag rule gives 90.3%, 93.7% if you   re clever about
handling unknown words.

all datasets have some errors; estimated upper bound for id32 is 98%.

61 / 98

supervised training of id48

given: annotated sequences (cid:104)(cid:104)x1, y1,(cid:105), . . . ,(cid:104)xn, yn(cid:105)(cid:105)

(cid:96)+1(cid:89)

i=1

p(x, y) =

  xi|yi      yi|yi   1

parameters: for each state/label y     l:
(cid:73)      |y is the    emission    distribution, estimating p(x | y) for each x     v
(cid:73)      |y is called the    transition    distribution, estimating p(y(cid:48) | y) for each y(cid:48)     l

62 / 98

supervised training of id48

given: annotated sequences (cid:104)(cid:104)x1, y1,(cid:105), . . . ,(cid:104)xn, yn(cid:105)(cid:105)

(cid:96)+1(cid:89)

i=1

p(x, y) =

  xi|yi      yi|yi   1

parameters: for each state/label y     l:
(cid:73)      |y is the    emission    distribution, estimating p(x | y) for each x     v
(cid:73)      |y is called the    transition    distribution, estimating p(y(cid:48) | y) for each y(cid:48)     l

maximum likelihood estimate: count and normalize!

63 / 98

back to pos

tnt, a trigram id48 tagger with smoothing: 96.7% (brants, 2000)

64 / 98

back to pos

tnt, a trigram id48 tagger with smoothing: 96.7% (brants, 2000)

state of the art:    97.5% (toutanova et al., 2003); uses a feature-based model with:

(cid:73) capitalization features
(cid:73) spelling features
(cid:73) name lists (   gazetteers   )
(cid:73) context words
(cid:73) hand-crafted patterns

65 / 98

back to pos

tnt, a trigram id48 tagger with smoothing: 96.7% (brants, 2000)

state of the art:    97.5% (toutanova et al., 2003); uses a feature-based model with:

(cid:73) capitalization features
(cid:73) spelling features
(cid:73) name lists (   gazetteers   )
(cid:73) context words
(cid:73) hand-crafted patterns

there might be very recent improvements to this.

66 / 98

other labels

parts of speech are a minimal syntactic representation.

sequence labeling can get you a lightweight semantic representation, too.

67 / 98

supersenses

a problem with a long history: word-sense disambiguation.

68 / 98

supersenses

a problem with a long history: word-sense disambiguation.

classical approaches assumed you had a list of ambiguous words and their senses.

(cid:73) e.g., from a dictionary

69 / 98

supersenses

a problem with a long history: word-sense disambiguation.

classical approaches assumed you had a list of ambiguous words and their senses.

(cid:73) e.g., from a dictionary

ciaramita and johnson (2003) and ciaramita and altun (2006) used a lexicon called
id138 to de   ne 41 semantic classes for words.

(cid:73) id138 (fellbaum, 1998) is a fascinating resource in its own right! see

http://id138web.princeton.edu/perl/webwn to get an idea.

70 / 98

supersenses

a problem with a long history: word-sense disambiguation.

classical approaches assumed you had a list of ambiguous words and their senses.

(cid:73) e.g., from a dictionary

ciaramita and johnson (2003) and ciaramita and altun (2006) used a lexicon called
id138 to de   ne 41 semantic classes for words.

(cid:73) id138 (fellbaum, 1998) is a fascinating resource in its own right! see

http://id138web.princeton.edu/perl/webwn to get an idea.

this represents a coarsening of the annotations in the semcor corpus (miller et al.,
1993).

71 / 98

example: box   s thirteen synonym sets, eight supersenses

1. box: a (usually rectangular) container; may have a lid.    he rummaged through a box of spare parts   

2. box/loge: private area in a theater or grandstand where a small group can watch the performance.    the

royal box was empty   

3. box/boxful: the quantity contained in a box.    he gave her a box of chocolates   

4. corner/box: a predicament from which a skillful or graceful escape is impossible.    his lying got him into a

tight corner   

5. box: a rectangular drawing.    the    owchart contained many boxes   

6. box/boxwood: evergreen shrubs or small trees

7. box: any one of several designated areas on a ball    eld where the batter or catcher or coaches are

positioned.    the umpire warned the batter to stay in the batter   s box   

8. box/box seat: the driver   s seat on a coach.    an armed guard sat in the box with the driver   

9. box: separate partitioned area in a public place for a few people.    the sentry stayed in his box to avoid

the cold   

10. box: a blow with the hand (usually on the ear).    i gave him a good box on the ear   

11. box/package: put into a box.    box the gift, please   

12. box: hit with the    st.    i   ll box your ears!   

13. box: engage in a boxing match.

72 / 98

example: box   s thirteen synonym sets, eight supersenses

1. box: a (usually rectangular) container; may have a lid.    he rummaged through a box of spare parts    (cid:32)

n.artifact

2. box/loge: private area in a theater or grandstand where a small group can watch the performance.    the

royal box was empty    (cid:32) n.artifact

3. box/boxful: the quantity contained in a box.    he gave her a box of chocolates    (cid:32) n.quantity
4. corner/box: a predicament from which a skillful or graceful escape is impossible.    his lying got him into a

tight corner    (cid:32) n.state

5. box: a rectangular drawing.    the    owchart contained many boxes    (cid:32) n.shape
6. box/boxwood: evergreen shrubs or small trees (cid:32) n.plant
7. box: any one of several designated areas on a ball    eld where the batter or catcher or coaches are

positioned.    the umpire warned the batter to stay in the batter   s box    (cid:32) n.artifact

8. box/box seat: the driver   s seat on a coach.    an armed guard sat in the box with the driver    (cid:32)

n.artifact

the cold    (cid:32) n.artifact

9. box: separate partitioned area in a public place for a few people.    the sentry stayed in his box to avoid

10. box: a blow with the hand (usually on the ear).    i gave him a good box on the ear    (cid:32) n.act
11. box/package: put into a box.    box the gift, please    (cid:32) v.contact
12. box: hit with the    st.    i   ll box your ears!    (cid:32) v.contact
13. box: engage in a boxing match. (cid:32) v.competition

73 / 98

supersense tagging example

clara

harris

,

one

of

the

n.person

guests

n.person

in

the

box

,

stood

up

and

demanded

n.artifact

v.motion

v.communication

water

.

n.substance

74 / 98

ciaramita and altun   s approach

features at each position in the sentence:

(cid:73) word
(cid:73)       rst sense    from id138 (also conjoined with word)
(cid:73) pos, coarse pos
(cid:73) shape (case, punctuation symbols, etc.)
(cid:73) previous label

all of these    t into      (x, i, y, y(cid:48)).   

75 / 98

featurizing id48s

log-id203 score of y (given x) decomposes into a sum of local scores:

(cid:96)+1(cid:88)

(cid:122)

score(x, y) =

local score at position i

(log p(xi | yi) + log p(yi | yi   1))

(cid:123)

featurized id48:

i=1

(cid:96)+1(cid:88)

i=1

score(x, y) =

= w   

(cid:125)(cid:124)

(cid:125)(cid:124)

(cid:123)(cid:122)

(cid:122)
(cid:96)+1(cid:88)
(cid:124)

i=1

local score at position i
(w      (x, i, yi, yi   1))

  (x, i, yi, yi   1)

global features,   (x, y)

(cid:123)

(cid:125)

(1)

(2)

(3)

76 / 98

what changes?

algorithmically, not much!
viterbi recurrence before (using log math):

s1(y) = log p(x1 | y) + log p(y | y0)
si(y) = log p(xi | y) + max
s(cid:96)(y) = log p((cid:56) | y) + log p(x(cid:96) | y) + max

y(cid:48)   l log p(y | y(cid:48)) + si   1(y(cid:48))

y(cid:48)   l log p(y | y(cid:48)) + s(cid:96)   1(y(cid:48))

after:

s1(y) = w      (x, 1, y, y0)
si(y) = max

y(cid:48)   l w      (x, i, y, y(cid:48)) + si   1(y(cid:48))

y(cid:48)   l w   (cid:0)  (x, (cid:96), y, y(cid:48)) +   (x, (cid:96) + 1,(cid:56), y)(cid:1) + s(cid:96)   1(y(cid:48))

s(cid:96)(y) = max

77 / 98

supervised training of sequence models (discriminative)

given: annotated sequences (cid:104)(cid:104)x1, y1,(cid:105), . . . ,(cid:104)xn, yn(cid:105)(cid:105)
assume:

predict(x) = argmax
y   l(cid:96)+1

= argmax
y   l(cid:96)+1

= argmax
y   l(cid:96)+1

= argmax
y   l(cid:96)+1

score(x, y)

(cid:96)+1(cid:88)
w    (cid:96)+1(cid:88)

i=1

i=1

w      (x, y)

w      (x, i, yi, yi   1)

  (x, i, yi, yi   1)

estimate: w

78 / 98

id88

id88 algorithm for classi   cation:

(cid:73) for t     {1, . . . , t}:
(cid:73) pick it uniformly at random from {1, . . . , n}.
(cid:16)
(cid:73)   (cid:96)it     argmax
(cid:96)   l
(cid:73) w     w       

(cid:17)
w      (xit, (cid:96))
  (xit,   (cid:96)it)       (xit, (cid:96)it)

79 / 98

structured id88
collins (2002)

id88 algorithm for classi   cation id170:

(cid:73) for t     {1, . . . , t}:
(cid:73) pick it uniformly at random from {1, . . . , n}.
(cid:73)   yit     argmax
y   l(cid:96)+1

(cid:73) w     w       (cid:0)  (xit,   yit)       (xit, yit)(cid:1)

w      (xit, y)

this can be viewed as stochastic subid119 on the structured hinge loss:

n(cid:88)

i=1

w      (xi, y)

max
y   l(cid:96)i+1

(cid:124)

(cid:123)(cid:122)

fear

(cid:125)

(cid:125)
(cid:124)
    w      (xi, yi)

(cid:123)(cid:122)

hope

80 / 98

back to supersenses

clara

harris

,

one

of

the

n.person

guests

n.person

in

the

box

,

stood

up

and

demanded

n.artifact

v.motion

v.communication

water

.

n.substance

shouldn   t clara harris and stood up be respectively    grouped   ?

81 / 98

segmentations

segmentation:

(cid:73) input: x = (cid:104)x1, x2, . . . , x(cid:96)(cid:105)
(cid:73) output:

where (cid:96) =(cid:80)m

i=1 (cid:96)i.

(cid:42) x1:(cid:96)1,
x(1+(cid:80)m   1

x(1+(cid:96)1):((cid:96)1+(cid:96)2),
x(1+(cid:96)1+(cid:96)2):((cid:96)1+(cid:96)2+(cid:96)3), . . . ,

i=1 (cid:96)i):(cid:80)m

i=1 (cid:96)i

(cid:43)

application: id40 for writing systems without whitespace.

(4)

82 / 98

segmentations

segmentation:

(cid:73) input: x = (cid:104)x1, x2, . . . , x(cid:96)(cid:105)
(cid:73) output:

where (cid:96) =(cid:80)m

i=1 (cid:96)i.

(cid:42) x1:(cid:96)1,
x(1+(cid:80)m   1

x(1+(cid:96)1):((cid:96)1+(cid:96)2),
x(1+(cid:96)1+(cid:96)2):((cid:96)1+(cid:96)2+(cid:96)3), . . . ,

i=1 (cid:96)i):(cid:80)m

i=1 (cid:96)i

(cid:43)

application: id40 for writing systems without whitespace.
with arbitrarily long segments, this does not look like a job for   (x, i, y, y(cid:48))!

(4)

83 / 98

segmentation as sequence labeling
ramshaw and marcus (1995)

two labels: b (   beginning of new segment   ), i (   inside segment   )

(cid:73) (cid:96)1 = 4, (cid:96)2 = 3, (cid:96)3 = 1, (cid:96)4 = 2        (cid:104)b, i, i, i, b, i, i, b, b, i(cid:105)

three labels: b, i, o (   outside segment   )

five labels: b, i, o, e (   end of segment   ), s (   singleton   )

84 / 98

segmentation as sequence labeling
ramshaw and marcus (1995)

two labels: b (   beginning of new segment   ), i (   inside segment   )

(cid:73) (cid:96)1 = 4, (cid:96)2 = 3, (cid:96)3 = 1, (cid:96)4 = 2        (cid:104)b, i, i, i, b, i, i, b, b, i(cid:105)

three labels: b, i, o (   outside segment   )

five labels: b, i, o, e (   end of segment   ), s (   singleton   )

bonus: combine these with a label to get labeled segmentation!

85 / 98

id39 as segmentation and labeling

an older and narrower subset of supersenses used in information extraction:

(cid:73) person,
(cid:73) location,
(cid:73) organization,
(cid:73) geopolitical entity,
(cid:73) . . . and perhaps domain-speci   c additions.

86 / 98

id39

with commander chris ferguson at the helm ,

person

atlantis touched down at kennedy space center .
spacecraft

location

87 / 98

id39

with commander chris ferguson at the helm ,

person

o

b

i

i

o o o o

atlantis touched down at kennedy space center .
spacecraft

location

b

o

o o

b

i

i

o

88 / 98

id39: evaluation

1

3

2

9
x = britain sent warships across the english channel monday to
o
y = b
y(cid:48) = o
o

o o
o o

o
o

o
o

b
b

b
b

i
i

4

5

6

7

8

12

10

13

11

19
rescue britons stranded by eyjafjallaj  okull    s volcanic ash cloud .
o o o
o o o

o
o

o
o

o
o

o
o

o
o

b
b

b
b

14

16

17

18

15

89 / 98

segmentation evaluation

typically: precision, recall, and f1.

90 / 98

multiword expressions
schneider et al. (2014b)

talk

to injury, make o    with

(cid:73) mw compounds: red tape, motion picture, daddy longlegs, bayes net, hot air balloon, skinny dip, trash
(cid:73) verb-particle: pick up, dry out, take over, cut short
(cid:73) verb-preposition: refer to, depend on, look for, prevent from
(cid:73) verb-noun(-preposition): pay attention (to), go bananas, lose it, break a leg, make the most of
(cid:73) support verb: make decisions, take breaks, take pictures, have fun, perform surgery
(cid:73) other phrasal verb: put up with, miss out (on), get rid of, look forward to, run amok, cry foul, add insult
(cid:73) pp modi   er: above board, beyond the pale, under the weather,at all, from time to time, in the nick of
(cid:73) coordinated phrase: cut and dry, more or less, up and leave
(cid:73) conjunction/connective: as well as, let alone, in spite of, on the face of it/on its face
(cid:73) semi-   xed vp: smack <one>   s lips, pick up where <one> left o   , go over <thing> with a
(cid:73)    xed phrase: easy as pie, scared to death, go to hell in a handbasket, bring home the bacon, leave of
(cid:73) phatic: you   re welcome. me neither!
(cid:73) proverb: beggars can   t be choosers. the early bird gets the worm. to each his own. one man   s

   ne-tooth(ed) comb, take <one>   s time, draw <oneself> up to <one>   s full height

absence, sense of humor

time

<thing1> is another man   s <thing2>.

91 / 98

sequence labeling with nesting
schneider et al. (2014a)

he was willing
o

o

o

to
o

budge1

b

a2
b

little2

    

on1
  i

the
o

price

o

which means4

o

b

a4
3
  i

lot4
3
  i

to4 me4
  i
  i

.
o

strong (subscript) vs. weak (superscript) mwes.

one level of nesting, plus strong/weak distinction, can be handled with an eight-tag
scheme.

92 / 98

back to syntax

base noun phrase chunking:

[he]np reckons [the current account de   cit]np will narrow to

[only $ 1.8 billion]np in [september]np

(what is a base noun phrase?)

   chunking    used generically includes base verb and prepositional phrases, too.

sequence labeling with bio tags and features can be applied to this problem (sha and
pereira, 2003).

93 / 98

remarks

sequence models are extremely useful:

(cid:73) syntax: part-of-speech tags, base noun phrase chunking
(cid:73) semantics: supersense tags, id39, multiword expressions

all of these are called    shallow    methods (why?).

94 / 98

remarks

sequence models are extremely useful:

(cid:73) syntax: part-of-speech tags, base noun phrase chunking
(cid:73) semantics: supersense tags, id39, multiword expressions

all of these are called    shallow    methods (why?).

issues to be aware of:

(cid:73) supervised data for these problems is not cheap.
(cid:73) performance always su   ers when you test on a di   erent style, genre, dialect, etc.

than you trained on.

(cid:73) runtime depends on the size of l and the number of consecutive labels that

features can depend on.

95 / 98

references i

daniel m. bikel, richard schwartz, and ralph m. weischedel. an algorithm that learns what   s in a name.

machine learning, 34(1   3):211   231, 1999. url
http://people.csail.mit.edu/mcollins/6864/slides/bikel.pdf.

thorsten brants. tnt     a statistical part-of-speech tagger. in proc. of anlp, 2000.

kenneth w. church. a stochastic parts program and noun phrase parser for unrestricted text. in proc. of

anlp, 1988.

massimiliano ciaramita and yasemin altun. broad-coverage sense disambiguation and information extraction

with a supersense sequence tagger. in proc. of emnlp, 2006.

massimiliano ciaramita and mark johnson. supersense tagging of unknown nouns in id138. in proc. of

emnlp, 2003.

michael collins. discriminative training methods for id48: theory and experiments with

id88 algorithms. in proc. of emnlp, 2002.

michael collins. tagging with id48, 2011. url

http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/id48s.pdf.

john m. conroy and dianne p. o   leary. text summarization via id48. in proc. of sigir,

2001.

christiane fellbaum, editor. id138: an electronic lexical database. mit press, 1998.

96 / 98

references ii

kevin gimpel, nathan schneider, brendan o   connor, dipanjan das, daniel mills, jacob eisenstein, michael

heilman, dani yogatama, je   rey flanigan, and noah a. smith. part-of-speech tagging for twitter:
annotation, features, and experiments. in proc. of acl, 2011.

daniel jurafsky and james h. martin. id48 (draft chapter), 2016a. url

https://web.stanford.edu/~jurafsky/slp3/9.pdf.

daniel jurafsky and james h. martin. information extraction (draft chapter), 2016b. url

https://web.stanford.edu/~jurafsky/slp3/21.pdf.

daniel jurafsky and james h. martin. part-of-speech tagging (draft chapter), 2016c. url

https://web.stanford.edu/~jurafsky/slp3/10.pdf.

john s. justeson and slava m. katz. technical terminology: some linguistic properties and an algorithm for

identi   cation in text. natural language engineering, 1:9   27, 1995.

mark d. kernighan, kenneth w. church, and william a. gale. a id147 program based on a noisy

channel model. in proc. of coling, 1990.

mitchell p. marcus, beatrice santorini, and mary ann marcinkiewicz. building a large annotated corpus of

english: the id32. computational linguistics, 19(2):313   330, 1993.

g. a. miller, c. leacock, t. randee, and r. bunker. a semantic concordance. in proc. of hlt, 1993.

lance a ramshaw and mitchell p. marcus. text chunking using transformation-based learning, 1995. url

http://arxiv.org/pdf/cmp-lg/9505040.pdf.

97 / 98

references iii

nathan schneider and noah a. smith. a corpus and model integrating multiword expressions and supersenses.

in proc. of naacl, 2015.

nathan schneider, emily danchik, chris dyer, and noah a. smith. discriminative lexical semantic

segmentation with gaps: running the mwe gamut. transactions of the association for computational
linguistics, 2:193   206, april 2014a.

nathan schneider, spencer onu   er, nora kazour, emily danchik, michael t. mordowanec, henrietta conrad,
and noah a. smith. comprehensive annotation of multiword expressions in a social web corpus. in proc. of
lrec, 2014b.

fei sha and fernando pereira. id66 with conditional random    elds. in proc. of naacl, 2003.

kristina toutanova, dan klein, christopher d. manning, and yoram singer. feature-rich part-of-speech tagging

with a cyclic dependency network. in proc. of naacl, 2003.

stephan vogel, hermann ney, and christoph tillmann. id48-based word alignment in statistical translation. in

proc. of coling, 1996.

98 / 98

