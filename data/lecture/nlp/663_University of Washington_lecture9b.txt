natural language processing (csep 517):

machine translation (continued),

summarization, & finale

noah smith

c(cid:13) 2017

university of washington

nasmith@cs.washington.edu

may 22, 2017

1 / 30

to-do list

(cid:73) online quiz: due sunday
(cid:73) a5 due may 28 (sunday)
(cid:73) watch for    nal exam instrutions around may 29 (monday)

2 / 30

id4

original idea proposed by forcada and   neco (1997); resurgence in interest starting
around 2013.

strong starting point for current work: bahdanau et al. (2014). (my exposition is
borrowed with gratitude from a lecture by chris dyer.)

this approach eliminates (hard) alignment and phrases.

take care: here, the terminology    encoder    and    decoder    are used di   erently than in
the noisy-channel pattern.

3 / 30

high-level model

p(e = e | f ) = p(e = e | encode(f ))

(cid:96)(cid:89)

=

p(ej | e0, . . . , ej   1, encode(f ))

j=1

the encoding of the source sentence is a deterministic function of the words in that
sentence.

4 / 30

building block: recurrent neural network
review from lecture 2!

(cid:73) each input element is understood to be an element of a sequence: (cid:104)x1, x2, . . . , x(cid:96)(cid:105)
(cid:73) at each timestep t:

(cid:73) the tth input element xt is processed alongside the previous state st   1 to calculate

the new state (st).

(cid:73) the tth output is a function of the state st.
(cid:73) the same functions are applied at each iteration:

st = grecurrent(xt, st   1)
yt = goutput(st)

5 / 30

neural mt source-sentence encoder

f is a d    m matrix encoding the source sentence f (length m).

6 / 30

ich m  chte  ein  bierlookupsforward id56backward id56[  ]source sentence encoding00decoder: contextual language model

two inputs, the previous word and the source sentence context.

st = grecurrent(eet   1,

p(et = v | e1, . . . , et   1, f ) = [yt]v

yt = goutput(st)

fat(cid:124)(cid:123)(cid:122)(cid:125)

   context   

, st   1)

(the forms of the two component gs are suppressed; just remember that they (i) have
parameters and (ii) are di   erentiable with respect to those parameters.)

the neural language model we discussed earlier (mikolov et al., 2010) didn   t have the
context as an input to grecurrent.

7 / 30

neural mt decoder

8 / 30

[  ]0neural mt decoder

9 / 30

[               ][  ]0a1   neural mt decoder

10 / 30

[               ][  ]0i   d    like     a    beer    stop  a1   neural mt decoder

11 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ]a1a2      neural mt decoder

12 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ]a1a2      neural mt decoder

13 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ]a1a2a3         neural mt decoder

14 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ]a1a2a3         neural mt decoder

15 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ][               ]a1a2a3a4            neural mt decoder

16 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ][               ]a1a2a3a4            neural mt decoder

17 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ][               ][               ]a1a2a3a4a5               neural mt decoder

18 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ][               ][               ]a1a2a3a4a5               neural mt decoder

19 / 30

[               ][  ]0i   d    like     a    beer    stop  [               ][               ][               ][               ]a1a2a3a4a5               [               ][               ][               ][               ][               ]computing    attention   

let vst   1 be the    expected    input embedding for timestep t.
(parameters: v.)

attention is at = softmax(cid:0)f(cid:62)vst   1

(cid:1).

context is fat, i.e., a weighted sum of the source words    in-context representations.

20 / 30

learning and decoding

log p(e | encode(f )) =

m(cid:88)

i=1

log p(ei | e0:i   1, encode(f ))

is di   erentiable with respect to all parameters of the neural network, allowing
   end-to-end    training.

trick: train on shorter sentences    rst, then add in longer ones.

decoding typically uses id125.

21 / 30

remarks

we covered two approaches to machine translation:

(cid:73) phrase-based statistical mt following koehn et al. (2003), including probabilistic
noisy-channel models for alignment (a key preprocessing step; brown et al., 1993),
and

(cid:73) neural mt with attention, following bahdanau et al. (2014).

note two key di   erences:

(cid:73) noisy channel p(e)    p(f | e) vs.    direct    model p(e | f )
(cid:73) alignment as a discrete random variable vs. attention as a deterministic,

di   erentiable function

at the moment, neural mt is winning when you have enough data; if not,
phrase-based mt dominates.

when monolingual target-language data is plentiful, we   d like to use it! recent neural
models try (sennrich et al., 2016; xia et al., 2016; yu et al., 2017).

22 / 30

summarization

23 / 30

automatic text summarization

mani (2001) provides a survey from before statistical methods came to dominate; more
recent survey by das and martins (2008).

parallel history to machine translation:

(cid:73) noisy channel view (knight and marcu, 2002)
(cid:73) automatic evaluation (lin, 2004)

di   erences:

(cid:73) natural data sources are less obvious
(cid:73) human information needs are less obvious

we   ll brie   y consider two subtasks: compression and selection

24 / 30

sentence compression as id170
(mcdonald, 2006)

input: a sentence

output: the same sentence, with some words deleted

mcdonald   s approach:

(cid:73) de   ne a scoring function for compressed sentences that factors locally in the

output.

(cid:73) he factored into bigrams but considered input parse tree features.

(cid:73) decoding is id145 (not unlike viterbi).
(cid:73) learn feature weights from a corpus of compressed sentences, using structured

id88 or similar.

25 / 30

sentence selection

input: one or more documents and a    budget   

output: a within-budget subset of sentences (or passages) from the input

challenge: diminishing returns as more sentences are added to the summary.

classical greedy method:    maximum marginal relevance    (carbonell and goldstein,
1998)

casting the problem as submodular optimization: lin and bilmes (2009)

joint selection and compression: martins and smith (2009)

26 / 30

finale

27 / 30

mental health for exam preparation (and beyond)

most lectures included discussion of:

(cid:73) representations or tasks (input/output)
(cid:73) evaluation criteria
(cid:73) models (often with variations)
(cid:73) learning/estimation algorithms
(cid:73) nlp algorithms
(cid:73) practical advice

for each task, keep these elements separate in your mind, and reuse them where
possible.

28 / 30

references i

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly learning to align

and translate. in proc. of iclr, 2014. url https://arxiv.org/abs/1409.0473.

peter f. brown, vincent j. della pietra, stephen a. della pietra, and robert l. mercer. the mathematics of

id151: parameter estimation. computational linguistics, 19(2):263   311, 1993.

jaime carbonell and jade goldstein. the use of mmr, diversity-based reranking for reordering documents and

producing summaries. in proc. of sigir, 1998.

dipanjan das and andr  e f. t. martins. a survey of methods for automatic text summarization, 2008.
mikel l. forcada and ram  on p.   neco. recursive hetero-associative memories for translation. in international

work-conference on arti   cial neural networks, 1997.

kevin knight and daniel marcu. summarization beyond sentence extraction: a probabilistic approach to

sentence compression. arti   cial intelligence, 139(1):91   107, 2002.

philipp koehn, franz josef och, and daniel marcu. statistical phrase-based translation. in proc. of naacl,

2003.

chin-yew lin. id8: a package for automatic evaluation of summaries. in proc. of acl workshop: text

summarization branches out, 2004.

hui lin and je    a. bilmes. how to select a good training-data subset for transcription: submodular active

selection for sequences. in proc. of interspeech, 2009.

inderjeet mani. id54. john benjamins publishing, 2001.

29 / 30

references ii

andr  e f. t. martins and noah a. smith. summarization with a joint model for sentence extraction and

compression. in proc. of the acl workshop on integer id135 for natural langauge
processing, 2009.

ryan t. mcdonald. discriminative sentence compression with soft syntactic evidence. in proc. of eacl, 2006.

tomas mikolov, martin kara     at, lukas burget, jan cernock`y, and sanjeev khudanpur. recurrent neural

network based language model. in proc. of interspeech, 2010. url http:
//www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_is100722.pdf.

rico sennrich, barry haddow, and alexandra birch. improving id4 models with

monolingual data. in proc. of acl, 2016. url http://www.aclweb.org/anthology/p16-1009.

yingce xia, di he, tao qin, liwei wang, nenghai yu, tie-yan liu, and wei-ying ma. dual learning for

machine translation. in nips, 2016.

lei yu, phil blunsom, chris dyer, edward grefenstette, and tomas kocisky. the neural noisy channel. in proc.

of iclr, 2017.

30 / 30

