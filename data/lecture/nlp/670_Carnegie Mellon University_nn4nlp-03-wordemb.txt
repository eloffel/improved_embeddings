cs11-747 neural networks for nlp 

models of words

graham neubig

https://phontron.com/class/nn4nlp2017/

site

what do we want to know 

about words?

    are they the same part of speech? 

    do they have the same conjugation? 

    do these two words mean the same thing? 

    do they have some semantic relation (is-a, part-of, 

went-to-school-at)?

a manual attempt: 

id138

    id138 is a large database of words including parts of 

speech, semantic relations   

    major effort to develop, projects in many languages.  
    but can we do something similar, more complete, and 

without the effort?

image credit: nltk

   
   
   
   
   
an answer (?): 
id27s!

    a continuous vector representation of words   

    within the id27, these features of syntax and 

semantics may be included 
    element 1 might be more positive for nouns 
    element 2 might be positive for animate objects
    element 3 might have no intuitive meaning whatsoever

   
   
id27s are cool! 

(an obligatory slide)

    e.g. king-man+woman = queen (mikolov et al. 

2013)

       what is the female equivalent of king?    is not 
easily accessible in many traditional resources

how to train word 

embeddings?

    initialize randomly, train jointly with the task 
    pre-train on a supervised task (e.g. id52) 

and test on another, (e.g. parsing) 

    pre-train on an unsupervised task (e.g. 

id97)

unsupervised pre-training of word 

embeddings 
(summary of goldberg 10.4)

distributional vs. distributed 

representations

    distributional representations

    words are similar if they appear in similar contexts 

(harris 1954); distribution of words indicative of usage 
    in contrast: non-distributional representations created 

from lexical resources such as id138, etc. 

    distributed representations

    basically, something is  represented by a vector of 

values, each representing activations 

    in contrast: local representations, where represented by 

a discrete symbol (one-hot vector)

distributional representations 

(see goldberg 10.4.1)

    words appear in a context

(try it yourself w/ kwic.py)

count-based methods

    create a word-context count matrix 

    count the number of co-occurrences of word/

context, with rows as word, columns as contexts 
    maybe weight with pointwise mutual information 
    maybe reduce dimensions using svd 

    measure their closeness using cosine similarity 

(or generalized jaccard similarity, others)

prediction-basd methods 

(see goldberg 10.4.2)

    instead, try to predict the words within a neural 

network 

    id27s are the byproduct

id27s from 

language models
a

giving

lookup

lookup

tanh(   
  w1*h + b1)

w

+
bias

=
scores

softmax

probs

context window methods

    if we don   t need to calculate the id203 of the 

sentence, other methods possible! 

    these can move closer to the contexts used in 

count-based methods 

    these drive id97, etc.

cbow 

(mikolov et al. 2013)

    predict word based on sum of surrounding embeddings

giving

a

***

at

the

lookup

lookup

lookup

lookup

+

+
=

w

+

softmax

=
scores

probs

talk

loss

let   s try it out!
wordemb-cbow.py

skip-gram 
(mikolov et al. 2013)

    predict each word in the context given the word

talk
lookup

w

=

giving

a

at

the

loss

let   s try it out!

wordemb-skipgram.py

other notes

    strong connection between count-based methods and 
prediction-based methods (levy and goldberg 2014) 

    skip-gram objective is equivalent to matrix 

factorization with pmi and discount for number of 
samples k (sampling covered next time)   

mw,c = pmi(w, c)   log(k)

    other estimation methods: glove (pennington et al. 

2014), etc.

   
what contexts?

    context has a large effect! 
    small context window: more syntax-based 

embeddings 

    large context window: more semantics-based, 

topical embeddings 

    context based on syntax: more functional, w/ 

words with same in   ection grouped

evaluating embeddings

types of evaluation

    intrinsic vs. extrinsic 

    intrinsic: how good is it based on its features? 
    extrinsic: how useful is it downstream? 

    qualitative vs. quantitative 

    qualitative: examine the characteristics of 

examples. 

    quantitative: calculate statistics

visualization of embeddings
    reduce high-dimensional embeddings into 2/3d 
for  visualization (e.g. mikolov et al. 2013)

non-linear projection

    non-linear projections group things that are close in high-

dimensional space 

    e.g. sne/id167 (van der maaten and hinton 2008) group things 
that give each other a high id203 according to a gaussian

pca

id167

(image credit: derksen 2016)

let   s try it out!

wordemb-vis-tsne.py

id167 visualization can be 
misleading! (wattenberg et al. 2016)

    settings matter 
       

    linear correlations cannot be interpreted

   
   
intrinsic evaluation of embeddings 

(categorization from schnabel et al 2015)

    relatedness: the correlation btw. embedding 
cosine similarity and human eval of similarity? 

    analogy: find x for    a is to b, as x is to y   . 
    categorization: create clusters based on the 
embeddings, and measure purity of clusters. 
    selectional preference: determine whether a 

noun is a typical argument of a verb.

extrinsic evaluation: 

using id27s in systems

    initialize w/ the embeddings 
    concatenate pre-trained embeddings with learned 

embeddings 

    latter has the potential to provide better 

generalization, but

how do i choose 

embeddings?

    no one-size-   ts-all embedding (schnabel et al 2015)

    be aware, and use the best one for the task

when are pre-trained 
embeddings useful?

    basically, when training data is insuf   cient 
    very useful: tagging, parsing, text classi   cation 
    less useful: machine translation 
    basically not useful: id38

improving embeddings

limitations of embeddings

    sensitive to super   cial differences (dog/dogs) 
    insensitive to context (   nancial bank, bank of a river) 
    not necessarily coordinated with knowledge or 

across languages 
    not interpretable
    can encode bias (encode stereotypical gender roles, 

racial biases)

sub-id27s (1)

    can capture sub-word regularities
morpheme-based 
(luong et al. 2013)

character-based 
(ling et al. 2015)

sub-id27s (2)
    bag of character id165s used to represent word 

(bojanowski et al. 2017)
where

<wh, whe, her, ere, re>

    use id165s from 3-6 plus word itself 

    used in the    fasttext    toolkit

multi-prototype embeddings

    simple idea, words with multiple meanings should have 

different embeddings (reisinger and mooney 2010)

    non-parametric estimation (neelakantan et al. 2014) also possible

multilingual coordination of 
embeddings (faruqui et al. 2014)

    we have id27s in two languages, and want them to match

retro   tting of embeddings 

to existing lexicons
    we have an existing lexicon like id138, and 

would like our vectors to match (faruqui et al. 2015)

sparse embeddings

    each dimension of a id27 is not interpretable 
    solution: add a sparsity constraint to increase the 

information content of non-zero dimensions for each word 
(e.g. murphy et al. 2012)

de-biasing word 

embeddings (bolukbasi et al. 2016)
    id27s re   ect bias in statistics

    identify pairs to    neutralize   ,    nd the direction of the trait to 
neutralize, and ensure that they are neutral in that direction

questions?

