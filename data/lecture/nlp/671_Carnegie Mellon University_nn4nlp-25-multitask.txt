cs11-747 neural networks for nlp 

multi-task, multi-lingual 

learning 

graham neubig

https://phontron.com/class/nn4nlp2017/

site

remember, neural nets are 

feature extractors!

    create a vector representation of sentences or 

words for use in downstream tasks

this is an example

this is an example

    in many cases, the same representation can be 
used in multiple tasks (e.g. id27s)

types of learning

    id72 is a general term for training on 

multiple tasks 

    id21 is a type of id72 
where we only really care about one of the tasks 
    id20 is a type of id21, 

where the output is the same, but we want to 
handle different topics or genres, etc.

when to multi-task?

plethora of tasks in nlp

    in nlp, there are a plethora of tasks, each requiring 

different varieties of data 
    only text: e.g. id38 
    naturally occurring data: e.g. machine 

translation 

    hand-labeled data: e.g. most analysis tasks 
    and each in many languages, many domains!

rule of thumb 1:   

multitask to increase data

    perform multi-tasking when one of your two tasks has 

many fewer data 

    general domain     speci   c domain   

(e.g. web text     medical text) 

    high-resourced language     low-resourced 

language   
(e.g. english     telugu) 
    plain text     labeled text   

(e.g. lm -> parser)

rule of thumb 2:

    perform multi-tasking when your tasks are related
    e.g. predicting eye gaze and summarization 

(klerke et al. 2016)

methods for multi-task 

learning

standard multi-task 

learning

    train representations to do well on multiple tasks at 

once

this is an example

encoder

translation

tagging

    in general, as simple as randomly choosing minibatch from one 

of multiple tasks 

    many many examples, starting with collobert and weston (2011)

pre-training

    first train on one task, then train on another

this is an example

encoder

translation

initialize

this is an example

encoder

tagging

    widely used in id27s (turian et al. 2010) 

    also pre-training sentence representations (dai et al. 

2015)

examples of pre-training 

encoders

    common to pre-train encoders for downstream 

tasks, common to use: 

    language models (dai and le 2015) 
    translation models (mccann et al. 2017) 
    bidirectional language models (peters et al. 

2017)

id173 for pre-training   

(e.g. barone et al. 2017)

    pre-training relies on the fact that we won   t move too far from the 

initialized values 

    we need some form of id173 to ensure this 

    early stopping: implicit id173     stop when the 

model starts to over   t 

    explicit id173: l2 on difference from initial 

parameters   

   adapt =    pre +    dif f

  log p (y | x;    adapt) + ||   dif f||

`(   adapt) = xhx,y i2hx ,yi

    dropout: also implicit id173, works pretty well

   
selective parameter 

adaptation

    sometimes it is better to adapt only some of the parameters 
    e.g. in cross-lingual transfer for neural mt, zoph et al. 

(2016) examine best parameters to adapt

soft parameter tying

    it is also possible to share parameters loosely between 

various tasks 

    parameters are regularized to be closer, but not tied in a 

hard fashion (e.g. duong et al. 2015)

id20

id20

    basically one task, but incoming data could be 

from very different distributions
news text
medical text
spoken 
language

encoder

translation

    often have big grab-bag of all domains, and want to 

tailor to a speci   c domain 

    two settings: supervised and unsupervised

supervised/unsupervised 

adaptation

    supervised adaptation: have data in target domain

    simple pre-training on all data, tailoring to 
domain-speci   c data (luong et al. 2015) 

    learning domain-speci   c networks/features 
    unsupervised adaptations: no data in target 

domain

    matching distributions over features

supervised id20 
through feature augmentation
    e.g. train general-domain and domain-speci   c feature 

extractors, then sum their results (kim et al. 2016)

    append a domain tag to input (chu et al. 2016)

<news> news text
<med> medical text

unsupervised learning 
through feature matching
    adapt the latter layers of the network to match 
labeled and unlabeled data using multi-kernel 
mean maximum discrepancy (long et al. 2015)   

    similarly, adversarial nets (ganin et al. 2016)

   
   
   
   
   
multi-lingual models

multilingual inputs

    often as simple as training a single (large) encoder  
    optionally: use adversarial objective to help ensure that 

information is shared (chen et al. 2016)

    quite successful in a number of tasks

multilingual id170/   

multilingual outputs

    things are harder when predicting a sequence of 

actions (parsing) or words (mt) in different languages 

    one simple method: add embedding of the expected 

output to your model (e.g. tsvetkov et al. 2016)

multi-lingual sequence-to-

sequence models

    it is possible to translate into several languages by 
adding a tag about the target language (johnson 
et al. 2016, ha et al. 2016)
<fr> this is an example      ceci est un exemple
<ja> this is an example                          
    potential to allow for    zero-shot    learning:   
train on fr(cid:15511)en and ja(cid:15511)en, and use on fr(cid:15511)ja 
    works, but not as effective as translating 

fr   en   ja

teacher-student networks for 

multilingual adaptation (chen et al. 2017)
    use a better pivoted model to    teach    a worse 

zero-shot model to translate well

multi-task models

types of multi-tasking

    most common: train on plain text or translated text, 

use information for syntactic analysis task 

    also, training on multiple annotation tasks 

    other examples: 

    training with multiple annotation standards 

    training w/ different layers for different tasks

multiple annotation 

standards

    for analysis tasks, it is 

possible to have different 
annotation standards 

    solution: train models that 

adjust to annotation 
standards for tasks such as 
id29 (peng et al. 
2017), id40 () 

    we can even adapt to 

individual annotators! (guan 
et al. 2017)

different layers for different 

tasks (hashimoto et al. 2017)

    depending on the 
complexity of the 
task we might need 
deeper layers 

    choose the layers 

to use based on the 
level of semantics 
required

questions?

