cs11-747 neural networks for nlp  
advanced search 

algorithms

daniel clothiaux 

https://phontron.com/class/nn4nlp2017/

why search?

    so far, decoding has mostly been greedy 

    chose the most likely output from softmax, repeat 

    can we    nd a better solution? 

    oftentimes, yes!

basic search algorithms

id125

    instead of picking the highest id203/score, 

maintain multiple paths 

    at each time step 

    expand each path 

    choose top n paths from the expanded set

why will this help

next word

p(next word)

pittsburgh

new york

new jersey

other

0.4

0.3

0.25

0.05

potential problems

    unbalanced action sets 

    larger beam sizes may be signi   cantly slower 

    lack of diversity in beam 

    outputs of variable length 

    will not always improve evaluation metric

dealing with disparity in actions 

effective id136 for generative neural parsing 

(mitchell stern et al., 2017)

    in generative parsing there are shifts (or 
generates) equal to the vocabulary size  

    opens equal to # of labels

solution

    group sequences of actions of the same length 

taken after the ith  shift. 

    create buckets based off of the number of shifts 

and actions after the shift 

    fast tracking: 

    to further reduce comparison bias, certain shifts 

are immediately added to the next bucket

pruning

    expanding each path with large beams is slow 

    pruning the search tree speeds things up 

    remove paths from the tree 

    predict what paths to expand

threshold based pruning  

   google   s id4 system: bridging the gap 
between human and machine translation     (y wu et al. 2016)

    compare the path score with best path score 

    compare expanded node score with best node 

    if either falls beneath threshold, drop them

predict what nodes to 

expand

    effective id136 for generative neural parsing (stern et 

al., 2017): 
    a simple feed forward network predicts actions to prune 
    this works well in parsing, as most of the possible 
actions are open, vs. a few closes and one shift 

    transition-based id33 with heuristic 

backtracking 
    early cutoff based off of single id200

backtrack to points most likely to be wrong 
transition-based id33 with heuristic backtracking (buckman et al, 

2016)

improving diversity in top n choices 
mutual information and diverse decoding improve neural machine 

translation (li et al., 2016)
    entries in the beam can be very similar 
    improving the diversity of the top n list can help 
    score using source->target and target-> source translation 

models, language model

improving diversity through sampling 

generating high-quality and informative conversation responses 

with sequence-to-sequence models (shao et al., 2017)

    stochastically sampling from the softmax gives 

great diversity! 

    unlike in translation, the distributions in 

conversation are less peaky 

    this makes sampling reasonable

variable length output 

sequences

    in many tasks (eg. mt), the output sequences will be of 

variable length 

    running id125 may then favor short sentences 
    simple idea: 

    normalize by the length-divide by |n| 

    on the properties of id4: 

encoder   decoder (cho et al., 2014) 

    can we do better?

more complicated id172 

   google   s id4 system: bridging the gap 
between human and machine translation     (y wu et al. 2016)

    x,y: source, target sentence 

      : 0 <    < 1, normally in [0.6, 0.7] 

      : coverage penalty 
    this is found empirically

predict the output length 

tree-to-sequence attentional id4 

(eriguchi et al. 2016)

    add a penalty based off of length differences 

between sentences 

    calculate p(len(y) | len(x)) using corpus statistics

what beam size should i 

use?

    larger beam sizes will be slower, and may not give 

better results 

    mostly done empirically-experiment! 

    many papers use less than 15, but i   ve seen as 

high as 1000

id125-bene   ts and 

drawbacks

    bene   ts: 

    generally easy to implement off of an existing model 
    guaranteed to not decrease model score 

    otherwise, something   s wrong 

    drawbacks 

    larger beam sizes may be signi   cantly slower 

    will not always improve evaluation metric 
    depending on how complicated you want to get, there will be a few 

more hyper-parameters to tune

using id125 in training 

sequence-to-sequence learning 

as beam-search optimization (wiseman et al., 2016)

    decoding with id125 has biases 

    exposure: model not exposed to errors during training 

    label:  scores are locally normalized 

    possible solution: train with id125

more id125 in training 

a continuous relaxation of id125 for end-to-end 
training of neural sequence models (goyal et al., 2017)

a* algorithms

id67

    basic idea: 

    iteratively expand paths that have the cheapest 

total cost along the path 

    total cost = cost to current point + estimated cost 

to goal

    f(n) = g(n) + h(n) 

    g(n): cost to current point 

    h(n): estimated cost to goal 

    h should be admissible and consistent

classical a* parsing 

a* parsing: fast exact viterbi parse selection (klein et 

    pid18 based parser 

al., 2003)

    inside (g) and outside (h) scores are maintained 

    inside: cost of building this constituent  

    outside: cost of integrating constituent with rest of tree

adoption with neural networks:  

id35 parsing 

lstm id35 parsing (lewis et al. 2014)

id35 parsing:

    a* for parsing 

    g(n): sum of encoded lstm scores over current  

span 

    h(n): sum of maximum encoded scores for each 

constituent outside of current span

is the heuristic admissible? 

global neural id35 parsing with optimality guarantees 

(lee et al. 2016)

    no! 
    fix this by adding a global model score < 0 to the elements outside of the current 

span 
    this makes the estimated cost lower than the actual cost 

    global model: tree lstm over completed parse 

    this is signi   cantly slower than the embedding lstm, so    rst evaluate g(n), 

then lazily expand good scores

estimating future costs 
learning to decode for future success (li et al., 2017)

id67: bene   ts and 

drawbacks

    bene   ts: 

    with heuristic, has nice optimality guarantees 

    strong results in id35 parsing 

    drawbacks: 

    needs more construction than id125, can   t 

easily throw on existing model 

    requires a good heuristic for optimality guarantees

other search 
algorithms

id143s 

a bayesian model for generative transition-based 

id33 (buys et al., 2015)

    similar to id125 

    think of it as id125 with a width that depends on 

certainty of it   s paths 
    more certain, smaller, less certain, wider 

    there are k total particles 
    divide particles among paths based off of id203 of 

paths, dropping any path that would get <1 particle 

    compare after the same number of shifts

reranking 

recurrent neural network grammars 

(dyer et al. 2016)

    if you have multiple different models, using one to rerank outputs can 

improve performance 

    classically: use a target language language model to rerank the best 

outputs from an mt system 

    going back to the generative parsing problem, directly decoding from a 

generative model is dif   cult 

    however, if you have both a generative model b and a discriminative 

model a 

    decode with a then rerank with b 
    results are superior to decoding then reranking with a separately 

trained b

monte-carlo tree search 

human-like id86 using 

id169

