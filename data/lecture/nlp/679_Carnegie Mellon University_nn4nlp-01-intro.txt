cs11-747 neural networks for nlp 

intro/   

why neural nets for nlp?

graham neubig

https://phontron.com/class/nn4nlp2017/

site

language is hard!

are these sentences ok?

    jane went to the store. 

    store to jane went the. 

    jane went store. 

    jane goed to the store. 

    the store went to jane. 

    the food truck went to jane.

engineering solutions

    jane went to the store. 

    store to jane went the. 

    jane went store. 

    jane goed to the store. 

    the store went to jane. 

    the food truck went to jane.

} create a grammar of 
the language
} consider 
morphology and exceptions
} semantic categories, 
preferences
} and their exceptions

are these sentences ok?
                                      

                                      

                                   

                                      

                                                    

phenomena to handle

    morphology 

    syntax 

    semantics/world knowledge 

    discourse 

    pragmatics 

    multilinguality

neural networks: 

a tool for doing hard things

an example prediction problem: 

sentence classi   cation
i   hate   this  movie

very good 

good 
neutral 
bad 

very bad

i   love   this   movie

very good 

good 
neutral 
bad 

very bad

a first try: 

bag of words (bow)
hate

this movie

i

lookup

lookup

lookup

lookup

bias

scores

+

+

+

+

=

probs

softmax

build it, break it

i don   t love this movie

there   s nothing i don   t 
love about this movie

very good 

good 
neutral 
bad 

very bad

very good 

good 
neutral 
bad 

very bad

https://bibinlp.umiacs.umd.edu

combination features

    does it contain    don   t    and    love   ? 

    does it contain    don   t   ,    i   ,    love   , and    nothing   ? 

basic idea of neural networks 

(for nlp prediction tasks)
i

this movie

hate

lookup

lookup

lookup

lookup

some complicated 
function to extract 
combination features 

(neural net)

scores

probs

softmax

computation graphs 

the lingua franca of neural nets

expression:
y = x>ax + b    x + c
graph:

a node is a {tensor, matrix, vector, scalar} value

x

an edge represents a function argument   
expression:
(and also an data dependency). they are just   
y = x>ax + b    x + c
pointers to nodes.
a node with an incoming edge is a function of 
graph:
that edge   s tail node.
a node knows how to compute its value and the 
value of its derivative w.r.t each argument (edge) 
times a derivative of an arbitrary input       .
@f
@f (u)

f (u) = u>

@f (u)

@u

@f
@f (u)

=    @f

@f (u)   >

x

expression:
y = x>ax + b    x + c
graph:

functions can be nullary, unary,   
binary,     n-ary. often they are unary or binary.

f (u, v) = uv

f (u) = u>

a

x

expression:
y = x>ax + b    x + c
graph:

f (m, v) = mv

f (u, v) = uv

f (u) = u>

a

x

computation graphs are directed and acyclic (in dynet)

expression:
y = x>ax + b    x + c
graph:

f (m, v) = mv

f (x, a) = x>ax

f (u, v) = uv

f (u) = u>

a

x

x

a

@f (x, a)

@x

@f (x, a)

@a

= (a> + a)x

= xx>

expression:
y = x>ax + b    x + c
graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

expression:
y = x>ax + b    x + c
graph:

f (x1, x2, x3) =xi

y

xi

f (m, v) = mv

f (u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

variable names are just labelings of nodes.

algorithms (1)

    graph construction
    forward propagation

    in topological order, compute the value of the 

node given its inputs

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv

f (u) = u>

f (u, v) = u    v

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv

f (u) = u>
x>

a

f (u, v) = u    v

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv
x>a

f (u) = u>
x>

a

f (u, v) = u    v

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv

f (u, v) = uv
x>a

f (u) = u>
x>

f (u, v) = u    v

b    x

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi

xi

f (m, v) = mv
x>ax

f (u, v) = uv
x>a

f (u) = u>
x>

f (u, v) = u    v

b    x

a

x

b

c

forward propagation

graph:

f (x1, x2, x3) =xi
xi
x>ax + b    x + c

f (m, v) = mv
x>ax

f (u, v) = uv
x>a

f (u) = u>
x>

f (u, v) = u    v

b    x

a

x

b

c

algorithms (2)

    back-propagation:

    process examples in reverse topological order 

    calculate the derivatives of the parameters with respect to 

the    nal value   
(this is usually a    id168   , a value we want to minimize) 

    parameter update:

    move the parameters in the direction of this derivative   

w -=    * dl/dw

   
a concrete example

neural network frameworks
dynamic frameworks 
static frameworks
(recommended!)

+gluon

+fold

basic process in dynamic 
neural network frameworks
    create a model 

    for each example 

    create a graph that represents the computation 

you want 

    calculate the result of that computation 
    if training, perform back propagation and 

update

dynet

    examples in this class will be in dynet: 

    intuitive, program like you think (c.f. tensorflow, 

theano) 

    fast for complicated networks on cpu (c.f. 

autodiff libraries, chainer, pytorch) 
    has nice features to make ef   cient 

implementation easier (automatic batching)

computation graph 
and expressions

import dynet as dy 
dy.renew_cg() # create a new computation graph 
v1 = dy.inputvector([1,2,3,4]) 
v2 = dy.inputvector([5,6,7,8]) 
# v1 and v2 are expressions 
v3 = v1 + v2 
v4 = v3 * 2 
v5 = v1 + 1 
v6 = dy.concatenate([v1,v2,v3,v5]) 
print v6            
print v6.npvalue() 

computation graph 
and expressions

import dynet as dy 
dy.renew_cg() # create a new computation graph 
v1 = dy.inputvector([1,2,3,4]) 
v2 = dy.inputvector([5,6,7,8]) 
# v1 and v2 are expressions 
v3 = v1 + v2 
v4 = v3 * 2 
v5 = v1 + 1 
v6 = dy.concatenate([v1,v2,v3,v5]) 
print v6            
print v6.npvalue() 

expression 5/1

computation graph 
and expressions

import dynet as dy 
dy.renew_cg() # create a new computation graph 
v1 = dy.inputvector([1,2,3,4]) 
v2 = dy.inputvector([5,6,7,8]) 
# v1 and v2 are expressions 
v3 = v1 + v2 
v4 = v3 * 2 
v5 = v1 + 1 
v6 = dy.concatenate([v1,v2,v3,v5]) 
print v6            
print v6.npvalue() 

array([  1.,   2.,   3.,   4.,   2.,   4.,   6.,   8.,   4.,   8.,  12.,  16.])

computation graph 
and expressions

    create basic expressions. 
    combine them using operations. 
    expressions represent symbolic computations. 
    use:   
.value()   
.npvalue()    
.scalar_value()   
.vec_value()   
.forward()    
           to perform actual computation.

model and parameters

    parameters are the things that we optimize over 

(vectors, matrices). 

    model is a collection of parameters. 
    parameters out-live the computation graph.

model and parameters

model = dy.model() 
pw = model.add_parameters((20,4)) 
pb = model.add_parameters(20) 

dy.renew_cg() 
x = dy.inputvector([1,2,3,4]) 
w = dy.parameter(pw) # convert params to expression 
b = dy.parameter(pb) # and add to the graph 
y = w * x + b 

parameter initialization

model = dy.model() 
pw = model.add_parameters((4,4)) 
pw2 = model.add_parameters((4,4), init=dy.glorotinitializer()) 
pw3 = model.add_parameters((4,4), init=dy.normalinitializer(0,1)) 
pw4 = model.parameters_from_numpu(np.eye(4)) 

trainers and backdrop

    initialize a trainer with a given model. 
    compute gradients by calling expr.backward() 

from a scalar node. 

    call trainer.update() to update the model 

parameters using the gradients.

trainers and backdrop

model = dy.model() 
trainer = dy.simplesgdtrainer(model) 
p_v = model.add_parameters(10) 
for i in xrange(10): 
    dy.renew_cg() 
    v = dy.parameter(p_v) 
    v2 = dy.dot_product(v,v) 
    v2.forward()  
    v2.backward()  # compute gradients 
    trainer.update()

trainers and backdrop

  dy.simplesgdtrainer(model,...) 
  dy.momentumsgdtrainer(model,...) 
  dy.adagradtrainer(model,...) 
  dy.adadeltatrainer(model,...) 
  dy.adamtrainer(model,...) 

model = dy.model() 
trainer = dy.simplesgdtrainer(model) 
p_v = model.add_parameters(10) 
for i in xrange(10): 
    dy.renew_cg() 
    v = dy.parameter(p_v) 
    v2 = dy.dot_product(v,v) 
    v2.forward()  
    v2.backward()  # compute gradients 
    trainer.update()

training with dynet

    create model, add parameters, create trainer. 

    for each training example: 

    create computation graph for the loss 

    run forward (compute the loss) 

    run backward (compute the gradients) 

    update parameters

example implementation 

(in dynet)

bag of words (bow)

i

hate

this movie

lookup

lookup

lookup

lookup

bias

scores

+

+

+

+

=

probs

softmax

continuous bag of words 

(cbow)
this movie

i

hate

lookup

lookup

lookup

lookup

+
=

+

w

+

+

=
scores

bias

deep cbow

i

hate

this movie

+

+
=

+

tanh(   
  w1*h + b1)

tanh(   
  w2*h + b2)

w

+

=
scores

bias

class format/structure

class format

     reading: before the class 
     quiz: simple questions about the required reading 

(should be easy) 

     summary/elaboration/questions: instructor or 

tas will summarize the material, elaborate on 
details, and    eld questions 

     code walk: the tas (or instructor) will walk 

through some demonstration code

assignments

    course is group (2-3) assignment/project based 
    assignment 1: survey the    eld and implement a 

baseline model 

    assignment 2: re-implement and reproduce 

results from a state-of-the-art model 

    project: perform a unique research project that 

either (1) improves on state-of-the-art, or (2) 
applies neural net models to a unique task

instructors/of   ce hours

    instructor: graham neubig   

                   (mon., 4:00-5:00pm ghc5409) 

    tas:

    zhengzhong (hector) liu (mon. 1:00-2:00pm, 

ghc5517) 

    xuezhe (max) ma (tue. 12:00-1:00pm, ghc5517) 
    daniel clothiaux (fri. 9:00-10:00am, ghc5505) 

    piazza: http://piazza.com/cmu/fall2017/cs11747/home

class plan

section 1:   

models of words

undeserved

nn

    word representations using context 

    word representations using word form 

    speed tricks for neural networks

section 2:   

models of sentences
this movie   s reputation is
undeserved

nn

    bag of words, bag of id165s, convolutional nets 

    recurrent neural networks and variations 

    applications of sentence modeling

sec.3: sequence-to-sequence models

hate

this

movie

</s>

i

lstm

lstm

lstm

lstm

lstm

      

      

   

      

lstm

lstm

lstm

lstm

argmax
      

argmax
      

argmax
   

argmax
      

argmax
</s>

    encoder decoder models 

    attentional models

section 4:   

id170 models

hate

this

movie

i

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

prp

vb

dt

nn

    structured id88, structured max margin 

    conditional random    elds

section 5:   

models of tree structure

hate

this

movie

i

id56

id56

id56

    shift reduce, minimum spanning tree parsing 

    tree structured compositions 

    models of graph structures

section 6:   

advanced learning techniques

    variational auto-encoders 

    adversarial networks 

    marginal likelihood, id23 

    semi-supervised and unsupervised learning

section 7:   

neural networks and knowledge

animal

is-a

dog

is-a
cat

    learning from/for id208 

    interfacing with id208 

    machine reading models 

    reasoning with neural nets

section 8:   

multi-task and multilingual learning

i  hate   this   movie

                        
prp  vb   dt   nn

    id72 models 

    multilingual learning of representations 

    universal analysis models

section 9:   

advanced search techniques

    id125 and its variants 

    id67

any questions?

