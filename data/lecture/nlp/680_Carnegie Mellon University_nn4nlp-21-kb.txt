cs11-747 neural networks for nlp 

learning from/for 
knowledge bases

graham neubig

https://phontron.com/class/nn4nlp2017/

site

knowledge bases

    structured databases of knowledge usually 

containing 

    entities (nodes in a graph) 

    relations (edges between nodes) 

    how can we learn to create/expand knowledge 

bases with neural networks? 

    how can we learn from the information in knowledge 

bases to improve neural representations?

types of knowledge bases

id138 (miller 1995)

    id138 is a large database of words including 

parts of speech, semantic relations

    nouns: is-a relation (hatch-back/car), part-of (wheel/car), type/instance distinction 
    verb relations: ordered by speci   city (communicate -> talk -> whisper) 
    adjective relations: antonymy (wet/dry)

image credit: nltk

cyc (lenant 1995)

    a manually curated database attempting to encode 
all common sense knowledge, 30 years in the making

image credit: nltk

dbpedia (auer et al. 2007)

    extraction of structured data from wikipedia

structured data

yago (suchanek et al. 2007)

    a meta-knowledge base, combining information 

from multiple sources (e.g. wikipedia and 
id138) 

    expansions to include temporal/spatial information

babelnet   

(navigli and ponzetto 2008)

    like yago, meta-database including various 
sources such as id138 and wikipedia, but 
augmented with multi-lingual information

freebase (bollacker et al. 2008)

    curated database of entities, linked, and extremely 

large scale

wikidata   

(vrande  i   and kr  tzsch 2014)

    knowledge base run by wikimedia foundation and 

successor to freebase 

    incorporates many of the good points of previous 

work: multilingual, automatically extracted + 
curated, sparql interface

learning relations from 

embeddings

knowledge base 
incompleteness

    even w/ extremely large scale, knowledge bases 

are by nature incomplete 

    e.g. in freebase 71% of humans were missing 

   date of birth    (west et al. 2014) 

    can we perform    id36    to extract 

information for knowledge bases?

remember: consistency in 

embeddings

    e.g. king-man+woman = queen (mikolov et al. 

2013)

id36 w/ neural 
tensor networks (socher et al. 2013)
    a    rst attempt at predicting relations: a multi-layer 
id88 that predicts whether a relation exists

    neural tensor network: adds bi-linear feature 
extractors, equivalent to projections in space

    powerful model, but perhaps overparameterized!

learning relations from 
embeddings (bordes et al. 2013)

    try to learn a transformation vector that shifts word 

embeddings based on their relation 

    optimize these vectors to minimize a margin-based loss

    note: one vector for each relation, additive 

modi   cation only, intentionally simpler than ntn

id36 w/ hyperplane 

translation (wang et al. 2014)

    motivation: it is not realistic to assume that all dimensions are 

relevant to a particular relation 

    solution: project the word vectors on a hyperplane speci   cally for 

that relation, then verify relation

    also, transr (lin et al. 2015), which uses full matrix projection

decomposable relation 

model (xie et al. 2017)

    idea: there are many relations, but each can be 
represented by a limited number of    concepts    
    method: treat each relation map as a mixture of 

concepts, with sparse mixture vector   

    better results, and also somewhat interpretable relations

learning from text directly

distant supervision for 

id36 (mintz et al. 2009)
    given an entity-relation-entity triple, extract all text 
that matches this and use it to train

    creates a large corpus of (noisily) labeled text to 

train a system

relation classi   cation w/ 
recursive nns (socher et al. 2012)
    create a syntax tree and do tree-structured encoding 

    classify the relation using the representation of the 

minimal constituent containing both words

relation classi   cation w/ 

id98s (zeng et al. 2014)

    extract features w/o syntax using id98 

    lexical features of the words themselves 

    features of the whole span extracted using convolution

jointly modeling kb relations 

and text (toutanova et al. 2015)

    to model textual links between words w/ neural net: 

aggregate over multiple instances of links in dependency tree

    model relations w/ id98

modeling distant supervision 
noise in neural models (luo et al. 2017)
    idea: there is noise in distant supervision labels, so we 

want to model it

    by controlling the    transition matrix   , we can adjust to the 

amount of noise expected in the data 
    trace id172 to try to make matrix close to identity 

    start training w/ no transition matrix on data expected to 

be clean, then phase in on full data

learning from relations 

themselves

modeling id27s 

vs. modeling relations

    id27s give information of the word in 

context, which is indicative of kb traits 

    however, other relations (or combinations thereof) 

are also indicative

tensor decomposition 

(sutskever et al. 2009)

    can model relations by decomposing a tensor 

containing entity/relation/entity tuples

modeling relation paths   

(lao and cohen 2010)

    multi-step paths can be informative for indicating 

individual relations 

    e.g.    given word, recommend venue in which to 

publish the paper   

optimizing relation embeddings 

over paths (guu et al. 2015)

    traveling over relations might result in error propagation 
    simple idea: optimize so that after traveling along a path, 

we still get the correct entity

differentiable logic rules 

(yang et al. 2017)

    consider whole paths in a differentiable framework

    treat path as a sequence of matrix multiplies, 

where the rule weight is   

using knowledge bases to 

inform embeddings

lexicon-aware learning of word 
embeddings (e.g. yu and dredze 2014)
    incorporate knowledge in the training objective for 

id27s 

    similar words should be in close places in the space

retro   tting of embeddings to 
existing lexicons (faruqui et al. 2015)

    similar to joint learning, but done through post-hoc 

transformation of embeddings 

    advantage of being usable with any pre-trained embeddings 

    double objective of making transformed embeddings close to 

neighbors, and close to original embedding

    can also force antonyms away from each-other (mrksic et al. 2016)

multi-sense embedding w/ 

lexicons (jauhar et al. 2015)

    create model with latent sense 

    sense can be optimized using em or hard em 

(select the most probable)

questions?

