cs11-747 neural networks for nlp 

attention

graham neubig

https://phontron.com/class/nn4nlp2017/

site

encoder-decoder models 

(sutskever et al. 2014)

encoder

kono

eiga

ga

kirai

</s>

lstm

lstm

lstm

lstm

lstm

i

hate

this

movie

lstm

lstm

lstm

lstm

argmax
hate

argmax

i
decoder

argmax
this

argmax
movie

argmax
</s>

sentence representations

problem!

   you can   t cram the meaning of a whole %&!$ing 

sentence into a single $&!*ing vector!    

    ray mooney

    but what if we could use multiple vectors, based on 

the length of the sentence.

this is an example

this is an example

attention

basic idea 
(bahdanau et al. 2015)

    encode each word in the sentence into a vector 

    when decoding, perform a linear combination of 

these vectors, weighted by    attention weights    

    use this combination in picking the next word

calculating attention (1)

    use    query    vector (decoder state) and    key    vectors (all encoder states) 
    for each query-key pair, calculate weight 
    normalize to add to one using softmax

kono

eiga

ga

kirai

key 
vectors

i hate

a1=2.1 a2=-0.1 a3=0.3

a4=-1.0

query vector

softmax

  1=0.76   2=0.08   3=0.13   4=0.03

calculating attention (2)

    combine together value vectors (usually encoder 
states, like key vectors) by taking the weighted sum

kono

eiga

ga

kirai

value 
vectors

*

*

*

*

  1=0.76   2=0.08   3=0.13   4=0.03

    use this in any part of the model you like

a graphical example

attention score functions (1)
    q is the query and k is the key 
    multi-layer id88 (bahdanau et al. 2015)   

a(q, k) = w|

2 tanh(w1[q; k])

    flexible, often very good with large data 

    bilinear (luong et al. 2015)

a(q, k) = q|w k

   
attention score functions (2)
    dot product (luong et al. 2015)   

a(q, k) = q|k

    no parameters! but requires sizes to be the same. 

    scaled dot product (vaswani et al. 2017) 

    problem: scale of dot product increases as dimensions get 

larger 

    fix: scale by size of the vector

a(q, k) =

q|k

p|k|

   
let   s try it out! 
attention.py

what do we attend to?

input sentence

    like the previous explanation 
    but also, more directly 

    copying mechanism (gu et al. 2016)   

    lexicon bias (arthur et al. 2016)

   
   
   
   
   
   
   
   
previously generated things

    in id38, attend to the previous words (merity 

et al. 2016)   

    in translation, attend to either input or previous output 

(vaswani et al. 2017)

   
   
   
   
   
   
   
various modalities

    images (xu et al. 2015)   

    speech (chan et al. 2015)

   
   
   
   
   
hierarchical structures 

(yang et al. 2016)

    encode with 

attention over each 
sentence, then 
attention over each 
sentence in the 
document

multiple sources

    attend to multiple sentences (zoph et al. 2015)   

    libovicky and helcl (2017) compare multiple strategies 

    attend to a sentence and an image (huang et al. 2016)

   
   
   
intra-attention / self attention 

(cheng et al. 2016)

    each element in the sentence attends to other 

elements     context sensitive encodings!

this

is

an example

this
is
an

example

improvements to attention

coverage

    problem: neural models tends to drop or repeat 

content 

    solution: model how many times words have been 

covered 

    impose a penalty if attention not approx. 1 (cohn 

et al. 2015) 

    add embeddings indicating coverage (mi et al. 

2016)

incorporating markov properties   

(cohn et al. 2015)

    intuition: attention from last time tends to be 

correlated with attention this time   

    add information about the last attention when 

making the next decision

   
   
   
   
   
   
bidirectional training 

(cohn et al. 2015)

    intuition: our attention should be 

roughly similar in forward and 
backward directions 

    method: train so that we get a bonus 

based on the trace of the matrix 
product for training in both directions

tr(ax!y a|

y !x )

supervised training 

(mi et al. 2016)

    sometimes we can get    gold standard    alignments 

a-priori 

    manual alignments 

    pre-trained with strong alignment model 

    train the model to match these strong alignments

attention is not alignment! 

(koehn and knowles 2017)

    attention is often 

blurred 

    attention is often off 

by one

specialized attention 

varieties

hard attention

    instead of a soft interpolation, make a zero-one decision about 

where to attend (xu et al. 2015) 

    harder to train, requires methods such as reinforcement 

learning (see later classes) 

    perhaps this helps interpretability? (lei et al. 2016)

monotonic attention 

(e.g. yu et al. 2016)

    in some cases, we might know the output will be the same order as the input 

    id103, incremental translation, morphological in   ection (?), 

summarization (?)

    basic idea: hard decisions about whether to read more 

convolutional attention 

(allamanis et al. 2016)

    intuition: we might want to be able to attend to    the word 

after    mr.      , etc.

multi-headed attention

    idea: multiple attention    heads    focus on different parts of the sentence
    e.g. different 

heads for    copy    vs 
regular (allamanis 
et al. 2016)

    or multiple 

independently learned 
heads (vaswani et al. 
2017)

an interesting case study: 
   attention is all you need    

(vaswani et al. 2017)

summary of the 
   transformer" 
(vaswani et al. 2017)

    a sequence-to-

sequence model based 
entirely on attention 

    strong results on 

standard wmt datasets 

    fast: only matrix 

multiplications

attention tricks

    self attention: each layer combines words with 

others 

    multi-headed attention: 8 attention heads learned 

independently 

    normalized dot-product attention: remove bias 

in dot product when using large networks 

    positional encodings: make sure that even if we 

don   t have id56, can still distinguish positions

training tricks

    layer id172: help ensure that layers 

remain in reasonable range 

    specialized training schedule: adjust default 

learning rate of the adam optimizer 

    label smoothing: insert some uncertainty in the 

training process 

    masking for ef   cient training

masking for training

    we want to perform training in as few operations as 

possible using big matrix multiplies 

    we can do so by    masking    the results for the output
i hate this movie </s>

kono eiga ga kirai

questions?

