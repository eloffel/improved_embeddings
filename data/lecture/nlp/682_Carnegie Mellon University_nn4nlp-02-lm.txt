cs11-747 neural networks for nlp 

a simple (?) exercise:   
predicting the next word

graham neubig

https://phontron.com/class/nn4nlp2017/

site

are these sentences ok?

    jane went to the store. 

    store to jane went the. 

    jane went store. 

    jane goed to the store. 

    the store went to jane. 

    the food truck went to jane.

calculating the id203 of 

a sentence
iyi=1

p (xi | x1, . . . , xi 1)
next word

context

p (x) =

the big problem: how do we predict

p (xi | x1, . . . , xi 1)

?!?!

review: count-based 
language models

count-based language 

models

    count up the frequency and divide:

pm l(xi | xi n+1, . . . , xi 1) :=
    add smoothing, to deal with zero counts:

c(xi n+1, . . . , xi)
c(xi n+1, . . . , xi 1)

p (xi | xi n+1, . . . , xi 1) = pm l(xi | xi n+1, . . . , xi 1)

+ (1    )p (xi | x1 n+2, . . . , xi 1)

    modi   ed kneser-ney smoothing

a refresher on evaluation
    log-likelihood:   

log p (e)

ll(etest) = xe2etest

    per-word log likelihood:   
1

w ll(etest) =

    per-word (cross) id178:   

h(etest) =

    perplexity:   

pe2etest |e| xe2etest
pe2etest |e| xe2etest

1

log p (e)

  log2 p (e)

ppl(etest) = 2h(etest) = e w ll(etest)

what can we do w/ lms?

    score sentences:

jane went to the store .     high 
store to jane went the .     low

(same as calculating loss for training)

    generate sentences:
while didn   t choose end-of-sentence symbol: 
   calculate id203 
   sample a new word from the id203 distribution

problems and solutions?

    cannot share strength among similar words
she bought a car
she bought a bicycle
she purchased a car
    solution: class based language models

she purchased a bicycle

    cannot condition on context with intervening words

dr. jane smith

dr. gertrude smith
    solution: skip-gram language models
    cannot handle long-distance dependencies
for tennis class he wanted to buy his own racquet

for programming class he wanted to buy his own computer
    solution: cache, trigger, topic, syntactic models, etc.

an alternative:   

featurized id148

an alternative:   
featurized models

    calculate features of the context 

    based on the features, calculate probabilities 

    optimize feature weights using id119, 

etc.

example:

previous words:    giving a"
-6.0 
a 
-5.1 
the 
0.2 
talk 
gift 
0.1 
0.5 
hat 
   
   

3.0 
2.5 
-0.2 
0.1 
1.2 
   

w1,a=

b=

w2,giving=

-0.2 
-0.3 
1.0 
2.0 
-1.2 
   

s=

words we   re 
predicting

how likely 
are they?

how likely 
are they 
given prev. 
word is    a   ?

how likely 
are they 

given 2nd prev. 
word is    giving   ?

-3.2 
-2.9 
1.0 
2.2 
0.6 
   

total 
score

softmax

    convert scores into probabilities by taking the 

exponent and normalizing (softmax)
es(xi|xi 1

i n+1)
es(  xi|xi 1

i n+1)

p (xi | xi 1
i n+1) =
-3.2 
-2.9 
1.0 
2.2 
0.6 
   

s=

p  xi

p=

0.002 
0.003 
0.329 
0.444 
0.090 
   

a computation graph view

giving

a

lookup2

lookup1

bias

scores

+

+

=

probs

softmax

each vector is size of output vocabulary

a note:    lookup   

    lookup can be viewed as    grabbing    a single 
vector from a big matrix of id27s

num. words

vector 
size

lookup(2)

    similarly, can be viewed as multiplying by a    one-

hot    vector

num. words

vector 
size

0 
0 
1
0 
0 
   
    former tends to be faster

*

training a model

    reminder: to train, we calculate a    loss 

function    (a measure of how bad our predictions 
are), and move the parameters to reduce the loss 

    the most common id168 for probabilistic 

models is    negative log likelihood   

if element 3  

(or zero-indexed, 2) 
is the correct answer:

p=

0.002 
0.003 
0.329 
0.444 
0.090 
   

-log

1.112

parameter update

    back propagation allows us to calculate the 

derivative of the loss with respect to the parameters

@`
@   

    simple stochastic id119 optimizes 

parameters according to the following rule

               

@`
@   

choosing a vocabulary

unknown words

    necessity for unk words 

    we won   t have all the words in the world in training data 
    larger vocabularies require more memory and 

computation time 

    common ways: 

    frequency threshold (usually unk <= 1) 

    rank threshold

evaluation and vocabulary

    important: the vocabulary must be the same over 

models you compare 

    or more accurately, all models must be able to 

generate the test set (it   s ok if they can generate 
more than the test set, but not less) 

    e.g. comparing a character-based model to a 

word-based model is fair, but not vice-versa

let   s try it out! 
(loglin-lm.py)

what problems are handled?
    cannot share strength among similar words
she bought a bicycle
she bought a car
she purchased a car
    not solved yet     

she purchased a bicycle

    cannot condition on context with intervening words

dr. jane smith dr. gertrude smith

    solved!     

    cannot handle long-distance dependencies
for tennis class he wanted to buy his own racquet

for programming class he wanted to buy his own computer

    not solved yet     

beyond linear models

linear models can   t learn 

feature combinations

farmers eat steak     high
farmers eat hay     low

cows eat steak     low
cows eat hay     high

    these can   t be expressed by linear features 
    what can we do? 

    remember combinations as features (individual 

scores for    farmers eat   ,    cows eat   )   
    feature space explosion! 

    neural nets

neural language models

    (see bengio et al. 2004) 

giving

a

lookup

lookup

tanh(   
  w1*h + b1)

w

+
bias

=
scores

softmax

probs

where is strength shared?

giving

a

lookup

lookup

similar output words 
get similar rows in 
in the softmax matrix

tanh(   
  w1*h + b1)

similar contexts get 
similar hidden states

id27s: 
similar input words 
get similar vectors

w

+
bias

=
scores

softmax

probs

what problems are handled?
    cannot share strength among similar words
she bought a bicycle
she bought a car
she purchased a car
    solved, and similar contexts as well!     

she purchased a bicycle

    cannot condition on context with intervening words

dr. jane smith dr. gertrude smith

    solved!     

    cannot handle long-distance dependencies
for tennis class he wanted to buy his own racquet

for programming class he wanted to buy his own computer

    not solved yet     

let   s try it out! 

(nn-lm.py)

tying input/output 
a

embeddings

giving

    we can share parameters 

between the input and output 
embeddings (press et al. 
2016, inter alia)

pick row

pick row

tanh(   
  w1*h + b1)

w

+
bias

=
scores

softmax

probs
want to try? delete the input embeddings, and 

instead pick a row from the softmax matrix.

training tricks

shuf   ing the training data

    stochastic gradient methods update the 

parameters a little bit at a time 

    what if we have the sentence    i love this 

sentence so much!    at the end of the training 
data 50 times? 

    to train correctly, we should randomly shuf   e the 

order at each time step

other optimization options

    sgd with momentum: remember gradients from past 

time steps to prevent sudden changes 

    adagrad: adapt the learning rate to reduce learning 
rate for frequently updated parameters (as measured 
by the variance of the gradient) 

    adam: like adagrad, but keeps a running average of 

momentum and gradient variance 

    many others: rmsprop, adadelta, etc.   

(see ruder 2016 reference for more details)

early stopping, learning 

rate decay

    neural nets have tons of parameters: we want to 

prevent them from over-   tting 

    we can do this by monitoring our performance on 
held-out development data and stopping training 
when it starts to get worse 

    it also sometimes helps to reduce the learning rate 

and continue training

which one to use?

    adam is usually fast to converge and stable 

    but simple sgd tends to do very will in terms of 

generalization 

    you should use learning rate decay, (e.g. on machine 

translation results by denkowski & neubig 2017)

dropout

    neural nets have lots of parameters, and are prone 

to over   tting 

    dropout: randomly zero-out nodes in the hidden 

layer with id203 p at training time only

x
x

    because the number of nodes at training/test is 

different, scaling is necessary: 
    standard dropout: scale by p at test time 
    inverted dropout: scale by 1/(1-p) at training time

let   s try it out! 
(nn-lm-optim.py)

ef   ciency tricks:   
operation batching

ef   ciency tricks:   
mini-batching

    on modern hardware 10 operations of size 1 is 

much slower than 1 operation of size 10 

    minibatching combines together smaller operations 

into one big one

minibatching

manual mini-batching

    group together similar operations (e.g. loss 

calculations for a single word) and execute them all 
together 

    in the case of a feed-forward language model, each 

word prediction in a sentence can be batched 

    for recurrent neural nets, etc., more complicated 

    dynet has special minibatch operations for lookup 

and id168s, everything else automatic

mini-batched code example

let   s try it out! 
(nn-lm-batch.py)

automatic mini-batching!

    (see neubig et al. 2017) 
    try it with the    dynet-autobatch command line option

autobatching usage

    for each minibatch: 

    for each data point in mini-batch: 

    de   ne/add data

    sum losses
    forward (autobatch engine does magic!) 
    backward 
    update

speed improvements

questions?

