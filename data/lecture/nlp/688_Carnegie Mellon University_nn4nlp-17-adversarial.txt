cs11-747 neural networks for nlp 
adversarial methods

graham neubig

https://phontron.com/class/nn4nlp2017/

site

generative models

    generate a sentence randomly from distribution 

p(x) 

    generate a sentence conditioned on some other 

information using distribution p(x|y)

problems with generation

    over-emphasis of common outputs, fuzziness
adversarial

real

id113

    note: this is probably a good idea if you are doing 

maximum likelihood!

image credit: lotter et al. 2015

adversarial training

    basic idea: create a    discriminator    that criticizes 

some aspect of the generated output 

    id3: criticize the 

generated output 

    adversarial id171: criticize the 

generated features to    nd some trait

generative adversarial 

networks

basic paradigm

    two players: generator and discriminator 

    discriminator: given an image, try to tell 

whether it is real or not 

    generator: try to generate an image that fools 

the discriminator into answering    real   

training method

sample latent vars.

z

sample minibatch

convert w/ generator

xreal

xfake

predict w/ discriminator

y

discriminator loss 

(higher if fail predictions)

generator loss 

(higher if make predictions)

in equations

    discriminator id168:

`d(   d,    g) =  

1
2ex   pdata log d(x)  

high prob for real data

    generator id168: 

    zero sum loss:   

1
2ez log(1   d(g(z)))
high prob for fake data

    heuristic non-saturating game loss:   

`g(   d,    g) =  `d(   d,    g)

`g(   d,    g) =  

1
2ez log d(g(z))

    latter gives better gradients when discriminator accurate

   
   
problems w/ training: 

mode collapse

    gans are great, but training is notoriously dif   cult 
    e.g. mode collapse: generator learns to map all z to 

a single x in the training data 

    one solution: use other examples in the minibatch 
as side information, making it easier to push similar 
examples apart (salimans et al. 2016)

problems w/ training: 

over-con   dent discriminator

    at the beginning of training it is easy to learn the 

discriminator, causing it to be over-con   dent 

    one way to    x this: label smoothing to reduce the 

con   dence of the target 

    salimans et al. (2016) suggest one-sided label 
smoothing, which only smooths predictions over

applying gans to text

applications of gan 
objectives to language

    gans for language generation (yu et al. 2017) 

    gans for mt (yang et al. 2017, wu et al. 2017, gu 

et al. 2017) 

    gans for dialogue generation (li et al. 2016)

problem! can   t backprop 

through sampling

sample latent vars.

z

sample minibatch

convert w/ generator

xreal

xfake

predict w/ discriminator

y

discrete! 

can   t backprop

solution: use learning 

methods for latent variables

    policy gradient id23 methods 

(e.g. yu et al. 2016) 

    reparameterization trick for latent variables using 

gumbel softmax (gu et al. 2017)

discriminators for 

sequences

    decide whether a particular generated output is true or not 
    commonly use id98s as discriminators, either on sentences (e.g. 

yu et al. 2017), or pairs of sentences (e.g. wu et al. 2017)

gans for text are hard! 

(yang et al. 2017)

type of discriminator

strength of discriminator

gans for text are hard! 

(wu et al. 2017)

learning rate for generator

learning rate for discriminator

stabilization trick: 

assigning reward to speci   c actions
    getting a reward at the end of the sentence gives a 

credit assignment problem 

    solution: assign reward for partial sequences (yu et 

al. 2016, li et al. 2017)

d(this)
d(this is)
d(this is a)

d(this is a fake)

d(this is a fake sentence)

stabilization tricks: 

performing multiple rollouts
    like other methods using discrete samples, instability 
is a problem 

    this can be helped somewhat by doing multiple 

rollouts (yu et al. 2016)

interesting application: 

gan for data cleaning (yang et al. 2017)

    the discriminator tries to    nd    fake data    

    what about the real data it marks as fake? this 

might be noisy data! 

    selecting data in order of discriminator score does 

better than selecting data randomly.

adversarial feature 

learning

adversaries over features 

vs. over outputs

    id3

x

h

y

adversary!

    adversarial id171

x

h

y
adversary!

    why adversaries over features? 

    non-generative tasks 
    continuous features easier than discrete outputs

learning domain-invariant 
representations (ganin et al. 2016)
    learn features that cannot be distinguished by domain

    interesting application to synthetically generated or stale 

data (kim et al. 2017)

learning language-

invariant representations
representations for text classi   cation

    chen et al. (2016) learn language-invariant 

    also on multi-lingual machine translation (xie et al. 

2017)

adversarial multi-task 
learning (liu et al. 2017)

    basic idea: want some features in a shared space 

across tasks, others separate

    method: adversarial discriminator on shared features, 

orthogonality constraints on separate features

implicit discourse connection 

classi   cation w/ adversarial objective 
    idea: implicit discourse relations are not explicitly 
marked, but would like to detect them if they are 

(qin et al. 2017)

    text with explicit discourse connectives should be 

the same as text without!

professor forcing 

(lamb et al. 2016)

    halfway in between a discriminator on discrete 

outputs and id171 

    generate output sequence according to model 

    but train discriminator on hidden states

x

h

(sampled or true 
output sequence)

y
adversary!

unsupervised style transfer 

for text (shen et al. 2017)

    two potential styles (e.g. positive and negative 

sentences) 

    use professor forcing to discriminate between true 
style 1, fake style 2->1, and another for vice-versa

questions?

