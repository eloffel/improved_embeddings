computing	
   with	
   a	
   

thesaurus

word	
   senses and	
   
word	
   relations

terminology:	
   lemma	
   and	
   wordform

    a	
   lemma or	
   citation	
   form
    a	
   wordform

    same	
   stem,	
   part	
   of	
   speech,	
   rough	
   semantics

    the	
   inflected	
   word	
   as	
   it	
   appears	
   in	
   text

wordform lemma
banks
sung
duermes

bank
sing
dormir

lemmas	
   have	
   senses

    one	
   lemma	
      bank   	
   can	
   have	
   many	
   meanings:

       a bank can hold the investments in a custodial 

sense	
   1:

sense	
   2:

1
account   

          as agriculture burgeons on the east bank the 

river will shrink even more   

2

    sense	
   (or	
   word	
   sense)

    a	
   discrete	
   representation	
   

of	
   an	
   aspect	
   of	
   a	
   word   s	
   meaning.
    the	
   lemma	
   bank here	
   has	
   two	
   senses

homonymy

homonyms:	
   words	
   that	
   share	
   a	
   form	
   but	
   have	
   
unrelated,	
   distinct	
   meanings:
    bank1:	
   financial	
   institution,	
   	
   	
   	
   bank2:	
   	
   sloping	
   land
    bat1:	
   club	
   for	
   hitting	
   a	
   ball,	
   	
   	
   	
   bat2:	
   	
   nocturnal	
   flying	
   mammal

1. homographs (bank/bank,	
   bat/bat)
2. homophones:
1. write and	
   right
2. piece and	
   peace

homonymy	
   causes	
   problems	
   for	
   nlp	
   
applications
information	
   retrieval
       bat care   

   

    machine	
   translation

    text-     to-     speech

    bat:	
   	
   murci  lago (animal)	
   or	
   	
   bate (for	
   baseball)

    bass (stringed	
   instrument)	
   vs.	
   bass (fish)

polysemy

    1.	
   the	
   bank	
   was	
   constructed	
   in	
   1875	
   out	
   of	
   local	
   red	
   brick.
    2.	
   i	
   withdrew	
   the	
   money	
   from	
   the	
   bank
    are	
   those	
   the	
   same	
   sense?
    sense	
   2:	
      a	
   financial	
   institution   
    sense	
   1:	
      the	
   building	
   belonging	
   to	
   a	
   financial	
   institution   

    a	
   polysemous word	
   has	
   related meanings

    most	
   non-     rare	
   words	
   have	
   multiple	
   meanings

metonymy	
   or	
   systematic	
   polysemy:	
   
a	
   systematic	
   relationship	
   between	
   senses

    lots	
   of	
   types	
   of	
   polysemy	
   are	
   systematic

    school, university, hospital
    all	
   can	
   mean	
   the	
   institution	
   or	
   the	
   building.

    a	
   systematic	
   relationship:
organization

    building

    other	
   such	
   kinds	
   of	
   systematic	
   polysemy:	
   
author (jane austen wrote emma)	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

works	
   of	
   author	
   (i love jane austen)
tree (plums have beautiful blossoms)    

fruit (i ate a preserved plum)

how	
   do	
   we	
   know	
   when	
   a	
   word	
   has	
   more	
   
than	
   one	
   sense?

    the	
      zeugma   	
   test:	
   two	
   senses	
   of	
   serve?

    which flights serve breakfast?
    does lufthansa serve philadelphia?
    ?does	
   lufthansa	
   serve	
   breakfast	
   and	
   san	
   jose?

    since	
   this	
   conjunction	
   sounds	
   weird,	
   

    we	
   say	
   that	
   these	
   are	
   two	
   different	
   senses	
   of	
      serve   

    word	
   that	
   have	
   the	
   same	
   meaning	
   in	
   some	
   or	
   all	
   contexts.

synonyms

    filbert	
   /	
   hazelnut
    couch	
   /	
   sofa
    big	
   /	
   large
    automobile	
   /	
   car
    vomit	
   /	
   throw	
   up
    water	
   /	
   h20

    two	
   lexemes	
   are	
   synonyms	
   

    if	
   they	
   can	
   be	
   substituted	
   for	
   each	
   other	
   in	
   all	
   situations
    if	
   so	
   they	
   have	
   the	
   same	
   propositional	
   meaning

synonyms

    but	
   there	
   are	
   few	
   (or	
   no)	
   examples	
   of	
   perfect	
   synonymy.

    even	
   if	
   many	
   aspects	
   of	
   meaning	
   are	
   identical
    still	
   may	
   not	
   preserve	
   the	
   acceptability	
   based	
   on	
   notions	
   of	
   politeness,	
   

slang,	
   register,	
   genre,	
   etc.

    example:

    water/h20
    big/large
    brave/courageous

synonymy	
   is	
   a	
   relation	
   
between	
   senses	
   rather	
   than	
   words

    consider	
   the	
   words	
   big and	
   large
    are	
   they	
   synonyms?

    how	
   big is	
   that	
   plane?
    would	
   i	
   be	
   flying	
   on	
   a	
   large or	
   small	
   plane?

    how	
   about	
   here:

    why?

    miss	
   nelson became	
   a	
   kind	
   of	
   big	
   sister	
   to	
   benjamin.
    ?miss	
   nelson became	
   a	
   kind	
   of	
   large sister	
   to	
   benjamin.

    big has	
   a	
   sense	
   that	
   means	
   being	
   older,	
   or	
   grown	
   up
    large lacks	
   this	
   sense

antonyms

    senses	
   that	
   are	
   opposites	
   with	
   respect	
   to	
   one	
   feature	
   of	
   meaning
    otherwise,	
   they	
   are	
   very	
   similar!

fast/slow
in/out

rise/fall

dark/light
hot/cold

short/long
up/down

    more	
   formally:	
   antonyms	
   can
    define	
   a	
   binary	
   opposition

or	
   be	
   at	
   opposite	
   ends	
   of	
   a	
   scale
    long/short, fast/slow

    be	
   reversives:

    rise/fall, up/down

hyponymy	
   and	
   hypernymy

    one	
   sense	
   is	
   a	
   hyponym of	
   another	
   if	
   the	
   first	
   sense	
   is	
   more	
   

specific,	
   denoting	
   a	
   subclass	
   of	
   the	
   other
    car is	
   a	
   hyponym	
   of	
   vehicle
    mango is	
   a	
   hyponym	
   of	
   fruit

    conversely	
   hypernym/superordinate (   hyper	
   is	
   super   )

    vehicle is	
   a	
   hypernym of	
   car
    fruit is	
   a	
   hypernym of	
   mango
vehicle fruit

superordinate/hyper
subordinate/hyponym car

mango

furniture
chair

    the	
   class	
   denoted	
   by	
   the	
   superordinate	
   extensionally	
   includes	
   the	
   class	
   

    a	
   sense	
   a	
   is	
   a	
   hyponym	
   of	
   sense	
   b	
   if	
   being	
   an	
   a	
   entails	
   being	
   a	
   b

hyponymy	
   more	
   formally

    extensional:

denoted	
   by	
   the	
   hyponym

    entailment:

    hyponymy	
   is	
   usually	
   transitive	
   

    (a	
   hypo	
   b	
   and	
   b	
   hypo	
   c	
   entails	
   a	
   hypo	
   c)

    another	
   name:	
   the	
   is-     a	
   hierarchy

    a	
   is-     a b	
   	
   	
   	
   	
   	
   (or	
   a	
   isa b)
    b	
   subsumes a

hyponyms	
   and	
   instances
    id138 has	
   both	
   classes and	
   instances.
    an	
   instance is	
   an	
   individual,	
   a	
   proper	
   noun	
   that	
   is	
   a	
   unique	
   entity

    san francisco is	
   an	
   instance of	
   city

    but	
   city is	
   a	
   class

    city is	
   a	
   hyponym of	
   	
   	
   	
   municipality...location...

15

meronymy

    the	
   part-     whole	
   relation

    a	
   leg	
   is	
   part	
   of	
   a	
   chair;	
   a	
   wheel	
   is	
   part	
   of	
   a	
   car.	
   

    wheel	
   is	
   a	
   meronym of	
   car,	
   and	
   car	
   is	
   a	
   holonym of	
   wheel.	
   

16

computing	
   with	
   a	
   

thesaurus

word	
   senses and	
   
word	
   relations

computing	
   with	
   a	
   

thesaurus
id138

id138 3.0

    a	
   hierarchically	
   organized	
   lexical	
   database
    on-     line	
   thesaurus	
   +	
   aspects	
   of	
   a	
   dictionary

    some	
   other	
   languages	
   available	
   or	
   under	
   development

    (arabic,	
   finnish,	
   german,	
   portuguese   )
unique	
   strings
category
117,798
noun
11,529
verb
22,479
adjective
4,481
adverb

senses	
   of	
      bass   	
   in	
   id138

how	
   is	
      sense   	
   defined	
   in	
   id138?

    the synset (synonym	
   set),	
   the	
   set	
   of	
   near-     synonyms,	
   

instantiates	
   a	
   sense	
   or	
   concept,	
   with	
   a	
   gloss
    example:	
   chump	
   as	
   a	
   noun	
   with	
   the	
   gloss:

   a	
   person	
   who	
   is	
   gullible	
   and	
   easy	
   to	
   take	
   advantage	
   of   

    this	
   sense	
   of	
      chump   	
   is	
   shared	
   by	
   9	
   words:
chump1, fool2, gull1, mark9, patsy1, fall guy1, 
sucker1, soft touch1, mug2

    each	
   of	
   these senses	
   have	
   this	
   same	
   gloss
    (not	
   every sense;	
   sense	
   2	
   of	
   gull	
   is	
   the	
   aquatic	
   bird)

id138 hypernym hierarchy	
   for	
      bass   

id138 noun	
   relations

16.4

    id51: overview 7

from concepts to subtypes
from instances to their concepts

also called de   nition
superordinate from concepts to superordinates
subordinate

relation
hypernym
hyponym
instance hypernym instance
instance hyponym has-instance from concepts to concept instances
member meronym has-member from groups to their members
member holonym member-of
from members to their groups
from wholes to parts
part meronym
from parts to wholes
part holonym
substance meronym
from substances to their subparts
from parts of substances to wholes
substance holonym
semantic opposition between lemmas leader1 () follower1
antonym
lemmas w/same morphological root
derivationally

example
breakfast1 ! meal1
meal1 ! lunch1
austen1 ! author1
composer1 ! bach1
faculty2 ! professor1
copilot1 ! crew1
table2 ! leg3
course7 ! meal1
water1 ! oxygen1
gin1 ! martini1
destruction1 () destroy1

has-part
part-of

related form

figure 16.2 noun relations in id138.

id138	
   verbrelations

from substances to their subparts
from parts of substances to wholes
semantic opposition between lemmas leader1 () follower1
lemmas w/same morphological root

course7 ! meal1
water1 ! oxygen1
gin1 ! martini1
destruction1 () destroy1

substance meronym
substance holonym
antonym
derivationally

related form

figure 16.2 noun relations in id138.

relation
hypernym
troponym

entails
antonym
derivationally

related form

de   nition
from events to superordinate events
from events to subordinate event
(often via speci   c manner)
from verbs (events) to the verbs (events) they entail
semantic opposition between lemmas
lemmas with same morphological root

example
   y9 ! travel5
walk1 ! stroll1
snore1 ! sleep1
increase1 () decrease1
destroy1 () destruction1

figure 16.3 verb relations in id138.

respond to the notion of immediate hyponymy discussed on page 5. each synset is
related to its immediately more general and more speci   c synsets through direct hy-
pernym and hyponym relations. these relations can be followed to produce longer

id138:	
   viewed	
   as	
   a	
   graph

id51: a survey

10:9

25

change
emotion

location
time
event

communic.    417 review

though id138 synsets are associated with lex-
ical entries, the supersense categories are unlexical-
ized. the n:person category, for instance, contains
synsets for both principal and student. a different
sense of principal falls under n:possession.

   supersenses   
the	
   top	
   level	
   hypernyms in	
   the	
   hierarchy
(counts	
   from	
   schneider	
   and	
   smith	
   2013   s	
   streusel	
   corpus)

638 area
274    x
530 day
249 love
431 experience perception 143 see
consumption 93 have
82 get. . . done
possession 339 price
body
64 cook
205 quality
creation
attribute
46 put
102 amount
contact
quantity
noun
noun
verb
the 26 noun and 15 verb supersense categories are
competition 11 win
88 dog
animal
listed with examples in table 1. some of the names
1469 place
1469 place
2922 is
stative
group
0    
87 hair
stative
weather
body
overlap between the noun and verb inventories, but
1202 people
cognition 1093 know
cognition 1093 know
1202 people
person
all 15 vssts 7806
56 pain
state
971 car
971 car
they are to be considered separate categories; here-
artifact
natural obj. 54    ower
771 way
944 use
944 use
771 way
cognition
social
35 portion
social
after, we will distinguish the noun and verb categories
relation
766 food
602 go
602 go
766 food
food
motion
34 oil
motion
`a
substance
with pre   xes, e.g. n:cognition vs. v:cognition.
700 service
possession 309 pay
700 service
possession 309 pay
act
34 discomfort `
feeling
though id138 synsets are associated with lex-
638 area
274    x
274    x
638 area
location
change
28 process
change
`j
process
ical entries, the supersense categories are unlexical-
530 day
249 love
530 day
249 love
time
emotion
emotion
25 reason
motive
ized. the n:person category, for instance, contains
431 experience perception 143 see
431 experience perception 143 see
event
phenomenon 23 result
synsets for both principal and student. a different
consumption 93 have
consumption 93 have
is short for
6 square
shape
82 get. . . done
possession 339 price
sense of principal falls under n:possession.
82 get. . . done
body
body
5 tree
plant
64 cook
64 cook
attribute
creation
creation
2 stuff
other
46 put
46 put
contact
quantity
all 26 nssts 9018
contact
competition 11 win
competition 11 win
animal
table 1: summary of noun and verb supersense cate-
0    
0    
body
weather
weather
gories. each entry shows the label along with the count
all 15 vssts 7806
all 15 vssts 7806
state

the 26 noun and 15 verb supersense categories are
listed with examples in table 1. some of the names
overlap between the noun and verb inventories, but
they are to be considered separate categories; here-
after, we will distinguish the noun and verb categories
with pre   xes, e.g. n:cognition vs. v:cognition.
though id138 synsets are associated with lex-
ical entries, the supersense categories are unlexical-
ized. the n:person category, for instance, contains
synsets for both principal and student. a different
sense of principal falls under n:possession.

as far as we are aware, the supersenses were
originally intended only as a method of organizing
the id138 structure. but ciaramita and johnson
(2003) pioneered the coarse word sense disambigua-
tion task of supersense tagging, noting that the su-
persense categories provided a natural broadening
of the traditional named entity categories to encom-
pass all nouns. ciaramita and altun (2006) later
expanded the task to include all verbs, and applied
a supervised sequence modeling framework adapted
from ner. evaluation was against manually sense-
tagged data that had been automatically converted to
the coarser supersenses. similar taggers have since
been built for italian (picca et al., 2008) and chi-
nese (qiu et al., 2011), both of which have their own
id138s mapped to english id138.

as far as we are aware, the supersenses were
originally intended only as a method of organizing
the id138 structure. but ciaramita and johnson
(2003) pioneered the coarse word sense disambigua-

as far as we are aware, the supersenses were
originally intended only as a method of organizing
the id138 structure. but ciaramita and johnson
(2003) pioneered the coarse word sense disambigua-

n/a (see   3.2)
1191 have
821 anyone
54 fried

communic.    974 recommend

communic.    974 recommend

possession 339 price
attribute
quantity
animal
body
state

group
person
artifact
cognition
food
act
location
time
event

communic.    417 review

communic.    417 review

205 quality
102 amount
88 dog
87 hair
56 pain

205 quality
102 amount
88 dog
87 hair
56 pain

   communic.

verb
2922 is

communication

26

facets: a segmentation into minimal semantic units, and a labeling of some of those units with semantic
classes known as supersenses.
for example, given the pos-tagged sentence

supersenses

    a	
   word   s	
   supersense can	
   be	
   a	
   useful	
   coarse-     grained	
   

iprp googledvbd restaurantsnns inin thedt areann andcc fujinnp sushinnp camevbd uprb andcc
reviewsnns werevbd greatjj sorb iprp madevbd adt carryvb outrp ordeid56
representation	
   of	
   word	
   meaning	
   for	
   nlp	
   tasks

the goal is to predict the representation

i googledcommunication restaurantsgroup in the arealocation and fuji_sushigroup
came_upcommunication and reviewscommunication werestative great so i made_ a
carry_outpossession _ordercommunication

where lowercase labels are verb supersenses, uppercase labels are noun supersenses, and _ joins tokens
within a multiword expression. (carry_outpossession and made_ordercommunication are separate mwes.)
the two facets of the representation are discussed in greater detail below. systems are expected to produce
the both facets, though the manner in which they do this (e.g., pipeline vs. joint model) is up to you.

27

    http://id138web.princeton.edu/perl/webwn

id138 3.0

    where	
   it	
   is:

    libraries

    python:	
   	
   id138 from	
   nltk
    http://www.nltk.org/home

    java:

    jwnl,	
   extjwnl on	
   sourceforge

other	
   (domain	
   specific)	
   thesauri

mesh:	
   medical	
   subject	
   headings
thesaurus	
   from	
   the	
   national	
   library	
   of	
   medicine

    mesh (medical	
   subject	
   headings)

    177,000	
   entry	
   terms	
   	
   that	
   correspond	
   to	
   26,142	
   biomedical	
   

   headings   
    hemoglobins

synset

entry	
   terms:	
   	
   eryhem, ferrous	
   hemoglobin,	
   hemoglobin
definition:	
   	
   the	
   oxygen-     carrying	
   proteins	
   of	
   erythrocytes.	
   
they	
   are	
   found	
   in	
   all	
   vertebrates	
   and	
   some	
   invertebrates.	
   
the	
   number	
   of	
   globin	
   subunits	
   in	
   the	
   hemoglobin	
   quaternary	
   
structure	
   differs	
   between	
   species.	
   structures	
   range	
   from	
   
monomeric	
   to	
   a	
   variety	
   of	
   multimeric arrangements

the	
   mesh hierarchy

    a

31

uses	
   of	
   the	
   mesh ontology

    provide	
   synonyms	
   (   entry	
   terms   )

    e.g.,	
   glucose	
   and	
   dextrose

    provide	
   hypernyms (from	
   the	
   hierarchy)

    e.g.,	
   glucose	
   isa	
   monosaccharide

    indexing	
   in	
   medline/pubmed database

    nlm   s	
   bibliographic	
   database:	
   

    20	
   million	
   journal	
   articles
    each	
   article	
   hand-     assigned	
   10-     20	
   mesh terms

computing	
   with	
   a	
   

thesaurus
id138

computing	
   with	
   a	
   

thesaurus

word	
   similarity:	
   
thesaurus	
   methods

word	
   similarity
    synonymy:	
   a	
   binary	
   relation

    two	
   words	
   are	
   either	
   synonymous	
   or	
   not
    similarity	
   (or distance):	
   a	
   looser	
   metric

    two	
   words	
   are	
   more	
   similar	
   if	
   they	
   share	
   more	
   features	
   of	
   meaning

    similarity	
   is	
   properly	
   a	
   relation	
   between	
   senses
    the	
   word	
      bank   	
   is	
   not	
   similar	
   to	
   the	
   word	
      slope   
    bank1 is	
   similar	
   to	
   fund3
    bank2 is	
   similar	
   to	
   slope5

    but	
   we   ll	
   compute	
   similarity	
   over	
   both	
   words	
   and	
   senses

why	
   word	
   similarity

    a	
   practical	
   component	
   in	
   lots	
   of	
   nlp	
   tasks

    question	
   answering
    natural	
   language	
   generation
    automatic	
   essay	
   grading
    plagiarism	
   detection

    historical	
   semantics
    models	
   of	
   human	
   word	
   learning
    morphology	
   and	
   grammar	
   induction

    a	
   theoretical	
   component	
   in	
   many	
   linguistic	
   and	
   cognitive	
   tasks

word	
   similarity	
   and	
   word	
   relatedness
    we	
   often	
   distinguish	
   word	
   similarity	
    from	
   word	
   

relatedness
    similar words:	
   near-     synonyms
    related	
   words:	
   can	
   be	
   related	
   any	
   way

    car, bicycle:	
    similar
    car, gasoline:	
    related,	
   not	
   similar

two	
   classes	
   of	
   similarity	
   algorithms

    thesaurus-     based	
   algorithms

    are	
   words	
      nearby   	
   in	
   hypernym hierarchy?
    do	
   words	
   have	
   similar	
   glosses	
   (definitions)?

    distributional	
   algorithms

    do	
   words	
   have	
   similar	
   distributional	
   contexts?
    distributional	
   (vector)	
   semantics	
   on	
   thursday!

path	
   based	
   similarity

    two	
   concepts	
   (senses/synsets)	
   are	
   similar	
   if	
   

they	
   are	
   near	
   each	
   other	
   in	
   the	
   thesaurus	
   
hierarchy	
   
    =have	
   a	
   short	
   path	
   between	
   them
    concepts	
   have	
   path	
   1	
   to	
   themselves

refinements	
   to	
   path-     based	
   similarity

hypernym graph	
   between	
   sense	
   nodes	
   c1 and	
   c2
ranges	
   from	
   0	
   to	
   1	
   (identity)

    pathlen(c1,c2) =	
   1	
   +	
   number	
   of	
   edges	
   in	
   the	
   shortest	
   path	
   in	
   the	
   
   
    simpath(c1,c2) = 

1

pathlen(c1,c2)

    wordsim(w1,w2) =   max

sim(c1,c2)

c1   senses(w1),c2   senses(w2)

example:	
   path-     based	
   similarity
simpath(c1,c2) = 1/pathlen(c1,c2)

simpath(nickel,coin)	
   =	
   1/2 = .5
simpath(fund,budget)	
   =	
   1/2 = .5
simpath(nickel,currency)	
   =	
   1/4 = .25
simpath(nickel,money)	
   =	
   1/6 = .17
simpath(coinage,richter scale)	
   =	
   1/6 = .17 

problem	
   with	
   basic	
   path-     based	
   similarity

    assumes	
   each	
   link	
   represents	
   a	
   uniform	
   distance

    but	
   nickel to	
   money seems	
   to	
   us	
   to	
   be	
   closer	
   than	
   nickel to	
   
standard
    nodes	
   high	
   in	
   the	
   hierarchy	
   are	
   very	
   abstract

    we	
   instead	
   want	
   a	
   metric	
   that

    represents	
   the	
   cost	
   of	
   each	
   edge	
   independently
    words	
   connected	
   only	
   through	
   abstract	
   nodes	
   

    are	
   less	
   similar

information	
   content	
   similarity	
   metrics

resnik 1995

    let   s	
   define	
   p(c) as:

of	
   concept	
   c

    the	
   id203	
   that	
   a	
   randomly	
   selected	
   word	
   in	
   a	
   corpus	
   is	
   an	
   instance	
   

    formally:	
   there	
   is	
   a	
   distinct	
   random	
   variable,	
   ranging	
   over	
   words,	
   

associated	
   with	
   each	
   concept	
   in	
   the	
   hierarchy
    for	
   a	
   given	
   concept,	
   each	
   observed	
   noun	
   is	
   either

    a	
   member	
   of	
   that	
   concept	
   	
   with	
   id203	
   p(c)
    not	
   a	
   member	
   of	
   that	
   concept	
   with	
   id203	
   1-p(c)

    all	
   words	
   are	
   members	
   of	
   the	
   root	
   node	
   (entity)

    p(root)=1

    the	
   lower	
   a	
   node	
   in	
   hierarchy,	
   the	
   lower	
   its	
   id203

information	
   content	
   similarity

entity

   

geological-     formation

    train	
   by	
   counting	
   in	
   a	
   corpus

natural	
   elevation

    each	
   instance	
   of	
   hill counts	
   toward	
   frequency	
   
of	
   natural	
   elevation,	
   geological	
   formation,	
   entity,	
   etc
    let	
   words(c) be	
   the	
   set	
   of	
   all	
   words	
   that	
   are	
   children	
   of	
   node	
   c

hill

ridge

    words(   geo-     formation   )	
   = {hill,ridge,grotto,coast,cave,shore,natural elevation}
    words(   natural	
   elevation   )	
   =	
   {hill,	
   ridge}

cave

shore

grotto

coast

p(c) =

   

count(w)
w   words(c)
n

information	
   content	
   similarity
    id138 hierarchy	
   augmented	
   with	
   probabilities	
   p(c)

d.	
   lin.	
   1998.	
   an	
   information-     theoretic	
   definition	
   of	
   similarity.	
   icml	
   1998

information	
   content	
   and	
   id203
    the	
   self-     information	
   of	
   an	
   event,	
   also	
   called	
   its	
   surprisal:
    how	
   surprised	
   we	
   are	
   to	
   know	
   it;	
   how	
   much	
   we	
   learn	
   by	
   knowing	
   it.
    the	
   more	
   surprising	
   something	
   is,	
   the	
   more	
   it	
   tells	
   us	
   when	
   it	
   happens
    we   ll	
   measure	
   self-     information	
   in	
   bits.
i(w)=	
   -     log2	
   p(w)
i	
   flip	
   a	
   coin;	
   p(heads)=	
   0.5

   
    how	
   many	
   bits	
   of	
   information	
   do	
   i	
   learn	
   by	
   flipping	
   it?

    i(heads)	
   =	
   -     log2(0.5)	
   =	
   -     log2	
   (1/2)	
   =	
   log2	
   (2)	
   =	
   1	
   bit
i	
   flip	
   a	
   biased	
   coin:	
   p(heads	
   )=	
   0.8	
   i	
   don   t	
   learn	
   as	
   much
    i(heads)	
   =	
   -     log2(0.8)	
   =	
   -     log2(0.8)	
   =	
   .32	
   bits

   

46

information	
   content:	
   definitions
1.3	
   bits

ic(c) = -log p(c)

    information	
   content:
    most	
   informative	
   subsumer
(lowest	
   common	
   subsumer)
lcs(c1,c2) = 
the	
   most	
   informative	
   (lowest)	
   
node	
   in	
   the	
   hierarchy	
   
subsuming	
   both	
   c1 and	
   c2

5.9	
   bits

9.1	
   bits

15.7	
   bits

using	
   information	
   content	
   for	
   similarity:	
   	
   
the	
   resnik method

philip	
   resnik.	
   1995.	
   using	
   information	
   content	
   to	
   evaluate	
   semantic	
   similarity	
   in	
   a	
   taxonomy.	
   ijcai	
   1995.
philip	
   resnik.	
   1999.	
   semantic	
   similarity	
   in	
   a	
   taxonomy:	
   an	
   information-     based	
   measure	
   and	
   its	
   application	
   
to	
   problems	
   of	
   ambiguity	
   in	
   natural	
   language.	
   jair	
   11,	
   95-     130.

common	
   information

    the	
   similarity	
   between	
   two	
   words	
   is	
   related	
   to	
   their	
   
    the	
   more	
   two	
   words	
   have	
   in	
   common,	
   the	
   more	
   
    resnik:	
   measure	
   common	
   information	
   as:

similar	
   they	
   are

    the	
   information	
   content	
   of	
   the	
   most	
   informative
(lowest)	
   subsumer (mis/lcs)	
   of	
   the	
   two	
   nodes
    simresnik(c1,c2) = -log p( lcs(c1,c2) )

dekang lin	
   method

dekang lin.	
   1998.	
   an	
   information-     theoretic	
   definition	
   of	
   similarity.	
   icml

   

intuition:	
   similarity	
   between	
   a	
   and	
   b	
   is	
   not	
   just	
   what	
   they	
   have	
   
in	
   common

    the	
   more	
   differences between	
   a	
   and	
   b,	
   the	
   less	
   similar	
   they	
   are:

    commonality:	
   the	
   more	
   a	
   and	
   b	
   have	
   in	
   common,	
   the	
   more	
   similar	
   they	
   are
    difference:	
   the	
   more	
   differences	
   between	
   a	
   and	
   b,	
   the	
   less	
   similar

    commonality:	
   ic(common(a,b))
    difference:	
   ic(description(a,b)-     ic(common(a,b))

dekang lin	
   similarity	
   theorem

    the	
   similarity	
   between	
   a	
   and	
   b	
   is	
   measured	
   by	
   the	
   ratio	
   
between	
   the	
   amount	
   of	
   information	
   needed	
   to	
   state	
   the	
   
commonality	
   of	
   a	
   and	
   b	
   and	
   the	
   information	
   needed	
   to	
   fully	
   
describe	
   what	
   a	
   and	
   b	
   are
simlin(a,b)    

ic(common(a,b))
ic(description(a,b))

   

lin	
   (altering	
   resnik)	
   defines	
   ic(common(a,b))	
   as	
   2	
   x	
   information	
   of	
   the	
   lcs

simlin(c1,c2) =

2logp(lcs(c1,c2))
logp(c1) +logp(c2)

lin	
   similarity	
   function

simlin(a,b) =

2logp(lcs(c1,c2))
logp(c1) +logp(c2)

simlin(hill,coast) =

2logp(geological-formation)

logp(hill) +logp(coast)

2ln0.00176

=
ln0.0000189 +ln0.0000216
=.59

the	
   (extended)	
   lesk algorithm	
   
    a	
   thesaurus-     based	
   measure	
   that	
   looks	
   at	
   glosses
    two	
   concepts	
   are	
   similar	
   if	
   their	
   glosses	
   contain	
   similar	
   words

    drawing	
   paper:	
   paper that	
   is	
   specially	
   prepared	
   for	
   use	
   in	
   drafting
    decal:	
   the	
   art	
   of	
   transferring	
   designs	
   from	
   specially	
   prepared	
   paper to	
   a	
   

wood	
   or	
   glass	
   or	
   metal	
   surface

    for	
   each	
   n-     word	
   phrase	
   that   s	
   in	
   both	
   glosses

    add	
   a	
   score	
   of	
   n2
    paper	
   and	
   specially	
   prepared	
   for	
   1	
   +	
   22 =	
   5
    compute	
   overlap	
   also	
   for	
   other	
   relations
    glosses	
   of	
   hypernyms and	
   hyponyms

summary:	
   thesaurus-     based	
   similarity

simpath(c1,c2) =

1

pathlen(c1,c2)

simresnik(c1,c2) =    logp(lcs(c1,c2))

2logp(lcs(c1,c2))
logp(c1) +logp(c2)

simlin(c1,c2) =
1

sim jiangconrath(c1,c2) =
simelesk(c1,c2) =

   

r,q   rels

logp(c1) +logp(c2)    2logp(lcs(c1,c2))

overlap(gloss(r(c1)),gloss(q(c2)))

libraries	
   for	
   computing	
   thesaurus-     based	
   
similarity

    nltk

    http://nltk.github.com/api/nltk.corpus.reader.html?highlight=similarity	
   -     

nltk.corpus.reader.id138corpusreader.res_similarity

    id138::similarity

    http://wn-     similarity.sourceforge.net/
    web-     based	
   interface:

    http://marimba.d.umn.edu/cgi-     bin/similarity/similarity.cgi

54

evaluating	
   similarity

    extrinsic	
   (task-     based,	
   end-     to-     end)	
   evaluation:

    question	
   answering
    spell	
   checking
    essay	
   grading
intrinsic	
   evaluation:
    correlation	
   between	
   algorithm and	
   human	
   word	
   similarity	
   ratings
    wordsim353:	
   353	
   noun	
   pairs	
   rated	
   0-     10.	
   	
   	
   sim(plane,car)=5.77

   

    taking	
   toefl	
   multiple-     choice	
   vocabulary	
   tests

    levied is closest in meaning to:
imposed, believed, requested, correlated

