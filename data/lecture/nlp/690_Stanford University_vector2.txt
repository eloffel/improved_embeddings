dan jurafsky and james martin
speech and language processing

chapter 6:
vector semantics, part ii

tf-idf and ppmi are 
sparse representations

tf-idf and ppmi vectors are
   long (length |v|= 20,000 to 50,000)
   sparse (most elements are zero)

alternative: dense vectors

vectors which are
    short (length 50-1000)
    dense (most elements are non-zero)

3

sparse versus dense vectors

why dense vectors?
    short vectors may be easier to use as features in machine 
learning (less weights to tune)
    dense vectors may generalize better than storing explicit 
counts
    they may do better at capturing synonymy:
    car and automobile are synonyms; but are distinct dimensions
    a word with car as a neighbor and a word with automobile as a 
neighbor should be similar, but aren't

4

    in practice, they work better

dense embeddings you can 
download!

id97 (mikolov et al.)
https://code.google.com/archive/p/id97/
fasttext http://www.fasttext.cc/
glove (pennington, socher, manning)
http://nlp.stanford.edu/projects/glove/

id97

popular embedding method
very fast to train
code available on the web
idea: predict rather than count 

id97
   instead of counting how often each 
word w occurs near "apricot"
   train a classifier on a binary 
prediction task:
   is w likely to show up near "apricot"?

   we don   t actually care about this task
   but we'll take the learned classifier weights 
as the id27s

brilliant insight: use running text as 
implicitly supervised training data!

    a word s near apricot 

    acts as gold    correct answer    to the 
       is word w likely to show up near apricot?    

question 

    no need for hand-labeled supervision
    the idea comes from neural language 

modeling 
    bengio et al. (2003)
    collobert et al. (2011) 

id97: skip-gram task

id97 provides a variety of options. let's do
    "skip-gram with negative sampling" (sgns)

skip-gram algorithm
1. treat the target word and a neighboring 

context word as positive examples.

2. randomly sample other words in the 

lexicon to get negative samples

3. use id28 to train a classifier 

to distinguish those two cases

4. use the weights as the embeddings

9/7/18

10

skip-gram training data
training sentence:
... lemon, a tablespoon of apricot jam   a pinch ... 

c1            c2   target c3    c4

asssume context words are those in +/- 2 
word window

11

9/7/18

skip-gram goal

given a tuple (t,c)  = target, context
   (apricot, jam)
   (apricot, aardvark)
return id203 that c is a real context word:

p(+|t,c)
p(   |t,c) = 1   p(+|t,c)

9/7/18

12

how to compute p(+|t,c)?
intuition:
    words are likely to appear near similar words
    model similarity with dot-product!

    similarity(t,c)      t    c

problem:
   dot product is not a id203!
    (neither is cosine)

of course, the dot product t    c is not a id203, it   s just a number ranging
from 0 to    . (recall, for that matter, that cosine isn   t a id203 either). to turn
the dot product into a id203, we   ll use the logistic or sigmoid function s (x),
the fundamental core of id28:

turning dot product into a 
id203

the sigmoid lies between 0 and 1:

s (x) =

1

1 + e x

the id203 that word c is a real context word for target word t is thus com-

p(+|t,c) =

1

1 + e t  c

the id203 that word c is a real context word for target word t is thus com-

s (x) =

1 + e x

turning dot product into a 
id203

p(+|t,c) =

1 + e t  c

1

the sigmoid function just returns a number between 0 and 1, so to make it a
id203 we   ll need to make sure that the total id203 of the two possible
events (c being a context word, and c not being a context word) sum to 1.

1

p(+|t,c) =

1 + e t  c

the id203 that word c is not a real context word for t is thus:

the sigmoid function just returns a number between 0 and 1, so to make it a
id203 we   ll need to make sure that the total id203 of the two possible
events (c being a context word, and c not being a context word) sum to 1.

p( |t,c) = 1  p(+|t,c)

e t  c

the id203 that word c is not a real context word for t is thus:

=

1 + e t  c

p( |t,c) = 1  p(+|t,c)

    id97

equation 6.19 give us the id203 for one word, but we need to take account
of the multiple context words in the window. skip-gram makes the strong but very
for all the context words:
useful simplifying assumption that all context words are independent, allowing us to
assume all context words are 
just multiply their probabilities:
independent

p(+|t,c1:k) =

logp(+|t,c1:k) =

1

1 + e t  ci

log

1

1 + e t  ci

kyi=1
kxi=1

in summary, skip-gram trains a probabilistic classi   er that, given a test target

skip-gram training data
training sentence:
... lemon, a tablespoon of apricot jam   a pinch ... 

c1              c2     t

c3    c4

training data: input/output pairs centering 
on apricot
asssume a +/- 2 word window

17

9/7/18

above:

... lemon,

id97 learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don   t occur nearby.

skip-gram training
let   s start by considering a single piece of the training data, from the sentence
training sentence:
... lemon, a tablespoon of apricot jam   a pinch ... 
this example has a target word t (apricot), and 4 context words in the l =   2

t
c1              c2     t

a [tablespoon of apricot jam,

a] pinch ...
c4

c3
c3    c4

window, resulting in 4 positive training instances (on the left below):
negative examples -
c
twelve

c1

c2

t

c

   for each positive example, 
t
we'll create k negative 
apricot aardvark apricot
examples.
apricot puddle
   using noise words
apricot where
   any random word that isn't t
apricot coaxial

apricot hello
apricot dear
apricot forever

18

c
tablespoon

positive examples +
t
apricot
apricot of
apricot preserves
apricot or

9/7/18

for training a binary classi   er we also need negative examples, and in fact skip-
gram uses more negative examples than positive examples, the ratio set by a param-

c2

c1

above:

above:

... lemon,

... lemon,
c1

id97 learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don   t occur nearby.

id97 learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don   t occur nearby.

let   s start by considering a single piece of the training data, from the sentence

skip-gram training
let   s start by considering a single piece of the training data, from the sentence
training sentence:
a] pinch ...
a [tablespoon of apricot jam,
a [tablespoon of apricot jam,
... lemon, a tablespoon of apricot jam   a pinch ... 
c3
c4
t
c3
this example has a target word t (apricot), and 4 context words in the l =   2
c1              c2     t
c3    c4

this example has a target word t (apricot), and 4 context words in the l =   2

window, resulting in 4 positive training instances (on the left below):
positive examples +
positive examples +
negative examples -
t
c
t
c
apricot
tablespoon
twelve
apricot
apricot of
apricot of
apricot preserves
apricot preserves
apricot or
apricot or

window, resulting in 4 positive training instances (on the left below):
negative examples -
t
c
c
t
t
apricot aardvark apricot
twelve
apricot aardvark apricot
apricot puddle
apricot hello
apricot puddle
apricot where
apricot dear
apricot where
apricot coaxial
apricot coaxial
apricot forever
for training a binary classi   er we also need negative examples, and in fact skip-
gram uses more negative examples than positive examples, the ratio set by a param-

for training a binary classi   er we also need negative examples, and in fact skip-
gram uses more negative examples than positive examples, the ratio set by a param-

apricot hello
apricot dear
apricot forever

c
tablespoon

a] pinch ...
c4

k=2

c2

c

9/7/18

t

t

19

    vector semantics

18 chapter 6

choosing noise words
    vector semantics

the word the as a noise word, with unigram id203 p(   aardvark   ) we would
choose aardvark, and so on. but in practice it is common to set a = .75, i.e. use the
weighting p 3

the word the as a noise word, with unigram id203 p(   aardvark   ) we would
choose aardvark, and so on. but in practice it is common to set a = .75, i.e. use the
weighting p 3

could pick w according to their unigram frequency p(w)
more common to chosen then according to p  (w)

4 (w):

4 (w):

pa (w) =

pa (w) =

count(w)a
count(w)a

pw count(w)a
pw count(w)a

setting a = .75 gives better performance because it gives rare noise words
slightly higher id203: for rare words, pa (w) > p(w). to visualize this intu-
ition, it might help to work out the probabilities for an example with two events,
p(a) = .99 and p(b) = .01:

setting a = .75 gives better performance because it gives rare noise words
slightly higher id203: for rare words, pa (w) > p(w). to visualize this intu-
ition, it might help to work out the probabilities for an example with two events,
p(a) = .99 and p(b) = .01:

  =    works well because it gives rare noise words slightly higher 
id203
to show this, imagine two events p(a)=.99 and p(b) = .01:

pa (a) =

pa (b) =

pa (a) =

.99.75

.99.75 + .01.75 = .97
.99.75 + .01.75 = .03
.99.75 + .01.75 = .97

.01.75
.99.75

.01.75

given the set of positive and negative training instances, and an initial set of

(6.23)

(6.24)

setup
let's represent words as vectors of some length (say 
300), randomly initialized. 
so we start with 300 * v random parameters
over the entire training set, we   d like to adjust those 
word vectors such that we
    maximize the similarity of the target word, context 
word pairs (t,c) drawn from the positive data
    minimize the similarity of the (t,c) pairs drawn from 
the negative data. 

21

9/7/18

learning the classifier
iterative process.
we   ll start with 0 or random weights
then adjust the word weights to
    make the positive pairs more likely 
    and the negative pairs less likely
over the entire training set:

objective criteria
we want to maximize   
logp (+|t, c) + x(t,c)2 
x(t,c)2+

logp ( |t, c)

maximize the + label for the pairs from the positive 
training data, and the     label for the pairs sample 
from the negative data.

9/7/18

23

we can express this formally over the whole training set as:

l(q ) = x(t,c)2+
focusing on one target word t:

logp(+|t,c) + x(t,c)2 

logp( |t,c)

or, focusing in on one word/context pair (t,c) with its k noise words n1...nk, the

(6.25)

learning objective l is:

logp( |t,ni)

kxi=1
l(q ) = logp(+|t,c) +
kxi=1
logs ( ni   t)
kxi=1

= logs (c  t) +

1 + e c  t +

1 + eni  t

= log

log

1

1

(6.26)

that is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the k negative sampled non-
neighbor words.

w
apricot

1.2      .j         v

1
.
.
.
d

similarity( apricot , jam)

increase
wj . ck

c

1. ..          d

1
.
k
.
n
.
v

      apricot jam      

similarity( apricot , aardvark)

decrease
wj . cn

neighbor word

jam
aardvark

random noise

word

train using id119

actually learns two separate embedding matrices w and c
can use w and throw away c, or merge them somehow

summary: how to learn id97 
(skip-gram) embeddings
start with v random 300-dimensional vectors as 
initial embeddings
use id28, the second most basic 
classifier used in machine learning after na  ve 
bayes
    take a corpus and take pairs of words that co-occur as 
positive examples
    take pairs of words that don't co-occur as negative 
examples
    train the classifier to distinguish these by slowly adjusting 
all the embeddings to improve the classifier performance
    throw away the classifier code and keep the embeddings.

evaluating embeddings
compare to human scores on word 
similarity-type tasks:
    wordsim-353 (finkelstein et al., 2002)
    siid113x-999 (hill et al., 2015)
    stanford contextual word similarity (scws) dataset 

(huang et al., 2012) 

    toefl dataset: levied is closest in meaning to: imposed, 

believed, requested, correlated 

properties of embeddings

similarity depends on window size c

c =   2 the nearest words to hogwarts:
    sunnydale
    evernight
c =   5 the nearest words to hogwarts:
    dumbledore
    malfoy
    halfblood

29

analogy: embeddings capture 
relational meaning!
vector(   king   ) - vector(   man   ) + vector(   woman   )      vector(   queen   )
vector(   paris   ) - vector(   france   ) + vector(   italy   )     vector(   rome   )

30

embeddings can help study 
word history!
train embeddings on old books to study 
changes in word meaning!!

will hamilton

diachronic id27s for 
studying language change!

word vectors for 1920

   dog    1990 word vector

word vectors 1990

   dog    1920 word vector

1900

vs.

1950

2000

3
4

visualizing changes

project 300 dimensions down into 2

~30 million books, 1850-1990, google books data

the evolution of sentiment words

negative words change faster than positive words

36

embeddings and bias

embeddings reflect cultural bias

bolukbasi, tolga, kai-wei chang, james y. zou, venkatesh saligrama, and 
adam t. kalai. "man is to computer programmer as woman is to 
homemaker? debiasing id27s." in advances in neural 
information processing systems, pp. 4349-4357. 2016.

ask    paris : france :: tokyo : x    
    x = japan
ask    father : doctor :: mother : x    
    x = nurse
ask    man : computer programmer :: woman : x    
    x = homemaker

embeddings reflect cultural bias

caliskan, aylin, joanna j. bruson and arvind narayanan. 2017. semantics derived automatically from 
language corpora contain human-like biases. science 356:6334, 183-186.

implicit association test (greenwald et al 1998): how associated are 
    concepts (flowers, insects) &  attributes (pleasantness, unpleasantness)?
    studied by measuring timing latencies for categorization.
psychological findings on us participants:
    african-american names are associated with unpleasant words (more than european-

american names)

    male names associated more with math, female names with arts
    old people's names with unpleasant words, young people with pleasant words.
caliskan et al. replication with embeddings:
    african-american names (leroy, shaniqua) had a higher glove cosine 
with unpleasant words  (abuse, stink, ugly)
    european american names (brad, greg, courtney) had a higher cosine 
with pleasant words (love, peace, miracle)

embeddings reflect and replicate all sorts of pernicious biases.

directions
debiasing algorithms for embeddings
    bolukbasi, tolga, chang, kai-wei, zou, james y., 
saligrama, venkatesh, and kalai, adam t. (2016). man is 
to computer programmer as woman is to homemaker? 
debiasing id27s. in advances in neural infor-
mation processing systems, pp. 4349   4357. 

use embeddings as a historical tool to study bias

embeddings as a window onto history

garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender 
and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 

use the hamilton historical embeddings
the cosine similarity of embeddings for decade x 
for occupations (like teacher) to male vs female 
names
    is correlated with the actual percentage of women 
teachers in decade x

history of biased framings of women

garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender 
and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 

embeddings for competence adjectives are 
biased toward men
   smart, wise, brilliant, intelligent, resourceful, 
thoughtful, logical, etc.

this bias is slowly decreasing 

embeddings reflect ethnic 
stereotypes over time

garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender 
and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 

    princeton trilogy experiments
    attitudes toward ethnic groups (1933, 
1951, 1969) scores for adjectives
    industrious, superstitious, nationalistic, etc
    cosine of chinese name embeddings with 
those adjective embeddings correlates with 
human ratings.

results on the speci   c word lists used and that the recall of
our methods in capturing human biases may not be adequate.
we take extensive care to reproduce similar results with other
word lists and types of measurements to demonstrate recall. for
example, in si appendix, section b.1, we repeat the static occu-
pation analysis using only professional occupations and repro-
duce an identical    gure to fig. 1 in si appendix, section b.1.
furthermore, the plots themselves contain bootstrapped con   -
dence intervals; i.e., the coef   cients for random subsets of the
occupations/adjectives and the intervals are tight. similarly, for
adjectives, we use two different lists: one list from refs. 6 and 7
for which we have labeled stereotype scores and then a larger
one for the rest of the analysis where such scores are not needed.
we note that we do not tune either the embeddings or the word
lists, instead opting for the largest/most general publicly avail-
able data. for reproducibility, we share our code and all word
lists in a repository. that our methods replicate across many dif-
ferent embeddings and types of biases measured suggests their

materials and methods
in this section we describe the datasets, embeddings, and word lists used,
as well as how bias is quanti   ed. more detail, including descriptions of
additional embeddings and the full word lists, are in si appendix, section
a. all of our data and code are available on github (https://github.com/
nikhgarg/embeddingdynamicstereotypes), and we link to external data
sources as appropriate.

change in linguistic framing 
1910-1990

garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender 
and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 

embeddings. this work uses several pretrained id27s publicly
available online; refer to the respective sources for in-depth discussion of
their training parameters. these embeddings are among the most com-
monly used english embeddings, vary in the datasets on which they were

change in association of chinese names with adjectives 
framed as "othering" (barbaric, monstrous, bizarre)

a common challenge in historical analysis is that the written
text in, say 1910, may not completely re   ect the popular social
attitude of that time. this is an important caveat to consider in
interpreting the results of the embeddings trained on these ear-
lier text corpora. the fact that the embedding bias for gender
and ethnic groups does track with census proportion is a positive
control that the embedding is still capturing meaningful patterns
despite possible limitations in the training text. even this con-
trol may be limited in that the census proportion does not fully
capture gender or ethnic associations, even in the present day.
however, the written text does serve as a window into the atti-
tudes of the day as expressed in popular culture, and this work

fig. 6. asian bias score over time for words related to outsiders in coha

changes in framing:
adjectives associated with chinese

table 3. top asian (vs. white) adjectives in 1910, 1950, and 1990
by relative norm difference in the coha embedding
1910

garg, nikhil, schiebinger, londa, jurafsky, dan, and zou, james (2018). id27s quantify 100 years of gender 
and ethnic stereotypes. proceedings of the national academy of sciences, 115(16), e3635   e3644 

1990

1950

irresponsible
envious
barbaric
aggressive
transparent
monstrous
hateful
cruel
greedy
bizarre

disorganized
outrageous
pompous
unstable
effeminate
unprincipled
venomous
disobedient
predatory
boisterous

inhibited
passive
dissolute
haughty

complacent

forceful

fixed
active
sensitive
hearty

qualitatively through the results in the snapshot analysis for gen-

embeddings used are fully    black box,    where the dimensions
have no inherent meaning. to provide a more causal explana-
tion of how the stereotypes appear in language, and to under-
stand how they function, future work can leverage more recent
embedding models in which certain dimensions are designed to
capture various aspects of language, such as the polarity of a
word or its parts of speech (45). similarly, structural proper-
ties of words   beyond their census information or human-rated
stereotypes   can be studied in the context of these dimensions.
one can also leverage recent bayesian embeddings models and
train more    ne-grained embeddings over time, rather than a sep-
arate embedding per decade as done in this work (46, 47). these
approaches can be used in future work.

and validating a framework for exploring the temporal dynam-
ics of stereotypes through the lens of id27s. our
framework enables the computation of simple but quantitative

conclusion
concepts or word senses
    have a complex many-to-many association with words
(homonymy, multiple senses)
    have relations with each other
    synonymy, antonymy, superordinate
    but are hard to define formally (necessary & sufficient 
conditions)

embeddings = vector models of meaning
    more fine-grained than just a string or index
    especially good at modeling similarity/analogy
    just download them and use cosines!!
    can use sparse models (tf-idf) or dense models (id97, 
glove)
    useful in practice but know they encode cultural stereotypes

