word	
   sense	
   disambiguation

word	
   sense	
   disambiguation	
   (wsd)

    given	
   

    a word	
   in	
   context	
   
    a	
   fixed	
   inventory	
   of	
   potential	
   word	
   senses
    decide	
   which	
   sense	
   of	
   the	
   word	
   this	
   is

    why?	
   machine	
   translation,	
   qa,	
   speech	
   synthesis
    what	
   set	
   of	
   senses?

    english-     to-     spanish	
   mt:	
   set	
   of	
   spanish	
   translations
    speech	
   synthesis:	
   	
   homographs	
   like	
   bass and	
   bow
    in	
   general:	
   the	
   senses	
   in	
   a	
   thesaurus	
   like	
   id138

two	
   variants	
   of	
   wsd	
   task

    lexical	
   sample	
   task

    small	
   pre-     selected	
   set	
   of	
   target	
   words	
   (line,	
   plant)
    and	
   inventory	
   of	
   senses	
   for	
   each	
   word
    supervised	
   machine	
   learning:	
   train	
   a	
   classifier	
   for	
   each	
   word

    all-     words	
   task

    every	
   word	
   in	
   an	
   entire	
   text
    a	
   lexicon	
   with	
   senses	
   for	
   each	
   word
    data	
   sparseness:	
   can   t	
   train	
   word-     specific	
   classifiers

wsd	
   methods

    supervised	
   machine	
   learning
    thesaurus/dictionary	
   methods
    semi-     supervised	
   learning

4

word	
   sense	
   
disambiguation

supervised	
   

machine	
   learning

supervised	
   machine	
   learning	
   approaches

    supervised	
   machine	
   learning	
   approach:

    a	
   training	
   corpus of	
   words	
   tagged	
   in	
   context	
   with	
   their	
   sense
    used	
   to	
   train	
   a	
   classifier	
   that	
   can	
   tag	
   words	
   in	
   new	
   text

    summary	
   of	
   what	
   we	
   need:
    the	
   tag	
   set (   sense	
   inventory   )
    the	
   training	
   corpus
    a	
   set	
   of	
   features extracted	
   from	
   the	
   training	
   corpus
    a	
   classifier

supervised	
   wsd	
   1:	
   wsd	
   tags

    what   s	
   a	
   tag?

a	
   dictionary	
   sense?

    for	
   example,	
   for	
   id138	
   an	
   instance	
   of	
      bass    in	
   a	
   text	
   has	
   8	
   

possible	
   tags	
   or	
   labels	
   (bass1	
   through	
   bass8).

8	
   senses	
   of	
      bass   	
   in	
   id138

1. bass	
   -      (the	
   lowest	
   part	
   of	
   the	
   musical	
   range)
2. bass,	
   bass	
   part	
   -      (the	
   lowest	
   part	
   in	
   polyphonic	
   	
   music)
3. bass,	
   basso	
   -      (an	
   adult	
   male	
   singer	
   with	
   the	
   lowest	
   voice)
4. sea	
   bass,	
   bass	
   -      (flesh	
   of	
   lean-     fleshed	
   saltwater	
   fish	
   of	
   the	
   family	
   

serranidae)

5. freshwater	
   bass,	
   bass	
   -      (any	
   of	
   various	
   north	
   american	
   lean-     fleshed	
   

freshwater	
   fishes	
   especially	
   of	
   the	
   genus	
   micropterus)

6. bass,	
   bass	
   voice,	
   basso	
   -      (the	
   lowest	
   adult	
   male	
   singing	
   voice)
7. bass	
   -      (the	
   member	
   with	
   the	
   lowest	
   range	
   of	
   a	
   family	
   of	
   musical	
   

instruments)

8. bass	
   -      (nontechnical	
   name	
   for	
   any	
   of	
   numerous	
   edible	
   	
   marine	
   and	
   

freshwater	
   spiny-     finned	
   fishes)

inventory	
   of	
   sense	
   tags	
   for	
   bass

16.5

    supervised id51
roget

id138 spanish
sense
bass4
bass4
bass7
bass7
figure 16.5 possible de   nitions for the inventory of sense tags for bass.

fish/insect . . .    sh as paci   c salmon and striped bass and. . .
fish/insect . . . produce    lets of smoked bass or sturgeon. . .
. . . exciting jazz bass player since ray brown. . .
music
. . . play bass because he doesn   t have to solo. . .
music

translation category
lubina
lubina
bajo
bajo

target word in context

9

the set of senses are small, supervised machine learning approaches are often used
to handle lexical sample tasks. for each word, a number of corpus instances (con-
text sentences) can be selected and hand-labeled with the correct sense of the target

supervised	
   wsd	
   2:	
   get	
   a	
   corpus

    lexical	
   sample	
   task:

    line-     hard-     serve	
   corpus	
   -      4000	
   examples	
   of	
   each
    interest corpus	
   -      2369	
   sense-     tagged	
   examples

    all	
   words:

    semantic	
   concordance:	
   a	
   corpus	
   in	
   which	
   each	
   open-     class	
   word	
   is	
   labeled	
   

with	
   a	
   sense	
   from	
   a	
   specific	
   dictionary/thesaurus.
    semcor:	
   234,000	
   words	
   from	
   brown	
   corpus,	
   manually	
   tagged	
   with	
   

id138	
   senses

    senseval-     3	
   competition	
   corpora	
   -      2081	
   tagged	
   word	
   tokens

semcor

<wf pos=prp>he</wf>
<wf pos=vb	
   lemma=recognize	
   wnsn=4	
   lexsn=2:31:00::>recognized</wf>
<wf pos=dt>the</wf>
<wf pos=nn	
   lemma=gesture	
   wnsn=1	
   lexsn=1:04:00::>gesture</wf>
<punc>.</punc>

11

supervised	
   wsd	
   3:	
   extract	
   feature	
   vectors
intuition	
   from	
   warren	
   weaver	
   (1955):

   if	
   one	
   examines	
   the	
   words	
   in	
   a	
   book,	
   one	
   at	
   a	
   time	
   as	
   through	
   
an	
   opaque	
   mask	
   with	
   a	
   hole	
   in	
   it	
   one	
   word	
   wide,	
   then	
   it	
   is	
   
obviously	
   impossible	
   to	
   determine,	
   one	
   at	
   a	
   time,	
   the	
   meaning	
   
of	
   the	
   words   	
   
but	
   if	
   one	
   lengthens	
   the	
   slit	
   in	
   the	
   opaque	
   mask,	
   until	
   one	
   can	
   
see	
   not	
   only	
   the	
   central	
   word	
   in	
   question	
   but	
   also	
   say	
   n	
   words	
   
on	
   either	
   side,	
   then	
   if	
   n	
   is	
   large	
   enough	
   one	
   can	
   unambiguously	
   
decide	
   the	
   meaning	
   of	
   the	
   central	
   word   	
   
the	
   practical	
   question	
   is	
   :	
   ``what	
   minimum	
   value	
   of	
   n	
   will,	
   at	
   
least	
   in	
   a	
   tolerable	
   fraction	
   of	
   cases,	
   lead	
   to	
   the	
   correct	
   choice	
   
of	
   meaning	
   for	
   the	
   central	
   word?   

feature	
   vectors

    a	
   simple	
   representation	
   for	
   each	
   observation

(each	
   instance	
   of	
   a	
   target	
   word)
    vectors of	
   sets	
   of	
   feature/value	
   pairs
    represented	
   as	
   a	
   ordered	
   list	
   of	
   values
    these	
   vectors	
   represent,	
   e.g.,	
   the	
   window	
   of	
   words	
   around	
   
the	
   target

two	
   kinds	
   of	
   features	
   in	
   the	
   vectors

    collocational features	
   and	
   bag-     of-     words	
   features

    collocational

    bag-     of-     words

    features	
   about	
   words	
   at	
   specific positions	
   near	
   target	
   word

    often	
   limited	
   to	
   just	
   word	
   identity	
   and	
   pos

    features	
   about	
   words	
   that	
   occur	
   anywhere	
   in	
   the	
   window	
   (regardless	
   

of	
   position)
    typically	
   limited	
   to	
   frequency	
   counts

examples

    example	
   text	
   (wsj):

an	
   electric	
   guitar	
   and	
   bass player	
   stand	
   off	
   to	
   
one	
   side	
   not	
   really	
   part	
   of	
   the	
   scene
    assume	
   a	
   window	
   of	
   +/-      2	
   from	
   the	
   target

examples

    example	
   text	
   (wsj)

an	
   electric	
   guitar	
   and	
   bass player	
   stand	
   off	
   to	
   
one	
   side	
   not	
   really	
   part	
   of	
   the	
   scene,	
   
    assume	
   a	
   window	
   of	
   +/-      2	
   from	
   the	
   target

a given sense.
encoding local lexical and grammatical information that can often accurately isolate
for example consider the ambiguous word bass in the following wsj sentence:
a given sense.

collocational features

(16.17) an electric guitar and bass player stand off to one side, not really part of

for example consider the ambiguous word bass in the following wsj sentence:
(16.17) an electric guitar and bass player stand off to one side, not really part of
the scene, just as a sort of nod to gringo expectations perhaps.
the scene, just as a sort of nod to gringo expectations perhaps.
    position-     specific	
   information	
   about	
   the	
   words	
   and	
   
a collocational feature vector, extracted from a window of two words to the right
and left of the target word, made up of the words themselves, their respective parts-
a collocational feature vector, extracted from a window of two words to the right
collocations	
   in	
   window
of-speech, and pairs of words, that is,
and left of the target word, made up of the words themselves, their respective parts-
    guitar	
   and	
   bass player	
   stand
of-speech, and pairs of words, that is,
[wi 2,posi 2,wi 1,posi 1,wi+1,posi+1,wi+2,posi+2,wi 1
[wi 2,posi 2,wi 1,posi 1,wi+1,posi+1,wi+2,posi+2,wi 1

i 2,wi+1
i 2,wi+1

]
(16.18)

(16.18)

i
]

i

would yield the following vector:
would yield the following vector:

[guitar, nn, and, cc, player, nn, stand, vb, and guitar, player stand]

[guitar, nn, and, cc, player, nn, stand, vb, and guitar, player stand]

high performing systems generally use pos tags and word collocations of length
1, 2, and 3 from a window of words 3 to the left and 3 to the right (zhong and ng,
2010).

high performing systems generally use pos tags and word collocations of length
1, 2, and 3 from a window of words 3 to the left and 3 to the right (zhong and ng,

    word	
   1,2,3	
   grams	
   in	
   window	
   of	
     3	
   is	
   common

bag-     of-     words	
   features

       an	
   unordered	
   set	
   of	
   words   	
       position	
   ignored
    counts	
   of	
   words	
   occur	
   within	
   the	
   window.
    first	
   choose	
   a	
   vocabulary
    then	
   count	
   how	
   often	
   each	
   of	
   those	
   terms	
   occurs	
   in	
   a	
   

given	
   window
    sometimes	
   just	
   a	
   binary	
      indicator   	
   1	
   or	
   0

co-     occurrence	
   example

    assume	
   we   ve	
   settled	
   on	
   a	
   possible	
   vocabulary	
   of	
   12	
   words	
   in	
   

   bass   	
   sentences:	
   

[fishing,	
   big,	
   sound,	
   player,	
   fly,	
   rod,	
   pound,	
   double,	
   runs,	
   playing,	
   guitar,	
   band]	
   

    the	
   vector	
   for:

guitar and	
   bass player stand
[0,0,0,1,0,0,0,0,0,0,1,0]	
   

word	
   sense	
   
disambiguation

classification

dan	
   jurafsky

classification:	
   definition

    input:

    a	
   word	
   w	
   and	
   some	
   features	
   f
    a	
   fixed	
   set	
   of	
   classes	
   	
   c	
   = {c1,	
   c2,   ,	
   cj}

    output:	
   a	
   predicted	
   class	
   c   c

dan	
   jurafsky classification	
   methods:

supervised	
   machine	
   learning

    input:	
   

    a	
   word	
   w	
   in	
   a	
   text	
   window	
   d	
   (which	
   we   ll	
   call	
   a	
      document   )
    a	
   fixed	
   set	
   of	
   classes	
   	
   c	
   = {c1,	
   c2,   ,	
   cj}
    a	
   training	
   set	
   of	
   m hand-     labeled	
   text	
   windows	
   again	
   called	
   
   documents   	
   (d1,c1),....,(dm,cm)

    output:	
   

22

    a	
   learned	
   classifier	
     :d      c

dan	
   jurafsky classification	
   methods:

supervised	
   machine	
   learning

    any	
   kind	
   of	
   classifier

    naive bayes
    logistic	
   regression
    neural	
   networks
    support-     vector	
   machines
    k-     nearest	
   neighbors

       

applying	
   naive	
   bayes	
   to	
   wsd

    counting	
   in	
   a	
   labeled	
   training	
   set.

    p(c)	
   is	
   the	
   prior	
   id203	
   of	
   that	
   sense
    p(w|c)	
   	
   conditional	
   id203	
   of	
   a	
   word	
   given	
   a	
   particular	
   sense
    we	
   get	
   both	
   of	
   these	
   from	
   a	
   tagged	
   corpus	
   like	
   semcor

    p(w|c)	
   =	
   count(w,c)/count(c)

    can	
   also	
   generalize	
   to	
   look	
   at	
   other	
   features	
   besides	
   words.

    then	
   it	
   would	
   be	
   p(f|c)	
   

    conditional	
   id203	
   of	
   a	
   feature	
   given	
   a	
   sense

dan	
   jurafsky

  p(w |c) =

  p(c) =

nc
n
count(w,c) +1
count(c)+|v |

training

test

doc words
1
2
3
4
5

fish	
   smoked	
   fish
fish	
   line
fish	
   haul	
   smoked
guitar	
   jazz	
   line
line	
   guitar	
   jazz	
   jazz

class
f
f
f
g
?

v	
   =	
   {fish,	
   smoked,	
   line,	
   haul,	
   guitar,	
   jazz}

priors:
p(f)=	
   
p(g)=	
   

3
4 1
4

conditional	
   probabilities:
p(line|f)	
   =
p(guitar|f)	
   	
   	
   	
   =
p(jazz|f)	
   	
   	
   	
   	
   =
p(line|g)	
   =
p(guitar|g)	
   	
   	
   	
   	
   =
p(jazz|g)	
   	
   	
   	
   	
   	
   =	
   

(1+1)	
   /	
   (8+6)	
   =	
   2/14
(0+1)	
   /	
   (8+6)	
   =	
   1/14
(0+1)	
   /	
   (8+6)	
   =	
   1/14
(1+1)	
   /	
   (3+6)	
   =	
   2/9	
   
(1+1)	
   /	
   (3+6)	
   =	
   2/9	
   
(1+1)	
   /	
   (3+6)	
   =	
   2/9	
   

25

choosing	
   a	
   class:
p(f|d5)	
   

   

3/4	
   *	
   2/14	
   *	
   (1/14)2 *	
   1/14	
   

   	
   0.00003

p(g|d5)	
   

   

1/4	
   *	
   2/9	
   *	
   (2/9)2 *	
   2/9	
   

   	
   0.0006

word	
   sense	
   
disambiguation

evaluations	
   and	
   

baselines

wsd	
   evaluations	
   and	
   baselines

    best	
   evaluation:	
   extrinsic	
   (   end-     to-     end   ,	
   `task-     based   )	
   evaluation

    embed	
   wsd	
   algorithm	
   in	
   a	
   task	
   and	
   see	
   if	
   you	
   can	
   do	
   the	
   task	
   better!

    what	
   we	
   often	
   do	
   for	
   convenience:	
   intrinsic	
   evaluation

    exact	
   match	
   sense accuracy

    %	
   of	
   words	
   tagged	
   identically	
   with	
   the	
   human-     manual	
   sense	
   tags

    usually	
   evaluate	
   using	
   held-     out	
   data	
   from	
   same	
   labeled	
   corpus

    baselines

    most	
   frequent	
   sense
    the	
   lesk algorithm

most	
   frequent	
   sense

    id138	
   senses	
   are	
   ordered	
   in	
   frequency	
   order
    so	
      most	
   frequent	
   sense    in	
   id138	
   =	
      take	
   the	
   first	
   sense   
    sense	
   frequencies	
   come	
   from	
   the	
   semcor corpus

ceiling

    human	
   inter-     annotator	
   agreement
    compare	
   annotations	
   of	
   two	
   humans
    on	
   same	
   data
    given	
   same	
   tagging	
   guidelines

    human	
   agreements	
   on	
   all-     words	
   corpora	
   with	
   

id138	
   style	
   senses
    75%-     80%	
   

word	
   sense	
   
disambiguation

dictionary	
   and	
   

thesaurus	
   methods

overlap  computeoverlap(signature, context)
if overlap > max-overlap then

max-overlap overlap
best-sense sense
the	
   simplified	
   lesk algorithm

end
return(best-sense)

    let   s	
   disambiguate	
      bank    in	
   this	
   sentence:
figure 16.6 the simpli   ed lesk algorithm. the computeoverlap function returns the
number of words in common between two sets, ignoring function words or other words on a
the	
   bank can	
   guarantee	
   deposits	
   will	
   eventually	
   cover	
   future	
   tuition	
   costs	
   
stop list. the original lesk algorithm de   nes the context in a more complex way. the cor-
because	
   it	
   invests	
   in	
   adjustable-     rate	
   mortgage	
   securities.	
   
pus lesk algorithm weights each overlapping word w by its  logp(w) and includes labeled
    given	
   the	
   following	
   two	
   id138	
   senses:	
   
training corpus data in the signature.

bank1 gloss:

examples:

bank2 gloss:

examples:

a    nancial institution that accepts deposits and channels the
money into lending activities
   he cashed a check at the bank   ,    that bank holds the mortgage
on my home   
sloping land (especially the slope beside a body of water)
   they pulled the canoe up on the bank   ,    he sat on the bank of
the river and watched the currents   

overlap  computeoverlap(signature, context)
if overlap > max-overlap then

end
return(best-sense)

max-overlap overlap
the	
   simplified	
   lesk algorithm
best-sense sense
choose	
   sense	
   with	
   most	
   word	
   overlap	
   between	
   gloss	
   and	
   context
figure 16.6 the simpli   ed lesk algorithm. the computeoverlap function returns the
the	
   bank can	
   guarantee	
   deposits	
   will	
   eventually	
   cover	
   future	
   
number of words in common between two sets, ignoring function words or other words on a
stop list. the original lesk algorithm de   nes the context in a more complex way. the cor-
tuition	
   costs	
   because	
   it	
   invests	
   in	
   adjustable-     rate	
   mortgage	
   
pus lesk algorithm weights each overlapping word w by its  logp(w) and includes labeled
securities.	
   
training corpus data in the signature.

(not	
   counting	
   function	
   words)

bank1 gloss:

examples:

bank2 gloss:

examples:

a    nancial institution that accepts deposits and channels the
money into lending activities
   he cashed a check at the bank   ,    that bank holds the mortgage
on my home   
sloping land (especially the slope beside a body of water)
   they pulled the canoe up on the bank   ,    he sat on the bank of
the river and watched the currents   

the	
   corpus	
   lesk algorithm

    assumes	
   we	
   have	
   some	
   sense-     labeled	
   data	
   (like	
   semcor)
    take	
   all	
   the	
   sentences	
   with	
   the	
   relevant	
   word	
   sense:

these	
   short,	
   "streamlined"	
   meetings	
   usually	
   are	
   sponsored	
   by	
   local	
   banks1,	
   
chambers	
   of	
   commerce,	
   trade	
   associations,	
   or	
   other	
   civic	
   organizations.
    now	
   add	
   these	
   to	
   the	
   gloss	
   +	
   examples	
   for	
   each	
   sense,	
   call	
   it	
   the	
   

   signature   	
   of	
   a	
   sense.

    choose	
   sense	
   with	
   most	
   word	
   overlap	
   between	
   context	
   and	
   

signature.

corpus	
   lesk:	
   idf	
   weighting

   

instead	
   of	
   just	
   removing	
   function	
   words
    weigh	
   each	
   word	
   by	
   its	
   `promiscuity   	
   across	
   documents
    down-     weights	
   words	
   that	
   occur	
   in	
   every	
   `document   	
   (gloss,	
   example,	
   etc)
    these	
   are	
   generally	
   function	
   words,	
   but	
   is	
   a	
   more	
   fine-     grained	
   measure
    weigh	
   each	
   overlapping	
   word	
   by	
   inverse	
   document	
   frequency

34

corpus	
   lesk:	
   idf	
   weighting

    weigh	
   each	
   overlapping	
   word	
   by	
   inverse	
   document	
   frequency

    n	
   is	
   the	
   total	
   number	
   of	
   documents
    dfi =	
      document	
   frequency	
   of	
   word	
   i   
   

=	
   #	
   of	
   documents	
   with	
   word	
   i

idfi = log n
dfi

!
#
#
"

$
&
&
%

score(sensei, contextj) =

   

w     overlap(signaturei, contextj)

idfw

35

graph-     based	
   methods
    first,	
   id138	
   can	
   be	
   viewed	
   as	
   a	
   graph
    senses	
   are	
   nodes
    relations	
   (hypernymy,	
   meronymy)	
   are	
   edges
    also	
   add	
   edge	
   between	
   word	
   and	
   unambiguous	
   gloss	
   words

helpingn

1

foodn

1

liquidn

1

beveragen

1

1

milkn

toastn

4

drinkn

1

1

supv

consumev

1

consumern

1

1

drinkv
1

drinkern

1

sipv

drinkingn

1

36

consumptionn

1

1

sipn

potationn

1

how	
   to	
   use	
   the	
   graph	
   for	
   wsd

   

insert	
   target	
   word	
   and	
   words	
   in	
   its	
   sentential	
   context	
   into	
   the	
   
graph,	
   with	
   directed	
   edges	
   to	
   their	
   senses

   she	
   drank	
   some	
   milk   
    now	
   choose	
   the

most	
   central	
   sense
add	
   some	
   id203	
   to
   drink   	
   and	
      milk   	
   and	
   
compute	
   node	
   with
highest	
      id95   
37

drinkv

1

drinkv

2

drinkv

3

drinkv

4

drinkv

5

drinkn

1

beveragen

1

drinkern

1

foodn

1

boozingn

1

nutrimentn

1

   drink   

   milk   

milkn

1

milkn

2

milkn

3

milkn

4

word	
   sense	
   
disambiguation

semi-     supervised	
   

learning

semi-     supervised	
   learning

problem:	
   supervised	
   and	
   dictionary-     based	
   
approaches	
   require	
   large	
   hand-     built	
   resources

what	
   if	
   you	
   don   t	
   have	
   so	
   much	
   training	
   data?

solution:	
   id64

generalize	
   from	
   a	
   very	
   small	
   hand-     labeled	
   seed-     set.

id64

    for	
   bass

    rely	
   on	
      one	
   sense	
   per	
   collocation    rule

    a	
   word	
   reoccurring	
   in	
   collocation	
   with	
   the	
   same	
   word	
   will	
   almost	
   

surely	
   have	
   the	
   same	
   sense.

    the	
   word	
   play occurs	
   with	
   the	
   music	
   sense	
   of	
   bass	
   
    the	
   word	
   fish occurs	
   with	
   the	
   fish	
   sense	
   of	
   bass

figure 16.9 the yarowsky algorithm disambiguating    plant    at two stages;    ?    indicates an unlabeled ob-
servation, a and b are observations labeled as sense-a or sense-b. the initial stage (a) shows only seed
sentences l0 labeled by collocates (   life    and    manufacturing   ). an intermediate stage is shown in (b) where
more collocates have been discovered (   equipment   ,    microscopic   , etc.) and more instances in v0 have been
moved into l1, leaving a smaller unlabeled set v1. figure adapted from yarowsky (1995).

sentences	
   extracting	
   using	
      fish    and	
   
   play   

we need more good teachers     right now, there are only a half a dozen who can play
the free bass with ease.
an electric guitar and bass player stand off to one side, not really part of the scene, just
as a sort of nod to gringo expectations perhaps.
the researchers said the worms spend part of their life cycle in such    sh as paci   c
salmon and striped bass and paci   c rock   sh or snapper.
and it all started when    shermen decided the striped bass in lake mead were too
skinny.

figure 16.10 samples of bass sentences extracted from the wsj by using the simple cor-
relates play and    sh.

summary:	
   generating	
   seeds

1) hand	
   labeling
2)    one	
   sense	
   per	
   collocation   :

have	
   the	
   same	
   sense.

3)    one	
   sense	
   per	
   discourse   :

    a	
   word	
   reoccurring	
   in	
   collocation	
   with	
   the	
   same	
   word	
   will	
   almost	
   surely	
   

   
   

the	
   sense	
   of	
   a	
   word	
   is	
   highly	
   consistent	
   within	
   a	
   document	
   	
   -      yarowsky
(1995)
(at	
   least	
   for	
   non-     function	
   words,	
   and	
   especially	
   topic-     specific	
   words)

stages	
   in	
   the	
   yarowsky id64	
   
algorithm	
   for	
   the	
   word	
      plant   

?

?
?

?
?
?
??
?
?
?
?
??
?
?

?

?
a
a

a
a
?
?

?

?

?
?
?
?
?

?

?

?

?
?

?
??

?

?
?
?

?

life
?

?

?

?

?
?
?
?

?
??
?
?

?
?
a
?
a
a
a
a
a
aa
a
a
a
a
a
a
a
a
a

?
?

  0

?
?

?

?
?

?

?

?

?

?

?

?

?
?

?

?

?
?

?

?

?

?

?

?

?

?

?

?

?

?
?

?

?

?

?
?
?

?

??

?

?

?

?
?
?

?

?

?

?

?
?

?

?

?

?

?
?

?

?

v0

?
??

?

?

?
?

?
?
?
?

?

?

?
?

?

?

?

?
?
?

?
??
?

?

?

?
?
?

?
?

?

?

?

?
?

?

?
?

?

?

?

?

b

b
b
b

b
b

?
b
?
?
manufacturing

b
b
b
b
?

b

?

?
?

?
??

?
?

?

?

?
?
?
(a)

?
?
a

?

life
?

?

?

?

?
?
?
?

?
?a
a
a

?
?
a
?
a
a
a
a
a
aa
a
a
a
a
a
a
a
a
a

?

?
a
a
?
aa
a
?
?
?
?
?
??
?
?

?

?

?
a
a

a
a
?
?

?

?

?
?

?
?

?

a
?

?
?

?

  1

?

?

?
?

?
a
animal
?

?

?

?

?

a

?
?

?

?

?

?

?

?

?

?

?
?

?

?

?

?
?
?

?

?

?

?

??

?
?
?
?

?
?
?

?

?

?
?
?
?
?
?
?
microscopic
?

?
?
?

?

?

?
?

?

?

?
?

v1

?

?

?

?

?

?

?

?
?

?
employee
??

?
??
b

b

?

?
?

?

?

?

?

b

?
??

?

?

?
?

?

?

?
?

?
?
?

?

?
?

?

b

b
b
b

?
b
b
?
manufacturing

b
b
b
b
?

b
b

b

?

?

b

b

b
??

?

b
?
?
?
equipment
(b)

?

summary

    word	
   sense	
   disambiguation:	
   choosing	
   correct	
   sense	
   in	
   context
    applications:	
   mt,	
   qa,	
   etc.
    three	
   classes	
   of	
   methods

    supervised	
   machine	
   learning:	
   naive	
   bayes	
   classifier
    thesaurus/dictionary	
   methods
    semi-     supervised	
   learning

    main	
   intuition

    there	
   is	
   lots	
   of	
   information	
   in	
   a	
   word   s	
   context
    simple	
   algorithms	
   based	
   just	
   on	
   word	
   counts	
   can	
   be	
   surprisingly	
   good

44

