selectional
restrictions

selectional
restrictions

introduction

selectional restrictions

consider	
   the	
   two	
   interpretations	
   of:

i	
   want	
   to	
   eat	
   someplace	
   nearby.	
   

a) sensible:

eat	
   is	
   intransitive	
   and	
      someplace	
   nearby   	
   is	
   a	
   location	
   adjunct

b) speaker	
   is	
   godzilla

eat	
   is	
   transitive	
   and	
      someplace	
   nearby   	
   is	
   a	
   direct	
   object

how	
   do	
   we	
   know	
   speaker	
   didn   t	
   mean	
   b)	
   	
   ?

because	
   the	
   theme of	
   eating	
   tends	
   to	
   be	
   something	
   edible

3

selectional restrictions	
   are	
   associated	
   with	
   
senses

    the	
   restaurant	
   serves	
   green-     lipped	
   mussels.	
   

    theme is	
   some	
   kind	
   of	
   food

    which	
   airlines	
   serve	
   denver?	
   
    theme is	
   an	
   appropriate	
   location

4

selectional restrictions	
   vary	
   in	
   specificity

i	
   often	
   ask	
   the	
   musicians	
   to	
   imagine	
   a	
   tennis	
   game.
to	
   diagonalize a	
   matrix	
   is	
   to	
   find	
   its	
   eigenvalues.	
   
radon	
   is	
   an	
   odorless	
   gas	
   that	
   can   t	
   be	
   detected	
   by	
   human	
   senses.	
   

5

=> dish

=> food, nutrient

=> physical entity

=> substance
=> matter

=> nutriment, nourishment, nutrition...

instead	
   of	
   representing	
      eat   	
   as:

contribution of a verb like eat might look like the following:

of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. ignoring the issue of
the l-structures and using thematic roles rather than deep event roles, the semantic
9e,x,y eating(e)^ agent(e,x)^ t heme(e,y)
contribution of a verb like eat might look like the following:

representing	
   selectional restrictions

with this representation, all we know about y, the    ller of the theme role, is that
it is associated with an eating event through the theme relation. to stipulate the
selectional restriction that y must be something edible, we simply add a new term to
that effect:

9e,x,y eating(e)^ agent(e,x)^ theme(e,y)

with this representation, all we know about y, the    ller of the theme role, is that
it is associated with an eating event through the theme relation. to stipulate the
selectional restriction that y must be something edible, we simply add a new term to
that effect:

figure 22.6 evidence from id138 that hamburgers are edible.
just	
   add:
9e,x,y eating(e)^ agent(e,x)^ t heme(e,y)^ ediblet hing(y)
when a phrase like ate a hamburger is encountered, a semantic analyzer can
form the following kind of representation:
and	
      eat	
   a	
   hamburger   	
   becomes
9e,x,y eating(e)^ eater(e,x)^ t heme(e,y)^ ediblet hing(y)^ hamburger(y)
this representation is perfectly reasonable since the membership of y in the category
hamburger is consistent with its membership in the category ediblething, assuming
a reasonable set of facts in the knowledge base. correspondingly, the representation

9e,x,y eating(e)^ agent(e,x)^ theme(e,y)^ ediblething(y)

but	
   this	
   assumes	
   we	
   have	
   a	
   large	
   knowledge	
   base	
   of	
   facts	
   
about	
   edible	
   things	
   and	
   hamburgers	
   and	
   whatnot.

=> entity

6

let   s	
   use	
   id138 synsets to	
   specify	
   
selectional restrictions

    the	
   theme of	
   eat	
   must	
   be id138 synset {food,	
   nutrient}	
   
   any	
   substance	
   that	
   can	
   be	
   metabolized	
   by	
   an	
   animal	
   to	
   give	
   energy	
   and	
   build	
   tissue   
    similarly

theme of	
   imagine:	
   synset {entity}
theme of	
   lift:	
   synset {physical	
   entity}
theme of	
   diagonalize:	
   synset {matrix}	
   

    this	
   allows

imagine	
   a	
   hamburger	
   	
   and	
   	
   lift	
   a	
   hamburger,	
   

    correctly	
   rules	
   out	
   

7

diagonalize a	
   hamburger.	
   

selectional
restrictions

selectional preferences

selectional preferences

in	
   early	
   implementations,	
   selectional restrictions	
   were	
   strict	
   
constraints	
   (katz	
   and	
   fodor	
   1963)
    eat	
   [+food]

    but	
   it	
   was	
   quickly	
   realized	
   selectional constraints	
   are	
   really	
   

preferences (wilks 1975)
    but	
   it	
   fell	
   apart	
   in	
   1931,	
   perhaps	
   because	
   people	
   realized	
   you	
   can   t	
   eat	
   gold	
   

for	
   lunch	
   if	
   you   re	
   hungry.	
   

    in	
   his	
   two	
   championship	
   trials,	
   mr.	
   kulkarni	
   ate	
   glass	
   on	
   an	
   empty	
   stomach,	
   

accompanied	
   only	
   by	
   water	
   and	
   tea.	
   

   

9

selectional association	
   (resnik 1993)

    selectional preference	
   strength:	
   amount	
   of	
   information	
   that	
   a	
   

predicate	
   tells	
   us	
   about	
   the	
   semantic	
   class	
   of	
   its	
   arguments.	
   
    eat	
   tells	
   us	
   a	
   lot	
   about	
   the	
   semantic	
   class	
   of	
   its	
   direct	
   objects
    be	
   doesn   t	
   tell	
   us	
   much

    the	
   selectional preference	
   strength	
   

    difference	
   in	
   information	
   between	
   two	
   distributions:	
   

p(c)	
   the	
   distribution	
   of	
   expected	
   semantic	
   classes	
   for	
   any	
   direct	
   object
p(c|v)	
   the	
   distribution	
   of	
   expected	
   semantic	
   classes	
   for	
   this	
   verb

    the	
   greater	
   the	
   difference,	
   the	
   more	
   the	
   verb	
   is	
   constraining	
   its	
   object

10

relative id178
kl divergence

relative id178
kl divergence

it that the direct object of the speci   c verb v will fall into semantic class c). the
greater the difference between these distributions, the more information the verb is
giving us about possible objects. the difference between these two distributions can
be quanti   ed by relative id178, or the id181 (kullback and
leibler, 1951). the kullback-leibler or kl divergence d(p||q) expresses the dif-
    relative	
   id178,	
   or	
   the	
   kullback-     leibler divergence	
   is	
   the	
   difference	
   
ference between two id203 distributions p and q (we   ll return to this when we
discuss distributional models of meaning in chapter 17).

it that the direct object of the speci   c verb v will fall into semantic class c). the
greater the difference between these distributions, the more information the verb is
giving us about possible objects. the difference between these two distributions can
selectional preference	
   strength
be quanti   ed by relative id178, or the id181 (kullback and
leibler, 1951). the kullback-leibler or kl divergence d(p||q) expresses the dif-
ference between two id203 distributions p and q (we   ll return to this when we
discuss distributional models of meaning in chapter 17).

between	
   two	
   distributions

d(p||q) = xx
d(p||q) = xx

p(x)log
p(x)log

p(x)
q(x)

p(x)
q(x)

(22.38)

(22.38)

   

   

selectional preference:	
   how	
   much	
   information	
   (in	
   bits)	
   the	
   verb	
   expresses	
   
about	
   the	
   semantic	
   class	
   of	
   its	
   argument

the selectional preference sr(v) uses the kl divergence to express how much
information, in bits, the verb v expresses about the possible semantic class of its
the selectional preference sr(v) uses the kl divergence to express how much
argument.
information, in bits, the verb v expresses about the possible semantic class of its
argument.

sr(v) = d(p(c|v)||p(c))
p(c|v)
p(c|v)log
sr(v) = d(p(c|v)||p(c))
p(c)
selectional association	
   of	
   a	
   verb	
   with	
   a	
   class:	
   the	
   relative	
   contribution	
   of	
   the	
   
p(c|v)
p(c|v)log
p(c)
class	
   to	
   the	
   general	
   preference	
   of	
   the	
   verb
1
p(c|v)
p(c)

as the relative contribution of that class to the general selectional preference of the
(22.39)
verb:

(22.40)
resnik then de   nes the selectional association of a particular class and verb

resnik then de   nes the selectional association of a particular class and verb

    selectional restrictions
(22.39)

= xc
= xc

p(c|v)log

ar(v,c) =

22.7

15

sr(v)

selectional
association

11
selectional
association

22.7

    selectional restrictions

as the relative contribution of that class to the general selectional preference of the
verb:

computing	
   selectional association
p(c|v)log

    a	
   probabilistic	
   measure	
   of	
   the	
   strength	
   of	
   association	
   between	
   a	
   

sr(v)
predicate	
   and	
   a	
   semantic	
   class	
   of	
   its	
   argument
    parse	
   a	
   corpus
    count	
   all	
   the	
   times	
   each	
   predicate	
   appears	
   with	
   each	
   argument	
   word
    assume	
   each	
   word	
   is	
   a	
   partial	
   observation	
   of	
   all	
   the	
   id138 concepts	
   

the selectional association is thus a probabilistic measure of the strength of as-
sociation between a predicate and a class dominating the argument to the predicate.
resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the id138 concepts containing the
word. the following table from resnik (1996) shows some sample high and low
associated	
   with	
   that	
   word
selectional associations for verbs and some id138 semantic classes of their direct
objects.

    some	
   high	
   and	
   low	
   associations:

p(c|v)
p(c)

ar(v,c) =

1

12

verb
read
write
see

direct object
semantic class assoc
writing
writing
entity

6.80
7.26
5.79

direct object
semantic class assoc
activity
commerce
method

-.20
0
-0.01

results	
   from	
   similar	
   models

   s  aghdha and	
   korhonen (2012)

eat
drink
appoint
publish

food#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2
   uid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1
individual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1
abstract entity#n#1, piece of writing#n#1, communication#n#2, publication#n#1

table 2: most probable cuts learned by wn-cut for the object argument of selected verbs

13

verb-object

seen

r

 

unseen
r
 

noun-noun

adjective-noun

seen

r

 

unseen
r
 

seen

r

 

unseen
r
 

an alternative to using selectional association between a verb and the id138 class
instead	
   of	
   using	
   classes,
of its arguments, is to simply use the id155 of an argument word
a	
   simpler	
   model	
   of	
   selectional association
given a predicate verb. this simple model of selectional preferences can be used to
directly modeling the strength of association of one verb (predicate) with one noun
(argument).

    model	
   just	
   the	
   association	
   of	
   predicate	
   v with	
   a	
   noun	
   n
(one	
   noun,	
   as	
   opposed	
   to	
   the	
   whole	
   semantic	
   class	
   in	
   id138)
    parse	
   a	
   huge	
   corpus
    count	
   how	
   often	
   a	
   noun	
   n	
   occurs	
   in	
   relation	
   r	
   with	
   verb	
   v:

the id155 model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. the id155 of an
argument noun given a verb for a particular relation p(n|v,r) can then be used as a
selectional preference metric for that pair of words (brockmann and lapata, 2003):

log count(n,v,r)

    or	
   the	
   id203:

p(n|v,r) =( c(n,v,r)

c(v,r)

if c(n,v,r) > 0

0 otherwise

the inverse id203 p(v|n,r) was found to have better performance in some

14

cases (brockmann and lapata, 2003):

evaluation	
   from	
   bergsma,	
   lin,	
   goebel

verb
see
read
   nd
hear
write
urge
warn
judge
teach
show
expect
answer
recognize
repeat
understand
remember

plaus./implaus.
friend/method
article/fashion
label/fever
story/issue
letter/market
daughter/contrast
driver/engine
contest/climate
language/distance
sample/travel
visit/mouth
request/tragedy
author/pocket
comment/journal
concept/session
reply/smoke

resnik
5.79/-0.01
6.80/-0.20
1.10/0.22
1.89/1.89*
7.26/0.00
1.14/1.86*
4.73/3.61
1.30/0.28
1.87/1.86
1.44/0.41
0.59/5.93*
4.49/3.88
0.50/0.50*
1.23/1.23*
1.52/1.51
1.31/0.20

dagan et al. erk
0.20/1.40*
3.00/0.11
1.50/2.20*
0.66/1.50*
2.50/-0.43
0.14/1.60*
1.20/0.05
1.50/1.90*
2.50/1.30
1.60/0.14
1.40/1.50*
2.70/1.50
0.03/0.37*
2.30/1.40
2.70/0.25
2.10/1.20

0.46/-0.07
3.80/1.90
0.59/0.01
2.00/2.60*
3.60/-0.24
1.10/3.60*
2.30/0.62
1.70/1.70*
3.60/2.70
0.40/-0.82
1.40/0.37
3.10/-0.64
0.77/1.30*
2.90/   
2.00/-0.28
0.54/2.60*

mi
1.11/-0.57
4.00/   
0.42/0.07
2.99/-1.03
5.06/-4.12
-0.95/    -0.34/-0.62
2.87/   
3.90/   
3.53/   
0.53/-0.49
1.05/-0.65
2.93/   
0.48/   
2.59/   
3.96/   
1.13/-0.06

table 2: selectional ratings for plausible/implausible direct objects (holmes et al., 1989). mistakes are marked with

15

selectional
restrictions

conclusion

summary:	
   selectional restrictions

    two	
   classes	
   of	
   models	
   of	
   the	
   semantic	
   type	
   constraint	
   that	
   a	
   

predicate	
   places	
   on	
   its	
   argument:
    represent	
   the	
   constraint	
   between	
   predicate	
   and	
   id138 class
    represent	
   the	
   constraint	
   between	
   predicate	
   and	
   a	
   word

    one	
   fun	
   recent	
   use	
   case:	
   detecting	
   metonomy (type	
   coercion)
pustejovsky et	
   al	
   (2010)

    coherent	
   with	
   selectional restrictions:

the	
   spokesman	
   denied	
   the	
   statement	
   (proposition).	
   
the	
   child	
   threw	
   the	
   stone	
   (physical	
   object)	
   

    coercion:

17

the	
   president	
   denied	
   the	
   attack	
   (event	
      	
   proposition).	
   
the	
   white	
   house	
   (location	
      	
   human)	
   denied	
   the	
   statement.	
   

