language	
   
modeling

introduction	
   to	
   n-     grams

dan	
   jurafsky

probabilistic	
   language	
   models

    today   s	
   goal:	
   assign	
   a	
   id203	
   to	
   a	
   sentence

why?

    machine	
   translation:

    p(high	
   winds	
   tonite)	
   >	
   p(large winds	
   tonite)

    spell	
   correction

    the	
   office	
   is	
   about	
   fifteen	
   minuets from	
   my	
   house

    p(about	
   fifteen	
   minutes from)	
   >	
   p(about	
   fifteen	
   minuets from)

    speech	
   recognition

    p(i	
   saw	
   a	
   van)	
   >>	
   p(eyes	
   awe	
   of	
   an)

    +	
   summarization,	
   question-     answering,	
   etc.,	
   etc.!!

dan	
   jurafsky

probabilistic	
   language	
   modeling

    goal:	
   compute	
   the	
   id203	
   of	
   a	
   sentence	
   or	
   

sequence	
   of	
   words:

p(w)	
   =	
   p(w1,w2,w3,w4,w5   wn)

    related	
   task:	
   id203	
   of	
   an	
   upcoming	
   word:
    a	
   model	
   that	
   computes	
   either	
   of	
   these:

p(w5|w1,w2,w3,w4)

p(w)	
   	
   	
   	
   	
   or	
   	
   	
   	
   	
   p(wn|w1,w2   wn-     1)	
   	
   	
   	
   	
   	
   	
   	
   	
    is	
   called	
   a	
   language	
   model.
    better:	
   the	
   grammar	
   	
   	
   	
   	
   	
   	
   but	
   language	
   model	
   or	
   lm	
   is	
   standard

dan	
   jurafsky

how	
   to	
   compute	
   p(w)
    how	
   to	
   compute	
   this	
   joint	
   id203:

    p(its,	
   water,	
   is,	
   so,	
   transparent,	
   that)

   

intuition:	
   let   s	
   rely	
   on	
   the	
   chain	
   rule	
   of	
   id203

dan	
   jurafsky

reminder:	
   the	
   chain	
   rule

    recall	
   the	
   definition	
   of	
   conditional	
   probabilities
rewriting:	
   	
   	
   p(a,b)	
   =	
   p(a)p(b|a)

p(b|a)	
   =	
   p(a,b)/p(a)

    more	
   variables:

p(a,b,c,d)	
   =	
   p(a)p(b|a)p(c|a,b)p(d|a,b,c)

    the	
   chain	
   rule	
   in	
   general
p(x1,x2,x3,   ,xn)	
   =	
   p(x1)p(x2|x1)p(x3|x1,x2)   p(xn|x1,   ,xn-     1)

dan	
   jurafsky the	
   chain	
   rule	
   applied	
   to	
   compute	
   

joint	
   id203	
   of	
   words	
   in	
   sentence

p(w1w2   wn) =
  

   
i

p(wi | w1w2   wi   1)

    

p(   its	
   water	
   is	
   so	
   transparent   )	
   =

p(its)	
      p(water|its)	
      p(is|its water)	
   

   p(so|its water	
   is)	
      p(transparent|its water	
   is	
   

so)

dan	
   jurafsky

how	
   to	
   estimate	
   these	
   probabilities

    could	
   we	
   just	
   count	
   and	
   divide?

p(the |its water is so transparent that) =
count(its water is so transparent that the)
count(its water is so transparent that)

    no!	
   	
   too	
   many	
   possible	
   sentences!
    we   ll	
   never	
   see	
   enough	
   data	
   for	
   estimating	
   these

dan	
   jurafsky

markov	
   assumption
    simplifying	
   assumption:
p(the |its water is so transparent that)     p(the |that)
    or	
   maybe
p(the |its water is so transparent that)     p(the |transparent that)

andrei	
   markov

dan	
   jurafsky

markov	
   assumption

p(w1w2   wn)    
  

   
i

p(wi | wi   k   wi   1)

    in	
   other	
   words,	
   we	
   approximate	
   each	
   
component	
   in	
   the	
   product
p(wi | w1w2   wi   1)     p(wi | wi   k   wi   1)
  

dan	
   jurafsky

simplest	
   case:	
   unigram	
   model
p(wi)
p(w1w2   wn)    
  

   
i

some	
   automatically	
   generated	
   sentences	
   from	
   a	
   unigram	
   model

fifth, an, of, futures, the, an, incorporated, a, 
a, the, inflation, most, dollars, quarter, in, is, 
mass

thrift, did, eighty, said, hard, 'm, july, bullish

    

that, or, limited, the

dan	
   jurafsky

bigram	
   model
condition	
   on	
   the	
   previous	
   word:

p(wi | w1w2   wi   1)     p(wi | wi   1)
  

texaco, rose, one, in, this, issue, is, pursuing, growth, in, 
a, boiler, house, said, mr., gurria, mexico, 's, motion, 
control, proposal, without, permission, from, five, hundred, 
fifty, five, yen

outside, new, car, parking, lot, of, the, agreement, reached

this, would, be, a, record, november

dan	
   jurafsky

n-     gram	
   models

    we	
   can	
   extend	
   to	
   trigrams,	
   4-     grams,	
   5-     grams
    in	
   general	
   this	
   is	
   an	
   insufficient	
   model	
   of	
   language

    because	
   language	
   has	
   long-     distance	
   dependencies:
   the	
   computer(s)	
   which	
   i	
   had	
   just	
   put	
   into	
   the	
   machine	
   room	
   
on	
   the	
   fifth	
   floor	
   is	
   (are)	
   crashing.   

    but	
   we	
   can	
   often	
   get	
   away	
   with	
   n-     gram	
   models

language	
   
modeling

introduction	
   to	
   n-     grams

language	
   
modeling

estimating	
   n-     gram	
   

probabilities

dan	
   jurafsky

estimating	
   bigram	
   probabilities

    the	
   maximum	
   likelihood	
   estimate

p(wi | wi   1) =

p(wi | wi   1) =

count(wi   1,wi)
count(wi   1)
c(wi   1,wi)
c(wi   1)

    

dan	
   jurafsky

an	
   example

p(wi | wi   1) =

c(wi   1,wi)
c(wi   1)

<s>	
   i	
   am	
   sam	
   </s>
<s>	
   sam	
   i	
   am	
   </s>
<s>	
   i	
   do	
   not	
   like	
   green	
   eggs	
   and	
   ham	
   </s>

dan	
   jurafsky more	
   examples:	
   

berkeley	
   restaurant	
   project	
   sentences

tell	
   me	
   about	
   chez	
   panisse

    can	
   you	
   tell	
   me	
   about	
   any	
   good	
   cantonese restaurants	
   close	
   by
    mid	
   priced	
   thai food	
   is	
   what	
   i   m looking	
   for
   
    can	
   you	
   give	
   me	
   a	
   listing	
   of	
   the	
   kinds	
   of	
   food	
   that	
   are	
   available
   
i   m looking	
   for	
   a	
   good	
   place	
   to	
   eat	
   breakfast
    when	
   is	
   caffe venezia open	
   during	
   the	
   day

dan	
   jurafsky

raw	
   bigram	
   counts

    out	
   of	
   9222	
   sentences

dan	
   jurafsky

raw	
   bigram	
   probabilities

    normalize	
   by	
   unigrams:

    result:

dan	
   jurafsky

bigram	
   estimates	
   of	
   sentence	
   probabilities

p(<s>	
   i	
   want	
   english food	
   </s>)	
   =

p(i|<s>)	
   	
   	
   
   p(want|i)	
   	
   
   p(english|want)	
   	
   	
   
   p(food|english)	
   	
   	
   
   p(</s>|food)
=	
   	
   .000031

dan	
   jurafsky

what	
   kinds	
   of	
   knowledge?

    p(english|want)	
   	
   =	
   .0011
    p(chinese|want)	
   =	
   	
   .0065
    p(to|want)	
   =	
   .66
    p(eat	
   |	
   to)	
   =	
   .28
    p(food	
   |	
   to)	
   =	
   0
    p(want	
   |	
   spend)	
   =	
   0
    p	
   (i |	
   <s>)	
   =	
   .25

dan	
   jurafsky

practical	
   issues

    we	
   do	
   everything	
   in	
   log	
   space

    avoid	
   underflow
    (also	
   adding	
   is	
   faster	
   than	
   multiplying)

log(p1    p2    p3    p4) = log p1 +log p2 +log p3 +log p4

dan	
   jurafsky

language	
   modeling	
   toolkits

    srilm

    kenlm

    http://www.speech.sri.com/projects/srilm/

    https://kheafield.com/code/kenlm/

dan	
   jurafsky

google	
   n-     gram	
   release,	
   august	
   2006

   

dan	
   jurafsky

google	
   n-     gram	
   release

    serve as the incoming 92
    serve as the incubator 99
    serve as the independent 794
    serve as the index 223
    serve as the indication 72
    serve as the indicator 120
    serve as the indicators 45
    serve as the indispensable 111
    serve as the indispensible 40
    serve as the individual 234

http://googleresearch.blogspot.com/2006/08/all-our-id165-are-belong-to-you.html

dan	
   jurafsky

google	
   book	
   n-     grams

    http://ngrams.googlelabs.com/

language	
   
modeling

estimating	
   n-     gram	
   

probabilities

language	
   
modeling

evaluation	
   and	
   

perplexity

dan	
   jurafsky

evaluation:	
   how	
   good	
   is	
   our	
   model?

    does	
   our	
   language	
   model	
   prefer	
   good	
   sentences	
   to	
   bad	
   ones?
    assign	
   higher	
   id203	
   to	
      real   	
   or	
      frequently	
   observed   	
   sentences	
   

    than	
      ungrammatical   	
   or	
      rarely	
   observed   	
   sentences?
    we	
   train	
   parameters	
   of	
   our	
   model	
   on	
   a	
   training	
   set.
    we	
   test	
   the	
   model   s	
   performance	
   on	
   data	
   we	
   haven   t	
   seen.
    a	
   test	
   set	
   is	
   an	
   unseen	
   dataset	
   that	
   is	
   different	
   from	
   our	
   training	
   set,	
   

totally	
   unused.

    an	
   evaluation	
   metric	
   tells	
   us	
   how	
   well	
   our	
   model	
   does	
   on	
   the	
   test	
   set.

dan	
   jurafsky

training	
   on	
   the	
   test	
   set

    we	
   can   t	
   allow	
   test	
   sentences	
   into	
   the	
   training	
   set
    we	
   will	
   assign	
   it	
   an	
   artificially	
   high	
   id203	
   when	
   we	
   set	
   it	
   in	
   

the	
   test	
   set

       training	
   on	
   the	
   test	
   set   
    bad	
   science!
    and	
   violates	
   the	
   honor	
   code

30

dan	
   jurafsky

extrinsic	
   evaluation	
   of	
   n-     gram	
   models

    best	
   evaluation	
   for	
   comparing	
   models	
   a	
   and	
   b

    put	
   each	
   model	
   in	
   a	
   task

    spelling	
   corrector,	
   speech	
   recognizer,	
   mt	
   system

    run	
   the	
   task,	
   get	
   an	
   accuracy	
   for	
   a	
   and	
   for	
   b

    how	
   many	
   misspelled	
   words	
   corrected	
   properly
    how	
   many	
   words	
   translated	
   correctly

    compare	
   accuracy	
   for	
   a	
   and	
   b

dan	
   jurafsky difficulty	
   of	
   extrinsic	
   (in-     vivo)	
   evaluation	
   

of	
   	
   n-     gram	
   models

    extrinsic	
   evaluation
    so

    time-     consuming;	
   can	
   take	
   days	
   or	
   weeks

    sometimes	
   use	
   intrinsic evaluation:	
   perplexity
    bad	
   approximation	
   

    unless	
   the	
   test	
   data	
   looks	
   just like	
   the	
   training	
   data
    so	
   generally	
   only	
   useful	
   in	
   pilot	
   experiments

    but	
   is	
   helpful	
   to	
   think	
   about.

dan	
   jurafsky

intuition	
   of	
   perplexity

    the	
   shannon	
   game:

    how	
   well	
   can	
   we	
   predict	
   the	
   next	
   word?
i	
   always	
   order	
   pizza	
   with	
   cheese	
   and	
   ____
the	
   33rd president	
   of	
   the	
   us	
   was	
   ____
i	
   saw	
   a	
   ____

    unigrams	
   are	
   terrible	
   at	
   this	
   game.	
   	
   (why?)

    a	
   better	
   model	
   of	
   a	
   text

mushrooms   0.1
pepperoni   0.1
anchovies   0.01
   .
fried   rice   0.0001
   .
and   1e-  100

   

is	
   one	
   which	
   assigns	
   a	
   higher	
   id203	
   to	
   the	
   word	
   that	
   actually	
   occurs

dan	
   jurafsky

perplexity

the	
   best	
   language	
   model	
   is	
   one	
   that	
   best	
   predicts	
   an	
   unseen	
   test	
   set

    gives	
   the	
   highest	
   p(sentence)
perplexity	
   is	
   the	
   inverse	
   id203	
   of	
   
the	
   test	
   set,	
   normalized	
   by	
   the	
   number	
   
of	
   words:

pp(w) = p(w1w2...wn )   

1
n

            =

n

1

p(w1w2...wn )

chain	
   rule:

for	
   bigrams:

minimizing	
   perplexity	
   is	
   the	
   same	
   as	
   maximizing	
   id203

dan	
   jurafsky

perplexity	
   as	
   branching	
   factor

    let   s	
   suppose	
   a	
   sentence	
   consisting	
   of	
   random	
   digits
    what	
   is	
   the	
   perplexity	
   of	
   this	
   sentence	
   according	
   to	
   a	
   model	
   

that	
   assign	
   p=1/10	
   to	
   each	
   digit?

dan	
   jurafsky

lower	
   perplexity	
   =	
   better	
   model

    training	
   38	
   million	
   words,	
   test	
   1.5	
   million	
   words,	
   wsj

n-     gram	
   
order
perplexity 962

unigram bigram trigram

170

109

language	
   
modeling

evaluation	
   and	
   

perplexity

language	
   
modeling

generalization	
   and	
   

zeros

dan	
   jurafsky

the	
   shannon	
   visualization	
   method

   

choose	
   a	
   random	
   bigram	
   
(<s>,	
   w)	
   according	
   to	
   its	
   id203
    now	
   choose	
   a	
   random	
   bigram	
   	
   	
   	
   	
   	
   	
   	
   
(w,	
   x)	
   according	
   to	
   its	
   id203
and	
   so	
   on	
   until	
   we	
   choose	
   </s>
then	
   string	
   the	
   words	
   together

   
   

<s> i

i want

want to

to eat

eat chinese

chinese food

food  </s>

i want to eat chinese food

dan	
   jurafsky

bigrams by    rst generating a random bigram that starts with <s> (according to its
bigram id203), then choosing a random bigram to follow (again, according to
its bigram id203), and so on.

approximating	
   shakespeare

to give an intuition for the increasing power of higher-order id165s, fig. 4.3
shows random sentences generated from unigram, bigram, trigram, and 4-gram
models trained on shakespeare   s works.

gram

gram

rote life have
   hill he late speaks; or! a more to leg less    rst you enter

king. follow.
   what means, sir. i confess she? then all sorts, he is trim, captain.

1    to him swallowed confess hear both. which. of save on trail for are ay device and
2    why dost stand forth thy canopy, forsooth; he is this palpable hit the king henry. live
3    fly, and will rid me these news of price. therefore the sadness of parting, as they say,
4    king henry. what! i will go seek the traitor gloucester. exeunt some of the watch. a

   tis done.
   this shall forbid it should be branded, if renown made it empty.

great banquet serv   d in;
   it cannot be but so.

gram

gram

figure 4.3 eight sentences randomly generated from four id165s computed from shakespeare   s works. all
characters were mapped to lower-case and punctuation marks were treated as words. output is hand-corrected
for capitalization to improve readability.

dan	
   jurafsky

shakespeare	
   as	
   corpus
    n=884,647	
   tokens,	
   v=29,066
    shakespeare	
   produced	
   300,000	
   bigram	
   types	
   
out	
   of	
   v2=	
   844	
   million	
   possible	
   bigrams.
    so	
   99.96%	
   of	
   the	
   possible	
   bigrams	
   were	
   never	
   seen	
   
(have	
   zero	
   entries	
   in	
   the	
   table)
    quadrigrams worse:	
   	
   	
   what's	
   coming	
   out	
   looks	
   
like	
   shakespeare	
   because	
   it	
   is shakespeare

dan	
   jurafsky the	
   wall	
   street	
   journal	
   is	
   not	
   shakespeare	
   

(no	
   offense)

4.3

    generalization and zeros

11

gram

were recession exchange new endorsed a acquire to six executives

1 months the my and issue of year foreign new exchange   s september
2 last december through the way to preserve the hudson corporation n.

b. e. c. taylor would seem to complete the major central planners one
point    ve percent of u. s. e. has already old m. x. corporation of living
on information such as more frequently    shing to keep her

gram

3 they also point to ninety nine point six billion dollars from two hundred

four oh six three percent of the rates of interest stores as mexico and
brazil on market conditions

gram

figure 4.4 three sentences randomly generated from three id165 models computed from
40 million words of the wall street journal, lower-casing all characters and treating punctua-
tion as words. output was then hand-corrected for capitalization to improve readability.

dan	
   jurafsky can	
   you	
   guess	
   the	
   author	
   of	
   these	
   random	
   

3-     gram	
   sentences?

    they	
   also	
   point	
   to	
   ninety	
   nine	
   point	
   six	
   billion	
   dollars	
   from	
   two	
   
hundred	
   four	
   oh	
   six	
   three	
   percent	
   of	
   the	
   rates	
   of	
   interest	
   stores	
   
as	
   mexico	
   and	
   gram	
   brazil	
   on	
   market	
   conditions	
   

    this	
   shall	
   forbid	
   it	
   should	
   be	
   branded,	
   if	
   renown	
   made	
   it	
   empty.	
   
       you	
   are	
   uniformly	
   charming!   	
   cried	
   he,	
   with	
   a	
   smile	
   of	
   

associating	
   and	
   now	
   and	
   then	
   i	
   bowed	
   and	
   they	
   perceived	
   a	
   
chaise	
   and	
   four	
   to	
   wish	
   for.	
   

43

dan	
   jurafsky

the	
   perils	
   of	
   overfitting

    n-     grams	
   only	
   work	
   well	
   for	
   word	
   prediction	
   if	
   the	
   test	
   

corpus	
   looks	
   like	
   the	
   training	
   corpus
    in	
   real	
   life,	
   it	
   often	
   doesn   t
    we	
   need	
   to	
   train	
   robust	
   models that	
   generalize!
    one	
   kind	
   of	
   generalization:	
   zeros!

    things	
   that	
   don   t	
   ever	
   occur	
   in	
   the	
   training	
   set

    but	
   occur	
   in	
   the	
   test	
   set

dan	
   jurafsky

zeros
    training	
   set:

   	
   denied	
   the	
   allegations
   	
   denied	
   the	
   reports
   	
   denied	
   the	
   claims
   	
   denied	
   the	
   request

    test	
   set

   	
   denied	
   the	
   offer
   	
   denied	
   the	
   loan

p(   offer   	
   |	
   denied	
   the)	
   =	
   0

dan	
   jurafsky

zero	
   id203	
   bigrams

    bigrams	
   with	
   zero	
   id203

    mean	
   that	
   we	
   will	
   assign	
   0	
   id203	
   to	
   the	
   test	
   set!

    and	
   hence	
   we	
   cannot	
   compute	
   perplexity	
   (can   t	
   divide	
   by	
   0)!

language	
   
modeling

generalization	
   and	
   

zeros

language	
   
modeling

smoothing:	
   add-     one	
   
(laplace)	
   smoothing

dan	
   jurafsky

the   intuition   of   smoothing   (from   dan   klein)

    when	
   we	
   have	
   sparse	
   statistics:

   

p(w	
   |	
   denied	
   the)
3	
   allegations
2	
   reports
1	
   claims
1	
   request
7	
   total

steal	
   id203	
   mass	
   to	
   generalize	
   better

p(w	
   |	
   denied	
   the)
2.5	
   allegations
1.5	
   reports
0.5	
   claims
0.5	
   request
2	
   other
7	
   total

s
n
o
i
t
a
g
e
l
l
a

s
n
s
o
n
i
o
t
i
a
t
g
a
e
g
l
e
l
a
l
l
a

s
t
r
o
p
e
r

s
t
r
o
p
e
r

k
c
a

t
t

a

n
a
m

e
m
o
c
t

u
o

   

s
m
i
a
l
c

t
s
e
u
q
e
r

k
c
a
t
t

a

n
a
m

e
m
o
c
t

u
o

   

s
m
i
a
l
c

t
s
e
u
q
e
r

dan	
   jurafsky

add-     one	
   estimation
    also	
   called	
   laplace	
   smoothing
    pretend	
   we	
   saw	
   each	
   word	
   one	
   more	
   time	
   than	
   we	
   did
    just	
   add	
   one	
   to	
   all	
   the	
   counts!

    id113	
   estimate:

    add-     1	
   estimate:

pid113(wi | wi   1) =

padd   1(wi | wi   1) =

c(wi   1,wi)
c(wi   1)
c(wi   1,wi) +1
c(wi   1) +v

dan	
   jurafsky

maximum	
   likelihood	
   estimates

   

the	
   maximum	
   likelihood	
   estimate
    of	
   some	
   parameter	
   of	
   a	
   model	
   m	
   from	
   a	
   training	
   set	
   t
    maximizes	
   the	
   likelihood	
   of	
   the	
   training	
   set	
   t	
   given	
   the	
   model	
   m
   
suppose	
   the	
   word	
      bagel   	
   occurs	
   400	
   times	
   in	
   a	
   corpus	
   of	
   a	
   million	
   words
    what	
   is	
   the	
   id203	
   that	
   a	
   random	
   word	
   from	
   some	
   other	
   text	
   will	
   be	
   

   bagel   ?

    id113	
   estimate	
   is	
   400/1,000,000	
   =	
   .0004
    this	
   may	
   be	
   a	
   bad	
   estimate	
   for	
   some	
   other	
   corpus

    but	
   it	
   is	
   the	
   estimate that	
   makes	
   it	
   most	
   likely that	
      bagel   	
   will	
   occur	
   400	
   times	
   in	
   

a	
   million	
   word	
   corpus.

dan	
   jurafsky berkeley   restaurant   corpus:   laplace   

smoothed   bigram   counts

dan	
   jurafsky

laplace-  smoothed   bigrams

dan	
   jurafsky

reconstituted   counts

dan	
   jurafsky

compare   with   raw   bigram   counts

dan	
   jurafsky

add-     1	
   estimation	
   is	
   a	
   blunt	
   instrument

    so	
   add-     1	
   isn   t	
   used	
   for	
   n-     grams:	
   

    we   ll	
   see	
   better	
   methods

    but	
   add-     1	
   is	
   used	
   to	
   smooth	
   other	
   nlp	
   models

    for	
   text	
   classification	
   
    in	
   domains	
   where	
   the	
   number	
   of	
   zeros	
   isn   t	
   so	
   huge.

language	
   
modeling

smoothing:	
   add-     one	
   
(laplace)	
   smoothing

language	
   
modeling

interpolation,	
   backoff,	
   
and	
   web-     scale	
   lms

dan	
   jurafsky

backoff and   interpolation

    sometimes	
   it	
   helps	
   to	
   use	
   less context

    condition	
   on	
   less	
   context	
   for	
   contexts	
   you	
   haven   t	
   learned	
   much	
   about	
   

    backoff:	
   

    use	
   trigram	
   if	
   you	
   have	
   good	
   evidence,
    otherwise	
   bigram,	
   otherwise	
   unigram
interpolation:	
   
    mix	
   unigram,	
   bigram,	
   trigram

interpolation	
   works	
   better

   

   

dan	
   jurafsky

from all the id165 estimators, weighing and combining the trigram, bigram, and
unigram counts.

in simple linear interpolation, we combine different order id165s by linearly

linear	
   interpolation

interpolating all the models. thus, we estimate the trigram id203 p(wn|wn 2wn 1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
    simple	
   interpolation
by a l:

  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)

+l2p(wn|wn 1)
+l3p(wn)

such that the ls sum to 1:
  p(wn|wn 2wn 1) = l1p(wn|wn 2wn 1)
+l2p(wn|wn 1)
+l3p(wn)
    lambdas	
   conditional	
   on	
   context:

xi

li = 1
(4.24)

li = 1

such that the ls sum to 1:

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in

in a slightly more sophisticated version of linear interpolation, each l weight is
computed in a more sophisticated way, by conditioning on the context. this way,
if we have particularly accurate counts for a particular bigram, we assume that the
counts of the trigrams based on this bigram will be more trustworthy, so we can
make the ls for those trigrams higher and thus give that trigram more weight in

xi

(4.25)

dan	
   jurafsky

how	
   to	
   set	
   the	
   lambdas?

    use	
   a	
   held-     out corpus

training	
   data

held-     out	
   

data

test	
   
data

    choose	
     s to	
   maximize	
   the	
   id203	
   of	
   held-     out	
   data:

    fix	
   the	
   n-     gram	
   probabilities	
   (on	
   the	
   training	
   data)
    then	
   search	
   for	
     s that	
   give	
   largest	
   id203	
   to	
   held-     out	
   set:

logp(w1...wn | m(  1...  k)) =

logpm (  1...  k )(wi | wi   1)

   
i

dan	
   jurafsky unknown	
   words:	
   open	
   versus	
   closed	
   

vocabulary	
   tasks

if	
   we	
   know	
   all	
   the	
   words	
   in	
   advanced
    vocabulary	
   v	
   is	
   fixed
    closed	
   vocabulary	
   task
    often	
   we	
   don   t	
   know	
   this

   

   

    out	
   of	
   vocabulary =	
   oov	
   words
    open	
   vocabulary	
   task
instead:	
   create	
   an	
   unknown	
   word	
   token	
   <unk>
    training	
   of	
   <unk>	
   probabilities
    create	
   a	
   fixed	
   lexicon	
   l	
   of	
   size	
   v
    at	
   text	
   id172	
   phase,	
   any	
   training	
   word	
   not	
   in	
   l	
   changed	
   to	
   	
   <unk>
    now	
   we	
   train	
   its	
   probabilities	
   like	
   a	
   normal	
   word

    at	
   decoding	
   time

   

if	
   text	
   input:	
   use	
   unk	
   probabilities	
   for	
   any	
   word	
   not	
   in	
   training

dan	
   jurafsky

huge	
   web-     scale	
   n-     grams

    how	
   to	
   deal	
   with,	
   e.g.,	
   google	
   n-     gram	
   corpus
    pruning

    only	
   store	
   n-     grams	
   with	
   count	
   >	
   threshold.

    remove	
   singletons	
   of	
   higher-     order	
   n-     grams

    id178-     based	
   pruning

    efficiency

    efficient	
   data	
   structures	
   like	
   tries
    bloom	
   filters:	
   approximate	
   language	
   models
    store	
   words	
   as	
   indexes,	
   not	
   strings

    use	
   huffman	
   coding	
   to	
   fit	
   large	
   numbers	
   of	
   words	
   into	
   two	
   bytes

    quantize	
   probabilities	
   (4-     8	
   bits	
   instead	
   of	
   8-     byte	
   float)

dan	
   jurafsky

smoothing	
   for	
   web-     scale	
   n-     grams

       stupid	
   backoff   	
   (brants et	
   al.	
   2007)
    no	
   discounting,	
   just	
   use	
   relative	
   frequencies	
   

s(wi | wi   k+1

i   1 ) =

"
$$
#
$
$
%

i

)

count(wi   k+1
count(wi   k+1
0.4s(wi | wi   k+2
i   1

i   1 )   if  count(wi   k+1
)      otherwise

i

) > 0

s(wi) =

count(wi)

n

64

dan	
   jurafsky

n-     gram	
   smoothing	
   summary

    add-     1	
   smoothing:

    ok	
   for	
   text	
   categorization,	
   not	
   for	
   language	
   modeling

    the	
   most	
   commonly	
   used	
   method:

    extended	
   interpolated	
   kneser-     ney

    for	
   very	
   large	
   n-     grams	
   like	
   the	
   web:

    stupid	
   backoff

65

dan	
   jurafsky

advanced   language   modeling

    choose	
   n-     gram	
   weights	
   to	
   improve	
   a	
   task,	
   not	
   to	
   fit	
   the	
   	
   

    discriminative	
   models:

training	
   set

    parsing-     based	
   models
    caching	
   models

    recently	
   used	
   words	
   are	
   more	
   likely	
   to	
   appear

pcache(w | history) =   p(wi | wi   2wi   1) +(1     )c(w     history)
| history |
    these	
   perform	
   very	
   poorly	
   for	
   speech	
   recognition	
   (why?)

language	
   
modeling

interpolation,	
   backoff,	
   
and	
   web-     scale	
   lms

language 
modeling

advanced: 

kneser-ney smoothing

dan	
   jurafsky absolute   discounting:   just   subtract   a   

little   from   each   count
    suppose	
   we	
   wanted	
   to	
   subtract	
   a	
   little	
   

from	
   a	
   count	
   of	
   4	
   to	
   save	
   id203	
   
mass	
   for	
   the	
   zeros

    how	
   much	
   to	
   subtract	
   ?
    church	
   and	
   gale	
   (1991)   s	
   clever	
   idea
    divide	
   up	
   22	
   million	
   words	
   of	
   ap	
   

newswire
    training	
   and	
   held-     out	
   set
    for	
   each	
   bigram	
   in	
   the	
   training	
   set
    see	
   the	
   actual	
   count	
   in	
   the	
   held-     out	
   set!

bigram	
   count	
   
in	
   training
0
1
2
3
4
5
6
7
8
9

bigram	
   count	
   in	
   
heldout set
.0000270
0.448
1.25
2.24
3.23
4.21
5.23
6.21
7.21
8.26

dan	
   jurafsky

absolute   discounting   interpolation
    save	
   ourselves	
   some	
   time	
   and	
   just	
   subtract	
   0.75	
   (or	
   some	
   d)!

pabsolutediscounting(wi | wi   1) =

discounted bigram

c(wi   1,wi)     d

c(wi   1)

interpolation weight

+  (wi   1)p(w)

unigram

(maybe	
   keeping	
   a	
   couple	
   extra	
   values	
   of	
   d	
   for	
   counts	
   1	
   and	
   2)

   
    but	
   should	
   we	
   really	
   just	
   use	
   the	
   regular	
   unigram	
   p(w)?

70

dan	
   jurafsky

kneser-  ney   smoothing   i

    better	
   estimate	
   for	
   probabilities	
   of	
   lower-     order	
   unigrams!

    shannon	
   game:	
   	
   i	
   can   t	
   see	
   without	
   my	
   reading___________?
       francisco   	
   is	
   more	
   common	
   than	
      glasses   
       	
   but	
      francisco   	
   always	
   follows	
      san   

francisco
glasses

    the	
   unigram	
   is	
   useful	
   exactly	
   when	
   we	
   haven   t	
   seen	
   this	
   bigram!
   
    pcontinuation(w):	
   	
      how	
   likely	
   is	
   w	
   to	
   appear	
   as	
   a	
   novel	
   continuation?

instead	
   of	
   	
   p(w):	
      how	
   likely	
   is	
   w   

    for	
   each	
   word,	
   count	
   the	
   number	
   of	
   bigram	
   types	
   it	
   completes
    every	
   bigram	
   type	
   was	
   a	
   novel	
   continuation	
   the	
   first	
   time	
   it	
   was	
   seen

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

dan	
   jurafsky

kneser-  ney   smoothing   ii

    how	
   many	
   times	
   does	
   w	
   appear	
   as	
   a	
   novel	
   continuation:

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

    normalized	
   by	
   the	
   total	
   number	
   of	
   word	
   bigram	
   types

{(wj   1,wj):c(wj   1,wj) > 0}

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}

{(wj   1,wj):c(wj   1,wj) > 0}

dan	
   jurafsky

kneser-  ney   smoothing   iii

    alternative	
   metaphor:	
   the	
   number	
   of	
   	
   #	
   of	
   word	
   types	
   seen	
   to	
   precede	
   w

|{wi   1 :c(wi   1,w) > 0}|

    normalized	
   by	
   the	
   #	
   of	
   words	
   preceding	
   all	
   words:

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}
{w'i   1 :c(w'i   1,w') > 0}

   
w'

    a	
   frequent	
   word	
   (francisco)	
   occurring	
   in	
   only	
   one	
   context	
   (san)	
   will	
   have	
   a	
   

low	
   continuation	
   id203

dan	
   jurafsky

kneser-  ney   smoothing   iv

pkn(wi | wi   1) =

max(c(wi   1,wi)     d,0)

c(wi   1)

+  (wi   1)pcontinuation(wi)

   is	
   a	
   normalizing	
   constant;	
   the	
   id203	
   mass	
   we   ve	
   discounted

  (wi   1) =

d
c(wi   1) {w :c(wi   1,w) > 0}

the normalized discount

74

the number of word types that can follow wi-1
= # of word types we discounted
= # of times we applied normalized discount

dan	
   jurafsky kneser-  ney   smoothing:   recursive   

formulation

pkn(wi | wi   n+1
i   1

) =

i

)     d,0)
)

max(ckn(wi   n+1
ckn(wi   n+1
i   1
count(   )   for the highest order

+  (wi   n+1
i   1

continuationcount(   )    for lower order

)pkn(wi | wi   n+2
i   1

)

ckn(   ) =

!
#
"
$#

continuation count = number of unique single word contexts for      
75

language 
modeling

advanced: 

kneser-ney smoothing

