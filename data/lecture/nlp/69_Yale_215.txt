nlp
introduction to nlp
id52
the pos task
example
bahrainis vote in second round of parliamentary election
jabberwocky (by lewis carroll, 1872)
`twas brillig, and the slithy toves
did gyre and gimble in the wabe:
all mimsy were the borogoves,
and the mome raths outgrabe.
parts of speech
open class:
nouns, non-modal verbs, adjectives, adverbs
closed class: 
prepositions, modal verbs, conjunctions, particles, determiners, pronouns
id32 tagset (1/2)
id32 tagset (2/2)
universal pos
http://universaldependencies.org/u/pos/
universal features
http://universaldependencies.org/u/feat/
some observations
ambiguity
count (noun) vs. count (verb)
11% of all types but 40% of all tokens in the brown corpus are ambiguous.
examples
like can be tagged as adp verb adj adv noun 
present can be tagged as adj noun verb adv
example from j&m
pos ambiguity
some observations
more examples: 
transport, object, discount, address
content
french pronunciation:
 est, pr  sident, fils
three main techniques: 
rule-based
machine learning (e.g., id49, maximum id178 markov models, neural networks)
transformation-based
useful for parsing, translation, text to speech, id51, etc.
example
bethlehem/nnp steel/nnp corp./nnp ,/, hammered/vbn by/in higher/jjr costs/nns
bethlehem/nnp steel/nnp corp./nnp ,/, hammered/vbn by/in higher/jjr costs/vbz

classifier-based id52
a baseline method would be to use a classifier to map each individual word into a likely pos tag
why is this method unlikely to work well?
sources of information
bethlehem/nnp steel/nnp corp./nnp ,/, hammered/vbn by/in higher/jjr costs/nns
bethlehem/nnp steel/nnp corp./nnp ,/, hammered/vbn by/in higher/jjr costs/vbz

knowledge about individual words
lexical information
spelling (-or)
capitalization (ibm)
knowledge about neighboring words

evaluation
baseline
tag each word with its most likely tag
tag each oov word as a noun.
around 90%
current accuracy
around 97% for english
compared to 98% human performance
rule-based id52
use dictionary or finite-state transducers to find all possible parts of speech
use disambiguation rules 
e.g., art+v
hundreds of constraints need to be designed manually
example in french
<s>         ^                     beginning of sentence 
la          rf b nms u            article
teneur      nfs nms               noun feminine singular
moyenne     jfs nfs v1s v2s v3s   adjective feminine singular
en          p a b                 preposition
uranium     nms                   noun masculine singular        
des         p r                   preposition       
rivi  res    nfp                   noun feminine plural     
,           x                     punctuation        
bien_que    cs                    subordinating conjunction       
d  licate    jfs                   adjective feminine singular
             p                     preposition
calculer    v                     verb
sample rules
bs3 bi1
a bs3 (3rd person subject personal pronoun) cannot be followed by a bi1 (1st person indirect personal pronoun). 
in the example:    il nous faut    (=    we need   )        il    has the tag bs3ms and    nous    has the tags [bd1p bi1p bj1p br1p bs1p]. 
the negative constraint    bs3 bi1    rules out    bi1p'', and thus leaves only 4 alternatives for the word    nous   .
n k
the tag n (noun) cannot be followed by a tag k (interrogative pronoun); an example in the test corpus would be:    ... fleuve qui ...    (...river that...). 
since    qui    can be tagged both as an    e    (relative pronoun) and a    k    (interrogative pronoun), the    e    will be chosen by the tagger since an interrogative pronoun cannot follow a noun (   n   ).
r v
a word tagged with r (article) cannot be followed by a word tagged with v (verb): for example    l' appelle    (calls him/her). 
the word    appelle    can only be a verb, but    l'    can be either an article or a personal pronoun.  
thus, the rule will eliminate the article tag, giving preference to the pronoun.
nlp
