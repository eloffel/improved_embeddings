nlp
introduction to nlp
text id91
id91
exclusive/overlapping clusters
hierarchical/flat clusters

the cluster hypothesis
documents in the same cluster are relevant to the same query
how do we use it in practice?
example
id116
iteratively determine which cluster a point belongs to, then adjust the cluster centroid, then repeat
needed: small number k of desired clusters
hard decisions
id116
1 initialize cluster centroids to arbitrary vectors
2 while further improvement is possible do
3     for each document d do
4        find the cluster c whose centroid is closest to d
5        assign d to cluster c
6   end for
7   for each cluster c do
8       recompute the centroid of cluster c based on its documents
9   end for
10 end while
example
cluster the following vectors into two groups:
a = <1,6>
b = <2,2>
c = <4,0>
d = <3,3>
e = <2,5>
f = <2,1>
demos
http://home.dei.polimi.it/matteucc/id91/tutorial_html/appletkm.html
http://cgm.cs.mcgill.ca/~godfried/student_projects/bonnef_id116 
http://www.cs.washington.edu/research/imagedatabase/demo/kmcluster 
http://www.cc.gatech.edu/~dellaert/frankdellaert/software.html
http://www-2.cs.cmu.edu/~awm/tutorials/kmeans11.pdf 
http://web.archive.org/web/20110223234358/http://www.ece.neu.edu/groups/rpl/projects/kmeans/

id203 and likelihood
example:
what is j in this case?
bayesian formulation
posterior     likelihood x prior

em algorithms
class of iterative algorithms for id113 in problems with incomplete data. given a model of data generation and data with some missing values, em alternately uses the current model to estimate the missing values, and then uses the missing value estimates to improve the model. using all the available data, em will locally maximize the likelihood of the generative parameters giving estimates for the missing values.
[dempster et al. 77]
[mccallum & nigam 98]
em algorithm
initialize id203 model
repeat
e-step: use the best available current classifier to classify some datapoints
m-step: modify the classifier based on the classes produced by the e-step.
until convergence
soft id91 method
em example 
figure from chris bishop
demos
http://home.dei.polimi.it/matteucc/id91/tutorial_html/mixture.html
http://lcn.epfl.ch/tutorial/english/gaussian/html/
http://www.cs.cmu.edu/~alad/em/
http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html 
http://people.csail.mit.edu/mcollins/papers/wpeii.4.ps 



evaluation of id91
purity 
considering the majority class in each cluster
rand index
see next slide

purity
three clusters
xxxoo
ooox%
%%%%xx
purity: 
(3+3+4)/16=62.5%
rand index
accuracy when preserving object-object relationships
ri=(tp+tn)/(tp+fp+fn+tn)
in the example: 
rand index
ri = (tp+tn)/(tp+tn+fp+fn)=(13+64)/(13+64+22+21)=0.64
hierarchical id91 methods
single-linkage
one common pair is sufficient
disadvantages: long chains
complete-linkage
all pairs have to match
disadvantages: too conservative
average-linkage
non-hierarchical methods
also known as flat id91
centroid method (online)
id116
expectation maximization

hierarchical id91
hierarchical agglomerative id91
dendrograms
http://odur.let.rug.nl/~kleiweg/id91/id91.html
e.g., language similarity:
id91 using dendrograms
repeat
compute pairwise similarities
identify closest pair
merge pair into single node
until only one node left
example: cluster the following sentences:

	a b c b a
	a d c c a d e
	c d e f c d a
	e f g f d a
	a c d a b a
q: what is the equivalent venn diagram representation?
nlp
