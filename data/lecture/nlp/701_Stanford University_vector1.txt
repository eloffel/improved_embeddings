dan jurafsky and james martin
speech and language processing

chapter 6:
vector semantics

what do words mean?

first thought: look in a dictionary

http://www.oed.com/

oxford english dictionary | the definitive record of the english
language
words, lemmas, senses, definitions
pepper, n.
pronunciation:
oxford english dictionary | the definitive record of the english
forms:  oe peopor (rare), oe pipcer (transmission error), oe pipor, oe pipur (rare ...
language
frequency (in current use):  
etymology:  a borrowing from latin. etymon: latin piper.
pepper, n.
< classical latin piper, a loanword < indo-aryan (as is ancient greek              ); compare sanskrit ...

definition

lemma

sense

 /  p  p  / , u.s. 

 /  p  p  r/

  brit. 

betel-, malagueta, wall pepper, etc.: see the first element. see also water pepper n. 1.
betel-, malagueta, wall pepper, etc.: see the first element. see also water pepper n. 1.

  brit. 

 /  p  p  r/

 /  p  p  / , u.s. 

 i. the spice or the plant.
 1.
pronunciation:
forms:  oe peopor (rare), oe pipcer (transmission error), oe pipor, oe pipur (rare ...
 a. a hot pungent spice derived from the prepared fruits (peppercorns) of
the pepper plant, piper nigrum (see sense 2a), used from early times to
frequency (in current use):  
season food, either whole or ground to powder (often in association with
etymology:  a borrowing from latin. etymon: latin piper.
salt). also (locally, chiefly with distinguishing word): a similar spice
< classical latin piper, a loanword < indo-aryan (as is ancient greek              ); compare sanskrit ...
derived from the fruits of certain other species of the genus piper; the
 i. the spice or the plant.
fruits themselves.
 1.
 a. a hot pungent spice derived from the prepared fruits (peppercorns) of
the pepper plant, piper nigrum (see sense 2a), used from early times to
season food, either whole or ground to powder (often in association with
salt). also (locally, chiefly with distinguishing word): a similar spice
derived from the fruits of certain other species of the genus piper; the
 b. with distinguishing word: any of certain other pungent spices derived
fruits themselves.
from plants of other families, esp. ones used as seasonings.

the ground spice from piper nigrum comes in two forms, the more pungent black pepper, produced
from black peppercorns, and the milder white pepper, produced from white peppercorns: see black
adj. and n. special uses 5a, peppercorn n. 1a, and white adj. and n.  special uses 7b(a).
 
cubeb, mignonette pepper, etc.: see the first element.

1

1

the ground spice from piper nigrum comes in two forms, the more pungent black pepper, produced
cayenne, jamaica pepper, etc.: see the first element.
from black peppercorns, and the milder white pepper, produced from white peppercorns: see black
adj. and n. special uses 5a, peppercorn n. 1a, and white adj. and n.  special uses 7b(a).
 
cubeb, mignonette pepper, etc.: see the first element.

 2.
 a. the plant piper nigrum (family piperaceae), a climbing shrub
indigenous to south asia and also cultivated elsewhere in the tropics,
 b. with distinguishing word: any of certain other pungent spices derived
which has alternate stalked entire leaves, with pendulous spikes of small
from plants of other families, esp. ones used as seasonings.
green flowers opposite the leaves, succeeded by small berries turning red
when ripe. also more widely: any plant of the genus piper or the family
piperaceae.

cayenne, jamaica pepper, etc.: see the first element.

 2.
 b. usu. with distinguishing word: any of numerous plants of other
 a. the plant piper nigrum (family piperaceae), a climbing shrub
families having hot pungent fruits or leaves which resemble pepper ( 1a)
indigenous to south asia and also cultivated elsewhere in the tropics,
in taste and in some cases are used as a substitute for it.
which has alternate stalked entire leaves, with pendulous spikes of small
green flowers opposite the leaves, succeeded by small berries turning red
when ripe. also more widely: any plant of the genus piper or the family
piperaceae.

 

 c. u.s. the california pepper tree, schinus molle. cf. pepper tree n. 3.
 c. u.s. the california pepper tree, schinus molle. cf. pepper tree n. 3.

 

 3. any of various forms of capsicum, esp. capsicum annuum var.
 3. any of various forms of capsicum, esp. capsicum annuum var.
annuum. originally (chiefly with distinguishing word): any variety of the
annuum. originally (chiefly with distinguishing word): any variety of the
c. annuum longum group, with elongated fruits having a hot, pungent
c. annuum longum group, with elongated fruits having a hot, pungent
taste, the source of cayenne, chilli powder, paprika, etc., or of the
taste, the source of cayenne, chilli powder, paprika, etc., or of the
perennial c. frutescens, the source of tabasco sauce. now frequently
perennial c. frutescens, the source of tabasco sauce. now frequently
(more fully sweet pepper): any variety of the c. annuum grossum
(more fully sweet pepper): any variety of the c. annuum grossum
group, with large, bell-shaped or apple-shaped, mild-flavoured fruits,
group, with large, bell-shaped or apple-shaped, mild-flavoured fruits,
usually ripening to red, orange, or yellow and eaten raw in salads or
usually ripening to red, orange, or yellow and eaten raw in salads or
cooked as a vegetable. also: the fruit of any of these capsicums.
cooked as a vegetable. also: the fruit of any of these capsicums.

sweet peppers are often used in their green immature state (more fully green pepper), but some
sweet peppers are often used in their green immature state (more fully green pepper), but some
new varieties remain green when ripe.
new varieties remain green when ripe.
 
 
bell-, bird-, cherry-, pod-, red pepper, etc.: see the first element. see also chilli n. 1, pimento n. 2, etc.
bell-, bird-, cherry-, pod-, red pepper, etc.: see the first element. see also chilli n. 1, pimento n. 2, etc.

 

 

 

 ii. extended uses.
 ii. extended uses.
 4.
 4.
 a. phrases. to have pepper in the nose: to behave superciliously or
contemptuously. to take pepper in the nose, to snuff pepper: to
 a. phrases. to have pepper in the nose: to behave superciliously or
take offence, become angry. now arch.
contemptuously. to take pepper in the nose, to snuff pepper: to
take offence, become angry. now arch.

 

 

 b. in other allusive and proverbial contexts, chiefly with reference to the
biting, pungent, inflaming, or stimulating qualities of pepper.
 b. in other allusive and proverbial contexts, chiefly with reference to the
biting, pungent, inflaming, or stimulating qualities of pepper.

lemma pepper
sense 1: spice from pepper plant
sense 2: the pepper plant itself
sense 3: another similar plant (jamaican 
pepper)
sense 4: another plant with peppercorns 
(california pepper)
sense 5: capsicum (i.e. chili, paprika, bell 
pepper, etc)

a sense or    concept    is the 
meaning component of a word

there are relations between 
senses

relation: synonymity

synonyms have the same meaning in some 
or all contexts.
   filbert / hazelnut
   couch / sofa
   big / large
   automobile / car
   vomit / throw up
   water / h20

relation: synonymity
note that there are probably no examples of 
perfect synonymy.
    even if many aspects of meaning are identical
    still may not preserve the acceptability based on 
notions of politeness, slang, register, genre, etc.

the linguistic principle of contrast:
    difference in form -> difference in meaning

relation: synonymity?
water/h20
big/large
brave/courageous

relation: antonymy

senses that are opposites with respect to one feature of 
meaning
otherwise, they are very similar!
dark/light   short/long
up/down
hot/cold

in/out

fast/slow rise/fall

more formally: antonyms can
    define a binary opposition

or be at opposite ends of a scale

    long/short, fast/slow
    be reversives:
    rise/fall, up/down

relation: similarity
words with similar meanings.  not 
synonyms, but sharing some element of 
meaning

car, bicycle
cow, horse

ask humans how similar 2 
words are

word1
vanish
behave
belief
muscle
modest
hole

word2
disappear
obey
impression 
bone 
flexible
agreement

similarity
9.8 
7.3 
5.95 
3.65 
0.98 
0.3 

siid113x-999 dataset (hill et al., 2015) 

relation: word relatedness

also called "word association"
words be related in any way, perhaps via a 
semantic frame or field

    car, bicycle:    similar
    car, gasoline:   related, not similar

semantic field

words that 
   cover a particular semantic domain 
   bear structured relations with each other. 

hospitals

restaurants

houses

surgeon, scalpel, nurse, anaesthetic, hospital

waiter, menu, plate, food, menu, chef), 

door, roof, kitchen, family, bed

relation: superordinate/ 
subordinate
one sense is a subordinate of another if the first 
sense is more specific, denoting a subclass of the 
other
    car is a subordinate of vehicle
    mango is a subordinate of fruit
conversely superordinate
    vehicle is a superordinate of car
    fruit is a subodinate of mango

superordinate
subordinate

vehicle
car

furniture

fruit
mango chair

these levels are not symmetric

one level of category is 
distinguished from the others
the "basic level"

name these items

superordinate        basic

subordinate

chair

furniture

lamp

table 

office chair 
piano chair 
rocking chair
torchiere

desk lamp
end table

coffee table 

cluster of interactional 
properties
basic level things are    human-sized   
consider chairs
   we know how to interact with a chair 
(sitting)
   not so clear for superordinate 
categories like furniture
      imagine a furniture without thinking of a 
bed/table/chair/specific basic-level 
category   

the basic level

is the level of distinctive actions
is the level which is learned earliest and at 
which things are first named
it is the level at which names are shortest 
and used most frequently

connotation
words have affective meanings
positive connotations (happy) 
negative connotations (sad)

positive evaluation (great, love) 
negative evaluation (terrible, hate). 

so far
concepts or word senses
    have a complex many-to-many association with words
(homonymy, multiple senses)

have relations with each other
    synonymy
    antonymy
    similarity
    relatedness
    superordinate/subordinate
    connotation

but how to define a concept?

classical (   aristotelian   ) theory of concepts
the meaning of a word:
a concept defined by necessary and sufficient conditions
a necessary condition for being an x is a condition c that x must satisfy in 
order for it to be an x.

    if not c, then not x
       having four sides    is necessary to be a square.

a sufficient condition for being an x is condition such that if something 
satisfies condition c, then it must be an x.

    if and only if c, then x
    the following necessary conditions, jointly, are sufficient to be a square

    x has (exactly) four sides
    each of x's sides is straight
    x is a closed figure
    x lies in a plane
    each of x's sides is equal in length to each of the others
    each of x's interior angles is equal to the others (right angles)
    the sides of x are joined at their ends

example 
from 
norman 
swartz, 
sfu

problem 1: the features are complex and 
may be context-dependent

william labov. 1975

what are these?
cup or bowl?

the category depends on complex 
features of the object (diameter, etc)

the category depends on the context! 
(if there is food in it, it   s a bowl)

were shown pictures of varying indeterminacy (appendix 1) and asked to label them. from this, 

labov was able to come up with a mathematical definition of    cup    as: 

labov   s definition of cup

figure 1: labov   s (2004) definition of    cup    

 

 

 

 

 

 

the term cup is used to denote round containers with a ratio of depth to width of 1  r 
where r   rb, and rb =   1 +   2 +         and   1 is a positive quality when the feature i is present 
and 0 otherwise. 

2 = made of opaque vitreous material 
3 = used for consumption of food 
4 = used for the consumption of liquid food 
5 = used for consumption of hot liquid food 
6 = with a saucer 
7 = tapering 
8 = circular in cross-section 

feature  1 = with one handle 
 
 
 
 
 
 
 
 
cup is used variably to denote such containers with ratios width to depth 1  r where rb   r   r1 
with a id203 of r1 - r/rt     rb. the quantity 1  rb expresses the distance from the modal 
value of width to height. 
 
 

(labov, 2004, p. 86) 

 

 

 

 

 

ludwig wittgenstein (1889-
1951)
philosopher of 
language
in his late years, a 
proponent of studying 
   ordinary language   

wittgenstein (1945)
philosophical
investigations.
paragraphs 66,67

what is a game?

wittgenstein   s thought experiment on 
"what is a game   :
pi #66: 

   don   t say    there must be something common, or they would 
not be called `games         but look and see whether there is 
anything common to all   

is it amusing?
is there competition?
is there long-term strategy?
is skill required?
must luck play a role?
are there cards?
is there a ball?

family resemblance

game 1 game 2 game 3 game 4
abc

abd

acd

bcd

   each item has at least one, and probably 
several, elements in common with one or 
more items, but no, or few, elements are 
common to all items       rosch and mervis

how about a radically different 
approach?

ludwig wittgenstein

pi #43: 

"the meaning of a word is its use in the 
language"

let's define words by their 
usages
in particular, words are defined by their 
environments (the words around them)

zellig harris (1954): if a and b have almost 
identical environments we say that they are 
synonyms.

what does ongchoi mean?
suppose you see these sentences:
    ong choi is delicious saut  ed with garlic. 
    ong choi is superb over rice
    ong choi leaves with salty sauces
and you've also seen these:
       spinach saut  ed with garlic over rice
    chard stems and leaves are delicious
    collard greens and other salty leafy greens
conclusion:
    ongchoi is a leafy green like spinach, chard, or collard 
greens

ong choi: ipomoea aquatica
"water spinach"

yamaguchi, wikimedia commons, public domain

we'll build a new model of 
meaning focusing on similarity
each word = a vector 
   not just "word" or word45.
similar words are "nearby in space"

to
that

a
than

by
now
i

with

   s

are

you
is

not good

bad

dislike
incredibly bad

worst

worse

very good

incredibly good
fantastic

wonderful

amazing

terrific

nice
good

we define a word as a vector
called an "embedding" because it's embedded 
into a space
the standard way to represent meaning in nlp
fine-grained model of meaning for similarity 
    nlp tasks like id31
    with words,  requires same word to be in training and test
    with embeddings: ok if similar words occurred!!! 
    id53, conversational agents, etc

we'll introduce 2 kinds of 
embeddings
tf-idf
    a common baseline model
    sparse vectors
    words are represented by a simple function of the counts 
of nearby words

id97
    dense vectors
    representation is created by training a classifier to 
distinguish nearby and far-away words

review: words, vectors, and 
co-occurrence matrices

vector space
dimension

term-document
matrix

of numbers. so as you like it is represented as the list [1,114,36,20] and julius
structed in various ways; let   s s begin by looking at one such co-occurrence matrix,
caesar is represented as the list [7,62,1,2]. a vector space is a collection of vectors,
a term-document matrix.
characterized by their dimension.
in the example in fig. 6.3, the vectors are of
dimension 4, just so they    t on the page; in real term-document matrices, the vectors
6.3.1 vectors and documents
representing each document would have dimensionality |v|, the vocabulary size.
in a term-document matrix, each row represents a word in the vocabulary and each
the ordering of the numbers in a vector space is not arbitrary; each position
column represents a document from some collection of documents. fig. 6.2 shows a
indicates a meaningful dimension on which the documents can vary. thus the    rst
small selection from a term-document matrix showing the occurrence of four words
dimension for both these vectors corresponds to the number of times the word battle
in four plays by shakespeare. each cell in this matrix represents the number of times
occurs, and we can compare each dimension, noting for example that the vectors for
a particular word (de   ned by the row) occurs in a particular document (de   ned by
as you like it and twelfth night have similar values (1 and 0, respectively) for the
the column). thus clown appeared 117 times in twelfth night.
   rst dimension.

term-document matrix

each document is represented by a vector of words

as you like it
as you like it

twelfth night
twelfth night

julius caesar
julius caesar

henry v
henry v

battle
battle
soldier
good
fool
fool
clown
wit
figure 6.2 the term-document matrix for four words in four shakespeare plays. each cell
figure 6.3 the term-document matrix for four words in four shakespeare plays. the red
contains the number of times the (row) word occurs in the (column) document.
boxes show that each document is represented as a column vector of length four.

1
0
2
80
58
58
117
15

1
1
2
114
37
36
5
20

15
13
36
89
5
4
0
3

8
7
12
62
1
1
0
2

vector space
model

vector

the term-document matrix of fig. 6.2 was    rst de   ned as part of the vector
we can think of the vector for a document as identifying a point in |v|-dimensional
space model of information retrieval (salton, 1971). in this model, a document is
space; thus the documents in fig. 6.3 are points in 4-dimensional space. since 4-
represented as a count vector, a column in fig. 6.3.
dimensional spaces are hard to draw in textbooks, fig. 6.4 shows a visualization in
to review some basic id202, a vector is, at heart, just a list or array of
numbers. so as you like it is represented as the list [1,2,37,5] and julius caesar is
represented as the list [8,12,1,0]. a vector space is a collection of vectors, character-

visualizing document vectors

40

15

10

5

e
l
t
t
a
b

henry v [4,13]

julius caesar [1,7]

as you like it [36,1]

twelfth night [58,0]

5

10

15

20

25

35

30
 fool

40

45

50

55

60

indicates a meaningful dimension on which the documents can vary. thus the    rst
dimension for both these vectors corresponds to the number of times the word battle
occurs, and we can compare each dimension, noting for example that the vectors for
as you like it and twelfth night have similar values (1 and 0, respectively) for the
   rst dimension.

vectors are the basis of 
information retrieval

as you like it

twelfth night

julius caesar

henry v

0
80
58
15

1
114
36
20

battle
good
fool
wit
figure 6.3 the term-document matrix for four words in four shakespeare plays. the red
boxes show that each document is represented as a column vector of length four.

vectors are similar for the two comedies
different than the history
we can think of the vector for a document as identifying a point in |v|-dimensional
space; thus the documents in fig. 6.3 are points in 4-dimensional space. since 4-
dimensional spaces are hard to draw in textbooks, fig. 6.4 shows a visualization in

13
89
4
3

7
62
1
2

comedies have more fools and wit and 
fewer battles.

term-document
matrix

6.3.1 vectors and documents
in a term-document matrix, each row represents a word in the vocabulary and each
column represents a document from some collection of documents. fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by shakespeare. each cell in this matrix represents the number of times
a particular word (de   ned by the row) occurs in a particular document (de   ned by
the column). thus fool appeared 58 times in twelfth night.

words can be vectors too

as you like it

twelfth night

julius caesar

henry v

1
114
36
20

battle
good
fool
wit
figure 6.2 the term-document matrix for four words in four shakespeare plays. each cell
contains the number of times the (row) word occurs in the (column) document.

0
80
58
15

13
89
4
3

7
62
1
2

vector space
model

vector

vector space
dimension

battle is "the kind of word that occurs in julius 
caesar and henry v"

the term-document matrix of fig. 6.2 was    rst de   ned as part of the vector
space model of information retrieval (salton, 1971). in this model, a document is
represented as a count vector, a column in fig. 6.3.

to review some basic id202, a vector is, at heart, just a list or array
of numbers. so as you like it is represented as the list [1,114,36,20] and julius
caesar is represented as the list [7,62,1,2]. a vector space is a collection of vectors,
characterized by their dimension.
in the example in fig. 6.3, the vectors are of
dimension 4, just so they    t on the page; in real term-document matrices, the vectors

fool is "the kind of word that occurs in 
comedies, especially twelfth night"

term-term
matrix
word-word
matrix

however, it is most common to use a different kind of context for the dimensions
of a word   s vector representation. rather than the term-document matrix we use the
term-term matrix, more commonly called the word-word matrix or the term-
more common: word-word matrix
context matrix, in which the columns are labeled by words rather than documents.
this matrix is thus of dimensionality |v|   |v| and each cell records the number of
(or "term-context matrix")
times the row (target) word and the column (context) word co-occur in some context
in some training corpus. the context could be the document, in which case the cell
represents the number of times the two words appear in the same document. it is
most common, however, to use smaller contexts, generally a window around the
word, for example of 4 words to the left and 4 words to the right, in which case
two words are similar in meaning if their context vectors 
the cell represents the number of times (in some training corpus) the column word
are similar
occurs in such a   4 word window around the row word.
brown corpus (just one example of each word):

for example here are 7-word windows surrounding four sample words from the

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

apricot
pineapple
digital
information

aardvark

apricot
pineapple

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 6.5 shows a selection from the word-word
co-occurrence matrix computed from the brown corpus for these four words.
sugar
1
sugar
1
1
1
0
0
0
0

aardvark computer
0
computer
0
2
1

data pinch result
0
result
0
0
0
1
1
4
4

information
figure 6.5 co-occurrence vectors for four words, computed from the brown corpus, show-
ing only six of the dimensions (hand-picked for pedagogical purposes). the vector for the
word digital is outlined in red. note that a real vector would have vastly more dimensions
and thus be much sparser.

1
pinch
1
0
0

0
0
0
0
0
0
0
0

0
0
1
6

...
...
...
...
...

digital

data

1
1
0
0

0
0
2
1

0
0
1
6

...

   

4

3

2

1

t
l

u
s
e
r

digital
 [1,1]

information

 [6,4] 

1

2

3
4
 data

5

6

nxi=1

viwi = v1w1 + v2w2 + ... + vnwn

to de   ne similarity between two target words v and w, we need a measure for taking
two such vectors and giving a measure of vector similarity. by far the most common
similarity metric is the cosine of the angle between the vectors.

dot-product(~v,~w) =~v   ~w =

reminders from id202

the cosine   like most measures for vector similarity used in nlp   is based on
as we will see, most metrics for similarity between vectors are based on the dot
the dot product operator from id202, also called the inner product:
product. the dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. alternatively,
vectors that have zeros in different dimensions   orthogonal vectors   will have a
dot product of 0, representing their strong dissimilarity.
viwi = v1w1 + v2w2 + ... + vnwn
this raw dot-product, however, has a problem as a similarity metric: it favors

dot-product(~v,~w) =~v   ~w =

(6.7)

long vectors. the vector length is de   ned as

vector length

as we will see, most metrics for similarity between vectors are based on the dot
product. the dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. alternatively,
vectors that have zeros in different dimensions   orthogonal vectors   will have a
dot product of 0, representing their strong dissimilarity.
the dot product is higher if a vector is longer, with higher values in each dimension.
more frequent words have longer vectors, since they tend to co-occur with more
long vectors. the vector length is de   ned as
words and have higher co-occurrence values with each of them. the raw dot product

this raw dot-product, however, has a problem as a similarity metric: it favors

(6.8)

v2
i

nxi=1
|~v| =vuut
nxi=1

~a  ~b
|~a||~b|

= cosq

(6.9)

the cosine similarity metric between two vectors ~v and ~w thus can be computed

cosine for computing similarity

sec. 6.3

cosine(~v,~w) =

~v   ~w
|~v||~w|

=

viwi

nxi=1
ivuut
nxi=1

v2

nxi=1

vuut

    vector semantics

(6.10)

w2
i

12 chapter 6

vi is the count for word v in context i
wi is the count for word w in context i.

for some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. thus we could compute a unit vector from ~a by
dividing it by |~a|. for unit vectors, the dot product is the same as the cosine.
~a  ~b = |~a||~b|cosq
the cosine value ranges from 1 for vectors pointing in the same direction, through
~a  ~b
0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.
|~a||~b|
but raw frequency values are non-negative, so the cosine for these vectors ranges
from 0   1.

cos(v,w) is the cosine similarity of v and w

= cosq

let   s see how the cosine computes which of the words apricot or digital is closer
viwi
in meaning to information, just using raw counts from the following simpli   ed table:

cosine

as:

nxi=1

the cosine similarity metric between two vectors ~v and ~w thus can be computed

cosine as a similarity metric

-1: vectors point in opposite directions 
+1:  vectors point in same directions
0: vectors are orthogonal

frequency is non-negative, so  cosine range 0-1

51

cos(   v,    w) =

   v    
   w
   v    w =

   v
   v    

   w
   w =

n
   
i=1
n
2
vi
i=1

viwi
n
   
i=1

   

which pair of words is more similar?
cosine(apricot,information) = 

cosine(digital,information) =

cosine(apricot,digital) =

2
wi

large
apricot
1
digital
0
information 1
     1+0 +0    
1+0 +0
     0 +6 +2    
0 +1+4

1+36 +1

1+36 +1

=

data
0
1
6

computer
0
2
1

=

1
38 =.16
8
38 5 =.58

     0 +0 +0    

1+0 +0

0 +1+4

= 0

52

visualizing cosines 
(well, angles)

   
e
g
r
a
l
   
 
:
1
 
n
o
i
s
n
e
m
d

i

3

2

1

apricot

information

digital

1

2

3

5
dimension 2:    data   

4

6

7

but raw frequency is a bad 
representation
frequency is clearly useful; if sugar appears a lot 
near apricot, that's useful information.
but overly frequent words like the, it, or they are 
not very informative about the context
need a function that resolves this frequency 
paradox!

fool or so common as to be completely non-discriminative since they occur in all 37
plays like good or sweet.3

0

word
romeo
salad
falstaff
forest
battle
fool
good
sweet

weight of 1 is assigned to terms that occur in all the documents. because of
the large number of documents in many collections, this measure is usually
squashed with a log function.

1. the    rst is the term frequency (luhn, 1957): the frequency of the word in the
document. normally we want to downweight the raw frequency a bit, since
tf-idf: combine two factors
a word appearing 100 times in a document doesn   t make that word 100 times
more likely to be relevant to the meaning of the document. so we generally
use the log10 of the frequency, resulting in the following de   nition for the term
tf: term frequency. frequency count (usually log-transformed):
frequency weight:

it   s usually clear what counts as a document: when processing a collection
of encyclopedia articles like wikipedia, the document is a wikipedia page; in
processing newspaper articles, the document is a single article. occasionally
your corpus might not have appropriate document divisions and you might
if count(t,d) > 0
need to break up the corpus into documents yourself.
otherwise

df
1
2
4
12
the resulting de   nition for inverse document frequency (idf) is thus
idf: inverse document frequency: tf-
21
thus terms which occur 10 times in a document would have a tf=2, 100 times
total # of  docs in collection
in a document tf=3, 1000 times tf=4, and so on.
36
37
37

tft,d =    1 + log10 count(t,d)
idfi = log    n
dfi   

idf
1.57
1.27
0.967
0.489
0.074
0.012
0
0
# of  docs that have word i

2. the second factor is used to give a higher weight to words that occur only
in a few documents. terms that are limited to a few documents are useful
for discriminating those documents from the rest of the collection; terms that
words like "the" or "good" have very low idf
occur frequently across the entire collection aren   t as helpful. the document
frequency dft of a term t is simply the number of documents it occurs in. by
contrast, the collection frequency of a term is the total number of times the
word appears in the whole collection in any document. consider in the col-
lection shakespeare   s 37 plays the two words romeo and action. the words
have identical collection frequencies of 113 (they both occur 113 times in all
the plays) but very different document frequencies, since romeo only occurs
in a single play. if our goal is    nd documents about the romantic tribulations
of romeo, the word romeo should be highly weighted:

tf-idf value for word t in document d:

wt,d = tft,d     idft

the tf-idf weighting of the value for word i in document j, wi j thus combines

(6.12)

the tf-idf weighting of the value for word t in document d, wt,d thus combines

term frequency with idf:

fig. 6.8 applies tf-idf weighting to the shakespeare term-document matrix in fig. 6.2.

tomatically discovering meanings of words in different corpora. for example, we
can    nd the 10 most similar words to any target word w by computing the cosines
between w and each of the v   1 other words, sorting, and looking at the top 10.
the tf-idf vector model can also be used to decide if two documents are similar.
we represent a document by taking the vectors of all the words in the document, and
computing the centroid of all those vectors. the centroid is the multidimensional
version of the mean; the centroid of a set of vectors is a single vector that has the
minimum sum of squared distances to each of the vectors in the set. given k word
vectors w1,w2, ...,wk, the centroid document vector d is:

summary: tf-idf
compare two words using tf-idf cosine to see 
if they are similar
compare two documents
    take the centroid of vectors of all the words in 
the document
    centroid document vector is:

d =

w1 + w2 + ... + wk

k

given two documents, we can then compute their document vectors d1 and d2,

and estimate the similarity between the two documents by cos(d1,d2).

an alternative to tf-idf

ask whether a context word is particularly 
informative about the target word.
   positive pointwise mutual information (ppmi)

57

pointwise mutual information

pointwise mutual information: 

do events x and y co-occur more than if they were independent?

pmi(x,y) = log2

p(x,y)
p(x)p(y)

pmi between two words:  (church & hanks 1989)

do words x and y co-occur more than if they were independent? 

pmi$%&'(,$%&'* =log* /($%&'(,$%&'*)
/$%&'(/($%&'*)

    pmi ranges from        to+   

positive pointwise mutual information

    but the negative values are problematic

    things are co-occurring less than we expect by chance
    unreliable without enormous corpora

imagine w1 and w2 whose id203 is each 10-6

   
    hard to be sure p(w1,w2) is significantly different than 10-12
    plus it   s not clear people are good at    unrelatedness   

    so we just replace negative pmi values by 0
    positive pmi (ppmi) between word1 and word2:

ppmi'()*+,'()*- =max log- 5('()*+,'()*-)
5'()*+5('()*-),0

computing ppmi on a term-context 
matrix
matrix f with w rows (words) and c columns (contexts)
fij is # of times wi occurs in context cj

pij =

fij
c
   
j=1

w
   
i=1

fij

pi* =

fij

c
   
j=1
w
c
   
   
i=1
j=1

fij

p*j =

fij

w
   
i=1
w
c
   
   
i=1
j=1

fij

pmiij = log2

pij
pi*p* j

ppmiij =

!
#
"
$#

pmiij
0

if  pmiij > 0
otherwise

60

pij =

fij
c
   
j=1

w
   
i=1

fij

p(w=information,c=data) = 
p(w=information) =
p(c=data) =

7/19 = .37

6/19
11/19 = .58

apricot
pineapple
digital
information

computer
0.00
0.00
0.11
0.05

= .32

c
fij
   
j=1
n

p(wi) =

p(w,context)

data pinch result
0.00
0.00
0.00
0.00
0.05
0.05
0.21
0.32

0.05
0.05
0.00
0.00

w
fij
   
i=1
n

p(cj) =
p(w)

0.11
0.11
0.21
0.58

sugar
0.05
0.05
0.00
0.00

p(context)

0.16

0.37

0.11

0.26

0.11

61

pmiij = log2

pij
pi*p* j

apricot
pineapple
digital
information
p(context)

computer
0.00
0.00
0.11
0.05

p(w,context)

data pinch result
0.00
0.00
0.00
0.00
0.05
0.05
0.32
0.21

0.05
0.05
0.00
0.00

p(w)

0.11
0.11
0.21
0.58

sugar
0.05
0.05
0.00
0.00

0.16

0.37

0.11

0.26

0.11

pmi(information,data) = log2 (

.32 / (.37*.58) ) = .58

apricot
pineapple
digital
information

computer
1
1
1.66
0.00

ppmi(w,context)

data pinch result
1
1
0.00
0.47

1
1
0.00
0.57

2.25
2.25
1
1

(.57 using full precision)

sugar
2.25
2.25
1
1

62

weighting pmi
pmi is biased toward infrequent events
    very rare words have very high pmi values
two solutions:
    give rare words slightly higher probabilities
    use add-one smoothing (which has a similar 
effect)

63

0

0

0

0.47

0.57

information
figure 19.4 the ppmi matrix showing the association between words and context words,
computed from the counts in fig. 17.2 again showing six dimensions.

weighting pmi: giving rare 
context words slightly higher 
id203

pmi has the problem of being biased toward infrequent events; very rare words
tend to have very high pmi values. one way to reduce this bias toward low frequency
events is to slightly change the computation for p(c), using a different function pa (c)
that raises contexts to the power of a (levy et al., 2015):

ppmia (w,c) = max(log2

raise the context probabilities to !=0.75:
this helps because '() >') for rare c
.,,.-./.01.-.=.97'(3 = .01.-.
'(+ = .,,.-.

pc count(c)a

levy et al. (2015) found that a setting of a = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams (mikolov et al., 2013a) and glove (pennington et al., 2014)). this works
because raising the id203 to a = 0.75 increases the id203 assigned to rare
contexts, and hence lowers their pmi (pa (c) > p(c) when c is rare).

consider two events, p(a) = .99 and p(b)=.01

.01.-./.01.-.=.03

another possible solution is laplace smoothing: before computing pmi, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. the larger the k, the more the non-zero counts
are discounted.

(19.8)

(19.9)

p(w)pa (c)

count(c)a

pa (c) =

p(w,c)

,0)

64

use laplace (add-1) 
smoothing

65

apricot
pineapple
digital
information

computer
2
2
4
3

add#2%smoothed%count(w,context)
sugar
3
3
2
2

data pinch result
2
2
3
6

2
2
3
8

3
3
2
2

apricot
pineapple
digital
information
p(context)

computer
0.03
0.03
0.07
0.05
0.19

p(w,context),[add02]

data pinch result
0.03
0.03
0.03
0.03
0.05
0.05
0.14
0.10
0.22
0.25

0.05
0.05
0.03
0.03
0.17

p(w)

0.20
0.20
0.24
0.36

sugar
0.05
0.05
0.03
0.03
0.17

66

ppmi versus add-2 smoothed 
ppmi

apricot
pineapple
digital
information

computer
1
1
1.66
0.00

ppmi(w,context)

1
1
0.00
0.57

data pinch result
1
1
0.00
0.47
ppmi(w,context).[add22]

2.25
2.25
1
1

apricot
pineapple
digital
information

computer
0.00
0.00
0.62
0.00

data pinch result
0.00
0.00
0.00
0.00
0.00
0.00
0.58
0.37

0.56
0.56
0.00
0.00

sugar
2.25
2.25
1
1

sugar
0.56
0.56
0.00
0.00

67

summary for part i
    survey of lexical semantics
    idea of embeddings: represent a word as a 
function of its distribution with other words
    tf-idf
    cosines
    ppmi

    next lecture: sparse embeddings, id97

