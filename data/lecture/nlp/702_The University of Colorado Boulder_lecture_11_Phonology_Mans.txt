phonology and speech applications

with weighted automata

natural language processing

ling/csci 5832

mans hulden

dept. of linguistics

mans.hulden@colorado.edu

feb 19 2014

overview

(1) recap unweighted    nite automata 

and transducers

(2) extend to probabilistic weighted 
automata/transducers

(3) see how these can be used in 
natural language applications + a brief 
look at speech applications

re: anatomy of a fsa

regular expression

l = a b* c

graph representation

b

a

1

c

0

2

formal de   nition

q = {0,1,2} (set of states) 
   = {a,b,c}  (alphabet)   
q0 = 0          (initial state) 
f = {2}       (set of final states) 
  (0,a) = 1,   (1,b) = 1,   (1,c) = 2 
    (transition function)

de   nes a set of strings

re: anatomy of an fst

formal de   nition

graph representation

a b d 

0

b d 
c 

a b 
c 

1

d 

a 
c 

<a:b> 

2

3

q = {0,1,2,3} (set of states) 
   = {a,b,c,d}  (alphabet)   
q0 = 0              (initial state) 
f = {0,1,2}     (set of final states) 
         (transition function)

string-to-string mapping

re: composition

neg+possible+ity+noun+plural

23

o 

r 

29

i 

t 

18

15

12

24

s 

e 

g 

k 

r 

19

16

13

25

s 

t 

e 

a 

20

17

30

10

+ 

s 
p 
u 

l 

14

11

27

b 

g 

22

26

i 

t 
l 

n 

21

31

l 

y 

28

e 

n 
a 

32

33

i 

e 

36

34

37

s 

l 

t 

35

c 

38

s 

y 

39

0

u i 

s 
a 

8

e 

u 

d 

2

5

3

6

+ 

n 

1

m 

t 

9

i 
o 

e 

4

7

in+possible+ity+s

@ + m p 

0

@ m p 

n 

<n:m> 

n 
+ 

@ + m 

n 

1

4

<n:m> 

<n:m> 

p 

2

+ 

3

im+possible+ity+s

@ + i l t y 

@ + e i l t y 

0

@ + e i t y 

b 

b 

1

@ e i l t y 

l 

b 

<l:i> 

@ + e l t y 

7

b 

2

e 

b 
<e:l> 

@ + e i l y 

8

b 

3

+ 

b 

9

i 

10

t 

<+:i> 

4

<i:t> 

5

<t:y> 

@ + e i l t 

<y:0> 

11

6

im+possibility+s

@ <+:0> 

0

impossibilities

neg+possible+ity+noun+plural

0

d

 

s
 

a
 

2

5

1

u

 
i
 

e
 

u

 

3

6

<
+
:
0
>

 

e
 

m

 

t
 

1
0

4

7

n

 

<
n
:
m
>

 

i
 

o

 

2
2

<
+
:
0
>

 

1
1

8

<
+
:
0
>

 

<
+
:
0
>

 

2
3

9

1
2

s
 

u

 

l
 

s
 

u

 

l
 

p

 

p

 

2
7

3
3

2
4

1
3

t
 

i
 

o

 

r
 

2
8

2
5

1
7

1
4

r
 

g

 

k

 

s
 

e
 

2
9

2
6

1
8

1
5

a
 

e
 

s
 

t
 

3
0

3
4

1
9

1
6

n

 

i
 

l
 

t
 

2
0

3
5

3
1

b
 

g
 

2
1

l
 

y
 

3
2

e
 

3
6

a
 

n
 

i
 

3
7

4
0

e
 

4
1

3
8

t
 

l
 

s
 

4
2

c
 

3
9

y

 

s
 

4
3

impossibilities

orthographic vs. phonetic representation

neg+possible+ity+noun+plural

23

o 

r 

29

i 

t 

18

15

12

24

s 

e 

g 

k 

r 

19

16

13

25

s 

t 

e 

a 

20

17

30

10

+ 

s 
p 
u 

l 

14

11

27

b 

g 

22

26

i 

t 
l 

n 

21

31

l 

y 

28

e 

n 
a 

32

33

i 

e 

36

34

37

s 

l 

t 

35

c 

38

s 

y 

39

0

u i 

s 
a 

8

e 

u 

d 

2

5

3

6

+ 

n 

1

m 

t 

9

i 
o 

e 

4

7

in+possible+ity+s

@ + m p 

0

@ m p 

n 

<n:m> 

n 
+ 

@ + m 

n 

1

4

<n:m> 

<n:m> 

p 

2

+ 

3

im+possible+ity+s

@ + i l t y 

@ + e i l t y 

0

@ + e i t y 

b 

b 

1

@ e i l t y 

l 

b 

<l:i> 

@ + e l t y 

7

b 

2

e 

b 
<e:l> 

@ + e i l y 

8

b 

3

+ 

b 

9

i 

10

t 

<+:i> 

4

<i:t> 

5

<t:y> 

@ + e i l t 

<y:0> 

11

6

impossibilities

g2p

[  mp  s  b  l  tis]

neg+possible+ity+noun+plural

0

d

 

s
 

a
 

2

5

1

u

 
i
 

e
 

u

 

3

6

<
+
:
0
>

 

e
 

m

 

t
 

1
0

4

7

n

 

<
n
:
m
>

 

i
 

o

 

2
2

<
+
:
0
>

 

1
1

8

<
+
:
0
>

 

<
+
:
0
>

 

2
3

9

1
2

s
 

u

 

l
 

s
 

u

 

l
 

p

 

p

 

2
7

3
3

2
4

1
3

t
 

i
 

o

 

r
 

2
8

2
5

1
7

1
4

r
 

g

 

k

 

s
 

e
 

2
9

2
6

1
8

1
5

a
 

e
 

s
 

t
 

3
0

3
4

1
9

1
6

n

 

i
 

l
 

t
 

2
0

3
5

3
1

b
 

g
 

2
1

l
 

y
 

3
2

e
 

3
6

a
 

n
 

i
 

3
7

4
0

e
 

4
1

3
8

t
 

l
 

s
 

4
2

c
 

3
9

y

 

s
 

4
3

[  mp  s  b  l  tis]

word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

id87s
guess at
original
word

noisy
word

word

decoder

source

noisy channel
the id87

figure 5.1
a general framework for thinking about 
the intuition of the id87 (see figure 5.1) is to treat
spell checking, id103, and 
the surface form (the    reduced    pronunciation or misspelled word) as an in-
other problems that involve decoding in 
stance of the lexical form (the    lexical    pronunciation or correctly-spelled
word) which has been passed through a noisy communication channel. this
probabilistic models
channel introduces    noise    which makes it hard to recognize the    true    word.
our goal is then to build a model of the channel so that we can    gure out how
it modi   ed this    true    word and hence recover it. for the complete speech
recognition tasks, there are many sources of    noise   ; variation in pronun-
ciation, variation in the realization of phones, acoustic variation due to the
channel (microphones, telephone networks, etc). since this chapter focuses
on pronunciation, what we mean by    noise    here is the variation in pronun-
ciation that masks the lexical or    canonical    pronunciation; the other sources
of noise in a id103 system will be discussed in chapter 7. for
spelling error detection, what we mean by noise is the spelling errors which
mask the correct spelling of the word. the metaphor of the noisy channel

similar problem to morphology 
   decoding   

noisy
channel

v

  w

o

source

decoder

word

noisy
word

figure 5.1

noisy channel
the id87

word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

example: spell checking
guess at
original
word

). we want to know which word corresponds to this string
phones (say
of phones. the bayesian interpretation of this task starts by considering all
possible classes     in this case, all possible words. out of this universe of
words, we want to chose the word which is most probable given the ob-
servation we have (
). in other words, we want, out of all words in the
vocabulary v the single word such that p word observation is highest. we
use   w to mean    our estimate of the correct w   , and we   ll use o to mean    the
observation sequence
    (we call it a sequence because we think of each
letter as an individual observation). then the equation for picking the best
word given is:

the intuition of the id87 (see figure 5.1) is to treat
the surface form (the    reduced    pronunciation or misspelled word) as an in-
stance of the lexical form (the    lexical    pronunciation or correctly-spelled
word) which has been passed through a noisy communication channel. this
channel introduces    noise    which makes it hard to recognize the    true    word.
our goal is then to build a model of the channel so that we can    gure out how
it modi   ed this    true    word and hence recover it. for the complete speech
recognition tasks, there are many sources of    noise   ; variation in pronun-
ciation, variation in the realization of phones, acoustic variation due to the
channel (microphones, telephone networks, etc). since this chapter focuses
on pronunciation, what we mean by    noise    here is the variation in pronun-
ciation that masks the lexical or    canonical    pronunciation; the other sources
of noise in a id103 system will be discussed in chapter 7. for
spelling error detection, what we mean by noise is the spelling errors which
mask the correct spelling of the word. the metaphor of the noisy channel

  w argmax
w v
the function argmaxx f x means    the x such that f x is maximized   .
while (5.1) is guaranteed to give us the optimal word w, it is not clear how
to make the equation operational; that is, for a given word w and observation
sequence o we don   t know how to directly compute p w o . the intuition of
bayesian classi   cation is to use bayes    rule to transform (5.1) into a product
of two probabilities, each of which turns out to be easier to compute than

problem form

p w o

noisy
channel

v

  w

o

source

decoder

noisy
word

p w o

noisy channel
the id87

word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

letter as an individual observation). then the equation for picking the best
phones (say
). we want to know which word corresponds to this string
word given is:
of phones. the bayesian interpretation of this task starts by considering all
possible classes     in this case, all possible words. out of this universe of
id87s
  w argmax
words, we want to chose the word which is most probable given the ob-
w v
servation we have (
). in other words, we want, out of all words in the
the function argmaxx f x means    the x such that f x is maximized   .
guess at
original
word
vocabulary v the single word such that p word observation is highest. we
word
while (5.1) is guaranteed to give us the optimal word w, it is not clear how
use   w to mean    our estimate of the correct w   , and we   ll use o to mean    the
to make the equation operational; that is, for a given word w and observation
figure 5.1
observation sequence
    (we call it a sequence because we think of each
sequence o we don   t know how to directly compute p w o . the intuition of
letter as an individual observation). then the equation for picking the best
the intuition of the id87 (see figure 5.1) is to treat
bayesian classi   cation is to use bayes    rule to transform (5.1) into a product
the surface form (the    reduced    pronunciation or misspelled word) as an in-
word given is:
stance of the lexical form (the    lexical    pronunciation or correctly-spelled
of two probabilities, each of which turns out to be easier to compute than
word) which has been passed through a noisy communication channel. this
  w argmax
p w o . bayes    rule is presented in (5.2); it gives us a way to break down
channel introduces    noise    which makes it hard to recognize the    true    word.
w v
our goal is then to build a model of the channel so that we can    gure out how
p x o into three other probabilities:
it modi   ed this    true    word and hence recover it. for the complete speech
the function argmaxx f x means    the x such that f x is maximized   .
recognition tasks, there are many sources of    noise   ; variation in pronun-
while (5.1) is guaranteed to give us the optimal word w, it is not clear how
ciation, variation in the realization of phones, acoustic variation due to the
p x y
channel (microphones, telephone networks, etc). since this chapter focuses
to make the equation operational; that is, for a given word w and observation
on pronunciation, what we mean by    noise    here is the variation in pronun-
sequence o we don   t know how to directly compute p w o . the intuition of
ciation that masks the lexical or    canonical    pronunciation; the other sources
we can see this by substituting (5.2) into (5.1) to get (5.3):
of noise in a id103 system will be discussed in chapter 7. for
bayesian classi   cation is to use bayes    rule to transform (5.1) into a product
spelling error detection, what we mean by noise is the spelling errors which
of two probabilities, each of which turns out to be easier to compute than
mask the correct spelling of the word. the metaphor of the noisy channel

p o w p w

problem form

p y x p x

(bayes    rule)

p w o

p y

noisy
channel

v

  w

o

source

decoder

noisy
word

figure 5.1

noisy channel
the id87

word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

phones (say
). we want to know which word corresponds to this string
word given is:
of phones. the bayesian interpretation of this task starts by considering all
  w argmax
p w o
possible classes     in this case, all possible words. out of this universe of
id87s
w v
the function argmaxx f x means    the x such that f x is maximized   .
words, we want to chose the word which is most probable given the ob-
while (5.1) is guaranteed to give us the optimal word w, it is not clear how
servation we have (
). in other words, we want, out of all words in the
guess at
original
to make the equation operational; that is, for a given word w and observation
word
vocabulary v the single word such that p word observation is highest. we
word
sequence o we don   t know how to directly compute p w o . the intuition of
use   w to mean    our estimate of the correct w   , and we   ll use o to mean    the
bayesian classi   cation is to use bayes    rule to transform (5.1) into a product
observation sequence
    (we call it a sequence because we think of each
of two probabilities, each of which turns out to be easier to compute than
letter as an individual observation). then the equation for picking the best
p w o . bayes    rule is presented in (5.2); it gives us a way to break down
the intuition of the id87 (see figure 5.1) is to treat
the surface form (the    reduced    pronunciation or misspelled word) as an in-
word given is:
p x o into three other probabilities:
stance of the lexical form (the    lexical    pronunciation or correctly-spelled
word) which has been passed through a noisy communication channel. this
channel introduces    noise    which makes it hard to recognize the    true    word.
our goal is then to build a model of the channel so that we can    gure out how
it modi   ed this    true    word and hence recover it. for the complete speech
recognition tasks, there are many sources of    noise   ; variation in pronun-
ciation, variation in the realization of phones, acoustic variation due to the
channel (microphones, telephone networks, etc). since this chapter focuses
on pronunciation, what we mean by    noise    here is the variation in pronun-
ciation that masks the lexical or    canonical    pronunciation; the other sources
of noise in a id103 system will be discussed in chapter 7. for
spelling error detection, what we mean by noise is the spelling errors which
mask the correct spelling of the word. the metaphor of the noisy channel

  w argmax
p y x p x
p x y
p y
w v
the function argmaxx f x means    the x such that f x is maximized   .
we can see this by substituting (5.2) into (5.1) to get (5.3):
while (5.1) is guaranteed to give us the optimal word w, it is not clear how
  w argmax
to make the equation operational; that is, for a given word w and observation
w v
sequence o we don   t know how to directly compute p w o . the intuition of
the probabilities on the right hand side of (5.3) are for the most part
bayesian classi   cation is to use bayes    rule to transform (5.1) into a product
easier to compute than the id203 p w o which we were originally try-
ing to maximize in (5.1). for example, p w , the id203 of the word
of two probabilities, each of which turns out to be easier to compute than

problem form

p o w p w

p w o

p o

noisy
channel

o

  w

decoder

v

noisy
word

figure 5.1

section 5.5.

noisy channel
the id87

applying the bayesian method to spelling

id87s
guess at
original
word

word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

). we want to know which word corresponds to this string
phones (say
of phones. the bayesian interpretation of this task starts by considering all
possible classes     in this case, all possible words. out of this universe of
words, we want to chose the word which is most probable given the ob-
servation we have (
). in other words, we want, out of all words in the
section 5.5.
vocabulary v the single word such that p word observation is highest. we
source
p o
use   w to mean    our estimate of the correct w   , and we   ll use o to mean    the
that p o w turns out to be easy to estimate as well. but p o , the probabil-
observation sequence
    (we call it a sequence because we think of each
ity of the observation sequence, turns out to be harder to estimate. luckily,
p o w p w
letter as an individual observation). then the equation for picking the best
argmax
p o w p w
we can ignore p o . why? since we are maximizing over all words, we will
word given is:
w v
be computing p o w p w
for each word. but p o doesn   t change for each
p o
word; we are always asking about the most likely word string for the same
  w argmax
observation o, which must have the same id203 p o . thus:
w v
the function argmaxx f x means    the x such that f x is maximized   .
  w argmax
while (5.1) is guaranteed to give us the optimal word w, it is not clear how
w v
to make the equation operational; that is, for a given word w and observation
to summarize, the most probable word w given some observation o
can be computing by taking the product of two probabilities for each word,
sequence o we don   t know how to directly compute p w o . the intuition of
  w argmax
and choosing the word for which this product is greatest. these two terms
bayesian classi   cation is to use bayes    rule to transform (5.1) into a product
w v
have names; p w is called the prior id203, and p o w is called the
of two probabilities, each of which turns out to be easier to compute than

that p o w turns out to be easy to estimate as well. but p o , the probabil-
ity of the observation sequence, turns out to be harder to estimate. luckily,
we can ignore p o . why? since we are maximizing over all words, we will
word
applying the bayesian method to spelling
be computing p o w p w
for each word. but p o doesn   t change for each
word; we are always asking about the most likely word string for the same
observation o, which must have the same id203 p o . thus:
  w argmax
(5.4)
the intuition of the id87 (see figure 5.1) is to treat
the surface form (the    reduced    pronunciation or misspelled word) as an in-
w v
stance of the lexical form (the    lexical    pronunciation or correctly-spelled
to summarize, the most probable word w given some observation o
word) which has been passed through a noisy communication channel. this
can be computing by taking the product of two probabilities for each word,
channel introduces    noise    which makes it hard to recognize the    true    word.
our goal is then to build a model of the channel so that we can    gure out how
and choosing the word for which this product is greatest. these two terms
it modi   ed this    true    word and hence recover it. for the complete speech
have names; p w is called the prior id203, and p o w is called the
p o w p w
recognition tasks, there are many sources of    noise   ; variation in pronun-
likelihood.
ciation, variation in the realization of phones, acoustic variation due to the
channel (microphones, telephone networks, etc). since this chapter focuses
on pronunciation, what we mean by    noise    here is the variation in pronun-
ciation that masks the lexical or    canonical    pronunciation; the other sources
of noise in a id103 system will be discussed in chapter 7. for
spelling error detection, what we mean by noise is the spelling errors which
mask the correct spelling of the word. the metaphor of the noisy channel

key concept #3.
(5.5)
error model
in the next sections we will show how to compute these two probabili-

p w o
p o w p w

likelihood
p o w

language model

problem form

argmax
w v

prior
p w

p o

p o

likelihood

noisy
channel

prior

section 5.4.

probabilistic models

neg+possible+ity+noun+plural

decoding

recognition, given a string of symbols representing the pronunciation of a
word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

impossibility

39

y 

s 

35

c 

38

23

o 

r 

29

i 

t 

18

15

12

24

s 

e 

g 

k 

r 

19

16

13

25

s 

t 

e 

a 

20

17

30

10

+ 

s 
p 
u 

l 

14

11

27

b 

g 

22

26

i 

t 
l 

n 

21

31

l 

y 

28

e 

n 
a 

32

33

i 

e 

36

34

37

s 

l 

t 

0

u i 

s 
a 

8

e 

u 

d 

2

5

3

6

+ 

n 

1

m 

t 

9

i 
o 

e 

4

7

in+possible+ity+s

@ + m p 

0

@ m p 

n 

<n:m> 

n 
+ 

@ + m 

n 

1

4

<n:m> 

<n:m> 

p 

2

+ 

3

im+possible+ity+s

source

word

@ + i l t y 

@ + e i l t y 

0

@ + e i t y 

b 

b 

1

@ e i l t y 

l 

b 

<l:i> 

@ + e l t y 

7

b 

2

e 

b 
<e:l> 

@ + e i l y 

8

b 

3

+ 

b 

9

i 

10

t 

<+:i> 

4

<i:t> 

5

<t:y> 

@ + e i l t 

<y:0> 

11

6

im+possibility+s

figure 5.1

noisy channel
the id87

noisy
word

decoder

@ <+:0> 

0

impossibilities

the intuition of the id87 (see figure 5.1) is to treat
the surface form (the    reduced    pronunciation or misspelled word) as an in-
stance of the lexical form (the    lexical    pronunciation or correctly-spelled

impssblity

section 5.4.

probabilistic models

decoding

neg+possible+ity+noun+plural

impossibility

recognition, given a string of symbols representing the pronunciation of a
word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

changes (errors)

probabilistic 

non-probabilistic 

changes

e
d
o
c
e
d
source

word

noisy
word

morphology/
phonology

impossibilities

figure 5.1

noisy channel
the id87
impssblity

the intuition of the id87 (see figure 5.1) is to treat

decoding/speech processing

probabilistic models

section 5.4.

neg+possible+ity+noun+plural

decoding is a problem

recognition, given a string of symbols representing the pronunciation of a
word in context, we need to    gure out the string of symbols representing
the lexical or dictionary pronunciation, so we can look the word up in the
dictionary. similarly, given the incorrect sequence of letters in a mis-spelled
word, we need to    gure out the correct sequence of letters in the correctly-
spelled word.

probabilistic 

changes

non-probabilistic 

changes

e
d
o
c
e
d
source

word

noisy
word

morphology/
phonology

impossibilities

figure 5.1

noisy channel
the id87

the intuition of the id87 (see figure 5.1) is to treat

probabilistic automata

intuition

strings

figure 4: a simple weighted automaton encoding four possible pronunciations of
the word about and associating a weight to each.

- de   ne id203 distributions over 
- symbols have transition probabilities
- states have    nal/halting probabilities
- probabilities are multiplied along paths
- probabilities are summed for several 

p([@baut]) = 0.336 (0.84     1     1     0.4     1)
p([@bau]) = 0.504 (0.84     1     1     0.6)
p([baut]) = 0.064 (0.16     1     0.4     1)
p([bau]) = 0.096 (0.16     1     0.6)

4.1 different weight structures
we need not restrict ourselves to interpreting the weights as probabilities only. in
fact, raw id203 values are rarely used in natural language processing because

parallel paths

probabilistic automata

intuition

part ii - weighted 
automata

probabilistic automaton (distribution over strings):

figure 4: a simple weighted automaton encoding four possible pronunciations of
the word about and associating a weight to each.

p([@baut]) = 0.336 (0.84     1     1     0.4     1)
p([@bau]) = 0.504 (0.84     1     1     0.6)
p([baut]) = 0.064 (0.16     1     0.4     1)
p([bau]) = 0.096 (0.16     1     0.6)

 

 

4.1 different weight structures
we need not restrict ourselves to interpreting the weights as probabilities only. in
fact, raw id203 values are rarely used in natural language processing because

aside: id48s and prob. automata

are equivalent (though automata may be more compact)
2004
transformation of a id48 into an equivalent pnfa

links between pa and id48s

0.04

0.1

11

0.1

21

0.9

[a 0.2]
[b 0.8]
0.7

0.7

[a 0.8]
[b 0.2]

0.42

0.36

12

[a 0.3]
[b 0.7]

0.3

0.9

 

0.3

22

[a 0.9]
[b 0.1]

0.18

a 0.02

b 0.08

0.04

11

b 0.02 a 0.08

a 0.18
b 0.72

a 0.21

b  0.49

0.36

12

b 0.21

a 0.09

a 0.72

b 0.18

a 0.63
b 0.07

22

0.18

21

0.42

a 0.27

b 0.03

 (q, a, q ) = b(q, a)a(q, q )

probabilistic automata

weighted automata
from probabilistic to weighted

probabilistic automaton:

as always, we would prefer using(negative) logprobs, since this makes 
calculations easier:

-log(0.16)     1.8326
-log(0.84)     0.1744
-log(1) = 0
-log(0) =    

since the more probable is now numerically smaller, we call them weights

 

 

semirings

paths with the same input labels is de   ned as abstract addition, and denoted by  .
a complete system for de   ning the behavior of a weighted automaton is usu-
ally encoded in a structure called a semiring. without going into too much math-
ematical detail, this entails de   ning    ve parameters: (s, ,   , 0, 1). here, s is
the set over which we operate (called the carrier set), which, for example, is the
set of real numbers r if we are dealing with probabilities   i.e. working with the
id203 semiring   as in the introductory examples above. all possible sums
and products of individual weights need to be members of this set. the operators
  and     represent the weight combination operations along and across paths: we
multiply (   ) the weights along paths, and add ( ) across paths. also, the values
0 and 1 (which need to be members of s) are abstract zeroes and ones for the
abstract addition and multiplication operations. in other words, they are identity
elements for   and     respectively: for all values s in our carrier set s, s   0
= s
needs to equal s, and s     1, also needs to equal s. also, s     0 needs to equal 0.
=
there are some additional constraints for a semiring algebraic structure   abstract

negative log weights case. the operation for combining values of several different
paths with the same input labels is de   ned as abstract addition, and denoted by  .
negative log weights case. the operation for combining values of several different
a complete system for de   ning the behavior of a weighted automaton is usu-
paths with the same input labels is de   ned as abstract addition, and denoted by  .
ally encoded in a structure called a semiring. without going into too much math-
a complete system for de   ning the behavior of a weighted automaton is usu-
ematical detail, this entails de   ning    ve parameters: (s, ,   , 0, 1). here, s is
ally encoded in a structure called a semiring. without going into too much math-
the set over which we operate (called the carrier set), which, for example, is the
ematical detail, this entails de   ning    ve parameters: (s, ,   , 0, 1). here, s is
set of real numbers r if we are dealing with probabilities   i.e. working with the
the set over which we operate (called the carrier set), which, for example, is the
id203 semiring   as in the introductory examples above. all possible sums
set of real numbers r if we are dealing with probabilities   i.e. working with the
id203 semiring   as in the introductory examples above. all possible sums
and products of individual weights need to be members of this set. the operators
and products of individual weights need to be members of this set. the operators
  and     represent the weight combination operations along and across paths: we
  and     represent the weight combination operations along and across paths: we
multiply (   ) the weights along paths, and add ( ) across paths. also, the values
multiply (   ) the weights along paths, and add ( ) across paths. also, the values
0 and 1 (which need to be members of s) are abstract zeroes and ones for the
0 and 1 (which need to be members of s) are abstract zeroes and ones for the
abstract addition and multiplication operations. in other words, they are identity
abstract addition and multiplication operations. in other words, they are identity
elements for   and     respectively: for all values s in our carrier set s, s   0
elements for   and     respectively: for all values s in our carrier set s, s   0
needs to equal s, and s     1, also needs to equal s. also, s     0 needs to equal 0.
needs to equal s, and s     1, also needs to equal s. also, s     0 needs to equal 0.
there are some additional constraints for a semiring algebraic structure   abstract
there are some additional constraints for a semiring algebraic structure   abstract

negative log weights case. the operation for combining values of several different
paths with the same input labels is de   ned as abstract addition, and denoted by  .
a complete system for de   ning the behavior of a weighted automaton is usu-
ally encoded in a structure called a semiring. without going into too much math-
ematical detail, this entails de   ning    ve parameters: (s, ,   , 0, 1). here, s is
the set over which we operate (called the carrier set), which, for example, is the
set of real numbers r if we are dealing with probabilities   i.e. working with the
id203 semiring   as in the introductory examples above. all possible sums
and products of individual weights need to be members of this set. the operators
  and     represent the weight combination operations along and across paths: we
multiply (   ) the weights along paths, and add ( ) across paths. also, the values
0 and 1 (which need to be members of s) are abstract zeroes and ones for the
abstract addition and multiplication operations. in other words, they are identity
elements for   and     respectively: for all values s in our carrier set s, s   0
needs to equal s, and s     1, also needs to equal s. also, s     0 needs to equal 0.
there are some additional constraints for a semiring algebraic structure   abstract
multiplication (   ) needs to distribute over addition ( ), multiplication needs to
be associative. put more concisely, the set s together with the zero, one, and ad-

   log is de   ned by: x   log y =     log(e   x +e   y) and     is longest common pre   x.
the string semiring is a left semiring.

    product: to compute the weight of a path (product of the weights of con-

    sum: to compute the weight of a sequence (sum of the weights of the paths

a semiring (k,    ,    , 0, 1) = a ring that may lack negation.

r     {      , +   }    log + +    0

r     {      , +   } min + +    0

labeled with that sequence).

weight sets: semirings

stituent transitions).

= s

part i. algorithms

         

id203

          {   }

preliminaries

semiring

tropical

boolean

string

openfst

{0, 1}

log

set

r+

   

   

+

  

   

   

   

0

1

0

1

0

1

2

semirings

weighted automaton/acceptor

a/1
b/4

a/2
b/1

0

b/1
c/3

b/3
c/5

1/2

2

3/2

id203 semiring (r+, +,   , 0, 1) tropical semiring (r+     {   }, min, +,    , 0)

[[a]](ab) = 14

[[a]](ab) = 4

(1    1    2 + 2    3    2 = 14)

(min(1 + 1 + 2, 3 + 2 + 2) = 4)

openfst

part i. algorithms

preliminaries

3

formal de   nition

de   nitions: weighted automata (1)

  

is an automaton,

  
initial output function ,
output function :
final output function ,
function

:   

  

,

associated with

:

.

m.mohri-m.riley-r.sproat

algorithms for id103 and language processing

part i

7

weighted transducers

intuition

figure 7: a simple weighted transducer representing a pronunciation lexicon
where sequences of phones are mapped to words.

5.1 properties of weighted machines
the algorithms regarding determinization and minimization do not directly trans-
fer to weighted automata. in fact, not every weighted automaton is determiniz-
able, although all acyclic ones are. with weighted transducers, many operations   
including sequentialization   are only possible with certain weight structures. for

weighted transducers

semirings

weighted transducer

a:  /1
a:r/3

0

1/2

2

b:r/2
b:  /2
c:s/1

3/2

id203 semiring (r+, +,   , 0, 1) tropical semiring (r+     {   }, min, +,    , 0)

[[t ]](ab, r) = 16

[[t ]](ab, r) = 5

(1    2    2 + 3    2    2 = 16)

(min(1 + 2 + 2, 3 + 2 + 2) = 5)

openfst

part i. algorithms

preliminaries

4

weighted transducers

formal de   nition

de   nitions: transducers (1)

      
finite alphabets    and    ,
finite set of states
,
transition function :
output function :

  
set of initial states,
set of    nal states.

de   nes a relation:

   2 :

  

2 ,
   ,

m.mohri-m.riley-r.sproat

algorithms for id103 and language processing

part i

10

operations on weighted automata

booleans

union: example

b/1

0 

a/3

b/5

1 

0 

a/5

1 

b/6

b/2

a/3

10 

  /0

  /0

b/5

a/3

0 

b/1

6 

1 

a/5

7 

b/6

b/2

c/0

2 

b/2

a/3

b/2

a/3

2 

a/6

c/0

2 

a/3

8 

a/6

b/7
a/4

a/4
b/7

c/1

3 

c/1

3 

9 /0

3 /0

b/3

4 

a/4

5 /0

b/3

4 

a/4

5 /0

figure 13: union of weighted automata (min

).

composition

x

t

y
u

z

x

t     u

z

composition

x

t

y
u

z

x

t     u

z

multiplicative ~ p(y|x) p(z|y)

composition

composition: example (2)

a

b

0 

0 

a:a/3

1 

b:  /1

2 

c:  /4

a:d/5

1 

:e   /7

d:a/6

2 

d:d/2

4 

3 

3 

a o b

(0,0)

a:d/15

(1,1)

b:e/7

(2,2)

c:  /4

(3,2)

d:a/12

(4,3)

figure 8: composition of weighted transducers (

).

m.mohri-m.riley-r.sproat

algorithms for id103 and language processing

part i

17

determinization: motivation (2)

determinization

leave/44.4

leave/61.9

leaves/51.4

leave/64.6

10 

detroit/110

detroit/109

detroit/106

15 /0

14 

detroit/91.9

detroit/91.6

detroit/88.5

detroit/103
detroit/102

detroit/99.1
detroit/106

detroit/105

detroit/102

detroit/99.7

detroit/99.4

detroit/96.3

12 

11 

13 

which/69.9

which/81.6

0 

which/72.9

which/77.7

flight/72.4

7 

1 

4 

2 

3 

flights/64
flights/61.8

flights/54.3

flights/53.5
flight/43.7
flights/50.2

flights/88.2

flight/45.4

flights/83.8

flights/83.4
flights/79

6 

5 

9 

8 

leaves/50.7

leave/68.9
leave/82.1

leave/45.8

leave/47.4

leave/57.7

leave/70.9
leaves/34.6

leaves/39.2

leave/53.4

leave/54.4

leaves/67.6

leave/31.3

leave/35.9
leaves/60.4

leave/73.6

leave/37.3

figure 14: toy language model (16 states, 53 transitions, 162 paths).

language model: 16 states, 53 transitions

leave/41.9

determinization

determinization: motivation (3)

0 

which/69.9

1 

flights/53.1

flight/53.2

2 

3 

leave/64.6

leaves/62.3

leave/63.6
leaves/67.6

4 

5 

6 

7 

detroit/103

detroit/105
detroit/105
detroit/101

8 /0

figure 15: determinized language model (9 states, 11 transitions, 4 paths).

same language model: 9 states, 11 transitions

minimization

by weight pushing

minimization: example (2)

a:3

b:2

a:3

b:0

a:3
b:0

d:0

1 

d:0

1 

d:0

1 

c:2

c:1

c:0

c:0

c:0

2 

d:4
d:3

4 

2 

d:6
d:6

4 

2 

d:6

0 

0 

0 

a:0
b:1

a:6
b:7

a:6
b:7

e:3
e:2

e:0
e:0

e:0

c:1

6 

c:1

6 

c:1

4 

3 

5 

3 

5 

3 

e:1

7 

e:0

e:0

7 

5 

figure 24: minimization of weighted automata (min

).

composition: example (2)

projection
b:  /1

2 

c:  /4

a:a/3

1 

a:d/5

1 

:e   /7

d:a/6

2 

0 

0 

a:d/15

c:  /4
(0,0)
trivial: just delete at in/out labels

b:e/7

(2,2)

(1,1)

d:d/2

4 

3 

3 

(3,2)

d:a/12

(4,3)

figure 8: composition of weighted transducers (

).

m.mohri-m.riley-r.sproat

algorithms for id103 and language processing

part i

example application

probabilistic spell checking

cat/0.001

cat/0.000035

language model

p(w)

cat/0.001

error model

p(o|w)

cxat/0.000035

cxat/0.000035

example application

constructing p(w) and p(o|w)

p(w) can be a id165 language model
converted to a transducer, easily estimated from data
p(o|w) is much more dif   cult

what   s the id203 of confusing    a    with    z   

is this word-dependent? context-dependent?

example application

example unigram language model (in 

more complex lm (still unigram)
kleene* weighted fst language)

$lm = ( the<3.3123733563043>|
             you<3.40834334278697>|
             i<3.47764362842074>|
             a<3.62151061674717>|
             to<3.74035111367985>|
             and<4.12455498051775>|
             of<4.2521768299548>|
             ...

unigram model from the simpsons word frequency list
(http://pastebin.com/ankcmdvk)

 

 

http://www.kleene-lang.org/

example application

example: weighted edit 
distance

example: weighted edit 
$rep = . ; $ins = "":.; $del = .:""; $chg = .:.-.;
distance
$em = ( $rep<0.0> | $ins<1.0> | $del<1.0> | 
$chg<1.0> )*; 

simple error model (insertion/deletion/replacements have a weight of one)

$corr = (cxat) _o_$em _o_ $lm;
// fsttype: vector, semiring: standard, 50 states, 139 
arcs, 938 paths, transducer, weighted, closed sigma

$corr = $^shortestpath( (cxat) _o_$em _o_ $lm );

errormodel.kl

 

 

   argmax   

composition

 

 

example application

example: weighted edit 
distance

example: weighted edit 
$rep = . ; $ins = "":.; $del = .:""; $chg = .:.-.;
distance
$em = ( $rep<0.0> | $ins<1.0> | $del<1.0> | 
$chg<1.0> )*; 

simple error model (insertion/deletion/replacements have a weight of one)

$corr = (cxat) _o_$em _o_ $lm;
// fsttype: vector, semiring: standard, 50 states, 139 
arcs, 938 paths, transducer, weighted, closed sigma

$corr = $^shortestpath( (cxat) _o_$em _o_ $lm );

= cat

errormodel.kl

 

 

   argmax   

composition

 

 

example application

example: weighted edit 
distance

example: weighted edit 
$rep = . ; $ins = "":.; $del = .:""; $chg = .:.-.;
distance
$em = ( $rep<0.0> | $ins<1.0> | $del<1.0> | 
$chg<1.0> )*; 

simple error model (insertion/deletion/replacements have a weight of one)

$corr = (cxat) _o_$em _o_ $lm;
// fsttype: vector, semiring: standard, 50 states, 139 
arcs, 938 paths, transducer, weighted, closed sigma

$corr = $^shortestpath( (cxat) _o_$em _o_ $lm );

= cat

errormodel.kl

 

 

   argmax   

composition

what about    home   ? does that get corrected and how?

 

 

id103

the id87

id87 for asr

search through space of all possible sentences.
pick the one that is most probable given the
waveform.

lsa 352 summer 2007

8

evaluation

evaluation

evaluation

asr birds-eye view

how to evaluate the word string output by a speech
recognizer?

how to evaluate the word string output by a speech
recognizer?

how to evaluate the word string output by a speech
recognizer?

example of recognition cascade
id103

[  f]/0.0001

if/0.000034

if/0.0000045

o

observations

a

phones

d

words

m

(mfccs)

lsa 352 summer 2007

61

lsa 352 summer 2007

62

lsa 352 summer 2007

lsa 352 summer 2007

word error rate

100 (insertions+substitutions + deletions)

100 (insertions+substitutions + deletions)

ref:   portable ****     phone upstairs last night so
hyp:   portable form  of       stores    last night so

61

recognition from observations o by composition:

61

lsa 352 summer 2007

lsa 352 summer 2007

62

62

    observations:

s s

o
1 if s
0 otherwise

nist sctk-1.3 scoring softare:
a p
computing wer with sclite
p w

    acoustic-phone transducer:
    pronunciation dictionary:
nist sctk-1.3 scoring softare:
w w
    language model:
computing wer with sclite
recognition:   w argmax

nist sctk-1.3 scoring softare:
computing wer with sclite

sclite aligns a hypothesized text (hyp) (from the recognizer)
with a correct or reference text (ref) (human transcribed)

a p
p w
w

http://www.nist.gov/speech/tools/

ref:  was an engineer so i   i was always with **** **** men um   and they

http://www.nist.gov/speech/tools/

o w

scores: (#c #s #d #i) 9 3 1 2

id: (2347-b-013)

http://www.nist.gov/speech/tools/
w
sclite aligns a hypothesized text (hyp) (from the recognizer)

sclite aligns a hypothesized text (hyp) (from the recognizer)

hyp:  was an engineer ** and i was always with them they all that and they

eval:                 d  s                     i    i    s   s

slightly more detail
speech models as weighted automata

quantized observations:

t0

phone model

t1

o1
o2
: observations
oi:  /p01(i)

on

tn

t2

. . .
phones

...

s0
...

oi:  /p12(i)

...

  :  /p2f

s2
...

s1
...

oi:  /p11(i)

oi:  /p22(i)

oi:  /p00(i)
acoustic transducer:
word pronunciations
d:  /1

dictionary:

words

data : phones
ey:  /.4
ae:  /.6

dx:  /.8
t:  /.2

ax:"data"/1

