 

natural language 

processing 

 

lecture 15   3/5/2015 

martha palmer 

 
 

today 

!    review cky 
!    earley 
!    partial parsing 

!    finite-state methods 
!    chunking 

!    sequence labeling methods 

3/5/15 

                                         speech and language processing - jurafsky and martin        

2 

cky algorithm 

looping over the columns 

filling the bottom cell 

filling row i in column j 

looping over the possible split locations 
between i and j. 

check the grammar for rules that 
link the constituents in [i,k] with 
those in [k,j]. for each rule 
found store the lhs of the rule in 
cell [i,j].  

3/5/15 

                                         speech and language processing - jurafsky and martin        

3 

example 

3/5/15 

                                         speech and language processing - jurafsky and martin        

4 

example 

filling column 5 

3/5/15 

                                         speech and language processing - jurafsky and martin        

5 

example 

3/5/15 

                                         speech and language processing - jurafsky and martin        

6 

example 

3/5/15 

                                         speech and language processing - jurafsky and martin        

7 

example 

3/5/15 

                                         speech and language processing - jurafsky and martin        

8 

example 

3/5/15 

                                         speech and language processing - jurafsky and martin        

9 

note 
!    an alternative is to fill a 

diagonal at a time. 
!    that still satisfies our 
requirement that the component 
parts of each constituent/cell will 
already be available when it is 
filled in. 

3/5/15 

                                         speech and language processing - jurafsky and martin        

10 

cky notes 

!    since it   s bottom up, cky populates the 
table with a lot of phantom constituents. 
!    segments that by themselves are constituents 
but cannot really occur in the context in which 
they are being suggested. 
!    to avoid this we can switch to a top-down 
control strategy 
!    or we can add some kind of filtering that 
blocks constituents where they can not 
happen in a final analysis. 

3/5/15 

                                         speech and language processing - jurafsky and martin        

11 

id117 

!    allows arbitrary id18s 
!    top-down control 
!    fills a table in a single sweep over the 

input 
!    table is length n+1; n is number of words 
!    table entries represent 

!    completed constituents and their locations 
!    in-progress constituents 
!    predicted constituents 
 

3/5/15 

                                         speech and language processing - jurafsky and martin        

12 

states 

!    the table-entries are called states and are 

represented with dotted-rules. 
s        vp
np     det    nominal
vp     v np     

 a vp is predicted 
 an np is in progress 
 a vp has been found 

 

 

 

 

3/5/15 

                                         speech and language processing - jurafsky and martin        

13 

states/locations 

!    s         vp [0,0] 

 
!    np     det     nominal 

[1,2] 

 

!    vp     v np       [0,3] 

!    a vp is predicted at the start 

of the sentence 

 
 
!    an np is in progress; the det 

goes from 1 to 2 

 
 
!    a vp has been found starting 

at 0 and ending at 3 

 

3/5/15 

                                         speech and language processing - jurafsky and martin        

14 

earley 

!    as with most id145 
approaches, the answer is found by 
looking in the table in the right place. 

!    in this case, there should be an s state in 

the final column that spans from 0 to n 
and is complete.  that is, 
!    s            [0,n] 

!    if that   s the case you   re done. 

3/5/15 

                                         speech and language processing - jurafsky and martin        

15 

earley 

!    so sweep through the table from 0 to n    
!    new predicted states are created by starting 
top-down from s 
!    new incomplete states are created by 
advancing existing states as new constituents 
are discovered 
!    new complete states are created in the same 
way.  

3/5/15 

                                         speech and language processing - jurafsky and martin        

16 

earley 

!    more specifically    

1.   predict all the states you can upfront 
2.    read a word 

1.    extend states based on matches 
2.    generate new predictions 
3.    go to step 2 

3.    when you   re out of words, look at the chart 

to see if you have a winner 

3/5/15 

                                         speech and language processing - jurafsky and martin        

17 

core earley code 

3/5/15 

                                         speech and language processing - jurafsky and martin        

18 

earley code 

3/5/15 

                                         speech and language processing - jurafsky and martin        

19 

example 

!    book that flight 
!    we should find    an s from 0 to 3 that is a 

completed state    

 

3/5/15 

                                         speech and language processing - jurafsky and martin        

20 

chart[0] 

note that given a grammar, these entries are 
the same for all inputs; they can be pre-loaded. 

3/5/15 

                                         speech and language processing - jurafsky and martin        

21 

chart[1] 

3/5/15 

                                         speech and language processing - jurafsky and martin        

22 

charts[2] and [3] 

3/5/15 

                                         speech and language processing - jurafsky and martin        

23 

efficiency 

!    for such a simple example, there seems to 

be a lot of useless stuff in there. 

!    why? 

       it   s predicting things that aren   t consistent 
with the input  
      that   s the flipside to the cky problem. 

3/5/15 

                                         speech and language processing - jurafsky and martin        

24 

details 

!    as with cky that isn   t a parser until we 
add the backpointers so that each state 
knows where it came from. 

3/5/15 

                                         speech and language processing - jurafsky and martin        

25 

