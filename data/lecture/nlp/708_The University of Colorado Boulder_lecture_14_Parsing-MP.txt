 

natural language 

processing 

 

lecture 14   3/2/2015 

martha palmer 

 
 

today 

!    start on parsing 

!    top-down vs. bottom-up 
!    cky 

3/3/15 

                                         speech and language processing - jurafsky and martin        

2 

top-down vs. bottom-up 

!    helps with pos 

ambiguities     only 
consider relevant 
pos 

!    rebuilds the same 
structure repeatedly 
!    spends a lot of time 
on impossible parses 
(trees that are not 
consistent with any of 
the words) 

!    has to consider every 

pos 

!    builds each structure 

once 

!    spends a lot of time on 
useless structures (trees 
that make no sense 
globally) 

what would be better? 

                                    
nlp 

3 

id145 

!    dp search methods fill tables with partial results 

and thereby 
!    avoid doing avoidable repeated work 
!    solve exponential problems in polynomial time 
!    efficiently store ambiguous structures with shared sub-

parts. 

!    we   ll cover two approaches that roughly 
correspond to top-down and bottom-up 
approaches. 
!    cky 
!    earley 

3/3/15 

                                         speech and language processing - jurafsky and martin        

4 

cky parsing 

!    first we   ll limit our grammar to epsilon-

free, binary rules  
!    consider the rule a      bc 

!    if there is an a somewhere in the input 
generated by this rule then there must be 
a b followed by a c in the input. 
!    if the a spans from i to j in the input then 
there must be some k st. i<k<j 
!    in other words, the b splits from the c 
someplace after the i and before the j. 

3/3/15 

                                         speech and language processing - jurafsky and martin        

5 

grammar rules in cnf 

3/3/15 

                                         speech and language processing - jurafsky and martin        

6 

cky 

!    let   s build a table so that an a spanning 
from i to j in the input is placed in cell [i,j] 
in the table. 
!    so a non-terminal spanning an entire string 
will sit in cell [0, n] 
!    hopefully it will be an s 

!    now we know that the parts of the a must 
go from i to k and from k to j, for some k 

3/3/15 

                                         speech and language processing - jurafsky and martin        

7 

cky 

!    meaning that for a rule like a     b c we 
should look for a b in [i,k] and a c in [k,j]. 
!    in other words, if we think there might be 

an a spanning i,j in the input    and  

   a     b c is a rule in the grammar then 
!    there must be a b in [i,k] and a c in [k,j] 

for some k such that i<k<j 
 
what about the b and the c? 

3/3/15 

                                         speech and language processing - jurafsky and martin        

8 

cky 

!    so to fill the table loop over the cell [i,j] 

values in some systematic way 
!    then for each cell, loop over the appropriate k 
values to search for things to add. 
!    add all the derivations that are possible for 
each [i,j] for each k 

3/3/15 

                                         speech and language processing - jurafsky and martin        

9 

bottom-up search 

                                         
speech and 

10 

cky table 

3/3/15 

                                         speech and language processing - jurafsky and martin        

11 

example 

3/3/15 

                                         speech and language processing - jurafsky and martin        

12 

cky algorithm 

3/3/15 

                                         speech and language processing - jurafsky and martin        

13 

cky algorithm 

looping over the columns 

filling the bottom cell 

filling row i in column j 

looping over the possible split locations 
between i and j. 

check the grammar for rules that 
link the constituents in [i,k] with 
those in [k,j]. for each rule 
found store the lhs of the rule in 
cell [i,j].  

3/3/15 

                                         speech and language processing - jurafsky and martin        

14 

example 

!    filling column 5 corresponds to processing 

word 5, which is houston. 
!    so j is 5. 
!    so i goes from 3 to 0 (3,2,1,0) 
 

3/3/15 

                                         speech and language processing - jurafsky and martin        

15 

example 

3/3/15 

                                         speech and language processing - jurafsky and martin        

16 

example 

3/3/15 

                                         speech and language processing - jurafsky and martin        

17 

example 

3/3/15 

                                         speech and language processing - jurafsky and martin        

18 

grammar rules in cnf 

3/3/15 

                                         speech and language processing - jurafsky and martin        

19 

example 

3/3/15 

                                         speech and language processing - jurafsky and martin        

20 

example 

!    since there   s an s in [0,5] we have a valid 

parse. 

!    are we done?  well, we sort of left 

something out of the algorithm 

3/3/15 

                                         speech and language processing - jurafsky and martin        

21 

cky notes 

!    since it   s bottom up, cky hallucinates a lot 

of silly constituents. 
!    segments that by themselves are constituents 
but cannot really occur in the context in which 
they are being suggested. 
!    to avoid this we can switch to a top-down 
control strategy 
!    or we can add some kind of filtering that 
blocks constituents where they can not 
happen in a final analysis. 

3/3/15 

                                         speech and language processing - jurafsky and martin        

22 

cky notes 

!    we arranged the loops to fill the table a 

column at a time, from left to right, 
bottom to top.  
!    this assures us that whenever we   re filling a 
cell, the parts needed to fill it are already in 
the table (to the left and below) 
!    it   s somewhat natural in that it processes the 
input left to right a word at a time 
!    known as online 

!    can you think of an alternative strategy? 
 

3/3/15 

                                         speech and language processing - jurafsky and martin        

23 

projects 

!    project proposals due march 12  
 
!    1 page writeup of topic and approach, + 
citations of selected papers, with 1 partner 

3/3/15 

                                         speech and language processing - jurafsky and martin        

24 

!    mohammed & yasmeen, arabic srl & ml 
!    michael     srl, how to integrate syntax & 

semantics, luc steels 

!    matt     id86, features, stages 
!    oliver    german parsing, ml, ir 
!    garret     deep learning for speech 

recognition 

!    nelson     id103, mari olsen 

uw, use of nlp?, nuance 

3/3/15 

                                         speech and language processing - jurafsky and martin        

25 

!    melissa & nima, text and images, 

automatic captioning 

!    kinjal     office 
!    harsha     nlp for social media, google 
multlingual id52 and parsing 
(universal) 

!    betty     ir, twitter, facebook 
!    rick     mt, how to scale up  
!    megan     writing a grammar     german,  
!    sarah     speech, comparing models 

                                         speech and language processing - jurafsky and martin        

3/3/15 

26 

!    keyla     id103 w/ garrett 
!    ryan     vector space models, nyu 

convolutional neural network, grammar 
induction 

!    audrey w/ megan     temporal realtions  
!    allison    nlp for sociolinguistics research 
!    ross  - word prediction 
!    megan w/ audrey     bioinformatics  

3/3/15 

                                         speech and language processing - jurafsky and martin        

27 

makeup exam 
!    march 16, monday , 12     1:15 

3/3/15 

                                         speech and language processing - jurafsky and martin        

28 

