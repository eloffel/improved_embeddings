 

natural language 

processing 

 

lecture 17.2   3/12/2015 

martha palmer 

 
 

probabilistic id18s 

1 the framework (model) 

!    how to assign probabilities to parse trees 

2 training the model (learning) 

!    how to acquire estimates for the probabilities 
specified by the model 

3 parsing with probabilities (decoding) 

!    given an input sentence and a model how can 
we efficiently find the best (or n best) tree(s) 
for that input 

back to id18 parsing 
!    we left id18 parsing (cky) with the 

problem of selecting the    right    parse out 
of all the possible parses... 

!    now if we reduce    right    parse to    most 

probable parse    we get our old friend 

!    argmax p(parse|words) 
 

3/12/15 

                                         speech and language processing - jurafsky and martin        

simple id203 model 

!    a derivation (tree) consists of the 

collection of grammar rules that are in the 
tree 
!    the id203 of a tree is the product of the 
probabilities of the rules in the derivation. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

3 

3/12/15 

                                         speech and language processing - jurafsky and martin        

2 

4 

1 

example 

!    how many    rules    are 

in this derivation? 

rule probabilities 

!    so... what   s the id203 of a rule? 
!    start at the top... 

!    a tree should have an s at the top. so given 
that we know we need an s, we can ask 
about the id203 of each particular s rule 
in the grammar. 
!    that is p(particular s rule | s is what i need) 
p(        |  )

!    so in general we need 

3/12/15 

                                         speech and language processing - jurafsky and martin        

5 

3/12/15 

                                         speech and language processing - jurafsky and martin        

   for each rule in the grammar   

training the model 

!    we can get the estimates we need from 
an annotated database (i.e., a treebank) 
 

!    for example, to get the id203 for a 
particular vp rule, just count all the times the 
rule is used and divide by the number of vps 
overall. 

    

parsing (decoding) 

!    so to get the best (most probable) parse 

for a given input 

 

1.    enumerate all the trees for a sentence 
2.    assign a id203 to each using the model 
3.    return the best (argmax) 

3/12/15 

                                         speech and language processing - jurafsky and martin        

7 

3/12/15 

                                         speech and language processing - jurafsky and martin        

6 

8 

2 

example 

!    consider... 

!    book the dinner flight 

examples 

!    these trees consist of the following rules. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

9 

3/12/15 

                                         speech and language processing - jurafsky and martin        

10 

example 

id145 
!     of course, as with normal parsing we 
don   t really want to do it that way... 

 
!    instead, we need to exploit dynamic 

programming 
!    for the parsing (as with cky) 
!    and for computing the probabilities and 
returning the best parse (as with viterbi and 
id48s) 

3/12/15 

                                         speech and language processing - jurafsky and martin        

11 

3/12/15 

                                         speech and language processing - jurafsky and martin        

12 

3 

probabilistic cky 

probabilistic cky 

!    alter cky so that the probabilities of 

constituents are stored in the table as they 
are derived 
!    id203 of a new constituent a derived 
from the rule a     b c : 
!    p(a     b c | a) * p(b) * p(c) 
!    where p(b) and p(c) are already in the table given 
the way that cky operates 
!    what we store is the max id203 over all the a 
rules. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

13 

3/12/15 

                                         speech and language processing - jurafsky and martin        

14 

for a detailed example 

problems with pid18s 

!    chris manning on youtube: 

!    https://www.youtube.com/watch?

v=hq80j8kbg-y 

!    the id203 model we   re using is just 

based on the the bag of rules in the 
derivation    
1.    doesn   t take the actual words into account 

in any useful way. 

2.    doesn   t take into account where in the 

derivation a rule is used 
3.    doesn   t work terribly well 

!    that is, the most probable parse isn   t usually the 

right one (the one in the treebank test set). 

3/12/15 

                                         speech and language processing - jurafsky and martin        

15 

3/12/15 

                                         speech and language processing - jurafsky and martin        

16 

4 

specific problems 

pp attachment 

!    attachment ambiguities 

!    pp attachment 
!    coordination problems 

3/12/15 

                                         speech and language processing - jurafsky and martin        

17 

3/12/15 

                                         speech and language processing - jurafsky and martin        

18 

pp attachment 

!    another view. 

coordination 

most grammars have a rule (implicitly) of 
the form  
x -> x and x. this leads to massive 
ambiguity problems. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

19 

3/12/15 

                                         speech and language processing - jurafsky and martin        

20 

5 

improved approaches to 

statistical parsing 

!    we   ll look at two approaches to overcoming 

these shortcomings 
1.    rewrite the grammar to better capture the 

dependencies among rules  

2.    integrate lexical dependencies into the model 
1.    and come up with the independence assumptions 

needed to make it work. 

 

solution 1: rule rewriting 
!    the grammar rewriting approach attempts to 
capture local tree information by rewriting the 
grammar so that the rules capture the 
regularities we want. 

 

!    by splitting and merging the non-terminals in the 

grammar. 
!    example: split nps into different classes    

!    remember, we rewrote the grammar rules for 
cky, and we rewrote the iob tags. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

21 

3/12/15 

                                         speech and language processing - jurafsky and martin        

22 

example: nps 

!    our id18 rules for nps don   t condition on 

where in a tree the rule is applied 

!    but we know that not all the rules occur 

with equal frequency in all contexts. 
!    consider nps that involve pronouns vs. those 
that don   t. 

other examples 

!    there are lots of other examples like this 

in any treebank 
!    many at the part of speech level 
!    recall that many decisions made in 
annotation efforts are directed towards 
improving annotator agreement, not towards 
doing the right thing. 
!    often this involves conflating distinct classes into a 
larger class 
!    to, in, det, etc. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

23 

3/12/15 

                                         speech and language processing - jurafsky and martin        

24 

6 

rule rewriting 

!    three approaches 

!    use linguistic knowledge to directly rewrite 
rules by hand 
!    np_obj and the np_subj approach 

!    automatically rewrite the rules using context 
to capture some of what we want 
!    ie. incorporate context into a context-free 
approach 

!    search through the space of rewrites for the 
grammar that maximizes the id203 of the 
training set 

local context approach 

!    condition the rules based on their parent 

nodes 
!    this splitting based on tree-context captures 
some of the linguistic intuitions 

3/12/15 

                                         speech and language processing - jurafsky and martin        

25 

3/12/15 

                                         speech and language processing - jurafsky and martin        

26 

parent annotation 

parent annotation 

!    now we have non-terminals np^s and np^vp that 

should capture the subject/object and pronoun/full np 
cases. that is... 
!    the rules are now 

!    np^s -> prp 
!    np^vp -> dt 
!    vp^s -> np^vp 

!    recall what   s going on here. we   re in effect rewriting 
!    and changing the probabilities since they   re being 

the treebank, thus rewriting the grammar. 

derived from different counts    
!    and if we   re splitting what   s happening to the counts? 

3/12/15 

                                         speech and language processing - jurafsky and martin        

27 

3/12/15 

                                         speech and language processing - jurafsky and martin        

28 

7 

auto rewriting 

!    if this is such a good idea we may as well 

apply a learning approach to it. 

!    start with a grammar (perhaps a treebank 

grammar) 

!    search through the space of splits/merges 

for the grammar that in some sense 
maximizes parsing performance on the 
training/development set.  

auto rewriting 

!    basic idea     

!    split every non-terminal into two new non-
terminals across the entire grammar (x 
becomes x1 and x2). 
!    duplicate all the rules of the grammar that 
use x, dividing the id203 mass of the 
original rule almost equally.  
!    run em to readjust the rule probabilities 
!    perform a merge step to back off the splits 
that look like they don   t really do any good. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

29 

3/12/15 

                                         speech and language processing - jurafsky and martin        

30 

solution 2:  

lexicalized grammars 
!    lexicalize the grammars with heads 
!    compute the rule probabilities on these 

lexicalized rules 

!    run prob cky as before 

dumped example 

3/12/15 

                                         speech and language processing - jurafsky and martin        

31 

3/12/15 

                                         speech and language processing - jurafsky and martin        

32 

8 

how? 

declare independence 

!    we used to have 

!    vp -> v np pp 

 

 p(rule|vp) 

!    that   s the count of this rule divided by the number 
of vps in a treebank 

!    now we have fully lexicalized rules... 

!    vp(dumped)-> v(dumped) np(sacks)pp(into) 
p(r|vp ^ dumped is the verb ^ sacks is the 

head of the np ^ into is the head of the pp) 

to get the counts for that.. 

!    when stuck, exploit independence and 

collect the statistics you can    

!    there are a larger number of ways to do 

this... 

!    let   s consider one generative story: 

given a rule we   ll 
1.    generate the head 
2.    generate the stuff to the left of the head 
3.    generate the stuff to the right of the head 

3/12/15 

                                         speech and language processing - jurafsky and martin        

33 

3/12/15 

                                         speech and language processing - jurafsky and martin        

34 

example 

!    so the id203 of a lexicalized rule such 

as  
!    vp(dumped)      v(dumped)np(sacks)pp(into) 

!    is the product of the id203 of 

!       dumped    as the head  
!    with nothing to its left 
!       sacks    as the head of the first right-side thing 
!       into    as the head of the next right-side 
element 
!    and nothing after that 

example 
!    that is, the rule id203 for 

is estimated as 
 

3/12/15 

                                         speech and language processing - jurafsky and martin        

35 

3/12/15 

                                         speech and language processing - jurafsky and martin        

36 

9 

framework 
!    that   s just one simple model 

!    collins model 1 

!    you can imagine a gazzillion other 

assumptions that might lead to better 
models 

!    you just have to make sure that you can 

get the counts you need 

!    and that it can be used/exploited 

efficiently during decoding 

3/12/15 

                                         speech and language processing - jurafsky and martin        

37 

10 

