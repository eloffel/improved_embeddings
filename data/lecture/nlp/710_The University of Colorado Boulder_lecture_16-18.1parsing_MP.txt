 

natural language 

processing 

 

lecture 16, 17, 18.1   3/7/2015     

3/12/2015 

martha palmer 

 
 

today 

!    review cky 
!    earley 
!    partial parsing 

!    finite-state methods 
!    chunking 

!    sequence labeling methods 

3/12/15 

                                         speech and language processing - jurafsky and martin        

cky algorithm 

example 

looping over the columns 

filling the bottom cell 

filling row i in column j 

looping over the possible split locations 
between i and j. 

check the grammar for rules that 
link the constituents in [i,k] with 
those in [k,j]. for each rule 
found store the lhs of the rule in 
cell [i,j].  

3/12/15 

                                         speech and language processing - jurafsky and martin        

3 

3/12/15 

                                         speech and language processing - jurafsky and martin        

2 

4 

1 

example 

example 

filling column 5 

3/12/15 

                                         speech and language processing - jurafsky and martin        

5 

3/12/15 

                                         speech and language processing - jurafsky and martin        

example 

example 

3/12/15 

                                         speech and language processing - jurafsky and martin        

7 

3/12/15 

                                         speech and language processing - jurafsky and martin        

6 

8 

2 

example 

note 
!    an alternative is to fill a 

diagonal at a time. 
!    that still satisfies our 
requirement that the component 
parts of each constituent/cell will 
already be available when it is 
filled in. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

9 

3/12/15 

                                         speech and language processing - jurafsky and martin        

10 

cky notes 

id117 

!    since it   s bottom up, cky populates the 
table with a lot of phantom constituents. 
!    segments that by themselves are constituents 
but cannot really occur in the context in which 
they are being suggested. 
!    to avoid this we can switch to a top-down 
control strategy 
!    or we can add some kind of filtering that 
blocks constituents where they can not 
happen in a final analysis. 

!    allows arbitrary id18s 
!    top-down control 
!    fills a table in a single sweep over the 

input 
!    table is length n+1; n is number of words 
!    table entries represent 

!    completed constituents and their locations 
!    in-progress constituents 
!    predicted constituents 
 

3/12/15 

                                         speech and language processing - jurafsky and martin        

11 

3/12/15 

                                         speech and language processing - jurafsky and martin        

12 

3 

states 

!    the table-entries are called states and are 

represented with dotted-rules. 
s        vp
np     det    nominal
vp     v np     

 a vp is predicted 
 an np is in progress 
 a vp has been found 

 

 

 

 

states/locations 

!    s         vp [0,0] 

 
!    np     det     nominal 

[1,2] 

 

!    vp     v np       [0,3] 

!    a vp is predicted at the start 

of the sentence 

 
 
!    an np is in progress; the det 

goes from 1 to 2 

 
 
!    a vp has been found starting 

at 0 and ending at 3 

 

3/12/15 

                                         speech and language processing - jurafsky and martin        

13 

3/12/15 

                                         speech and language processing - jurafsky and martin        

14 

earley 

!    as with most id145 
approaches, the answer is found by 
looking in the table in the right place. 

!    in this case, there should be an s state in 

the final column that spans from 0 to n 
and is complete.  that is, 
!    s            [0,n] 

!    if that   s the case you   re done. 

earley 

!    so sweep through the table from 0 to n    
!    new predicted states are created by starting 
top-down from s 
!    new incomplete states are created by 
advancing existing states as new constituents 
are discovered 
!    new complete states are created in the same 
way.  

3/12/15 

                                         speech and language processing - jurafsky and martin        

15 

3/12/15 

                                         speech and language processing - jurafsky and martin        

16 

4 

earley 

core earley code 

!    more specifically    

1.   predict all the states you can upfront 
2.    read a word 

1.    extend states based on matches 
2.    generate new predictions 
3.    go to step 2 

3.    when you   re out of words, look at the chart 

to see if you have a winner 

3/12/15 

                                         speech and language processing - jurafsky and martin        

17 

3/12/15 

                                         speech and language processing - jurafsky and martin        

18 

earley code 

example 

!    book that flight 
!    we should find    an s from 0 to 3 that is a 

completed state    

 

3/12/15 

                                         speech and language processing - jurafsky and martin        

19 

3/12/15 

                                         speech and language processing - jurafsky and martin        

20 

5 

chart[0] 

chart[1] 

note that given a grammar, these entries are 
the same for all inputs; they can be pre-loaded. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

21 

3/12/15 

                                         speech and language processing - jurafsky and martin        

22 

charts[2] and [3] 

efficiency 

!    for such a simple example, there seems to 

be a lot of useless stuff in there. 

!    why? 

       it   s predicting things that aren   t consistent 
with the input  
      that   s the flipside to the cky problem. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

23 

3/12/15 

                                         speech and language processing - jurafsky and martin        

24 

6 

details 

!    as with cky that isn   t a parser until we 
add the backpointers so that each state 
knows where it came from. 

back to ambiguity 

!    did we solve it? 

3/12/15 

                                         speech and language processing - jurafsky and martin        

25 

3/12/15 

                                         speech and language processing - jurafsky and martin        

26 

ambiguity 

!    no    

!    both cky and earley will result in multiple s 
structures for the [0,n] table entry. 
!    they both efficiently store the sub-parts that 
are shared between multiple parses. 
!    and they obviously avoid re-deriving those 
sub-parts. 
!    but neither can tell us which one is right. 

ambiguity 

!    in most cases, humans don   t notice 

incidental ambiguity (lexical or syntactic). 
it is resolved on the fly and never 
noticed. 

!    we   ll try to model that with probabilities. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

27 

3/12/15 

                                         speech and language processing - jurafsky and martin        

28 

7 

full syntactic parsing 
!    probably necessary for deep semantic 

analysis of texts (as we   ll see in a couple 
of weeks).  

!    probably not practical for many 

applications (given typical resources) 
!    o(n^3) for straight parsing 
!    o(n^5) for probabilistic versions 
!    too slow for applications that need to process texts in real time 

!    or that need to deal with large volumes of new material over 

(search engines) 

short periods of time 

two alternatives   

!    partial parsing 

!    approximate phrase-structure parsing with 
finite-state and statistical approaches 

!    id33 

!    change the underlying grammar formalism 

!    both of these approaches give up 

something (syntactic structure) in return 
for more robust and efficient parsing 

3/12/15 

                                         speech and language processing - jurafsky and martin        

29 

3/12/15 

                                         speech and language processing - jurafsky and martin        

30 

partial parsing 

examples 

!    for many applications you don   t really 

need a full-blown syntactic parse. you just 
need a good idea of where the base 
syntactic units are. 
!    often referred to as chunks. 

!    for example, if you   re interested in 
locating all the people, places and 
organizations in an english text it can be 
useful to know where all the nps are 
!    because that   s where you   ll find the people, 
places and things 

!    the first two are examples of full partial parsing or chunking. 

all of the elements in the text are part of a chunk. and the 
chunks are non-overlapping. 

!    note how the second example has no hierarchical structure. 
!    the last example illustrates base-np chunking. ignore 

anything that isn   t in the kind of chunk you   re looking for. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

31 

3/12/15 

                                         speech and language processing - jurafsky and martin        

32 

8 

rule-based partial parsing 
!    restrict the form of rules to exclude recursion 
!    group and order the rules so that the rhs of the 

rules can refer to non-terminals introduced in 
earlier transducers, but not later ones. 

we did with the rules for spelling changes. 

!    combine the rules in a group in the same way 
!    combine the groups into a cascade    
!    then compose, determinize and minimize the 

whole thing (optional). 

typical architecture 

!    phase 1:  part of speech tags 
!    phase 2: base syntactic phrases 
!    phase 3: larger verb and noun groups 
!    phase 4: sentential level rules 

3/12/15 

                                         speech and language processing - jurafsky and martin        

33 

3/12/15 

                                         speech and language processing - jurafsky and martin        

34 

partial parsing 

cascaded transducers 

!    no direct or indirect 
recursion allowed in 
these rules. 

!    that is, you can   t 

directly or indirectly 
reference the lhs of 
the rule on the rhs. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

35 

3/12/15 

                                         speech and language processing - jurafsky and martin        

36 

9 

partial parsing 

!    this cascaded approach can be used to 
find the sequence of flat chunks you   re 
interested in. 

!    or it can be used to approximate the kind 

of hierarchical trees you get from full 
parsing with a id18. 

the other way 

!    an alternative approach is to use statistical 

machine learning methods to do partial 
parsing 
!    analogous to the same situation with part-of-
speech tagging 
!    rules vs. id48s 

3/12/15 

                                         speech and language processing - jurafsky and martin        

37 

3/12/15 

                                         speech and language processing - jurafsky and martin        

38 

statistical sequence labeling 
!    as with id52, we can use rules to 
do partial parsing or we can train systems 
to do it for us. to do that we need training 
data and a way to view the problem as a 
classification problem 

 

!    training data 

!    hand tag a bunch of data (as with id52) 
!    or even better, extract partial parse bracketing 
information from a treebank. 

 

encoding 

!    with the right encoding you can turn any 

labeled bracketing task into a tagging 
task. and then proceed exactly as we did 
with id52. 

!    we   ll use what   s called iob labeling to do 

this 
!    i -> inside 
!    o -> outside 
!    b -> begin 

3/12/15 

                                         speech and language processing - jurafsky and martin        

39 

3/12/15 

                                         speech and language processing - jurafsky and martin        

40 

10 

iob encoding 

!    this example shows the encoding for just base-

nps. there are 3 tags in this scheme. 

 

!    this example shows full coverage. in this scheme 

there are 2*n+1 kinds of tags. where n is the 
number of constituents in your set.  

different encodings 

!    voting between multiple data 

representations for text chunking 
hong shen, anoop sarkar, in canadian aai, 
2005 
 
added s for singleton tag, increase from 94.22 
to 95.23 f1 score on base np   s. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

41 

3/12/15 

                                         speech and language processing - jurafsky and martin        

42 

methods 

!    argmax p(tags|words) 
 

!    id48s 
!    discriminative sequence classification 

!    using any kind of standard ml-based classifier. 

id48 tagging 
!    same as we did with id52 

!    argmax p(t|w) = p(w|t)p(t) 
!    the tags are the hidden states 

!    works ok, but has one significant shortcoming 
!    the typical kinds of things that we might think would 
be useful in this task aren   t easily squeezed into the 
id48 model 
!    we   d like to be able to make arbitrary features 
available for the statistical id136 being made. 
!    for that we   ll turn to classifiers created using 

classical machine learning techniques  

3/12/15 

                                         speech and language processing - jurafsky and martin        

43 

3/12/15 

                                         speech and language processing - jurafsky and martin        

44 

11 

supervised classification 

!    training a system to take an object 

represented as a set of features and apply 
a label to that object. 

 
!    methods typically include 

!    na  ve bayes 
!    id90 
!    id28 (maximum id178) 
!    support vector machines 
!        

from classification to 
sequence processing 

!    applying this to tagging    

!    the object to be tagged is a word in the 
sequence 
!    the features are  

!    features of the word,  
!    features of its immediate neighbors, 
!     and features derived from the entire context 

!    sequential tagging means sweeping a 
classifier across the input assigning tags to 
words as you proceed. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

45 

3/12/15 

                                         speech and language processing - jurafsky and martin        

46 

statistical sequence 

labeling 

typical features 

!    typical setup involves 

!    a small sliding window around the object 
being tagged 
!    features extracted from the window 

!    current word token 
!    previous/next n word tokens 
!    current word pos 
!    previous/next pos 
!    previous n chunk labels 
!    capitalization information 
!    ... 

3/12/15 

                                         speech and language processing - jurafsky and martin        

47 

3/12/15 

                                         speech and language processing - jurafsky and martin        

48 

12 

evaluation 

example 

!    suppose you employ this iob  scheme. 

what   s the best way to measure 
performance. 

!    probably not the per-tag accuracy we used 

for id52. 
!    why? 
      we need a metric that looks at the chunks 

      it   s not measuring what we care about 

not the tags 

!    suppose we were looking for pp chunks 

for some reason. 

!    if the system simply said o all the time it 
would do pretty well on a per-label basis 
since most words reside outside any pp. 

3/12/15 

                                         speech and language processing - jurafsky and martin        

49 

3/12/15 

                                         speech and language processing - jurafsky and martin        

50 

precision/recall/f 

!    precision: 

!    the fraction of chunks the system returned 
that were right 
!       right    means the boundaries and the label are 
correct given some labeled test set. 

!    recall: 

!    the fraction of the chunks that system got 
from those that it should have gotten. 

!    f: simple harmonic mean of those two 

numbers.  

3/12/15 

                                         speech and language processing - jurafsky and martin        

51 

13 

