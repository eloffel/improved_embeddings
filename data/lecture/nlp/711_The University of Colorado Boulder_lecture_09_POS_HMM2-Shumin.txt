natural language 

processing 

lecture 9   2/10/2015 

 

shumin wu 

 

today 

         more on id48s 

         review statistical id52 
         3 id48 problems and algorithms 

         decoding (viterbi) 
         forward/backward 
         em, (forward-backward or baum-welch) 

2/10/15 

                                         speech and language processing - jurafsky and martin        

2 

getting to id48s 
         this equation gives us the best tag 

sequence 

         but how to make it operational?  

         how to efficiently perform this computation? 

         intuition of bayesian id136: 

         use bayes rule to transform this equation into 
a generative model 

2/10/15 

                                         speech and language processing - jurafsky and martin        

3 

using bayes rule 

2/10/15 

                                         speech and language processing - jurafsky and martin        

4 

likelihood and prior 

p(w1

n t1

n)    

n
   
i=1

p(wi ti)

2/10/15 

                                         speech and language processing - jurafsky and martin        

5 

two kinds of probabilities 
         tag transition probabilities p(ti|ti-1) 

         determiners likely to precede adjs and nouns 

         that/dt flight/nn 
         the/dt yellow/jj hat/nn 
         so we expect p(nn|dt) and p(jj|dt) to be high, but 
p(dt|jj) to be low 

         compute p(nn|dt) by counting in a labeled 
corpus: 

2/10/15 

                                         speech and language processing - jurafsky and martin        

6 

two kinds of probabilities 

         word likelihood probabilities p(wi|ti) 
        vbz (3sg pres verb) likely to be    is    
        compute p(is|vbz) by counting in a labeled 
corpus: 

2/10/15 

                                         speech and language processing - jurafsky and martin        

7 

transition probabilities 

2/10/15 

                                         speech and language processing - jurafsky and martin        

8 

observation likelihoods 

2/10/15 

                                         speech and language processing - jurafsky and martin        

9 

question 

         if there are 30 or so tags in the penn set 
         and the average sentence is around 20 

words... 

         how many tag sequences do we have to 

enumerate argmax over? 

 
 

3020 

2/10/15 

                                         speech and language processing - jurafsky and martin        

10 

id48 

         states q = q1, q2   qn;   
         observations o= o1, o2   on;   

vv} 

         transition probabilities 

         each observation is a symbol from a vocabulary v = {v1,v2,   

         transition id203 matrix a = {aij} 

aij = p(qt = j |qt   1 = i)   1     i, j     n

         observation likelihoods 

         output id203 matrix b={bi(k)} 
 
bi(k) = p(xt = ok |qt = i)   
    
         special initial id203 vector    

  i = p(q1 = i)   1     i     n

2/10/15 

    

                                         speech and language processing - jurafsky and martin        

11 

3 problems 

         given this framework there are 3 problems 

that we can pose to an id48 
         given an observation sequence, what is the 
id203 of that sequence given a model? 
         given an observation sequence and a model, 
what is the most likely state sequence? 
         given an observation sequence, infer the best 
model parameters for a skeletal model 

2/10/15 

                                         speech and language processing - jurafsky and martin        

12 

problem 1 

         the id203 of a sequence given a model... 
 

         used in model development... how do i know if some 

change i made to the model is making it better 

         and in classification tasks 

         word spotting in asr, id46, speaker 

identification, author identification, etc. 
         train one id48 model per class 
         given an observation, pass it to each model and compute p(seq|model). 

2/10/15 

                                         speech and language processing - jurafsky and martin        

13 

problem 2 

         most probable state sequence given a model and 

an observation sequence 

         typically used in tagging problems, where the tags 

correspond to hidden states 
         as we   ll see almost any problem can be cast as a sequence 

labeling problem 

         viterbi solves problem 2 

2/10/15 

                                         speech and language processing - jurafsky and martin        

14 

problem 3 

         infer the best model parameters, given a 

skeletal model and an observation 
sequence... 
         that is, fill in the a and b tables with the right 
numbers... 
         the numbers that make the observation sequence 
most likely 

         useful for getting an id48 without having to 
hire annotators... 
         that is you tell me how many tags there are and 
give me a boatload of untagged text, and i give 
you back a part of speech tagger. 
                                         speech and language processing - jurafsky and martin        

2/10/15 

15 

solutions 

         problem 2: viterbi 
         problem 1: forward 
         problem 3: forward-backward 

         an instance of em 

2/10/15 

                                         speech and language processing - jurafsky and martin        

16 

problem 2: decoding 

         ok, now we have a complete model that can give us 

what we need. recall that we need to get 

 

         we could just enumerate all paths given the input and 

use the model to assign probabilities to each. 
         not a good idea. 
         luckily id145 helps us here 

 
 

2/10/15 

                                         speech and language processing - jurafsky and martin        

17 

intuition 

         you   re interested in the shortest distance 

from boulder to moab 

         consider a possible location on the way to 

moab, say glenwood springs. 

         what do you need to know about all the 

different possible ways to get to glenwood 
springs? 

the best way (the shortest path) 

2/10/15 

                                         speech and language processing - jurafsky and martin        

18 

intuition 

         consider a state sequence (tag sequence) 
that ends at state j (i.e., has a particular 
tag t at the end) 

         the id203 of that tag sequence can 

be broken into parts 
         the id203 of the best tag sequence up 
through j-1 
         multiplied by the transition id203 from 
the tag at the end of the j-1 sequence to t. 
         and the observation id203 of the 
observed word given tag t 
                                         speech and language processing - jurafsky and martin        

2/10/15 

19 

viterbi example 

2/10/15 

                                         speech and language processing - jurafsky and martin        

20 

the viterbi algorithm 

2/10/15 

                                         speech and language processing - jurafsky and martin        

21 

viterbi summary 

         create an array 

         with columns corresponding to inputs 
         rows corresponding to possible states 
         sweep through the array in one pass 

filling the columns left to right using our 
transition probs and observations probs 

         id145 key is that we need 
only store the max prob and path to each 
cell, (not all paths). 

2/10/15 

                                         speech and language processing - jurafsky and martin        

22 

evaluation 

         so once you have you pos tagger running 

how do you evaluate it? 
         overall error rate with respect to a gold-
standard test set 
         each token gets a tag, so overall accuracy is a 
decent measure (number correct/number tagged) 

         but to improve a system we want more 
detailed information 
         per word accuracy 
         confusion matrices 

2/10/15 

                                         speech and language processing - jurafsky and martin        

23 

evaluation 

         results are compared with a manually 

coded    gold standard    
         typically accuracy reaches 96-97% 
         this may be compared with result for a 
baseline tagger (one that uses no context) 
         important: 100% accuracy is impossible 

even for human annotators 
         goal is to get system performance near to 
human performance 
         beware of claims from systems that claim to 
exceed the accuracy of human annotators 

2/10/15 

                                         speech and language processing - jurafsky and martin        

24 

detailed error analysis 

         look at a confusion matrix 
 

         see what errors are causing problems 
         noun (nn) vs propernoun (nnp) vs adj (jj) 
         preterite (vbd) vs participle (vbn) vs adjective (jj) 

2/10/15 

                                         speech and language processing - jurafsky and martin        

25 

problem 1: forward 

         given an observation sequence return the 

id203 of the sequence given the 
model... 
         well in a normal markov model, the states 
and the sequences are identical... so the 
id203 of a sequence is the id203 of 
the path sequence 
         but not in an id48... remember that any 
number of sequences might be responsible for 
any given observation sequence. 

2/10/15 

                                         speech and language processing - jurafsky and martin        

26 

forward 

         efficiently computes the id203 of an 

observed sequence given a model 
         p(sequence|model) 

         nearly identical to viterbi; replace the max 

with a sum 

2/10/15 

                                         speech and language processing - jurafsky and martin        

27 

ice cream example 

2/10/15 

                                         speech and language processing - jurafsky and martin        

28 

ice cream example 

2/10/15 

                                         speech and language processing - jurafsky and martin        

29 

forward 

2/10/15 

                                         speech and language processing - jurafsky and martin        

30 

problem 3: learning the 

parameters 

         first an example to get the intuition down 
         we   ll do forward-backward next time 

2/10/15 

                                         speech and language processing - jurafsky and martin        

31 

urn example 

         a genie has two urns filled with red and 
blue balls. the genie selects an urn and 
then draws a ball from it (and replaces it). 
the genie then selects either the same urn 
or the other one and then selects another 
ball    
         the urns and actual draws are hidden 
         the balls are observed 

2/10/15 

                                         speech and language processing - jurafsky and martin        

32 

urn 

         based on the results of a long series of 

draws... 
         figure out the distribution of colors of balls in 
each urn 
         observation probabilities (b table) 

         figure out the genie   s preferences for going 
from one urn to the next 
         transition probabilities (a table) 

2/10/15 

                                         speech and language processing - jurafsky and martin        

33 

urns and balls 

         pi:  urn 1:  0.9; urn 2:  0.1 
         a 
urn 1  urn 2 
0.4 
0.7 

urn 1 
urn 2 

0.6 
0.3 

         b 

urn 1  urn 2 
0.4 
0.6 

0.7 
0.3 

red 
blue 

2/10/15 

                                         speech and language processing - jurafsky and martin        

34 

urns and balls 

         let   s assume the input 

(observables) is blue blue 
red (bbr) 

         since both urns contain 
   red and blue balls 
   any path of length 3 
through this machine 

   could produce this output 

.6 

how many paths are there? 

.7 

.4 

urn 1 

urn 2 

.3 

2/10/15 

                                         speech and language processing - jurafsky and martin        

35 

urns and balls 

blue blue red 
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/10/15 

                                         speech and language processing - jurafsky and martin        

36 

urns and balls 

viterbi: says  111 is the most likely state sequence  
1 1 1 
(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
1 1 2 
1 2 1 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 
1 2 2 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/10/15 

                                         speech and language processing - jurafsky and martin        

37 

urns and balls 

    

forward: p(bbr| model) = .0792  
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/10/15 

                                         speech and language processing - jurafsky and martin        

38 

urns and balls 

         em 

         what if i told you i lied about the numbers in 
the model (priors,a,b) for this example?  that 
is, i just made them up. 
         can i get better numbers just from the input 
sequence? 

2/10/15 

                                         speech and language processing - jurafsky and martin        

39 

urns and balls 

         yup 

         just count up and prorate the number of 
times a given transition is traversed while 
processing the observations inputs.  
         then use that pro-rated count to re-estimate 
the transition id203 for that transition 

2/10/15 

                                         speech and language processing - jurafsky and martin        

40 

urns and balls 

         but    we just saw that don   t know the 
actual path the input took, its hidden! 
         so prorate the counts from all the possible 
paths based on the path probabilities the 
model gives you 
         basically do what forward does 

         but you said the numbers were wrong 

         doesn   t matter; use the original numbers then 
replace the old ones with the new ones. 

2/10/15 

                                         speech and language processing - jurafsky and martin        

41 

urn example 

.6 

urn 1 

.4 

.3 

.7 

urn 2 

let   s re-estimate the urn1->urn2 transition 
and the urn1->urn1 transition (using blue blue 
red as training data). 

2/10/15 

                                         speech and language processing - jurafsky and martin        

42 

urns and balls 

blue blue red 
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/10/15 

                                         speech and language processing - jurafsky and martin        

43 

urns and balls 

         that   s  

         (.0077*1)+(.0136*1)+(.0181*1)+(.0020*1) 
=  .0414 

         of course, that   s not a id203, it needs to be 
divided by the id203 of leaving urn 1 total. 

         there   s only one other way out of urn 1 (going back to 

urn1) 
         so let   s reestimate urn1-> urn1 
 

2/10/15 

                                         speech and language processing - jurafsky and martin        

44 

urn example 

.6 

urn 1 

.4 

.3 

.7 

urn 2 

let   s re-estimate the urn1->urn1 transition 

2/10/15 

                                         speech and language processing - jurafsky and martin        

45 

urns and balls 

blue blue red 
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/10/15 

                                         speech and language processing - jurafsky and martin        

46 

urns and balls 

         that   s just 

         (2*.0204)+(1*.0077)+(1*.0052) = .0537 

         again not what we need but we   re 

closer     we just need to normalize using 
those two numbers. 

2/10/15 

                                         speech and language processing - jurafsky and martin        

47 

urns and balls 

         the 1->2 transition id203 is  

.0414/(.0414+.0537) = 0.435 

         the 1->1 transition id203 is 

.0537/(.0414+.0537) = 0.565 

         so in re-estimation the 1->2 transition 

went from .4 to .435 and the 1->1 
transition went from .6 to .565 

2/10/15 

                                         speech and language processing - jurafsky and martin        

48 

em re-estimation 

         not done yet.  no reason to think those 
values are right.  but they   re more right 
than they used to be. 
         so do it again, and again and.... 

         as with problems 1 and 2, you wouldn   t 

actually compute it this way. the forward-
backward algorithm re-estimates these 
numbers in the same dynamic 
programming way that viterbi and 
forward do.  

2/10/15 

                                         speech and language processing - jurafsky and martin        

49 

