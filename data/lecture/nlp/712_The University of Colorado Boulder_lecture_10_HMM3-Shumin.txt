natural language 

processing 

lecture 10   2/12/2015 

 

shumin wu 

 

today 

         more id48s 

         urns and balls 
         forward-backward 
         exam next tuesday 
 

2/12/15 

                                         speech and language processing - jurafsky and martin        

2 

id48 

         states q = q1, q2   qn;   
         observations o= o1, o2   on;   

vv} 

         transition probabilities 

         each observation is a symbol from a vocabulary v = {v1,v2,   

         transition id203 matrix a = {aij} 

aij = p(qt = j |qt   1 = i)   1     i, j     n

         observation likelihoods 

         output id203 matrix b={bi(k)} 
 
bi(k) = p(xt = ok |qt = i)   
    
         special initial id203 vector    

  i = p(q1 = i)   1     i     n

2/12/15 

    

                                         speech and language processing - jurafsky and martin        

3 

3 problems 

         given this framework there are 3 problems 

that we can pose to an id48 
         given an observation sequence, what is the 
id203 of that sequence given a model? 
         given an observation sequence and a model, 
what is the most likely state sequence? 
         given an observation sequence, infer the best 
model parameters for a skeletal model 

2/12/15 

                                         speech and language processing - jurafsky and martin        

4 

problem 1 

         the id203 of a sequence given a model... 
 

         used in model development... how do i know if some 

change i made to the model is making it better 

         and in classification tasks 

         word spotting in asr, id46, speaker 

identification, author identification, etc. 
         train one id48 model per class 
         given an observation, pass it to each model and compute p(seq|model). 

2/12/15 

                                         speech and language processing - jurafsky and martin        

5 

problem 2 

         most probable state sequence given a model and 

an observation sequence 

         typically used in tagging problems, where the tags 

correspond to hidden states 
         as we   ll see almost any problem can be cast as a sequence 

labeling problem 

         viterbi solves problem 2 

2/12/15 

                                         speech and language processing - jurafsky and martin        

6 

problem 3 

         infer the best model parameters, given a 

skeletal model and an observation 
sequence... 

 

         that is, fill in the a and b tables with the right 
numbers... 
         the numbers that make the observation sequence 
most likely 

         useful for getting an id48 without having to 
hire annotators... 

2/12/15 

                                         speech and language processing - jurafsky and martin        

7 

solutions 

         problem 1: forward 
         problem 2: viterbi 
         problem 3: em 

         or forward-backward (or baum-welch) 

2/12/15 

                                         speech and language processing - jurafsky and martin        

8 

forward 

         given an observation sequence return the 

id203 of the sequence given the 
model... 
         well in a normal markov model, the states 
and the sequences are identical... so the 
id203 of a sequence is the id203 of 
the path sequence 
         but not in an id48...  

2/12/15 

                                         speech and language processing - jurafsky and martin        

9 

forward 

         efficiently computes the id203 of an 

observed sequence given a model 
         p(sequence|model) 

         nearly identical to viterbi; replace the max 

with a sum 

2/12/15 

                                         speech and language processing - jurafsky and martin        

10 

ice cream example 

2/12/15 

                                         speech and language processing - jurafsky and martin        

11 

in general 

2/12/15 

                                         speech and language processing - jurafsky and martin        

12 

forward 

2/12/15 

                                         speech and language processing - jurafsky and martin        

13 

urn example 

         a genie has two urns filled with red and 
blue balls. the genie selects an urn and 
then draws a ball from it (and replaces it). 
the genie then selects either the same urn 
or the other one and then selects another 
ball    
         the urns are hidden 
         the balls are observed 

2/12/15 

                                         speech and language processing - jurafsky and martin        

14 

urn 

         based on the results of a long series of 

draws... 
         figure out the distribution of colors of balls in 
each urn 
         observation probabilities (b table) 

         figure out the genie   s preferences in going 
from one urn to the next 
         transition probabilities (a table) 

2/12/15 

                                         speech and language processing - jurafsky and martin        

15 

urns and balls 

         pi:  urn 1:  0.9; urn 2:  0.1 
         a 
urn 1  urn 2 
0.4 
0.7 

urn 1 
urn 2 

0.6 
0.3 

         b 

urn 1  urn 2 
0.4 
0.6 

0.7 
0.3 

red 
blue 

2/12/15 

                                         speech and language processing - jurafsky and martin        

16 

urns and balls 

         let   s assume the input 

(observables) is blue blue 
red (bbr) 

         since both urns contain 
   red and blue balls 
   any path of length 3 
through this machine 

   could produce this output 

.6 

.7 

.4 

urn 1 

urn 2 

.3 

2/12/15 

                                         speech and language processing - jurafsky and martin        

17 

urns and balls 

blue blue red 
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/12/15 

                                         speech and language processing - jurafsky and martin        

18 

urns and balls 

viterbi: says  111 is the most likely state sequence  
1 1 1 
(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
1 1 2 
1 2 1 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 
1 2 2 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/12/15 

                                         speech and language processing - jurafsky and martin        

19 

urns and balls 

    

forward: p(bbr| model) = .0792  
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/12/15 

                                         speech and language processing - jurafsky and martin        

20 

urns and balls 

         em 

         what if i told you i lied about the numbers in 
the model (priors,a,b). i just made them up. 
         can i get better numbers just from the input 
sequence? 

2/12/15 

                                         speech and language processing - jurafsky and martin        

21 

urns and balls 

         yup 

         just count up and prorate the number of 
times a given transition is traversed while 
processing the observations inputs.  
         then use that pro-rated count to re-estimate 
the transition id203 for that transition 

2/12/15 

                                         speech and language processing - jurafsky and martin        

22 

urns and balls 

         but    we just saw that don   t know the 
actual path the input took, its hidden! 
         so prorate the counts from all the possible 
paths based on the path probabilities the 
model gives you 
         basically do what forward does 

         but you said the numbers were wrong 
         doesn   t matter; use the original numbers 
then replace the old ones with the new ones. 

2/12/15 

                                         speech and language processing - jurafsky and martin        

23 

urn example 

.6 

urn 1 

.4 

.3 

.7 

urn 2 

let   s re-estimate the urn1->urn2 transition 
and the urn1->urn1 transition (using blue blue 
red as training data). 

2/12/15 

                                         speech and language processing - jurafsky and martin        

24 

another view 

         we can view all those products as a lattice 

         that is, unwind the state machine in time 

time 

    1

            2                      3                    4 

1 

2 

1 

2 

1 

2 

1 

2 

observations 

 blue 

         blue 

  red 

              blue 

2/12/15 

                                         speech and language processing - jurafsky and martin        

25 

another view 

         re-estimating the 1-> 2 transitions... 

1 

2 

1 

2 

1 

2 

1 

2 

blue 

         blue 

  red 

              blue 

time 

2/12/15 

                                         speech and language processing - jurafsky and martin        

26 

another view 

         re-estimating the 1-> 2 transitions... 

1 

2 

1 

2 

1 

2 

1 

2 

blue 

         blue 

  red 

              blue 

time 

2/12/15 

                                         speech and language processing - jurafsky and martin        

27 

urns and balls 

blue blue red 
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

first, what exactly is this id203?  
2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/12/15 

                                         speech and language processing - jurafsky and martin        

28 

urns and balls 

         that   s  

         (.0077*1)+(.0136*1)+(.0181*1)+(.0020*1) 
=  .0414 

         of course, that   s not the id203 we want, it needs 
to be divided by the id203 of leaving urn 1 total. 

         there   s only one other way out of urn 1 (going back to 

urn1) 
         so let   s reestimate urn1-> urn1 
 

2/12/15 

                                         speech and language processing - jurafsky and martin        

29 

urn example 

.6 

urn 1 

.4 

.3 

.7 

urn 2 

let   s re-estimate the urn1->urn1 transition 

2/12/15 

                                         speech and language processing - jurafsky and martin        

30 

urns and balls 

blue blue red 
1 1 1 
1 1 2 
1 2 1 
1 2 2 

(0.9*0.3)*(0.6*0.3)*(0.6*0.7)=0.0204 
(0.9*0.3)*(0.6*0.3)*(0.4*0.4)=0.0077 
(0.9*0.3)*(0.4*0.6)*(0.3*0.7)=0.0136 
(0.9*0.3)*(0.4*0.6)*(0.7*0.4)=0.0181 

2 1 1 
2 1 2 
2 2 1 
2 2 2 

(0.1*0.6)*(0.3*0.7)*(0.6*0.7)=0.0052 
(0.1*0.6)*(0.3*0.7)*(0.4*0.4)=0.0020 
(0.1*0.6)*(0.7*0.6)*(0.3*0.7)=0.0052 
(0.1*0.6)*(0.7*0.6)*(0.7*0.4)=0.0070 

2/12/15 

                                         speech and language processing - jurafsky and martin        

31 

urns and balls 

         that   s just 

         (2*.0204)+(1*.0077)+(1*.0052) = .0537 

         again not what we need but we   re closer     
we just need to normalize using those two 
numbers. 

2/12/15 

                                         speech and language processing - jurafsky and martin        

32 

urns and balls 

         the 1->2 transition id203 is  

.0414/(.0414+.0537) = 0.435 

         the 1->1 transition id203 is 

.0537/(.0414+.0537) = 0.565 

         so in re-estimation the 1->2 transition 

went from .4 to .435 and the 1->1 
transition went from .6 to .565 

2/12/15 

                                         speech and language processing - jurafsky and martin        

33 

em re-estimation 

         not done yet.  no reason to think those 
values are right.  they   re just more right 
than they used to be. 
         so do it again, and again and.... 
         until convergence 
         convergence does not guarantee a global 
optima, just a local one 

         as with problems 1 and 2, you wouldn   t 
actually compute it this way. enumerating 
all the paths is infeasible. 

2/12/15 

                                         speech and language processing - jurafsky and martin        

34 

forward-backward 

         baum-welch = forward-backward algorithm 

(baum 1972) 

         is a special case of the em or expectation-

maximization algorithm (dempster, laird, rubin) 

         the algorithm will let us train the transition 

probabilities a= {aij} and the emission probabilities 
b={bi(ot)} of the id48 

2/12/15 

                                         speech and language processing - jurafsky and martin        

35 

intuition for re-estimation of aij 
         we will estimate   ij via this intuition: 

  aij =

expected number of transitions from state i to state j

expected number of transitions from state i

         numerator intuition: 

         assume we had some estimate of id203 that a 

given transition i     j was taken at time t in a 
observation sequence. 

         if we knew this id203 for each time t, we could 
sum over all t to get expected value (count) for i     j. 

2/12/15 

                                         speech and language processing - jurafsky and martin        

36 

intuition for re-estimation of aij 

2/12/15 

                                         speech and language processing - jurafsky and martin        

37 

re-estimating aij 
expected number of transitions from state i

expected number of transitions from state i to state j

   a ij =

         the expected number of transitions from state 

i to state j is the sum over all t of    

         the total expected number of transitions out 
of state i is the sum over all transitions out of 
state i 

         final formula for reestimated aij 

    

2/12/15 

                                         speech and language processing - jurafsky and martin        

38 

the forward-backward alg 

2/12/15 

                                         speech and language processing - jurafsky and martin        

39 

summary: forward-backward 

algorithm 

1)    initialize   =(a,b) 
2)    compute   ,   ,    using observations 
3)    estimate new      =(a,b) 
4)    replace    with       
5)    if not converged go to 2 

2/12/15 

                                         speech and language processing - jurafsky and martin        

40 

break 
         exam (id48) questions? 

2/12/15 

                                         speech and language processing - jurafsky and martin        

41 

