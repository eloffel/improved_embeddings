 

natural language 

processing 

 

lecture 19.1   3/17/2015 

martha palmer 

 
 

example: nps 

!    our id18 rules for nps don   t condition on 

where in a tree the rule is applied 

!    but we know that not all the rules occur 

with equal frequency in all contexts. 
!    consider nps that involve pronouns vs. those 
that don   t. 

solution 1: rule rewriting 
!    the grammar rewriting approach attempts to 
capture local tree information by rewriting the 
grammar so that the rules capture the 
regularities we want. 

 

!    by splitting and merging the non-terminals in the 

grammar. 
!    example: split nps into different classes    

!    remember, we rewrote the grammar rules for 
cky, and we rewrote the iob tags. 

3/19/15 

                                         speech and language processing - jurafsky and martin        

other examples 

!    there are lots of other examples like this 

in any treebank 
!    many at the part of speech level 
!    recall that many decisions made in 
annotation efforts are directed towards 
improving annotator agreement, not towards 
doing the right thing. 
!    often this involves conflating distinct classes into a 
larger class 
!    to, in, det, etc. 

3/19/15 

                                         speech and language processing - jurafsky and martin        

3 

3/19/15 

                                         speech and language processing - jurafsky and martin        

2 

4 

1 

rule rewriting 

!    three approaches 

!    use linguistic knowledge to directly rewrite 
rules by hand 
!    np_obj and the np_subj approach 

!    automatically rewrite the rules using context 
to capture some of what we want 
!    ie. incorporate context into a context-free 
approach 

!    search through the space of rewrites for the 
grammar that maximizes the id203 of the 
training set 

local context approach 

!    condition the rules based on their parent 

nodes 
!    this splitting based on tree-context captures 
some of the linguistic intuitions 

3/19/15 

                                         speech and language processing - jurafsky and martin        

5 

3/19/15 

                                         speech and language processing - jurafsky and martin        

parent annotation 

parent annotation 

!    now we have non-terminals np^s and np^vp that 

should capture the subject/object and pronoun/full np 
cases. that is... 
!    the rules are now 

!    np^s -> prp 
!    np^vp -> dt 
!    vp^s -> np^vp 

!    recall what   s going on here. we   re in effect rewriting 
!    and changing the probabilities since they   re being 

the treebank, thus rewriting the grammar. 

derived from different counts    
!    and if we   re splitting what   s happening to the counts? 

3/19/15 

                                         speech and language processing - jurafsky and martin        

7 

3/19/15 

                                         speech and language processing - jurafsky and martin        

6 

8 

2 

auto rewriting 

!    if this is such a good idea we may as well 

apply a learning approach to it. 

!    start with a grammar (perhaps a treebank 

grammar) 

!    search through the space of splits/merges 

for the grammar that in some sense 
maximizes parsing performance on the 
training/development set.  

auto rewriting 

!    basic idea     

!    split every non-terminal into two new non-
terminals across the entire grammar (x 
becomes x1 and x2). 
!    duplicate all the rules of the grammar that 
use x, dividing the id203 mass of the 
original rule almost equally.  
!    run em to readjust the rule probabilities 
!    perform a merge step to back off the splits 
that look like they don   t really do any good. 

3/19/15 

                                         speech and language processing - jurafsky and martin        

9 

3/19/15 

                                         speech and language processing - jurafsky and martin        

10 

solution 2:  

lexicalized grammars 
!    lexicalize the grammars with heads 
!    compute the rule probabilities on these 

lexicalized rules 

!    run prob cky as before 

dumped example 

3/19/15 

                                         speech and language processing - jurafsky and martin        

11 

3/19/15 

                                         speech and language processing - jurafsky and martin        

12 

3 

how? 

declare independence 

!    we used to have 

!    vp -> v np pp 

 

 p(rule|vp) 

!    that   s the count of this rule divided by the number 
of vps in a treebank 

!    now we have fully lexicalized rules... 

!    vp(dumped)-> v(dumped) np(sacks)pp(into) 
p(r|vp ^ dumped is the verb ^ sacks is the 

head of the np ^ into is the head of the pp) 

to get the counts for that.. 

!    when stuck, exploit independence and 

collect the statistics you can    

!    there are a larger number of ways to do 

this... 

!    let   s consider one generative story: 

given a rule we   ll 
1.    generate the head 
2.    generate the stuff to the left of the head 
3.    generate the stuff to the right of the head 

3/19/15 

                                         speech and language processing - jurafsky and martin        

13 

3/19/15 

                                         speech and language processing - jurafsky and martin        

14 

example 

!    so the id203 of a lexicalized rule such 

as  
!    vp(dumped)      v(dumped)np(sacks)pp(into) 

!    is the product of the id203 of 

!       dumped    as the head  
!    with nothing to its left 
!       sacks    as the head of the first right-side thing 
!       into    as the head of the next right-side 
element 
!    and nothing after that 

example 
!    that is, the rule id203 for 

is estimated as 
 

3/19/15 

                                         speech and language processing - jurafsky and martin        

15 

3/19/15 

                                         speech and language processing - jurafsky and martin        

16 

4 

framework 
!    that   s just one simple model 

!    collins model 1 

!    you can imagine a gazzillion other 

assumptions that might lead to better 
models 

!    you just have to make sure that you can 

get the counts you need 

!    and that it can be used/exploited 

efficiently during decoding 

features 

!    she visited her.  

!    c for case, subjective/objective 
!    p for person agreement, (1st, 2nd, 3rd) 
!    n for number agreement, subject/verb 
!    g for gender agreement, subject/verb 

!    i like him, you like him, he likes him,  

!    he likes him, they like him. 

!    english, reflexive pronouns he washed himself. 
!    romance languages, det/noun 

!    t for tense,  

!    auxiliaries, sentential complements, etc.  
!    * will finished is bad 

3/19/15 

                                         speech and language processing - jurafsky and martin        

17 

                                    nlp 

cse391     
2004 
 

18 

5 

