lecture 2: id165 

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs 6501: natural language processing

1

this lecture

    language models

    what are id165 models?

    how to use probabilities
    what does p(y|x) mean?
    how can i manipulate it?
    how can i estimate its value in practice?

cs 6501: natural language processing

2

what is a language model?

    id203 distributions over sentences 

(i.e., word sequences )

p(w) = p(    1    2    3    4             )

    can use them to generate strings

p(                 2    3    4                1)
    rank possible sentences

    p(   today is tuesday   ) > p(   tuesday today is   )

    p(   today is tuesday   ) > p(   today is virginia   )

cs 6501: natural language processing

3

language model applications

context-sensitive id147

cs 6501: natural language processing

4

language model applications

autocomplete

cs 6501: natural language processing

5

language model applications

smart reply

cs 6501: natural language processing

6

language model applications

language generation

https://pdos.csail.mit.edu/archive/scigen/

cs 6501: natural language processing

7

bag-of-words with id165s 

    id165s: a contiguous sequence of n 

tokens from a given piece of text

http://recognize-speech.com/language-model/id165-model/comparison

cs 6501: natural language processing

8

id165 models

    unigram model:          1          2          3         (        )
    bigram model: 

         1          2|    1          3|    2         (        |           1)

    trigram model:

         1          2|    1          3|    2,     1         (        |           1           2)

    id165 model:

         1          2|    1         (        |           1           2                    )

cs 6501: natural language processing

9

random language via id165

    http://www.cs.jhu.edu/~jason/465/powerpo

int/lect01,3tr-ngram-gen.pdf

    behind the scenes     id203 theory

cs 6501: natural language processing

10

sampling with replacement

1. p(   ) = ?    2. p(   ) = ?  3. p(red,    ) = ?
4. p(blue) = ? 5. p(red |    ) = ? 
6. p(    | red) = ?   7. p(                ) = ?    
8. p(         ) = ?    9. p(2 x     , 3 x     , 4 x      ) = ?

cs 6501: natural language processing

11

sampling words with replacement 

example from julia hockenmaier, intro to nlp

cs 6501: natural language processing

12

implementation: how to sample?

    sample from a discrete distribution     (    )

    assume      outcomes in the event space     
1. divide the interval [0,1] into      intervals 

according to the probabilities of the outcomes

2. generate a random number      between 0 and 1
3. return          where      falls into

cs 6501: natural language processing

13

conditional on the previous word

example from julia hockenmaier, intro to nlp

cs 6501: natural language processing

14

conditional on the previous word

example from julia hockenmaier, intro to nlp

cs 6501: natural language processing

15

recap: id203 theory

    id155

    p(blue |    ) = ?

                   =     (    ,     )/    (    )

    bayes    rule:                =

    (a|b)         

    (    )

    verify: p(red |    ) , p(    | red ), p(   ), p(red)

    independent                = p(b)

    prove:      a, b = p a p(b)

cs 6501: natural language processing

16

the chain rule

    the joint id203 can be expressed in 

terms of the id155:

p(x,y) = p(x | y) p(y)

    more variables:

p(x, y, z) = p(x | y, z) p(y, z)

= p(x | y, z) p(y | z) p(z)

         x1,     2,             

=          1          2     1          3     2,     1                       1,                1
=          1       =2

             1,                1

    

cs 6501: natural language processing

17

language model for text

    id203 distribution over sentences

             1     2              =

we need independence assumptions!

         1          2     1          3     1,     2                       1,     2,     ,            1

    complexity -     (           

)

            - maximum sentence length

chain rule: from conditional 
   475,000 main headwords in webster's third new international dictionary
id203 to joint id203

   average english sentence length is 14.3 words
    a rough estimate:     (47500014)

how large is this?

47500014

8                        1024 4     3.38    66        

cs 6501: natural language processing

18

id203 models

    building a id203 model:

    defining the model

(making independent assumption)
    estimating the model   s parameters

    use the model (making id136)

param
values   

trigram model

(defined in terms of 

parameters like 
p(   is   |   today   ) )

definition

of p

cs 6501: natural language processing

19

independent assumption

    independent assumption

    even though x and y are not actually 

independent, we treat them as independent

    make the model compact 

(e.g., from 100    14 to  100    2) 

cs 6501: natural language processing

20

language model with id165

    the chain rule:

     x1,     2,             
=          1          2     1          3     2,     1                       1,                1

    id165 language model assumes each word 
depends only on the last n-1 words (markov 
assumption) 

cs 6501: natural language processing

21

language model with id165

    example: trigram (3-gram)

                  1,                1 =                          2,            1
    (    1,             )=
p     1          2     1                              2,            1

     "                                                                    "
=p(   today   )p(   is   |   today   )p(   a   |   is   ,    today   )   

p(   day   |   sunny   ,    a   )

cs 6501: natural language processing

22

unigram model

cs 6501: natural language processing

23

bigram model

    condition on the previous word

cs 6501: natural language processing

24

ngram model 

cs 6501: natural language processing

25

more examples

    yoav   s blog post: 

http://nbviewer.jupyter.org/gist/yoavg/d761
21dfde2618422139

    10-gram character-level lm:

first citizen: nay, then, that was hers, it 
speaks against your other service: but since the 
youth of the circumstance be spoken: your uncle 
and one baptista's daughter. 

sebastian: do i stand till the break off.

biron:
hide thy head.

cs 6501: natural language processing

26

more examples

~~/*
* linux/kernel/time.c
* please report this on hardware.
*/
    yoav   s blog post: 
void irq_mark_irq(unsigned long old_entries, eval);

/*
* divide only 1000 for ns^2 -> us^2 conversion values don't 

http://nbviewer.jupyter.org/gist/yoavg/d761
21dfde2618422139

seq_puts(m, "\ttramp: %ps",

overflow:

    10-gram character-level lm:

if (likely(t->flags & wq_unbound)) {

/*
* update inode information. if the
* slowpath and sleep time (abs or rel)

(void *)class->contending_point]++;

* @rmtp: remaining (either due
* to consume the state of ring buffer size. */

header_size - size, in bytes, of the chain.

*/
bug_on(!error);
} while (cgrp) {
if (old) {
if (kdb_continue_catastrophic;

#endif

cs 6501: natural language processing

27

questions?

cs 6501: natural language processing

28

id113

   best    means    data likelihood reaches maximum   

        =                                 (    |    )

unigram language model     

estimation

10/100
5/100
3/100
3/100

1/100

p(w|    )=?

   
text  ?
mining ?
assocation ?
database ?
   
query ?
   

document

text 10
mining 5
association 3
database 3
algorithm 2
   
query 1
efficient 1

a paper (total #words=100)

cs 6501: natural language processing

29

    which bag of words more likely generate:

aaadaaakoaaaa

a
a

a
d a
a

a

k

e

o

a

a
d

a

k

b

e

p

f

o

n

cs 6501: natural language processing

30

parameter estimation

    general setting:

    given a (hypothesized & probabilistic) model 

that governs the random experiment

    the model gives a id203 of any data 

    (    |    ) that depends on the parameter     

    now, given actual sample data x={x1,   ,xn},  

what can we say about the value of     ?

    intuitively, take our best guess of      --    best    

means    best explaining/fitting the data   

    generally an optimization problem

cs 6501: natural language processing

31

id113

    data: a collection of words,     1,     2,     ,         
    model: multinomial distribution p(    ) with parameters 

         =     (        )

    maximum likelihood estimator:         =                                      (    |    )

               =

    

         1 ,     ,     (        )

    

    

   
    =1

    
    (        )        
        
    =1

    (        )
        

    log                =    
    =1

             

log          +                     

    
        =                                      
    =1

             

log         

cs 6501: natural language processing

32

id113

            =                                         =1

                  

log         

    

         ,      =    
    =1

             

    
log          +         
    =1

        
            

=

             
        

+                   =    

             
    

    

             1

lagrange multiplier

set partial derivatives to zero

since

      =1

             =1

we have 

     =        
    =1

             

requirement from id203

         =

             
                  

      =1

ml 
estimate

cs 6501: natural language processing

33

id113

    for id165 language models

                             1,     ,                +1 =

    (        ,           1,   ,               +1)

    (           1,   ,               +1)

             =     

length of document or total 
number of words in a corpus

cs 6501: natural language processing

34

a bi-gram example

<s> i am sam </s>

<s> i am legend </s>

<s> sam i am </s>

p( i | <s>) = ?     p(am | i) = ?

p( sam | am) = ?    p( </s> | sam) = ?

p( <s>i am sam</s> | bigram model) = ?

cs 6501: natural language processing

35

practical issues

    we do everything in the log space

    avoid underflow
    adding is faster than multiplying 

log     1        2 = log     1 + log     2

    toolkits

    kenlm: https://kheafield.com/code/kenlm/
    srilm: 

http://www.speech.sri.com/projects/srilm

cs 6501: natural language processing

36

more resources

    google id165:

https://research.googleblog.com/2006/08/all-
our-id165-are-belong-to-you.html

file sizes: approx. 24 gb compressed (gzip'ed) text files

number of tokens: 1,024,908,267,229
number of sentences: 95,119,665,584
number of unigrams: 13,588,391
number of bigrams: 314,843,401
number of trigrams: 977,069,902
number of fourgrams: 1,313,818,354

number of fivegrams: 1,176,470,663

cs 6501: natural language processing

37

more resources

    google id165 viewer

https://books.google.com/ngrams/

data: 
http://storage.googleapis.com/books/ngrams/
books/datasetsv2.html

circumvallate 1978 335 91
circumvallate 1979 261 91

cs 6501: natural language processing

38

cs 6501: natural language processing

39

cs 6501: natural language processing

40

cs 6501: natural language processing

41

cs 6501: natural language processing

42

how about unseen words/phrases

    example: shakespeare corpus consists of 
n=884,647 word tokens and a vocabulary 
of v=29,066 word types

    only 30,000 word types occurred

    words not in the training data     0 id203

    only 0.04% of all possible bigrams 

occurred

cs 6501: natural language processing

43

next lecture

    dealing with unseen id165s

    key idea: reserve some id203 mass to 

events that don   t occur in the training data

    how much id203 mass should we 

reserve?

cs 6501: natural language processing

44

recap

    id165 language models

    how to generate text from a language model

    how to estimate a language model

    reading: speech and language 
processing chapter 4: id165s

cs 6501: natural language processing

45

