neural networks from scratch

http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/

written by
denny britz

"lane, mary e. (mel2q)" <mel2q@eservices.virginia.edu>

presented by
wasi uddin ahmad

3rd november, 2016

cs 6501 natural language processing

1

problem definition
    we have a dataset of medical records of patients

    input dataset dimension    500  2

    two class for prediction     male and female
    most important, the characteristics of the dataset is nonlinear

cs 6501 natural language processing

2

neural networks
    let   s consider a 3-layer neural network

cs 6501 natural language processing

3

how our nn makes predictions
    uses forward propagation
    just a bunch of id127s and applying the id180

    &=        &+    &
    &=tanh	(    &)
    4=    &    4+    4
    4=	    6=                            (    4)

    &,    &
                    ,    

    &

    &

activation	units

cs 6501 natural language processing

    4,    4
                        ,    6

4

learning the parameters
    what is our id168?

cross id178 loss,    (    ,    6)=   &i   

        k,l	                6k,l
l   p

k   i

    parameters that minimize the loss, maximizes the likelihood of training data
    how to minimize the id168?

    id119     batch or mini batch or stochastic!

    we need gradients of the id168 with respect to the parameters
    how to compute them?

    id26 algorithm!

cs 6501 natural language processing

5

applying id26
    using id26 formula, we can find the gradients

    &,    &
                    ,    

    r=	    6       
    4= 1                  4    & u	    r    4v
                4=    &v    r
                4=    r
                &=    v    4
                &=    4

    &

    &

activation	units

cs 6501 natural language processing

    4,    4
                        ,    6

6

update the parameters
    we have computed the gradients
    now update the model parameters,   

    (yz&)=    (y)           ]^    

    what   s next?
    how many hidden layers do we need?
    how many activation units do we need per hidden layers?

cs 6501 natural language processing

7

dimensions of hidden layer
    how to chose the dimensionality of the hidden layer?
    we can chose larger dimension

    need more computation to learn parameters
    more training data is required
    more prone to overfitting the data

    what is the best dimension for a hidden layer?

    it depends on the application!
    we can tune it.

    lets see what happens with different hidden layer size!

cs 6501 natural language processing

8

any question?

cs 6501 natural language processing

9

introduction to recurrent neural networks

http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/

written by
denny britz

presented by
wasi uddin ahmad

3rd november, 2016

cs 6501 natural language processing

10

motivation
    humans don   t start their thinking from scratch every second

    thoughts have persistence

    traditional neural networks can   t characterize this phenomena

    ex: classify what is happening at every point in a movie
    how a neural network can inform later events about the previous ones

    recurrent neural networks address this issue
    how?

cs 6501 natural language processing

11

what are id56s?
    main idea is to make use of sequential information
    how id56 is different from neural network?

    vanilla neural networks assume all inputs and outputs are independent of each other
    but for many tasks, that   s a very bad idea

    what id56 does?

    perform the same task for every element of a sequence (that   s what recurrent stands for)
    output depends on the previous computations!

    another way of interpretation     id56s have a    memory   

    to store previous computations

cs 6501 natural language processing

12

recurrent neural networks

output state at time step t

activation function

hidden state at time step t

input at time step        1

cs 6501 natural language processing

13

id56 extensions
    bidirectional id56s

cs 6501 natural language processing

14

id56 extensions
    deep (bidirectional) id56s

cs 6501 natural language processing

15

id56 extensions
    id137

    not fundamentally different from id56
    use different functions to compute hidden state
    memory of lstms are called cells
    cells decide what to keep in memory

    very effective in capturing long-term dependencies

cs 6501 natural language processing

16

any question?

cs 6501 natural language processing

17

understanding id137

written by

christopher colah

http://colah.github.io/posts/2015-08-understanding-lstms/

presented by
wasi uddin ahmad

3rd november, 2016

cs 6501 natural language processing

18

long-term dependencies
    is id56 capable of capturing long-term dependencies?
    why long-term dependencies?

    sometimes we only need to look at recent information to perform present task

    consider an example

    predict next word based on the previous words

the clouds are in the sky

cs 6501 natural language processing

19

problem of long-term dependencies
    what if we want to predict the next word in a long sentence?
    do we know which past information is helpful to predict the next word?
    in theory, id56s are capable of handling long-term dependencies.
    but in practice, they are not!

cs 6501 natural language processing

20

long short term memory (lstm)
    special kind of recurrent neural network
    works well in many problems and now widely used
    explicitly designed to avoid the long-term dependency problem
    remembering information for long periods of time is their default behavior

    not something they struggle to learn

    so, what is the structural difference between id56 and lstm?

cs 6501 natural language processing

21

difference between id56 and lstm

cs 6501 natural language processing

22

meaning of notations

cs 6501 natural language processing

23

core idea behind lstm
    key to lstms is the cell state

    the horizontal line running through the top of the diagram
    lstm can add or remove information to the cell state
    how? through regulated structures called gates.
    lstm has three gates to protect and control cell state

cs 6501 natural language processing

pointwise multiplication operation

sigmoid neural net layer

24

step-by-step lstm walk through
    forget gate layer decides what information will be thrown away

    looks at    y   & and     y and outputs a number between 0 and 1

    1 represents completely keep this, 0 represents completely get rid of this
    example: forget the gender of the old subject, when we see a new subject

cs 6501 natural language processing

25

step-by-step lstm walk through
    next step: decides what new information will be stored in the cell state
    two parts    

    a tanh	layer: creates a vector of new candidate values,     yb

    a sigmoid layer (input gate layer): decides what values we   ll update

    example: add the gender of the new subject to the cell state

    replace the old one we   re forgetting

input gate layer

tanh layer

cs 6501 natural language processing

26

step-by-step lstm walk through

    next step: update old state by     y   & into the new cell state     y
    multiply old state by     y
    then we add     y       yb

    forgetting the things we decided to forget earlier

cs 6501 natural language processing

27

step-by-step lstm walk through
    final step: decide what we   re going to output
    first, we run a sigmoid layer 

    which decides what parts of the cell state we   re going to output

    then, we put the cell state through tanh and multiply it by the output of the 
sigmoid gate

cs 6501 natural language processing

28

conclusion
    lstms were a big step in what we can accomplish with id56s
    common opinion among researchers     yes! there is a next step and it   s 
attention!
    attention neural network     another variant of id56
    main idea     give attention to every step of id56
    ex. image id134, machine translation

cs 6501 natural language processing

29

any question?

cs 6501 natural language processing

30

thank you

cs 6501 natural language processing

31

