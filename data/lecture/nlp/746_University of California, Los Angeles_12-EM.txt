lecture 12: 
em algorithm

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501 natural language processing

1

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

2

em algorithm

v pos induction     can we tag pos without 

annotated data?

v an old idea

v good mathematical intuition 
v tutorial paper: 

ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/t
r-97-021.pdf

v http://people.csail.mit.edu/regina/6864/em_note

s_mike.pdf

cs6501 natural language processing

3

hard em (intuition)

v we don   t know the hidden states (i.e., pos 

tags)

v if we know the model

cs6501 natural language processing

4

recap: learning from labeled data

v if we know the hidden

v we count how often we see     "#$    " and w&    "

states (labels)

h

h

h

h

h

h

c

c

c

c

2

1															2																	2																2																	3															2

3																	1															2																	3															2

h

h

then normalize.

cs6501 natural language processing

5

recap: tagging the input

v if we know the model, 

we can find the best tag sequence

cs6501 natural language processing

6

hard em (intuition)

v we don   t know the hidden states (i.e., pos 

tags)

1. let   s guess!
2. then, we have labels; we can estimate 

the model 

labels we guessed; if no     step 1.

3. check if the model is consistent with the 

cs6501 natural language processing

7

let   s	make	a	guess

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

?
?
0
0.8
0.2

0
?
?
0.2
0.8

-
-
-
0.5
0.5

?

?

?

?

?

?

1																2																	2															2																		3															2

?

2

?

?

?

?

?

3																	1															2																		3															2

cs6501 natural language processing

8

these	are	obvious

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

?
?
0
0.8
0.2

0
?
?
0.2
0.8

-
-
-
0.5
0.5

c

?

?

?

h

?

1																2																	2															2																		3															2

?

2

h

c

?

h

?

3																	1															2																		3															2

cs6501 natural language processing

9

guess	more

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

?
?
0
0.8
0.2

0
?
?
0.2
0.8

-
-
-
0.5
0.5

c

c

?

h

h

h

1																2																	2															2																		3															2

h

2

h

c

?

h

h

3																	1															2																		3															2

cs6501 natural language processing

10

guess	all	of	them

now	we	can	estimate	ml

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

?
?
0
0.8
0.2

0
?
?
0.2
0.8

-
-
-
0.5
0.5

c

c

c

h

h

h

1																2																	2															2																		3															2

h

2

h

c

h

h

h

3																	1															2																		3															2

cs6501 natural language processing

11

does	our	guess	consistent
with	the	model?

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.5
0.5
0
0.8
0.2

0

0.625
0.375
0.2
0.8

-
-
-
0.5
0.5

c

c

c

h

h

h

1																2																	2															2																		3															2

h

2

h

c

h

h

h

3																	1															2																		3															2

cs6501 natural language processing

12

how	to	find	latent	states
based	on	our	model?
viterbi!

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.5
0.5
0
0.8
0.2

0

0.625
0.375
0.2
0.8

-
-
-
0.5
0.5

?

?

?

?

?

?

1																2																	2															2																		3															2

?

2

?

?

?

?

?

3																	1															2																		3															2

cs6501 natural language processing

13

something	wrong   

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.5
0.5
0
0.8
0.2

0

0.625
0.375
0.2
0.8

-
-
-
0.5
0.5

from	viterbi				c														h

c

c

h

c

h																h														h			

h

h

h

from	viterbi				h															h
h

1																2																	2															2																		3															2
h																h														h			
h

c
c

h

h

h

2

3																	1															2																		3															2

cs6501 natural language processing

14

it   s	fine.
let   s	do	again

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

1
0
0
0.8
0.2

0
0.7
0.3
0.2
0.8

-
-
-
0.5
0.5

c

h

h

h

h

h

1																2																	2															2																		3															2

h

2

h

c

h

h

h

3																	1															2																		3															2

cs6501 natural language processing

15

this	time	it	is	consistent

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

1
0
0
0.8
0.2

0
0.7
0.3
0.2
0.8

-
-
-
0.5
0.5

from	viterbi				c														h

h																h																h														h			

c

h

h

h

h

h

from	viterbi				h															h

1																2																	2															2																		3															2
h																h														h			

c

h

2

h

c

h

h

h

3																	1															2																		3															2

cs6501 natural language processing

16

only one solution?

no!

em	is	sensitive	to	initialization	

p(   |	c) p(   	|	h)	 p(   |start)

0.22
0.77
0
0.8
0.2

0
0
1
0.2
0.8

-
-
-
0.5
0.5

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

c

c

c

c

h

c

1																2																	2															2																		3															2

c

2

h

c

c

h

c

3																	1															2																		3															2

cs6501 natural language processing

17

how about this?

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

?
?
0
?
?

0
?
?
?
?

-
-
-
0.5
0.5

?

?

?

?

?

?

1																2																	2															2																		3															2

?

2

?

?

?

?

?

3																	1															2																		3															2

cs6501 natural language processing

18

hard em

v we don   t know the hidden states (i.e., pos 

tags)

1. let   s guess based on our model!

v find the best sequence using viterbi algorithm 

2. then, we have labels; we can estimate 

the model 
v id113 

labels we guessed; if no     step 1.

3. check if the model is consistent with the 

cs6501 natural language processing

19

soft em

v we don   t know the hidden states (i.e., pos 

tags)

1. let   s guess based on our model!

v find the best sequence using viterbi algorithm 

2. then, we have labels; we can estimate 

let   s	use	expected	counts instead!
the model 
v id113 

labels we guessed; if no     step 1.

3. check if the model is consistent with the 

cs6501 natural language processing

20

expected counts

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

?

?

?

1																2																	2

cs6501 natural language processing

21

expected counts

some	sequences	are	more	
likely	to	occur	than	the	others

p(   |	c) p(   	|	h)	 p(   |start)

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

c

c

c

c

c

h

1																2																	2

1																2																	2

c

h

c

c

h

h

1																2																	2

1																2																	2

cs6501 natural language processing

22

expected counts

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

0.01024

c

c

c

1																2																	2

0.00064
h

c

c

0.00256

c

c

h

1																2																	2

0.00256

c

h

h

1																2																	2

1																2																	2

cs6501 natural language processing

23

expected counts
assume	we	draw	100,000	
random	samples   

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

1024

c

c

c

1																2																	2

64
h

c

c

256

c

c

h

1																2																	2

256
h

c

h

1																2																	2

1																2																	2

cs6501 natural language processing

24

expected counts

let   s	update	model

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

1024

c

c

c

1																2																	2

64
h

c

c

256

c

c

h

1																2																	2

256
h

c

h

1																2																	2

1																2																	2

cs6501 natural language processing

25

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
1024*2+256=2302
(	h	|	   )

expected counts

let   s	update	model
how	many	c-c?

1024

c

c

c

1																2																	2

64
h

c

c

p(   |	c) p(   	|	h)	 p(   |start)

-
-
-
0.5
0.5

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

256

c

c

h

1																2																	2

256
h

c

h

1																2																	2

1																2																	2

cs6501 natural language processing

26

p(   |	c) p(   	|	h)	 p(   |start)

expected counts
how	many	c-c?
how	many	c?
p(c|c)?

(	1|	   	)
(	2|	   	)
1024*2+256=2302
(	3	|	   )
( c|	   )
(	h	|	   )
2302/3968	=	0.580

1024

0.8
0.2
0
0.8
0.2

1024*3+256*2+64*2+256=3968

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

c

c

c

1																2																	2

64
h

c

c

256

c

c

h

1																2																	2

256
h

c

h

1																2																	2

1																2																	2

cs6501 natural language processing

27

expected counts
p(c|c)?

2302/3968	=	0.580

1024

c

c

c

p(   |	c) p(   	|	h)	 p(   |start)

0
0.2
0.8
0.2
0.8

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0

0.8	0.58

0.2

256

c

c

h

-
-
-
0.5
0.5

1																2																	2

do	this	for	all	the	other	entries!
64
h

256
h

c

c

1																2																	2

h

c

1																2																	2

1																2																	2

cs6501 natural language processing

28

are we done yet?

v what if we have 45 tags   ?
v what if our sentences has 20 tokens...?

v we need an efficent algorithm again!

cs6501 natural language processing

29

expected counts
p(c|c)?
2302/3968	=	0.580

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(1024+256+256+64)	 /3968	=	0.403
(	h	|	   )

p(1|c)?

p(   |	c) p(   	|	h)	 p(   |start)

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

1024

c

c

c

1																2																	2

64
h

c

c

256

c

c

h

1																2																	2

256
h

c

h

1																2																	2

1																2																	2

cs6501 natural language processing

30

    (1|c)?

expected counts
p(c|c)?
2302/3968	=	0.580

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(1024+256+256+64)	 /3968	=	0.403
(	h	|	   )

c

h

c

h

c

h

p(   |	c) p(   	|	h)	 p(   |start)

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

0.01024

0.00064

0.00256

0.00256

1																																				2																																						2

cs6501 natural language processing

31

in general

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

c

h

c

h

c

h

2																																				2																																						2

cs6501 natural language processing

32

in general

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

c

h

c

h

c

h

2																																				2																																						2

cs6501 natural language processing

33

in general

            ..    ,t/=    

let   s	say		#words	=	n

p(   |	c) p(   	|	h)	 p(   |start)

(	1|	   	)
(	2|	   	)
(	3	|	   )
( c|	   )
(	h	|	   )

0.8
0.2
0
0.8
0.2

0
0.2
0.8
0.2
0.8

-
-
-
0.5
0.5

   .

c

h

i=k

c

h

   .

c

h

2																																				2																																						2

cs6501 natural language processing

34

in general

id203	of	    $       7 and	tag	k	is	   c   		
            ..    ,t/=     =            ..    ,t/=                3    ..    |t/=    
id203	of	    73$       8 and	tag	k	is	   c   		

i=k

   .

c

h

c

h

   .

c

h

2																																				2																																						2

cs6501 natural language processing

35

in general

            ..    ,t/=     =            ..    ,t/=                3    ..    |t/=    
            ..    ,t/=     = 9             ..    ,        ..    #    ,t/=    

            3    ..    |t/=     = 9             ..    ,        3    ..    |t/=    

can	be	computed	by	
backward	algorithm

can	be	computed	by	
forward	algorithm

        ..    ;    

        <    ..    

i=k

c

h

   .

c

h

   .

c

h

2																																				2																																						2

cs6501 natural language processing

36

forward algorithm

    7     =        7    )        7#$    a

induction:

bc

i

i

    (    |       )

cs6501 natural language processing

37

backward algorithm

v            3    ..    |t/=    
=                3    ..    |    73$=                    (    73$|    )
v    7     =        73$                    (    73$|    )
    
b

	

cs6501 natural language processing

38

in general

            ..    ,t/=     =            ..    ,t/=                3    ..    |t/=    
            ..    ,t/=     = 9             ..    ,        3    ..    ,t/=    
            ..    ,t/=     = 9             ..    ,        ..    #    ,t/=    

can	be	computed	by	
forward	algorithm

can	be	computed	by	
backward	algorithm

        ..    ;    

        <    ..    

i=k

c

h

   .

c

h

   .

c

h

2																																				2																																						2

cs6501 natural language processing

39

emission counts

expected	counts	of	(2,c)

    2     =       (    "=2,    "=    ,        ..    )
"        (    "=    ,        ..    )
"

i=k

c

h

expected	counts	of	c

   .

c

h

   .

c

h

2																																				2																																						2

cs6501 natural language processing

40

how about the transition counts?

            ..    ,t/=    ,    73$=    
=            ..    ,t/=                3    ..    |	t/3$=                    (        3    |    )
=                73$                    (        3    |    )

   .

c

h

c

h

i=k

c

h

c

h

i=k+1

cs6501 natural language processing

41

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

42

trick: computing everything in log space

v homework:

v write forward, backward and viterbi algorithm 

in log-space

v hint: you need a function to compute log(a+b)

cs6501 natural language processing

43

in	contrast,	in	the	
supervised	situation,	
we	are	optimizing	

    

behind the scenes

v what is em optimized?

v log likelihood of the input!

log	    (    ,           )
vlog	    (           )
vlog               =log       (    ,           )
	=log     "v$8         "       "#$,    "#w    (    "       ")
x
log	        ,          =              "v$8         "       "#$,    "#w        "    "
									=   (log	        "       "#$,    "#w +log        "       ")
log	      is	hard;	   log	  =      log
"

this	is	hard;	in	contrast

is	easy

cs6501 natural language processing

44

intuition of em 
(from the optimization perspective)

    (b)    (b3$)

    (b3w)

    b

f     =	log             =                   (    ,           )
1.	define	gc     such	that
f            b    	       
andf    (b) =    b    b
    b3$
2.	optimize	gc    

key	idea:

cs6501 natural language processing

45

intuition of em 
(from optimization perspective)

    (b)    (b3$)

    (b3w)

>

    b

key	idea:

f     =	log             =                   (    ,           )
1.	define	gc     such	that
f            b    	       
andf    (b) =    b    b
    b3$
2.	optimize	gc    
different	gc    

hard	em,	soft	em define	

cs6501 natural language processing

46

gc     for soft em
v                        ,           
x
=log    f    ,        
f        ,    b
x
                    ,    b
x

              ,    b
let	       (    )=1
log            	    (    )
log f    ,        
k
	
f        ,    b

jensen	inequality:

       p(x)log        	
k

cs6501 natural language processing

47

gc    (b) =        b ?
v                        ,           
log f    ,        
x
                    ,    b
	
f        ,    b
x
        b =                        ,         (b)
 =log    (    |    b)
logf    ,        (b)
x
gb    (b) =                 ,    b
f        ,    b
x
=                 ,    b
log              (b)
=(log              (b) )                 ,    b
x
=log	          b
x

cs6501 natural language processing

48

intuition of em 
(from optimization perspective)

    (b)    (b3$)

    (b3w)

>

    b

key	idea:

f     =	log             =                   (    ,           )
1.	define	gc     such	that
f            b    	       
andf    (b) =    b    b
    b3$
2.	optimize	gc    
gc    =                 ,    b
log f    ,        
f        ,    b
x

soft	em define	

		

cs6501 natural language processing

49

optimizing gc    
log f    ,        
gc     =                 ,    (b)
f        ,    (b)
have	    
x
=                 ,    b
(log         ,             log              ,    (b) )
x
maxr gs     =                 ,    (b) (log         ,          )
	
x
=                 ,    (b)    (log	        "       "#$,    "#w +log        "       ")
x
"
log	        ,          =              "v$8         "       "#$,    "#w        "    "
									=   (log	        "       "#$,    "#w +log        "       ")
log	      is	hard;	   log	  =      log
"

in	contrast,	in	supervised	learning	case:

is	easy

 

this	term	doesn   t	

we	know	how	to	solve	this!!

cs6501 natural language processing

50

