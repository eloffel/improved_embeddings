lecture 6: vector 

space model

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

6501 natural language processing

1

this lecture

v how to represent a word, a sentence, or a 

document?

v how to infer the relationship among words?

v we focus on    semantics   : id65

v what is the meaning of    life   ?

6501 natural language processing

2

6501 natural language processing

3

how to represent a word

v na  ve way: represent words as atomic 

symbols: student, talk, university
v n-germ language model, logical analysis
v represent word as a    one-hot    vector

[ 0      0     0        1      0       0 ]
egg						student				talk								university				happy											buy					

v how large is this vector?

v ptb data: ~50k, google 1t data: 13m

v	           =?

6501 natural language processing

4

issues?

v dimensionality is large; vector is sparse
v no similarity  

       ())* =	[0		0		0		1		0	   	0 ]
    +(,						=	[0		0		1		0		0	   	0	]
    -./0 = [1		0		0		0		0	   	0	]
       ())*       +(,=	       ())*	       -./0 =	0

v cannot represent new words
v any idea?

6501 natural language processing

5

idea 1: taxonomy (word category)

6501 natural language processing

6

what is    car   ?

>>>	fromnltk.corpusimportid138 as wn
>>>	wn.synsets('motorcar')
[synset('car.n.01')]

>>>	motorcar.hypernyms()	[synset('motor_vehicle.n.01')]
>>>	paths	=	motorcar.hypernym_paths()	
>>>	[synset.name()	for synsetin paths[0]]	

['entity.n.01',	'physical_entity.n.01',	'object.n.01',	'whole.n.02',	'artifact.n.01', 'instrumentality.n.03',	
'container.n.01',	'wheeled_vehicle.n.01','self-propelled_vehicle.n.01',	'motor_vehicle.n.01',	'car.n.01']

>>>	[synset.name()	for synsetin paths[1]]
['entity.n.01',	'physical_entity.n.01',	'object.n.01',	'whole.n.02',	'artifact.n.01', 'instrumentality.n.03',	
'conveyance.n.03',	'vehicle.n.01',	'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01',	'motor_vehicle.n.01',	
'car.n.01']

6501 natural language processing

7

word similarity?

>>>	right	=	wn.synset('right_whale.n.01')
>>>	minke =	wn.synset('minke_whale.n.01')	
>>>	orca	=	wn.synset('orca.n.01')	
>>>	tortoise	=	wn.synset('tortoise.n.01')	
>>>	novel	=	wn.synset('novel.n.01')	

>>>	right.lowest_common_hypernyms(minke)	
[synset('baleen_whale.n.01')]
>>>	right.lowest_common_hypernyms(orca)	
[synset('whale.n.02')]
>>>right.lowest_common_hypernyms(tortoise)	
[synset('vertebrate.n.01')]
>>>	right.lowest_common_hypernyms(novel)	
[synset('entity.n.01')]

require	human	labor

6501 natural language processing

8

taxonomy (word category)

v synonym, hypernym (is-a), hyponym 

6501 natural language processing

9

idea 2: similarity = id91

6501 natural language processing

10

cluster id165 model

v can be generated from unlabeled corpora

v based on statistics, e.g., mutual information

implementation	 of	the	brown	 hierarchical	 word	id91	 algorithm.	percy	liang

6501 natural language processing

11

idea 3: distributional representation

"a word is characterized by the company it keeps    
--firth, john, 1957

v linguistic items with similar distributions 

have similar meanings
v i.e., words occur in the same contexts 

    similar meaning

6501 natural language processing

12

vector representation 
(id27s)

v discrete     distributed representations 

v word meanings are vector of    basic concept   

v what are    basic concept   ?
v how to assign weights?
v how to define the similarity/distance?

    0.23= [0.8
    45662= [0.8
    ())/*= [0.1

0.9
0.1
0.2

0.1
0.8
0.1	

0
0
0.8

   	]
   	]
   	]

royalty				masculinity					femininity						eatable	

6501 natural language processing

13

an illustration of vector space model

royalty	

w2

|d2-d4|

w4

w3

masculine	

eatable

w5

w1

6501 natural language processing

14

semantic similarity in 2d

v home depot product

6501 natural language processing

15

capture the structure of words

v example from glove

6501 natural language processing

16

how to use word vectors?

6501 natural language processing

17

pre-trained word vectors

v google book

https://code.google.com/archive/p/id97
v 100 billion tokens, 300 dimension, 3m words

v glove project

http://nlp.stanford.edu/projects/glove/
v pre-trained word vectors of wiki (6b), web

crawl data (840b), twitter (27b)

6501 natural language processing

18

v cosine similarity 

distance/similarity

v vector similarity measure

    similarity in meaning
vcos    ,     = 5	   	;
||5||   ||;||
v euclidean distance  ||           ||>
v inner product       	   	    

v word vector are normalized by length

5||5|| is	a	unit	vector

v same as cosine similarity if vectors are 

normalized

6501 natural language processing

19

distance/similarity

v vector similarity measure

    similarity in meaning
vcos    ,     = 5	   	;
||5||   ||;||
v euclidean distance  ||           ||>
v inner product       	   	    

linguistic	regularities	in	sparse	and	explicit	
v cosine similarity 
word	representations, levy goldberg, conll 14

v word vector are normalized by length

choosing the right similarity metric is important

v same as cosine similarity if vectors are 

normalized

6501 natural language processing

20

word similarity demo

v http://msrcstr.cloudapp.net/

6501 natural language processing

21

word analogy

v    -(2       ?@-(2+    52b/6	       (52d	

6501 natural language processing

22

from words to phrases

6501 natural language processing

23

neural language models

6501 natural language processing

24

how to    learn    word vectors?

what	are	   basic	concept   ?
how	to	assign	weights?
how	to	define	the	similarity/distance?	cosine	similarity

6501 natural language processing

25

back to distributional representation

v encode relational data in a matrix

v co-occurrence (e.g., from a general corpus)

v bag-of-word model: documents (clusters) as the basis 

for vector space 

6501 natural language processing

26

back to distributional representation

v encode relational data in a matrix

v co-occurrence (e.g., from a general corpus)
v skip-grams

6501 natural language processing

27

back to distributional representation

v encode relational data in a matrix

v co-occurrence (e.g., from a general corpus)
v skip-grams
v from taxonomy (e.g., id138, thesaurus)

input: synonyms from a thesaurus
joyfulness: joy, gladden
sad: sorrow, sadden

group 1:	   joyfulness   

group	2:	   sad   

group	3:	   affection   

joy

gladden

sorrow

sadden

goodwill

1

0

0

1

0

0

0

1

0

0

1

0

0

0

1

6501 natural language processing

28

back to distributional representation

v encode relational data in a matrix

v co-occurrence (e.g., from a general corpus)
v skip-grams
v from taxonomy (e.g., id138, thesaurus)

pros	and	cons?

input: synonyms from a thesaurus
joyfulness: joy, gladden
sad: sorrow, sadden

group 1:	   joyfulness   

group	2:	   sad   

group	3:	   affection   

joy

gladden

1

0

0

1

0

0

cosine	similarity?

sorrow

sadden

goodwill

0

1

0

0

1

0

0

0

1

6501 natural language processing

29

problems?

v number of basic concepts is large
v basis is not orthogonal 

(i.e., not linearly independent)

v some function words are too frequent (e.g., the)

v syntax has too much impact
v e.g, tf-idf can be applied
v e.g, skip-gram: scaling by distance to target

6501 natural language processing

30

latent semantic analysis (lsa)

v data representation

v encode single-relational data in a matrix

v co-occurrence (e.g., document-term matrix, 

skip-gram)

v synonyms (e.g., from a thesaurus)

v factorization

v apply svd to the matrix to find latent 

components

6501 natural language processing

31

principle component analysis (pca)

v decompose the similarity space into a set 

of orthonormal basis vectors

6501 natural language processing

32

principle component analysis (pca)

v decompose the similarity space into a set 

factorization such that 

of orthonormal basis vectors

v for an            matrix      , there exists a 
    =          l
v    ,     are orthogonal matrices

6501 natural language processing

33

low-rank approximation 

v idea: store the most important information in a 

small number of dimensions (e.g,. 100-1000)

v svd can be used to compute optimal low-rank 

approximation

v set smallest n-r singular value to zero

v similar words map to similar location in low 

dimensional space

6501 natural language processing

34

latent semantic analysis (lsa)

v factorization

v apply svd to the matrix to find latent 

components

6501 natural language processing

35

lsa example

v original matrix  c

example	from	christopher	manning	and	pandu nayak,	introduction	to	ir

6501 natural language processing

36

lsa example

v svd:     =          l

6501 natural language processing

37

lsa example

v dimension reduction                  l

v original matrix c

6501 natural language processing

38

lsa example

v original matrix      v.s. reconstructed matrix	    >

v what is the similarity between ship and boat?

6501 natural language processing

39

word vectors 

                 l
        l              l            l l	
=          l        l    l	
=         l    l	
=       u   l
v    :,+   .)       :,p@(d			          :,+   .)          :,p@(d

(why?)

6501 natural language processing

40

why we need low rank approximation?

v knowledge base (e.g., thesaurus) is never 

complete

v noise reduction by dimension reduction 
v intuitively, lsa brings together    related    

axes (concepts) in the vector space

v a compact model

6501 natural language processing

41

all problem solved?

6501 natural language processing

42

an analogy game

man is to computer programmer as woman is to homemaker? 
debiasing id27s, nips 16

6501 natural language processing

43

continuous semantic representations

sunny

rainy

cloudy

windy

cab

car

wheel

emotion

sad

joy

feeling

6501 natural language processing

44

semantics needs more than similarity

tomorrow will 

be sunny.

tomorrow will 

be rainy.

                            (rainy, sunny)?
                            (rainy, sunny)?

6501 natural language processing

45

continuous representations for entities

republic	party

democratic	party

?

george	w	bush

laura	bush

michelle	obama

6501 natural language processing

46

continuous representations for entities

    useful resources for nlp applications
    id29 & id53 
    information extraction

6501 natural language processing

47

next lecture: a more flexible framework

v directly learn word vectors using nn model

v more flexible 
v easier to learn new words
v incorporate other information
v optimize specific task loss.

v review calculus!

6501 natural language processing

48

