lecture 11: 

viterbi and forward 

algorithms

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501 natural language processing

1

quiz 1

quiz	1

30

25

20

15

10

5

0

[0-5]

[6-10]

[11-15]

[16-20]

[21-25]

v max: 24; mean: 18.1; median: 18; sd: 3.36

cs6501 natural language processing

2

this lecture

v two important algorithms for id136

v forward algorithm
v viterbi algorithm

cs6501 natural language processing

3

cs6501 natural language processing

   #   

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

5

likelihood of the input

v how likely a sentence    i love cat    occur

v compute     (           ) for the input      and 
id48     
v remember, we model         ,           
v               =            ,           

    

marginal	id203:
sum	over	all	possible	tag	sequences

cs6501 natural language processing

6

likelihood of the input

v how likely a sentence    i love cat    occur

v               =            ,           
					=     ./01         .    .        .       .40   
	
    
    
v            	                	                    
=             	                	               ,                        +           	                	               ,                       
										+           	                	               ,                        +            	                	               ,                       
										+           	                	               ,                        +            	                	               ,                       
v now, let   s write down     (       	                	                      ) with 
+            	                	               ,                        +            	                	               ,                       

v assume we have 2 tags n, v

45 tags   

cs6501 natural language processing

7

    

trellis diagram

     is	the	parameter	set	of	
v goal: p(           )=     ./01         .    .        .       .40   
    (    j|    j=1)
   

    (    c=2|    0=1)

    (    j=1|    c=1)

id48.	let   s	ignore	it	in	some	
slides	for	simplicity   s	sake	

    =1																					    =2																		    =3																		    =4

cs6501 natural language processing

8

trellis diagram

v p(   i eat a fish   , nvva )

    (    |    )

    (    |    )

    (    |    )

    (    |    )
    (    |<    >)

n

v

n

v

n

v

n

v

a

    (            |    )
    =1																					    =2																		    =3																		    =4

    (    |    )

a

a

a

   
    (               |a)

cs6501 natural language processing

9

trellis diagram

v     ./01         .    .        .       .40   
    

: sum over all paths

n

v

n

v

n

v

n

v

   

a

a

    =1																					    =2																		    =3																		    =4

a

a

cs6501 natural language processing

10

id145

v recursively decompose a problem into 

v similar to mathematical induction

smaller sub-problems

v base step: initial values for     =1
    =    , let   s compute     =    +1

v inductive step: assume we know the values for 

cs6501 natural language processing

11

forward algorithm

v inductive step: from     =    to	i=	k+1	
v    y:	tag	sequence	with	length	    ,	    y=    0,    c       y
v        (    y,        )
    (    y40,        ,    y=    )
    g

=	       
k

    gij

 

	p(        ,    y=    )

tag	sequences

tag	@	i=k

tag	sequences

n

v

n

v

n

v

n

v

   

a

a

    =1																					    =2																		    =3																		    =4

a

a

cs6501 natural language processing

12

forward algorithm

v inductive step: from     =    to	i=	k+1	
=    p(        ,     y=    )  
v        (    y,    )
k
    g
vp(        ,     y=    )	=	        (
kl         ,    y40=    m,    y=    )
=        (
km         40,    y40=    m)    (    y=           y40=       )    (    y       y=    )
   
    =       1																		    =    

n

n

n

n

a

a

a

a

v

v

v

v

cs6501 natural language processing

13

forward algorithm

v inductive step: from     =    to	i=	k+1	
let   s	call	it	    y(    )
=    p(        ,     y=    )  
v        (    y,    )
this	is	    y40(       )
k
    g
vp(        ,     y=    )	=	        (
kl         ,    y40=    m,    y=    )
=        (
km         40,    y40=    m)    (    y=           y40=       )    (    y       y=    )
   
    =       1																		    =    

14

n

n

n

n

a

a

a

a

v

v

v

v

cs6501 natural language processing

forward algorithm

v inductive step: from     =    to	i=	k+1	
v     y(    )=	        y40(       )
    (    y=           y40=       )    (    y       y=    )

km

n

v

a

n

v

a

v

v

n

n

   
    =       1																		    =    

a

a

15

cs6501 natural language processing

forward algorithm

v inductive step: from     =    to	i=	k+1	
v     y(    )=	        y40(       )
    (    y=           y40=       )    (    y       y=    )
    (    y=           y40=       )
km

km
=    (    y       y=    )        y40(       )

n

v

a

n

v

a

v

v

n

n

   
    =       1																		    =    

a

a

16

cs6501 natural language processing

    0=         (    0=           q)

forward algorithm

v base step: i=0 

v     0     =         0
    (    0=    )

initial	id203

n

v

a

n

v

a

v

v

n

n

   
    =       1																		    =    

a

a

17

cs6501 natural language processing

implementation using an array

v use a            table to keep     y(    )

from	julia	hockenmaier,	intro	to	nlp

cs6501 natural language processing

18

implementation using an array

trellis[1][q]	=         0

initial:

    0=         (    0=           q)

cs6501 natural language processing

19

implementation using an array

    y(    )=    (    y       y=    )        y40(       )

induction:

km

i

i

    (    y=           y40=       )

cs6501 natural language processing

20

the forward algorithm (pseudo code)

.fwd=0

cs6501 natural language processing

21

jason   s ice cream

scroll to the bottom to see a graph of what states and transitions the 
model thinks are likely on each day.  those likely states and transitions 
can be used to reestimate the red probabilities (this is the "forward-
backward" or baum-welch algorithm), incr
p(   |h)

p(   |start)

p(   |c)

#cones

v p(   1,2,1   )?

0.5

0.5

0.5
c

h

0.1

p(1|   )
p(2|   )
p(3|   )
p(c|   )
p(h|   )

0.8

0.2

0.2

0.8

0.5
0.4
0.1
0.8
0.2

0.4
c

h

0.2

0.1
0.2
0.7
0.2
0.8

0.5
0.5

0.8

0.2

0.2

0.8

0.5
c

h

0.1

cs6501 natural language processing

22

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

23

prediction in generative model

v id136: what is the most likely 

sequence of tags for the given sequence of 
words w

    (    0)

initial	id203

v what are the latent states that most likely 

generate the sequence of word w

cs6501 natural language processing

24

v find best tag sequence of     i love cat   

tagging the input

v remember, we model         ,           
v       =argmax              ,         

find	the	best	one	from	all	possible	tag	sequences

cs6501 natural language processing

25

tagging the input

v assume we have 2 tags n, v
v which one is the best?

            	                	               ,                        ,            	                	               ,                        ,
										            	                	               ,                        ,            	                	               ,                        ,
										            	                	               ,                        ,            	                	               ,                        , 
            	                	               ,                        ,            	                	               ,                       

v again! we need an efficient algorithm

cs6501 natural language processing

26

trellis diagram

v goal: argmax       ./01         .    .        .       .40
    (    j=1|    c=1)

    (    c=2|    0=1)

    (    j|    j=1)
   

    =1																					    =2																		    =3																		    =4

cs6501 natural language processing

27

trellis diagram

v goal: argmax       ./01         .    .        .       .40

v find the best path!

n

v

n

v

n

v

n

v

   

a

a

    =1																					    =2																		    =3																		    =4

a

a

cs6501 natural language processing

28

id145 again!

v recursively decompose a problem into 

v similar to mathematical induction

smaller sub-problems

v base step: initial values for     =1
    =    , let   s compute     =    +1

v inductive step: assume we know the values for 

cs6501 natural language processing

29

viterbi algorithm

v inductive step: from     =    to	i=	k+1	
v    y:	tag	sequence	with	length	    ,	    y=    0,    c       y
vmax             (    y,        )=            kmax        i        (    y40,    y=    ,        )

tag	sequences

tag	@	i=k

tag	sequences
n

v

n

v

n

v

n

v

   

a

a

    =1																					    =2																		    =3																		    =4

a

a

cs6501 natural language processing

30

viterbi algorithm

v inductive step: from     =    to	i=	k+1	
vmax        i        (    y40,    y=    ,        )
=            kl	max        i    	    (    y4    ,    y=    ,    y40=    m,        )
=            kl	max        i    	        y4    ,    y40=    m,        4            y=    ,    y40=    m        y

let   s	call	it	    y(    )
this	is		    y40(       )
    y=    
   
    =       1																		    =    

31

n

n

a

a

v

v

n

v

a

n

v

a

cs6501 natural language processing

viterbi algorithm

v inductive step: from     =    to	i=	k+1	
v     y     =maxkm     y40(    m)    (    y=           y40=       )    (    y       y=    )

n

v

a

n

v

a

v

v

n

n

   
    =       1																		    =    

a

a

32

cs6501 natural language processing

viterbi algorithm

v inductive step: from     =    to	i=	k+1	
v     y     =maxkl     y40    m         y=           y40=    m        y       y=    
=        y       y=    	maxkl     y40    m         y=           y40=    m

n

v

a

n

v

a

v

v

n

n

   
    =       1																		    =    

a

a

33

cs6501 natural language processing

    0=         (    0=           q)

viterbi algorithm

v base step: i=0 

v     0     =         0
    (    0=    )

initial	id203

n

v

a

n

v

a

v

v

n

n

   
    =       1																		    =    

a

a

34

cs6501 natural language processing

implementation using an array

trellis[1][q]	=         0

initial:

    0=         (    0=           q)

cs6501 natural language processing

35

implementation using an array

    y     =	        y       y=    		maxkl     y40    m         y=           y40=    m

induction:

cs6501 natural language processing

36

retrieving the best sequence

v keep one backpointer

cs6501 natural language processing

37

the viterbi algorithm (pseudo code)

.fwd=0

max	instead	of	sum

cs6501 natural language processing

38

cs6501 natural language processing

39

jason   s ice cream

scroll to the bottom to see a graph of what states and transitions the 
model thinks are likely on each day.  those likely states and transitions 
can be used to reestimate the red probabilities (this is the "forward-
backward" or baum-welch algorithm), incr
p(   |h)
0.1
0.2
0.7
0.2
0.8

p(   |c)
0.5
0.4
0.1
0.8
0.2

p(1|   )
p(2|   )
p(3|   )
p(c|   )
p(h|   )

p(   |start)

0.5
0.5

#cones

v best tag sequence for p(   1,2,1   )?

0.5

0.5

0.5
c

h

0.1

0.8

0.2

0.2

0.8

0.4
c

h

0.2

0.8

0.2

0.2

0.8

0.5
c

h

0.1

cs6501 natural language processing

40

trick: computing everything in log space

v homework:

v write forward and viterbi algorithm in log-space

v hint: you need a function to compute log(a+b)

cs6501 natural language processing

41

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

42

