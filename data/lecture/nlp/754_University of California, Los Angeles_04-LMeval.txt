lecture 4: language 
model evaluation and 
advanced methods

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

6501 natural language processing

1

this lecture

v kneser-ney smoothing
v discriminative language models 
v neural language models
v evaluation: cross-id178 and perplexity 

6501 natural language processing

2

recap: smoothing 

v add-     smoothing

v add-one smoothing

vparameters tuned by the cross-validation

v witten-bell smoothing

v t:  # word types n: # tokens
v t/(n+t): total prob. mass for unseen words
v n/(n+t): total prob. mass for observed tokens

v good-turing

v reallocate the id203 mass of id165s that 

occur r+1 times to id165s that occur r times.

6501 natural language processing

3

recap: back-off and interpolation

v idea: even if we   ve never seen    red 

glasses   , we know it is more likely to occur 
than    red abacus   

v interpolation:

paverage(z | xy) =   3 p(z | xy) +   2 p(z | y) +   1 p(z)
where   3 +   2 +   1 = 1 and all are    0

6501 natural language processing

4

absolute discounting

v save ourselves some time and just 

subtract 0.75 (or some d)!

p
absolutedi

scounting

(

ww
i
i

|

1
   

discounted	bigram
wwc
,
(
)
i
i
1
   
wc
(
i

=

1
   

)
)

d

   

interpolation	weight

(
  

+

)

1
   

wpw
(
)
i
unigram

v but should we really just use the regular 

unigram p(w)?

6501 natural language processing

5

kneser-ney smoothing

v better	estimate	for	probabilities	of	lower-order	

unigrams!
v shannon	game:		i can   t see without my 
glasses

reading___________?

francisco

v    francisco   	is	more	common	than	   glasses   
v    	but	   francisco   	always	follows	   san   

6501 natural language processing

6

kneser-ney smoothing

v instead	of		p(w):	   how	likely	is	w   
v pcontinuation(w):		   how	likely	is	w	to	appear	as	a	

novel	continuation?
v for	each	word,	count	the	number	of	bigram	types	it	

completes

v every	bigram	type	was	a	novel	continuation	the	first	

time	it	was	seen

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

6501 natural language processing

7

kneser-ney smoothing

v how	many	times	does	w	appear	as	a	novel	

continuation:

pcontinuation(w)      {wi   1 :c(wi   1,w) > 0}

v normalized by the total number of word bigram 

types

{(wj   1,wj):c(wj   1,wj) > 0}

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}

{(wj   1,wj):c(wj   1,wj) > 0}

6501 natural language processing

8

kneser-ney smoothing

v alternative metaphor: the number of  # of word 

types seen to precede w

|{wi   1 :c(wi   1,w) > 0}|

v normalized by the # of words preceding all words:

pcontinuation(w) =

{wi   1 :c(wi   1,w) > 0}
{w'i   1 :c(w'i   1,w') > 0}

   
w'

v a frequent word (francisco) occurring in only one 

context (san) will have a low continuation 
id203

6501 natural language processing

9

kneser-ney smoothing

max(c(wi   1,wi)    d,0)

pkn(wi | wi   1) =
c(wi   1)
   is a normalizing constant; 

the id203 mass we   ve discounted

+  (wi   1)pcontinuation(wi)

  (wi   1) =

d
c(wi   1) {w :c(wi   1,w) > 0}

the	normalized	discount

the	number	of	word	types	that	can	
follow	wi-1
=	#	of	word	types	we	discounted
=	#	of	times	we	applied	normalized	
discount

6501 natural language processing

10

kneser-ney smoothing: recursive 
formulation

pkn(wi | wi   n+1
i   1

) =

i

max(ckn(wi   n+1
ckn(wi   n+1
i   1

)     d,0)
)

+  (wi   n+1
i   1

)pkn(wi | wi   n+2
i   1

)

ckn(   ) =

!
#
"
$#

count(   )   for the highest order

continuationcount(   )    for lower order

continuation	count	=	number	of	unique	single	word	contexts	for	  

6501 natural language processing

11

practical issue: 
huge web-scale id165s
v how to deal with, e.g., google id165 

corpus
v pruning

v only store id165s with count > threshold.
v remove singletons of higher-order id165s

6501 natural language processing

12

huge web-scale id165s

v efficiency

v efficient data structures 

v e.g. tries

https://en.wikipedia.org/wiki/trie

v store words as indexes, not strings
v quantize probabilities (4-8 bits instead of 

8-byte float)

6501 natural language processing

13

smoothing

this dark art is why nlp is taught in the 
engineering school.

there are more principled 

smoothing methods, too.  we   ll 
look next at id148, 
which are a good and popular 
general technique.

600.465 - intro to nlp - j. eisner

6501 natural language processing

14

14

conditional modeling

v generative language model (tri-gram model):

    (    ),       +)=	p    )        0    )             +     +10,    +1)
v can we model          $     %,        directly?

v then, we compute the conditional  probabilities  by 

maximum likelihood  estimation 

v given a context x, which outcomes y are likely 

in that context?
p (nextword=y | precedingwords=x)

600.465 - intro to nlp - j. eisner

6501 natural language processing

15

15

ed

v let   s assume

modeling conditional probabilities

    (    |    ) = 	exp(scorex,y)/    exp	(                        ,    d)
v    (    |    )	 is high     score(x,y) is high
vrequire that p(y | x)     0, and         (    |    )
e

vthis is called soft-max

y: nextword, x: precedingwords

not true of score(x,y)

= 1;  

6501 natural language processing

16

linear scoring

v score(x,y): how well does y go with x?
v simplest option: a linear function of (x,y).  

    describe it by some numbers (i.e. numeric features) 

but (x,y) isn   t a number 

v then just use a linear function of those numbers.
weight of the kth feature. to be learned    

ranges over all features

whether (x,y) has feature k(0 or 1)
or how many times it fires (    0)
or how strongly it fires

(real #)

6501 natural language processing

17

what features should we use?

v model pwiw$1),	w$10):
       (       $1),    $10   ,       $   ) for score(       $1),    $10   ,       $   )can	be
v #        $1)    appears in the training corpus. 
v 1, if        $    is an unseen word; 0, otherwise.
v 1, if        $1),    $10    =    a red   ; 0, otherwise.
v 1, if        $10    belongs to the    color    category; 0 otherwise. 

6501 natural language processing

18

what features should we use?

v model p                                         	               ):
       (                  ,          ,                                  ) for score(                  ,          ,                                  )
v #                    appears in the training corpus. 
v 1, if            is an unseen word; 0, otherwise.
v 1, if    a	                =    a red   ; 0, otherwise.
v 1, if                    belongs to the    color    category; 0 otherwise. 

6501 natural language processing

19

log-linear id155

unnormalized
prob (at least
it   s positive!)

where we choose z(x) to ensure that 

thus,  

partition function

600.465 - intro to nlp - j. eisner

6501 natural language processing

20

20

training   

this version is    discriminative training   : 

to learn to predict y from x, maximize p(y|x).

whereas in    generative models   , we learn 

to model x, too, by maximizing p(x,y).

v n training examples
v feature functions f1, f2,    
v want to maximize p(training data|  )

v easier to maximize the log of that:

v alas, some weights   i may be optimal at -    or +   .
when would this happen?   what   s going    wrong   ?

6501 natural language processing

21

generalization via id173

v n training examples
v feature functions f1, f2,    
v want to maximize p(training data|  )     pprior(  )

v easier to maximize the log of that

v encourages weights close to 0.
v    l2 id173   :  corresponds to a gaussian prior

                xy/zy

6501 natural language processing

22

gradient-based training

v gradually adjust    in a direction that improves

gradient ascent to gradually increase f(  ):

while (   f(  )     0)       // not at a local max or min
   =    +        f(  )   // for some small      > 0

remember:    f(  ) = (   f(  )/     1,    f(  )/     2,    )
update means:   k +=    f(  ) /      k

6501 natural language processing

23

gradient-based training

v gradually adjust    in a direction that improves

v gradient w.r.t    

6501 natural language processing

24

more complex assumption?

v    (    |    ) = 	exp(scorex,y)/    exp(                        ,       )

       

y: nextword, x: precedingwords

v assume we saw:

red	glasses;	yellow	glasses;	green	glasses;	blue	glasses
red	shoes;	yellow	shoes;	green	shoes;	

what is p(shoes; blue)?

v can we learn categories of words(representation) 

automatically?

v can we build a high order id165 model without 

blowing up the model size?

6501 natural language processing

25

neural language model

v model     (    |    ) with a neural network 

example	1:
one	hot	vector:	each	
component	of	the	vector	
represents	one	word
[0,	0,	1,	0,	0]	

example	2:	
word	embeddings

6501 natural language processing

26

neural language model

v model     (    |    ) with a neural network 

   =tanh	(    b       +    )

non-linear	function	e.g.,	

obtain	(y|x)	by	
performing	 softmax

learned	matrices	to	project	the	input	vectors

concatenate	projected	vectors

6501 natural language processing

27

why?

v potentially generalize to unseen contexts 

v example: p(   red    |    the   ,    shoes   ,    are   )
v this does not occurs in training corpus but

[   the   ,    glasses   ,    are   ,    red   ] does.

v if the word representations  of    red    and    blue    

are similar, then the model can generalize.

v why are    red    and    blue    similar?

v because nn saw    red skirt   ,    blue skirt   ,    red 

pen   ,    blue pen   , etc.

6501 natural language processing

28

training neural language models

v can use gradient ascent as well

v using the chain rule to derive the gradient

a.k.a. back propagation 

v more complex nn architectures can be 
used     e.g., lstm, char-based models

6501 natural language processing

29

language model evaluation 

v how to compare models?

v we need an unseen text set, why?

v id205: 

study resolution of uncertainty.
v perplexity: measure how well a id203 

distribution predicts a sample

6501 natural language processing

30

cross-id178

v a common measure of model quality

v task-independent
v continuous     slight improvements show up here 

even if they don   t change # of right answers on task
v just measure id203 of (enough) test data
v higher prob means model better predicts the future
v there   s a limit to how well you can predict random 

stuff

v limit depends on    how random    the dataset is 

(easier to predict weather than headlines, 
especially in arizona)

6501 natural language processing

31

cross-id178 (   xent   )

v want prob of test data to be high:

average?
geometric average 
of  1/23,1/23, 1/23, 1/24
= 1/23.25     1/9.5

p(h | <s>, <s>) * p(o | <s>, h) * p(r | h, o) * p(s | o, r)    

1/8              *          1/8          *    1/8         *     1/16         

v high prob     low xent by 3 cosmetic improvements:

v take logarithm (base 2) to prevent underflow:

log (1/8 * 1/8 * 1/8 * 1/16    ) 
= log 1/8 + log 1/8 + log 1/8 + log 1/16     = (-3) + (-3) + (-3) + (-4) +    

v negate to get a positive value in bits
v divide by length of text    3.25 bits per letter (or per 

3+3+3+4+   

word)

6501 natural language processing

32

cross-id178 (   xent   )

v want prob of test data to be high:

average?
geometric average 
of  1/23,1/23, 1/23, 1/24
= 1/23.25     1/9.5

p(h | <s>, <s>) * p(o | <s>, h) * p(r | h, o) * p(s | o, r)    

1/8              *          1/8          *    1/8         *     1/16         
v cross-id178    3.25 bits per letter (or per word)
v want this to be small (equivalent to wanting good 

compression!)

v lower limit is called id178     obtained in principle as 
cross-id178 of the true model measured on an infinite 
amount of data 

v perplexity = 2xent (meaning    9.5 choices)

6501 natural language processing

33

more math: id178 h(x)

v the id178h(    ) of a discrete random variable     
hp =                log0    (    )

is the expected negative log id203:

k

v id178 is  measure of uncertainty

6501 natural language processing

34

id178 of coin tossing 

v toss a coin p(h)=    , p(t)=1   p
v h(p)=       log0        1        log01       

v p=0.5: h(p)= 1
v p=1: h(p) = 0

6501 natural language processing

35

id178 of coin tossing 

v toss a coin p(h)=    , p(t)=1   p
v h(p)=       log0        1        log01       

v p=0.5: h(p)= 1
v p=1: h(p) = 0

6501 natural language processing

36

how many bits to encode messages

in average to encode a message ~ p?

v consider three letters (a, b, c, d):
v if p=(  ,   , 0, 0),  how many bits per letter 

v encode a as 0, b as 1;aaabbbaa     00011100
v a: 00, b: 01, c:10, d:11; abda	    00011100 
v a: 0, b:10, c:11;   aaacba     00011100

v how about p=(  ,   ,   , 0)

v if p=(   ,    ,    ,    )

6501 natural language processing

37

v cross-id178:

more math: cross id178

vavg.	#	bits	to	encode	events	~	p(x)	using	a	coding	
scheme	m(x)	
vhp,     =                log0    (    )
v not symmetric: hp,        h    ,    

k

v lower bounded  by h(p)

v let p=(  ,   ,   , 0)

v we encode a:00, b:01, c:10, d:11

(i,e., m= (  ,   ,   ,   ))

v aaacba?

000000100100

6501 natural language processing

38

perplexity and geometric mean

perplexity
perplexity(w1. . . wn) = 2h(w1. . . wn )

v

n log2 m(w1. . . wn )

= 2  1
= m(w1. . . wn)  1

n

= ns

1

m(w1. . . wn)

language model m is better than m    if it assigns lower 
perplexity (i.e. lower cross-id178, and higher 
id203) to the test corpus w1...wn 

6501 natural language processing

39

an experiment

unigram, bigram, trigram model (with good-turing)
training data: 
38m words of wsj text (vocabulary: 20k types)
v train: 38m wsj text, |v|= 20k
test data:
v test: 1.5m wsj text
1.5m words of wsj text
results:

unigram bigram trigram

perplexity

962

170

109

v word level lstm ~85
cs498jh: introduction to nlp 
v char level ~79

6501 natural language processing

40

