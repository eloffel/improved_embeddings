lecture 3: language 

model smoothing

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501 natural language processing

1

this lecture

    zipf   s law

    dealing with unseen words/id165s

    add-one smoothing
    linear smoothing
    good-turing smoothing

    absolute discounting
    kneser-ney smoothing

cs6501 natural language processing

2

recap: bigram language model

<s> i am sam </s>
<s> i am legend </s>
<s> sam i am </s>

let p(<s>) = 1

p( i | <s>) = 2 / 3     p(am | i) = 1

p( sam | am) = 1/3    p( </s> | sam) = 1/2

p( <s> i am sam</s>) = 1*2/3*1*1/3*1/2

cs6501 natural language processing

3

more examples: 
berkeley restaurant project sentences

    can you tell me about any good cantonese restaurants 

close by

    mid priced thai food is what i   m looking for

    tell me about chez panisse

    can you give me a listing of the kinds of food that are 

available

    i   m looking for a good place to eat breakfast

    when is caffe venezia open during the day

cs6501 natural language processing

4

raw bigram counts

    out of 9222 sentences

cs6501 natural language processing

5

raw bigram probabilities

    normalize by unigrams:

    result:

cs6501 natural language processing

6

zeros

    training set:

    denied the allegations
    denied the reports
    denied the claims
    denied the request

   test set

    denied the offer
    denied the loan

p(   offer    | denied the) = 0

cs6501 natural language processing

7

smoothing

this dark art is why 

nlp is taught in the 

engineering school.

there are more principled 

smoothing methods, too.  we   ll 
look next at id148, 
which are a good and popular 
general technique.

but the traditional methods are 
easy to implement, run fast, and 
will give you intuitions about 
what you want from a smoothing 
method.

credit: the following slides are adapted from jason eisner   s nlp course

cs6501 natural language processing

8

what is smoothing?

20

200

2000

2000000

cs6501 natural language processing

9

ml 101: bias variance tradeoff

    different samples of size 20 vary considerably
    though on average, they give the correct bell 

curve!

20

20

20

20

cs6501 natural language processing

10

overfitting

cs6501 natural language processing

11

the perils of overfitting

    id165s only work well for word prediction if the 

test corpus looks like the training corpus
   in real life, it often doesn   t

    we need to train robust models that 

generalize!

    one kind of generalization: zeros!

   things that don   t ever occur in the training set
   but occur in the test set

cs6501 natural language processing

12

the intuition of smoothing

    when we have sparse statistics:

p(w | denied the)
3 allegations
2 reports
1 claims
1 request
7 total

s
n
o
i
t
a
g
e

l
l

a

s
t
r
o
p
e
r

k
c
a

t
t

a

n
a
m

e
m
o
c
t

u
o

   

s

i

m
a
l
c

t
s
e
u
q
e
r

    steal id203 mass to generalize better

p(w | denied the)

2.5 allegations
1.5 reports
0.5 claims
0.5 request
2 other
7 total

s
n
s
o
n
i
o
t
a
i
t
g
a
e
g
e
a
a

l
l

l
l

s
t
r
o
p
e
r

s

i

m
a
l
c

t
s
e
u
q
e
r

k
c
a

t
t

a

n
a
m

e
m
o
c
t

u
o

   

credit: dan klein

cs6501 natural language processing

13

add-one estimation (laplace smoothing)

    pretend we saw each word one more time than 

we did

    just add one to all the counts!

    id113 estimate:

    add-1 estimate:

cs6501 natural language processing

14

pid113(wi|wi-1)=c(wi-1,wi)c(wi-1)padd-1(wi|wi-1)=c(wi-1,wi)+1c(wi-1)+vadd-one smoothing

xya

xyb

xyc

xyd

xye

   

xyz

100 100/300

101 101/326

0

0

0/300

0/300

1

1

1/326

1/326

200 200/300

201 201/326

0

0

0/300

0/300

1

1

1/326

1/326

total xy

300 300/300

326 326/326

cs6501 natural language processing

15

berkeley restaurant corpus: laplace 
smoothed bigram counts

laplace-smoothed bigrams

v=1446 in the berkeley restaurant project corpus

reconstituted counts

compare with raw bigram counts

problem with add-one smoothing

we   ve been considering just 26 letter types    

xya

xyb

xyc

xyd

xye

   

xyz

total xy

1

0

0

2

0

0

3

1/3

0/3

0/3

2/3

0/3

0/3

3/3

2

1

1

3

1

2/29

1/29

1/29

3/29

1/29

1

29

1/29

29/29

cs6501 natural language processing

20

problem with add-one smoothing

suppose we   re considering 20000 word types

see the abacus  

see the abbot 

see the abduct

see the above

see the abram

   

see the zygote

total

1

0

0

2

0

0

3

1/3

0/3

0/3

2/3

0/3

0/3

3/3

2 2/20003

1 1/20003

1 1/20003

3 3/20003

1 1/20003

1 1/20003
20003 20003/20003

cs6501 natural language processing

21

problem with add-one smoothing

suppose we   re considering 20000 word types

see the abacus  

1

1/3

2 2/20003

see the abbot 

   novel event    = event never happened in training data.
see the abduct
here: 19998 novel events, with total estimated 

1 1/20003

0/3

1 1/20003

0/3

0

0

id203 19998/20003.  
see the above

2

2/3

3 3/20003

add-one smoothing thinks we are extremely likely to see 

see the abram

0

0/3

1 1/20003

novel events, rather than words we   ve seen.

   

see the zygote

total

0

3

0/3

3/3

1 1/20003
20003 20003/20003

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

22

22

infinite dictionary?

in fact, aren   t there infinitely many possible word types?

see the aaaaa  

see the aaaab 

see the aaaac

see the aaaad

see the aaaae

   

see the zzzzz

total

1

0

0

2

0

0

3

1/3

0/3

0/3

2/3

0/3

0/3

3/3

2 2/(   +3)

1 1/(   +3)

1 1/(   +3)

3 3/(   +3)

1 1/(   +3)

1 1/(   +3)
(   +3) (   +3)/(   +3)

cs6501 natural language processing

23

add-lambda smoothing

    a large dictionary makes novel events too probable.

    to fix: instead of adding 1 to all counts, add     = 0.01?

    this gives much less id203 to novel events.

    but how to pick best value for    ?  
    that is, how much should we smooth?

cs6501 natural language processing

24

add-0.001 smoothing

doesn   t smooth much (estimated distribution has high variance)

xya

xyb

xyc

xyd

xye

   

xyz

total xy

1

0

0

2

0

0

3

1/3

0/3

0/3

2/3

0/3

0/3

3/3

1.001

0.001

0.001

2.001

0.001

0.001

3.026

cs6501 natural language processing

0.331

0.0003

0.0003

0.661

0.0003

0.0003

1

25

add-1000 smoothing

smooths too much (estimated distribution has high bias)

xya

xyb

xyc

xyd

xye

   

xyz

total xy

1

0

0

2

0

0

3

1/3

0/3

0/3

2/3

0/3

0/3

3/3

1001

1000

1000

1002

1000

1000

26003

cs6501 natural language processing

1/26

1/26

1/26

1/26

1/26

1/26

1

26

add-lambda smoothing

    a large dictionary makes novel events too probable.

    to fix: instead of adding 1 to all counts, add     = 0.01?

    this gives much less id203 to novel events.

    but how to pick best value for    ?  
    that is, how much should we smooth?
    e.g., how much id203 to    set aside    for novel events?

    depends on how likely novel events really are!
    which may depend on the type of text, size of training corpus,    

    can we figure it out from the data?

    we   ll look at a few methods for deciding how much to smooth.

cs6501 natural language processing

27

setting smoothing parameters

    how to pick best value for    ?  (in add-       smoothing)
    try many     values & report the one that gets best 

results?

training

test

    how to measure whether a particular     gets good 

results?

    is it fair to measure that on test data (for setting    )?

    moral: selective reporting on test data can make a 
method look artificially good.  so it is unethical.   

    rule: test data cannot influence system 

development.  no peeking!  use it only to evaluate 
the final system(s). report all results on it.

cs6501 natural language processing

28

setting smoothing parameters

    how to pick best value for    ?  (in add-       smoothing)
    try many     values & report the one that gets best 

results?

    how to measure whether a particular     gets good 

feynman   s advice:    the 
training
first principle is that you 
must not fool yourself, and 
you are the easiest person 

test

results?

    is it fair to measure that on test data (for setting    )?

to fool.   

    moral: selective reporting on test data can make a 
method look artificially good.  so it is unethical.   

    rule: test data cannot influence system 

development.  no peeking!  use it only to evaluate 
the final system(s). report all results on it.

cs6501 natural language processing

29

setting smoothing parameters

    how to pick best value for    ?   

    try many     values & report the one that gets best 

results?

training

test

dev.

training

pick     that
gets best 
results on 
this 20%     

    when we collect counts 
from this 80% and smooth 
them using add-    smoothing.

now use that 
    to get 
smoothed 
counts from 
all 100%    

    and 
report 
results of 
that final 
model on 
test data.

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

30

large or small dev set?

    here we held out 20% of our training set 

(yellow) for development.

    would like to use > 20% yellow:

    20% not enough to reliably assess    

    would like to use > 80% blue:

   best     for smoothing 80%     best     for 

smoothing 100%

cs6501 natural language processing

31

cross-validation

    try 5 training/dev splits as below

    pick     that gets best average performance

dev.

dev.

dev.

test

dev.

dev.

        tests on all 100% as yellow, so we can more 

reliably assess    

        still picks a     that   s good at smoothing the 

80% size, not 100%.

    but now we can grow that 80% without trouble 

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

32

32

n-fold cross-validation (   leave one out   )

   

test

    test each sentence with smoothed model from 

other n-1 sentences

        still tests on all 100% as yellow, so we can 

reliably assess    

        trains on nearly 100% blue data ((n-1)/n) to 

measure whether     is good for smoothing that

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

33

33

n-fold cross-validation (   leave one out   )

   

test

        surprisingly fast: why?

    usually easy to retrain on blue by 

adding/subtracting 1 sentence   s counts

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

34

34

more ideas for smoothing

    remember, we   re trying to decide how 

much to smooth.
    e.g., how much id203 to    set aside    for 

novel events?

    depends on how likely novel events really 

are

    which may depend on the type of text, size 

of training corpus,    

    can we figure this out from the data?

cs6501 natural language processing

35

how likely are novel events? 

20000 types 300 tokens

a

both

candy

donuts

every

???

grapes

his

ice cream

150

18

0

0

50

0

0

38

0

versus

0/300

300 tokens
0

0

1

2

0

0

1

0

7

0/300

   

which zero would you expect is really rare?

cs6501 natural language processing

36

how likely are novel events?

20000 types 300 tokens

a

both

candy

donuts

every

farina

grapes

his

ice cream

150

18

0

0

50

0

0

38

0

determiners: a closed class

   

versus

300 tokens
0

0

1

2

0

0

1

0

7

cs6501 natural language processing

37

how likely are novel events? 

20000 types 300 tokens

150

18

0

0

50

0

0

38

0

versus

300 tokens
0

0

1

2

0

0

1

0

7

a

both

candy

donuts

every

farina

grapes

his

ice cream

   

(food) nouns: an open class

cs6501 natural language processing

38

zipfs    law

the r-th most
common word          has
p(        )     1/r

a few words 

are very 
frequent 

http://wugology.com/zipfs-law/

cs6501 natural language processing

39

cs6501 natural language processing

40

cs6501 natural language processing

41

how common are novel events?

1 *

1 *

n6 *

n5 *

n4 *

n3 *

n2 *

n1 *

n0 *

the

eos

abdomen, bachelor, caesar    

aberrant, backlog, cabinets    

abdominal, bach, cabana    

abbas, babel, cabot    

aback, babbitt, cabanas    

abaringe, babatinde, cabaret    

cs6501 natural language processing

42

0100002000030000400005000060000700008000001234565210869836how common are novel events?

counts from brown corpus (n     1 million tokens)

n6 *

n5 *

n4 *

n3 *

n2 *

n1 *

n0 *

doubletons (occur twice) 

singletons (occur once) 

novel words (in dictionary but never occur) 

n2       = # doubleton types
n2 * 2 = # doubleton tokens

   r nr
   r (nr * r) = total # tokens = n (all bars)

= total # types   = t (purple bars)

cs6501 natural language processing

43

05000100001500020000250000123456witten-bell smoothing idea

n6 *

n5 *

n4 *

n3 *

n2 *

n1 *

n0 *

if t/n is large, we   ve seen lots of novel types 
in the past, so we expect lots more.

doubletons

singletons

novel words 

unsmoothed     smoothed

2/n     2/(n+t)

1/n     1/(n+t)

0/n     (t/(n+t)) / n0

cs6501 natural language processing

44

05000100001500020000250000123456good-turing smoothing idea

n6*

n5*

n4*

n3*

n2*

n1*

n0*

partition the type vocabulary into classes 

(novel, singletons, doubletons,    ) 
by how often they occurred in training data

use observed total id203 of class r+1 

to estimate total id203 of class r

obs. (tripleton) 

obs. p(doubleton) 

obs. p(singleton)

1.2%

1.5%

2%

unsmoothed     smoothed

est. p(doubleton)

2/n     (n3*3/n)/n2
(n3*3/n)/n2

est. p(singleton)

est. p(novel)

1/n     (n2*2/n)/n1
(n2*2/n)/n1

0/n     (n1*1/n)/n0
(n1*1/n)/n0

cs6501 natural language processing

r/n = (nr*r/n)/nr

    (nr+1*(r+1)/n)/nr
45

00.0050.010.0150.020.0250/n1/n2/n3/n4/n5/n6/nwitten-bell vs. good-turing

    estimate p(z | xy) using just the tokens we   ve 

seen in context xy.  might be a small set    

    witten-bell intuition: if those tokens were 

distributed over many different types, then novel 
types are likely in future.  

    good-turing intuition: if many of those tokens 

came from singleton types , then novel types are 
likely in future.
    very nice idea (but a bit tricky in practice)

    see the paper    good-turing smoothing without tears   

cs6501 natural language processing

46

good-turing reweighting

    problem 1: what about    the   ? (k=4417)

   for small k, nk > nk+1
   for large k, too jumpy. 

    problem 2: we don   t observe

events for every k

n1

n2

n3

.
 
.
 
.
 
.

n3511
n4417

n0

n1

n2

.
 
.
 
.
 
.

n3510
n4416

good-turing reweighting

   simple good-turing [gale and sampson]: 

replace empirical nk with a best-fit 
regression (e.g., power law) once count 
counts get unreliable

f(k) = a + b log (k)

find a,b such that f(k)     nk

n1

n2 n3

n1

n2

backoff and interpolation 

    why are we treating all novel events as the 

same?

words

words

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

49

49

backoff and interpolation 

    p(zombie | see the) vs. p(baby | see the)

   what if count(see the zygote) = 

count(see the baby) = 0?

   baby beats zygote as a unigram

   the baby beats the zygote as a bigram

       see the baby beats see the zygote ?  

(even if both have the same count, such as 0)

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

50

50

backoff and interpolation 

   condition on less context for 

contexts you haven   t learned much 
about 

   backoff: use trigram if you have 

good evidence, otherwise bigram, 
otherwise unigram 

   interpolation:     mixture of unigram, 

bigram, trigram (etc.) models

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

51

51

smoothing + backoff

   basic smoothing (e.g., add-   , good-turing, 

witten-bell):
    holds out some id203 mass for novel 

events

    divided up evenly among the novel events

   backoff smoothing

    holds out same amount of id203 mass for 

novel events

    but divide up unevenly in proportion to backoff

prob.

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

52

52

smoothing + backoff

    when defining p(z | xy), the backoff prob

for novel z is p(z | y)

   even if z was never observed after xy, it 

may have been observed after y (why?).   

   then p(z | y) can be estimated without 

further backoff.  if not, we back off further to 

p(z).

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

53

53

linear interpolation 

    jelinek-mercer smoothing

    use a weighted average of backed-off na  ve 

models: 
paverage(z | xy) =    3 p(z | xy) +    2 p(z | y) +    1 p(z)
where    3 +    2 +    1 = 1 and all are    0

    the weights     can depend on the context xy
    e.g., we can make     3 large if  count(xy) is high
    learn the weights on held-out data w/ jackknifing

    different    3 when xy is observed 1 time, 2 times,

5 times,    

600.465 - intro to nlp - j. eisner

cs6501 natural language processing

54

54

absolute discounting interpolation

    save ourselves some time and just 

subtract 0.75 (or some d)!

discounted bigram

interpolation weight

    but should we really just use the regular 

unigram

unigram p(w)?

cs6501 natural language processing

55

)()()(),()|(1111scountingabsolutediwpwwcdwwcwwpiiiiii                        absolute discounting: just subtract a little 
from each count

    how much to subtract ?
    church and gale (1991)   s clever idea

    divide data into training and held-out sets
    for each bigram in the training set
    see the actual count in the held-out set!

bigram 
count in 
training
0
1
2
3
4
5
6

bigram count 
in heldout set

.0000270
0.448
1.25
2.24
3.23
4.21
5.23

56

    it sure looks like c* = (c - .75)

cs6501 natural language processing

