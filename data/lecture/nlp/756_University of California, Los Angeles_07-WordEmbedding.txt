lecture 7: word 

embeddings

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

6501 natural language processing

1

this lecture

v learning word vectors (cont.)

v representation  learning in nlp

6501 natural language processing

2

recap: latent semantic analysis

v data representation

v encode single-relational data in a matrix

v co-occurrence (e.g., from a general corpus)
v synonyms (e.g., from a thesaurus)

v factorization

v apply svd to the matrix to find latent 

components

v measuring degree of relation

v cosine of latent vectors

       

recap: mapping to latent space via svd

    
          

    
          

   
    
          
v term vectors projected to     -dim latent space
cosine of two column vectors in         $

v svd generalizes the original data

v word similarity: 

v uncovers relationships not explicit in the thesaurus

          

low rank approximation

v frobenius norm. c is a            matrix

6
||    ||/= 11|    34|5
478

9
378

v rank of a matrix.

v how many vectors in the matrix are 

independent  to each other

6501 natural language processing

5

low rank approximation

v low rank approximation problem:

min= ||           ||/				    .    .	                     =    

v if i can only use k independent  vectors to describe 
the points in the space, what are the best choices?

essentially,	we	minimize	the	   reconstruction	loss   	under	a	low	rank	constraint

6501 natural language processing

6

low rank approximation

v low rank approximation problem:

min= ||           ||/				    .    .	                     =    

v if i can only use k independent  vectors to describe 
the points in the space, what are the best choices?

essentially,	we	minimize	the	   reconstruction	loss   	under	a	low	rank	constraint

6501 natural language processing

7

low rank approximation

v assume rank of      is r
v svd:     =             ,  =diag(    8,    5       p,0,0,0,   0)
  =     8 0 0
     non-zeros
0 0	
0     0
0
v zero-out the r        trailing values
     =diag(    8,    5       u,0,0,0,   0)
v    v=u  v        is the best k-rank approximation: 
cv=            	min= ||           ||/				    .    .	                     =    

6501 natural language processing

8

id97

v lsa: a compact representation of co-

occurrence matrix

v id97:predict surrounding words (skip-gram)
v similar to using co-occurrence counts levy&goldberg

(2014), pennington et al. (2014)

v easy to incorporate new words

or sentences

6501 natural language processing

9

id97

v similar to language model, but predicting next 

word is not the goal.

v idea: words that are semantically similar often 

occur near each other in text
v embeddings that are good at predicting neighboring 

words are also good at representing similarity

6501 natural language processing

10

skip-gram v.s continuous bag-of-words

v what differences?

6501 natural language processing

11

skip-gram v.s continuous bag-of-words

6501 natural language processing

12

objective of id97 (skip-gram)

    \]9,    \]9^8,   ,    \]8,    \^8,    \^5,   ,    \^9
given word     \

v maximize the log likelihood of context word 

v m is usually 5~10

6501 natural language processing

13

objective of id97 (skip-gram)

v how to model log	    (    \^4|    \)?
    	    \^4    \ = cde	(fghij	   	lgh)
		
    cde	(fgn	   	lgh)
gn
v    p : when      is the center word
v    p: when      is the outside word (context word)

v every word has 2 vectors

v softmax function again! 

6501 natural language processing

14

how to update?

    	    \^4    \ = cde	(fghij	   	lgh)
		
    cde	(fgn	   	lgh)
gn
v how to minimize     (    )

v id119!
v how to compute the gradient?

6501 natural language processing

15

recap: calculus 

vgradient:        =     8
    5     z,
        (    )
        8        (    )
            =
        5        (    )
        z
            =    

v         =           	 (or represented as            )

6501 natural language processing

16

recap: calculus 

vif		    =         and	    =         (i.e,.	    =    (        )
(cid:131)(cid:132)(cid:131)(cid:133)=(cid:131)(cid:134)(f)(cid:131)f (cid:131)(cid:135)((cid:133))(cid:131)(cid:133)
( (cid:131)(cid:132)(cid:131)f(cid:131)f(cid:131)(cid:133) ) 
2. y=ln	(    5+5)
1.     =     (cid:136)+6 z
3. y=exp(x(cid:143)+3    +2)

6501 natural language processing

17

other useful formulation

v    =exp    
vy=logx

dydx=expx
dydx=1x

when	i	say	log		(in	this	course),	 usually	i	mean	ln

6501 natural language processing

18

6501 natural language processing

19

example

v assume vocabulary set is     . we have one 
center word     , and one context word     .
v what is the id155             
             = exp	(    (cid:149)	   	    (cid:150))
    exp	(    pn	   	    (cid:150))
	
pv
w.r.t    (cid:150)?    log            
=    (cid:149)       p   (cid:153)        [    p]
        (cid:150)

v what is the gradient of the log likelihood 

6501 natural language processing

20

id119minp     (    )
update w: 			                         (    )

6501 natural language processing

21

local minimum v.s. global minimum 

6501 natural language processing

22

v id119 update rule:

stochastic id119

v let          =86   
	    4(    )
6478
              (cid:158)6   
        4    
6478
v approximate 86   
        4    
6478
single example         3     (why?)
randomly	pick	an	example	    
                          3    

v stochastic id119: 

v at each step:

by the gradient at a 

6501 natural language processing

23

negative sampling

v with a large vocabulary set, stochastic 

id119 is still not enough (why?)

=    (cid:149)       p   (cid:153)        [    p]

    log            
        (cid:150)

vlet   s approximate it again!

vonly sample a few words that do not appear 

in the context

vessentially, put more weights on positive 

samples

6501 natural language processing

24

more about id97     relation to lsa

v lsa factorizes a matrix of co-occurrence 

counts

v (levy and goldberg 2014) proves that 
skip-gram model implicitly factorizes a 
(shifted) pmi matrix!

v pmi(w,c) =log  ((cid:132)|(cid:133))	

  ((cid:132)) =log   ((cid:133),(cid:132))	
=log#    ,     	   	|    |	
(cid:153)((cid:133))  ((cid:132))
#(    )#(    )

6501 natural language processing

25

all problem solved?

6501 natural language processing

26

continuous semantic representations

sunny

rainy

cloudy

windy

cab

car

wheel

emotion

sad

joy

feeling

6501 natural language processing

27

semantics needs more than similarity

tomorrow will 

be sunny.

tomorrow will 

be rainy.

                            (rainy, sunny)?
                            (rainy, sunny)?

6501 natural language processing

28

polarity inducing lsa [yih, zweig, platt  2012]

v data representation

v encode two opposite relations in a matrix using 

   polarity   
v synonyms & antonyms (e.g., from a thesaurus)

v factorization

v apply svd to the matrix to find latent 

components

v measuring degree of relation

v cosine of latent vectors

encode synonyms & antonyms in matrix

v joyfulness: joy, gladden; sorrow, sadden
v sad: sorrow, sadden; joy, gladden

target word: row-

vector

inducing polarity

group 1:	   joyfulness   

group	2:	   sad   

group	3:	   affection   

joy

1

-1

gladden

sorrow

sadden

goodwill

1

-1

-1

0

0

1

1

-1

1

0

0

0

0

cosine score: +	                                

encode synonyms & antonyms in matrix

v joyfulness: joy, gladden; sorrow, sadden
v sad: sorrow, sadden; joy, gladden

target word: row-

vector

inducing polarity

group 1:	   joyfulness   

group	2:	   sad   

group	3:	   affection   

joy

1

-1

gladden

sorrow

sadden

goodwill

1

-1

-1

0

0

1

1

-1

1

0

0

0

0

cosine score:    	                                

continuous representations for entities

republic	party

democratic	party

?

george	w	bush

laura	bush

michelle	obama

6501 natural language processing

32

continuous representations for entities

    useful resources for nlp applications
    id29 & id53 
    information extraction

6501 natural language processing

33

