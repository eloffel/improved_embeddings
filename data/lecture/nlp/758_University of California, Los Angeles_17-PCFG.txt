lecture 17: 

statistical parsing with pid18

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501-nlp

1

reading list

v look at mike collins    note on pid18s  and 

lexicalized pid18
http://www.cs.columbia.edu/~mcollins/

cs6501-nlp

2

phrase structure (constituency) trees

v can be modeled by context-free grammars

cs6501-nlp

3

cky algorithm

  for j := 1 to n

  add to [j-1,j] all categories for the jth word

  for width := 2 to n

  for start := 0 to n-width                  // this is i
  define end := start + width        // this is j
  for mid := start+1 to end-1        // find all i-to-j phrases

  for every rule x    y z in the grammar
if y in [start,mid] and z in [mid,end]

then add x to [start,end] 

cs6501-nlp

4

weighted cky: viterbi algorithm

    initialize all entries of chart to    
    for i := 1 to n

    for each rule r of the form x    word[i]

    chart[x,i-1,i] max ( weight(r) )

    for width := 2 to n

    for start := 0 to n-width

    define end := start + width
    for mid := start+1 to end-1

assume	the	weights	
are	log	probabilities	
of	rules	

    for each rule r of the form x    y z 
   

chart[x,start,end] = max( weight(r) +

chart[y,start,mid] + chart[z,mid,end])

    return chart[root,0,n]

cs6501-nlp
slides	are	modified	 from	jason	eisner   s	nlp	course

5

likelihood of a parse tree

why??

cs6501-nlp

6

probabilistic trees

v just like language models or id48 for id52
v we make independent assumptions!

np
time
vp
flies

s

p
like

vp

pp

np

det
an

n   
arrow

cs6501-nlp

7

chain rule: one word at a time

p(time flies like an arrow) 

= p(time)

* p(flies | time)
* p(like | time flies)
* p(an | time flies like)
* p(arrow | time flies like an)

cs6501-nlp

8

chain rule + indep. assumptions 
(to get trigram model)

p(time flies like an arrow) 

= p(time)

* p(flies | time)
* p(like | time flies)
* p(an | time flies like)
* p(arrow | time flies like an)

cs6501-nlp

9

chain rule     written differently

p(time flies like an arrow) 

= p(time)

* p(time flies | time)
* p(time flies like | time flies)
* p(time flies like an | time flies like)
* p(time flies like an arrow | time flies like an)

proof:p(x,y | x) = p(x | x) * p(y | x, x) = 1 * p(y | x)

cs6501-nlp

10

chain rule + indep. assumptions 

p(time flies like an arrow) 

= p(time)

* p(time flies | time)
* p(time flies like | time flies)
* p(time flies like an | time flies like)
* p(time flies like an arrow | time flies like an)

proof:p(x,y | x) = p(x | x) * p(y | x, x) = 1 * p(y | x)

cs6501-nlp

11

chain rule: one node at a time

s

p(time)

| s) = p( s
np vp

| s) * p( s
np
time

|

vp

s

np vp

)

p(

vp

np
time
vp
flies
p
like

pp

np

det
an

n   
arrow

s

* p(
np
time
vp
s

|

vp

pp
|

vp

* p(
np
time
pp
vp
cs6501-nlp
flies

s
p(flies,	time|time)

)

vp

np
time

s

np
time
vp

) *    

vp

pp

12

chain rule + indep. assumptions 

s

| s) = p( s
np vp

| s) * p( s
np
time

|

vp

s

np vp

)

p(

vp

np
time
vp
flies
p
like

pp

np

det
an

n   
arrow

s

* p(
np
time
vp
s

|

vp

pp
|

vp

* p(
np
time
pp
vp
cs6501-nlp
flies

)

s

vp

np
time

s

np
time
vp

) *    

vp

pp

13

simplified notation 

s

p(

vp

np
time
vp
flies
p
like

pp

np

det
an

n   
arrow

| s) = p(s     np vp | s) * p(np     time | np)

* p(vp     vp np | vp) 

* p(vp     flies | vp) *    

cs6501-nlp

14

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501-nlp

15

three basic problems for id48s

phrase	structure	trees

v likelihood of the input:

v inside algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (parsing) the input:

v cky algorithm

v estimation (learning):

v find the best model parameters

parse	tree	of	   i	love	cat   

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vinside-outside algorithm

cs6501-nlp

16

three basic problems for id48s

phrase	structure	trees

v likelihood of the input:

v inside algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (parsing) the input:

v cky algorithm

v estimation (learning):

v find the best model parameters

parse	tree	of	   i	love	cat   

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vinside-outside algorithm

cs6501-nlp

17

three basic problems for id48s

phrase	structure	trees

v likelihood of the input:

v inside algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (parsing) the input:

v cky algorithm

v estimation (learning):

v find the best model parameters

parse	tree	of	   i	love	cat   

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vinside-outside algorithm

cs6501-nlp

18

probabilistic cky: inside algorithm

    initialize all entries of chart to 0
    for i := 1 to n

    for each rule r of the form x    word[i]

    chart[x,i-1,i] += prob(r)

    for width := 2 to n

    for start := 0 to n-width

    define end := start + width
    for mid := start+1 to end-1

    for each rule r of the form x    y z 
   

chart[x,start,end] += prob(r) *

chart[y,start,mid] * chart[z,mid,end]

    return chart[root,0,n]

600.465 - intro to nlp - j. eisner

19

how to build a width-6 phrase

?

s    np vp
np    det n
np    np pp
vp    v np
vp    vp pp
pp    p np
1	7 =

1	2	+	2	7		

1	3	+	3	7

1	4	+	4	7		

1	5	+	5	7

1	6	+	6	7

cs6501: nlp

20

cky: recognition algorithm

v initialize all entries of chart to false
v for i := 1 to n

v for each rule r of the form x    word[i]

v chart[x,i-1,i] |= in_grammar(r)

v for width := 2 to n

v for start := 0 to n-width

v define end := start + width
v for mid := start+1 to end-1

pay attention to the 
orange code    

v for each rule r of the form x    y z 
v chart[x,start,end]  |= in_grammar(r)  &
chart[y,start,mid] & chart[z,mid,end]

v return chart[root,0,n]

600.465 - intro to nlp - j. eisner

21

weighted cky: viterbi algorithm 
(min-cost)
v initialize all entries of chart to    
v for i := 1 to n

v for each rule r of the form x    word[i]

v chart[x,i-1,i] min= weight(r)

v for width := 2 to n

v for start := 0 to n-width

v define end := start + width
v for mid := start+1 to end-1

pay attention to the 
orange code    

v for each rule r of the form x    y z 
v chart[x,start,end]  min= weight(r) +

chart[y,start,mid] + chart[z,mid,end]

v return chart[root,0,n]

600.465 - intro to nlp - j. eisner

22

weighted cky: viterbi algorithm 
(max-prob)
v initialize all entries of chart to 0
v for i := 1 to n

v for each rule r of the form x    word[i]

v chart[x,i-1,i] max= weight(r)

v for width := 2 to n

v for start := 0 to n-width

v define end := start + width
v for mid := start+1 to end-1

pay attention to the 
orange code    

v for each rule r of the form x    y z 
v chart[x,start,end]  max= weight(r) *

chart[y,start,mid] * chart[z,mid,end]

v return chart[root,0,n]

600.465 - intro to nlp - j. eisner

23

weighted cky: viterbi algorithm 
(max-logprob)
v initialize all entries of chart to -   
v for i := 1 to n

v for each rule r of the form x    word[i]

v chart[x,i-1,i] max= weight(r)

v for width := 2 to n

v for start := 0 to n-width

v define end := start + width
v for mid := start+1 to end-1

pay attention to the 
orange code    

v for each rule r of the form x    y z 
v chart[x,start,end]  max= weight(r) +

chart[y,start,mid] + chart[z,mid,end]

v return chart[root,0,n]

600.465 - intro to nlp - j. eisner

24

probabilistic cky: inside algorithm

    initialize all entries of chart to 0
    for i := 1 to n

    for each rule r of the form x    word[i]

    chart[x,i-1,i] += prob(r)

    for width := 2 to n

    for start := 0 to n-width

    define end := start + width
    for mid := start+1 to end-1

    for each rule r of the form x    y z 
   

chart[x,start,end] += prob(r) *

chart[y,start,mid] * chart[z,mid,end]

    return chart[root,0,n]

600.465 - intro to nlp - j. eisner

25

semiring-weighted cky: general algorithm!

    initialize all entries of chart to   
    for i := 1 to n

    for each rule r of the form x    word[i]
    chart[x,i-1,i]    = semiring_weight(r)

    for width := 2 to n

    is like    and   /   : 
combines all of several 
pieces into an x
    is like    or   /   : 
considers the 
alternative ways to 
build the x

    for start := 0 to n-width

    define end := start + width
    for mid := start+1 to end-1

    for each rule r of the form x    y z 
   

chart[x,start,end]    = semiring_weight(r)     

chart[y,start,mid]     chart[z,mid,end]

    return chart[root,0,n]

600.465 - intro to nlp - j. eisner

26

semiring-weighted cky: general algorithm!

   initialize all entries of chart to   
   for i := 1 to n

   for each rule r of the form x    word[i]
   chart[x,i-1,i]    = semiring_weight(r)

   for width := 2 to n

?

   for start := 0 to n-width

   define end := start + width
   for mid := start+1 to end-1

   for each rule r of the form x    y z 
  

chart[x,start,end]    = semiring_weight(r)    

chart[y,start,mid]     chart[z,mid,end]

   return chart[root,0,n]
600.465 - intro to nlp - j. eisner

27

weighted cky, general version

   initialize all entries of chart to   
   for i := 1 to n

the designs of the dynamic 
algorithms are mostly based on the 
structure

   for each rule r of the form x    word[i]
   chart[x,i-1,i]    = semiring_weight(r)
  
   
0
  
+
   
and false

   for width := 2 to n
total prob (inside)
min weight
recognizer
   for each rule r of the form x    y z 
  

virtual function
weights
   for start := 0 to n-width
[0,1]
[0,   ]
{true,false}

   
+
   define end := start + width
min
   for mid := start+1 to end-1
or

chart[x,start,end]    = semiring_weight(r)    

chart[y,start,mid]     chart[z,mid,end]

   return chart[root,0,n]
600.465 - intro to nlp - j. eisner

28

can we write a general version of 
viterbi/forward algorithms

yes, we can!

you can implement this using 
virtual function!

cs6501-nlp

29

three basic problems for id48s

phrase	structure	trees

v likelihood of the input:

v inside algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (parsing) the input:

v cky algorithm

v estimation (learning):

v find the best model parameters

parse	tree	of	   i	love	cat   

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vinside-outside algorithm

cs6501-nlp

30

id113 

v has a closed-form solution
v count how many times the rules are 

applied. then, normalize. 

em	can	be	applied	for	unsupervised	learning	setting!

cs6501-nlp

31

evaluation 

v precision: how many of the predicted 

constituents are correct?

v recall: how many of the correct 

constituents were predicted?

v usually use f1 score to combine the two

cs6501-nlp

32

cs6501-nlp

33

the id32

v wall street journal (50,000 sents, 1m words), 

also contains brown corpus and others

v the annotation:

v pos-tagged 
v manually annotated with phrase-structure trees

v standard data for english parsers

cs6501-nlp

34

treebank label set

v 48 preterminals (tags)

v 36 pos tags + 12 symbols

v 14 nonterminals:
v s, np ,pp, vp, ...

v function tags
v -sbj:    subject   
v -mnr:    manner   

cs6501-nlp

35

cs6501-nlp

36

id18 in the id32

v many rules appear only once
v many rules are similar

cs6501-nlp

37

pid18  (charniak 1996)

v train (300k words), test (30k words)

cs6501-nlp

38

ideas to improve pid18

v change the grammar by adding the type of 

the parent node to each nonterminal 
[johnson    98, klein&manning    03]

cs6501-nlp

39

ideas to improve pid18

v lexicalization [collins 99   , charniak 00   ]
v assume each constituent has one head 

word. take head word into account!

cs6501-nlp

40

price of lexicalization

v # rules increases
v difficult to get good estimates of 

probabilities

cs6501-nlp

41

dealing with unknown words

v training: replace some rare words in the 

training data with a token    unknown   

v test: replace unseen words with 

   unknown   

cs6501-nlp

42

using latent variables

v assume there are different types of np, vp, 

etc   

lexicalization

latent	variables

structure	annotation

cs6501-nlp

43

ideas to improve pid18    reranking

v [charniak&johnson 2005]
v find the top k solutions based on a 

generative model

v use a discriminative model to find the best 

solution among the k solutions

cs6501-nlp

44

some results

cs6501-nlp

45

what we have learned so far

cs6501-nlp

46

go beyond the keyword matching

v identify the structure and meaning of 

words, sentences, texts and conversations

v deep understanding of broad language
v nlp is all around us

cs6501    natural language processing

47

what will you learn from this course

v the nlp pipeline
v key components for
understanding  text

v nlp applications

v current techniques  & limitation

v build realistic nlp tools

cs6501    natural language processing

48

nlp pipeline

v word/token level:

morphology, id27, word 
id91, language models, id52

v sentence level:

chunking, ner, parsing

v we will see how to use these

tools for nlp applications

cs6501-nlp

49

