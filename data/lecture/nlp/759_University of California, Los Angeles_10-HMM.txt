lecture 9: 

hidden markov model

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501 natural language processing

1

this lecture

v hidden markov model
v different views of id48
v id48 in supervised learning setting

cs6501 natural language processing

2

recap: parts of speech

v traditional parts of speech

v ~ 8 of them

cs6501 natural language processing

3

recap: tagset

v id32 tagset   , 45 tags: 

v prp$, wrb, wp$, vbg
v penn pos annotations:

the/dt grand/jj jury/nn commmented/vbd on/in 
a/dt number/nn of/in other/jj topics/nns ./.

v universal tag set, 12 tags

v noun, verb, adj, adv, pron, det, adp, num, 

conj, prt,    .   , x

cs6501 natural language processing

4

recap: id52 v.s. word id91

v words often have more than one pos: 

back
v the back door = jj
v on my back = nn
v win the voters back = rb
v promised to back the bill = vb

v syntax v.s. semantics (details later)

cs6501 natural language processing

5

these examples from dekang lin

recap: pos tag sequences

v some tag sequences more likely occur 

than others 

v pos ngram view

https://books.google.com/ngrams/graph?co
ntent=_adj_+_noun_%2c_adv_+_nou
n_%2c+_adv_+_verb_

existing	methods	often	model	pos	tagging	as	a	
sequence	tagging problem

cs6501 natural language processing

6

evaluation

v how many words in the unseen test data 

can be tagged correctly?

v usually evaluated on id32

v state of the art ~97% 
v trivial baseline (most likely tag) ~94%
v human performance ~97%

cs6501 natural language processing

7

building a pos tagger

v supervised learning

v assume linguistics have annotated  several 

examples 

tag set:
dt,	jj,	nn,	

vbd   

pos	tagger

the/dt grand/jj	jury/nn commented/vbd
on/in	a/dt number/nn of/in other/jj
topics/nns ./.

cs6501 natural language processing

8

pos induction

v unsupervised learning

v assume we only have an unannotated  corpus 

pos	tagger

tag set:
dt,	jj,	nn,	

vbd   

the	grand jury	
commented	on a	
number	of	other	topics	.

cs6501 natural language processing

9

today: hidden markov model

v we focus on supervised learning setting 
v what is the most likely sequence of tags 

for the given sequence of words w

v we will talk about other ml models for this 

type of prediction tasks later.

cs6501 natural language processing

10

let   s try

don   t	worry!	there	is	no	problem	
with	your	eyes	or	computer.

a/dt d6g/nn 0s/vbz chas05g/vbg a/dt
cat/nn ./.
a/dt f6x/nn 0s/vbz 9u5505g/vbg ./.
a/dt b6y/nn 0s/vbz s05g05g/vbg ./.
a/dt ha77y/jj b09d/nn

what is the pos tag sequence of the following sentence?
a ha77y cat was s05g05g .

cs6501 natural language processing

11

let   s try

v a/dt d6g/nn 0s/vbz chas05g/vbg a/dt

cat/nn ./.
a/dt dog/nn is/vbz chasing/vbg a/dt cat/nn ./.
v a/dt f6x/nn 0s/vbz 9u5505g/vbg ./.

a/dt fox/nn is/vbz running/vbg ./. 

v a/dt b6y/nn 0s/vbz s05g05g/vbg ./.

a/dt boy/nn is/vbz singing/vbg ./.

v a/dt ha77y/jj b09d/nn

a/dt happy/jj bird/nn

v a ha77y cat was s05g05g .

a happy cat was singing . 

cs6501 natural language processing

12

how you predict the tags?

v two types of information are useful

v relations between words and tags
v relations between tags and tags

v dt nn, dt jj nn   

cs6501 natural language processing

13

statistical id52

v what is the most likely sequence of tags 

for the given sequence of words w

p(	dt	jj	nn	|	a	smart	dog)	= p(dd	jj	nn	a	smart	dog)	/	p	(a	smart	dog)

    p(dd	jj	nn	a	smart	dog)

= p(dd	jj	nn)	p(a	smart	dog	|	dd	jj	nn	)

cs6501 natural language processing

14

transition id203

v joint id203     (    ,    ) =             (    |    )
v         =         +,    ,,       .
		=        +        ,       +        1       ,,    +             .     +       .2+
   pt+p t,     +          1     ,        (    .       .2+)
	=  78+.         7       72+

markov	assumption	

v bigram model over pos tags!

(similarly, we can define a id165 model over pos 
tags, usually we called high-order id48)

cs6501 natural language processing

15

emission id203

v joint id203     (    ,    ) =             (    |    )
vassume	words	only	depend	on	their	pos-tag
    ,        (    .       .)
v                         +
    +          ,
																				=  78+.         7    7

independent	assumption	

i.e., p(a smart dog | dd jj nn )

= p(a | dd) p(smart | jj ) p( dog | nn )

cs6501 natural language processing

16

put them together

v joint id203     (    ,    ) =             (    |    )
v        ,    =pt+pt,
    .2+
    ,             .
    +          1
    ,        (    .       .)
    +          ,
				        +
							=  78+.         7    7        7       72+

e.g., p(a smart dog , dd jj nn )

= p(a | dd) p(smart | jj ) p( dog | nn )

p(dd | start) p(jj | dd) p(nn | jj )

cs6501 natural language processing

17

put them together

v two independent assumptions

v approximate p(t) by a bi(or n)-gram model 
v assume each word depends only on its postag

    (    +)

initial	id203

cs6501 natural language processing

18

id48s as probabilistic fsa

julia	hockenmaier:	intro	to	nlp

cs6501 natural language processing

19

table representation

let	    ={    ,    ,    } represents	all	parameters

cs6501 natural language processing

20

id48 (formal)

v each observation is a symbol from a vocabulary v = 

v states t = t1, t2   tn; 
v observations w= w1, w2   wn; 

{v1,v2,   vv}

v transition probabilities

v observation likelihoods

v transition id203 matrix a = {aij}

    72+=     		1       ,           

    7v=         7=    
    7(    )=         7=    _
    7=    
    7=        +=    		1              	

v output id203 matrix b={bi(k)}

v special initial id203 vector   

cs6501 natural language processing

21

how to build a second-order id48?

v second-order id48

v trigram model over pos tags

v         =  78+.         7       72+,    72,
v        ,     =  78+.         7       72+,    72,    (    7       7)

cs6501 natural language processing

22

probabilistic fsa for second-order id48

julia	hockenmaier:	intro	to	nlp

cs6501 natural language processing

23

prediction in generative model

v id136: what is the most likely 

sequence of tags for the given sequence of 
words w

    (    +)

initial	id203

v what are the latent states that most likely 

generate the sequence of word w

cs6501 natural language processing

24

example: the verb    race   

v secretariat/nnp is/vbz expected/vbn to/to 

race/vb tomorrow/nr

v people/nns continue/vb to/to inquire/vb

the/dt reason/nn for/in the/dt race/nn for/in
outer/jj space/nn

v how do we pick the right tag?

cs6501 natural language processing

25

disambiguating    race   

cs6501 natural language processing

26

disambiguating    race   

v p(nn|to) = .00047
v p(vb|to) = .83
v p(race|nn) = .00057
v p(race|vb) = .00012
v p(nr|vb) = .0027
v p(nr|nn) = .0012
v p(vb|to)p(nr|vb)p(race|vb) = .00000027
v p(nn|to)p(nr|nn)p(race|nn)=.00000000032
v so we (correctly) choose the verb reading,

cs6501 natural language processing

27

jason and his ice creams

v you are a climatologist in the year 2799
v studying global warming
v you can   t find any records of the weather in 

baltimore, ma for summer of 2007
v but you find jason eisner   s diary
v which lists how many ice-creams jason 

ate every date that summer

v our job: figure out how hot it was

http://videolectures.net/hltss2010_eisner_plm/
http://www.cs.jhu.edu/~jason/papers/eisner.id48.xls

cs6501 natural language processing

28

(c)old day v.s. (h)ot day

scroll to the bottom to see a graph of what states and transitions the 
model thinks are likely on each day.  those likely states and transitions 
can be used to reestimate the red probabilities (this is the "forward-
backward" or baum-welch algorithm), increasing the likelihood of the 

p(   |start)

#cones

p(1|   )
p(2|   )
p(3|   )
p(c|   )
p(h|   )

p(stop|   )

p(   |c)
0.7
0.2
0.1
0.8
0.1
0.1

p(   |h)
0.1
0.2
0.7
0.1
0.8
0.1

weather states that best explain ice cream consumption

0.5
0.5
0

ice creams
p(h)

3.5

3

2.5

2

1.5

1

0.5

0

1

3

5

7

9

11

13

15

17

19

21

23

25

27

29

31

33

diary day

cs6501 natural language processing

29

three basic problems for id48s

v compute     (           ) for the input      and id48     

v likelihood of the input:

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v find the best tag sequence

                        d	    (           ,    )

pos	tags	of	   i	love	cat   	occurs

v estimation (learning):

v find the best model parameters

how	to	learn	the	model?

v case 1: supervised     tags are annotated
v case 2: unsupervised  -- only unannotated  text 

cs6501 natural language processing

30

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

31

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501 natural language processing

32

learning from labeled data

v let play a game!

v we count how often we see     72+    7 and we    7

then normalize.

cs6501 natural language processing

33

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v find the best model parameters
we	need	dynamic	programming	
v case 1: supervised     tags are annotated
for	the	other	problems
vmaximum likelihood  estimation (id113)
v case 2: unsupervised  -- only unannotated  text
vforward-backward  algorithm

cs6501 natural language processing

34

