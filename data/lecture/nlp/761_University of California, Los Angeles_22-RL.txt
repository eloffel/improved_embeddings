lecture 22: 

representation learning

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501-nlp

1

feature representations

feature

representation

learning
algorithm

color_red

shape_round

has_leaf

   

cs6501-nlp

2

2

feature representation 

v e.g., conditional random field

v p            
   exp(        +    +    .,     +       2    2(    .,    .45,    )
)
+
.
    5
    (    .,    .45,    )
    (    .,    )
    5

    7
    7

    :
    :

    8
    8

edge	feature	

node	feature	

cs6501-nlp

3

feature representation 

v high-order combinations     kernel trick

cs6501-nlp

4

tree kernel

v how to measure the similarity between two 

parse trees?

cs6501-nlp

5

learning representations via nn

v identify high-order combinations
v nn architecture for encoding language 

structures

v learn hierarchical representations

v representations  for token/phrases/sentences

cs6501-nlp

6

how to represent words?

v token, bi-gram, id165 (one-hot featuers)
v id27s
v task-specific id27s

v e.g., for id31

cs6501-nlp

7

how to represent phrases/sentences?

v recursive nn [socher, manning, ng 11]
v many follow-up approaches

cs6501-nlp

8

unsupervised	feature	learning	&	deep	learning	,	andrew	ng

cs6501-nlp

9

auto-encoder and auto-decoder

cs6501-nlp

10

sequence to sequence models
[sutskever, vinyals & le 14]
v have been shown effective in machine 
translation, image captioning and  and 
many structured tasks

cs6501-nlp

11

id170     representation learning 

v nlp problems are structural 

v output variables are inter-correlated
v need joint predictions

v traditional approaches

v graphical model approaches

v e.g., probabilistic id114, structured 

id88

v sequence  of decisions

v e.g., incremental id88, l2s, transition-

based methods

spnlp

12

recent trends

v landscape of methods in deep   structure 

v deep learning/hidden  representation

e.g., id195, id56

v deep features into factors, traditional factor 

graph id136
e.g., lstm+crf, graph transformer networks

v globally optimized transitional-based 

approaches
e.g., beam-search id195, syntaxnet

v    

spnlp

13

