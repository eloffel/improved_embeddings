lecture 13: 

id170

kai-wei chang

cs @ university of virginia

kw@kwchang.net

couse webpage: http://kwchang.net/teaching/nlp16

cs6501: nlp

1

quiz 2

v lectures 9-13

v lecture 12: before page 44
v lecture 13: before page 33

v key points:
v id48 model
v three basic problems
v sequential tagging

cs6501: nlp

2

three basic problems for id48s

v likelihood of the input:

v forward algorithm

how	likely	the	sentence	   i	love	cat   	occurs

v decoding (tagging) the input:

v viterbi algorithm

v estimation (learning):

v find the best model parameters

pos	tags	of	   i	love	cat   	occurs

how	to	learn	the	model?

v case 1: supervised     tags are annotated
vmaximum likelihood  estimation (id113)

v case 2: unsupervised  -- only unannotated  text

vforward-backward  algorithm

cs6501: nlp

3

supervised learning setting

v assume we have annotated examples 

tag set:
dt,	jj,	nn,	

vbd   

pos	tagger

the/dt grand/jj	jury/nn commented/vbd
on/in	a/dt number/nn of/in other/jj
topics/nns ./.

cs6501: nlp

4

sequence tagging problems

v many problems in nlp (ml) have data with 

tag sequences

v brainstorm: 

name other sequential tagging problems

cs6501: nlp

5

ocr example

cs6501: nlp

6

noun phrase (np) chunking

v task: identify all non-recursive np chunks

cs6501: nlp

7

the bio encoding

v define three new tags

v b-np: beginning  of a noun phrase chunk
v i-np: inside of a noun phrase chunk
v o: outside of a noun phrase chunk

pos	tagging	with	a	restricted	tagset?

cs6501: nlp

8

id66

v task: identify all non-recursive np, verb (   vp   ) 

and preposition (   pp   ) chunks

cs6501: nlp

9

bio encoding for id66

v define new tags

v b-np b-vp b-pp: beginning of an    np   ,    vp   ,    pp    chunk
v i-np i-vp i-pp: inside of an    np   ,    vp   ,    pp    chunk
v o: outside of any chunk

pos	tagging	with	a	restricted	tagset?

cs6501: nlp

10

id39

v task: identify all mentions of named entities (people, 

organizations, locations, dates)

cs6501: nlp

11

bio encoding for ner

v define many new tags

v b-pers, b-date,   : beginning of a mention of a 

person/date...

v i-pers, i-date,   : inside of a mention of a person/date...
v o: outside of any mention of a named entity

cs6501: nlp

12

sequence tagging

v many nlp tasks are sequence tagging tasks

v input: a sequence  of tokens/words
v output:  a sequence of corresponding labels

v e.g., pos tags, bio encoding for ner

v       =                            		            

v solution: finding the most probable label 

sequence for the given word sequence

cs6501: nlp

13

sequential tagging v.s
independent prediction
sequence labeling

          =                                        
        is a vector/matrix

independent classifier

       =                        -    (    |    )
        is a single label

ti

wi

tj

wj

yi

xi

yj

xj

cs6501: nlp

14

sequential tagging v.s
independent prediction
sequence labeling

          =                                        
        is a vector/matrix
between both (    ,    )
and (    4,    5)

       =                        -    (    |    )
        is a single label
within (    ,    )

   dependency only 

independent classifiers

   dependency 

   structured output 
   difficult to solve the 
id136 problem

   independent output
   easy to solve the 
id136 problem

cs6501: nlp

15

recap: viterbi decoding 

    7     =	        7       7=    		max>?     7@a    b         7=           7@a=    b

induction:

cs6501 natural language processing

16

recap: viterbi algorithm

v store the best tag sequence for 

    a       4	that ends in     5 in     [    ][    ]
v    [    ][    ]=max	    (    a       4,    a   ,    4=    5	)
     =        4    5            7                 1        5	    7
v        

in the previous column t[j][i-1]

v recursively compute t[j][i] from the entries 

generating	the	current	
observation

the	best	i-1	tag	
sequence

transition	from	the	
previous	best	ending	
tag

cs6501: nlp

17

two modeling perspectives

v model the joint id203 of labels and words

v generative models

v       =                            	            
=                            m    ,    m    
=                            	    (    ,    )
v       =                            	            

v discriminative models

v directly model the id155 of 
often	modeled	by	
softmax function	

labels given the words

cs6501: nlp

18

generative v.s. discriminative models

v binary classification as an example

generative	model   s	view

discriminative	model   s	view

cs6501: nlp

19

generative v.s. discriminative models
generative
    joint distribution

discriminative 
    conditional distribution
    only explain the target 
variable

    full probabilistic 
specification for all the 
random variables

    arbitrary features can be 
incorporated for modeling 

p        

p         and p(    )

    dependence  assumption 
has to be specified for 

    can be used in 
unsupervised  learning

    need labeled data, suitable 
for (semi-) supervised 
learning

cs6501: nlp

20

independent classifiers

4

v             =       (    4|    4)
    o
    o

    a
    a

    p
    p

v~95% accuracy (token-wise)

    q
    q

cs6501: nlp

21

maximum id178 markov models

labels      given the observed input sequence     
v             =       (    4|    4,    4@a)

v memms  are discriminative models of the 

4

cs6501: nlp

22

design features

v emission-like features
v binary feature functions

vb

nnp

v ffirst-letter-capitalized-nnp(china) = 1
v ffirst-letter-capitalized-vb(know) = 0

know

china

v integer (or real-valued) feature functions

v fnumber-of-vowels-nnp(china) = 2

v transition-like features
v binary feature functions

not	necessarily	
independent	 features!

v ffirst-letter-capitalized-vb-nnp(china) = 1

cs6501: nlp

23

specific type of feature function

parameterization of p(    4|    4,    4@a)
v associate a real-valued weight      to each 
v    7 for ffirst-letter-capitalized-nnp(w)
v define a scoring function         4,    4@a,    4 =
        7    7(    4,    4@a,    4)
v naturally p    4    4,    4@a    exp        4,    4@a,    4
7
v    (    )>0	
v        (    )
=1
[

v recall the basic definition of id203

cs6501: nlp

24

4

parameterization of memms

=    abcdef,efgh,if
p         =       (    4|    4,    4@a)
=   exp    (    4,    4@a,    4)
4
    abcde,efgh,if
j
4      exp        ,    4@a,    4
4
e
constant	only	related	to	    
vlog             =       (    4,    4@a,    4)
       (    )
    :	parameters
4
on        (    4,    4@a,    4)
4

v viterbi algorithm can be used to decode the 
most probable label sequence solely based 

v it is a log-linear model

cs6501: nlp

25

parameter estimation (intuition)

v maximum likelihood estimator can be used 

in a similar way as in id48s

v       =                        k    log    (    |    )
	=                        k           (    4,    4@a,    4)
       (    ) 

    ,i

    ,i

4

	

decompose	the	
training	data	into	
such	units	

cs6501: nlp

26

parameter estimation (intuition)

v essentially, training local classifiers using 

previous assigned tags as features

cs6501: nlp

27

more about memms

v emission features can go across multiple 

v        4,    4@a,    4            7    7(    4,    4@a,    )

observations

7

v especially useful for id66 and ner 

tasks

cs6501: nlp

28

label biased problem

v consider the following tag sequences as the 

training data
thomas/b-per jefferson/i-per 
thomas/b-loc hall/i-loc

other

b-per

e-per

b-loc

e-loc

cs6501: nlp

29

label biased problem

v thomas/b-per jefferson/i-per 

thomas/b-loc hall/i-loc

v memm:
p(b-per|thomas,other)=    
p(b-loc|thomas,other)=    
p(i-per|jefferson, b-per)=1
p(i-loc|jefferson, b-loc)=1

should	globally	normalize!

other

b-per

e-per

b-loc

e-loc

cs6501: nlp

30

v model global dependency

conditional random field

v p            exp        ,    
=exp        ,     /    exp
    (    b,    )
    b
    p
    a
    o
    q
    o
    a
    q
    p

score	entire	sequence	directly

cs6501: nlp

31

)

q

    
7

conditional random field

v        ,     =   (        7    7    4,     +       q    q(    4,    4@a,    )
7
v p            exp        ,    
=   exp(        7    7    4,     +       q    q(    4,    4@a,    )
)
4
    o
    (    4,    4@a,    )
    (    4,    )
    o

    p
    p

    q
    q

    a
    a

edge	feature	

q

node	feature	

cs6501: nlp

32

design features

v emission-like features
v binary feature functions

vb

nnp

v ffirst-letter-capitalized-nnp(china) = 1
v ffirst-letter-capitalized-vb(know) = 0

know

china

v integer (or real-valued) feature functions

v fnumber-of-vowels-nnp(china) = 2

v transition-like features
v binary feature functions

not	necessarily	
independent	 features!

v ffirst-letter-capitalized-vb-nnp(china) = 1

cs6501: nlp

33

general idea

vwe	want	the	score	to	the	correct	answer	
           ,     higher than others.
           ,     >        b,    			       b       ,    b          
           ,                b,     +    (    b,       )		       b       

v different	level	of	mistakes

v several	ml	models	can	be	used

v structured	id88
v structured	id166
v learning	to	search	

cs6501: nlp

34

log-linear model

v p            exp        ,    
v        ,     =   (        7    7    4,     +           q(    4,    4@a,    )
=        7(       7    4,    )
+       q(       q(    4,    4@a,    ))
7
    
q
4
    
    
       a    4,    )
    a    o       a    o   
4       o    4,    )
   
4
   
       a(    4,    4@a,    ))
4       o(    4,    4@a,    ))
   
4
    		   		f(    ,    )

essentially,	we	aggregate	
transition	and	emission	
patterns	as	features

4

)

cs6501: nlp

35

like	in	the	previous	slide,	we	can	
rearrange	the	summations

memm v.s. crf

vscore	function	can	be	the	same:
        ,     =   (        7    7    4,     +           q(    4,    4@a,    )
q
7
    
=       (    4,    4@a,    4)
    
             =           4    4,    4@a
=    abcd(ef,efgh,if)
f       abcde,efgh,if
4
f
j
=    abcd(ef,efgh,if)
             = abc		((cid:142)    ,    )	
f      abcd(ef,efgh,if)
   abc(cid:143)    ,    
    
    
f

v memm:

globally	normalized

locally	normalized

v crf:

)

cs6501: nlp

36

id48 v.s. memm v.s. crf

p(x,y)

p(y|x)

cs6501: nlp

37

id170    beyond 
sequence tagging
assign values to a set of interdependent output variables

task

input

output

part-of-speech
tagging

they  operate   
ships and banks.

dependency 
parsing

they  operate   
ships and banks.

segmentation

pronoun

verb

noun

and

noun

root they			operate			ships				and				banks		.

38

id136

	    	    ,    	
	argmax-

v find the best scoring output given the model

v output space is usually exponentially large
v id136 algorithms:

v specific: e.g., viterbi (linear chain)
v general: integer id135 (ilp)
v approximate id136 algorithms: 

e.g., belief propagation,  id209

39

learning structured models

solve id136s

update the model

(stochastic)	gradient	updates

40

example: structured id88

v goal: we	want	the	score	to	the	correct	answer	
           ,    ;     higher than others.
           ,    ;     >        b,    ;    			       b       ,    b          
v let s    ,    ;     =           (    ,    ;    )
v give training data {(        ,        )},    =1       	
v let     b=argmax                (    ,    ;    )
v if     b          :			           +    (           ,    ;            (       ,    ;    ))

v loop until converge

v for i = 1   n

kai-wei chang 

41

