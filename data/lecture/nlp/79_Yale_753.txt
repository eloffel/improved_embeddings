nlp
sequence-to-sequence and id4
deep learning
machine translation (mt)
goal: translate text from one language to the other
both language understanding and language generation

progress in machine translation
https://nlp.stanford.edu/projects/id4/luong-cho-manning-id4-acl2016-v4.pdf
id151
rise in early 90s
exploit parallel text


word alignment
slide credit to karl stratos
id4
modeling the machine translation using neural networks
encoder for language understanding in source language
decoder for language generation in target language
https://nlp.stanford.edu/projects/id4/luong-cho-manning-id4-acl2016-v4.pdf
use id56s as encoder and decoder
a recurrent neural network, or id56, is a network that operates on a sequence and uses its own output as input for subsequent steps.
a sequence-to-sequence network, or encoder decoder network, is a model consisting of two id56s called the encoder and the decoder.
the encoder reads an input sequence and outputs vectors, and the decoder reads encoder outputs as input to produce an output sequence.
slides from rui zhang
sequence to sequence model
sutskever, le, and vinyals 2014
sutskever, le, and vinyals 2014
sequence to sequence model
sequence to sequence model
sutskever, le, and vinyals 2014
sutskever, le, and vinyals 2014
sequence to sequence model
from id38 to mt
[bengio et al. jmlr 2003]
multilingual embeddings
[hermann and blunsom 2013: https://arxiv.org/abs/1312.6173]
use id56s as encoder and decoder
https://nlp.stanford.edu/pubs/luong2016iclr_multi.pdf


[koehn 2017]
[koehn 2017]
[koehn 2017]
[koehn 2017]
[koehn 2017]
beam decoding
[koehn 2017]
[koehn 2017]
beam decoding
[koehn 2017]
domain transfer
[koehn 2017]
[koehn 2017]
gru encoder
for every input word in the sentence, it is first used to index a id27 matrix to get its embedding.
then the encoder produces an output vector and a hidden state from the id27 and the previous hidden state. 
the hidden state is used for the next input word.
the initial hidden state is initialized as a zero vector.
gru decoder
the decoder is another id56 that outputs a sequence of words.
 the simple decoder uses only the last output of the encoder, which is called context vector.
at each step, the decoder takes an input word and the previous hidden state.
 at the very beginning, the input token is the start-of-sentence<sos>token, and the hidden state is the context vector encoding the meaning of the source sentence.
 then the decoder will work as illustrated below to produce an output vector and a hidden state for next step.
 the output vector is a id203 distribution over the target language vocabulary.
adding attention to the decoder
the simple decoder takes the final hidden state of the encoder and uses that to decode the target sentence. this requires to encode the entire sentence into a single fixed-size vector, which is difficult.
to solve this, we use the attention mechanism such that all the hidden states of the encoder are used to decode the target sentence.
attention illustration

at each step of decoding, the decoder attention focuses on different parts of the input sentence.

[bahdanau, cho, bengio iclr 2015]
google neural translation system
https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
google neural translation system
https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
data needs: smt vs. id4
slide from philipp koehn
software/code in homework5 - id4
data processing
read normalize, filter sentence pairs
prepare_data.py
pytorch
build, train, evaluate id4 model
run_id4.py, network.py
matplotlib
plot attention produced by id4 model
plot.py
id7 evaluator
calcualte id7 scores given system outputs and gold standard sentences
id7.py
nlp
