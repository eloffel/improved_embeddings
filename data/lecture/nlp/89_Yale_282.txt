nlp
parsing
prepositional phrase attachment
id32 representation
( (s 
    (np-sbj 
      (np (nnp pierre) (nnp vinken) )
      (, ,) 
      (adjp 
        (np (cd 61) (nns years) )
        (jj old) )
      (, ,) )
    (vp (md will) 
      (vp (vb join) 
        (np (dt the) (nn board) )
        (pp-clr (in as) 
          (np (dt a) (jj nonexecutive) (nn director) ))
        (np-tmp (nnp nov.) (cd 29) )))
    (. .) ))
id32 representation
( (s 
    (np-sbj (nnp mr.) (nnp vinken) )
    (vp (vbz is) 
      (np-prd 
        (np (nn chairman) )
        (pp (in of) 
          (np 
            (np (nnp elsevier) (nnp n.v.) )
            (, ,) 
            (np (dt the) (nnp dutch) (vbg publishing) (nn group) )))))
    (. .) ))
prepositional phrase attachment
join board as director 


high (verbal):

low (nominal):


is chairman of elsevier 
jane
nnp
vbd
dt
nn
caught
the
butterfly
np
vp
np
s
with
the
net
dt
nn
np
in
pp
examples
examples:
lucy   s plane leaves detroit on monday. - high
jenna met mike at the concert. - high
this painting must cost millions of dollars. - low
examples
high or low attachment?
alicia ate spaghetti from italy.
alicia ate spaghetti with meatballs.
alicia ate spaghetti with a fork.
alicia ate spaghetti with justin.
alicia ate spaghetti with delight.
alicia ate spaghetti on friday.
solution
high or low attachment?
alicia ate spaghetti from italy. - low
alicia ate spaghetti with meatballs. - low
alicia ate spaghetti with a fork. - high
alicia ate spaghetti with justin. - high
alicia ate spaghetti with delight. - high
alicia ate spaghetti on friday. - high
actual headline
police shoot man with box cutters.
      (s (np (n police)) (vp (v shoot) (np (n man) (pp (p with) (np (n box) (n cutters))))))
(?) (s (np (n police)) (vp (v shoot) (np (n man)) (pp (p with) (np (n box) (n cutters)))))

prepositional phrase attachment
input
a prepositional phrase and the surrounding context
output
a binary label: 0(high) or 1(low)
in practice
the context consists only of four words: the preposition, the verb before the preposition, the noun before the preposition, and the noun after the preposition
example: 
join board as director
why?
answer
because almost all the information needed to classify a prepositional phrase   s attachment as high or low is contained in these four features.
furthermore, using only these tuples of four features allows for a consistent and scaleable approach.
sample tuples
sidebar (1/2)
the linguistics (and psycholinguistics) literature offers competitive explanations for attachment. 
one theory (kimball 1973) favors the so-called right association rule. it says that, given a new phrase and two choices for attachment, people tend to attach the new phrase with the more recent (   rightmost    within the sentence) of the candidate nodes, resulting in low attachment. 
alternatively, the minimal attachment principle (frazier 1978) favors an attachment that results in the syntax tree having fewer additional syntactic nodes (in this case, favoring high attachment). 
as one can see from the statistics, none of these methods alone can explain the high prevalence of both types of attachment.

sidebar (2/2)
some observations can be made using statistical analysis of the training set. 
the standard corpus used for this sort of analyses comes from (rrr 1994) and includes 27,937 prepositional phrases extracted from the id32 (marcus et al. 1993), divided into three groups (20,801 training, 4039 development, and 3097  test). 
this data representation makes the assumption that additional context is only marginally more useful for classification purposes compared to the four features in the table (verb, noun1, preposition, and noun2). 
for comparison, the sentence matching the data point    bring attention to problem    is actually    although preliminary findings were reported more than a year ago, the latest results appear in today's new england journal of medicine, a forum likely to bring new attention to the problem.    it is unlikely that the information in the first    of the sentence will affect the classification of the prepositional phrase    to the problem   . 

supervised learning: evaluation
prepare the experimental data
manually label a set of instances.
split the labeled data into training and testing sets.
use the training data to find patterns.
apply these patterns on the testing data set.
for evaluation: 
use accuracy (the percentage of correct labels that a given algorithm has assigned on the testing data).
compare with a simple baseline method.
what is the simplest baseline method?
answer
the simplest supervised baseline method is to find the more common class (label) in the training data and assign it to all instances of the testing data set.

algorithm 1
random baseline
a random unsupervised baseline would have been to label each instance in the testing data set with a random label, 0 or 1. 
practically, random performance is the lower bound against which any non-random methods should be compared.

measuring the accuracy of the supervised baseline method (algorithm 1)
in the official training data set (rrr94), the rate of occurrence of the 1 label (low attachment) is 52.2% (10,865/20,801).
is the accuracy of this baseline method then equal to 52.2%?

answer
no, this is not how accuracy is computed. 
it has to be computed on the testing set, not the training set.
using the official split, the accuracy of this method on the testing set is 59.0% (1,826/3,097). 
one shouldn   t think of this number as a good result. 
the difference (+6.8% going from training to testing) could have been in the opposite direction, resulting in a performance below random.
observations
if the baseline method is simple and if the testing set is randomly drawn from the full data set and the data set is large enough, one could expect that the accuracy on the testing set is comparable to the one on the training set. 
note that the ptb data set is drawn from business news stories. if one were to train a method on this data and test it on a different set of sentences, e.g., from a novel, it is possible that the two sets will have very different characteristics.
the more complicated the method is, however, the more likely it will be that it will    overfit    the training data, learning patterns that are too specific to the training data itself and which may not appear in the testing data or which may be associated with the opposite class in the testing data.
upper bounds on accuracy
the 52% accuracy we   ve seen so far is our current lower bound. 
now, what is the upper bound?
usually, human performance is used for the upper bound. for pp attachment, using the four features mentioned earlier, human accuracy is around 88%.
so, a hypothetical algorithm that achieves an accuracy of 87% is in fact very close to the upper bound (on a scale from 52% to 88%, it is 97% of the way to the upper bound).
using linguistic knowledge
one way to beat the two baselines is to use linguistic information. 
for example, the preposition    of    is much more likely to be associated with low attachment than high attachment.
in the training data set, this number is an astounding 98.7% (5,534/5,607)
therefore the feature prep_of is very valuable. 
what are the two main reasons?
answer
reason 1
it is very informative (98.7% of the time it is linked with the low attachment class)
reason 2
it is very frequent (27.0% of the entire training set - 5607/20801).
reason 1 alone would not be sufficient!
sidebar 1/3
evaluation pipeline
the ptb (id32) data set has been used for competitive evaluation of pp attachment algorithms since 1994. 
each new algorithm is allowed to look at the training and development sets and use any knowledge extracted from them. 
the test set data can never be looked at and can be used only once per algorithm for its evaluation. 
doing the contrary (repeatedly tuning a new algorithm based on its performance on the designated test set) results in a performance level that is irreproducible on new data and such approaches are not allowed in nlp research. 
note that the development set can be used in a well-specified way as part of the training process.

sidebar 2/3
let   s look at the training data then and see if we can learn some patterns that would help us improve over the silly    label everything as noun attachment    baseline and its 52% expected accuracy. 
for example, some prepositions tend to favor particular attachments. 
let   s start with    against   . 
it appears 172 times in the training set, of which 82 (48%) are noun attached, the rest (52%) being high attached. 
this ratio (48:52) is very similar to the baseline (52:48), so clearly, knowing that the preposition is    against    gives us very little new information. 
sidebar 3/3
cont   d
furthermore, the total occurrence of    against    in the training corpus is rather small (less than 1% of the prepositional phrases). 
in two special cases, however, the identity of the preposition gives a lot of information. 
at one extreme,    of    is associated with low attachment in 99% of the cases, whereas at the other end of the scale,    to    is associated with high attachment in 82% of its appearances. 
it is also important to note that both of these prepositions are fairly common in the training set (   to    representing 27% of all prepositions and    of    accounting for another 11% of them). 
with this knowledge, we can build a very simple decision list algorithm (brill and resnik) that looks like this (the rules are sorted by their expected accuracy on their majority class).
are there any other such features?
yes, prep_to: 2,182/2,699 = 80.8% in favor of high attachment.
which leads us to our next algorithm:
algorithm 2
sidebar 1/2
let   s compare the performance of this simple decision list algorithm with that of the baseline. 
on the training set, the first rule would fire in 5,577 cases, of which 5,527 will be correctly labeled as low attachment and another 50 will be incorrectly labeled as high attachment (the accuracy of this rule is expected to be 99% on the training set). 
the second rule (with an expected  accuracy of 82% on the training set) would result in 2,672 decisions, of which 2,172 will be correctly processed as high attachment and 500 will be mislabeled as low attachment. 
finally, the default rule will kick in, whenever the first two rules are not applicable. in this case, it will apply on all remaining phrases (20,801     5,577     2,672 = 12,552 cases). 
specifically, among these, it will result in 4,837 phrases correctly labeled as low and 7,714 incorrectly labeled as high. 

sidebar 2/2
overall, we have 5,527 + 2,172 + 4,837 = 12,536 correct decisions, or an accuracy of 12,536/20,801 = 60%. 
clearly algorithm 2 outperforms algorithm 1 on the training data. 
this is not surprising, since its expected accuracy should be no less than the worst expected accuracy of its rules (that of rule 3) and it is likely to be higher than that lowest number because of the priority given to the easier to classify cases (rules 1 and 2).
more complicated algorithms can look at additional features
e.g., the nouns or the verb, or some combination thereof. for example, the verb    dropped    appears 75/81 times (93%) along with high attachment and only 7% with low attachment.
algorithm 2a
some observations (1/2)
first, even though the expected performance of rule 3 was 52%, its actual performance on the training set dropped to 39% after rules 1 and 2 were applied. 
in other words, these rules used up some of the information hidden in the data ahead of rule 3 and left it less useful information to rely upon. 
even more, one can see that a better decision would have been to replace rule 3 with its exact opposite, label everything left at this stage as    high   , which would have boosted the combined performance. 
algorithm 2a would achieve 5,527 + 2,172 + 7,714 = 15,413 correct decisions for an overall accuracy of 74% on the training set.
second, one cannot help but notice that algorithms 2 and 2a each have only three rules. 
we can imagine a classifier with 20,801 rules, one per training example, each rule of the form    if the preposition is    of    and the nouns are such and such and the verbs are such and such, then classify the data point as the actual class observed in the training set   . 

some observations (2/2)
third, we so far reported performance on the training set. 
can we project the performance on the training set to the test set? 
let   s start with algorithms 1 and 3. 
algorithm 1 labels everything as low attachment. it achieved 52% on the training set. we expect its performance on the test set to be similar. in fact it is 59% (1,826/3,097).
this clearly demonstrates the variability of text across subsets of the data. 
in this case, this variability favors algorithm 1 since its performance actually goes up when moving to the test set. in other cases (e.g., if we had swapped the training and test sets), its performance would have gone down. 
on average though, its performance on the test data is expected to vary around the performance on the training data. 

algorithm 3
some observations 1/5
now, let   s consider algorithm 3. 
it achieved a very high performance on the training data (way above the    upper bound    achieved by humans). 
however, we will now see the meaning of the word overfitting in action. 
algorithm 3 was so specific to the training data that most of the rules it learned don   t apply at all in the test set. 
only 117 combinations (out of 3032) of words in the test set match a combination previously seen in the training set. 
in other words, algorithm 3 learned a lot of good rules, but it failed to learn many more. in fact, its accuracy on the test data is only around 4%. 
some observations 2/5
an alternative to algorithm 3 would be to combine it with a default rule (just like rule 3 in algorithm 2) that labels everything that algorithm 3 missed as noun attachment. 
unfortunately, even this algorithm (let   s call it algorithm 3a) would only achieve a performance slightly above the baseline (algorithm 1) of 59% on the test data. 
the lesson to learn here is that, on unseen data, a simple algorithm (algorithm 1) is much better than a really complicated one that overfits (algorithm 3). also, the combination of the two (overfitting + baseline) just barely outperforms the baseline itself and is nowhere close to competitive. 

some observations 3/5
clearly this algorithm (algorithm 3) would achieve close to 100% accuracy on the training set. 
why    close to 100%    and not    100%   ? 
it turns out that the training set there are mutually inconsistent labels for the same data point. 
for example,    won verdict in case    appears once as high and once as low attachment. 
there are a total of 56 such    discrepancies    in the training set. 
some of them are caused by inconsistent annotators whereas others would require more context (e.g., the entire paragraph or document) to be correctly disambiguated.
some observations 4/5
next, let   s see how algorithms 2 and 2a will fare on the test set. 
first, let   s look at algorithm 2. 
there are 3097 items to classify in the test set. 
rule 1 correctly classifies 918 out of 926 instances of    of    (99% accuracy) while rule 2 gets 70% accuracy (234/332 correctly classified). 
rule 3 achieves 810/1,839 = 44%. 
overall the accuracy of algorithm 2 on the test set is 63% (1,962/3,097). 
again, on the test data, algorithm 2a outperforms algorithm 2. its rule 3 gets 1,029/1,839 = 56% accuracy and the overall accuracy of algorithm 2a on the test set is 70% (2,181/3,097). 

some observations 5/5
let   s now summarize the performance of the five algorithms that we have looked at so far.

what   s next?
so far, so good. we have been able to go from 59% test set accuracy to 70% with two simple rules. 
what additional sources of information can we use to improve the algorithm? 
here are some ideas:
use a few more good word features (e.g., more prepositions, perhaps some verb and nouns)
use clever ways to deal with missing information
use lexical semantic information (e.g., synonyms)
use additional context beyond the four feature types used so far.

what   s next?
let   s first consider a combination of the first two ideas above: looking for ways to use all possible information that can be extracted from the training data. 
this is the approach that was used by collins and brooks (1995). 
their method was based on a principle called backoff which is somewhat of a combination of all the algorithms used so far (e.g., algorithms 1, 2, and 3). 
backoff allows us to use the most specific evidence from the training data, when available but then make reasonable approximations for the missing evidence.
backoff method
collins and brooks algorithm:
if a 4-tuple is available, use it. 
if not, combine the evidence from the triples that form the 4-tuple (looking only at the triples that include the preposition). 
if that is not available, look at the pairs, then the singletons, and finally use a default class. 
a 4-tuple is just a set of 4 features in a particular order, e.g., (verb, noun1, preposition, noun2). 
the matching term for 3 features is a triple; for 2 features it is a pair; and for 1 feature, the word singleton is used.
what   s next?
the idea behind algorithm 3 was quite reasonable     assume that if the same object appear again (as defined by the same set of four features), it will likely have the same tag. 
the problem with this approach is that there is not enough data in the training set to learn the likely classes of all possible combinations of features. 
let   s do the math. to cover all the data points in the test set, we   d need information in the training set for a total of 102,998,280,840 combinations (more than 100 billion combinations)! 
how did we arrive at this number? 
it is simply the product of the numbers 1123, 1295, 52, and 1362, which are, respectively, the numbers of distinct verbs, noun1s, prepositions, and noun2s in the test set. 
it is impossible to label so much data and even if it could be done, there would be billions more combinations needed to cover a new test set. 
other methods
zhao and lin 2004
nearest neighbors
find most similar examples     86.5% best accuracy
similar to zavrel, daelemans, and veenstra 1997     memory-based learning
abney et al. 1999
boosting
stetina and nagao 1997
semantics
toutanova et al. 2004
graph-based method
comparative results
nlp
