nlp
id137 (lstm)
deep learning
lstm motivation
remember how we update an id56?
the
cat
sat

cost
wy








[slides from catherine finegan-dollak]
the vanishing gradient problem
deep neural networks use id26.
back propagation uses the chain rule.
the chain rule multiplies derivatives.
often these derivatives between 0 and 1.
as the chain gets longer, products get smaller
until they disappear.


or do they explode?
with gradients larger than 1,
you encounter the opposite problem
with products becoming larger and larger 
as the chain becomes longer and longer,
causing overlarge updates to parameters.
this is the exploding gradient problem.
vanishing/exploding gradients 
are bad.
if we cannot backpropagate very far through the network, the network cannot learn long-term dependencies. 

my dog [chase/chases] squirrels.
vs.
my dog, whom i adopted in 2009, [chase/chases] squirrels. 
6

lstm solution
use memory cell to store information at each time step.
use    gates    to control the flow of information through the network.
input gate: protect the current step from irrelevant inputs
output gate: prevent the current step from passing irrelevant outputs to later steps
forget gate: limit information passed from one cell to the next

transforming id56 to lstm
transforming id56 to lstm
c0
transforming id56 to lstm
transforming id56 to lstm
transforming id56 to lstm
transforming id56 to lstm

transforming id56 to lstm

transforming id56 to lstm
lstm for sequences
the
cat
sat
lstm applications
id46 (gonzalez-dominguez et al., 2014)
paraphrase detection (cheng & kartsaklis, 2015)
id103 (graves, abdel-rahman, & hinton, 2013)
handwriting recognition (graves & schmidhuber, 2009)
music composition (eck & schmidhuber, 2002) and lyric generation (potash, romanov, & rumshisky, 2015)
robot control (mayer et al., 2008)
id86 (wen et al. 2015) (best paper at emnlp)
id39 (hammerton, 2003)
http://www.cs.toronto.edu/~graves/handwriting.html
related architectures: gru
wx

wh

x1
h0
  1
  
h1
z1
+
1-z1
r1
chung et al. (2014) reports comparable performance to lstm

related architectures: tree lstms
h0
c0

h1
c1
x2

h2
c2
u2
i
o
f
f
tai, socher, manning 2015
external links
http://colah.github.io/posts/2015-08-understanding-lstms/ 
nlp
