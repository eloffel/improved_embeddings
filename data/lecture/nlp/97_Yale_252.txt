nlp
introduction to nlp
probabilistic parsing
main tasks with pid18s
given a grammar g and a sentence s, let t(s) be all parse trees that correspond to s
task 1
find which tree t among t(s) maximizes the id203 p(t)
task 2
find the id203 of the sentence p(s) as the sum of all possible tree probabilities p(t)
probabilistic parsing methods
probabilistic earley algorithm
top-down parser with a id145 table
probabilistic cocke-kasami-younger (cky) algorithm
bottom-up parser with a id145 table
probabilistic grammars
probabilities can be learned from a training corpus
treebank
intuitive meaning
parse #1 is twice as probable as parse #2
possible to do reranking
possible to combine with other stages
e.g., id103, translation
maximum likelihood estimates
use the parsed training set for getting the counts
pml(       ) = count (       )/count(  )

example: 
pml(s   np vp) = count (s   np vp)/count(s)

example from jurafsky and martin
sample probabilistic grammar
      s -> np vp    [p0=1]
      np -> dt n    [p1=.8]
      np -> np pp   [p2=.2]
      pp -> prp np  [p3=1]
      vp -> v np    [p4=.7]
      vp -> vp pp   [p5=.3]
      dt -> 'a'     [p6=.25]
      dt -> 'the'   [p7=.75]
      n -> 'child'  [p8=.5]
      n -> 'cake'   [p9=.3]
      n -> 'fork'   [p10=.2]
      prp -> 'with' [p11=.1]
      prp -> 'to'   [p12=.9]
      v -> 'saw'    [p13=.4]
      v -> 'ate'    [p14=.6]
example











































the
child
ate
the
cake
with
the
fork











































the
child
ate
the
cake
with
the
fork
dt
.75






dt
.75





























n
.5















the
child
ate
the
cake
with
the
fork






dt
.75





























n
.5






np
.8









the
child
ate
the
cake
with
the
fork






dt
.75





























n
.5






np
.8*.5*.75








the
child
ate
the
cake
with
the
fork






dt
.75





























n
.5






np
.8*.5*.75








the
child
ate
the
cake
with
the
fork
keep only the highest score in each cell
question
now, on your own, compute the id203 of the entire sentence using probabilistic cky.
don   t forget that there may be multiple parses, so you will need to add the corresponding probabilities.

notes
stanford demo
http://nlp.stanford.edu:8080/parser/
ptb statistics
50,000 sentences (40,000 training; 2,400 testing)
ptb peculiarities
includes traces and other null elements
flat np structure (e.g., np -> dt jj jj nnp nns)
parent transformation
subject nps are more likely to be modified than object nps
e.g., replace np with np^s

nlp
