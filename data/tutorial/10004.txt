a repository of state of the art and competitive baseline summaries for

generic news summarization

kai hong1, john m. conroy2, benoit favre3, alex kulesza4, hui lin5, ani nenkova1

1university of pennsylvania, philadelphia, pa, usa

2ida/center for computing sciences, bowie, maryland, usa

3aix-marseille univ lif/cnrs, marseille, france

4university of michigan, ann arbor, mi, usa

5lingochamp, shanghai, china

1fhongkai1,nenkovag@seas.upenn.edu, 2conroy@super.org

3benoit.favre@lif.univ-mrs.fr, 4kulesza@umich.edu, 5h@liulishuo.com

abstract

in the period since 2004, many novel sophisticated approaches for generic id57 have been developed.
intuitive simple approaches have also been shown to perform unexpectedly well for the task. yet it is practically impossible to
compare the existing approaches directly, because systems have been evaluated on different datasets, with different evaluation measures,
against different sets of comparison systems. here we present a corpus of summaries produced by several state-of-the-art extractive
summarization systems or by popular baseline systems. the inputs come from the 2004 duc evaluation, the latest year in which
generic summarization was addressed in a shared task. we use the same settings for id8 automatic evaluation to compare the
systems directly and analyze the statistical signi   cance of the differences in performance. we show that in terms of average scores the
state-of-the-art systems appear similar but that in fact they produce very different summaries. our corpus will facilitate future research
on generic summarization and motivates the need for development of more sensitive evaluation measures and for approaches to system
combination in summarization.

keywords: generic summarization, duc 2004, evaluation

1.

introduction

in generic multi-document news summarization, a system
is expected to produce a summary of the most important
information conveyed in a set of topically related news
articles.
the task does not presuppose the existence
of a user query or information need (thus the name
generic), so it appears to call for an automatic method
of at
least minimal semantic processing to discover
important information, a problem with huge appeal for
cognitive science and arti   cial
intelligence enthusiasts.
not surprisingly, many competing approaches have been
developed for the task. the early editions of the document
understanding conference (duc) evaluations (over et
al., 2007), carried out between 2001 and 2004, have
had a strong formative in   uence on work on generic
multi-document news summarization, by providing annual
test collections for a shared task. in later years of evaluation
the task was abandoned, in favor of other summarization
problems such as topic-focused summarization in response
to a user need, summarizing given the readers    prior
knowledge of the topic or summarizing in speci   c domains
where the aspects of importance are clearly de   ned a priori.
the scienti   c literature on summarization,
much of
however,
remained focused on generic summarization,
rendering the duc 2004 (task 2) dataset
the most
appropriate one for comparing the systems. the dataset
consists of 50 inputs for id57,
each consisting of about 10 news articles related to a topic.
the task was to generate a 100-word summary for each

input.
some recent systems have been evaluated on duc 2004,
some have been evaluated on earlier datasets and some
on data for
topic-focused summarization, where the
summarizers simply ignore the topic. even those evaluated
on the duc   04 dataset cannot be directly compared
in most cases.
the reason for this confusion is the
number of parameters one can set for id8 (lin,
2004), the id54 evaluation of choice.
id8 evaluates summaries by comparing the id165
overlap between a summary and a set of gold-standard
summaries produced by people. obviously the choice
of id165 size   1-, 2-, 3-, 4-, skip bigram or basic
syntactic elements   would change the score. furthermore
the tool produces recall, precision and f-measure results, so
different researchers have reported a different combination
of the above. to make matters even more disorienting,
the overlap with the gold-standard can be computed with
function words preserved or excluded and with words
stemmed or not. the system summary can be left as is,
truncated to 100 words as required in the original task
de   nition of duc 2003 as well as tac 2008-2011, or to
665 bytes as in the original task de   nition of duc 2004,
resulting in variable length summaries.
system peer 65 (classy 04), the best of   cial entry in the
duc 2004 evaluation, is a speci   c case in point. a quick
survey of the literature shows that its id8-1 recall
has been reported as 39:1% (manna et al., 2012), 38:3%
(lin and bilmes, 2011), and 30:8% (conroy et al., 2006),
depending on the parameters used for evaluation.

1608

the repository we introduce in this paper will remove some
of the stumbling blocks in evaluation and comparison of
generic summarization systems. we provide the output of
several state-of-the-art summarizers on duc 2004. for
completeness of the comparison we also include several
popular baselines1.
we then adopt the recommendations from recent    ndings
on summarization evaluation (owczarzak et al., 2012) to
   x the id8 parameters to those that lead to highest
agreement with manual evaluation. we report
three
id8 scores which have been shown to have good and
complementary behavior. we perform paired tests for
statistical signi   cance to establish the superiority of one
system over another (rankel et al., 2011) by abstracting
away the noise of intrinsic input dif   culty (nenkova and
louis, 2008).
we    nd that according to id8-2 recall, the best systems
developed after classy 04 are in fact not signi   cantly
different from each other (nenkova and louis, 2008).
we further analyze one of the possible reasons for the
similar performance of the top systems. it is conceivable
that despite using markedly different approaches to select
important content, the systems may end up choosing the
same facts or even the same sentences. to quantify the
extent to which this overlap in selection choices occurs,
we compare the summaries produced by the three best
systems at three levels of granularity: sentences, words and
summary content units (scu) (nenkova et al., 2007). at all
three granularities the overlap in content across summaries
is low. our    ndings suggest that summarization research
may bene   t from the development of more sensitive
measures of content to capture    ner nuances in system
performance. they also reveal there is a high potential for
system combination in summarization.
below we    rst describe the systems whose output we
include in our repository. we start with a description of
the baselines, then present the state-of-the-art systems. in
the next section we present id8-1, -2 and -4 recall
scores produced with the same setting of id8 to    nally
enable direct comparison of the systems. then we present
results on statistical signi   cance in the differences between
finally we investigate the overlap between
systems.
summaries from our collection of
the state-of-the-art
systems.

2. baseline systems

word id203 (freqsum): a simple yet powerful
approach for multi-document news summarization is to
approximate the importance of words with their id203
in the input,
then select sentences with high average
word id203 (nenkova et al., 2006). we follow this
approach, called freqsum, changing only the way we
handle redundancy in the summary. we do not include
a sentence in the summary if its cosine similarity with
a sentence already in the summary exceeds a prede   ned
threshold determined on development data (duc 2003).
we use the same approach for handling redundancy for
the other summarizers   tssum, centroid, cont. lexrank,

1the corpus is publicly available: https://www.seas.upenn.

edu/~nlp/corpora/sumrepo.html

and regsum   which rank sentences according to other
criteria. we describe these next.
topic words (tssum): another powerful method of
weighting words is the application of the log-likelihood
ratio (llr) test, usually called topic signatures, which
compares the distribution of words in the input and a
large background corpus (lin and hovy, 2000). since
the log-likelihood ratio (cid:21) follows a (cid:31)-square distribution,
it provides a natural way to select topic words by setting
a prede   ned con   dence level cutoff. words with scores
above the threshold are topic words with equal importance
(their weight is set to 1). sentence scores are computed
as the ratio of unique topic words to the total number of
unique words in the sentence, as suggested by conroy et
al., (2006). also following their work we consider words
to be topic words if their (cid:31)-square statistic exceeds 10,
corresponding to a 99.9% con   dence level.
centroid: the centroid of a document is an abstract
sentence, containing all the words in the document. the
weight of each word in the centroid is the average tf-idf
weight of the word in all input sentences, assuming words
have zero weight
in sentences in which they do not
appear. the centroid score for the sentences in the input
is calculated as their cosine similarity with the centroid
(radev et al., 2004b). we compute these scores from the
popular mead system directly (radev et al., 2004a). note
the mead system scores sentence importance as a linear
combination of the centroid score and the position score
of the sentence in the document. we do not use position
information here.
cont.
lexrank: graph approaches (mihalcea and
tarau, 2004; wan and yang, 2008) are widely used
for summarization. lexrank (erkan and radev, 2004)
is arguably the most popular member of this class of
summarizers. the input text is represented as a graph
g(v; e), where v is the set of sentences in the input. there
is an edge eij between two nodes vi and vj if and only if the
cosine similarity between them is above a certain threshold.
since there could be multiple possibilities of choosing the
threshold in lexrank, we here employ continuous lexrank
instead (erkan and radev, 2004). the cosine similarity
between vi and vj is directly used as weight for eij.
sentence importance is calculated by running the id95
(lawrence et al., 1998) algorithm on the text graph. we
compute the sentence importance from mead directly.
greedy-kl: this method is related to the word id203
approach. here, however,
is to incorporate
knowledge about the overall distribution of words in the
input, instead of focusing on the id203 of individual
words. the idea is to minimize the kl divergence between
the id203 distribution of words estimated from the
summary and that from the input.
since    nding the
summary with smallest kl divergence is intractable, we
present results for the greedy kl summarizer, as described
by haghighi and vanderwende (2009). the approach
iteratively selects the next sentence si to be included in the
summary c, which is done by picking the sentence that will
minimize the kl divergence between the word distributions

the goal

1609

   

estimated from s = si

c and the original input d 2.

3. state-of-the-art systems

we present the state-of-the-art systems in this section.
peer 65 is introduced    rst, followed by the other systems
presented in alphabetical order.
classy 04 [peer 65]: this system (conroy et al., 2004)
was the best among those that entered the of   cial duc
2004 evaluation. it is often used as comparison system by
developers of novel summarization methods. it employs a
hidden markov model, using topic signature as the only
feature. the id203 of one sentence being selected in
the summary also depends on the importance assigned to
its adjacent sentences in the input document.
it is worth
noting that there is a linguistic preprocessing component in
this system.
classy 11: this is the successor of peer 65, developed
to handle query-focused summarization. classy 11 was
the best query-focused system according to one of the
manual evaluation measures (overall responsiveness) in
the of   cial tac 2011 evaluation (conroy et al., 2011).
like classy 04,
it uses topic signatures as features;
however, it attempts to estimate the id203 that a term
(bigram) will occur in a human-generated summary. a
subset of non-redundant sentences with highest scores is
selected using non-negative id105 algorithm.
two major changes to classy 11 were made for this
study. first, for ease of comparison with classy 04
(peer 65),
the 2004 linguistic preprocessing was used.
likewise, a generic summarization term-weighting method
was needed, so the lsa based approach of occams v
(which we will describe afterwards) was deployed.
dpp: determinantal point processes (dpps) (kulesza
and taskar, 2012) are probabilistic models of sets
which balance the selection of important
information
and diverse groups of sentences within a given length.
speci   cally, dpps combine a per-sentence quality model
that prefers relevant sentences with a global diversity
model encouraging non-overlapping content. this setup
has several advantages. first, by treating these opposing
objectives probabilistically, there is a rigorous framework
for trading off between them. second, the sentence quality
model can depend on arbitrary features, and its parameters
can be ef   ciently learned from reference summaries via
maximum likelihood training; in contrast, most standard
summarization techniques are tuned by hand.
finally,
because a dpp is a probabilistic model, at test time it is
possible to sample multiple summaries and apply minimum
bayes risk decoding, thus improving id8 scores. the
dpp model in this work is trained on the duc 2003 data to
optimize the id8-1 f-score.
icsisumm: the icsi summarization system adopts a
global linear optimization framework,    nding the globally
optimal summary rather than greedily choosing sentences
according to their importance. a summary is generated by

2this

is different

from the formula in haghighi and
vanderwende (2009), as we minimize kl(s     d) instead of
kl(d     s). the summarizer we present here is the one with
better performance.

covering the most important concepts in the document set.
even though integer id135 (ilp) is np-hard,
the exact solutions to this problem can still usually be found
by a standard ilp solver in a very fast fashion (gillick and
favre, 2009). we collect the summaries for duc 2004
generated by the icsi/utd summarization system at tac
2009 (gillick et al., 2009), which optimizes the coverage of
key bigrams weighted by their frequency in the document
collection.

occams v: this system (davis et al., 2012; conroy
et al., 2013) employs latent semantic analysis (lsa) to
compute term weights and a sentence selection algorithm
based on two combinatorial problems,
the budgeted
maximal coverage (bmc) problem and the knapsack
problem. the sentence selection algorithm extends the
work of samir khuller on bmc (khuller et al., 1999), who
proposed a (1 (cid:0) e
2 ) approximation algorithm, whereas
a greedy algorithm has an unbounded approximation ratio.
occams v improves the algorithm by applying a fully
polynomial-time approximation scheme (fptas) dynamic
programming algorithm to knapsack problems formed from
subsets of candidate sentences found by applying greedy
bmc with a larger bound (e.g., word counts). as was done
with classy 11, the classy 04 linguistic preprocessing
was used for occams v.

(cid:0) 1

regsum combines

regsum: the regsum system (hong and nenkova,
2014) employs a supervised model for predicting word
importance. this model is superior to prior methods
for identifying the words which are included in human
models.
the weights estimated
from three unsupervised approaches, along with features
including locations, part-of-speech, name-entity-tags, topic
categories and contexts. speci   cally, this system captures
words which are of intrinsic interest to people by analyzing
a large number of summary-abstract pairs from the new
york times corpus (sandhaus, 2008). the summarizer
employs the same greedy optimization framework as
freqsum and tssum.
it shows that the quality of the
summaries could be greatly improved by better estimation
of word importance.

submodular: treating id57
as a submodular maximization problem has proven
successful (lin and bilmes, 2011) and has spurred a
great deal of interest
in this line of research (sipos
et al., 2012; morita et al., 2013; dasgupta et al.,
2013). the advantage of using a submodular function to
estimate summary importance is that there is an ef   cient
algorithm for incrementally computing the importance of a
summary with a performance guarantee on how close the
approximate solution will be to the globally optimal one.

we collect summaries from (lin and bilmes, 2012), where
they employ structure learning to produce the submodular
functions. they    rst learn a mixture of submodular    shells   
in a max-margin id170 setting. then a
mixtures of the shells can be instantiated to generate a more
complex submodular function.

1610

4. id8 performance

5. signi   cance test

we conduct our id8 experiment following the standard
suggested by owczarzak et al. 20123, where id8-2
recall with id30 and stopwords not removed provides
the best agreement with manual evaluations. we also
compute id8-1 recall, which is the measure with
highest recall of ability to identify the better summary in
a pair, and id8-4 recall, which is the measure with
highest precision of ability to identify the better summary
in a pair (owczarzak et al., 2012).
each summary is truncated to 100 words automatically by
id8 while evaluation.4

system

cont. lexrank

centroid
freqsum
tssum

greedy-kl
classy 04
classy 11
submodular

dpp

regsum

occams v
icsisumm

r-1
35.95
36.41
35.30
35.88
37.98
37.62
37.22
39.18
39.79
38.57
38.50
38.41

r-2
7.47
7.97
8.11
8.15
8.53
8.96
9.20
9.35
9.62
9.75
9.76
9.78

r-4
0.82
1.21
1.00
1.03
1.26
1.51
1.48
1.39
1.57
1.60
1.33
1.73

table 1: system performance comparison (%)

table 1 shows the performance of all approaches according
to id8-1,2,4 recall, sorted by id8-2 recall in
ascending order. among the baseline systems, greedy-kl
performs the best according to three id8 scores.
it
even achieves id8-1 recall higher than classy 04
and classy 11. the system with highest id8-1
recall is dpp, which exceeds the r-1 recall of classy
04 by 2:37%. it also achieves better performance on r-1
recall than all of the other systems by at least 1%, except for
the submodular system for which the difference is 0:59%.
note that this system   s edge over the other approaches is
not so clear-cut on r-2 and r-4, which likely is a re   ection
of the fact that dpp optimizes r-1 f-score during the
learning process. the integer id135 system
(icsisumm) optimizes the best coverage over bigram
frequency, and achieves the highest scores on r-2 and
r-4. the systems perform extremely close on id8-2
recall, with the top six systems only differing within a
range of 0:58%. classy 04 has a strong performance
on id8-4 recall (1:51%), and only three systems (dpp,
regsum, icsisumm) exhibit better performance on this
evaluation.

3id8-1.5.5 with the parameters: -n 4 -m -a -l 100 -x -c 95

-r 1000 -f a -p 0.5 -t 0

4in the of   cial duc 2004 evaluation, summaries of length
665 bytes were required.
that systems could
produce different numbers of words. the variation in length
has a noticeable impact on id8 scores, especially recall,
which is the id8 score most highly correlated with manual
evaluations.

this meant

we conduct two-sided wilcoxon signed-rank tests between
each pair of
the state-of-the-art systems as well as
greedy-kl, as advocated in rankel et al. (2011). the
experiment
is performed on id8-1,2,4. detailed
p-values for the comparison are presented in table 6, table
7 and table 8 (see appendix a). the order of systems
in these tables are listed according to the order they are
described in section 2. and section 3. p-values indicating
signi   cance at the 95% con   dence level or above are shown
in bold. a plus sign before the p-value indicates that
the system in the row performs better than the one in the
column. a minus indicates the opposite relation, that the
system in the column is better than the one listed in the
row.
on id8-1, only dpp and submodular show signi   cant
improvement over classy 04 and greedy-kl. dpp
performs signi   cantly better than all but one systems
(submodular system) on r-1 recall.
on id8-2, there are no signi   cant differences between
the top six systems. regsum is the only one which provides
a signi   cant improvement over classy 04 (p = 0:0483).
the difference between classy 04 and occams v
and icsisumm however tends towards signi   cance, with
p-values of 0:0572 and 0:071 respectively.
five of
the state-of-the-art systems are signi   cantly better than
greedy-kl on r-2.
on id8-4, there are also no recently developed systems
that signi   cantly outperform classy 04.
icsisumm
has the best overall performance but the p-value for the
difference is 0:1798. dpp, icsisumm and regsum are
signi   cantly better than greedy-kl on r-4 recall.
6. overlap between summaries

recent

systems

the state-of-the-art
we have so far established that
summarization
years
in
developed
comfortably outperform standard baselines and also
work better than the state-of-the-art of a decade ago. the
newer systems however do not appear to be that different
from each other in automatic evaluation.
in the rest of
the paper we examine if the lack of difference between
systems is due to the fact that they select the same content,
albeit via different methods. we study the overlap of the
produced summaries in terms of sentences, words and, for
a subset of inputs, in terms of manually annotated summary
content units following the pyramid method (nenkova et
al., 2007).

it

6.1. sentence level comparison
given that all the systems we study are extractive, selecting
sentences directly from the input,
is fairly easy to
compute sentence overlap of the summaries for the same
input produced by different systems. we use the jaccard
coef   cient to compute the degree of sentence overlap. if
we denote the sets of sentences from two summaries of
the same input s as as and bs, the jaccard coef   cient
is de   ned as the size of the intersection divided by the size
of the union of the two sets:

   
   

jas
jas

bsj
bsj

j(as; bs) =

1611

greedy-kl

classy 04

classy 11

0.084

0.141
0.003

dpp
0.074
0.075
0.073

icsisumm occams v regsum submodular

0.098
0.082
0.092
0.143

0.035
0.003
0.169
0.071
0.083

0.111
0.136
0.060
0.124
0.149
0.090

0.135
0.050
0.070
0.082
0.038
0.050
0.113

table 2: sentence overlap using the jaccard coef   cient

greedy-kl

classy 04

classy 11

0.287

0.342
0.206

dpp
0.315
0.300
0.311

icsisumm occams v regsum submodular

0.353
0.315
0.295
0.387

0.265
0.208
0.385
0.307
0.302

0.359
0.407
0.262
0.372
0.375
0.274

0.348
0.289
0.317
0.367
0.307
0.292
0.366

greedy-kl
classy 04
classy 11

dpp

icsisumm
occams v

regsum

submodular

greedy-kl
classy 04
classy 11

dpp

icsisumm
occams v

regsum

submodular

table 3: word overlap using the jaccard coef   cient, including stopwords, with id30

the jaccard coef   cient between two systems is de   ned to
be the mean of j(as; bs) across all inputs s in the duc
2004 dataset.
occasionally, some sentences in the summary do not match
exactly any of the sentences in the input. this happens
for almost all last sentences in the summary which were
truncated mid-sentence in order to meet the 100 word limit.
we simply ignore the the last sentence if it does not match.
in other cases the sentence was not found because the
system performed some sentence editing or simpli   cation.
for example classy 04, classy 11 and occams v
delete appositive clauses.
in this case we    nd the most
similar sentence in the input measured by cosine similarity
and treat that input sentence as the one that appeared in
the summary. in table 2, we show the jaccard coef   cients
between each pair of systems.
the degree of overlap at sentence level is surprisingly
low. the jaccard coef   cients between two of the best
systems, dpp and icsisumm is 0:143. occams v and
classy 11 extract the most similar sets of sentences, with
a jaccard coef   cient of 0:169. the lowest overlap at the
sentence-level is between classy 04 and classy 11,
with jaccard coef   cient of only 0:003.

6.2. word level comparison
we next investigate the word overlap between summaries.
speci   cally, we compute the jaccard coef   cient between
the sets of unique words in each summary for a given
input. as in the id8 setting which correlates the best
with human evaluations, we perform id30 and include
stopwords while doing the comparison. table 3 lists the
jaccard coef   cients for word overlap between systems. the
coef   cients range from 0:206 (classy 04 vs classy
11) to 0:407 (classy 04 vs regsum), with a mean of
0:318. the numbers are low overall, with systems sharing

at most a third of the unique words in their summaries.

6.3. scu level comparison
the comparison of sentence and word overlap does not
directly reveal the overlap in semantic content expressed
in the summaries. to better investigate the coverage of
information in summaries, we compute the overlaps of
scus using the pyramid method (nenkova et al., 2007).
scus are de   ned as semantically motivated subsentential
units. they are annotated manually and map together
all expressions of the same content, even if the wording
across summaries differs. first, we manually create the
pyramids for 10 input sets in duc 2004; that is, we    nd
the scus expressed in the four human written abstracts
for the input which were provided as part of the duc
data. then we identify which scus are expressed in the
summaries of four of the automatic systems   greedy-kl,
dpp, icsisumm and regsum   for those inputs. scus
conveyed in truncated last sentences are also included in
the calculations of overlap.
on average there are 33:7 scus expressed in the four
human summaries. on average each scu is expressed
1:78 times across the four human summaries, with few
scus repeated frequently and most appearing in only one
summary. table 4 shows the average number of scus from
the reference human summaries that are also expressed in
the machine summaries. the total number of unique scus
covered by the four machine summaries is 13:6 on average
for the 10 inputs. we also show the modi   ed pyramid
scores for those four summaries in table 4. there are no
signi   cant differences when evaluated by pyramid scores.
we compute the similarity of systems in terms of scus
using the jaccard coef   cient. the result is shown in table
5. the similarity score ranges from 0:347 to 0:426, with an
average of 0:385.

1612

# scus

pyramid score

kl
7.3
0.517

dpp
6.9
0.506

icsisumm regsum

7.1
0.514

7.3
0.533

table 4: average number of scus per summary and
modi   ed pyramid scores on the    rst 10 input sets

even in the manual analysis of content overlap, the systems
appear to share some content but more than half of the
information appear only in machine summary produced
by one of the two systems. clearly the systems perform
similarly not because they end up choosing the same
content. the systems choose different but equally good
content. this    nding indicates that it is quite possible
to successfully exploit methods for system combination
in order to combine content from each of the systems.
this idea is also supported by work on fully automatic
evaluation for summarization which has shown that the
combination of different systems    input serves as an
excellent reference for estimating summary content quality
(louis and nenkova, 2013).
alternatively,
it may be possible to develop more
sensitive evaluation methods that are capable of identifying
differences in the importance of non-shared content in the
summaries. this development may be necessary for further
progress in content selection for summarization, as even the
manual evaluations do not    nd many signi   cant differences
between the top systems (rankel et al., 2013).

kl

dpp
0.382

icsisumm regsum
0.426
0.419
0.351

0.386
0.347

kl
dpp

icsisumm
regsum

table 5: scus overlap using the jaccard coef   cient

to more concretely illustrate the differences in summary
content, we show the machine summaries for the input
d30002t and d30010t in table 9 (see appendix b). these
are the two input with the least and most overlap between
scus, with an average jaccard coef   cient of 0:140 and
0:529, respectively. we marked the expressions of scus
which appear in multiple machine summaries in table 9.
for the input d30002t, 12 unique scus are expressed in
the machine summaries. only three of them are expressed
in more than one machine summary. scus a, b and c
are respectively hurricane mitch brought huge death
toll in central america, slow-moving mitch battered
the honduran for more than a day and taiwan sent aid
to central american countries. apart from those scus,
there are large differences in the content that appear in the
summaries. on the other hand, of the 14 unique scus
from the human summaries on input d30010t, eight of them
are expressed in more than one machine summary.
it is
worth noting that six of the 14 scus appear in at least three
machine summaries.

7. conclusion

we presented a repository of generic multi-document
summaries produced by a range of systems for
the
same input.
this resource allow us to carry out a
unique comparison between existing approaches. we
have also outlined the methodology for reporting results,
establishing informed choices for id8 settings and for
the computation of statistical signi   cance.
we demonstrate that the greedy kl baseline performs very
well, at times on par with the best systems existing to date.
given its simplicity and excellent performance, kl should
be used as a baseline in future studies.
there are no signi   cant differences between the best
systems on r-2 recall. the dpp supervised framework
and the unsupervised global optimization approach emerge
as the best systems on r-1 and r-2,-4 respectively.
moreover, we show that diverse contents get selected
by the summaries from different state-of-the-art systems.
this suggests that summary combination might
lead
to improvements in content selection. our repository,
along with the guidelines on reporting performance,
will enable further progress in id54.
the repository is publicly available via this
link:
https://www.seas.upenn.edu/~nlp/corpora/sumrepo.html .

8. references

conroy, john m., goldstein, jade, schlesinger, judith d.,
and o   leary, dianne p.
(2004). left-brain/right-brain
id57. in proceedings of the
document understanding conference (duc).

conroy, john m., schlesinger, judith d., and o   leary,
topic-focused multi-document
in

dianne p.
summarization using an approximate oracle score.
proceedings of coling/acl, pages 152   159.

(2006).

conroy, john m, schlesinger, judith d, kubina, jeff,
rankel, peter a, and o   leary, dianne p.
(2011).
classy 2011 at tac: guided and multi-lingual
summaries and id74. proceedings of the
text analysis conference.

conroy, john m., davis, sashka t., kubina, jeff, liu,
yi-kai, o   leary, dianne p., and schlesinger, judith d.
(2013). multilingual summarization: dimensionality
reduction and a step towards optimal term coverage.
in proceedings of
the multiling 2013 workshop
on multilingual id57, pages
55   63.

dasgupta, anirban, kumar, ravi, and ravi, sujith. (2013).
summarization through submodularity and dispersion.
in proceedings of acl, pages 1014   1022.

davis, sashka t., conroy, john m., and schlesinger,
judith d.
occams     an optimal
combinatorial covering algorithm for multi-document
summarization. in icdm workshops, pages 454   463.

(2012).

erkan, g  unes and radev, dragomir r. (2004). lexrank:
salience in text
int. res., 22(1):457   479,

graph-based lexical centrality as
summarization.
december.

j. artif.

gillick, dan and favre, benoit.

(2009). a scalable
global model for summarization. in proceedings of acl

1613

workshop on integer id135 for natural
langauge processing, pages 10   18.

gillick, dan, favre, benoit, hakkani-tur, dilek, bohnet,
(2009). the
in

berndt, liu, yang, and xie, shasha.
icsi/utd summarization system at tac 2009.
proceedings of the text understanding conference.

haghighi, aria and vanderwende, lucy. (2009). exploring
content models for id57. in
proceedings of hlt-naacl, pages 362   370.

hong, kai and nenkova, ani.

improving the
estimation of word importance for news multi-document
summarization. in proceedings of eacl, gothenburg,
sweden, april.

(2014).

khuller, samir, moss, anna, and naor, joseph. (1999).
the budgeted maximum coverage problem. inf. process.
lett., 70(1):39   45.

kulesza, alex and taskar, ben.

(2012). determinantal
point processes for machine learning. foundations and
trends in machine learning, 5(2   3).

lawrence, page, sergey, brin, motwani, rajeev, and
winograd, terry. (1998). the id95 citation ranking:
bringing order to the web. technical report, stanford
university.

lin, hui and bilmes, jeff. (2011). a class of submodular
functions for document summarization. in proceedings
of acl, pages 510   520.

lin, hui and bilmes, jeff.

(2012). learning mixtures
of submodular shells with application to document
summarization. in uncertainty in arti   cial intelligence
(uai), catalina island, usa, july. auai.

lin, chin-yew and hovy, eduard. (2000). the automated
acquisition of topic signatures for text summarization. in
proceedings of coling, pages 495   501.

lin, chin-yew. (2004). id8: a package for automatic
in text summarization
evaluation of
branches out: proceedings of the acl-04 workshop,
pages 74   81.

summaries.

louis, annie and nenkova, ani. (2013). automatically
assessing machine summary content without a gold
standard. comput. linguist., 39(2):267   300, june.

manna, sukanya, gao, byron j., and coke, reed. (2012).
a subjective logic framework for multi-document
in proceedings of coling 2012:
summarization.
posters, pages 797   808.

in   uence summarization.
pages 573   580.

in proceedings of sigir,

nenkova, ani, passonneau, rebecca, and mckeown,
kathleen. (2007). the pyramid method: incorporating
human content selection variation in summarization
evaluation. acm trans. speech lang. process., 4(2).

over, paul, dang, hoa, and harman, donna. (2007). duc

in context. inf. process. manage., 43(6):1506   1520.

owczarzak, karolina, conroy, john m., dang, hoa trang,
and nenkova, ani.
(2012). an assessment of the
accuracy of automatic evaluation in summarization. in
naacl-hlt 2012: workshop on id74
and system comparison for id54,
pages 1   9.

radev, d., allison, t., goldensohn, blair s., blitzer,
j., c   elebi, a., dimitrov, s., drabek, e., hakim, a.,
lam, w., and liu, d.
(2004a). mead: a platform
for multidocument multilingual text summarization. in
proceedings of lrec, pages 1   4.

radev, dragomir r., jing, hongyan, stys, malgorzata, and
tam, daniel. (2004b). centroid-based summarization
inf. process. manage.,
of multiple documents.
40(6):919   938, november.

rankel, peter, conroy, john, slud, eric, and o   leary,
ranking human and machine
in proceedings of emnlp,

dianne.
summarization systems.
pages 467   473.

(2011).

rankel, peter a., conroy, john m., dang, hoa trang, and
nenkova, ani. (2013). a decade of automatic content
evaluation of news summaries: reassessing the state of
the art. in proceedings of acl (volume 2: short papers),
pages 131   136.

sandhaus, evan. (2008). the new york times annotated
corpus. linguistic data consortium, philadelphia, pa.
sipos, ruben, shivaswamy, pannaga, and joachims,
thorsten. (2012). large-margin learning of submodular
summarization models. in proceedings of eacl, pages
224   233.

wan, xiaojun and yang, jianwu. (2008). multi-document
in

summarization using cluster-based link analysis.
proceedings of sigir, pages 299   306.

mihalcea, rada and tarau, paul.

bringing order into text.
pages 404   411.

(2004). textrank:
in proceedings of emnlp,

morita, hajime, sasano, ryohei, takamura, hiroya,
(2013). subtree extractive
in

and okumura, manabu.
summarization via submodular maximization.
proceedings of acl, pages 1023   1032.

nenkova, ani and louis,

(2008). can you summarize
this?
identifying correlates of input dif   culty for
id57. in proceedings of acl,
pages 825   833.

nenkova, ani, vanderwende, lucy, and mckeown,
kathleen.
(2006). a compositional context sensitive
multi-document summarizer: exploring the factors that

1614

greedy-kl
classy 04
classy 11

dpp

icsisum

occams v

regsum

submodular

greedy-kl
classy 04
classy 11

dpp

icsisum

occams v

regsum

submodular

greedy-kl
classy 04
classy 11

dpp

icsisum

occams v

regsum

submodular

greedy-kl

classy 04
(+) 0.6157

appendix a

classy 11
(+) 0.3431
(+) 0.7081

dpp

(-) 0.0025
(-) 0.0005
(-) 0.0001

icsisum
(-) 0.1992
(-) 0.1145
(-) 0.0321
(+) 0.0113

occams v

(-) 0.2754
(-) 0.1296
(-) 0.0057
(+) 0.0049
(-) 0.9714

regsum
(-) 0.2362
(-) 0.0800
(-) 0.0285
(+) 0.0165
(-) 0.7906
(-) 0.7779

submodular
(-) 0.0176
(-) 0.0078
(-) 0.0009
(+) 0.0719
(-) 0.1346
(-) 0.1925
(-) 0.2165

table 6: p-values for paired two-sided wilcoxon signed-rank test, on id8-1 recall

greedy-kl

classy 04
(-) 0.3884

classy 11
(-) 0.0836
(-) 0.5211

dpp

(-) 0.0066
(-) 0.1331
(-) 0.1641

icsisum occams v
(-) 0.0002
(-) 0.0710
(-) 0.0554
(-) 0.4816

(-) 0.0013
(-) 0.0572
(-) 0.1303
(-) 0.6516
(+) 0.7906

regsum
(-) 0.0036
(-) 0.0483
(-) 0.1161
(-) 0.5785
(+) 0.7583
(-) 0.9762

submodular
(-) 0.0277
(-) 0.3173
(-) 0.6555
(+) 0.3173
(+) 0.1860
(+) 0.1563
(+) 0.1260

table 7: p-values for paired two-sided wilcoxon signed-rank test, on id8-2 recall

greedy-kl

classy 04
(-) 0.3269

classy 11
(-) 0.1503
(+) 0.6958

dpp

(-) 0.0373
(-) 0.6234
(-) 0.4687

icsisum occams v
(-) 0.0029
(-) 0.1798
(-) 0.0806
(-) 0.2290

(-) 0.5531
(+) 0.7536
(+) 0.2672
(+) 0.1222
(+) 0.0050

regsum
(-) 0.0360
(-) 0.4407
(-) 0.4396
(-) 0.9056
(+) 0.3556
(-) 0.1206

submodular
(-) 0.4863
(+) 0.7394
(+) 0.5183
(+) 0.0400
(+) 0.0095
(-) 0.4207
(+) 0.0660

table 8: p-values for paired two-sided wilcoxon signed-rank test, on id8-4 recall

1615

appendix b

input d30002t
greedy-kl summary: in washington on thursday, president bill clinton ordered dlrs 30 million in defense department equipment
and services and dlrs 36 million in food, fuel and other aid be sent to honduras, nicaragua, el salvador and guatemala. with the
storm seemingly anchored off honduras, of   cials in mexico to the north eased emergency measures on the caribbean coast of the
yucatan peninsula, where hundreds of people remained in shelters as a precaution wednesday night. the honduran president closed
schools and public of   ces on the coast monday and ordered all air force planes and helicopters to evacuate people from the islas de
la bahia, a string of small islands off the country   s central coast.
dpp summary: at 0900 gmt tuesday, mitch was 95 miles north of honduras, near the swan islands. hurricane mitch cut through
the honduran coast like a ripsaw thursday, [its devastating winds whirling for a third day]b through resort islands and mainland
communities. mexico reported one death from mitch last monday. [hurricane mitch killed an estimated 9,000 people throughout
central america]a in a disaster of such proportions that relief agencies have been overwhelmed. he urged the more than 1.5 million
hondurans affected by the storm to help with the recovery effort. [taiwan said today it will donate dlrs 2.6 million in relief to
honduras, nicaragua, el salvador and guatemala]c.
icsisumm summary: at least 231 people have been con   rmed dead in honduras from former-hurricane mitch, bringing the storm   s
death toll in the region to 357. pope john paul ii appealed for aid wednesday for the central american countries stricken by hurricane
mitch and said he feels close to the thousands who are suffering. [of   cials in central america estimated tuesday that about
7,000 people have died in the region]a. [that meant the honduran coast had been under hurricane conditions for more than
a day]b. [taiwan said today it will donate dlrs 2.6 million in relief to honduras, nicaragua, el salvador and guatemala]c.
regsumm summary: president carlos flores declared hurricane mitch had set back honduras development by 50 years. el salvador
reported 239 dead; guatemala said 194 of its people had been killed. [of   cials in central america estimated tuesday that about
7,000 people have died in the region]a. [hurricane mitch killed an estimated 9,000 people throughout central america]a
in a disaster of such proportions that relief agencies have been overwhelmed. mexico reported one death from mitch last monday.
the strongest hurricane to hit honduras in recent memory was fi    in 1974, which ravaged honduras caribbean coast, killing at least
2,000 people.

input d30010t
greedy-kl summary: [the radical group islamic jihad claimed responsibility saturday]a for the [suicide bombing of a
crowded jerusalem market]c and [promised more attacks]d to try to block the new peace accord]h. david bar-illan, a top aide
to israeli prime minister benjamin netanyahu, said sunday that israel expects palestinian leader yasser arafat to formally outlaw
the military wings of islamic jihad and the larger militant group hamas. [a palestinian security of   cial said several islamic holy
war members were arrested]f in the west bank on friday night. [two people were killed and at least 21 injured]b when an
explosives-rigged car [blew up friday at jerusalem   s mahane yehuda market]e, only meters(yards) from the site of a suicide
bombing 16 months ago.
dpp summary: [the israeli cabinet also announced it will begin to build houses]g in the controversial [jewish settlement of har
homa in east jerusalem]g. [the radical group islamic jihad claimed responsibility saturday]a for [the market bombing]c
and [vowed more attacks]d to [try to block the new peace accord]h. palestinian political leaders said israel should not use
friday   s suicide bombing, [which killed the two assailants and wounded 21 israelis]b, as an excuse to stop the peace process. the
militant palestinian movement islamic holy war said saturday that it [carried out the suicide bombing in a jerusalem market on
friday]e, [which prompted arrests by the palestinian authority overnight]f .
icsisumm summary: a de   ant prime minister [benjamin netanyahu said saturday that israel would continue to build jewish
neighborhoods throughout jerusalem]g, including at a controversial site in the traditionally arab sector of the city. [the radical
group islamic jihad claimed responsibility saturday]a for [the market bombing]c and [vowed more attacks]d to [try to block
the new peace accord]h. hassan asfour, a palestinian peace negotiator, said the palestinian authority condemned the attack.
an islamic jihad of   cial in the syrian capital of damascus con   rmed that the group   s leader ramadan abdullah shallah [claimed
responsibility for friday   s bombing]e in an interview with the paris-based radio monte carlo.
regsum summary: [the radical group islamic jihad claimed responsibility saturday]a for [the suicide bombing of a crowded
jerusalem market]c and [promised more attacks]d to [try to block the new peace accord]h. the militant palestinian movement
islamic holy war said saturday that [it carried out the suicide bombing in a jerusalem market on friday]e, [which prompted
arrests by the palestinian authority overnight]f . the islamic militant group hamas, which has [tried to stop the peace
agreement]d, claimed responsibility, police said.
[two people were killed and 21 others were wounded]b in the attack for
which [the islamic militant group hamas claimed responsibility]a.

table 9: summaries generated from the greedy-kl, dpp, icsisumm and regsum systems for the input d30002t, d30010t.
contributors of the scus which appear in multiple machine summaries are labeled in brackets.

1616

