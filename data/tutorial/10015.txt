1

pattern recognition letters

journal homepage: www.elsevier.com

summarization of films and documentaries based on subtitles and scripts

marta apar    cioa,b, paulo figueiredoa,c, francisco raposoa,c, david martins de matosa,c,      , ricardo ribeiroa,b, lu    s marujoa

al2f - inesc id lisboa, rua alves redol, 9, 1000-029 lisboa, portugal
binstituto universit  ario de lisboa (iscte-iul), av. das forc  as armadas, 1649-026 lisboa, portugal
cinstituto superior t  ecnico, universidade de lisboa, av. rovisco pais, 1049-001 lisboa, portugal

abstract

we assess the performance of generic text summarization algorithms applied to    lms and documen-
taries, using extracts from news articles produced by reference models of extractive summarization.
we use three datasets: (i) news articles, (ii)    lm scripts and subtitles, and (iii) documentary subtitles.
standard id8 metrics are used for comparing generated summaries against news abstracts, plot
summaries, and synopses. we show that the best performing algorithms are lsa, for news articles
and documentaries, and lexrank and support sets, for    lms. despite the different nature of    lms and
documentaries, their relative behavior is in accordance with that obtained for news articles.
c(cid:13) 2016 elsevier ltd. all rights reserved.

1. introduction

input media for id54 has varied from
text [18, 5] to speech [21, 39, 34] and video [1], but the ap-
plication domain has been, in general, restricted to informa-
tive sources: news [2, 30, 33, 11], meetings [26, 8], or lec-
tures [7]. nevertheless, application areas within the entertain-
ment industry are gaining attention: e.g. summarization of lit-
erary short stories [12], music summarization [31], summariza-
tion of books [24], or inclusion of character analyses in movie
summaries [36]. we follow this direction, creating extractive,
text-driven video summaries for    lms and documentaries.

documentaries started as cinematic portrayals of reality [10].
today, they continue to portray historical events, argumenta-
tion, and research. they are commonly understood as capturing
reality and therefore, seen as inherently non-   ctional. films, in
contrast, are usually associated with    ction. however,    lms and
documentaries do not fundamentally differ: many of the strate-
gies and narrative structures employed in    lms are also used in
documentaries [27].

in the context of our work,    lms (   ctional) tell stories based
on    ctive events, whereas documentaries (non-   ctional) ad-
dress, mostly, scienti   c subjects. we study the parallelism be-
tween the information carried in subtitles and scripts of both
   lms and documentaries. extractive summarization methods

have been extensively explored for news documents [16, 22,
37, 29, 30, 23]. our main goal is to understand the qual-
ity of automatic summaries, produced for    lms and documen-
taries, using the well-known behavior of news articles as ref-
erence. generated summaries are evaluated against manual
abstracts using id8 metrics, which correlate with human
judgements [15, 17].

this article is organized as follows: section 2 presents the
summarization algorithms; section 3 presents the collected
datasets; section 4 presents the evaluation setup; section 5 dis-
cusses our results; section 6 presents conclusions and direc-
tions for future work.

2. generic summarization

six text-based summarization approaches were used to sum-
marize newspaper articles, subtitles, and scripts. they are de-
scribed in the following sections.

2.1. maximal marginal relevance (mmr)

mmr is a query-based summarization method [4]. it iter-
atively selects sentences via equation 1 (q is a query; sim1
and sim2 are similarity metrics; si and sj are non-selected and
previously selected sentences, respectively).    balances rele-
vance and novelty. mmr can generate generic summaries by
considering the input sentences centroid as a query [25, 38].

(cid:20)

(cid:21)

      corresponding author

e-mail: david.matos@inesc-id.pt (david martins de matos)

arg max

si

  sim1 (si, q)     (1       ) max

sim2 (si, sj)

(1)

sj

6
1
0
2

 
r
a

m
9

 

 
 
]
l
c
.
s
c
[
 
 

3
v
3
7
2
1
0

.

6
0
5
1
:
v
i
x
r
a

2.2. lexrank

lexrank [6] is a centrality-based method based on google   s
id95 [3]. a graph is built using sentences, represented by
tf-idf vectors, as vertexes. edges are created when the cosine
similarity exceeds a threshold. equation 2 is computed at each
vertex until the error rate between two successive iterations is
lower than a certain value.
in this equation, d is a damping
factor to ensure the method   s convergence, n is the number of
vertexes, and s (vi) is the score of the ith vertex.

+d   (cid:88)

(cid:80)

sim (vi, vj)

vj   adj[vi]

vk   adj[vj ] sim (vj, vk)

s (vi) =

(1     d)

n

s (vj)

(2)

2.3. latent semantic analysis (lsa)

lsa infers contextual usage of text based on word co-
occurrence [13, 14]. important topics are determined without
the need for external lexical resources [9]: each word   s oc-
currence context provides information concerning its meaning,
producing relations between words and sentences that correlate
with the way humans make associations. singular value de-
composition (svd) is applied to each document, represented by
a t   n term-by-sentences matrix a, resulting in its decomposi-
tion u   v t . summarization consists of choosing the k highest
singular values from   , giving   k. u and v t are reduced to
k .
k , respectively, approximating a by ak = uk  kv t
uk and v t
the most important sentences are selected from v t
k .

2.4. support sets

documents are typically composed by a mixture of subjects,
involving a main and various minor themes. support sets are
de   ned based on this observation [35]. important content is de-
termined by creating a support set for each passage, by compar-
ing it with all others. the most semantically-related passages,
determined via geometric proximity, are included in the support
set. summaries are composed by selecting the most relevant
passages, i.e., the ones present in the largest number of support
sets. for a segmented information source i (cid:44) p1, p2, . . . , pn ,
support sets si for each passage pi are de   ned by equation 3,
where sim is a similarity function, and  i is a threshold. the
most important passages are selected by equation 4.
si (cid:44) {s     i : sim(s, pi) >  i     s (cid:54)= pi}

(3)

arg max

s   u n

i=1si

|{si : s     si}|

(4)

2.5. key phrase-based centrality (kp-centrality)

ribeiro et al. [32] proposed an extension of the centrality
algorithm described in section 2.4, which uses a two-stage im-
portant passage retrieval method. the    rst stage consists of
a feature-rich supervised key phrase extraction step, using the
maui toolkit with additional semantic features: the detection
of rhetorical signals, the number of named entities, part-of-
speech (pos) tags, and 4 id165 domain model probabilities
[20, 19]. the second stage consists of the extraction of the most
important passages, where key phrases are considered regular
passages.

2

2.6. graph random-walk with absorbing states that hops

among peaks for ranking (grasshopper)

grasshopper [40] is a re-ranking algorithm that maxi-
mizes diversity and minimizes redundancy. it takes a weighted
graph w (n    n: n vertexes representing sentences; weights
are de   ned by a similarity measure), a id203 distribu-
tion r (representing a prior ranking), and        [0, 1], that
balances the relative importance of w and r.
if there is no
prior ranking, a uniform distribution can be used. sentences
are ranked by applying the teleporting id93 method
in an absorbing markov chain, based on the n    n transi-
tion matrix   p (calculated by normalizing the rows of w ), i.e.,
p =      p + (1       ) 1r(cid:62). the    rst sentence to be scored is
the one with the highest stationary id203 arg maxn
i=1   i
according to the stationary distribution of p :    = p (cid:62)  . al-
ready selected sentences may never be visited again, by de   ning
pgg = 1 and pgi = 0,   i (cid:54)= g. the expected number of visits
   1 (where nij is the expected
is given by matrix n = (i     q)
number of visits to the sentence j, if the random walker began
at sentence i). we obtain the average of all possible starting
sentences to get the expected number of visits to the jth sen-
tence, vj. the sentence to be selected is the one that satis   es
arg maxn

i=|g|+1 vi.

3. datasets

we use three datasets: newspaper articles (baseline data),
   lms, and documentaries. film data consists of subtitles and
scripts, containing scene descriptions and dialog. documentary
data consists of subtitles containing mostly monologue. refer-
ence data consists of manual abstracts (for newspaper articles),
plot summaries (for    lms and documentaries), and synopses
(for    lms). plot summaries are concise descriptions, suf   cient
for the reader to get a sense of what happens in the    lm or docu-
mentary. synopses are much longer and may contain important
details concerning the turn of events in the story. all datasets
were normalized by removing punctuation inside sentences and
timestamps from subtitles.

3.1. newspaper articles

tem  ario [28] is composed by 100 newspaper articles in
brazilian portuguese (table 1), covering domains such as
   world   ,    politics   , and    foreign affairs   . each article has a
human-made reference summary (abstract).

table 1: tem  ario corpus properties.

#sentences news story
summary
news story
summary

#words

avg min max
29
9
608
192

68
18
1315
345

12
5
421
120

3.2. films

we collected 100    lms, with an average of 4 plot summaries
(minimum of 1, maximum of 7) and 1 plot synopsis per    lm

(table 2). table 3 presents the properties of the subtitles,
scripts, and the concatenation of both. not all the information
present in the scripts was used: dialogs were removed in order
to make them more similar to plot summaries.

table 2: properties of plot summaries and synopses.

#sentences

#words

avg min max

plot summaries
plot synopses
plot summaries
plot synopses

5
89
107
1677

1
6
14
221

29
399
600
7110

3

table 5: properties of the documentary plot summaries.

#sentences

#words

avg min max

informative
interrogative
inviting
challenge
informative
interrogative
inviting
challenge

4
4
6
5
82
103
146
104

1
1
2
2
26
40
63
59

18
19
11
9
384
377
234
192

4. experimental setup

table 3: properties of subtitles and scripts.

#sentences

#words

subtitles
script
script + subtitles
subtitles
script
script + subtitles

avg min max
1573
4065
3720
1367
5388
2787
27800
10460
14560
34700
47140
24640

309
281
1167
1592
3493
11690

3.3. documentaries

we collected 98 documentaries. table 4 presents the prop-
erties of their subtitles: note that the number of sentences is
smaller than in    lms, in   uencing id8 (recall-based) scores.

table 4: properties of documentaries subtitles.

#sentences
#words

avg min max
656
340
5864
10490

212
3961

we collected 223 manual plot summaries and divided them
into four classes (table 5): 143    informative   , 63    interrog-
ative   , 9    inviting   , and 8    challenge   .    informative    sum-
maries contain factual information about the program;    inter-
rogative    summaries contain questions that arouse viewer cu-
riosity, e.g.    what is the meaning of life?   ;    inviting    are in-
vitations, e.g.    got time for a 24 year vacation?   ; and,    chal-
lenge    entice viewers on a personal basis, e.g.    are you ready
for...?   . we chose    informative    summaries due to their resem-
blance to the sentences extracted by the summarization algo-
rithms. on average, there are 2 informative plot summaries per
documentary (minimum of 1, maximum of 3).

for news articles, summaries were generated with the aver-

age size of the manual abstracts (    31% of their size).

for each    lm, two summaries were generated, by selecting a
number of sentences equal to (i) the average length of its man-
ual plot summaries, and (ii) the length of its synopsis. in con-
trast with news articles and documentaries, three types of input
were considered: script, subtitles, script+subtitles.

for each documentary, a summary was generated with the
same average number of sentences of its manual plot summaries
(    1% of the documentary   s size).

content quality of summaries is based on word overlap (as
de   ned by id8) between generated summaries and their
references. id8-n computes the fraction of selected words
that are correctly identi   ed by the summarization algorithms
(cf. equation 5: rs are reference summaries, gramn is the n-
gram length, and countmatch(gramn) is the maximum number
of id165s of a candidate summary that co-occur with a set
of reference summaries). id8-su measures the overlap of
skip-bigrams (any pair of words in their sentence order, with
the addition of unigrams as counting unit). id8-su4 limits
the maximum gap length of skip-bigrams to 4.

id8-n =

gramn   s countmatch(gramn)

s   rs

gramn   s count(gramn)

(5)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

s   rs

5. results and discussion

subtitles and scripts were evaluated against manual plot sum-
maries and synopses to de   ne an optimal performance ref-
erence. the following sections present averaged id8-1,
id8-2, and id8-su4 scores (henceforth r-1, r-2, and
r-su4), and the performance of each summarization algorithm,
as a ratio between the score of the generated summaries and
this reference (relative performance). several parametrizations
of the algorithms were used (we present only the best results).
concerning mmr, we found that the best    corresponds to
a higher average number of words per summary. concerning
grasshopper, we used the uniform distribution as prior.

5.1. newspaper articles (tem  ario)

table 6 presents the scores for each summarization algo-
rithm. lsa achieved the best scores for r-1, r-2, and r-su4.
figure 1 shows the relative performance results.

table 6: id8 scores for generated summaries and original
documents against manual references. for mmr,    = 0.50;
support sets used manhattan distance and support set cardi-
nality = 2; kp-centrality used 10 key phrases.

mmr
support sets
kp
lsa
grassh.
lexrank
original docs

r-1
0.43
0.52
0.54
0.56
0.54
0.55
0.75

r-2 r-su4 avg #words
0.15
0.19
0.20
0.20
0.19
0.20
0.34

0.18
0.23
0.24
0.24
0.23
0.24
0.38

195
254
268
297
270
277
608

fig. 1: relative performance for news articles. for mmr,    =
0.50; support sets used manhattan distance and support set
cardinality = 2; kp-centrality used 10 key phrases.

5.2. films

4

table 7: id8 scores for generated summaries for subti-
tles, scripts, and scripts concatenated with subtitles, against plot
summaries. for mmr,    = 0.50; support sets used the co-
sine distance and threshold = 50%; kp-centrality used 50 key
phrases.

mmr

support sets

kp

lsa

grassh.

lexrank

original
docs

subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles

r-1
0.07
0.14
0.12
0.23
0.25
0.29
0.22
0.24
0.28
0.22
0.28
0.28
0.17
0.21
0.22
0.24
0.29
0.30
0.77
0.74
0.83

r-2
0.01
0.01
0.01
0.02
0.02
0.03
0.02
0.02
0.02
0.02
0.03
0.03
0.01
0.02
0.02
0.02
0.02
0.02
0.21
0.23
0.31

r-su4
0.02
0.03
0.03
0.06
0.07
0.09
0.06
0.07
0.08
0.06
0.08
0.08
0.04
0.06
0.06
0.06
0.09
0.08
0.34
0.36
0.43

avg #words

52
53
71
150
133
195
144
123
184
167
190
219
135
121
118
177
168
217
10460
14560
24640

table 8: id8 scores for generated summaries for subtitles,
scripts, and scripts+subtitles, against plot synopses. for mmr,
   = 0.50; support sets used the cosine distance and threshold
= 50%; kp-centrality used 50 key phrases.

mmr

support
sets

kp

lsa

grassh.

lexrank

original
docs

subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles
subtitles
script
script + subtitles

r-1
0.08
0.16
0.11
0.25
0.37
0.42
0.24
0.36
0.40
0.31
0.42
0.45
0.34
0.44
0.47
0.34
0.45
0.48
0.70
0.73
0.83

r-2
0.01
0.03
0.01
0.04
0.07
0.08
0.04
0.07
0.08
0.06
0.09
0.10
0.06
0.09
0.10
0.06
0.10
0.10
0.18
0.24
0.32

r-su4
0.02
0.06
0.03
0.08
0.15
0.16
0.08
0.14
0.16
0.11
0.17
0.18
0.12
0.18
0.19
0.12
0.18
0.19
0.30
0.37
0.44

avg #words

435
745
498
1033
1536
1736
952
1419
1580
1303
1934
2065
1553
1946
1768
1585
1975
2222
10460
14560
24640

table 7 presents the scores for the    lm data combina-
tions against plot summaries. overall, support sets, lsa,
and lexrank, capture the most relevant sentences for plot
summaries.
it would be expected, for algorithms such as
grasshopper and mmr, that maximize diversity, to per-
form well in this context, because plot summaries are relatively
small and focus on the more important aspects of the    lm, ide-
ally, without redundant content. however, our results show oth-
erwise. for scripts, lsa and lexrank are the best approaches
in terms of r-1 and r-su4.

table 8 presents the scores for the    lm data combinations
against plot synopses. the size of synopses is very different
from that of plot summaries. although synopses also focus on
the major events of the story, their larger size allows for a more
re   ned description of    lm events. additionally, because sum-
maries are created with the same number of sentences of the
corresponding synopsis, higher scores are expected. from all
algorithms, lexrank clearly stands out with the highest scores
for all metrics (except for r-su4, for scripts).

the script+subtitles combination was used in order to de-
termine whether the inclusion of redundant content would im-
prove the scores, over the separate use of scripts or subtitles.
however, in all cases (figure 4), script+subtitles leads to worse
scores, when compared to scripts alone. the same behavior
is observed when using subtitles except for support sets-based
methods (support sets and kp-centrality). for plot synopses,
the best scores are achieved by lexrank and grasshop-
per, while, for plot summaries, the best scores are achieved by
lexrank and lsa. by inspection of the summaries produced
by each algorithm, we observed that mmr chooses sentences
with fewer words in comparison with all other algorithms (nor-
mally, leading to lower scores). overall, the algorithms behave
similarly for both subtitles and scripts.

5.3. documentaries

from all algorithms (table 9), lsa achieved the best results
for r-1 and r-su4, along with lexrank for r-1. kp-centrality
achieved the best results for r-2. it is important to notice that
lsa also produces the summaries with the highest word count
(favoring recall). figure 2 shows the relative performance re-
sults: lsa outperformed all other algorithms for r-1 and r-
su4, and kp-centrality was the best for r-2; support sets and
kp-centrality performed closely to lsa for r-su4; the best
mmr results were consistently worse across all metrics (mmr
summaries have the lowest word count).

table 9: id8 scores for generated summaries and origi-
nal subtitles against human-made plot summaries. for mmr,
   = 0.75; support sets used the cosine distance and threshold
= 50%; kp-centrality used 50 key phrases.

mmr
support sets
kp
lsa
grassh.
lexrank
original docs

r-1
0.17
0.37
0.37
0.38
0.31
0.38
0.83

r-2 r-su4 avg #words
0.01
0.06
0.07
0.06
0.04
0.05
0.37

78
158
149
199
150
183
5864

0.04
0.12
0.12
0.13
0.10
0.12
0.46

fig. 2: relative performance for documentaries against plot
summaries. for mmr,    = 0.75; support sets used cosine dis-
tance and threshold=50%; kp-centrality used 50 key phrases.

5

5.4. discussion

news articles intend to answer basic questions about a partic-
ular event: who, what, when, where, why, and often, how. their
structure is sometimes referred to as    inverted pyramid   , where
the most essential information comes    rst. typically, the    rst
sentences provide a good overview of the entire article and are
more likely to be chosen when composing the    nal summary.
although documentaries follow a narrative structure similar to
   lms, they can be seen as more closely related to news than
   lms, especially regarding their intrinsic informative nature. in
spite of their different natures, however, summaries created by
humans produce similar scores for all of them. it is possible
to observe this behavior in figure 3. note that documentaries
achieve higher scores than news articles or    lms, when using
the original subtitles documents against the corresponding man-
ual plot summaries.

fig. 3: id8 scores for news articles,    lms, and documen-
taries against manual references, plot summaries and synopses,
and plot summaries, respectively.

figure 4 presents an overview of the performance of each
summarization algorithm across all domains. the results con-
cerning news articles were the best out of all three datasets for
all experiments. however, summaries for this dataset preserve,
approximately, 31% of the original articles, in terms of sen-
tences, which is signi   cantly higher than for    lms and docu-
mentaries (which preserve less than 1%), necessarily leading
to higher scores. nonetheless, we can observe the differences
in behavior between these domains. notably, documentaries
achieve the best results for plot summaries, in comparison with
   lms, using scripts, subtitles, or the combination of both. the
relative scores on the    lms dataset are in   uenced by two ma-
jor aspects: the short sentences found in the    lms dialogs; and,
since the generated summaries are extracts from subtitles and
scripts, they are not able to represent the    lm as a whole, in
contrast with what happens with plot summaries or synopses.
additionally, the experiments conducted for script+subtitles for
   lms, in general, do not improve scores above those of scripts
alone, except for support sets for r-1. overall, lsa performed
consistently better for news articles and documentaries. similar
relatively good behavior had already been observed for meeting

recordings, where the best summarizer was also lsa [26]. one
possible reason for these results is that lsa tries to capture the
relation between words in sentences. by inferring contextual
usage of text based on these relations, high scores, apart from
r-1, are produced for r-2 and r-su4. for    lms, lexrank was
the best performing algorithm for subtitles, scripts and the com-
bination of both, using plot synopses, followed by lsa and
support sets for plot summaries. mmr has the lowest scores
for all metrics and all datasets. we observed that sentences
closer to the centroid typically contain very few words, thus
leading to shorter summaries and the corresponding low scores.
interestingly, by observing the average of r-1, r-2, and r-
su4, it is possible to notice that it follows very closely the val-
ues of r-su4. these results suggest that r-su4 adequately
re   ects the scores of both r-1 and r-2, capturing the concepts
derived from both unigrams and bigrams.

overall,

summaries,

considering plot

documentaries
achieved higher results in comparison with    lms. however, in
general, the highest score for these two domains is achieved
using    lms scripts against plot synopses. note that synopses
have a signi   cant difference in terms of sentences in com-
parison with plot summaries. the average synopsis has 120
sentences, while plot summaries have, on average, 5 sentences
for    lms, and 4 for documentaries. this gives synopses a clear
advantage in terms of id8 (recall-based) scores, due to the
high count of words.

6. conclusions and future work

we analyzed the impact of the six summarization algorithms
on three datasets. the newspaper articles dataset was used as a
reference. the other two datasets, consisting of    lms and docu-
mentaries, were evaluated against plot summaries, for    lms and
documentaries, and synopses, for    lms. despite the different
nature of these domains, the abstractive summaries created by
humans, used for evaluation, share similar scores across met-
rics.

the best performing algorithms are lsa, for news and doc-
umentaries, and lexrank for    lms. moreover, we conducted
experiments combining scripts and subtitles for    lms, in order
to assess the performance of generic algorithms by inclusion of
redundant content. our results suggest that this combination
is unfavorable. additionally, it is possible to observe that all
algorithms behave similarly for both subtitles and scripts. as
previously mentioned, the average of the scores follows closely
the values of r-su4, suggesting that r-su4 is able to capture
concepts derived from both unigrams and bigrams.

we plan to use subtitles as a starting point to perform video
summaries of    lms and documentaries. for    lms, the results
from our experiments using plot summaries show that the sum-
marization of scripts only marginally improved performance,
in comparison with subtitles. this suggests that subtitles are
a viable approach for text-driven    lm and documentary sum-
marization. this positive aspect is compounded by their being
broadly available, as opposed to scripts.

6

7. acknowledgements

this work was supported by national

funds through
fundac    ao para a ci  encia e a tecnologia (fct) with reference
uid/cec/50021/2013.

references

[1] ajmal, m., ashraf, m., shakir, m., abbas, y., shah, f., 2012. video
summarization: techniques and classi   cation, in: id161 and
graphics. springer berlin heidelberg, pp. 1   13.

[2] barzilay, r., elhadad, n., mckeown, k., 2002. inferring strategies for
sentence ordering in multidocument news summarization. journal of ar-
ti   cial intelligence research , 35   55.

[3] brin, s., page, l., 1998. the anatomy of a large-scale hypertextual
web search engine, in: proc. of the 7th intl. conf. on world wide web,
pp. 107   117.

[4] carbonell, j., goldstein, j., 1998. the use of mmr, diversity-based
reranking for reordering documents and producing summaries, in:
proc. of the 21st annual intl. acm sigir conf. on research and de-
velopment in information retrieval, pp. 335   336.

[5] edmundson, h.p., 1969. new methods in automatic abstracting. journal

of the association for computing machinery 16, 264   285.

[6] erkan, g., radev, d.r., 2004. lexrank: graph-based lexical central-
ity as salience in text summarization. journal of arti   cial intelligence
research , 457   479.

[7] fujii, y., kitaoka, n., nakagawa, s., 2007. automatic extraction of cue
phrases for important sentences in lecture speech and automatic lecture
speech summarization., in: proc. of interspeech 2007, pp. 2801   
2804.

[8] garg, n., favre, b., reidhammer, k., hakkani-t  ur, d., 2009. cluster-
rank: a graph based method for meeting summarization, in: proc. of
interspeech 2009, pp. 1499   1502.

[9] gong, y., liu, x., 2001. generic text summarization using relevance
measure and latent semantic analysis, in: proc. of the 24th annual
intl. acm sigir conf. on research and development in information
retrieval, pp. 19   25.

[10] grant, b.k., sloniowski, j., 1998. documenting the documentary: close
readings of documentary film and video. wayne state university press.
[11] hong, k., conroy, j.m., favre, b., kulesza, a., lin, h., nenkova, a.,
2014. a repository of state of the art and competitive baseline summaries
for generic news summarization, in: proc. of the ninth intl. conf. on
language resources and evaluation (lrec-2014), reykjavik, iceland,
may 26-31, 2014., pp. 1608   1616.

[12] kazantseva, a., szpakowicz, s., 2010. summarizing short stories. com-

putational linguistics 36, 71   109.

[13] landauer, t.k., dutnais, s.t., 1997. a solution to plato   s problem: the
latent semantic analysis theory of acquisition, induction, and representa-
tion of knowledge. psychological review 104, 211   240.

[14] landauer, t.k., foltz, p.w., laham, d., 1998. an introduction to latent

semantic analysis. discourse processes 25, 259   284.

[15] lin, c.y., 2004. id8: a package for automatic evaluation of sum-
maries, in: text summ. branches out: proc. of the acl-04 workshop,
pp. 74   81.

[16] lin, c.y., hovy, e., 2000. the automated acquisition of topic signatures
for text summarization, in: proc. of the 18th conf. on computational
linguistics - volume 1, pp. 495   501.

[17] liu, f., liu, y., 2010. exploring correlation between id8 and human
evaluation on meeting summaries. ieee transactions on audio, speech
& language processing 18, 187   196.

[18] luhn, h.p., 1958. the automatic creation of literature abstracts. ibm

journal of research and development 2, 159   165.

[19] marujo, l., gershman, a., carbonell, j., frederking, r., neto, j.p.,
2012. supervised topical key phrase extraction of news stories using
id104, light    ltering and co-reference id172, in: chair),
n.c.c., choukri, k., declerck, t., do  gan, m.u., maegaard, b., mari-
ani, j., moreno, a., odijk, j., piperidis, s. (eds.), proceedings of the
eight international conference on language resources and evaluation
(lrec   12), european language resources association (elra).

[20] marujo, l., viveiros, m., neto, j.p., 2011. keyphrase cloud generation of

broadcast news., in: interspeech, isca. pp. 2393   2396.

7

fig. 4: relative performance for all datasets. for    lms the relative performance was measured against plot synopses and plot
summaries: mmr used    = 0.50; and support sets used the cosine distance and threshold = 50%; kp-centrality used 50 key
phrases.

bonell, j., 2013. self reinforcement for important passage retrieval. digi-
tal. url: http://dl.acm.org/citation.cfm?id=2484028.
2484134.

[33] ribeiro, r., de matos, d., 2007. extractive summarization of broadcast
news: comparing strategies for european portuguese., in: tsd, pp. 115   
122.

[34] ribeiro, r., de matos, d., 2012. summarizing speech by contextual re-
inforcement of important passages, in: proc. of propor 2012, pp. 392   
402.

[35] ribeiro, r., de matos, d.m., 2011. revisiting centrality-as-relevance:
support sets and similarity as geometric proximity. journal of arti   cial
intelligence research , 275   308.

[36] sang, j., xu, c., 2010. character-based movie summarization, in: proc.

of the intl. conf. on multimedia, pp. 855   858.

[37] sp  arck jones, k., 2007. automatic summarising: the state of the art. inf.

process. manage. 43, 1449   1481.

[38] xie, s., liu, y., 2008. using corpus and knowledge-based similarity
measure in maximum marginal relevance for meeting summarization, in:
proc. - icassp, ieee intl. conf. on acoustics, speech and signal pro-
cessing, pp. 4985   4988.

[39] zhang, j.j., chan, r.h.y., fung, p., 2010. extractive speech summariza-
tion using shallow rhetorical structure modeling. ieee transactions on
audio speech and language processing 18, 1147   1157.

[40] zhu, x., goldberg, a.b., gael, j.v., andrzejewski, d., 2007. improving
diversity in ranking using absorbing id93, in: proc. of the
5th naacl - hlt, pp. 97   104.

[21] maskey, s.r., hirschberg, j., 2005.

comparing lexical, acous-
tic/prosodic, structural and discourse features for speech summariza-
tion, in: proc. of the 9th eurospeech - interspeech 2005, pp.
621   624.

[22] mckeown, k., hirschberg, j., galley, m., maskey, s., 2005. from text
to speech summarization, in: acoustics, speech, and signal processing,
2005. proceedings. (icassp    05). ieee intl. conf. on, pp. v/997   v1000
vol. 5.

[23] mckeown, k.r., barzilay, r., evans, d., hatzivassiloglou, v., klavans,
j.l., nenkova, a., sable, c., schiffman, b., sigelman, s., summariza-
tion, m., 2002. tracking and summarizing news on a daily basis with
columbia   s newsblaster, in: proc. of hlt 2002, pp. 280   285.

[24] mihalcea, r., ceylan, h., 2007. explorations in automatic book summa-

rization, in: emnlp-conll   07, pp. 380   389.

[25] murray, g., renals, s., carletta, j., 2005a. extractive summarization
of meeting recordings, in: proc. of the 9th european conf. on speech
communication and technology, pp. 593   596.

[26] murray, g., renals, s., carletta, j., 2005b. extractive summarization of
meeting records, in: proc. of the 9th eurospeech - interspeech
2005, pp. 593   596.

[27] nichols, b., 1991. representing reality: issues and concepts in docu-

mentary. bloomington: indiana university press.

[28] pardo, t.a.s., rino, l.h.m., 2003.

temario: a corpus for auto-
matic text summarization. technical report. n  ucleo interinstitucional
de lingu    stica computacional (nilc).

[29] radev, d.r., blair-goldensohn, s., zhang, z., raghavan, r.s., 2001.
newsinessence: a system for domain-independent, real-time news
id91 and id57, in: proc. of the first intl.
conf. on human language technology research, pp. 1   4.

[30] radev, d.r., otterbacher, j., winkel, a., blair-goldensohn, s., 2005.
newsinessence: summarizing online news topics. communications of
the acm 48, 95   98.

[31] raposo, f., ribeiro, r., de matos, d.m., 2015. on the application of
ieee signal processing

generic summarization algorithms to music.
letters , 26   30.

[32] ribeiro, r., marujo, l., de matos, d., neto, j.p., gershman, a., car-

