 

 

leveraging id27s for spoken document summarization 
kuan-yu chen*   , shih-hung liu*, hsin-min wang*, berlin chen#, hsin-hsi chen    

*institute of information science, academia sinica, taiwan 

#national taiwan normal university, taiwan 

   national taiwan university, taiwan 

*{kychen, journey, whm}@iis.sinica.edu.tw, #berlin@ntnu.edu.tw,    hhchen@csie.ntu.edu.tw 

without human annotations involved. popular methods include 
the vector space model (vsm) [9], the latent semantic analysis 
(lsa) method [9], the markov random walk (mrw) method 
[10],  the  maximum  marginal  relevance  (mmr)  method [11], 
the  sentence  significant  score  method  [12],  the  unigram 
language  model-based  (ulm)  method    [4],  the  lexrank 
method  [13],  the  submodularity-based  method  [14],  and  the 
integer  linear  programming  (ilp)  method  [15].  statistical 
features  may  include  the  term  (word)  frequency,  linguistic 
score, 
recognition  confidence  measure,  and  prosodic 
information.  in  contrast,  supervised  sentence  classification 
methods, such as the gaussian mixture model (gmm) [9], the 
bayesian  classifier  (bc)  [16],  the  support  vector  machine 
(id166)  [17],  and  the  conditional  random  fields  (crfs)  [18], 
usually formulate sentence selection as a binary classification 
problem, i.e., a sentence can either be included in a summary 
or not. interested readers may refer to [4-7] for comprehensive 
reviews  and  new  insights  into  the  major  methods  that  have 
been developed and applied with good success to a wide range 
of text and speech summarization tasks.  

is 

to 

different from the above methods, we explore in this paper 
various id27 methods [19-22] for use in extractive 
sds, which have recently demonstrated excellent performance 
in many natural language processing (nlp)-related tasks, such 
as  relational  analogy  prediction,  sentiment  analysis,  and 
sentence  completion  [23-26].  the  central  idea  of  these 
methods 
learn  continuously  distributed  vector 
representations  of  words  using  neural  networks,  which  can 
probe latent semantic and/or syntactic cues that can in turn be 
used  to  induce  similarity  measures  among  words,  sentences, 
and  documents.  a  common  thread  of  leveraging  word 
embedding  methods  to  nlp-related  tasks  is  to  represent  the 
document  (or  query  and  sentence)  by  averaging  the  word 
embeddings  corresponding  to  the  words  occurring  in  the 
document  (or  query  and  sentence).  then,  intuitively,  the 
cosine  similarity  measure  can  be  applied  to  determine  the 
relevance degree between a pair of representations. however, 
such  a  framework  ignores  the  inter-dimensional  correlation 
between  the  two  vector  representations.  to  mitigate  this 
deficiency,  we  further  propose  a  novel  use  of  the  triplet 
learning  model  to  enhance  the  estimation  of  the  similarity 
degree  between  a  pair  of  representations.  in  addition,  since 
most id27 methods are founded on a probabilistic 
objective function, a probabilistic similarity measure might be 
a  more  natural  choice 
than  non-probabilistic  ones. 
consequently,  we  also  propose  a  new  language  model-based 
framework, which incorporates the id27 methods 
with  the  document  likelihood  measure.  to  recapitulate, 
beyond the continued and tremendous efforts made to improve 
the  representation  of  words,  this  paper  focuses  on  building 
novel and efficient ranking models on top of the general word 
embedding methods for extractive sds.  

abstract 

owing to the rapidly growing multimedia content available on 
the internet, extractive spoken document summarization, with 
the  purpose  of  automatically  selecting  a  set  of  representative 
sentences  from  a  spoken  document  to  concisely  express  the 
most important theme of the document, has been an active area 
of  research  and  experimentation.  on  the  other  hand,  word 
embedding has  emerged  as  a  newly  favorite  research  subject 
because of its excellent performance in many natural language 
processing  (nlp)-related  tasks.  however,  as  far  as  we  are 
aware, there are relatively few studies investigating its use in 
extractive text or speech summarization. a common thread of 
leveraging id27s in the summarization process is 
to represent the document (or sentence) by averaging the word 
embeddings  of  the  words  occurring  in  the  document  (or 
sentence). then, intuitively, the cosine similarity measure can 
be employed to determine the relevance degree between a pair 
of  representations.  beyond  the  continued  efforts  made  to 
improve  the  representation  of  words,  this  paper  focuses  on 
building  novel  and  efficient  ranking  models  based  on  the 
general  word  embedding  methods  for  extractive  speech 
summarization.  experimental 
the 
effectiveness  of  our  proposed  methods,  compared  to  existing 
state-of-the-art methods. 
index  terms:  spoken  document,  summarization,  word 
embedding, ranking model 

results  demonstrate 

1.  introduction 

owing  to  the  popularity  of  various  internet  applications, 
rapidly  growing  multimedia  content,  such  as  music  video, 
broadcast  news  programs,  and  lecture  recordings,  has  been 
continuously filling our daily life [1-3]. obviously, speech is 
one  of  the  most  important  sources  of  information  about 
multimedia.  by  virtue  of  spoken  document  summarization 
(sds),  one  can  efficiently  digest  multimedia  content  by 
listening  to  the  associated  speech  summary.  extractive  sds 
manages to select a set of indicative sentences from a spoken 
document  according  to  a  target  summarization  ratio  and 
concatenate them together to form a summary [4-7]. the wide 
spectrum of extractive sds methods developed so far may be 
divided into three categories [4, 7]: 1) methods simply based 
on the sentence position or structure information, 2) methods 
based on unsupervised sentence ranking, and 3) methods based 
on supervised sentence classification. 

for the first category, the important sentences are selected 
from some salient parts of a spoken document [8], such as the 
introductory and/or concluding parts. however, such methods 
can  be  only  applied  to  some  specific  domains  with  limited 
document structures. unsupervised sentence ranking methods 
attempt  to  select  important  sentences  based  on  the  statistical 
features  of  the  sentences  or  of  the  words  in  the  sentences 

 

 

2.  review of id27 methods 
perhaps one of the most-known seminal studies on developing 
id27 methods was presented in [19]. it estimated a 
statistical  (id165)  language  model,  formalized  as  a  feed-
forward neural network, for predicting future words in context 
while inducing id27s (or representations) as a by-
product. such an attempt has already motivated many follow-
up  extensions  to  develop  similar  methods  for  probing  latent 
semantic  and  syntactic  regularities  in  the  representation  of 
words. representative methods include, but are not limited to, 
the  continuous  bag-of-words  (cbow)  model  [21],  the  skip-
gram  (sg)  model  [21,  27],  and  the  global  vector  (glove) 
model [22]. as far as we are aware, there is little work done to 
contextualize these methods for use in speech summarization. 
2.1. the continuous bag-of-words (cbow) model 
rather than seeking to learn a statistical language model, the 
cbow model manages to obtain a dense vector representation 
(embedding)  of  each  word  directly  [21].  the  structure  of 
cbow is similar to a  feed-forward neural network, with the 
exception  that  the  non-linear  hidden  layer  in  the  former  is 
removed.  by  getting  around  the heavy  computational burden 
incurred  by  the  non-linear  hidden  layer,  the  model  can  be 
trained  on  a  large  corpus  efficiently,  while  still  retains  good 
performance.  formally,  given  a 
sequence  of  words, 
w1,w2,   ,wt, the objective function of cbow is to maximize 
the log-id203, 
wwp
(
|

,...,

,...,

(1) 

ct
+

ct
   

1
+

1
   

w

w

w

   

,)

,

t

t

t

t
    =
t

log1

where c is the window size of context words being considered 
for  the  central  word  wt,  t  denotes  the  length  of  the  training 
corpus, and  

t

wwp
(

|

ct
   

,...,

w

t

1
   

,

w

t

1
+

,...,

w

ct
+

)

exp(

v
t
w
v
exp(

= v
    =
i
1

t
w
v
   

)

w
i

   

v

w

t
 

,

)
(2) 

where                          denotes the vector representation of the word w at 
position t; v is the size of the vocabulary; and                             denotes the 

(weighted) average of the vector representations of the context 
words of wt [21, 26]. the concept of cbow is motivated by 
the distributional hypothesis [27], which states that words with 
similar meanings often occur in similar contexts, and it is thus 
suggested  to  look  for  wt  whose  word  representation  can 
capture its context distributions well. 
2.2. the skip-gram (sg) model 
in  contrast  to  the  cbow  model,  the  sg  model  employs  an 
inverse  training  objective  for  learning  word  representations 
with a simplified feed-forward neural network [21, 28]. given 
the word sequence, w1,w2,   ,wt, the objective function of sg 
is to maximize the following log-id203, 

t
    =    
t
1

c
j

   =

jc
,

   

0

log

wp
(

t

+

j

t

|

w

 

,)

 

 

(3) 

where c is the window size of the context words for the central 
word wt, and the id155 is computed by 

 

(4) 

wp
(

t

+

j

t

|

w

)

=

exp(
v
1    =
i

v
t
+
w
v
exp(

   

j

)

t

v
   

w
v

 

,

t

)

w

w
i

where                         +          and                           denote  the  word  representations  of  the 

words  at  positions 
the 
implementations of cbow and sg, the hierarchical soft-max 
algorithm [28, 29] and the negative sampling algorithm [28, 30] 
can make the training process more efficient and effective. 

respectively. 

t+j  and 

in 

t, 

 

j

j

j

j

1

w

v

+

+

   

x

)(

w
i

b
w

(5) 

log

ww
i

ww
i

2
,)

b
w
i

xf
(

v
v
    =     =
i
j
1

2.3. the global vector (glove) model 
the  glove  model  suggests  that  an  appropriate  starting  point 
for word representation learning should be associated with the 
ratios of co-occurrence probabilities rather than the prediction 
probabilities  [22].  more  precisely,  glove  makes  use  of 
weighted least squares regression, which aims at learning word 
representations  by  preserving  the  co-occurrence  frequencies 
between each pair of words: 
v
   

where                                          denotes the number of times words wi and wj co-

occur  in  a  pre-defined  sliding  context  window;  f(    )  is  a 
monotonic smoothing function used to modulate the impact of 
each pair of words involved in the model training; and vw and 
bw denote the word representation and the bias term of word w, 
respectively. 
2.4. analytic comparisons 
there  are  several  analytic  comparisons  can  be  made  among 
the  above  three  word  embedding  methods.  first,  they  have 
different model structures and learning strategies. cbow and 
sg  adopt  an  on-line  learning  strategy,  i.e.,  the  parameters 
(word representations) are trained sequentially. therefore, the 
order  that  the  training  samples  are  used  may  change  the 
resulting models dramatically. in contrast, glove uses a batch 
learning  strategy,  i.e.,  it  accumulates  the  statistics  over  the 
entire  training  corpus  and  updates  the  model  parameters  at 
once.  second,  it  is  worthy  to  note  that  sg  (trained  with  the 
negative 
an 
implicit/explicit  relation  with  the  classic  weighted  matrix 
factorization  approach,  while  the  major  difference  is  that  sg 
and  glove  concentrate  on  rendering  the  word-by-word  co-
occurrence matrix but weighted id105 is usually 
concerned with decomposing the word-by-document matrix [9, 
31, 32].  

and  glove  have 

algorithm) 

sampling 

the observations made above on the relation between word 
embedding  methods  and  matrix  factorization  bring  us  to  the 
notion of leveraging the singular value decomposition (svd) 
method  as  an  alternative  mechanism  to  derive  the  word 
embeddings  in  this  paper.  given  a  training  text  corpus,  we 
have  a  word-by-word  co-occurrence  matrix  a.  each  element 
aij of a is the log-frequency of times words wi and wj co-occur 
in  a  pre-defined  sliding  context  window  in  the  corpus. 
subsequently, svd decomposes a into three sub-matrices: 

,~
t a

(6) 

      

vua

where  u  and  v  are  orthogonal  matrices,  and    is  a  diagonal 

matrix.  finally,  each  row  vector  of  matrix  u (or  the  column 
vector  of  matrix  vt,  u=v  since  a  is  a  symmetric  matrix) 
designates  the  word  embedding  of  a  specific  word  in  the 
vocabulary. it is worthy to note that using svd to derive the 
word  representations  is  similar  in  spirit  to  latent  semantic 
analysis (lsa) but using the word-word co-occurrence matrix 
instead of the word-by-document co-occurrence matrix [33]. 
3.  sentence ranking based on word 

=

 

 

 

 

embeddings 

3.1. the triplet learning model 
inspired by the vector space model (vsm), a straightforward 
way  to  leverage  the  word  embedding  methods  for  extractive 
sds  is  to  represent  a  sentence  si  (and  a  document  d  to  be 
summarized) by averaging the vector representations of words 
occurring in the sentence si (and the document d) [23, 25]: 

 

 

(7) 

id203 of word wj given another word wi can be calculated 
by 

 

v

s

i

=

       
sw

i

)

swn
(
,
i
s
|
|
i

 

v

.

w

by doing so, the document d and each sentence si of d will 
have a respective fixed-length dense vector representation, and 
their  relevance  degree  can  be  evaluated  by  the  cosine 
similarity measure. 

however,  such  an  approach  ignores  the  inter-dimensional 
correlation  between  two  vector  representations.  to  mitigate 
the deficiency of the cosine similarity measure, we  employ a 
triplet  learning  model  to  enhance  the  estimation  of  the 
similarity  degree  between  a  pair  of  representations  [34-36]. 
without  loss  of  generality,  our  goal  is  to  learn  a  similarity 
function,  r(    ,    ),  which  assigns  higher  similarity  scores  to 
summary sentences than to non-summary sentences, i.e., 

j

 

 

  

(8) 

   
v
,
sd

+ >
i

)

(

(

v

v

),

r

r

v
,
sd

representation for a non-summary sentence sj. the parametric 
ranking function has a bi-linear form as follows: 

where                         + denotes the sentence representation (in the form of a 
column  vector)  for  a  summary  sentence  si,  while                              is  the 
where                                  ,  and  d  is  the  dimension  of  the  vector 

representation.  by  applying  the  passive-aggressive  learning 
algorithm  presented  in  [34],  we  can  derive  the  similarity 
function r such that all triplets obey 

v
sdr
(

wv

(9) 

t
d

   

v

v

s

)

,

 

 

 

 

r

(

v

+
v
,
sd
i

)

>

r

(

v

   
v
,
sd

j

)

.
  +

 

 

 

(10) 

j

,

(

)

(

)

(

v

v

v

v

=

+

   
s

r

r

   
  

loss

   
v
,
sd

,0max{

+
v
,
sd
i

+
v
,
sd
i

that  is,  not  only  the  similarity  function  will  distinguish 
summary  and  non-summary  sentences,  but  also  there  is  a 

safety  margin  of           between  them.  with          ,  a  hinge  loss 

function can be defined as 

)}.
j
(11) 
then, w can be obtained by applying an efficient sequential 
learning  algorithm  iteratively  over  the  triplets [34,  35].  with 
w, sentences can be ranked in descending order of similarity 
measure, and the top sentences will be selected and sequenced 
to form a summary according to a target summarization ratio. 
3.2. the document likelihood measure 
a recent school of thought for extractive sds is to employ a 
language  modeling  (lm)  approach  for  the  selection  of 
important  sentences.  a  principal  realization  is  to  use  a 
probabilistic generative paradigm for ranking each sentence s 
of  a  document  d,  which  can  be  expressed  by  p(d|s).  the 
simplest way is to estimate a unigram language model (ulm) 
based on the frequency of each distinct word w occurring in s, 
with the maximum likelihood (ml) criterion [37, 38]: 

swp
(

|

)

=

)

swn
(
|
|

,
s

 

,

 

 

(12) 

where n(w,s) is the number of times that word w occurs in s 
and  |s|  is  the  length  of  s.  obviously,  one  major  challenge 
facing  the  lm  approach  is  how  to  accurately  estimate  the 
model parameters for each sentence. 

stimulated by the document likelihood measure adopted by 
the ulm method, for the various word representation methods 
studied  in  this  paper,  we  can  construct  a  new  word-based 
language  model  for  predicting  the  occurrence  id203  of 
any  arbitrary  word  wj.  taking  cbow  as  an  example,  the 

 

wwp
(
i

|

j

)

=

exp(

v
   
w
j
v
exp(

v

w
l

)
w
i
v
   

)

w
i

  

.

 

(13) 

       
vw
l

as  such,  we  can  linearly  combine  the  associated  word-based 
language models of the words occurring in sentence s to form 
a  composite  sentence-specific  language  model  for  s,  and 
express the document likelihood measure as 

dwn
(

,

j

)

   
   
   
   

   
sw
   
i

 

,

(14) 

sdp
(

|

)

=

       
dw

  
w
i

   

wwp
(
i

|

j

)

j

frequency  of  word  wi  occurring  in  sentence  s,  subject  to 

where the weighting factor                          is set to be proportional to the 
   
                        
=1 .  the  sentences  offering  higher  document 
                           

likelihoods  will  be  selected  and  sequenced  to  form  the 
summary according to a target summarization ratio. 

   
   
   
   

4.  experimental setup 

the dataset used in this study is the matbn broadcast news 
corpus  collected  by  the  academia  sinica  and  the  public 
television service foundation of taiwan between november 
2001 and april 2003 [39]. the corpus has been segmented into 
separate stories and transcribed manually. each story contains 
the  speech  of  one  studio  anchor,  as  well  as  several  field 
reporters  and  interviewees.  a  subset  of  205  broadcast  news 
documents  compiled  between  november  2001  and  august 
2002  was  reserved  for  the  summarization  experiments.  we 
chose  20  documents  as  the  test  set  while  the  remaining  185 
documents  as  the  held-out  development  set.  the  reference 
summaries  were  generated  by  ranking  the  sentences  in  the 
manual  transcript  of  a  spoken  document  by  importance 
without assigning a score to each sentence. each document has 
three reference summaries annotated by three subjects. for the 
assessment  of  summarization  performance,  we  adopted  the 
widely-used  id8  metrics  [40].  all  the  experimental 
results  reported  hereafter  are  obtained  by  calculating  the  f-
scores [17] of these id8 metrics. the summarization ratio 
was  set  to  10%.  a  corpus  of  14,000  text  news  documents, 
compiled  during  the  same  period  as  the  broadcast  news 
documents, was used to estimate related models compared in 
this paper, which are cbow, sg, glove, and svd. a subset 
of  25-hour  speech  data  from  matbn  compiled  from 
november 2001 to december 2002 was used to bootstrap the 
acoustic  training  with  the  minimum  phone  error  rate  (mpe) 
criterion  and  a  training  data  selection  scheme  [41].  the 
vocabulary size is about 72 thousand words. the average word 
error rate of automatic transcription is about 40%. 
5.  experimental results 

at  the  outset,  we  assess  the  performance  levels  of  several 
well-practiced  or/and  state-of-the-art  summarization  methods 
for extractive sds, which will serve as the baseline systems in 
this  paper,  including  the  lm-based  summarization  method 
(i.e., ulm, c.f. eq. (12)), the vector-space methods (i.e., vsm, 
lsa,  and  mmr),  the  graph-based  methods  (i.e.,  mrw  and 
lexrank),  the  submodularity  method  (sm),  and  the  integer 
linear  programming  (ilp)  method.  the  results  are  illustrated 
in table 1, where td denotes the results obtained based on the 
manual  transcripts  of  spoken  documents  and  sd  denotes  the 
results  using  the  speech  recognition  transcripts  that  may 
contain  recognition  errors.  several  noteworthy  observations 
can  be  drawn  from  table  1.  first,  the  two  graph-based 
methods (i.e., mrw and lexrank) are quite competitive with 
each  other  and perform  better  than  the  vector-space  methods 

 

table 1. summarization results achieved by several well-studied 

or/and state-of-the-art unsupervised methods. 
text documents (td) 

spoken documents (sd) 

id8-1  id8-2 id8-l id8-1 id8-2 id8-l 

 

method 
0.411 
ulm 
0.347 
vsm 
0.362 
lsa 
0.368 
mmr 
mrw 
0.412 
lexrank  0.413 
0.414 
0.442 

sm 
ilp 

0.298 
0.228 
0.233 
0.248 
0.282 
0.309 
0.286 
0.337 

0.361 
0.290 
0.316 
0.322 
0.358 
0.363 
0.363 
0.401 

0.364 
0.342 
0.345 
0.366 
0.332 
0.305 
0.332 
0.348 

0.210 
0.189 
0.201 
0.215 
0.191 
0.146 
0.204 
0.209 

0.307 
0.287 
0.301 
0.315 
0.291 
0.254 
0.303 
0.306 

 

(i.e.,  vsm,  lsa,  and  mmr)  for  the  td  case.  however,  for 
the sd case, the situation is reversed. it reveals that imperfect 
id103 may affect the graph-based methods more 
than  the  vector-space  methods;  a  possible  reason  for  such  a 
phenomenon is that the id103 errors may lead to 
inaccurate similarity measures between each pair of sentences. 
the id95-like procedure of the graph-based methods, in  
turn, will be performed based on these problematic measures, 
potentially  leading  to  degraded  results.  second,  lsa,  which 
represents  the  sentences  of  a  spoken  document  and  the 
document  itself  in  the  latent  semantic  space  instead  of  the 
index term (word) space, performs slightly better than vsm in 
both the td and sd cases. third, sm and ilp achieve the best 
results in the td case, but only have comparable performance 
to  other  methods  in  the  sd  case.  finally,  ulm  shows 
competitive results compared to other state-of-the-art methods, 
confirming 
the  applicability  of 
language  modeling 
approach for speech summarization. 

the 

we now turn to investigate the utilities of three state-of-the-
art  word  embedding  methods  (i.e.,  cbow,  sg,  and  glove) 
and  the  proposed svd  method  (c.f.  section 2.4),  working  in 
conjunction  with  the  cosine  similarity  measure  for  speech 
summarization.  the  results  are  shown  in  table  2.  from  the 
results, several observations can be made. first, the three state-
of-the-art  word  embedding  methods  (i.e.,  cbow,  sg,  and 
glove),  though  with  disparate  model  structures  and  learning 
strategies, achieve comparable results to each other in both the 
td  and  sd  cases.  although  these  methods  outperform  the 
conventional vsm model, they only achieve almost the same 
level  of  performance  as  lsa  and  mmr,  two  improved 
versions  of  vsm,  and  perform  worse  than  mrw,  lexrank, 
sm,  and  ilp  in  the  td  case.  to  our  surprise,  the  proposed 
svd method outperforms other id27 methods by 
a substantial margin in the td case and slightly in the sd case. 
it should be noted that the svd method outperforms not only 
cbow,  sg,  and  glove,  but  also  lsa  and  mmr.  it  even 
outperforms  all  the  methods  compared  in  table  1  in  the  sd 
case. 

learning  model  outperforms 

in the next set of experiments, we evaluate the capability of 
the  triplet  learning  model  for  improving  the  measurement  of 
similarity when applying id27 methods in speech 
summarization.  the  results  are  shown  in  table  3.  from  the 
table, two observations can be drawn. first, it is clear that the 
triplet 
the  baseline  cosine 
similarity  measure  (c.f.  table  2)  in  all  cases.  this  indicates 
that triplet learning is able to improve the measurement of the 
similarity  degree  for  sentence  ranking  and  considering  the 
inter-dimensional  correlation 
the  similarity  measure 
between  two  vector  representations  is  indeed  beneficial. 
second,     cbow  with  triplet  learning     outperforms  all  the 
methods  compared  in  table 1  in  both  the  td  and  sd  cases. 
however,  we  have  to  note  that  learning  w  in  eq.  (9) has  to 
resort  to  a  set  of  documents  with  reference  summaries;  thus 
the comparison is unfair since all the methods in table 1 are  
unsupervised ones. we used the development set (c.f. section 
4) to learn w. so far, we have not figured out systematic and 

in 

 

table 2. summarization results achieved by various word-embedding 

methods in conjunction with the cosine similarity measure. 
 

text documents (td) 

spoken documents (sd) 

method 
cbow 

sg 

glove 
svd 

id8-1 id8-2 id8-l id8-1 id8-2 id8-l 

0.369 
0.367 
0.367 
0.409 

0.224 
0.230 
0.231 
0.265 

0.308 
0.306 
0.308 
0.342 

0.365 
0.358 
0.364 
0.374 

0.206 
0.205 
0.214 
0.215 

0.313 
0.303 
0.312 
0.319 

table 3. summarization results achieved by various word-embedding 

methods in conjunction with the triplet learning model. 

 

text documents (td) 

spoken documents (sd) 

method 
cbow 

sg 

glove 
svd 

id8-1 id8-2 id8-l id8-1 id8-2 id8-l 

0.472 
0.404 
0.372 
0.422 

0.367 
0.284 
0.248 
0.303 

0.432 
0.348 
0.315 
0.364 

0.396 
0.374 
0.375 
0.376 

0.258 
0.223 
0.225 
0.223 

0.347 
0.321 
0.319 
0.323 

table 4. summarization results achieved by various word-embedding 

methods in conjunction with the document likelihood measure. 
spoken documents (sd) 

text documents (td) 

 

method 
cbow 

sg 

glove 
svd 

id8-1 id8-2 id8-l id8-1 id8-2 id8-l 

0.456 
0.436 
0.422 
0.411 

0.342 
0.320 
0.309 
0.298 

0.398 
0.385 
0.372 
0.361 

0.385 
0.371 
0.380 
0.364 

0.237 
0.225 
0.239 
0.222 

0.333 
0.322 
0.332 
0.313 

 
effective  ways  to  incorporate  word  embeddings  into  existing 
supervised  speech  summarization  methods.  we  leave  this  as 
our future work.  

in the last set of experiments, we pair the id27 
methods with the document likelihood measure for extractive 
sds.  the  deduced  sentence-based  language  models  were 
linearly  combined  with  ulm  in  computing  the  document 
likelihood using eq. (14) [40]. the results are shown in table 
4.  comparing  the  results  to  that  of  the  word  embedding 
methods paired with the cosine similarity measure (c.f. table 
2),  it  is  evident  that  the  document  likelihood  measure  works 
pretty well as a vehicle to leverage id27 methods 
for  speech  summarization.  we  also  notice  that  cbow 
outperforms the other three id27 methods in both 
the td and sd cases, just as it had done previously in table 3 
when  combined  with  triplet  learning,  whereas     svd  with 
document 
the 
superiority  as     svd  with  triplet  learning     (c.f.  table  3). 
moreover, comparing the results with that of various state-of-
the-art  methods  (c.f.  table  1),  the  word  embedding  methods 
with the document likelihood measure are quite competitive in 
most cases. 

likelihood  measure     does  not  preserve 

6.  conclusions & future work 

evidence 

in this paper, both the triplet learning model and the document 
likelihood measure have been proposed to leverage the  word 
embeddings learned by various id27 methods for 
speech  summarization.  in  addition,  a  new  svd-based  word 
embedding  method  has  also  been  proposed  and  proven 
efficient and as effective as existing id27 methods. 
experimental 
proposed 
summarization methods are comparable to several state-of-the-
art methods, thereby indicating the potential of the new word 
embedding-based  speech  summarization  framework.  for 
future work, we will explore other effective ways to enrich the 
representations  of  words  and  integrate  extra  cues,  such  as 
speaker identities or prosodic (emotional) information, into the 
proposed  framework.  we  are  also  interested  in  investigating 
more 
represent  spoken 
documents in an elegant way. 

techniques 

indexing 

supports 

robust 

that 

the 

to 

 

7.  references 

[1]  s.  furui  et  al.,     fundamental  technologies  in  modern  speech 
recognition,    ieee signal processing magazine, 29(6), pp. 16   
17, 2012. 

[2]  m.  ostendorf,     speech  technology  and  information  access,    

ieee signal processing magazine, 25(3), pp. 150   152, 2008.  

[3]  l.  s.  lee  and  b.  chen,     spoken  document  understanding  and 
organization,    ieee signal processing magazine, vol. 22, no. 5, 
pp. 42   60, 2005. 

[4]  y.  liu  and  d.  hakkani-tur,     speech  summarization,    chapter 
13 in spoken language understanding: systems for extracting 
semantic information from speech, g. tur and r. d. mori (eds), 
new york: wiley, 2011. 

[5]  g.   penn   and   x.   zhu,      a   critical   reassessment  of  evaluation  
baselines  for  speech  summarization,     in proc. of acl, pp. 470   
478, 2008. 

[6]  a.  nenkova  and  k.  mckeown,     automatic  summarization,    
foundations and trends in information retrieval, vol. 5, no. 2   3, 
pp. 103   233, 2011. 
i. mani and m. t. maybury (eds.), advances in automatic text 
summarization, cambridge, ma: mit press, 1999. 

[7] 

[8]  p. b. baxendale,    machine-made index for technical literature-

an experiment,    ibm journal, october, 1958. 

[9]  y.  gong  and  x.  liu,     generic  text  summarization  using 
relevance  measure  and  latent  semantic  analysis,     in  proc.  of 
sigir, pp. 19   25, 2001. 

[10]  x.  wan  and  j.  yang,     multi-document  summarization  using 
cluster-based  link  analysis,     in  proc.  of  sigir,  pp.  299   306, 
2008. 

[11]  j. carbonell and j. goldstein,    the use of mmr, diversity based 
reranking for reordering documents and producing summaries,    
in proc. of sigir, pp. 335   336, 1998. 

[12]  s.  furui  et  al.,     speech-to-text  and 

speech-to-speech 
summarization  of  spontaneous  speech   ,  ieee  transactions  on 
speech and audio processing, vol. 12, no. 4, pp. 401   408, 2004. 
[13]  g.  erkan  and  d.  r.  radev,     lexrank:  graph-based  lexical 
centrality  as  salience  in  text  summarization   ,  journal  of 
artificial intelligent research, vol. 22, no. 1, pp. 457   479, 2004. 
[14]  h.  lin  and  j.  bilmes,     multi-document  summarization  via 
budgeted  maximization  of  submodular  functions,     in  proc.  of 
naacl hlt, pp. 912   920, 2010. 

[15]  k. riedhammer et al.,    long story short - global unsupervised 
models  for  keyphrase  based  meeting  summarization,     speech 
communication, vol. 52, no. 10, pp. 801   815, 2010. 

[16]  j. kupiec et al.,    a trainable document summarizer,    in proc. of 

sigir, pp. 68   73, 1995. 

[17]  j.  zhang  and  p.  fung,     speech  summarization  without  lexical 
features for mandarin broadcast news   , in proc. of naacl hlt, 
companion volume, pp. 213   216, 2007. 

[18]  m.  galley,     skip-chain  conditional  random  field  for  ranking 
meeting  utterances  by  importance,     in  proc.  of  emnlp,  pp. 
364   372, 2006. 

[19]  y.  bengio  et  al.,     a  neural  probabilistic  language  model,    
journal  of  machine  learning  research  (3),  pp.  1137   1155, 
2003. 

[20]  a.  mnih  and  g.  hinton,     three  new  graphical  models  for 
statistical id38,    in proc. of icml, pp. 641   648, 
2007. 

 

[21]  t. mikolov et al.,    efficient estimation of word representations 

in vector space,    in proc. of iclr, pp. 1   12, 2013. 

[22]  j.  pennington  et  al.,     glove:  global  vector  for  word 

representation,    in proc. of emnlp, pp. 1532   1543, 2014. 

[23]  d. tang et al.,    learning sentiment-specific id27 for 
twitter sentiment classification    in proc. of acl, pp. 1555   1565, 
2014. 

[24]  r. collobert  and  j.  weston,     a  unified architecture  for natural 
language  processing:  deep  neural  networks  with  multitask 
learning,    in proc. of icml, pp. 160   167, 2008 

[25]  m. kageback et al.,    extractive summarization using continuous 

vector space models,    in proc. of cvsc, pp. 31   39, 2014. 

[26]  l.  qiu  et  al.,     learning  word  representation  considering 
proximity  and  ambiguity,     in  proc.  of  aaai,  pp.  1572   1578, 
2014. 

[27]  g.  miller  and  w.  charles,     contextual  correlates  of  semantic 
similarity,    language and cognitive processes, 6(1), pp. 1   28, 
1991. 

[28]  t.  mikolov  et  al.,     distributed  representations  of  words  and 
phrases and their compositionality,    in proc. of iclr, pp. 1   9, 
2013. 

[29]  f.  morin  and  y.  bengio,     hierarchical  probabilistic  neural 
network  language  model,     in  proc.  of  aistats,  pp.  246   252, 
2005. 

[30]  a.  mnih  and  k.  kavukcuoglu,     learning  word  embeddings 
efficiently with noise-contrastive estimation,    in proc. of nips, 
pp. 2265   2273, 2013. 

[31]  o. levy and y. goldberg,    neural id27 as implicit 

id105,    in proc. of nips, pp. 2177   2185, 2014. 

[32]  k.  y.  chen  et  al.,     weighted  matrix  factorization  for  spoken 

document retrieval,    in proc. of icassp, pp. 8530   8534, 2013. 

[33]  m. afify et al.,    gaussian mixture language models for speech 

recognition,    in proc. of icassp, pp. iv-29   iv-32, 2007. 

[34]  k.  crammer  et  al.,     online  passive-aggressive  algorithms,    

journal of machine learning research (7), pp. 551   585, 2006. 

[35]  g.  chechik  et  al.,     large  scale  online  learning  of  image 
similarity  through  ranking,     journal  of  machine  learning 
research (11), pp. 1109   1135, 2010. 

[36]  m. norouzi et al.,    hamming distance metric learning,    in proc. 

of nips, pp. 1070   1078, 2012. 

[37]  y.  t.  chen  et  al.,      a   probabilistic   generative  framework  for 
ieee 
extractive  broadcast  news  speech  summarization,     
transactions  on  audio,  speech  and  language  processing,  vol. 
17, no. 1, pp. 95   106, 2009. 

[38]  c.  zhai  and  j.  lafferty,     a  study  of  smoothing  methods  for 
language  models  applied  to  ad  hoc  information  retrieval,     in 
proc. of sigir, pp. 334   342, 2001. 

[39]  h.  m.  wang  et  al.,     matbn:  a  mandarin  chinese  broadcast 
international  journal  of  computational 
news  corpus,    
linguistics and chinese language processing, vol. 10, no. 2, pp. 
219   236, 2005. 

[40]  c.  y.  lin,     id8:  recall-oriented  understudy  for  gisting 
available: 

[online]. 

evaluation.    
http://haydn.isi.edu/id8/. 

2003 

[41]  g. heigold et al.,    discriminative training for automatic speech 
recognition:  modeling,  criteria,  optimization,  implementation, 
and  performance,     ieee  signal  processing  magazine,  vol.  29, 
no. 6, pp. 58   69, 2012. 

 

