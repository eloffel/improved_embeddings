 

 

id57 via discriminative summary reranking 

xiaojun wan1, ziqiang cao1, furu wei2, sujian li1 and ming zhou2 

1peking university 

2microsoft research asia 

1{wanxiaojun, ziqiangyeah, lisujian}@pku.edu.cn 

2{fuwei, mingzhou}@microsoft.com 

 
 
 
 

abstract 

specific 

summarization  model 

existing  multi-document  summarization  systems  usually 
rely  on  a 
(i.e.,  a 
summarization method with a specific parameter setting) to 
extract summaries for different document sets with different 
topics.  however,  according  to  our  quantitative  analysis, 
none  of  the  existing  summarization  models  can  always 
produce high-quality summaries for different document sets, 
and  even  a  summarization  model  with  good  overall 
performance  may  produce  low-quality  summaries  for  some 
document  sets.  on  the  contrary,  a  baseline  summarization 
model  may  produce  high-quality  summaries  for  some 
document  sets.  based  on  the  above  observations,  we  treat 
the summaries produced by different summarization models 
as  candidate  summaries,  and  then  explore  discriminative 
reranking  techniques  to  identify  high-quality  summaries 
from  the  candidates  for  difference  document  sets.  we 
propose  to  extract  a  set  of  candidate  summaries  for  each 
document set based on an ilp framework, and then leverage 
ranking  id166  for  summary  reranking.  various  useful 
features  have  been  developed  for  the  reranking  process, 
including  word-level  features,  sentence-level  features  and 
features.  evaluation 
summary-level 
the 
benchmark  duc  datasets  validate 
the  efficacy  and 
robustness of our proposed approach.  

results  on 

introduction   

given  a  set  of  documents  about  a  topic,  multi-document 
summarization  systems  aim  to  produce  a  short  and  fluent 
summary to deliver the salient information in the document 
set.  most  existing  summarization  systems  are  based  on 
sentence  extraction,  and  they  rely  on  a  specific  method  to 
rank some kinds of units (e.g. words, bigrams, or sentences) 
and  then  extract  summary  sentences  according  to  the 
ranking  results.  with  the  development  of  the  duc  and 
tac  benchmark  tests,  document  summarization  has  been 
well  studied  and  many  different  summarization  methods 

                                                 
 
 

the 

is  used 

analysis,  none  of 

have been proposed, e.g., centroid-based method (radev et 
al. 2004), graph-based ranking methods (erkan and radev 
2004) and ilp-based methods (mcdonald 2007; gillick et 
al. 2008).  
 
in an existing document summarization system, a single 
summarization model (i.e., a summarization method with a 
specific  parameter  setting) 
for  extracting 
summaries  from  different  document  sets.  for  example, 
there  are  50  different  document  sets  in  the  duc2004 
dataset,  and  a  summarization  system  usually  adopts  a 
single  summarization  model  (e.g.  an  ilp-based  method 
with a specific parameter setting) to extract summaries for 
all the 50 document sets. the common assumption is that a 
single  summarization  model  can  well  deal  with  all  the 
different  document  sets.  however,  according  to  our 
quantitative  data 
existing 
summarization  models  can  always  produce  high-quality 
summaries  for  different  document  sets,  and  even  a 
summarization  model  with  good  overall  performance  may 
produce  low-quality  summaries  for  some  document  sets. 
on  the  contrary,  a  baseline  summarization  model  may 
produce  high-quality  summaries  for  some  document  sets. 
the  reason  lies  in  the  different  characteristics  of  different 
document  sets  and  none  of  the  summarization  models  can 
be fit for all different document sets.  
     based  on  the  above  observations,  we  attempt  to 
improve 
the  overall  summarization  performance  by 
leveraging  the  summaries  produced  by  multiple  different 
summarization  models 
(i.e.,  different  summarization 
methods,  or  a  summarization  method  with  different 
parameter  settings)  for  each  document  set.  we  treat  the 
summaries produced by different summarization models as 
candidate  summaries,  and  then  explore  discriminative 
reranking  techniques  to  identify  high-quality  summaries 
from  the  candidates  for  difference  document  sets.  in  this 
way,  we  can  take  advantage  of  different  summarization 
models  and  produce  better  summaries  for  different 

in  order 

document sets, and thus achieve better overall performance. 
in  particular,  we  propose  to  extract  a  set  of  candidate 
summaries  for  each  document  set  based  on  an  ilp 
framework. 
to  discriminate  high-quality 
summaries from low-quality summaries, we adopt ranking 
id166 and develop various useful features for the reranking 
process, 
including  word-level  features,  sentence-level 
features and summary-level features. evaluation results on 
the  benchmark  duc  datasets  show  that  our  proposed 
approach can outperform each single summarization model 
used and achieve state-of-the-art performances in terms of 
id8  scores.  the  efficacy  of  each  group  of  features  is 
also verified.   
    to  the  best  of  our  knowledge,  we  are  the  first  to  apply 
reranking 
discriminative 
extractive 
document summarization.  

techniques 

for 

related work 

in 

id57 methods can be extraction-
based  or  abstraction-based,  and  we  focus  on  extractive 
summarization  methods 
this  paper.  extractive 
summarization  methods  usually  produce  a  summary  by 
selecting  some  original  sentences  in  the  document  set. 
sentences can be scored by employing rule based methods 
to simply combine a few feature weights, e.g., the centroid-
based  method  (radev  et  al.  2004)  and  neats  (lin  and 
hovy  2002).  machine  learning  techniques  have  been  used 
for better combining various  sentence  features (ouyang et 
al.  2007;  shen  et  al.  2007;  schilder  and  kondadadi  2008; 
wong  et  al.  2008).  many  advanced  methods  have  been 
proposed  for  extractive  summarization  in  recent  years, 
which  are  based  on  various  techniques:  budgeted  median 
method  (takamura  and  okumura  2009),  a*  search 
algorithm  (aker  et  al.  2010),  minimum  dominating  set 
(shen  and  li  2010),  matrix  factorization  (wang  et  al. 
2008),  topic  model  (wang  et  al.  2009)  integer  linear 
programming (mcdonald 2007; gillick et al. 2008; gillick 
and  favre  2009;  li  et  al.  2013),  and  submodular  function 
(lin  and  bilmes  2010;  li  et  al.  2012).  graph-based 
methods  have 
for  various 
summarization  tasks,  such  as  lexrank  (erkan  and  radev 
2004),  textrank 
(mihalcea  and  tarau  2005)  and 
clustercmrw  (wan  and  yang  2008).  furthermore, 
ensemble  methods  have  also  been  used  for  sentence 
ranking.  for  example,  wang  and  li  (2010)  propose  a 
weighted consensus method to aggregate different sentence 
ranking results by different summarization methods.  
    however,  all  the  above  studies  focus  on  sentence 
scoring  and  ranking,  and  none  of  them  has  attempted  to 
rank  summaries  directly.  different  from  previous  studies, 
we rank summaries directly in this study. the advantage of 
the 
ranking  summaries 

that  we  can  optimize 

also  been  proposed 

is 

summarization  performance  directly  based  on 
characteristics of the summaries.  

the 

automatic summary evaluation is partially related to this 
work.  most  researches  in  this  area  focus  on  how  to 
measure  the  quality  (i.e.,  content  and  readability)  of  a 
summary  when  one  or  more  reference  summaries  written 
by human experts are given (lin and hovy 2003; hovy et 
al.  2006;  pitler  et  al.  2010;  lin  et  al.  2012).  in  particular, 
id8 is one of the most popular metrics for comparing 
peer summaries with reference summaries. in recent years, 
several  pilot  studies  have  investigated  to  automatically 
assess  the  qualities  of  peer  summaries  without  reference 
summaries (saggion et al. 2010; louis and nenkova 2013), 
and  they  mainly  rely  on  the  similarity  (e.g.  js  divergence 
and  kl  divergence)  between  the  peer  summary  and  the 
source  document  text.  however,  only  the  summary-to-
document similarity is not adequate for ranking summaries, 
as shown in our experiments, and we have to develop more 
useful features on different levels.   

data analysis and motivation 

in  this  section,  we  take  the  duc2004  dataset  as  example 
and  perform  a  quantitative  analysis  on  the  dataset  to 
validate our assumption and present our motivation.  
     in  the  multi-document  summarization  task  (i.e.  task  2) 
on  duc2004,  there  are  a  total  of  50  english  document 
clusters  and  each  cluster  contains  10  news  document  on 
average.  given  each  document  cluster,  the  task  aims  to 
create  a  short  summary  (<=  665  bytes)  of  the  cluster. 
reference  summaries  for  each  cluster  have  been  created 
for evaluation. there were a total of 34 runs submitted by 
15  teams,  and  these  runs  were  produced  by  different 
summarization  models 
(i.e.,  different  summarization 
methods  with  different  parameter  settings).  therefore,  all 
the runs can represent a variety of different summarization 
models.  the  overall  performances  of  these  summarization 
models  range  from  1.851  to  9.178  in  terms  of  id8-2 
recall (%)1. 

 

                                                 
1 note  that  the  id8  scores  reported  in  this  paper  are  the  percentage 
values,  which  means  the  real  id8  scores  times  100.    we  used 
id8-1.5.5 toolkit for evaluation in this paper.  

figure 1. id8-2(%) comparison of sample runs on 

candidate summary extraction 

different document clusters. 

since  the  overall  performance  of  a  summarization 
model  is  the  average  of  the  performance  values  of  the 
model  across  the  50  document  clusters,  we  now  compare 
the  detailed  performance  values  of  two  different  runs  (i.e. 
65  and  35)  on  different  document  clusters,  as  shown  in 
figure  1.  we  can  see  from  the  figures  that  different  runs 
produced by different summarization models have variable 
performances on different document clusters. for example, 
run 65 has the best overall performance, but it achieves the 
best scores over only 7 document clusters (e.g., clusters 1, 
29,  47);  moreover,  its  performance  values  over  a  few 
document  clusters  (e.g.,  clusters  13,  21,  34)  are  very  low. 
on the other hand, run 35 has a lower overall performance 
and ranks 6th, but it can achieve the  highest scores over 4 
document  clusters  (e.g.  clusters  3,  20,  21,  34),  and  it  can 
produce better summaries than run 65 for 19 clusters.  

ideally, if we can select the best one from the summaries 
of different runs for each document cluster, we can obtain 
the  upper  bound  performance,  as  shown  in  table  1.  the 
average performance of all runs are also presented. we can 
see that the upper bound performance is much higher than 
the top performance of the submitted runs.       

table 1: the upper bound vs. top run 
id8-2(%) 

 

upper bound 

average 

top run (65) 

worst run (111) 

11.76 

7.010 

9.178 

1.851 

to say the least, if we can select the one close to the best 
for each document cluster, we can still achieve high overall 
performance.  so  the  question  is  whether  and  how  we  can 
select the best one or the one close to the best from a set of 
candidate 
cluster. 
fortunately,  this  is  a  typical  ranking  problem  and  we  can 
leverage  learning  to  ranking  techniques  to  address  it,  as 
described in the next section.  

each  document 

summaries 

for 

 proposed approach 

given  a  document  set,  our  proposed  approach  aims  to 
select for the document set a high-quality summary from a 
set of candidate summaries of various qualities. it consists 
of  two  stages:  the  first  stage  aims  to  extract  candidate 
summaries  for  each  document  set,  and  the  second  stage 
aims to rerank the candidate summaries and select the best 
one  as  the  final  summary  for  each  document  set.  the 
technical details of the two stages  will be described in  the 
following subsections, respectively.  

the summaries of the submitted runs can be simply used as 
candidate  summaries  for  the  duc  document  sets,  but  in 
practice,  we  do  not  have     submitted  runs     for  a  new 
document set, so we have to develop a method to produce 
candidate  summaries  for  any  given  document  set.  there 
are several ways to achieve this goal. for example, we can 
develop  different  summarization  methods 
to  extract 
candidate  summaries,  or  we  can  make  use  of  a  single 
summarization method by using different parameter values 
to  extract  summaries,  or  we  can  combine  the  above  two 
ways.  
    in 
to  extract  candidate 
summaries for a document set based on an ilp framework, 
which  is  very  popular  in  the  summarization  area  in  recent 
the  following  ilp 
years.  particularly,  we 
formulation 
summary 
generation: 

sentence  extraction  and 

this  study,  we  propose 

leverage 

for 

    

    

    

1

max   {                 
    
      s.t.                      

                             + (1         )    

                               }             (1) 
                                                            (2) 
                                   ,                 ,                   ,                                              (3) 
                                       ,    
            ,                                                        (4) 
                                {0, 1},                                                            
                                {0, 1},                                                  
   where the notations are defined as follows:      
   s: the set of sentences in a document set; 
   c: the set of words in a document set; 
            : the importance score of sentence  i,  which is learned 
by a regression method described later; 
            : the importance score of word j, which is learned by a 
regression method described later; 
           ,     : the indicator of whether word j occurs in sentence i;  
            : the indicator of whether sentence i is selected into the 
summary;  
             :  the  indicator  of  whether  word  j  appears  in  the 
summary;  
            : the length of sentence i;  
   l: the length limit of the summary; 
        : the parameter between [0, 1] to control the influences 
of two parts;  
   in  the  above  formulation,  both  the  scores  of  sentences 
and  words  are  considered.  constraint  (2)  ensures  the 
summary   s length limit. constraints (3) and (4) ensure that 
a  sentence  is  selected  iff  the  words  in  the  sentence  are 
selected,  and  a  word  is  selected  if  at  least  one  sentence 
containing  the  word  is  selected.    the  first  part  aims  to 
select sentences with higher importance scores. we add the 

sentence length ratio 

        
    

 as a multiplication factor in order to 

penalize the very short sentences, or the objective function 
tends  to  select  more  and  shorter  sentences.  at  the  same 
time, the objective function does not tend to select the very 
long sentences. the total length of the sentences selected is 
fixed. so if the objective function tends to select the longer 

sentences,  the  fewer  sentences  can  be  selected.  a  tradeoff 
needs  to  be  made  between  the  number  and  the  average 
length  of  the  sentences  selected.  the  second  part  aims  to 
let  the  summary  contain  important  words  as  many  as 
possible. this part can address the redundancy issue of the 
summary.  the  intuition  is  that  the  more  unique  words  the 
summary contains, the less redundancy the summary has. 

learn 

implemented 

in  liblinear 2  to 

in order to better assess the importance of each sentence 
and word, we leverage the support vector regression (svr) 
method 
the 
importance  scores            and           .  in  the  training  phase,  the 
frequency of a word in the reference summaries is used as 
the  target  score  of  the  word,  and  the  maximum  similarity 
between  a  sentence  and  the  sentences  in  the  reference 
summaries  is  used  as  the  target  score  of  the  sentence. 
there  are  a  total  of  15  features  used  for  word  score 
regression,  including  term  frequency  (tf),  document 
frequency  (df),  pos-based  features  (whether  a  word  is  a 
noun/verb/adjective/adverb),  ner-based  feature  (whether 
a  word  belongs  to  a  named  entity)  and  number-based 
feature (whether a word is a number), and also the features 
extracted  from  the  sentences  containing  the  word  (e.g. 
max/min  positions,  etc.)  there  are  a  total  of  13  features 
used  for  sentence  score  regression,  including  sentence   s 
position,  length,  number  of  subsentences,  depth  of  the 
parse  tree,  proportion  of  stopwords,  mean  tf  of  words, 
mean  df  of  words, 
in 
noun/verb/adjective/adverb/ner/number.   

proportion  of  words 

based on the above ilp formulation, we first change        
from  0  to  0.9  with  a  step  of  0.1,  and  thus  produce  10 
candidate summaries for each document set. moreover, for 
each value of     , we iteratively produce 10-best summaries 
by adding the following new constraint at each iteration: 

               

        

   

               
|        |

        ,                                                            (5) 

where           is  the  set  of  all  sentences  in  all  the  summaries 
produced  from  the  first  to  the  k-th  iteration.         controls 
how  the  new  summary  at  the  (k+1)-th  iteration  can 
resemble  existing  summaries.  we  simply  set       to  0.6  in 
our  experiments,  which  means  at  most  60%  sentences  in 
the summary produced at the (k+1)-th iteration are allowed 
be  the  same  with  the  sentences  in  all  the  previous 
summaries.  for  example,  if  we  have  selected  sentences 
x1={1,  2,  3,  4,  5}  at  the  first  iteration,  we  are  allowed  to 
select  at  most  60%  the  same  sentences  from  x1  at  the 
second iteration, for example, {1, 2, 3, 6, 7}, and then  we 
are allowed to select at most 60% the same sentence from 
x2={1, 2, 3, 4, 5, 6, 7} at the third iteration, and so on.  
    finally,  we  can  produce  a  total  of  100  summaries  for 
each  document  set,  and  these  summaries  are  treated  as 
candidate summaries.  

summary reranking 

learning 

method 
after  we  obtain  a  set  of  candidate  summaries  for  a 
document set,  we adopt the  learning to rank techniques to 
discriminate  the  high-quality  ones  from  the  low-quality 
ones.  in  recent  years,  various 
to  ranking 
algorithms have been proposed in the information retrieval 
field,  such  as  rankid166  (joachims  2002),  ranknet 
(burges  et  al.  2005),  rankboost  (freund  et  al.  2003),  etc. 
without  loss  of  generality,  we  adopt  rankid166  for 
summary  reranking,  because  rankid166  is  the  most 
popular  learning  to  rank  technique  and  it  has  been 
successfully used in many applications. due to page limit, 
the comparison of different learning to ranking algorithms 
is out of the scope of this paper.  
    rankid166  (ranking  id166)  is  a  pairwise  approach  for 
learning  to  rank.  it  makes  use  of  the  regular  id166  qp 
optimization and trains for a classification of order of pairs. 
in  this  study,  we  use  the  id166rank  toolkit 3  in  our 
experiments.  

in  the  training  phase,  since  we  have  the  reference 
summaries  for  each  document  set,  we  use  the  id8-2 
recall  scores  between  the  candidate  summaries  and  the 
reference  summaries  to  derive  the  ranking  order  of  the 
candidate summaries.  

features 
given  a  document  set  and  a  set  of  candidate  summaries, 
we  can  extract  a  variety  of  features  on  different  levels  to 
indicate  different  aspects  of  the  quality  of  each  candidate 
summary.  note  that  reference  summaries  cannot  be  used 
for  feature  extraction.  three  group  of  features  have  been 
extracted  for  the  reranking  process:  word-level  features, 
sentence-level  features  and  summary-level  features.  the 
details  of  the  three  groups  of  features  are  presented  in 
tables  2,  3  and  4,  respectively.  the  values  of  these 
features are scaled to [0,1]. 
 

feature 
tf 

df 

pos 

ner 

stopword 

number 

table 2. word-level features 

description 

sum  of  word  frequency  in  a  summary, 
where  word  frequency  is  computed  from 
the document set; 
sum  of  document  frequency  of  words  in  a 
summary,  where  document  frequency  is 
computed from the document set; 
the 
noun/verb/adverb/adjective  words 
summary; 
the  ratio  of  named  entity  number  to  the 
summary length; 
the  ratio  of  stopword  number  to  the 
summary length; 
the  ratio  of  number  word  count  to  the 

of 
in  a 

proportion 

                                                 
2 http://www.csie.ntu.edu.tw/~cjlin/liblinear/ 

                                                 
3 http://www.cs.cornell.edu/people/tj/id166_light/id166_rank.html 

unique 
words 
lead 
words 

feature 
sentence 
length 
position 

sentence 
number 

summary length; 
the ratio of the number of unique words in 
a summary to the summary length; 
the  ratio  of  the  number  of  words  in  a 
summary  which  appear 
first 
sentences  of  documents  to  the  summary 
length; 

the 

in 

 
 

table 3. sentence-level features 

description 

the min/max/mean length of sentences in a 
summary; 
the  mean/max 
of 
sentences in a summary, where the position 
weight is computed as 1    

position  weight 

(                                   1)

(                                                          1)
the number of sentences in a summary; 

; 

table 4. summary-level features 

feature 

sum-doc 
cosine with 
tfidf 

sum-doc js 

sum-doc 
word overlap 
sum-doc 
cosine with  
embedding 

sum-sum 
word overlap 

sum-sum 
cosine with 
embedding 

description 

for 

text 

the  concatenated 

the cosine similarity between a summary 
and the document set, where the summary 
and 
the 
document  set  are  represented  by  two 
tfidf vector of words; 
the  jensen  shannon  divergence  between 
a summary and the document set;  
the  word  overlap  similarity  between  a 
summary and the document set; 
the cosine similarity between a summary 
and the document set, where the summary 
text  and  the  concatenated  text  for  the 
document set is represented by averaging 
the embedding vectors of all words in the 
text4;  
the  average  word  overlap  similarities 
between  a  summary  and  other  candidate 
summaries;  
the  average  cosine  similarity  between  a 
summary and other candidate summaries, 
where  each  summary  text  is  represented 
by averaging the id27s; 

evaluation  

evaluation setup 

in  the  experiments,  we  used  three  benchmark  duc 
datasets 
for  evaluation:  duc2001,  duc2002  and 
duc2004. in each dataset, generic summaries are required 
to be created for different news clusters (i.e. document set). 
reference  summaries  have  been  manually  provided  for 
each document set. the datasets are summarized in table 5.  

                                                 
4 word  embeddings  are  downloaded  from  http://ml.nec-labs.com/senna/.  
the  dataset  contains  130000  words  and  each  word  is  associated  to  an 
embedding with a dimension of 50. 

table 5. summary of datasets 
duc 2002 

duc 2001 

 
task 
number of documents 
number of clusters 
data source 
summary length 

task 2 

309 
30 

task 2 

567 
59 

trec-9 
100 words 

trec-9 
100 words 

duc2004 

task 2 

500 
50 

tdt 

665 bytes 

in the experiments, we used duc2002 and duc2004 as 
test  set.  when  duc2002  was  used  as  test  set,  duc2001 
and duc2004  were used as training  set. when  duc2004 
was used as test set, duc2001 and duc2002 were used as 
training  set.    both  the  regression  models  and  the  ranking 
id166 model were trained and tuned on the training set. the 
models were then applied on the test set.  

in this study, we used id8-2 recall score (%) as the 
evaluation  metric,  because  id8-2  was  the  most 
reliable evaluation metric for document summarization and 
it  has  been  shown  to  be  highly  correlated  with  human 
judges.  due  to  page  limit,  other  id8  scores  were 
ignored  in  this  paper,  as  the  conclusions  based  on  these 
scores are the same with that based on id8-2.  

evaluation results and discussion 

main results 
in  our  proposed  approach,  we  have  100  summarization 
models  based  on  the  ilp  framework  to  produce  a  total  of 
100  different  candidate  summaries  for  each  document  set. 
we  compare  our  reranking  approach  with  the  best  model 
and  the  worst  model  among  them.  we  also  compute  the 
average scores of the 100 models.  the upper bound scores 
are  computed  by  selecting  the  best  summary  for  each 
document set.  the comparison results are shown in table 
6.  we  can  see  that  on  both  datasets,  the  performance  gap 
between  the  best  model  and  the  worst  model  is  very  big, 
and  the  upper  bound  is  much  higher  than  that  of  the  best 
model.  moreover,  our  proposed  reranking  approach  can 
sensibly outperform the best models on both datasets. the 
results verify the effectiveness of our proposed approach.  
     in  our  proposed  approach,  we  use  rankid166  to  rerank 
the candidate summaries.  as  mentioned earlier, louis and 
nenkova  (2013)  have  proposed  to  use  js  divergence  and 
kl  divergence  between  a  candidate  summary  and  a 
document  set  to  automatically  evaluate  the  summary   s 
quality  without  reference  summaries,  and  therefore,  js 
divergence  and  kl  divergence  can  be  used  for  reranking 
the  candidate  summaries.  the  comparison  between 
rankid166,  js  divergence  and  kl  divergence  is  shown  in 
table  7.  we  can  see 
that  rankid166  significantly 
outperforms js divergence and kl divergence. the results 
verify  the  effectiveness  of  the  use  of  rankid166  with 
multiple useful features for reranking candidate summaries.  
     table 8  further  compare  our  proposed  approach  with  a 
variety  of  state-of-the-art  methods,  besides  the  best  duc 
participating  system  (top  run).  the  id8  scores  of  the 

methods  are  directly  borrowed  from  the  corresponding 
literatures.  as  shown  in  the  table,  our  proposed  approach 
can achieve state-of-the-art performance.  
     in  table  9,  we  compare  different  feature  sets  in  our 
proposed approach.    w/o word-level    means removing the 
word-level features from the feature set.     w/o embedding    
means  removing  the  features  relying  on  word  embedding. 
we can see that all kinds of features are beneficial for the 
reranking  process, 
including  the  features  with  word 
embedding.  

with  a  step  of  0.05.  the  results  with  more  candidates  are 
shown in table 10. we can see that the upper bound scores 
have been improved due to the more candidate summaries.  
compared  with  table  5,  the  performance  scores  of  our 
proposed  approach  have  also  been  improved  slightly.  the 
new  feature  comparison  results  are  presented  in  table  11, 
and all kinds of features are still very useful.  
 

table 10. comparison results with more candidates 

 

duc2002 

duc2004 

table 6. comparison results5 

duc2002 

duc2004 

upper bound 

average 

11.199 

7.298 

11.85 

8.754 

best model 
worst model 
our approach 

8.192 [7.238-9.138] 
6.679 [5.583-7.764] 
8.555 [7.616     9.520]  10.051 [9.334-10.680] 

9.760 [8.909-10.617] 
7.526 [6.711-8.357] 

table 7. comparison of reranking strategies 
duc2004 

duc2002 

 

 

rankid166 

8.555 [7.616    9.520]  10.051 [9.334-10.680] 

js divergence 
7.209 [6.404-8.053] 
kl divergence  7.665 [6.779 -8.604] 

8.876 [8.083-9.669] 

8.660 [7.877-9.461] 

 

 

table 8. comparison with state-of the-art methods 

duc2002 

duc2004 

our approach 

clusterhits  

(wan and yang 2008) 

ilp 

(mcdonald 2007) 

mssf 

(li et al. 2012) 

bstm 

(wang et al. 2009) 

mds 

(shen and li 2010) 

top run 

8.555 
8.135 

7.2 

- 

- 

- 

7.642 

10.051 

- 

- 

9.897 

9.01 

8.934 

9.178 

table 9. feature analysis results 

all features 

w/o word-level 

w/o sentence-level 
w/o summary-level 

w/o embedding 

duc2002 

duc2004 

8.555 
8.245    
8.265    
8.4    
8.458    

10.051 
9.988    
10.06 
9.65    
10.037    

results with more candidates 
in  the  above  experiments,  we  produce  100  candidate 
summaries  for  each  document  set.  we  now  add  more 
candidates  for  reranking  by  produce  a  set  of  candidate 
summaries  based  on  the  lexrank  method.  we  range  the 
damping  factor  in  the  lexrank  algorithm  from  0.1  to  0.9 
                                                 
5   the  95%  confidence  interval  for  each  id8  score  is  reported  in 
brackets.  for  the  upper  bound  and  average  scores,  the  confidence 
intervals are not available.  

upper bound 

average 

11.486 
7.367 

12.064 
8.763 

best model 
worst model 
our approach 

8.192 [7.238-9.138] 
6.679 [5.583-7.764] 
8.649 [7.604-9.682]  10.154 [9.337-11.074] 

9.760 [8.909-10.617] 
7.526 [6.711-8.357] 

table 11. feature analysis results with more candidates 

 

duc2002 

duc2004 

all features 

w/o word-level 

w/o sentence-level 
w/o summary-level 

w/o embedding 

8.649 
8.023    
8.491    
8.457    
8.596    

10.154 
10.009    
9.893    
10.036    
9.996    

reranking results for submitted runs 
lastly,  we simply consider the submitted runs as different 
summarization  models, 
candidate 
summaries  of  different  submitted  runs  by  our  approach. 
the  results  on  duc2004  are  shown  in  table  12.  we  can 
see  that  our  reranking  algorithm  can  achieve  better 
id8  score  than  the  best  run.  the  results  further 
validate the efficacy of our reranking strategy.  

rerank 

and 

the 

table 12. reranking results based on submitted runs 

 

upper bound 

best run 
rankid166 

duc2004 

11.76 

9.178 [8.361-10.033] 
9.545 [8.823-10.312] 

conclusion and future work  

in  this  paper,  we  observe  that  different  summarization 
models  have  variable  performances  over  different 
document  sets.  based  on  this  observation,  we  proposed  a 
two-stage  approach  for  multi-document  summarization.  in 
the  first  stage,  we  explore  an  ilp  framework  to  produce 
candidate summaries for each document set. in the second 
stage,  we  propose  to  use  ranking  id166  to  rerank  the 
candidate 
the  overall 
summarization  performance  can  be  improved.  evaluation 
results  on  two  duc  datasets  verify  the  efficacy  of  our 
proposed approach and the usefulness of the features.   
    in  future  work,  we  will  try  to  make  use  of  more 
summarization  methods 
to  produce  more  candidate 
investigate 
summaries  for  reranking.  we  will  also 

summaries. 

this  way, 

in 

advanced  deep  learning  techniques  to  derive  more  useful 
features. for example, we can make use of recurrent neural 
network  or  recursive  neural  network  to  obtain  more 
reliable  semantic  representations  of  sentences,  summaries 
and  documents  via  compositional  semantic  computation, 
and  then  derive  new  features  based  on  the  new  semantic 
representations.  

references 

a.  aker,  t.  cohn,  and  r.  gaizauskas.  2010.  multi-document 
summarization  using  a*  search  and  discriminative  training.  in 
proceedings of emnlp2010. 

c.  burges,  t.  shaked,  e.  renshaw,  a.  lazier,  m.  deeds,  n. 
hamilton & g. hullender. 2005. learning to rank using gradient 
descent.  in  proceedings  of  the  22nd  international  conference  on 
machine learning (pp. 89-96). acm. 

g.  erkan  and  d.  r.  radev.  2004.  lexid95:  prestige  in 
multi-document  text  summarization.  in  proceedings  of  emnlp-
04. 

y.  freund,  r.  iyer,  r.  e.  schapire  &  y.  singer.  2003.  an 
efficient  boosting  algorithm  for  combining  preferences.  the 
journal of machine learning research, 4, 933-969. 

d.  gillick,  b.  favre  and  d.  hakkani-tur.  2008.  the  icsi 
summarization  system  at  tac  2008.  in  proceedings  of  the  text 
understanding conference. 

d.  gillick  and  b.  favre.  2009.  a  scalable  global  model  for 
summarization.  in  proceedings  of  the  workshop  on  integer 
linear  programming  for  natural  langauge  processing  on 
naacl. 

e.  hovy,  c.  y.  lin,  l.  zhou  &  j.  fukumoto.  2006.  automated 
summarization evaluation with basic elements. in proceedings of 
the  fifth  conference  on  language  resources  and  evaluation 
(lrec 2006) (pp. 604-611). 
t. joachims. 2002. optimizing search engines using clickthrough 
data.  in  proceedings  of  the  eighth  acm  sigkdd  international 
conference  on  knowledge  discovery  and  data  mining  (pp.  133-
142). acm. 
j.  li,  l.  li  &  t.  li.  2012.  multi-document  summarization  via 
submodularity. applied intelligence, 37(3), 420-430. 

c. li, x. qian and y. liu. 2013. using supervised bigram-based 
ilp  for  extractive  summarization.  in  proceedings  of  acl  (pp. 
1004-1013). 

h.  lin  and  j.  bilmes.  2010.  multi-document  summarization  via 
budgeted  maximization  of  submodular  functions.  in  human 
language  technologies:  the  2010  annual  conference  of  the 
north  american  chapter  of  the  association  for  computational 
linguistics (pp. 912-920).  

c.-y. lin and e.. h. hovy. 2002. from single to multi-document 
summarization:  a  prototype  system  and  its  evaluation.  in 
proceedings of acl-02. 

c.-y.  lin  and  e.h.  hovy.  2003.  automatic  evaluation  of 
summaries  using  id165  co-occurrence  statistics.  in proceedings 
of hlt-naacl -03. 

z.  lin,  c.  liu,  h.  t.  ng  &  m.  y.  kan.  2012.  combining 
coherence models and machine translation id74 for 
summarization  evaluation.  in  proceedings  of  the  50th  annual 
meeting  of  the  association  for  computational  linguistics:  long 

1 

for 

1006-1014).  association 

papers-volume 
(pp. 
computational linguistics. 
a. louis & a. nenkova. 2013. automatically assessing machine 
summary  content  without  a  gold  standard.  computational 
linguistics, 39(2), 267-300. 
r.  mcdonald.  2007.  a  study  of  global  id136  algorithms  in 
multi-document  summarization  (pp.  557-564).  springer  berlin 
heidelberg. 
r.  mihalcea  and  p.  tarau.  2005.  a  language  independent 
algorithm  for  single  and  multiple  document  summarization.  in 
proceedings of ijcnlp-05. 

y. ouyang, s. li, w. li. 2007. developing learning strategies for 
topic-focused summarization. in proceedings of cikm-07. 

in  multi-document 

e. pitler, a. louis & a. nenkova. 2010. automatic evaluation of 
linguistic  quality 
in 
proceedings  of  the  48th  annual  meeting  of  the  association  for 
linguistics 
computational 
for 
computational linguistics. 
d.  r.  radev,  h.  y.  jing,  m.  stys  and  d.  tam.  2004.  centroid-
based  summarization  of  multiple  documents. 
information 
processing and management, 40: 919-938. 

(pp.  544-554).  association 

summarization. 

j. rantala, r. raisamo, j. lylykangas, v. surakka, j. raisamo, k. 
salminen,  t.  pakkanen  and  a.  hippula.  2009.  methods  for 
presenting  braille  characters  on  a  mobile  device  with  a 
touchscreen and tactile feedback. haptics, ieee transactions on, 
2(1), 28-39. 

h.  saggion,  j.  m.  torres-moreno,  i.  d.  cunha  &  e.  sanjuan. 
2010.  multilingual  summarization  evaluation  without  human 
models.  in  proceedings  of  the  23rd  international  conference  on 
computational  linguistics: posters  (pp. 1059-1067).  association 
for computational linguistics. 
f.  schilder  and  r.  kondadadi.  2008.  fastsum:  fast  and  accurate 
query-based  multi-document  summarization.  in  proceedings  of 
acl-08: hlt. 

c. shen and t. li.  2010. id57 via the 
minimum dominating set. in proceedings of coling-10. 

d. shen, j.-t. sun, h. li, q. yang, and z. chen. 2007. document 
summarization using id49. in proceedings of 
ijcai-07. 

h. takamura and m. okumura. 2009. text summarization model 
based on the budgeted median problem. in proceedings of cikm-
09. 

x. wan and j. yang. 2008. id57 using 
cluster-based link analysis. in proceedings of sigir-08. 

d.  wang,  &  t.  li.  2010.  many  are  better  than  one:  improving 
multi-document  summarization  via  weighted  consensus.  in 
proceedings of the 33rd international acm sigir conference on 
research and development in information retrieval (pp. 809-810). 
acm. 

d.  wang,  t.  li,  s.  zhu,  c.  ding.  2008.  multi-document 
summarization  via 
semantic  analysis  and 
symmetric id105. in proceedings of sigir-08. 

sentence-level 

d.  wang,  s.  zhu,  t.  li  &  y.  gong.  2009,  august.  multi-
document  summarization  using  sentence-based  topic  models.  in 
proceedings  of  the  acl-ijcnlp  2009  conference  short  papers 
(pp. 297-300). association for computational linguistics. 
k.-f.  wong,  m.  wu  and  w.  li.  2008.  extractive  summarization 
using  supervised  and  semisupervised  learning.  in proceedings of 
coling-08. 

 

