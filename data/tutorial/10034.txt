unsupervised sentence simpli   cation using deep semantics

shashi narayan

school of informatics

claire gardent

cnrs, loria, umr 7503

the university of edinburgh
edinburgh, eh8 9ab, uk

vandoeuvre-l`es-nancy, f-54500, france

claire.gardent@loria.fr

shashi.narayan@ed.ac.uk

6
1
0
2

 

p
e
s
7

 

 
 
]
l
c
.
s
c
[
 
 

3
v
2
5
4
8
0

.

7
0
5
1
:
v
i
x
r
a

abstract

we present a novel approach to sentence
simpli   cation which departs from previ-
ous work in two main ways. first,
it
requires neither hand written rules nor a
training corpus of aligned standard and
simpli   ed sentences.
second, sentence
splitting operates on deep semantic struc-
ture. we show (i) that the unsupervised
framework we propose is competitive with
four state-of-the-art supervised systems
and (ii) that our semantic based approach
allows for a principled and effective han-
dling of sentence splitting.

1 introduction

sentence simpli   cation maps a sentence to a sim-
pler, more readable one approximating its content.
as has been argued in (shardlow, 2014), sentence
simpli   cation has many potential applications. it
is useful as a preprocessing step for a variety of
nlp systems such as parsers and machine trans-
lation systems (chandrasekar et al., 1996), sum-
marisation (knight and marcu, 2000),
sentence
fusion (filippova and strube, 2008) and seman-
tic role labelling (vickrey and koller, 2008).
it
also has wide ranging potential societal appli-
cations as a reading aid for people with apha-
sia (carroll et al., 1999), for low literacy readers
(watanabe et al., 2009) and for non native speak-
ers (siddharthan, 2002).

in this paper, we present a novel approach to
sentence simpli   cation which departs from pre-
vious work in two main ways. first, it requires
neither hand written rules nor a training corpus
of aligned standard and simpli   ed sentences. in-
stead, we exploit non aligned simple and english
wikipedia to learn the id203 of lexical sim-
pli   cations, of the semantics of simple sentences
and of optional phrases i.e., phrase which may be

deleted when simplifying. second, sentence split-
ting is semantic based. we show (i) that our unsu-
pervised framework is competitive with four state-
of-the-art systems and (ii) that our semantic based
approach allows for a principled and effective han-
dling of sentence splitting.

2 related work

split

simpli   cation

simpler clauses or

to capture
to

earlier work on sentence simpli   cation re-
lied on handcrafted rules
syn-
coordi-
tactic
e.g.,
nated and subordinated sentences
into sev-
to model e.g., ac-
eral,
tive/passive transformations (siddharthan, 2002;
chandrasekar and srinivas, 1997; canning, 2002;
siddharthan, 2011; siddharthan, 2010). while
these hand-crafted approaches can encode pre-
cise and linguistically well-informed syntactic
transformations, they do not account for lexical
simpli   cations and their interaction with the sen-
tential context. siddharthan and mandya (2014)
therefore propose an approach where hand-crafted
syntactic simpli   cation rules are combined with
lexical simpli   cation rules extracted from aligned
english and simple english sentences,
and
revision histories of simple wikipedia.

using the parallel dataset formed by simple en-
glish wikipedia (swkp)1 and traditional english
wikipedia (ewkp)2, further work has focused on
developing machine learning approaches to sen-
tence simpli   cation.
zhu et al. (2010)

parallel
wikipedia corpus (pwkp) of 108,016/114,924
complex/simple
sentences by aligning sen-
tences from ewkp and swkp and used the
resulting bitext to train a simpli   cation model
inspired by syntax-based machine translation
(yamada and knight, 2001). their simpli   cation
model encodes the probabilities for four rewriting

constructed

a

1http://simple.wikipedia.org
2http://en.wikipedia.org

operations on the parse tree of an input sentences
namely, substitution,
reordering, splitting and
deletion. it is combined with a language model to
improve grammaticality and the decoder translates
sentences into simpler ones by greedily selecting
the output sentence with highest id203.

a

quasi

describing

synchronous

using both the pwkp corpus developed by
zhu et al. (2010) and the edit history of sim-
ple wikipedia, woodsend and lapata (2011)
grammar
learn
(smith and eisner, 2006)
a
loose
alignment between parse trees of complex and
of simple sentences.
following dras (1999),
they then generate all possible rewrites for a
source tree and use integer id135 to
select the most appropriate simpli   cation. they
evaluate their model on the same dataset used by
zhu et al. (2010) namely, an aligned corpus of
100/131 ewkp/swkp sentences.

wubben et al. (2012),

a

trained

in this way,

coster and kauchak (2011) and xu et al. (2016)
saw simpli   cation as a monolingual
transla-
tion task where the complex sentence is the
source and the simpler one is the target. to
account for deletions, reordering and substitution,
coster and kauchak (2011)
phrase
based machine translation system on the pwkp
corpus while modifying the word alignment
output by giza++ in moses to allow for null
phrasal alignments.
they allow
for phrases to be deleted during translation.
similarly, wubben et al. (2012) used moses and
the pwkp data to train a phrase based machine
translation system augmented with a post-hoc
reranking procedure designed to rank the output
based on their dissimilarity from the source
sentence.
unlinke wubben et al. (2012) and
coster and kauchak (2011) who used machine
translation as a black box, xu et al. (2016) pro-
posed to modify the optimization function of
smt systems by tuning them for the sentence
simpli   cation task. however, in their work they
primarily focus on lexical simpli   cation.

finally, narayan and gardent (2014) present a
hybrid approach combining a probabilistic model
for sentence splitting and deletion with a statistical
machine translation system trained on pwkp for
substitution and reordering.

our proposal differs from all these approaches
in that it does not use the parallel pwkp cor-
pus for training. nor do we use hand-written

rules. another difference is that we use a deep
semantic representation as input for simpli   ca-
tion. while a similar approach was proposed
in (narayan and gardent, 2014), the probabilistic
models differ in that we determine splitting points
based on the maximum likelihood of sequences
of thematic role sets present in swkp whereas
narayan and gardent (2014) derive the probabil-
ity of a split from the aligned ewkp/swkp
corpus using expectation maximisation. as we
shall see in section 4, because their data is more
sparse, narayan and gardent (2014) predicts less
and lower quality simpli   cations by sentence split-
ting.

3 simpli   cation framework

our simpli   cation framework pipelines three ded-
icated modules inspired from previous work on
lexical simpli   cation, syntactic simpli   cation and
sentence compression. all three modules are un-
supervised.

3.1 example simpli   cation
before describing the three main modules of our
simpli   cation framework, we illustrate its work-
ing with an example. figure 1 shows the input
semantic representation associated with sentence
(1c) and illustrates the successive simpli   cation
steps yielding the intermediate and    nal simpli   ed
sentences shown in (1s1-s).

(1) c. in 1964 peter higgs published his second paper in
physical review letters describing higgs mechanism
which predicted a new massive spin-zero boson for the
   rst time.
s1 (lex simp).
in 1964 peter higgs wrote his sec-
ond paper in physical review letters explaining higgs
mechanism which predicted a new massive elementary
particle for the    rst time.
s2 (split). in 1964 peter higgs wrote his second pa-
per in physical review letters explaining higgs mech-
anism. higgs mechanism predicted a new massive ele-
mentary particle for the    rst time.
s (deletion).
in 1964 peter higgs wrote his paper
explaining higgs mechanism. higgs mechanism pre-
dicted a new elementary particle.

first, the input (1c) is rewritten as (1s1) by re-
placing standard words with simpler ones using
the context aware lexical simpli   cation method
proposed in (biran et al., 2011).

splitting is

then applied to the

seman-
tic
following
narayan and gardent (2014), we use boxer 3

representation

(1s1).

of

3http://svn.ask.it.usyd.edu.au/trac/candc,

version 1.00

in 1964 peter higgs published his second paper in physical review letters describing higgs mechanism which predicted

a new massive spin-zero boson for the    rst time .

lex simpl.

wwww(cid:127)

in 1964 peter higgs wrote his second paper in physical review letters explaining higgs mechanism which predicted

a new massive elementary particle for the    rst time .

x0

((

named(x0, higgs, per)
named(x0, peter, per)

   (

x1
male(x1)

   (

x2
second(x2)
paper(x2)
of(x2, x1)

x3

x4

   (

write(x3)

agent(x3, x0)
patient(x3, x2)

; (

named(x4, physical, org)
named(x4, review, org)
named(x4, letters, org)

   

x5

thing(x5)
event(x3)
in(x3, x4)
in(x3, x5)

timex(x5) = 1964

))))) ; (

x6

x7, x8

; (

mechanism(x8)

nn(x7, x8)

   

named(x7, higgs, org)

x9, x10, x11, x12
new(x9)

massive(x9)

elementary(x9)

particle(x9)
predict(x10)

event(x10)
explain(x11)
event(x11)
   rst(x12)
time(x12)

agent(x10, x8)
patient(x10, x9)
agent(x11, x6)
patient(x11, x8)
for(x10, x12)

))

[discourse representation structure produced by boxer]

node pos. in s

predicate/type

root

x3

r4

r1 r2

x0

x2

r3

r5

x11

r6

r7

r9

x4

x5

x6

x8

x9

o1

x10

r11

r10

x12

r8

x1

[drs graph representation]

x7

root

split

wwww(cid:127)

root

x0

x1

x2

x3

3, 4

6

higgs/per, peter/per

male/a

6, 7, 8

second/a, paper/n

5

x4 10, 11, 12

x5

x6

x7

x8

2

6, 7, 8

14

14, 15

write/v, event
physical/org

review/org, letters/org

thing/n, 1964

      

higgs/org

mechanism/n

x10

)

x9

r11

18, 19, 20

21, 22

new/a, elementary/a
massive/a, particle/n

r10

x9

x12

(

r1 r2

x3

r5

x11

r6

x0

r4

x2

x4

x5

x6

r7

x8

r3

x1

r8

x7

root

r9

x8

r8

x7

deletion

wwww(cid:127)

root

x10

)

r10

   

x
9

(

r1

x0

x3

r2

   

x
2

r3

x1

r5

r6

x11

x5

x6

r7

x8

r9

x8

r8

x7

r8

x7

in 1964 peter higgs wrote his

paper explaining higgs mechanism.

higgs mechanism predicted
a new elementary particle.

x10

x11

17

13

x12 24, 25, 26

o1

16

predict/v, event
explain/v, event
   rst/a, time/n
which/wdt

rel pos. in s

predicate

r1

r2

r3

r4

r5

r6

r7

r8

r9

r10

r11

5

5

6

9

1

13

13

      

17

17

23

agent, x3     x0

patient, x3     x2

of, x2     x1

in, x3     x4

in, x3     x5

agent, x11     x6

patient, x11     x8

nn, x8     x7

agent, x10     x8

patient, x10     x9

f or, x10     x12

figure 1: simpli   cation of    in 1964 peter higgs published his second paper in physical review letters
describing higgs mechanism which predicted a new massive spin-zero boson for the    rst time.   

(curran et al., 2007) to map the output sentence
from the lexical simpli   cation step (here s1)
to a discourse representation structure (drs,
(kamp, 1981)). the drs for s1 is shown at the
top of figure 1 and a graph representation4 of
the dependencies between its variables is shown
immediately below.
in this graph, each drs
variable labels a node in the graph and each edge
is labelled with the relation holding between the
variables labelling its end vertices. the two tables
to the right of the picture show the predicates
(top table) associated with each variable and the
relation label (bottom table) associated with each
edge. boxer also outputs the associated positions
in the complex sentence for each predicate
(not shown in the drs but shown in the graph
tables). orphan words i.e., words which have no
corresponding material in the drs (e.g., which
at position 16), are added to the graph (node o1)
thus ensuring that the position set associated with
the graph exactly generates the input sentence.

using probabilities over sequences of thematic
role sets acquired from the drs representations
of swkp, the split module determines where and
how to split the input drs. in this case, one split is
applied between x11 (explain) and x10 (predict).
the simpler sentences resulting from the split are
then derived from the drs using the word or-
der information associated with the predicates, du-
plicating or pronominalising any shared element
(e.g., higgs mechanism in figure 1) and deleting
any orphan words (e.g., which) which occurs at
the split boundary. splitting thus derives s2 from
s1.

finally, deletion or sentence compression ap-

plies transforming s2 into s3.

3.2 context-aware lexical simpli   cation
we extract context-aware lexical simpli   cation
rules from ewkp and swkp5 using the approach
described by biran et al. (2011). the underlying
intuition behind these rules is that the word c from
ewkp can be replaced with a word s from swkp

4the drs to graph conversion goes through several pre-
processing steps:
the relation nn is inverted making modi-
   er noun (higgs) dependent of modi   ed noun (mechanism),
named and timex are converted to unary predicates, e.g.,
named(x, peter) is mapped to peter(x) and timex(x) =
1964 is mapped to 1964(x); and nodes are introduced for
orphan words (e.g., which).

5we

downloaded

the

wikipedia
dated
glish wikipedia
http://dumps.wikimedia.org.

2013-12-31
dated

2014-01-01

snapshots
of

and

english
of
simple en-
available
at

if c and s share similar contexts (ten token win-
dow) in ewkp and swkp respectively. given an
input sentence and the set of simpli   cation rules
extracted from ewkp and swkp, we then con-
sider all possible (c, s) substitutions licensed by
the extracted rules and we identify the best com-
bination of lexical simpli   cations using dynamic
programming and rule scores which capture the
adequacy, in context, of each possible substitu-
tion6.

3.3 sentence splitting
approach
feature
a distinguishing
se-
deep
is
is
splitting
that
phrase
representations
mantic
structure
(zhu et al., 2010;
as
woodsend and lapata, 2011)     or dependency
trees     as in (siddharthan and mandya, 2014).

of
based

rather
in

our
on

trees

than

   

while woodsend and lapata (2011)

report
learning 438 splitting rules for their simpli   cation
approach operating on phrase structure trees
siddharthan and mandya (2014) de   nes 26 hand-
crafted rules for simplifying apposition and/or
relative clauses in dependency structures and 85
rules to handle subordination and coordination.

in contrast, we do not need to specify or to learn
complex rewrite rules for splitting a complex sen-
tence into several simpler sentences. instead, we
simply learn the id203 of sequences of the-
matic role sets likely to cooccur in a simpli   ed
sentence.

the intuition underlying our approach is that:

semantic representations give a clear handle
on events, on their associated roles sets and
on shared elements thereby facilitating both the
identi   cation of possible splitting points and the
reconstruction of shared elements in the sen-
tences resulting from a split.

for instance, the drs in figure 1 makes clear
that sentence (1s1) contains 3 main events and that
higgs mechanism is shared between two proposi-
tions.

to determine whether and where to split the in-
put sentence, we use a probabilistic model trained
on the drss of the simple wikipedia sentences
and a language model also trained on simple
wikipedia. given the event variables contained
in the drs of the input sentence, we consider

6for more details on the extraction of lexical simpli   ca-
tion rules, we refer the reader to biran et al. (2011). for more
details on the application of these rules using dynamic pro-
gramming, we refer the reader to narayan (2014).

all possible splits between subsequences of events
and choose the split(s) with maximum split score.
for instance, in the sentence shown in figure 1,
there are three event variables x3, x10 and x11
in the drs. so we will consider 5 split possi-
bilities namely, no split ({x3, x10, x11}), two
splits resulting in three sentences describing an
event each ({x3}, {x10}, {x11}) and one split
resulting in two sentences describing one and
two events respectively (i.e., ({x3}, {x10, x11}),
({x3, x10}, {x11}) and {x10}, {x3, x11}). the
split {x10}, {x3, x11} gets the maximum split
score and is chosen to split the sentence (1s1) pro-
ducing the sentences (1s2).

semantic pattern
h (agent, patient) i
h (agent, in, in, patient) i
h (agent, patient), (agent, in, in, patient) i

prob.
0.059
0.002
0.023

table 1: split feature table (sft) showing some of the
semantic patterns from figure 1.

formally, the split score psplit associated with
the splitting of a sentence s into a sequence of
sentences s1...sn is de   ned as:

psplit =

1
n x

si

lsplit

lsplit+ | lsplit     lsi |

   lmsi    sf tsi

where n is the number of sentences produced af-
ter splitting; lsplit is the average length of the split
sentences (lsplit = ls
n where ls is the length
of the sentence s); lsi is the length of the sen-
tence si; lmsi is the id203 of si given by
the language model and sf tsi is the likelihood of
the semantic pattern associated with si. the split
feature table (sft, table 1) is derived from the
corpus of drss associated with the swkp sen-
tences and the counts of sequences of thematic
role sets licenced by the drss of swkp sen-
tences.
intuitively, psplit favors splits involving
frequent semantic patterns (frequent sequences of
thematic role sets) and sub-sentences of roughly
equal length. this way of semantic pattern based
splitting also avoids over-splitting of a complex
sentence.

3.4 phrasal deletion
following filippova and strube (2008), we for-
mulate phrase deletion as an optimization prob-
lem which is solved using integer linear program-
ming7. given the drs k associated with a sen-
tence to be simpli   ed, for each relation r     k,

7in

our

implementation,

we

use

lp solve,

http://sourceforge.net/projects/lpsolve.

the deletion module determines whether r and its
associated drs subgraphs should be deleted by
maximising the following objective function:

xr

h,w  p (r|h)  p (w)

r 6    {agent, patient, theme, eq}

x

x

where for each relation r     k, xr

h,w = 1 if r
h,w = 0 otherwise; p (r|h) is
is preserved and xr
the id155 (estimated on the drs
corpus derived from swkp) of r given the head
label h; and p (w) is the relative frequency of w in
swkp8.

intuitively,

this objective function will favor
obligatory dependencies over optional ones and
simple words (i.e., words that are frequent
in
swkp). in addition, the objective function is sub-
jected to constraints which ensure (i) that some
deletion takes place and (ii) that the resulting drs
is a well-formed graph.

4 evaluation

we evaluate our approach both globally and by
module focusing in particular on the splitting com-
ponent of our simpli   cation approach.

4.1 global evaluation

supervised systems

the testset provided by zhu et al. (2010) was
used by four
for auto-
matic evaluation using metrics such as id7,
sentence length and number of edits.
in ad-
dition, most
recent simpli   cation approaches
carry out a human evaluation on a small
set
complex/simple
thus wubben et al. (2012),
sentence pairs.
narayan and gardent (2014)
and
siddharthan and mandya (2014)
a
human evaluation on 20, 20 and 25 sentences
respectively.

randomly

selected

carry

out

of

accordingly, we perform an automatic com-
parative evaluation using (zhu et al., 2010)   s test-
set namely,
an aligned corpus of 100/131
ewkp/swkp sentences; and we carry out a
human-based evaluation.

8to account for modi   ers which are represented as predi-
cates on nodes rather than relations, we preprocess the drss
and transform each of these predicates into a single node sub-
tree of the node it modi   es. for example in figure 1, the node
x2 labeled with the modi   er predicate second is updated to a
new node x    
2 dominating a child labeled with that predicate
and related to x    

2 by a modi   er relation.

levenshtein id153

system

gold
zhu
woodsend
wubben
narayan
unsup

complex
system

to

ld no edit
3
2
24
6
4
3

12.24
7.87
8.63
3.33
6.32
6.75

system to sim-
ple

ld no edit
100
0
2
2
3
0

0
14.64
16.03
13.57
11.53
14.29

id7
w.r.t simple

sentences
with splits

average
sentence
length

average
token
length

100
37.4
42
41.4
53.6
38.47

28
80
63
1
10
49

27.80
24.21
28.10
28.25
26.24
26.22

4.40
4.38
4.50
4.41
4.36
4.40

table 2: automatic evaluation results. zhu, woodsend, wubben, narayan are the best output of the models of zhu et al.
(2010), woodsend and lapata (2011), wubben et al. (2012) and narayan and gardent (2014) respectively. unsup is our
model.

system

complex
lexsimpl
split
deletion
lexsimpl-split
lexsimpl-deletion
split-deletion
lexsimpl-split-deletion
gold (simple)

levenshtein id153

complex
system

to

ld no edit
100
22
51
4
11
3
4
3
3

0
2.07
2.27
2.39
4.43
4.29
4.63
6.75
12.24

system to sim-
ple

ld no edit
3
1
1
0
0
0
0
0
100

12.24
13.00
13.62
12.34
14.39
13.09
13.42
14.29
0

id7 scores
with respect to

complex
100
82.05
89.70
85.15
73.20
69.84
77.82
63.41
49.85

simple
49.85
44.29
46.15
47.33
41.18
41.91
43.44
38.47
100

average
sentence
length

average
token
length

27.80
27.80
29.10
25.41
29.15
25.42
26.19
26.22
23.38

4.62
4.46
4.63
4.54
4.48
4.38
4.55
4.40
4.40

table 3: automated metrics for simpli   cation: modular evaluation. lexsimpl-split-deletion is our    nal system unsup.

automatic
evaluation following
wubben et al. (2012),
zhu et al. (2010)
and
woodsend and lapata (2011), we use metrics
that are directly related to the simpli   cation task
namely, the number of splits in the overall data,
the number of output sentences with no edits (i.e.,
sentences which have not been simpli   ed) and
the average levenshtein distance (ld) between
the system output and both the complex and the
simple reference sentences. we use id79 as a
means to evaluate how close the systems output
are to the reference corpus.

table 2 shows the results of the automatic eval-
uation. the most noticeable result is that our un-
supervised system yields results that are similar to
those of the supervised approaches.

the results also show that, in contrast to wood-
send system which often leaves the input unsim-
pli   ed (24% of the input), our system almost al-
ways modi   es the input sentence (only 3% of the
input are not simpli   ed); and that the number of
simpli   cations including a split is relatively high
(49% of the cases) suggesting a good ability to
split complex sentences into simpler ones.

human evaluation human judges were asked
to rate input/output pairs w.r.t. to adequacy (how
much does the simpli   ed sentence(s) preserve the

meaning of the input?), to simpli   cation (how
much does the generated sentence(s) simplify the
complex input?) and to    uency (how grammatical
and    uent are the sentences?).

we randomly selected 18 complex sentences
from zhu   s test corpus and included in the eval-
uation corpus:
the corresponding simple (gold)
sentence from zhu   s test corpus, the output of our
system (unsup) and the output of the other four
systems (zhu, woodsend, narayan and wubben)
which were provided to us by the system au-
thors10. we collected ratings from 18 partici-
pants. all were either native speakers or pro   cient
in english, having taken part in a master taught
in english or lived in an english speaking coun-
try for an extended period of time. the evalu-
ation was done online using the lg-eval toolkit
(kow and belz, 2012)11 and a latin square ex-
perimental design (lsed) was used to ensure a
fair distribution of the systems and the data across
raters.

table 4 shows the average ratings of the hu-
man evaluation on a scale from 0 to 5. pair-
wise comparisons between all models and their
statistical signi   cance were carried out using a
one-way anova with post-hoc tukey hsd tests.

9moses

support

tools:

multi-id7

mentary material with this paper.

http://www.statmt.org/moses/?n=moses.supporttools.

11http://www.nltg.brighton.ac.uk/research/lg-eval/

10we upload the outputs from all the systems as supple-

system pairs

a

b

all-a

average score (number of split sentences)
all-b only-a

both-ab

unsup

gold
zhu
woodsend
wubben
narayan

2.37(49)

3.85(28)
2.25(80)
2.08(63)
2.73(1)
2.09(10)

2.15(32)
1.53(4)
2.42(11)
2.32(48)
2.29(41)

a
2.80(17)
2.45(45)
2.36(38)
4.75(1)
2.78(8)

b
3.70(17)
2.42(45)
2.29(38)
2.73(1)
1.79(8)

only-b

4.05(11)
2.02(35)
1.78(25)
0(0)
3.81(2)

table 5: pairwise split evaluation: each row shows the pairwise comparison of the quality of splits in unsup and some other
system. last six columns show the average scores and number of associated split sentences. the second column (all-a)
and the third column (all-b) present the quality of all splits by systems a and b respectively. the fourth column (only-a)
represents sentences where a splits but not b. the    fth and sixth columns represents sentences where both a and b split. the
seventh column (only-b) represents sentences where b splits but not a.

systems
gold
zhu
woodsend
wubben
narayan
unsup

simplicity
3.62
2.62
1.69
1.52
2.30
2.83

fluency adequacy
3.80
2.47
3.15
3.38
3.35
2.83

4.69
2.56
3.15
3.05
3.03
3.56

table 4: average human ratings for simplicity,    uency
and adequacy.

if we group together systems for which there is
no signi   cant difference (signi   cance level: p <
0.05), our system is in the    rst group together
with narayan and zhu for simplicity; in the    rst
group for    uency; and in the second group for ade-
quacy (together with woodsend and zhu). a man-
ual examination of the results indicates that un-
sup achieves good simplicity rates through both
deletion and sentence splitting. indeed, the aver-
age word length of simpli   ed sentences is smaller
for unsup (26.22) than for wubben (28.25)
and woodsend (28.10); comparable with narayan
(26.19) and higher only than zhu (24.21).

4.2 modular evaluation

to assess the relative impact of each module (lexi-
cal simpli   cation, deletion and sentence splitting),
we also conduct an automated evaluation on each
module separately. the results are shown in ta-
ble 3.

one    rst observation is that each module has an
impact on simpli   cation. thus the average lev-
enshtein id153 (ld) to the source clause
(complex) is never null for any module while the
number of    no edit    indicates that lexical simpli-
   cation modi   es the input sentence in 78%, sen-
tence splitting 49% and deletion 96% of the cases.
in terms of output quality and in particular, sim-
ilarity with respect to the target clause, deletion is
the most effective (smallest ld, best id7 score
w.r.t. target). further, the results for average token
length indicate that lexical simpli   cation is effec-

tive in producing shorter words (smaller average
length for this module compared to the other two
modules).

predictably, combining modules yields systems
that have stronger impact on the source clause
(higher ld to complex, lower number of no ed-
its) with the full system (i.e., the system combin-
ing the 3 modules) showing the largest ld to the
sources (ld to complex) and the smallest number
of source sentences without simpli   cation (3 no
edits).
4.3 sentence splitting using deep semantics
to compare our sentence splitting approach with
existing systems, we collected in a second human
evaluation, all the outputs for which at least one
system applied sentence splitting. the raters were
then asked to compare pairs of split sentences pro-
duced by two distinct systems and to evaluate the
quality (0:very bad to 5:very good) of these split
sentences taking into account boundary choice,
sentence completion and sentence reordering.

table 5 shows the results of this second evalua-
tion. for each system pair comparing unsup (a)
with another system (b), the table gives the scores
and the number of splits of both systems: for the
inputs on which both systems split (both-ab),
on which only unsup splits (only-a) and on
which only the compared system split (only-b).
unsup achieves a better average score (all-
a = 2.37) than all other systems (all-b column)
except wubben (2.73). however wubben only
achieves one split and on that sentence, unsup
score is 4.75 while wubben has a score of 2.73
and produces an incorrect split (cf. s3 in figure 6).
unsup

in terms of numbers of splits, three systems of-
ten simplify by splitting namely zhu (80 splits),
woodsend (63) and unsup (49).
interest-
ingly, narayan, trained on the parallel corpus of
wikipedia and simpli   ed wikipedia splits less of-

s1 complex. this array distributes data across multiple disks, but the array is seen by the computer user and operating

system as one single disk.
zhu. this array sells data across multiple disks but the array is seen. the computer user and operating as one disk. [0]
woodsend. this array distributes data across several disks. the array is seen by the computer user and operating
system as one single disk. [4.85]
unsup. this array moves data across disks. the array is seen by the computer user and operating system as one disk.
[4.83]

s2 complex. here, simulations of evolution using evolutionary algorithms and arti   cial life started with the work of
nils aall barricelli in the 1960s, and was extended by alex fraser, who published a series of papers on simulation of
arti   cial selection.
zhu. here, simulations of evolution using evolutionary algorithms and arti   cial life started with the work of nils aall
barricelli in the 1960s, and was extended by alex fraser. alex fraser published a series of papers on feeling of arti   cial
selection. [3.6]
woodsend. it has id107. evolution is using arti   cial life. this is nils aall barricelli in the 1960s. here,
simulations started with the work. he was extended by alex fraser. he published a group of papers on simulation of
arti   cial selection. [0.33]
unsup. here, models of evolution using evolutionary algorithms and arti   cial life started with the work in the 1960s,
and was led by alex fraser. alex fraser wrote a series of papers on model of selection. [5]

s3 complex. by 1928, the regional government was moved from the old cossack capital novocherkassk to rostov, which

also engulfed the nearby armenian town of nor nakhijevan.
zhu. by 1928, the government was moved from the old cossack capital novocherkassk to rostov. rostov also of the
city the nearby armenian town of nor nakhijevan. [2.8]
woodsend. by 1928, the regional government was moved from the old cossack capital novocherkassk to rostov. both
also engulfed the nearby armenian town of nor nakhijevan. [3]
wubben. by 1928 , the regional government was moved from the old cossack capital novocherkassk to rostov. the
nearby armenian town of nor nakhichevan. [2.7]
narayan. by 1928, the regional government was moved from the old cossack capital novocherkassk to rostov. rostov
that engulfed the nearby armenian town of nor nakhichevan. [2.7]
unsup. the regional government was moved from the old cossack capital novocherkassk to rostov. rostov also
absorbed the nearby town of nor nakhijevan. [4.75]

table 6: example outputs for sentence splitting with their average human annotation scores.

ten (10 splits vs 49 for unsup) and less well (2.09
average score versus 2.37 for unsup). this is un-
surprising as the proportion of splits in swkp was
reported in (narayan and gardent, 2014) to be a
low 6%.
in contrast, the set of observations we
use to learn the splitting id203 is the set of
all sequences of thematic role sets derived from
the drss of the swkp corpus.

in sum, the unsupervised, semantic-based split-
ting strategy allows for a high number (49%) of
good quality (2.37 score) sentence splits . be-
cause there are less possible patterns of thematic
role sets in simple sentences than possible con-
   gurations of parse/dependency trees for complex
sentences, it is less prone to data sparsity than the
syntax based approach. because the probabilities
learned are not tied to speci   c syntactic structures
but to more abstract semantic patterns, it is also
perhaps less sensitive to parse errors.

4.4 examples from the test set
table 6 shows some examples from the evaluation
dataset which were selected to illustrate the work-
ings of our approach and to help interpret the re-
sults in table 2, 4 and 5.

s1 and s2 and s3 show examples of context-
aware unsupervised lexical substitutions which are

nicely performed by our system. in s1, the array
distributes data is correctly simpli   ed to the ar-
ray moves data whereas zhu   s system incorrectly
simpli   es this clause to the array sells data. simi-
larly, in s2, our system correctly simpli   es papers
on simulation of arti   cial selection to papers on
models of selection while the other systems either
do not simplify or simplify to papers on feeling.

for splitting,

the examples show two types
of splitting performed by our approach namely,
splitting of coordinated sentences (s1) and split-
ting between a main and a relative clause
(s2,s3). s2 illustrates how the woodsend sys-
tem over-splits, an issue already noticed in
(siddharthan and mandya, 2014); and how zhu   s
system predicts an incorrect split between a verb
(seen) and its agent argument (by the user).
barring a parse error, such incorrect splits will
not be predicted by our approach since, in our
cases, splits only occur between (verbalisations of)
events. s1, s2 and s3 also illustrates how our se-
mantic based approach allows for an adequate re-
construction of shared elements.

5 conclusion

a major limitation for supervised simpli   cation
systems is the limited amount of available paral-

lel standard/simpli   ed data. in this paper, we have
shown that it is possible to take an unsupervised
approach to sentence simpli   cation which requires
a large corpus of standard and simpli   ed language
but no alignment between the two. this allowed
for the implementation of contextually aware sub-
stitution module; and for a simple, linguistically
principled account of sentence splitting and shared
element reconstruction.

6 acknowledgements

we are grateful to zhemin zhu, kristian wood-
send and sander wubben for sharing their data.
we would like to thank our annotators for partic-
ipating in our human evaluation experiments and
to anonymous reviewers for their insightful com-
ments. this research was supported by an ep-
src grant (ep/l02411x/1) and an eu h2020
grant (688139/h2020-ict-2015; summa). we
also thank the french national research agency
for funding the research presented in this paper in
the context of the webid86 project.

references
[biran et al.2011] or biran, samuel brody, and no  emie
elhadad. 2011. putting it simply: a context-aware
approach to lexical simpli   cation. in proceedings of
the 49th annual meeting of the association for com-
putational linguistics: human language technolo-
gies: short papers-volume 2, pages 496   501. asso-
ciation for computational linguistics.

[canning2002] yvonne margaret canning. 2002. syn-
tactic simpli   cation of text. ph.d. thesis, university
of sunderland.

[carroll et al.1999] john carroll, guido minnen, dar-
ren pearce, yvonne canning, siobhan devlin, and
john tait. 1999. simplifying text for language-
impaired readers.
in proceedings of 9th confer-
ence of the european chapter of the association
for computational linguistics (eacl), volume 99,
pages 269   270. citeseer.

[chandrasekar and srinivas1997] raman chandrasekar
and bangalore srinivas. 1997. automatic induction
of rules for text simpli   cation. knowledge-based
systems, 10(3):183   190.

[chandrasekar et al.1996] raman chandrasekar, chris-
tine doran, and bangalore srinivas. 1996. mo-
tivations and methods for text simpli   cation.
in
proceedings of the 16th international conference on
computational linguistics (coling), pages 1041   
1044. association for computational linguistics.

[coster and kauchak2011] william coster and david
kauchak. 2011. learning to simplify sentences

using wikipedia.
in proceedings of the workshop
on monolingual text-to-text generation, pages 1   
9. association for computational linguistics.

[curran et al.2007] james r curran, stephen clark, and
johan bos. 2007. linguistically motivated large-
scale nlp with c&c and boxer. in proceedings of
the 45th annual meeting of the association for com-
putational linguistics (acl) on interactive poster
and demonstration sessions, pages 33   36. associa-
tion for computational linguistics.

[dras1999] mark dras. 1999. id34
and the reluctant id141 of text. ph.d. thesis,
macquarie university nsw 2109 australia.

filippova

[filippova and strube2008] katja

and
2008. dependency tree based
michael strube.
sentence compression.
the
fifth international id86
conference (iid86), pages 25   32. association for
computational linguistics.

in proceedings of

[kamp1981] hans kamp. 1981. a theory of truth
and semantic representation. in j.a.g. groenendijk,
t.m.v. janssen, b.j. stokhof, and m.j.b. stokhof,
editors, formal methods in the study of language,
number pt. 1 in mathematical centre tracts. mathe-
matisch centrum.

[knight and marcu2000] kevin knight and daniel
marcu. 2000. statistics-based summarization-step
one: sentence compression. in proceedings of the
seventeenth national conference on arti   cial in-
telligence (aaai) and twelfth conference on inno-
vative applications of arti   cial intelligence (iaai),
pages 703   710. aaai press.

[kow and belz2012] eric kow and anja belz. 2012.
lg-eval: a toolkit for creating online language
evaluation experiments. in proceedings of the 8th
international conference on language resources
and evaluation (lrec), pages 4033   4037.

[narayan and gardent2014] shashi narayan and claire
gardent. 2014. hybrid simpli   cation using deep
semantics and machine translation. in proceedings
of the 52nd annual meeting of the association for
computational linguistics. association for compu-
tational linguistics.

[narayan2014] shashi narayan. 2014. generating and
simplifying sentences. ph.d. thesis, universit  e de
lorraine.

[shardlow2014] matthew shardlow. 2014. a survey
of automated text simpli   cation. international jour-
nal of advanced computer science and applications
(ijacsa), special issue on natural language pro-
cessing.

[siddharthan and mandya2014] advaith

siddharthan
and angrosh mandya. 2014. hybrid text simpli   ca-
tion using synchronous dependency grammars with
hand-written and automatically harvested rules. in
proceedings of the 14th conference of the european

text simpli   cation. transactions of the association
for computational linguistics.

[yamada and knight2001] kenji yamada and kevin
2001. a syntax-based statistical trans-
knight.
lation model.
in proceedings of the 39th annual
meeting on association for computational linguis-
tics (acl), pages 523   530. association for compu-
tational linguistics.

[zhu et al.2010] zhemin zhu, delphine bernhard, and
iryna gurevych. 2010. a monolingual tree-based
translation model for sentence simpli   cation.
in
proceedings of the 23rd international conference on
computational linguistics (coling), pages 1353   
1361, stroudsburg, pa, usa. association for com-
putational linguistics.

chapter of
the association for computational
linguistics, pages 722   731, gothenburg, sweden,
april. association for computational linguistics.

[siddharthan2002] advaith siddharthan. 2002. an ar-
chitecture for a text simpli   cation system. in pro-
ceedings of the language engineering conference
(lec), pages 64   71. ieee computer society.

[siddharthan2010] advaith siddharthan. 2010. com-
plex lexico-syntactic reformulation of sentences us-
ing typed dependency representations. in proceed-
ings of the 6th international natural language gen-
eration conference (iid86), pages 125   133. associ-
ation for computational linguistics.

[siddharthan2011] advaith siddharthan. 2011. text
simpli   cation using typed dependencies: a compar-
ison of the robustness of different generation strate-
gies. in proceedings of the 13th european workshop
on id86 (eid86), pages 2   
11. association for computational linguistics.

[smith and eisner2006] david a smith and jason eis-
ner. 2006. quasi-synchronous grammars: align-
ment by soft projection of syntactic dependencies.
in proceedings of the hlt-naacl workshop on
id151, pages 23   30. as-
sociation for computational linguistics.

[vickrey and koller2008] david vickrey and daphne
koller. 2008. sentence simpli   cation for seman-
tic role labeling. in proceedings of the 46th annual
meeting of the association for computational lin-
guistics (acl) and the human language technol-
ogy conference (hlt), pages 344   352.

[watanabe et al.2009] willian massami watanabe, ar-
naldo candido junior, vin    cius rodriguez uz  eda,
renata pontin de mattos fortes, thiago alexan-
dre salgueiro pardo, and sandra maria alu    sio.
2009. facilita: reading assistance for low-literacy
readers.
in proceedings of the 27th acm inter-
national conference on design of communication,
pages 29   36. acm.

[woodsend and lapata2011] kristian woodsend and
mirella lapata. 2011. learning to simplify sen-
tences with quasi-synchronous grammar and integer
programming. in proceedings of the conference on
empirical methods in natural language processing
(emnlp), pages 409   420. association for compu-
tational linguistics.

[wubben et al.2012] sander wubben, antal van den
bosch, and emiel krahmer. 2012. sentence sim-
pli   cation by monolingual machine translation. in
proceedings of the 50th annual meeting of the asso-
ciation for computational linguistics (acl): long
papers-volume 1, pages 1015   1024. association for
computational linguistics.

[xu et al.2016] wei xu, courtney napoles, ellie
pavlick, quanze chen, and chris callison-burch.
2016. optimizing id151 for

