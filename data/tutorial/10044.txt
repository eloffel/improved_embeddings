fast k-best sentence compression

katja filippova & enrique alfonseca

google research

katjaf|ealfonseca@google.com

5
1
0
2

 
t
c
o
8
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
1
4
8
0

.

0
1
5
1
:
v
i
x
r
a

abstract

a popular approach to sentence compression
is to formulate the task as a constrained opti-
mization problem and solve it with integer lin-
ear programming (ilp) tools. unfortunately,
dependence on ilp may make the compres-
sor prohibitively slow, and thus approxima-
tion techniques have been proposed which are
often complex and offer a moderate gain in
speed. as an alternative solution, we intro-
duce a novel compression algorithm which
generates k-best compressions relying on lo-
cal deletion decisions. our algorithm is two
orders of magnitude faster than a recent ilp-
based method while producing better com-
pressions. moreover, an extensive evaluation
demonstrates that the quality of compressions
does not degrade much as we move from sin-
gle best to top-   ve results.

introduction

1
there has been a surge in sentence compression re-
search in the past decade because of the promise it
holds for extractive text summarization and the util-
ity it has in the age of mobile devices with small
screens. similar to text summarization, extractive
approaches which do not introduce new words into
the result have been particularly popular. there, the
main challenge lies in knowing which words can be
deleted without negatively affecting the information
content or grammaticality of the sentence. given
the complexity of the compression task (the num-
ber of possible outputs is exponential), many sys-
tems frame it, sometimes combined with summa-
rization, as an ilp problem which is then solved

with off-the-shelf tools (martins & smith, 2009;
berg-kirkpatrick et al., 2011; thadani & mckeown,
2013). while ilp formulations are clear and the
translation to an ilp problem is often natural (clarke
& lapata, 2008), they come with a high solution
cost and prohibitively long processing times (wood-
send & lapata, 2012; almeida & martins, 2013).
thus, robust algorithms capable of generating infor-
mative and grammatically correct compressions at
much faster running times are still desirable.

towards this goal, we propose a novel supervised
sentence compression method which combines lo-
cal deletion decisions with a recursive procedure of
getting most probable compressions at every node
in the tree. to generate the top-scoring compres-
sion a single tree traversal is required. to extend
the k-best list with a k + 1th compression, the algo-
rithm needs o(m    n) comparisons where n is the
node count and m is the average branching factor
in the tree.
importantly, approximate search tech-
niques like id125 (galanis & androutsopou-
los, 2010; wang et al., 2013), are not required.

compared with a recent ilp method (filippova &
altun, 2013), our algorithm is two orders of magni-
tude faster while producing shorter compressions of
equal quality. both methods are supervised and use
the same training data and features. the results in-
dicate that good readability and informativeness, as
perceived by human raters, can be achieved without
impairing algorithm ef   ciency. furthermore, both
scores remain high as one moves from the top result
to the top    ve. to our knowledge we are the    rst to
report evaluation results beyond single best output.
to address cases where local decisions may be in-

suf   cient, we present an extension to the algorithm
where we tradeoff the guarantee of obtaining the top
scoring solution for the bene   t of scoring a node
subset as a whole. this extension only moderately
affects the running time while eliminating a source
of suboptimal compressions.

comparison to related work many compression
systems have been introduced since the very    rst
approaches by grefenstette (1998), jing & mck-
eown (2000) and knight & marcu (2000). al-
most all of them make use of syntactic information
(e.g., clarke & lapata (2006), mcdonald (2006),
toutanova et al. (2007)), and our system is not an
exception. like nomoto (2009), wang et al. (2013)
we operate on syntactic trees provided by a state-
of-the-art parser. the bene   t of modifying a given
syntactic structure is that the space of possible com-
pressions is signi   cantly constrained: instead of all
possible token subsequences, the search space is re-
stricted to all the subtrees of the input parse. while
some methods rewrite the source tree and produce
an alternative derivation at every consituent (knight
& marcu, 2000; galley & mckeown, 2007), oth-
ers prune edges in the source tree (filippova &
strube, 2008; galanis & androutsopoulos, 2010;
wang et al., 2013). most of these approaches are su-
pervised in that they learn from a parallel compres-
sion corpus either the rewrite operations, or deletion
decisions.
in our work we also adopt the pruning
approach and use parallel data to estimate the prob-
ability of deleting an edge given context.

several text-to-text generation systems use ilp
as an optimization tool to generate new sentences
by combining pieces from the input (clarke & lap-
ata, 2008; martins & smith, 2009; woodsend et al.,
2010; filippova & altun, 2013). while off-the-
shelf general purpose lp solvers are designed to be
fast, in practice they may make the compressor pro-
hibitively slow, in particular if compression is done
jointly with summarization (berg-kirkpatrick et al.,
2011; qian & liu, 2013; thadani, 2014). recent
improvements to the ilp-based methods have been
signi   cant but not dramatic. for example, thadani
(2014) presents an approximation technique result-
ing in a 60% reduction in average id136 time.
compared with this work, the main practical ad-
vantage of our system is that it is very fast with-

out trading compression quality for speed improve-
ments. on the modeling side, it demonstrates that
local decisions are suf   cient to produce an informa-
tive and grammatically correct sentence.

our recursive procedure of generating k best com-
pressions at every node is partly inspired by frame
semantics (fillmore et al., 2003) and its extension
from predicates to any node type (titov & klemen-
tiev, 2011). the core idea is that there are two com-
ponents to a high-quality compression at every node
in the tree: (1) it should keep all the essential ar-
guments of that node; (2) these arguments should
themselves be good compressions. this motivates
an algorithm with a recursively de   ned scoring func-
tion which allows us to obtain k-best compressions
nearly as fast as the single best one. in this respect
our algorithm is similar to the k-best parsing algo-
rithm by huang & chiang (2005).

2 the top-down approach

our approach is syntax-driven and operates on de-
pendency trees (sec. 2.1). the input tree is pruned
to obtain a valid subtree from which a compression
is read off. the pruning decisions are carried out
based on predictions of a maximum id178 classi-
   er which is trained on a parallel corpora with a rich
feature set (sec. 2.2). section 2.3 explains how to
generate the single, top-scoring compression; sec-
tion 2.4 extends the idea to arbitrary k.

2.1 preprocessing
similar to previous work, we have a special treat-
ment for function words like determiners, preposi-
tions, auxiliary verbs. unsurprisingly, dealing with
function words is much easier than deciding whether
a content word can be removed. approaches which
use a constituency parser and prune edges pointing
to constituents, deal with function words implicitly
(berg-kirkpatrick et al., 2011; wang et al., 2013).
approaches which use a dependency representation
either formulate hard constraints (almeida & mar-
tins, 2013), or collapse function words with their
heads. we use the latter approach and transform ev-
ery input tree (nivre, 2006) following filippova &
strube (2008) and also add edges from the dummy
root to    nite verbs. finally, we run an entity tagger
and collapse nodes referring to entities.

root

subj
root the police
-1

1

root

root

rcmod

ccomp

subj

in

subj

dobj

tmod

at

said
2

the man who
5

4

robbed

a bank

in arizona was arrested

at his home

6

8

10

12

15

coref

amod

late friday
16

17

figure 1: transformed parse tree of the example sentence. the compression subtree is highlighted with blue color.

figure 1 provides an example of a transformed
tree with extra edges from the dummy root node
and an undirected coreference edge for the follow-
ing sentence to which we will refer throughout this
section:

(1) the police said the man who robbed a bank in

arizona was arrested at his home late friday.

2.2 estimating deletion probabilities
the supervised component of our system is a binary
maximum id178 classi   er (berger et al., 1996)
which is trained to estimate the id203 of delet-
ing an edge given its local context. in what follows,
we are going to refer to two probabilities, pdel(e)
and pret(e):

pdel(en,m) + pret(en,m) = 1,

(1)
where del stands for deleting, ret stands for retain-
ing edge e from node n to node m, and pdel(en,m) is
estimated with maxent.

the features we use are inspired by most recent
work (almeida & martins, 2013; filippova & altun,
2013; wang et al., 2013) and are as follows:

syntactic: edge labels for the child and its siblings;

ne type and pos tags;

lexical: head and child lemmas; negation; concate-

nation of parent lemmas and labels;

numeric: depth; node length in words and charac-
ters; children count for the parent and the child.

the compression subtrees provide all the negative
items for training (blue edges in fig. 1). the pos-
itive items are all other edges originating from the
nodes in the compression (red edges). the remain-
ing edges (black) cannot be used for training.

although we chose to implement hard constraints
for function words (see sec. 2.1 above), we could
also apply no tree transformations and instead ex-
pect the classi   er to learn that, e.g., the probabil-
ity of deleting an edge pointing to a determiner is
zero. however, given the universality of these rules,
it made more sense to us to encode them as prepro-
cessing transformations.

2.3 obtaining top-scoring compression
to    nd the best compression of the sentence we start
at the dummy root node and select a child n with
the highest pret(eroot,n). the root of the example
tree in figure 1 has three children (sa , robbed6,
was arrested12). assuming that pret   s for the three
predicates are .07, .5, .9, the third child is selected.
from there, we recursively continue in a top-down
manner and at every node n whose children are m =
{m1, m2, ...} search for a children subset cn     m
maximizing

score(cn) =

log pdel(en,m)

log pret(en,m).

(2)

(cid:88)
(cid:88)

m   cn

m   m\cn

+

note that no feature refers to the compression gener-
ated so far and therefore the id203 of removing
an edge needs to be calculated only once on a    rst
tree traversal.

assuming that we have a training set comprising
pairs of a transformed tree, like the one in figure
1, and a compression subtree (e.g., the subtree cov-
ering all the nodes from the man to at his home),

since pdel and pret sum to one, this implies that ev-
ery edge with pret < 0.5 is deleted. however, we
can take any        [0, 1] to be a threshold for deciding
between keeping vs. deleting an edge and linearly
scale pdel and pret so that after scaling   pdel < 0.5
if and only if pdel <   . of course,    nding a single
   value that would be universally optimal is hardly
possible and we will return to this point in sec. 3.

consider the node was arrested in figure 1 and
its three children listed in table 1 with pret given in
brackets.

when k     {   1, 0}, i.e., when we either delete a
child or take its best compression, the score is the
familiar probabilities:

the man4 (1.0)

was arrested12

at his home15 (.22) friday17 (.05)

score(ck

m) =

log pdel(en,m)
log pret(en,m)

if k =    1
if k = 0

(6)

(cid:40)

table 1: arguments of was arrested with their pret   s.
with    = 0.5, the top scoring subset is c12 = {4},
its score being 0 + log .78 + log .95. the next step
is to decide whether node 4 (the man) should retain
its relative clause modi   er or not. there is no need
to go further down the friday node and consider the
score of its sole argument (late).

2.4 from top-scoring to k-best
a single best compression may appear too long or
too short, or fail to satisfy some other requirement.
in many cases it is desirable to have a pool of k-best
results to choose from and in this subsection we will
present our algorithm for ef   ciently generating a k-
best list (summarized in fig. 2).

first, let us slightly modify the notation used up
to this point to be able to refer to the kth best result
instead of cn     m, we are going to
at node n.
n, where k     n     {   1}. unlike cn, every
use ck
n is an ordered sequence of exactly |m| elements,
ck
corresponding to n   s children:

ck

n = [ck1

m1, ck2

m2, ..., c

k|m|
m|m|].

(3)

for every child mi not retained in the compression,
the superscript ki is -1. for example, for the single-
ton subset c12 containing only node 4 in the previ-
ous subsection the corresponding best result c0
12 is:

c0

12 = [c0

4 , c   1

15 , c   1
17 ].

(4)

note that at this point we do not need to know what
4 actually is. we simply state that the best result
c0
for node 12 must include the best result for node 4.
n is the averaged sum
of the scores of n   s chlidren and must be decreasing
over k     0 (score(ck+a

the scoring function for ck

)     score(ck

n), a > 0):

n

greater values of k correspond to k+1   th best result
at node n. consider again node 12 from table 1.
the k-best results at that node may include any of
the following variants (the list is not complete):
4 , c   1
[c 0
15 , c 0
17],
4 , c   1
17].
[c 1
15, c 0
how should these be scored so that high quality
compressions are ranked higher? our assumption
is that the quality of a compression at any node is
subject to the following two conditions:

15 , c   1
17 ],
15 , c 1
17],

15, c   1
17 ],
15, c 1
17]

4 , c   1
4 , c 0

4 , c 0
4 , c 0

[c 2
[c   1

[c 0
[c 0

1. the child subset includes essential arguments
and does not include those that can be omitted.

2. the variants for the children retained in the
compression are themselves high-quality com-
pressions.

6], cl

8 , c0

4 = [cl

5 , c   1

for example, a compression at node 12 which
deletes the    rst child (the man) is of a poor qual-
ity because it misses the subject and thus violates
the    rst condition. a compression which retains
the    rst node but with a misleading compression,
like the man robbed in arizona (ck
6 =
[c   1
10]), is not good either because it vio-
lates the second condition, which is in turn due to the
   rst condition being violated in cl
6. hence, a robust
scoring function should balance these two consid-
erations and promote variants with good compres-
sion at every node retained. note that for    nding the
single best result it is suf   cient to focus on the    rst
condition only, ignoring the second one, because the
best possible result is returned for every child, and
the scoring function in eq. 2 does exactly that. how-
ever, once we begin to generate more than a single
best result, we start including compressions which
may no longer be optimal. so the main challenge in
extending the scoring function lies in how to propa-
gate the scores from node   s descendants so that both
conditions are satis   ed.

score(ck

n) =

1
|m|

c

score(cki

mi).

(5)

given the best result at node n, which is obtained
in a single pass (sec. 2.3), the second best result
must be one of the following:

(cid:88)

ki

mi   ck

n

    the next best scoring child subset whose score
we know how compute from eq. (5-6) (e.g., for
node 12 it would be [c0
17 ], see eq. 4).
    a subset of the same children as the best one
but with one of ki   s which were 0 in the best
result increased to 1 (e.g., for node 12 it would
be [c1

15, c   1

17 ], see eq. 4):

15 , c   1

4 , c   1

4 , c0

no other variant can have a higher score than ei-
ther of these. unless there is a tie in the scores,
there is a single new second-best subset. and it
follows from the decreasing property and the de   -
nition of the scoring function that if more than a sin-
gle ki is increased from zero, the score is lower than
when only one of the ki   s is modi   ed. for example,
17])     score([c0
17])    
score([c2
17 ]), the latter comparison is be-
score([c0
tween two new subsets whose scores can be com-
puted directly from eq. (5-6). hence, the second
best result c1
n is either the next best subset, or one
of the at most |m| candidates.

4 , c   1
4 , c0

15 , c1
15, c   1

4 , c   1

15 , c0

assuming that kj = 0 in the best result, the score
n by increment-

k   
n generated from c0

of candidate c
ing kj is de   ned as

j

score(c

k   
n ) = score(c0

j

n) +

score(c0+1
mj )

|m|

.

(7)

generalizing to an arbitrary k, the k+1   th result is
also either an unseen subset, whose score is de   ned
in eq. 5, or it can be obtained by increasing a ki from
a non-zero value in one of the k-best results gener-
ated so far. given a ck
n, the score of a candidate
generated by incrementing the value of kj is:

score(c

k   
n ) = score(c k

j

n) +

(cid:40)

)

mj

1
|m| score(c kj +1
if kj = 0,
if kj > 0.

(8)

   

0
1|m| score(c kj
mj )

notice the similarity between eq. 7 and eq. 8.
the difference is that when we explore candidates
of kj   s greater than zero, we replace the contribution
of mj   th child: the kj   th best score is replaced with
mj )
kj + 1   th best score. however, the edge score (c0
is never taken out of the total score of ck
n. this is
motivated by the    rst of the two conditions above.
as an illustratation to this point, consider the pred-
icate from table 1 one more time and assume that

pret(e17,16) = 0.4, i.e., the id203 of late be-
ing the argument of friday is 0.4. the information
that the temporal modi   er (node 17) is an argument
with a very low score should not disappear from the
subsequent scorings of node 12   s candidates. other-
wise a subsequent result may get a higher score than
the best one, violating the decreasing property of the
scoring function, as the    nal line below shows:

15 , c   1
17 ]
15 , c 0
17]
15 , c 1
17]
17]   
15 , c 1

(0 + log .78 + log .95)/3
(0 + log .78 + log .05)/3
(0 + log .78 + log .05 + log .4)/3
(0 + log .78 + log .4)/3.

4 , c   1
[c 0
4 , c   1
[c 0
4 , c   1
[c 0
4 , c   1
[c 0
to sum up, we have de   ned a monotonically de-
creasing scoring function and outlined our algorithm
for generating k-best compressions (see fig. 2). as
at every request the pool of candidates for node n is
extended by not more than |m| + 1 candidates, the
complexity of the algorithm is o(k    n    m) (k
times node count times the average branching fac-
tor).

3 adding a node subset scorer

on the    rst pass, the top-down compressor attempts
to    nd the best possible children subset of every
node by considering every child separately and mak-
ing the retain-or-delete decisions independently of
one another. how conservative or aggressive the
algorithm is, is determined by a single parameter
       [0, 1] which places a boundary between the two
decisions. with smaller values of    a low probabil-
ity of deletion (pdel) would suf   ce for a node child to
be removed. conversely, a greater value of    would
mean that only children about which the classi   er
is fairly certain that they must be deleted would be
removed.

unsurprisingly, the value of    is hard to opti-
mize as it may be too low or too high, depending
on a node. while retaining a child which could be
dropped would not result in an ungrammatical sen-
tence, omitting an important argument may make
the compression incomprehensible. when doing
an error analysis on a development set, we did not
encounter many cases where the compression was
clearly ungrammatical due to a wrongly omitted ar-
gument. however, results like that do have a high
cost and thus need to be addressed. consider the
following example:

, the best candidate for n.

score   (ck

n) = log p(|ck

n|)+

score(cki

mi).

function kbestcompress(g, k)
r}, heaps     {}

r = findbestcompression(g)
c 0
result     {c 0
while |result| < k do
result     result
heaps)}
end while
return result

end function

(cid:46) heaps for every node n.

    {findnextbest(c

|result   1|
r

,

function findnextbest(c k

n, heaps)

(cid:46) copy the result and update one    eld.

(cid:46) generate candidates by increasing a ki in the recent result.
mi     c k
for all c ki
n do
if ki >    1 then
c k   
n     c k
mi    findnextbest(c ki
c ki+1
c k   
n [mi]     c ki+1
heaps[n]     heaps[n]    {c k   
n }

mi, heaps)
, updatescore(c k   
n )

mi

n

i

i

i

i

end if
end for
c k   
n     generatenextbestsubset(g, n)
heaps[n]     heaps[n]    {c k   
n }
return pop(heaps[n])
(cid:46) c k+1

n

end function

function findbestcompression(g)
r     [], best        1, max        1
c 0
for all n     children(root(g)) do

r + findbestresult(n, g)

r     c 0
c 0
if pret(er,n) > max then
max     pret(er,n), best     n

end if

end for
for all n     children(root(g)) do
r [n]     c   1

if n (cid:54)= best then c 0
end if

n

end for
return c 0
r
end function

(cid:46) in the list, only one child is selected.

function findbestresult(n, g)

n     []
c 0
for all m     children(n) do
if pret(en,m)     0.5 then

n     c 0
c 0
n     c 0
else c 0
end if

n+ findbestresult(m, g)
n + c   1

m

end for
return c 0
n
end function

figure 2: pseudocode of the algorithm for    nding k-best
compressions of graph g. obvious checks for termination
conditions and empty outputs are not included for read-
n[m] refers to the result for child m of n in c k
ability. c k
n.
scoring is de   ned in equations 5-8. similar to huang
& chiang (2005) we use a heap for ef   ciency; heaps[n]
refers to the heap of candidates for node n.

(2) yesterday the world was ablaze with the news

that the ceo will step down.

in this sentence, ablaze is analyzed as an adverbial
modi   er of the verb to be and the classi   er assigns
a score of 0.35 to the edge pointing to ablaze. with
a decision boundary above 0.35, the meaningful part
of the predicate is deleted and the compression be-
comes incomplete. with the boundary at 0.5, the top
scoring subset is a singleton containing only the sub-
ject. however, there are hardly any cases where the
verb to be has a single argument, and our algorithm
could bene   t from this knowledge.
set (eq. 5) gets an additional summand, log p(|ck
where |ck
tually retained in ck

in the extended model, the score of a children sub-
n|),
n| refers to the number of n   s children ac-

n, i.e., with ki     0:
(cid:88)

1
|m|

ki

mi   ck

n

c

(9)
unfortunately, with the updated formula, we can no
longer generate k-best compressions as ef   ciently as
before. however, we can keep a beam of b subset
candidates for every node and select the one maxi-
mizing the new score.
to estimate the id203 of a children subset
size after compression, p(|ck
n|), we use an aver-
aged id88 implementation (freund & shapire,
1999) and the features described in sec. 2.2. we do
not differentiate between sizes greater than four and
have    ve classes in total (0, 1, 2, 3, 4+).

4 evaluation

the purpose of the evaluation is to validate the fol-
lowing two hypotheses, when comparing the new
algorithm with a competitive ilp-based sentence
compressor (filippova & altun, 2013):

1. the top-down algorithm was designed to per-
form local decisions at each node in the parse
tree, as compared to the global optimization
carried out by the ilp-based compressor. we
want to verify whether the local model can at-
tain similar accuracy levels or even outperform
the global model, and do so not only for the
single best but the top k results.

2. automatic ilp optimization can be quite slow
when the number of candidates that need to be
evaluated for any given input is large. we want
to quantify the speed-up that can be attained
without a loss in accuracy by taking simpler,
local decisions in the input parse tree.

4.1 evaluation settings
training, development and test set the aligned
sentences and compressions were collected us-
ing the procedure described in filippova & altun
(2013). the training set comprises 1,800,000 items,
each item consisting of two elements: the    rst sen-
tence in a news article and an extractive compres-
sion obtained by matching content words from the
sentence with those from the headline (see filip-
pova & altun (2013) for the technical details). a
part of this set was held out for classi   ers evaluation
and development. for testing, we use the dataset re-
leased by filippova & altun (2013)1. this test set
contains 10,000 items, each of which includes the
original sentence and the extractive compression and
the url of the source document. from this set, we
used the    rst 1,000 items only, leaving the remaining
9,000 items unseen, reserved for possible future ex-
periments. we made sure that our training set does
not include any of the sentences from the test set.

the training set provided us with roughly 16 mil-
lion edges for training maxent with 40% of positive
examples (deleted edges). for training the percep-
tron classi   er we had about 6 million nodes at our
disposal with the instances distributed over the    ve
classes as follows:
1

4+
19.5% 40.6% 31.2% 7.9% 1%

2

3

0

baseline we used the recent ilp-based algorithm
of filippova & altun (2013) as a baseline. we
trained the compressor with all the same features as
our model (sec. 2.2) on the same training data using
an averaged id88 (collins, 2002). to make
this system comparable to ours, when training the
model, we did not provide the ilp decoder with the
oracle compression length so that the model learned
to produce compressions in the absense of length ar-
gument. thus, both methods accept the same input

1http://storage.googleapis.com/sentencecomp/compression-

data.json

figure 3: per-edge precision, recall and f1-score using
different thresholds on the prediction values of maxent.

and are comparable.

4.2 automatic evaluation
to measure the quality of the two classi   ers (max-
ent from sec. 2.2 and id88 from sec. 3), we
performed a    rst, direct evaluation of each of them
on a small held out portion of the training set. the
maxent classi   er predicts the id203 of delet-
ing an edge and outputs a score between zero and
one. figure 3 plots precision, recall and f1-score at
different threshold values. the highest f1-score is
obtained at 0.45. regarding the id88 classi   er
that predicts the number of children that we should
retain for each node, its accuracy and per-class pre-
cision and recall values are given in table 2.

acc
72.7

0

1

2

3

4+

69 / 63

75 / 78

76 / 81

60 / 42

44 / 16

table 2: accuracy and precision / recall for the classi   er
predicting the optimal children subset size.

for an automatic evaluation of the quality of the
sentence compressions, we followed the same ap-
proach as (riezler et al., 2003; filippova & al-
tun, 2013) and measured f1-score by comparing the
trees of the generated compressions to the golden,
extractive compression. table 3 shows the results
of the ilp baseline and the two variants of the top-
down approach on the test data (top-down + nss
is the extended variant described in sec. 3). the
nss version, which incorporates a prediction on the
number of children to keep for each node, is slightly
better than the original top-down approach, but the

lllllllllllllllllllll0.00.20.40.60.81.00.40.60.81.0thresholdlprecisionrecallfscoreresults are not statistically signi   cant.

ilp
top-down
top-down + nss

f1-score compr. rate
46.5%
38.3%
38.1%

73.9
76.7
77.2

table 3: results for the baseline and our two algorithms.

it is important to point out the difference in com-
pression rates between ilp and top-down: 47%
vs. 38% (the average compression rate on the test
set is 40.5%). despite a signi   cant advantage due to
compression rate (napoles et al., 2011, see next sub-
section), ilp performs slightly worse than the pro-
posed methods.

finally, table 4 shows the results when comput-
ing the f1-score for each of the top-5 compressions
as generated by the top-down algorithms. as can
be seen, in both cases there is a sharp drop between
the top two compressions but further scores are very
close. since the test set only contains a single oracle
compression for every sentence, to understand how
big the gap in quality really is, we need an evaluation
with human raters.

top-down

top-down + nss

76.7; 60.4; 62; 60.9; 59.6

77.2; 60.5; 64; 62.6; 60

table 4: f1 scores for the top    ve compressions (k =
1, 2, 3, 4, 5).

4.3 manual evaluation
the    rst 100 items in the test data were manually
rated by humans. we asked raters to rate both read-
ability and informativeness of the compressions for
the golden output, the baseline and our systems2.
for both metrics a 5-point likert scale was used, and
three ratings were collected for every item. note that
in a human evaluation between ilp and top-down
(+ nss) the baseline has an advantage because (1) it
prunes less aggressively and thus has more chances
of producing a grammaticaly correct and informa-
tive outputs, and (2) it gets a hint to the optimal
compression length in edges. we have used intra-
class correlation (icc) (shrout & fleiss, 1979;

2the evaluation template and rated sentences are included

in the supplementary material.

cicchetti, 1994) as a measure of inter-judge agree-
ment. icc for readability was 0.59 (95% con   dence
interval [0.56, 0.62]) and for informativeness it was
0.51 (95% con   dence interval [0.48, 0.54]), indicat-
ing fair reliability in both cases.

results are shown in tables 5 and 6. as in the au-
tomatic evaluations, the two top-down systems pro-
duced indistinguishable results, but both are signif-
icantly better than the ilp baseline at 95% con   -
dence. the top-down results are also now indistin-
guishable from the extractive compressions.

extractive
ilp
top-down
top-down + nss

readability
4.33
4.20
4.41
4.38

informativeness
3.84
3.78
3.91
3.87

table 5: results of the manual evaluation.

k
1
2
3
4
5

ilp

4.20 / 3.78
3.85 / 3.09
3.53 / 2.73
3.31 / 2.27
3.00 / 2.42

top-down
4.41 / 3.91
4.11 / 3.31
4.03    / 3.37   
3.80    / 3.16   
3.90    / 3.41   

top-down + nss

4.38 / 3.87
4.26    / 3.37
3.97    / 3.40   
3.90    / 3.19   
4.12    / 3.41   

table 6: readability and informativeness for the top    ve
compressions;     indicates that one of the systems is sta-
tistically signi   cantly better than ilp at 95% con   dence
using a t-test.

4.4 ef   ciency
the average per-sentence processing time is 32,074
microseconds (intel xeon machine with 2.67 ghz
cpu) using ilp, 929 using top-down + nss, and
678 using top-down. this means that we have ob-
tained almost a 50x performance increase over ilp.
figure 4 shows the processing time for each of the
1,000 sentences in the test set with sentence length
measured in tokens.

for obtaining k-best solutions, the decrease in
time is even more remarkable:
the average time
for generating each of the top-5 compressions using
ilp is 42,213 microseconds, greater than that of the
single best result. conversely, the average time for
each of the top-5 results decreases to 143 microsec-

(a)

(a)

(b)

(c)

figure 4: per-sentence processing time for the test set: (a) ilp; (b) top-down; (c) top-down + nss.

(b)

(c)

figure 5: average processing time for getting all of the top-5 results: (a) ilp; (b) top-down; (c) top-down + nss.

onds using top-down, and 195 microseconds us-
ing top-down + nss, which means a 300x im-
provement. the reason is that the top-down meth-
ods, in order to produce the top-ranked compression,
have already computed all the per-edge predictions
(and the per-node nss predictions in the case of
top-down + nss), and generating the next best
solutions is cheap.

5 conclusions

we presented a fast and accurate supervised algo-
rithm for generating k-best compressions of a sen-
tence. compared with a competitive ilp-based sys-
tem, our method is 50x faster in generating the best
result and 300x faster for subsequent k-best com-
pressions. quality-wise it is better both in terms of
readability and informativeness. moreover, an eval-
uation with human raters demonstrates that the qual-
ity of the output remains high for the top-5 results.

references

almeida, m. b. & a. f. t. martins (2013). fast and
robust compressive summarization with dual de-

composition and id72. in proc. of
acl-13.

berg-kirkpatrick, t., d. gillick & d. klein (2011).
jointly learning to extract and compress. in proc.
of acl-11.

berger, a., s. a. della pietra & v. j. della pietra
(1996). a maximum id178 approach to natural
language processing. computational linguistics,
22(1):39   71.

cicchetti, d. v. (1994). guidelines, criteria, and
rules of thumb for evaluating normed and stan-
dardized assessment instruments in psychology.
psychological assessment, 6(4):284.

clarke, j. & m. lapata (2006). constraint-based
sentence compression: an integer programming
in proc. of coling-acl-06 poster
approach.
session, pp. 144   151.

clarke, j. & m. lapata (2008). global id136
for sentence compression: an integer linear pro-
gramming approach. journal of arti   cial intelli-
gence research, 31:399   429.

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll2040608050000150000sentence lengthtime (microseconds)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll204060805001500sentence lengthtime (microseconds)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll2040608010003000sentence lengthtime (microseconds)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll204060800100000250000sentence lengthtime (microseconds)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll20406080100300sentence lengthtime (microseconds)lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll2040608005001500sentence lengthtime (microseconds)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllcollins, m. (2002). discriminative training methods
for id48: theory and exper-
in proc. of
iments with id88 algorithms.
emnlp-02, pp. 1   8.

filippova, k. & y. altun (2013). overcoming the
lack of parallel data in sentence compression. in
proc. of emnlp-13, pp. 1481   1491.

filippova, k. & m. strube (2008). dependency tree
in proc. of iid86-

based sentence compression.
08, pp. 25   32.

fillmore, c. j., c. r. johnson & m. r. petruck
(2003). background to framenet. international
journal of id69, 16:235   260.

freund, y. & r. e. shapire (1999). large margin
classi   cation using the id88 algorithm. ma-
chine learning, 37:277   296.

galanis, d. & i. androutsopoulos (2010). an ex-
tractive supervised two-stage method for sentence
in proc. of naacl-hlt-10, pp.
compression.
885   893.

mcdonald, r. (2006). discriminative sentence com-
pression with soft syntactic evidence. in proc. of
eacl-06, pp. 297   304.

napoles, c., c. callison-burch & b. van durme
(2011). evaluating sentence compression: pit-
falls and suggested remedies. in proceedings of
the workshop on monolingual text-to-text gener-
ation, prtland, or, june 24 2011, pp. 91   97.

nivre, j. (2006).

inductive id33.

springer.

nomoto, t. (2009). a comparison of model free ver-
sus model intensive approaches to sentence com-
pression. in proc. of emnlp-09, pp. 391   399.

qian, x. & y. liu (2013). fast joint compression
in proc. of

and summarization via graph cuts.
emnlp-13, pp. 1492   1502.

riezler, s., t. h. king, r. crouch & a. zaenen
(2003). statistical sentence condensation using
ambiguity packing and stochastic disambiguation
methods for lexical-functional grammar.
in
proc. of hlt-naacl-03, pp. 118   125.

galley, m. & k. r. mckeown (2007). lexicalized
markov grammars for sentence compression. in
proc. of naacl-hlt-07, pp. 180   187.

shrout, p. e. & j. l. fleiss (1979). intraclass cor-
relations: uses in assessing rater reliability. psy-
chological bulletin, 86(2):420.

grefenstette, g. (1998). producing intelligent tele-
graphic text reduction to provide an audio scan-
in working notes of
ning service for the blind.
the workshop on intelligent text summarization,
palo alto, cal., 23 march 1998, pp. 111   117.

huang, l. & d. chiang (2005). better k-best pars-
ing. technical report ms-cis-05-08: university
of pennsylvania.

jing, h. & k. mckeown (2000). cut and paste based
in proc. of naacl-00, pp.

text summarization.
178   185.

knight, k. & d. marcu (2000). statistics-based
summarization     step one: sentence compression.
in proc. of aaai-00, pp. 703   711.

martins, a. f. t. & n. a. smith (2009). summa-
rization with a joing model for sentence extraction
and compression. in ilp for nlp-09, pp. 1   9.

thadani, k. (2014). approximating strategies for
multi-structure sentence compression. in proc. of
acl-14, p. to appear.

thadani, k. & k. mckeown (2013). sentence com-
pression with joint structural id136. in proc.
of conll-13, pp. 65   74.

titov, i. & a. klementiev (2011). a bayesian model
in proc. of

for unsupervised id29.
acl-11, pp. 1445   1455.

toutanova, k., c. brockett, m. gamon, j. jagarla-
mundi, h. suzuki & l. vanderwende (2007). the
pythy summarization system: microsoft research
at duc 2007. in proc. of duc-07.

wang, l., h. raghavan, v. castelli, r. florian
a sentence compres-
& c. cardie (2013).
sion based framework to query-focused multi-
document summarization. in proc. of acl-13, pp.
1384   1394.

woodsend, k., y. feng & m. lapata (2010). title
generation with quasi-synchronous grammar. in
proc. of emnlp-10, pp. 513   523.

woodsend, k. & m. lapata (2012). multiple as-
pect summarization using integer linear pro-
gramming. in proc. of emnlp-12, pp. 233   243.

