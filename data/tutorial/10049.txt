better summarization evaluation with id27s for id8

jun-ping ng
bloomberg l.p.
new york, usa

viktoria abrecht
bloomberg l.p.
new york, usa

jng324@bloomberg.net

vkanchakousk@bloomberg.net

5
1
0
2

 

g
u
a
5
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
4
3
0
6
0

.

8
0
5
1
:
v
i
x
r
a

abstract

id8 is a widely adopted, automatic
evaluation measure for text summariza-
tion. while it has been shown to corre-
late well with human judgements, it is bi-
ased towards surface lexical similarities.
this makes it unsuitable for the evalua-
tion of abstractive summarization, or sum-
maries with substantial id141. we
study the effectiveness of word embed-
dings to overcome this disadvantage of
id8. speci   cally, instead of measur-
ing lexical overlaps, id27s are
used to compute the semantic similarity of
the words used in summaries instead. our
experimental results show that our pro-
posal is able to achieve better correlations
with human judgements when measured
with the spearman and kendall rank co-
ef   cients.

1 introduction

automatic text summarization is a rich    eld of re-
search. for example, shared task evaluation work-
shops for summarization were held for more than
a decade in the document understanding con-
ference (duc), and subsequently the text anal-
ysis conference (tac). an important element of
these shared tasks is the evaluation of participat-
ing systems. initially, manual evaluation was car-
ried out, where human judges were tasked to as-
sess the quality of automatically generated sum-
maries. however in an effort to make evalua-
tion more scaleable, the automatic id81 mea-
sure (lin, 2004b) was introduced in duc-2004.
id8 determines the quality of an automatic
summary through comparing overlapping units
such as id165s, word sequences, and word pairs
with human written summaries.

1recall-oriented understudy of gisting evaluation

id8 is not perfect however. two problems
with id8 are that 1) it favors lexical simi-
larities between generated summaries and model
summaries, which makes it unsuitable to evaluate
abstractive summarization, or summaries with a
signi   cant amount of id141, and 2) it does
not make any provision to cater for the readability
or    uency of the generated summaries.

in

such as

automatic

of peers

summaries
tac

there has been on-going efforts
summarization

to im-
evalua-
prove on
the automatically
tion measures,
(aesop)
evaluating
(dang and owczarzak, 2009;
task
owczarzak, 2010; owczarzak and dang, 2011).
however, id8 remains as one of the most
popular metric of choice, as it has repeatedly
been shown to correlate very well with human
judgements
(lin, 2004a; over and yen, 2004;
owczarzak and dang, 2011).

in this work, we describe our efforts to tackle
the    rst problem of id8 that we have iden-
ti   ed above     its bias towards lexical similari-
ties. we propose to do this by making use of word
embeddings (bengio et al., 2003). word embed-
dings refer to the mapping of words into a multi-
dimensional vector space. we can construct the
mapping, such that the distance between two word
projections in the vector space corresponds to the
semantic similarity between the two words. by in-
corporating these id27s into id8,
we can overcome its bias towards lexical similar-
ities and instead make comparisons based on the
semantics of words sequences. we believe that
this will result in better correlations with human
assessments, and avoid situations where two word
sequences share similar meanings, but get unfairly
penalized by id8 due to differences in lexico-
graphic representations.

as an example, consider these two phrases: 1)
it is raining heavily, and 2) it is pouring. if we
are performing a lexical string match, as id8

does, there is nothing in common between the
terms    raining   ,    heavily   , and    pouring   . how-
ever, these two phrases mean the same thing. if
one of the phrases was part of a human written
summary, while the other was output by an auto-
matic summarization system, we want to be able
to reward the automatic system accordingly.

in our experiments, we show that word embed-
dings indeed give us better correlations with hu-
man judgements when measured with the spear-
man and kendall rank coef   cient. this is a signif-
icant and exciting result. beyond just improving
the evaluation prowess of id8, it has the po-
tential to expand the applicability of id8 to
abstractive summmarization as well.

2 related work

as we have
while id8 is widely-used,
there is a signi   cant body of
noted earlier,
work studying the evaluation of automatic text
summarization systems.
a good survey of
many of these measures has been written by
steinberger and je  zek (2012). we will thus not at-
tempt to go through every measure here, but rather
highlight the more signi   cant efforts in this area.
elements
(hovy et al., 2005) has also been used
(be)
in the duc/tac shared task evaluations.
it is
an automatic method which evaluates the content
completeness of a generated summary by breaking
up sentences into smaller, more granular units of
information (referred to as    basic elements   ).

id8,

besides

basic

the pyramid method originally proposed by
staple in
is another
passonneau et al. (2005)
duc/tac. however
is a semi-automated
it
method, where signi   cant human intervention
is required to identify units of
information,
called summary content units (scus), and
then to map content within generated summaries
recently however, an auto-
to these scus.
mated variant of
this method has been pro-
posed (passonneau et al., 2013).
in this variant,
id27s are used, as we are proposing
in this paper, to map text content within generated
summaries to scus. however the scus still need
to be manually identi   ed, limiting this variant   s
scalability and applicability.

many systems have

also been proposed
in the aesop task in tac from 2009 to
2011.
the top system re-
ported in owczarzak and dang (2011), autosum-

for example,

meng (giannakopoulos and karkaletsis, 2009),
is a graph-based system which scores summaries
based on the similarity between the graph struc-
tures of the generated summaries and model sum-
maries.

3 methodology

let us now describe our proposal to integrate word
embeddings into id8 in greater detail.

to start off, we will    rst describe the word
embeddings that we intend to adopt. a word
embedding is really a function w , where w :
w     rn, and w is a word or word sequence.
for our purpose, we want w to map two words
w1 and w2 such that their respective projections
are closer to each other if the words are se-
mantically similar, and further apart if they are
not. mikolov et al. (2013) describe one such vari-
ant, called id97, which gives us this de-
sired property2. we will thus be making use of
id97.

id8-2,

we will now explain how word embed-
dings can be incorporated into id8. there
are several variants of id8, of which
and id8-su4
id8-1,
have often been used.
this is because they
have been found to correlate well with human
(lin, 2004a; over and yen, 2004;
judgements
owczarzak and dang, 2011).
id8-1 mea-
sures the amount of unigram overlap between
model
summaries and automatic summaries,
and id8-2 measures the amount of bigram
overlap. id8-su4 measures the amount of
overlap of skip-bigrams, which are pairs of words
in the same order as they appear in a sentence.
in each of these variants, overlap is computed by
matching the lexical form of the words within the
target pieces of text. formally, we can de   ne this
as a similarity function fr such that:

fr(w1, w2) =(1,

0,

if w1 = w2
otherwise

(1)

where w1 and w2 are the words (could be unigrams
or id165s) being compared.

in our proposal3, which we will refer to as
id8-we, we de   ne a new similarity function

2the effectiveness of the learnt mapping is such that we
can now compute analogies such as king     man + woman =
queen.

3https://github.com/ng-j-p/id8-we

fw e such that:

fw e(w1, w2) =(0, if v1or v2 are oov

v1    v2, otherwise

(2)

where w1 and w2 are the words being compared,
and vx = w (wx). oov here means a situation
where we encounter a word w that our word em-
bedding function w returns no vector for. for the
purpose of this work, we make use of a set of 3
million pre-trained vector mappings4 trained from
part of google   s news dataset (?) for w .
reducing oov terms for id165s. with our
formulation for fw e, we are able to compute
variants of id8-we that correspond to those
of id8, including id8-we-1, id8-
we-2, and id8-we-su4. however, despite
the large number of vector mappings that we have,
there will still be a large number of oov terms in
the case of id8-we-2 and id8-we-su4,
where the basic units of comparison are bigrams.
to solve this problem, we can compose in-
dividual id27s together. we follow
the simple multiplicative approach described by
mitchell and lapata (2008), where individual vec-
tors of constituent tokens are multiplied together
to produce the vector for a id165, i.e.,

w (w) = w (w1)    . . .    w (wn)

(3)

where w is a id165 composed of individual word
tokens, i.e., w = w1w2 . . . wn. multiplication be-
tween two vectors w (wi) = {vi1, . . . , vik} and
w (wj) = {vj 1, . . . , vjk} in this case is de   ned
as:

{vi1    vj 1, . . . , vik    vjk}

(4)

4 experiments

4.1 dataset and metrics
for our experiments, we make use of the dataset
used in aesop (owczarzak and dang, 2011),
and the corresponding correlation measures.

for clarity, let us    rst describe the dataset used
in the main tac summarization task. the main
summarization dataset consists of 44 topics, each
of which is associated with a set of 10 docu-
ments. there are also four human-curated model
summaries for each of these topics. each of the
51 participating systems generated a summary for
each of these topics. these automatically gener-
ated summaries, together with the human-curated

model summaries,
dataset for aesop.

then form the basis of the

as reviewed in section 2,

this
semi-automated measure described in

to assess how effective an automatic evaluation
system is, the system is    rst tasked to assign a
score for each of the summaries generated by all of
the 51 participating systems. each of these sum-
maries would also have been assessed by human
judges using these three key metrics:
pyramid.
is
passonneau et al. (2005).
responsiveness. human judges are tasked to
evaluate how well a summary adheres to the infor-
mation requested, as well as the linguistic quality
of the generated summary.
readability. human judges give their judgement
on how    uent and readable a summary is.

a

the evaluation system   s scores are then tested to
see how well they correlate with the human assess-
ments. the correlation is evaluated with a set of
three metrics, including 1) pearson correlation (p),
2) spearman rank coef   cient (s), and 3) kendall
rank coef   cient (k).

4.2 results

we evaluate three different variants of our
proposal, id8-we-1, id8-we-2, and
id8-we-su4, against
their corresponding
variants of id8 (i.e., id8-1, id8-2,
id8-su4). it is worth noting here that in ae-
sop in 2011, id8-su4 was shown to corre-
late very well with human judgements, especially
for pyramid and responsiveness, and out-performs
most of the participating systems.

tables 1, 2, and 3 show the correlation of the
scores produced by each variant of id8-we
with human assessed scores for pyramid, respon-
siveness, and readability respectively. the tables
also show the correlations achieved by id8-1,
id8-2, and id8-su4. the best result for
each column has been bolded for readability.

id8-we-1 is observed to correlate very
well with the pyramid, responsiveness, and read-
ability scores when measured with the spear-
man and kendall rank correlation. however,
id8-su4 correlates better with human assess-
ments for the pearson correlation. the key differ-
ence between the pearson correlation and spear-
man/kendall rank correlation, is that the former
assumes that the variables being tested are nor-
mally distributed. it also further assumes that the

4https://drive.google.com/file/d/0b7xkcwpi5kdynlnuttlss21pqmm/edit?usp=sharing

measure

id8-we-1
id8-we-2

id8-we-su4

id8-1
id8-2

id8-su4

p

0.9492
0.9765
0.9783
0.9661
0.9606
0.9806

s

0.9138
0.8984
0.8808
0.9085
0.8943
0.8935

k

0.7534
0.7439
0.7198
0.7466
0.7450
0.7371

measure

id8-we-1
id8-we-2

id8-we-su4

id8-1
id8-2

id8-su4

p

0.7846
0.7819
0.7931
0.7900
0.7524
0.7840

s

0.4312
0.4141
0.4068
0.3914
0.3975
0.3953

k

0.3216
0.3042
0.3020
0.2846
0.2925
0.2925

table 1: correlation with pyramid scores, mea-
sured with pearson r (p), spearman    (s), and
kendall    (k) coef   cients.

table 3: correlation with readability scores, mea-
sured with pearson r (p), spearman    (s), and
kendall    (k) coef   cients.

measure

id8-we-1
id8-we-2

id8-we-su4

id8-1
id8-2

id8-su4

p

0.9155
0.9534
0.9538
0.9349
0.9416
0.9545

s

0.8192
0.7974
0.7872
0.8182
0.7897
0.7902

k

0.6308
0.6149
0.5969
0.6334
0.6096
0.6017

table 2: correlation with responsiveness scores,
measured with pearson r (p), spearman    (s), and
kendall    (k) coef   cients.

variables are linearly related to each other. the lat-
ter two measures are however non-parametric and
make no assumptions about the distribution of the
variables being tested. we argue that the assump-
tions made by the pearson correlation may be too
constraining, given that any two independent eval-
uation systems may not exhibit linearity.

looking at

the two bigram based variants,
id8-we-2 and id8-we-su4, we ob-
serve that id8-we-2 improves on id8-2
most of the time, regardless of the correlation met-
ric used. this lends further support to our proposal
to use id27s with id8.

however id8-we-su4 is only better than
id8-su4 when evaluating readability. it does
consistently worse than id8-su4 for pyramid
and responsiveness. the reason for this is likely
due to how we have chosen to compose unigram
word vectors into bigram equivalents. the mul-
tiplicative approach that we have taken worked
better for id8-we-2 which looks at contigu-
ous bigrams. these are easier to interpret seman-
tically than skip-bigrams (the target of id8-
we-su4). the latter, by nature of their construc-
tion, loses some of the semantic meaning attached
to each word, and thus may not be as amenable to
the linear composition of word vectors.

owczarzak and dang (2011) reports only the
results of the top systems in aesop in terms
of pearson   s correlation. to get a more com-
plete picture of the usefulness of our proposal,
it will be instructive to also compare it against
the other top systems in aesop, when mea-
sured with the spearman/kendall correlations.
we show in table 4 the top three systems
which correlate best with the pyramid score when
measured with the spearman rank coef   cient.
c s iiith3 (kumar et al., 2011) is a graph-
based system which assess summaries based on
differences in word co-locations between gener-
ated summaries and model summaries. be-hm
(baseline by the organizers of the aesop task)
is the be system (hovy et al., 2005), where ba-
sic elements are identi   ed using a head-modi   er
criterion on parse results from minipar. lastly,
catolicasc1 (de oliveira, 2011)
is also a
graph-based system which frames the summary
evaluation problem as a maximum bipartite graph
matching problem.

measure

id8-we-1

c s iiith3

be-hm

catolicasc1

s

0.9138
0.9033
0.9030
0.9017

k

0.7534
0.7582
0.7456
0.7351

table 4: correlation with pyramid scores of
top systems in aesop 2011, measured with the
spearman    (s), and kendall    (k) coef   cients.

we see that id8-we-1 displays better cor-
relations with pyramid scores than the top system
in aesop 2011 (i.e., c s iiith3) when mea-
sured with the spearman coef   cient. the latter
does slightly better however for the kendall coef-
   cient. this observation further validates that our

proposal is an effective enhancement to id8.

references

5 conclusion

we proposed an enhancement
to the popu-
lar id8 metric in this work, id8-we.
id8 is biased towards identifying lexical sim-
ilarity when assessing the quality of a generated
summary. we improve on this by incorporat-
ing the use of id27s. this enhance-
ment allows us to go beyond surface lexicographic
matches, and capture instead the semantic similar-
ities between words used in a generated summary
and a human-written model summary. experi-
menting on the tac aesop dataset, we show that
this proposal exhibits very good correlations with
human assessments, measured with the spear-
man and kendall rank coef   cients. in particular,
id8-we-1 outperforms leading state-of-the-
art systems consistently.

looking ahead, we want to continue building
on this work. one area to improve on is the
use of a more inclusive evaluation dataset. the
aesop summaries that we have used in our ex-
periments are drawn from systems participating
in the tac summarization task, where there is a
strong exhibited bias towards extractive summa-
rizers. it will be helpful to enlarge this set of sum-
maries to include output from summarizers which
carry out substantial id141 (li et al., 2013;
ng et al., 2014; liu et al., 2015).

another immediate goal is to study the use
of better compositional embedding models. the
generalization of unigram id27s into
bigrams (or phrases),
is still an open prob-
lem (yin and sch  utze, 2014; yu et al., 2014). a
better compositional embedding model than the
one that we adopted in this work should help us
improve the results achieved by bigram variants of
id8-we, especially id8-we-su4. this
is important because earlier works have demon-
strated the value of using skip-bigrams for sum-
marization evaluation.

an effective and accurate automatic evaluation
measure will be a big boon to our quest for bet-
ter text summarization systems. id27s
add a promising dimension to summarization eval-
uation, and we hope to expand on the work we
have shared to further realize its potential.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian janvin. 2003. a neu-
ral probabilistic language model. the journal of
machine learning research, 3:1137   1155.

[dang and owczarzak2009] hoa trang dang and
karolina owczarzak. 2009. overview of the tac
2009 summarization track. in proceedings of the
text analysis conference (tac).

[de oliveira2011] paulo c. f. de oliveira.

2011.
catolicasc at tac 2011. in proceedings of the text
analysis conference (tac).

gi-
[giannakopoulos and karkaletsis2009] george
annakopoulos and vangelis karkaletsis.
2009.
autosummeng and memog in evaluating guided
summaries.
in proceedings of the text analysis
conference (tac).

[hovy et al.2005] eduard hovy, chin-yew lin, and
liang zhou. 2005. evaluating duc 2005 using
basic elements.
in proceedings of the document
understanding conference (duc).

[kumar et al.2011] niraj kumar, kannan srinathan,
and vasudeva varma. 2011. using unsupervised
system with least linguistic features for tac-
aesop task. in proceedings of the text analysis
conference (tac).

[li et al.2013] chen li, fei liu, fuliang weng, and
yang liu. 2013. document summarization via
guided sentence compression.
in proceedings of
the conference on empirical methods in natural
language processing (emnlp), pages 490   500.

[lin2004a] chin-yew lin. 2004a. looking for a few
good metrics: id8 and its evaluation. in work-
ing notes of the 4th ntcir workshop meeting.

[lin2004b] chin-yew lin. 2004b. id8: a pack-
age for automatic evaluation of summaries. in text
summarization branches out: proceedings of the
acl-04 workshop.

[liu et al.2015] fei liu, jeffrey flanigan, sam thom-
son, norman sadeh, and noah a. smith. 2015.
toward abstractive summarization using semantic
representations. in proceedings of the conference
of the north american chapter of the association
for computational linguistics: human language
technologies (naacl-hlt), pages 1077   1086.

[mikolov et al.2013] tomas mikolov, wen-tau yih, and
geoffrey zweig. 2013. linguistic regularities in
continuous space word representations.
in pro-
ceedings of the conference of the north american
chapter of the association for computational lin-
guistics: human language technologies (naacl-
hlt), pages 746   751.

[mitchell and lapata2008] jeff mitchell and mirella
2008. vector-based models of seman-
lapata.
tic composition.
in proceedings of the 46th an-
nual meeting of the association for computational
linguistics: human language technologies (acl),
pages 236   244.

[ng et al.2014] jun-ping ng, yan chen, min-yen kan,
and zhoujun li. 2014. exploiting timelines to
enhance id57.
in pro-
ceedings of the 52nd annual meeting of the asso-
ciation for computational linguistics (acl), pages
923   933.

[over and yen2004] paul over and james yen. 2004.
an introduction to duc 2004 intrinsic evaluation
of generic new text summarization systems.
in
proceedings of the document understanding con-
ference (duc).

[owczarzak and dang2011] karolina owczarzak and
hoa trang dang.
2011. overview of the tac
2011 summarization track: guided task and ae-
sop task. in proceedings of the text analysis con-
ference (tac).

[owczarzak2010] karolina owczarzak.

2010.
overview of the tac 2010 summarization track.
in proceedings of
the text analysis conference
(tac).

[passonneau et al.2005] rebecca j passonneau, ani
nenkova, kathleen mckeown, and sergey sigel-
man. 2005. applying the pyramid method in duc
2005. in proceedings of the document understand-
ing conference (duc).

[passonneau et al.2013] rebecca j passonneau, emily
chen, weiwei guo, and dolores perin. 2013. auto-
mated pyramid scoring of summaries using distri-
butional semantics. in proceedings of the 51st an-
nual meeting of the association for computational
linguistics (acl), pages 143   147.

[steinberger and je  zek2012] josef

and
karel je  zek.
evaluation measures for
text summarization. computing and informatics,
28(2):251   275.

steinberger

2012.

[yin and sch  utze2014] wenpeng yin

and hinrich
sch  utze. 2014. an exploration of embeddings for
generalized phrases.
in proceedings of the acl
2014 student research workshop, pages 41   47.

[yu et al.2014] mo yu, matthew gorid113y, and mark
dredze. 2014. factor-based compositional embed-
ding models. in proceedings of the nips 2014 deep
learning and representation learning workshop.

