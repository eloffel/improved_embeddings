5
1
0
2
 
c
e
d
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
1
1
0
0

.

2
1
5
1
:
v
i
x
r
a

inferring interpersonal relations in narrative summaries

shashank srivastava1, snigdha chaturvedi2, tom mitchell1

1carnegie mellon university 2university of maryland, college park
ssrivastava@cmu.edu, snigdhac@cs.umd.edu, tom.mitchell@cmu.edu

abstract

characterizing relationships between people is funda-
mental for the understanding of narratives. in this work,
we address the problem of inferring the polarity of re-
lationships between people in narrative summaries. we
formulate the problem as a joint id170
for each narrative, and present a model that combines
evidence from linguistic and semantic features, as well
as features based on the structure of the social commu-
nity in the text. we also provide a id91-based ap-
proach that can exploit regularities in narrative types.
e.g., learn an af   nity for love-triangles in romantic sto-
ries. on a dataset of movie summaries from wikipedia,
our structured models provide more than a 30% error-
reduction over a competitive baseline that considers
pairs of characters in isolation.

1 introduction

understanding narratives requires the ability to inter-
pret character intentions, desires and relationships. the
importance of characters and characterization in narra-
tives has been explored in recent works that focus on
their roles and representations [4, 24, 7], as against a
plot-centric perspective of a narrative as primarily a
sequence of events [15, 22, 8]. however, while such
approaches can identify characters types, they do not
model relationships between characters in a narrative.
in this work, we address the problem of inferring
cooperative and adversarial relationships between peo-
ple in narrative summaries. identifying character coop-
eration and con   ict is essential for narrative compre-
hension. it can guide interpretation of narrative events,
explain character actions and behavior and steer the
reader   s expectation about the plot. as such, it can have
value for applications such as machine reading, qa and
document summarization.

young drifter axel nordmann goes to work in a gang of
stevedores headed by charlie malik, a vicious bully, and is
befriended by tommy tyler, who also supervises a stevedore
gang. malik resents blacks in positions of authority, and is
antagonized when axel goes to work for tommy. axel moves
into tommy   s neighborhood and becomes friends with tommy   s
wife lucy. axel is hiding something, and it emerges that he is
a deserter from the united states army. malik is aware of that,
and is extorting money from him. malik frequently tries to pro-
voke tommy and axel into    ghts, with tommy coming to axel   s
aid ...

(a)

(b)

figure 1: (a) sample summary extract for the 1957
movie    edge of the city   ; and (b) inferred relationship
polarities with supporting evidence

as a motivating example, let us consider the plot
summary in figure 1a (condensed here for brevity). in
this passage, the relations between the principal char-
acters are explicated through a combination of cues,
as seen in figure 1b. for instance, one can infer that
alex (a) and tommy (t) have a cooperative relation-
ship through a combination of the following observa-
tions (among others): (1) t initially    befriends    a, (2)
a works for t, and its connotation that a is likely to co-
operate with t , (3) t aids a in    ghts, (4) a is a friend
of t   s wife , (5) a and t have a common adversary. in
particular, we note that cues (4) and (5) cannot be ex-
tracted from looking at the relation between a and t

1

in isolation, but depend on their relations with others.
in this work, we show that such indirect structural cues
can be very signi   cant for id136 of character rela-
tionships.

our problem formulation assumes a    xed relation be-
tween a pair of characters within a narrative. while this
can be problematic since relationships can transform
over time; in a wide range of examples, the assump-
tion is reasonable. even in complex narratives, relation-
ships remain persistent within sub-parts. from a prag-
matic perspective, the approximation serves as a useful
starting point for research. our main contributions are:
    we introduce the problem of characterizing coop-
eration in interpersonal relations in narrative texts;
and provide an annotated dataset of 153 movie sum-
maries for the task.

    we design linguistic, semantic and discourse features
for this task, and demonstrate their relative impor-
tance.

    we formulate the problem as id170,
and introduce a joint model that incorporates both
text-based cues and structural id136s.

    we provide an extension to exploit narrative-speci   c

regularities, enabling content-based models. 1

the layout of the paper is as follows: in section 3, we
present our models, and formulation of the problem as a
id170. in section 4, we describe the fea-
tures used by our models in detail. we then describe our
dataset, and present quantitative and qualitative evalua-
tions of our models.

2 related work

existing research on characterizing relationships be-
tween people has almost exclusively focused on dia-
logue or social network data. such methods have ex-
plored aspects of relations such as power [5], address
formality [19] and sentiment [16] in conversations. re-
cently, [1] studied the problem of parsing movie screen-
plays for extracting social networks. however, analysis
of character relationships in narrative texts has largely
been limited to simplistic schemes based on counting
character co-occurrences in quoted conversations [12]

1e.g., predict more hostile relations in revenge-dramas, rather than

love stories

2

or social events [2]. we believe this is the    rst attempt
to infer relation polarities in narrative texts.

in terms of approach, our use of structural triads as
features is most closely related to [19] who use an unsu-
pervised joint probabilistic model of text and structure
for the task of inducing formality from address terms
in dialogue, and [20] who empirically analyze signed
triads in social networks from a perspective of struc-
tural theories. such social triads have previously been
studied from perspectives of social psychology and net-
works [17, 6].

3 relation classi   cation as struc-

tured prediction

we formulate the problem of relation classi   cation to
allow arbitrary text-based and structural features. we
consider the problem as a id170, where
we jointly infer the collective assignment of relations-
labels for all pairs of characters in a document. let x
denote a narrative document for which we want to in-
fer relationship structure y. we could think of x as a
graph with characters as nodes, and relationship pre-
dictions corresponding to edge-labels. we assume a su-
pervised learning setting where we have labeled train-
ing set t := {(x1, y1), ..., (xm, ym)}. for each x, we
have a set of allowed assignments y(x) (consisting of
combinations of binary assignments to each edge-label
in x). following standard approaches in structured clas-
si   cation, we consider linear classi   ers of form:

hw = argmax
y   y(x)

wt   (x, y)

(1)

here,   (x, y) is a feature vector that can depend on
both the narrative document x and a relation-polarity
assignment y, w is a weight vector, and wt   (x, y)
denotes a linear score indicating the goodness of the
assignment. finding the best assignment corresponds
to the decoding problem,
i.e.    nding the highest
scoring assignment under a given model. on the other
hand, the model parameters w can be learnt using
a voted structured id88 training algorithm [9].
the structured id88 updates can also be seen as
stochastic sub-id119 steps minimizing the
following structured hinge loss:

wt(cid:0)  (x, y(cid:48))       (x, y)(cid:1) (2)

l(w, x, y) := max

y(cid:48)   y(x)

a

as

concatenation of

for our problem, we de   ne the feature vector
features based
  (x, y)
on text and structural components:   (x, y)
:=

(cid:0)  text(x, y)   struct(x, y)(cid:1). the text-based compo-
ceptron framework as   text(x, y) := (cid:80)

nent can be de   ned by extending the traditional per-
ye  (xe).

xe   e(x)

here e(x) consists of the set of annotated character-
pair relationships for the narrative text x,   (xe)
denotes the text-based feature-representation for the
character-pair (as described in section 4.1), and ye is
the binary assignment label (  1) for the pair in y. on
the other hand, our structural features   struct(x, y)
focus on con   gurations of relationship assignments of
triads of characters, and are motivated in our discussion
of transitive relations in section 4.2 . we note that
while this is not the case in the current work, structural
features can also encode character attributes (such as
age or gender) in conjunction with assignment labels y.
learning and id136: structured id88s have
been conventionally used for simple structured inputs
(sequences and trees) and locally factoring models,
which are amenable to ef   cient id145
id136 algorithms. this is because updates require
id136 over an exponentially large space (solving the
decoding problem in equation 1), and updates from in-
exact search can violate convergence properties. how-
ever, [18] show that exact search is not needed for con-
vergence as long as we can guarantee updates on    viola-
tions    only, i.e. it suf   ces to    nd a labeling assignment
with higher score than the correct update. additionally,
edge labels are expected to be relatively sparse for our
domain since character graphs in most narratives are not
fully connected. hence, the id136 problem decom-
poses for relation-edges which are not parts of struc-
tural triangles, and the decoding problem can be exactly
solved for the vast majority of narrative texts.

for j : 1 to m do

algorithm 1 id88 training for relations
1: initialize w to 0
2: for iter : 1 to numepochs do
3:
4:
5:
6:
7:
8:
9:
10: end for

randomly choose i     {1..m}
  y     decode(xi, w)
if ( wt   (xi,   y)     wt   (xi, yi)) then

w     w +   (  (xi, yi)       (xi, y(cid:48)))

end for

end if

for id136 on a new document where the edge
relations are not known, decoding can proceed by ini-
tializing the narrative graph to high con   dence edges
from the text-based model only (character relationships
   rmly embedded in text), and appending single edges
which complete triads. to avoid speculative id136
of relations between character pairs that are ungrounded
in the text, we only consider structural triads for which
at least two edges are grounded in the text while decod-
ing with the structural model.

3.1 accounting for narrative types
the framework described in the previous section pro-
vides a simple model to incorporate text-based and
structural features for relation classi   cation. however, a
shortcoming in the approach is that the model is agnos-
tic to narrative types. ideally, a model could allow dif-
ferential weights to features depending on the narrative
type. as speculative illustrations,    mexican standoffs   
might be common in    revenge/gangster    narratives, or
family-relations might be highly indicative of coopera-
tion in children stories; and a model would ideally learn
and leverage such regularities in the data.

we present a id91-based extension to our struc-
tured model, which can incorporate features descrip-
tive of the narrative text to infer regularities, and make
content-based predictions. let us surmise that the data
consists of k natural clusters of narrative-types, with
a speci   c structured model for each cluster (speci   ed
by weights wk). for each narrative text x, we associate
a vector f (x) that represents content and determines
narrative type. examples of such representations could
be keywords for a document, genre information for a
movie or novel, topic proportions of words in the text
from a topic-model, etc. we model the membership of
narrative x to the cluster ck by a softmax logistic multi-
nomial.

p (c = ck; x) =

k f (x))

exp(  t
k(cid:48) exp(  t

k(cid:48)f (x))

(3)

(cid:80)

from our observation of the loss objective for the struc-
tured id88 in equation 2, we can de   ne the ex-
pected loss for a narrative text (x, y) under the cluster-
ing model as:

k f (x))

exp(  t
k(cid:48) exp(  t

k(cid:48)f (x))

l(wk, x, y)

(4)

(cid:88)

(cid:80)

l(x, y) =

k

3

then the overall objective loss over the training set t
is:

(cid:88)
(cid:88)

i

j =

=

l(xi, yi)

(cid:88)

exp(  t

(cid:80)

i

k

k f (xi))l(wk, xi, yi)
k(cid:48) exp(  t

k(cid:48)f (xi))

(5)

we jointly minimize the overall objective through a
block-coordinate descent procedure. this consists of
a two-step alternating minimization of the objective
w.r.t. the prediction model weights wk and the clus-
tering parameters   k, respectively. in the    rst step,
we optimize the prediction model weights wk while
   xing the id91 parameters   k. this can be done
by weighting the training examples for each cluster by
their cluster membership; and invoking the structured
id88 procedure for each cluster. in the alternating
step, we    x the predictions model weights; and update
the id91 parameters using id119:

     k j = (cid:80)

exp(  t

k f (xi))
z2
i
k, xi, yi))f (xi)

i=1

l(w(cid:48)

(cid:80)

k(cid:48)

id91 model.

to ef   ciently use training data, we allow parameter-
sharing across cluster-speci   c prediction models, draw-
ing from methods in id72 [13]. in partic-
ular, we model each wk as composed of a shared base
model, and additive cluster-speci   c weights:

wk = w0 + vk

implementationally, we can do this by simply augment-
ing cluster-speci   c feature representations as follows:
  k(x, y) =

(1       )  (x, y), 0, ..., 0

,     (x, y), 0, ..., 0

(cid:16)

(cid:17)

(cid:124) (cid:123)(cid:122) (cid:125)

k-1

(cid:124) (cid:123)(cid:122) (cid:125)

k-k

here    is a hyper-parameter between 0 and 1, which
speci   es the weighting of the shared and cluster-
speci   c models.    = 0 negates id91, and reduces
the id91 model to the plain structured model with-
out id91. conversely,    = 1 implies no parameter
sharing across clusters.

exp(  (cid:48)t

k f (xi))(l(wk, xi, yi)    

4 features

algorithm 2 narrative-speci   c model
1: initialize   k to random vectors
2: repeat
3: update id88 weights: train structured per-
ceptron models for each cluster ck, weighting train-
ing instance (xi, yi) in t by

k f (xi))

(cid:80)

exp(  t
k(cid:48) exp(  t

k(cid:48) f (xi))

4: update id91 model
5: for i : 1 to numiter do
6:
7:
8: end for

for k : 1 to k do
end for

  k       k            k j

this can be interpreted as a id64 procedure,
where given cluster assignments of points, we update
the prediction model weights; and given losses from the
prediction model, update data clusters parameters to re-
assign the most violating data-points. we note that the
objective is non-convex due to the softmax, and hence
different initializations of the procedure can lead to dif-
ferent solutions. however, since each sub-procedure de-
creases the objective value; the overall objective de-
creases for small enough step sizes. the procedure is
summarized in algorithm 2. for prediction, each narra-
tive text is assigned to the most likely cluster with the

4

in this section, we outline the text-based and structural
features used by our classi   cation models. the text-
based features make use of existing linguistic and se-
mantic resources, whereas the structural features are
based on counts of speci   c signed social triads, which
can be enumerated for any assignment. we provide im-
plementations for these features, as well as the complete
pipeline for annotating relationship polarities for a new
text document on our project webpage.

4.1 text-based cues (  text)
these features aim to ident   y relationships between
pairs of characters in isolation. these are based on re-
sources such as sentiment lexicons, syntactic and se-
mantic parsers, distributional word-representations, and
multi-word-expression dictionaries, and are engineered
to capture:
    overall polarities of interactions between charac-
ters (from text-spans between coreferent mentions)
based on lexical and phrasal-level polarity clues.
    semantic connotations of actions one agent does to
the other, actions they share as agents or patients,
and how often they act as a team.
    character co-occurrences in semantic frames that
evoke positivity, negativity or social relationship .

1. are team: this models if the two characters partici-
pate in acts together. it is a binary feature indicating
if the two characters were both agents (or patients)
of a verb in a sentence e.g.,    malik provokes tommy
and axel   .

2. acts together: these features count the number of
actions with positive and negative connotations that
either character (in an agent role) does to the other
(in a patient role). there are three variants based on
different word connotation resources, viz., semantic
lexicons for subjectivity clues [14], sentiment [21]
and prior-polarity [25] of verbs. the feature does not
   re for neutral verbs. e.g,    malik blackmails axel   .
3. surrogate acts together: coverage for the above
features suffers from limitations of nlp process-
ing tools. e.g., in    on being provoked by malik,
tommy...    , tommy is not a direct patient of the verb.
these features extend coverage to verbs which have
either of the characters as the agent or the patient
in sentences that did not contain any other character
apart from the two of interest.

4. adverb connotations: this feature models the nar-
rative   s overall bias in describing either character   s
actions by summing the semantic connotations of
adverbs that modify their joint(or surrogate) acts.
e.g.,    tommy nobly befriends axel   . positive ad-
verbs count as +1, negative as -1. uses the same con-
notation resources as 2.

5. character similarity: models similarity of two char-
acters as the average pairwise similarity of adjectives
describing each (where lexical similarity is given by
the cosine similarity of embeddings from [10]). this
is computed for the entire document, and not at the
per-sentence level.

6. lexical sentiment: these features count the num-
ber of positive and negative sentiments words or
multi-word phrases in spans between mentions of
the two characters using sentiment lexicons (simi-
lar to 2). for multi-word phrases (identi   ed from a
list of mwes), we use a dictionary to map these to
single words if possible, and look for these words
in connotation lexicons. e.g.,    kick the bucket    maps
to    die   . this helps with phrases like    fell in love   ,
where    fell    has a negative connotation by itself.

7. relation keywords: this feature indicates presence
of keywords denoting familial ties between charac-
ters (   father   ,    wife   , etc.) in spans between character
mentions.

8. frame semantic: these are based on framenet-style
parses of the sentence from the semafor parser [11].

figure 2: primary triadic structural features.    +    signs
indicate cooperative and           indicate adversarial rela-
tionships

    character similarity based on whether they are de-
scribed by similar adjectives; and the narrative sen-
timent of adverbs describing their actions.
    existence of familial relations between characters.
we base our entity-tracking on the stanford core-
the computation of all
nlp system; and augment
sentiment
features with basic negation handling.
based on such features extracted for each character
pair, relationship characterization can be treated as
supervised classi   cation (with y =   1 corresponding
to cooperative or adversarial relations). our baseline
unstructured approach uses only these features with a
id28 model.

feature details: texts are initially processed with
the stanford core nlp system to identify personal
named entities, and obtain dependency parses. as ba-
sic post-processing, we mark coreferent text-spans with
their corresponding personal entities, using basic string-
matching and pronominal resolution from the stanford
coreference system. for enumerating actions by/on two
characters of interest, we identify verbs in sentences for
which the characters occurred in a agent or a patient
role (identi   ed using    nsubj    or    agent   ; and    dobj    or
   nsubjpass    dependencies respectively). we extend this
for conjoint verbs, identi   ed by the    conj    relation (e.g.,
   a shot and injured b   ). the dependency relation    neg   
is used to determine negation status of verbs.

given a pair of characters, we identify the set of
sentences which contain mentions of both (identi   ed
by coreferent text-spans). for this set, we extract the
arithmetic means, maximum and cumulative sums for
the following sentence-level cues as text-based features
(whenever meaningful):

5

we compiled lists of frames associated with: (i) pos-
itive or (ii) negative connotations (e.g., frames like
cause hurt or rescue), (iii) personal or professional
relationships (e.g., forming relationships). three bi-
nary features denote frame evocation for each of
these lists.

4.2 structural cues (  struct)
as motivated earlier, relationships between people can
also be inferred from their relationships with others in
a narrative. our thesis is that a joint id136 model
that incorporates both structure and text would perform
better than one that considers pairwise relations in iso-
lation. in some domains, observed relations between
entities can directly imply unknown relations among
others dues to natural orderings. for example, tempo-
ral relations among events yield natural transitive con-
straints. for the current task; such constraints do not
apply. while structural regularities like    a friend of a
friend is a friend    might be prevalent, these con   gura-
tions are not logically entailed; and af   nities for such
structural regularities must be directly learnt from the
observed data.

in figure 2, we characterize the primary triadic struc-
tural features that we use in our models, along with our
informal appellations for them. the values of the four
structural features for a narrative document x and re-
lation polarity assignment y are simply the number of
such con   gurations in the assignment, and are easily
computed. the empirical af   nities for such con   gura-
tions, as re   ected in corresponding weights can then be
learnt from the data.

5 dataset

we processed the cmu movie summary corpus, a col-
lection of movie plot summaries from wikipedia, along
with aligned meta-data [3]; and set up an online annota-
tion task using brat [23]. we use stanford core nlp
annotations and basic post-processing to identify per-
sonal entities in each text.

annotators could choose pairs of characters in the
text, and characterize a directed relationship between
them on an ordinal    ve-point scale as    hostile   ,    adver-
sarial   ,    neutral   ,    cooperative    or    friendly   . this re-
sulted in a dataset of 153 movie summaries, consisting
of 1044 character relationship annotations.2 the dataset
2most relations were annotated symmetrically. for relations with

figure 3: sample annotation for a very short summary

is made publicly available for research on our project
webpage.

for evaluation, we aggregated    hostile    and    adver-
sarial    edge-labels, and    friendly    and    cooperative   
edge-labels to have two classes (neutral annotations
were sparse, and ignored in the evaluation). of these,
58% of the relations were classi   ed as cooperative or
friendly, while 42% were hostile or adversarial. the es-
timated annotator agreement for the collapsed classes
on a small subset of the data was >0.95.

6 evaluation and analysis

in this section, we discuss quantitative and qualitative
evaluation of our methods. first, we make an ablation
study to assess the relative importance of families of
text-based features. we then make a comparative eval-
uation of our methods in recovering gold-standard an-
notations on a held-out test set of movie summaries. fi-
nally, we qualitatively analyze the performance of the
model, and brie   y discuss common sources of errors.
feature ablation: figure 4 shows the cross-validation
performance of major feature families of text features
on the training set. we note that frame-semantic fea-
tures and adverbial connotations of character actions
do not add signi   cantly to model performance. this is
perhaps because both these families of features were
sparse. additionally, frame-semantic parses were ob-
served to have frequent errors in frame evocation, and

asymmetric labels, we    averaged    the annotations in the two direc-
tions to get the annotation for the relation.

6

features can signi   cantly improve id136 of char-
acter relations. further, we observe that the narrative-
speci   c model (with k = 2) slightly outperforms the
structured id88 model.

naive (majority class)
lr
spr
spr +narrative types

avg p avg r avg f1 acc
0.520
0.269
0.701
0.702
0.794
0.792
0.804
0.806

0.520
0.697
0.793
0.804

0.355
0.699
0.793
0.805

figure 4: ablation study for text feature families

table 1: test set results for relation classi   cation

frame element assignment. on the other hand, we ob-
serve that joint participation in actions (as agent or pa-
tient) is a strong indicator of cooperative relationships.
in particular, incorporating these (   are team   ) features
was seen to improve both precision and recall for the co-
operative class; while not degrading recall for the non-
cooperative class. further, while ignoring sentiment and
connotation features for surrogate action features re-
sults in marginal degradation in performance; the most
signi   cant features are seen to be sentiment and con-
notation features for actions where characters occur in
svo roles (   acts together    features); and overall sen-
timent characterizations for words and phrases in spans
of text between character mentions (span based    lexical
sentiment    features).
structured vs unstructured models: we now analyze
the performance of our proposed models; and evalu-
ate the signi   cance of adding structural features to our
models. in our experiments, we found the structured
models to consistently outperform text-based models.
we tune values of hyper-parameters, i.e. number of
training epochs for the structured id88 (10), the
weighting parameter for the id91 model (  =0.8),
and the number of clusters (k=2) through cross val-
idation on training data.3 table 1 compares the per-
formance of the models on our held-out test set of 27
movie summaries (comprising about 20% of the all an-
notated character relations). for the structured models,
reported results are averages over 10 initializations.

we observe that the structured id88 model for
relations (spr) outperforms the text-only model trained
with id28 (lr) by a very large margin.
these results are consistent with our cross-validation
   ndings, and vindicate our hypothesis that structural

3for representations f (x) of movie summaries, we use genre key-
words from the metadata for movies (provided with dataset) and the
average of text-feature vectors for all character pairs

let us consider the af   nities for structural fea-
tures learnt by the model. over 10 runs of spr,
the average weights were: wclique =    2.79,
wlovetriangle =    0.84, wcommonenemy = 10.26
and wmexicanstandoff =    5.49. from the perspec-
tive of structural balance,
the social con   gurations
lovetriangle and mexicanstandoff are inherently
unstable. hence, the learnt af   nity for the con   gura-
tion lovetriangle seems higher than expected. this
is unsurprising, however, if we consider the domain of
the data (movies), where it might be a common plot
element. we also note that the    friend of a friend is a
friend    maxim is not supported by the feature weights
(even though it is a stable con   guration), and hence a
model based on this as a hard transitive constraint could
be expected to perform poorly.
cluster analysis: we brie   y analyze a particular run of
the id91 model for k = 2. in figure 5, we plot
the overall feature weights (wk) for a run (we plot 8
most signi   cant features from the text model, and the
primary structural features). we note that the two clus-
ters are reasonably well delineated; and thus id91
is meaningful. for this run, cluster 1 appears correlated
with higher weights of positive polarity features. clus-
ter 2 appears less differentiated in terms of structural
features than cluster 1 or the non-id91 structured
model.
qualitative evaluation we observe that relation char-
acterizations for character pairs are reasonable for most
narrative texts in the test set. figure 6 shows labels in-
ferred by the model for two well-known movies in the
test set. further, analysis of highest contributing evi-
dences that lead to predictions indicated that the model
usually provides meaningful justi   cations for relation-
ship characterization in terms of narrative acts, or im-
plied relations from structural features.
error analysis revealed that mismatched coreference la-

7

references

[1] a. agarwal, s. balasubramanian, j. zheng, and
s. dash. parsing screenplays for extracting social
networks from movies. eacl 2014, pages 50   58,
2014.

[2] a. agarwal, a. kotalwar, and o. rambow. auto-
matic extraction of social networks from literary
text: a case study on alice in wonderland. in the
proceedings of the 6th international joint confer-
ence on natural language processing (ijcnlp
2013), 2013.

[3] d. bamman, b. o   connor, and n. a. smith.
learning latent personas of    lm characters.
in
proceedings of the 51st annual meeting of the
association for computational linguistics, acl
2013, 4-9 august 2013, so   a, bulgaria, volume
1: long papers, pages 352   361, 2013.

[4] d. bamman, t. underwood, and n. a. smith. a
bayesian mixed effects model of literary charac-
ter. in proceedings of the 52nd annual meeting
of the association for computational linguistics,
volume 1, pages 370   379, 2014.

[5] p. bramsen, m. escobar-molano, a. patel, and
extracting social power relation-
r. alonso.
in proceedings
ships from natural language.
of the 49th annual meeting of the association
for computational linguistics: human language
technologies-volume 1, pages 773   782. associa-
tion for computational linguistics, 2011.

[6] d. cartwright and f. harary. structural balance:
a generalization of heider   s theory. psychological
review, 63(5):277, 1956.

[7] n. chambers. event schema induction with a
probabilistic entity-driven model. in emnlp, vol-
ume 13, pages 1797   1807, 2013.

[8] n. chambers and d. jurafsky. unsupervised
learning of narrative event chains. in acl 2008,
proceedings of the 46th annual meeting of the as-
sociation for computational linguistics, june 15-
20, 2008, columbus, ohio, usa, pages 789   797,
2008.

[9] m. collins. discriminative training methods for
id48: theory and experiments
with id88 algorithms. in proceedings of the

figure 5: heatmap of cluster-weights

figure 6: inferred relationships for movies    titanic   
(1997) and    ice age    (2002)

belings are the most common source of errors for the
model. secondly, in some cases, the text-based fea-
tures mistakenly identify negative sentiments due to
our coarse model of sentiment. for example, consider
the following segment for the movie smokin    aces 2:
   baker drags the wounded redstone to the    spider
trap    ... used to safeguard people   . here, the model
mistakenly predicts the relation between redstone and
baker as adversarial because of the negative connota-
tion of    drag   , inspite of other structural evidence.

7 conclusion

we have presented a framework for automatically infer-
ring interpersonal cooperation and con   ict in narrative
summaries. while our testbed was movie summaries,
the framework could potentially apply to other domains
of texts with social narratives, such as news stories. our
id91 framework provides a natural approach for
such id20. in the future, the framework
could be extended to handle nuanced relation catego-
rizations and asymmetric relationships. conceptually,
a natural extension would be to use predictions about
character relations to infer subtle character attributes
such as agenda, intentions and goals.

8

human language technologies, pages 142   151.
association for computational linguistics, 2012.

[19] v. krishnan and j. eisenstein.

   you   re mr.
lebowski, i   m the dude   : inducing address term
in human
formality in signed social networks.
language technologies: conference of the north
american chapter of the association of compu-
tational linguistics, (to appear in proceedings),
2015.

[20] j. leskovec, d. huttenlocher, and j. kleinberg.
in proceed-
signed networks in social media.
ings of the sigchi conference on human factors
in computing systems, pages 1361   1370. acm,
2010.

[21] b. liu, m. hu, and j. cheng. opinion observer:
analyzing and comparing opinions on the web. in
proceedings of the 14th international conference
on world wide web, www 2005, chiba, japan,
may 10-14, 2005, pages 342   351, 2005.

[22] r. c. schank and r. p. abelson. scripts, plans,
goals and understanding: an inquiry into human
knowledge structures. hillsdale, nj: l. n.].: erl-
baum, 1977.

[23] p. stenetorp, s. pyysalo, g. topi  c, t. ohta,
s. ananiadou, and j. tsujii. brat: a web-based
tool for nlp-assisted text annotation. in proceed-
ings of the demonstrations at the 13th conference
of the european chapter of the association for
computational linguistics, pages 102   107. asso-
ciation for computational linguistics, 2012.

[24] j. valls-vargas, j. zhu, and s. onta  n  on. toward
automatic role identi   cation in unannotated folk
tales. in tenth arti   cial intelligence and interac-
tive digital entertainment conference, 2014.

[25] t. wilson, j. wiebe, and p. hoffmann. recogniz-
ing contextual polarity in phrase-level sentiment
analysis. in hlt/emnlp 2005, human language
technology conference and conference on em-
pirical methods in natural language processing,
proceedings of the conference, 6-8 october 2005,
vancouver, british columbia, canada, 2005.

acl-02 conference on empirical methods in nat-
ural language processing-volume 10, pages 1   8.
association for computational linguistics, 2002.

[10] r. collobert, j. weston, l. bottou, m. karlen,
k. kavukcuoglu, and p. kuksa. natural language
processing (almost) from scratch. the journal
of machine learning research, 12:2493   2537,
2011.

[11] d. das, d. chen, a. f. t. martins, n. schneider,
and n. a. smith. frame-id29. com-
putational linguistics, 40(1):9   56, 2014.

[12] d. k. elson, n. dames, and k. r. mckeown. ex-
tracting social networks from literary    ction.
in
proceedings of the 48th annual meeting of the
association for computational linguistics, pages
138   147. association for computational linguis-
tics, 2010.

[13] t. evgeniou and m. pontil. regularized multi   
in proceedings of the tenth acm
task learning.
sigkdd international conference on knowledge
discovery and data mining, pages 109   117. acm,
2004.

[14] s. feng, j. s. kang, p. kuznetsova, and y. choi.
connotation lexicon: a dash of sentiment beneath
the surface meaning. in proceedings of the 51st
annual meeting of the association for computa-
tional linguistics, acl 2013, 4-9 august 2013,
so   a, bulgaria, volume 1: long papers, pages
1774   1784, 2013.

[15] m. a. finlayson.

learning narrative struc-
ture from annotated folktales. phd thesis, mas-
sachusetts institute of technology, 2012.

[16] a. hassan, a. abu-jbara, and d. radev. extract-
in work-
ing signed social networks from text.
shop proceedings of textgraphs-7 on graph-
based methods for natural language processing,
pages 6   14. association for computational lin-
guistics, 2012.

[17] f. heider. attitudes and cognitive organization.
the journal of psychology, 21(1):107   112, 1946.

[18] l. huang, s. fayong, and y. guo. structured per-
ceptron with inexact search. in proceedings of the
2012 conference of the north american chapter
of the association for computational linguistics:

9

