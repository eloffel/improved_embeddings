abstractive text summarization using sequence-to-sequence id56s and

beyond

ramesh nallapati

ibm watson

bowen zhou
ibm watson

cicero dos santos

ibm watson

nallapati@us.ibm.com

zhou@us.ibm.com

cicerons@us.ibm.com

  a  glar g  ul  ehre

universit   de montr  al

bing xiang
ibm watson

gulcehrc@iro.umontreal.ca

bingxia@us.ibm.com

abstract

in this work, we model abstractive text
summarization using attentional encoder-
decoder recurrent neural networks, and
show that they achieve state-of-the-art per-
formance on two different corpora. we
propose several novel models that address
critical problems in summarization that
are not adequately modeled by the basic
architecture, such as modeling key-words,
capturing the hierarchy of sentence-to-
word structure, and emitting words that
are rare or unseen at training time. our
work shows that many of our proposed
models contribute to further improvement
in performance. we also propose a new
dataset consisting of multi-sentence sum-
maries, and establish performance bench-
marks for further research.

introduction

1
abstractive text summarization is the task of gen-
erating a headline or a short summary consisting
of a few sentences that captures the salient ideas of
an article or a passage. we use the adjective    ab-
stractive    to denote a summary that is not a mere
selection of a few existing passages or sentences
extracted from the source, but a compressed para-
phrasing of the main contents of the document,
potentially using vocabulary unseen in the source
document.

this task can also be naturally cast as map-
ping an input sequence of words in a source doc-
ument to a target sequence of words called sum-
mary. in the recent past, deep-learning based mod-
els that map an input sequence into another out-
put sequence, called sequence-to-sequence mod-
els, have been successful in many problems such
as machine translation (bahdanau et al., 2014),

id103 (bahdanau et al., 2015) and
video captioning (venugopalan et al., 2015).
in
the framework of sequence-to-sequence models,
a very relevant model to our task is the atten-
tional recurrent neural network (id56) encoder-
decoder model proposed in bahdanau et al.
(2014), which has produced state-of-the-art per-
formance in machine translation (mt), which is
also a natural language task.

despite the similarities, abstractive summariza-
tion is a very different problem from mt. unlike
in mt, the target (summary) is typically very short
and does not depend very much on the length of
the source (document) in summarization. addi-
tionally, a key challenge in summarization is to op-
timally compress the original document in a lossy
manner such that the key concepts in the original
document are preserved, whereas in mt, the trans-
lation is expected to be loss-less. in translation,
there is a strong notion of almost one-to-one word-
level alignment between source and target, but in
summarization, it is less obvious.

we make the following main contributions in
this work:
(i) we apply the off-the-shelf atten-
tional encoder-decoder id56 that was originally
developed for machine translation to summariza-
tion, and show that it already outperforms state-
of-the-art systems on two different english cor-
pora. (ii) motivated by concrete problems in sum-
marization that are not suf   ciently addressed by
the machine translation based model, we propose
novel models and show that they provide addi-
tional improvement in performance. (iii) we pro-
pose a new dataset for the task of abstractive sum-
marization of a document into multiple sentences
and establish benchmarks.

the rest of the paper is organized as follows.
in section 2, we describe each speci   c problem
in abstractive summarization that we aim to solve,
and present a novel model that addresses it. sec-

6
1
0
2

 

g
u
a
6
2

 

 
 
]
l
c
.
s
c
[
 
 

5
v
3
2
0
6
0

.

2
0
6
1
:
v
i
x
r
a

tion 3 contextualizes our models with respect to
closely related work on the topic of abstractive text
summarization. we present the results of our ex-
periments on three different data sets in section 4.
we also present some qualitative analysis of the
output from our models in section 5 before con-
cluding the paper with remarks on our future di-
rection in section 6.

2 models

in this section, we    rst describe the basic encoder-
decoder id56 that serves as our baseline and then
propose several novel models for summarization,
each addressing a speci   c weakness in the base-
line.

2.1 encoder-decoder id56 with attention

and large vocabulary trick

our baseline model corresponds to the neural ma-
chine translation model used in bahdanau et al.
(2014). the encoder consists of a bidirectional
gru-id56 (chung et al., 2014), while the decoder
consists of a uni-directional gru-id56 with the
same hidden-state size as that of the encoder, and
an attention mechanism over the source-hidden
states and a soft-max layer over target vocabu-
lary to generate words.
in the interest of space,
we refer the reader to the original paper for a de-
tailed treatment of this model. in addition to the
basic model, we also adapted to the summariza-
tion problem, the large vocabulary    trick    (lvt)
described in jean et al. (2014). in our approach,
the decoder-vocabulary of each mini-batch is re-
stricted to words in the source documents of that
batch. in addition, the most frequent words in the
target dictionary are added until the vocabulary
reaches a    xed size. the aim of this technique
is to reduce the size of the soft-max layer of the
decoder which is the main computational bottle-
neck. in addition, this technique also speeds up
convergence by focusing the modeling effort only
on the words that are essential to a given example.
this technique is particularly well suited to sum-
marization since a large proportion of the words in
the summary come from the source document in
any case.

2.2 capturing keywords using feature-rich

encoder

in summarization, one of the key challenges is to
identify the key concepts and key entities in the

document, around which the story revolves.
in
order to accomplish this goal, we may need to
go beyond the word-embeddings-based represen-
tation of the input document and capture addi-
tional linguistic features such as parts-of-speech
tags, named-entity tags, and tf and idf statis-
tics of the words. we therefore create additional
look-up based embedding matrices for the vocab-
ulary of each tag-type, similar to the embeddings
for words. for continuous features such as tf
and idf, we convert them into categorical values
by discretizing them into a    xed number of bins,
and use one-hot representations to indicate the bin
number they fall into. this allows us to map them
into an embeddings matrix like any other tag-type.
finally, for each word in the source document, we
simply look-up its embeddings from all of its as-
sociated tags and concatenate them into a single
long vector, as shown in fig. 1. on the target side,
we continue to use only word-based embeddings
as the representation.

figure 1: feature-rich-encoder: we use one embedding
vector each for pos, ner tags and discretized tf and idf
values, which are concatenated together with word-based em-
beddings as input to the encoder.

2.3 modeling rare/unseen words using

switching generator-pointer

often-times in summarization, the keywords or
named-entities in a test document that are central
to the summary may actually be unseen or rare
with respect to training data. since the vocabulary
of the decoder is    xed at training time, it cannot
emit these unseen words. instead, a most common
way of handling these out-of-vocabulary (oov)
words is to emit an    unk    token as a placeholder.
however this does not result in legible summaries.
in summarization, an intuitive way to handle such
oov words is to simply point to their location in
the source document instead. we model this no-

hidden stateinput layeroutput layerwposnertfidfwposnertfidfwposnertfidfwposnertfidfattention mechanismencoderdecodertion using our novel switching decoder/pointer ar-
chitecture which is graphically represented in fig-
ure 2. in this model, the decoder is equipped with
a    switch    that decides between using the genera-
tor or a pointer at every time-step. if the switch
is turned on, the decoder produces a word from its
target vocabulary in the normal fashion. however,
if the switch is turned off, the decoder instead gen-
erates a pointer to one of the word-positions in the
source. the word at the pointer-location is then
copied into the summary. the switch is modeled
as a sigmoid activation function over a linear layer
based on the entire available context at each time-
step as shown below.
p (si = 1) =   (vs    (ws

hhi + ws

ee[oi   1]

+ ws

cci + bs)),

j

(p a

c hd

h, ws

where p (si = 1) is the id203 of the switch
turning on at the ith time-step of the decoder, hi
is the hidden state, e[oi   1] is the embedding vec-
tor of the emission from the previous time step,
ci is the attention-weighted context vector, and
c, bs and vs are the switch parame-
ws
ters. we use attention distribution over word posi-
tions in the document as the distribution to sample
the pointer from.
i (j)     exp(va    (wa
p a
j + ba)),

hhi   1 + wa

e e[oi   1]

+ wa

e, ws

pi = arg max

i (j)) for j     {1, . . . , nd}.
in the above equation, pi is the pointer value at
ith word-position in the summary, sampled from
the attention distribution pa
i over the document
word-positions j     {1, . . . , nd}, where p a
i (j) is
the id203 of the ith time-step in the decoder
pointing to the jth position in the document, and
j is the encoder   s hidden state at position j.
hd
at training time, we provide the model with ex-
plicit pointer information whenever the summary
word does not exist in the target vocabulary. when
the oov word in summary occurs in multiple doc-
ument positions, we break the tie in favor of its
   rst occurrence. at training time, we optimize the
conditional log-likelihood shown below, with ad-
ditional id173 penalties.

log p (y|x) =
(gi log{p (yi|y   i, x)p (si)}
+(1     gi) log{p (p(i)|y   i, x)(1     p (si))})
where y and x are the summary and document
words respectively, gi is an indicator function that

i

(cid:88)

is set to 0 whenever the word at position i in the
summary is oov with respect to the decoder vo-
cabulary. at test time, the model decides automat-
ically at each time-step whether to generate or to
point, based on the estimated switch id203
p (si). we simply use the arg max of the poste-
rior id203 of generation or pointing to gener-
ate the best output at each time step.

the pointer mechanism may be more robust in
handling rare words because it uses the encoder   s
hidden-state representation of rare words to decide
which word from the document to point to. since
the hidden state depends on the entire context of
the word, the model is able to accurately point to
unseen words although they do not appear in the
target vocabulary.1

figure 2: switching generator/pointer model: when the
switch shows    g   , the traditional generator consisting of the
softmax layer is used to produce a word, and when it shows
   p   , the pointer network is activated to copy the word from
one of the source document positions. when the pointer is
activated, the embedding from the source is used as input for
the next time-step as shown by the arrow from the encoder to
the decoder at the bottom.

2.4 capturing hierarchical document

structure with hierarchical attention

in datasets where the source document is very
long, in addition to identifying the keywords in
the document, it is also important to identify the
key sentences from which the summary can be
drawn. this model aims to capture this notion of
two levels of importance using two bi-directional

1even when the word does not exist in the source vocabu-
lary, the pointer model may still be able to identify the correct
position of the word in the source since it takes into account
the contextual representation of the corresponding    unk    to-
ken encoded by the id56. once the position is known, the
corresponding token from the source document can be dis-
played in the summary even when it is not part of the training
vocabulary either on the source side or the target side.

hidden stateencoderdecoderinput layeroutput layergpgggid56s on the source side, one at the word level
and the other at the sentence level. the attention
mechanism operates at both levels simultaneously.
the word-level attention is further re-weighted by
the corresponding sentence-level attention and re-
normalized as shown below:

p a(j) =

(cid:80)nd

p a
w(j)p a
k=1 p a

s (s(j))

,

w(k)p a

s (s(k))

where p a
w(j) is the word-level attention weight at
jth position of the source document, and s(j) is
the id of the sentence at jth word position, p a
s (l)
is the sentence-level attention weight for the lth
sentence in the source, nd is the number of words
in the source document, and p a(j) is the re-scaled
attention at the jth word position. the re-scaled
attention is then used to compute the attention-
weighted context vector that goes as input to the
hidden state of the decoder. further, we also con-
catenate additional positional embeddings to the
hidden state of the sentence-level id56 to model
positional importance of sentences in the docu-
ment. this architecture therefore models key sen-
tences as well as keywords within those sentences
jointly. a graphical representation of this model is
displayed in figure 3.

figure 3: hierarchical encoder with hierarchical attention:
the attention weights at the word level, represented by the
dashed arrows are re-scaled by the corresponding sentence-
level attention weights, represented by the dotted arrows.
the dashed boxes at the bottom of the top layer id56 rep-
resent sentence-level positional embeddings concatenated to
the corresponding hidden states.

3 related work

a vast majority of past work in summarization
has been extractive, which consists of identify-
ing key sentences or passages in the source doc-
ument and reproducing them as summary (neto et

al., 2002; erkan and radev, 2004; wong et al.,
2008a; filippova and altun, 2013; colmenares et
al., 2015; litvak and last, 2008; k. riedhammer
and hakkani-tur, 2010; ricardo ribeiro, 2013).

humans on the other hand, tend to paraphrase
the original story in their own words. as such, hu-
man summaries are abstractive in nature and sel-
dom consist of reproduction of original sentences
from the document. the task of abstractive sum-
marization has been standardized using the duc-
2003 and duc-2004 competitions.2 the data for
these tasks consists of news stories from various
topics with multiple reference summaries per story
generated by humans. the best performing system
on the duc-2004 task, called topiary (zajic
et al., 2004), used a combination of linguistically
motivated compression techniques, and an unsu-
pervised topic detection algorithm that appends
keywords extracted from the article onto the com-
pressed output. some of the other notable work in
the task of abstractive summarization includes us-
ing traditional phrase-table based machine transla-
tion approaches (banko et al., 2000), compression
using weighted tree-transformation rules (cohn
and lapata, 2008) and quasi-synchronous gram-
mar approaches (woodsend et al., 2010).

with the emergence of deep learning as a viable
alternative for many nlp tasks (collobert et al.,
2011), researchers have started considering this
framework as an attractive, fully data-driven alter-
native to abstractive summarization.
in rush et
al. (2015), the authors use convolutional models
to encode the source, and a context-sensitive at-
tentional feed-forward neural network to generate
the summary, producing state-of-the-art results on
gigaword and duc datasets. in an extension to
this work, chopra et al. (2016) used a similar con-
volutional model for the encoder, but replaced the
decoder with an id56, producing further improve-
ment in performance on both datasets.

in another paper that is closely related to our
work, hu et al. (2015) introduce a large dataset
for chinese short text summarization. they show
promising results on their chinese dataset using
an encoder-decoder id56, but do not report exper-
iments on english corpora.

in another very recent work, cheng and lapata
(2016) used id56 based encoder-decoder for ex-
tractive summarization of documents. this model
is not directly comparable to ours since their

2http://duc.nist.gov/

hidden stateword layerencoderdecoderinput layeroutput layerhidden statesentence layer<eos>sentence-level attentionword-level attentionframework is extractive while ours and that of
(rush et al., 2015), (hu et al., 2015) and (chopra
et al., 2016) is abstractive.

our work starts with the same framework as
(hu et al., 2015), where we use id56s for both
source and target, but we go beyond the standard
architecture and propose novel models that ad-
dress critical problems in summarization. we also
note that this work is an extended version of nal-
lapati et al. (2016). in addition to performing more
extensive experiments compared to that work, we
also propose a novel dataset for document summa-
rization on which we establish benchmark num-
bers too.

below, we analyze the similarities and differ-
ences of our proposed models with related work
on summarization.
feature-rich encoder (sec. 2.2): linguistic fea-
tures such as pos tags, and named-entities as well
as tf and idf information were used in many
extractive approaches to summarization (wong et
al., 2008b), but they are novel in the context of
deep learning approaches for abstractive summa-
rization, to the best of our knowledge.
switching generator-pointer model (sec. 2.3):
this model combines extractive and abstractive
approaches to summarization in a single end-to-
end framework. rush et al. (2015) also used
a combination of extractive and abstractive ap-
proaches, but their extractive model is a sepa-
rate log-linear classi   er with handcrafted features.
id193 (vinyals et al., 2015) have also
been used earlier for the problem of rare words
in the context of machine translation (luong et
al., 2015), but the novel addition of switch in our
model allows it to strike a balance between when
to be faithful to the original source (e.g., for named
entities and oov) and when it is allowed to be cre-
ative. we believe such a process arguably mim-
ics how human produces summaries. for a more
detailed treatment of this model, and experiments
on multiple tasks, please refer to the parallel work
published by some of the authors of this work
(gulcehre et al., 2016).
hierarchical attention model (sec. 2.4): pre-
viously proposed hierarchical encoder-decoder
models use attention only at sentence-level (li et
al., 2015). the novelty of our approach lies in joint
modeling of attention at both sentence and word
levels, where the word-level attention is further in-
   uenced by sentence-level attention, thus captur-

ing the notion of important sentences and impor-
tant words within those sentences. concatenation
of positional embeddings with the hidden state at
sentence-level is also new.

4 experiments and results
4.1 gigaword corpus
in this series of experiments3, we used the anno-
tated gigaword corpus as described in rush et al.
(2015). we used the scripts made available by
the authors of this work4 to preprocess the data,
which resulted in about 3.8m training examples.
the script also produces about 400k validation
and test examples, but we created a randomly sam-
pled subset of 2000 examples each for validation
and testing purposes, on which we report our per-
formance. further, we also acquired the exact test
sample used in rush et al. (2015) to make precise
comparison of our models with theirs. we also
made small modi   cations to the script to extract
not only the tokenized words, but also system-
generated parts-of-speech and named-entity tags.
training: for all the models we discuss below, we
used 200 dimensional id97 vectors (mikolov
et al., 2013) trained on the same corpus to initial-
ize the model embeddings, but we allowed them
to be updated during training. the hidden state di-
mension of the encoder and decoder was    xed at
400 in all our experiments. when we used only
the    rst sentence of the document as the source,
as done in rush et al. (2015), the encoder vocabu-
lary size was 119,505 and that of the decoder stood
at 68,885. we used adadelta (zeiler, 2012) for
training, with an initial learning rate of 0.001. we
used a batch-size of 50 and randomly shuf   ed the
training data at every epoch, while sorting every
10 batches according to their lengths to speed up
training. we did not use any dropout or regular-
ization, but applied gradient clipping. we used
early stopping based on the validation set and used
the best model on the validation set to report all
test performance numbers. for all our models, we
employ the large-vocabulary trick, where we re-
strict the decoder vocabulary size to 2,0005, be-
cause it cuts down the training time per epoch by
nearly three times, and helps this and all subse-

3we used kyunghyun cho   s code (https://github.
com/kyunghyuncho/dl4mt-material) as the start-
ing point.

4https://github.com/facebook/namas
5larger values improved performance only marginally,

but at the cost of much slower training.

quent models converge in only 50%-75% of the
epochs needed for the model based on full vocab-
ulary.
decoding: at decode-time, we used id125
of size 5 to generate the summary, and limited the
size of summary to a maximum of 30 words, since
this is the maximum size we noticed in the sam-
pled validation set. we found that the average sys-
tem summary length from all our models (7.8 to
8.3) agrees very closely with that of the ground
truth on the validation set (about 8.7 words), with-
out any speci   c tuning.
computational costs: we trained all our mod-
els on a single tesla k40 gpu. most models took
about 10 hours per epoch on an average except the
hierarchical attention model, which took 12 hours
per epoch. all models typically converged within
15 epochs using our early stopping criterion based
on the validation cost. the wall-clock training
time until convergence therefore varies between
6-8 days depending on the model. generating
summaries at test time is reasonably fast with a
throughput of about 20 summaries per second on
a single gpu, using a batch size of 1.
id74: similar to (nallapati et al.,
2016) and (chopra et al., 2016), we use the full
length f1 variant of id86 to evaluate our sys-
tem. although limited length recall was the pre-
ferred metric for most previous work, one of its
disadvantages is choosing the length limit which
varies from corpus to corpus, making it dif   cult
for researchers to compare performances. full-
length recall, on the other hand, does not impose
a length restriction but unfairly favors longer sum-
maries. full-length f1 solves this problem since it
can penalize longer summaries, while not impos-
ing a speci   c length restriction.

in addition, we also report the percentage of
tokens in the system summary that occur in the
source (which we call    src. copy rate    in table 1).
we describe all our experiments and results on the
gigaword corpus below.
words-lvt2k-1sent: this is the baseline attentional
encoder-decoder model with the large vocabulary
trick. this model is trained only on the    rst sen-
tence from the source document, as done in rush
et al. (2015).
words-lvt2k-2sent: this model is identical to the
model above except for the fact that it is trained

6http://www.beid8.com/pages/default.

aspx

on the    rst two sentences from the source. on
this corpus, adding the additional sentence in the
source does seem to aid performance, as shown
in table 1. we also tried adding more sentences,
but the performance dropped, which is probably
because the latter sentences in this corpus are not
pertinent to the summary.
words-lvt2k-2sent-hieratt: since we used two sen-
tences from source document, we trained the hi-
erarchical attention model proposed in sec 2.4.
as shown in table 1, this model improves perfor-
mance compared to its    atter counterpart by learn-
ing the relative importance of the    rst two sen-
tences automatically.
feats-lvt2k-2sent: here, we still train on the    rst
two sentences, but we exploit the parts-of-speech
and named-entity tags in the annotated gigaword
corpus as well as tf, idf values, to augment the
input embeddings on the source side as described
in sec 2.2. in total, our embedding vector grew
from the original 100 to 155, and produced incre-
mental gains compared to its counterpart words-
lvt2k-2sent as shown in table 1, demonstrating the
utility of syntax based features in this task.
feats-lvt2k-2sent-ptr: this is the switching gener-
ator/pointer model described in sec. 2.3, but in
addition, we also use feature-rich embeddings on
the document side as in the above model. our ex-
periments indicate that the new model is able to
achieve the best performance on our test set by all
three id8 variants as shown in table 1.
comparison with state-of-the-art: we com-
pared the performance of our model words-lvt2k-
1sent with state-of-the-art models on the sample
created by rush et al. (2015), as displayed in the
bottom part of table 1. we also trained another
system which we call words-lvt5k-1sent which
has a larger lvt vocabulary size of 5k, but also
has much larger source and target vocabularies of
400k and 200k respectively.

the reason we did not evaluate our best vali-
dation models here is that this test set consisted of
only 1 sentence from the source document, and did
not include nlp annotations, which are needed
in our best models. the table shows that, despite
this fact, our model outperforms the abs+ model
of rush et al. (2015) with statistical signi   cance.
in addition, our models exhibit better abstractive
ability as shown by the src. copy rate metric in the
last column of the table. further, our larger model
words-lvt5k-1sent outperforms the state-of-the-art

model of (chopra et al., 2016) with statistically
signi   cant improvement on id8-1.

we believe the bidirectional id56 we used to
model the source captures richer contextual infor-
mation of every word than the bag-of-embeddings
representation used by rush et al. (2015) and
chopra et al. (2016) in their convolutional atten-
tional encoders, which might explain our superior
performance. further, explicit modeling of im-
portant information such as multiple source sen-
tences, word-level linguistic features, using the
switch mechanism to point to source words when
needed, and hierarchical attention, solve speci   c
problems in summarization, each boosting perfor-
mance incrementally.

4.2 duc corpus
the duc corpus7 comes in two parts: the 2003
corpus consisting of 624 document, summary
pairs and the 2004 corpus consisting of 500 pairs.
since these corpora are too small to train large
neural networks on, rush et al. (2015) trained their
models on the gigaword corpus, but combined it
with an additional log-linear extractive summa-
rization model with handcrafted features, that is
trained on the duc 2003 corpus. they call the
original neural attention model the abs model,
and the combined model abs+. chopra et al.
(2016) also report the performance of their ras-
elman model on this corpus and is the current
state-of-the-art since it outperforms all previously
published baselines including non-neural network
based extractive and abstractive systems, as mea-
sured by the of   cial duc metric of recall at 75
bytes. in these experiments, we use the same met-
ric to evaluate our models too, but we omit report-
ing numbers from other systems in the interest of
space.

in our work, we simply run the models trained
on gigaword corpus as they are, without tuning
them on the duc validation set. the only change
we made to the decoder is to suppress the model
from emitting the end-of-summary tag, and force
it to emit exactly 30 words for every summary,
since the of   cial evaluation on this corpus is based
on limited-length id8 recall. on this corpus
too, since we have only a single sentence from
source and no nlp annotations, we ran just the
models words-lvt2k-1sent and words-lvt5k-1sent.
the performance of this model on the test set

7http://duc.nist.gov/duc2004/tasks.html

is compared with abs and abs+ models, ras-
elman from (chopra et al., 2016), as well as top-
iary, the top performing system on duc-2004 in
table 2. we note our best model words-lvt5k-1sent
outperforms ras-elman on two of the three vari-
ants of id8, while being competitive on id8-
1.

model
topiary
abs
abs+
ras-elman
words-lvt2k-1sent
words-lvt5k-1sent

id8-1 id8-2 id8-l
20.12
22.05
23.81
24.06
24.59
25.24

25.12
26.55
28.18
28.97
28.35
28.61

6.46
7.06
8.49
8.26
9.46
9.42

table 2: evaluation of our models using the limited-length
id8 recall at 75 bytes on duc validation and test sets.
our best model, although trained exclusively on the gi-
gaword corpus, consistently outperforms the abs+ model
which is tuned on the duc-2003 validation corpus in addi-
tion to being trained on the gigaword corpus.

4.3 id98/daily mail corpus
the existing abstractive text summarization cor-
pora including gigaword and duc consist of only
one sentence in each summary.
in this section,
we present a new corpus that comprises multi-
sentence summaries. to produce this corpus, we
modify an existing corpus that has been used
for the task of passage-based id53
(hermann et al., 2015).
in this work, the au-
thors used the human generated abstractive sum-
mary bullets from new-stories in id98 and daily
mail websites as questions (with one of the enti-
ties hidden), and stories as the corresponding pas-
sages from which the system is expected to an-
swer the    ll-in-the-blank question. the authors re-
leased the scripts that crawl, extract and generate
pairs of passages and questions from these web-
sites. with a simple modi   cation of the script, we
restored all the summary bullets of each story in
the original order to obtain a multi-sentence sum-
mary, where each bullet is treated as a sentence. in
all, this corpus has 286,817 training pairs, 13,368
validation pairs and 11,487 test pairs, as de   ned
by their scripts. the source documents in the train-
ing set have 766 words spanning 29.74 sentences
on an average while the summaries consist of 53
words and 3.72 sentences. the unique character-
istics of this dataset such as long documents, and
ordered multi-sentence summaries present inter-
esting challenges, and we hope will attract future

# model name

id8-1 id8-2 id8-l src. copy rate (%)

full length f1 on our internal test set

1 words-lvt2k-1sent
2 words-lvt2k-2sent
3 words-lvt2k-2sent-hieratt
4
5

feats-lvt2k-2sent
feats-lvt2k-2sent-ptr

6 abs+ (rush et al., 2015)
7 words-lvt2k-1sent
8 ras-elman (chopra et al., 2016)
9 words-lvt5k-1sent

34.97
35.73
36.05
35.90
*36.40

29.78
32.67
33.78
*35.30

17.17
17.38
18.17
17.57
17.77

11.89
15.59
15.97
16.64

32.70
33.25
33.52
33.38
*33.71

26.97
30.64
31.15
*32.62

full length f1 on the test set used by (rush et al., 2015)

75.85
79.54
78.52
78.92
78.70

91.50
74.57

table 1: performance comparison of various models.    *    indicates statistical signi   cance of the corresponding model with
respect to the baseline model on its dataset as given by the 95% con   dence interval in the of   cial id8 script. we report
statistical signi   cance only for the best performing models.    src. copy rate    for the reference data on our validation sample is
45%. please refer to section 4 for explanation of notation.
id8-1 id8-2 id8-l
29.47
29.01
*32.65

model
words-lvt2k
words-lvt2k-hieratt
words-lvt2k-temp-att

32.49
32.75
*35.46

11.84
12.21
*13.30

table 3: performance of various models on id98/daily
mail test set using full-length id8-f1 metric. bold faced
numbers indicate best performing system.

researchers to build and test novel models on it.

the dataset is released in two versions: one
consisting of actual entity names, and the other,
in which entity occurrences are replaced with
document-speci   c integer-ids beginning from 0.
since the vocabulary size is smaller
in the
anonymized version, we used it in all our exper-
iments below. we limited the source vocabulary
size to 150k, and the target vocabulary to 60k,
the source and target lengths to at most 800 and
100 words respectively. we used 100-dimensional
id97 embeddings trained on this dataset as
input, and we    xed the model hidden state size at
200. we also created explicit pointers in the train-
ing data by matching only the anonymized entity-
ids between source and target on similar lines as
we did for the oov words in gigaword corpus.
computational costs: we used a single tesla k-
40 gpu to train our models on this dataset as well.
while the    at models (words-lvt2k and words-
lvt2k-ptr) took under 5 hours per epoch, the hier-
archical attention model was very expensive, con-
suming nearly 12.5 hours per epoch. convergence
of all models is also slower on this dataset com-
pared to gigaword, taking nearly 35 epochs for
all models. thus, the wall-clock time for train-
ing until convergence is about 7 days for the    at
models, but nearly 18 days for the hierarchical at-
tention model. decoding is also slower as well,
with a throughput of 2 examples per second for

   at models and 1.5 examples per second for the
hierarchical attention model, when run on a single
gpu with a batch size of 1.
evaluation: we evaluated our models using the
full-length id8 f1 metric that we employed for
the gigaword corpus, but with one notable differ-
ence: in both system and gold summaries, we con-
sidered each highlight to be a separate sentence.8
results: results from the basic attention encoder-
decoder as well as the hierarchical attention model
are displayed in table 3. although this dataset is
smaller and more complex than the gigaword cor-
pus, it is interesting to note that the id8 num-
bers are in the same range. however, the hier-
archical attention model described in sec. 2.4
outperforms the baseline attentional decoder only
marginally.

upon visual inspection of the system output, we
noticed that on this dataset, both these models pro-
duced summaries that contain repetitive phrases
or even repetitive sentences at times. since the
summaries in this dataset involve multiple sen-
tences, it is likely that the decoder    forgets    what
part of the document was used in producing earlier
highlights. to overcome this problem, we used
the temporal attention model of sankaran et al.
(2016) that keeps track of past attentional weights
of the decoder and expliticly discourages it from
attending to the same parts of the document in fu-
ture time steps. the model works as shown by the

8on this dataset, we used the pyid8 script (https://
pypi.python.org/pypi/pyid8/0.1.0) that al-
lows evaluation of each sentence as a separate unit. addi-
tional pre-processing involves assigning each highlight to its
own "<a>" tag in the system and gold xml    les that go as
input to the id8 evaluation script. similar evaluation was
also done by (cheng and lapata, 2016).

source document
( @entity0 ) wanted :    lm director , must be eager to shoot footage of golden lassos and invisible
jets . <eos> @entity0 con   rms that @entity5 is leaving the upcoming " @entity9 " movie ( the
hollywood reporter    rst broke the story ) . <eos> @entity5 was announced as director of the movie
in november . <eos> @entity0 obtained a statement from @entity13 that says , " given creative
differences , @entity13 and @entity5 have decided not to move forward with plans to develop and
direct     @entity9     together . <eos> " ( @entity0 and @entity13 are both owned by @entity16
. <eos> ) the movie , starring @entity18 in the title role of the @entity21 princess , is still set
for release on june 00 , 0000 . <eos> it    s the    rst theatrical movie centering around the most
popular female superhero . <eos> @entity18 will appear beforehand in " @entity25 v. @entity26
: @entity27 , " due out march 00 , 0000 . <eos> in the meantime , @entity13 will need to    nd
someone new for the director    s chair . <eos>
ground truth summary
@entity5 is no longer set to direct the    rst " @entity9 " theatrical movie <eos> @entity5 left the
project over " creative differences " <eos> movie is currently set for 0000
words-lvt2k
@entity0 con   rms that @entity5 is leaving the upcoming " @entity9 " movie <eos> @entity13
and @entity5 have decided not to move forward with plans to develop <eos> @entity0 con   rms
that @entity5 is leaving the upcoming " @entity9 " movie
words-lvt2k-hieratt
@entity5 is leaving the upcoming " @entity9 " movie <eos> the movie is still set for release on
june 00 , 0000 <eos> @entity5 is still set for release on june 00 , 0000
words-lvt2k-temp-att
@entity0 con   rms that @entity5 is leaving the upcoming " @entity9 " movie <eos> the movie is
the    rst    lm to around the most popular female actor <eos> @entity18 will appear in " @entity25
, " due out march 00 , 0000

table 4: comparison of gold truth summary with summaries from various systems. named entities and
numbers are anonymized by the preprocessing script. the "<eos>" tags represent the boundary between
two highlights. the temporal attention model (words-lvt2k-temp-att) solves the problem of repetitions in
summary as exhibited by the models words-lvt2k and words-lvt2k-hieratt by encouraging the attention
model to focus on the uncovered portions of the document.

following simple equations:

t   1(cid:88)

k=1

  t =

k;   t       (cid:48)
  (cid:48)

t
  t

(1)

where   (cid:48)
t is the unnormalized attention-weights
vector at the tth time-step of the decoder.
in
other words, the temporal attention model down-
weights the attention weights at the current time
step if the past attention weights are high on the
same part of the document.

using this strategy,

the temporal attention
model improves performance signi   cantly over
both the baseline model as well as the hierarchical
attention model. we have also noticed that there
are fewer repetitions of summay highlights pro-
duced by this model as shown in the example in
table 4.

these results, although preliminary, should
serve as a good baseline for future researchers to
compare their models against.

5 qualitative analysis

table 5 presents a few high quality and poor qual-
ity output on the validation set from feats-lvt2k-
2sent, one of our best performing models. even
when the model differs from the target summary,
its summaries tend to be very meaningful and rel-
evant, a phenomenon not captured by word/phrase
matching id74 such as id8. on
the other hand, the model sometimes    misinter-
prets    the semantics of the text and generates a
summary with a comical interpretation as shown
in the poor quality examples in the table. clearly,
capturing the    meaning    of complex sentences re-
mains a weakness of these models.

our next example output, presented in figure
4, displays the sample output from the switching
generator/pointer model on the gigaword corpus.
it is apparent from the examples that the model
learns to use pointers very accurately not only for
named entities, but also for multi-word phrases.
despite its accuracy, the performance improve-
ment of the overall model is not signi   cant. we
believe the impact of this model may be more pro-
nounced in other settings with a heavier tail distri-
bution of rare words. we intend to carry out more
experiments with this model in the future.

on id98/daily mail data, although our models
are able to produce good quality multi-sentence
summaries, we notice that the same sentence or

good quality summary output
s: a man charged with the murder last year of a british back-
packer confessed to the slaying on the night he was charged
with her killing , according to police evidence presented at a
court hearing tuesday . ian douglas previte , ## , is charged
with murdering caroline stuttle , ## , of yorkshire , england
t: man charged with british backpacker    s death confessed
to crime police of   cer claims
o: man charged with murdering british backpacker con-
fessed to murder
s: following are the leading scorers in the english premier
league after saturday    s matches : ## - alan shearer -lrb-
newcastle united -rrb- , james beattie .
t: leading scorers in english premier league
o: english premier league leading scorers
s: volume of transactions at the nigerian stock exchange
has continued its decline since last week , a nse of   cial said
thursday . the latest statistics showed that a total of ##.###
million shares valued at ###.### million naira -lrb- about
#.### million us dollars -rrb- were traded on wednesday in
#,### deals .
t: transactions dip at nigerian stock exchange
o: transactions at nigerian stock exchange down
poor quality summary output
s: broccoli and broccoli sprouts contain a chemical that kills
the bacteria responsible for most stomach cancer , say re-
searchers , con   rming the dietary advice that moms have
been handing out for years . in laboratory tests the chemical
, <unk> , killed helicobacter pylori , a bacteria that causes
stomach ulcers and often fatal stomach cancers .
t: for release at #### <unk> mom was right broccoli is
good for you say cancer researchers
o: broccoli sprouts contain deadly bacteria
s: norway delivered a diplomatic protest to russia on mon-
day after three norwegian    sheries research expeditions
were barred from russian waters . the norwegian research
ships were to continue an annual program of charting    sh
resources shared by the two countries in the barents sea re-
gion .
t: norway protests russia barring    sheries research ships
o: norway grants diplomatic protest to russia
s: j.p. morgan chase    s ability to recover from a slew of
recent losses rests largely in the hands of two men , who are
both looking to restore tarnished reputations and may be
considered for the top job someday . geoffrey <unk> , now
the co-head of j.p. morgan    s investment bank , left goldman
, sachs & co. more than a decade ago after executives say
he lost out in a bid to lead that    rm .
t: # executives to lead j.p. morgan chase on road to recov-
ery
o: j.p. morgan chase may be considered for top job

table 5: examples of generated summaries from our best
model on the validation set of gigaword corpus. s: source
document, t: target summary, o: system output. although
we displayed equal number of good quality and poor quality
summaries in the table, the good ones are far more prevalent
than the poor ones.

of the 38th annual meeting on association for com-
putational linguistics, 22:318   325.

[cheng and lapata2016] jianpeng cheng and mirella
lapata. 2016. neural summarization by extracting
sentences and words. in proceedings of the 54th an-
nual meeting of the association for computational
linguistics.

[cheng et al.2016] jianpeng cheng, li dong, and
long short-term
corr,

mirella lapata.
memory-networks for machine reading.
abs/1601.06733.

2016.

[chopra et al.2016] sumit chopra, michael auli, and
alexander m. rush. 2016. abstractive sentence
summarization with attentive recurrent neural net-
works. in hlt-naacl.

[chung et al.2014] junyoung chung,   aglar g  l  ehre,
kyunghyun cho, and yoshua bengio. 2014. em-
pirical evaluation of gated recurrent neural networks
on sequence modeling. corr, abs/1412.3555.

[cohn and lapata2008] trevor cohn and mirella lap-
ata. 2008. sentence compression beyond word dele-
tion. in proceedings of the 22nd international con-
ference on computational linguistics - volume 1,
pages 137   144.

[collobert et al.2011] ronan collobert, jason weston,
l  on bottou, michael karlen, koray kavukcuoglu,
and pavel p. kuksa.
lan-
guage processing (almost) from scratch. corr,
abs/1103.0398.

natural

2011.

[colmenares et al.2015] carlos a. colmenares, marina
litvak, amin mantrach, and fabrizio silvestri.
2015. heads: headline generation as sequence pre-
diction using an abstract feature-rich space. in pro-
ceedings of the 2015 conference of the north amer-
ican chapter of the association for computational
linguistics: human language technologies, pages
133   142.

[erkan and radev2004] g. erkan and d. r. radev.
2004. lexrank: graph-based lexical centrality as
salience in text summarization. journal of arti   cial
intelligence research, 22:457   479.

2013.

filippova

[filippova and altun2013] katja

and
yasemin altun.
overcoming the lack
in pro-
of parallel data in sentence compression.
ceedings of
the 2013 conference on empirical
methods in natural language processing, pages
1481   1491.

figure 4: sample output from switching generator/pointer
networks. an arrow indicates that a pointer to the source po-
sition was used to generate the corresponding summary word.

phrase often gets repeated in the summary. we be-
lieve models that incorporate intra-attention such
as cheng et al. (2016) can    x this problem by en-
couraging the model to    remember    the words it
has already produced in the past.

6 conclusion

in this work, we apply the attentional encoder-
decoder for the task of abstractive summarization
with very promising results, outperforming state-
of-the-art results signi   cantly on two different
datasets. each of our proposed novel models ad-
dresses a speci   c problem in abstractive summa-
rization, yielding further improvement in perfor-
mance. we also propose a new dataset for multi-
sentence summarization and establish benchmark
numbers on it. as part of our future work, we plan
to focus our efforts on this data and build more ro-
bust models for summaries consisting of multiple
sentences.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
corr, abs/1409.0473.

[bahdanau et al.2015] dzmitry

jan
chorowski, dmitriy serdyuk, philemon brakel,
and yoshua bengio. 2015. end-to-end attention-
based large vocabulary id103. corr,
abs/1508.04395.

bahdanau,

[gulcehre et al.2016] caglar gulcehre, sungjin ahn,
ramesh nallapati, bowen zhou, and yoshua ben-
in pro-
gio. 2016. pointing the unknown words.
ceedings of the 54th annual meeting of the associa-
tion for computational linguistics.

[banko et al.2000] michele banko, vibhu o. mittal,
and michael j witbrock. 2000. headline genera-
tion based on statistical translation. in proceedings

[hermann et al.2015] karl moritz hermann, tom  s
kocisk  , edward grefenstette, lasse espeholt, will
kay, mustafa suleyman, and phil blunsom. 2015.

[rush et al.2015] alexander m. rush, sumit chopra,
and jason weston. 2015. a neural attention model
corr,
for abstractive sentence summarization.
abs/1509.00685.

[sankaran et al.2016] b. sankaran, h. mi, y. al-
onaizan, and a. ittycheriah. 2016. temporal atten-
tion model for id4. arxiv
e-prints, august.

[venugopalan et al.2015] subhashini

venugopalan,
marcus rohrbach,
jeff donahue, raymond j.
mooney, trevor darrell, and kate saenko. 2015.
corr,
sequence to sequence - video to text.
abs/1505.00487.

[vinyals et al.2015] o. vinyals, m. fortunato, and
n. jaitly. 2015. id193. arxiv e-prints,
june.

[wong et al.2008a] kam-fai wong, mingli wu, and
wenjie li. 2008a. extractive summarization using
in pro-
supervised and semi-supervised learning.
ceedings of the 22nd international conference on
computational linguistics - volume 1, pages 985   
992.

[wong et al.2008b] kam-fai wong, mingli wu, and
wenjie li. 2008b. extractive summarization using
in pro-
supervised and semi-supervised learning.
ceedings of the 22nd annual meeting of the associa-
tion for computational linguistics, pages 985   992.

[woodsend et al.2010] kristian woodsend, yansong
feng, and mirella lapata. 2010. title generation
with quasi-synchronous grammar. in proceedings of
the 2010 conference on empirical methods in natu-
ral language processing, emnlp    10, pages 513   
523, stroudsburg, pa, usa. association for com-
putational linguistics.

[zajic et al.2004] david zajic, bonnie j. dorr, and
richard schwartz. 2004. bbn/umd at duc-2004:
in proceedings of the north american
topiary.
chapter of the association for computational lin-
guistics workshop on document understanding,
pages 112   119.

[zeiler2012] matthew d. zeiler. 2012. adadelta:
corr,

an adaptive learning rate method.
abs/1212.5701.

teaching machines to read and comprehend. corr,
abs/1506.03340.

[hu et al.2015] baotian hu, qingcai chen, and fangze
zhu. 2015. lcsts: a large scale chinese short text
summarization dataset. in proceedings of the 2015
conference on empirical methods in natural lan-
guage processing, pages 1967   1972, lisbon, portu-
gal, september. association for computational lin-
guistics.

[jean et al.2014] s  bastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2014. on
using very large target vocabulary for neural ma-
chine translation. corr, abs/1412.2007.

[k. riedhammer and hakkani-tur2010] b.

favre
k. riedhammer and d. hakkani-tur. 2010. long
story short      a  s global unsupervised models for
keyphrase based meeting summarization. in speech
communication, pages 801   815.

[li et al.2015] jiwei li, minh-thang luong, and dan
2015. a hierarchical neural autoen-
corr,

for paragraphs and documents.

jurafsky.
coder
abs/1506.01057.

[litvak and last2008] m. litvak and m. last. 2008.
single-
in coling 2008, pages

graph-based
document summarization.
17   24.

extraction

keyword

for

[luong et al.2015] thang luong,

ilya sutskever,
quoc v. le, oriol vinyals, and wojciech zaremba.
2015. addressing the rare word problem in neural
in proceedings of the 53rd
machine translation.
annual meeting of the association for computa-
tional linguistics and the 7th international joint
conference on natural language processing of the
asian federation of natural language processing,
pages 11   19.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg corrado, and jeffrey dean. 2013.
distributed representations of words and phrases
and their compositionality. corr, abs/1310.4546.

[nallapati et al.2016] ramesh nallapati, bing xiang,
2016. sequence-to-sequence
iclr workshop,

and bowen zhou.
id56s for text summarization.
abs/1602.06023.

[neto et al.2002] joel larocca neto, alex alves fre-
itas, and celso a. a. kaestner. 2002. automatic
text summarization using a machine learning ap-
proach. in proceedings of the 16th brazilian sym-
posium on arti   cial intelligence: advances in arti-
   cial intelligence, pages 205   215.

[ricardo ribeiro2013] david martins de matos jo    co
p. neto anatole gershman jaime carbonell ri-
cardo ribeiro, lu   s marujo. 2013. self reinforce-
ment for important passage retrieval. in 36th inter-
national acm sigir conference on research and
development in information retrieval, pages 845   
848.

