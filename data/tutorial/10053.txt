6
1
0
2

 

b
e
f
1
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
0
6
3
0

.

2
0
6
1
:
v
i
x
r
a

variations of the similarity function of
textrank for automated summarization

federico barrios1, federico l  opez1, luis argerich1, rosita wachenchauzer12

1 facultad de ingenier    a, universidad de buenos aires,

ciudad aut  onoma de buenos aires, argentina.

2 universidad nacional de tres de febrero, caseros, argentina.

{fbarrios,fjlopez}@fi.uba.ar

abstract. this article presents new alternatives to the similarity func-
tion for the textrank algorithm for automated summarization of texts.
we describe the generalities of the algorithm and the di   erent functions
we propose. some of these variants achieve a signi   cative improvement
using the same metrics and dataset as the original publication.

keywords: textrank variations, automated summarization, informa-
tion retrieval ranking functions

1

introduction

in the    eld of natural language processing, an extractive summarization task can
be described as the selection of the most important sentences in a document.
using di   erent levels of compression, a summarized version of the document of
arbitrary length can be obtained.

textrank is a graph-based extractive summarization algorithm. it is domain
and language independent since it does not require deep linguistic knowledge,
nor domain or language speci   c annotated corpora [16]. these features makes the
algorithm widely used: it performs well summarizing structured text like news
articles, but it has also shown good results in other usages such as summarizing
meeting transcriptions [8] and assessing web content credibility [1].

in this article we present di   erent proposals for the construction of the graph

and report the results obtained with them.

the    rst sections of this article describe previous work in the area and an
overview of the textrank algorithm. then we present the variations and describe
the di   erent metrics and dataset used for the evaluation. finally we report the
results obtained using the proposed changes.

2 previous work

the    eld of automated summarization has attracted interest since the late 50   s
[14]. traditional methods for text summarization analyze the frequency of words
or sentences in the    rst paragraphs of the text to identify the most important

lexical elements. the mainstream research in this    eld emphasizes extractive ap-
proaches to summarization using statistical methods [4]. several statistical mod-
els have been developed based on training corpora to combine di   erent heuristics
using keywords, position and length of sentences, word frequency or titles [13].
other methods are based in the representation of the text as a graph. the
graph-based ranking approaches consider the intrinsic structure of the texts in-
stead of treating texts as simple aggregations of terms. thus it is able to capture
and express richer information in determining important concepts [19].

the selected text fragments to use in the graph construction can be phrases
[6], sentences [14], or paragraphs [18]. currently, many successful systems adopt
the sentences considering the tradeo    between content richness and grammar
correctness. according to these approach the most important sentences are the
most connected ones in the graph and are used for building a    nal summary
[2]. to identify relations between sentences (edges for the graph) there are sev-
eral measures: overlapping words, cosine distance and query-sensitive similarity.
also, some authors have proposed combinations of the previous with supervised
learning functions [19].

these algorithms use di   erent information retrieval techniques to determine
the most important sentences (vertices) and build the summary [23]. the tex-
trank algorithm developed by mihalcea and tarau [17] and the lexrank al-
gorithm by erkan and radev [7] are based in ranking the lexical units of the
text (sentences or words) using variations of id95 [20]. other graph-based
ranking algorithms such as hits [11] or positional function [10] may be also
applied.

3 textrank

3.1 description

textrank is an unsupervised algorithm for the automated summarization of
texts that can also be used to obtain the most important keywords in a document.
it was introduced by rada mihalcea and paul tarau in [17].

the algorithm applies a variation of id95 [20] over a graph constructed
speci   cally for the task of summarization. this produces a ranking of the el-
ements in the graph: the most important elements are the ones that better
describe the text. this approach allows textrank to build summaries without
the need of a training corpus or labeling and allows the use of the algorithm
with di   erent languages.

3.2 text as a graph

for the task of automated summarization, textrank models any document as
a graph using sentences as nodes [3]. a function to compute the similarity of
sentences is needed to build edges in between. this function is used to weight
the graph edges, the higher the similarity between sentences the more important

the edge between them will be in the graph. in the domain of a random walker,
as used frequently in id95 [20], we can say that we are more likely to go
from one sentence to another if they are very similar.

textrank determines the relation of similarity between two sentences based
on the content that both share. this overlap is calculated simply as the number
of common lexical tokens between them, divided by the lenght of each to avoid
promoting long sentences.

the function featured in the original algorithm can be formalized as:

de   nition 1. given si, sj two sentences represented by a set of n words that
in si are represented as si = wi
n. the similarity function for si, sj
can be de   ned as:

2, ..., wi

1, wi

sim(si, sj) =

|{wk|wk     si&wk     sj}|

log(|si|) + log(|sj|)

(1)

the result of this process is a dense graph representing the document. from
this graph, id95 is used to compute the importance of each vertex. the
most signi   cative sentences are selected and presented in the same order as they
appear in the document as the summary.

4 experiments

4.1 our variations

this section will describe the di   erent modi   cations that we propose over the
original textrank algorithm. these ideas are based in changing the way in which
distances between sentences are computed to weight the edges of the graph used
for id95. these similarity measures are orthogonal to the textrank model,
thus they can be easily integrated into the algorithm. we found some of these
variations to produce signi   cative improvements over the original algorithm.

longest common substring from two sentences we identify the longest
common substring and report the similarity to be its length [9].

cosine distance the cosine similarity is a metric widely used to compare
texts represented as vectors. we used a classical tf-idf model to represent the
documents as vectors and computed the cosine between vectors as a measure
of similarity. since the vectors are de   ned to be positive, the cosine results in
values in the range [0,1] where a value of 1 represents identical vectors and 0
represents orthogonal vectors [24].

(cid:40)

bm25 bm25 / okapi-bm25 is a ranking function widely used as the state of
the art for information retrieval tasks. bm25 is a variation of the tf-idf model
using a probabilistic model [22].

de   nition 2. given two sentences r, s, bm25 is de   ned as:
f (si, r)    (k1 + 1)
f (si, r) + k1    (1     b + b   

idf (si)   

bm 25(r, s) =

n(cid:88)

i=1

|r|

avgdl )

(2)

where k and b are parameters. we used k = 1.2 and b = 0.75. avgdl is the

average length of the sentences in our collection.

this function de   nition implies that if a word appears in more than half the
documents of the collection, it will have a negative value. since this can cause
problems in the next stage of the algorithm, we used the following correction
formula:

idf (si) =

log(n     n(si) + 0.5)     log(n(si) + 0.5)
      avgidf

if n(si) > n/2
if n(si)     n/2

(3)

where    takes a value between 0.5 and 0.30 and avgidf is the average idf
for all terms. other corrective strategies were also tested, setting    = 0 and using
simpler modi   cations of the classic idf formula.

we also used bm25+, a variation of bm25 that changes the way long docu-

ments are penalized [15].

4.2 evaluation

for testing the proposed variations, we used the database of the 2002 document
understanding conference (duc) [5]. the corpus has 567 documents that are
summarized to 20% of their size, and is the same corpus used in [17].

to evaluate results we used version 1.5.5 of the id8 package [12]. the
con   guration settings were the same as those in duc, where id8-1, id8-
2 and id8-su4 were used as metrics, using a con   dence level of 95% and
applying id30. the    nal result is an average of these three scores.

to check the correct behaviour of our test suite we implemented the refer-
ence method used in [17], which extracts the    rst sentences of each document.
we found the resulting scores of the original algorithm to be identical to those
reported in [17]: a 2.3% improvement over the baseline.

4.3 results

we tested lcs, cosine sim, bm25 and bm25+ as di   erent ways to weight
the edges for the textrank graph. the best results were obtained using bm25
and bm25+ with the corrective formula shown in equation 3. we achieved

table 1. evaluation results for the proposed textrank variations.

method

id8-1 id8-2 id8-su4 improvement

bm25 (   = 0.25)
bm25+ (   = 0.25)
cosine tf-idf
bm25+ (idf = log(n /ni))
bm25 (idf = log(n /ni))
longest common substring
bm25+ (   = 0)
bm25 (   = 0)
textrank
bm25
bm25+
duc baseline

0.4042
0.404
0.4108
0.4022
0.4012
0.402
0.3992
0.3991
0.3983
0.3916
0.3903

0.39

0.1831
0.1818
0.177
0.1805
0.1808
0.1783
0.1803
0.1778
0.1762
0.1725
0.1711
0.1689

0.2018
0.2008
0.1984
0.1997
0.1998
0.1971
0.1976
0.1966
0.1948
0.1906
0.1894
0.186

2.92%
2.60%
2.54%
2.05%
1.97%
1.40%
1.36%
0.89%

   

-1.57%
-2.07%
-2.84%

fig. 1. id8-1 and id8-2 scores comparison.

an improvement of 2.92% above the original textrank result using bm25 and
   = 0.25. the following chart shows the results obtained for the di   erent varia-
tions we proposed.

the result of cosine similarity was also satisfactory with a 2.54% improve-
ment over the original method. the lcs variation also performed better than
the original textrank algorithm with 1.40% total improvement.

the performance in time was also improved. we could process the 567 doc-
uments from the duc2002 database in 84% of the time needed in the original
version.

bm25mod.bm25+mod.costf-idflcstextrankbm25bm25+ducbaseline0.20.30.4scoreid8-1bm25mod.bm25+mod.costf-idflcstextrankbm25bm25+ducbaseline0.10.120.140.160.18scoreid8-25 reference implementation and gensim contribution

a reference implementation of our proposals was coded as a python module3
and can be obtained for testing and to reproduce results.

we also contributed the bm25-textrank algorithm to the gensim project4

[21].

6 conclusions

this work presented three di   erent variations to the textrank algorithm for au-
tomatic summarization. the three alternatives improved signi   cantly the results
of the algorithm using the same test con   guration as in the original publication.
given that textrank performs 2.84% over the baseline, our improvement of
2.92% over the textrank score is an important result.

the combination of textrank with modern information retrieval ranking
functions such as bm25 and bm25+ creates a robust method for automatic
summarization that performs better than the standard techniques used previ-
ously.

based on these results we suggest the use of bm25 along with textrank for
the task of unsupervised id54 of texts. the results obtained
and the examples analyzed show that this variation is better than the original
textrank algorithm without a performance penalty.

references

1. balcerzak, b., jaworski, w., wierzbicki, a.: application of textrank algorithm for
credibility assessment. in: proceedings of the 2014 ieee/wic/acm international
joint conferences on web intelligence (wi) and intelligent agent technologies
(iat) - volume 01. pp. 451   454. wi-iat    14, ieee computer society, washington,
dc, usa (2014), http://dx.doi.org/10.1109/wi-iat.2014.70

2. barzilay, r., mckeown, k.: sentence fusion for multidocument news summariza-
tion. computational linguistics 31(3), 297   328 (2005), http://dblp.uni-trier.
de/db/journals/coling/coling31.html#barzilaym05

3. christopher d. manning, prabhakar raghavan, h.s.: introduction to information

retrieval. cambridge university press (2008)

4. das, d., martins, a.f.t.: a survey on automatic text summarization. tech. rep.,

carnegie mellon university, language technologies institute (2007)

5. document understanding conference: duc 2002 guidelines (july 2002), http://

www-nlpir.nist.gov/projects/duc/guidelines/2002.html

6. ercan, g., cicekli, i.: using lexical chains for keyword extraction. inf. process.
manage. 43(6), 1705   1714 (nov 2007), http://dx.doi.org/10.1016/j.ipm.2007.
01.015

3 source code available at: https://github.com/summanlp/textrank
4 source code available at: https://github.com/summanlp/gensim

7. erkan, g., radev, d.r.: lexrank: graph-based lexical centrality as salience in text
summarization. j. artif. intell. res. (jair) 22, 457   479 (2004), http://dblp.
uni-trier.de/db/journals/jair/jair22.html#erkanr04

8. garg, n., favre, b., riedhammer, k., hakkani-t  ur, d.: clusterrank: a graph based
method for meeting summarization. in: interspeech 2009, 10th annual con-
ference of the international speech communication association, brighton, united
kingdom, september 6-10, 2009. pp. 1499   1502 (2009), http://www.isca-speech.
org/archive/interspeech_2009/i09_1499.html

9. gus   eld, d.: algorithms on strings, trees, and sequences: computer science and
computational biology. cambridge university press, new york, ny, usa (1997)
10. herings, p.j.j., van der laan, g., talman, d.: measuring the power of nodes in
digraphs. research memorandum 007, maastricht university, maastricht research
school of economics of technology and organization (meteor) (2001), http:
//econpapers.repec.org/repec:unm:umamet:2001007

11. kleinberg, j.m.: authoritative sources in a hyperlinked environment. j. acm

46(5), 604   632 (sep 1999), http://doi.acm.org/10.1145/324133.324140

12. lin, c.y.: id8: a package for automatic evaluation of summaries. in: proceedings
of the workshop on text summarization branches out (was 2004), barcelona,
spain (2004)

13. lin, c.y., hovy, e.h.: identifying topics by position. in: proceedings of 5th con-
ference on applied natural language processing. washington d.c. (march 1997)
14. luhn, h.p.: the automatic creation of literature abstracts. ibm j. res. dev. 2(2),

159   165 (apr 1958), http://dx.doi.org/10.1147/rd.22.0159

15. lv, y., zhai, c.: lower-bounding term frequency id172. in: proceedings
of the 20th acm conference on information and knowledge management, cikm
2011, glasgow, united kingdom, october 24-28, 2011. pp. 7   16 (2011)

16. mihalcea, r.: graph-based ranking algorithms for sentence extraction, applied to
text summarization. in: proceedings of the acl 2004 on interactive poster and
demonstration sessions. acldemo    04, association for computational linguistics,
stroudsburg, pa, usa (2004), http://dx.doi.org/10.3115/1219044.1219064

17. mihalcea, r., tarau, p.: textrank: bringing order into texts. in: lin, d., wu, d.
(eds.) proceedings of emnlp 2004. pp. 404   411. association for computational
linguistics, barcelona, spain (july 2004)

18. mitrat, m., singhal, a., buckleytt, c.: automatic text summarization by para-
graph extraction. in: intelligent scalable text summarization. pp. 39   46 (1997),
http://www.aclweb.org/anthology/w97-0707

19. ouyang, y., li, w., wei, f., lu, q.: learning similarity functions in graph-
based document summarization. in: li, w., aliod, d.m. (eds.) iccpol. lec-
ture notes in computer science, vol. 5459, pp. 189   200. springer (2009), http:
//dblp.uni-trier.de/db/conf/iccpol/iccpol2009.html#ouyanglwl09

20. page, l., brin, s., motwani, r., winograd, t.: the id95 citation ranking:
bringing order to the web. in: proceedings of the 7th international world wide
web conference. pp. 161   172. brisbane, australia (1998), citeseer.nj.nec.com/
page98id95.html

21.   reh  u  rek, r., sojka, p.: software framework for topic modelling with large cor-
pora. in: proceedings of the lrec 2010 workshop on new challenges for nlp
frameworks. pp. 45   50. elra, valletta, malta (may 2010), http://is.muni.cz/
publication/884893/en

22. robertson, s., walker, s., jones, s., hancock-beaulieu, m., gatford, m.: okapi at
trec-3. in: proceedings of the third text retrieval conference, trec 1994,

gaithersburg, maryland, usa, november 2-4, 1994. pp. 109   126 (1994), http:
//trec.nist.gov/pubs/trec3/papers/city.ps.gz

23. salton, g., singhal, a., mitra, m., buckley, c.: automatic text structuring and
summarization. information processing and management 33(2), 193     207 (1997)
24. singhal, a.: modern information retrieval: a brief overview. bulletin of the ieee
computer society technical committee on data engineering 24(4), 35   43 (2001),
http://singhal.info/ieee2001.pdf

