neural summarization by extracting sentences and words

jianpeng cheng

mirella lapata

ilcc, school of informatics, university of edinburgh

10 crichton street, edinburgh eh8 9ab

6
1
0
2

 
l
u
j
 

1

 
 
]
l
c
.
s
c
[
 
 

3
v
2
5
2
7
0

.

3
0
6
1
:
v
i
x
r
a

jianpeng.cheng@ed.ac.uk

mlap@inf.ed.ac.uk

abstract

to

approaches

traditional
extractive
summarization rely heavily on human-
engineered features.
in this work we
propose a data-driven approach based on
neural networks and continuous sentence
features. we develop a general frame-
work for single-document summarization
composed of a hierarchical document
encoder and an attention-based extractor.
this architecture allows us to develop
different classes of summarization models
which can extract sentences or words. we
train our models on large scale corpora
containing hundreds of
thousands of
document-summary pairs1. experimental
results on two summarization datasets
demonstrate that our models obtain results
comparable to the state of the art without
any access to linguistic annotation.

1

introduction

the need to access and digest large amounts of
textual data has provided strong impetus to de-
velop id54 systems aiming to
create shorter versions of one or more documents,
whilst preserving their information content. much
effort in id54 has been de-
voted to sentence extraction, where a summary is
created by identifying and subsequently concate-
nating the most salient text units in a document.

most extractive methods

to date identify
sentences based on human-engineered features.
these include surface features such as sentence
position and length (radev et al., 2004), the words
in the title, the presence of proper nouns, content
features such as word frequency (nenkova et al.,
2006), and event features such as action nouns (fi-
latova and hatzivassiloglou, 2004). sentences are

1resources are available for download at http://

homepages.inf.ed.ac.uk/s1537177/resources.html

typically assigned a score indicating the strength
of presence of these features. several methods
have been used in order to select the summary sen-
tences ranging from binary classi   ers (kupiec et
al., 1995), to id48 (conroy and
o   leary, 2001), graph-based algorithms (erkan
and radev, 2004; mihalcea, 2005), and integer lin-
ear programming (woodsend and lapata, 2010).
in this work we propose a data-driven approach
to summarization based on neural networks and
continuous sentence features. there has been a
surge of interest recently in repurposing sequence
transduction neural network architectures for nlp
tasks such as machine translation (sutskever et
al., 2014), id53 (hermann et al.,
2015), and sentence compression (rush et al.,
2015). central to these approaches is an encoder-
decoder architecture modeled by recurrent neu-
ral networks. the encoder reads the source se-
quence into a list of continuous-space representa-
tions from which the decoder generates the target
sequence. an attention mechanism (bahdanau et
al., 2015) is often used to locate the region of focus
during decoding.

we develop a general framework for single-
document summarization which can be used to
extract sentences or words. our model includes
a neural network-based hierarchical document
reader or encoder and an attention-based content
extractor. the role of the reader is to derive the
meaning representation of a document based on its
sentences and their constituent words. our models
adopt a variant of neural attention to extract sen-
tences or words. contrary to previous work where
attention is an intermediate step used to blend hid-
den units of an encoder to a vector propagating ad-
ditional information to the decoder, our model ap-
plies attention directly to select sentences or words
of the input document as the output summary.
similar neural attention architectures have been
previously used for geometry reasoning (vinyals
et al., 2015), under the name id193.

one stumbling block to applying neural net-
work models to extractive summarization is the
lack of training data, i.e., documents with sen-
tences (and words) labeled as summary-worthy.
inspired by previous work on summarization
(woodsend and lapata, 2010; svore et al., 2007)
and reading comprehension (hermann et al.,
2015) we retrieve hundreds of thousands of news
articles and corresponding highlights from the
dailymail website. highlights usually appear as
bullet points giving a brief overview of the infor-
mation contained in the article (see figure 1 for
an example). using a number of transformation
and scoring algorithms, we are able to match high-
lights to document content and construct two large
scale training datasets, one for sentence extraction
and the other for word extraction. previous ap-
proaches have used small scale training data in the
range of a few hundred examples.

our work touches on several strands of research
within summarization and neural sequence model-
ing. the idea of creating a summary by extracting
words from the source document was pioneered in
banko et al. (2000) who view summarization as a
problem analogous to statistical machine transla-
tion and generate headlines using statistical mod-
els for selecting and ordering the summary words.
our word-based model is similar in spirit, how-
ever, it operates over continuous representations,
produces multi-sentence output, and jointly se-
lects summary words and organizes them into sen-
tences. a few recent studies (kobayashi et al.,
2015; yogatama et al., 2015) perform sentence ex-
traction based on pre-trained sentence embeddings
following an unsupervised optimization paradigm.
our work also uses continuous representations to
express the meaning of sentences and documents,
but importantly employs neural networks more di-
rectly to perform the actual summarization task.

rush et al. (2015) propose a neural attention
model for abstractive sentence compression which
is trained on pairs of headlines and    rst sentences
in an article. in contrast, our model summarizes
documents rather than individual sentences, pro-
ducing multi-sentential discourse. a major archi-
tectural difference is that our decoder selects out-
put symbols from the document of interest rather
than the entire vocabulary. this effectively helps
us sidestep the dif   culty of searching for the next
output symbol under a large vocabulary, with low-
frequency words and named entities whose rep-

resentations can be challenging to learn. gu et
al. (2016) and gulcehre et al. (2016) propose a
similar    copy    mechanism in sentence compres-
sion and other tasks; their model can accommo-
date both generation and extraction by selecting
which sub-sequences in the input sequence to copy
in the output.

we evaluate our models both automatically (in
terms of id8) and by humans on two datasets:
the benchmark duc 2002 document summariza-
tion corpus and our own dailymail news high-
lights corpus. experimental results show that
our summarizers achieve performance compara-
ble to state-of-the-art systems employing hand-
engineered features and sophisticated linguistic
constraints.

2 problem formulation
in this section we formally de   ne the summariza-
tion tasks considered in this paper. given a doc-
ument d consisting of a sequence of sentences
{s1,       ,sm} and a word set {w1,       ,wn}, we are
interested in obtaining summaries at two levels of
granularity, namely sentences and words.

sentence extraction aims to create a sum-
mary from d by selecting a subset of j sentences
(where j < m). we do this by scoring each sen-
tence within d and predicting a label yl     {0,1}
indicating whether the sentence should be in-
cluded in the summary. as we apply supervised
training, the objective is to maximize the likeli-
hood of all sentence labels yl = (y1
l ) given
the input document d and model parameters   :

l,       ,ym

log p(yl|d;  ) =

m

   

i=1

log p(yi

l|d;  )

(1)

although extractive methods yield naturally
grammatical summaries and require relatively
little linguistic analysis,
the selected sentences
make for long summaries containing much redun-
dant information. for this reason, we also de-
velop a model based on word extraction which
seeks to    nd a subset of words2 in d and
their optimal ordering so as to form a summary
i     d. compared to sentence
ys = (w(cid:48)
extraction which is a sequence labeling problem,
this task occupies the middle ground between
full abstractive summarization which can exhibit
a wide range of rewrite operations and extractive
2the vocabulary can also be extended to include a small

1,       ,w(cid:48)

k),w(cid:48)

set of commonly-used (high-frequency) words.

:::::

::::::::

limit::::::::

telling::a :::::

license, ::::::

driving::::::::

crows:::::::::

court:::he ::::was ::::::::
speeding

distracted:::by:::his::::sick::::cat.

because ::he::::was:::::::::

defender ::::::daniel:::::talia::::has ::::kept:::his:::::::

afl star blames vomiting cat for speeding
adelaide::::::
36km::::over::::the ::::
the 22-year-old afl star, who drove 96km/h in a 60km/h road works zone on the south eastern
expressway in february, said he didn   t see the reduced speed sign because he was so distracted by his
cat vomiting violently in the back seat of his car.
::in ::::the :::::::::
exceeding :::the::::::
he lost four demerit points, instead of seven, because of his signi   cant training commitments.
    adelaide crows defender daniel talia admits to speeding but says he didn   t see road signs be-
cause his cat was vomiting in his car.
    22-year-old talia was    ned $824 and four demerit points, instead of seven, because of his    signif-
icant    training commitments.

wednesday,::::::::::
limit:::by:::::more::::than::::::::
30km/h.

harrap::::::   ned :::::talia::::::

magistrate:::::bob:::::::

adelaide :::::::::::

court:::on::::::::::::

magistrates :::::

speed ::::

$824 :::for

:::::::::

figure 1: dailymail news article with highlights. underlined sentences bear label 1, and 0 otherwise.

summarization which exhibits none. we formu-
late word extraction as a language generation task
with an output vocabulary restricted to the original
document. in our supervised setting, the training
goal is to maximize the likelihood of the generated
sentences, which can be further decomposed by
enforcing conditional dependencies among their
constituent words:
k
log p(ys|d;  )=

log p(w(cid:48)
   

1,      ,w(cid:48)

i|d,w(cid:48)

i   1;  )

(2)

i=1

in the following section, we discuss the data elici-
tation methods which allow us to train neural net-
works based on the above de   ned objectives.

3 training data for summarization
data-driven neural summarization models require
a large training corpus of documents with labels
indicating which sentences (or words) should be
in the summary. until now such corpora have
been limited to hundreds of examples (e.g., the
duc 2002 single document summarization cor-
pus) and thus used mostly for testing (woodsend
and lapata, 2010). to overcome the paucity of
annotated data for training, we adopt a methodol-
ogy similar to hermann et al. (2015) and create
two large-scale datasets, one for sentence extrac-
tion and another one for word extraction.

in a nutshell, we retrieved3 hundreds of thou-
sands of news articles and their corresponding
highlights from dailymail (see figure 1 for an ex-
ample). the highlights (created by news editors)
3the script for constructing our datasets is modi   ed from

the one released in hermann et al. (2015).

are genuinely abstractive summaries and therefore
not readily suited to supervised training. to cre-
ate the training data for sentence extraction, we
reverse approximated the gold standard label of
each document sentence given the summary based
on their semantic correspondence (woodsend and
lapata, 2010). speci   cally, we designed a rule-
based system that determines whether a document
sentence matches a highlight and should be la-
beled with 1 (must be in the summary), and 0 oth-
erwise. the rules take into account the position
of the sentence in the document, the unigram and
bigram overlap between document sentences and
highlights, the number of entities appearing in the
highlight and in the document sentence. we ad-
justed the weights of the rules on 9,000 documents
with manual sentence labels created by woodsend
and lapata (2010). the method obtained an accu-
racy of 85% when evaluated on a held-out set of
216 documents coming from the same dataset and
was subsequently used to label 200k documents.
approximately 30% of the sentences in each doc-
ument were deemed summary-worthy.

for the creation of the word extraction dataset,
we examine the lexical overlap between the high-
lights and the news article. in cases where all high-
light words (after id30) come from the orig-
inal document, the document-highlight pair con-
stitutes a valid training example and is added to
the word extraction dataset. for out-of-vocabulary
(oov) words, we try to    nd a semantically equiv-
alent replacement present
in the news article.
speci   cally, we check if a neighbor, represented

by pre-trained4 embeddings, is in the original doc-
ument and therefore constitutes a valid substitu-
tion. if we cannot    nd any substitutes, we discard
the document-highlight pair. following this pro-
cedure, we obtained a word extraction dataset con-
taining 170k articles, again from the dailymail.

4 neural summarization model
the key components of our summarization model
include a neural network-based hierarchical doc-
ument reader and an attention-based hierarchical
content extractor. the hierarchical nature of our
model re   ects the intuition that documents are
generated compositionally from words, sentences,
paragraphs, or even larger units. we therefore em-
ploy a representation framework which re   ects the
same architecture, with global information being
discovered and local information being preserved.
such a representation yields minimum informa-
tion loss and is    exible allowing us to apply neural
attention for selecting salient sentences and words
within a larger context. in the following, we    rst
describe the document reader, and then present the
details of our sentence and word extractors.

4.1 document reader
the role of the reader is to derive the meaning rep-
resentation of the document from its constituent
sentences, each of which is treated as a sequence
of words. we    rst obtain representation vectors
at the sentence level using a single-layer convo-
lutional neural network (id98) with a max-over-
time pooling operation (kalchbrenner and blun-
som, 2013; zhang and lapata, 2014; kim et al.,
2016). next, we build representations for docu-
ments using a standard recurrent neural network
(id56) that recursively composes sentences. the
id98 operates at the word level, leading to the
acquisition of sentence-level representations that
are then used as inputs to the id56 that acquires
document-level representations, in a hierarchical
fashion. we describe these two sub-components
of the text reader below.
convolutional sentence encoder we opted for
a convolutional neural network model for repre-
senting sentences for two reasons. firstly, single-
layer id98s can be trained effectively (without
any long-term dependencies in the model) and
secondly, they have been successfully used for
4we used the python gensim library and the

300-dimensional googlenews vectors.

sentence-level classi   cation tasks such as senti-
ment analysis (kim, 2014). let d denote the
dimension of id27s, and s a docu-
ment sentence consisting of a sequence of n words
(w1,       ,wn) which can be represented by a dense
column matrix w     rn  d. we apply a tempo-
ral narrow convolution between w and a kernel
k     rc  d of width c as follows:

j = tanh(w j: j+c   1     k + b)
fi

(3)
where     equates to the hadamard product fol-
lowed by a sum over all elements. fi
j denotes the
j-th element of the i-th feature map fi and b is the
bias. we perform max pooling over time to obtain
a single feature (the ith feature) representing the
sentence under the kernel k with width c:

si,k = max

j

fi
j

(4)

in practice, we use multiple feature maps to
compute a list of features that match the dimen-
sionality of a sentence under each kernel width. in
addition, we apply multiple kernels with different
widths to obtain a set of different sentence vectors.
finally, we sum these sentence vectors to obtain
the    nal sentence representation. the id98 model
is schematically illustrated in figure 2 (bottom).
in the example, the sentence embeddings have six
dimensions, so six feature maps are used under
each kernel width. the blue feature maps have
width two and the red feature maps have width
three. the sentence embeddings obtained under
each kernel width are summed to get the    nal sen-
tence representation (denoted by green).
recurrent document encoder at the docu-
ment level, a recurrent neural network composes a
sequence of sentence vectors into a document vec-
tor. note that this is a somewhat simplistic attempt
at capturing document organization at the level of
sentence to sentence transitions. one might view
the hidden states of the recurrent neural network
as a list of partial representations with each fo-
cusing mostly on the corresponding input sentence
given the previous context. these representations
altogether constitute the id194,
which captures local and global sentential infor-
mation with minimum compression.

the id56 we used has a long short-term
memory (lstm) activation unit for ameliorat-
ing the vanishing gradient problem when train-
ing long sequences (hochreiter and schmidhuber,

figure 3: neural attention mechanism for word
extraction.

with both the encoded document and the previ-
ously labeled sentences in mind. given encoder
hidden states (h1,       ,hm) and extractor hidden
states (  h1,       ,   hm) at time step t, the decoder at-
tends the t-th sentence by relating its current de-
coding state to the corresponding encoding state:

  ht = lstm(pt   1st   1,   ht   1)

(8)

p(yl(t) = 1|d) =   (mlp(  ht : ht))

(9)
where mlp is a multi-layer neural network with as
input the concatenation of   ht and ht. pt   1 repre-
sents the degree to which the extractor believes the
previous sentence should be extracted and memo-
rized (pt   1=1 if the system is certain; 0 otherwise).
in practice, there is a discrepancy between train-
ing and testing such a model. during training
we know the true label pt   1 of the previous sen-
tence, whereas at test time pt   1 is unknown and
has to be predicted by the model. the discrep-
ancy can lead to quickly accumulating prediction
errors, especially when mistakes are made early in
the sequence labeling process. to mitigate this,
we adopt a curriculum learning strategy (bengio
et al., 2015): at the beginning of training when
pt   1 cannot be predicted accurately, we set it to
the true label of the previous sentence; as training
goes on, we gradually shift its value to the pre-
dicted label p(yl(t     1) = 1|d).
4.3 word extractor
compared to sentence extraction which is a purely
sequence labeling task, word extraction is closer
to a generation task where relevant content must
be selected and then rendered    uently and gram-
matically. a small extension to the structure of
the sequential labeling model makes it suitable
for generation:
instead of predicting a label for
the next sentence at each time step, the model di-
rectly outputs the next word in the summary. the

figure 2: a recurrent convolutional document
reader with a neural sentence extractor.
1997). given a document d = (s1,       ,sm), the
hidden state at time step t, denoted by ht, is up-

dated as:             it

             =

               

            w  

(cid:20)ht   1

st

  
  
tanh

ft
ot
  ct
ct = ft (cid:12) ct   1 + it (cid:12)   ct
ht = ot (cid:12) tanh(ct)

(cid:21)

(5)

(6)

(7)

where w is a learnable weight matrix. next, we
discuss a special attention mechanism for extract-
ing sentences and words given the recurrent docu-
ment encoder just described, starting from the sen-
tence extractor.

4.2 sentence extractor
in the standard neural sequence-to-sequence mod-
eling paradigm (bahdanau et al., 2015), an atten-
tion mechanism is used as an intermediate step
to decide which input region to focus on in order
to generate the next output. in contrast, our sen-
tence extractor applies attention to directly extract
salient sentences after reading them.

the extractor is another recurrent neural net-
work that labels sentences sequentially, taking into
account not only whether they are individually
relevant but also mutually redundant. the com-
plete architecture for the document encoder and
the sentence extractor is shown in figure 2. as
can be seen, the next labeling decision is made

model uses a hierarchical attention architecture:
at time step t, the decoder softly5 attends each
document sentence and subsequently attends each
word in the document and computes the probabil-
ity of the next word to be included in the summary
p(w(cid:48)
t   1) with a softmax classi-
   er:

t = wi|d,w(cid:48)

1,       ,w(cid:48)

  ht = lstm(w(cid:48)

t   1,   ht   1)6

j = zt tanh(we   ht + wrh j),h j     d
at

j = softmax(at
bt
j)

  ht =

m

   

j=1

bt
jh j

(10)

(11)

(12)

(13)

1,       ,w(cid:48)

i = vt tanh(we(cid:48)   ht + wr(cid:48)wi),wi     d
ut
(14)
t = wi|d,w(cid:48)
p(w(cid:48)
t   1) = softmax(ut
i) (15)
in the above equations, wi corresponds to the vec-
tor of the i-th word in the input document, whereas
z, we, wr, v, we(cid:48), and wr(cid:48) are model weights.
the model architecture is shown in figure 3.

the word extractor can be viewed as a con-
ditional language model with a vocabulary con-
straint. in practice, it is not powerful enough to
enforce grammaticality due to the lexical diversity
and sparsity of the document highlights. a pos-
sible enhancement would be to pair the extractor
with a neural language model, which can be pre-
trained on a large amount of unlabeled documents
and then jointly tuned with the extractor during
decoding (gulcehre et al., 2015). a simpler al-
ternative which we adopt is to use id165 features
collected from the document to rerank candidate
summaries obtained via beam decoding. we incor-
porate the features in a log-linear reranker whose
feature weights are optimized with minimum error
rate training (och, 2003).

5 experimental setup

in this section we present our experimental setup
for assessing the performance of our summariza-
tion models. we discuss the datasets used for
5a simpler model would use hard attention to select a sen-
tence    rst and then a few words from it as a summary, but this
would render the system non-differentiable for training. al-
though hard attention can be trained with the reinforce
algorithm (williams, 1992), it requires sampling of discrete
actions and could lead to high variance.

6we empirically found that feeding the previous sentence-
level attention vector as additional input to the lstm would
lead to small performance improvements. this is not shown
in the equation.

training and evaluation, give implementation de-
tails, brie   y introduce comparison models, and ex-
plain how system output was evaluated.
datasets we trained our sentence- and word-
based summarization models on the two datasets
created from dailymail news. each dataset was
split into approximately 90% for training, 5% for
validation, and 5% for testing. we evaluated the
models on the duc-2002 single document sum-
marization task. in total, there are 567 documents
belonging to 59 different clusters of various news
topics. each document is associated with two ver-
sions of 100-word7 manual summaries produced
by human annotators. we also evaluated our mod-
els on 500 articles from the dailymail test set
(with the human authored highlights as goldstan-
dard). we sampled article-highlight pairs so that
the highlights include a minimum of 3 sentences.
the average byte count for each document is 278.
as there is no established evaluation standard for
this task, we also report id8 evaluation on
the entire dailymail test set with varying limits.
please refer to the appendix for more information.
implementation details we trained our mod-
els with adam (kingma and ba, 2014) with ini-
tial learning rate 0.001. the two momentum pa-
rameters were set to 0.99 and 0.999 respectively.
we performed mini-batch training with a batch
size of 20 documents. all input documents were
padded to the same length with an additional mask
variable storing the real length for each document.
the size of word, sentence, and document em-
beddings were set to 150, 300, and 750, respec-
tively. for the convolutional sentence model, we
followed kim et al. (2016)8 and used a list of ker-
nel sizes {1, 2, 3, 4, 5, 6, 7}. for the recurrent doc-
ument model and the sentence extractor, we used
as id173 dropout with id203 0.5 on
the lstm input-to-hidden layers and the scoring
layer. the depth of each lstm module was 1.
all lstm parameters were randomly initialized
over a uniform distribution within [-0.05, 0.05].
the word vectors were initialized with 150 dimen-
sional pre-trained embeddings.9

7according

to

the duc2002

guidelines

http:

//www-nlpir.nist.gov/projects/duc/guidelines/
2002.html, the generated summary should be within 100
words.

8the id98-lstm architecture is publicly available at

https://github.com/yoonkim/lstm-char-id98.

9we used the id97 (mikolov et al., 2013) skip-gram
model with context window size 6, negative sampling size 10

proper nouns pose a problem for embedding-
based approaches, especially when these are rare
or unknown (e.g., at test time). rush et al. (2015)
address this issue by adding a new set of features
and a log-linear model component to their sys-
tem. as our model enjoys the advantage of gener-
ation by extraction, we can force the model to in-
spect the context surrounding an entity and its rel-
ative position in the sentence in order to discover
extractive patterns, placing less emphasis on the
meaning representation of the entity itself. specif-
ically, we perform id39 with
the package provided by hermann et al. (2015)
and maintain a set of randomly initialized entity
embeddings. during training, the index of the en-
tities is permuted to introduce some noise but also
robustness in the data. a similar data augmenta-
tion approach has been used for reading compre-
hension (hermann et al., 2015).

a common problem with extractive methods
based on sentence labeling is that there is no con-
straint on the number of sentences being selected
at test time. we address this by reranking the posi-
tively labeled sentences with the id203 scores
obtained from the softmax layer (rather than the
label itself).
in other words, we are more inter-
ested in is the relative ranking of each sentence
rather than their exact scores. this suggests that
an alternative to training the network would be to
employ a ranking-based objective or a learning to
rank algorithm. however, we leave this to future
work. we use the three sentences with the highest
scores as the summary (also subject to the word or
byte limit of the evaluation protocol).

another issue relates to the word extraction
model which is challenging to batch since each
document possesses a distinct vocabulary. we
sidestep this during training by performing neg-
ative sampling (mikolov et al., 2013) which trims
the vocabulary of different documents to the same
length. at each decoding step the model is trained
to differentiate the true target word from 20 noise
samples. at test time we still loop through the
words in the input document (and a stop-word list)
to decide which word to output next.

system comparisons we compared the output
of our models to various summarization meth-
ods. these included the standard baseline of sim-
ply selecting the    leading    three sentences from

and hierarchical softmax 1. the model was trained on the
google 1-billion word benchmark (chelba et al., 2014).

each document as the summary. we also built
a sentence extraction baseline classi   er using lo-
gistic regression and human engineered features.
the classi   er was trained on the same datasets
as our neural network models with the follow-
ing features: sentence length, sentence position,
number of entities in the sentence, sentence-to-
sentence cohesion, and sentence-to-document rel-
evance. sentence-to-sentence cohesion was com-
puted by calculating for every document sentence
its embedding similarity with every other sentence
in the same document. the feature was the nor-
malized sum of these similarity scores. sentence
embeddings were obtained by averaging the con-
stituent id27s. sentence-to-document
relevance was computed similarly. we calculated
for each sentence its embedding similarity with the
document (represented as bag-of-words), and nor-
malized the score. the id27s used in
this baseline are the same as the pre-trained ones
used for our neural models.

in addition, we included a neural abstractive
summarization baseline. this system has a similar
architecture to our word extraction model except
that it uses an open vocabulary during decoding.
it can also be viewed as a hierarchical document-
level extension of the abstractive sentence summa-
rizer proposed by rush et al. (2015). we trained
this model with negative sampling to avoid the ex-
cessive computation of the id172 constant.
finally, we compared our models to three previ-
ously published systems which have shown com-
petitive performance on the duc2002 single doc-
ument summarization task. the    rst approach is
the phrase-based extraction model of woodsend
and lapata (2010). their system learns to produce
highlights from parsed input (phrase structure
trees and dependency graphs); it selects salient
phrases and recombines them subject to length,
coverage, and grammar constraints enforced via
integer id135 (ilp). like ours, this
model is trained on document-highlight pairs, and
produces telegraphic-style bullet points rather than
full-blown summaries. the other two systems,
tgraph (parveen et al., 2015) and urank (wan,
2010), produce more typical summaries and repre-
sent the state of the art. tgraph is a graph-based
sentence extraction model, where the graph is con-
structed from topic models and the optimization
is performed by constrained ilp. urank adopts a
uni   ed ranking system for both single- and multi-

duc 2002 id8-1 id8-2 id8-l
lead
lreg
ilp
nn-abs
tgraph
urank
nn-se
nn-we

40.2
40.3
42.8
13.8
   
   
43.5
22.8

21.0
20.7
21.3
5.2
24.3
21.5
23.0
7.9

43.6
43.8
45.4
15.8
48.1
48.5
47.4
27.0

dailymail
lead
lreg
nn-abs
nn-se
nn-we

id8-1 id8-2 id8-l

20.4
18.5
7.8
21.2
15.7

7.7
6.9
1.7
8.3
6.4

11.4
10.2
7.1
12.0
9.8

table 1: id8 evaluation (%) on the duc-
2002 and 500 dailymail samples.

document summarization.

evaluation we evaluated the quality of the
summaries automatically using id8 (lin and
hovy, 2003). we report unigram and bigram over-
lap (id8-1,2) as a means of assessing infor-
mativeness and the longest common subsequence
(id8-l) as a means of assessing    uency.

in addition, we evaluated the generated sum-
maries by eliciting human judgments for 20 ran-
domly sampled duc 2002 test documents. par-
ticipants were presented with a news article and
summaries generated by a list of systems. these
include two neural network systems (sentence-
and word-based extraction), the neural abstrac-
tive system described earlier, the lead baseline, the
phrase-based ilp model10 of woodsend and la-
pata (2010), and the human authored summary.
subjects were asked to rank the summaries from
best to worst (with ties allowed) in order of in-
formativeness (does the summary capture impor-
tant information in the article?) and    uency (is the
summary written in well-formed english?). we
elicited human judgments using amazon   s me-
chanical turk id104 platform. partici-
pants (self-reported native english speakers) saw
2 random articles per session. we collected 5 re-
sponses per document.

10we are grateful to kristian woodsend for giving us ac-
cess to the output of his system. unfortunately, we do not
have access to the output of tgraph or urank for inclusion
in the human evaluation.

models
lead
ilp
nn-se
nn-we
nn-abs
human

5th

4th

3rd

2nd

1st
0.10 0.17 0.37 0.15 0.16 0.05
0.19 0.38 0.13 0.13 0.11 0.06
0.22 0.28 0.21 0.14 0.12 0.03
0.00 0.04 0.03 0.21 0.51 0.20
0.00 0.01 0.05 0.16 0.23 0.54
0.27 0.23 0.29 0.17 0.03 0.01

6th meanr
3.27
2.77
2.74
4.79
5.24
2.51

table 2: rankings (shown as proportions) and
mean ranks given to systems by human partici-
pants (lower is better).

6 results
table 1 (upper half) summarizes our results on
the duc 2002 test dataset using id8. nn-se
represents our neural sentence extraction model,
nn-we our word extraction model, and nn-abs
the neural abstractive baseline. the table also in-
cludes results for the lead baseline, the logistic
regression classi   er (lreg), and three previously
published systems (ilp, tgraph, and urank).

the nn-se outperforms the lead and lreg
baselines with a signi   cant margin, while per-
forming slightly better than the ilp model. this
is an encouraging result since our model has
only access to embedding features obtained from
raw text.
in comparison, lreg uses a set of
manually selected features, while the ilp system
takes advantage of syntactic information and ex-
tracts summaries subject to well-engineered lin-
guistic constraints, which are not available to our
models. overall, our sentence extraction model
achieves performance comparable to the state of
the art without sophisticated constraint optimiza-
tion (ilp, tgraph) or sentence ranking mech-
anisms (urank). we visualize the sentence
weights of the nn-se model in the top half of fig-
ure 4. as can be seen, the model is able to locate
text portions which contribute most to the overall
meaning of the document.

id8 scores for the word extraction model
are less promising. this is somewhat expected
given that id8 is id165 based and not very
well suited to measuring summaries which contain
a signi   cant amount of id141 and may de-
viate from the reference even though they express
similar meaning. however, a meaningful com-
parison can be carried out between nn-we and
nn-abs which are similar in spirit. we observe
that nn-we consistently outperforms the purely
abstractive model. as nn-we generates sum-
maries by picking words from the original docu-
ment, decoding is easier for this model compared

sentence extraction:
a gang of at least three people poured gasoline on a car that stopped to    ll up at entity5 gas station early on saturday morning and set the vehicle on    re
a gang of at least three people poured gasoline on a car that stopped to    ll up at entity5 gas station early on saturday morning and set the vehicle on    re
the driver of the car, who has not been identi   ed, said he got into an argument with the suspects while he was pumping gas at a entity13 in entity14
the driver of the car, who has not been identi   ed, said he got into an argument with the suspects while he was pumping gas at a entity13 in entity14
the group covered his white entity16 in gasoline and lit it ablaze while there were two passengers inside
the group covered his white entity16 in gasoline and lit it ablaze while there were two passengers inside
at least three people poured gasoline on a car and lit it on    re at a entity14 gas station explosive situation
the passengers and the driver were not hurt during the incident but the car was completely ruined
the man   s grandmother said the    re was lit after the suspects attempted to carjack her grandson, entity33 reported
the man   s grandmother said the    re was lit after the suspects attempted to carjack her grandson, entity33 reported
she said:    he said he was pumping gas and some guys came up and asked for the car
    they pulled out a gun and he took off running
    they took the gas tank and started spraying
    no one was injured during the    re , but the car    s entire front end was torched , according to entity52
the entity53 is investigating the incident as an arson and the suspects remain at large
the entity53 is investigating the incident as an arson and the suspects remain at large
surveillance video of the incident is being used in the investigation
before the    re , which occurred at 12:15am on saturday , the suspects tried to carjack the man hot case
the entity53 is investigating the incident at the entity67 station as an arson
word extraction:
gang poured gasoline in the car, entity5 saturday morning. the driver argued with the suspects. his grandmother said the    re was lit by the suspects attempted to
carjack her grandson.
entities:
entity5:california entity13:76-station entity14: south la entity16:dodge charger entity33:abc entity52:nbc entity53:lacfd entity67:la76

figure 4: visualization of the summaries for a dailymail article. the top half shows the relative attention
weights given by the sentence extraction model. darkness indicates sentence importance. the lower half
shows the summary generated by the word extraction.

to nn-abs which deals with an open vocabulary.
the extraction-based generation approach is more
robust for proper nouns and rare words, which
pose a serious problem to open vocabulary mod-
els. an example of the generated summaries for
nn-we is shown at the lower half of figure 4.

table 1 (lower half) shows system results on
the 500 dailymail news articles (test set). in gen-
eral, we observe similar trends to duc 2002, with
nn-se performing the best in terms of all id8
metrics. note that scores here are generally lower
compared to duc 2002. this is due to the fact
that the gold standard summaries (aka highlights)
tend to be more laconic and as a result involve a
substantial amount of id141. more exper-
imental results on this dataset are provided in the
appendix.

the results of our human evaluation study are
shown in table 2. speci   cally, we show, propor-
tionally, how often our participants ranked each
system 1st, 2nd, and so on. perhaps unsurpris-
ingly, the human-written descriptions were con-
sidered best and ranked 1st 27% of the time, how-
ever closely followed by our nn-se model which
was ranked 1st 22% of the time. the ilp system
was mostly ranked in 2nd place (38% of the time).
the rest of the systems occupied lower ranks. we
further converted the ranks to ratings on a scale of
1 to 6 (assigning ratings 6. . .1 to rank placements
1. . .6). this allowed us to perform analysis of
variance (anova) which revealed a reliable ef-
fect of system type. speci   cally, post-hoc tukey
tests showed that nn-se and ilp are signi   cantly
(p < 0.01) better than lead, nn-we, and nn-abs

but do not differ signi   cantly from each other or
the human goldstandard.

7 conclusions

in this work we presented a data-driven summa-
rization framework based on an encoder-extractor
architecture. we developed two classes of mod-
els based on sentence and word extraction. our
models can be trained on large scale datasets and
learn informativeness features based on continu-
ous representations without recourse to linguistic
annotations. two important ideas behind our work
are the creation of hierarchical neural structures
that re   ect the nature of the summarization task
and generation by extraction. the later effectively
enables us to sidestep the dif   culties of generat-
ing under a large vocabulary, essentially covering
the entire dataset, with many low-frequency words
and named entities.

directions for future work are many and var-
ied. one way to improve the word-based model
would be to take structural information into ac-
count during generation, e.g., by combining it with
a tree-based algorithm (cohn and lapata, 2009). it
would also be interesting to apply the neural mod-
els presented here in a phrase-based setting similar
to lebret et al. (2015). a third direction would be
to adopt an information theoretic perspective and
devise a purely unsupervised approach that selects
summary sentences and words so as to minimize
information loss, a task possibly achievable with
the dataset created in this work.

dm 75b
lead
nn-se
nn-we

id8-1 id8-2 id8-l

21.9
22.7
16.0

7.2
8.5
6.4

11.6
12.5
10.2

[chelba et al.2014] ciprian chelba, tomas mikolov,
mike schuster, qi ge, thorsten brants, phillipp
koehn, and tony robinson. 2014. one billion word
benchmark for measuring progress in statistical lan-
guage modeling. arxiv preprint arxiv:1312.3005.

dm 275b id8-1 id8-2 id8-l
lead
nn-se
nn-we

40.5
42.2
33.9

14.9
17.3
10.2

32.6
34.8
23.5

dm full
lead
nn-se
nn-we

id8-1 id8-2 id8-l

53.5
56.0

-

21.7
24.9

-

48.5
50.2

-

table 3: id8 evaluation (%) on the entire 500
dailymail samples, with different length limits.

acknowledgments

we would like to thank three anonymous review-
ers and members of the ilcc at the school of in-
formatics for their valuable feedback. the support
of the european research council under award
number 681760    translating multiple modalities
into text    is gratefully acknowledged.

8 appendix

in addition to the duc 2002 and 500 dailymail
samples, we additionally report results on the en-
tire dailymail test set (table 3). since there is no
established evaluation standard for this task, we
experimented with three different id8 limits:
75 bytes, 275 bytes and full length.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in proceedings of iclr 2015, san diego, califor-
nia.

[banko et al.2000] michele banko, vibhu o. mittal,
and michael j. witbrock. 2000. headline genera-
tion based on statistical translation. in proceedings
of the 38th acl, pages 318   325, hong kong.

[cohn and lapata2009] trevor anthony cohn and
mirella lapata. 2009. sentence compression as tree
transduction. journal of arti   cial intelligence re-
search, pages 637   674.

[conroy and o   leary2001] conroy

o   leary.
text summarization via hidden markov
in proceedings of the 34th annual acl

2001.
models.
sigir, pages 406   407, new oleans, louisiana.

and

[erkan and radev2004] g  unes   erkan and dragomir r.
radev.
2004. lexid95: prestige in multi-
document text summarization. in proceedings of the
2004 emnlp, pages 365   371, barcelona, spain.

[filatova and hatzivassiloglou2004] elena filatova and
2004.
event-based
vasileios hatzivassiloglou.
extractive summarization.
in stan szpakowicz
marie-francine moens, editor, text summarization
branches out: proceedings of the acl-04 work-
shop, pages 104   111, barcelona, spain.

[gu et al.2016] jiatao gu, zhengdong lu, hang li, and
victor o.k. li. 2016. incorporating copying mech-
in pro-
anism in sequence-to-sequence learning.
ceedings of the 54th acl, berlin, germany.
to ap-
pear.

[gulcehre et al.2015] caglar gulcehre, orhan firat,
kelvin xu, kyunghyun cho, loic barrault, huei-
chi lin, fethi bougares, holger schwenk, and
yoshua bengio. 2015. on using monolingual cor-
pora in id4. arxiv preprint
arxiv:1503.03535.

[gulcehre et al.2016] caglar gulcehre, sungjin ahn,
ramesh nallapati, bowen zhou, and yoshua ben-
in pro-
gio. 2016. pointing the unknown words.
ceedings of the 54th acl, berlin, germany.
to ap-
pear.

[hermann et al.2015] karl moritz hermann, tomas
kocisky, edward grefenstette, lasse espeholt, will
kay, mustafa suleyman, and phil blunsom. 2015.
teaching machines to read and comprehend. in ad-
vances in neural information processing systems
28, pages 1684   1692. curran associates, inc.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

[bengio et al.2015] samy bengio, oriol vinyals,
navdeep jaitly, and noam shazeer. 2015. sched-
uled sampling for sequence prediction with recurrent
neural networks. in advances in neural information
processing systems 28, pages 1171   1179. curran
associates, inc.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. recurrent convolutional
neural networks for discourse compositionality. in
proceedings of the workshop on continuous vector
space models and their compositionality, pages
119   126, so   a, bulgaria.

[radev et al.2004] dragomir radev, timothy allison,
sasha blair-goldensohn, john blitzer, arda celebi,
stanko dimitrov, elliott drabek, ali hakim, wai
lam, danyu liu, et al. 2004. mead-a platform
for multidocument multilingual text summarization.
technical report, columbia university academic
commons.

[rush et al.2015] alexander m. rush, sumit chopra,
and jason weston. 2015. a neural attention model
for abstractive sentence summarization. in proceed-
ings of the 2015 emnlp, pages 379   389, lisbon,
portugal.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc vv le. 2014. sequence to sequence
learning with neural networks. in advances in neu-
ral information processing systems 27, pages 3104   
3112. curran associates, inc.

[svore et al.2007] krysta svore, lucy vanderwende,
and christopher burges. 2007. enhancing single-
document summarization by combining ranknet
and third-party sources. in proceedings of the 2007
emnlp-conll, pages 448   457, prague, czech re-
public.

[vinyals et al.2015] oriol vinyals, meire fortunato,
and navdeep jaitly. 2015. id193.
in
advances in neural information processing systems
28, pages 2674   2682. curran associates, inc.

[wan2010] xiaojun wan. 2010. towards a uni   ed ap-
proach to simultaneous single-document and multi-
in proceedings of the
document summarizations.
23rd coling, pages 1137   1145.

[williams1992] ronald j williams.

simple
statistical gradient-following algorithms for connec-
tionist id23. machine learning,
8(3-4):229   256.

1992.

[woodsend and lapata2010] kristian woodsend and
2010. automatic generation of
in proceedings of the 48th acl,

mirella lapata.
story highlights.
pages 565   574, uppsala, sweden.

[yogatama et al.2015] dani yogatama, fei liu, and
noah a. smith. 2015. extractive summarization by
maximizing semantic volume. in proceedings of the
2015 emnlp, pages 1961   1966, lisbon, portugal.

[zhang and lapata2014] xingxing zhang and mirella
lapata. 2014. chinese poetry generation with re-
in proceedings of 2014
current neural networks.
emnlp, pages 670   680, doha, qatar.

[kim et al.2016] yoon kim, yacine jernite, david son-
tag, and alexander m rush. 2016. character-aware
neural language models. in proceedings of the 30th
aaai, phoenix, arizon. to appear.

[kim2014] yoon kim. 2014. convolutional neural net-
works for sentence classi   cation. in proceedings of
the 2014 emnlp, pages 1746   1751, doha, qatar.

[kingma and ba2014] diederik kingma and jimmy
ba. 2014. adam: a method for stochastic opti-
mization. arxiv preprint arxiv:1412.6980.

[kobayashi et al.2015] hayato kobayashi, masaki
noguchi, and taichi yatsuka. 2015. summarization
in proceedings
based on embedding distributions.
of the 2015 emnlp, pages 1984   1989, lisbon,
portugal.

[kupiec et al.1995] julian kupiec, jan o. pedersen, and
francine chen. 1995. a trainable document sum-
marizer. in proceedings of the 18th annual interna-
tional acm sigir, pages 68   73, seattle, washing-
ton.

[lebret et al.2015] r  emi lebret, pedro o pinheiro, and
ronan collobert. 2015. phrase-based image cap-
in proceedings of the 32nd icml, lille,
tioning.
france.

[lin and hovy2003] chin-yew lin and eduard h.
hovy. 2003. automatic evaluation of summaries
in pro-
using id165 co-occurrence statistics.
ceedings of hlt naacl, pages 71   78, edmonton,
canada.

[mihalcea2005] rada mihalcea. 2005. language inde-
in proceedings
pendent extractive summarization.
of the acl interactive poster and demonstration
sessions, pages 49   52, ann arbor, michigan.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013.
distributed representations of words and phrases
in advances in neu-
and their compositionality.
ral information processing systems 26, pages 3111   
3119. curran associates, inc.

[nenkova et al.2006] ani nenkova, lucy vander-
a
wende, and kathleen mckeown.
compositional context
sensitive multi-document
summarizer: exploring the factors that in   uence
summarization. in proceedings of the 29th annual
acm sigir, pages 573   580, washington, seattle.

2006.

[och2003] franz josef och. 2003. minimum error rate
in pro-
training in id151.
ceedings of the 41st acl, pages 160   167, sapporo,
japan.

[parveen et al.2015] daraksha parveen, hans-martin
ramsl, and michael strube. 2015. topical coher-
ence for graph-based extractive summarization. in
proceedings of the 2015 emnlp, pages 1949   1954,
lisbon, portugal, september.

