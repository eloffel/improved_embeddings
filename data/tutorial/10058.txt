58

san diego, california, june 12-17, 2016. c(cid:13)2016 association for computational linguistics

proceedings of naacl-hlt 2016, pages 58   68,

alow-rankapproximationapproachtolearningjointembeddingsofnewsstoriesandimagesfortimelinesummarizationwilliamyangwang1   ,yasharmehdad3,dragomirr.radev2,amandastent41schoolofcomputerscience,carnegiemellonuniversity,pittsburgh,pa15213,usa2departmentofeecs,universityofmichigan,annarbor,mi48109,usa3yahoo,sunnyvale,ca94089,usaand4newyork,ny10036,usayww@cs.cmu.edu,{ymehdad,stent}@yahoo-inc.com,radev@umich.eduabstractakeychallengefortimelinesummarizationistogenerateaconcise,yetcompletestorylinefromlargecollectionsofnewsstories.pre-viousstudiesinextractivetimelinegenerationarelimitedintwoways:   rst,mostpriorworkfocusesonfully-observablerankingmodelsorid91modelswithhand-designedfeaturesthatmaynotgeneralizewell.second,mostsummarizationcorporaaretext-only,whichmeansthattextisthesolesourceofinfor-mationconsideredintimelinesummarization,andthus,therichvisualcontentfromnewsimagesisignored.tosolvetheseissues,weleveragethesuccessofmatrixfactorizationtechniquesfromrecommendersystems,andcasttheproblemasasentencerecommenda-tiontask,usingarepresentationlearningap-proach.toaugmenttext-onlycorpora,foreachcandidatesentenceinanewsarticle,wetakeadvantageoftop-rankedrelevantimagesfromthewebandmodeltheimageusingaconvolutionalneuralnetworkarchitecture.fi-nally,weproposeascalablelow-rankapprox-imationapproachforlearningjointembed-dingsofnewsstoriesandimages.inexperi-ments,wecompareourmodeltovariouscom-petitivebaselines,anddemonstratethestate-of-the-artperformanceoftheproposedtext-basedandmultimodalapproaches.1introductiontimelinesummarizationisthetaskoforganizingcrucialmilestonesofanewsstoryinatemporalor-der,e.g.(kedzieetal.,2014;linetal.,2012).a   thisworkwasperformedwhenwilliamwanganddragomirradevwerevisitingyahoonyc.timelineexampleforthe2010britishoilspillgen-eratedbyoursystemisshowninfigure1.thetaskischallenging,becausetheinputoftenincludesalargenumberofnewsarticlesasthestoryisde-velopingeachday,butonlyasmallportionofthekeyinformationisneededfortimelinegeneration.inadditiontotheconcisenessrequirement,timelinesummarizationalsohastobecomplete   allkeyin-formation,inwhateverform,mustbepresentedinthe   nalsummary.todistillkeyinsightsfromnewsreports,priorworkinsummarizationoftenreliesonfeatureen-gineering,andusesid91techniques(radevetal.,2004b)toselectimportanteventstobeincludedinthe   nalsummary.whilethisapproachisun-supervised,theprocessoffeatureengineeringisal-waysexpensive,andthenumberofclustersisnoteasytoestimate.topresentacompletesummary,researchersfromthenaturallanguageprocessing(nlp)communityoftensolelyrelyonthetextualinformation,whilestudiesinthecomputervision(cv)communityrelysolelyontheimageandvideoinformation.however,eventhoughnewsimagesareabundantlyavailabletogetherwithnewsstories,ap-proachesthatjointlylearntextualandvisualrepre-sentationsforsummarizationarenotcommon.inthispaper,wetakeamoreradicalapproachtotimelinesummarization.weformulatetheproblemasasentencerecommendationtask   insteadofrec-ommendingitemstousersasinarecommendersys-tem,werecommendimportantsentencestoatime-line.ourapproachdoesnotrequirefeatureengi-neering:byusingamatrixfactorizationframework,weareessentiallyperformingrepresentationlearn-59

figure1:atimelineexampleforthebpoilspillgeneratedbyourproposedmethod.notethatweuseyahoo!imagesearchtoobtainthetop-rankedimageforeachcandidatesentence.ingtomodelthecontinuousrepresentationofsen-tencesandwords.sincemostprevioustimelinesummarizationwork(andtherefore,corpora)onlyfocusesontextualinformation,wealsoprovideanovelweb-basedapproachforharvestingnewsim-ages:wequeryyahoo!imagesearchwithsen-tencesfromnewsarticles,andextractvisualcuesusinga15-layerconvolutionalneuralnetworkar-chitecture.byunifyingtextandimagesinthelow-rankapproximationframework,ourapproachlearnsajointembeddingofnewsstorytextsandimagesinaprincipledmanner.inempiricalevaluations,weconductexperimentsontwopubliclyavailabledatasets,anddemonstratetheef   ciencyandeffec-tivenessofourapproach.bycomparingtovariousbaselines,weshowthatourapproachishighlyscal-ableandachievesstate-of-the-artperformance.ourmaincontributionsarethree-fold:   weproposeanovelmatrixfactorizationap-proachforextractivesummarization,leverag-ingthesuccessofcollaborative   ltering;   weareamongthe   rsttoconsiderrepresenta-tionlearningofajointembeddingfortextandimagesintimelinesummarization;   ourmodelsigni   cantlyoutperformsvariouscompetitivebaselinesontwopubliclyavailabledatasets.60

2relatedworksupervisedlearningiswidelyusedinsummariza-tion.forexample,theseminalstudybykupiecetal.(1995)usedanaivebayesclassi   erforselectingsentences.recently,wangetal.(2015)proposedaregressionmethodthatusesajointlossfunction,combiningnewsarticlesandcomments.addition-ally,unsupervisedtechniquessuchaslanguagemod-eling(allanetal.,2001)havebeenusedfortem-poralsummarization.inrecentyears,rankingandgraph-basedmethods(radevetal.,2004b;erkanandradev,2004;mihalceaandtarau,2004;faderetal.,2007;hassanetal.,2008;meietal.,2010;yanetal.,2011b;yanetal.,2011a;zhaoetal.,2013;ngetal.,2014;zhouetal.,2014;glava  sand  snajder,2014;tranetal.,2015;dehghaniandasadpour,2015)havealsoprovedpopularforex-tractivetimelinesummarization,ofteninanunsu-pervisedsetting.dynamicprogramming(kiernanandterzi,2009)andgreedyalgorithms(althoffetal.,2015)havealsobeenconsideredforconstruct-ingsummariesovertime.ourworkalignswithrecentstudiesonlatentvariablemodelsformulti-documentsummarizationandstorylineid91.conroyetal.(2001)wereamongthe   rsttoconsiderlatentvariablemodels,eventhoughitisdif   culttoincorporatefeaturesandhigh-dimensionallatentstatesinaid48-basedmodel.ahmedetal.(2011)proposedahierarchi-calnonparametricmodelthatintegratesarecurrentchineserestaurantprocesswithlatentdirichletallocationtoclusterwordsovertime.themainissueswiththisapproacharethatitdoesnotgen-eratehuman-readablesentences,andthatscalingnonparametricbayesianmodelsisoftenchalleng-ing.similarly,huangandhuang(2013)introducedajointmixture-event-aspectmodelusingagenera-tivemethod.navarro-coloradoandsaquete(2015)combinedtemporalinformationwithtopicmodel-ing,andobtainedthebestperformanceinthecross-documenteventorderingtaskofsemeval2015.therehasbeenpriorwork(wangetal.,2008;leeetal.,2009)usingmatrixfactorizationtoper-formsentenceid91.akeydistinctionbetweenourworkandthispreviousworkisthatourmethodrequiresnoadditionalsentenceselectionstepsaftersentenceid91,soweavoiderrorcascades.zhuandchen(2007)wereamongthe   rsttoconsidermultimodaltimelinesummarization,buttheyfocusonvisualization,anddonotmakeuseofimages.wangetal.(2012)investigatedmulti-modaltimelinesummarizationbyconsideringco-sinesimilarityamongvariousfeaturevectors,andthenusingagraphbasedalgorithmtoselectsalienttopics.inthecomputervisioncommunity,kimandxing(2014)madeuseofcommunitywebpho-tos,andgeneratestorylinegraphsforimagerecom-mendation.interestingly,kimetal.(2014)com-binedimagesandvideosforstorylinereconstruc-tion.however,noneoftheabovestudiescombinetextualandvisualinformationfortimelinesumma-rization.3ourapproachwenowdescribethetechnicaldetailsofourlow-rankapproximationapproach.first,wemotivateourapproach.next,weexplainhowweformulatethetimelinesummarizationtaskasamatrixfactor-izationproblem.then,weintroduceascalableap-proachforlearninglow-dimensionalembeddingsofnewsstoriesandimages.3.1motivationweformulatetimelinesummarizationasalow-rankmatrixcompletiontaskbecauseofthefollowingconsiderations:   simplicityinthepastdecade,asigni   cantamountofworkonsummarizationhasfocusedondesigningvariouslexical,syntacticandse-manticfeatures.incontrasttopriorwork,wemakeuseoflow-rankapproximationtech-niquestolearnrepresentationsdirectlyfromdata.thisway,ourmodeldoesnotrequirestrongdomainknowledgeorlotsoffeatureen-gineering,anditiseasyfordeveloperstode-ploythesysteminreal-worldapplications.   scalabilityamajorreasonthatrecommendersystemsandcollaborative   lteringtechniqueshavebeenverysuccessfulinindustrialapplica-tionsisthatmatrixcompletiontechniquesarerelativelysophisticated,andareknowntoscaleuptolargerecommendationdatasetswithmorethan100millionratings(bennettandlanning,61

haywood	
   s)ll	
   receives	
   	
   salary	
   as	
   the	
   ceo	
   of	
   bp.	
   some	
   es)mates	
   are	
   around	
   	
   40,000	
   barrels	
   a	
   day.	
   	
                                        	
   dudley	
   replaced	
   haywood	
   	
   as	
   the	
   new	
   ceo	
   of	
   bp.	
   id8	
   0.2	
   0	
   words	
   dudley	
   1	
   haywood	
   1	
   barrels	
   	
   	
                  	
   	
   	
   bp	
   	
   	
   	
   	
   	
   largest	
   1	
   1	
   0.3	
   1	
   events	
   receives(haywood,	
   salary)	
   1	
   time	
                  	
   	
   replaced(dudley,	
   haywood)	
   0.8	
   2010/04	
   	
                  	
   	
   	
   2010/05	
   1	
   1	
   0.7	
   implicit	
   word	
   relanons	
   implicit	
   event	
   relanons	
   impl.	
   time	
   relanons	
   implicit	
   text-     vision	
   relanons	
   the	
   size	
   of	
   the	
   oil	
   spill	
   	
   was	
   one	
   of	
   the	
   largest.	
   0.8	
   1	
   1	
   input	
   images:	
   224	
   x	
   224	
   rgb	
   conv3-     64	
   	
   	
   conv3-     64	
   	
   	
   maxpool	
   	
   	
   conv3-     128	
   	
   	
   conv3-     128	
   	
   	
   maxpool	
   	
   	
   conv3-     256	
   	
   	
   conv3-     256	
   	
   	
   conv3-     256	
   	
   	
   maxpool	
   	
   	
   conv3-     512	
   	
   	
   conv3-     512	
   	
   	
   conv3-     512	
   	
   	
   maxpool	
   	
   	
   conv3-     512	
   	
   	
   conv3-     512	
   	
   	
   conv3-     512	
   	
   	
   maxpool	
   	
   	
   fc-     4096	
   	
   	
   fc-     4096	
   	
   vision	
   2.8	
   2.0	
   1.9	
   2.8	
   0.2	
   implicit	
   vision	
   relanons	
   1	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
                       4096	
   figure2:ourlow-rankapproximationframeworkforlearningjointembeddingofnewsstoriesandimagesfortimelinesummarization.2007).therefore,webelievethatourapproachispracticalforprocessinglargedatasetsinthissummarizationtask.   jointmultimodalmodelingakeychallengeofsupervisedlearningapproachesforsumma-rizationistoselectinformativesentences.inthiswork,wemakeuseofmultimodalitytose-lectimportantsentences.3.2problemformulationsincethenet   ixcompetition(bellandkoren,2007),collaborative   lteringtechniqueswithlatentfactormodelshavehadhugesuccessinrecom-mendersystems.theselatentfactors,oftenintheformoflow-rankembeddings,capturenotonlyex-plicitinformationbutalsoimplicitcontextfromtheinputdata.inthiswork,weproposeanovelmatrixfactorizationframeworkto   recommend   keysen-tencestoatimeline.figure2showsanoverviewoftheframework.morespeci   cally,weformulatethistaskasama-trixcompletionproblem.givenanewscorpus,weassumethattherearemtotalsentences,whicharetherowsinthematrix.the   rstcolumnisthemet-ricsection,whereweuseid8(lin,2004)asthemetrictopre-computeasentenceimportancescorebetweenacandidatesentenceandahuman-generatedsummary.duringtraining,weusethesescorestotunemodelparameters,andduringtesting,wepredictthesentenceimportancescoresgiventhefeaturesinothercolumns.thatis,welearntheem-beddingofimportantsentences.thesecondsetofcolumnsisthetextfeaturesec-tion.inourexperiments,thisincludeswordobser-vations,subject-verb-object(svo)events,andthepublicationdateofthedocumentfromwhichthecandidatesentenceisextracted.inourpreprocess-ingstep,werunthestanfordpart-of-speechtag-ger(toutanovaetal.,2003)andmaltparser(nivreetal.,2006)togeneratesvoeventsbasedonde-pendencyparses.additionalfeaturescaneasilybeincorporatedintothisframework;weleavethecon-siderationofadditionalfeaturesforfuturework.finally,foreachsentence,weuseanimagesearchenginetoretrieveatop-rankedrelevantimage,andthenweuseaconvolutionalneuralnetwork(id98)architecturetoextractvisualfeaturesinanunsu-pervisedfashion.weuseaid98modelfromsi-monyanandzisserman(2015),whichistrainedontheid163challenge2014dataset(russakovskyetal.,2014).inourwork,wekeepthe16convo-lutionallayersandmax-pooloperations.toextract62

neuralnetworkfeatures,weremovethe   nalfully-connected-1000layerandthesoftmaxfunction,re-sultingin4096featuresforeachimage.thetotalnumberofcolumnsintheinputmatrixisn.ourmatrixmnowencodespreferencesforasentence,togetherwithitslexical,event,andtempo-ralattributes,andvisualfeaturesforanimagehighlyrelevanttothesentence.hereweuseitoindexthei-thsentenceandjtoindexthej-thcolumn.wescalethecolumnsbythestandarddeviation.3.3low-rankapproximationfollowingpriorwork(korenetal.,2009),weareinterestedinlearningtwolow-rankmatricesp   rk  mandq   rk  n.theintuititionisthatpistheembeddingofallcandidatesentences,andqistheembeddingoftextualandvisualfeatures,aswellasthesentenceimportancescore,event,andtempo-ralfeatures.herekisthenumberoflatentdimen-sions,andwewouldliketoapproximatem(i,j)   ~pit~qj,where~piisthelatentembeddingvectorforthei-thsentenceand~qjisthelatentembeddingvec-torforthej-thcolumn.weseektoapproximatethematrixmbythesetwolow-rankmatricespandq.wecanthenformulatetheoptimizationproblemforthistask:minp,qx(i,j)   m(m(i,j)   ~pit~qj)2+  p||~pi||2+  q||~qj||2here,  pand  qareid173coef   cientstopreventthemodelfromover   tting.tosolvethisop-timizationproblemef   ciently,apopularapproachisstochasticgradientdescent(sgd)(korenetal.,2009).incontrasttotraditionalmethodsthatrequiretime-consuminggradientcomputation,sgdtakesonlyasmallnumberofrandomsamplestocom-putethegradient.sgdisalsonaturaltoonlineal-gorithmsinreal-timestreamingapplications,whereinsteadofretrainingthemodelwithallthedata,pa-rametersmightbeupdatedincrementallywhennewdatacomesin.oncewehaveselectedarandomsamplem(i,j),wecansimplifytheobjectivefunc-tion:(m(i,j)   ~pit~qj)2+  p(~pit~pi)+  q(~qjt~qj)now,wecancalculatethesub-gradientsofthetwolatentvectors~piand~qjtoderivethefollowingvari-aid7pdaterules:~pi   ~pi+  (   (i,j)qj     p~pi)(1)~qj   ~qj+  (   (i,j)pi     q~qj)(2)here,  isthelearningrate,whereas   (i,j)isthelossfunctionthatestimateshowwellthemodelapproxi-matesthegroundtruth:   (i,j)=m(i,j)   ~pit~qjthelow-rankapproximationhereisaccomplishedbyreconstructingthemmatrixwiththetwolow-rankmatricespandq,andweusetherowandcol-umnregularizerstopreventthemodelfromover   t-tingtothetrainingdata.sgd-basedoptimizationformatrixfactorizationcanalsobeeasilyparallelized.forexample,hog-wild!(rechtetal.,2011)isalock-freeparalleliza-tionapproachforsgd.incontrasttosynchronousapproacheswhereidlethreadshavetowaitforbusythreadstosyncupparameters,hogwild!isanasynchronousmethod:itassumesthatbecausetextfeaturesaresparse,thereisnoneedtoperformsyn-chronizationofthethreads.inreality,althoughthisapproachmightnotworkforspeechorimagere-latedtasks,itperformswellinvarioustextbasedtasks.inthiswork,wefollowarecentlyproposedapproachcalledfastparallelstochasticgradientde-scent(fpsg)(chinetal.,2015),whichispartlyin-spiredbyhogwild!.3.4jointmodelingofmixedeffectsmatrixfactorizationisarelativelycomplexmethodformodelinglatentfactors.so,animportantques-tiontoaskis:inthecontextoftimelinesumma-rization,whatisthismatrixfactorizationframeworkmodeling?fromequation(1),wecanseethatthelatentsen-tencevector~piwillbeupdatedwheneverween-counteram(i,  )sample(e.g.,alltheword,event,time,andvisualfeaturesforthisparticularsentence)inafullpassoverthetrainingdata.aninterestingaspectaboutmatrixfactorizationisthat,inadditiontousingthepreviousrowembedding~pitoupdatethevariablesinequation(1),thecolumnembedding~qjwillalsobeused.similarly,whenupdatingthela-tentcolumnembedding~qjinequation(2),thepasswillvisitallsamplesthathavenon-zeroitemsinthat63

column,whilemakinguseofthe~pivector.essen-tially,intimelinesummarization,thisapproachismodelingthemixedeffectsofsentenceimportance,lexicalfeatures,events,temporalinformation,andvisualfactors.forexample,ifwearepredictingtheid8scoreofanewsentenceattesting,themodelwilltaketheexplicitsentence-levelfeaturesintoaccount,togetherwiththelearnedlatentembed-dingofid8,whichisrecursivelyin   uencedbyothermetricsandfeaturesduringtraining.ourapproachsharessimilaritieswithsomerecentadvancesinwordembeddingtechniques.forex-ample,id97usesthecontinuousbag-of-words(cbow)andskipgramalgorithms(mikolovetal.,2013)tolearncontinuousrepresentationsofwordsfromlargecollectionsoftextandrelationaldata.arecentstudy(levyandgoldberg,2014)showsthatthetechniquebehindid97isverysimilartoim-plicitmatrixfactorization.inourwork,weconsidermultiplesourcesofinformationtolearnthejointembeddinginauni   edmatrixfactorizationframe-work.inadditiontowordinformation,wealsocon-sidereventandtemporalcues.3.5thematrixfactorizationbasedtimelinesummarizationweoutlineourmatrixfactorizationbasedtimelinesummarizationmethodinalgorithm1.sincethisisasupervisedlearningapproach,weassumethecor-pusincludesacollectionofnewsdocumentss,aswellashuman-writtensummarieshforeachdayofthestory.wealsoassumethepublicationdateofeachnewsdocumentisknown(orcomputable).duringtraining,wetraverseeachsentenceinthiscorpus,andcomputeasentenceimportancescore(ii)bycomparingthesentencetothehumangen-eratedsummaryforthatdayusingid8(lin,2004).ifahumansummaryisnotgivenforthatday,iiwillbezero.wealsoextractsubject-verb-objecteventrepresentations,usingthestanfordpart-of-speechtagger(toutanovaetal.,2003)andmalt-parser(nivreetal.,2006).weusethepublicationdateofthenewsdocumentasthepublicationdateofthesentence.visualfeaturesareextractedusingaverydeepid98(simonyanandzisserman,2015).finally,wemergethesevectorsintoajointvectortorepresentarowinourmatrixfactorizationframe-work.then,weperformstochasticgradientdescentalgorithm1amatrixfactorizationbasedtime-linesummarizationalgorithm1:input:newsdocumentss,humansummarieshforeachdayt.2:proceduretraining(str,h)3:foreachtrainingsentencestriinstrdo4:ii   computeimportancescores(stri,ht)5:~ei   extractsvoevents(stri)6:~di   extractpublicationdate(stri)7:~vi   extractvisualfeatures(vtri)8:~mi   mergevectors(ii,~ei,~di,~vi)9:endfor10:foreachepochedo11:foreachcelli,jinmdo12:~pi(e)   ~pi(e)+  (   (i,j)q(e)j     p~pi(e))13:~qj(e)   ~qj(e)+  (   (i,j)p(e)i     q~qj(e))14:endfor15:endfor16:endprocedure17:proceduretesting(ste)18:foreachtestsentencesteiinstedo19:~ei   extractsvoevents(stei)20:~di   extractpublicationdate(stei)21:~vi   extractvisualfeatures(stei)22:~mi   mergevectors(~ei,~di,~vi)23:ii   predictid8(~mi,p,q)24:endfor25:foreachdaytinstedo26:hte   selecttopsentences(stet,~it)27:endfor28:endproceduretrainingtolearnthehiddenlow-rankembeddingsofsentencesandfeaturespandqusingtheupdaterulesoutlinedearlier.duringtesting,westillextracteventsandpubli-cationdates,andthepredictid8functionesti-matesthesentenceimportancescoreii,usingthetrainedlatentlow-rankmatricespandq.tobemorespeci   c,weextractthetext,vision,event,andpublicationdatefeaturesforacandidatesentencei.then,giventhesefeatures,weupdatetheembed-dingsforthissentence,andmakethepredictionbytakingthedotproductofthisi-thcolumnofp(i.e.,~pi)andtheid8columnofq(i.e.,~q1).thispre-dictedscalarvalueiiindicatesthelikelihoodofthesentencebeingincludedinthe   naltimelinesum-mary.finally,wegothroughthepredictedresultsofeachsentenceinthetimelineintemporalorder,and64

includethetop-rankedsentenceswiththehighestsentenceimportancescores.itisnaturaltoscalethismethodfromdailysummariestoweeklyormonthlysummaries.4experimentsinthissection,weinvestigatetheempiricalperfor-manceoftheproposedmethod,comparingtovari-ousbaselines.we   rstdiscussourexperimentalset-tings,includingourprimarydatasetandbaselines.then,wediscussourevaluationresults.wedemon-stratetherobustnessofourapproachbyvaryingthelatentdimensionsofthelow-rankmatrices.next,weshowadditionalexperimentsonaheadline-basedtimelinesummarizationdataset.finally,weprovideaqualitativeanalysisoftheoutputofoursystem.4.1comparativeevaluationonthe17timelinesdatasetweusethe17timelinesdatasetwhichhasbeenusedinseveralpriorstudies(tranetal.,2013b;tranetal.,2013a).itincludes17timelinesfrom9topics1frommajornewsagenciessuchasid98,bbc,andnbcnews.onlyenglishdocumentsareincluded.thedatasetcontains4,650newsdocu-ments.weuseyahoo!imagesearchtoretrievethetop-rankedimageforeachsentence.2wefollowexactlythesametopic-basedcross-validationsetupthatwasusedinpriorwork(tranetal.,2013b):wetrainoneighttopics,testontheremainingtopic,andrepeattheprocesseighttimes.thenumberoftrainingiterationswassetto20;thekwassetto200forthetextonlymodel,and300forthejointtext/imagemodel;andthevocabularyis10kwordsforallsystems.thecommonsummarizationmetricsid8-1,id8-2,andid8-sareusedtoevaluatethequalityofthemachine-generatedtime-lines.weconsiderthefollowingbaselines:1theninetopicsarethebpoilspill,egyptianprotests,fi-nancialcrisis,h1n1,haitiearthquake,iraqwar,libyawar,michaeljacksondeath,andsyriancrisis.2wearenotawareofanypubliclyavailabledatasetfortime-linesummarizationthatincludesbothtextandimages.mostofthesedatasetsaretext-only,notincludingtheoriginalarticle   leorlinkstoaccompanyingimages.weadoptedthisweb-basedcorpusenhancementtechniqueasaproximityfornewsimages.ourlow-rankapproximationtechniquecanbeappliedtotheoriginalnewsimagesinthesameway.(a)id8:0(b)id8:.009.figure3:examplesofretrievedwebimages.theleftimagewasretrievedbyusinganon-informativesentence:   thelatest   veminutenewsbulletinfrombbcworldservice   .therightimagewasretrievedusingacrucialsentencewithanon-zeroid8scorevs.ahumansummary,   casestudy:gulfofmexicooilspillandbpon20april2010adeepwa-teroilwellexplodedinthegulfofmexico   .   random:summarysentencesarerandomlyse-lectedfromthecorpus.   mead:afeature-rich,classicmulti-documentsummarizationsystem(radevetal.,2004a)thatusescentroid-basedsummarizationtech-niques.   chieuetal.(chieuandlee,2004):amulti-documentsummarizationsystemthatusestfidfscorestoindicatethe   popularity   ofasentencecomparedtoothersentences.   ets(yanetal.,2011b):astate-of-the-artun-supervisedtimelinesummarizationsystem.   tranetal.(tranetal.,2013b):anotherstate-of-the-arttimelinesummarizationsystembasedonlearningtoranktechniques,andforwhichresultsonthe17timelinesdatasethavebeenpreviouslyreported.   regression:apartofastate-of-the-artextrac-tivesummarizationmethod(wangetal.,2015)thatformulatesthesentenceextractiontaskasasupervisedregressionproblem.weuseastate-of-the-artregressionimplementationinvowpalwabbit3.wereportresultsforoursystemandthebaselinesonthe17timelinesdatasetintable1.weseethattherandombaselineclearlyperformsworsethantheothermethods.eventhoughchieuetal.(2004)3https://github.com/johnlangford/vowpalwabbit65

methodsid8-1id8-2id8-srandom0.1280.0210.026chieuetal.0.2020.0370.041mead0.2080.0490.039ets0.2070.0470.042tranetal.0.2300.0530.050regression0.3030.0780.081ourapproachtext0.3120.0890.112text+vision0.3310.0910.115table1:comparingthetimelinesummarizationper-formancetovariousbaselinesonthe17timelinesdataset.thebest-performingresultsarehighlightedinbold.andmead(radevetal.,2004a)arenotspeci   -callydesignedforthetimelinesummarizationtask,theyperformrelativelywellagainsttheetssys-temfortimelinesummarization(yanetal.,2011b).tranetal.(2013b)waspreviouslythestate-of-the-artmethodonthe17timelinesdataset.theid8regressionmethodisshownasastrongsupervisedbaseline.ourmatrixfactorizationapproachoutper-formsallofthesemethods,achievingthebestre-sultsinallthreeid8metrics.wealsoseethatthereisanextraboostintheperformancewhencon-sideringvisualfeaturesfortimelinesummarization.figure3showsanexampleoftheretrievedimagesweused.ingeneral,imagesretrievedbyusingmoreimportantsentences(measuredbyid8)includeobjects,aswellamorevividanddetailedscene.4.2comparativeevaluationresultsforheadlinebasedtimelinesummarizationtoevaluatetherobustnessofourapproach,weshowtheperformanceofourmethodontherecentlyre-leasedcrisisdataset(tranetal.,2015).themaindifferencebetweenthecrisisdatasetandthe17time-linesdatasetisthatherewefocusonaheadlinebasedtimelinesummarizationtask,ratherthanus-ingsentencesfromthenewsdocuments.thecrisisdatasetincludesfourtopics:egyptrevolution,libyawar,syriawar,andyemencrisis.thereareatotalof15,534newsdocumentsinthedataset,andeachtopichasaround4kdocuments.thereare25man-uallycreatedtimelinesforthesetopics,collectedfrommajornewsagenciessuchasbbc,id98,andreuters.weperformstandardcross-validationonmethodsid8-1id8-2id8-sregression0.2070.0450.039ourapproachtext0.2110.0460.040text+vision0.2320.0520.044table3:comparingthetimelinesummarizationper-formancetothestate-of-the-artsupervisedsentenceregressionapproachonthecrisisdataset.thebest-performingresultsarehighlightedinbold.thisdataset:wetrainonthreetopics,andtestontheother.herekissetto300,andthevocabularyis10kwordsforallsystems.table3showstheper-formanceofoursystem.oursystemissigni   cantlybetterthanthestrongsupervisedregressionbaseline.whenconsideringjointlearningoftextandvision,weseethatthereisafurtherimprovement.4.3headlinebasedtimelinesummarization:aqualitativeanalysisinthissection,weperformaqualitativeanalysisoftheoutputofoursystemfortheheadlinebasedtime-linesummarizationtask.wetrainthesystemonthreetopics,andshowasampleoftheoutputonthesyriawar.table2showsasubsetofthetime-lineforthesyriawargeneratedbyoursystem.weseethatmostofthedailysummariesarerelevanttothetopic,excepttheonegeneratedon2011-11-24.whenevaluatingthequality,wenoticethatmostofthemareofhighquality:aftertheinitialhypothesisofthesyriawaron2011-11-18,thefollowingdailysummariesconcerntheworld   sresponsetothecri-sis.weshowthatmostoftherelevantsummariesarealsoprovidingspeci   cinformation,withanex-ceptionon2011-12-02.wesuspectthatthisisbe-causethisheadlinecontainsthreekeywords   syria   ,   civil   ,   war   ,andalsothekeydateinformation:themodelwastrainedpartlyonthelibyawartime-line,andthereforemanyfeaturesandparameterswereactivatedinthematrixfactorizationframeworktogiveahighrecommendationinthistestingsce-nario.incontrast,whenevaluatingtheoutputofthejointtextandvisionsystem,weseethatthiserroriseliminated:theselectedsentenceon2011-12-02is   elevenkilledafterweeklyprayersinsyriaoneveofarableaguedeadline   .66

datesummaryrelevant?good?2011-11-18syriaisheadinginexorablyforacivilwarandanappallingbloodbathxx2011-11-19davidignatiussortingouttherebelforcesinsyriaxx2011-11-20syriacommittedcrimesagainsthumanity,u.n.panel   ndsxx2011-11-21iraqjoinssyriacivilwarwarningsxx2011-11-22thepathtoacivilwarinsyriaxx2011-11-23reportiran,hezbollahsettingupmilitiastoprepareforpost-assadsyriaxx2011-11-24q&asyria   sdaringactressfeaturesaljazeeraenglish    2011-11-25syriacon   icthowresidentsofaleppostruggleforsurvivalxx2011-11-27syrianjetsbombrebelareasneardamascusastroopsbattlexx2011-11-28istheregionalshowdowninsyriarekindlingiraqscivilwar?xx2011-11-29syriacrisisarmydropslea   etsoverdamascusxx2011-11-30russiasayswest   ssyriapush   pathtocivilwar   xx2011-12-01unextendssyriawarcrimesinvestigationdespiteoppositionfromchinaxx2011-12-02unsyriacivilwar1222011x  2011-12-03israelsays   resintosyriaaftergolanattackontroopsxxtable2:atimelineexampleforsyriawargeneratedbyourtext-onlysystem.5conclusionsinthispaper,weintroducealow-rankapproxima-tionbasedapproachforlearningjointembeddingsofnewsstoriesandimagesfortimelinesummarization.weleveragethesuccessofmatrixfactorizationtech-niquesinrecommendersystems,andcastthemulti-documentextractivesummarizationtaskasasen-tencerecommendationproblem.foreachsentenceinthecorpus,wecomputeitssimilaritytoahuman-generatedabstract,andextractlexical,event,andtemporalfeatures.weuseaconvolutionalneuralarchitecturetoextractvisionfeatures.wedemon-stratetheeffectivenessofthisjointlearningmethodbycomparisonwithseveralstrongbaselinesonthe17timelinesdatasetandaheadlinebasedtimelinesummarizationdataset.weshowthatimagefea-turesimprovetheperformanceofourmodelsignif-icantly.thisfurthermotivatesinvestmentinjointmultimodallearningfornlptasks.acknowledgmentstheauthorswouldliketothankkapilthadaniandtheanonymousreviewersfortheirthoughtfulcom-ments.referencesamrahmed,qirongho,choonhteo,jacobeisenstein,ericxing,andalexsmola.2011.onlineid136forthein   nitetopic-clustermodel:storylinesfromstreamingtext.inproceedingsofaistats.jamesallan,rahulgupta,andvikaskhandelwal.2001.temporalsummariesofnewtopics.inproceedingsofsigir.timalthoff,xinlunadong,kevinmurphy,safaalai,vandang,andweizhang.2015.timemachine:timelinegenerationforknowledge-baseentities.inproceedingsofkdd.robertbellandyehudakoren.2007.lessonsfromthenet   ixprizechallenge.acmsigkddexplorationsnewsletter,9(2).jamesbennettandstanlanning.2007.thenet   ixprize.inproceedingsofthekddcupandworkshop.haileongchieuandyoongkeoklee.2004.querybasedeventextractionalongatimeline.inproceed-ingsofsigir.wei-shengchin,yongzhuang,yu-chinjuan,andchih-jenlin.2015.afastparallelstochasticgradientmethodformatrixfactorizationinsharedmemorysys-tems.acmtransactionsonintelligentsystemsandtechnology,6(1).jamesconroy,judithschlesinger,dianeo   leary,andmaryokurowski.2001.usingid48andlogisticre-gressiontogenerateextractsummariesforduc.inproceedingsofduc.nazanindehghaniandmasoudasadpour.2015.graph-basedmethodforsummarizedstorylinegenerationintwitter.arxivpreprintarxiv:1504.07361.g  uneserkananddragomirradev.2004.lexrank:graph-basedlexicalcentralityassalienceintextsum-marization.journalofarti   cialintelligencere-search,22(1).anthonyfader,dragomirradev,michaelcrespin,burtmonroe,kevinquinn,andmichaelcolaresi.2007.mavenrank:identifyingin   uentialmembersofthe67

ussenateusinglexicalcentrality.inproceedingsofemnlp-conll.goranglava  sandjan  snajder.2014.eventgraphsforinformationretrievalandmulti-documentsummariza-tion.expertsystemswithapplications,41(15).ahmedhassan,anthonyfader,michaelcrespin,kevinquinn,burtmonroe,michaelcolaresi,anddragomirradev.2008.trackingthedynamicevolutionofpar-ticipantsalienceinadiscussion.inproceedingsofcoling.lifuhuangandlian   enhuang.2013.optimizedeventstorylinegenerationbasedonmixture-event-aspectmodel.inproceedingsofemnlp.chriskedzie,kathleenmckeown,andfernandodiaz.2014.summarizingdisastersovertime.inproceed-ingsofthebloombergworkshoponsocialgoodatkdd.jerrykiernanandevimariaterzi.2009.constructingcomprehensivesummariesoflargeeventsequences.acmtransactionsonknowledgediscoveryfromdata,3(4).gunheekimandericxing.2014.reconstructingstory-linegraphsforimagerecommendationfromwebcom-munityphotos.inproceedingsofcvpr.gunheekim,leonidsigal,andericpxing.2014.jointsummarizationoflarge-scalecollectionsofwebim-agesandvideosforstorylinereconstruction.inpro-ceedingsofcvpr.yehudakoren,robertbell,andchrisvolinsky.2009.matrixfactorizationtechniquesforrecommendersys-tems.computer,8.juliankupiec,janpedersen,andfrancinechen.1995.atrainabledocumentsummarizer.inproceedingsofsigir.ju-honglee,sunpark,chan-minahn,anddaehokim.2009.automaticgenericdocumentsummarizationbasedonnon-negativematrixfactorization.informa-tionprocessing&management,45(1).omerlevyandyoavgoldberg.2014.neuralwordem-beddingasimplicitmatrixfactorization.inproceed-ingsofnips.chenlin,chunlin,jingxuanli,dingdingwang,yangchen,andtaoli.2012.generatingeventstorylinesfrommicroblogs.inproceedingsofcikm.chin-yewlin.2004.id8:apackageforautomaticevaluationofsummaries.inproceedingsoftheaclworkshop   textsummarizationbranchesout   .qiaozhumei,jianguo,anddragomirradev.2010.di-vrank:theinterplayofprestigeanddiversityininfor-mationnetworks.inproceedingsofkdd.radamihalceaandpaultarau.2004.textrank:bring-ingorderintotexts.inproceedingsofemnlp.tomasmikolov,kaichen,gregcorrado,andjeffreydean.2013.ef   cientestimationofwordrepresenta-tionsinvectorspace.arxivpreprintarxiv:1301.3781.borjanavarro-coloradoandestelasaquete.2015.gplsiua:combiningtemporalinformationandtopicmodelingforcross-documenteventordering.inpro-ceedingsofsemeval.jun-pingng,yanchen,min-yenkan,andzhoujunli.2014.exploitingtimelinestoenhancemulti-documentsummarization.inproceedingsoftheacl.joakimnivre,johanhall,andjensnilsson.2006.malt-parser:adata-drivenparser-generatorfordependencyparsing.inproceedingsoflrec.dragomirradev,timothyallison,sashablair-goldensohn,johnblitzer,ardacelebi,stankodimitrov,elliottdrabek,alihakim,wailam,danyuliu,jahnaotterbacher,hongqi,horaciosaggion,simoneteufel,michaeltopper,adamwinkel,andzhuzhang.2004a.mead   aplatformformultidocumentmultilingualtextsummarization.inproceedingsoflrec.dragomirrradev,hongyanjing,ma  gorzatasty  s,anddanieltam.2004b.centroid-basedsummarizationofmultipledocuments.informationprocessing&man-agement.benjaminrecht,christopherre,stephenwright,andfengniu.2011.hogwild:alock-freeapproachtoparallelizingstochasticgradientdescent.inproceed-ingsofnips.olgarussakovsky,jiadeng,haosu,jonathankrause,sanjeevsatheesh,seanma,zhihenghuang,andrejkarpathy,adityakhosla,michaelbernstein,alexan-derc.berg,andlifei-fei.2014.id163largescalevisualrecognitionchallenge.internationaljour-nalofcomputervision,115(3).karensimonyanandandrewzisserman.2015.verydeepconvolutionalnetworksforlarge-scaleimagerecognition.inproceedingsoficlr.kristinatoutanova,danklein,christopherdmanning,andyoramsinger.2003.feature-richpart-of-speechtaggingwithacyclicdependencynetwork.inpro-ceedingsofnaacl-hlt.giangbinhtran,mohammadalrifai,anddatquocnguyen.2013a.predictingrelevantnewseventsfortimelinesummaries.inproceedingsofwww.giangbinhtran,tuanatran,nam-khanhtran,mo-hammadalrifai,andnattiyakanhabua.2013b.leveraginglearningtorankinanoptimizationframe-workfortimelinesummarization.inproceedingsofthesigirworkshopontime-awareinformationac-cess.giangtran,mohammadalrifai,andeelcoherder.2015.timelinesummarizationfromrelevantheadlines.inproceedingsofecir.68

dingdingwang,taoli,shenghuozhu,andchrisding.2008.multi-documentsummarizationviasentence-levelsemanticanalysisandsymmetricmatrixfactor-ization.inproceedingsofsigir.dingdingwang,taoli,andmitsunoriogihara.2012.generatingpictorialstorylinesviaminimum-weightconnecteddominatingsetapproximationinmulti-viewgraphs.inproceedingsofaaai.luwang,clairecardie,andgalenmarchetti.2015.socially-informedtimelinegenerationforcomplexevents.inproceedingsofnaacl-hlt.ruiyan,liangkong,congruihuang,xiaojunwan,xi-aomingli,andyanzhang.2011a.timelinegener-ationthroughevolutionarytrans-temporalsummariza-tion.inproceedingsofemnlp.ruiyan,xiaojunwan,jahnaotterbacher,liangkong,xiaomingli,andyanzhang.2011b.evolution-arytimelinesummarization:abalancedoptimizationframeworkviaiterativesubstitution.inproceedingsofsigir.xinwaynezhao,yanweiguo,ruiyan,yulanhe,andxiaomingli.2013.timelinegenerationwithsocialattention.inproceedingsofsigir.wubaizhou,chaoshen,taoli,shu-chingchen,andningxie.2014.generatingtextualstorylinetoim-provesituationawarenessindisastermanagement.inproceedingsoftheieeeinternationalconferenceoninformationreuseandintegration.weizhongzhuandchaomeichen.2007.storylines:vi-sualexplorationandanalysisinlatentsemanticspaces.computers&graphics,31(3).