revisiting summarization evaluation for scienti   c articles

arman cohan and nazli goharian

information retrieval lab

department of computer science

georgetown university

arman@ir.cs.georgetown.edu, nazli@ir.cs.georgetown.edu

6
1
0
2

 
r
p
a
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
0
4
0
0

.

4
0
6
1
:
v
i
x
r
a

abstract

evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated
summaries with a set of human written gold-standard summaries. the most widely used metric in summarization evaluation has been
the id8 family. id8 solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of
terminology variations and id141, id8 is not as effective. scienti   c article summarization is one such case that is different
from general domain summarization (e.g. newswire data). we provide an extensive analysis of id8   s effectiveness as an evaluation
metric for scienti   c summarization; we show that, contrary to the common belief, id8 is not much reliable in evaluating scienti   c
summaries. we furthermore show how different variants of id8 result in very different correlations with the manual pyramid
scores. finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a
system generated summary and the corresponding human written summaries. we call our metric sera (summarization evaluation by
relevance analysis). unlike id8, sera consistently achieves high correlations with manual scores which shows its effectiveness in
evaluation of scienti   c article summarization.

keywords: summarization, evaluation, scienti   c articles

1.

introduction

the most

automatic text summarization has been an active research
area in natural language processing for several decades.
to compare and evaluate the performance of different
summarization systems,
intuitive approach
is assessing the quality of the summaries by human
evaluators. however, manual evaluation is expensive
and the obtained results are subjective and dif   cult to
reproduce (giannakopoulos and karkaletsis, 2013). to
address these problems, automatic evaluation measures
for summarization have been proposed. id8 (lin,
2004) is one of the    rst and most widely used metrics
in summarization evaluation.
it facilitates evaluation of
system generated summaries by comparing them to a set
of human written gold-standard summaries. it is inspired
by the success of a similar metric id7 (papineni et
al., 2002) which is being used in machine translation
(mt) evaluation. the main success of id8 is due
to its high correlation with human assessment scores on
standard benchmarks (lin, 2004). id8 has been used as
one of the main id74 in later summarization
benchmarks such as tac1 (owczarzak and dang, 2011).
since the establishment of id8, almost all research in
text summarization have used this metric as the main means
for evaluating the quality of the proposed approaches. the
public availability of id8 as a toolkit for summarization
evaluation has contributed to its wide usage. while
id8 has originally shown good correlations with human
assessments, the study of its effectiveness was only limited
to a few benchmarks on news summarization data (duc2
since 2003, summarization
2001-2003 benchmarks).

1text analysis conference (tac) is a series of workshops for

evaluating research in natural language processing

has grown to much further domains and genres such as
scienti   c documents, social media and id53.
while there is not enough compelling evidence about
the effectiveness of id8 on these other summarization
tasks, published research is almost always evaluated by
id8. in addition, id8 has a large number of possible
variants and the published research often (arbitrarily)
reports only a few of these variants.
by de   nition, id8 solely relies on lexical overlaps
(such as id165 and sequence overlaps) between the system
generated and human written gold-standard summaries.
higher lexical overlaps between the two show that the
system generated summary is of higher quality. therefore,
in cases of terminology nuances and id141, id8
is not accurate in estimating the quality of the summary.
we study the effectiveness of id8 for evaluating
scienti   c summarization. scienti   c summarization targets
much more technical and focused domains in which
the goal is providing summaries for scienti   c articles.
scienti   c articles are much different than news articles
in elements such as length, complexity and structure.
thus, effective summarization approaches usually have
much higher compression rate, terminology variations and
id141 (teufel and moens, 2002).
scienti   c summarization has attracted more attention
recently (examples include works by abu-jbara and radev
(2011), qazvinian et al. (2013), and cohan and goharian
(2015)). thus, it is important to study the validity of
existing methodologies applied to the evaluation of news
article summarization for this task. in particular, we raise
the important question of how effective is id8, as

2document understanding conference (duc) was one of
nist workshops that provided infrastructure for evaluation of text
summarization methodologies (http://duc.nist.gov/).

an evaluation metric for scienti   c summarization? we
answer this question by comparing id8 scores with
semi-manual evaluation score (pyramid) in tac 2014
scienti   c summarization dataset1. results reveal
that,
contrary to the common belief, correlations between
id8 and the pyramid scores are weak, which challenges
its effectiveness for scienti   c summarization. furthermore,
we show a large variance of correlations between different
id8 variants and the manual evaluations which further
makes the reliability of id8 for evaluating scienti   c
summaries less clear. we then propose an evaluation
metric based on relevance analysis of summaries which
aims to overcome the limitation of high lexical dependence
in id8. we call our metric sera (summarization
evaluation by relevance analysis). results show that
the proposed metric achieves higher and more consistent
correlations with semi-manual assessment scores.
our contributions are as follows:

    study the validity of id8 as the most widely-used
summarization evaluation metric in the context of
scienti   c summarization.

    compare and contrast the performance of all variants of

id8 in scienti   c summarization.

    propose an alternative content relevance based evaluation
metric for assessing the content quality of the summaries
(sera).

    provide human pyramid annotations for summaries in

tac 2014 scienti   c summarization dataset.2

2. summarization evaluation by id8

id8 has been the most widely used family of metrics
in summarization evaluation. in the following, we brie   y
describe the different variants of id8:

    id8-n: id8-n was originally a recall oriented
metric that considered id165 recall between a system
generated summary and the corresponding gold human
summaries.
in later versions, in addition to the recall,
precision was also considered in id8-n, which is the
precision of id165s in the system generated summary
with respect to the gold human summary. to combine
both precision and recall, f1 scores are often reported.
common values of n range from 1 to 4.

    id8-l: this variant of id8 compares the system
generated summary and the human generated summary
based on the longest common subsequences (lcs)
between them. the premise is that, longer lcs between
the system and human summaries shows more similarity
and therefore higher quality of the system summary.

1http://www.nist.gov/tac/2014/biomedsumm/
2the annotations can be accessed via the following

repository: https://github.com/acohan/
tac-pyramid-annotations/

    id8-w: one problem with id8-l is that all lcs
with same lengths are rewarded equally. the lcs can
be either related to a consecutive set of words or a long
sequence with many gaps. while id8-l treats all
sequence matches equally, it makes sense that sequences
with many gaps receive lower scores in comparison
with consecutive matches.
id8-w considers an
additional weighting function that awards consecutive
matches more than non-consecutive ones.
the

skip-bigram
co-occurrence statistics between the two summaries. it is
similar to id8-2 except that it allows gaps between
the bigrams by skipping middle tokens.

    id8-s: id8-s

computes

    id8-su: id8-s does not give any credit to a
system generated sentence if the sentence does not have
any word pair co-occurring in the reference sentence. to
solve this potential problem, id8-su was proposed
which is an extension of id8-s that also considers
unigram matches between the two summaries.

id8-l, id8-w, id8-s and id8-su were
later extended to consider both the recall and precision.
in calculating id8, stopword removal or id30 can
also be considered, resulting in more variants.
in the summarization literature, despite the large number of
variants of id8, only one or very few of these variants
are often chosen (arbitrarily) for evaluation of the quality
of the summarization approaches. when id8 was
proposed, the original variants were only recall-oriented
and hence the reported correlation results (lin, 2004). the
later extension of id8 family by precision were only
re   ected in the later versions of the id8 toolkit and
additional evaluation of its effectiveness was not reported.
nevertheless,
later published work in summarization
adopted this toolkit for its ready implementation and
relatively ef   cient performance.
the original id8 metrics show high correlations
with human judgments of
the quality of summaries
on the duc 2001-2003 benchmarks. however,
these
benchmarks consist of newswire data and are intrinsically
very different
than other summarization tasks such as
summarization of scienti   c papers. we argue that
id8 is not the best metric for all summarization tasks
and we propose an alternative metric for evaluation of
scienti   c summarization. the proposed alternative metric
shows much higher and more consistent correlations with
manual judgments in comparison with the well-established
id8.

3. summarization evaluation by relevance

analysis (sera)

id8 functions based on the assumption that in order for
a summary to be of high quality, it has to share many words
or phrases with a human gold summary. however, different
terminology may be used to refer to the same concepts and
thus relying only on lexical overlaps may underrate content
quality scores. to overcome this problem, we propose an
approach based on the premise that concepts take meanings

from the context they are in, and that related concepts
co-occur frequently.
our proposed metric is based on analysis of the content
relevance between a system generated summary and the
corresponding human written gold-standard summaries.
on high level, we indirectly evaluate the content relevance
between the candidate summary and the human summary
using information retrieval. to accomplish this, we use the
summaries as search queries and compare the overlaps of
the retrieved results. larger number of overlaps, suggest
that the candidate summary has higher content quality with
respect to the gold-standard. this method, enables us
to also reward for terms that are not lexically equivalent
but semantically related. our method is based on the
well established linguistic premise that semantically related
words occur in similar contexts (turney et al., 2010). the
context of the words can be considered as surrounding
words, sentences in which they appear or the documents.
for scienti   c summarization, we consider the context of
the words as the scienti   c articles in which they appear.
thus, if two concepts appear in identical set of articles, they
are semantically related. we consider the two summaries
as similar if they refer to same set of articles even if the
two summaries do not have high lexical overlaps. to
capture if a summary relates to a article, we use information
retrieval by considering the summaries as queries and the
articles as documents and we rank the articles based on
their relatedness to a given summary. for a given pair of
system summary and the gold summary, similar rankings
of the retrieved articles suggest that the summaries are
semantically related, and thus the system summary is of
higher quality.
based on the domain of interest, we    rst construct an index
from a set of articles in the same domain. since tac 2014
was focused on summarization in the biomedical domain,
our index also comprises of biomedical articles. given a
candidate summary c and a set of gold summaries gi (i =
1, ..., m; m is the total number of human summaries), we
submit the candidate summary and gold summaries to the
search engine as queries and compare their ranked results.
let i = (cid:104)d1, ..., dn(cid:105) be the entire index which comprises
of n total documents.
let rc = (cid:104)d(cid:96)1 , ..., d(cid:96)n(cid:105) be the ranked list of retrieved
documents for candidate summary c, and rgi =
(cid:104)d(cid:96)(i)
(cid:105) the ranked list of results for the gold
summary gi. these lists of results are based on a rank
cut-off point n that is a parameter of the system. we
provide evaluation results on different choices of cut-off
point n in the section 5. we consider the following
(i) simple intersection and (ii) discounted
two scores:
intersection by rankings. the simple intersection just
considers the overlaps of the results in the two ranked lists
and ignores the rankings. the discounted ranked scores, on
the other hand, penalizes ranking differences between the
two result sets. as an example consider the following list
of retrieved documents (denoted by dis) for a candidate and
a gold summary as queries:
results for candidate summary: (cid:104)d1, d2, d3, d4(cid:105)
results for gold summary: (cid:104)d3, d2, d1, d4(cid:105)

, ..., d(cid:96)(i)

n

1

(cid:1)

these two sets of results consist of identical documents but
the ranking of the retrieved documents differ. therefore,
the simple intersection method assigns a score of 1.0 while
in the discounted ranked score, the score will be less than
1.0 (due to ranking differences between the result lists).
we now de   ne the metrics more precisely. using the
above notations, without loss of generality, we assume that
|rc|     |rgi|. sera is de   ned as follows:
|rc     rgi|

m(cid:88)

sera =

1
m

i=1

|rc|

to also account for the ranked position differences, we
modify this score to discount rewards based on rank
differences. that is, in ideal score, we want search results
from candidate summary (rc) to be the same as results
for gold-standard summaries (rg) and the rankings of the
results also be the same. if the rankings differ, we discount
the reward by log of the differences of the ranks. more
speci   cally, the discounted score (sera-dis) is de   ned as:

m(cid:80)

(cid:0) |rc|(cid:80)

|rgi|(cid:80)

(cid:40)

sera-dis =

i=1

j=1

k=1

0

m    dmax

(

1

log(|j   k|+2) )

c = r(k)
gi

if r(j)
otherwise

result

in addition, r(j)

where, as previously de   ned, m, rc and rgi are total
number of human gold summaries,
list for the
candidate summary and result list for the human gold
summary, respectively.
c shows the jth
results in the ranked list rc and dmax is the maximum
attainable score used as the normalizing factor.
we use elasticsearch1, an open-source search engine, for
indexing and querying the articles. for retrieval model, we
use the id38 retrieval model with dirichlet
smoothing (zhai and lafferty, 2001). since tac 2014
benchmark is on summarization of biomedical articles,
the appropriate index would be the one constructed from
articles in the same domain. therefore, we use the open
access subset of pubmed2 which consists of published
articles in biomedical literature.
we also experiment with different query (re)formulation
approaches.
query reformulation is a method in
information retrieval that aims to re   ne the query for better
retrieval of results. query reformulation methods often
consist of removing ineffective terms and expressions from
the query (query reduction) or adding terms to the query
that help the retrieval (id183). query reduction
is specially important when queries are verbose. since we
use the summaries as queries, the queries are usually long
and therefore we consider query reductions.
in our experiments, the query reformulation is done by
3 different ways: (i) plain: the entire summary without
stopwords and numeric values; (ii) noun phrases (np): we
only keep the noun phrases as informative concepts in the
summary and eliminate all other terms; and (iii) keywords

1https://github.com/elastic/elasticsearch
2pubmed is a comprehensive resource of articles and abstracts
published in life sciences and biomedical literature http://
www.ncbi.nlm.nih.gov/pmc/

(kw): we only keep the keywords and key phrases in
the summary. for extracting the keywords and keyphrases
(with length of up to 3 terms), we extract expressions whose
idf 1 values is higher than a prede   ned threshold that is
set as a parameter. we set this threshold to the average
idf values of all terms except stopwords.
idf values are
calculated on the same index that is used for the retrieval.
we hypothesize that using only informative concepts in the
summary prevents query drift and leads to retrieval of more
relevant documents. noun phrases and keywords are two
heuristics for identifying the informative concepts.

4. experimental setup

is

benchmark

4.1. data
the only scienti   c
to the best of our knowledge,
from tac
2014
summarization
summarization track.
for evaluating the effectiveness
of id8 variants and our metric (sera), we use this
benchmark, which consists of 20 topics each with a
biomedical
journal article and 4 gold human written
summaries.
4.2. annotations
in the tac 2014 summarization track, id8 was
suggested as the evaluation metric for summarization
and no human assessment was provided for the topics.
therefore,
to study the effectiveness of the evaluation
metrics, we use the semi-manual pyramid evaluation
framework (nenkova and passonneau, 2004; nenkova et
al., 2007). in the pyramid scoring, the content units in the
gold human written summaries are organized in a pyramid.
in this pyramid, the content units are organized in tiers and
higher tiers of the pyramid indicate higher importance. the
content quality of a given candidate summary is evaluated
with respect to this pyramid.
to analyze the quality of the id74, following
the pyramid framework, we design an annotation scheme
that is based on identi   cation of important content units.
consider the following example:
genetically
endogeneous small rnas
screened and studied to    nd the mirnas which are related
to tumorigenesis.
in the above example, the underlined expressions are the
content units that convey the main meaning of the text.
we call these small units, nuggets which are phrases or
concepts that are the main contributors to the content
quality of the summary.
we asked two human annotators to review the gold
summaries and extract content units in these summaries.
the pyramid tiers represent the occurrences of nuggets
across all the human written gold-standard summaries, and
therefore the nuggets are weighted based on these tiers.
the intuition is that, if a nugget occurs more frequently in
the human summaries, it is a more important contributor
(thus belongs to higher tier in the pyramid). thus, if
a candidate summary contains this nugget, it should be
rewarded more. an example of the nuggets annotations
in pyramid framework is shown in table 1.
in this

(mirna) were

id
n1
n2

n3

n4
n5
n6

nugget
idh1/2
isocitrate
dehydrogenase 1 & 2
alpha
ketoglutarate-dependent
enzyme
tet2
cell mutation
dna methylation

tier
3
2

1

1
4
2

table 1: example of nugget annotation for pyramid scores.
the pyramid tier represents the number of occurrences of
the nugget in all the human written gold summaries.

example, the nugget    cell mutation    belongs to the 4th tier
and it suggests that the    cell mutation    nugget is a very
important representative of the content of the corresponding
document.
let ti de   ne the tiers of the pyramid with t1 being the
bottom tier and tn the top tier. let ni be the number of the
nuggets in the candidate summary that appear in the tier ti.
then the pyramid score p of the candidate summary will
be:

n(cid:88)

1

p =

pmax

i=1

i    ni

n(cid:80)

where pmax is the maximum attainable score used for
normalizing the scores:

n(cid:88)

i    |ti| + j    (x     n(cid:88)

|ti|)

pmax =

i=j+1

i=j+1

where x is the total number of nuggets in the summary and

|tt|     x.

i

t=i

j = max
we release the pyramid annotations of the tac 2014
dataset through a public repository2.
4.3. summarization approaches
we study the effectiveness of id8 and our proposed
method (sera) by analyzing the correlations with
semi-manual human judgments.
very few teams
participated in tac 2014 summarization track and the
results and the review paper of tac 2014
of   cial
systems were never published. therefore,
to evaluate
the effectiveness of id8, we applied 9 well-known
summarization approaches on the tac 2014 scienti   c
summarization dataset. obtained id8 and sera results
of each of these approaches are then correlated with
semi-manual human judgments.
in the following, we
brie   y describe each of these summarization approaches.

1. lexrank (erkan and radev, 2004): lexrank    nds
the most
important (central) sentences in a document
by using id93 in a graph constructed from the
document sentences. in this graph, the sentences are nodes

1inverted document frequency

2https://github.com/acohan/

tac-pyramid-annotations

and the similarity between the sentences determines the
edges. sentences are ranked according to their importance.
importance is measured in terms of centrality of the
sentence     the total number of edges incident on the node
(sentence) in the graph. the intuition behind lexrank is
that a document can be summarized using the most central
sentences in the document that capture its main aspects.

2. latent semantic analysis (lsa) based summarization
(steinberger and jezek, 2004):
in this summarization
method, singular value decomposition (svd) (deerwester
et al., 1990) is used for deriving latent semantic structure of
the document. the document is divided into sentences and
a term-sentence matrix a is constructed. the matrix a
is then decomposed into a number of linearly-independent
singular vectors which represent the latent concepts in
the document. this method, intuitively, decomposes the
document into several latent topics and then selects the
most representative sentences for each of these topics as
the summary of the document.

3. maximal marginal relevance (mmr) (carbonell and
goldstein, 1998): maximal marginal relevance (mmr) is
a greedy strategy for selecting sentences for the summary.
sentences are added iteratively to the summary based on
their relatedness to the document as well as their novelty
with respect to the current summary.

4. citation based summarization (qazvinian et al., 2013):
in this method, citations are used for summarizing an
article. using the lexrank algorithm on the citation
network of the article, top sentences are selected for the
   nal summary.

5. using frequency of the words (luhn, 1958):
in
this method, which is one the earliest works in text
summarization, raw word frequencies are used to estimate
the saliency of sentences in the document. the most salient
sentences are chosen for the    nal summary.

6. sumbasic (vanderwende et al., 2007): sumbasic is an
approach that weights sentences based on the distribution
of words that is derived from the document. sentence
selection is applied iteratively by selecting words with
highest id203 and then    nding the highest scoring
sentence that contains that word. the word weights are
updated after each iteration to prevent selection of similar
sentences.

7. summarization using citation-context and discourse
structure (cohan and goharian, 2015):
in this method,
the set of citations to the article are used to    nd
the article sentences that directly re   ect those citations
(citation-contexts). in addition, the scienti   c discourse of
the article is utilized to capture different aspects of the
article. the scienti   c discourse usually follows a structure
in which the authors    rst describe their hypothesis, then the
methods, experiment, results and implications. sentence
selection is based on    nding the most important sentences
in each of the discourse facets of the document using the
mmr heuristic.

pyramid

spearman(  ) kendall(  )

metric
id8-1-f
id8-1-p
id8-1-r
id8-2-f
id8-2-p
id8-2-r
id8-3-f
id8-3-p
id8-3-r
id8-l-f
id8-l-p
id8-l-r
id8-s-f
id8-s-p
id8-s-r
id8-su-f
id8-su-p
id8-su-r
id8-w-1.2-f
id8-w-1.2-p
id8-w-1.2-r
sera-5
sera-10
sera-kw-5
sera-kw-10
sera-np-5
sera-np-10
sera-dis-5
sera-dis-10
sera-dis-kw-5
sera-dis-kw-10
sera-dis-np-5
sera-dis-np-10

pearson(r)

0.454
0.257
0.513
0.816
0.824
0.803
0.878
0.875
0.875
0.454
0.262
0.52
0.603
0.344
0.664
0.601
0.338
0.662
0.607
0.418
0.626
0.823
0.788
0.848
0.641
0.859
0.806
0.631
0.687
0.838
0.766
0.834
0.86

0.174
0.116
0.229
0.696
0.841
0.696
0.841
0.725
0.841
0.261
0.29
0.261
0.406
0.174
0.406
0.493
0.174
0.406
0.493
0.377
0.667
0.941
0.647
0.765
0.618
1.0
0.941
0.824
0.824
0.941
0.712
0.941
0.941

0.138

0

0.138
0.552
0.69
0.552
0.69
0.552
0.69
0.276
0.138
0.276
0.414
0.138
0.414
0.462
0.138
0.414
0.414
0.276
0.552
0.857
0.429
0.571
0.486
1.0
0.857
0.714
0.714
0.857
0.729
0.857
0.857

table 2: correlation between variants of id8 and sera, with
human pyramid scores. all variants of id8 are displayed.
f : f-score; r: recall; p : precision; dis: discounted variant
of sera; kw: using keyword query reformulation; np: using
noun phrases for query reformulation. the numbers in front of
the sera metrics indicate the rank cut-off point.

8. kl divergence (haghighi and vanderwende, 2009) in
this method, the document unigram distribution p and the
summary unigram distributation q are considered; the goal
is to    nd a summary whose distribution is very close to the
document distribution. the difference of the distributions is
captured by the kullback-lieber (kl) divergence, denoted
by kl(p||q).
9. summarization based on topic models (haghighi
and vanderwende, 2009):
instead of using unigram
distributions for modeling the content distribution of the
document and the summary,
this method models the
document content using an lda based topic model (blei
et al., 2003). it then uses the kl divergence between the
document and the summary content models for selecting
sentences for the summary.

5. results and discussion

we calculated all variants of id8 scores, our proposed
metric, sera, and the pyramid score on the generated
summaries from the summarizers described in section 4.3..
we do not report the id8, sera or pyramid scores
of individual systems as it is not the focus of this study.

our aim is to analyze the effectiveness of the evaluation
metrics, not the summarization approaches. therefore,
we consider the correlations of the automatic evaluation
metrics with the manual pyramid scores to evaluate their
effectiveness;
the metrics that show higher correlations
with manual judgments are more effective.
table 2 shows the pearson, spearman and kendall
correlation of id8 and sera, with pyramid scores.
both id8 and sera are calculated with stopwords
removed and with id30. our experiments with
inclusion of stopwords and without id30 showed
similar results and thus, we do not include those to avoid
redundancy.
5.1. sera
the results of our proposed method (sera) are shown
in the bottom part of table 2.
in general, sera shows
better correlation with pyramid scores in comparison with
id8. we observe that the pearson correlation of sera
with cut-off point of 5 (shown by sera-5) is 0.823 which
is higher than most of the id8 variants. similarly, the
spearman and kendall correlations of the sera evaluation
score is 0.941 and 0.857 respectively, which are higher than
all id8 correlation values. this shows the effectiveness
of the simple variant of our proposed summarization
evaluation metric.
table 2 also shows the results of other sera variants
including discounting and query reformulation methods.
some of these variants are the result of applying query
reformulation in the process of document retrieval which
are described in section 3. as illustrated, the noun phrases
(np) query reformulation at cut-off point of 5 (shown as
sera-np-5) achieves the highest correlations among all
the sera variants (r = 0.859,    =    = 1.0).
in
the case of keywords (kw) query reformulation, without
using discounting, we can see that there is no positive gain
in correlation. however, keywords when applied on the
discounted variant of sera, result in higher correlations.
discounting has more positive effect when applied on
query reformulation-based sera than on the simple
variant of sera.
in the case of discounting and np
query reformulation (sera-dis-np), we observe higher
correlations in comparison with simple sera. similarly,
in the case of keywords (kw), positive correlation gain is
obtained in most of correlation coef   cients. np without
discounting and at cut-off point of 5 (sera-np-5) shows
the highest non-parametric correlation.
in addition, the
discounted np at cut-off point of 10 (sera-np-dis-10)
shows the highest parametric correlations.
in general, using np and kw as heuristics for    nding the
informative concepts in the summary effectively increases
the correlations with the manual scores.
selecting
informative terms from long queries results in more
relevant documents and prevents query drift. therefore, the
overall similarity between the two summaries (candidate
and the human written gold summary) is better captured.
5.2. id8
another
the
effectiveness of id8 scores (top part of table 2).

observation

important

regarding

is

metric
sera-5
sera-10
sera-kw-5
sera-kw-10
sera-np-5
sera-np-10
sera-dis-5
sera-dis-10
sera-dis-kw-5
sera-dis-kw-10
sera-dis-np-5
sera-dis-np-10

id8-2-f

id8-3-f

r

.408
.447
.867
.574
.588
.416
.154
.280
.891
.751
.584
.583

  

.522
.406
.754
.174
.696
.522
.464
.464
.812
.696
.522
.522

  
.414
.276
.690
.138
.552
.414
.276
.276
.690
.552
.414
.414

r

.540
0.6
.770
.343
.720
.609
.396
.502
.842
.650
.744
.763

  

.725
.667
.899
.029
.841
.725
.667
.667
.899
.551
.725
.725

  
.552
.414
.828

0

.690
.552
.414
.414
.828
.414
.552
.552

table 3: correlation between sera and id8 scores.
np: query reformulation with noun phrases; kw: query
reformulation with keywords; dis: discounted variant of
sera; the numbers in front of the sera metrics indicate
the rank cut-off point.

in fact,

interestingly, we observe that many variants of id8
scores do not have high correlations with human pyramid
scores. the lowest f-score correlations are for id8-1
and id8-l (with r=0.454). weak correlation of
id8-1 shows that matching unigrams between the
candidate summary and gold summaries is not accurate
in quantifying the quality of the summary. on higher
order id165s, however, we can see that id8 correlates
the highest overall r is
better with pyramid.
obtained by id8-3.
id8-l and its weighted
version id8-w, both have weak correlations with
pyramid. skip-bigrams (id8-s) and its combination
with unigrams
(id8-su) also show sub-optimal
correlations. note that    and    correlations are more
reliable in our setup due to the small sample size.
these results con   rm our initial hypothesis that id8
is not accurate estimator of the quality of the summary
in scienti   c summarization. we attribute this to the
differences of
scienti   c summarization with general
domain summaries. when humans summarize a relatively
long research paper, they might use different terminology
and id141. therefore, id8 which only relies on
term matching between a candidate and a gold summary,
is not accurate in quantifying the quality of the candidate
summary.
5.3. correlation of sera with id8
table 3 shows correlations of our metric sera with
id8-2 and id8-3, which are the highest correlated
id8 variants with pyramid. we can see that in general,
the correlation is not strong. keyword based reduction
variants are the only variants for which the correlation with
id8 is high. looking at the correlations of kw variants
of sera with pyramid (table 2, bottom part), we observe
that these variants are also highly correlated with manual
evaluation.
5.4. effect of the rank cut-off point
finally, figure 1 shows    correlation of different variants
of sera with pyramid based on selection of different
cut-off points (r and    correlations result in very similar

since introduction of id8, there have been other efforts
for improving id54 evaluation. hovy
et al. (2006) proposed an approach based on comparison
of so called basic elements (be) between the candidate
and reference summaries. bes were extracted based
on syntactic structure of the sentence.
the work by
conroy et al.
(2011) was another attempt for improving
id8 for update summarization which combined two
different id8 variants and showed higher correlations
with manual judgments for tac 2008 update summaries.
apart from the content, other aspects of summarization
such as linguistic quality have been also studied. pitler et
al.
(2010) evaluated a set of models based on syntactic
features,
language models and entity coherences for
assessing the linguistic quality of the summaries. machine
translation id74 such as blue have also been
compared and contrasted against id8 (graham, 2015).
despite these works, when gold-standard summaries are
available, id8 is still the most common evaluation
metric that is used in the summarization published research.
apart from id8   s initial good results on the newswire
data,
the availability of the software and its ef   cient
performance have further contributed to its popularity.

7. conclusions

we provided an analysis of existing id74
for scienti   c summarization with evaluation of all variants
of id8. we showed that id8 may not be
the best metric for summarization evaluation; especially
in summaries with high terminology variations and
id141 (e.g. scienti   c summaries). furthermore, we
showed that different variants of id8 result in different
correlation values with human judgments, indicating that
not all id8 scores are equally effective. among
all variants of id8, id8-2 and id8-3 are
better correlated with manual judgments in the context
of scienti   c summarization. we furthermore proposed
an alternative and more effective approach for scienti   c
summarization evaluation (summarization evaluation by
relevance analysis - sera). results revealed that in
general, the proposed evaluation metric achieves higher
correlations with semi-manual pyramid evaluation scores
in comparison with id8.
our analysis on the effectiveness of evaluation measures
for scienti   c summaries was performed using correlations
with manual judgments. an alternative approach to follow
would be to use statistical signi   cance testing on the ability
of the metrics to distinguish between the summarizers
(similar to rankel et al.
(2011)). we studied the
effectiveness of existing summarization id74
in the scienti   c text genre and proposed an alternative
superior metric. another extension of this work would
be to evaluate id54 evaluation in other
genres of text (such as social media). our proposed method
only evaluates the content quality of the summary. similar
to most of existing summarization id74,
other qualities such as linguistic cohesion, coherence and
readability are not captured by this method. developing
metrics that also incorporate these qualities is yet another
future direction to follow.

figure 1:    correlation of sera with pyramid based on
different cut-off points. the x-axis shows the cut-off
point parameter. dis: discounted variant of sera; np:
query reformulation with noun phrases; kw: query
reformulation with keywords.

graphs). when the cut-off point increases, more documents
are retrieved for the candidate and the gold summaries,
and therefore the    nal sera score is more    ne-grained.
a general observation is that as the search cut-off point
increases, the correlation with pyramid scores decreases.
this is because when the retrieved result list becomes
larger, the id203 of including less related documents
increases which negatively affects correct estimation of the
similarity of the candidate and gold summaries. the most
accurate estimations are for metrics with cut-off points of
5 and 10 which are included in the reported results of all
variants in table 2.

6. related work

id8 (lin, 2004) assesses the content quality of a
candidate summary with respect to a set of human gold
summaries based on their lexical overlaps. id8 consists
of several variants. since its introduction, id8 has
been one of the most widely reported metrics in the
summarization literature, and its high adoption has been
due to its high correlation with human assessment scores
in duc datasets (lin, 2004). however, later research has
casted doubts about the accuracy of id8 against manual
evaluations. conroy and dang (2008) analyzed duc 2005
to 2007 data and showed that while some systems achieve
high id8 scores with respect to human summaries, the
linguistic and responsiveness scores of those systems do not
correspond to the high id8 scores.
we studied the effectiveness of id8 through correlation
analysis with manual scores. besides correlation with
human assessment scores, other approaches have been
explored for analyzing the effectiveness of summarization
evaluation. rankel et al. (2011) studied the extent to which
a metric can distinguish between the human and system
generated summaries. they also proposed the use of paired
two-sample t-tests and the wilcoxon signed-rank test as an
alternative to id8 in evaluating several summarizers.
similarly, owczarzak et al.
(2012) proposed the use
of multiple binary signi   cance tests between the system
summaries for ranking the best summarizers.

51020501002005000.00.20.40.60.81.0sera-dissera-dis-npsera-npsera-dis-kwacknowledgments

we would like to thank all three anonymous reviewers for
their feedback and comments, and maryam iranmanesh for
helping in annotation. this work was partially supported
by national science foundation (nsf) through grant
cns-1204347.

8. bibliographical references

abu-jbara, a. and radev, d.

coherent
citation-based summarization of scienti   c papers.
in
acl    11, pages 500   509. association for computational
linguistics.

(2011).

blei, d. m., ng, a. y., and jordan, m. i. (2003). latent
the journal of machine learning

dirichlet allocation.
research, 3:993   1022.

carbonell, j. and goldstein, j. (1998). the use of mmr,
diversity-based reranking for reordering documents and
producing summaries. in sigir, pages 335   336. acm.
(2015). scienti   c article
summarization using citation context and article   s
in emnlp, pages 390   400.
discourse structure.
association for computational linguistics.

cohan, a. and goharian, n.

conroy, j. m. and dang, h. t.

(2008). mind the
gap: dangers of divorcing evaluations of summary
in proceedings of
content
the 22nd international conference on computational
linguistics-volume 1, pages 145   152. association for
computational linguistics.

from linguistic quality.

conroy, j. m., schlesinger, j. d., and o   leary, d. p.
(2011). nouveau-id8: a novelty metric for update
summarization. computational linguistics, 37(1):1   8.

deerwester, s., dumais, s. t., furnas, g. w., landauer,
t. k., and harshman, r.
indexing by latent
semantic analysis. journal of the american society for
information science, 41(6):391.

(1990).

erkan, g. and radev, d. r. (2004). lexrank: graph-based
lexical centrality as salience in text summarization. j.
artif. intell. res.(jair), 22(1):457   479.

giannakopoulos, g.

and karkaletsis, v.

(2013).
summary evaluation: together we stand npower-ed.
in computational linguistics and intelligent text
processing, pages 436   450. springer.

y.

(2015).

graham,

re-evaluating

automatic
summarization with id7 and 192 shades of id8.
in emnlp    15   , pages 128   137, lisbon, portugal,
september. association for computational linguistics.
(2009). exploring
content models for id57.
in naacl-hlt    09, pages 362   370. association for
computational linguistics.

haghighi, a. and vanderwende, l.

hovy, e., lin, c.-y., zhou, l., and fukumoto,

j.
(2006). automated summarization evaluation with basic
elements. in lrec    06, pages 604   611. citeseer.

lin, c.-y.

(2004). id8: a package for automatic
summarization
evaluation of
branches out: proceedings of the acl-04 workshop,
volume 8.

summaries.

in text

luhn, h. p. (1958). the automatic creation of literature
ibm journal of research and development,

abstracts.
2(2):159   165.

nenkova, a. and passonneau, r.

(2004). evaluating
content selection in summarization:
the pyramid
method. in proceedings of the north american chapter
of
the association for computational linguistics:
hlt-naacl 2004.

nenkova, a., passonneau, r., and mckeown, k. (2007).
the pyramid method:
incorporating human content
selection variation in summarization evaluation. acm
transactions on speech and language processing
(tslp), 4(2):4.

owczarzak, k. and dang, h. t. (2011). overview of the
tac 2011 summarization track: guided task and aesop
task. in tac 2011.

owczarzak, k., conroy,

j. m., dang, h. t., and
nenkova, a. (2012). an assessment of the accuracy of
automatic evaluation in summarization. in proceedings
of workshop on id74 and system
comparison for id54, pages 1   9.
association for computational linguistics.

papineni, k., roukos, s., ward, t., and zhu, w.-j. (2002).
id7: a method for automatic evaluation of machine
translation. in acl    02, pages 311   318. association for
computational linguistics.

pitler, e., louis, a., and nenkova, a. (2010). automatic
evaluation of
linguistic quality in multi-document
summarization. in proceedings of the acl 2010, pages
544   554. association for computational linguistics.

qazvinian, v., radev, d. r., mohammad, s., dorr, b. j.,
zajic, d. m., whidby, m., and moon, t.
(2013).
generating extractive summaries of scienti   c paradigms.
j. artif. intell. res.(jair), 46:165   201.

rankel, p., conroy, j. m., slud, e. v., and o   leary, d. p.
(2011). ranking human and machine summarization
systems. emnlp    11, pages 467   473, stroudsburg, pa,
usa. association for computational linguistics.

steinberger, j. and jezek, k. (2004). using latent semantic
analysis in text summarization and summary evaluation.
in proc. isim   04, pages 93   100.

teufel, s. and moens, m. (2002). summarizing scienti   c
experiments with relevance and rhetorical

articles:
status. computational linguistics, 28(4):409   445.

turney, p. d., pantel, p., et al. (2010). from frequency to
meaning: vector space models of semantics. journal of
arti   cial intelligence research, 37(1):141   188.

vanderwende, l., suzuki, h., brockett, c.,

and
nenkova, a. (2007). beyond sumbasic: task-focused
summarization with sentence simpli   cation and lexical
information processing & management,
expansion.
43(6):1606   1618.

zhai, c. and lafferty, j. (2001). a study of smoothing
methods for
language models applied to ad hoc
information retrieval. in proceedings of the 24th annual
international acm sigir conference on research and
development in information retrieval, pages 334   342.
acm.

