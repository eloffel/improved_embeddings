learning-based single-document summarization with

compression and anaphoricity constraints

greg durrett

computer science division

uc berkeley

taylor berg-kirkpatrick
school of computer science
carnegie mellon university

dan klein

computer science division

uc berkeley

6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

2
v
7
8
8
8
0

.

3
0
6
1
:
v
i
x
r
a

gdurrett@cs.berkeley.edu

tberg@cs.cmu.edu

klein@cs.berkeley.edu

abstract

summarization

we present a discriminative model for
single-document
that
compression and
integrally combines
anaphoricity constraints.
our model
selects textual units to include in the
summary based on a rich set of sparse
features whose weights are learned on a
large corpus. we allow for the deletion
of content within a sentence when that
deletion is licensed by compression rules;
in our framework, these are implemented
as dependencies between subsentential
units of text. anaphoricity constraints
then improve cross-sentence coherence
by guaranteeing that, for each pronoun
included in the summary, the pronoun   s
antecedent
is included as well or the
pronoun is rewritten as a full mention.
when trained end-to-end, our    nal sys-
tem1 outperforms prior work on both
id8 as well as on human judgments
of linguistic quality.

introduction

1
while id57 is well-
studied in the nlp literature (carbonell and gold-
stein, 1998; gillick and favre, 2009; lin and
bilmes, 2011; nenkova and mckeown, 2011),
single-document summarization (mckeown et al.,
1995; marcu, 1998; mani, 2001; hirao et al.,
2013) has received less attention in recent years
and is generally viewed as more dif   cult. con-
tent selection is tricky without redundancy across
multiple input documents as a guide and sim-
ple positional information is often hard to beat
(penn and zhu, 2008).
in this work, we tackle
the single-document problem by training an ex-
pressive summarization model on a large nat-

1available at http://nlp.cs.berkeley.edu

urally occurring corpus   the new york times
annotated corpus (sandhaus, 2008) which con-
tains around 100,000 news articles with abstrac-
tive summaries   learning to select important con-
tent with lexical features. this corpus has been
explored in related contexts (dunietz and gillick,
2014; hong and nenkova, 2014), but
to our
knowledge it has not been directly used for single-
document summarization.

to increase the expressive capacity of our
model we allow more aggressive compression of
individual sentences by combining two different
formalisms   one syntactic and the other discur-
sive. additionally, we incorporate a model of
id2 and give our system the abil-
ity rewrite pronominal mentions, further increas-
ing expressivity. in order to guide the model, we
incorporate (1) constraints from coreference en-
suring that critical pronoun references are clear in
the    nal summary and (2) constraints from syntac-
tic and discourse parsers ensuring that sentence re-
alizations are well-formed. despite the complex-
ity of these additional constraints, we demonstrate
an ef   cient id136 procedure using an ilp-
based approach. by training our full system end-
to-end on a large-scale dataset, we are able to learn
a high-capacity structured model of the summa-
rization process, contrasting with past approaches
to the single-document task which have typically
been heuristic in nature (daum  e and marcu, 2002;
hirao et al., 2013).

we focus our evaluation on the new york times
annotated corpus (sandhaus, 2008). according to
id8, our system outperforms a document pre-
   x baseline, a bigram coverage baseline adapted
from a strong multi-document system (gillick and
favre, 2009), and a discourse-informed method
from prior work (yoshida et al., 2014).
impos-
ing discursive and referential constraints improves
human judgments of linguistic clarity and ref-
erential structure   outperforming the method of

figure 1: ilp formulation of our single-document summarization model. the basic model extracts a set of textual units with
binary variables xunit subject to a length constraint. these textual units u are scored with weights w and features f. next, we
add constraints derived from both syntactic parses and rhetorical structure theory (rst) to enforce grammaticality. finally,
we add anaphora constraints derived from coreference in order to improve summary coherence. we introduce additional binary
variables xref that control whether each pronoun is replaced with its antecedent using a candidate replacement rij. these are
also scored in the objective and are incorporated into the length constraint.

yoshida et al. (2014) and approaching the clar-
ity of a sentence-extractive baseline   and still
achieves substantially higher id8 score than
either method. these results indicate that our
model has the expressive capacity to extract im-
portant content, but is suf   ciently constrained to
ensure    uency is not sacri   ced as a result.

past work has explored various kinds of struc-
ture for summarization. some work has focused
on improving content selection using discourse
structure (louis et al., 2010; hirao et al., 2013),
topical structure (barzilay and lee, 2004), or re-
lated techniques (mithun and kosseim, 2011).
other work has used structure primarily to re-
order summaries and ensure coherence (barzilay
et al., 2001; barzilay and lapata, 2008; louis and
nenkova, 2012; christensen et al., 2013) or to
represent content for sentence fusion or abstrac-
tion (thadani and mckeown, 2013; pighin et al.,
2014). similar to these approaches, we appeal
to structures from upstream nlp tasks (syntactic
parsing, rst parsing, and coreference) to restrict
our model   s capacity to generate. however, we go
further by optimizing for id8 subject to these
constraints with end-to-end learning.

2 model

our model is shown in figure 1. broadly, our
ilp takes a set of textual units u = (u1, . . . , un)
from a document and    nds the highest-scoring
extractive summary by optimizing over variables

1

n

, . . . , xunit

, which are binary in-
xunit = xunit
dicators of whether each unit is included. tex-
tual units are contiguous parts of sentences that
serve as the fundamental units of extraction in
our model. for a sentence-extractive model, these
would be entire sentences, but for our compressive
models we will have more    ne-grained units, as
shown in figure 2 and described in section 2.1.
textual units are scored according to features f
and model parameters w learned on training data.
finally, the extraction process is subject to a length
constraint of k words. this approach is similar
in spirit to ilp formulations of multi-document
summarization systems, though in those systems
content is typically modeled in terms of bigrams
(gillick and favre, 2009; berg-kirkpatrick et al.,
2011; hong and nenkova, 2014; li et al., 2015).
for our model, type-level id165 scoring only
arises when we compute our id168 in max-
margin training (see section 3).

in section 2.1, we discuss grammaticality con-
straints, which take the form of introducing de-
pendencies between textual units, as shown in fig-
ure 2. if one textual unit requires another, it can-
not be included unless its prerequisite is. we will
show that different sets of requirements can cap-
ture both syntactic and discourse-based compres-
sion schemes.

furthermore, we introduce anaphora constraints
(section 2.2) via a new set of variables that capture
the process of rewriting pronouns to make them

8i,kxuniti   xunitkif9jwithxrefij=0wheretheantecedentofrijisinuk8jxrefij=1i   nopriorincludedtextualunitmentionstheentitythatrijrefersto8i,kxuniti   xunitkifuirequiresukonthebasisofpronounanaphora8i,kxuniti   xunitkifuirequiresuk{x(i,j)   xrefij(w>f(rij))   ++x(i,j)xrefij(|rij| 1)xixuniti|ui|xi   xuniti(w>f(ui))   extraction scoregrammaticality constraints (section 2.1)anaphora scoresubjecttohianaphora constraints (section 2.2)   klength adjustment for explicit mentionlength constraintmaxxunit,xreffigure 2: compression constraints on an example sentence. (a) rst-based compression structure like that in hirao et al.
(2013), where we can delete the elaboration clause. (b) two syntactic compression options from berg-kirkpatrick et al.
(2011), namely deletion of a coordinate and deletion of a pp modi   er. (c) textual units and requirement relations (arrows) after
merging all of the available compressions. (d) process of augmenting a textual unit with syntactic compressions.

explicit mentions. that is, xref
ij = 1 if we should
rewrite the jth pronoun in the ith unit with its an-
tecedent. these pronoun rewrites are scored in the
objective and introduced into the length constraint
to make sure they do not cause our summary to
be too long. finally, constraints on these variables
control when they are used and also require the
model to include antecedents of pronouns when
the model is not con   dent enough to rewrite them.

2.1 grammaticality constraints
following work on isolated sentence compression
(mcdonald, 2006; clarke and lapata, 2008) and
compressive summarization (lin, 2003; martins
and smith, 2009; berg-kirkpatrick et al., 2011;
woodsend and lapata, 2012; almeida and mar-
tins, 2013), we wish to be able to compress sen-
tences so we can pack more information into a
summary. during training, our model learns how
to take advantage of available compression options
and select content to match human generated sum-
maries as closely possible.2 we explore two ways
of deriving units for compression: the rst-based
compressions of hirao et al. (2013) and the syntac-
tic compressions of berg-kirkpatrick et al. (2011).
rst compressions figure 2a shows how to de-
rive compressions from rhetorical structure the-
ory (mann and thompson, 1988; carlson et al.,
2001). we show a sentence broken into elemen-

2the features in our model are actually rich enough to
learn a sophisticated compression model, but the data we
have (abstractive summaries) does not directly provide ex-
amples of correct compressions; past work has gotten around
this with id72 (almeida and martins, 2013),
but we simply treat grammaticality as a constraint from up-
stream models.

tary discourse units (edus) with rst relations
between them. units marked as same-unit must
both be kept or both be deleted, but other nodes in
the tree structure can be deleted as long as we do
not delete the parent of an included node. for ex-
ample, we can delete the elaboration clause,
but we can delete neither the    rst nor last edu.
arrows depict the constraints this gives rise to in
the ilp (see figure 1): u2 requires u1, and u1 and
u3 mutually require each other. this is a more con-
strained form of compression than was used in past
work (hirao et al., 2013), but we    nd that it im-
proves human judgments of    uency (section 4.3).

syntactic compressions figure 2b shows two
examples of compressions arising from syntactic
patterns (berg-kirkpatrick et al., 2011): deletion
of the second part of a coordinated np and dele-
tion of a pp modi   er to an np. these patterns were
curated to leave sentences as grammatical after be-
ing compressed, though perhaps with damaged se-
mantic content.

combined compressions figure 2c shows the
textual units and requirement relations yielded by
combining these two types of compression. on
this example, the two schemes capture orthogo-
nal compressions, and more generally we    nd that
they stack to give better results for our    nal sys-
tem (see section 4.3). to actually synthesize tex-
tual units and the constraints between them, we
start from the set of rst textual units and intro-
duce syntactic compressions as new children when
they don   t cross existing brackets; because syntac-
tic compressions are typically narrower in scope,
they are usually completely contained in edus.

ms. johnson, dressed in jeans  and a sweatshirt  , is a claims adjuster  with aetna  .npccnpnpnpnpppsame-unit(b) syntactic compressions(a) discourse compressionsms. johnson  , dressed in jeans and a sweatshirt ,  is a claims adjuster with aetna .elaboration(c) combined compressionsms. johnson  , dressed in jeans  and a sweatshirt  ,  is a claims adjuster  with aetna  .u1u2u3u4u5u1u2u3u1u2u3u4u5u6u7(d) augmentation processis a claims adjuster  with aetna  .is a claims adjuster  with aetna  .infigure 2d shows an example of this process: the
possible deletion of with aetna is grafted onto the
textual unit and appropriate requirement relations
are introduced. the net effect is that the textual
unit is wholly included, partially included (with
aetna removed), or not at all.

formally, we de   ne an rst tree as trst =
(srst,   rst) where srst is a set of edu spans (i, j)
and    : s     2s is a mapping from each edu span
to edu spans it depends on. syntactic compres-
sions can be expressed in a similar way with trees
tsyn. these compressions are typically smaller-
scale than edu-based compressions, so we use
the following modi   cation scheme. denote by
tsyn(kl) a nontrivial (supports some compression)
subtree of tsyn that is completely contained in an
edu (i, j). we build the following combined
compression tree, which we refer to as the aug-
mentation of trst with tsyn(kl):

tcomb = (s     ssyn(kl)     {(i, k), (l, j)},   rst       syn(kl)   
{(i, k)     (l, j), (l, j)     (i, k), (k, l)     (i, k)})

that is, we maintain the existing tree structure ex-
cept for the edu (i, j), which is broken into three
the outer two depend on each other (is a
parts:
claims adjuster and . from figure 2d) and the in-
ner one depends on the others and preserves the
tree structure from tsyn. we augment trst with all
maximal subtrees of tsyn, i.e. all trees that are not
contained in other trees that are used in the aug-
mentation process.

this is broadly similar to the combined com-
pression scheme in kikuchi et al. (2014) but we
use a different set of constraints that more strictly
enforce grammaticality.3

2.2 anaphora constraints
what kind of cross-sentential coherence do we
need to ensure for the kinds of summaries our
system produces? many notions of coherence
are useful, including centering theory (grosz et
al., 1995) and lexical cohesion (nishikawa et al.,
2014), but one of the most pressing phenomena to
deal with is pronoun anaphora (clarke and lapata,
2010). cases of pronouns being    orphaned    dur-
ing extraction (their antecedents are deleted) are

3we also differ from past work in that we do not use cross-
sentential rst constraints (hirao et al., 2013; yoshida et al.,
2014). we experimented with these and found no improve-
ment from using them, possibly because we have a feature-
based model rather than a heuristic content selection proce-
dure, and possibly because automatic discourse parsers are
less good at recovering cross-sentence relations.

figure 3: modi   cations to the ilp to capture pronoun coher-
it, which refers to kellogg, has several possible an-
ence.
tecedents from the standpoint of an automatic coreference
system (durrett and klein, 2014).
if the coreference sys-
tem is con   dent about its selection (above a threshold    on
the posterior id203), we allow for the model to explic-
itly replace the pronoun if its antecedent would be deleted
(section 2.2.1). otherwise, we merely constrain one or more
probable antecedents to be included (section 2.2.2); even if
the coreference system is incorrect, a human can often cor-
rectly interpret the pronoun with this additional context.

relatively common: they occur in roughly 60% of
examples produced by our summarizer when no
anaphora constraints are enforced. this kind of
error is particularly concerning for summary inter-
pretation and impedes the ability of summaries to
convey information effectively (grice, 1975). our
solution is to explicitly impose constraints on the
model based on pronoun id2.4

figure 3 shows an example of a problem case.
if we extract only the second textual unit shown,
the pronoun it will lose its antecedent, which in
this case is kellogg. we explore two types of con-
straints for dealing with this: rewriting the pro-
noun explicitly, or constraining the summary to in-
clude the pronoun   s antecedent.

2.2.1 pronoun replacement
one way of dealing with these pronoun reference
issues is to explicitly replace the pronoun with
what it refers to. this replacement allows us to
maintain maximal extraction    exibility, since we

4we focus on pronoun coreference because it is the most
pressing manifestation of this problem and because existing
coreference systems perform well on pronouns compared to
harder instances of coreference (durrett and klein, 2013).

this hasn   t been  kellogg   s   year .replacement (2.2.1): if                                      :the  oat-bran craze  has cost  it  market share.otherwise (i.e. if no replacement is possible):xunit2   xunit1u1u2p1p2p3allow pronoun replacement with the predicted	

antecedent and add the following constraint:add the following constraint:kelloggityearitno replacement	

necessaryreplace the    rst pronoun in the second textual unitmax(p1,p2,p3)>   p1+p2> antecedent inclusion (2.2.2): ifxref2,1=1i   xunit1=0andxunit2=1can make an isolated textual unit meaningful even
if it contains a pronoun. figure 3 shows how this
process works. we run the berkeley entity reso-
lution system (durrett and klein, 2014) and com-
pute posteriors over possible links for the pronoun.
if the coreference system is suf   ciently con   dent
in its prediction (i.e. maxi pi >    for a speci-
   ed threshold    > 1
2), we allow ourselves to re-
place the pronoun with the    rst mention of the en-
tity corresponding to the pronoun   s most likely an-
tecedent. in figure 3, if the system correctly deter-
mines that kellogg is the correct antecedent with
high id203, we enable the    rst replacement
shown there, which is used if u2 is included the
summary without u1.5

as shown in the ilp in figure 1, we instanti-
ate corresponding pronoun replacement variables
xref where xref
ij = 1 implies that the jth pronoun
in the ith sentence should be replaced in the sum-
mary. we use a candidate pronoun replacement
if and only if the pronoun   s corresponding (pre-
dicted) entity hasn   t been mentioned previously in
the summary.6 because we are generally replac-
ing pronouns with longer mentions, we also need
to modify the length constraint to take this into
account. finally, we incorporate features on pro-
noun replacements in the objective, which helps
the model learn to prefer pronoun replacements
that help it to more closely match the human sum-
maries.

2.2.2 pronoun antecedent constraints
explicitly replacing pronouns is risky: if the coref-
erence system makes an incorrect prediction, the
intended meaning of the summary may be dam-
aged. fortunately, the coreference model   s pos-
terior probabilities have been shown to be well-
calibrated (nguyen and o   connor, 2015), mean-
ing that cases where it is likely to make errors are
signaled by    atter posterior distributions. in this
case, we enable a more conservative set of con-
straints that include additional content in the sum-
mary to make the pronoun reference clear without
explicitly replacing it. this is done by requiring
the inclusion of any textual unit which contains

5if the proposed replacement is a proper mention, we re-
place the pronoun just with the subset of the mention that con-
stitutes a named entity (rather than the whole noun phrase).
we control for possessive pronouns by deleting or adding    s
as appropriate.

6such a previous mention may be a pronoun; however,
note that that pronoun would then be targeted for replacement
unless its antecedent were included somehow.

possible pronoun references whose posteriors sum
to at least a threshold parameter   . figure 3 shows
that this constraint can force the inclusion of u1 to
provide additional context. although this could
still lead to unclear pronouns if text is stitched to-
gether in an ambiguous or even misleading way, in
practice we observe that the textual units we force
to be added almost always occur very recently be-
fore the pronoun, giving enough additional context
for a human reader to    gure out the pronoun   s an-
tecedent unambiguously.

2.3 features
the features in our model (see figure 1) consist of
a set of surface indicators capturing mostly lex-
ical and con   gurational information. their pri-
mary role is to identify important document con-
tent. the    rst three types of features    re over tex-
tual units, the last over pronoun replacements.

lexical these include indicator features on non-
stopwords in the textual unit that appear at least
   ve times in the training set and analogous pos
features. we also use lexical features on the    rst,
last, preceding, and following words for each tex-
tual unit. finally, we conjoin each of these fea-
tures with an indicator of bucketed position in the
document (the index of the sentence containing the
textual unit).

structural these features include various con-
junctions of the position of the textual unit in the
document, its length, the length of its correspond-
ing sentence, the index of the paragraph it occurs
in, and whether it starts a new paragraph (all val-
ues are bucketed).

centrality these features capture rough infor-
mation about the centrality of content: they consist
of bucketed word counts conjoined with bucketed
sentence index in the document. we also    re fea-
tures on the number of times of each entity men-
tioned in the sentence is mentioned in the rest of
the document (according to a coreference system),
the number of entities mentioned in the sentence,
and surface properties of mentions including type
and length

pronoun replacement these target properties
of the pronoun replacement such as its length, its
sentence distance from the current mention, its
type (nominal or proper), and the identity of the
pronoun being replaced.

3 learning
we learn weights w for our model by training on
a large corpus of documents u paired with ref-
erence summaries y. we formulate our learning
problem as a standard instance of structured id166
(see smith (2011) for an introduction). because
we want to optimize explicitly for id8-1,7 we
de   ne a id8-based id168 that accom-
modates the nature of our supervision, which is in
terms of abstractive summaries y that in general
cannot be produced by our model. speci   cally,
we take:
(cid:96)(xngram, y) = maxx    id8-1(x   , y)     id8-1(xngram, y)

i.e. the gap between the hypothesis   s id8
score and the oracle id8 score achievable
under the model (including constraints). here
xngram are indicator variables that track, for each
id165 type in the reference summary, whether
that id165 is present in the system summary.
these are the suf   cient statistics for computing
id8.

we train the model via stochastic subgradient
descent on the primal form of the structured id166
objective (ratliff et al., 2007; kummerfeld et al.,
2015). in order to compute the subgradient for a
given training example, we need to    nd the most
violated constraint on the given instance through a
loss-augmented decode, which for a linear model
takes the form arg maxx w(cid:62)f (x) + (cid:96)(x, y). to do
this decode at training time in the context of our
model, we use an extended version of our ilp in
figure 1 that is augmented to explicitly track type-
level id165s:

max

xunit ,xref ,xngram

(cid:88)

+

(cid:62)

xunit
i

(w

f (ui))

(cid:105)     (cid:96)(xngram, y)

      

xref
ij (w

(cid:62)

f (rij))

(i,j)

subject to all constraints from figure 1, and
xngram
i

= 1 iff an included textual unit or replacement

contains the ith reference id165

these kinds of variables and constraints are com-
mon in id57 systems
7we found that optimizing for id8-1 actually resulted
in a model with better performance on both id8-1 and
id8-2. we hypothesize that this is because framing our
optimization in terms of id8-2 would lead to a less nu-
anced set of constraints: bigram matches are relatively rare
when the reference is a short, abstractive summary, so a loss
function based on id8-2 will express a    atter preference
structure among possible outputs.

(cid:104)

(cid:34)(cid:88)
(cid:104)

i

(cid:105)

that score bigrams (gillick and favre, 2009 in-
ter alia). note that since id8 is only com-
puted over non-stopword id165s and pronoun
replacements only replace pronouns, pronoun re-
placement can never remove an id165 that would
otherwise be included.

for all experiments, we optimize our objective
using adagrad (duchi et al., 2011) with (cid:96)1 regu-
larization (   = 10   8, chosen by grid search), with
a step size of 0.1 and a minibatch size of 1. we
train for 10 iterations on the training data, at which
point held-out model performance no longer im-
proves. finally, we set the anaphora thresholds
   = 0.8 and    = 0.6 (see section 2.2). the val-
ues of these and other hyperparameters were de-
termined on a held-out development set from our
new york times training data. all ilps are solved
using glpk version 4.55.

4 experiments
we primarily evaluate our model on a roughly
3000-document evaluation set from the new york
times annotated corpus (sandhaus, 2008). we
also investigate its performance on the rst dis-
course treebank (carlson et al., 2001), but be-
cause this dataset is only 30 documents it pro-
vides much less robust estimates of performance.8
throughout this section, when we decode a docu-
ment, we set the word budget for our summarizer
to be the same as the number of words in the corre-
sponding reference summary, following previous
work (hirao et al., 2013; yoshida et al., 2014).

4.1 preprocessing
we preprocess all data using the berkeley parser
(petrov et al., 2006),
speci   cally the gpu-
accelerated version of the parser from hall et al.
(2014), and the berkeley entity resolution sys-
tem (durrett and klein, 2014). for rst discourse
analysis, we segment text into edus using a semi-
markov crf trained on the rst treebank with
features on boundaries similar to those of hernault
et al. (2010), plus novel features on spans includ-
ing span length and span identity for short spans.
to follow the conditions of yoshida et al. (2014)
as closely as possible, we also build a discourse
parser in the style of hirao et al. (2013), since
their parser is not publicly available. speci   cally,

8tasks like duc and tac have focused on multi-
document summarization since around 2003, hence the lack
of more standard datasets for single-document summariza-
tion.

figure 4: examples of an article kept in the nyt50 dataset (top) and an article removed because the summary is too short.
the top summary has a rich structure to it, corresponding to various parts of the document (bolded) and including some text
that is essentially a direct extraction.

110,540 articles with abstractive summaries; we
split these into 100,834 training and 9706 test ex-
amples, based on date of publication (test is all
articles published on january 1, 2007 or later).
examples of two documents from this dataset
are shown in figure 4. the bottom example
demonstrates that some summaries are extremely
short and formulaic (especially those for obituar-
ies and editorials). to counter this, we    lter the
raw dataset by removing all documents with sum-
maries that are shorter than 50 words. one bene   t
of    ltering is that the length distribution of our re-
sulting dataset is more in line with standard sum-
marization evaluations like duc; it also ensures a
suf   cient number of tokens in the budget to pro-
duce nontrivial summaries. the    ltered test set,
which we call nyt50, includes 3,452 test exam-
ples out of the original 9,706.

interestingly, this dataset is one where the clas-
sic document pre   x baseline can be substantially
outperformed, unlike in some other summariza-
tion settings (penn and zhu, 2008). we show this
fact explicitly in section 4.3, but figure 5 provides
additional analysis in this regard. we compute or-
acle id8-1 sentence-extractive summaries on
a 1000-document subset of the training set and
look at where the extracted sentences lie in the
document. while they certainly skew earlier in
the document, they do not all fall within the doc-

figure 5: counts on a 1000-document sample of how fre-
quently both a document pre   x baseline and a id8 ora-
cle summary contain sentences at various indices in the docu-
ment. there is a long tail of useful sentences later in the doc-
ument, as seen by the fact that the oracle sentence counts drop
off relatively slowly. smart selection of content therefore has
room to improve over taking a pre   x of the document.

we use the    rst-order projective parsing model of
mcdonald et al. (2005) and features from soricut
and marcu (2003), hernault et al. (2010), and joty
et al. (2013). when using the same head anno-
tation scheme as yoshida et al. (2014), we out-
perform their discourse dependency parser on un-
labeled dependency accuracy, getting 56% as op-
posed to 53%.

4.2 new york times corpus
we now provide some details about the new york
times annotated corpus. this dataset contains

article on speak-up, program begun by westchester county office for the aging to bring together elderly and college students.national center for education statistics reports students in 4th, 8th and 12th grades scored modestly higher on american history test than five years earlier. says more than half of high school seniors still show poor command of basic facts. only 4th graders made any progress in civics test. new exam results are another ingredient in debate over renewing pres bush   s signature no child left behind act.filtered article: nyt50 article:summary: summary: federal officials reported yesterday that students in 4th, 8th and 12th grades had scored modestly higher on an american history test than five years earlier, although more than half of high school seniors still showed poor command of basic facts like the effect of the cotton gin on the slave economy or the causes of the korean war. federal officials said they considered the results encouraging because at each level tested, student performance had improved since the last time the exam was administered, in 2001.    in u.s. history there were higher scores in 2006 for all three grades,    said mark schneider, commissioner of the national center for education statistics, which administers the test, at a boston news conference that the education department carried by webcast. the results were less encouraging on a national civics test, on which only fourth graders made any progress. the best results in the history test were also in fourth grade, where 70 percent of students attained the basic level of achievement or better. the test results in the two subjects are likely to be closely studied, because congress is considering the renewal of president bush's signature education law, the no child left behind act. a number of studies have shown that because no child left behind requires states   long before president bush's proposal to rethink social security became part of the national conversation, westchester county came up with its own dialogue to bring issues of aging to the forefront. before the white house conference on aging scheduled in october, the county's office for the aging a year ago started speak-up, which stands for student participants embrace aging issues of key concern, to reach students in the county's 13 colleges and universities. through a variety of events to bring together the elderly and college students, organizers said they hoped to have by this spring a series of recommendations that could be given to washington   count02505007501000sentence index in document05101520oracle sentencesfirst k sentences   first sentences
first k words
bigram frequency

tree knapsack

cg     up    

8.21
   
   

0.28
   
   

r-1     r-2    
baselines

28.6
35.7
25.1

17.3
21.6
9.8

past work

this work

34.7

19.6

7.20

0.42

38.8
41.9
42.2

sentence extraction
edu extraction
full
ablations from full
26.3
25.0
24.7

no anaphoricity
no syntactic compr
no discourse compr

42.5
41.1
40.5

23.5
0.32
0.65
25.3
25.9 *   7.52 *0.36

7.93
6.38

7.46
   
   

0.44
   
   

table 1: results on the nyt50 test set (documents with sum-
maries of at least 50 tokens) from the new york times anno-
tated corpus (sandhaus, 2008). we report id8-1 (r-1),
id8-2 (r-2), clarity/grammaticality (cg), and number of
unclear pronouns (up) (lower is better). on content selection,
our system substantially outperforms all baselines, our imple-
mentation of the tree knapsack system (yoshida et al., 2014),
and learned extractive systems with less compression, even
an edu-extractive system that sacri   ces grammaticality. on
clarity metrics, our    nal system performs nearly as well as
sentence-extractive systems. the symbols * and     indicate
statistically signi   cant gains compared to no anaphoricity
and tree knapsack (respectively) with p < 0.05 according to
a bootstrap resampling test. we also see that removing either
syntactic or edu-based compressions decreases id8.

ument pre   x summary. one reason for this is that
many of the articles are longer-form pieces that be-
gin with a relatively content-free lede of several
sentences, which should be identi   able with lexi-
cosyntactic indicators as are used in our discrimi-
native model.

4.3 new york times results
we evaluate our system along two axes:    rst, on
content selection, using id89 (lin and hovy,
2003), and second, on clarity of language and ref-
erential structure, using annotators from amazon
mechanical turk. we follow the method of gillick
and liu (2010) for this evaluation and ask turkers
to rate a summary on how grammatical it is using
a 10-point likert scale. furthermore, we ask how
many unclear pronouns references there were in
the text. the turkers do not see the original docu-
ment or the reference summary, and rate each sum-
mary in isolation. gillick and liu (2010) showed
that for linguistic quality judgments (as opposed to
content judgments), turkers reproduced the rank-
ing of systems according to expert judgments.

to speed up preprocessing and training time

9we use the id8 1.5.5 script with the following com-
mand line arguments: -n 2 -x -m -s. all given results
are macro-averaged recall values over the test set.

on this corpus, we further restrict our training set
to only contain documents with fewer than 100
edus. all told, the    nal system takes roughly 20
hours to make 10 passes through the subsampled
training data (22,000 documents) on a single core
of an amazon ec2 r3.4xlarge instance.

table 1 shows the results on the nyt50 cor-
pus. we compare several variants of our sys-
tem and baselines. for baselines, we use two
variants of    rst k: one which must stop on a
sentence boundary (which gives better linguistic
quality) and one which always consumes k to-
kens (which gives better id8). we also use
a heuristic sentence-extractive baseline that maxi-
mizes the document counts (term frequency) of bi-
grams covered by the summary, similar in spirit to
the multi-document method of gillick and favre
(2009).10 we also compare to our implementa-
tion of the tree knapsack method of yoshida et al.
(2014), which matches their results very closely
on the rst discourse treebank when discourse
trees are controlled for. finally, we compare sev-
eral variants of our system: purely extractive sys-
tems operating over sentences and edus respec-
tively, our full system, and ablations removing ei-
ther the anaphoricity component or parts of the
compression module.

in terms of content selection, we see that all of
the systems that incorporate end-to-end learning
(under    this work   ) substantially outperform our
various heuristic baselines. our full system using
the full compression scheme is substantially better
on id8 than ablations where the syntactic or
discourse compressions are removed. these im-
provements re   ect the fact that more compression
options give the system more    exibility to include
key content words. removing the anaphora res-
olution constraints actually causes id8 to in-
crease slightly (as a result of granting the model
   exibility), but has a negative impact on the lin-
guistic quality metrics.

on our linguistic quality metrics, it is no sur-
prise that the sentence pre   x baseline performs
the best. our sentence-extractive system also does
well on these metrics. compared to the edu-
extractive system with no constraints, our con-
strained compression method improves substan-
tially on both linguistic quality and reduces the

10other heuristic multi-document approaches could be
compared to, e.g. he et al. (2012), but a simple term fre-
quency method suf   ces to illustrate how these approaches can
underperform in the single-document setting.

first k words
tree knapsack
full

id8-1 id8-2
8.3
8.7
8.0

23.5
25.1
26.3

table 2: results for rst discourse treebank (carlson et al.,
2001). differences between our system and the tree knap-
sack system of yoshida et al. (2014) are not statistically sig-
ni   cant, re   ecting the high variance in this small (20 docu-
ment) test set.

number of unclear pronouns, and adding the pro-
noun anaphora constraints gives further improve-
ment. our    nal system is approaches the sentence-
extractive baseline, particularly on unclear pro-
nouns, and achieves substantially higher id8
score.

4.4 rst treebank

we also evaluate on the rst discourse tree-
bank, of which 30 documents have abstractive
summaries. following hirao et al. (2013), we use
the gold edu segmentation from the rst corpus
but automatic rst trees. we break this into a 10-
document development set and a 20-document test
set. table 2 shows the results on the rst cor-
pus. our system is roughly comparable to tree
knapsack here, and we note that none of the differ-
ences in the table are statistically signi   cant. we
also observed signi   cant variation between multi-
ple runs on this corpus, with scores changing by
1-2 id8 points for slightly different system
variants.11

5 conclusion

we presented a single-document summarization
system trained end-to-end on a large corpus. we
integrate a compression model that enforces gram-
maticality as well as pronoun anaphoricity con-
straints that enforce coherence. our system im-
proves substantially over baseline systems on
id8 while still maintaining good linguistic
quality.

our system and models are publicly available at

http://nlp.cs.berkeley.edu

acknowledgments

this work was partially supported by nsf grant
cns-1237265 and a google faculty research
award. thanks to tsutomu hirao for providing
assistance with our reimplementation of the tree
knapsack model, and thanks the anonymous re-
viewers for their helpful comments.

references
miguel almeida and andre martins. 2013. fast and
robust compressive summarization with dual de-
composition and id72. in proceed-
ings of the association for computational linguis-
tics (acl).

regina barzilay and mirella lapata. 2008. modeling
local coherence: an entity-based approach. com-
putational linguistics, 34(1):1   34, march.

regina barzilay and lillian lee. 2004. catching the
drift: probabilistic content models, with applica-
tions to generation and summarization. in proceed-
ings of the north american chapter of the associa-
tion for computational linguistics (naacl).

regina barzilay, noemie elhadad, and kathleen r.
mckeown. 2001. sentence ordering in multidocu-
ment summarization. in proceedings of the interna-
tional conference on human language technology
research.

taylor berg-kirkpatrick, dan gillick, and dan klein.
2011. jointly learning to extract and compress. in
proceedings of the association for computational
linguistics (acl).

jaime carbonell and jade goldstein. 1998. the use
of mmr, diversity-based reranking for reorder-
ing documents and producing summaries. in pro-
ceedings of the international acm sigir confer-
ence on research and development in information
retrieval.

lynn carlson, daniel marcu,

and mary ellen
okurowski.
2001. building a discourse-tagged
corpus in the framework of rhetorical structure
in proceedings of the second sigdial
theory.
workshop on discourse and dialogue.

janara christensen, mausam, stephen soderland, and
2013. towards coherent multi-
in proceedings of the
the association for

oren etzioni.
document summarization.
north american chapter of
computational linguistics (naacl).

11the system of yoshida et al. (2014) is unavailable, so we
use a reimplementation. our results differ from theirs due
to having slightly different discourse trees, which cause large
changes in metrics due to high variance on the test set.

james clarke and mirella lapata. 2008. global in-
ference for sentence compression an integer linear
programming approach. journal of arti   cial intel-
ligence research, 31(1):399   429, march.

james clarke and mirella lapata. 2010. discourse
constraints for document compression. computa-
tional linguistics, 36(3):411   441, september.

hal daum  e, iii and daniel marcu. 2002. a noisy-
channel model for document compression. in pro-
ceedings of the association for computational lin-
guistics (acl).

john duchi, elad hazan, and yoram singer. 2011.
adaptive subgradient methods for online learning
and stochastic optimization. journal of machine
learning research, 12:2121   2159, july.

jesse dunietz and daniel gillick. 2014. a new entity
salience task with millions of training examples.
in proceedings of the european chapter of the as-
sociation for computational linguistics (eacl).

greg durrett and dan klein. 2013. easy victories and
in pro-
uphill battles in coreference resolution.
ceedings of the conference on empirical methods in
natural language processing (emnlp), october.

greg durrett and dan klein. 2014. a joint model
for entity analysis: coreference, typing, and link-
ing. in transactions of the association for compu-
tational linguistics (tacl).

dan gillick and benoit favre.

2009. a scalable
in proceedings
global model for summarization.
of the workshop on integer id135 for
natural language processing.

dan gillick and yang liu. 2010. non-expert eval-
uation of summarization systems is risky.
in
proceedings of the naacl workshop on creating
speech and language data with amazon   s mechan-
ical turk.

h.p. grice. 1975. logic and conversation. syntax and

semantics 3: speech acts, pages 41   58.

barbara j. grosz, scott weinstein, and aravind k.
joshi. 1995. centering: a framework for modeling
the local coherence of discourse. computational
linguistics, 21(2):203   225, june.

tsutomu hirao, yasuhisa yoshida, masaaki nishino,
norihito yasuda, and masaaki nagata.
2013.
single-document summarization as a tree knap-
sack problem. in proceedings of the conference on
empirical methods in natural language processing
(emnlp).

kai hong and ani nenkova. 2014.

improving the
estimation of word importance for news multi-
in proceedings of the
document summarization.
european chapter of the association for computa-
tional linguistics (eacl).

sha   q joty, giuseppe carenini, raymond ng, and
yashar mehdad.
2013. combining intra- and
multi-sentential rhetorical parsing for document-
level discourse analysis. in proceedings of the as-
sociation for computational linguistics (acl).

yuta kikuchi, tsutomu hirao, hiroya takamura, man-
abu okumura, and masaaki nagata. 2014. sin-
gle document summarization based on nested tree
in proceedings of the association for
structure.
computational linguistics (acl).

jonathan k. kummerfeld, taylor berg-kirkpatrick,
and dan klein. 2015. an empirical analysis of
optimization for max-margin nlp. in proceedings
of the conference on empirical methods in natural
language processing (emnlp).

chen li, yang liu, and lin zhao.

2015. using
external resources and joint learning for bigram
weighting in ilp-based multi-document summa-
in proceedings of the north american
rization.
chapter of the association for computational lin-
guistics (naacl).

hui lin and jeff bilmes. 2011. a class of submodu-
lar functions for document summarization. in pro-
ceedings of the association for computational lin-
guistics (acl).

chin-yew lin and eduard hovy.

2003. auto-
matic evaluation of summaries using id165 co-
occurrence statistics. in proceedings of the north
american chapter of the association for computa-
tional linguistics (naacl).

david hall, taylor berg-kirkpatrick, john canny, and
dan klein. 2014. sparser, better, faster gpu pars-
ing. in proceedings of the association for compu-
tational linguistics (acl).

chin-yew lin. 2003. improving summarization per-
formance by sentence compression: a pilot study.
in proceedings of the international workshop on in-
formation retrieval with asian languages.

zhanying he, chun chen, jiajun bu, can wang, lijun
zhang, deng cai, and xiaofei he. 2012. document
summarization based on data reconstruction.
in
proceedings of the association for the advancement
of arti   cial intelligence (aaai).

annie louis and ani nenkova. 2012. a coherence
in proceed-
model based on syntactic patterns.
ings of the joint conference on empirical methods
in natural language processing and computational
natural language learning (emnlp-conll).

hugo hernault, helmut prendinger, david a. duverle,
mitsuru ishizuka, and tim paek. 2010. hilda: a
discourse parser using support vector machine clas-
si   cation. dialogue and discourse, 1:1   33.

annie louis, aravind joshi, and ani nenkova. 2010.
discourse indicators for content selection in sum-
in proceedings of the sigdial 2010
marization.
conference.

daniele pighin, marco cornolti, enrique alfonseca,
and katja filippova.
2014. modelling events
through memory-based, open-ie patterns for ab-
stractive summarization. in proceedings of the as-
sociation for computational linguistics (acl).

2007.

nathan j. ratliff, andrew bagnell, and martin zinke-
vich.
(online) subgradient methods for
id170. in proceedings of the inter-
national conference on arti   cial intelligence and
statistics.

evan sandhaus. 2008. the new york times anno-

tated corpus. in linguistic data consortium.

noah a. smith. 2011. linguistic structure prediction.

morgan & claypool publishers, 1st edition.

radu soricut and daniel marcu. 2003. sentence level
discourse parsing using syntactic and lexical in-
in proceedings of the north american
formation.
chapter of the association for computational lin-
guistics (naacl).

kapil thadani and kathleen mckeown. 2013. super-
vised sentence fusion with single-stage id136.
in proceedings of the international joint conference
on natural language processing (ijcnlp).

kristian woodsend and mirella lapata. 2012. mul-
tiple aspect summarization using integer linear
programming. in proceedings of the joint confer-
ence on empirical methods in natural language
processing and computational natural language
learning (emnlp-conll).

yasuhisa yoshida, jun suzuki, tsutomu hirao, and
masaaki nagata. 2014. dependency-based dis-
course parser for single-document summariza-
in proceedings of the conference on em-
tion.
pirical methods in natural language processing
(emnlp).

inderjeet mani. 2001. automaticsummarization. john

benjamins publishing.

william c. mann and sandra a. thompson. 1988.
rhetorical structure theory: toward a functional
theory of text organization. text, 8(3):243   281.

daniel marcu.

1998.

through rhetorical parsing tuning.
of the workshop on very large corpora.

improving summarization
in proceedings

andre martins and noah a. smith. 2009. summariza-
tion with a joint model for sentence extraction and
in proceedings of the workshop on
compression.
integer id135 for natural language
processing.

ryan mcdonald, koby crammer, and fernando
pereira. 2005. online large-margin training of de-
pendency parsers. in proceedings of the association
for computational linguistics (acl).

ryan mcdonald.

2006. discriminative sentence
compression with soft syntactic evidence. in pro-
ceedings of the european chapter of the association
for computational linguistics (eacl).

kathleen mckeown, jacques robin, and karen ku-
kich. 1995. generating concise natural language
information processing and manage-
summaries.
ment, 31(5):703   733, september.

shamima mithun and leila kosseim. 2011. discourse
structures to reduce discourse incoherence in blog
summarization. in proceedings of recent advances
in natural language processing.

ani nenkova and kathleen mckeown. 2011. auto-
matic summarization. foundations and trends in
information retrieval, 5(2?3):103   233.

khanh nguyen and brendan o   connor. 2015. poste-
rior calibration and exploratory analysis for nat-
ural language processing models. in proceedings
of the conference on empirical methods in natural
language processing (emnlp).

hitoshi nishikawa, kazuho arita, katsumi tanaka,
tsutomu hirao, toshiro makino, and yoshihiro
matsuo.
2014. learning to generate coherent
summary with discriminative hidden semi-markov
model. in proceedings of the international confer-
ence on computational linguistics (coling).

gerald penn and xiaodan zhu. 2008. a critical re-
assessment of evaluation baselines for speech sum-
in proceedings of the association for
marization.
computational linguistics (acl).

slav petrov, leon barrett, romain thibaux, and dan
klein. 2006. learning accurate, compact, and in-
terpretable tree annotation. in proceedings of the
conference on computational linguistics and the
association for computational linguistics (acl-
coling).

