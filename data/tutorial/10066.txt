generating news headlines with recurrent neural

networks

konstantin lopyrev
klopyrev@stanford.edu

abstract

we describe an application of an encoder-decoder recurrent neural network with
lstm units and attention to generating headlines from the text of news articles.
we    nd that the model is quite effective at concisely id141 news articles.
furthermore, we study how the neural network decides which input words to pay
attention to, and speci   cally we identify the function of the different neurons in a
simpli   ed attention mechanism. interestingly, our simpli   ed attention mechanism
performs better that the more complex attention mechanism on a held out set of
articles.

1 background

recurrent neural networks have recently been found to be very effective for many transduction tasks
- that is transforming text from one form to another. examples of such applications include machine
translation [1,2] and id103 [3]. these models are trained on large amounts of input and
expected output sequences, and are then able to generate output sequences given inputs never before
presented to the model during training.
recurrent neural networks have also been applied recently to reading comprehension [4]. there, the
models are trained to recall facts or statements from input text.
our work is closely related to [5] who also use a neural network to generate news headlines using
the same dataset as this work. the main difference to this work is that they do not use a recurrent
neural network for encoding, instead using a simpler attention-based model.

2 model

2.1 overview

we use the encoder-decoder architecture described in [1] and [2], and shown in    gure 1. the ar-
chitecture consists of two parts - an encoder and a decoder - both by themselves recurrent neural
networks.

5
1
0
2
 
c
e
d
5

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
1
7
1
0

.

2
1
5
1
:
v
i
x
r
a

figure 1: encoder-decoder neural network architecture

1

the encoder is fed as input the text of a news article one word of a time. each word is    rst passed
through an embedding layer that transforms the word into a distributed representation. that dis-
tributed representation is then combined using a multi-layer neural network with the hidden layers
generated after feeding in the previous word, or all 0   s for the    rst word in the text.
the decoder takes as input the hidden layers generated after feeding in the last word of the input text.
first, an end-of-sequence symbol is fed in as input, again using an embedding layer to transform the
symbol into a distributed representation. then, the decoder generates, using a softmax layer and the
attention mechanism, described in the next section, each of the words of the headline, ending with
an end-of-sequence symbol. after generating each word that same word is fed in as input when
generating the next word.
the id168 we use is the log id168:

    log p(y1, . . . , yt (cid:48)|x1, . . . , xt ) =     t (cid:48)(cid:88)

log p(yt|y1, . . . , yt   1, x1, . . . , xt )

t=1

where y represent output words and x represent input words.
note that during training of the model it is necessary to use what is called    teacher forcing    [6].
instead of generating a new word and then feeding in that word as input when generating the next
word, the expected word in the actual headline is fed in. however, during testing the previously
generated word is fed in when generating the next word. that leads to a disconnect between training
and testing. to overcome this disconnect, during training we randomly feed in a generated word,
instead of the expected word, as suggested in [7]. speci   cally, we do this 10% of the time, as also
done in [8]. during testing we use a beam-search decoder which generates input words one at a
time, at each step extending the b highest id203 sequences.
we use 4 hidden layers of lstm units, speci   cally the variant described in [9]. each layer has 600
hidden units. we attempted using dropout as is also described in [9]. however we did not    nd it to
be useful. thus, the models analyzed below do not use dropout. we initialize most parameters of
the model uniformly in the range [   0.1; 0.1]. we initialize the biases for each word in the softmax
layer to the log-id203 of its occurence in the training data, as suggested in [10].
we use a learning rate of 0.01 along with the rmsprop [11] adaptive gradient method. for rmsprop
we use a decay of 0.9 and a momentum of 0.9. we train for 9 epochs, starting to half the learning
rate at the end of each epoch after 5 epochs.
additionally, we batch examples, processing 384 examples at a time. this batching complicates
the implementation due to the varying lengths of different sequences. we simply    x the maximum
lengths of input and output sequences and use special logic to ensure that the correct hidden states
are fed in during the    rst step of the decoder, and that no loss is incurred past the end of the output
sequence.

2.2 attention

attention is a mechanism that helps the network remember certain aspects of the input better, in-
cluding names and numbers. the attention mechanism is used when outputting each word in the
decoder. for each output word the attention mechanism computes a weight over each of the input
words that determines how much attention should be paid to that input word. the weights sum up to
1, and are used to compute a weighted average of the last hidden layers generated after processing
each of the input words. this weighted average, referred to as the context, is then input into the
softmax layer along with the last hidden layer from the current step of the decoding.
we experiment with two different attention mechanisms. the    rst attention mechanism, which we
refer to as complex attention, is the same as the dot mechanism in [2]. this mechanism is shown in
   gure 2. the attention weight for the input word at position t, computed when outputting the t(cid:48)-th
word is:

where hxt represents the last hidden layer generated after processing the t-th input word, and hyt(cid:48)
represents the last hidden layer from the current step of decoding. note one of the characteristics

ayt(cid:48) (t) =

(cid:80)t

hyt(cid:48) )

exp(ht
xt
  t exp(ht
x  t

hyt(cid:48) )

2

of this mechanism is that the same hidden units are used for computing the attention weight as for
computing the context.
the second attention mechanism, which we refer to as simple attention, is a slight variation of
the complex mechanism that makes it easier to analyze how the neural network learns to compute
the attention weights. this mechanism is shown in    gure 3. here, the hidden units of the last
layer generated after processing each of the input words are split into 2 sets: one set of size 50
used for computing the attention weight, and the other of size 550 used for computing the context.
analogously, the hidden units of the last layer from the current step of decoding are split into 2 sets:
one set of size 50 used for computing the attention weight, and the other of size 550 fed into the
softmax layer. aside from these changes the formula for computing the attention weights, given the
correponding hidden units, and the formula for computing the context are kept the same.

figure 2: complex attention

figure 3: simple attention

3 dataset

3.1 overview

the model is trained using the english gigaword dataset, as available from the stanford linguistics
department. this dataset consists of several years of news articles from 6 major news agencies,
including the new york times and the associated press. each of the news articles has a clearly
delineated headline and text, where the text is broken up into paragraphs. after the preprocessing
described below the training data consists of 5.5m news articles with 236m words.

3.2 preprocessing

the headline and text are lowercased and tokenized, separating punctuation from words. only the
   rst paragraph of the text is kept. an end-of-sequence token is added to both the headline and the
text. articles that have no headline or text, or where the headline or text lengths exceed 25 and
50 tokens, respectively, are    ltered out, for computational ef   ciency purposes. all rare words are
replaced with the < unk > symbol, keeping only the 40,000 most frequently occuring words.
the data is split into a training and a holdout set. the holdout set consists of articles from the last
month of data, with the second last month not included in either the training or holdout sets. this
split helps ensure that no nearly duplicate articles make it into both the training and holdout sets.
finally, the training data is randomly shuf   ed.

3.3 dataset issues

the dataset as used has a number of issues. there are many training examples where the headline
does not in fact summarize the text very well or at all. these include many articles that are formatted
incorrectly, having the actual headline in the text section and the headline section containing words
such as    (for use by new york times news service clients)   . there are many articles where the
headline has some coded form, such as    biz-cover-1stld-writethru-nyt    or    bc-iraq-post 1stld-sub-
pickup4thgraf   .

3

no    ltering of such articles was done. an ideal model should be able to handle such issues automat-
ically, and attempts were made to do so using, for example, randomly feeding in generated words
during training, as described in the model section.

4 evaluation

the performance of the model was measured in two different ways. first, we looked at the training
and holdout loss. second, we used the id7 [12] evaluation metric over the holdout set, de   ned
next. for ef   ciency reasons, the holdout metrics were computed over only 384 examples.
the id7 evaluation metric looks at what fraction of id165s of different lengths from the expected
headlines are actually output by the model.
it also considers the number of words generated in
comparison to the number of words used in the expected headlines. both of these are computed over
all 384 heldout example, instead of over each example separately. for the exact de   nition see [12].

5 analysis

each model takes 4.5 days to train on a gtx 980 ti gpu. figures 4 and 5 show the evaluation
metrics as a function of training epoch. note that in our setup the training loss is generally higher
than holdout loss, since when computing the holdout loss we don   t feed in generated words 10% of
time.

figure 4: loss vs epoch

figure 5: id7 vs epoch

the model is quite effective in predicting headlines from the same newspapers as it was trained on.
table 1 lists 7 examples chosen at random from the held-out examples. the model generally seems
to capture the gist of the text and manages to paraphrase the text, sometimes using completely new
words. however, it does make mistakes, for example, in sentences 2, 4 and 7.
the model has much more mixed performance when used to generate headlines for news articles
from sources that are different from training. table 2 shows generated headlines for articles from
several major news websites. the model does quite well with articles from the bbc, the wall street
journal and the guardian. however, it performs very poorly on articles from the huf   ngton post
and forbes. in fact, the model performed poorly on almost all tested articles from forbes. it seems
that there is a major difference in how articles from forbes are written, when compared to articles
used to train the model.

5.1 understanding information stored in last layer of the neural network

we notice that there are multiple ways to go about understanding the function of the attention mech-
anism. consider the formula for computing the input to the softmax function:

oyt(cid:48) = wcocyt(cid:48) + whohyt(cid:48) + bo

4

table 1: example predictions

text
1. at least 72 people died and scores more
were hurt when a truck crowded with pilgrims
plunged into a gorge in the desert state of ra-
jasthan on friday, police told the press trust of
india.
2. sudanese president omer al-bashir has an-
nounced his refusal of discharging a govern-
ment minister who had been accused by the
international criminal court (icc) of commit-
ting war crimes in the western sudanese region
of darfur, sudan   s <unk> daily reported on
monday.
3. a chief of afghanistan   s ousted taliban
militia said al-qaeda chief osama bin laden
is alive and has sent him a letter of condolences,
in an interview broadcast on tuesday on al-
jazeera television.
4. one of the last remaining routes for iraqis
trying to    ee their country has effectively been
closed off by new visa restrictions imposed by
syria, the u. n. refugee agency said tuesday.
5. members of the u.n.   s new human rights
watchdog on tuesday formally adopted a series
of reforms to its future work, including how
and when to launch investigations into some of
the world   s worst rights offenders.
6. democratic presidential candidates said
thursday they would step up pressure on pak-
istan   s president pervez musharraf over democ-
racy, and criticized white house policy to-
wards islamabad.
7. manchester united   s strength in depth is
set to be tested for the    rst time this season in
the wake of a last-gasp win over liverpool that
has signi   cantly shortened the odds on sir alex
ferguson   s side reclaiming the premiership ti-
tle.

actual headline
urgent: truck crashes
killing 72 pilgrims in
india

predicted headline
at least 72 dead in in-
dian road accident

president
sudanese
refuses to discharge
state minister
in-
dicted by icc

president
sudanese
refuses to of alleged
war crimes

taliban leader says
bin laden still alive

urgent: bin laden
alive,
says taliban
chief

u.n. refugee agency
closes last routes to
iraq

u.n. human rights
body adopts reforms

democratic presiden-
tial hopefuls call for
pressure on mushar-
raf

united set for test test

visa

unhcr says new
syrian
rules
blocking iraqis from
entering country
u.n. human rights
watchdog
adopts
reforms on how to
investigate countries
for abuses
democrats call
more
pressure
pakistan

for
on

football: united face
test of reserves after
scholes red card by
neil johnston

where cyt(cid:48) is the context computed for the current step of decoding, hyt(cid:48) is the last hidden layer from
the current step of decoding, and wco, who and bo are model parameters. first, note that by looking
at the word with the highest values for

we can get an idea of what exactly the hidden layer from the current step of decoding is contributing
to the    nal generated output. analogously, by looking at the words with the highest values for

whohyt(cid:48) + bo

we can do the same for the attention context. moreover, since the context is just a weighted sum
over the hidden layers of the decoder we can compute

wcocyt(cid:48) + bo

for each of the input positions and get a good idea of what words would be recalled if the network
paid attention to each of the input positions.

wcohxt + bo

5

table 2: example predictions from sources different from training

source
bbc

huf   ngton
post

wall
street
journal

forbes

the
guardian

text
russia   s president vladimir putin has
condemned turkey   s shooting down
of a russian warplane on its border
with syria.
when sarah palin defended governors
who are refusing to accept refugees by
claiming there   s no vetting process to
keep out terrorists,    late night    host
seth meyers completely shut down
her argument.
the top commander of u.s. troops
in afghanistan said wednesday that
the american service members most
closely associated with the deadly
bombing of a doctors without bor-
ders hospital
in afghanistan have
been suspended from duty.
tuesday and wednesday, you are
likely to read online or in a newspaper
and are even more likely to hear some
tv or radio news person say that the
day before thanksgiving is the busi-
est air travel day of the year. it long
has been a staple of reporting in what
usually is a very slow news week.
presidential candidate hillary clin-
ton has plunged into the heated de-
bate surrounding the police killing of
a black teenager in chicago, saying
we cannot go on like this, following
the release of a video showing laquan
mcdonald being shot multiple times
by an of   cer on the street.

actual headline
turkey   s
downing
of russian warplane
- what we know

predicted headline
condemns
putin
turkey   s
shooting
of russian plane

seth meyers calls
out sarah palin for
repeating refugee
lies

   night of the night   
is <unk>

u.s. troops sus-
after
pended
afghan
hospital
bombing

commander
afghanistan
from

u.s.
in
suspended
hospital

be

how crazy will
travel
on
the day before
thanksgiving?
not as crazy as
you   ve been led to
believe

hillary clinton on
laquan mcdonald
shooting:    we can-
not go on like this   

the on the air

hillary
clinton
plunges into debate

as an example, the last hidden layer generated after processing the    and    in the    rst example in
table 1 is closest to the following words: 72, at, death, in, died, dead, to, <eos>, toll, people. we
see many of the words that appeared before the    and   . it is interesting note that the words closest
to the hidden layer for    died    are: 72, at, to, in, <eos>, <unk>, for, as,    ,   , more. note that the
word    died    doesn   t appear. this example demonstrates our observation that the network sometimes
takes multiple steps to encode a particular word. that makes it slightly trickier to understand what
the network is paying attention to.
as an example of what the hidden layer during decoding contains, the hidden layer during the    rst
step of decoding for the same example as above is closest to the following words: urgent, (, at, one,
two, three, <unk>, four, 1st, 1. after generating the    at    the hidden layer during the next step is
closest to: least, :, -, killed,    ,   , ld, lead, in, <unk>, to.

5.2 understanding how the attention weight vector is computed

we had an initial hypothesis for how the network learned to compute the attention weight vector.
we hypothesized that the network remembered roughly which word should be generated next, and
hyt(cid:48) would compute the similarity between the remembered word and the actual
the dot product ht
xt
word at the position. for example, if the network remembered that the text talked about some

6

table 3: attention neuron purposes

neuron discovered purposes

1
2

3

4

5
6
7

8
9
10
11

12
13
14
15
16
17
18
19
20

person names; country names
multi-part numbers (e.g.    $ 1 , 000   ); noun after a number (e.g.    people    in    four people
killed   ); noun after an adjective
end of multi-part sequence (e.g.    000    in    $ 1 , 000   ,    qaida    in    al - qaida   ,    france    in
   tour de france   )
verb after auxiliary verb or particle (e.g.    meet    in    will meet    or    to meet   ); past tense
verbs; noun following a preposition
beginning of text (e.g.    rst of two sentences)
verbs; prepositions
end of noun phrase (e.g.       nance minister    in    former    nance minister    or    probe    in
   ethics probe   ); past tense verbs
present participles; noun phrases after    for   
objects and subjects of a verb (positive activation for object, negative for subject)
subjects; objects after some verbs; words after a dash
number following a conjuction (e.g.    four    in    three people killed and four injured   ); verb
after auxiliary verb; noun following a preposition;
objects and subjects of a verb (positive activation for object, negative for subject)
most nouns and verbs
verbs; locations; word after    by   
days of the week; some adjectives
function words; negations (e.g.    not   )
end of noun phrase
objects and subjects of a verb (positive activation for object, negative for subject)
names; some verbs; some adjectives
objects, subjects and corresponding verbs

number and was able to output a representation that was close to numbers in general, the attention
mechanism would then allow it to get the exact number.
one implication of this hypothesis is that the units of the last hidden layer of the decoder would be
in the same space as the units of the last hidden layer of the encoder. thus, computing

wcohyt(cid:48) + bo

and looking at the words with the highest values would tell us something meaningful. it turned out
that this was not the case.
further analysis into which units contributed to the attention weight vector computation revealed
the existence of a few units which played key roles. that led us to simplify the neural network
architecture as described in the model section. interestingly, as shown in    gures 4 and 5, the simpli-
   ed model performs signi   cantly better. one possible explanation is that by separating the attention
weight vector computation from the context computation we reduce the noise in both of these com-
putations.
the simpli   ed attention mechanism is also much easier to understand since only a small number of
hidden units is used to compute the attention weight vector. in one of the smaller model that we
trained we used only 20 units for the attention weight vector computation, and were able to    gure
out some of the functions of these 20 units. table 3 catalogs the functions of the 20 neurons in
the encoding part of the network. each position in the input text has 20 neurons that serve these
functions.
the neural network learns to spot many linguistic phenomena. our analysis was also somewhat
shallow due to time constraints. we suspect that some of the neurons activate for even more complex
phenomena than we are able to    nd, such as different types of sentence structure.
it is important to note that the neurons in the encoding part of the network interact with the neurons
in the decoding part of the network. indeed, the neurons in the decoding part of the network activate

7

at different times to test for different phenomena. for example, unit 12 starts off being positive so
that the network pays attention to the object    rst. then, unit 12 becomes negative so that the network
pays attention to the subject. interestingly, unit 9, which appears to work almost the same at unit 12
in the encoder, is usually 0 at the beginning of decoding, and only later on becomes activated.

5.3 errors

the network makes a lot of different types of errors. while we didn   t do an in-depth error analysis,
a few error still stood out.
one    aw with the neural network mechanism is its tendency to    ll in details when details are miss-
ing. for example, after simplifying the text given in table 1, example 1 to    72 people died when a
truck plunged into a gorge on friday.    the model predicts    72 killed in truck accident in russia   .
the model makes up the fact that the accident happened in russia. these errors happen most often
when the number of decoding beams is small, since the model stops considering the decoding where
the sentence ends early before outputing the made up details.
what also happens occasionally is the network outputs some headline that is completely unrelated
to the input text (e.g.    urgent   ,    bc-times    or even    can make individual purchases by calling 212
- 556 - 4204 or - 1927 .)   ). this problem is caused by the fact that such headlines occur somewhat
often in the input dataset. these errors happen most often when the number of decoding beams is
large, since that increases the id203 of the model starting to generate one such high id203
sequence.
these 2 examples demonstrate that our network is very sensitive to the number of decoding beams
used. we used only 2 decoding beams for the id7 evaluation. we suspect that if we    xed the
second problem we could get much better results with a large number of beams. one solution worthy
of investigation is to use the scheduled sampling mechanism described in [7].

6 future work

we demostrate above that the recurrent neural network learns to model complex linguistic phenom-
ena, given large amounts of training data. however, such large amounts of training data are usually
not available for real-world nlp problems. one interesting direction to pursue is using a dataset,
like gigaword, to pretrain a recurrent neural network that is then    ne-tuned to solve a task such as
part-of-speech tagging on a much smaller dataset.
more immediately, in addition to the already discussed idea to use scheduled sampling, another way
to improve the model is to use a bi-directional id56. we suspect that the attention mechanism would
work better with a bi-directional id56, since more information would be available to model some
of the phenomena outlined in table 3. in our current model, the network must make a decision about
which values to assign the neurons used to compute the attention weight for the current input word
given only the current and previous words and not any of the following words. giving the model
information about the following words would make this decision easier for the network to make.

7 conclusion

we   ve trained an encoder-decoder recurrent neural network with lstm units and attention for gen-
erating news headlines using the texts of news articles from the gigaword dataset. using only the
   rst 50 words of a news article, the model generates a concise summary of those 50 words, and most
of the time the summary is valid and grammatically correct. the model doesn   t perform quite as
well on general text, demonstrating that a lot of the articles in gigaword follow a particular form.
we study 2 different versions of the attention mechanism with the goal of understanding how the
model decides which words of the input text to pay attention to when generating each output word.
we introduce a simpli   ed attention mechanism that uses a small set of neurons for computing the
attention weights. this simpli   ed mechanism makes it easier to study the function of the network.
we    nd that the network learns to detect linguistic phenomena, such as verbs, objects and subjects
of a verb, ends of noun phrases, names, prepositions, negations, and so on. interestingly, we    nd
that our simpli   ed attention mechanism does better on the id74.

8

acknowledgements

we   d like to thank thang luong for his helpful suggestions and for sharing his machine translation
code. we   d like to thank samy bengio and andrej karpathy for suggesting a few interesting design
alternatives. finally, we   d like to thank chris r  e. some of the code written as part of doing research
in his lab was used to complete this work.

references
[1] ilya sutskever, oriol vinyals, and quoc v. le. sequence to sequence learning with neural networks.

corr, abs/1409.3215, 2014.

[2] minh-thang luong, hieu pham, and christopher d. manning. effective approaches to attention-based

id4. corr, abs/1508.04025, 2015.

[3] alex graves, abdel-rahman mohamed, and geoffrey e. hinton. id103 with deep recurrent

neural networks. corr, abs/1303.5778, 2013.

[4] karl moritz hermann, tom  as kocisk  y, edward grefenstette, lasse espeholt, will kay, mustafa suley-

man, and phil blunsom. teaching machines to read and comprehend. corr, abs/1506.03340, 2015.

[5] alexander m. rush, sumit chopra, and jason weston. a neural attention model for abstractive sentence

summarization. corr, abs/1509.00685, 2015.

[6] ian goodfellow, aaron courville, and yoshua bengio. deep learning. book in preparation for mit press,

2015.

[7] samy bengio, oriol vinyals, navdeep jaitly, and noam shazeer. scheduled sampling for sequence

prediction with recurrent neural networks. corr, abs/1506.03099, 2015.

[8] william chan, navdeep jaitly, quoc v. le, and oriol vinyals. listen, attend and spell. corr,

abs/1508.01211, 2015.

[9] wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173. corr,

abs/1409.2329, 2014.

[10] andrej karpathy and fei-fei li. deep visual-semantic alignments for generating image descriptions.

corr, abs/1412.2306, 2014.

[11] tijmen tieleman and geoffrey hinton. lecture 6.5 - rmsprop: divide the gradient by a running average

of its recent magnitude. coursera: neural networks for machine learning, 2012.

[12] kishore papineni, salim roukos, todd ward, and wei-jing zhu. id7: a method for automatic evalua-
tion of machine translation. in proceedings of the 40th annual meeting on association for computational
linguistics, acl    02, pages 311   318, stroudsburg, pa, usa, 2002. association for computational lin-
guistics.

9

