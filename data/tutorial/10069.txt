neural headline generation with sentence-wise optimization

ayana, shiqi shen, yu zhao, zhiyuan liu   , maosong sun

department of computer science and technology,

state key lab on intelligent technology and systems,

national lab for information science and technology, tsinghua university, beijing, china

6
1
0
2

 
t
c
o
9

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
0
9
1
0

.

4
0
6
1
:
v
i
x
r
a

abstract

recently, neural models have been pro-
posed for headline generation by learning
to map documents to headlines with re-
current neural networks. nevertheless, as
traditional neural network utilizes maxi-
mum likelihood estimation for parameter
optimization, it essentially constrains the
expected training objective within word
level rather than sentence level. moreover,
the performance of model prediction sig-
ni   cantly relies on training data distribu-
tion. to overcome these drawbacks, we
employ minimum risk training strategy in
this paper, which directly optimizes model
parameters in sentence level with respect
to id74 and leads to signif-
icant improvements for headline genera-
tion. experiment results show that our
models outperforms state-of-the-art sys-
tems on both english and chinese head-
line generation tasks.

1

introduction
automatic text summarization is the process of
creating a coherent, informative and brief sum-
mary for a document. text summarization is ex-
pected to understand the main theme of the docu-
ments and then output a condensed summary con-
tains as many key points of the original document
as it can within a length limit. text summarization
approaches can be divided into two typical cate-
gories: extractive and generative. most extrac-
tive summarization systems simply select a sub-
set of existing sentences from original documents
as summary. despite of its simplicity, extractive
summarization has some intrinsic drawbacks, e.g.,
unable to generate coherent and compact summary
in arbitrary length or shorter than one sentence.
   corresponding author: z. liu (liuzy@tsinghua.edu.cn)

in contrast, generative summarization builds se-
mantic representation of a document and creates a
summary with sentences not necessarily present-
ing in the original document explicitly. when the
generated summary is required to be a single com-
pact sentence, we name the summarization task as
headline generation (dorr et al., 2003). most pre-
vious works heavily rely on modeling latent lin-
guistic structures of input document, via syntactic
parsing and id29, which always bring
inevitable errors and degrade summarization qual-
ity.

recent years have witnessed great success of
deep neural models for various natural language
processing tasks (cho et al., 2014; sutskever et al.,
2014; bahdanau et al., 2015; ranzato et al., 2015)
including text summarization. taking neural head-
line generation (nhg) for example, it learns to
build a large neural network, which takes a docu-
ment as input and directly outputs a compact sen-
tence as headline of the document. compared with
conventional generative methods, nhg exhibits
the following advantages: (1) nhg is fully data-
driven, requiring no linguistic information.
(2)
nhg is completely end-to-end, which does not
explicitly model latent linguistic structures, and
thus prevents error propagation. moreover, the at-
tention mechanism (bahdanau et al., 2015) is in-
troduced in nhg, which learns a soft alignment
over input document to generate more accurate
headline (rush et al., 2015).

nevertheless, nhg still confronts a signi   cant
problem: current models are mostly optimized at
the word level instead of sentence level, which
prevents them from capturing various aspects of
summarization quality. in fact, it is essentially de-
sirable to incorporate the implicit sentence-wise
information contained in the evaluation criteria,
e.g. id8, into nhg model.

to address this issue, we propose to apply the
minimum bayes risk technique in tuning nhg

model with respect to id74. specif-
ically, we utilize minimum risk training (mrt),
which aims at minimizing a sentence-wise loss
fuction over the training data. to the best of our
knowledge, although mrt has been widely used
in many nlp tasks such as statistical machine
translation (och, 2003; smith and eisner, 2006;
gao et al., 2014; shen et al., 2015), it has not been
well considered in the research of text summariza-
tion.

we conduct experiments on three real-world
datasets in english and chinese respectively. ex-
periment results show that, nhg with mrt can
signi   cantly and consistently improve the summa-
rization performance as compared to nhg with
id113, and other baseline systems. moreover, we
explore the in   uence of employing different evalu-
ation metrics and    nd the superiority of our model
stable in mrt.

2 background

in this section, we formally de   ne the problem
of neural headline generation and introduce the
notations used in our model. denote the input doc-
ument x as a sequence of words {x1,       , xm},
where each word xi comes from a    xed vocabu-
lary v . headline generator aims to take x as input,
and generates a short headline y = {y1,       , yn}
with length n < m, such that the conditional
id203 of y given x is maximized. the log
id155 can be further formalized
as:

log pr(y|x;   ) =

log pr(yj|x, y<j;   ),

(1)

j=1

where y<j = {y1, . . . , yj   1},    indicates model
parameters. that is, the j-th word yj in head-
line is generated according to all y<j generated
in past and the input document x.
in nhg, we
adopt an encoder-decoder framework to parame-
terize pr(yj|x, y<j;   ), as shown in fig. 1.

the encoder of the model encodes the in-
put document x into low-dimensional vectors
(h1, . . . , hi, . . . , hm ) using bi-directional recur-
rent neural network with gru units, where hi is
the concatenation of forward and backward states
corresponding to word xi. then,
the decoder
sequentially generates headline words based on
these vectors and decoder hidden states, using
a uni-directional gru recurrent neural network

n(cid:88)

figure 1: the framework of nhg.

with attention, i.e.,

pr(yj|x, y<j;   ) = pr(yj|cj, sj, yj   1,   ),

(2)

where cj stands for the context for generating the
j-th headline word and is calculated utilizing the
attention mechanism. sj is the j-th hidden state
of the decoder, and    denotes a set of model pa-
rameters. please refer to (bahdanau et al., 2015;
sutskever et al., 2014) for more details.

3 minimum risk training for nhg

given a dataset d with large-scale document-
headline pairs {(x(1), y(1)), . . . , (x(t ), y(t ))}, we
propose to use minimum risk training to opti-
mize model parameters instead of the conventional
id113. we employ the
famous id8 id74 to compose
the expected id168. in this section, we in-
troduce our basic idea in detail.

3.1 minimum risk training

in the traditional training strategy, the optimized
nhg model parameters are estimated by maxi-
mizing the log likelihood of generated headlines
over training set d:

lid113(  ) =

log pr(y|x;   ).

(3)

(cid:88)

(x,y)   d

according to eq.

(1), the training procedure
is fundamentally maximizing the id203 of
each word in headline step by step, which will in-
evitably lose global information. moreover, y<j
are authentic words from reference headline in the

(cid:88)

(x,y)   d

training phase, while in the training phase they are
predicted by model. it will lead to error propaga-
tion and inaccurate headline generation.

in order to tackle these problems, we propose to
use minimum risk training (mrt) strategy. given
a document x, we de   ne y(x;   ) as the set of all
possible headlines generated with parameters   .
regarding y(cid:48) as the reference headline of x, we
denote    (y(cid:48), y) as the distance between y and a
generated headline y(cid:48). mrt de   nes the objective
id168 as follows:

lmrt(  ) =

ey(x;  )   (y(cid:48), y).

(4)

here ey(x;  ) indicates the expectation over all
possible headlines. thus the objective function of
mrt can be further formalized as:
lmrt(  ) =

pr(y(cid:48)|x;   )   (y(cid:48), y).

(cid:88)

(cid:88)

(x,y)   d

y(cid:48)   y(x;  )

(5)
in this way, mrt manages to minimize the ex-
pected loss by perceiving the distance as a measure
of assessing the overall risk. nevertheless, it is
usually time-consuming and inef   cient to enumer-
ate all possible instances. for simplicity, we draw
a subset of samples s(x;   )     y(x;   ) from the
current id203 distribution of generated head-
lines. the id168 can be approximated as:

lmrt(  ) =(cid:88)
(cid:88)

(x,y)   d

y(cid:48)   s(x;  )

(cid:80)
y      s(x;  ) pr(y   |x;   )     (y(cid:48), y),

pr(y(cid:48)|x;   ) 

(6)

where   is a hyper-parameter that controls the
smoothness of the objective function (och, 2003).
a proper   value can signi   cantly enhance the ef-
fectiveness of mrt. in the experiment, we set   to
5    10   3.
3.2 id8

mrt exploits the distance between two sen-
tences to compose the id168, which enables
us to directly optimize nhg model with respect
to a speci   c evaluation metric of the task. as we
know, the most widely used evaluation metric for
document summarization is id8 (lin, 2004).
the basic idea of id8 is to count the number
of overlapping units between computer-generated

summaries and the reference summaries, such as
overlapped id165s, word sequences, and word
pairs.
it is the most common evaluation metric
in document understanding conference (duc),
a large-scale summarization evaluation sponsored
by nist (lin, 2004). when training english mod-
els, we adopt negative recall value of id8-1,2
and l to compose    (y(cid:48), y). for chinise models,
we utilize negative f1 value of id8-1,2 and l
to compose    (y(cid:48), y).
4 experiments

in this paper, we evaluate our methodology
on both english and chinese headline genera-
tion tasks. we    rst introduce the datasets and
id74 used in the experiment. then
we demonstrate that our model performs the best
compared with state-of-the-art baseline systems.
we also analyze the in   uence of different parame-
ters in detail to gain more insights.

4.1 datasets and id74

english datasets. in the experiment, we utilize
the english gigaword fifth edition (parker et al.,
2011) corpus, containing 10 million news articles1
with corresponding headlines. we follow the ex-
perimental settings in (rush et al., 2015) to collect
4 million article-headline pairs as the training set.
we use the dataset from duc-2004 task-1 as
our test set. it consists of 500 news articles, each
of which is paired with four human-generated ref-
erence headlines. 2 we also take gigaword test
set 3 to evaluate our models. we use the duc-
2003 evaluation dataset of size 624 as develop-
ment set to tune the hyper-parameters in mrt.

chinese dataset. we conduct experiments on
the chinese lcsts dataset (hu et al., 2015),
consisting of article-headline pairs extracted from
sina weibo 4. lcsts are composed of three parts,
containing 2.4 million, 10, 666 and 1, 106 article-
headline pairs respectively. those pairs in part-ii

1to avoid noises in articles and headlines that may in-
   uence the performance, we    lter out headlines with by-
lines, extraneous editing marks and question marks. for en-
glish dataset, we preprocess the corpus with id121 and
lower-casing.

2the dataset can be downloaded from http://duc.

nist.gov/ with agreements.

3this dataset is provided by (rush et al., 2015).
4the website of sina weibo is http://weibo.com/.
a typical news article posted in weibo is limited to 140 chi-
nese characters, and the corresponding headline is usually set
in a pair of square brackets at the beginning of the news arti-
cle.

and part-iii are labeled with relatedness scores by
human annotation that indicate how relevant an ar-
ticle and its headline are 5. in the experiment, we
take part-i of lcsts as training set, part-ii as de-
velopment set and part-iii as test set. we only re-
serve those pairs with scores no less than 3. it is
worth mentioning that, we take chinese characters
as inputs of nhg instead of words in order to pre-
vent the in   uence of chinese id40
errors. in addition, we replace all digits with # for
both english and chinese corpus.

id74. in the experiment, we use
id8 (lin, 2004), as introduced in section 3.2,
to evaluate the performance of headline genera-
tion.

following (rush et al., 2015; chopra et al.,
2016; nallapati et al., 2016), for duc2003 and
duc2004, we report recall scores of id8-
1(r1r) , id8-2(r2r) and id8-l(rlr)
with of   cial 75 bytes ceiling limit. and follow-
ing (rush et al., 2015; chopra et al., 2016; nal-
lapati et al., 2016), for gigaword test set, we re-
port full-length f1 scores of id8-1(r1f 1) ,
id8-2(r2f 1) and id8-l(rlf 1). since
a shorter summary tends to get lower recall score,
when testing on duc datasets, we set the mini-
mum length of a generated headline as 10. note
that we report 75 bytes capped recall scores only.
in this case summaries that longer than 75 bytes
obtain no bonus on recall scores. due to full-
length f1 makes the evaluation result unbiased to
summary length, we set no limitation to headline
length when testing on gigaword test set.

for chinese, we report full-length f1 scores
(r1f 1, r2f 1 and rlf 1) following previous
works (hu et al., 2015; gu et al., 2016). we set no
length limitation on chinese experiments either.

4.2 baseline systems
4.2.1 english baseline systems

topiary (zajic et al., 2004) is the winner
system of duc2004 task-1. it utilizes linguistic-
based sentence compression method and unsuper-
vised topic detection at the same time.

moses+ (rush et al., 2015) generates head-
lines based on moses, a widely-used phrase-
based machine translation system (koehn et al.,
2007). it also enlarges the phrase table and uses

5each pair in part-ii is labeled by only one annotator, and

in part-iii is by three annotators.

mert to improve the quality of generated head-
lines.

abs and abs+ (rush et al., 2015) are both
attention-based neural models that generate short
summary for given sentence. the difference is that
abs+ extracts additional id165 features at word
level to revise the output of abs model.

ras-elman and ras-lstm (chopra et al.,
2016) both utilize convolutional encoders that take
input words and word position information into
account. they also make use of attention-based
decoders. the differernce is that, ras-elman se-
lects elman id56 (elman, 1990) as decoder, while
ras-lstm selects long short term memory ar-
chitecture (hochreiter and schmidhuber, 1997).
bwl, namely big-words-lvt2k-lsent (nallapati
et al., 2016) implements a trick that restricts the
vocabulary size at the decoder end, by means of
constructing the vocabulary of documents in each
mini-batch respectively (jean et al., 2015).

all the english baseline systems listed above
except topiary utilize gigaword dataset for
training, as described in section 4.1.

4.2.2 chinese baseline systems

id56-context (hu et al., 2015) is a simple
character based encoder-decoder architecture that
takes the concatenation of all hidden states at the
encoder end as the input of decoder end.

copynet (gu et al., 2016) incorporates
copying mechanism into sequence-to-sequence
framework, which replicates certain segments
from the input sentence into the output sentence.

4.3

implementation details

in id113, the id27s are randomly
initialized and then updated during training.
in
mrt, we initialize model parameters using the op-
timized parameters learned from nhg with id113.
for english models, we set the id27
dimension to 620, the hidden unit size to 1,000 and
the vocabulary size to 30,000. the correspond-
ing values for chinese models are 400, 500 and
3,500 respectively. in particular, the size of subset
s(x;   ) in eq.(6) has a great impact on the per-
formance. when the size is too small, the sam-
pling will not be suf   cient. when the size is too
large, the learning time will grow correspondingly.
in this paper, we set the size to 100 to achieve
a trade-off between effectiveness and ef   ciency.
these samples are drawn from the id203 dis-
tribution of generated headlines by the up-to-date

evaluation metric

criterion

id113

mrt

loss
n/a
   r1
   r2
   rl

r1
23.70
28.81
26.94
28.19

r2
7.85
9.58
9.56
9.64

rl
21.20
25.31
24.01
25.02

table 1: effects of distance measures on the en-
glish validation set.    r1,    r2 and    rl repre-
sent the opposite value of id8-1, id8-2
and id8-l respectively.

evaluation metric

criterion
id113

mrt

loss
n/a
   r1
   r2
   rl

r1
24.61
29.84
27.97
29.18

r2
8.52
10.24
10.18
10.44

rl
22.00
26.33
24.99
25.88

table 2: effects of using different distance mea-
sures on the english test set.

nhg model6. we use adadelta (zeiler, 2012) to
adapt learning rates in stochastic id119
for both id113 and mrt. we utilize no dropout or
id173, but we take gradient clipping dur-
ing training and the training is early stopped based
on duc2003 data. all our models are trained on
geforce gtx titan gpu. for nhg+id113 on
the english dataset, it takes about 2.5 hours for
each 10, 000 iterations, for nhg+mrt, it takes
about 3.75 hours. during testing, we use beam
search of size 10 (chopra et al., 2016) to generate
headlines.

4.4 choices of model setup

in the training process of nhg model, there are
several signi   cant factors that greatly in   uence the
performance, such as the choice of distance mea-
sure in id168 and the treatment of unknown
words. to determine the most appropriate choices
of model setup, we investigate the effects of these
factors on the development set respectively.

4.4.1 effect of distance measure
as described in section 3.2,

the distance
   (y(cid:48), y) in the id168 is computed by the
negative value of id8. we investigate the ef-
fect of utilizing various distance measures in mrt.
table 1 shows the experiment results on english
development set using different evaluation met-
rics. we    nd that all nhg+mrt models con-
sistently outperform nhg+id113, which indicates

6an alternative subset building strategy is to choose top-k
headlines. considering the ef   ciency and parallel architec-
ture of gpus, we opt sampling.

original
ignore
copy
mapping

r1r
28.08
28.81
29.68
29.62

r2r rlr
9.19
25.00
25.31
9.58
25.94
9.98
9.94
25.91

table 3: effect of using different unk post processing
methods on english development set.

input-only
extended-input
full-vocab

r1r r2r rlr
23.96
27.17
24.50
28.08
29.68
25.94

8.98
9.19
9.98

table 4: effect of using different restrictions of output vo-
cabulary on english development set.

that the mrt technique is robust when loss func-
tion varies.    r1r statistically brings signi   cant
improvement for all id74 over id113,
one possible reason is that r1 score correlates
well with human judgement (lin and och, 2004).
hence we decide to utilize    r1r as the default
semantic measure in the experiments (e.g., in sec-
tion 4.5). in addition, table 2 shows that this ar-
gument is still valid on duc2004.

4.4.2 effect of unk post processing

in the training procedure of nhg model, a com-
mon experiment setup is to keep a    xed size of
vocabulary on both input and output side. these
vocabularies are usually shortlists that only con-
tain most frequent words, so that the out of vocab-
ulary words are usually mapped to a special token
   unk   .

there are three typical post-processing meth-
ods to deal with unk tokens. a simplest way is
to ignore them, and we denote it as ignore. an-
other way (jean et al., 2015) is to copy words from
original input directly, and we denote it as copy.
the third way is to replace the unknown words ac-
cording to a dictionary built upon the whole train-
ing set, and we denote it as mapping. we con-
duct experiments on the english development set
to investigate the performance of these methods.
the    xed vocabulary size in nhg model is set to
30,000. experiment results shown in table 3 in-
dicate that the    copy    method performs the best
among three methods and generally improves the
original model. hence, we decide to utilize it as
the default post processing method in our experi-
ments on the test set.

4.5 evaluation results

table 5 shows the evaluation results of head-
line generation on different english test sets.

system

training model architecture

   

linguistic-based
phrase-based

non-neural systems

neural systems

attention-based enc + nnlm
abs + extractive tuning
id98 enc + elman-id56 dec
id98 enc + lstm dec
g-id56 enc + g-id56 dec + trick
g-id56 enc + g-id56 dec
g-id56 enc + g-id56 dec

id113

id113
mrt

duc-2004

gigaword

r2r

rlr r1f 1 r2f 1 rlf 1

6.46
8.13

7.06
8.49
8.26
7.69
9.46
8.60
10.87

20.12
22.85

22.05
23.81
24.06
23.06
24.59
22.25
26.79

-
-

29.55
29.76
33.78
32.55
33.17
32.67
36.54

-
-

11.32
11.88
15.97
14.70
16.02
15.23
16.59

-
-

26.42
26.96
31.15
30.03
30.98
30.56
33.44

r1r

25.12
26.50

26.55
28.18
28.97
27.41
28.35
24.92
30.41

topiary
moses+

abs
abs+
ras-elman
ras-lstm
bwl
this work

table 5: comparison with baseline systems on duc-2004 and gigaword english test sets. g-id56 stands for gated recurrent
neural networks, enc and dec are shorts for encoder and decoder respectively.

system
id56-context
copynet
this work

training model

id113
id113
mrt

g-id56 enc + g-id56 dec + minimum length
g-id56 enc + g-id56 dec + copy mechanism
g-id56 enc + g-id56 dec
g-id56 enc + g-id56 dec

lcsts

r1f 1 r2f 1 rlf 1
27.2
29.9
32.0
35.0
34.9
32.7
35.4
38.2

17.4
22.3
23.3
25.2

table 6: comparison with baseline systems on chinese test set. note that the id56-context has the same model architecture
as ours. but they set a minimum length limit when decoding.

the baseline systems are introduced in section
4.2. these results indicate that nhg model with
id113 achieves comparable performance to exist-
ing headline generation systems. moreover, re-
placing id113 with mrt signi   cantly and consis-
tently improves the performance of nhg model,
and outperforms the state-of-the-art systems on
both test set.

similar results can be observed from the exper-
iment results on chinese headline generation task
as well, as shown in table 67. nhg with mrt
improves the id8 scores up to over 3 points
compared with baseline systems. we also notice
that id113 model is already better than (hu et al.,
2015) and comparable to (gu et al., 2016). this
indicates that a character based model indeed per-
forms good on chinese summary task. moreover,
when evaluating with f1 scores, longer summaries
would be penalized and get lower scores.

figure 2 shows the r1 scores of headlines gen-
erated by nhg+id113 and nhg+mrt on the en-
glish dataset with respect to input lengths. as we
can see, nhg+mrt consistently improves over
nhg+mrt for all lengths.

to reduce the computation complexity when
training nhg model, a possible approach is to re-
strict the size of vocabulary for generated head-
lines. there are three typical methods to deal with

7the mrt result reported here is obtained by taking the
negative f1 score of id8-1 as loss fuction. several re-
alted experiment results are not given due to the length limit.

figure 2: recall scores of id8-1 on duc-2004 test set
over various input lengths.

the size. the    rst one is to restrict the output
words of headline within the input sentence, de-
noted as input-only. the second one is to con-
struct an extended vocabulary that includes simi-
lar words with those appear in the input sentence,
denoted as extended-input. the third one is to
use full vocabulary, denoted as full-vocab. ta-
ble 4 illustrates the experiment results of using dif-
ferent restrictions on the english development set.
the extended vocabulary is constructed by col-
lecting 100 nearest neighbors for each input word,
according to pre-trained google-news word vec-
tors (mikolov et al., 2013). we observe that the
   input-only    and    extened-input    achieve compa-
rable performance while using hundreds of times
less vocabulary. it indicates that it is feasible to
utilize these tricks to train nhg model much more

article (1):

reference:
nhg+id113:
nhg+mrt:
article (2):

reference:
nhg+id113:
nhg+mrt:
article (3):

jose saramago became the    rst writer in portuguese to win the nobel prize for literature on thursday , his
personal delight was seconded by a burst of public elation in his homeland .
jose saramago becomes    rst writer in portuguese to win nobel prize for literature
portuguese becomes portuguese president to win the nobel prize for literature
jose saramago is the    rst writer in the portuguese language to win nobel
a slain russian lawmaker was honored tuesday as a martyr to democratic ideals in a stately funeral service
in which anger mingled freely with tears .
russian lawmaker buried beside greats; mourned as martyr; killers unknown.
slain russian lawmaker remembered as martyr to democracy ( moscow )
slain russian lawmaker honored as martyr in stately funeral service
voting mainly on party lines on a question that has become a touchstone in the debate over development
and preservation of wilderness , the senate on thursday approved a gravel road through remote wildlife
habitat in alaska .
senate approves 30-mile road in alaskan wilderness; precedent? veto likely.

reference:
nhg+id113: us senate passes law allowing road drilling in alaska , alaska
nhg+mrt:
table 7: examples of original articles, reference headlines and generated outputs by different training strategy on duc-2004
test set.

senate passes gravel road through alaska wildlife habitat in alaska

ef   ciently.

4.6 case study

we present several examples for comparison
as shown in table 7. we can observe that: (1)
nhg with mrt is generally capable of capturing
the core information of an article. for example,
the main subject in article 1 is    jose saramago   .
nhg+mrt can successfully    nd the correct topic
and generate a headline about it, but nhg+id113
failed. (2) when both systems capture the same
topic, nhg+mrt can generate more informative
headline. for article 2, nhg+id113 generates
   remembered as    when nhg+mrt generates    
honored as   . considering the context,    honored
as    would be more appropriate. (3) nhg+id113
usually suffer from generating duplicated words
or phrases in headlines. as shown in article 3,
nhg+id113 repeats the phrase    alaska    several
times which leads to a semantically incomplete
headline. nhg+mrt seems to be able to over-
come this problem, bene   tting from directly opti-
mizing sentence-level id8.

5 related work

headline generation is a well-de   ned task stan-
dardized in duc-2003 and duc-2004. various
approaches have been proposed for headline gen-
eration: rule-based, statistical-based and neural-
based.

the rule-based models create a headline for a
news article using handcrafted and linguistically
motivated rules to guide the choice of a potential
headline. hedge trimmer (dorr et al., 2003) is
a representative example of this approach which
creates a headline by removing constituents from

the parse tree of the    rst sentence until it reaches
a speci   c length limit. statistical-based meth-
ods make use of large scale training data to learn
correlations between words in headlines and ar-
ticles (banko et al., 2000). the best system on
duc-2004, topiary (zajic et al., 2004) com-
bines both linguistic and statistical information to
generate headlines. there is also method make use
of knowledge bases to generate better headlines.
with the advances of deep neural networks, there
are growing works that design neural networks
for headline generation. (rush et al., 2015) pro-
poses an attention-based model to generate head-
lines.
(filippova et al., 2015) proposes a recur-
rent neural network with long short term memory
(lstm) (hochreiter and schmidhuber, 1997) for
headline generation. (gu et al., 2016) introduces
copying mechanism into encoder-deconder archi-
tecture inspired by the id193 (vinyals
et al., 2015).

in this work, we propose the nhg model re-
alized by a bidirectional recurrent neural network
with id149. we also propose to ap-
ply minimum risk training (mrt) to optimize pa-
rameters of nhg model. mrt has been widely
used in machine translation (och, 2003; smith and
eisner, 2006; gao et al., 2014; shen et al., 2015),
but less been explored in document summariza-
tion. to the best of our knowledge, this work is
the    rst attempt to utilize mrt in neural headline
generation.

6 conclusion and future work

in this paper, we build an end-to-end neural
headline generation model, which does not re-
quire heavy linguistic analysis and is fully data-

driven. we apply minimum risk training for
model optimization, which effectively incorpo-
rates sentence-wise information by taking various
id74 into consideration. evaluation
result shows that nhg with mrt achieves signi   -
cant and consistent improvements on both english
and chinese datasets, as compared to state-of-the-
art baseline systems including nhg with id113.
there are still many open problems to be explored
as future work: (1) besides article-headline pairs,
there are also rich plain text data not considered in
nhg training. we will investigate the id203
of integrating these plain texts to enhance nhg for
semi-supervised learning. (2) we will investigate
the hybrid approach of incorporating nhg with
other successful headline generation approaches
like sentence compression models.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in proceedings of iclr.

[banko et al.2000] michele banko, vibhu o. mittal,
and michael j. witbrock. 2000. headline genera-
tion based on statistical translation. in proceedings
of acl, pages 318   325.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, dzmitry bahdanau, fethi
bougares, holger schwenk, and yoshua bengio.
2014. learning phrase representations using id56
encoder-decoder for id151.
in proceedings of emnlp, pages 1724   1734.

[chopra et al.2016] sumit chopra, michael auli,
2016.
alexander m. rush, and seas harvard.
abstractive sentence summarization with attentive
in proceedings of
recurrent neural networks.
naacl.

[dorr et al.2003] bonnie dorr, david zajic,

and
richard schwartz.
2003. hedge trimmer: a
parse-and-trim approach to headline generation. in
proceedings of hlt-naacl, pages 1   8.

[elman1990] jeffrey l elman. 1990. finding structure

in time. cognitive science, 14(2):179   211.

[filippova et al.2015] katja filippova, enrique alfon-
seca, carlos a. colmenares, lukasz kaiser, and
oriol vinyals. 2015. sentence compression by dele-
in proceedings of emnlp, pages
tion with lstms.
360   368.

[gu et al.2016] jiatao gu, zhengdong lu, hang li, and
victor o.k. li. 2016. incorporating copying mech-
in pro-
anism in sequence-to-sequence learning.
ceedings of acl.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, pages 1735   1780.

[hu et al.2015] baotian hu, qingcai chen, and fangze
zhu. 2015. lcsts: a large scale chinese short text
summarization dataset. in proceedings of emnlp,
pages 1967   1972.

[jean et al.2015] s  ebastien jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural ma-
chine translation. in proceedings of acl-ijcnlp,
pages 1   10.

[koehn et al.2007] philipp koehn, hieu hoang,
alexandra birch, chris callison-burch, marcello
federico, nicola bertoldi, brooke cowan, wade
shen, christine moran, richard zens, et al. 2007.
moses: open source toolkit for statistical machine
translation. in proceedings of acl, pages 177   180.

[lin and och2004] chin-yew lin and fj och. 2004.
looking for a few good metrics: id8 and its eval-
uation. ntcir workshop.

[lin2004] chin-yew lin. 2004. id8: a package for
automatic evaluation of summaries. in proceedings
of acl, pages 74   81.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg corrado, and jeffrey dean. 2013.
distributed representations of words and phrases
and their compositionality. advances in neural in-
formation processing systems, pages 3111   3119.

[nallapati et al.2016] ramesh nallapati, bing xiang,
2016. sequence-to-sequence

and bowen zhou.
id56s for text summarization. corr.

[och2003] franz josef och. 2003. minimum error rate
in pro-

training in id151.
ceedings of acl, pages 160   167.

[parker et al.2011] robert parker, david graff, junbo
kong, ke chen, and kazuaki maeda. 2011. english
gigaword    fth edition, june.

[ranzato et al.2015] marc   aurelio ranzato,

sumit
chopra, michael auli, and wojciech zaremba.
2015. sequence level training with recurrent neural
networks. arxiv preprint arxiv:1511.06732.

[rush et al.2015] alexander m. rush, sumit chopra,
and jason weston. 2015. a neural attention model
for abstractive sentence summarization. in proceed-
ings of emnlp, pages 379   389.

[gao et al.2014] jianfeng gao, xiaodong he, wen-tau
yih, and li deng.
2014. learning continuous
phrase representations for translation modeling. in
proceedings of acl, pages 699   709.

[shen et al.2015] shiqi shen, yong cheng, zhongjun
he, wei he, hua wu, maosong sun, and yang liu.
2015. minimum risk training for neural machine
translation. arxiv preprint arxiv:1512.02433.

[smith and eisner2006] david a smith and jason eis-
ner. 2006. minimum risk annealing for training
id148. in proceedings of coling/acl,
pages 787   794.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc vv le. 2014. sequence to sequence
in proceedings of
learning with neural networks.
nips, pages 3104   3112.

[vinyals et al.2015] oriol vinyals, meire fortunato,
and navdeep jaitly. 2015. id193.
in
advances in neural information processing systems
28, pages 2674   2682. curran associates, inc.

[zajic et al.2004] david zajic, bonnie dorr,

and
richard schwartz. 2004. bbn/umd at duc-2004:
topiary. in proceedings of hlt-naacl, pages 112   
119.

[zeiler2012] matthew d. zeiler. 2012. adadelta: an

adaptive learning rate method. computer science.

