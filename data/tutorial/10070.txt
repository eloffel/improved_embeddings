4
1
0
2

 

n
u
j
 

5
1

 
 
]
l
c
.
s
c
[
 
 

1
v
0
3
8
3

.

6
0
4
1
:
v
i
x
r
a

modelling, visualising and summarising documents

with a single convolutional neural network

misha denil1 alban demiraj1 nal kalchbrenner1

phil blunsom1 nando de freitas1,2
1university of oxford, united kingdom

{misha.denil,nal.kalchbrenner,phil.blunsom,nando}@cs.ox.ac.uk

2canadian institute for advanced research

a.demiraj@oxfordalumni.org

abstract

capturing the compositional process which maps the meaning of words to that
of documents is a central challenge for researchers in natural language process-
ing and information retrieval. we introduce a model that is able to represent the
meaning of documents by embedding them in a low dimensional vector space,
while preserving distinctions of word and sentence order crucial for capturing nu-
anced semantics. our model is based on an extended dynamic convolution neu-
ral network, which learns convolution    lters at both the sentence and document
level, hierarchically learning to capture and compose low level lexical features
into high level semantic concepts. we demonstrate the effectiveness of this model
on a range of document modelling tasks, achieving strong results with no fea-
ture engineering and with a more compact model. inspired by recent advances in
visualising deep convolution networks for id161, we present a novel vi-
sualisation technique for our document networks which not only provides insight
into their learning process, but also can be interpreted to produce a compelling
automatic summarisation system for texts.

1

introduction

encoding symbolic concepts with distributed representations is a dream that has excited researchers
for decades [9, 1]. in recent years this idea has re-emerged with the successes of neural networks
in language modelling [17], machine translation [19, 11, 4] and other natural language processing
tasks such as chunking and id39 [2].
a particularly fruitful vein of recent research has tackled compositional models of vector space se-
mantics. while algebraic approaches have proved popular for their simplicity [18, 6], it is, arguably,
approaches based on deep neural networks, which have generated the most recent interest. this is
exempli   ed by work on id56s that have been used to great effect for embedding
sentences for tasks such as id31 [22, 21, 8].
most work fusing nlp and neural networks has been based upon fully connected networks, in
either an entirely feed forward manner [21] or including recurrent connections [23, 11, 12]. a
notable exception to this is the model of kalchbrenner et al. [13], which uses a convolutional neural
network (convnet for short) to build continuous distributed representations for sentences.
a fundamental building block of nearly all applications of neural networks to nlp is the creation
of continuous representations for words. since text is fundamentally symbolic and neural networks
operate on continuous inputs, it is necessary to create a mapping from the symbolic representation
of text into the continuous space in which the networks operate. the creation of id27s
has received much attention, and several excellent methods now exist for creating these mappings.

1

in addition to providing a suitable representation for neural networks, id27s have been
shown to capture many semantic relationships between the concepts they represent. word embed-
dings learned by neural networks also serve as an excellent general purpose representation for words
that can be used in non-neural models.
in this paper we show how convnets can be used to build distributed representations of documents.
our model is compositional; it combines id27s into sentence embeddings and then
further combines the sentence embeddings into document embeddings. the combinations at every
level use convnets inspired by the convolution networks that have seen great success in computer
vision. since our model is based on convolutions, it is able to preserve ordering information between
words in a sentence and between sentences in a document. this information is lost in bag-of-words
or id165 models.
going beyond classi   cation, we also show how visualisation techniques developed in the computer
vision literature for understanding the activation patterns of convnets [25, 20], can be applied di-
rectly to our convolutional document model. these visualisations give direct insights into what
the models learn. furthermore, they can also be used to identify important sections of a sentence
or document. as a novel application of this we show how to use the visualisation technique of
simonyan et al. [20] to automatically generate summaries for movie reviews.

2 model description

our model is divided into two levels, a sentence level and a document level, both of which are
implemented using convnets. at the sentence level we use a convnet to transform embeddings for
the words in each sentence into an embedding for the entire sentence. at the document level we
use another convnet to transform sentence embeddings from the    rst level into a single embedding
vector that represents the entire document.
the model is trained by feeding the document embeddings from the second level of the model
into a softmax classi   er, and the convnets in both levels are trained jointly by backpropogation
through the entire model. at the sentence level the weights between convnets that process different
sentences are tied, so every sentence in the document has an embedding which is produced by the
same convnet.
the levels in our model work at different levels of abstraction: the    rst level operates on words in
a sentence and the second level operates on the sentences in a document. the transformations in
each level are performed by convnets; each convnet contains one or more layers of convolution,
pooling and tanh transformations.
the architecture of our model forces information to pass through an intermediate sentence based
representation. this architecture is inspired by gulechere and bengio [7] who show that learning
appropriate intermediate representations helps generalisation, and also by hinton et al. [10] who
show that by forcing information to pass through carefully chosen bottlenecks it is possible to control
the types of intermediate representations that are learned.
for both levels in our model we use a modi   ed version of the dynamic convolutional neural net-
work (did98) from kalchbrenner et al. [13]. this model is similar to the convolutional networks
used in id161 with a cascade of convolution, pooling and tanh transformations, but has
been adapted for text modelling.
a detailed schematic of the sentence level of our model is shown in figure 1, and an overview of the
full model is shown in figure 2 (right). in the following sections we describe how the convolutional
layers operate within each level of our model.

2.1 embedding matrix

the input to each level of our model is an embedding matrix. at the sentence level the columns
of this matrix correspond to embeddings of the words in the sentence being processed, while at the
document level the columns correspond to sentence embeddings produced by the sentence level of
the model.

2

figure 1: id27s are concatenated into columns to form a sentence matrix. the convnet
applies a cascade of convolution, pooling and nonlinearity operations to transform the projected
sentence matrix into an embedding for the sentence. the sentence embeddings are then concatenated
into columns to form a document matrix. the document model then applies its own cascade of
convolution, pooling and nonlinearity operations to form an embedding for the whole document
which is then fed into a softmax classi   er.

at the sentence level, the embedding matrix is built by concatenating embeddings for each word
into the columns of a matrix. the words are drawn from a    xed vocabulary v , which we represent
using a matrix of id27s w     rd  |v |. each column of this matrix is a d dimensional
vector that gives an embedding for a single word in the vocabulary. the id27 vectors
are parameters of the model, and are optimised using backpropogation.

for each sentence s =(cid:2)ws

1

       ws|s|(cid:3) we generate a sentence matrix ss     rd  |s| by concatenat-

ing together the id27 vector corresponding to each word in the sentence.

(cid:34) |

w1
|

       |

wws
|

1

w =

|
|
       w|v |
|
|

ss =

|
|
       wws|s|
|
|

(cid:35)

      

the sentence level of the model produces an embedding vector for each sentence in the document.
the input to the document level is obtained by assembling these sentence embeddings into a docu-
ment matrix, in the same way id27s are assembled into a sentence matrix at the sentence
level.

2.2 convolution
a convolutional layer contains a    lter bank f     rd  wf  nf where wf and nf refer to the width and
number of feature maps respectively. the    rst dimension of each feature map f     rd  wf is equal
to the number of dimensions in the embeddings generated by the layer below.
the convolution operation in our model is one dimensional. we align the    rst axis of each feature
map with the embedding axis and convolve along the rows. at the sentence level this corresponds to
convolving across words and at the document level it corresponds to convolving across sentences.

3

sentencematrixwideconvolutionfeaturemapk-maxpoolingk-maxpoolingpooledrepresentationvectorisationdocumentmatrixthecatsatonthematitwasanicemat!sentenceembeddingsembeddingdimensionsembeddingdimensionsfigure 2: left: a comparison between the convolutions in our model (left) and in the model of
kalchbrenner et al. [13] (right), both using two feature maps (only one is shown). in our model
each feature map looks at every dimension of the layer below and generates a single value for each
position along the sentence matrix, in the model of kalchbrenner et al. each row of each feature
map generates an output in the layer above, so two feature maps convolved with embeddings of
dimension two generate four outputs. right: a schematic of our full model. only one layer of
convolution and pooling is shown in each level, but any part of the model can be made deeper.

each feature map generates a 1d row of numbers, where each value is obtained by applying the
feature map at a different location along the sentence matrix. the outputs of different feature maps
are then stacked to form a new matrix of latent representations which is fed as input to the next layer.
in all cases we use wide (   full   ) convolutions in order for all weights in the feature maps to reach
every word/sentence, including ones on the edges.
unlike the did98, we treat embedding dimensions as channels.
in the did98 each dimension
of each feature map generates its own hidden representation, i.e. a d    wf feature map generates a
representation of size d  |s| + wf     1. in our model the same feature map generates a representation
of size 1  |s| + wf     1. our approach also obviates the need for the    folding    operation that appears
in the did98. this difference is illustrated in figure 2 (left). this change also means our model has
substantially fewer parameters than the did98, since the output of each convolution layer is smaller
by a factor of d.
our approach corresponds to the typical setup in id161 where each feature map is small
in the spatial domain, but spans all channels of the input image [14]. since we work with text, there
is only one    spatial    dimension (the direction of reading) and the different embedding dimensions
correspond to channels.

2.3 k-max pooling

since different sentences and documents have different lengths, not all embedding matrices will be
of the same width. this is not an issue for the convolutional layers, since convolutions can handle
inputs of arbitrary width, but is problematic to use as input to a fully connected layer, or if we want
to generate sentence embeddings for use by another model which expects    xed size inputs.
the solution to this is k-max pooling [13], which is applied to each row of the embedding matrix
separately. to apply k-max pooling to a single row, we keep the k largest values along that row and
discard the rest. since k is a    xed parameter this always generates a    xed size output (if the input
has length less than k we pad it with zeros). for example, applying 2-max pooling to [3, 1, 5, 2]
yields [3, 5]. this procedure is also illustrated graphically in figure 1.
it is not necessary to impose a    xed length on the representations if the next layer is a convolution
layer; however, some pooling is desirable to decrease the size of the representation and has proved
to be very effective in id161. the compromise proposed by kalchbrenner et al. called
dynamic k-max pooling is to set k to be a fraction of the length of the input sentence. in this way
the pooling operation discards (say) half of the inputs instead of all-but-k of them, which can help
information from long sentences or documents to propagate through the model.

4

ourmodeldid98convpoolingsentence1convpoolingsentence2convpoolingsentence3convpoolingtiledsentencemodelsdocumentmodelsoftmaxmodel
id166
binb
maxent
max-tdnn
nbow
did98
our model

errors
66
62
61
76
68
45
46

model
bow (b   t   c)
full+bow
full+unlabelled+bow
wrrbm
wrrbm+bow (bnc)
id166-bi
nbid166-uni
nbid166-bi
paragraph vector
our model

accuracy
88.23%
88.33%
88.89%
87.42%
89.23%
86.95%
88.29%
91.22%
92.58%
89.38%

table 1: left: number of test set errors on the twitter sentiment dataset. the    rst block of three
entries is from go et al. [5], the second block is from kalchbrenner et al. [13]. right: error rates
on the imdb movie review data set. the    rst block is from maas et al. [16], the second from
dahl et al. [3], the third from wang and manning [24] and the fourth from le and mikolov [15].

3 application to multiple tasks

learning a single model that is capable of solving multiple tasks has been one of the holy grails of
the    eld of machine learning. our convnet approach is strongly motivated by this vision. in this
section, we will demonstrate that we can learn a single convnet model to visualise the saliency of
words and sentences in documents, to classify sentences and documents and to summarise docu-
ments.

3.1 classi   cation

in the preceding section, we argued that our convnet model has substantially fewer parameters than
the convnet model of kalchbrenner et al. [13]. parameter parsimony is an important feature for
scaling up our models or embedding them in mobile devices. it is natural to ask ourselves if we have
paid a price in classi   cation performance to attain this feature.
our    rst experiment, which closely reproduces the tweet sentiment classi   cation setting of kalch-
brenner et. al. [13], shows that both models achieve comparable results. the training set contains
1.6 million tweets with weak sentiment labels that were automatically derived from the presence of
emoticons in the tweet text [5]. the test set contains 359 tweets where the sentiment label has been
assigned by a human annotator.
the model we use has two layers of convolution, pooling and tanh transformations using 6 and
14 feature maps with widths 7 and 5, respectively. the id27s are 60 dimensional. this
exactly follows the set up used in kalchbrenner et. al. [13], except that we have used our own version
of the convolution operation and our model has no sum pooling. as a consequence, our model has
substantially fewer parameters than the model of kalchbrenner et. al.. the results are reported in
table 1, which also lists a selection of results of previous work on this data set for comparison. both
our model, with 46 errors, and kalchbrenner   s, with 45 errors, are very close in performance and
signi   cantly better than other competitors.
a central goal of this paper is to develop novel convnet methods for visualisation and summarisa-
tion of reviews. however, to do well in these tasks we also want the same convnet to work well as
a review sentiment classi   er. this is also in line with our aspiration of building models that can be
deployed to solve multiple tasks.
the previous classi   cation experiment showed that our model is a very good sentence classi   er. in
the next experiment, we show that it is also a good document classi   er.
we focus on the imdb movie review sentiment data set, which was originally introduced by
maas et al. [16] as a benchmark for id31. the dataset contains a total of 100000
movie reviews posted on imdb. there are 50000 unlabelled reviews and the remaining 50000 re-
views are divided into a 25000 review training set and a 25000 review test set. each of the labelled

5

reviews has a binary label, either positive or negative. in our experiments, we train only on the
labelled part of the training set.
we pre-process each review by    rst stripping html markup and breaking the review into sentences
and then breaking each sentence into words. we use nltk1 to perform these tasks. we also map
numbers to a generic number token, any symbol that is not in .?! to symbol and any word
that appears fewer than 5 times in the training set to unknown. this leaves us with a 29493 word
vocabulary.
our best model for this data set uses a one layer convnet model to process each sentence, followed
by a one layer convnet model to process each document. the id27s are 10 dimensional
and the sentence model uses 6 feature maps of width 5 followed by a k-max pooling layer of width
4, which leads to sentence embeddings with 360 dimensions. the document model uses 15 feature
maps that each look at 5 adjacent sentences followed by a k-max pooling layer of width 2 which
leads to document embeddings with 30 dimensions.
the results of this experiment are shown in table 1. our model achieves what is, to the best of
our knowledge, the third best published result on this data set. we found this result to be very
encouraging because this dataset is too small to train a convnet model as well as we would like too.
our main challenge in achieving good performance on this task was regularising the model strongly
enough that it would not over   t.
the best result on this data set is achieved by the paragraph vector of le and mikolov [15] . however,
id136 in their model is expensive, since they must perform an optimisation in order to infer
the paragraph vector for an unseen document.
in contrast, we are able to compute a document
embedding using a single feed forward pass.
having shown that we can train our convnet model to be a good sentiment classi   er, in the following
section we capitalise on this to show that our model also enables us to visualise salient features of
documents and to provide users with compact summaries of reviews.

3.2 visualisation and summarisation

in this section we show that recent work in visualising the activations of convnets in computer
vision can also be applied to visualising convnets for text. in addition to providing insights into
what the model has learned, the same techniques can be used to extract automatic summaries of
texts.
deconvolutional networks [26, 27] have been used to great effect to generate interpretable visualisa-
tions of the activations in deep layers of convolutional neural networks used in id161 [25].
more recent work has shown that good visualisations can be obtained by using a single backpro-
pogation pass through the network. in fact this procedure is formally quite similar to the operations
carried out in a deconvolutional net [20]. visualisation through backpropogation is a generalisation
of the deconvolutional approach, since one can backpropogate through non-convolutional layers.
the    rst step in our summarisation procedure is to create a saliency map for the document by as-
signing an importance score to each sentence. to generate the saliency map for a given document,
we adopt the technique of simoyan et al. [20] with a modi   ed objective function.
we    rst perform a forward pass through the network to generate a class prediction for the document.
we then construct a pseudo-label by inverting the network predictions, and feed this to the training
id168 as the true label. this choice of pseudo-label allows us to induce the greatest loss.
to infer saliency for words we take a    rst order taylor expansion of the id168 using the
pseudo-label. formally we use the network function f (x), where x denotes the words that compose
the document being summarised. we approximate the loss as a linear function of x

where   y is the inverted label and

1http://www.nltk.org/

w =

   l
   x

l(  y, f (x))     wt x + b

(cid:12)(cid:12)(cid:12)(cid:12)(  y,f (x))

.

6

the vector w has one entry for each word in the document and we can use |wi| as a measure of the
saliency of word i. these saliency scores can be easily computed by performing a single pass of
backpropogation through the network. the intuition behind using gradient magnitudes as a saliency
measure is that the magnitude of the derivative indicates which words need to be changed the least
to affect the score the most.
we have explained how to generate saliency maps for words but we can use the same technique to
generate saliency maps for sentences in the same way, as our model has a clear separation between
sentence and document level representations. to generate sentence level saliency we simply perform
a partial backpropogation pass through the model, or equivalently we take the taylor expansion with
respect to a partial evaluation of the network. that is,

(cid:32)

w =

   l

   fs(x)

(cid:33)t

(cid:12)(cid:12)(cid:12)(cid:12)(  y,f (x))

fs(x)

where fs denotes the evaluation of the network up to the sentence level.
we use the saliency scores for each sentence to rank the sentences in each document. to generate
a summary of a    xed length k we simply take the k most highly ranked sentences from the review
and use them to form the summary.

3.2.1 evaluation

in order to evaluate the automatic summaries produced by our model we train a na    ve bayes classi-
   er on the imdb movie review sentiment data set and use it to classify our review summaries. we
use tf/idf weighted unigram features with no further processing to train the na    ve bayes model.
the results of this experiment are shown in table 2. we compare the accuracy of na    ve bayes
to summaries of different sizes created by taking the top ranked sentences using the visualisation
technique. even keeping only 20% of each review the accuracy of the classi   er trained on full
reviews drops by less than 1% on the test set. as a baseline we also report the accuracy of the same
classi   er on summaries created by choosing random sentences from each review, and it is clear from
the results that the summaries we create preserve a signi   cant amount of information that is lost by
choosing random sentences.
we also compare against the common summarisation heuristic of building a summary by choosing
the    rst and last sentence of each review. this heuristic performs particularly badly on this data
set, which can be explained by the the fact that many reviews begin with a few sentences of plot
summary, which are generally not relevant to the sentiment of the review.
we show several examples of summaries created by our model in figure 3. as can be seen from
the examples, many of the reviews begin with short descriptions of where the reviewer saw the    lm,
or with a brief summary of the plot. these sentences are not useful as part of the summaries since
they do not express an opinion on the movie being reviewed. our model learns to ignore these
background sentences very consistently.

4 conclusion

in this paper we have introduced a convolutional neural network model that is able to represent
the meaning of documents by embedding them in a low dimensional vector space while preserving
distinctions of word and sentence order, crucial for capturing nuanced semantics.
our model builds the id194 in a compositional manner by combining word em-
beddings into sentence embeddings and then further combining sentence embeddings into a repre-
sentation for the full document.
we have shown that a single model can be used to accomplish a wide variety of document modelling
tasks, including classi   cation and summarisation and visualisation of document structure. these
tasks are all accomplished with a single model trained once on a classi   cation task and no re-training
is needed to apply the model beyond the task for which it was created.
the structure of our model allows us to learn word, sentence and id194s simul-
taneously. an important avenue for future work work is exploring how representations at all three

7

proportion
100%
50%
33%
25%
20%
first and last

summary random margin
   
+3.74
+6.38
+8.04
+9.47

83.03
79.79
76.72
74.87
73.20

83.03
83.53
83.10
82.91
82.67
68.62

fixed

summary random margin

pick 5
pick 4
pick 3
pick 2

83.07
83.09
82.88
82.04

80.02
79.05
77.15
74.48

+3.05
+4.04
+5.73
+7.56

table 2: results of classifying summaries with na    ve bayes. results labelled proportion indicate
selecting up to the indicated percentage of sentences in the review, and results labelled    xed show
the result of selecting a    xed number of sentences from each. the summary column shows the
accuracy of na    ve bayes on summaries produced by our model. the random column shows the
same model classifying summaries created by selecting sentences at random. the margin column
shows the difference in accuracy between our model and the random summaries.

i caught this movie on the sci-fi channel recently. it actually turned out to be pretty decent as far as b-list horror/suspense    lms go. two guys (one naive and one
loud mouthed a **) take a road trip to stop a wedding but have the worst possible luck when a maniac in a freaky, make-shift tank/truck hybrid decides
to play cat-and-mouse with them. things are further complicated when they pick up a ridiculously whorish hitchhiker. what makes this    lm unique is that the
combination of comedy and terror actually work in this movie, unlike so many others. the two guys are likable enough and there are some good chase/suspense
scenes. nice pacing and comic timing make this movie more than passable for the horror/slasher buff. definitely worth checking out.

i just saw this on a local independent station in the new york city area. the cast showed promise but when i saw the director, george cosmotos, i became
suspicious. and sure enough, it was every bit as bad, every bit as pointless and stupid as every george cosmotos movie i ever saw. he   s like a stupid man   s
michael bey     with all the awfulness that accolade promises. there   s no point to the conspiracy, no burning issues that urge the conspirators on. we are left to
ourselves to connect the dots from one bit of graf   ti on various walls in the    lm to the next. thus, the current budget crisis, the war in iraq, islamic extremism, the
fate of social security, 47 million americans without health care, stagnating wages, and the death of the middle class are all subsumed by the sheer terror of graf   ti. a
truly, stunningly idiotic    lm.

graphics is far from the best part of the game. this is the number one best th game in the series. next to underground. it deserves strong love. it is an insane
game. there are massive levels, massive unlockable characters... it   s just a massive game. waste your money on this game. this is the kind of money that is
wasted properly. and even though graphics suck, thats doesn   t make a game good. actually, the graphics were good at the time. today the graphics are crap. who
cares? as they say in canada, this is the fun game, aye. (you get to go to canada in thps3) well, i don   t know if they say that, but they might. who knows. well,
canadian people do. wait a minute, i   m getting off topic. this game rocks. buy it, play it, enjoy it, love it. it   s pure brilliance.

the    rst was good and original. i was a not bad horror/comedy movie. so i heard a second one was made and i had to watch it . what really makes this movie work
is judd nelson   s character and the sometimes clever script. a pretty good script for a person who wrote the final destination    lms and the direction was okay.
sometimes there   s scenes where it looks like it was    lmed using a home video camera with a grainy - look. great made - for - tv movie. it was worth the rental
and probably worth buying just to get that nice eerie feeling and watch judd nelson   s stanley doing what he does best. i suggest newcomers to watch the    rst
one before watching the sequel, just so you   ll have an idea what stanley is like and get a little history background.

when the movie was released it was the biggest hit and it soon became the blockbuster. but honestly the movie is a ridiculous watch with a plot which glori   es
a loser. the movie has a tag - line -    preeti madhura, tyaga amara    which means love   s sweet but sacri   ce is immortal. in the movie the hero of the movie
(ganesh) sacri   ces his love for the leading lady (pooja gandhi) even though the two loved each other! his justi   cation is the meaning of the tag - line. this
movie in   uenced so many young broken hearts that they found this    loser - like sacri   cial    attitude very thoughtful and hence became the cult movie it is, when they
could have moved on with their lives. ganesh   s acting in the movie is amateurish, crass and childishly stupid. he actually looks funny in a song, (onde ondu
sari ...) when he   s supposed to look all stylish and cool. his looks don   t help the leading role either. his hair style is badly done in most part of the movie. pooja
gandhi cant act. her costumes are horrendous in the movie and very inconsistent. the good part about the movie is the excellent cinematography and
brilliant music by mano murthy which are actually the true saving graces of the movie. also the lyrics by jayant kaikini are very well penned. the director
yograj bhat has to be lauded picturization the songs in a tasteful manner. anyway all - in - all except for the songs, the movie is a very ordinary one !!!!!!

a friend and i went through a phase some (alot of) years ago of selecting the crappest horror    lms in the video shop for an evening   s entertainment. for some reason,
i ended up buying this one (probably v. v. cheap). the cheap synth soundtrack is a classic of its time and genre. there   s also a few very amusing scenes. among
them is a scene where a man   s being attacked and defends himself with a number of unlikely objects, it made me laugh at the time (doesn   t seem quite so funny in
retrospect but there you go). apart from that it   s total crap, mind you. but probably worth a watch if you like    lms like    chopping mall   . yes, i   ve seen that too.

i tried restarting the movie twice. i put it in three machines to see what was wrong . did steven seagal   s voice change? did he die during    lming and the studio
have to dub the sound with someone who doesn   t even resemble him? or was the sound on the dvd destroyed? after about 10 minutes, you    nally hear the
actor   s real voice. though throughout most of the    lm, it sounds like the audio was recorded in a bathroom. i would be ashamed to donate a copy of this movie to
goodwill, if i owned a copy. i rented it, but i will never do that again. i will check this database before renting any more of his movies, all of which were (more or
less) good movies. you usually knew what you were getting when you watched a steven seagal movie. i guess that is no more.

vertigo co - stars stewart (in his last turn as a romantic lead) and novak elevate this, stewart   s other    christmas movie,    movie to above mid - level entertainment.
the chemistry between the two stars makes for a fairly moving experience and further revelation can be gleaned from the movie if witchcraft is seen as a
metaphor for the private pain that hampers many people   s relationships. all in all, a nice diversion with legendary stars, 7/10

figure 3: several example summaries created by our convnet. the full text of the review is shown
in black and the sentences selected by the convnet appear in colour. while summarising a review
with the    rst sentence is a popular pragmatic approach, it is clear in these examples that this heuristic
is not as effective as the convnet summarisation scheme. each summary is created by selecting up
to 20% of the sentences in the review.

levels can be further exploited. another important directions for future work is understanding how
our model can be trained using unlabelled data.

8

references
[1] y. bengio, r. ducharme, p. vincent, and c. jauvin. a neural probabilistic language model. journal of

machine learning research, 3:1137   1155, 2003.

[2] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa. natural language process-

ing (almost) from scratch. the journal of machine learning research, 12:2461   2505, 2011.

[3] g. e. dahl, r. p. adams, and h. larochelle. training restricted id82s on word observa-

tions. in international conference on machine learning, 2012.

[4] j. devlin, r. zbib, z. huang, t. lamar, r. schwartz, and j. makhoul. fast and robust neural network

joint models for id151. in association for computational linguistics, 2014.

[5] a. go, r. bhayani, and l. huang. twitter sentiment classi   cation using distant supervision. technical

report, 2009.

[6] e. grefenstette and m. sadrzadeh. experimental support for a categorical compositional distributional
model of meaning. in proceedings of the conference on empirical methods in natural language pro-
cessing, pages 1394   1404, 2011.

[7] c. gulcehre and y. bengio. knowledge matters: importance of prior information for optimization. in

international conference on learning representations, 2013.

[8] k. m. hermann and p. blunsom. the role of syntax in vector space models of id152.
proceedings of the 51st annual meeting of the association for computational linguistics, pages 894   904,
2013.

[9] g. e. hinton. learning distributed representations of concepts. in annual conference of the cognitive

science society, pages 1   12, 1986.

[10] g. e. hinton, a. krizhevsky, and s. d. wang. transforming auto-encoders. in international conference

on arti   cial neural networks, 2011.

[11] n. kalchbrenner and p. blunsom. recurrent continuous translation models. in empirical methods in

natural language processing, 2013.

[12] n. kalchbrenner and p. blunsom. recurrent convolutional neural networks for discourse composition-

ality. in meeting of the association for computational linguistics, 2013.

[13] n. kalchbrenner, e. grefenstette, and p. blunsom. a convolutional neural network for modelling sen-

tences. in association for computational linguistics, 2014.

[14] a. krizhevsky, i. sutskever, and g. hinton. id163 classi   cation with deep convolutional neural net-

works. in advances in neural information processing systems, 2012.

[15] q. le and t. mikolov. distributed representations of sentences and documents. in international con-

ference on machine learning, volume 32, 2014.

[16] a. l. maas, r. e. daly, p. t. pham, d. huang, a. y. ng, and c. potts. learning word vectors for
id31. in the 49th annual meeting of the association for computational linguistics, 2011.
[17] t. mikolov, k. chen, g. corrado, and j. dean. distributed representations of words and phrases and

their compositionality. in neural information processing systems, pages 1   9, 2013.

[18] j. mitchell and m. lapata. composition in distributional models of semantics. cognitive science,

34(8):1388   1439, 2010.

[19] h. schwenk. continuous space translation models for phrase-based id151. in

international conference on computational linguistics, pages 1071   1080, 2012.

[20] k. simonyan, a. vedaldi, and a. zisserman. deep inside convolutional networks : visualising image

classi   cation models and saliency maps. technical report, 2013.

[21] r. socher, b. huval, c. d. manning, and a. y. ng. semantic compositionality through recursive matrix-

vector spaces. in conference on empirical methods in natural language processing, 2012.

[22] r. socher, j. pennington, e. h. huang, a. y. ng, and c. d. manning. semi-supervised recursive
autoencoders for predicting sentiment distributions. in conference on empirical methods in natural
language processing, number i, 2011.

[23] i. sutskever, j. martens, and g. hinton. generating text with recurrent neural networks. in international

conference on machine learning, 2011.

[24] s. wang and c. d. manning. baselines and bigrams: simple , good sentiment and topic classi   cation.

in association for computational linguistics, volume 94305, 2012.

[25] m. d. zeiler and r. fergus. visualizing and understanding convolutional networks. technical report,

2012.

9

[26] m. d. zeiler, d. krishnan, g. w. taylor, and r. fergus. deconvolutional networks. in id161

and pattern recognition, 2010.

[27] m. d. zeiler, g. w. taylor, and r. fergus. adaptive deconvolutional networks for mid and high level

id171. in international conference on id161, 2011.

10

