real-time web scale event summarization using sequential decision making

chris kedzie

columbia university

dept. of computer science
kedzie@cs.columbia.edu

fernando diaz

microsoft research
fdiaz@microsoft.com

kathleen mckeown
columbia university

dept. of computer science

kathy@cs.columbia.edu

6
1
0
2

 

y
a
m
2
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
4
6
6
3
0

.

5
0
6
1
:
v
i
x
r
a

abstract

we present a system based on sequential deci-
sion making for the online summarization of mas-
sive document streams, such as those found on the
web. given an event of interest (e.g.
   boston
marathon bombing   ), our system is able to    lter the
stream for relevance and produce a series of short
text updates describing the event as it unfolds over
time. unlike previous work, our approach is able
to jointly model the relevance, comprehensiveness,
novelty, and timeliness required by time-sensitive
queries. we demonstrate a 28.3% improvement in
summary f1 and a 43.8% improvement in time-
sensitive f1 metrics.

1 introduction
tracking unfolding news at web-scale continues to be a chal-
lenging task. crisis informatics, monitoring of breaking
news, and intelligence tracking all have dif   culty in identi-
fying new, relevant information within the massive quantities
of text that appear online each second. one broad need that
has emerged is the ability to provide realtime event-speci   c
updates of streaming text data which are timely, relevant, and
comprehensive while avoiding redundancy.

unfortunately, many approaches have adapted standard au-
tomatic id57 techniques that are
inadequate for web scale applications. typically, such sys-
tems assume full retrospective access to the documents to
be summarized, or that at most a handful of updates to the
summary will be made [dang and owczarzak, 2008]. fur-
thermore, evaluation of these systems has assumed reliable
and relevant input, something missing in an inconsistent, dy-
namic, noisy stream of web or social media data. as a result,
these systems are poor    ts for most real world applications.

in this paper, we present a novel streaming-document
summarization system based on sequential decision making.
speci   cally, we adopt the    learning to search    approach, a
technique which adapts methods from id23
for id170 problems [daum  e iii et al., 2009;
ross et al., 2011]. in this framework, we cast streaming sum-
marization as a form of greedy search and train our system to
imitate the behavior of an oracle summarization system.

4:19 p.m.     two explosions shattered the euphoria of the
boston marathon    nish line on monday, sending authorities
out on the course to carry off the injured while the stragglers
were rerouted away...
4:31 p.m.     police in new york city and london are stepping
up security following explosions at the boston marathon.
4:31 p.m.     a senior u.s. intelligence of   cial says two more
explosive devices have been found near the scene of the boston
marathon where two bombs detonated earlier.
5:10 p.m.     several candidates for massachusetts senate spe-
cial election have suspended campaign activity in response to
the explosions...

figure 1: excerpt of summary for the query    boston
marathon bombing    generated from an input stream.

given a stream of sentence-segmented news webpages and
an event query (e.g.    boston marathon bombing   ), our sys-
tem monitors the stream to detect relevant, comprehensive,
novel, and timely content. in response, our summarizer pro-
duces a series of short text updates describing the event as
it unfolds over time. we present an example of our realtime
update stream in figure 1. we evaluate our system in a crisis
informatics setting on a diverse set of event queries, covering
severe storms, social unrest, terrorism, and large accidents.
we demonstrate a 28.3% improvement in summary f1 and a
43.8% improvement in time-sensitive f1 metrics against sev-
eral state-of-the-art baselines.

2 related work
id57 (mds) has long been stud-
ied by the natural language processing community. we fo-
cus speci   cally on extractive summarization, where the task
is to take a collection of text and select some subset of sen-
tences from it that adequately describes the content subject to
some budget constraint (e.g. the summary must not exceed k
words). for a more in depth survey of the    eld, see [nenkova
and mckeown, 2012].

because labeled training data is often scarce, unsupervised

approaches to id91 and ranking predominate the    eld.

popular approaches involve ranking sentences by various
notions of input similarity or graph centrality [radev et al.,

2000; erkan and radev, 2004].

other ranking based methods use coverage of topic sig-
natures [lin and hovy, 2000], or kl divergence between
input/summary word distributions [haghighi and vander-
wende, 2009] as the ranking, possibly adding some diversity
penalty to ensure broader coverage.

update summarization research has primarily focused on
producing a 100 word summary from a set of 10 documents,
assuming the reader is familiar with a different initial set
of 10 documents [dang and owczarzak, 2008]. generally
the approaches to update summarization have adapted the
above techniques. top performers at the 2011 text analysis
conference (the last year update summarization was a task)
made use of graph ranking algorithms, and topic model/topic-
signature style importance estimates [du et al., 2011; mason
and charniak, 2011; conroy et al., 2011].

streaming or temporal summarization was    rst explored in
the context of topic detection and tracking [allan et al., 2001]
and more recently at the text retrieval conference (trec)
[aslam et al., 2013]. top performers at trec included an
af   nity propagation id91 approach [kedzie et al., 2015]
and a ranking/mds system combination method [mccreadie
et al., 2014]. both methods are unfortunately constrained to
work in hourly batches, introducing potential latency. per-
haps most similar to our work is that of [guo et al., 2013]
which iteratively    ts a pair of regression models to predict
ngram recall and precision of candidate updates to a model
summary. however, their learning objective fails to account
for errors made in subsequent prediction steps.

3 problem de   nition
a streaming summarization task is composed of a brief text
query q, including a categorical event type (e.g.    earthquake   ,
   hurricane   ), as well as a document stream [x0, x1, . . .]. in
practice, we assume that each document is segmented into a
sequence of sentences and we therefore consider a sentence
stream [x0, x1, . . .]. a streaming summarization algorithm
then selects or skips each sentence x as it is observed such
that the end user is provided a    ltered stream of sentences
that are relevant, comprehensive, low redundancy, and timely
(see section 5.2). we refer to the selected sentences as up-
dates and collectively they make up an update summary. we
show a fragment of an update summary for the query    boston
marathon bombing    in figure 1.

4 streaming summarization as sequential

decision making

we could na    vely treat this problem as classi   cation and pre-
dict which sentences to select or skip. however, this would
make it dif   cult to take advantage of many features (e.g. sen-
tence novelty w.r.t. previous updates). what is more concern-
ing, however, is that the classi   cation objective for this task
is somewhat ill-de   ned: successfully predicting select on one
sentence changes the true label (from select to skip) for sen-
tences that contain the same information but occur later in the
stream.

in this work, we pose streaming summarization as a greedy
search over a binary branching tree where each level corre-

stream position

1

s ele ct

skip

2

3

figure 2: search space for a stream of size two. the depth
of the tree corresponds to the position in the stream. left
branches indicate selecting the current sentence as an update.
right branches skip the current sentence. the path in green
corresponds to one trajectory through this space consisting of
a select sentence one, then skip sentence 2. the state repre-
sented by the hollow dot corresponds to the stream at sentence
position 2 with the update summary containing sentence 1.

sponds to a position in the stream (see figure 2). the height
of the tree corresponds to the length of stream. a path through
the tree is determined by the system select and skip decisions.
when treated as a sequential decision making problem,
our task reduces to de   ning a policy for selecting a sen-
tence based on its properties as well as properties of its an-
cestors (i.e. all of the observed sentences and previous deci-
sions). the union of properties   also known as the features   
represents the current state in the decision making process.

the feature representation provides state abstraction both
within a given query   s search tree as well as to states in other
queries    search trees, and also allows for complex interactions
between the current update summary, candidate sentences,
and stream dynamics unlike the classi   cation approach.

in order to learn an effective policy for a query q, we can
take one of several approaches. we could use a simulator to
provide feedback to a id23 algorithm. al-
ternatively, if provided access to an evaluation algorithm at
training time, we can simulate (approximately) optimal deci-
sions. that is, using the training data, we can de   ne an oracle
policy that is able to omnisciently determine which sentences
to select and which to skip. moreover, it can make these de-
terminations by starting at the root or at an arbitrary node in
the tree, allowing us to observe optimal performance in states
unlikely to be reached by the oracle. we adopt locally op-
timal learning to search to learn our model from the oracle
policy [chang et al., 2015].

in this section, we will begin by describing the learning
algorithm abstractly and then in detail for our task. we will
conclude with details on how to train the model with an oracle
policy.
4.1 algorithm
in the induced search problem, each search state st cor-
responds to observing the    rst t sentences in the stream
x1, . . . , xt and a sequence of t     1 actions a1, . . . , at   1. for
all states s     s, the set of actions is a     {0, 1} with 1 indicat-
ing we add the t-th sentence to our update summary, and 0 in-
dicating we ignore it. for simplicity, we assume a    xed length
stream of size t but this is not strictly necessary. from each
input stream, x = x1, . . . , xt , we produce a corresponding
output a     {0, 1}t . we use x:t to indicate the    rst t elements
of x.

input: {xq,      

q}q   q, number of iterations n, and a

mixture parameter        (0, 1) for roll-out.

output:     
1 initialize     0
2 i     0
3 for n     {1, 2, . . . , n} do
4
5
6
7

for q     q do

          
for t     {0, 1, . . . , t     1} do

roll-in by executing     i for t rounds and
reach st.
for a     a(st) do
let   o =      
compute ct(a) by rolling out with   o
              {(cid:104)  (st), a, ct(a)(cid:105)}

q with id203   , else     i.

    i+1     update cost-sensitive classi   er(    i,   )
i     i + 1

8
9
10
11
12
13
14 return     i.

algorithm 1: locally optimal learning to search.

for a training query q, a reference policy      

q can be con-
structed from the training data. speci   cally, with access to
the relevance and novelty of every xi in the stream, we can
omnisciently make a decision whether to select or skip based
on a long term metric (see section 5.2). the goal then is to
learn a policy      that imitates the reference well across a set
of training queries q. we encode each state as a vector in rd
with a feature function    and our learned policy is a mapping
     : rd     a of states to actions.

we train      using locally optimal learning to search [chang
et al., 2015], presented in algorithm 1. the algorithm op-
erates by iteratively updating a cost-sensitive classi   er. for
each training query, we construct a query-speci   c training set
   by simulating the processing of the training input stream
xq. the instances in    are triples comprised of a feature vec-
tor derived from the current state s, a candidate action a, and
the cost c(a) associated with taking action a in state s. con-
structing    consists of (1) selecting states and actions, and (2)
computing the cost for each state-action pair.

the number of states is exponential in t , so constructing
   using the full set of states may be computationally pro-
hibitive. beyond this, the states in    would not be represen-
tative of those visited at test time. in order to address this,
we sample from s by executing the current policy      through-
out the training simulation, resulting in t state samples for   ,
(lines 5-12).
given a sampled state s, we need to compute the cost of
taking actions a     {0, 1}. with access to a query-speci   c or-
acle,      
q , we can observe it   s preferred decision at s and penal-
ize choosing the other action. the magnitude of this penalty
is proportional to the difference in expected performance be-
tween the oracle decision and the alternative decision. the
performance of a decision is derived from a id168 (cid:96),
to be introduced in section 4.3. importantly, our id168
is de   ned over a complete update summary, incorporating the
implications of selecting an action on future decisions. there-
fore, our cost needs to incorporate a sequence of decisions af-

ter taking some action in state s. the algorithm accomplishes
this by rolling out a policy after a until the stream has been
exhausted (line 10). as a result, we have a pre   x de   ned by
    , an action, and then a suf   x de   ned by the roll out pol-
icy. in our work, we use a mixture policy that combines both
the current model      and the oracle       (line 9). this mixture
policy encourages learning from states that are likely to be
visited by the current learned policy but not by the oracle.

after our algorithm has gathered    for a speci   c q using     i,
we train on the data to produce     i+1. here     i is implemented
as a cost-sensitive classi   er, i.e. a id75 of the
costs on features and actions; the natural policy is to select
the action with lowest predicted cost. with each query, we
update the regression with stochastic id119 on the
newly sampled (state, action, cost) tuples (line 12). we repeat
this process for n passes over all queries in the training set.
in the following sections, we specify the feature function
  , the loss (cid:96), and our reference policy      .
4.2 features
as mentioned in the previous section, we represent each state
as a feature vector. in general, at time t, these features are
functions of the current sentence (i.e. xt), the stream history
(i.e. x:t), and/or the decision history (a:t   1). we refer to
features only determined by xt as static features and all others
as dynamic features. 1
static features
basic features our most basic features look at the length in
words of a sentence, its position in the document, and the ra-
tio of speci   c named entity tags to non-named entity tokens.
we also compute the average number of sentence tokens that
match the event query words and synonyms using id138.
language model features similar to [kedzie et al.,
2015], we compute the average token log id203 of the
sentence on two language models: i) an event type speci   c
language model and ii) a general newswire language model.
the    rst language model is built from wikipedia articles rel-
evant to the event-type domain. the second model is built
from the new york times and associate press sections of the
gigaword-5 corpus [graff and cieri, 2003].

single document summarization features these fea-
tures are computed using the current sentence   s document as
a context and are also commonly used as ranking features in
other document summarization systems. where a similarity
or distance is need, we use either a tf-idf bag-of-words or k-
dimensional latent vector representation. the latter is derived
by projecting the former onto a k-dimensional space using the
weighted textual id105 method [guo and diab,
2012].

we compute sumbasic features [nenkova and vander-
wende, 2005]:
the average and sum of unigram probabili-
ties in a sentence. we compute the arithmetic and geometric
means of the sentence   s cosine distance to the other sentences
of the document [guo et al., 2013]. we refer to this quantity
as novelty and compute it with both vector representations.

1we have attempted to use a comprehensive set of static features used in previ-
ous summarization systems. we omit details for space but source code is available at:
https://github.com/kedz/ijcai2016

we also compute the centroid rank [radev et al., 2000] and
lexrank of each sentence [erkan and radev, 2004], again
using both vector representations.

summary content id203 for a subset of the stream
sentences we have manual judgements as to whether they
match to model summary content or not (see sec. 5.1, ex-
panding relevance judgments). we use this data (restricted
to sentences from the training query streams), to train a deci-
sion tree classi   er, using the sentences    term ngrams as clas-
si   er features. as this data is aggregated across the training
queries, the purpose of this classi   er is to capture the impor-
tance of general ngrams predictive of summary worthy con-
tent.

using this classi   er, we obtain the id203 that the cur-
rent sentence xt contains summary content and use this as a
model feature.
dynamic features
stream language models we maintain several unigram lan-
guage models that are updated with each new document in
the stream. using these counts, we compute the sum, aver-
age, and maximum token id203 of the non-stop words
in the sentence. we compute similar quantities restricted to
the person, location, and organization named entities.

update similarity the average and maximum cosine sim-
ilarity of the current sentence to all previous updates is com-
puted under both the tf-idf bag-of-words and latent vector
representation. we also include indicator features for when
the set of updates is empty (i.e. at the beginning of a run) and
when either similarity is 0.

document frequency we also compute the hour-to-hour
percent change in document frequency of the stream. this
feature helps gauge breaking developments in an unfolding
event. as this feature is also heavily affected by the daily
news cycle (larger average document frequencies in the morn-
ing and evening) we compute the 0-mean/unit-variance of this
feature using the training streams to    nd the mean and vari-
ance for each hour of the day.

feature interactions many of our features are helpful for
determining the importance of a sentence with respect to its
document. however, they are more ambiguous for determin-
ing importance to the event as a whole. for example, it is not
clear how to compare the document level id95 of sen-
tences from different documents. to compensate for this, we
leverage two features which we believe to be good global in-
dicators of update selection: the summary content id203
and the document frequency. these two features are prox-
ies for detecting (1) a good summary sentences (regardless of
novelty with respect to other previous decisions) and (2) when
an event is likely to be producing novel content. we compute
the conjunctions of all previously mentioned features with the
summary content id203 and document frequency sepa-
rately and together.
4.3 oracle policy and id168
much of the id57 literature em-
ploys greedy selection methods. we adopt a greedy oracle
that selects a sentence if it improves our evaluation metric
(see section 5.2).

we design our id168 to penalize policies that
severely over- or under-generate. given two sets of decisions,
usually one from the oracle and another from the candidate
model, we de   ne the loss as the complement of the dice co-
ef   cient between the decisions,
(cid:96)(a, a(cid:48)) = 1     2   

(cid:80)
(cid:80)
i aia(cid:48)
i ai + a(cid:48)

.

i

i

this encourages not only local agreement between policies
(the numerator of the second term) but that the learned and
oracle policy should generate roughly the same number of
updates (the denominator in the second term).

5 materials and methods
5.1 data
we evaluate our method on the publicly available trec tem-
poral summarization track data.2 this data is comprised of
three parts.

the corpus consists of a 16.1 terabyte set of 1.2 billion
timestamped documents crawled from the web between oc-
tober, 2011 and february 2013 [frank et al., 2012]. the crawl
includes news articles, forum data, weblogs, as well as a va-
riety of other crawled web pages.3

the queries consist of a set of 44 events which occurred
during the timespan of the corpus. each query has an as-
sociated time range to limit the experiment to a timespan of
interest, usually around two weeks. in addition, each query is
associated with an    event category    (e.g.    earthquake   ,    hur-
ricane   ). each query is also associated with an ideal sum-
mary, a set of short, timestamped textual descriptions of facts
about the event. the items in this set, also known as nuggets,
are considered the completed and irreducible sub-events as-
sociated with the query. for example, the phrases    multi-
ple people have been injured    and    at least three people have
been killed    are two of the nuggets extracted for the query
   boston marathon bombing   . on average, 73.35 nuggets were
extracted for each event.

the relevance judgments consist of a sample of sentences
pooled from participant systems, each of which has been
manually assessed as related to one or more of a query   s
nuggets or not. for example, the following sentence,    two
explosions near the    nish line of the boston marathon on
monday killed three people and wounded scores,    matches
the nuggets mentioned above. the relevance judgments can
be used to compute id74 (section 5.2) and, as a
result, to also de   ne our oracle policy (section 4.3).
expanding relevance judgments
because of the large size of the corpus and the limited size
of the sample, many good candidate sentences were not man-
ually reviewed. after aggressive document    ltering (see be-
low), less than 1% of the sentences received manual review.
in order to increase the amount of data for training and eval-
uation of our system, we augmented the manual judgements
with automatic or    soft    matches. a separate gradient boost-
ing classi   er was trained for each nugget with more than

2http://www.trec-ts.org/
3http://streamcorpus.org/

10 manual sentence matches. manually matched sentences
were used as positive training data and an equal number of
manually judged non-matching sentences were used as nega-
tive examples. sentence ngrams (1-5), percentage of nugget
terms covered by the sentence, semantic similarity of the sen-
tence to nugget were used as features, along with an inter-
action term between the semantic similarity and coverage.
when augmenting the relevance judgments with these nugget
match soft labels, we only include those that have a prob-
ability greater than 90% under the classi   er. overall these
additional labels increase the number of matched sentences
by 1600%.

for evaluation, the summarization system only has access
to the query and the document stream, without knowledge of
any nugget matches (manual or automatic).
document filtering
for any given event query, most of the documents in the cor-
pus are irrelevant. because our queries all consist of news
events, we restrict ourselves to the news section of the cor-
pus, consisting of 7,592,062 documents.

these documents are raw web pages, mostly from local
news outlets running stories from syndication services (e.g.
reuters), in a variety of layouts. in order to normalize these
inputs we    ltered the raw stream for relevancy and redun-
dancy with the following three stage process.

we    rst preprocessed each document   s raw html using an
article extraction library.4 articles were truncated to the    rst
20 sentences. we then removed any articles that did not con-
tain all of the query keywords in the article text, resulting
in one document stream for each query. finally, documents
whose cosine similarity to any previous document was > .8
were removed from the stream.
5.2 metrics
we are interested in measuring a summary   s relevance, com-
prehensiveness, redundancy, and latency (the delay in select-
ing nugget information). the temporal summarization track
adopts three principle metrics which we review here. com-
plete details can be found in the track   s of   cial metrics doc-
ument.5 we use the of   cial evaluation code to compute all
metrics.

given a system   s update summary a and our sentence-level
relevance judgments, we can compute the number of match-
ing nuggets found. importantly, a summary only gets credit
for the number of unique matching nuggets, not the number
of matching sentences. this prevents a system from receiving
credit for selecting several sentences which match the same
nugget. we refer to the number of unique matching nuggets
as the gain. we can also penalize a system which retrieves
a sentence matching a nugget far after the timestamp of the
nugget. the latency-penalized gain discounts each match   s
contribution to the gain proportionally to the delay of the    rst
matching sentence.

the gain value can be used to compute latency and
redundancy-penalized analogs to precision and recall. specif-
ically, the expected gain divides the gain by the number of

4https://github.com/grangier/python-goose
5 http://www.trec-ts.org/trec2015-ts-metrics.pdf

system updates. this precision-oriented metric can be con-
sidered the expected number of new nuggets in a sentence se-
lected by the system. the comprehensiveness divides the gain
by the number of nuggets. this recall-oriented metric can be
considered the completeness of a user   s information after the
termination of the experiment. finally, we also compute the
harmonic mean of expected gain and comprehensiveness (i.e.
f1). we present results using either gain or latency-penalized
gain in order to better understand system behavior.

to evaluate our model, we randomly select    ve events to
use as a development set and then perform a leave-one-out
style evaluation on the remaining 39 events.

5.3 model training
even after    ltering, each training query   s document stream is
still too large to be used directly in our combinatorial search
space.
in order to make training time reasonable yet rep-
resentative, we downsample each stream to a length of 100
sentences. the downsampling is done uniformly over the en-
tire stream. this is repeated 10 times for each training event
to create a total of 380 training streams. in the event that a
downsample contains no nuggets (either human or automati-
cally labeled) we resample until at least one exists in the sam-
ple.

in order to avoid over-   tting, we select the model iteration
for each training fold based on its performance (in f1 score
of expected gain and comprehensiveness) on the development
set.

5.4 baselines and model variants
we refer to our    learning to search    model in the results as
ls. we compare our proposed model against several base-
lines and extensions.

cosine similarity threshold one of the top perform-
ing systems in temporal-summarization at trec 2015 was
a heuristic method that only examined article    rst sentences,
selecting those that were below a cosine similarity threshold
to any of the previously selected updates. we implemented a
variant of that approach using the latent-vector representation
used throughout this work. the development set was used
to set the threshold. we refer to this model as cos (team
waterlooclarke at trec 2015).

af   nity propagation the next baseline was a top per-
former at the previous year   s trec evaluations [kedzie et al.,
2015]. this system processes the stream in non-overlapping
windows of time, using af   nity propagation (ap) id91
[frey and dueck, 2007] to identify update sentences (i.e. sen-
tences that are cluster centers). as in the cos model, a simi-
larity threshold is used to    lter out updates that are too similar
to previous updates (i.e. previous id91 outputs). we use
the summary content id203 feature as the preference or
salience parameter. the time window size, similarity thresh-
old, and an offset for the cluster preference are tuned on the
development set. we use the authors    publicly available im-
plementation and refer to this method as apsal.

learning2search+cosine similarity threshold in this
model, which we refer to as lscos, we run ls as before,
but    lter the resulting updates using the same cosine similar-

exp. gain
0.119c
0.075
0.097
0.115c,l

unpenalized
comp.
0.09
0.176s
0.207s,f
0.189s

f1
0.094
0.099
0.112
0.127s,c,l

apsal
cos
ls
lscos

latency-penalized

exp. gain
0.105
0.095
0.136c
0.162s,c,l

comp.
0.088
0.236s
0.306s,c,f
0.276s

f1
0.088
0.128s
0.162s
0.184s,c,l

num. updates

8.333

145.615s,f
89.872s,f
29.231s,c

figure 3: average system performance and average number of updates per event. superscripts indicate signi   cant improve-
ments (p < 0.05) between the run and competing algorithms using the paired randomization test with the bonferroni correction
for multiple comparisons (s: apsal, c: cos, l: ls, f: lscos).

latency-penalized
f1
0.128
0.157
0.163

comp.
0.236
0.220
0.18

exp. gain
0.095
0.164
0.207

cos
lsfs
lscosfs

figure 4: average system performance. lsfs and lscosfs
runs are trained and evaluated on    rst sentences only (like the
cos system). unpenalized results are omitted for space but
the rankings are consistent.

miss
miss
lead
body
empty dupl. total
29.6% 68.7%
1.6% 0.1% 15,986
17.8% 39.4% 41.1% 1.7% 12,873
2.0% 0.9% 13,088
25.4% 71.7%
27.9% 70.8%
1.0% 0.2% 15,756
19.6% 55.3% 19.9% 5.1% 13,380
7.5% 1.2% 11,613
24.6% 66.7%

apsal
cos
lsfs
lscosfs
ls
lscos

figure 5: percent of errors made and total errors on test set.

ity threshold method as in cos. the threshold was also tuned
on the development set.

6 results
results for system runs are shown in figure 3. on average,
ls and lscos achieve higher f1 scores than the baseline
systems in both latency penalized and unpenalized evalua-
tions. for lscos, the difference in mean f1 score was sig-
ni   cant compared to all other systems (for both latency set-
tings).

apsal achieved the overall highest expected gain, par-
tially because it was the tersest system we evaluated. how-
ever, only cos was statistically signi   cantly worse than it on
this measure.

in comprehensiveness, ls recalls on average a    fth of the
nuggets for each event. this is even more impressive when
compared to the average number of updates produced by each
system (figure 3); while cos achieves similar comprehen-
siveness, it takes on average about 62% more updates than
ls and almost 400% more updates than lscos. the output
size of cos stretches the limit of the term    summary,    which
is typically shorter than 145 sentences in length. this is es-
pecially important if the intended application is negatively af-
fected by verbosity (e.g. crisis monitoring).

7 discussion
since cos only considers the    rst sentence of each docu-
ment, it may miss relevant sentences below the article   s lead.
in order to con   rm the importance of modeling the oracle, we
also trained and evaluated the ls based approaches on    rst
sentence only streams. figure 4 shows the latency penalized
results of the    rst sentence only runs. the ls approaches
still dominate cos and receive larger positive effects from
the latency penalty despite also being restricted to the    rst
sentence. clearly having a model (beyond similarity) of what
to select is helpful. ultimately we do much better when we
can look at the whole document.

we also performed an error analysis to further understand
how each system operates. figure 5 shows the errors made
by each system on the test streams. errors were broken down
into four categories. miss lead and miss body errors occur
when a system skips a sentence containing a novel nugget in
the lead or article body respectively. an empty error indicates
an update was selected that contained no nugget. duplicate
errors occur when an update contains nuggets but none are
novel.

overall, errors of the miss type are most common and sug-
gest future development effort should focus on summary con-
tent identi   cation. about a    fth to a third of all system error
comes from missing content in the lead sentence alone.

after misses, empty errors (false positives) are the next
largest source of error. cos was especially prone to empty er-
rors (41% of its total errors). ls is also vulnerable to empties
(19.9%) but after applying the similarity    lter and restricting
to    rst sentences, these errors can be reduced dramatically (to
1%).

surprisingly, duplicate errors are a minor issue in our eval-
uation. this is not to suggest we should ignore this compo-
nent, however, as efforts to increase recall (reduce miss er-
rors) are likely to require more robust redundancy detection.

8 conclusion
in this paper we presented a fully online streaming document
summarization system capable of processing web-scale data
ef   ciently. we also demonstrated the effectiveness of    learn-
ing to search    algorithms for this task. as shown in our error
analysis, improving the summary content selection especially
in article body should be the focus of future work. we would
like to explore deeper linguistic analysis (e.g. coreference
and discourse structures) to identify places likely to contain
content rather than processing whole documents.

9 acknowledgements
we would like to thank hal daum  e iii for answering our
questions about learning to search. the research described
here was supported in part by the national science founda-
tion (nsf) under iis-1422863. any opinions,    ndings and
conclusions or recommendations expressed in this paper are
those of the authors and do not necessarily re   ect the views
of the nsf.

references
[allan et al., 2001] james allan, rahul gupta, and vikas
khandelwal. temporal summaries of new topics. in pro-
ceedings of the 24th annual international acm sigir con-
ference on research and development in information re-
trieval, pages 10   18. acm, 2001.

[aslam et al., 2013] javed aslam, matthew ekstrand-
abueg, virgil pavlu, fernado diaz, and tetsuya sakai.
trec 2013 temporal summarization. in proceedings of the
22nd text retrieval conference (trec), november, 2013.
[chang et al., 2015] kai-wei chang, akshay krishna-
murthy, alekh agarwal, hal daume, and john langford.
learning to search better than your teacher.
in david
blei and francis bach, editors, proceedings of the 32nd
icml (icml-15), pages 2058   2066. jmlr workshop
and conference proceedings, 2015.

[conroy et al., 2011] john m conroy, judith d schlesinger,
jeff kubina, peter a rankel, and dianne p oleary. classy
2011 at tac: guided and multi-lingual summaries and eval-
uation metrics. in proceedings of the text analysis con-
ference, 2011.

[dang and owczarzak, 2008] hoa trang dang and karolina
owczarzak. overview of the tac 2008 update summariza-
tion task. in proceedings of text analysis conference, pages
1   16, 2008.

[daum  e iii et al., 2009] hal daum  e iii, john langford, and
daniel marcu. search-based id170. ma-
chine learning, 75(3):297   325, 2009.

[du et al., 2011] pan du, jipeng yuan, xianghui lin, jin
zhang, jiafeng guo, and xueqi cheng. decayed divrank
for guided summarization. in proceedings of text analysis
conference, 2011.

[erkan and radev, 2004] g  unes erkan and dragomir r
radev. lexrank: graph-based lexical centrality as salience
in text summarization. jair, pages 457   479, 2004.

[frank et al., 2012] john r frank, max kleiman-weiner,
daniel a roberts, feng niu, ce zhang, christopher r  e,
and ian soboroff. building an entity-centric stream    lter-
ing test collection for trec 2012. technical report, dtic
document, 2012.

[frey and dueck, 2007] brendan j frey and delbert dueck.
id91 by passing messages between data points. sci-
ence, 315(5814):972   976, 2007.

[graff and cieri, 2003] david graff and c cieri. english gi-

gaword corpus. linguistic data consortium, 2003.

[guo and diab, 2012] weiwei guo and mona diab. a sim-
ple unsupervised latent semantics based approach for sen-
tence similarity. in proceedings of the sixth international
workshop on semantic evaluation, semeval    12, pages
586   590, stroudsburg, pa, usa, 2012. association for
computational linguistics.

[guo et al., 2013] qi guo, fernando diaz, and elad yom-
tov. updating users about time critical events. in ecir,
ecir, pages 483   494, berlin, heidelberg, 2013. springer-
verlag.

[haghighi and vanderwende, 2009] aria haghighi and lucy
vanderwende.
exploring content models for multi-
document summarization. in proceedings of human lan-
guage technologies: the 2009 annual conference of the
north american chapter of the association for computa-
tional linguistics, pages 362   370. association for com-
putational linguistics, 2009.

[kedzie et al., 2015] chris kedzie, kathleen mckeown, and
fernando diaz.
predicting salient updates for disas-
in proceedings of the 53rd annual
ter summarization.
meeting of the association for computational linguis-
tics and the 7th international joint conference on natural
language processing, pages 1608   1617. association for
computational linguistics, july 2015.

[lin and hovy, 2000] chin-yew lin and eduard hovy. the
automated acquisition of topic signatures for text summa-
rization. in acl, pages 495   501. acl, 2000.

[mason and charniak, 2011] rebecca mason and eugene
charniak. extractive multi-document summaries should
explicitly not contain document-speci   c content. in pro-
ceedings of the workshop on id54 for
different genres, media, and languages, pages 49   54.
association for computational linguistics, 2011.

[mccreadie et al., 2014] richard mccreadie, craig mac-
donald, and iadh ounis. incremental update summariza-
tion: adaptive sentence selection based on prevalence and
novelty. in cikm, pages 301   310. acm, 2014.

[nenkova and mckeown, 2012] ani nenkova and kathleen
mckeown. a survey of text summarization techniques. in
mining text data, pages 43   76. springer, 2012.

[nenkova and vanderwende, 2005] a. nenkova and l. van-
derwende.
the impact of frequency on summariza-
tion. technical report msr-tr-2005-101, msr-tr-
2005-101, january 2005.

[radev et al., 2000] dragomir r radev, hongyan jing, and
malgorzata budzikowska. centroid-based summariza-
tion of multiple documents: sentence extraction, utility-
based evaluation, and user studies. in proceedings of the
2000 naacl-anlp workshop on automatic summariza-
tion. association for computational linguistics, 2000.

[ross et al., 2011] st  ephane ross, geoffrey j. gordon, and
drew bagnell. a reduction of imitation learning and struc-
tured prediction to no-regret online learning. in proceed-
ings of the fourteenth international conference on ais-
tats 2011, pages 627   635, 2011.

