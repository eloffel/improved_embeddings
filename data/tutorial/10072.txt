towards abstraction from extraction: multiple timescale gated

recurrent unit for summarization

minsoo kim

school of electronics engineering
kyungpook national university

daegu, south korea

moirangthem dennis singh

school of electronics engineering
kyungpook national university

daegu, south korea

minsoo9574@gmail.com

mdennissingh@gmail.com

minho lee

school of electronics engineering
kyungpook national university

daegu, south korea

6
1
0
2

 
l
u
j
 

4

 
 
]
l
c
.
s
c
[
 
 

1
v
8
1
7
0
0

.

7
0
6
1
:
v
i
x
r
a

mholee@gmail.com

abstract

in this work, we introduce temporal hi-
erarchies to the sequence to sequence
(id195) model to tackle the problem of
abstractive summarization of scienti   c ar-
ticles. the proposed multiple timescale
model of the gated recurrent unit (mt-
gru) is implemented in the encoder-
decoder setting to better deal with the
presence of multiple compositionalities in
larger texts. the proposed model is com-
pared to the conventional id56 encoder-
decoder, and the results demonstrate that
our model trains faster and shows signi   -
cant performance gains. the results also
show that the temporal hierarchies help
improve the ability of id195 models to
capture compositionalities better without
the presence of highly complex architec-
tural hierarchies.
introduction and related works

1
summarization has been extensively researched
over the past several decades. jones (2007) and
nenkova et al. (2011) offer excellent overviews
of the    eld. broadly, summarization methods
can be categorized into extractive approaches and
abstractive approaches (hahn and mani, 2000),
based on the type of computational task. extrac-
tive summarization is a selection problem, while
abstractive summarization requires a deeper se-
mantic and discourse understanding of the text, as
well as a novel text generation process. extractive
summarization has been the focus in the past, but
abstractive summarization remains a challenge.

recently, sequence-to-sequence (id195) re-
current neural networks (id56s) have seen wide

application in a number of tasks. such id56
encoder-decoders (cho et al., 2014; bahdanau et
al., 2014) combine a representation learning en-
coder and a id38 decoder to perform
mappings between two sequences. similarly, re-
cent works have proposed to cast summarization
as a mapping problem between an input sequence
and a summary sequence. recent successes such
as rush et al. (2015); nallapati et al. (2016) have
shown that the id56 encoder-decoder performs re-
markably well in summarizing short text. such
id195 approaches offer a fully data-driven solu-
tion to both semantic and discourse understanding
and text generation.

while id195 presents a promising way for-
ward for abstractive summarization, extrapolating
the methodology to other tasks, such as the sum-
marization of a scienti   c article, is not trivial. a
number of practical and theoretical concerns arise:
1) we cannot simply train id56 encoder-decoders
on entire articles: for the memory capacity of cur-
rent gpus, scienti   c articles are too long to be
processed whole via id56s. 2) moving from one
or two sentences, to several sentences or several
paragraphs, introduces additional levels of com-
positionality and richer discourse structure. how
can we improve the conventional id56 encoder-
decoder to better capture these? 3) deep learning
approaches depend heavily on good quality, large-
scale datasets. collecting source-summary data
pairs is dif   cult, and datasets are scarce outside
of the newswire domain.

in this paper, we present a    rst,

intermedi-
ate step towards end-to-end abstractive summa-
rization of scienti   c articles. our aim is to ex-
tend id195 based summarization to larger text
with a more complex summarization task. to ad-

dress each of the issues above, 1) we propose a
paragraph-wise summarization system, which is
trained via paragraph-salient sentence pairs. we
use term frequency-inverse document frequency
(tf-idf) (luhn, 1958; jones, 1972) scores to ex-
tract a salient sentence from each paragraph. 2)
we introduce a novel model, multiple timescale
gated recurrent unit (mtgru), which adds a
temporal hierarchy component that serves to han-
dle multiple levels of compositionality. this is
inspired by an analogous concept of temporal hi-
erarchical organization found in the human brain,
and is implemented by modulating different layers
of the multilayer id56 with different timescales
(yamashita and tani, 2008). we demonstrate that
our model is capable of understanding the seman-
tics of a multi-sentence source text and knowing
what is important about it, which is the    rst nec-
essary step towards abstractive summarization. 3)
we build a new dataset of computer science (cs)
articles from arxiv.org, extracting their introduc-
tions from the latex source    les. the introduc-
tions are decomposed into paragraphs, each para-
graph acting as a natural unit of discourse.

finally, we concatenate the generated summary
of each paragraph to create a non-expert summary
of the article   s introduction, and evaluate our re-
sults against the actual abstract. we show that
our model is capable of summarizing multiple sen-
tences to its most salient part on unseen data, fur-
ther supporting the larger view of summarization
as a id195 mapping task. we demonstrate that
our mtgru model satis   es some of the major re-
quirements of an abstractive summarization sys-
tem. we also report that mtgru has the capa-
bility of reducing training time signi   cantly com-
pared to the conventional id56 encoder-decoder.
the paper is structured as follows: section 2 de-
scribes the proposed model in detail. in section 3,
we report the results of our experiments and show
the generated summary samples. in section 4 we
analyze the results of our model and comment on
future work.

2 proposed model

in this section we discuss the background related
to our model, and describe in detail the newly de-
veloped architecture and its application to summa-
rization.

figure 1: a gated recurrent unit.

2.1 background
the principle of compositionality de   nes the
meaning conveyed by a linguistic expression as a
function of the syntactic combination of its con-
stituent units.
in other words, the meaning of
a sentence is determined by the way its words
are combined with each other. in multi-sentence
text, sentence-level compositionality (the way sen-
tences are combined with one another) is an ad-
ditional function which will add meaning to the
overall text. when dealing with such larger texts,
compositionality at the sentence and even para-
graph levels should be considered, in order to cap-
ture the text meaning completely. an approach ex-
plored in recent literature is to create dedicated ar-
chitectures in a hierarchical fashion to capture sub-
sequent levels of compositionality: li et al. (2015)
and nallapati et al. (2016) build dedicated word
and sentence level id56 architectures to capture
compositionality at different levels of text-units,
leading to improvements in performance.

however, architectural modi   cations to the
id56 encoder-decoder such as these suffer from
the drawback of a major increase in both train-
ing time and memory usage. therefore, we pro-
pose an alternative enhancement to the architec-
ture that will improve performance with no such
overhead. we draw our inspiration from neu-
roscience, where it has been shown that func-
tional differentiation occurs naturally in the human
brain, giving rise to temporal hierarchies (meunier
et al., 2010; botvinick, 2007).
it has been well
documented that neurons can hierarchically orga-
nize themselves into layers with different adapta-
tion rates to stimuli. the quintessential example of
this phenomenon is the auditory system, in which
syllable level information in a short time window
is integrated into word level information over a
longer time window, and so on. previous works
have applied this concept to id56s in movement
tracking (paine and tani, 2004) and speech recog-

xht-1xtztrtutx1-htx+(1)

rt =   (wxrxt + whrht   1)
zt =   (wxzxt + whzht   1)
ut = tanh(wxuxt + whu(rt (cid:12) ht   1))
ht = ((1     zt)ht   1 + ztut)
+ (1     1
  

1
  

)ht   1

(2)
the time constant    added to the activation ht
of the mtgru is shown in eq.(2).    is used to
control the timescale of each gru cell. larger   
meaning slower cell outputs but it makes the cell
focus on the slow features of a dynamic sequence
input. the proposed mtgru model is illustrated
in fig. 2. the conventional gru will be a special
case of mtgru where    = 1, where no attempt is
made to organize layers into different timescales.

  e
  ht   1

=

1
  

+

1
  

[((

+

[

  e
  ht
  e
  ht
1
  

[(((

(cid:12) (ut     ht   1) (cid:12)   (cid:48)(zt)wzh]
(cid:12) zt (cid:12) tanh(cid:48)(ut))wuh) (cid:12) rt]
(cid:12) zt (cid:12) tanh(cid:48)(ut))wuh)
  e
  ht
(cid:12)  (cid:48)(rt) (cid:12) ht   1)wrh]
  e
  ht

(cid:12) (1     zt)] + (1     1
  

)

+

1
  

[

  e
  ht

  e

(3)
eq. (3) shows the learning algorithm derived for
the mtgru according to the de   ned forward pro-
cess and the back propagation through time rules.
is the error of the cell outputs at time t     1
  ht   1
and   e
is the current gradient of the cell outputs.
  ht
different timescale constants are set for each layer
where larger    means slower context units and
   = 1 de   nes the default or the input timescale.
based on our hypothesis that later layers should
learn features that operate over slower timescales,
we set larger    as we go up the layers.

in this application, the question is whether the
word sequences being analyzed by the id56 pos-
sess information that operates over different tem-
poral hierarchies, as they do in the case of the con-
tinuous audio signals received by the human audi-
tory system. we hypothesize that they do, and that
word level, clause level, and sentence level com-
positionalities are strong candidates. in this light,
the multiple timescale modi   cation functions as a
way to explicitly guide each layer of the neural
network to facilitate the learning of features op-
erating over increasingly slower timescales, corre-

figure 2: proposed multiple timescale gated re-
current unit.

nition (heinrich et al., 2012).

2.2 multiple timescale gated recurrent unit

our proposed multiple timescale gated recur-
rent unit (mtgru) model applies the tempo-
ral hierarchy concept to the problem of id195
text summarization, in the framework of the id56
encoder-decoder. previous works such as (ya-
mashita and tani, 2008)   s multiple timescale
recurrent neural network (mtid56) have em-
ployed temporal hierarchy in motion prediction.
however, mtid56 is prone to the same problems
present in the id56, such as dif   culty in captur-
ing long-term dependencies and vanishing gradi-
ent problem (hochreiter et al., 2001). long short
term memory network (hochreiter et al., 2001)
utilizes a complex gating architecture to aid the
learning of long-term dependencies and has been
shown to perform much better than the id56 in
tasks with long-term temporal dependencies such
as machine translation (sutskever et al., 2014).
gated recurrent unit (gru) (cho et al., 2014),
which has been proven to be comparable to lstm
(chung et al., 2014), has a similar complex gating
architecture, but requires less memory. the stan-
dard gru architecture is shown in fig. 1.

because id195 summarization involves po-
tentially many long-range temporal dependencies,
our model applies temporal hierarchy to the gru.
we apply a timescale constant at the end of a gru,
essentially adding another constant gating unit that
modulates the mixture of past and current hidden
states. the reset gate rt, update gate zt, and the
candidate activation ut are computed similarly to
that of the original gru as shown in eq.(1).

xht-1xtztrtutx1-1/  htx+1-+xxid56 type layers hidden units
gru
mtgru

1792
1792

4
4

table 1: network parameters for each model.

sponding to subsequent levels in the compositional
hierarchy.

2.3 summarization
to apply our newly proposed multiple timescale
model to summarization, we build a new dataset
of academic articles. we collect latex source
   les of articles in the cs.{cl,cv,lg,ne} do-
mains from the arxiv preprint server, extracting
their introductions and abstracts. we decompose
the introduction into paragraphs, and pair each
paragraph with its most salient sentence as the tar-
get summary. these target summaries are gen-
erated using the widely adopted tf-idf scoring.
fig. 3 shows the structure of our summarization
model.

our dataset contains rich compositionality and
longer text sequences, increasing the complexity
of the summarization problem. the temporal hier-
archy function has the biggest impact when com-
plex compositional hierarchies exist in the input
data. hence, the multiple timescale concept will
play a bigger role in our context compared to
previous summarization tasks such as rush et al.
(2015).

the model using mtgru is trained using these
paragraphs and their targets. the generated sum-
maries of each introduction is evaluated using the
abstracts of the collected articles. we chose the
abstracts as gold summaries, because they usu-
ally contain important discourse structures such as
goal, related works, methods, and results, making
them good baseline summaries. to test the ef-
fectiveness of the proposed method, we compare
it with the conventional id56 encoder-decoder in
terms of training speed and performance.

3 experiments and results

we trained two id195 models, the    rst model us-
ing the conventional gru in the id56 encoder de-
coder, and the second model using the newly pro-
posed mtgru. both models are trained using the
same hyperparamenter settings with the optimal
con   guration which    ts our existing hardware ca-
pability.

following sutskever et al. (2014), the inputs are
divided into multiple buckets. both gru and mt-

figure 3: paragraph level approach to summariza-
tion.

train perplexity test perplexity

steps
74750
74750 mtgru

id56
gru

6.8
5.87

29.72
18.53

table 2: training results of the models.

gru models consist of 4 layers and 1792 hidden
units. as our models take longer input and tar-
get sequence sizes, the hidden units size and num-
ber of layers are limited. an embedding size of
512 was used for both networks. the timescale
constant    for each layer is set to 1, 1.25, 1.5, 1.7,
respectively. the models are trained on 110k text-
summary pairs. the source text are the paragraphs
extracted from the introduction of academic ar-
ticles and the targets are the most salient sen-
tence extracted from the paragraphs using tf-idf
scores. for comparison of the training speed of the
models, fig. 4 shows the plot of the training curve
until the train perplexity reaches 9.5. both of the
models are trained using 2 nvidia ge-force gtx
titan x gpus which takes roughly 4 days and 3
days respectively. during test, greedy decoding
was used to generate the most likely output given
a source introduction.

for evaluation, we adopt the recall-oriented
understudy for gisting evaluation (id8) met-
rics (lin, 2004) proposed by lin and hovy (2003).
id8 is a recall-oriented measure to score sys-
tem summaries which is proven to have a strong
correlation with human evaluations. it measures

evaluation metric recall
0.48135
0.32399
0.46588

id8-1
id8-2
id8-l

precision
0.59030
0.39505
0.57218

f   score
0.50835
0.34089
0.49234

table 3: id8 scores of gru model

summarymtgru model introductionparagraph 1paragraph 2paragraph nsummary 1summary 2summary nslow context unitsfast context unitsslowest context unitsslower context unitsevaluation metric recall
0.50901
0.34148
0.49406

id8-1
id8-2
id8-l

precision
0.61571
0.40824
0.59830

f   score
0.53870
0.35925
0.52318

table 4: id8 scores of mtgru model

figure 6: an example of the output summary vs
the extracted targets

4 discussion and future work

the id8 scores obtained for the summariza-
tion model using gru and mtgru show that the
multiple timescale concept improves the perfor-
mance of the conventional id195 model without
the presence of highly complex architectural hier-
archies. another major advantage is the increase
in training speed by as much as 1 epoch. more-
over, the sample summary shown in fig. 5 demon-
strates that the model has successfully generalized
on the dif   cult task of summarizing a large para-
graph into a one line salient summary.

in setting the    timescale parameters, we fol-
low (yamashita and tani, 2008) . we gradually
increase    as we go up the layers such that higher
layers have slower context units. moreover, we
experiment with multiple settings of    and com-
pare the training performance, as shown in fig.
7. the    of mtrgu-2 and mtrgu-3 are set as
{1, 1.42, 2, 2.5} and {1, 1, 1.25, 1.25}, respec-
tively. mtgru-1 is the    nal model adopted in
our experiment described in the previous section.
mtgru-2 has comparatively slower context lay-
ers and mtgru-3 has two fast and two slow con-
text layers. as shown in the comparison, the train-
ing performance of mtrgu-1 is superior to the
remaining two, which justi   es our selection of the
timescale settings.

the results of our experiment provide evidence
that an organizational process akin to functional
differentiation occurs in the id56 in language
tasks. the mtgru is able to train faster than the
conventional gru by as much as 1 epoch. we
believe that the mtrgu expedites a type of func-
tional differentiation process that is already ocur-
ring in the id56, by explicitly guiding the lay-
ers into multiple timescales, where otherwise this
temporal hierarchical organization occurs more
gradually.

figure 4: comparison of training speed between
gru and mtgru.

the id165 recall between the candidate summary
and gold summaries. in this work, we only have
one gold summary which is the abstract of an ar-
ticle, thus the id8 score is calculated as given
in li et al. (2015). id8-1, id8-2 and
id8-l are used to report the performance of
the models. for the performance evaluation, both
the models are trained up to 74750 steps where
the training perplexity of gru and mtgru are
shown in table 2. this step was chosen as the
early stopping point as at this step we get the low-
est test perplexity of the gru model. the id8
scores calculated using these trained networks are
shown in table 3 and table 4 for the gru and mt-
gru models, respectively. a sample summary
generated by the mtgru model is shown in fig.
5.

figure 5: an example of the generated summary
with mtgru.

number of steps6250 11750172502275028250337503925044750502505575061250perplexity0102030405060708090100110120130140150training speed comparisoid4grugruinput text:the input is the introduction of this paper.generated summary:1. summarization has been the topic explored as a challenge of text semantic understanding2. recently , _unk neural networks have emerged as a success in wide range of practical problems3. in particular , we need to use a new way to evaluate three important questions into the algorithms4. we use a concept to define the temporal hierarchy of each sentence in the context of paragraph5. we demonstrate that our model outperforms a conventional  _unk system and significantly lead to optimize6. in section # , we evaluate the experimental results on our model and evaluate our results in section #the paper is structured as follows: section 2 describes the related works. section 3 describes the data collection and processing steps. section 4 describes the proposed models in detail. in section 5, we report the results of our experiments and show the sample generated summaries. in section 6 we analyze the results of our models.section describes the data collection, models and the experimental results.in section 5, we report the results of our experiments and show the sample generated summaries.mtgru output summaryinputtf-idf extracted summaryhas already been shown, implicitly, in previous
works such as rush et al. (2015; nallapati et al.
(2016), but is made explicit in our work due to
our choice of data consisting of paragraph-salient
sentence pairs.
secondly, our results indicate
that probabilistic language models can solve the
task of novel word generation in the summariza-
tion setting, meeting a key criteria of abstractive
summarization. bengio et al. (2003) originally
demonstrated that probabilistic language models
can achieve much better generalization over simi-
lar words. this is due to the fact that the probabil-
ity function is a smooth function of the word em-
bedding vectors. since similar words are trained
to have similar embedding vectors, a small change
in the features induces a small change in the pre-
dicted id203. this makes a strong case for
id56 language models as the best available so-
lution for abstractive summarization, where it is
necessary to generate novel sentences. for ex-
ample, in fig. 5, the    rst summary shows that
our model generates the word    explored    which is
not present in the paper. furthermore, our results
suggest that if given abstractive targets, the same
model could train a fully abstractive summariza-
tion system.

in the future, we hope to explore the organi-
zational effect of the mtgru in different tasks
where temporal hierarchies can arise, as well
as investigating ways to effectively optimize the
timescale constant. finally, we will work to move
towards a fully abstractive end-to-end summariza-
tion system of multi-paragraph text by utilizing a
more abstractive target which can potentially be
generated with the help of the abstract from the
articles.

5 conclusion

in this paper, we have demonstrated the capabil-
ity of the mtgru in the multi-paragraph text
summarization task. our model ful   lls a funda-
mental requirement of abstractive summarization,
deep semantic understanding of text and impor-
tance identi   cation. the method draws from a
well-researched phenomenon in the human brain
and can be implemented without any hierarchical
architectural complexity or additional memory re-
quirements during training. although we show
its application to the task of capturing composi-
tional hierarchies in text summarization only, mt-
gru also shows the ability to enhance the learning

figure 7: comparison of training performance
between multiple time constants.

in fig. 6, we show the comparison of a gen-
erated summary of the input paragraph to an ex-
tracted summary. as seen in the example, our
model has successfully extracted the key infor-
mation from multiple sentences and reproduces
it into a single line summary. while the sys-
tem was trained only on the extractive summary,
the abstraction of the entire paragraph is possi-
ble because of the generalization capability of
our model. the id195 objective maximizes
the joint id203 of the target sequence con-
ditioned on the source sequence. when a sum-
marization model is trained on source-extracted
salient sentence target pairs, the objective can be
viewed as consisting of two subgoals: one is to
correctly perform saliency    nding (importance ex-
traction) in order to identify the most salient con-
tent, and the other is to generate the precise order
of the sentence target. in fact, during training, we
observe that the optimization of the    rst subgoal
is achieved before the second subgoal. the second
subgoal is fully achieved only when over   tting oc-
curs on the training set. the generalization capa-
bility of the model is attributable to the fact that
the model is expected to learn multiple points of
saliency per given paragraph input (not only a sin-
gle salient section corresponding to a single sen-
tence) as many training examples are seen. this
explains how the results such as those in fig. 6
can be obtained from this model.

we believe our work has some meaningful im-
plications for id195 abstractive summarization
going forward. first, our results con   rm that it
is possible to train an encoder-decoder model to
perform saliency identi   cation, without the need
to refer to an external corpus at test time. this

number of steps550011250170002275028500342504000045750515005725063000perplexity0102030405060708090100110120130140150160170180190200210220comparison of multiple timescalesmtgru-1mtgru-2mtgru-3speed thereby reducing training time signi   cantly.
in the future, we hope to extend our work to a
fully abstractive end-to-end summarization system
of multi-paragraph text.

acknowledgment

this research was supported by basic science
research program through the national re-
search foundation of korea(nrf) funded by
the ministry of science, ict and future plan-
ning(2013r1a2a2a01068687) (50%), and by the
industrial strategic technology development pro-
gram (10044009) funded by the ministry of trade,
industry and energy (motie, korea) (50%).

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
corr, abs/1409.0473.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian jauvin. 2003. a neu-
journal of ma-
ral probabilistic language model.
chine learning research, 3(feb):1137   1155.

[botvinick2007] matthew m botvinick. 2007. mul-
tilevel structure in behaviour and in the brain: a
model of fuster   s hierarchy. philosophical trans-
actions of the royal society b: biological sciences,
362(1485):1615   26, september.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, c   aglar g  ulc  ehre, fethi bougares, holger
schwenk, and yoshua bengio.
learn-
ing phrase representations using id56 encoder-
decoder for id151. corr,
abs/1406.1078.

2014.

[chung et al.2014] junyoung chung, c   aglar g  ulc  ehre,
kyunghyun cho, and yoshua bengio. 2014. em-
pirical evaluation of gated recurrent neural networks
on sequence modeling. corr, abs/1412.3555.

[hahn and mani2000] udo hahn and inderjeet mani.
2000. the challenges of id54.
computer, 33(11):29   36, november.

[heinrich et al.2012] stefan heinrich, cornelius we-
ber, and stefan wermter, 2012. arti   cial neural
networks and machine learning     icann 2012:
22nd international conference on arti   cial neu-
ral networks, lausanne, switzerland, september
11-14, 2012, proceedings, part i, chapter adap-
tive learning of linguistic hierarchy in a multiple
timescale recurrent neural network, pages 555   
562. springer berlin heidelberg, berlin, heidel-
berg.

[hochreiter et al.2001] sepp hochreiter, yoshua ben-
gio, and paolo frasconi. 2001. gradient    ow in
recurrent nets: the dif   culty of learning long-term
dependencies.
in j. kolen and s. kremer, edi-
tors, field guide to dynamical recurrent networks.
ieee press.

[jones1972] karen sparck jones. 1972. a statistical
interpretation of term speci   city and its application
in retrieval. journal of documentation, 28(1):11   
21.

[jones2007] karen sparck jones. 2007. automatic
summarising: the state of the art. information pro-
cessing and management: an international journal,
43(6):1449   1481.

[li et al.2015] jiwei li, minh-thang luong, and dan
2015. a hierarchical neural autoen-
corr,

for paragraphs and documents.

jurafsky.
coder
abs/1506.01057.

[lin and hovy2003] chin-yew lin and eduard hovy.
2003. automatic evaluation of summaries using n-
in proceedings of
gram co-occurrence statistics.
the 2003 conference of the north american chap-
ter of the association for computational linguistics
on human language technology-volume 1, pages
71   78. association for computational linguistics.

[lin2004] chin-yew lin. 2004. id8: a package for
automatic evaluation of summaries. in text summa-
rization branches out: proceedings of the acl-04
workshop, volume 8.

[luhn1958] h. p. luhn. 1958. the automatic creation
of literature abstracts. ibm j. res. dev., 2(2):159   
165, april.

[meunier et al.2010] d. meunier, r. lambiotte, a. for-
nito, k. d. ersche, and e. t. bullmore. 2010. hi-
erarchical modularity in human brain functional net-
works. arxiv e-prints, april.

[nallapati et al.2016] ramesh nallapati, bing xiang,
and bowen zhou.
2016. sequence-to-sequence
id56s for text summarization. 4th international con-
ference on learning representations - workshop
track (iclr 2016).

[nenkova et al.2011] ani nenkova, sameer maskey,
and yang liu. 2011. id54.
in proceedings of the 49th annual meeting of the
association for computational linguistics: tutorial
abstracts of acl 2011, hlt    11, pages 3:1   3:86,
stroudsburg, pa, usa. association for computa-
tional linguistics.

[paine and tani2004] rainer w. paine and jun tani.
motor primitive and sequence self-
2004.
organization in a hierarchical recurrent neural net-
work. neural networks, 17(89):1291     1309. new
developments in self-organizing systems.

[rush et al.2015] alexander m. rush, sumit chopra,
and jason weston. 2015. a neural attention model
for sentence summarization. in proceedings of the
2015 conference on empirical methods in natural
language processing, pages 379   389. association
for computational linguistics, lisbon, portugal.

2014.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
sequence to sequence
and quoc v le.
learning with neural networks.
in z. ghahramani,
m. welling, c. cortes, n. d. lawrence, and k. q.
weinberger, editors, advances in neural informa-
tion processing systems 27, pages 3104   3112. cur-
ran associates, inc.

[yamashita and tani2008] yuichi yamashita and jun
tani.
2008. emergence of functional hierarchy
in a multiple timescale neural network model: a
humanoid robot experiment. plos comput biol,
4(11):1   18, 11.

